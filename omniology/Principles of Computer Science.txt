Principles of
Computer Science

Principles of 
Computer Science


Principles of  
Computer Science
Editor
Donald R. Franceschetti, PhD
The University of Memphis
SALEM PRESS
A Division of EBSCO Information Services, Inc.
Ipswich, Massachusetts
GREY HOUSE PUBLISHING

Copyright © 2016, by Salem Press, a Division of EBSCO Information Services, Inc., and Grey House Publishing, Inc.
All rights reserved. No part of this work may be used or reproduced in any manner whatsoever or transmitted in 
any form or by any means, electronic or mechanical, including photocopy, recording, or any information ­storage 
and retrieval system, without written permission from the copyright owner.  For permissions requests, contact 
­proprietarypublishing@ebsco.com.
∞ The paper used in these volumes conforms to the American National Standard for Permanence of Paper for Printed 
Library Materials, Z39.48‑1992 (R2009)
Publisher’s Cataloging-In-Publication Data
(Prepared by The Donohue Group, Inc.)
Names: Franceschetti, Donald R., 1947- editor.
Title: Principles of computer science / editor, Donald R. Franceschetti, PhD, the University of Memphis.
Description: [First edition]. | Ipswich, Massachusetts : Salem Press, a division of EBSCO Information Services, Inc. ; 
[Amenia, New York] : Grey House Publishing, [2016] | Series: Principles of | Includes bibliographical references 
and index.
Identifiers: ISBN 978-1-68217-139-4 (hardcover)
Subjects: LCSH: Computer science.
Classification: LCC QA76 .P75 2016 | DDC 004—dc23
Printed in the United States of America

v
Contents
Publisher's Note. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . vii
Editor's Introduction. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . ix
Contributors. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . xiii
3-D printing. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 1
Agile robotics. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 4
ALGOL. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 6
Algorithms. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 9
Android OS . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 11
Application. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 13
Applied linguistics. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 16
Architecture software. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 18
ASCII. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 21
Assembly Language. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 23
Autonomic computing . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 25
Avatars and simulation. . . . . . . . . . . . . . . . . . . . . . . 28
BASIC. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 31
Binary/hexadecimal representations. .  .  .  .  .  .  .  .  .  .  . 34
Biochemical engineering . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 38
Biomedical engineering . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 40
Biometrics. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 43
Biotechnology . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 46
C++. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 49
CAD/CAM. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 52
Cloud computing. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 54
Combinatorics. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 56
Communication technology. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 58
CompTIA A+ certification. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 60
Computer animation. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 62
Computer Circuitry: Flip flops. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 65
Computer Circuitry: Semiconductors . .  .  .  .  .  .  .  .  .  . 67
Computer memory . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 69
Computer modeling . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 71
Computer programming: Image editing	����������������
73
Computer programming: Music editing 	����������������
76
Computer programming: Video editing 	����������������
78
Computer security. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 81
Computer-assisted instruction. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 83
Connection machine. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 86
Constraint programming. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 87
Control systems . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 90
CPU design. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 92
Cryptography. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 95
Deadlock. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 98
Debugging. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 100
Demon dialing/war dialing. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 102
Device drivers. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 104
Digital citizenship . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 106
Digital forensics. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 108
Digital signal processors . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 110
Digital watermarking. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 112
Dirty paper coding. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 115
DOS. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
Drones . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 119
Electronic circuits . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 122
Electronic communication software. .  .  .  .  .  .  .  .  .  .  . 125
Electronic waste. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 127
Encryption. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 130
Firewalls. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 133
Firmware. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 136
Fitbit. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 138
FORTRAN . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 140
Functional design. . . . . . . . . . . . . . . . . . . . . . . . . . 143
Game programming . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 145
Graphical user interface. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 147
Graphics formats. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 149
Green computing. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 152
Information technology . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 154
Integrated development environments. .  .  .  .  .  .  .  . 156
Intelligent tutoring system . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 159
Internet privacy. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 161
iOS . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 164
LISP. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
Malware . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 170
Medical technology. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 172
Mesh networking. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 175
Metacomputing. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 178
Microprocessors. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 180
Microscale 3-D printing. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 183
Mobile apps. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 185
Mobile operating systems . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 188
Molecular Computers. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 190
Motherboards. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 192

Contents
Principles of Physics
vi
Multiprocessing operating systems. .  .  .  .  .  .  .  .  .  .  .  . 194
Multitasking operating systems. .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 196
Multithreading operating systems. .  .  .  .  .  .  .  .  .  .  .  .  . 199
Multitouch displays. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 201
Multi-user operating systems. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 203
Natural language processing. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 206
Networking: routing and switches . .  .  .  .  .  .  .  .  .  .  .  . 208
Neural networks. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 211
Neuromorphic chips. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 214
Object-oriented design. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 217
Parallel processors. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 220
Personal health monitor technology. .  .  .  .  .  .  .  .  .  . 223
Personalized medicine. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 226
Privacy regulations. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 228
Programming languages. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 230
PROLOG . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 233
Quantum computers. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 235
Quantum computing. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 238
Random-access memory. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 240
Removable memory. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 242
Scaling systems. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 245
Signal processing. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 248
Smart homes . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 251
Software architecture. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 254
Software regulations . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 256
Software testing. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 259
Software-defined radio . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 262
Speech-recognition software. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 264
Turing machine. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 267
Turing test . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 270
Unicode. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 272
UNIX . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 275
Web design programming tools. .  .  .  .  .  .  .  .  .  .  .  .  .  . 278
Web graphic design. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 281
Windows operating system . .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 284
Wireframes. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 286
Wireless networks. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 289
Workplace monitoring. .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  . 292
Appendices
History of Events Leading up to the  
Development of Modern Computers����������������
295
Timeline of Microprocessors����������������������������������
307
The Pioneers of Computer Science����������������������
309
Glossary ������������������������������������������������������������������
315
Bibliography������������������������������������������������������������
337
Index������������������������������������������������������������������������
361

vii
Publisher's Note
Salem Press is pleased to add Principles of Computer 
Science as the fourth title in a new Principles of se-
ries that includes Chemistry, Physics, Astronomy, and 
Computer Science. This new resource introduces stu-
dents and researchers to the fundamentals of com-
puter science using easy-to-understand language, 
giving readers a solid start and deeper understanding 
and appreciation of this complex subject.
The 117 entries range from 3D printing to Workplace 
Monitoring and are arranged in an A to Z order, 
making it easy to find the topic of interest. Entries 
include the following:
• Related fields of study to illustrate the connec-
tions between the various branches of computer 
science including computer engineering, soft-
ware engineering, biotechnology, security, robot-
ics, gaming, and programming languages;
• A brief, concrete summary of the topic and how 
the entry is organized;
• Principal terms that are fundamental to the dis-
cussion and to understanding the concepts 
presented;
• Illustrations that clarify difficult concepts via mod-
els, diagrams, and charts of such key topics as 
wide area networks (WAN), electronic circuits, 
and quantum computing;
• Photographs of significant contributors to the 
field of computer science;
• Sample problems that further demonstrate the 
concept presented;
• Further reading lists that relate to the entry.
This reference work begins with a comprehen-
sive introduction to the field, written by editor  
Donald R. Franceschetti, PhD, starting with the 
development of the first computers, an explana-
tion of analog versus digital computing, neural 
networks, the contributions of Alan Turing, and a 
discussion of how advances from vacuum tubes to 
digital processors have fundamentally changed the 
way we live.
The book’s backmatter is another valuable resource 
and includes:
• A timeline of the developments that related to 
and led up to the first modern computer, starting 
in 2400 BCE with the invention of the abacus in 
Babylonia and ending with the 1949 Popular Me-
chanics prediction that computers of the future 
might weigh no more than 1.5 tons;
• A time of the development of microprocessors 
from the Intel 4004 in 1971 to the IBM POWER8 
in 2014; 
• Nobel Notes that explain the significance of the 
prizes in physics to the study of the science and 
• List of important figures in computer science and 
their key accomplishment;
• Glossary;
• General bibliography; and
• Subject index.
Salem Press and Grey House Publishing extend their 
appreciation to all involved in the development 
and production of this work. The entries have been 
written by experts in the field. Their names and affili-
ations follow the Editor’s Introduction.
Principles of Computer Science, as well as all Salem Press 
reference books, is available in print and as an e-
book. Please visit www.salempress.com for more 
information.


ix
What is Computer Science?
Computer science generally refers to the body of 
knowledge that allows humans to use mechanical 
aids to do mathematical calculations very efficiently. 
While the term computer originally referred to a 
human who was able to do simple arithmetic quickly 
and accurately, the name has been transferred to pro-
grammable automata, which are able to do the same 
thousands to billions of times faster.
Analogue or Digital
There are two basic kinds of computers: analogue 
and digital. In an analog process, the system of in-
terest is modeled by a system obeying the same dy-
namical laws. Thus a system of mechanical vibrations 
might be modeled by a system of capacitors, induc-
tors, and resistors. In a digital computer, the quantity 
of interest is generally encoded in some way and one 
obtains coded representation of the solution. Some 
of the advances are fairly mundane and are actually 
possible to achieve with low-tech adding machines, 
if one has enough time. Others are a bit more eso-
teric, requiring some abstract mathematics.  Almost 
all computing machines, with the notable exceptions 
of the abacus and slide rule, have come into existence 
since the beginning of the twentieth century and rest 
on the fundamental discoveries made about electro-
magnetism since 1800, as well as more recent insights 
gained into the process of computation.
Digital Computation Requires a Code
Digital computation begins with the selection of 
a code so that information can be stored and sent 
over significant distances. It can be difficult for those 
living in the twenty-first century to appreciate the full 
impact of the development of telegraphy, by which 
information (letters and numbers) could be trans-
mitted from one place to another as a series of dots 
and dashes, using a pair of telegraph wires strung be-
tween two cities along with the necessary relay equip-
ment. For the first time, the results of a baseball game 
or a battle or the day’s stock trading could be known 
almost instantly. It’s not surprising that so many in-
novators in the area of information exchange began 
their careers as telegraph operators.
The earliest innovations in telegraphy greatly in-
creased the number of messages that could be sent si-
multaneously. It was eventually learned that by using 
more than two conductors, several symbols could be 
sent at the same time. Of course, it should be noted 
that the original Morse code used to tap out messages 
with a series of dots and dashes was highly inefficient 
and error prone. Suppose seven wires carried voltage 
simultaneously. With seven voltages, one can encode 
27 =128 symbol choices, a calculation that accounts 
for the digits of the decimal system plus upper and 
lower case letters plus several punctuation marks. 
The telegraph thus becomes a teletype machine ca-
pable of transmitting information a hundred times 
faster.
Once the process of telegraphy was in place, it was 
a simple matter to switch from the network of tele-
graph stations to radio. Once amplitude modulation 
was discovered voice could be transmitted over the 
air or over wires with ease. Television eventually fol-
lowed. Many of the pioneers of electronic technology 
were self-educated men—men like Thomas Edison, 
and Michael Faraday before him—who could read 
well enough but functioned outside the establish-
ment of colleges and academic degrees. Even today 
there is some controversy over the value of traditional 
education as many pioneers of the computer sciences 
dropped their pursuit of traditional degrees, such as 
Bill Gates and Steve Jobs, while means that the ques-
tion still remains open.
Still, formal education came to be an important 
aspect of the inexorable march toward the computer 
age. In the United Kingdom, while social standing may 
have prevent many from attending a university enroll-
ment, entire institutions were set up to teach those 
who had not the pedigree to be accepted at Oxford 
or Cambridge, notably the Royal Institution, a school 
operating under Royal charter and featuring both 
Sir Humphrey Davy and Michael Faraday (who both 
declined to be knighted) as lecturers. Things were 
somewhat more democratic in the United States, with 
public universities in almost every state that were sup-
ported by agriculture and expected to make a contri-
bution in return. Private institutions set up chapters 
of Phi Beta Kappa to encourage traditional study of 
Editor's Introduction

x
Latin and Greek, while Institutes of Technology like 
the Massachusetts Institute of Technology offered de-
grees in the Sciences and Engineering. 
One of the important areas of computer science is 
the selection and security of codes. An in-depth treat-
ment of the topic is provided by Claude Shannon and 
Warren Weaver in two papers originally published 
in Scientific American and in the Bell System Technical 
Journal. The very existence of a widely respected sci-
entific journal published not by a non-profit orga-
nization, but rather as a house organ of Bell System 
shows how the system of scientific publication had to 
adapt to the emergence of a very large corporation 
that of necessity has control of a major portion of the 
work to be done. 
Shannon begins by defining the entropy of a 
text in terms of the probabilities of the symbols. 
Scrambling the symbols in a text corresponds to an 
increase in entropy and the probabilities of certain 
errors. Shannon and others have made very effec-
tive use of the entropy concept, which was originally 
proposed by Ludwig Boltzmann in the context of the 
molecular theory of heat. Among other things, the 
creation of information results in the generation of 
heat and so one of the major technical issues in the 
design of highly powerful computers is the need to 
transport the heat away as it is generated.
The Development of the Digital Computer
In 1900, the great German mathematician David 
Hilbert gave a talk at the Second International 
Conference of Mathematicians in Paris, France, 
during which he described twenty-three math-
ematical problems that he expected to be solved 
in the twentieth century. His tenth problem dealt 
with polynomial equations with integer coefficients 
(Diophantine equations). Hilbert asked if it could be 
determined in a finite number of steps whether the 
equation could be solved in rational numbers. The 
matter in question was not to find the solution, but 
simply to find out if a solution could be found.
One of the world’s true polymaths, Alan 
Turing, took on the challenge of solving Hilbert’s 
problem–the 
so-called 
“decision 
problem” 
or 
Entscheidungsproblem. To solve this problem, Turing 
looked closely at the process of solution. He begins 
by looking at a solution as a practical problem, in 
other words, as the sequence of steps that a mathe-
matician or a young child might go through solving 
an arithmetic problem by working out the solution 
one digit at a time. It is a truism that the solution to 
practical problems is sometimes found by “thinking 
outside the box” and considering mechanisms that 
might seem to have nothing to do with the problem 
at hand. In the process of solving Hilbert’s decision 
problem, Turing conceived of a machine that could 
read and write one digit at a time, based on instruc-
tions that could be written on the same tape that 
would carry the solution—known today as the Turing 
Machine.
It has been suggested by a number of Turing 
biographers that since Alan enjoyed working with 
actual typewriters, it was in fact his practical bent 
that led him to conceptualize the Turing machine 
and then the universal Turing machine. The set 
of instructions constitutes the computer program. 
The numbers written on the tape at the start of the 
tape constitute the input or data and the numbers 
written on the tape after the computer is done are 
the output. Numbers which might be written by such 
a machine are called computable numbers. Turing 
published his first paper “On Computable Numbers 
with an application to the Entscheidungsproblem” in 
1937. This paper marks the theoretical develop-
ment of the digital computer.
Turing received his PhD in mathematics in 1938 
and by 1939 was at work as a cryptographer in the 
British Foreign Office. It was during this period of 
his life—which became the subject of the Academy 
Award-winning movie, The Imitation Game—that he 
was able to apply his efforts to produce the “enigma” 
coding machine capable of breaking the German 
code and ultimately, saving numerous lives.
It turns out that one can write a program capable 
of accepting the description of another Turing ma-
chine as data and then emulating it. Such programs 
are called universal Turing machines. The modern 
programmable digital computer is, from an abstract 
point of view, just such a machine. As one of the first 
individuals to take the possibility of artificial intelli-
gence seriously, Turing proposed a test, known today 
as the Turing test, to tell whether a machine could be 
considered intelligent.
Background of the Turing Machine
The digital computer has undergone many changes 
since Turing first developed it to deal with the ur-
gencies of the Second World War. Today’s digital 

xi
computer as a practical device is the result of devel-
opments in a large number of fields.
The nineteenth century had been a trying time 
for the natural sciences. While the physics of 1900 
left little room for new discoveries, the discovery 
of subatomic particles like the electron and the 
nuclear structure of the atom called for a funda-
mental reexamination of assumptions not chal-
lenged since the time of Isaac Newton. Einstein had 
opened the possibility that the geometry of space 
was not that proposed by Euclid over two thousand 
years ago. Philosophically-minded scientists turned 
to mathematics for sure and certain knowledge. Of 
all areas of mathematics, the most noncontrover-
sial was the theory of arithmetic, in which there was 
only one item of unfinished business: the principle 
of mathematical induction, which seemed to be an 
unnecessary part of the structure of mathematics. 
This principle stated that if N was an integer, and 
if P(N) was a true statement about N that implied 
the truth of the statement P(N+1), then P(N) was 
true for all N. A number of philosophers of math-
ematics went to work trying to show that math-
ematics could be constructed in a self- consistent 
way without the troublesome postulate.
By far the most thoroughgoing approach to 
eliminating the principle of mathematical induc-
tion could be found the three-volume Principia 
Mathematica, published in 1911 (and still in print!) 
by Bertrand Russell and Alfred North Whitehead. 
Although a ­fallacy inherent in Principia Mathematica 
was soon discovered, the book remains a valuable 
­exposition of symbolic logic. In 1931 Kurt Godel 
,who would later become a very close friend of Albert 
Einstein published a paper “On formally undecid-
able systems…” that put an end to the matter by 
showing that in any system of axioms for arithmetic 
that allowed multiplication, there would necessarily 
be propositions which could not be decided within 
the system.
Neural Networks
In 1943, however, Warren S. McCulloch and Walter 
Pitts published “A logical calculus of the ideas im-
manent in nervous activity” showing that for each 
formula in the notation of Principia Mathematica, an 
equivalent network of all or none neurons could be 
found. Work on neural networks has continued to 
this day. 
Von Neumann’s Contributions
Von Neumann was a very unusual physicist, one of a 
group of immigrants known collectively as “Martians” 
because their Hungarian speech resembled no 
known European language; other Martians included 
Edward Teller and Leo Szilard. Unlike the majority 
of academics, he had adequate financial means and 
was able to attend classes at several universities. He 
did not shy away from positions of political and social 
influence, even accepting President Eisenhower’s 
appointment as Commissioner of Atomic Energy. 
While his PhD degree was given for a thesis on the 
“Mathematical Foundations of Quantum Mechanics,” 
he was also interested in practical electronics. As a 
member of the Institute for Advanced Studies at 
Princeton New Jersey, he oversaw the construction of 
one of the first vacuum tube computers. In the last 
years of his life, Dr. John von Neumann prepared to 
give the Silliman lectures at Yale University, his topic: 
The Computer and the Brain. While he was unable to 
deliver the Silliman lectures for health reasons, the 
manuscript was eventually published,
The First Electronic Computers
Prepared with a basic understanding of neural net-
works as devices that model human thought, we 
turn now to the problems of electronic devices that 
can undertake computation. The essential require-
ment is for a two-state (or greater) device. The 
Edison effect, observed by Thomas Edison in 1883, 
provided the basis for the first such devices. Edison 
had noted the emission of electrons from a heated 
metal electrode when maintained in vacuum, but he 
merely noted the effect in his laboratory notebook. 
Eventually, however, the British engineer, Sir John 
Ambrose Fleming, used a heated cathode in the first 
vacuum tube diode, also called a rectifier since it only 
allowed electron flow from the heated cathode. An 
American inventor, Lee de Forest, added a third ele-
ment, or grid, that allowed a small voltage to control 
a much larger current.
The Vacuum Tube Era
The first electronic computers employed vacuum 
tube circuitry and were not always dependable. 
Vacuum tubes contained a heated element, or fila-
ment, that was essential to its proper function. Once 
the filament burned out, the tube would not conduct 
when it should. There were two possible approaches 

xii
to solving the problem. One could design a test calcu-
lation that would use every tube in the computer and 
for which the result is known. If one ran the test cal-
culation before and after a new calculation was run 
and obtained the same results, one could be relatively 
sure that no tubes had burned out during the calcula-
tion. An alternative suggested by von Neumann was 
to build redundancy into the circuit so that the re-
sults obtained could be trusted even if a few compo-
nents proved unreliable.
The Transistor Era: Computers become 
Profitable
While there were fortunes to be made making busi-
ness machines, the computer industry offered new 
opportunities to creative individuals. As semicon-
ductor devices replaced vacuum tubes, it became 
clear that a few creative individuals could compete 
effectively against older and well-established corpo-
rations. While there was still a need for large main-
frame computers, the personal computer had the 
virtues of both versatility and low cost. The leaders 
in the field became the hobbyists, many of them still 
in high school, who established Microsoft and Apple 
Computer. Bill Gates, founder of Microsoft, would be-
come one of the world’s richest men. Steve Jobs, co-
founder of Apple Computer, brought the standalone 
microcomputer to new levels of competitiveness. 
Facebook’s evolution created an increasing appetite 
for social media. New mechanisms of marketing were 
found. The internet, first started in order to connect 
a few physics research laboratories, became a “place” 
where all manner of goods and services could be 
bought and sold. 
The Future
Clearly computer science is in the midst of a revolu-
tion of its own making. It is not possible to predict 
where or when things will “settle down,” or if they 
ever actually will. Nonetheless, a few outcomes are 
already clear: Information will become more and 
more available and there will be an increased need 
for professionals in the great many fields impacted 
by the advances described here. If this volume helps 
the reader to see even a bit more clearly through the 
maze of opportunity, it has been well worth the effort 
to bring it together.
—Donald Franceschetti, PhD
Bodanis, David. Electric Universe: The Shocking True 
Story of Electricity. New York: Crown Publishers, 
2005. Print.
Penrose, Roger. The Emperor’s New Mind: Concerning 
Computers, Minds, and the Laws of Physics. Oxford: 
Oxford University Press, 1989. Print.
Shannon, Claude E, and Warren Weaver. The 
Mathematical Theory of Communication. Urbana: 
University of Illinois Press, 1999. Print.
Yandell, Ben. The Honors Class: Hilbert’s Problems 
and Their Solvers. Natick, Mass: A.K. Peters, 2002. 
Print.
Von, Neumann J, and Ray Kurzweil. The Computer 
& the Brain. New Haven, Conn.; London : Yale 
University Press, 2012. Print.

xiii
Contributors
Andrew Farrell, MLIS 
Andrew Hoelscher, MEng 
Daniel Horowitz
Donald R. Franceschetti, PhD
John Vines
Joseph Dewey 
Joy Crelin 
Kenrick Vezina, MS
Marianne M. Madsen, MS
Maura Valentino, MSLIS 
Melvin O 
Micah L. Issitt
Richard M. Renneboog, MSc
Scott Zimmer
Teresa E. Schmidt 
Trudy Mercadal, PhD 


1
3-D printing
Fields of Study
Computer Science; Digital Media
Abstract
Additive manufacturing (AM), or 3-D printing, com-
prises several automated processes for building three-
dimensional objects from layers of plastic, paper, 
glass, or metal. AM creates strong, light 3-D objects 
quickly and efficiently.
Prinicipal Terms

 binder jetting: the use of a liquid binding agent to 
fuse layers of powder together.

 directed energy deposition: a process that deposits 
wire or powdered material onto an object and then 
melts it using a laser, electron beam, or plasma arc.

 material extrusion: a process in which heated fila-
ment is extruded through a nozzle and deposited 
in layers, usually around a removable support.

 material jetting: a process in which drops of liquid 
photopolymer are deposited through a printer 
head and heated to form a dry, stable solid.

 powder bed fusion: the use of a laser to heat layers 
of powdered material in a movable powder bed.

 sheet lamination: a process in which thin layered 
sheets of material are adhered or fused together 
and then extra material is removed with cutting 
implements or lasers.

 vat photopolymerization: a process in which a laser 
hardens layers of light-sensitive material in a vat.
Additive Manufacturing
3-D printing, also called additive manufacturing 
(AM), builds three-dimensional objects by adding suc-
cessive layers of material onto a platform. AM differs 
from traditional, or subtractive, manufacturing, also 
called machining. In machining, material is removed 
from a starting sample until the desired structure 
­remains. Most AM processes use less raw material and 
are therefore less wasteful than machining.
The first AM process was developed in the 1980s, 
using liquid resin hardened by ultraviolet (UV) light. 
By the 2000s, several different AM processes had 
been developed. Most of these processes use liquid, 
powder, or extrusion techniques. Combined with 
complex computer modeling and robotics, AM could 
launch a new era in manufacturing. Soon even com-
plex mechanical objects could be created by AM.
Software and Modeling
3-D printing begins with a computer-aided design 
(CAD) drawing or 3-D scan of an object. These draw-
ings or scans are usually saved in a digital file format 
known as STL, originally short for “stereolithog-
raphy” but since given other meanings, such as “sur-
face tessellation language.” STL files “tessellate” the 
object—that is, cover its surface in a repeated pattern 
of shapes. Though any shape can be used, STL files 
use a series of non-overlapping triangles to model the 
curves and angles of a 3-D object. Errors in the file 
may need repair. “Slices” of the STL file determine 
the number and thickness of the layers of material 
needed.
Liquid 3-D Printing
The earliest AM technique was stereolithography 
(SLA), patented in 1986 by Chuck Hull. SLA uses 
liquid resin or polymer hardened by UV light to 
create a 3-D object. A basic SLA printer consists of 
an elevator platform suspended in a tank filled with 
light-sensitive liquid polymer. A UV laser hardens a 
thin layer of resin. The platform is lowered, and the 
laser hardens the next layer, fusing it to the first. This 
process is repeated until the object is complete. The 

2
3-D printing
Principles of Computer Science
Liquifier head
(moves in x
and y direction)
Build platform
(moves in
z direction)
Foam
slab
Extrusion
nozzles
Supports
Support
material spool
Build
material spool
FDM Process
Scanning system
Recoater blade
Platform
Supports
Liquid Photopolymer Resin
Laser
SLA Process
Scanning system
CO2 laser
Powder-leveling roller
Powder bed on
build piston
Excess powder
takeup
Powder
feed
Temperature-controlled
build chamber with
nitrogen atmosphere
SLS Process
This presents a comparison of the three common 3-D printing processes: SLA (in which liquid polymer resin is solidifi ed by a laser and sup-
port material is removed after completion), SLS (in which powder is fused by a CO2 laser and unfused powder acts as support), and FDM (in 
which liquid modeling material is extruded through extrusion nozzles and solidifi es quickly, and a build material and a support material can 
be used in tandem, with the support material being removed after completion). EBSCO illustration.

3
Principles of Computer Science
3-D printing
object is then cleaned and cured by UV. This AM 
technique is also called vat photopolymerization 
because it takes place within a vat of liquid resin. 
Various types of SLA printing processes have been 
given alternate names, such as “photofabrication” 
and “photo-solidification.”
Powder-Based 3-D Printing
In the 1980s, engineers at the University of Texas cre-
ated an alternate process that uses powdered solids 
instead of liquid. Selective layer sintering (SLS), or 
powder bed fusion, heats powdered glass, metal, ce-
ramic, or plastic in a powder bed until the material 
is “sintered.” To sinter something is to cause its par-
ticles to fuse through heat or pressure without lique-
fying it. A laser is used to selectively sinter thin layers 
of the powder, with the unfused powder underneath 
giving structural support. The platform is lowered 
and the powder compacted as the laser passes over 
the object again.
Extrusion Printing
Material extrusion printing heats plastic or polymer 
filament and extrudes it through nozzles to deposit a 
layer of material on a platform. One example of this 
process is called fused deposition modeling (FDM). 
As the material cools, the platform is lowered and 
another layer is added atop the last layer. Creating 
extruded models often requires the use of a struc-
tural support to prevent the object from collapsing. 
Extrusion printing is the most affordable and com-
monly available 3-D printing process.
Emerging and Alternative Methods
Several other 3-D printing methods are also 
emerging. In material jetting, an inkjet printer head 
deposits liquefied plastic or other light-sensitive ma-
terial onto a surface, which is then hardened with UV 
light. Another inkjet printing technique is binder 
jetting, which uses an inkjet printer head to deposit 
drops of glue-like liquid into a powdered medium. 
The liquid then soaks into and solidifies the medium. 
In directed energy deposition (DED), metal wire or 
powder is deposited in thin layers over a support be-
fore being melted with a laser or other heat source. 
Sheet lamination fuses together thin sheets of paper, 
metal, or plastic with adhesive. The resulting object 
is then cut with a laser or other cutting tool to refine 
the shape. This method is less costly but also less ac-
curate than others.
The Future of 3-D Printing
While AM techniques have been in use since the 
1980s, engineers believe that the technology has not 
yet reached its full potential. Its primary use has been 
in rapid prototyping, in which a 3-D printer is used to 
quickly create a 3-D model that can be used to guide 
production. In many cases, 3-D printing can create 
objects that are stronger, lighter, and more customiz-
able than objects made through machining. Printed 
parts are already being used for planes, race cars, 
medical implants, and dental crowns, among other 
items. Because AM wastes far less material than sub-
tractive manufacturing, it is of interest for conserva-
tion, waste management, and cost reduction. The 
technology could also democratize manufacturing, 
as small-scale 3-D printers allow individuals and small 
businesses to create products that traditionally re-
quire industrial manufacturing facilities. However, 
intellectual property disputes could also occur more 
often as AM use becomes more widespread.
—Micah L. Issitt
Bibliography
“About 
Additive 
Manufacturing.” 
Additive 
Manufacturing Research Group. Loughborough U, 
2015. Web. 6 Jan. 2016.
Hutchinson, Lee. “Home 3D Printers Take Us on a 
Maddening Journey into Another Dimension.” Ars 
Technica. Condé Nast, 27 Aug. 2013. Web. 6 Jan. 
2016.
“Knowledge Base: Technologies in 3D Printing.” 
DesignTech. DesignTech Systems, n.d. Web. 6 Jan. 
2016.
Matulka, Rebecca. “How 3D Printers Work.” Energy.
gov. Dept. of Energy, 19 June 2014. Web. 6 Jan. 
2016.
“The 
Printed 
World.” 
Economist. 
Economist 
Newspaper, 10 Feb. 2011. Web. 6 Jan. 2016.
“3D Printing Processes: The Free Beginner’s Guide.” 
3D Printing Industry. 3D Printing Industry, 2015. 
Web. 6 Jan. 2016.

4
Agile robotics
Fields of Study
Robotics
Abstract
Movement poses a challenge for robot design. 
Wheels are relatively easy to use but are severely lim-
ited in their ability to navigate rough terrain. Agile 
robotics seeks to mimic animals’ biomechanical de-
sign to achieve dexterity and expand robots’ useful-
ness in various environments.
Prinicipal Terms

 anthropomorphic: resembling a human in shape 
or behavior; from the Greek words anthropos 
(human) and morphe (form).

 autonomous: able to operate independently, 
without external control.

 biomechanics: the study of how living things move 
and the laws governing their movement.

 dexterity: finesse; skill at performing delicate or 
precise tasks.

 dynamic balance: the ability to maintain balance 
while in motion.

 humanoid: resembling a human.
Robots That Can Walk
Developing robots that can match humans’ and other 
animals’ ability to navigate and manipulate their en-
vironment is a serious challenge for scientists and 
engineers. Wheels offer a relatively simple solution 
for many robot designs. However, they have severe 
limitations. A wheeled robot cannot navigate simple 
stairs, to say nothing of ladders, uneven terrain, or 
the aftermath of an earthquake. In such scenarios, 
legs are much more useful. Likewise, tools such as 
simple pincers are useful for gripping objects, but 
they do not approach the sophistication and adapt-
ability of a human hand with opposable thumbs. The 
cross-disciplinary subfield devoted to creating robots 
that can match the dexterity of living things is known 
as “agile robotics.”
Inspired by Biology
Agile robotics often takes inspiration from nature. 
Biomechanics is particularly useful in this respect, 
combining physics, biology, and chemistry to describe 
how the structures that make up living things work. 
For example, biomechanics would describe a run-
ning human in terms of how the human body—mus-
cles, bones, circulation—interacts with forces such 
as gravity and momentum. Analyzing the activities of 
living beings in these terms allows roboticists to at-
tempt to recreate these processes. This, in turn, often 
reveals new insights into biomechanics. Evolution 
has been shaping life for millions of years through a 
process of high-stakes trial-and-error. Although evo-
lution’s “goals” are not necessarily those of scientists 
and engineers, they often align remarkably well.
Boston Dynamics, a robotics company based in 
Cambridge, Massachusetts, has developed a pro-
totype robot known as the Cheetah. This robot 
mimics the four-legged form of its namesake in an 
attempt to recreate its famous speed. The Cheetah 
has achieved a land speed of twenty-nine miles per 
hour—slower than a real cheetah, but faster than any 
other legged robot to date. Boston Dynamics has an-
other four-legged robot, the LS3, which looks like a 
sturdy mule and was designed to carry heavy supplies 
over rough terrain inaccessible to wheeled trans-
port. (The LS3 was designed for military use, but the 
project was shelved in December 2015 because it was 
too noisy.) Researchers at the Massachusetts Institute 
of Technology (MIT) have built a soft robotic fish. 
There are robots in varying stages of development 
A

5
Principles of Computer Science
Agile robotics
that mimic snakes’ slithering motion or caterpil-
lars’ soft-bodied flexibility, to better access cramped 
spaces.
In nature, such designs help creatures succeed in 
their niches. Cheetahs are effective hunters because 
of their extreme speed. Caterpillars’ flexibility and 
strength allow them to climb through a complex 
world of leaves and branches. Those same traits could 
be incredibly useful in a disaster situation. A small, 
autonomous robot that moved like a caterpillar could 
maneuver through rubble to locate survivors without 
the need for a human to steer it.
Humanoid Robots in a Human World
Humans do not always compare favorably to other an-
imals when it comes to physical challenges. Primates 
are often much better climbers. Bears are much 
stronger, cheetahs much faster. Why design anthro-
pomorphic robots if the human body is, in physical 
terms, relatively unimpressive?
NASA has developed two different robots, 
Robonauts 1 and 2, that look much like a person 
in a space suit. This is no accident. The Robonaut 
is designed to fulfill the same roles as a flesh-and-
blood astronaut, particularly for jobs that are too 
dangerous or dull for humans. Its most remark-
able feature is its hands. They are close enough in 
design and ability to human hands that it can use 
tools designed for human hands without special 
modifications.
Consider the weakness of wheels in dealing with 
stairs. Stairs are a very common feature in the houses 
and communities that humans have built for them-
selves. A robot meant to integrate into human society 
could get around much more easily if it shared a sim-
ilar body plan. Another reason to create humanoid 
robots is psychological. Robots that appear more 
human will be more accepted in health care, cus-
tomer service, or other jobs that traditionally require 
human interaction.
Agile robots are designed to have dexterity, flexibility, and a wider range of motions to allow for more re-
sponsiveness to their surroundings. By Manfred Werner - Tsui, CC-BY-SA-3.0 (http://creativecommons.org/
licenses/by-sa/3.0/), via Wikimedia Commons.

6
ALGOL
Principles of Computer Science
Perhaps the hardest part of designing robots 
that can copy humans’ ability to walk on two legs is 
achieving dynamic balance. To walk on two legs, one 
must adjust one’s balance in real time in response to 
each step taken. For four-legged robots, this is less of 
an issue. However, a two-legged robot needs sophisti-
cated sensors and processing power to detect and re-
spond quickly to its own shifting mass. Without this, 
bipedal robots tend to walk slowly and awkwardly, if 
they can remain upright at all.
The Future of Agile Robotics
As scientists and engineers work out the major chal-
lenges of agile robotics, the array of tasks that can 
be given to robots will increase markedly. Instead of 
being limited to tires, treads, or tracks, robots will 
navigate their environments with the coordination 
and agility of living beings. They will prove invalu-
able not just in daily human environments but also in 
more specialized situations, such as cramped-space 
disaster relief or expeditions into rugged terrain.
—Kenrick Vezina, MS
Bibliography
Bibby, Joe. “Robonaut: Home.” Robonaut. NASA, 31 
May 2013. Web. 21 Jan. 2016.
Gibbs, Samuel. “Google’s Massive Humanoid Robot 
Can Now Walk and Move without Wires.” Guardian. 
Guardian News and Media, 21 Jan. 2015. Web. 21 
Jan. 2016.
Murphy, Michael P., and Metin Sitti. “Waalbot: Agile 
Climbing with Synthetic Fibrillar Dry Adhesives.” 
2009 IEEE International Conference on Robotics and 
Automation. Piscataway: IEEE, 2009. IEEE Xplore. 
Web. 21 Jan. 2016.
Sabbatini, Renato M. E. “Imitation of Life: A History 
of the First Robots.” Brain & Mind 9 (1999): n. 
pag. Web. 21 Jan. 2016.
Schwartz, John. “In the Lab: Robots That Slink and 
Squirm.” New York Times. New York Times, 27 Mar. 
2007. Web. 21 Jan. 2016.
Wieber, Pierre-Brice, Russ Tedrake, and Scott 
Kuindersma. “Modeling and Control of Legged 
Robots.” Handbook of Robotics. Ed. Bruno Siciliano 
and Oussama Khatib. 2nd ed. N.p.: Springer, 
n.d. (forthcoming). Scott Kuindersma—Harvard 
University. Web. 6 Jan. 2016
ALGOL
Fields of Study
Programming Languages
Abstract
The ALGOL programming language was developed in 
1958 as a program for the display of algorithms. It was 
elegant and included several design features that have 
since become staple features of advanced program-
ming languages. ALGOL programs and procedures 
employ a head-body format, and procedures can be 
nested within other procedures. ALGOL was the first 
programming language to make use of start-end de-
limiters for processes within procedures, a feature now 
common in advanced object-oriented programming 
languages. The language allowed recursion and itera-
tive procedures, dynamic array structures, and user-
defined data types. Despite its elegance and advanced 
features, however, ALGOL never became widely used 
and is now one of the oldest and least used of program-
ming languages.
Prinicipal Terms

 algorithm: a set of step-by-step instructions for per-
forming computations. 

 character: a unit of information that represents 
a single letter, number, punctuation mark, blank 
space, or other symbol used in written language. 

 function: instructions read by a computer’s pro-
cessor to execute specific events or operations. 

 object-oriented programming: a type of pro-
gramming in which the source code is organized 
into objects, which are elements with a unique 
identity that have a defined set of attributes and 
behaviors. 

7
Principles of Computer Science
ALGOL

 main loop: the overarching process being carried 
out by a computer program, which may then in­
voke subprocesses. 

 programming languages: sets of terms and rules of 
syntax used by computer programmers to create 
instructions for computers to follow. This code is 
then compiled into binary instructions for a com­
puter to execute. 

 syntax: rules that describe how to correctly struc­
ture the symbols that comprise a language.
History and Development of ALGOL
The name ALGOL is a contraction from ALGO­
rithmic Language. It was developed by a committee 
of American and European computer scientists at 
ETH, in Zurich, Switzerland, as a language for the 
display of algorithms. The detailed specifications 
for the language were first reported in 1960, as 
ALGOL-58, and there have been several revisions 
and variations of the language since its inception. 
These include the updated ALGOL-60, ALGOL-N, 
ALGOL-68, ALGOL-W and Burroughs Extended 
ALGOL. The language was developed from the 
FORTRAN language, which appeared in 1956. Both 
ALGOL and FORTRAN strongly influenced the de­
velopment of later languages such as BASIC, PL/1 
and PL/C, Euler and Pascal. Many ALGOL pro­
grammers agree that the development of ALGOL-68 
made the language much more complex and less 
elegant than the previous version ALGOL-60, ulti­
mately leading to its fall into disfavor with computer 
users. It is now one of the oldest and least used of all 
programming languages.
Program Characteristics
The syntax of ALGOL is rather logical, using natural-
language reserved keywords such as comment, begin 
and end and the “:” (colon) character to identify 
standard arithmetical operators. The ALGOL pro­
gram format utilizes a two-part “block” structure in 
its main loop that has since become a familiar fea­
ture of most modern computer languages such as 
C/C++, Java and many others. The first block, called 
the “head,” consists of a series of type declarations 
similar to those used in FORTRAN type declaration 
statements. The declarations specify the class of the 
entities used in the program, such as integers, real 
numbers, arrays, and so on. For example, the head 
section of an ALGOL-60 program may be
comment An ALGOL-60 sorting program 
procedure Sort (Y, Z)
value Z;
integer Z; real array Y;
As can be seen, comment lines for documenta­
tion are identified by the keyword comment, and 
procedures are identified by the keyword procedure 
followed by the name of the procedure. The second 
block, which is the main part of the program, con­
sists of a sequence of statements that are to be ex­
ecuted. It is initiated by the keyword begin and 
terminated by the keyword end. In fact this is the 
common structure of ALGOL procedures, and the 
program format supports procedures within proce­
dures. For example, the following body section of 
a sorting procedure contains a second procedure 
within it, as
begin
real x;
integer i, j;
for i := 2 until Z do begin
x := Y[i];
for j := i-1 step -1 until 1 do
if x >= A[j] then begin
A[j+1] := x; goto Found
end else
A]j+1] := A[j];
A[1] := x;
Found:
end
end
end Sort
Combining these two code segments produces an 
ALGOL-60 program that sorts a series of numbers. 
Examination of this body code segment reveals that 
the begin statement is followed by the head of a 
nested procedure, which is in turn followed by the 
body statements of the secondary procedure before 
the terminating end statements. The ability to sup­
port nested procedures foreshadowed the concept of 
a function in the context of object-oriented program­
ming. It is a vitally important aspect of computer 
programming, and has proven the robustness of the 

8
ALGOL
Principles of Computer Science
form in many languages since the development of 
ALGOL.
Sample Problem
Comment the following section of code:
begin
real x;
integer i, j;
for i := 2 until Z do begin
x := Y[i];
Answer:
begin
comment identify the variable types as real 
numbers and integers
real x;
integer i, j;
comment an iterative routine to as-
sign elements in the array 
for i := 2 until Z do begin
comment assigns the value of x 
to the array element
x := Y[i];
The ALGOL Legacy
While the ALGOL programming language was over-
shadowed by the FORTRAN programming language 
and did not become popular, it featured a number 
of innovative aspects that have since become staples 
of essentially all major programming languages. In 
particular, the nested procedure structure foreshad-
owed the object-oriented programming style. It was 
the first language to make use of start-end identi-
fiers as block delimiters, a convention that has car-
ried over in object-oriented languages using func-
tion delimiters such as the { and } brace characters. 
ALGOL procedures included conditional statements 
(if...then and if...else), and iterative statements (for...
until). Functions could call themselves in recursive 
computations (as in the statement i = i + j), and 
ALGOL enabled dynamic arrays in which the sub-
script range of the array elements was defined by the 
value of a variable, as in the array element ij. ALGOL 
also used reserved keywords that could not be used as 
identifiers by a programmer, but it also allowed user-
defined data types.
—Richard M. Renneboog M.Sc.
Bibliography
Malcolme-Lawes, D.J. (1969) Programming – ALGOL 
London, UK: Pergamon Press. Print.
Organick, E.I., Forsythe, A.I. and Plummer, R.P. 
(1978) Programming Language Structures New York, 
NY: Academic Press. Print.
O’Regan, Gerard (2012) A Brief History of Computing 
2nd ed., New York, NY: Springer-Verlag. Print.
Wexelblat, Richard L. (1981) History of Programming 
L:anguages New York, NY: Academic Press. Print.
Rutishauer, Heinz (1967) “Description of ALGOL-
60” Chapter in Bauer, F.L., Householder, I.S., 
Olver, F.W.J., Rutishauer, H., Samelson, K. and 
Stiefel, E., eds. Handbook for Automatic Computation 
Berlin, GER: Springer-Verlag. Print.

9
Principles of Computer Science
Algorithms
Algorithms
Fields of Study
Computer Science; Operating Systems; Software 
Engineering
Abstract
An algorithm is set of precise, computable instruc-
tions that, when executed in the correct order, will 
provide a solution to a certain problem. Algorithms 
are widely used in mathematics and engineering, 
and understanding the design of algorithms is funda-
mental to computer science.
Prinicipal Terms

 deterministic algorithm: an algorithm that when 
given a particular input will always produce the 
same output.

 distributed algorithm: an algorithm designed to 
run across multiple processing centers and so is 
capable of directing a concentrated action be-
tween several computer systems.

 drakon chart: a flowchart used to model algo-
rithms and programmed in the hybrid DRAKON 
computer language.

 function: instructions read by a computer’s pro-
cessor to execute specific events or operations.

 recursive: describes a method for problem solving 
that involves solving multiple smaller instances of 
the central problem.

 state: a technical term for all of the stored infor-
mation, and the configuration thereof, that a pro-
gram or circuit can access at a given time.
An Ancient Idea
The term “algorithm” is derived from the name al-
Khwarizmi. Muhammad ibn Musa al- Khwarizmi was a 
ninth-century Persian mathematician who is credited 
with introducing the decimal system to the West. He 
has been celebrated around the world as a pioneer of 
mathematics and conceptual problem solving.
“Algorithm” has no precise definition. Broadly, 
it refers to a finite set of instructions, arranged in a 
specific order and described in a specific language, 
for solving a particular problem. In other words, an 
algorithm is like a plan or a map that tells a person or 
a machine what steps to take in order to complete a 
given task.
Algorithm Basics
In computer science, an algorithm is a series of in-
structions that tells a computer to perform a certain 
function, such as sorting, calculating, or finding data. 
Each step in the instructions causes the computer to 
transition from one state to another until it reaches 
the desired end state.
Any procedure that takes a certain set of inputs 
(a data list, numbers, information, etc.) and reaches 
a desired goal (finding a specific datum, sorting the 
list, etc.) is an algorithm. However, not all algorithms 
are equal. Algorithms can be evaluated for “ele-
gance,” which measures the simplicity of the coding. 
An elegant algorithm is one that takes a minimum 
number of steps to complete. Algorithms can also be 
evaluated in terms of “goodness,” which measures the 
speed with which an algorithm reaches completion.
Lamp isn’t working
Plug in the lamp
Replace the bulb
Buy a new lamp
Is the lamp 
plugged in?
Is the bulb
burned out?
No
Yes
Yes
No
Algorithms are a set of operations or a procedure for solving a 
problem or processing data. Flowcharts are often used as a visual-
ization of the process, showing the order in which steps are per-
formed. EBSCO illustration.

10
Algorithms
Principles of Computer Science
Algorithms can be described in a number of ways. 
Flowcharts are often used to visualize and map the 
steps of an algorithm. The DRAKON computer lan-
guage, developed in the 1980s, allows users to pro-
gram algorithms into a computer by creating a flow-
chart that shows the steps of each algorithm. Such a 
flowchart is sometimes called a drakon chart.
Algorithms often specify conditional processes that 
occur only when a certain condition has been met. 
For instance, an algorithm about eating lunch might 
begin with the question, “Are you hungry?” If the an-
swer is “yes,” the algorithm will instruct the user to eat 
a sandwich. It will then ask again if the user is hungry. 
If the answer is still yes, the “eat a sandwich” instruc-
tion will be repeated. If the answer is “no,” the algo-
rithm will instruct the user to stop eating sandwiches. 
In this example, the algorithm repeats the “eat a 
sandwich” step until the condition “not hungry” is 
reached, at which point the algorithm ends.
Types of Algorithms
Various types of algorithms take different approaches 
to solving problems. An iterative algorithm is a simple 
form of algorithm that repeats the same exact steps 
in the same way until the desired result is obtained. 
A recursive algorithm attempts to solve a problem by 
completing smaller instances of the same problem. 
One example of a recursive algorithm is called a “di-
vide and conquer” algorithm. This type of algorithm 
addresses a complex problem by solving less complex 
examples of the same problem and then combining 
the results to estimate the correct solution.
Algorithms can also be serialized, meaning that 
the algorithm tells the computer to execute one in-
struction at a time in a specific order. Other types 
of algorithms may specify that certain instructions 
should be executed simultaneously. Distributed algo-
rithms are an example of this type. Different parts of 
the algorithm are executed in multiple computers or 
nodes at once and then combined.
An algorithm may have a single, predetermined 
output, or its output may vary based on factors other 
than the input. Deterministic algorithms use exact, 
specific calculations at every step to reach an answer 
to a problem. A computer running a deterministic 
algorithm will always proceed through the same se-
quence of states. Nondeterministic algorithms incor-
porate random data or “guessing” at some stage of 
the process. This allows such algorithms to be used 
as predictive or modeling tools, investigating prob-
lems for which specific data is lacking. In computa-
tional biology, for instance, evolutionary algorithms 
can be used to predict how populations will change 
over time, given estimations of population levels, 
breeding rates, and other environmental pressures.
Algorithm Applications
One of the most famous applications of algorithms is 
the creation of “search” programs used to find infor-
mation on the Internet. The Google search engine 
can search through vast amounts of data and rank mil-
lions of search results in a specific order for different 
users. Sorting large lists of data was one of the earliest 
problems that computer scientists attempted to solve 
using algorithms. In the 1960s, the quicksort algo-
rithm was the most successful sorting algorithm. Using 
a random element from the list as a “pivot,” quicksort 
tells the computer to pick other elements from the 
list and compare them to the pivot. If the element is 
less than the pivot, it is placed above it; if it is greater, 
it is placed below. The process is repeated until each 
pivot is in its proper place and the data is sorted into 
a list. Computer scientists are still attempting to find 
search and sorting algorithms that are more “elegant” 
or “good” in terms of completing the function quickly 
and with the least demand on resources.
Searching and sorting are the most famous examples 
of algorithms. However, these are just two of the thou-
sands of algorithm applications that computer scientists 
have developed. The study of algorithm design has be-
come a thriving subfield within computer science.
—Micah L. Issitt
Bibliography
“Intro to Algorithms.” Khan Academy. Khan Acad., 
2015. Web. 19 Jan. 2016.
Anthes, 
Gary. 
“Back 
to 
Basics: 
Algorithms.” 
Computerworld. Computerworld, 24 Mar. 2008. 
Web. 19 Jan. 2016.
Bell, Tim, et al. “Algorithms.” Computer Science Field Guide. 
U of Canterbury, 3 Feb. 2015. Web. 19 Jan. 2016.
Cormen, Thomas H. Algorithms Unlocked. Cambridge: 
MIT P, 2013. Print.
Cormen, Thomas H., et al. Introduction to Algorithms. 
3rd ed. Cambridge: MIT P, 2009. Print.
Toal, Ray. “Algorithms and Data Structures.” Ray 
Toal. Loyola Marymount U, n.d. Web. 19 Jan. 2016.

11
Principles of Computer Science
Android OS
AnDroiD os
Fields oF study
Computer Science; Operating Systems; Mobile 
Platforms
AbstrAct
This article briefl y discusses the general issues in-
volved with mobile computing and presents a history 
and analysis of Google’s Android operating system. 
It concludes with a look at Android’s future in the 
growing market for mobile technology.
PriniciPAl terms
 
 application program interface (Api): the code 
that defi nes how two pieces of software interact, 
particularly a software application and the oper-
ating system on which it runs.
 
 immersive mode: a full-screen mode in which the 
status and navigation bars are hidden from view 
when not in use.
 
 material Design: a comprehensive guide for visual, 
motion, and interaction design across Google plat-
forms and devices.
 
 multitasking: in the mobile phone environment, 
allowing different apps to run concurrently, much 
like the ability to work in multiple open windows 
on a PC.
 
 multi-touch gestures: touch-screen technology 
that allows for different gestures to trigger the be-
havior of installed software.
A Force in mobile computing
Mobile computing is the fastest-growing segment of 
the tech market. As pricing has become more afford-
able, developing nations, particularly in Africa, are 
the largest growing market for smartphones. With 
smartphones, users shop, gather information, con-
nect via social media such as Twitter and Facebook, 
and communicate—one of the uses more tradition-
ally associated with phones.
By far the most popular operation system run-
ning on mobile phones is Android. It has outpaced 
Apple’s iOS with nearly double the sales. As of 2014, 
more than a million Android devices were being acti-
vated daily. Since its launch in 2008, Android has far 
and away overtaken the competition.
Android takes off
Android came about amid a transformative moment 
in mobile technology. Prior to 2007, slide-out key-
boards mimicked the typing experience of desktop 
PCs. In June of that year, Apple released its fi rst 
iPhone, forever altering the landscape of mobile 
phones. Apple focused on multi-touch gestures and 
touch-screen technology. Nearly concurrent with 
Awesome
The swype keyboard, originally designed for Android operating 
systems, was developed to speed up typing capabilities by allowing 
the user to slide a fi nger over the keyboard from letter to letter 
without lifting their fi nger to choose each character. This standard 
software in the Android operating system allows for quick texting. 
EBSCO illustration.

12
Android OS
Principles of Computer Science
this, Google’s Android released its first application 
program interface (API).
The original API of Google’s new operating 
system (OS) first appeared in October 2008. The 
Android OS was first installed on the T-Mobile G1, 
also known as the HTC Dream. This prototype had 
a very small set of preinstalled apps, and as it had a 
slide-out QWERTY keyboard, there were no touch-
screen capabilities. It did have native multitasking, 
which Apple’s iOS did not yet have. Still, to compete 
with Apple, Google was forced to replace physical 
keyboards and access buttons with virtual onscreen 
controls. The next iteration of Android shipped 
with the HTC Magic and was accompanied by a vir-
tual keyboard and a more robust app marketplace. 
Among the other early features that have stood the 
test of time are the pull-down notification list, home-
screen widgets, and strong integration with Google’s 
Gmail service.
One later feature, the full-screen immersive mode, 
has become quite popular as it reduces distractions. 
First released with Android 4.4, “KitKat,” in 2013, it 
hides the navigation and status bars while certain 
apps are in use. It was retained for the release of 
Android 5.0, “Lollipop,” in 2015.
Android Changes and Grows
Both of Google’s operating systems—Android and 
its cloud-based desktop OS, Chrome— are based 
on the free open-source OS Linux, created by en-
gineer Linus Torvalds and first released in 1991. 
Open-source software is created using publicly avail-
able source code. The open-source development of 
Android has allowed manufacturers to produce ro-
bust, affordable products that contribute to its wide-
spread popularity in emerging and developing mar-
kets. This may be one reason why Android has had 
more than twice as many new users as its closest rival, 
Apple’s iOS. This strategy has kept costs down and 
has also helped build Android’s app marketplace, 
which offers more than one million native apps, 
many free of charge. By 2014 Android made up 54 
percent of the global smartphone market.
This open-source development of Android has 
had one adverse effect: the phenomenon known as 
“forking,” which occurs primarily in China. Forking 
is when a private company takes the OS and creates 
their own products apart from native Google services 
such as e-mail. Google seeks to prevent this loss of 
control (and revenue) by not supporting these com-
panies or including their apps in its marketplace. 
Forked versions of Android made up nearly a quarter 
of the global market in early 2014.
Google’s business model has always focused on a 
“rapid-iteration, web-style update cycle.” By contrast, 
rivals such as Microsoft and Apple have had a far 
slower, more deliberate pace due to hardware issues. 
One benefit of Google’s faster approach is the ability 
to address issues and problems in a timely manner. A 
drawback is the phenomenon known as “cloud rot.” 
As the cloud-based OS grows older, servers that were 
once devoted to earlier versions are repurposed. 
Since changes to the OS initially came every few 
months, apps that worked a month prior would sud-
denly lose functionality or become completely unus-
able. Later Android updates have been released on a 
timescale of six months or more.
Android’s Future
In 2014 alone, more than one billion devices using 
Android were activated. One of the biggest con-
cerns about Android’s future is the issue of forking. 
Making the code available to developers at no cost 
has made Android a desirable and cost-effective al-
ternative to higher-end makers such as Microsoft 
and Apple, but it has also made Google a target of 
competitors.
Another consideration for Android’s future is its 
inextricable link to the Chrome OS. Google plans 
to keep the two separate. Further, Google execu-
tives have made it clear that Chromebooks (laptops 
that run Chrome) and Android devices have distinct 
purposes. Android’s focus has been on touch-screen 
technology, multi-touch gesturing, and screen resolu-
tion, making it a purely mobile OS for phones, tab-
lets, and more recently wearable devices and TVs. 
Meanwhile, Chrome has developed tools that are 
more useful in the PC and laptop environment, such 
as keyboard shortcuts. However, an effort to unify the 
appearance and functionality of Google’s different 
platforms and devices called Material Design was in-
troduced in 2014. Further, Google has ensured that 
Android apps can be executed on Chrome through 
Apps Runtime on Chrome (ARC). Such implemen-
tations suggest a slow merging of the Android and 
Chrome user experiences.
—Andrew Farrell, MLIS

13
Principles of Computer Science
Application
Bibliography
Amadeo, Ron. “The History of Android.” Ars Technica. 
Condé Nast, 15 June 2014. Web. 2 Jan. 2016.
“Android: A Visual History.” Verge. Vox Media, 7 Dec. 
2011. Web. 2 Jan. 2016.
Bajarin, Tim. “Google Is at a Major Crossroads with 
Android and Chrome OS.” PCMag. Ziff Davis, 21 
Dec. 2015. Web. 4 Jan. 2016.
Edwards, Jim. “Proof That Android Really Is for the 
Poor.” Business Insider. Business Insider, 27 June 
2014. Web. 4 Jan. 2016.
Goldsborough, Reid. “Android on the Rise.” Tech 
Directions May 2014: 12. Academic Search Complete. 
Web. 2 Jan. 2016.
Manjoo, Farhad. “Planet Android’s Shaky Orbit.” New 
York Times 28 May 2015: B1. Print.
Newman, Jared. “Android Laptops: The $200 Price 
Is Right, but the OS May Not Be.” PCWorld. IDG 
Consumer & SMB, 26 Apr. 2013. Web. 27 Jan. 
2016.
Newman, Jared. “With Android Lollipop, Mobile 
Multitasking Takes a Great Leap Forward.” Fast 
Company. Mansueto Ventures, 6 Nov. 2014. Web. 
27 Jan. 2016.
Application
Fields of Study
Applications; Software Engineering
Abstract
In the field of information technology, an applica-
tion is a piece of software created to perform a task, 
such as word processing, web browsing, or chess 
playing. Each application is designed to run on a 
particular platform, which is a type of system soft-
ware that is installed on desktop computers, lap-
tops, or mobile devices such as tablet computers or 
smartphones.
Prinicipal Terms

 app: an abbreviation for “application,” a program 
designed to perform a particular task on a com-
puter or mobile device.

 application suite: a set of programs designed to 
work closely together, such as an office suite that 
includes a word processor, spreadsheet, presenta-
tion creator, and database application.

 platform: the specific hardware or software infra-
structure that underlies a computer system; often 
refers to an operating system, such as Windows, 
Mac OS, or Linux.

 system software: the basic software that manages 
the computer’s resources for use by hardware and 
other software.

 utility program: a type of system software that per-
forms one or more routine functions, such as disk 
partitioning and maintenance, software installa-
tion and removal, or virus protection.

 web application: an application that is down-
loaded either wholly or in part from the Internet 
each time it is used.
Applications in Context
Applications are software programs that perform 
particular tasks, such as word processing or web 
browsing. They are designed to run on one or more 
specific platforms. The term “platform” can refer 
to any basic computer infrastructure, including the 
hardware itself and the operating system (OS) that 
manages it. An OS is a type of system software that 
manages a device’s hardware and other software re-
sources. Application designers may create different 
versions of an application to run on different plat-
forms. A cross-platform application is one that can be 
run on more than one platform.
In the context of mobile devices such as tablets 
and smartphones, the term “application” is typi-
cally shortened to app. Since the introduction of 
the iPhone in 2007, apps have taken center stage 
in the realm of consumer electronics. Previously, 
consumers tended to be attracted more to a de-
vice’s hardware or OS features. A consumer might 
have liked a certain phone for its solid design or 

14
Application
Principles of Computer Science
fast processor, or they might have preferred the 
graphical interface of Microsoft Windows and 
Mac OS to the command-line interface of Linux. 
These features have since become much less of a 
concern for the average consumer. Instead, con-
sumers tend to be more interested in finding a 
device that supports the specific apps they wish 
to use.
Evolution of Applications
Over the years, apps have become more and more 
specialized. Even basic utility programs that were 
once included with OSs are now available for pur-
chase as separate apps. In some cases, these apps are 
a more advanced version of the utility software that 
comes with the OS. For example, an OS may come 
with free antivirus software, but a user may choose 
to purchase a different program that offers better 
protection.
Some software companies offer application 
suites of interoperating programs. Adobe Creative 
Cloud is a cloud-based graphic design suite that in-
cludes popular design and editing programs such 
as Photoshop and InDesign. Microsoft Office is an 
office suite consisting of a word processor (Word), 
a spreadsheet program (Excel), and other applica-
tions commonly used in office settings. These pro-
grams are expensive and can take up large amounts 
of storage space on a user’s computer. Before broad-
band Internet access became widely available, ap-
plication suites were distributed on portable media 
such as floppy disks, CD-ROMs, or DVDs, because 
downloading them over a dial-up connection would 
have taken too long.
As high-speed Internet access has become 
much more common, application developers have 
taken a different approach. Instead of investing in 
bulky application suites, users often have the op-
tion of using web applications. These applications 
run partly or entirely on remote servers, avoiding 
the need to install them on the computer’s hard 
drive.
Types of Applications
Many different types of software fall under the 
broad heading of applications. A large segment of 
the application market is focused on office and pro-
ductivity software. This category includes e-mail ap-
plications, word processors, spreadsheet software, 
presentation software, and database management 
systems. In an office environment, it is critical that 
users be able to create documents using these ap-
plications and share them with others. This often 
means that a business or organization will select a 
particular application suite and then require all em-
ployees to use it.
Other types of applications include games, audio-
video editing and production software, and even 
software that helps programmers write new soft-
ware. Due the complexity of software engineering, 
programmers have developed many applications to 
help them produce more polished, bug-free pro-
grams. Software developers may use multiple appli-
cations to code a single program. They might use 
a word processor or text editor to write the source 
code and explanatory comments, a debugging 
tool to check the code for errors, and a compiler 
to convert the code into machine language that a 
computer can execute. There is even a type of ap-
plication that can emulate a virtual computer run-
ning inside another computer. These applications 
are often used by web-hosting companies. Instead 
of having to set up a new physical server for each 
customer that signs up, they can create another vir-
tual server for the user to access.
Security Implications
Applications must have certain privileges in order 
to use the resources of the computer they are run-
ning on. As a result, they can sometimes be a point 
of weakness for attackers to exploit. A clever attacker 
can take over a vulnerable application and then use 
its privileges to make the computer behave in ways 
it should not. For example, the attacker could send 
spam e-mails, host illegally shared files, or even 
launch additional attacks against other computers on 
the same network.
Careers in Applications
Applications are the focus of a variety of career op-
tions for those interested in working with software. 
Apart from the obvious role of computer pro-
grammer, there are several other paths one might 
take. One option is quality assurance. Quality assur-
ance staff are responsible for testing software under 
development to make sure it performs as it should. 
Technical support is another option. Technical sup-
port specialists assist users with operating the software 

15
Principles of Computer Science
Application
SYSTEMS
SOFTWARE
Databases
Games
Spreadsheets
Graphics
Word
Processing
Communications
File Mgmt
Tools
Operating
System
Utilities
Debugger
Assembler
Compilers
APPLICATIONS
SOFTWARE
COMPUTER HARDWARE
The variety and quantity of application software available is massive compared to the limited array of system software and hardware that 
support them. EBSCO illustration.

16
Applied linguistics
Principles of Computer Science
and fixing errors it might cause. Yet another path is 
technical writing. Technical writers create software 
user manuals and training materials. Finally, some 
applications are so complex that using them can be a 
career in itself.
—Scott Zimmer, JD
Bibliography
Bell, Tom. Programming: A Primer; Coding for Beginners. 
London: Imperial Coll. P, 2016. Print.
Calude, Cristian S., ed. The Human Face of Computing. 
London: Imperial Coll. P, 2016. Print.
Dey, Pradip, and Manas Ghosh. Computer Fundamentals 
and Programming in C. 2nd ed. New Delhi: Oxford 
UP, 2013. Print.
Goriunova, Olga, ed. Fun and Software: Exploring 
Pleasure, Paradox, and Pain in Computing. New York: 
Bloomsbury, 2014. Print.
Neapolitan, Richard E. Foundations of Algorithms. 5th 
ed. Burlington: Jones, 2015. Print.
Talbot, James, and Justin McLean. Learning Android 
Application Programming: A Hands- On Guide to 
Building Android Applications. Upper Saddle River: 
Addison, 2014. Print.
Applied linguistics
Fields of Study
Computer Science; Programming Language; Software 
Engineering
Abstract
Applied linguistics is a linguistics subfield that fo-
cuses on solving the linguistics problems of daily 
life. Areas of interest include language learning, 
language preservation, and automated linguistic 
tools. Applied and computational linguistics 
overlap in the design of algorithms and com-
puter programs for educational and commercial 
applications.
Prinicipal Terms

 computational linguistics: a branch of linguistics 
that uses computer science to analyze and model 
language and speech.

 lexicon: the total vocabulary of a person, lan-
guage, or field of study.

 morphology: a branch of linguistics that studies 
the forms of words.

 semantics: a branch of linguistics that studies the 
meanings of words and phrases.

 syntax: a branch of linguistics that studies how 
words and phrases are arranged in sentences to 
create meaning.
Linguistics and Technology
Applied linguistics seeks to address real-world lan-
guage issues. While its emergence predates com-
puter science, modern applied linguistics over-
laps with computational linguistics. In particular, 
computer science tools are useful for modeling 
linguistic concepts and creating automated trans-
lation tools. Applied and computational linguis-
tics research has produced many Internet-based 
language learning programs, as well as programs 
that allow machines to recognize and respond to 
linguistic cues.
Development of the Field
Applied linguistics was first recognized as a distinct 
field of linguistics research in the 1950s. However, the 
application of linguistics knowledge is a far older tra-
dition within academic linguistics. Some of the first 
applied linguistics departments and organizations in 
the United States focused on guiding language policy 
in government initiatives.
Another early area of concern was the application 
of linguistics research to language learning, both for 
native language speakers and for those learning a 
second language. In the 1960s, linguistics researchers 
began working on ways to use computer technology 
in linguistics education and research. Computer-
assisted language learning (CALL), first developed 

17
Principles of Computer Science
Applied linguistics
in the 1960s, studies ways to use computers to assist 
in language acquisition. In the twenty-first century, 
tools and techniques developed for CALL programs 
are regularly used to design software-based language 
learning programs.
Computational linguistics focuses on both ap-
plied and theoretical research. It grew out of 
military efforts in the 1950s to use computers to 
translate foreign languages, particularly Russian. 
Computers were widely used in linguistics re-
search by the late 1980s. In the 1990s and 2000s, 
web-based linguistic applications began to become 
common.
Linguistics research focuses generally on “nat-
ural languages,” which develop “naturally” through 
human use and refinement. They are distinct from 
formal languages, such as computer programming 
languages, and constructed languages, such as 
Esperanto. Developing better learning materials, 
both traditional text-based materials and modern 
software-based materials, is a primary focus of ap-
plied linguistics.
Linguistic Analysis
One major area of linguistics research is linguistic 
analysis, which seeks to dissect natural languages 
into their component parts to better understand how 
languages function and change. Each language can 
be analyzed in terms of its lexicon. This is the total 
vocabulary available in a certain language or subset 
of a language. The term can also refer to an individ-
ual’s total vocabulary. To understand a language, its 
specific lexicon must be broken into smaller units. 
Linguists may study the morphology of words (the 
specific pattern of characters and phonemes that 
make up each word). They may also study a language’s 
semantics (how words and phrases relate to what they 
represent). Beyond semantics is syntax, which looks 
at how words and phrases are arranged to form sen-
tences and other higher-order units of language. In 
the digital age, linguistic analysis increasingly relies 
on software that can compile and evaluate properties 
of language.
Computer Science Applications
Automated translation is among the best-known ex-
amples of applied computational linguistics. Web-
based services such as Babelfish and Google Translate 
use algorithms to analyze user-provided words and 
sentences and translate the semantic concepts into 
equivalent expressions in other languages. Computer 
scientists and linguists are working to refine such soft-
ware to provide more accurate translations. Applied 
linguistics has also resulted in the development of soft-
ware systems that allow users to “speak” to automated 
systems. Apple’s iOS assistant Siri is an example of a 
program that can analyze the syntax and semantics of 
a spoken prompt and respond in kind.
Computer science and linguistics have created 
innovative solutions to a wide range of linguistic 
problems. For instance, the Enable Talk glove is 
a Bluetooth-enabled glove that, when worn, can 
translate sign language into spoken language. This 
enables sign-language users to communicate with 
individuals who do not understand sign language. 
Various similar therapeutic applications exist for in-
dividuals with speech, hearing, or other linguistic 
challenges. Another real-world issue being addressed 
through applied linguistics is the preservation, docu-
mentation, and analysis of the world’s “endangered 
languages.” These languages are in danger of being 
sentence:  Joe found his book.
S = sentence
N = noun
VP = verb phrase
V = verb
NP = noun phrase
D = determiner
Joe found his
book.
S
N
VP
NP
V
D
N
A parse tree, also known as a “syntax tree,” shows the grammatical 
structure of a sentence. In the example sentence (S), “Joe found 
his book,” there are two nouns (N), “Joe” and “book.” Based on 
the verb “found,” we can determine that “his book” is part of the 
predicate, and the subject—the noun doing the finding— is “Joe.” 
Determiners (D) are used to distinguish the specific noun about 
which the sentence is talking. These parse trees are used as a basis 
for developing computer programs that can identify grammatical 
structure and draw meaning from a sentence in the same way 
human brains do. EBSCO illustration.

18
Architecture software
Principles of Computer Science
lost because of a lack of native speakers or a low rate 
of transmission to new generations.
Applied computational linguistics also seeks to 
create tools that can search through text and written 
data. Computer scientists and linguistics specialists 
have developed software that can use programmed 
linguistic rules and structural cues to locate keywords, 
sentences, and other elements within text. Basic 
keyword searches are one type of computational lin-
guistic search tool. Computer scientists are working 
on new search tools that will allow more complex 
searches and meta-searches within text data. Given 
the wealth of textual data online, better text search 
and analysis tools could have a major impact on the 
future of education.
—Micah L. Issitt
Bibliography
Davies, Alan. An Introduction to Applied Linguistics: From 
Practice to Theory. 2nd ed. Edinburgh: Edinburgh 
UP, 2007. Print.
Jurafasky, Daniel, and James H. Martin. Speech and 
Language Processing. 2nd ed. Upper Saddle River: 
Prentice Hall, 2008. Print.
Lardinois, Frederic. “Ukrainian Students Develop 
Gloves That Translate Sign Language into Speech.” 
TechCrunch. AOL, 9 July 2012. Web. 19 Jan 2016.
Ramasubbu, 
Suren. 
“How 
Technology 
Can 
Help 
Language 
Learning.” 
Huffington 
Post. 
TheHuffingtonPost.com, 3 June 2015. Web. 19 
Jan. 2016.
Simpson, James, ed. The Routledge Handbook of Applied 
Linguistics. New York: Routledge, 2011. Print.
Tomlinson, Brian, ed. Applied Linguistics and 
Materials Development. New York: Bloomsbury, 
2013. Print.
Architecture software
Fields of Study
Applications; Graphic Design
Abstract
Architecture software is a category of specialized 
computer software intended for use by architects. 
Through the use of such software, architects may 
design, model, and modify plans for homes or other 
buildings digitally rather than on paper. As the type 
and scale of the structures being designed vary 
greatly, many types of architecture software are avail-
able to address the differing requirements of indi-
vidual projects.
Prinicipal Terms

 building information modeling (BIM): the cre-
ation of a model of a building or facility that ac-
counts for its function, physical attributes, cost, 
and other characteristics.

 modeling: the process of creating a 2-D or 3-D rep-
resentation of the structure being designed.

 postproduction: the period after a model has 
been designed and an image has been rendered, 
when the architect may manipulate the created 
image by adding effects or making other aesthetic 
changes.

 raster: a means of storing, displaying, and editing 
image data using individual pixels.

 rendering: the process of transforming one or 
more models into a single image.

 vector: a means of storing, displaying, and editing 
image data using defined points and lines.
The Introduction of Software to 
Architecture
During the late twentieth and early twenty-first cen-
turies, architecture became one of many fields to be 
dramatically transformed through the use of com-
puter technology. For centuries, architects designed 
single structures such as homes as well as larger, mul-
tibuilding facilities by hand. They drafted plans on 
paper and created 3-D scale models out of materials 
such as cardboard, wood, and foam. These processes 
relied heavily on physical components. Therefore, it 

19
Principles of Computer Science
Architecture software
was relatively difficult to collaborate with fellow ar-
chitects or other professionals who were not located 
nearby. Making changes to one’s work, particularly to 
models that were already assembled, could be time 
consuming and difficult. The advent of architecture 
software brought advances in areas such as drafting, 
modeling, and modifying models and floorplans as 
well as collaborating with other professionals.
Although typically designed for and marketed to 
architects, architecture software is similar in many 
ways to other types of software used in the broader 
field of art and design. Some architecture programs 
are essentially specialized versions of computer-aided 
design (CAD) software. CAD programs are typically 
used in fields such as manufacturing. For example, 
one particularly popular architecture program, 
AutoCAD Architecture, is sold by the software com-
pany Autodesk. Autodesk is known for its CAD and 
computer-aided manufacturing (CAM) programs. 
Architects began to adopt the use of CAD programs 
in the 1980s. By the following decade, such software 
was widely used in the field.
Conceptual design
Design development 
collaboration
and documentation
Architecture
Conceptual design
Logistics and
construction management
Construction
Structural design 
analysis
Structural design 
analysis
Construction documentation
Construction
4D/5D projections
Shop drawings and fabrication
Engineering
Long-term facility management
Redevelopment
Owners
Conceptual design
Architecture software is designed to assist in one or more of the many tasks required for building informa-
tion modeling (BIM). Depending on one’s role and how one relates to the building process, certain tasks 
may be more important. Determining the proper architecture software first requires one to know what BIM 
tasks one hopes to take on. EBSCO illustration.

20
Architecture software
Principles of Computer Science
Features
Architecture software is created and sold by a 
number 
of 
different 
technology 
companies. 
Therefore, the available programs vary in terms of 
features, although there are certain common ones. 
The first and perhaps most essential feature is the 
ability to draft floor plans, a process that was his-
torically done with paper and pencil. Architecture 
software also typically allows users to create 3-D 
models of structures and easily insert elements such 
as doors and windows. Unlike physical models of 
proposed structures, digital models can be modi-
fied with just a few clicks of a mouse. Architecture 
software also sometimes includes features that sup-
port collaboration among architects and other pro-
fessionals. These can include the ability to check 
files in and out, add notes to floor plans, and record 
measurements.
Specialized software devoted to building infor-
mation modeling (BIM) has much in common with 
standard architecture software. However, BIM also 
allows the user to incorporate information such as 
expected time frames, costs, and the functions of a 
particular building or facility into the design. BIM al-
lows the various professionals involved in the design 
process to evaluate and modify the design prior to 
moving forward with the project.
Rendering and Postproduction
In addition to creating floor plans and 3-D models, 
architects are frequently tasked with creating images 
of their designs for clients. Transforming a 3-D model 
into a 2-D image is known as rendering. In postpro-
duction, an architect may use graphic design soft-
ware to add visual effects to the rendered image. For 
instance, if the architect has designed a waterfront 
apartment complex, he or she may use a graphic de-
sign program such as Adobe Photoshop to add pho-
torealistic water textures to the image and increase its 
aesthetic appeal.
When creating floor plans and models, architects 
typically use vector images. Vector images are based 
on defined points and lines. This allows the images 
to remain true to scale no matter how much their 
overall size is modified. Graphic design software such 
as Photoshop, on the other hand, is a raster-based 
program. In a raster-based program, the images are 
based on individual pixels. As such, architects also 
sometimes use software capable of converting from 
vector to raster.
Advantages and Disadvantages
The use of specialized software in the field of archi-
tecture has a number of advantages and disadvan-
tages. Chief among the advantages is a potential in-
crease in productivity and time savings. Digital tools 
to draft floor plans and create models have largely 
replaced more time-consuming physical methods. 
The abilities to modify designs, test designs through 
digital modeling, and easily collaborate with fellow 
professionals on a project are cited as key benefits of 
architecture software.
However, the use of architecture software is not 
a substitute for architectural training. Some archi-
tects argue that the availability of such software 
can enable some users to produce substandard 
work. In addition, ongoing changes in the field 
require architects to maintain up-to-date knowl-
edge of current technology. This can put archi-
tects accustomed to working with older systems at 
a disadvantage.
—Joy Crelin
Bibliography
Ambrose, Gavin, Paul Harris, and Sally Stone. The 
Visual Dictionary of Architecture. Lausanne: AVA, 
2008. Print.
Bergin, Michael S. “History of BIM.” Architecture 
Research Lab. Architecture Research Lab, 21 Aug. 
2011. Web. 31 Jan. 2016.
“A History of Technology in the Architecture Office.” 
Architizer. Architizer, 23 Dec. 2014. Web. 31 Jan. 
2016.
Kilkelly, Michael. “Which Architectural Software 
Should You Be Using?” ArchDaily. ArchDaily, 4 May 
2015. Web. 31 Jan. 2016.
Marble, Scott, ed. Digital Workflows in Architecture. 
Basel: Birkhäuser, 2012. Print.
Solomon, Nancy B., ed. Architecture: Celebrating 
the Past, Designing the Future. New York: Visual 
Reference, 2008. Print.

21
Principles of Computer Science
ASCII
ASCII
Fields of Study
Computer Science; Computer Engineering
Abstract
The American Standard Code for Information 
Interchange (ASCII) is a character encoding system. 
It enables computers and other electronic communi-
cation devices to store, process, transmit, print, and 
display text and other graphic characters. Initially 
published in 1963, ASCII formed the basis for several 
other character encoding systems developed for use 
with PCs and the Internet.
Prinicipal Terms

 bit width: the number of bits used by a computer 
or other device to store integer values or other 
data.

 character: a unit of information that repre-
sents a single letter, number, punctuation mark, 
blank space, or other symbol used in written 
language.

 control characters: units of information used to 
control the manner in which computers and other 
devices process text and other characters.

 hamming distance: a measurement of the differ-
ence between two characters or control characters 
that effects character processing, error detection, 
and error correction.

 printable characters: characters that can be 
written, printed, or displayed in a manner that can 
be read by a human.
Understanding Character Encoding
Written language, or text, is composed of a variety 
of graphic symbols called characters. In many lan-
guages, these characters include letters, numbers, 
punctuation marks, and blank spaces. Such charac-
ters are also called printable characters because they 
can be printed or otherwise displayed in a form that 
can be read by humans. Another type of character is 
a control character. Control characters effect the pro-
cessing of other characters. For example, a control 
character might instruct a printer to print the next 
character on a new line. Character encoding is the 
process of converting characters into a format that 
can be used by an electronic device such as a com-
puter or telegraph.
Originally designed for use with Samuel Morse’s 
telegraph system, Morse code was one of the first 
character encoding schemes adopted for widespread 
use. Telegraphs transmit information by sending 
electronic pulses over telegraph wires. Morse code 
assigns each character to a unique combination of 
short and long pulses. For example, the letter A was 
assigned to the combination of one short followed by 
one long pulse, while the letter T was assigned to a 
single long pulse. Using Morse code, a telegraph op-
erator can send messages by transmitting a sequence 
of pulses. The sequence, or string, of pulses repre-
sents the characters that comprise the message text.
Other character encoding systems were created 
to meet the needs of new types of electronic devices 
including teleprinters and computers. By the early 
1960s, the use of character encoding systems had 
become widespread. However, no standard char-
acter encoding system existed to ensure that systems 
from different manufacturers could communicate 
with each other. In fact, by 1963, over sixty different 
encoding systems were in use. Nine different sys-
tems were used by IBM alone. To address this issue, 
the American Standards Association (ASA) X3.4 
Committee developed a standardized character en-
coding scheme called ASCII.
Understanding the ASCII Standard
The ASCII standard is based on English. It encodes 
128 characters into integer values from 0 to 127. 
Thirty-three of the characters are control charac-
ters, and ninety-five are printable characters that in-
clude the upper- and lowercase letters from A to Z, 
the numbers zero to nine, punctuation marks, and a 
blank space. For example, the letter A is encoded as 
65 and a comma as 44.
The encoded integers are then converted to bits, 
the smallest unit of data that can be stored by a com-
puter system. A single bit can have a value of either 
zero or one. In order to store integers larger than 
one, additional bits must be used. The number of 
bits used to store a value is called the bit width. ASCII 
specifies a bit width of seven. For example, in ASCII, 
the integer value 65 is stored using seven bits, which 
can be represented as the bit string 1000001.

22
ASCII
Principles of Computer Science
The ASCII seven-bit integer values for specific 
characters were not randomly assigned. Rather, the 
integer values of specific characters were selected to 
maximize the hamming distance between each value. 
Hamming distance is the number of bits set to dif-
ferent values when comparing two bit strings. For ex-
ample, the bit strings 0000001 (decimal value 1) and 
0000011 (decimal value 3) have a hamming distance 
of 1 as only the second to last bit differs between the 
two strings. The bit patterns 0000111 (decimal value 
7) and 0000001 (decimal value 1) have a hamming dis-
tance of two as the bit in the third to last position also 
differs between the two strings. ASCII was designed 
to maximize hamming distance because larger ham-
ming distances enable more efficient data processing 
as well as improved error detection and handling.
Beyond ASCII
Following its introduction in 1963, ASCII continued 
to be refined. It was gradually adopted for use on a 
wide range of computer systems including the first 
IBM PC. Other manufacturers soon followed IBM’s 
lead. The ASCII standard was also widely adopted for 
Sample Problem
ASCII defines the integer values for the first 
eleven lowercase letters of the alphabet as 
follows:
a = 97; b = 98; c = 99; d = 100; e = 101; f = 
102; g = 103; h = 104; i = 105; j = 106; k = 107
Using this information, translate the word hijack to 
the correct ASCII integer values.
Answer:
The ASCII representation of the word hijack 
can be determined by comparing each char-
acter in the word to its defined decimal value 
as follows:
h i j a c k
h (104) i (105) j (106) a (97) c (99) k (107)
104 105 106 97 99 107
The correct ASCII encoding for the word 
hijack is 104 105 106 97 99 107.
This chart presents the decimal and hexadecimal ASCII codes for common characters on a keyboard. Public 
domain, via Wikimedia Commons

23
Principles of Computer Science
Assembly Language
use on the Internet. However, as the need for more 
characters to support languages other than English 
grew, other standards were developed to meet this 
need. One such standard, Unicode can encode 
more than 120,000 characters. ASCII remains an im-
portant technology, however. Many systems still use 
ASCII. Character encoding systems such as Unicode 
incorporate ASCII to promote compatibility with ex-
isting systems.
—Maura Valentino, MSLIS
Bibliography
Amer. Standards Assn. American Standard Code for 
Information Interchange. Amer. Standards Assn., 17 
June 1963. Digital file.
Anderson, Deborah. “Global Linguistic Diversity 
for the Internet.” Communications of the ACM Jan. 
2005: 27. PDF file.
Fischer, Eric. The Evolution of Character Codes, 1874–
1968. N.p.: Fischer, n.d. Trafficways.org. Web. 22 
Feb. 2016.
Jennings, Tom. “An Annotated History of Some 
Character Codes.” World Power Systems. Tom 
Jennings, 29 Oct. 2004. Web. 16 Feb. 2016.
McConnell, Robert, James Haynes, and Richard 
Warren. 
“Understanding 
ASCII 
Codes.” 
NADCOMM. NADCOMM, 14 May 2011. Web. 16 
Feb. 2016.
Silver, H. Ward. “Digital Code Basics.” Qst 98.8 
(2014): 58–59. PDF file.
“Timeline of Computer History: 1963.” Computer 
History Museum. Computer History Museum, 1 
May 2015. Web. 23 Feb. 2016.
Assembly Language
Fields of Study
Programming Languages; System-level Programming; 
Embedded Systems
Abstract
Assembly language is the most fundamental pro-
gramming language. It is used to directly manipulate 
data in working memory, allowing the programmer 
to account for each clock cycle and use the available 
computing resources most efficiently. Assembly lan-
guage programming is very compact, making it ideal 
for use in embedded systems that are designed to 
carry out specific functions or are not generally ac-
cessible to the end user.
Prinicipal Terms

 embedded systems: computer systems that are in-
corporated into larger devices or systems to mon-
itor performance or to regulate system functions.

 hardware: the physical parts that make up a com-
puter. These include the motherboard and pro-
cessor, as well as input and output devices such as 
monitors, keyboards, and mice.

 hexadecimal: a base-16 number system that uses 
the digits 0 through 9 and the letters A, B, C, D, E, 
and F as symbols to represent numbers.

 imperative programming: programming that pro-
duces code that consists largely of commands is-
sued to the computer, instructing it to perform 
specific actions.

 main loop: the overarching process being carried 
out by a computer program, which may then in-
voke subprocesses.

 process: the execution of instructions in a com-
puter program.

 syntax: rules that describe how to correctly struc-
ture the symbols that comprise a language.
CPU Architecture and Addressing
Digital electronic devices function by the manipula-
tion of binary signals, typically the voltages applied 
to a series of conducting circuits under the control 
of a central processing unit, or CPU. A binary signal 
has only two states, either on (“high”) or off (“low”). 
In CPU architecture and other systems within the 
hardware of a computer or digital device, a par-
allel set of binary signals is controlled by a “clock” 
signal that triggers the progression from one set of 

24
Assembly Language
Principles of Computer Science
binary signals to the next in a process. Each series 
of signals, and all other counting procedures in the 
device, including the ordering of memory locations, 
are expressed as a hexadecimal number that corre-
sponds to the pattern of high and low signals in the 
particular series`. Some locations are reserved for 
specific purposes and functions, and cannot be al-
tered. These locations are vital components for the 
functioning of an assembly language program by “ac-
cumulating” and “registering” data values that the 
program uses. The special registers are identified by 
specific symbols in the syntax of assembly language 
and their addresses are typically specific to the archi-
tecture of the CPU chip, although assembly language 
itself is a general standard. Assembly language pro-
grams are strictly linear, and all operations and logic 
are contained within the main loop of the program. 
Assembly language programs are not compiled, but 
are entered directly into the CPU memory locations 
for their operation. Assembly language programs are 
accordingly the most basic examples of imperative 
programming.
Assembly Language Program Format
The principal register in CPU memory is the accu-
mulator. It is the standard location for the accumu-
lated value of a combination of instructions that are 
being carried out. Locations called “R registers” hold 
values being used in intermediate processes to gen-
erate the value that will go into the accumulator. The 
third vital register is the “program counter,” which 
holds the location of the next instruction to be ex-
ecuted in code memory. Other locations are termed 
“data pointers,” and contain the location in memory 
at which a series of stored data is located. Each line 
of an assembly language program consists typically of 
three parts: an operation instruction, followed by a 
memory location and data, and finally a documen-
tation phrase. Though not absolutely required, it is 
perhaps the most essential for understanding what 
the purpose of the instruction. It is a statement that is 
not recognized as an instruction or data by the CPU. 
For example, the statement
MOV A,30h ; load value from memory address  
30h into accumulator 
is made readily understandable to a human by 
the documentation phrase, while the CPU under-
stands only the MOV A,30h statement. Data can 
be addressed in an Assembly language program 
in various ways. The statement MOV A,#20h is an 
example of “immediate addressing,” in which the 
value to be stored in memory immediately follows 
the “op-code” in code memory. The above statement 
MOV A,30h is an example of “direct addressing.” 
The statement MOV A,@R0 is an example of “indi-
rect addressing,” in which the value to be stored in 
memory is retrieved from the location in internal 
RAM specified, or “pointed to,” by the value stored 
in the first R-register (R0). Two commands are used 
for “external direct” addressing. The first, MOVX 
A,@DPTR reads a value in external RAM at the lo-
cation identified by the value stored in the DPTR 
(data pointer) register and loads it into the accu-
mulator. The second, MOVX @DPTR,A writes the 
value in the accumulator to the location in external 
RAM identified by the value stored in the DPTR reg-
ister. The statement MOVX @R0,A is an example 
of external indirect addressing and functions in 
the same way using the value stored in the specified 
R-register. A final example is termed “code indirect” 
addressing, and is used to access data stored in the 
program code memory. Since code memory repre-
sents only a fairly small amount of actual memory, 
code indirect addressing is generally useful only for 
very small projects.
Sample Problem
Provide documentation phrases for the fol-
lowing statements:
MOV DPTR,#2021h
CLR A
MOVC A,@A+DPTR
Answer:
MOV DPTR,#2021h; sets the value of the 
DPTR register to 2021h
CLR A; clears the accumulator (sets the value 
in the accumulator to 00)
MOVC A,@A+DPTR; reads the value stored in 
code memory location 2021h and loads it into 
the accumulator

25
Principles of Computer Science
Autonomic computing
Assembly Language is Everywhere
Because the coding of Assembly language programs is 
so compact and functions directly within the CPU of a 
device, it is the programming language of choice for 
all manner of embedded systems and other digital con-
trol devices that do not require the power or resources 
of a larger computer. The programs are typically in-
stalled directly into the device during manufacture 
and so are not accessible to the user afterwards. The 
vast majority of electronically-controlled devices such 
as household appliances, mp3 players, digital cameras, 
and any number of other devices, are controlled by 
the functioning of Assembly language programming.
—Richard M. Renneboog M.Sc.
Bibliography
Cavanagh, Joseph (2013) X86 Assembly Language and 
C Fundamentals Boca Raton, FL: CRC Press. Print.
Duntemann, 
Jeff 
(2011) 
Assembly 
Language 
Programming Step-by-Step: Programming with Linux 3rd 
ed., Hoboken, NJ: John Wiley & Sons. Print.
Gilder, Jules H. (1986) Apple Iic and Iie Assembly 
Language New York, NY: Chapman and Hall. Print.
Hyde, Randall (2010) The Art of Assembly Language 2nd 
ed., San Francisco, CA: No Starch Press. Print.
Margush, Timothy S. (2012) Some Assembly Required. 
Assembly Language Programming with the AVR 
Microcontroller Boca Raton, FL: CRC Press. Print.
Peterson, James L. (1978) Computer Organization and 
Assembly Language Programming New York, NY: 
Academic Press. Print.
Steiner, Craig (1990) The 8051/8052 Microcontroller: 
Architecture, Assembly Language and Hardware 
Interfacing Boca Raton, FL: Universal Publishers. 
Print.
Streib, James T. (2011) Guide to Assembly Language. A 
Concise Introduction New York, NY: Springer. Print.
Autonomic computing
Fields of Study
Computer Science; Embedded Systems; System-Level 
Programming
Abstract
Autonomic computing is a subfield of computer sci-
ence that focuses on enabling computers to operate 
independently of user input. First articulated by IBM 
in 2001, the concept has particular relevance to fields 
such as robotics, artificial intelligence (AI), and ma-
chine learning.
Prinicipal Terms

 autonomic components: self-contained software 
or hardware modules with an embedded capacity 
for self-management, connected via input/out-
puts to other components in the system.

 bootstrapping: a self-starting process in a com-
puter system, configured to automatically initiate 
other processes after the booting process has been 
initiated.

 multi-agent system: a system consisting of mul-
tiple separate agents, either software or hardware 
systems, that can cooperate and organize to solve 
problems.

 resource distribution: the locations of resources 
available to a computing system through various 
software or hardware components or networked 
computer systems.

 self-star properties: a list of component and system 
properties required for a computing system to be 
classified as an autonomic system.
Self-Managing Systems
Autonomic computing is a branch of computer sci-
ence aimed at developing computers capable of 
some autonomous operation. An autonomic system 
is one that is, in one or more respects, self-managing. 
Such systems are sometimes described as “self-*” or 
“self-star.” The asterisk, or “star,” represents different 
properties of autonomic systems (self-organization, 
self-maintenance). Autonomic computing aims to 
develop systems that require less outside input, al-
lowing users to focus on other activities.

26
Autonomic computing
Principles of Computer Science
Self-
Configuring
Self-
Healing
Self-
Protecting
Self-
Optimizing
Autonomic
Level 5
Adaptive
Level 4
Predictive
Level 3
Managed
Level 2
Basic
Level 1
Self-
Configuring
Self-
Optimizing
Self-
Optimizing
Self-
Optimizing
Self-
Optimizing
Self-
Protecting
Self-
Protecting
Self-
Protecting
Self-
Healing
Self-
Healing
Manual
Autonomic
Self-Star Systems
The concept of autonomic computing is based on au-
tonomic systems found in nature. Examples of such 
systems include the autonomic nervous system of hu-
mans and the self-regulation of colonial insects such as 
bees and ants. In an autonomic system, the behaviors 
of individual components lead to higher-order self-
maintenance properties of the group as a whole.
The properties that a system needs to function 
autonomically are often called self-star properties. 
One is self-management, meaning that the system 
can manage itself without outside input after an 
As computer systems have advanced from very basic technologies needing intense IT management toward autonomic systems that can 
self-manage, there have been four major stepping stones: self-optimizing, self-protecting, self-healing, and self-configuring. Each of these 
steps toward fully autonomic systems allows for more expansive computing while reducing the skill level required of the end users. EBSCO 
illustration.

27
Principles of Computer Science
Autonomic computing
initial setup. Computer scientists disagree about 
what other self-star properties a system must have 
to be considered autonomic. Proposed properties 
include:
self-stabilization, the ability to return to a stable 
state after a change in configuration;
self-healing, the ability to recover from external 
damage or internal errors;
self-organization, the ability to organize compo-
nent parts and processes toward a goal;
self-protection, the ability to combat external 
threats to operation; and
self-optimization, the ability to manage all re-
sources and components to optimize operation.
Autonomic systems may also display self-aware-
ness and self-learning. Self-awareness in a computer 
system differs from self-awareness in a biological 
system. In a computer system, self-awareness is better 
defined as the system’s knowledge of its internal com-
ponents and configuration. Self-learning is the ability 
to learn from experiences without a user program-
ming new information into the system.
Design of Autonomic Systems
An autonomic computer system is typically envi-
sioned as having autonomic components (ACs), 
which are at least partly self-managing. An ex-
ample of an AC is bootstrapping. Bootstrapping is 
the process by which a computer configures and 
initiates various processes during start-up. After a 
user turns on the computer, the bootstrapping pro-
cess is self-managed. It proceeds through a self-di-
agnostic check and then activates various hardware 
and software components.
There are two basic models for autonomic com-
puter design: a feedback control system and a 
multi-agent system. In a feedback control system, 
changing conditions provide feedback to the 
system that triggers changes in the system’s func-
tion. Feedback control is often found in biological 
systems. In the autonomic nervous system, for ex-
ample, levels of various neurotransmitters are linked 
to feedback systems that activate or deactivate ACs. 
A multi-agent system uses the collective functions 
of separate components to complete higher-order 
functions. For instance, groups of computers can 
be networked such that by performing individual 
functions, the components can collectively manage 
the system’s functions and resources with reduced 
need for outside input. Linking multiple processors 
together changes the system’s resource distribu-
tion, requiring that it be able to locate computing 
resources within all connected agents in order to 
handle tasks effectively.
Semi-autonomic software and hardware systems 
are commonly used. Peer-to-peer (P2P) systems for 
social networking and communication generally 
have some autonomic properties, including self-
configuration, self-tuning, and self-organization. 
These systems can determine a user’s particular 
computer setup, tune themselves to function in var-
ious environments, and self-organize in response to 
changing data or configuration. Most modern com-
puting systems contain ACs but are not considered 
fully autonomic, as they still require some external 
management.
The Promise of Autonomic Systems
The main goal of autonomic computing is to enable 
computer systems to perform basic maintenance 
and optimization tasks on their own. Maximizing the 
number of automated functions that a computer can 
handle allows engineers and system administrators 
to focus on other activities. It also enhances ease of 
operation, especially for those less adept at data man-
agement or system maintenance. For instance, the 
bootstrapping system and the self-regulatory systems 
that detect and correct errors have made computing 
more friendly for the average user.
Autonomic computer systems are particularly im-
portant in robotics, artificial intelligence (AI), and 
machine learning. These fields seek to design ma-
chines that can work unaided after initial setup and 
programming. The science of autonomic computing 
is still in its infancy, but it has been greatly enhanced 
by advancements in processing power and dynamic 
computer networking. For instance, the AI system 
Amelia, developed by former IT specialist Chetan 
Dube, not only responds to verbal queries and an-
swers questions but can also learn by listening to 
human operators answer questions that it cannot 
answer. To some, systems that can learn and alter 
their own programming are the ultimate goal of au-
tonomic design.
—Micah L. Issitt

28
Avatars and simulation
Principles of Computer Science
Bibliography
Bajo, Javier, et al., eds. Highlights of Practical Applications 
of Agents, Multi-Agent Systems, and Sustainability. 
Proc. of the International Workshops of PAAMS 
2015, June 3–4, 2015, Salamanca, Spain. Cham: 
Springer, 2015. Print.
Berns, Andrew, and Sukumar Ghosh. “Dissecting 
Self-* Properties.” SASO 2009: Third IEEE 
International Conference on Self-Adaptive and Self-
Organizing Systems. Los Alamitos: IEEE, 2009. 10–
19. Andrew Berns: Homepage. Web. 20 Jan. 2016.
Follin, Steve. “Preparing for IT Infrastructure 
Autonomics.” IndustryWeek. Penton, 19 Nov. 2015. 
Web. 20 Jan. 2016.
Gibbs, W. Wayt. “Autonomic Computing.” Scientific 
American. Nature Amer., 6 May 2002. Web. 20 Jan. 
2016.
Lalanda, Philippe, Julie A. McCann, and Ada 
Diaconescu, eds. Autonomic Computing: Principles, 
Design and Implementation. London: Springer, 2013. 
Print.
Parashar, Manish, and Salim Hariri, eds. Autonomic 
Computing: Concepts, Infrastructure, and Applications. 
Boca Raton: CRC, 2007. Print.
Avatars and simulation
Fields of Study
Digital Media; Graphic Design
Abstract
Avatars and simulation are elements of virtual reality 
(VR), which attempts to create immersive worlds for 
computer users to enter. Simulation is the method 
by which the real world is imitated or approximated 
by the images and sounds of a computer. An avatar 
is the personal manifestation of a particular person. 
Simulation and VR are used for many applications, 
from entertainment to business.
Prinicipal Terms

 animation variables (avars): defined variables used 
in computer animation to control the movement 
of an animated figure or object.

 keyframing: a part of the computer animation pro-
cess that shows, usually in the form of a drawing, 
the position and appearance of an object at the 
beginning of a sequence and at the end.

 modeling: reproducing real-world objects, people, 
or other elements via computer simulation.

 render farm: a cluster of powerful computers that 
combine their efforts to render graphics for ani-
mation applications.

 virtual reality: the use of technology to create 
a simulated world into which a user may be im-
mersed through visual and auditory input.
Virtual Worlds
Computer simulation and virtual reality (VR) have 
existed since the early 1960s. While simulation has 
been used in manufacturing since the 1980s, avatars 
and virtual worlds have yet to be widely embraced 
outside gaming and entertainment. VR uses comput-
erized sounds, images, and even vibrations to model 
some or all of the sensory input that human beings 
constantly receive from their surroundings every day. 
Users can define the rules of how a VR world works 
in ways that are not possible in everyday life. In the 
real world, people cannot fly, drink fire, or punch 
through walls. In VR, however, all of these things are 
possible, because the rules are defined by human 
coders, and they can be changed or even deleted. 
This is why users’ avatars can appear in these virtual 
worlds as almost anything one can imagine—a loaf of 
bread, a sports car, or a penguin, for example. Many 
users of virtual worlds are drawn to them because of 
this type of freedom.
Because a VR simulation does not occur in phys-
ical space, people can “meet” regardless of how far 
apart they are in the real world. Thus, in a com-
pany that uses a simulated world for conducting its 
meetings, staff from Hong Kong and New York can 
both occupy the same VR room via their avatars. 
Such virtual meeting spaces allow users to convey 
nonverbal cues as well as speech. This allows for a 
greater degree of authenticity than in telephone 
conferencing.

29
Principles of Computer Science
Avatars and simulation
Mechanics of Animation
The animation of avatars in computer simulations 
often requires more computing power than a single 
workstation can provide. Studios that produce ani-
mated films use render farms to create the smooth 
and sophisticated effects audiences expect.
Before the rendering stage, a great deal of effort 
goes into designing how an animated character 
or avatar will look, how it will move, and how its 
textures will behave during that movement. For 
example, a fur-covered avatar that moves swiftly 
outdoors in the wind should have a furry or hairy 
texture, with fibers that appear to blow in the wind. 
All of this must be designed and coordinated by 
computer animators. Typically, one of the first 
steps is keyframing, in which animators decide 
what the starting and ending positions and appear-
ance of the animated object will be. Then they de-
sign the movements between the beginning and 
end by assigning animation variables (avars) to 
different points on the object. This stage is called 
“in-betweening,” or “tweening.” Once avars are as-
signed, a computer algorithm can automatically 
change the avar values in coordination with one 
another. Alternatively, an animator can change 
“in-between” graphics by hand. When the program 
is run, the visual representation of the changing 
avars will appear as an animation.
In general, the more avars specified, the more 
detailed and realistic that animation will be in its 
movements. In an animated film, the main charac-
ters often have hundreds of avars associated with 
them. For instance, the 1995 film Toy Story used 
712 avars for the cowboy Woody. This ensures that 
the characters’ actions are lifelike, since the audi-
ence will focus attention on them most of the time. 
Coding standards for normal expressions and mo-
tions have been developed based on muscle move-
ments. The MPEG-4 international standard includes 
86 face parameters and 196 body parameters for an-
imating human and humanoid movements. These 
parameters are encoded into an animation file and 
can affect the bit rate (data encoded per second) or 
size of the file.
Educational Applications
Simulation has long been a useful method of training 
in various occupations. Pilots are trained in flight 
simulators, and driving simulators are used to pre-
pare for licensing exams. Newer applications have 
included training teachers for the classroom and 
improving counseling in the military. VR holds the 
promise of making such vocational simulations much 
more realistic. As more computing power is added, 
simulated environments can include stimuli that 
better approximate the many distractions and de-
tailed surroundings of the typical driving or flying 
situation, for instance.
VR in 3-D
Most instances of VR that people have experienced 
so far have been two-dimensional (2-D), occurring 
on a computer or movie screen. While entertaining, 
such experiences do not really capture the concept 
of VR. Three-dimensional (3-D) VR headsets such as 
the Oculus Rift may one day facilitate more lifelike 
business meetings and product planning. They may 
also offer richer vocational simulations for military 
and emergency personnel, among others.
—Scott Zimmer, JD
Bibliography
Chan, Melanie. Virtual Reality: Representations in 
Contemporary Media. New York: Bloomsbury, 2014. 
Print.
Gee, James Paul. Unified Discourse Analysis: Language, 
Reality, Virtual Worlds, and Video Games. New York: 
Routledge, 2015. Print.
Griffiths, Devin C. Virtual Ascendance: Video Games and 
the Remaking of Reality. Lanham: Rowman, 2013. 
Print.
Avatars were around long before social media. As computers 
have become more powerful and the rendering capabilities 
more efficient, avatars have improved in detail and diversity. 
(left) Anandeeta Gurung, CC0, via Wikimedia Commons. 
(right) Jason Rohrer, public domain, via Wikimedia Commons

30
Avatars and simulation
Principles of Computer Science
Hart, Archibald D., and Sylvia Hart Frejd. The Digital 
Invasion: How Technology Is Shaping You and Your 
Relationships. Grand Rapids: Baker, 2013. Print.
Kizza, Joseph Migga. Ethical and Social Issues in the 
Information Age. 5th ed. London: Springer, 2013. 
Print.
Lien, Tracey. “Virtual Reality Isn’t Just for Video 
Games.” Los Angeles Times. Tribune, 8 Jan. 2015. 
Web. 23 Mar. 2016.
Parisi, Tony. Learning Virtual Reality: Developing 
Immersive Experiences and Applications for Desktop, 
Web, and Mobile. Sebastopol: O’Reilly, 2015. Print.

31
BASIC
Fields of Study
Programming language
Abstract
BASIC is an imperative programming language that 
employs a linear logic format. Subroutines are inte-
gral components of a BASIC program, in contrast 
to functions in object-oriented language programs. 
Some aspects of the BASIC language facilitate errors 
in program code, but are normally easily amended. 
Modern versions of the BASIC language provide 
features of the object-oriented programming ap-
proach, making it more powerful and versatile. The 
majority of .NET developers use Visual BASIC.NET 
exclusively.
Prinicipal Terms

 algorithm: a set of step-by-step instructions for per-
forming computations. 

 cathode ray tube (CRT): a vacuum tube used to 
create images in devices such as older television 
and computer monitors. 

 central processing unit (CPU): electronic circuitry 
that provides instructions for how a computer 
handles processes and manages data from applica-
tions and programs. 

 imperative programming: programming that pro-
duces code that consists largely of commands is-
sued to the computer, instructing it to perform 
specific actions.

 main loop: the overarching process being carried 
out by a computer program, which may then in-
voke subprocesses. 

 programming languages: sets of terms and rules of 
syntax used by computer programmers to create 
instructions for computers to follow. This code is 
then compiled into binary instructions for a com-
puter to execute. 

 source code: the set of instructions written in a 
programming language to create a program. 

 variable: a symbol representing a quantity with no 
fixed value. 
BASIC Background
The Beginners All-Purpose Symbolic Instruction 
Code, or BASIC, was developed in 1964 by Kemeny 
and Kurtz and is one of the first programming lan-
guages made available to the general public. Prior to 
its development, programming often required each 
instruction to be made as a punched card that was 
then fed into a card reader in sequential order as a 
‘batch’ (hence the term ‘batch process’). Preparation 
of such programs was a specialized skill, primarily of 
physicists and mathematicians. Programs were run 
on mainframe computers such as Digital Equipment 
Corporation’s DEC PDP-11 in universities and major 
businesses that had the financial resources to afford 
them and required specialized training in languages 
such as ALGOL or FORTRAN with their complex 
syntax and terminology. The BASIC language was de-
veloped to make computer programming accessible 
to anyone by employing a simple, linear program 
structure and familiar ‘human’ words in its instruc-
tion set. The structure of a BASIC program is very 
similar to the structure of an ASSEMBLY language 
program, which is often referred to as ‘machine lan-
guage’ or ‘machine code’. The functioning of the 
BASIC language can therefore be understood as con-
verting ‘human’ code into ‘machine code’.
In the 1970s, an electronics revolution began 
with the introduction of the personal or micro 
computer, the first of which to be released com-
mercially was the MITS Altair 8800. Several other 
B

32
BASIC
Principles of Computer Science
electronics manufacturers released their own ver-
sions of personal computers in quick succession, 
each designed to connect to a TV set or other 
cathode ray tube (CRT) device and came with a 
version of BASIC as its standard programming 
language. The ready availability of such an easy-
to-learn programming language resulted in an 
explosion of user-created programs that could be 
readily transferred from one machine to another 
and amended or enhanced at will. These machines 
used 8-bit central processing units (CPU) designed 
and manufactured by various companies. Each 
different CPU chip used a slightly different archi-
tecture requiring its own version of ASSEMBLY 
language, and therefore its own version of BASIC. 
With only minor alterations, however, a BASIC 
program written for one type of microcomputer 
could be made to function on practically any other 
BASIC-capable microcomputer. With the introduc-
tion of the IBM 8088 microcomputer, and the es-
tablishment of the Microsoft Corporation’s propri-
etary version of BASIC as the de facto standard in 
those machines, the competition from other man-
ufacturers eventually fell by the wayside. The intro-
duction of the IBM 80286 CPU solidified the place 
of the IBM standard architecture with the intro-
duction of DOS and Windows operating systems in 
competition with Apple Corporation’s Macintosh 
graphics-based operating system. Despite these his-
torical changes, however, the BASIC programming 
language continues to be a valuable and reliable 
‘workhorse’ programming language, having been 
developed through several iterations. In 2006, for 
example, it was reported that some 59% of pro-
gram and application developers working on the 
.NET Framework for Microsoft Corporation used 
Visual BASIC.NET exclusively.
BASIC Program Structure
BASIC is an imperative programming language that 
is linear in structure and entirely self-contained, 
consisting of a logical series of commands. Each 
BASIC program is essentially an algorithm designed 
to produce a specific type of output from the data 
that is provided to it. Within the main loop of the 
program, or source code, any number of secondary 
algorithms may be accommodated. Each of these is 
designed to carry out a specific function or calcula-
tion using the values determined or provided for an 
input variable. First-generation BASIC was capable 
of carrying out fairly complex mathematical calcula-
tions such as complex polynomials and multi-expo-
nent relations, provided that they could be written 
out in tan executable format. Iterative calculations 
or operations are carried out using FOR-NEXT com-
mands, and selections are made using IF-THEN-
ELSE commands. The user ‘communicated’ with 
the BASIC program via the INPUT and PRINT com-
mands. Comments in the program code began with 
the REM command, allowing the programmer to 
document the source code without interfering with 
the program function. The program commands are 
carried out in sequential order until encountering 
the END command. Each command is predicated 
by a line number, followed by the command instruc-
tion and the input/output (I/O) variables as ap-
propriate. (Mainframe computers running BASIC 
normally used printer terminals to display the I/O 
of the program as hard copy.) More recent versions 
of BASIC incorporated more advanced features and 
operations, as well as object classes and structures 
that are typical of non-linear object-oriented pro-
gramming languages.
Subroutines versus Functions
The linear structure of BASIC programs relies on the 
use of subroutines that are integrated into the main 
loop. As the program code is executed line by line, 
a decision point in the program may direct the ex-
ecution to a specific section of commands that does 
not appear in the sequence. That section of code is 
a secondary algorithm that performs a specific task 
or operation. When that operation has been com-
pleted, execution returns to the main code and 
continues from where it left off. Different sections 
of the program code can refer to the same subrou-
tine any number of times. This eliminates the need 
to repeat the coding throughout the program. In 
more advanced versions of BASIC, a subroutine can 
exist outside of the main code, but must be identi-
fied by the SUB and END SUB identifiers. A func-
tion, on the other hand, is a feature of non-linear 
“object oriented” programming languages, such as 
C++ and Java, in which the main program may simply 
be a short list of functions that are to be executed. 
Unlike subroutines, functions are not dependent 
on the main program, and are portable between en-
tirely different programs. The source code of a single 

33
Principles of Computer Science
BASIC
function to be much longer than the source code of 
the main program.
Sample Problem
Write a simple BASIC program that ac-
cepts a value of the temperature in degrees 
Fahrenheit and outputs the corresponding 
temperature in degrees Celsius and Kelvin, 
then prompts for a new input value.
Answer:
10	
PRINT “Enter temperature in de-
grees Fahrenheit:”; Tf
20	
Tc = ((Tf – 32)*5)/9);
30	
Tk = Tc + 273.15;
40	
PRINT “Corresponding temperature 
in degrees Celsius is: ” Tc
50	
PRINT “Corresponding temperature 
in Kelvin is: ” Tk
60	
PRINT
70	
PRINT “Enter another value?”; A$
80	
IF LEN (A$) = 0 THEN GOTO 70
90	
A$ = LEFT$ (A$, 1)
100	 IF A$ = “Y” OR A$ = “y” THEN GOTO 10
110	 END
The algorithm here is to accept a numerical value 
from the user as the input variable, convert it to a new 
value, and then convert that to another new value. It 
then displays the calculated values, and prompts the 
user for a new input value. The program scans for the 
new input and if none is received it repeats sending 
the prompt and scanning for the new input until a 
keystroke is registered. If the keystroke is either an 
upper or lower case ‘’y’ as the LEFT character of the 
words ‘YES’ or ‘yes’, the program cycles back to the 
beginning and runs again. Any other keystroke fails 
this test and the program ENDs. The $ attached to 
any variable indicates a text ‘string’, and is typically 
called ‘string’’ instead of ‘dollar sign’.
Weaknesses of BASIC
BASIC continues to be a popular and versatile lan-
guage that is relatively easy to learn and to use. It 
does have some syntactical limitations that result 
in common errors. One of the most common er-
rors made by novice users of BASIC is failing to in-
clude the END statement at the end of the program, 
causing the computer to idle as it awaits a next com-
mand to appear. The other major flaw is the ease with 
which an error in syntax can lock the computer into 
an infinite loop in which it performs an operation 
continuously without advancing the program opera-
tion. With attention to detail, however, the BASIC 
language is a very versatile programming language 
that is highly applicable in many fields.
—Richard M. Renneboog MSc
Bibliography
Kemeny, John G. and Kurtz, Thomas E. (1985) Back 
To BASIC: The History, Corruption, and Future of the 
Language. Boston, MA: Addison-Wesley. Print.
Kumari, Ramesh (2005) Computers and Their 
Applications to Chemistry. 2nd ed. Oxford, UK: Alpha 
Science International. Print.
O’Regan, Gerard (2012) A Brief History of Computing. 
2nd ed., New York, NY: Springer-Verlag. Print.
Scott,Michael L. (2016) Programming Language 
Pragmatics. 4th ed., Waltham, MA: Morgan 
Kaufmann. Print.
Vick, Paul (2004) The Visual BASIC .NET Programming 
Language. Boston, MA: Addison-Wesley. Print.

34
Binary/hexadecimal representations
Principles of Computer Science
Binary/hexadecimal representations
Fields of Study
Computer Science; Computer Engineering; Software 
Engineering
Abstract
The binary number system is a base-2 number system. 
It is used by digital devices to store data and perform 
mathematical operations. The hexadecimal number 
system is a base-16 number system. It enables humans 
to work efficiently with large numbers stored as bi-
nary data.
Prinicipal Terms

 base-16: a number system using sixteen symbols, 0 
through 9 and A through F.

 base-2 system: a number system using the digits 0 
and 1.

 bit: a single binary digit that can have a value of 
either 0 or 1.

 byte: a group of eight bits.

 nibble: a group of four bits.
Understanding the Binary Number 
System
A mathematical number system is a way of representing 
numbers using a defined set of symbols. Number sys-
tems take their names from the number of symbols 
the system uses to represent numbers. For example, 
the most common mathematical number system is the 
decimal system, or base-10 system. Deci- means “ten.” 
It uses the ten digits 0 through 9 as symbols for num-
bers. Number systems can be based on any number of 
unique symbols, however. For example, the number 
system based on the use of two digit symbols (0 and 1) 
is called the binary or base-2 system.
Both the decimal and binary number systems use 
the relative position of digits in a similar way when rep-
resenting numbers. The value in the rightmost, or first, 
position is multiplied by the number of digits used in 
the system to the zero power. For the decimal system, 
this value is 100. For the binary system, this value is 20. 
Both 100 and 20 are equal to 1. Any number x raised to 
the zero power is equal to 1. The power used increases 
by one for the second position and so on.
Position 8
7
6
5
4
3
2
1
Seventh
Power
Sixth
Power
Fifth
Power
Fourth
Power
Third
Power
Second
Power
First
Power
Zero 
Power
Decimal 10,000,000
or 10 7
1,000,000
or 10 6
100,000
or 10 5
10,000
or 10 4
1,000
or 10 3
100
or 10 2
10
or 10 1
1
or 10 0
Binary
128 or 2 7
64 or 2 6
32 or 2 5 16 or 2 4 8 or 2 3
4 or 2 2
2 or 2 1
1 or 2 0
Position 3
2
1
Digits
2
3
4
Decimal 100 or 10 2 10 or 10 1
1 or 10 0
Position 8
7
6
5
4
3
2
1
Binary
Bit
1
1
1
0
1
0
1
0
128 or 2 7
64 or 2 6
32 or 2 5
16 or 2 4
8 or 2 3
4 or 2 2
2 or 2 1
1 or 2 0
Using the decimal number system, the integer 234 is represented by placing the symbols 2, 3, and 4 in  
positions 3, 2, and 1, respectively.
In the decimal system, 234 = (2 × 100) + (3 × 10) + (4 × 1), or (2 × 102) + (3 × 101) + (4x100). The binary system 
uses the relative position of the symbols 0 and 1 to express the integer 234 in a different manner.

35
Principles of Computer Science
Binary/hexadecimal representations
The American Standard Code for Information Interchange (ASCII) was an early system used to translate basic 
characters into a numerical code readable by computers. The common characters on a keyboard are provided 
with decimal and hexadecimal codes. EBSCO illustration.
DECIMAL
HEXADECIMAL
BINARY
00000000
00000001
00000010
00000011
00000100
00000101
00000110
00000111
00001000
00001001
00001010
00001011
00001100
00001101
00001110
00001111
00010000
00010001
00010010
00010011
00010100
00010101
00010110
00010111
00011000
00011001
00011010
00011011
00011100
00011101
00011110
00011111
00100000
00100001
00100010
00100011
00100100
00100101
00100110
00100111
00101000
00101001
00101010
00101011
00101100
00101101
00101110
00101111
00110000
00110001
00110010
00
01
02
03
04
05
16
15
14
13
12
11
17
10
0F
0E
0D
0C
0B
0A
09
08
07
06
18
19
1E
1D
1C
1B
1A
20
1F
22
24
26
23
21
29
28
27
25
2B
2D
2E
2C
30
32
31
2A
2F
0
1
2
3
4
5
22
21
20
19
18
17
23
16
15
14
13
12
11
10
9
8
7
6
24
25
30
29
28
27
26
32
31
34
36
38
35
33
41
40
39
37
43
45
46
44
48
50
49
42
47

36
Binary/hexadecimal representations
Principles of Computer Science
In the binary system, 234 = (1 × 128) + (1 × 64) + (1 
× 32) + (0 × 16) + (1 × 8) + (0 × 4) + (1 × 2) + (0 × 1), 
or 234 = (1 × 27) + (1 × 26) + (1 × 25) + (0 × 24) + (1 × 
23) + (0 × 22) + (1 × 21) + (0 × 20).
The Importance of the Binary Number 
System
The binary number system is used to store numbers and 
perform mathematical operations in computers systems. 
Such devices store data using transistors, electronic parts 
that can each be switched between two states. One state 
represents the binary digit 0 and the other, the binary 
digit 1. These binary digits are bits, the smallest units of 
data that can be stored and manipulated. A single bit can 
be used to store the value 0 or 1. To store values larger 
than 1, groups of bits are used. A group of four bits is a 
nibble. A group of eight bits is a byte.
Using Hexadecimal to Simplify Binary 
Numbers
The hexadecimal number system is a base-16 system. 
It uses the digits 0 through 9 and the letters A through 
F to represent numbers. The hexadecimal digit, or 
hex digit, A has a decimal value of 10. Hex digit B 
equals 11, C equals 12, D equals 13, E equals 14, and 
F equals 15. In hexadecimal, the value 10 is equal to 
16 in the decimal system. Using hexadecimal, a bi-
nary nibble can be represented by a single symbol. 
For example, the hex digit F can be used instead 
of the binary nibble 1111 for the decimal value 15. 
Sixteen different combinations of bits are possible 
in a binary nibble. The hexadecimal system, with six-
teen different symbols, is therefore ideal for working 
with nibbles.
Sample Problem
To work with binary numbers in digital applications, it is important to be able 
to translate numbers from their binary values to their decimal values. Translate 
the following binary byte to its decimal value: 10111001
Answer:
The decimal value of the binary byte 10111001 is 185. The decimal value can 
be determined using a chart and then calculating.
1
0
1
1
1
0
0
1
128 or 2 7
64 or 2 6
32 or 2 5
16 or 2 4
8 or 2 3
4 or 2 2
2 or 2 1
1 or 2 0
= (1 × 27) + (0 × 26) + (1 × 25) + (1 × 24) + (1 × 23) + (0 × 22) + (0 × 21) + (1 × 20)
= (1 × 128) + (0 × 64) + (1 × 32) + (1 ×16) + (1 × 8) + (0 × 4) + (0 × 2) + (1 × 1)
= 185
Position 8
7
6
5
4
3
2
1
Seventh
Power
Sixth
Power
Fourth
Power
Second
Power
First
Power
Zero 
Power
Decimal 10,000,000
or 10 7
1,000,000
or 10 6
10,000
or 10 4
100
or 10 2
10
or 10 1 1
or 10 0
268,435,456 
or 16 7
16,777,216 
or 16 6
Fifth
Power
100,000
or 10 5
1,048,576 
or 16 5
65,536 
or 16 4
Third
Power
1,000
or 10 3
4,096 
or 16 3
256 or 
16 2
16 or 
16 1
1
or 16 0
Hexa-
decimal

37
Principles of Computer Science
Binary/hexadecimal representations
Computers can quickly and easily work with large 
numbers in binary. Humans have a harder time using 
binary to work with large numbers. Binary uses many 
more digits than hexadecimal does to represent large 
numbers. Hex digits are therefore easier for humans 
to use to write, read, and process than binary. 
—Maura Valentino, MSLIS 
Bibliography 
Australian National University. Binary Representation 
and Computer Arithmetic. Australian National U, 
n.d. Digital file. 
Cheever, 
Erik. 
“Representation 
of 
Numbers.” 
Swarthmore College. Swarthmore College, n.d. Web. 
20 Feb. 2016. 
Govindjee, S. Internal Representation of Numbers. Dept. 
of Civil and Environmental Engineering, U of 
California Berkeley, Spring 2013. Digital File. 
Glaser, Anton. History of Binary and Other Nondecimal 
Numeration. Rev. ed. Los Angeles: Tomash, 1981. 
Print. 
Lande, Daniel R. “Development of the Binary 
Number System and the Foundations of Computer 
Science.” Mathematics Enthusiast 1 Dec. 2014: 513–
40. Print. 
“A Tutorial on Data Representation: Integers, 
Floating-Point Numbers, and Characters.” NTU.
edu. Nanyang Technological U, Jan. 2014. Web. 20 
Feb. 2016.
One disadvantage of using binary is that large 
numbers of digits are needed to represent large in-
tegers. For example, 1,000,000 is shown in binary 
digits as 11110100001001000000. The same number 
is shown in hex digits as F4240, which is equal to (15 × 
65,536) + (4 × 4,096) + (2 × 256) + (4 × 16) + (0 × 1).
Position
5
4
3
2
1
65,536 
or 16 4
4,096 
or 16 3
256 or 
16 2
16 or 
16 1
1
or 16 0
Hexadecimal
F or 15
4
2
4
0
Hex digits

38
Biochemical engineering
Principles of Computer Science
Biochemical engineering
Fields of Study
Biotechnology
Abstract
Biochemical engineering is a subfield of engineering 
focused on the creation of substances to be used in the 
production of food or industrial materials, using biolog-
ical as well as manufactured chemical ingredients. Some 
biochemical engineers, for example, have discovered 
how to get bacteria to help break down oil spills in order 
to minimize the environmental impact of such disasters.
Prinicipal Terms

 affinity chromatography: a technique for sepa-
rating a particular biochemical substance from a 
mixture based on its specific interaction with an-
other substance.

 blotting: a method of transferring RNA, DNA, and 
other proteins onto a substrate for analysis.

 interferometry: a technique for studying bio-
chemical substances by superimposing light waves, 
typically one reflected from the substance and one 
reflected from a reference point, and analyzing 
the interference.

 nuclear magnetic resonance (NMR) spectroscopy: 
a technique for studying the properties of atoms 
or molecules by applying an external magnetic 
field to atomic nuclei and analyzing the resulting 
difference in energy levels.

 polymerase chain reaction (PCR) machine: a ma-
chine that uses polymerase chain reaction to am-
plify segments of DNA for analysis; also called a 
thermal cycler.
Analyzing and Creating Materials
Much of a biochemical engineer’s time is spent 
studying different substances and biological pro-
cesses to better understand how they work. With this 
knowledge, these substances and processes can be 
repurposed or redirected to meet society’s needs for 
advanced materials.
The main purpose of DNA and similar pro-
teins in an organism is to produce different chem-
ical substances to keep the organism functioning. 
Biochemical engineers can cause DNA strands to act 
like miniature factories, producing the chemicals 
needed for a particular application. If a certain type 
of enzyme is needed to treat a disease, for example, 
it may be possible to locate a DNA segment in an or-
ganism that either naturally creates this material or 
can be “reprogrammed” to create it. In order to do 
so, biochemical engineers must be able to study the 
structure and composition of biomolecules (biolog-
ical molecules).
Methods of Analysis
Scientists use various techniques to study and analyze 
substances of interest. First, a sample of the substance 
must be prepared. This usually begins with blotting, 
which isolates DNA and other proteins for further 
study. When studying DNA, the sample may then be 
amplified in a polymerase chain reaction (PCR) ma-
chine. Polymerase chain reaction is a technique for 
producing many copies of a DNA strand. This allows 
close analysis or multiple tests to be performed on 
even a very small sample.
Once the sample is ready, there are several dif-
ferent methods that may be used to study it. Which 
A PCR machine is responsible for reading DNA code 
and replicating the code to make many copies through 
a series of very specific chemical reactions. Public do-
main, via Wikimedia Commons.

39
Principles of Computer Science
Biochemical engineering
method to use depends in part on the type of infor-
mation needed. Nuclear magnetic resonance (NMR) 
spectroscopy involves exposing a substance to a mag-
netic field in order to alter the spin of its atomic nu-
clei. This change in spin results in a change in the 
energy level of the nuclei. The difference in energy 
levels provides information about the physical and 
chemical properties of the atoms. NMR spectroscopy 
is most often used to study the structure of organic 
compounds.
Interferometry refers to several related tech-
niques involving wave superposition. In most cases, 
identical electromagnetic waves are reflected from 
both the sample material and a reference, then 
superimposed. The reference may be another sub-
stance, or it may be a mirror that reflects the wave 
unaltered. The resulting interference pattern will 
reveal any structural differences between the two 
samples. Interferometry can be used to identify ma-
terials, study their molecular structures, or provide 
detailed tissue imaging.
Affinity chromatography is a method for sepa-
rating specific substances from mixtures. “Affinity” 
refers to the fact that certain biomolecules have a 
strong tendency to bind to other specific types of 
molecules. For example, an antigen is a harmful bio-
molecule that elicits an immune response in an or-
ganism. The immune system responds by producing 
antibodies, which are proteins that are “designed” 
to bind to that specific antigen. Thus, if a researcher 
wanted to separate antibodies from a biochemical 
mixture, they would introduce antigens specific to 
those antibodies into the mixture. The antibody mol-
ecules would separate from the mixture and bind to 
the antigens instead. Affinity chromatography can be 
used to purify or concentrate a substance or to iden-
tify substances in a mixture.
Duties of a Biochemical Engineer
The task of a biochemical engineer is to look at real-
world problems and at the chemical and biological 
properties of organisms and materials as if they were 
pieces of a jigsaw puzzle, and find matches where 
two pieces fit together. They match up problems with 
solutions.
Biochemical engineers spend much of their time on 
design and analysis, but they are also involved in other 
tasks. Much of their time is spent working on product 
development. The overarching goal of the design 
work they do is to further the creation of products 
that will benefit society and generate profits for the 
company that markets them. Product development re-
quires a biochemical engineer to develop a thorough 
understanding of the needs and goals of a project in 
order to research potential solutions that fit with those 
needs. In the case of products with medical applica-
tions, there are also elaborate safety precautions that 
must be taken, as well as lengthy approval processes 
requiring cooperation with regulatory agencies.
Documentation and scholarly publishing are 
also important duties for biochemical engineers. 
Each stage of their research must be recorded and 
analyzed. Biochemical engineers are encouraged to 
share their findings with the global research commu-
nity whenever possible. While this sometimes raises 
concerns about the confidentiality of proprietary in-
formation, in the long run the sharing of research 
helps advance the field and encourage innovation. 
Many discoveries in the field of biochemical engi-
neering are carefully guarded because of their profit 
potential, as patents on biochemically engineered 
substances can be worth large amounts of money. 
This is why the patent system tries to balance mon-
etary rewards for inventors against society’s need for 
access to information.
Applications of Biochemical Engineering
One of the greatest applications of biochemical 
engineering is in the field of drug manufacturing. 
Traditional pharmaceutical manufacturing processes 
involve multiple types of chemical reactions, each 
a laborious step in the process of creating a batch 
of medicinal compound. Biochemical engineers 
reinvent this process by putting nature to work for 
them. They identify biological processes that can be 
tweaked to produce the same drugs without having 
to mix chemicals.
One newer area of research in biochemical en-
gineering is the artificial production of human or-
gans through processes similar to 3-D printing. By 
studying the ways that different types of tissue cells 
reproduce, biochemical engineers have been able 
to grow rudimentary structures such as replacement 
ears. Scientists anticipate that they will eventually be 
able to produce fully functioning hearts, livers, and 
other organs, avoiding the need for organ donation.
—Scott Zimmer, JD

40
Biomedical engineering
Principles of Computer Science
Bibliography
Katoh, Shigeo, Jun-ichi Horiuchi, and Fumitake 
Yoshida. Biochemical Engineering: A Textbook for 
Engineers, Chemists and Biologists. 2nd rev. and enl. 
ed. Weinheim: Wiley, 2015. Print.
Kirkwood, Patricia Elaine, and Necia T. Parker-
Gibson. Informing Chemical Engineering Decisions 
with Data, Research, and Government Resources. San 
Rafael: Morgan, 2013. Digital file.
Pourhashemi, Ali, ed. Chemical and Biochemical 
Engineering: New Materials and Developed Components. 
Rev. Gennady E. Zaikov and A. K. Haghi. Oakville: 
Apple Acad., 2015. Print.
Zeng, An-Ping, ed. Fundamentals and Application of 
New Bioproduction Systems. Berlin: Springer, 2013. 
Print.
Zhong, Jian-Jiang, ed. Future Trends in Biotechnology. 
Berlin: Springer, 2013. Print.
Zhou, Weichang, and Anne Kantardjieff, eds. 
Mammalian Cell Cultures for Biologics Manufacturing. 
Berlin: Springer, 2014. Print
Biomedical engineering
Fields of Study
Biotechnology; Computer Engineering
Abstract
Biomedical engineering (BME) applies engi-
neering design principles to biology in order to 
improve human health. It encompasses low-tech 
tools, such as crutches and simple prostheses, as 
well as highly sophisticated devices, such as pace-
makers and x-ray machines. Computer science is 
crucial to the design and production of many BME 
devices.
Prinicipal Terms

 bioinstrumentation: devices that combine biology 
and electronics in order to interface with a pa-
tient’s body and record or monitor various health 
parameters.

 biomechanics: the mechanics (structure and func-
tion) of living things in response to various forces.

 bioMEMS: short for “biomedical microelectrome-
chanical system”; a microscale or nanoscale self-
contained device used for various applications in 
health care.

 genetic modification: direct manipulation of an 
organism’s genome, often for the purpose of engi-
neering useful microbes or correcting for genetic 
disease.

 telemedicine: health care provided from a dis-
tance using communications technology, such as 
video chats, networked medical equipment, smart-
phones, and so on.
What Is Biomedical Engineering?
Biomedical engineering (BME) applies engineering 
design principles to the study of biology in order to 
improve health care. It developed from the overlap 
between engineering, biology, and medical research 
into its own branch of engineering. BME and related 
fields, such as biomedical research, remain distinct, 
as the case of the antibiotic penicillin shows. The ac-
cidental discovery that the Penicillium mold has anti-
bacterial properties was a by-product of basic medical 
research, not BME. However, efforts to design and 
implement system to mass-produce and distribute 
the drug were a textbook example of BME. Products 
of BME include prosthetics, imaging equipment, re-
generative medicine, medical implants, advances in 
drug production, and even telecommunications.
Although many innovations in BME are high tech, 
not all are. Consider crutches, which assist people 
with mobility after a foot or leg injury. Modern 
crutches are designed to be strong, lightweight, 
and comfortable during extended use. They are in-
formed by knowledge of human biology, especially 
biomechanics, to minimize strain on the patient’s 
bones and muscles.
Developing the Tools of Medicine
Many BME projects involve instrumentation, the 
various devices used in medical care. Such devices 
include imaging tools such as magnetic resonance 

41
Principles of Computer Science
Biomedical engineering
To build an artificial organ, live cells are placed on a scaffolding (an exact replica mold of the 
organ) and subjected to specific conditions to induce growth to develop the desired organ. By 
BruceBlaus, CC BY 3.0 (http://creativecommons.org/licenses/by/3.0), via Wikimedia Commons.

42
Biomedical engineering
Principles of Computer Science
imaging (MRI) machines, positron emission tomog-
raphy (PET) scanners, and x-ray machines. It also 
includes bioinstrumentation, devices specifically 
designed to connect to the human body. These ei-
ther monitor health parameters or provide a thera-
peutic benefit. Implanted pacemakers, for example, 
use low-voltage electrical impulses to keep the heart 
pumping in proper rhythm. Different bioMEMS 
can act as sensors, analyze blood or genes, or deliver 
drugs at a much smaller scale than traditional de-
vices. Some bioMEMS are known as “labs-on-a-chip” 
because they incorporate the functions of a full-sized 
biology lab on a computer chip.
The development of smaller, highly portable, easy-
to-use instrumentation is a keystone of telemedicine. 
With smartphone apps, bioMEMS, and cell-phone 
networks, it is increasingly possible to diagnose 
and treat patients far from hospitals and clinics. 
Telemedicine is especially helpful developing coun-
tries that lack a strong infrastructure.
Engineering Biology
Another major area of BME involves the engineering 
of life itself. This includes regenerative medicine, in 
which organs and tissues are grown in a lab, often 
from a combination of synthetic and organic bioma-
terials, to replace damaged or diseased ones. It also 
includes genetic engineering, in which an organism 
or cell’s genes are manipulated toward a desired 
end. Genetic engineering of bacteria can produce 
useful drugs. Mice and other animals have been en-
gineered to manifest traits such as cancer suscepti-
bility to make them better study subjects. Genetic en-
gineering could one day correct genetic conditions 
such as Huntington’s disease in humans. Traditional 
breeding for desired traits among study organisms is 
also used in BME, though it is a slower, less powerful 
technique.
Computers in Biomedical Engineering
Computer use is widespread in BME design and de-
vice creation. Consider the design of a custom pros-
thetic. Computers scan the amputation site to ensure 
a secure fit, model the limb and a prosthetic to the 
correct proportions, and even print components 
of the prosthesis using a 3-D printer. Computer sci-
ence techniques are also used to process the growing 
amount of patient data, searching out patterns of dis-
ease incidence.
Instrumentation is often built around computers. 
Medical imaging devices depend on embedded mi-
crocomputers and specialized software to process pa-
tient data and display it in a way that is useful to doc-
tors. Smaller devices such as digital thermometers 
depend on microprocessors. An electrocardiograph 
(EKG) monitors a patient’s heart rate and alerts the 
medical team if the patient goes into cardiac arrest. 
This is made possible through BME that combines bi-
ological knowledge (normal human heart rates) with 
sensors (electrodes that transmit electrical impulses 
from a patient’s skin to a display) and computer sci-
ence (processing the information, programming the 
device response). Advances in computer technology 
are driving advances in BME instrumentation. For 
instance, short-range wireless signals enable double 
amputees to walk by activating motorized joints and 
allowing the prostheses to communicate with one 
another.
—Kenrick Vezina, MS
Bibliography
Badilescu, Simona, and Muthukumaran Packirisamy. 
BioMEMS: Science and Engineering Perspectives. Boca 
Raton: CRC, 2011. Print.
Enderle, John Denis, and Joseph D. Brozino. 
Introduction to Biomedical Engineering. 3rd ed. 
Burlington: Elsevier, 2012. Print.
“Examples and Explanations of BME.” Biomedical 
Engineering Society. Biomedical Engineering Soc., 
2012–14. Web. 23 Jan. 2016.
“Biomedical 
Engineers.” 
Occupational 
Outlook 
Handbook, 2016–2017 Edition. Bureau of Labor 
Statistics, US Dept. of Labor, 17 Dec. 2015. Web. 
23 Jan. 2016.
“Milestones of Innovation.” American Institute for 
Medical and Biological Engineering. Amer. Inst. for 
Medical and Biological Engineering, 2016. Web. 
25 Jan. 2016.
Pavel, M., et al. “The Role of Technology and 
Engineering Models in Transforming Healthcare.” 
IEEE Reviews in Biomedical Engineering. IEEE, 2013. 
Web. 25 Jan. 2016.
Saltzman, W. Mark. “Lecture 1—What Is Biomedical 
Engineering?” BENG 100: Frontiers of Biomedical 
Engineering. Yale U, Spring 2008. Web. 23 Jan. 
2016.

43
Principles of Computer Science
Biometrics
Biometrics
Fields of Study
Biotechnology; Security; Computer Science
Abstract
Biometrics is the study of biology and metrics, or 
measurement, especially for the purpose of identi-
fying individuals based on unique characteristics. 
Digital scanning, measurement, and matching 
tools are used to create automated systems for veri-
fying identity through the comparison of biometric 
samples.
Prinicipal Terms

 bioinformatics: the scientific field focused on de-
veloping computer systems and software to ana-
lyze and examine biological data.

 biosignal processing: the measurement and moni-
toring of a biological signal, such as heart rate or 
blood oxidation, rather than an image.

 false match rate: the probability that a biometric 
system incorrectly matches an input to a template 
contained within a database.

 Hamming distance: a measurement of the differ-
ence between two strings of information.

 identifiers: measurable characteristics used to 
identify individuals.
The Science of Identifying Individuals
Biometrics is a scientific field that focuses on using 
human characteristics as identifiers to distinguish 
between different individuals. These include facial 
geometry, dermal patterns, biorhythms, and vocal 
character. Fingerprint identification was one of the 
first biometric techniques. It was first used in antiq-
uity before becoming a global standard in forensic 
identification in the 1800s. Advances in computer sci-
ence have allowed for the development of automated 
systems capable of identifying and matching other 
biometric identifiers. These include retinal blood 
vessel patterns, iris morphology, voices, and DNA.
Biometrics Basics
The ability to identify individuals is an important 
facet of human culture and society. Most people are 
instinctively able to identify each other by external 
features such as facial geometry and voice recogni-
tion. However, in some settings these methods are 
problematic and can lead to high false match rates. 
Individuals with similar voices or appearances may 
be mistaken for one another. Biometric science at-
tempts to reduce the probability of false matches 
by determining the most accurate characteristics to 
use for identification and by creating systems for in-
creasing the reliability of biometric security systems.
The skin of the human finger contains a pattern of 
ridges that are unique to each individual. The unique 
nature of fingerprints was discovered in antiquity, 
and in some cultures, including ancient Egypt and 
Babylonia, fingerprints were used in place of written 
signatures on documents or artworks. In the late 
1800s, scientists began experimenting with the use 
of fingerprints to identify individuals for the purpose 
of forensic analysis. Fingerprinting quickly became 
the most widespread form of biometric classification. 
However, fingerprints can be accidentally or pur-
posefully altered and can be imitated in a variety of 
ways, limiting their reliability for security and identi-
fication. In the twentieth and twenty-first centuries, 
biometrics specialists identified a number of other 
unique characteristics that can be used for identifica-
tion. These include patterns of blood vessels in the 
retina, the structure of the iris, the geometry of the 
human face, and the unique genetic code in each 
individual’s DNA. Biometrics became closely tied to 
advances in computer science that allowed such iden-
tifiers to be measured and recorded.
Sampling and Matching
Biometric identification involves two primary pro-
cesses: sampling and comparing biometric character-
istics. Bioinformatics is a multidisciplinary field that 
develops and uses technology to analyze or model 
biological data. Bioinformatics software enables 
a scanned sample, such as a fingerprint, to be con-
verted into a digital signal. A fingerprint scan creates 
a geometric map representing length, curvature, and 
other measurements, for instance. Various digital 
maps taken from samples can then be compared to 
determine if two samples came from the same indi-
vidual. In the case of iris identification, the pattern 
of the iris can be mapped geometrically and then 
compressed into a set of binary codes. Two binary 

44
Biometrics
Principles of Computer Science
codes can be compared, and a measurement called 
Hamming distance can be used to determine how 
different they are. Similar measurement methods are 
used in DNA matching. Patterns of genetic codes can 
be compared, and algorithms automatically calculate 
the potential that one or more samples represent the 
same individual.
When an individual’s identifi ers are added to a 
database, the process is called “enrollment.” Future 
biometric samples can be compared against all sam-
ples previously enrolled in the database. In biometric 
security technology, authorized individuals must fi rst 
create a template of biometric samples matched with 
their identity. Any user who attempts to access the 
system is then asked to submit a biometric sample 
for validation. If the submitted sample matches an 
approved template according to a matching algo-
rithm, access is granted. If it does not match, access 
can be denied.
Applied biometrics
The twenty-fi rst century has witnessed the commer-
cialization of biometric identifi cation for personal se-
curity. Most individuals use passwords, physical keys, 
or other types of codes to protect property and data. 
However, these security methods have signifi cant dis-
advantages. They can be lost, forgotten, stolen, or 
duplicated. Biometric characteristics, meanwhile, are 
unique, more diffi cult to forge, and cannot be lost or 
forgotten.
Improvements in computer technology have put 
biometrics in the reach of consumers. By the early 
Acquire
Collect biometric data
of the physical feature
metric
i
c 
h
i
l f
biometr
b
Match
Compare extracted pattern with
stored patterns to identify a match
Create a digital version
of the feature from 
the biometric data
Extract/isolate fine
details including
highly variable
data points to create
a distinct pattern
Localize
An attribute is scanned to make a digital representation, isolated details of that attribute are used 
to create an ID template for a particular person, and the template is matched to the “key” ID to 
retrieve the correct “key” for an individual. The number and location of isolated details must 
have a large enough hamming distance to ensure there is no duplication of templates. EBSCO 
illustration.

45
Principles of Computer Science
Biometrics
2000s, a number of companies began offering finger-
print scanning for security on personal computers 
and home security systems. Companies have also pro-
duced consumer versions of facial recognition, voice 
recognition, and iris/retinal scanning technology 
for home or commercial use. The most common use 
of biometric systems has been in personal computer 
security. For instance, technology company Apple’s 
Touch ID system uses fingerprint scanning. Certain 
smart devices using the Android operating system 
can use a facial recognition program to allow user ac-
cess. Some companies have introduced fingerprint 
or facial recognition as a key for electronic payment 
and debit transactions. Supporters claim that such 
systems offer greater security than traditional debit 
or credit cards, which can be stolen and used by 
other individuals.
Each available biometric system has its own ad-
vantages and disadvantages. For example, DNA 
analysis is the most accurate biometric characteristic. 
However, it depends on the ability to collect and an-
alyze DNA samples, which is a time consuming and 
technologically advanced process. Opponents argue 
that if fingerprints, for instance, are lifted and stolen, 
the identity theft victim has no recourse like a pass-
word reset. Thus, biometrics’ use for security may 
in fact present a great risk. In the 2010s, biometrics 
systems shifted toward a multimodal approach using 
multiple identifiers simultaneously to identify indi-
viduals. For instance, a number of researchers have 
suggested that biosignal processing could be used in 
conjunction with other biometric measures to pro-
duce more accurate biometric security and identifi-
cation systems.
—Micah L. Issitt
Bibliography
Biometric Center of Excellence. Federal Bureau of 
Investigation, 2016. Web. 21 Jan. 2016.
“Book—Understanding 
Biometrics.” 
Griaule 
Biometrics. Griaule Biometrics, 2008. Web. 21 Jan. 
2016.
“Introduction 
to 
Biometrics.” 
Biometrics.gov. 
Biometrics.gov, 2006. Web. 21 Jan. 2016.
Jain, Anil K., Arun A. Ross, and Karthik Nandakumar. 
Introduction to Biometrics. New York: Springer, 2011. 
Print.
Modi, Shimon K. Biometrics in Identity Management. 
Boston: Artech House, 2011. Print.
Shahani, Aarthi. “Biometrics May Ditch the Password, 
But Not the Hackers.” All Things Considered. NPR, 
26 Apr. 2015. Web. 21 Jan. 2016.
Van den Broek, Egon L. “Beyond Biometrics.” Procedia 
Computer Science 1.1 (2010): 2511–19. Print.

46
Biotechnology
Principles of Computer Science
Biotechnology
Fields of Study
Biotechnology; Robotics
Abstract
Biotechnology uses living organisms or their derivatives 
to make products. Farming and animal husbandry are 
early examples. Modern examples include biomedi-
cines, gene therapies, prostheses, and skin grafts.
Prinicipal Terms

 bioinformatics: the use of computer-based tools 
and techniques to obtain and evaluate biological 
data.

 bioinstrumentation: the use of instruments to 
record data about an organism’s physiology and 
functions.

 biomaterials: natural or synthetic materials that 
can be used to replace, repair, or modify organic 
tissues or systems.

 biomechanics: the various mechanical processes 
such as the structure, function, or activity of or-
ganisms.

 bionics: the use of biologically based concepts and 
techniques to solve mechanical and technological 
problems.
Biological Technology
“Technology” can be broadly defined as the use of 
scientific knowledge to solve practical problems. All 
devices, from the earliest stone tools to modern com-
puters, are examples of technology. Biotechnology 
is the use of organisms or biological products in this 
problem-solving process.
Farming and animal husbandry are among the 
earliest forms of biotechnology. They represent the 
application of knowledge to increase food produc-
tion. In the twenty-first century, biotechnology spans 
a broad range of industrial and scientific processes, 
from gene therapies to skin grafts.
Studying Biological Structures and 
Functions
Using animal skins for clothing, or plant mate-
rial for roofing and food, may seem intuitive by 
modern standards. However, it took considerable 
experimentation and careful study to learn how to 
use natural products. Secondary natural products 
such as wine and cheese require deeper knowledge 
of organic interactions. A wine or cheese producer 
must know how chemical processes and microor-
ganisms modify other biological products (grape 
juice, milk).
Basic science research into organisms’ physiology 
and behavior drives biotechnology and its subfields. 
It was first recognized as a unique field of study in the 
1970s and soon became one of the fastest-growing in-
dustrial fields. Among the more controversial early 
advancements was the genetic engineering of crops, 
which became common in the US food market in the 
1990s.
Bioinformatics combines basic science, engi-
neering, math, and computer science to advance the 
study of organisms. Thanks to twenty-first-century 
computer technology, bioinformatics is a leading 
field in biological research. Bioinformatics data is 
often directly used to develop biotechnology.
Bioinstrumentation develops specialized tools to 
record data about living organisms. Bioinstruments 
that record physiological processes such as blood 
pressure and brain activity have been crucial to ad-
vances in medicine. They have also led to products 
such as wearable heart-rate monitors, which became 
a popular fitness tool in the 2010s.
Imitating Nature
Another facet of biotechnology is the development of 
synthetic technology that mimics biological systems. 
The subfield of biomechanics has resulted in a variety 
of medical and industrial products. It also informs 
the industrial science of bionics, which develops syn-
thetic tools that mirror the form or function of or-
ganic structures. For example, Velcro was designed to 
mimic the way the burrs of some plants cling to soft 
material such as clothing. Newer adhesive technology 
uses hydrogen bonding, the same phenomenon that 
allows geckos to cling even to smooth surfaces.
Bionics is actually a very old field. Many machines 
and devices, including the earliest flying machines 
and even building structures, have been inspired 
by similar biological structures. Bionics and biome-
chanics are also key to robotics and prosthetics.

47
Principles of Computer Science
Biotechnology
Modifying Nature
In some cases, biotechnology can involve creating tools, 
techniques, and processes to modify living organisms. 
For instance, biomaterials science creates synthetic or 
biological tissues to modify organisms. Skin grafts, arti-
ficial tooth replacement, and biosynthetic heart valves 
are examples of biomaterials used in medicine.
Biotechnology may also involve creating new 
hybrid or modified organisms. Though genetic 
modification of plants and animals is still in its in-
fancy, it could have dramatic impact on agriculture 
in coming decades. The ability to modify specific 
genes and genetic systems also has implications for 
the future of medicine. For instance, it might lead to 
the eventual elimination of many genetically trans-
mitted conditions. In addition, the emerging field 
of pharmacogenomics studies patient DNA to learn 
how different patients react differently to medica-
tions. It may one day be possible to tailor medicine 
to a patient’s particular genetic complement.
Biotechnology in the Future
The development of genetics, genomics, and other 
gene-related fields has opened a wealth of new possi-
bilities for biotechnology research. From the earliest 
wooden tools and animal-skin clothing to robotic sys-
tems that mimic biomechanical movement, biotech-
nology has long played an important role in human 
life. Today, with scientists studying biological forms 
and functions at the level of individual genes and 
cells, biotechnology has become a highly profitable 
field of industrial and medical research.
—Micah L. Issitt
A pacemaker is an example of technology working with a biological system. Biotechnology can enhance the 
efficiency of a biological system or replace nonfunctional components of the system. By National Heart Lung 
and Blood Institute (NIH), public domain, via Wikimedia Commons.

48
Biotechnology
Principles of Computer Science
Bibliography
“Biotechnology.” ACS. Amer. Chemical Soc., n.d. 
Web. 27 Jan. 2016.
Godbey, W. T. An Introduction to Biotechnology: 
The Science, Technology and Medical Applications. 
Waltham: Academic, 2014. Print.
Intro to Biotechnology: Techniques and Applications. 
Cambridge: NPG Educ., 2010. Scitable. Web. 20 
Jan. 2016.
“Introduction to Biotechnology.” Center for Bioenergy 
and Photosynthesis. Arizona State U, 13 Feb. 2006. 
Web. 20 Jan 2016.
Pele, Maria, and Carmen Cimpeanu. Biotechnology: 
An Introduction. Billerica: WIT, 2012. Print.
Thieman, William J., and Michael A. Palladino. 
Introduction to Biotechnology. 3rd ed. San Francisco: 
Benjamin, 2012. Print.

49
C++
Fields of Study
Programming 
languages; 
Software; 
Software 
engineering
Abstract
C++ is an object-oriented programming language 
developed by Bjarne Stroustrup as an extension of 
the C programming language that preceded it. Both 
languages are very versatile and widely used. C++ 
programs use the same header-body format as C pro-
grams, but all input-output operations have been 
streamlined into a single header called “iostream,” 
with input and output controlled by the “cin >>“ and 
“cout <<“ operators, respectively. C and C++ are the 
principal languages that form the basis of the Java 
universal programming language.
Prinicipal Terms

 class: a collection of independent objects that 
share similar properties and behaviors. 

 function: instructions read by a computer’s pro-
cessor to execute specific events or operations. 

 object: an element with a unique identity and a de-
fined set of attributes and behaviors. 

 object-oriented programming: a type of program-
ming in which the source code is organized into 
objects, which are elements with a unique identity 
that have a defined set of attributes and behaviors.

 pseudocode: a combination of a programming 
language and a spoken language, such as English, 
that is used to outline a program’s code. 
 software: the sets of instructions that a computer fol-
lows in order to carry out tasks. Software may be stored 
on physical media, but the media is not the software. 

 source code: the set of instructions written in a 
programming language to create a program.

 syntax: rules that describe how to correctly struc-
ture the symbols that comprise a language. 

 variable: a symbol representing a quantity with no 
fixed value. 
Historical Aspects and Characteristics 
of C++
The C++ programming language was developed 
by Bjarne Stroustrup in the early 1980s, at Bell 
Laboratories as an advanced form of the C program-
ming language. The name C++ comes from the use 
of the “++” modifier within the C++ language to in-
crementally increase the value of the variable that it 
modifies. The C language was developed in its turn 
from the BCPL language (1967) that was used for 
writing operating system software and compilers, and 
from the B language (1970) that was used to create 
early versions of the UNIX operating system. It is also 
capable of incorporating program segments written 
in other languages such as Assembly. C and C++ form 
the basis of the Java language, which is used on al-
most all electronic devices. C++ is an object-oriented 
programming language, and is “backwards com-
patible” with C; any program or part of a program 
written in C will compile and run under C++. Object-
oriented programs are easier to understand, correct 
and modify than programming techniques charac-
teristic of languages like BASIC and FORTRAN. A 
C++ program consists of two parts: a “head” and a 
“body.” The head of the program is a collection of 
class definitions and function statements, while the 
body contains the functions and variables that will be 
used to manipulate data.
Classes, Functions and Objects
The variables and functions of a C++ program 
are assigned to defined classes. As an analogy, the 
C

50
C++
Principles of Computer Science
assignment of classes in a C++ program has the same 
relationship as the different materials used in building 
a house. Each function exists independently of the 
program that uses it, just as the bricks used to build a 
wall of a house exist independently of the wall they are 
used to build. Different shapes and sizes of bricks and 
blocks may be used in the construction, but they can 
all be assigned to the same class of “stone and stone-
like building materials” based on their common fea-
tures. Different shapes and sizes of wood pieces may 
be assigned to another class of building materials, and 
so on. In the same way, variables and functions that 
have certain common characteristics belong to the 
same class. Just as a brick is an object used to construct 
a wall, a function is an object used to construct a pro-
gram. In general, the structure of a C++ program has a 
“head” section consisting of standard class statements 
that provide standard functionalities such as control 
of input and output, mathematical functions and op-
erations, and so on, that are specified by “include” 
statements. This is followed by a main program called 
“main,” which is essentially just a list of functions that 
the “main” program will use, as well as some variable 
designations and relations. Following the “main” func-
tion are the functions that “main” will utilize. The gen-
eral format of a C++ program is thus
#include statement
#include statement
main()
{variables
function1
function2
function3, etc
decision statements, relations, etc
}
function1 ()
{function1 statements, etc
}
function2 ()
{function2 statements, etc
}
function3 ()
{function3 statements, etc
}
This description shows how the program is assem-
bled from various independent objects rather than 
being entirely self-contained as would be the case 
with a program written in a language such as BASIC. 
It should also be apparent that any of the individual 
functions could be utilized as is or with minor altera-
tions in any other program.
Designing a C++ Program
Much as a building is designed and constructed from 
various standard and customized components to 
achieve the desired end product, a C++ program is 
designed and constructed from a variety of standard 
and customized components. The foundation parts 
of the program are the libraries of standard class 
definitions and functions, and the syntax of the pro-
gramming language. The program is typically first 
designed using pseudocode, which is just an easily 
modified “blueprint” for the construction of the ac-
tual program. An example of pseudocode might be
header components (iostream)
main()
{   define graphics interface
cout<< graphics interface
{data input ()
{   prompt for data input
cin>> data
}
}
calculate output values()
{   calculate power loss ()
calculate output frequency ()
calculate integral of frequency equation ()
calculate first derivative of frequency equa-
tion ()
calculate second derivative of frequency 
equation ()
return values
}
cout<< calculated values to file
cout<< calculated values to graphic display
new data input?
If yes then data input () else end
end
}
calculate power loss()
calculate output frequency ()
calculate integral of frequency equation ()
calculate first derivative of frequency equation ()
calculate second derivative of frequency equation ()

51
Principles of Computer Science
C++
Pseudocode will not compile and run. Each of the 
functions that is specified in the above example after 
the end of the main () function may in fact be pieces 
of independent source code that are much longer 
than the actual program source code. Some features 
of note are the designations “iostream,” “cin >>” and 
“cout<<.” “iostream” is the standard C++ header and 
includes all of the standard definitions of file and 
variable types, input and output functions used in 
C++ programs. Including this header eliminates that 
need to repeat the corresponding information when 
writing out a program, and inclusion of the iostream 
header is actually a standardization requirement. 
As their names suggest “cin>>” is the standard state-
ment for input of all data types and formats in C++, 
and “cout<<” is the corresponding output statement. 
They take the place of several different input and 
output functions that were used in the C language 
before C++ was developed.
Sample Problem
Write a simple C++ program that asks for the 
users name and outputs a simple greeting.
Answer:
#include <iostream> 
int main ()
{
std::cout << “What is your name?\n”;
std::cin >> name;
cout << “Hello, ” name;
}
The iostream header is included at the beginning 
of every program to define the methods and for-
mats of input and output used in C++ programs. The 
main() function defines the order of operations to 
be carried out in the program. The standard output 
(std::cout <<) displays the text within quotation marks 
on the screen exactly as they are stated. The standard 
input (std::cin >>) accepts all subsequent keystrokes in 
order until the “enter” key is pressed, and assigns them 
to a variable called “name.” The output function then 
prints the text phrase “Hello,” to the screen followed 
by the value of the variable “name” (which in this case 
is supposed to be the name of the person using the 
program, but in reality could be any string of charac-
ters that was entered after the input prompt).
Importance and Development of C++
The C and C++ languages are essentially universal 
programming languages. It is possible to write C++ 
programs that will compile and run on any other 
computer. A number of variants of the language have 
been developed, including Visual C++ and C#. The 
language is well suited to game design and program-
ming due to the extensive libraries of calculation and 
graphics functions that have been developed, and its 
ability to incorporate modules written in compatible 
languages. Perhaps the most important development 
of C++ is that it is the foundation of the Java program-
ming language which is used by almost every elec-
tronic device in the world as a universal standard. A 
program written in Java will run on any Java-enabled 
device of any kind, such as smartphones, calculators, 
programmable logic controllers, gaming consoles, 
tablets, guidance control systems, and many others, 
including computers.
—Richard M. Renneboog M.Sc.
Bibliography
Davis, Stephen R. (2015) Beginning Programming with 
C++ for Dummies 2nd ed. Joboken, NJ: John Wiley & 
Sons. Print.
Deitel, H.M. And Deitel, P.J. (2009) C++ for 
Programmers Upper Saddle River, NJ: Pearson 
Education Incorporated. Print.
Graham W. Seed (2012) An Introduction to Object-
Oriented Programming in C++ with Applications 
in Computer Graphics New York, NY: Springer 
Science+Business Media. Print.
Kernighan, Brian W. and Ritchie, Dennis M. (1978) 
The C Programming Language Englewood Cliffs, NJ: 
Prentice-Hall. Print
Lippman, Stanley B, Lajoie, Josée and Moo, Barbara 
E. (2013) C++ Primer 5th ed. Upper Saddle River, 
NJ: Addison-Wesley. Print.
McGrath, Mike (2015) C++ Programming in Easy Steps 4th 
ed. Leamington Spa, UK: Easy Steps Limited. Print.
Stroustrup, Bjarne (2013) The C++ Programming 
Language 4th ed. Upper Saddle River, NJ: Addison-
Wesley. Print.

52
CAD/CAM
Principles of Computer Science
CAD/CAM
Fields of Study
Applications; Graphic Design
Abstract
Computer-aided design (CAD) and computer-aided 
manufacturing (CAM) are software that enable users 
to design products and, through the use of computer-
guided machinery, manufacture them according to 
the necessary specifications. CAD/CAM programs are 
used in a wide range of industries and play a key role in 
rapid prototyping, a process that allows companies to 
manufacture and test iterations of a product.
Prinicipal Terms

 four-dimensional building information modeling 
(4-D BIM): the process of creating a 3-D model 
that incorporates time-related information to 
guide the manufacturing process.

 rapid prototyping: the process of creating physical 
prototype models that are then tested and evalu-
ated.

 raster: a means of storing, displaying, and editing 
image data based on the use of individual pixels.

 solid modeling: the process of creating a 3-D rep-
resentation of a solid object.

 vector: a means of storing, displaying, and editing 
image data based on the use of defined points and 
lines.
Applications of CAD/CAM
The term “CAD/CAM” is an acronym for “computer-
aided design” and “computer-aided manufacturing.” 
CAD/CAM refers collectively to a wide range of com-
puter software products. Although CAD software and 
CAM software are considered two different types of 
programs, they are frequently used in concert and 
thus associated strongly with each other. Used pri-
marily in manufacturing, CAD/CAM software en-
ables users to design, model, and produce various 
objects—from prototypes to usable parts—with the 
assistance of computers.
CAD/CAM software originated in the 1960s, when 
researchers developed computer programs to assist 
professionals with design and modeling. Prior to that 
point, designing objects and creating 3-D models 
was a time-consuming process. Computer programs 
designed to aid with such tasks represented a signifi-
cant time savings. By the late 1960s, CAD programs 
began to be used alongside early CAM software. CAM 
enabled users to instruct computer-compatible ma-
chinery to manufacture various objects according to 
digital designs. The use of CAD/CAM software be-
came widespread over the following decades.
CAD/CAM is now a key part of the manufac-
turing process for numerous companies, from large 
corporations to small start-ups. Industries in which 
CAD/CAM proved particularly useful include the 
automotive and computer technology industries. 
However, CAM software has also been widely used 
in less obvious fields, including dentistry and textile 
manufacturing.
Physical
models
CAM
Digital
representations
Design
Display
Analysis
CAD
Computer-aided design (CAD) and computer-aided manufac-
turing (CAM) are used in many industries to fulfill the same 
basic goals. 3-D items are scanned and analyzed, new items 
are designed, and those designs can then be translated into 
manufactured items through CAM, which can develop the 
program necessary for machines to properly create the new 
item. EBSCO illustration.

53
Principles of Computer Science
CAD/CAM
In addition to its use alongside CAM software, CAD 
software functions alone in a number of fields. CAD 
software allows users to create 3-D models of objects 
or structures that do not need to be manufactured 
by machine. For instance, specialized CAD software 
are used in architecture to design floor plans and 3-D 
models of buildings.
Computer-Aided Design
Using CAD/CAM software is a two-part process that 
begins with design. In some cases, the user begins de-
signing an object by using CAD software to create 2-D 
line drawings of the object. This process is known as 
“drafting.” He or she may then use tools within the 
CAD software to transform those 2-D plans into a 3-D 
model. As CAD/CAM is used to create physical ob-
jects, the modeling stage is the most essential stage 
in the design process. In that stage, the user creates 
a 3-D representation of the item. This item may be 
a part for a machine, a semiconductor component, 
or a prototype of a new product, among other 
possibilities.
In some cases, the user may create what is known 
as a “wire-frame model,” a 3-D model that resembles 
the outline of an object. However, such models do 
not include the solid surfaces or interior details of 
the object. Thus, they are not well suited for CAM, 
the goal of which is to manufacture a solid object. 
As such, those using CAD software in a CAD/CAM 
context often focus more on solid modeling. Solid 
modeling is the process of creating a 3-D model of 
an object that includes the object’s edges as well as 
its internal structure. CAD software typically allows 
the user to rotate or otherwise manipulate the cre-
ated model. With CAD, designers can ensure that all 
the separate parts of a product will fit together as in-
tended. CAD also enables users to modify the digital 
model. This is less time-consuming and produces less 
waste than modifying a physical model.
When designing models with the intention of 
manufacturing them through CAM technology, users 
must be particularly mindful of their key measure-
ments. Precision and accurate scaling are crucial. As 
such, users must be sure to use vector images when 
designing their models. Unlike raster images, which 
are based on the use of individual pixels, vector im-
ages are based on lines and points that have defined 
relationships to one another. No matter how much 
a user shrinks or enlarges a vector image, the image 
will retain the correct proportions in terms of the 
relative placement of points and lines.
Computer-Aided Manufacturing
After designing an object using CAD software, a user 
may use a CAM program to manufacture it. CAM pro-
grams typically operate through computer numerical 
control (CNC). In CNC, instructions are transmitted 
to the manufacturing machine as a series of numbers. 
Those instructions tell the machine how to move and 
what actions to perform in order to construct the ob-
ject. The types of machines used in that process vary 
and may include milling machines, drills, and lathes.
In the early twenty-first century, 3-D printers, de-
vices that manufacture objects out of thin layers of 
plastic or other materials, began to be used in CAM. 
Unlike traditional CNC machinery, 3-D printers 
are typically used by individuals or small companies 
for whom larger-scale manufacturing technology is 
excessive.
Specialized Applications
As CAD/CAM technology has evolved, it has come 
to be used for a number of specialized applications. 
Some CAD software, for instance, is used to perform 
four-dimensional building information modeling 
(4-D BIM). This process enables a user to incorpo-
rate information related to time. For instance, the 
schedule for a particular project can be accounted 
for in the modeling process with 4-D BIM.
Another common CAD/CAM application is rapid 
prototyping. In that process, a company or individual 
can design and manufacture physical prototypes of 
an object. This allows the designers to make changes 
in response to testing and evaluation and to test dif-
ferent iterations of the product. The resulting pro-
totypes are often manufactured using 3-D printers. 
Rapid prototyping results in improved quality con-
trol and a reduced time to bring a product to market.
—Joy Crelin
Bibliography
Bryden, Douglas. CAD and Rapid Prototyping for 
Product Design. London: King, 2014. Print.
Chua, Chee Kai, Kah Fai Leong, and Chu Sing 
Lim. Rapid Prototyping: Principles and Applications. 
Hackensack: World Scientific, 2010. Print.

54
Cloud computing
Principles of Computer Science
“Computer-Aided Design (CAD) and Computer-
Aided Manufacturing (CAM).” Inc. Mansueto 
Ventures, n.d. Web. 31 Jan. 2016.
“Design and Technology: Manufacturing Processes.” 
GCSE Bitesize. BBC, 2014. Web. 31 Jan. 2016.
Herrman, John. “How to Get Started: 3D Modeling 
and Printing.” Popular Mechanics. Hearst Digital 
Media, 15 Mar. 2012. Web. 31 Jan. 2016.
Krar, Steve, Arthur Gill, and Peter Smid. Computer 
Numerical Control Simplified. New York: Industrial, 
2001. Print.
Sarkar, Jayanta. Computer Aided Design: A Conceptual 
Approach. Boca Raton: CRC, 2015. Print.
Cloud computing
Fields of Study
Information 
Technology; 
Computer 
Science; 
Software
Abstract
Cloud computing is a networking model in which 
computer storage, processing, and program access 
are handled through a virtual network. Cloud com-
puting is among the most profitable IT trends. A host 
of cloud-oriented consumer products are available 
through subscription.
Prinicipal Terms

 hybrid cloud: a cloud computing model that com-
bines public cloud services with a private cloud 
platform linked through an encrypted connec-
tion.

 infrastructure as a service: a cloud computing 
platform that provides additional computing re-
sources by linking hardware systems through the 
Internet; also called “hardware as a service.”

 multitenancy: a software program that allows mul-
tiple users to access and use the software from dif-
ferent locations.

 platform as a service: a category of cloud com-
puting that provides a virtual machine for users to 
develop, run, and manage web applications.

 software as a service: a software service system in 
which software is stored at a provider’s data center 
and accessed by subscribers.

 third-party data center: a data center service pro-
vided by a separate company that is responsible for 
maintaining its infrastructure.
Cloud Network Design
Cloud computing is a networking model that allows 
users to remotely store or process data. Several major 
Internet service and content providers offer cloud-
based storage for user data. Others provide virtual 
access to software programs or enhanced processing 
capabilities. Cloud computing is among the fastest-
growing areas of the Internet services industry. It 
has also been adopted by government and research 
organizations.
Types of Cloud Networks
Private clouds are virtual networks provided to a lim-
ited number of known users. These are often used in 
corporations and research organizations. Operating 
a private cloud requires infrastructure (software, 
servers, etc.), either on-site or through a third party. 
Public clouds are available to the public or to paying 
subscribers. The public-cloud service provider owns 
and manages the infrastructure. Unlike private 
clouds, public clouds provide access to an unknown 
pool of users, making them less secure. Public clouds 
tend to be based on open-source code, which is free 
and can be modified by any user.
The hybrid cloud lies somewhere between the two. 
It offers access to private cloud storage or software 
services, such as database servers, while keeping some 
services or components in a public cloud. Setup costs 
may be lower with hybrid cloud services. A group 
using a hybrid cloud outsources some aspects of in-
frastructure investment and maintenance but still en-
joys greater security than with a public cloud. Hybrid 
clouds have become widespread in the health care, 
law, and investment fields, where sensitive data must 
be protected on-site.

55
Principles of Computer Science
Cloud computing
cloud computing as a service
The infrastructure as a service (IaaS) model offers 
access to virtual storage and processing capability 
through a linked network of servers. Cloud-based 
storage has become popular, with services such as 
Apple iCloud and Dropbox offering storage alter-
natives beyond the memory on users’ physical com-
puters. IaaS can also give users greater computing 
power by allowing certain processes to run on virtual 
networks, rather than on the hardware of a single 
system. Using IaaS enables companies to create a cor-
porate data center through third-party data centers. 
These third-party centers provide expert IT assistance 
and server resources, generally for subscription fees.
The platform as a service (PaaS) model mainly of-
fers access to a specifi c platform that multiple users 
can use to develop software applications, or apps. 
Many apps require access to specifi c development 
programs. The Google App Engine and IBM’s de-
veloper Works Open provide an environment that 
stores, supports, and runs web apps. PaaS allows 
software developers to create apps without 
investing in infrastructure and data center 
support. Providers may also offer virtual 
storage, access to virtual networks, and 
other services.
The software as a service (SaaS) model 
offers users subscription-based or shared 
access to software programs through a 
virtual network. Adobe Systems’ Creative 
Cloud provides access to programs such as 
Photoshop, Illustrator, and Lightroom for 
a monthly fee. Users pay a smaller amount 
over time rather than paying a higher cost 
up front to purchase the program. SaaS sup-
ports multitenancy, in which a single copy 
of a program is available to multiple clients. 
This allows software providers to earn rev-
enue from multiple clients through a single 
instance of a software program.
Advantages and disadvantages of 
the cloud
Cloud networking allows small compa-
nies and individuals access to develop-
ment tools, digital storage, and software 
that once were prohibitively expensive or 
required signifi cant management and ad-
ministration. By paying subscription fees, 
users can gain monthly, yearly, or as-used 
access to software or other computing tools with out-
sourced administration. For service providers, cloud 
computing is cost effective because it eliminates the 
cost of packaging and selling individual programs and 
other products.
Data security is the chief concern among those 
considering cloud computing. The private and hybrid 
cloud models provide a secure way for companies to 
reap the benefi ts of cloud computing. Firewalls and 
encryption are common means of securing data in 
these systems. Providers are working to increase the 
security of public clouds, thus reducing the need for 
private or hybrid systems.
—Micah L. Issitt
bibliography
Beattie, Andrew. “Cloud Computing: Why the Buzz?” 
Techopedia. Techopedia, 30 Nov. 2011. Web. 21 Jan. 
2016.
CLOUD
Cloud computing refers to the use of processors, memory, and other peripheral 
devices offsite, connected by a network to one’s workstation. Use of the cloud 
protects data by storing it and duplicating it offsite and reduces infrastructure 
and personnel needs. EBSCO illustration.

56
Combinatorics
Principles of Computer Science
Huth, Alexa, and James Cebula. The Basics of Cloud 
Computing. N.p.: Carnegie Mellon U and US 
Computer Emergency Readiness Team, 2011. PDF 
file.
Kale, Vivek. Guide to Cloud Computing for Business and 
Technology Managers. Boca Raton: CRC, 2015. Print.
Kruk, Robert. “Public, Private and Hybrid Clouds: 
What’s the Difference?” Techopedia. Techopedia, 
18 May 2012. Web. 21 Jan. 2016.
Rountree, Derrick, and Ileana Castrillo. The Basics of 
Cloud Computing. Waltham: Elsevier, 2014. Print.
Ryan, Janel. “Five Basic Things You Should Know 
about Cloud Computing.” Forbes. Forbes.com, 30 
Oct. 2013. Web. 30 Oct. 2013.
Sanders, James. “Hybrid Cloud: What It Is, Why It 
Matters.” ZDNet. CBS Interactive, 1 July 2014. Web. 
10 Jan. 2016.
Combinatorics
Fields of Study
Information 
Technology; 
Algorithms; 
System 
Analysis
Abstract
Combinatorics is a branch of mathematics that is con-
cerned with sets of objects that meet certain condi-
tions. In computer science, combinatorics is used to 
study algorithms, which are sets of steps, or rules, de-
vised to address a certain problem.
Prinicipal Terms

 analytic combinatorics: a method for creating pre-
cise quantitative predictions about large sets of 
objects.

 coding theory: the study of codes and their use in 
certain situations for various applications.

 combinatorial design: the study of the creation and 
properties of finite sets in certain types of designs.

 enumerative combinatorics: a branch of com-
binatorics that studies the number of ways that 
certain patterns can be formed using a set of ob-
jects.

 graph theory: the study of graphs, which are dia-
grams used to model relationships between ob-
jects.
Basics of Combinatorics
Combinatorics is a branch of mathematics that 
studies counting methods and combinations, 
permutations, and arrangements of sets of ob-
jects. For instance, given a set of fifteen different 
objects, combinatorics studies equations that de-
termine how many different sets of five can be cre-
ated from the original set of fifteen. The study of 
combinatorics is crucial to the study of algorithms. 
Algorithms are sets of rules, steps, or processes that 
are linked together to address a certain problem.
The Science of Counting and 
Combinations
Combinatorics is often called the “science of 
counting.” It focuses on the properties of finite sets of 
objects, which do not have infinite numbers of objects 
and so are theoretically countable. The process of de-
scribing or counting all of the items in a specific set 
is called “enumeration.” Combinatorics also includes 
the study of combinations, a process of selecting 
items from a set when the order of selection does not 
matter. Finally, combinatorics also studies permuta-
tions. Permutations involve selecting or arranging 
items in a list, when the order of arrangement is im-
portant. Combinatorics also studies the relationships 
between objects organized into sets in various ways.
There are numerous subfields of combinatorics used 
to study sets of objects in different ways. Enumerative 
combinatorics is the most basic branch of the field. 
It can be described as the study of counting methods 
used to derive the number of objects in a given set. By 
contrast, analytic combinatorics is a subfield of enu-
merative combinatorics. It deals with predicting the 
properties of large sets of objects, using quantitative 
analysis. All combinatorics analysis requires detailed 
knowledge of calculus. Many subfields make extensive 
use of probability theory and predictive analysis.

57
Principles of Computer Science
Combinatorics
combinatorics Applications
There are many different applications for combina-
torics in analytic mathematics, engineering, physics, 
and computer science. Among the most familiar 
basic examples of combinatorics is the popular game 
sudoku. The game challenges players to fi ll in the 
blanks in a “magic square” diagram with specifi c 
column and row values. Sudoku puzzles are an ex-
ample of combinatorial design, which is a branch of 
combinatorics that studies arrangements of objects 
that have symmetry or mathematical/geometric bal-
ance between the elements.
Combinatorics is also crucial to graph theory. Graph 
theory is a fi eld of mathematics that deals with graphs, 
or representations of objects in space. Graphs are used 
in geometry, computer science, and other fi elds to 
model relationships between objects. In computer sci-
ence, for instance, graphs are typically used to model 
computer networks, computational fl ow, and the struc-
ture of links within websites. Combinatorics is used to 
study the enumeration of graphs. This can be seen as 
counting the number of different possible graphs that 
can be used for a certain application or model.
combinatorics in Algorithm design
In computer science, combinatorics is used in the 
creation and analysis of algorithms. Algorithms are 
sets of instructions, computing steps, or other pro-
cesses that are linked together to address a certain 
computational problem. As algorithms are essen-
tially sets, the steps within algorithms can be studied 
using combinatorial analysis, such as enumeration 
or permutation. As combinatorics can help re-
searchers to fi nd more effi cient arrangements of ob-
jects, or sets, combinatorial analysis is often used to 
test and assess the effi ciency of computer algorithms. 
Combinatorics is also key to the design of sorting al-
gorithms. Sorting algorithms allow computers to sort 
objects (often pieces of data, web pages, or other in-
formational elements), which is a necessary step in 
creating effective algorithms for web searching.
combinatorics in coding
Combinatorics is also used in coding theory, the 
study of codes and their associated properties and 
characteristics. Codes are used for applications, in-
cluding cryptography, compressing or translating 
data, and correcting errors in mathematical, elec-
trical, and information systems. Coding theory 
emerged from combinatorial analysis. These two 
branches of mathematics are distinct but share theo-
ries and techniques.
Combinatorics is an advanced fi eld of study. The 
arrangement, organization, and study of relation-
ships between objects provides analytical information 
applicable to many academic and practical fi elds. 
Combinatorics infl uences many aspects of computer 
design and programming including the development 
D
A
C
B
D
A
C
B
The Konigsberg bridge problem is a common example of combinatorial structures used to identify and quan-
tify possible combinations of values. Each landmass becomes a vertex or node, and each bridge becomes an 
arc or edge. In computer science, these graphs are used to represent networks or the fl ow of computation. 
EBSCO illustration.

58
Communication technology
Principles of Computer Science
of codes and the precise study of information and ar-
rangement of data.
—Micah L. Issit
Bibliography
Beeler, Robert A., How to Count: An Introduction to 
Combinatorics. New York: Springer, 2015. Print.
“Combinatorics.” Mathigon. Mathigon, 2015. Web. 10 
Feb. 2016.
Faticoni, Theodore G., Combinatorics: An Introduction. 
New York: Wiley, 2014. Digital file.
Guichard, David. “An Introduction to Combinatorics 
and Graph Theory.” Whitman. Whitman Coll., 4 
Jan 2016. Web. 10 Feb. 2016.
Roberts, Fred S., and Barry Tesman. Applied 
Combinatorics. 2nd ed. Boca Raton: Chapman, 
2012. Print.
Wagner, Carl. “Choice, Chance, and Inference.” 
Math.UTK.edu. U of Tennessee, Knoxville, 2015. 
Web. 10 Feb. 2016.
Communication technology
Fields of Study
Digital Media; Information Technology
Abstract
Communication technology is a broad field that in-
cludes any type of tool or process that makes it pos-
sible for individuals and groups to communicate 
over large areas. Many types of communication tech-
nology have been used throughout recorded his-
tory—smoke signals, carved signs and tokens, letters, 
radio waves, telegraph, television, and microwave sig-
nals, to name only a few.
Prinicipal Terms

 attenuation: the loss of intensity from a signal 
being transmitted through a medium.

 broadcast: an audio or video transmission sent via 
a communications medium to anyone with the ap-
propriate receiver.

 multiplexing: combining multiple data signals 
into one in order to transmit all signals simultane-
ously through the same medium.

 receiver: a device that reads a particular type of 
transmission and translates it into audio or video.

 transmission medium: the material through which 
a signal can travel.

 transmitter: a device that sends a signal through a 
medium to be picked up by a receiver.
Sending and Receiving Signals
Communication technology at its most basic involves 
sending a signal from a transmitter to a receiver. The 
signal carries information in audio, video, or some 
other data format. It travels in the form of energy 
waves through a transmission medium. This medium 
might be a device designed to carry the signal, such 
as a fiber-optic cable, or it might simply be the air 
through which a sound wave travels.
The purpose of most communication tech-
nology is to translate human language into a signal 
that can be transmitted through the appropriate 
medium, then revert the signal to the original 
format once it is received. For example, audio is 
sent through a digital telephone line in the form 
of binary data. When one person speaks, the sound 
waves they produce are converted into a stream of 
ones and zeros that represents a digital model of 
the analog waveform. This stream is transmitted 
in the form of electronic pulses through an elec-
trical network. (If the network is optical rather 
than electrical, it is transmitted as light pulses in-
stead.) When the data stream reaches its destina-
tion, a receiver reads the binary data and converts 
it back into analog sound waves that the person on 
the other end of the phone line can understand. In 
short, communication technology is all about en-
coding and decoding messages so they can be sent 
from one place to another.

59
Principles of Computer Science
Communication technology
Considerations of Communication 
Technology
Communication technology is not just about the 
transfer of information from one individual to an-
other. Mass communication involves the transmis-
sion of information on what is called a “one-to-
many” basis. A broadcast is the transmission of a 
single signal to multiple receivers in a given area. 
This signal can be received by anyone within range 
who possesses the necessary receiver, such as a radio, 
a television, or some other device. When a broadcast 
is transmitted over a very large area, such as a radio 
broadcast in a major city, the signal may experience 
some attenuation, or weakening of intensity. This at-
tenuation can be overcome by installing repeaters 
along the transmission route to help boost the 
signal. These repeaters work by receiving the orig-
inal transmission and then rebroadcasting it from a 
new location. A chain of repeaters can carry a signal 
significantly farther than it would otherwise travel.
One fact of modern communications is that at 
any given moment, there are far more messages 
in need of transmission than there is transmission 
capacity. This problem is handled in a number of 
different ways. First, different types of communica-
tion technology are used, so that not all communi-
cations need to share the same medium. Second, 
some types of communication technology, such as 
radio, make it possible to transmit at different fre-
quencies at the same time, so that multiple trans-
missions can occur simultaneously. Combining 
transmissions in this way is known as multiplexing. 
Multiplexing can be accomplished either by di-
viding up frequencies or by dividing transmission 
times into very small intervals. These approaches 
are called frequency-division multiplexing and 
time-division multiplexing, respectively. In the 
United States, there is stiff competition for broad-
cast frequency space among communications tech-
nology companies seeking to control as much of 
the broadcast spectrum as possible.
Internet Communications
The fact that the Internet can carry so many dif-
ferent types of communications—video, audio, im-
ages, text, and so on—sometimes causes confusion 
Advances in technology have provided a wide variety of modes of communication. Phones, tablets, com-
puters, and other devices allow people to write, talk, and send images and videos all around the globe. Public 
domain, via Wikimedia Commons.

60
CompTIA A+ certification
Principles of Computer Science
about what type of communication technology it is. 
Fundamentally, the Internet is a tool for digital com-
munication. Regardless of the type of data contained 
in an Internet transmission, it is all transmitted in 
ones and zeros. Only once it arrives at its destination 
is the binary code translated back into a video, an 
image, or some other format.
IT and Communication Overlap
The end of the twentieth century and the beginning 
of the twenty-first have seen a growing convergence of 
the fields of information technology (IT) and commu-
nication technology. IT has largely “taken over” com-
munication technology, since most modern communi-
cation technology being used and developed involves 
computers and similar types of technology. The digital 
revolution has transformed the way society thinks 
about information and its transmission. It is difficult to 
imagine a new type of communication technology that 
would not rely either wholly or in part on computers.
—Scott Zimmer, JD
Bibliography
Adams, Ty, and Stephen A. Smith. Communication 
Shock: The Rhetoric of New Technology. Newcastle 
upon Tyne: Cambridge Scholars, 2015. Print.
Dor, Daniel. The Instruction of Imagination: Language 
as a Social Communication Technology. New York: 
Oxford UP, 2015. Print.
Englander, Irv. The Architecture of Computer Hardware, 
Systems Software, & Networking: An Information 
Technology Approach. 5th ed. Hoboken: Wiley, 2014. 
Print.
Fuchs, Christian, and Marisol Sandoval, eds. Critique, 
Social Media and the Information Society. New York: 
Routledge, 2014. Print.
Hart, Archibald D., and Sylvia Hart Frejd. The Digital 
Invasion: How Technology Is Shaping You and Your 
Relationships. Grand Rapids: Baker, 2013. Print.
Tosoni, Simone, Matteo Tarantino, and Chiara 
Giaccardi, eds. Media and the City: Urbanism, 
Technology and Communication. Newcastle upon 
Tyne: Cambridge Scholars, 2013. Print.
CompTIA A+ certification
Fields of Study
Computer Science; Information Technology
Abstract
The CompTIA A+ certification is one of many cer-
tifications in the field of information technology 
(IT) granted by the Computing Technology Industry 
Association (CompTIA). An IT professional may gain 
the certification after passing a pair of exams that test 
their knowledge of computer hardware, operating 
systems, troubleshooting, and other essential topics.
Prinicipal Terms

 American National Standards Institute (ANSI): a 
nonprofit organization that oversees the creation 
and use of standards and certifications such as 
those offered by CompTIA.

 computer technician: a professional tasked with 
the installation, repair, and maintenance of com-
puters and related technology.

 information technology: the use of computers and 
related equipment for the purpose of processing 
and storing data.

 Initiative for Software Choice (ISC): a consortium 
of technology companies founded by CompTIA, 
with the goal of encouraging governments to 
allow competition among software manufacturers.
Certifying IT
The CompTIA A+ certification is a professional certi-
fication in information technology (IT). IT is a broad 
field that deals with the use of computers and related 
devices. IT professionals known as computer techni-
cians must know a variety of hardware, software, and 
other technological information. College degrees in IT 
or computer science and hands-on IT experience are 
typically important to employers. Some companies seek 
job candidates who have earned professional certifica-
tions. In the United States, the Computer Technology 
Industry Association (CompTIA), an IT trade associa-
tion, is one of the chief providers of such certifications.

61
Principles of Computer Science
CompTIA A+ certification
CompTIA was founded in 1982 as the Association 
of Better Computer Dealers. The organization 
began offering certifications in 1992, two years after 
changing its name to CompTIA. CompTIA’s certi-
fications are vendor neutral. This means that they 
test knowledge of computer hardware, operating 
systems, and peripherals from multiple manufac-
turers, not just one. The organization also offers 
certifications in computer networking, servers, and 
subfields such as healthcare IT. CompTIA offers ed-
ucational and professional development opportuni-
ties for IT professionals as well. In the early twenty-
first century, the organization became increasingly 
involved in public policy advocacy. It cofounded 
the Initiative for Software Choice (ISC) in 2002. 
The ISC is concerned with government policies re-
garding software use.
Understanding the CompTIA A+ 
Certification
The CompTIA A+ certification is a popular creden-
tial among IT professionals. As of early 2016, it had 
been awarded to more than one million people glob-
ally, according to CompTIA. The certification shows 
that the holder is skilled in the key areas of IT. These 
areas include computer troubleshooting and repair, 
setup and installation, and maintenance. To earn 
the certification, a professional must prove their 
knowledge of a number of specific topics, including 
the use of operating systems, computer networking 
procedures, and computer security. In addition to 
desktop and laptop computers, the certification 
signals the holder’s skill with mobile devices, such 
as smartphones, and peripherals, such as printers. 
The A+ certification and two other CompTIA cer-
tifications have been accredited by the American 
National Standards Institute (ANSI). ANSI oversees 
the granting of certifications for many professions.
Obtaining the CompTIA A+ Certification
To earn the CompTIA A+ certification, one must pass 
two exams. These exams are periodically updated to 
keep up with changes in technology. An updated set 
of exams, CompTIA A+ 220-901 and 220-902, were 
introduced on December 15, 2015. Those exams 
replaced the 220-801 and 220-802 exams, which 
were set to be retired on June 30, 2016, for English-
speaking test takers, and December 31 of that year 
for all others.
Each exam is ninety minutes long and has up to 
ninety questions. Some questions are multiple-choice, 
while others are performance based. Performance-
based questions simulate real-world problems that 
the test taker must solve. Both tests are scored out 
of a possible 900 points. The test taker must score at 
least 675 points to pass the first exam and 700 to pass 
the second. Exams must be completed in person at a 
testing center approved by CompTIA.
Together, the two exams cover a wide range of 
topics generally considered essential basic knowl-
edge for a computer technician. The first exam 
deals mostly with hardware, including computers, 
mobile devices, and peripherals. The exam also 
covers troubleshooting. The second exam focuses 
on operating systems, including Widows and Apple 
systems as well as open-source operating systems 
such as Linux. CompTIA does not require test 
takers to complete formal schooling before taking 
the exams, but it recommends that those seeking 
A+ certification have at least six months of practical 
experience with the covered topics. To aid IT pro-
fessionals in preparing for the exams, CompTIA of-
fers optional preparatory courses and self-guided 
training materials.
CompTIA A+ in the Field
While the computer skills evaluated by the CompTIA 
A+ exams are not possessed solely by those profes-
sionals who are certified, the certification provides 
clear evidence of an IT professional’s understanding 
of the field’s core concepts and procedures. As such, 
some employers prefer to hire employees who have 
already attained that certification. However, other 
employers may prefer to hire computer technicians 
with formal education in computer science or IT 
or with significant hands-on experience. Likewise, 
some employers may not require job seekers to have 
obtained A+ certification or may pay for their com-
puter technicians to take the exams once employed. 
Because of these varying requirements, individuals 
seeking work in IT should consider their options 
carefully before pursuing the CompTIA A+ or any 
other certification.
In 2010, CompTIA announced that all A+ certi-
fications, which had previously been issued for life, 
would in future need to be renewed every three 
years. The organization later ruled that previously 
issued certifications would be exempt from this 

62
Computer animation
Principles of Computer Science
requirement. However, all A+ certifications issued 
on or after January 1, 2011, expire after three years 
unless renewed. To remain certified, a professional 
holding the certification must pay a fee and complete 
continuing-education programs to remain current in 
the field.
—Joy Crelin
Bibliography
“About ANSI.” ANSI. American Natl. Standards Inst., 
2016. Web. 31 Jan. 2016.
Anderson, Nate. “CompTIA Backs Down; Past Certs 
Remain Valid for Life.” Ars Technica. Condé Nast, 
26 Jan. 2010. Web. 31 Jan. 2016.
“ANSI Accredits Four Personnel Certification 
Programs.” ANSI. American Natl. Standards Inst., 
8 Apr. 2008. Web. 31 Jan. 2016.
“CompTIA A+.” CompTIA. Computing Technology 
Industry Assn., 2015. Web. 31 Jan. 2016.
“Our Story.” CompTIA. Computing Technology 
Industry Assn., n. d. Web. 31 Jan. 2016.
“What Is the CompTIA A+ Certification?” 
Knowledge Base. Indiana U, 15 Jan. 2015. Web. 
31 Jan. 2016.
Computer animation
Fields of Study
Digital Media; Graphic Design
Abstract
Computer animation is the creation of animated 
projects for film, television, or other media using spe-
cialized computer programs. As animation projects 
may range from short, simple clips to detailed and 
vibrant feature-length films, a wide variety of anima-
tion software is available, each addressing the par-
ticular needs of animators. The computer animation 
process includes several key steps, including mod-
eling, keyframing, and rendering. These stages are 
typically carried out by a team of animators.
Prinicipal Terms

 animation variables (avars): defined variables that 
control the movement of an animated character 
or object.

 keyframing: the process of defining the first 
and last—or key—frames in an animated tran-
sition.

 render farms: large computer systems dedicated 
to rendering animated content.

 3-D rendering: the process of creating a 2-D ani-
mation using 3-D models.

 virtual reality: a form of technology that enables 
the user to view and interact with a simulated en-
vironment.
History of Computer Animation
Since the early twentieth century, the field of anima-
tion has been marked by frequent, rapid change. 
Innovation in the field has been far reaching, fil-
tering into film, television, advertising, video games, 
and other media. It was initially an experimental 
method and took decades to develop. Computer ani-
mation revitalized the film and television industries 
during the late twentieth and early twenty-first centu-
ries, in many ways echoing the cultural influence that 
animation had decades before.
Prior to the advent of computer animation, most 
animated projects were created using a process that 
later became known as “traditional,” or “cel,” ani-
mation. In cel animation, the movement of charac-
ters, objects, and backgrounds was created frame by 
frame. Each frame was drawn by hand. This time-
consuming and difficult process necessitated the cre-
ation of dozens of individual frames for each second 
of film.
As computer technology developed, computer 
researchers and animators began to experiment 
with creating short animations using computers. 
Throughout the 1960s, computers were used to 

63
Principles of Computer Science
Computer animation
create 2-D images. Ed Catmull, who later founded the 
studio Pixar in 1986, created a 3-D animation of his 
hand using a computer in 1972. This was the first 3-D 
computer graphic to be used in a feature film when 
it appeared in Futureworld (1976). Early attempts at 
computer animation were found in live-action films. 
The 1986 film Labyrinth, for instance, notably features 
a computer-animated owl flying through its opening 
credits. As technology improved, computer anima-
tion became a major component of special effects in 
live-action media. While cel animation continued to 
be used in animated feature films, filmmakers began 
to include some computer-generated elements in 
such works. The 1991 Walt Disney Studios film Beauty 
and the Beast, for instance, featured a ballroom in one 
scene that was largely created using a computer.
In 1995, the release of the first feature-length 
computer-animated film marked a turning point in 
the field of animation. That film, Toy Story, was cre-
ated by Pixar, a pioneer in computer animation. 
Over the following decades, Pixar and other studios, 
including Disney (which acquired Pixar in 2006) 
and DreamWorks, produced numerous computer-
animated films. Computer animation became a 
common process for creating animated television 
shows as well as video games, advertisements, music 
videos, and other media.
In the early twenty-first century, computer anima-
tion also began to be used to create simulated envi-
ronments accessed through virtual reality equipment 
such as the head-mounted display Oculus Rift. Much 
of the computer-animated content created during 
From designing the original animation model to creating algorithms that control the movement of fluids, 
hair, and other complex systems, computer software has drastically changed the art of animation. Through 
software that can manipulate polygons, a face can be rendered and further manipulated to create a number 
of images much more efficiently than with hand-drawn illustrations. Thus, the detail of the imaging is in-
creased, while the time needed to develop a full animation is reduced. By Diego Emanuel Viegas, CC BY-SA 
3.0 (http://creativecommons.org/licenses/by-sa/3.0), via Wikimedia Commons.

64
Computer animation
Principles of Computer Science
this time featured characters and surroundings that 
appeared 3-D. However, some animators opted to 
create 2-D animations that more closely resemble tra-
ditionally animated works in style.
Three-Dimensional Computer Animation
Creating a feature-length computer-animated pro-
duction is a complex and time-intensive process that 
is carried out by a large team of animators, working 
with other film-industry professionals. When cre-
ating a 3-D computer-animated project, the anima-
tion team typically begins by drawing storyboards. 
Storyboards are small sketches that serve as a rough 
draft of the proposed scenes.
Next, animators transform 2-D character designs 
into 3-D models using animation software. They use 
animation variables (avars) to control the ways in 
which the 3-D characters move, assigning possible di-
rections of movement to various points on the char-
acters’ bodies. The number of avars used and the 
areas they control can vary widely. The 2006 Pixar 
film Cars reportedly used several hundred avars to 
control the characters’ mouths alone. Using such 
variables gives animated characters a greater range 
of motion and often more realistic expressions and 
gestures. After the characters and objects are mod-
eled and animated, they are combined with back-
grounds as well as lighting and special effects. All of 
the elements are then combined to transform the 
3-D models into a 2-D image or film. This process is 
known as 3-D rendering.
Two-Dimensional Computer Animation
Animating a 2-D computer-animated work is some-
what different from its 3-D counterpart, in that it 
does not rely on 3-D modeling. Instead, it typically 
features the use of multiple layers, each of which con-
tains different individual elements. This method of 
animating typically features keyframing. In this pro-
cedure, animators define the first and last frames in 
an animated sequence and allow the computer to fill 
in the movement in between. This process, which 
in traditionally animated films was a laborious task 
done by hand, is often known as “inbetweening,” or 
“tweening.”
Tools
Various animation programs are available to anima-
tors, each with its own strengths and weaknesses. 
Some animation software, such as Maya and Cinema 
4D, are geared toward 3-D animation. Others, such 
as Adobe Flash, are better suited to 2-D animation. 
Adobe Flash in particular has commonly been used 
to produce 2-D cartoons for television, as it is con-
sidered a quick and low-cost means of creating such 
content. Animation studios such as Pixar typically use 
proprietary animation software, thus ensuring that 
their specific needs are met.
In addition to animation software, the process of 
computer animation relies heavily on hardware, as 
many steps in the process can be taxing for the sys-
tems in use. Rendering, for example, often demands 
a sizable amount of processing power. As such, many 
studios make use of render farms, large, powerful 
computer systems devoted to that task.
—Joy Crelin
Bibliography
Carlson, Wayne. “A Critical History of Computer 
Graphics and Animation.” Ohio State University. 
Ohio State U, 2003. Web. 31 Jan. 2016.
Highfield, Roger. “Fast Forward to Cartoon Reality.” 
Telegraph. Telegraph Media Group, 13 June 2006. 
Web. 31 Jan. 2016.
Parent, Rick. Computer Animation: Algorithms and 
Techniques. Waltham: Elsevier, 2012. Print.
“Our Story.” Pixar. Pixar, 2016. Web. 31 Jan. 2016.
Sito, Tom. Moving Innovation: A History of Computer 
Animation. Cambridge: MIT P, 2013. Print.
Winder, Catherine, and Zahra Dowlatabadi. Producing 
Animation. Waltham: Focal, 2011. Print.

65
Principles of Computer Science
Computer Circuitry: Flip flops
Computer Circuitry: Flip flops
Fields of Study
Computer Engineering; Computer Science
Abstract
Flip-flops are bistable multivibrator devices because 
they have two stable states. The device holds a one-bit 
signal, either high or low, that is sent as output from 
the device when it is triggered by a clock signal. The 
device is used to construct counters, accumulators and 
registers in central processing chips, and is often used 
as the triggering device for hardware interruption 
processes. When triggered, a flip-flop outputs its saved 
state and sets a new state according to its input signals.
Prinicipal Terms

 clock speed: the speed at which a microprocessor 
can execute instructions; also called “clock rate.” 

 counter: a digital sequential logic gate that records 
how many times a certain event occurs in a given 
amount of time. 

 hardware: the physical parts that make up a com-
puter. These include the motherboard and pro-
cessor, as well as input and output devices such as 
monitors, keyboards, and mice. 

 hardware interruption: a device attached to a com-
puter sending a message to the operating system to 
inform it that the device needs attention, thereby 
“interrupting” the other tasks that the operating 
system was performing. 

 inverter: a logic gate whose output is the inverse of 
the input; also called a NOT gate. 

 negative-AND (NAND) gate: a logic gate that pro-
duces a false output only when both inputs are 
true 

 transistor: a computing component generally 
made of silicon that can amplify electronic sig-
nals or work as a switch to direct electronic signals 
within a computer system. 

 vibrator: an electronic component that (oscil-
lates) or switches between electronic states
Transistors and Gates
A flip-flop is a digital electronic device that holds 
a particular output state until it is triggered by a 
“clock signal” to “flip-flop” the output state. A typical 
flip-flop is constructed from a set of transistor “gates” 
in such a way that the output signal from each gate is 
used as an input signal to the other gate. Each gate 
is formed by a specific interconnection of transistor 
structures. There are three basic types of transistor-
based gates, designated as AND, OR and NOT. A 
NOT gate is also known as an “inverter,” because it 
inverts the value of the digital signal passing through 
it. The AND gate produces a “high” output signal 
only when all of its input signals are “high,” but 
a “low” signal if this condition is not met. The OR 
gate produces a “high” output signal only if some, 
but not all, of its input signals are “high,” and a “low” 
output signal otherwise. The combination of a NOT 
gate and an AND or OR gate produces a negative-
AND (NAND) gate and a negative-OR (NOR) gate, 
respectively. Accordingly, digital logic circuits can 
be designed using AND, OR, NOT, NAND and NOR 
gates as needed. However, transistor-transistor logic 
is more compact and efficient when circuits are con-
structed in a negative sense using NAND and NOR 
gates instead of in the positive sense using AND, OR 
and NOT gates. Flip-flops are constructed exclusively 
from NAND gates and NOR gates.
Types of Flip-Flops
There are two basic types of flip-flops used in the 
construction of digital computer hardware devices, 
termed J-K and R-S flip-flops. In both types, the output 
signal from each gate is used as an input signal to the 
other gate. Another input signal required for the op-
eration of a flip-flop is the “clock” signal. The “clock” 
signal may be either a regular square wave signal that 
determines the clock speed of the particular device, 
or an irregular square wave signal from another seg-
ment of the digital circuitry. Square wave signals 
are nominally instantaneous changes between high 
and low signals. However, the tiny amount of time 
required for the change to take place as the semi-
conductor responds to the new condition produces 
a “ramping” effect on both the leading and trailing 
edge of the signal change. The state that exists on 
the ramped portion of the signal change is not “al-
lowed,” since it is neither “high” nor “low.” Flip-flops 
are designed to trigger their output signal when one 
of the disallowed states is detected. The difference 
in the manner of changing state determines whether 

66
Computer Circuitry: Flip flops
Principles of Computer Science
the structure functions as a flip-flop or as a latch. 
Triggering either one causes the device to output the 
state that it has been holding and allows the device to 
read the states of its input signals. This sets a new state 
that the device will hold until it is again triggered.
Sample Problem
A flip-flop constructed from two NAND gates 
has four inputs. The states of one pair of 
inputs is both high, while the other pair of 
inputs is one high and one low. What is the 
output from the flip-flop?
Answer:
A NAND gate outputs a low signal only when 
both inputs are high, and a low signal other-
wise. In this example, the NAND gate with the 
two high inputs would output a low signal. 
This is the low input to the other NAND gate, 
which must therefore output a low signal. 
Since the output signal of this NAND gate is 
the second input to the other NAND gate in 
the device, and that NAND gate has two high 
input signals, this situation is not allowed.
Applications of Flip-Flops
Flip-flops and latches are termed bistable multivi-
brator devices: bistable because they have only two 
stable states, and multivibrators because they can 
switch back and forth between those two states. A 
series of latch structures in combination can be used 
to hold a corresponding series of bits in a parallel 
data stream and pass that set of data on at each clock 
cycle. This feature is the basic structure of an accu-
mulator, counter or a register in central processing 
chips. Flip-flops are also typically used as the trig-
gering signal generator for hardware interruption. 
Every central processing chip must have a signal 
input to indicate that another hardware device, such 
as a modem or a printer, requires attention. When 
that signal is received at the appropriate terminal on 
the chip, the process being carried out by the central 
processing unit is interrupted, allowing the function 
of the device to take precedence temporarily. Flip-
flops can also be interconnected in a “master-slave” 
combination, in which input is registered on the 
leading edge of the signal and output is registered on 
the trailing edge.
—Richard M. Renneboog M.Sc.
Bibliography
Bishop, Owen (2011) Electronics. Circuits and Systems 
4th ed., New York, NY: E;sevier. Print.
Brindley, Keith (2011) Starting Electronics 4th ed., New 
York, NY: Elsevier. Print.
Clements, Alan (2006) Principles of Computer Hardware 
4th ed., New York, NY: Oxford University Press. 
Print.
Gibson, J.R. (2011) Electronic Logic Circuits 3rd ed., 
New York, NY: Routledge. Print.
Hsu, John Y. (2002) Computer Logic Design Principles 
and Applications New York, NY: Springer. Print.

67
Principles of Computer Science
Computer Circuitry: Semiconductors
Computer Circuitry: Semiconductors
Fields of Study
Computer Engineering
Abstract
Semiconductor materials are the principal com-
ponent of computer technology, and are primarily 
based on silicon and germanium. Pure silicon does 
not have enough ‘free’ valence electrons to conduct 
electrical current adequately, and small percentages 
of other elements are blended into the silicon to en-
hance the conductivity of the material. Current flows 
in semiconductors as electrons moving through the 
material driven by a voltage difference, and as ‘holes’ 
equivalent to positive charge. Production of silicon-
based semiconductor material and the subsequent 
manufacture of integrated circuit chips is a complex, 
multistep process requiring stringent quality control 
measures.
Prinicipal Terms

 band gap: the energy difference between the 
ground state of electrons in a material and the 
conduction band.

 central processing unit (CPU): electronic circuitry 
that provides instructions for how a computer 
handles processes and manages data from applica-
tions and programs. 

 conduction band: the region of atomic and mo-
lecular orbitals in a material through which free 
electrons move in an electrical current.

 hole: a vacant location in the normally-filled 
valence shell of an atom, created when an elec-
tron is excited to a higher energy level in the 
same atom; a hole behaves like a positive charge 
by attracting an available electron of the corre-
sponding energy.

 semiconductor intellectual property (SIP) block: a 
quantity of microchip layout design that is owned 
by a person or group; also known as an “IP core.” 

 solid-state storage: computer memory that stores 
information in the form of electronic circuits, 
without the use of disks or other read/write equip-
ment. 

 transistor: a computing component gener-
ally made of silicon that can amplify electronic  
signals or work as a switch to direct electronic sig-
nals within a computer system. 
Conductivity and Electrical Current
The ability of any material to conduct electricity is a 
function of the distribution of electrons and energy 
levels in the atoms and molecules of that material. 
Materials in which electrons can easily enter the 
conduction band of the material across a small band 
gap between the normal ground state of the elec-
trons and the conduction band generally are good 
conductors of electrical current. Materials that have 
a large band gap between the normal ground state 
of the electrons and the conduction band are gener-
ally poor conductors of electrical current. Materials 
that have a band gap too large to be good conduc-
tors, and too small to be non-conductors, are called 
semiconductors. Good conductors have little en-
ergy difference between the occupied valence shell 
of electrons and the vacant orbitals in the next 
highest electron shell, and the valence electrons are 
able to move between them and from atom to atom 
through them relatively easily. In poor conductors 
and non-conductors, this energy difference is much 
greater, making it more difficult for electrons to 
move through the material. The essential semicon-
ductor materials used in computer technology are 
silicon and germanium. The electron distribution 
in the atoms of these materials is very stable and not 
easily disrupted.
Charge Carriers
All electrical current results from the displacement of 
charge. Current flow in semiconductors is described 
in terms of charge carriers. Electrons moving though 
the conduction band of the material is the conven-
tional definition of an electrical current. However, in 
semiconductor applications, electrical current is also 
produced by the movement of a hole. Heat energy 
or an applied voltage can excite electrons from their 
ground state in the valence shell of atoms to higher 
energy levels within those same atoms, where they 
become free electrons that can move through the 
conduction band of the material. The orbital posi-
tion in the valence shell that the electrons leave be-
hind, called ‘holes’, can then act as though they are 

68
Computer Circuitry: Semiconductors
Principles of Computer Science
positive charges by attracting any nearby electron of 
the corresponding energy from adjacent atoms. This 
creates another hole in the adjacent atom. The pro-
gression of holes through the material is equivalent 
to the movement of positive charge as electrons shift 
from one atom to another without flowing through 
the conduction band.
Preparation of Semiconductor 
Materials
The principal material for the production of semi-
conductor-based devices is silicon. Pure silicon, how-
ever, does not function well as a semiconducting ma-
terial for the production of transistor structures used 
for all kinds of integrated circuit (IC) chips, and es-
pecially for central processing unit (CPU) chips and 
solid-state storage devices. To improve the desired 
characteristics of the material, a small percentage 
of another element can be added as a “dopant” to 
either increase or decrease the number of available 
electrons in the resulting alloy. This silicon blend 
is prepared by melting a quantity of pure silicon to-
gether with the proper amount of dopant. A single, 
large cylindrical crystal is then drawn slowly from the 
molten mass and then cut into thin wafers for use in 
the manufacture of chips.
Transistor Structures
The thin wafers obtained from slicing up the large 
silicon crystal are polished to produce a highly uni-
form surface. Millions of transistor structures are 
then etched onto the surface of each wafer in a mul-
tistep process. The pattern and order of etching can 
constitute a proprietary or patented segment of the 
final product, known as a semiconductor intellectual 
property (SIP) block within the overall design of the 
integrated circuit. The overall process requires sev-
eral individual steps that are carried out with strin-
gent quality control at all stages of production. The 
number of transistor structures that can be etched 
onto the surface of a silicon chip, and hence the 
computing capabilities of computers, has followed 
the empirical observation known as Moore’s Law 
quite well for several decades. Moore’s Law states 
that the number of possible transistor structures 
and the corresponding\ computing power doubles 
approximately every eighteen months. Logically, 
this means that there is a finite limit to both, deter-
mined by the physical minimum size of the struc-
tures themselves, and when that limit is reached no 
further advance can be made. However,current re-
search with novel materials such as graphene and 
nanotubes, and toward the successful development 
of quantum computers may eventually end de-
pendency on traditional semiconductor materials 
entirely.
—Richard M. Renneboog M.Sc.
Bibliography
Haug, Hartmut, and Stephan W. Koch. Quantum 
Theory of the Optical and Electronic Properties of 
Semiconductors. New Jersey [u.a.]: World Scientific, 
2009. Print.
Köhler, Anna, and Heinz Bässler. Electronic Processes in 
Organic Semiconductors: An Introduction. Wiley-VHC 
Verlag, 2015. Print.
Könenkamp, 
Rolf. 
Photoelectric 
Properties 
and 
Applications of Low-Mobility Semiconductors. Berlin: 
Springer, 2000. Print.
Mishra, Umesh. Semiconductor Device Physics and 
Design. Place of publication not identified: 
Springer, 2014. Print.
Rockett, Angus. The Materials Science of Semiconductors. 
New York, NY: Springer, 2010. Print.
Yu, P.Y, and M Cardona. Fundamentals of Semiconductors: 
Physics and Materials Properties. Berlin: Springer, 
2010. Print. 

69
Principles of Computer Science
Computer memory
Computer memory
Fields of Study
Computer 
Science; 
Computer 
Engineering; 
Information Technology
Abstract
Computer memory is the part of a computer used for 
storing information that the computer is currently 
working on. It is different from computer storage 
space on a hard drive, disk drive, or storage medium 
such as CD-ROM or DVD. Computer memory is one 
of the determining factors in how fast a computer 
can operate and how many tasks it can undertake at 
a time.
Prinicipal Terms

 flash memory: nonvolatile computer memory that 
can be erased or overwritten solely through elec-
tronic signals, i.e. without physical manipulation 
of the device.

 nonvolatile memory: memory that stores informa-
tion regardless whether power is flowing to the 
computer.

 random access memory (RAM): memory that the 
computer can access very quickly, without regard 
to where in the storage media the relevant infor-
mation is located.

 virtual memory: memory used when a computer 
configures part of its physical storage (on a hard 
drive, for example) to be available for use as addi-
tional RAM. Information is copied from RAM and 
moved into virtual memory whenever memory re-
sources are running low.

 volatile memory: memory that stores information 
in a computer only while the computer has power; 
when the computer shuts down or power is cut, 
the information is lost.
Overview of Computer Memory
Computer memory is an extremely important part 
of configuring and using computers. Many users find 
themselves confused by the concepts of computer 
memory and computer storage. After all, both store 
information. Storage serves a very different purpose 
from memory, however. Storage is slower, because it 
uses a series of spinning platters of magnetic disks 
and a moving read/write head. These create a tiny 
magnetic field that can modify the polarity of tiny 
sections of the platters in order to record or read in-
formation. The benefit of storage is that it is nonvola-
tile memory, meaning that its contents remain even 
when the power is turned off. Computer memory, 
by contrast, is volatile memory because it only stores 
information while power is flowing through the 
computer.
This can be a drawback, especially when a user has 
vital information, such as a newly created but un-
saved document, that is lost when a power surge 
shuts down the system. However, memory is also 
MEMORY
Secondary
Primary
ROM
RAM
Optical 
memory
Dynamic 
RAM
EPROM
EEPROM
PROM
Floppy
disk
Hard 
disk
CD R/W
DVD R/W
SD 
cards
USB flash 
drive
Static
RAM
Disk drives
Tape drives
Computer memory comes in a number of formats. Some are the primary CPU memory, such 
as RAM and ROM; others are secondary memory, typically in an external form. External 
memory formats have changed over the years and have included hard drives, floppy drives, 
optical memory, flash memory, and secure digital (SD) memory. Adapted from the Its All About 
Embedded blog.

70
Computer memory
Principles of Computer Science
incredibly useful because it allows the computer 
to store information that it is currently working 
on. For example, if a user wished to update a ré-
sumé file saved on their hard drive, they would 
first tell the computer to access its storage to find 
a copy of the file. The computer would locate the 
file and then copy the file’s contents into its vola-
tile memory and open the document. As the user 
makes changes to the file, these changes are re-
flected only in the memory, until the user saves 
them to storage. Some have compared storage and 
memory to a librarian. Storage is like the thou-
sands of books the librarian has in the library, 
which they can consult if given enough time. 
Memory is like the knowledge the librarian carries 
around in their head. It is accessible very quickly 
but holds a smaller amount of information than 
the books.
Random and Virtual Memory
One feature of memory that makes it much faster 
than other types of information storage is that it 
is a type of random access memory (RAM). Any 
address in the memory block can be accessed di-
rectly, without having to sort through all of the 
other entries in the memory space. This contrasts 
with other types of memory, such as magnetic tape, 
which are sequential access devices. In order to get 
to a certain part of the tape, one must move for-
ward through all the other parts of the tape that 
come first. This adds to the time it takes to access 
that type of memory.
Memory, more than almost any other factor, de-
termines how fast a computer responds to requests 
and completes tasks. This means that more memory 
is constantly in demand, as consumers want systems 
that are faster and can on more tasks at the same time. 
When a computer is running low on memory because 
too many operations are going on at once, it may use 
virtual memory to try to compensate. Virtual memory 
is a technique in which the computer supplements its 
memory space by using some of its storage space. If 
the computer is almost out of memory and another 
task comes in, the computer copies some contents of 
its memory onto the hard drive. Then it can remove 
this information from memory and make space for 
the new task. Once the new task is managed, the com-
puter pulls the information that was copied to the 
hard drive and loads it back into memory.
Flash and Solid State Memory
Some newer types of memory straddle the line be-
tween memory and storage. Flash memory, used in 
many mobile devices, can retain its contents even 
when power to the system is cut off. However, it can 
be accessed or even erased purely through electrical 
signals. Wiping all the contents of flash memory and 
replacing them with a different version is sometimes 
called “flashing” a device.
Many newer computers have incorporated solid 
state disks (SSDs), which are similar in many respects 
to flash memory. Like flash memory, SSDs have no 
moving parts and can replace hard drives because 
they retain their contents after system power has 
shut down. Many advanced users of computers have 
adopted SSDs because they are much faster than tra-
ditional computer configurations. A system can be 
powered on and ready to use in a matter of seconds 
rather than minutes.
—Scott Zimmer, JD
Bibliography
Biere, Armin, Amir Nahir, and Tanja Vos, eds. 
Hardware and Software: Verification and Testing. New 
York: Springer, 2013. Print.
Englander, Irv. The Architecture of Computer Hardware 
and System Software: An Information Technology 
Approach. 5th ed. Hoboken: Wiley, 2014. Print.
Kulisch, Ulrich. Computer Arithmetic and Validity: 
Theory, Implementation, and Applications. 2nd ed. 
Boston: De Gruyter, 2013. Print.
Pandolfi, Luciano. Distributed Systems with Persistent 
Memory: Control and Moment Problems. New York: 
Springer, 2014. Print.
Patterson, David A., and John L. Hennessy. Computer 
Organization and Design: The Hardware/Software 
Interface. 5th ed. Waltham: Morgan, 2013. Print.
Soto, María, André Rossi, Marc Sevaux, and Johann 
Laurent. Memory Allocation Problems in Embedded 
Systems: Optimization Methods. Hoboken: Wiley, 
2013. Print.

71
Principles of Computer Science
Computer modeling
Computer modeling
Fields of Study
Computer Science; Computer Engineering; Software 
Engineering
Abstract
Computer modeling is the process of designing a 
representation of a particular system of interacting 
or interdependent parts in order to study its be-
havior. Models that have been implemented and ex-
ecuted as computer programs are called computer 
simulations.
Prinicipal Terms

 algorithm: a set of step-by-step instructions for per-
forming computations.

 data source: the origin of the information used in 
a computer model or simulation, such as a data-
base or spreadsheet.

 parameter: a measurable element of a system that 
affects the relationships between variables in the 
system.

 simulation: a computer model executed by a com-
puter system.

 system: a set of interacting or interdependent 
component parts that form a complex whole.

 variable: a symbol representing a quantity with no 
fixed value.
Understanding Computer Models
A computer model is a programmed representation 
of a system that is meant to mimic the behavior of 
the system. A wide range of disciplines, including 
meteorology, physics, astronomy, biology, and eco-
nomics, use computer models to analyze different 
types of systems. When the program representing 
the system is executed by a computer, it is called a 
simulation.
One of the first large-scale computer models was 
developed during the Manhattan Project by scientists 
designing and building the first atomic bomb. Early 
computer models produced output in the form of ta-
bles or matrices that were difficult to analyze. It was later 
discovered that humans can see data trends more easily 
if the data is presented visually. For example, humans 
find it easier to analyze the output of a storm-system 
simulation if it is presented as graphic symbols on a 
map rather than as a table of meteorological data. 
Thus, simulations that produced graphic outputs were 
developed.
Computer models are used when a system is too 
complex or hard to study using a physical model. 
For example, it would be difficult if not impossible 
to create a physical model representing the gravita-
tional effects of planets and moons on each other 
and on other objects in space.
There are several different types of models. Static 
models simulate a system at rest, such as a building 
design. Dynamic models simulate a system that 
changes over time. A dynamic model could be used 
to simulate the effects of changing ocean tempera-
tures on the speed of ocean currents throughout 
the year. A continuous model simulates a system that 
changes constantly, while a discrete model simulates 
a system that changes only at specific times. Some 
models contain both discrete and continuous ele-
ments. A farming model might simulate the effects of 
both weather patterns, which constantly change, and 
pesticide spraying, which occurs at specified times.
How Computer Models Work
To create a computer model, one must first determine 
the boundaries of the system being modeled and what 
aspect of the system is being studied. For example, 
if the model is of the solar system, it might be used 
to study the potential effect on the orbits of the ex-
isting planets if another planet were to enter the solar 
system.
To create such a model, a computer programmer 
would develop a series of algorithms that contain 
the equations and other instructions needed to 
replicate the operation of the system. Variables are 
used to represent the input data needed. Examples 
of variables that might be used for the solar system 
model include the mass, diameter, and trajectory of 
the theoretical new planet. The values that define 
the system, and thus how the variables affect each 
other, are the parameters of the system. The param-
eters control the outputs of the simulation when it 
is run. Different values can be used to test different 
scenarios related to the system and problem being 
studied. Example parameters for the solar system 

72
Computer modeling
Principles of Computer Science
model might include the orbits of the known planets, 
their distance from the sun, and the equations that 
relate an object’s mass to its gravity. Certain param-
eters can be changed to test different scenarios each 
time a simulation is run. Because parameters are not 
always constant, they can be difficult to distinguish 
from variables at times.
The model must also have a data source from 
which it will draw the input data. This data may be 
directly entered into the program or imported 
from an external source, such as a file, database, or 
spreadsheet.
Why Computer Models Are Important
Computer models have provided great benefits to 
society. They help scientists explore the universe, 
understand the earth, cure diseases, and discover 
and test new theories. They help engineers design 
buildings, transportation systems, power systems, 
and other items that affect everyday life. With the 
development of more powerful computer systems, 
computer models will remain an important mecha-
nism for understanding the world and improving the 
human condition.
—Maura Valentino, MSLIS
Perform 
simulations
Perform 
experiments
Evaluate
and improve 
model
Evaluate
and improve 
theory
Real system
Develop a 
mathematical 
model
Computer program 
of model
Experimental 
results
Simulation
results
Mathematical models are used to identify and understand the details influencing a real system. Computer programs allow one 
to evaluate many variations in a mathematical model quickly and accurately, thus efficiently evaluating and improving models.
Adapted from Allen and Tildesly, “Computer Simulation of Liquids.

73
Principles of Computer Science
Computer programming: Image editing
Sample Problem
It is important to select appropriate variables 
and parameters when designing a computer 
model. List two variables and two parameters 
that might be used to determine the speed of 
a baseball after it has traveled three feet from 
the pitcher’s throwing hand.
Answer:
Variables needed to determine the ball’s 
speed might include the mass and size of the 
ball, the angle at which the ball is released, 
and the force with which it is thrown.
Parameters that define the system, which 
may be changed to test different scenarios, 
include wind speed, aerodynamic drag on the 
ball, and whether or not it is raining when the 
ball is thrown.
Bibliography
Agrawal, Manindra, S. Barry Cooper, and Angsheng Li, 
eds. Theory and Applications of Models of Computation: 
9th Annual Conference, TAMC 2012, Beijing, China, 
May 16–21, 2012. Berlin: Springer, 2012. Print.
Edwards, Paul N. A Vast Machine: Computer Models, 
Climate Data, and the Politics of Global Warming. 
Cambridge: MIT P, 2010. Print.
Kojic´, Miloš, et al. Computer Modeling in Bioengineering: 
Theoretical Background, Examples and Software. 
Hoboken: Wiley, 2008. Print.
Law, Averill M. Simulation Modeling and Analysis. 5th 
ed. New York: McGraw, 2015. Print.
Morrison, Foster. The Art of Modeling Dynamic Systems: 
Forecasting for Chaos, Randomness, and Determinism. 
1991. Mineola: Dover, 2008. Print.
Seidl, Martina, et al. UML@Classroom: An Introduction 
to Object-Oriented Modeling. Cham: Springer, 2015. 
Print.
Computer programming: Image editing
Fields of Study
Digital Media; Graphic Design; Software Engineering
Abstract
Image editing software uses computing technology to 
change digital images. Image editing can involve al-
tering the appearance of an image, such as showing a 
hot air balloon underwater, or improving the quality 
of a low-resolution image. Images may also be com-
pressed so that they require less computer storage 
space.
Prinicipal Terms

 interpolation: a process of estimating interme-
diate values when nearby values are known; used 
in image editing to “fill in” gaps by referring to nu-
merical data associated with nearby points.
 lossless compression: a method of reducing image file 
size that maintains image quality without degradation.

 lossy compression: a method of decreasing image 
file size by discarding some data, resulting in some 
image quality being irreversibly sacrificed.

 nondestructive editing: a mode of image editing 
in which the original content of the image is not 
destroyed because the edits are made only in the 
editing software.

 rendering: the production of a computer image 
from a 2-D or 3-D computer model.
Overview of Image Editing
There are as many ways of digitally altering images as 
there are uses for digital art. The first step in image 
editing is to obtain an image in digital format. The 
easiest method is to use a digital camera to take a 
photograph and then transfer the photograph to 
a computer for editing. Another approach that is 
occasionally necessary is to scan a print photograph 
or film negative. This converts the photo to a digital 
image ready for editing. It is even possible to create 
an image by hand in native digital format, by using 

74
Computer programming: Image editing
Principles of Computer Science
a tablet and stylus to draw and paint. Finally, ren-
dering makes it possible for a computer to produce 
a digital image from a 2-D or 3-D model.
Once the digital image is available, the next step 
is to determine what will be done to it. Most often, 
the image will be enhanced (improving the image 
quality through interpolation or other techniques), 
compressed (decreasing the file size by sacrificing 
some image quality or clarity), or altered (made to de-
pict something that was not originally there). These 
changes may be through destructive or nondestruc-
tive editing. In destructive editing, the changes are 
applied to the original file. By contrast, in nondestruc-
tive editing, they are saved in a separate version file.
In the early days of the Internet, image compres-
sion was an especially important type of image editing. 
Bandwidth was limited then, and it could take several 
minutes to transmit even a medium sized image file. 
Image compression algorithms were invented to help 
reduce the size of these files, with some loss of quality. 
Lossless compression can avoid degradation of the 
image, but in most cases, it does not reduce file size as 
much as lossy compression does.
How Compression Works
Computers store image data as sets of numeric values. 
Each pixel onscreen is lit in a particular way when an 
image is displayed, and the colors of each pixel are 
stored as numbers. For example, if the color black 
were represented by the number eight, then anywhere 
in a picture that has three black pixels in a row would 
be stored as 8, 8, 8. Because an image is composed of 
thousands of pixels, all of the numbers needed to de-
scribe the colors of those pixels, when combined, take 
up a lot of storage space. One way to store the same 
information in less space is to create substitutions for 
recurring groups of numbers. The symbol q1 could 
be used to represent three black pixels in a row, for 
Compressed
Original file
Restored
Original file
Compressed
LOSSY
LOSSLESS
Restored
Image formats store varying levels and arrangements of data. Choosing the appropriate file 
format can make a difference when images are manipulated, resized, or compressed and restored 
EBSCO illustration.

75
Principles of Computer Science
Computer programming: Image editing
instance. Thus, instead of having to store three copies 
of the value “8” to represent each of the three black 
pixels, the computer could simply store the two-letter 
symbol q1, thus saving one-third of the storage space 
that otherwise would be required. This is the basis for 
how digital images are compressed.
Most images are compressed using lossy compres-
sion algorithms, such as JPEG. Compression thus usu-
ally requires that the sacrifice of some image quality. 
For most purposes, the reduction in quality is not 
noticeable and is made up for by the convenience of 
more easily storing and transmitting the smaller file. 
It is not uncommon for compression algorithms to 
reduce the file size of an image by 75 to 90 percent, 
without noticeably affecting the image’s quality.
A Numbers Game
Image enhancement typically relies on the mathe-
matical adjustment of the numeric values that repre-
sent pixel hues. For instance, if an image editor were 
to desaturate a photograph, the software would first 
recognize all of the pixel values and compare them to 
a grayscale value. It would then interpolate new values 
for the pixels using a linear operation. Similarly, a 
filtering algorithm would find and apply a weighted 
average of the pixel values around a given pixel value 
in order to identify the new color codes for each 
pixel being adjusted. The median or the mode (most 
common) value could also be used. The type of filter 
being applied determines which mathematical oper-
ation is performed. Filters are often used to correct 
for noise, or unwanted signal or interference.
Image Editing Goes Mobile
Image editing is now even possible on mobile plat-
forms. Certain programs work only on PCs, others 
strictly on mobile devices, and still others on both. 
Besides the well-known Adobe Photoshop, other 
programs, including iPhoto, Apple Photos, Google’s 
Picasa, and Gimp, also provide image editing for 
desktop computers. Fotor and Pixlr Editor work 
across platforms, giving users flexibility between 
their desktop, mobile device, and the Web. Similarly, 
photo collaging software abounds. Among these 
programs are Photoshop CC and CollageIt on desk-
tops, Ribbet and Fotojet online, and Pic Stitch and 
BeFunky on mobile devices.
Nokia and Apple have developed the capability to 
create “live photos” with their smartphones. These 
are a hybrid of video and still images in which a few 
seconds of video are recorded prior to the still photo 
being taken. This feature represents yet another direc-
tion for image capture, alteration, and presentation.
—Scott Zimmer, JD
Bibliography
Busch, David D. Mastering Digital SLR Photography. 
2nd ed. Boston: Thompson Learning, 2008. Print.
Freeman, Michael. Digital Image Editing & Special 
Effects: Quickly Master the Key Techniques of Photoshop 
& Lightroom. New York: Focal, 2013. Print.
Galer, Mark, and Philip Andrews. Photoshop CC 
Essential Skills: A Guide to Creative Image Editing. 
New York: Focal, 2014. Print.
Goelker, Klaus. Gimp 2.8 for Photographers: Image 
Editing with Open Source Software. Santa Barbara: 
Rocky Nook, 2013. Print.
Holleley, Douglas. Photo-Editing and Presentation: 
A Guide to Image Editing and Presentation for 
Photographers 
and 
Visual 
Artists. 
Rochester: 
Clarellen, 2009. Print.
Xue, Su. Data-Driven Image Editing for Perceptual 
Effectiveness. New Haven: Yale U, 2013. Print.

76
Computer programming: Music editing
Principles of Computer Science
CoMputer progrAMMIng: MusIC eDItIng
Fields oF study
Digital Media; Software Engineering
AbstrAct
Music editing involves the use of computer tech-
nology to alter fi les of recorded sound. Music fi les 
can be compressed, enhanced, combined, or sepa-
rated through editing software. Both the recording 
and fi lm industries rely on music editing software to 
create their products. Compression has enabled the 
music industry to move away from physical media 
and has driven the growth of online music transfer. 
The sharing of audio fi les has also led to a more col-
laborative music scene.
PriniciPAl terms
 
 audio codec: a program that acts as a “coder-de-
coder” to allow an audio stream to be encoded 
for storage or transmission and later decoded for 
playback.
 
 lossy compression: a method of reducing the 
size of an audio fi le while sacrifi cing some of the 
quality of the original fi le.
 
 mastering: the creation of a master recording that 
can be used to make other copies for distribution.
 
 mixing: the process of combining different sounds 
into a single audio recording.
  nondestructive: a form of editing that alters a digital 
audio fi le without destroying the fi le’s original form.
 
 scrubbing: navigating through an audio recording 
repeatedly in order to locate a specifi c cue or word.
Compressed
Compressed
Original file
Restored
Original file
LOSSY
LOSSLESS
Restored
Digital audio formats that store varying amounts and arrangements of data. Choosing the appropriate 
fi le format can make a difference when the audio speed amplitude is manipulated, or compressed and 
restored. EBSCO illustration.

77
Principles of Computer Science
Computer programming: Music editing
Mixing and Mastering
Music editing software is used by sound engineers 
working with musicians to record and produce 
music for commercial purposes. Performers record 
their music in studios, often in separate sessions. 
Once all of the recording has been completed, 
sound engineers begin mixing the tracks together. 
Mixing combines different sounds into a single 
audio recording. One track might include the vo-
cals, another the backup singers, and a third the 
instruments. Recording these separately can help 
preserve the full depth of sound produced by each 
source (or input). Sound engineers mix these re-
cordings together such that they support and en-
hance one another instead of competing. Mixers 
allow them to adjust the volume of each input 
(channel), equalize frequencies, reduce noise, add 
effects, and control channel subgroups (buses). 
Music editing software usually includes a virtual 
mixer and a timeline with a scroll bar, allowing the 
editor to move back and forth along an audio track 
in a process called scrubbing. Scrubbing is partic-
ularly helpful for aligning vocal tracks with other 
audio tracks or with film visuals.
When the mixing stage has been completed for 
an album, the next step is mastering. This is the 
creation of a master recording that can be used to 
produce copies for sale thereafter. Mastering was 
much more complex when music was mainly sold 
on physical media, such as record albums or cas-
sette tapes. The recording master had to be pro-
tected from theft, damage, and the ravages of time. 
Today, most music editing is done entirely digitally. 
In digital mastering, the mixed audio recording is 
checked for errors, reformatted for distribution, 
and then subtly adjusted for the best sound quality 
possible.
Compression and Transmission
Music editing can be destructive, meaning that 
changing a recording destroys the original version 
of the recording. Or it can be nondestructive, in 
which editing preserves the original recording. The 
technology that made it possible for music to move 
away from physical media was compression, specifi-
cally the MP3 compression algorithm and file type. 
MP3 was developed in the early 1990s as a type of 
lossy compression for audio files. It can greatly re-
duce the size of an ordinary audio file while losing 
only a negligible amount of sound quality. MP3 is 
an audio codec that makes music portable by com-
pressing it into files small enough for transfer over 
the Internet and for digital storage. A codec can 
reduce the size of audio files by using shorthand 
to describe repeated data within the file more con-
cisely. When a compressed recording is played back, 
it is decoded by the device playing it, provided that 
the device is equipped with the proper codec. The 
recording’s fidelity (similarity to the original) de-
pends on how much information is kept and how 
much of the original frequency and dynamic range 
are captured. For instance, 16 bits of data are en-
coded for a high fidelity CD as compared to 24 bits 
in a studio recording.
Motion Picture Music Editing
Motion pictures rely on the skills of sound engi-
neers to mix the film’s dialogue, musical score and 
soundtrack, and sound effects into a single audio ac-
companiment to the visuals. The process is like that 
used in a recording studio. However, there is the 
added complication that not only must the audio 
tracks all be in harmony, they must also follow the 
timing of the movie’s scenes. Usually the dialogue 
takes priority with the music and sound effects being 
mixed in later.
Culture of Remixing
Unfortunately, the development and adoption 
of the MP3 audio codec made it easier and more 
common than ever for music to be shared illegally. 
People found it preferable to make illegal copies of 
music than to buy it, since those copies essentially 
retain the same quality as the original. Despite this, 
some praise the sharing of music online as giving 
rise to a new culture of remixing. Artists release 
their creative output into the world for audiences 
to enjoy it and other artists to build off it and even 
transform it. In remixing, other artists add their 
own perspectives to produce a collaborative work 
that is greater than the sum of its parts. Consumer 
audio editing software, such as Audacity, Adobe 
Audition, or Lexis Audio Editor, enables such art-
ists to record and mix music using just their PC in-
stead of a studio.
—Scott Zimmer, JD

78
Computer programming: Video editing
Principles of Computer Science
Bibliography
Collins, Mike. Pro Tools 11: Music Production, Recording, 
Editing, and Mixing. Burlington: Focal, 2014. Print.
Cross, Mark. Audio Post Production for Film and 
Television. Boston: Berklee, 2013. Print.
Kefauver, Alan P., and David Patschke. Fundamentals 
of Digital Audio. Middleton: A-R Editions, 2007. 
Print.
Pinch, T. J., and Karin Bijsterveld. The Oxford 
Handbook of Sound Studies. New York: Oxford UP, 
2013. Print.
Saltzman, Steven. Music Editing for Film and Television: 
The Art and the Process. Burlington: Focal, 2015. 
Print.
Shen, Jialie, John Shepherd, Bin Cui, and Ling 
Liu. Intelligent Music Information Systems: Tools and 
Methodologies. Hershey: IGI Global, 2008. Print.
Computer programming: Video editing
Fields of Study
Digital Media; Graphic Design
Abstract
Video editing is the process of altering, combining, 
or otherwise manipulating video frames or sequences 
of video frames after recording. Early video editing 
was done with time-consuming and visually limited 
linear editing techniques. Revolutionary advances 
in digital technology ushered in the era of nonlinear 
video editing.
Prinicipal Terms

 bit rate: the amount of data encoded for each 
second of video; often measured in kilobits per 
second (kbps) or kilobytes per second (Kbps).

 edit decision list (EDL): a list that catalogs the 
reel or time code data of video frames so that the 
frames can be accessed during video editing.

 frames per second (FPS): a measurement of the 
rate at which individual video frames are dis-
played.

 nonlinear editing: a method of editing video in 
which each frame of video can be accessed, al-
tered, moved, copied, or deleted regardless of 
the order in which the frames were originally re-
corded.

 video scratching: a technique in which a video se-
quence is manipulated to match the rhythm of a 
piece of music.

 vision mixing: the process of selecting and 
combining multiple video sources into a single 
video.
What Is Video Editing?
Video editing is the process of altering, combining, or 
otherwise manipulating video frames or sequences of 
video frames after recording. Cameras create video 
by recording a series of still images, or “frames,” 
which when rapidly displayed in sequence, create the 
appearance of a moving picture. The speed at which 
video frames are recorded and displayed is the frame 
rate. This is measured in frames per second (FPS). 
Early recording technologies used frame rates as low 
as 16 FPS. By contrast, digital video cameras such as 
the Go Pro Hero 4 can record at speeds as high as 240 
FPS. To achieve the semblance of smooth motion, 
frame rates of 24 to 30 FPS are used. Higher frame 
rates are said to provide clear, more lifelike images, 
while lower ones produce jerkier motion or blurrier 
video.
The frames recorded digitally are composed 
of bits. A bit is the smallest unit of data that can be 
stored and manipulated. The amount of data en-
coded, in bits, for each second of video is the bit rate. 
The greater the bit rate is, the higher the resolution 
and, thus, the quality. However, higher bit rates also 
result in larger video files.
Video Editing in Practice
In the past, video was edited using linear video ed-
iting, a technique for reordering sequences of video 
frames captured on analog video tape. Linear video 

79
Principles of Computer Science
Computer programming: Video editing
editing was initially achieved by physically cutting 
videotape into sections and then splicing the sec-
tions together in the desired sequence. By the 1960s, 
electronic tape editing technology allowed different 
sections of videotape to be combined mechanically 
onto a single videotape. Even with this advance, how-
ever, linear video editing remained diffi cult and time 
consuming.
The advent of digital technology revolutionized 
video editing. Computer hardware and video-ed-
iting software enabled the use of nonlinear editing. 
Nonlinear editing allows individual video frames to 
be accessed, altered, moved, copied, or deleted re-
gardless of the order in which they were recorded. 
The frames can then be combined into a single video 
sequence using software.
Nonlinear editing enabled video editors to em-
ploy a range of new techniques that would have 
been impossible to achieve previously. One such 
technique is video scratching, where a sequence of 
video frames is manipulated to match the rhythm 
of a piece of music. Another is digital compositing. 
In this process, video frames from different sources 
can be combined with each other and with still im-
ages and computer graphics to create the illusion 
that they were all recorded at once. This is related 
to vision mixing, in which various video sources are 
combined into a single recording. An example is the 
combining of footage from multiple cameras at a 
live event, such as a concert, for a seamless broadcast 
viewing experience.
An important tool used in video editing is the edit 
decision list (EDL). An EDL is an ordered list of video 
Compressed
Original file
Restored
Original file
LOSSY
LOSSLESS
Compressed
Restored
Digital video formats store varying quantities and qualities of data. Choosing the appropriate fi le 
format can make a difference when the audio speed amplitude is manipulated, or compressed and 
restored. With so many video formats available, it is important to choose editing software that is com-
patible with one’s fi le format of choice. EBSCO illustration.

80
Computer programming: Video editing
Principles of Computer Science
frames or frame sequences (“clips”) that contains in-
formation such as time data. That information allows 
the video editor to find individual clips quickly and 
efficiently during editing.
Video Editing in a Digital World
The development of affordable PCs, digital video 
cameras, and smartphones has brought video ed-
iting from the studio to the realm of ordinary 
people. Video-editing software ranges from appli-
cations for PCs, such as Adobe’s Premiere, to those 
designed for smartphones, such as Corel’s Pinnacle 
Studio Pro. Others, such as Apple’s iMovie, work 
on both PC and mobile devices. When used along 
with social media websites, such as YouTube and 
Facebook, video-editing software allows users to 
easily share videos with anyone who has an Internet 
connection. Thus, anyone with a smartphone can 
be a video writer, director, editor, and movie studio. 
This has led to a powerful new form of personal and 
artistic expression that will continue to grow and 
change as new video technologies are developed 
and adopted.
—Maura Valentino, MSLIS
Bibliography
Anderson, Gary H. Video Editing and Post Production: A 
Professional Guide. Woburn: Focal, 1999. Print.
Ascher, 
Steven. 
The 
Filmmaker’s 
Handbook: 
A 
Comprehensive Guide for the Digital Age. New York: 
Plume, 2012. Print.
Dancyger, Ken. The Technique of Film and Video Editing: 
History, Theory, and Practice. Burlington: Focal, 
2013. Digital file.
Goodman, Robert, and Patrick McGrath. Editing 
Digital Video: The Complete Creative and Technical 
Guide. New York: McGraw, 2003. Print.
Ohanian, Thomas. Digital Nonlinear Editing: Editing 
Film and Video on the Desktop. Woburn: Focal, 1998. 
Print.
Rubin, Michael. Nonlinear—A Field Guide to Digital 
Video and Film Editing. Gainesville: Triad, 2000, 
Print.
Tarantola, Andrew. “Why Frame Rate Matters.” 
Gizmodo. Gizmodo, 14 Jan. 2015. Web. 11 Mar. 
2016.
Sample Problem
When editing video footage in a digital envi-
ronment, the size and quality of the final file 
must be considered. Video file size and video 
quality are directly related. As the quality of 
a digital video increases, so does its file size 
(measured in bits). A video’s file size can be 
calculated using the following formulas:
video size = bit rate × video length
bit rate = pixel density × color depth × 
frame rate
Calculate the maximum frame rate in FPS 
that can be used to ensure that a video’s file 
size will be 7,200,000 bits if the pixel density 
is 1,000 pixels per frame, the color depth is 24 
bits per pixel, and the video is 15 seconds long.
Note that this formula only takes the visual 
portion of a video into account and that to ob-
tain the actual final file size, one would need 
to calculate the size of the related audio data 
and add the two together.
Answer:
First, substitute the formula for bit rate into 
the formula for video size:
video size = bit rate × video length
video size = (pixel density × color depth × 
frame rate) × video length
Then rewrite the equation in terms of frame 
rate:
frame rate = video size / (pixel density × color 
depth × video length)
Next substitute in the known values for video 
size, pixel density, color depth, and video 
length, and solve:
frame rate = 7,200,000 bits / [(1,000 pixels/
frame)(24 bits/pixel)(15 seconds)]
= (7,200,000 bits / 24,000 bits/frame)(1/15 
seconds)
= (300 frames)(1/15 seconds)
= 20 frames/second (FPS)

81
Principles of Computer Science
Computer security
CoMputer seCurIty
Fields oF study
Information Technology; Security
AbstrAct
The goal of computer security is to prevent computer 
and network systems from being accessed by those 
without proper authorization. It encompasses dif-
ferent aspects of information technology, from hard-
ware design and deployment to software engineering 
and testing. It even includes user training and work-
fl ow analysis. Computer security experts update 
software with the latest security patches, ensure that 
hardware is designed appropriately and stored safely, 
and train users to help protect sensitive information 
from unauthorized access.
PriniciPAl terms
 
 backdoor: a hidden method of accessing a com-
puter system that is placed there without the 
knowledge of the system’s regular user in order to 
make it easier to access the system secretly.
 
  device fi ngerprinting: information that uniquely 
identifi es a particular computer, component, or 
INTERNET
INTERNET
Hardware firewall
Software firewall
Access denied
Encrypted wireless 
connection
Key encryption
Key encryption
Password denied
Computer security can come in many forms to ensure data and programs are protected. Passwords limit 
who can access a computer, key encryption limits who can read transmitted data, and fi rewalls can limit 
intrusion from the Internet. EBSCO illustration.

82
Computer security
Principles of Computer Science
piece of software installed on the computer. This 
can be used to find out precisely which device ac-
cessed a particular online resource.

 intrusion detection system: a system that uses hard-
ware, software, or both to monitor a computer or 
network in order to determine when someone at-
tempts to access the system without authorization.

 phishing: the use of online communications in 
order to trick a person into sharing sensitive per-
sonal information, such as credit card numbers or 
social security numbers.

 principle of least privilege: a philosophy of com-
puter security that mandates users of a computer 
or network be given, by default, the lowest level 
of privileges that will allow them to perform their 
jobs. This way, if a user’s account is compromised, 
only a limited amount of data will be vulnerable.

 trusted platform module (TPM): a standard used 
for designing cryptoprocessors, which are special 
chips that enable devices to translate plain text 
into cipher text and vice versa.
Hardware Security
The first line of defense in the field of computer 
security concerns the computer hardware itself. At 
a basic level, computer hardware must be stored in 
secure locations where it can only be accessed by 
authorized personnel. Thus, in many organizations, 
access to areas containing employee workstations is 
restricted. It may require a badge or other identifica-
tion to gain access. Sensitive equipment such as an 
enterprise server is even less accessible, locked away 
in a climate-controlled vault. It is also possible to add 
hardware security measures to existing computer 
systems to make them more secure. One example 
of this is using biometric devices, such as fingerprint 
scanners, as part of the user login. The computer 
will only allow logins from people whose fingerprints 
are authorized. Similar restrictions can be linked to 
voice authentication, retina scans, and other types 
of biometrics. Computer security can come in many 
forms to ensure data and programs are protected. 
Passwords limit who can access a computer, key en-
cryption limits who can read transmitted data, and 
firewalls can limit intrusion from the Internet.
Inside a computer, a special type of processor 
based on a trusted platform module (TPM) can 
manage encrypted connections between devices. 
This ensures that even if one device is compromised, 
the whole system may still be protected. Another type 
of security, device fingerprinting, can make it pos-
sible to identify which device or application was used 
to access a system. For example, if a coffee shop’s 
wireless access point was attacked, the access point’s 
logs could be examined to find the machine address 
of the device used to launch the attack. One highly 
sophisticated piece of security hardware is an intru-
sion detection system. These systems can take dif-
ferent forms but generally consist of a device through 
which all traffic into and out of a network or host is 
filtered and analyzed. The intrusion detection system 
examines the flow of data in order to pinpoint any 
attempt at hacking into the network. The system can 
then block the attack before it can cause any damage.
Network Security
Network security is another important aspect of com-
puter security. In theory, any computer connected to 
the Internet is vulnerable to attack. Attackers can try 
to break into systems by exploiting weak points in the 
software’s design or by tricking users into giving away 
their usernames and passwords. The latter method is 
called phishing, because it involves “fishing” for in-
formation. Both of these methods can be time con-
suming, however. So once a hacker gains access, they 
may install a backdoor. Backdoors allow easy, unde-
tected access to a system in future.
One way of preventing attackers from tricking au-
thorized users into granting access is to follow the 
principle of least privilege. According to this prin-
ciple, user accounts are given the minimum amount 
of access rights required for each user to perform 
their duties. For instance, a receptionist’s account 
would be limited to e-mail, scheduling, and switch-
board functions. This way, a hacker who acquired the 
receptionist’s username and password could not do 
things such as set their own salary or transfer com-
pany assets to their own bank account. Keeping privi-
leges contained thus allows an organization to mini-
mize the damage an intruder may try to inflict.
Software Security
Software represents another vulnerable point of com-
puter systems. This is because software running on a 
computer must be granted certain access privileges 
in order to function. If the software is not written in a 
secure fashion, then hackers may be able to enhance 
the software’s privileges. Hackers can then use these 

83
Principles of Computer Science
Computer-assisted instruction
enhanced privileges to perform unintended func-
tions or even take over the computer running the 
software. In the vernacular of hackers, this is known 
as “owning” a system.
A Moving Target
Computer security professionals have an unenviable 
task. They must interfere with the way users wish 
to use their computers, in order to make sure that 
hardware and software vulnerabilities are avoided as 
much as possible. Often, the same users whom they 
are trying to protect attempt to circumvent those 
protective measures, finding them inconvenient or 
downright burdensome. Computer security in these 
cases can become a balancing act between safety and 
functionality.
—Scott Zimmer, JD
Bibliography
Boyle, Randall, and Raymond R. Panko. Corporate Computer 
Security. 4th ed. Boston: Pearson, 2015. Print.4 Science 
Reference Center™ Computer Security
Brooks, R. R. Introduction to Computer and Network 
Security: Navigating Shades of Gray. Boca Raton: 
CRC, 2014. Digital file.
Jacobson, Douglas, and Joseph Idziorek. Computer 
Security Literacy: Staying Safe in a Digital World. Boca 
Raton: CRC, 2013. Print.
Schou, Corey, and Steven Hernandez. Information 
Assurance Handbook: Effective Computer Security and 
Risk Management Strategies. New York: McGraw, 
2015. Print.
Vacca, John R. Computer and Information Security 
Handbook. Amsterdam: Kaufmann, 2013. Print.
Williams, Richard N. Internet Security Made Easy: Take 
Control of Your Computer. London: Flame Tree, 
2015. Print.
Computer-assisted instruction
Fields of Study
Computer Science; Information Systems
Abstract
Computer-assisted instruction is the use of com-
puter technology as a means of instruction or an aid 
to classroom teaching. The instructional content 
may or may not pertain to technology. Computer-
assisted instruction often bridges distances between 
instructor and student and allows for the instruction 
of large numbers of students by a few educators.
Prinicipal Terms

 learner-controlled program: software that allows 
a student to set the pace of instruction, choose 
which content areas to focus on, decide which 
areas to explore when, or determine the medium 
or difficulty level of instruction; also known as a 
“student-controlled program.”

 learning strategy: a specific method for acquiring 
and retaining a particular type of knowledge, such 
as memorizing a list of concepts by setting the list 
to music.

 learning style: an individual’s preferred approach 
to acquiring knowledge, such as by viewing visual 
stimuli, reading, listening, or using one’s hands to 
practice what is being taught.

 pedagogy: a philosophy of teaching that addresses 
the purpose of instruction and the methods by 
which it can be achieved.

 word prediction: a software feature that recog-
nizes words that the user has typed previously and 
offers to automatically complete them each time 
the user begins typing.
Computer-Assisted vs. Traditional 
Instruction
In a traditional classroom, a teacher presents infor-
mation to students using basic tools such as pencils, 
paper, chalk, and a chalkboard. Most lessons con-
sist of a lecture, group and individual work by stu-
dents, and the occasional hands-on activity, such as 
a field trip or a lab experiment. For the most part, 
students must adapt their learning preferences to 

84
Computer-assisted instruction
Principles of Computer Science
the teacher’s own pedagogy, because it would be im-
practical for one teacher to try to teach to multiple 
learning styles at once.
Computer-assisted instruction (CAI) supplements 
this model with technology, namely a computing de-
vice that students work on for some or all of a lesson. 
Some CAI programs offer limited options for how 
the material is presented and what learning strategies 
are supported. Others, known as learner-controlled 
programs, are more flexible.
A typical CAI lesson focuses on one specific con-
cept, such as long division or the history of Asia. The 
program may present information through audio, 
video, text, images, or a combination of these. It then 
quizzes the student to make sure that they have paid 
attention and understood the material. Such instruc-
tion has several benefits. First, students often receive 
information through several mediums, so they do 
not have to adapt to the teacher’s preferred style. 
Second, teachers can better support students as they 
move through the lessons without having to focus on 
presenting information to a group.
Advantages and Disadvantages
CAI has both benefits and drawbacks. Certain 
software features make it easier to navigate the 
learning environment, such as word prediction to 
make typing easier and spell-checking to help avoid 
spelling mistakes. Copy-and-paste features save 
users time that they would otherwise spend reen-
tering the same information over and over. Speech 
recognition can assist students who are blind, have 
physical disabilities, or have a learning disability that 
affects writing. Other helpful features are present 
despite not having been intended to benefit stu-
dents. For example, CAI video lessons include the 
option to pause playback, skip ahead or back, or re-
start. These functions can be vital to a student who 
is struggling to grasp an especially difficult lesson 
or is practicing note-taking. They can stop the video 
Yes
No
Another 
Try?
If Incorrect
If Correct
Help Session
Remediation
Provide Feedback 
to Student
Expert 
Solution
Student
Solution
Sample Problem
Compare Solutions and 
Evaluate Students
New Lesson
Computer-assisted instruction uses programming to determine whether a student understands the lesson (correctly an-
swers sample problems) or needs more help or remediation (incorrectly answers sample problems). This flowchart indi-
cates a general path of computer-assisted instruction. Adapted from S. Egarievwe, A. O. Ajiboye, and G. Biswas, “Internet 
Application of LabVIEW in Computer Based Learning,” 2000.

85
Principles of Computer Science
Computer-assisted instruction
at any time and restart it after catching up with the 
content that has been presented. By contrast, in a 
regular classroom, the lecturer often continues at 
the same pace regardless how many students may be 
struggling to understand and keep up.
Learning is rarely a “one size fits all” affair. 
Different topics pose greater or lesser challenges to 
different students, depending on their natural abili-
ties and study habits. In regular classrooms, teachers 
can often sense when some students are not ben-
efiting from a lesson and adapt it accordingly. With 
some CAI, this is not an option because the lesson is 
only presented in one way.
Adaptive Instruction
Fortunately, some forms of CAI address different 
learning rates by using adaptive methods to present 
material. These programs test students’ knowledge 
and then adapt to those parts of the lesson with 
which they have more difficulty. For instance, if a 
math program notices that a student often makes 
mistakes when multiplying fractions, it might give 
the student extra practice in that topic. Adaptive 
programs give teachers the means to better assess stu-
dents’ individual needs and track their progress. As 
the technology improves, more detailed and specific 
results may bolster teachers’ efforts to tailor instruc-
tion further.
Distance Education
CAI is especially important to the growing field of on-
line education. Online instructors often use elements 
of CAI to supplement their curricula. For example, 
an online course might require students to watch a 
streaming video about doing library research so that 
they will know how to complete their own research 
paper for the course. Online education also enables 
just a few instructors to teach large numbers of stu-
dents across vast distances. Tens of thousands of 
students may enroll in a single massive open online 
course (MOOC).
—Scott Zimmer, JD
Bibliography
Abramovich, Sergei, ed. Computers in Education. 2 
vols. New York: Nova, 2012. Print.
Erben, Tony, Ruth Ban, and Martha E. Castañeda. 
Teaching 
English 
Language 
Learners 
through 
Technology. New York: Routledge, 2009. Print.
Miller, Michelle D. Minds Online: Teaching Effectively 
with Technology. Cambridge: Harvard UP, 2014. 
Print.
Roblyer, M. D., and Aaron H. Doering. Integrating 
Educational Technology into Teaching. 6th ed. Boston: 
Pearson, 2013. Print.
Tatnall, Arthur, and Bill Davey, eds. Reflections on 
the History of Computers in Education: Early Use of 
Computers and Teaching about Computing in Schools. 
Heidelberg: Springer, 2014. Print.
Tomei, Lawrence A., ed. Encyclopedia of Information 
Technology Curriculum Integration. 2 vols. Hershey: 
Information Science Reference, 2008. Print.

86
Connection machine
Principles of Computer Science
Connection machine
Fields of Study
Computer 
Science; 
Computer 
Engineering; 
Information Technology
Abstract
The Connection Machines grew out of Daniel Hillis’s 
Ph.D. Thesis research in Electrical Engineering and 
Computer Science at the Massachusetts Institute 
of Technology. The basic idea was that of having a 
massively parallel computer network, with each pro-
cessor connected to all the others. The majority of 
digital computers have the so-called Von Neumann 
Architecture, in which a Central Processing Unit 
does the actual computation, taking intermediate 
results from specified locations and storing inter-
mediate results for later use. But the human brain 
and other brains found in nature do not work in 
that way. Instead there is no central processor, and 
each neuron is connected to a great many others. 
The connection machine series was one of the first 
commercial attempts to capture the brain’s way of 
thinking. 
Principal Terms

 central processing unit: the main operating func-
tion of a conventional computer which can be pro-
grammed to read characters from a storage me-
dium, combine them as instructed and store the 
output for later use.

 Hebbian learning: machine learning as a result of 
modifying the weights in a neural network.

 Hopfield neural network: a one layer neural net 
where each neuron knows the state of all other 
neurons at each time step.

 parallel processing: several computational steps 
done simultaneously

 Von Neumann architecture: The simplest com-
puter organization, in which the central pro-
cessing unit modifies one location in memory at 
a time
History & Development of Connection 
Machines
The connection machines were among the first 
parallel processing computers to be commercially 
available. They were an outgrowth of Daniel 
Hillis’s Ph. D. Thesis at MIT, built partially on the 
neural network ideas of John Hopfield and influ-
enced in its early stages by Nobel Laureate Richard 
P. Feynman, a colorful figure who began his career 
as head of a computational group at Los Alamos 
and went on to be one of the founders of quantum 
electrodynamics.
In the early days of computation, McCullough 
and Pitts studied the behavior of collections of neu-
rons whose output was a step function of a weighed 
sum of their inputs. If the weights were allowed to 
change based on the number of times a neuron 
fired, learning was found to take place. This was an 
example of Hebbian learning, so named after D. 
O. Hebb who promoted the concept. In the 1970’s 
John Hopfield studied the behavior of a single net-
work of such neurons. The Hopfield arrangement of 
neurons was the basis for the first versions of the con-
nection machine. Hopfield’s method proved ideally 
suited to the connection machine, with the weights 
represented as decimal numbers. This allowed all 
the processors to be used concurrently resulting in 
computing times orders of magnitude faster then in 
conventional machines. 
Thinking Machines Corporation was founded 
by Daniel Hillis and Sheryl Handler in Waltham, 
Massachusetts in 1983 and later moved to 
Cambridge MA. In 1999 it was acquired by Sun 
Microsystems.
While it would in principle be desirable for a 
computer to simultaneously update all processors, 
it proved much too unwieldy for the first connec-
tion machines, which instead adopted a hypercube 
arrangement of processors. The CM-1 had as many 
as 65,536 processors each processing one bit at a 
time. The CM-2 launched in 1987 added floating 
point numeric coprocessors. In 1991 the CM 5 
was announced, Thinking Machines went to a new 
architecture.
Much thinking had been devoted to the phys-
ical layout of the connection machines. The ini-
tial ­designs reflected the machines hypercube 
­architecture. The CM-5 had large panels of red 
blinking light emitting diodes. Because of its de-
sign, a CM 5 was included in the control room in 
Jurassic Park.

87
Principles of Computer Science
Constraint programming
The involvement of Richard Feynman, who 
shared in the Nobel Prize for Physics in 1965 is 
noteworthy for several reasons. Feynman, one of 
whose major contribution to quantum electrody-
namics was in introducing a diagrammatic method 
for summing term in the solution of the basic equa-
tions of motion was a, graduate student in Physics 
at Princeton University when World War II began. 
While completing his thesis he was also head of 
the theoretical division at Los Alamos, which then 
consisted mainly of high school graduates working 
with adding machines or similar devices to solve 
problems which had grown out of the atomic bomb 
project. It was this great familiarity with the innards 
of machines that did arithmetic that may have led 
him to the ultimate techniques involving diagrams 
in the theory of electrons and photons. By the time 
the connection machine was contemplated, physi-
cists had moved to the theory of particles within 
the nucleus, quantum chromodynamics, so named 
after the quantum rules that explained which par-
ticles could be observed. And indeed one of the 
first problems to which the connection machine 
was applies involved the application of quantum 
chromodynamics.
—Donald Franceschetti, PhD
Bibliography
Anderson, James A, Edward Rosenfeld, and Andras 
Pellionisz. Neurocomputing. Cambridge, Mass: MIT 
Press, 1990. Print.
Hillis, W. Daniel. “Richard Feynman and the 
Connection Machine.” Phys. Today Physics Today 
42.2 (1989): 78. Web.
Hopfield, J. J. “Neural Networks and Physical 
Systems with Emergent Collective Computational 
Abilities.” Proceedings of the National Academy of 
Sciences 79.8 (1982): 2554-558. Web.
Constraint programming
Fields of Study
Software Engineering; System-Level Programming
Abstract
Constraint programming typically describes the 
process of embedding constraints into another lan-
guage, referred to as the “host” language since it 
hosts the constraints. Not all programs are suitable 
for constraint programming. Some problems are 
better addressed by different approaches, such as 
logic programming. Constraint programming tends 
to be the preferred method when one can envision 
a state in which multiple constraints are simultane-
ously satisfied, and then search for values that fit that 
state.
Prinicipal Terms

 constraints: limitations on values in computer 
programming that collectively identify the so-
lutions to be produced by a programming 
problem.

 domain: the range of values that a variable may 
take on, such as any even number or all values less 
than −23.7.

 functional programming: a theoretical approach 
to programming in which the emphasis is on ap-
plying mathematics to evaluate functional rela-
tionships that are, for the most part, static.

 imperative programming: programming that pro-
duces code that consists largely of commands is-
sued to the computer, instructing it to perform 
specific actions.
Models of Constraint Programming
Constraint programming tends to be used in situa-
tions where the “world” being programmed has mul-
tiple constraints, and the goal is to have as many of 
them as possible satisfied at once. The aim is to find 
values for each of the variables that collectively de-
scribe the world, such that the values fall within that 
variable’s constraints.
To achieve this state, programmers use one of 
two main approaches. The first approach is the 
perturbation model. Under this model, some of 

88
Constraint programming
Principles of Computer Science
the variables are given initial values. Then, at dif-
ferent times, variables are changed (“perturbed”) 
in some way. Each change then moves through the 
system of interrelated variables like ripples across 
the surface of a pond: other values change in ways 
that follow their constraints and are consistent with 
the relationships between values. The perturbation 
model is much like a spreadsheet. Changing one 
cell’s value causes changes in other cells that con-
tain formulas that refer to the value stored in the 
original cell. A single change to a cell can propa-
gate throughout many other cells, changing their 
values as well.
A contrasting approach is the refinement model. 
Whereas the perturbation model assigns particular 
values to variables, under the refinement model, 
each variable can assume any value within its domain. 
Then, as time passes, some values of one variable will 
inevitably be ruled out by the values assumed by other 
variables. Over time, each variable’s possible values 
are refined down to fewer and fewer options. The 
refinement model is sometimes considered more 
flexible, because it does not confine a variable to one 
possible value. Some variables will occasionally have 
multiple solutions.
A Different Approach to Programming
Constraint programming is a major departure from 
more traditional approaches to writing code. Many 
programmers are more familiar with imperative pro-
gramming, where commands are issued to the com-
puter to be executed, or functional programming, 
Define 
problem
Best fit solution 
according to current 
constraints
Decision-making 
(and retracting)
Constraint 
propagation
Deduce 
contradictions
New constraints
(decisions)
Define initial
constraints
Current set of
constraints
In constraint programming, the design of the program is dictated by specific constraints determined prior to development. After 
a problem is identified, the initial constraints are defined and condensed into the current program constraints. As solutions are 
developed and tested, decisions are made as to whether the constraints must be modified or retracted to remove duplications or 
whether new ones must be developed. The process cycles through current restraints and solutions until a solution is developed 
that meets all the constraints and solves the initial problem. Adapted from Philippe Baptiste, “Combining Operations Research 
and Constraint Programming to Solve Real-Life Scheduling Problems.”

89
Principles of Computer Science
Constraint programming
where the program receives certain values and then 
performs various mathematical functions using those 
values. Constraint programming, in contrast, can be 
less predictable and more flexible.
A classic example of a problem that is especially 
well suited to constraint programming is that of map 
coloring. In this problem, the user is given a map of 
a country composed of different states, each sharing 
one or more borders with other states. The user is 
also given a palette of colors. The user must find a 
way to assign colors to each state, such that no adja-
cent states (i.e., states sharing a border) are the same 
color. Map makers often try to accomplish this in real 
life so that their maps are easier to read.
Those experienced at constraint programming 
can immediately recognize some elements of this 
problem. The most obvious is the constraint, which 
is the restriction that adjacent states may not be the 
same color. Another element is the domain, which is 
the list of colors that may be assigned to states. The 
fewer the colors included in the domain, the more 
challenging the problem becomes. While this map-
coloring problem may seem simplistic, it is an excel-
lent introduction to the concept of constraint pro-
gramming. It provides a useful situation for student 
programmers to try to translate into code.
Feasibility vs. Optimization
Constraint programming is an approach to problem 
solving and coding that looks only for a solution that 
works. It is not concerned with finding the optimal 
solution to a problem, a process known as “optimi-
zation.” Instead, it seeks values for the variables that 
fit all of the existing constraints. This may seem like 
a limitation of constraint programming. However, 
its flexibility can mean that it solves a problem faster 
than expected.
Another example of a problem for which con-
straint programming is well suited is that of creating 
a work schedule. The department or team contains 
multiple variables (the employees), each with their 
own constraints. Mary can work any day except 
Friday, Thomas can work mornings on Monday 
through Thursday but only evenings on Friday, and 
so forth. The goal is to simply find a schedule that fits 
all of the constraints. It does not matter whether it is 
the best schedule, and in fact, there likely is no “best” 
schedule.
—Scott Zimmer, JD
Bibliography
Baptiste, Philippe, Claude Le Pape, and Wim Nuijten. 
Constraint-Based Scheduling: Applying Constraint 
Programming to Scheduling Problems. New York: 
Springer, 2013. Print.
Ceberio, Martine, and Vladik Kreinovich. Constraint 
Programming and Decision Making. New York: 
Springer, 2014. Print.
Henz, Martin. Objects for Concurrent Constraint 
Programming. New York: Springer, 1998. Print.
Hofstedt, Petra. Multiparadigm Constraint Programming 
Languages. New York: Springer, 2013. Print.
Pelleau, Marie, and Narendra Jussien. Abstract Domains 
in Constraint Programming. London: ISTE, 2015. Print.
Solnon, Christine. Ant Colony Optimization and Constraint 
Programming. Hoboken: Wiley, 2010. Print.

90
Control systems
Principles of Computer Science
Control systems
Fields of Study
Embedded Systems; System Analysis
Abstract
A control system is a device that exists to control mul-
tiple other systems or devices. For example, the con-
trol system in a factory would coordinate the opera-
tion of all of the factory’s interconnected machines. 
Control systems are used because the coordination 
of these functions needs to be continuous and nearly 
instantaneous, which would be difficult and tedious 
for human beings to manage.
Prinicipal Terms

 actuator: a motor designed to move or control an-
other object in a particular way.

 automatic sequential control system: a mechanism 
that performs a multistep task by triggering a se-
ries of actuators in a particular sequence.

 autonomous agent: a system that acts on behalf of 
another entity without being directly controlled by 
that entity.

 fault detection: the monitoring of a system in 
order to identify when a fault occurs in its opera-
tion.

 system agility: the ability of a system to respond 
to changes in its environment or inputs without 
failing altogether.

 system identification: the study of a system’s inputs 
and outputs in order to develop a working model 
of it.
Types of Control Systems
Different types of control systems are used in dif-
ferent situations. The nature of the task being per-
formed usually determines the design of the system. 
At the most basic level, control systems are classified 
as either open-loop or closed-loop systems. In an 
open-loop system, the output is based solely on the 
input fed into the system. In a closed-loop system, the 
output generated is used as feedback to adjust the 
system as necessary. Closed-loop systems can make 
fault detection more difficult, as they are designed to 
minimize the deviations created by faults.
One of the first steps in designing a control system 
is system identification. This is the process of modeling 
the system to be controlled. It involves studying the in-
puts and outputs of the system and determining how 
they need to be manipulated in order to produce the 
desired outcome. Some control systems require a cer-
tain degree of human interaction or guidance during 
their operation. However, control-system designers 
generally prefer systems that can function as autono-
mous agents. The purpose of a control system is to 
reduce human involvement in the process as much 
as possible. This is partly so personnel can focus on 
other, more important tasks and partly because prob-
lems are more likely to arise from human error.
The downside of an autonomous agent is that it 
requires greater system agility, so that when problems 
are encountered, the control system can either con-
tinue to perform its role or “fail gracefully” rather 
than failing catastrophically or ceasing to function al-
together. This could mean the difference between a 
jammed conveyor belt being automatically corrected 
and that same belt forcing an entire assembly line to 
shut down.
Control System Performance
Control systems are more than just computers that 
monitor other processes. Often, they also control 
the physical movements of system components. A 
control system may cause an assembly-line robot 
to pivot an automobile door so that another ma-
chine can attach its handle, or it may cause a de-
vice to fold cardboard into boxes to be filled with 
candy. Such tasks are accomplished via actuators, 
which are motors that the control system manipu-
lates to execute the necessary physical movements. 
Sometimes only a single movement is required 
from a device. At other times, a device may have to 
perform several different movements in a precisely 
timed sequence. If this is the case, an automatic se-
quential control system is used to tell the machine 
what to do and when.
The development of very small and inexpensive 
microprocessors and sensors has made it possible for 
them to be incorporated into control systems. Similar 
to the closed-loop approach, these tiny computers 
help a control system monitor its performance and 

91
Principles of Computer Science
Control systems
provide detailed information about potential issues. 
For example, if a control system were using a new 
type of raw material that produced increased heat 
due to friction and caused the system to operate less 
efficiently, microsensors inside the machinery could 
detect this and alert those in charge of supervising 
the system. Such information could help avoid costly 
breakdowns of equipment and delays in production 
by allowing supervisors to address problems before 
they become severe.
Linear Control Systems
Linear control systems are a type of closed-loop 
system. They receive linear negative feedback from 
their outputs and adjust their operating param-
eters in response. This allows the system to keep 
relevant variables within acceptable limits. If the 
sensors in a linear control system detect excess 
heat, as in the example above, this might cause the 
system to initiate additional cooling. An example 
of this is when a computer’s fan speeds up after 
an intensive application causes the system to begin 
generating heat.
Behind the Scenes
Control systems are a vital part of everyday life in the 
industrialized world. Most of the products people use 
every day are produced or packaged using dozens of 
different control systems. Human beings have come 
to rely heavily on technology to assist them in their 
work, and in some cases to completely take over that 
work. Control systems are the mechanisms that help 
make this happen.
—Scott Zimmer, JD
Feedback
Disturbance
Output
Controller
System
Feed Forward/Anticipation/Planning
Disturbance
Output
Controller
System
Control systems are used to alter a process pathway to reach a desired output. A controller senses some type of information 
along the path-way and causes a change in the system. Making changes to the system prior to system input is called a “feed-
forward control”; making changes to the system based on the output is called “feedback control.” Adapted from http://www.
csci.csusb.edu/dick/cs372/a1.html. EBSCO illustration

92
CPU design
Principles of Computer Science
Bibliography
Ao, Sio-Iong, and Len Gelman, eds. Electrical 
Engineering and Intelligent Systems. New York: 
Springer, 2013. Print.
Chen, Yufeng, and Zhiwu Li. Optimal Supervisory 
Control of Automated Manufacturing Systems. Boca 
Raton: CRC, 2013. Print.
Janert, Philipp K. Feedback Control for Computer Systems. 
Sebastopol: O’Reilly, 2014. Print.
Li, Han-Xiong, and XinJiang Lu. System Design and 
Control Integration for Advanced Manufacturing. 
Hoboken: Wiley, 2015. Print.
Song, Dong-Ping. Optimal Control and Optimization of 
Stochastic Supply Chain Systems. London: Springer, 
2013. Print.
Van Schuppen, Jan H., and Tiziano Villa, eds. 
Coordination Control of Distributed Systems. Cham: 
Springer, 2015. Print
CPU design
Fields of Study
Computer Engineering; Information Technology
Abstract
CPU design is an area of engineering that focuses 
on the design of a computer’s central processing 
unit (CPU). The CPU acts as the “brain” of the ma-
chine, controlling the operations carried out by the 
computer. Its basic task is to execute the instructions 
contained in the programming code used to write 
software. Different CPU designs can be more or less 
efficient than one another. Some designs are better 
at addressing certain types of problems.
Prinicipal Terms

 control unit design: describes the part of the CPU 
that tells the computer how to perform the in-
structions sent to it by a program.

 datapath design: describes how data flows through 
the CPU and at what points instructions will be 
­decoded and executed.

 logic implementation: the way in which a 
CPU is designed to use the open or closed 
state of ­combinations of circuits to represent 
­information.

 microcontroller: a tiny computer in which all of 
the essential parts of a computer are united on 
a single microchip—input and output channels, 
memory, and a processor.

 peripherals: devices connected to a computer but 
not part of the computer itself, such as scanners, 
external storage devices, and so forth.

 protocol processor: a processor that acts in a 
secondary capacity to the CPU, relieving it from 
some of the work of managing communication 
protocols that are used to encode messages on the 
network.
CPU Design Goals
The design of a CPU is a complex undertaking. The 
main goal of CPU design is to produce an architec-
ture that can execute instructions in the fastest, most 
efficient way possible. Both speed and efficiency are 
relevant factors. There are times when having an in-
struction that is fast to execute is adequate, but there 
are also situations where it would not make sense to 
have to execute that simple instruction hundreds of 
times in order to accomplish a task.
Often the work begins by designers considering 
what the CPU will be expected to do and where it will 
be used. A microcontroller inside an airplane per-
forms quite different tasks than one inside a kitchen 
appliance, for instance. The CPU’s intended function 
tells what type of instruction-set architecture to use in 
the microchip that contains the CPU. Knowing what 
types of programs will be used most frequently also 
allows CPU designers to develop the most efficient 
logic implementation. Once this has been done, the 
control unit design can be defined. Defining the 
datapath design is usually the next step, as the CPU’s 
handling of instructions is given physical form.
Often a CPU will be designed with additional sup-
ports to handle the processing load, so that the CPU 
itself does not become overloaded. Protocol proces-
sors, for example, may assist with communications 

93
Principles of Computer Science
CPU design
protocol translation involving the transmission of 
data between the computer and its peripherals or 
over the Internet. Internet communication proto-
cols are quite complex. They involve seven different, 
nested layers of protocols, and each layer must be ne-
gotiated before the next can be addressed. Protocol 
processors take this workload off the CPU.
instruction sets
CPU design is heavily infl uenced by the type of in-
struction sets being used. In general, there are two 
approaches to instructions. The fi rst is random 
logic. Sometimes this is referred to as “hardwired 
instructions.” Random logic uses logic devices, such 
as decoders and counters, to transport data and to 
perform calculations. Random logic can make it pos-
sible to design faster chips. The logic itself takes up 
space that might otherwise be used to store instruc-
tions, however. Therefore, it is not practical to use 
random logic with very large sets of instructions.
The second approach to instruction sets is micro-
code. Microcode is sometimes called “emulation” be-
cause it references an operations table and uses sets 
of microinstructions indexed by the table in order to 
execute each CPU instruction. Microcode can some-
times be slower to run than random logic, but it also 
Decode for execution
Execute instructions
Fetch from memory
A generic state diagram shows the simple processing loop: fetch instructions from memory, decode instruc-
tions to determine the proper execute cycle, execute instructions, and then fetch next instructions from 
memory and continue the cycle. A state diagram is the initial design upon which data paths and logic controls 
can be designed. EBSCO illustration.

94
CPU design
Principles of Computer Science
has advantages that offset this weakness. Microcode 
breaks down complex instructions into sets of mi-
croinstructions. These microinstructions are used 
in several complex instructions. A CPU executing 
microcode would therefore be able to reuse microin-
structions. Such reuse saves space on the microchip 
and allows more complex instructions to be added.
The most influential factor to consider when 
weighing random logic against microcode is memory 
speed. Random logic usually produces a speedier 
CPU when CPU speeds outpace memory speeds. 
When memory speeds are faster, microcode is faster 
than random logic.
Reduced Instruction Set Computer 
(RISC)
Early in the history of CPU design, it was felt that the 
best way to improve CPU performance was to con-
tinuously expand instruction sets to give program-
mers more options. Eventually, studies began to 
show that adding more complex instructions did not 
always improve performance, however. In response, 
CPU manufacturers produced reduced instruction 
set computer (RISC) chips. RISC chips could use less 
complex instructions, even though this meant that a 
larger number of instructions were required.
Moore’s Law
Moore’s law is named after Gordon Moore, a co-
founder of the computer manufacturer Intel. In 
1975, Moore observed that the computing power of 
an integrated circuit or microchip doubles, on av-
erage, every two years. This pace of improvement has 
been responsible for the rapid development in tech-
nological capability and the relatively short lifespan 
of consumer electronics, which tend to become obso-
lete soon after they are purchased.
—Scott Zimmer, JD
Bibliography
Englander, Irv. The Architecture of Computer Hardware, 
Systems Software, & Networking: An Information 
Technology Approach. Hoboken: Wiley, 2014. Print.
Hyde, Randall. Write Great Code: Understanding the 
Machine. Vol. 1. San Francisco: No Starch, 2005. 
Print.
Jeannot, Emmanuel, and J. Žilinskas. High Performance 
Computing on Complex Environments. Hoboken: 
Wiley, 2014. Print.
Lipiansky, Ed. Electrical, Electronics, and Digital 
Hardware Essentials for Scientists and Engineers. 
Hoboken: Wiley, 2013. Print.
Rajasekaran, 
Sanguthevar. 
Multicore 
Computing: 
Algorithms, Architectures, and Applications. Boca 
Raton: CRC, 2013. Print.
Stokes, Jon. Inside the Machine: An Illustrated 
Introduction 
to 
Microprocessors 
and 
Computer 
Architecture. San Francisco: No Starch, 2015. Print.
Wolf, Marilyn. High Performance Embedded Computing: 
Architectures, 
Applications, 
and 
Methodologies. 
Amsterdam: Elsevier, 2014. Print.

95
Principles of Computer Science
Cryptography
Cryptography
Fields of Study
Computer 
Science; 
Computer 
Engineering; 
Algorithms
Abstract
Cryptography is the process of encrypting messages 
and other data in order to transmit them in a form 
that can only be accessed by the intended recipients. 
It was initially applied to written messages. With the 
introduction of modern computers, cryptography 
became an important tool for securing many types of 
digital data.
Prinicipal Terms

 hash function: an algorithm that converts a string 
of characters into a different, usually smaller, 
fixed-length string of characters that is ideally im-
possible either to invert or to replicate.

 public-key cryptography: a system of encryption 
that uses two keys, one public and one private, to 
encrypt and decrypt data.

 substitution cipher: a cipher that encodes a mes-
sage by substituting one character for another.

 symmetric-key cryptography: a system of encryp-
tion that uses the same private key to encrypt and 
decrypt data.

 transposition cipher: a cipher that encodes a 
message by changing the order of the characters 
within the message.
What Is Cryptography?
The word “cryptography” comes from the Greek 
words kryptos (“hidden,” “secret”) and graphein 
(“writing”). Early cryptography focused on ensuring 
that written messages could be sent to their intended 
recipients without being intercepted and read by 
other parties. This was achieved through various en-
cryption techniques. Encryption is based on a simple 
principle: the message is transformed in such a way 
that it becomes unreadable. The encrypted message 
is then transmitted to the recipient, who reads it by 
transforming (decrypting) it back into its original 
form.
Alice’s 
Private Key
Random 
Session Key
Bob’s
Public Key
Public Key
Cryptography
Public Key
Cryptography
Hash 
Function
Digital 
Envelope
Sent to 
Bob
Secret Key
Crytography
Alice’s 
Message
Encrypted 
Message
Digital
Signature
Encrypted 
Session Key
This is a diagram of cryptography techniques: public (asymmetric) key, private (symmetric) key, and hash functions. Each se-
cures data in a different way and requires specific types of keys to encrypt and decrypt the data. Adapted from Gary C. Kessler, 
“An Overview of Cryptography.”

96
Cryptography
Principles of Computer Science
Early forms of encryption were based on ciphers. 
A cipher encrypts a message by altering the charac-
ters that comprise the message. The original mes-
sage is called the “plaintext,” while the encrypted 
message is the “ciphertext.” Anyone who knows the 
rules of the cipher can decrypt the ciphertext, but 
it remains unreadable to anyone else. Early ciphers 
were relatively easy to break, given enough time and 
a working knowledge of statistics. By the 1920s, elec-
tromechanical cipher machines called “rotor ma-
chines” were creating complex ciphers that posed 
a greater challenge. The best-known example of a 
rotor machine was the Enigma machine, used by the 
German military in World War II. Soon after, the 
development of modern computer systems in the 
1950s would change the world of cryptography in 
major ways.
Cryptography in the Computer Age
With the introduction of digital computers, the 
focus of cryptography shifted from just written lan-
guage to any data that could be expressed in binary 
format. The encryption of binary data is accom-
plished through the use of keys. A key is a string of 
data that determines the output of a cryptographic 
algorithm. While there are many different types of 
cryptographic algorithms, they are usually divided 
into two categories. Symmetric-key cryptography 
uses a single key to both encrypt and decrypt the 
data. Public-key cryptography, also called “asym-
metric-key cryptography,” uses two keys, one public 
and one private. Usually, the public key is used to 
encrypt the data, and the private key is used to de-
crypt it.
When using symmetric-key cryptography, both the 
sender and the recipient of the encrypted message 
must have access to the same key. This key must be 
exchanged between parties using a secure channel, 
or else it may be compromised. Public-key cryptog-
raphy does not require such an exchange. This is one 
reason that public-key cryptography is considered 
more secure.
Another cryptographic technique developed 
for use with computers is the digital signature. A 
digital signature is used to confirm the identity 
of the sender of a digital message and to ensure 
that no one has tampered with its contents. Digital 
signatures use public-key encryption. First, a hash 
function is used to compute a unique value based 
on the data contained in the message. This unique 
value is called a “message digest,” or just “digest.” 
The signer’s private key is then used to encrypt the 
digest. The combination of the digest and the pri-
vate key creates the signature. To verify the digital 
signature, the recipient uses the signer’s public key 
to decrypt the digest. The same hash function is 
then applied to the data in the message. If the new 
digest matches the decrypted digest, the message 
is intact.
Decrypting a Basic Cipher
Among the earliest ciphers used were the transpo-
sition cipher and the substitution cipher. A trans-
position cipher encrypts messages by changing 
the order of the letters in the message using a 
well-defined scheme. One of the simplest transpo-
sition ciphers involves reversing the order of let-
ters in each word. When encrypted in this fashion, 
the message “MEET ME IN THE PARK” becomes 
“TEEM EM NI EHT KRAP.” More complicated 
transposition ciphers might involve writing out 
the message in a particular orientation (such as in 
stacked rows) and then reading the individual let-
ters in a different orientation (such as successive 
columns).
A substitution cipher encodes messages by substi-
tuting certain letters in the message for other letters. 
One well-known early substitution cipher is the Caesar 
cipher, named after Julius Caesar. This cipher encodes 
messages by replacing each letter with a letter that is 
a specified number of positions to its right or left in 
the alphabet. For example, Caesar is reported to have 
used a left shift of three places when encrypting his 
messages.
Using this cipher, Julius Caesar’s famous message 
“I came, I saw, I conquered” becomes “F ZXJB F PXT 
F ZLKNRBOBA” when encrypted.
Original
A
B
C
D
E
F
G H
I
J
K
L M N O
P
Q R
S
T
U
V W X
Y
Z
Replacement
X
Y
Z
A
B
C
D
E
F
G H
I
J
K
L M N O
P
Q R
S
T
U
V W

97
Principles of Computer Science
Cryptography
Sample Problem
The following message has been encoded 
with a Caesar cipher using a left shift of five:
JSHWDUYNTS NX KZS
What was the original text of the message?
Answer:
The answer can be determined by replacing 
each letter in the encoded message with the 
letter five places to the left of its position in 
the alphabet, as shown in the following chart:
Original
A B C D E F G H I
J
Replacement V W X Y Z A B C D E
The original text read “ENCRYPTION IS 
FUN.”
Why Is Cryptography Important?
The ability to secure communications against inter-
ception and decryption has long been an important 
part of military and international affairs. In the 
modern age, the development of new computer-
based methods of encryption has had a major im-
pact on many areas of society, including law enforce-
ment, international affairs, military strategy, and 
business. It has also led to widespread debate over 
how to balance the privacy rights of organizations 
and individuals with the needs of law enforcement 
and government agencies. Businesses, governments, 
and consumers must deal with the challenges of se-
curing digital communications for commerce and 
banking on a daily basis. The impact of cryptog-
raphy on society is likely to increase as computers 
grow more powerful, cryptographic techniques im-
prove, and digital technologies become ever more 
important.
—Maura Valentino, MSLIS
Bibliography
Esslinger, Bernhard, et al. The CrypTool Script: 
Cryptography, Mathematics, and More. 11th ed. 
Frankfurt: CrypTool, 2013. CrypTool Portal. Web. 2 
Mar. 2016.
Hoffstein, Jeffrey, Jill Pipher, and Joseph H. Silverman. 
An Introduction to Mathematical Cryptography. 2nd 
ed. New York: Springer, 2014. Print.
Katz, Jonathan, and Yehuda Lindell. Introduction to 
Modern Cryptography. 2nd ed. Boca Raton: CRC, 
2015. Print.
Menezes, Alfred J., Paul C. van Oorschot, and Scott 
A. Vanstone. Handbook of Applied Cryptography. 
Boca Raton: CRC, 1996. Print.
Neiderreiter, Harald, and Chaoping Xing. Algebraic 
Geometry in Coding Theory and Cryptography. 
Princeton: Princeton UP, 2009. Print.
Paar, 
Christof, 
and 
Jan 
Pelzi. 
Understanding 
Cryptography: A Textbook for Students and Practitioners. 
Heidelberg: Springer, 2010. Print.

98
Deadlock
Fields of Study
Information Systems; Operating Systems; System 
Analysis
Abstract
In computing, a deadlock is when two or more pro-
cesses are running simultaneously and each one 
needs access to a resource that the other is using. 
Neither process can terminate until the other one 
does, so they become stuck, or “locked.” Deadlock 
can occur in several different areas of computer sys-
tems, including operating systems, database transac-
tion processing, and software execution.
Prinicipal Terms

 circular wait: a situation in which two or more pro-
cesses are running and each one is waiting for a 
resource that is being used by another; one of the 
necessary conditions for deadlock.

 livelock: a situation in which two or more pro-
cesses constantly change their state in response to 
one another in such a way that neither can com-
plete.

 mutual exclusion: a rule present in some database 
systems that prevents a resource from being ac-
cessed by more than one operation at a time; one 
of the necessary conditions for deadlock.

 resource holding: a situation in which one process 
is holding at least one resource and is requesting 
further resources; one of the necessary conditions 
for deadlock.

 transactional database: a database management 
system that allows a transaction— a sequence of 
operations to achieve a single, self-contained 
task—to be undone, or “rolled back,” if it fails to 
complete properly.
Holding Resources
In order to understand how a deadlock can arise in 
a computer system, one must first understand how 
resources are used in a computer system. One way 
is to think of the system as a bank account. A data-
base keeps records of debits and credits to the ac-
count, and each new transaction is recorded in the 
database. If money is transferred from one account 
to another—for example, from a customer’s savings 
account to their checking account—the transfer 
must be recorded twice, as both a debit from the sav-
ings account and a credit to the checking account. 
Recording the debit and recording the credit are 
two separate actions. However, in a transactional da-
tabase, they represent a single transaction, because 
together they form a logical unit: money cannot 
be added to one account without being subtracted 
from another. In this case, the balances of both ac-
counts represent system resources. In order to pre-
vent another process, such as a withdrawal from the 
checking account, from interrupting the transaction 
before both the debit and the credit are recorded, 
the system enforces a rule known as mutual exclu-
sion. This rule means that no two processes can ac-
cess the same resources at once. Instead, the transfer 
process places a hold on the account balances until 
the transaction is complete. This practice is called re-
source holding, or sometimes “hold and wait.” Only 
after the transfer is recorded as both a debit and a 
credit will the transfer process release its hold on the 
account balance resources. At this point, the with-
drawal process is free to access the checking account 
balance.
Resource holding is necessary in such cases be-
cause if resources were not locked, it would not 
be clear how a system would resolve simultaneous 
events. In the example above, if the system had tried 
to calculate a withdrawal from the checking account 
D

99
Principles of Computer Science
Deadlock
before the credit to the account was recorded, the 
customer might have overdrawn their account. 
Resource holding is used to make systems more con-
sistent, since it defines rules that allow ambiguities to 
be resolved in a straightforward manner.
Standoff
Deadlock can be understood as an unintended conse-
quence of resource holding. While resource holding 
serves a legitimate purpose, inevitably situations arise 
in which two processes each hold a resource that the 
other needs in order to finish its work. Process A needs 
a resource locked by process B in order to finish, while 
process B needs a resource locked by process A to 
finish, and neither can relinquish its resources until 
it is completed. This situation is called a circular wait.
In 1971, computer scientist Edward G. Coffman 
described the four conditions, now known as the 
Coffman conditions, necessary for deadlock to occur:
Mutual exclusion,
Resource holding,
No preemption, and
Circular wait
Mutual exclusion, resource holding, and circular 
wait are described above. Preemption is when one 
process takes control of a resource being used by 
another process. Some designated processes have 
priority over other operations. As such, they can 
force other processes to stop and let them finish first. 
Preemption is typically reserved for processes that are 
fundamental to the operation of the system. If one of 
the conflicting processes can preempt another, dead-
lock will not occur. In fact, if any one of the Coffman 
conditions is not true, deadlock will not occur.
Responses to Deadlock
System designers have several options for dealing 
with deadlocks. One is to simply ignore the issue al-
together, though this is not practical in most circum-
stances. A second approach is to focus on detecting 
deadlocks and then have a variety of options avail-
able for resolving them. A third approach is to try to 
avoid deadlocks by understanding which system op-
erations tend to cause them. Finally, system designers 
may adopt a preventive approach in which they make 
changes to the system architecture specifically to 
avoid potential deadlocks.
Usually some type of outside intervention is re-
quired to resolve a deadlock situation. To accom-
plish this, many systems will include mechanisms 
that function to detect and resolve deadlock. 
Ironically, in trying to resolve deadlock, these sys-
tems may produce a similar situation known as 
livelock. Like a deadlock, a livelock results when 
two or more concurrent processes are unable to 
complete. Unlike a deadlock, this happens not 
Held up by
Held up by
Requires
Requires
Process
2
Process
1
Resource
A
Resource
B
This is a diagram of a circular wait deadlock. Process 1 requires resource A to complete, resource A is held up by process 2, process 2 cannot 
complete without resource B, and resource B is held up by process 1. EBSCO illustration.

100
Debugging
Principles of Computer Science
because the processes are at a standstill but because 
they are continually responding to one another. 
The situation can be compared to two people trav-
eling in opposite directions down a hallway. To 
avoid bumping into each other, one person moves 
to their left, while the other moves to the right. In 
response to the other’s actions, the first person then 
moves to their right, but at the same time the other 
person moves to their left. In real life, this situation 
is eventually resolved. In a computer system, it can 
continue on an infinite loop until one or both pro-
cesses are forced to end. If a deadlock detection al-
gorithm can resolve a deadlock and the previously 
stuck processes begin to function again, they may 
produce another deadlock. This will trigger the 
detection algorithm again, and so on, creating a 
livelock. One way to avoid this would be to have the 
detection algorithm release one of the deadlocked 
processes and either abort the other or keep it on 
hold until it is completed.
—Scott Zimmer, JD
Bibliography
Connolly, Thomas M., and Carolyn E. Begg. 
Database Systems: A Practical Approach to Design, 
Implementation, and Management. 6th ed. Boston: 
Pearson, 2015. Print.
Harth, Andreas, Katja Hose, and Ralf Schenkel, eds. 
Linked Data Management. Boca Raton: CRC, 2014. 
Print.
Hoffer, Jeffrey A., V. Ramesh, and Heikki Topi. 
Modern Database Management. 12th ed. Boston: 
Pearson, 2016. Print.
Kshemkalyani, Ajay D., and Mukesh Singhal. 
Distributed Computing: Principles, Algorithms, and 
Systems. New York: Cambridge UP, 2008. Print.
Rahimi, Saeed K., and Frank S. Haug. Distributed 
Database Management Systems: A Practical Approach. 
Hoboken: Wiley, 2010. Print.
Wills, Craig E. “Process Synchronization and 
Interprocess 
Communication.” 
Computing 
Handbook: Computer Science and Software Engineering. 
Ed. Teofilo Gonzalez and Jorge Díaz-Herrera. 3rd 
ed. Boca Raton: CRC, 2014. 52-1–21. Print.
Debugging
Fields of Study
Computer Science; Software Engineering
Abstract
Debugging is the process of identifying and ad-
dressing errors, known as “bugs,” in computer sys-
tems. It is an essential step in the development of all 
kinds of programs, from consumer programs such 
as web browsers and video games to the complex 
systems used in transportation and infrastructure. 
Debugging can be carried out through a number of 
methods depending on the nature of the computer 
system in question.
Prinicipal Terms

 delta debugging: an automated method of debug-
ging intended to identify a bug’s root cause while 
eliminating irrelevant information.

 in-circuit emulator: a device that enables the de-
bugging of a computer system embedded within a 
larger system.

 integration testing: a process in which multiple 
units are tested individually and when working in 
concert.

 memory dumps: computer memory records from 
when a particular program crashed, used to pin-
point and address the bug that caused the crash.

 software patches: updates to software that correct 
bugs or make other improvements.
Understanding Debugging
Debugging is the process of testing software or 
other computer systems, noting any errors that 
occur, and finding the cause of those errors. Errors, 
or “bugs,” in a computer program can seriously af-
fect the program’s operations or even prevent it 
from functioning altogether. The ultimate goal of 
debugging is to get rid of the bugs that have been 

101
Principles of Computer Science
Debugging
identified. This should ensure the smooth and 
error-free operation of the computer program or 
system.
Computer programs consist of long strings of 
specialized code that tell the computer what to do 
and how to do it. Computer code must use specific 
vocabulary and structures in order to function prop-
erly. As such code is written by human programmers, 
there is always the possibility of human error, which 
is the cause of many common bugs. Perhaps the most 
common bugs are syntax errors. These are the re-
sult of small mistakes, such as typos, in a program’s 
code. In some cases, a bug may occur because the 
programmer neglected to include a key element in 
the code or structured it incorrectly. For example, 
the code could include a command instructing the 
computer to begin a specific process but lack the cor-
responding command to end it.
Bugs fall into one of several categories, based on 
when and how they affect the program. Compilation 
errors prevent the program from running. Run-time 
errors, meanwhile, occur as the program is run-
ning. Logic errors, in which flaws in the program’s 
logic produce unintended results, are a particularly 
common form of bug. Such errors come about 
when a program’s code is syntactically correct but 
does not make logical sense. For instance, a string of 
code with flawed logic may cause the program to be-
come caught in an unintended loop. This can cause 
it to become completely unresponsive, or freeze. 
In other cases, a logic error might result when a 
­program’s code instructs the computer to divide a 
numerical value by zero, a mathematically impos-
sible task.
Why Debug?
Bugs may interfere with a program’s ability to per-
form its core functions or even to run. Not all bugs 
are related to a program’s core functions, and some 
programs may be usable despite the errors they con-
tain. However, ease of use is an important factor that 
many people consider when deciding which program 
to use. It is therefore in the best interest of software 
creators to ensure that their programs are as free of 
errors as possible. In addition to testing a program 
or other computer system in house prior to releasing 
them to the general public, many software compa-
nies collect reports of bugs from users following 
its release. This is often done through transfers of 
collected data commonly referred to as memory 
dumps. They can then address such errors through 
updates known as software patches.
While bugs are an inconvenience in consumer 
computer programs, in more specialized computer 
systems, they can have far more serious conse-
quences. In areas such as transportation, infrastruc-
ture, and finance, errors in syntax and logic can 
place lives and livelihoods at risk. Perhaps the most 
prominent example of such a bug was the so-called 
Y2K bug. This bug was projected to affect numerous 
computer systems beginning on January 1, 2000. 
The problem would have resulted from existing 
practices related to the way dates were written in 
computer programs. However, it was largely averted 
through the work of programmers who updated 
the affected programs to prevent that issue. As the 
example of the far-reaching Y2K bug shows, the 
world’s growing reliance on computers in all areas 
of society has made thorough debugging even more 
important.
Identifying and Addressing Bugs
The means of debugging vary based on the nature 
of the computer program or system in question. 
However, in most cases bugs may be identified and 
addressed through the same general process. When a 
bug first appears, the programmer or tester must first 
attempt to reproduce the bug in order to identify the 
results of the error and the conditions under which 
they occur. Next, the programmer must attempt 
to determine which part of the program’s code is 
causing the error to occur. As programs can be quite 
complex, the programmer must simplify this process 
by eliminating as much irrelevant data as possible. 
Once the faulty segment of code has been found, the 
programmer must identify and correct the specific 
problem that is causing the bug. If the cause is a typo 
or a syntax error, the programmer may simply need 
to make a small correction. If there is a logic error, 
the programmer may need to rewrite a portion of the 
code so that the program operates logically.
Debugging in Practice
Programmers use a wide range of tools to debug 
programs. As programs are frequently complex and 
lengthy, automating portions of the debugging pro-
cess is often essential. Automated debugging pro-
grams, or “debuggers,” search through code line by 

102
Demon dialing/war dialing
Principles of Computer Science
line for syntax errors or faulty logic that could cause 
bugs. A technique known as delta debugging pro-
vides an automated means of filtering out irrelevant 
information when the programmer is looking for the 
root cause of a bug.
Different types of programs or systems often re-
quire different debugging tools. An in-circuit emu-
lator is used when the computer system being tested 
is an embedded system (that is, one located within 
a larger system) and cannot otherwise be accessed. 
A form of debugging known as integration testing 
is often used when a program consists of numerous 
components. After each component is tested and 
debugged on its own, they are linked together and 
tested as a unit. This ensures that the different com-
ponents function correctly when working together.
—Joy Crelin
Bibliography
Foote, Steven. Learning to Program. Upper Saddle 
River: Pearson, 2015. Print.
McCauley, Renée, et al. “Debugging: A Review of 
the Literature from an Educational Perspective.” 
Computer Science Education 18.2 (2008): 67–92. 
Print.
Myers, Glenford J., Tom Badgett, and Corey Sandler. 
The Art of Software Testing. Hoboken: Wiley, 2012. 
Print.
St. Germain, H. James de. “Debugging Programs.” 
University of Utah. U of Utah, n.d. Web. 31 Jan. 2016.
“What Went Wrong? Finding and Fixing Errors 
through Debugging.” Microsoft Developer Network 
Library. Microsoft, 2016. Web. 31 Jan. 2016.
Zeller, Andreas. Why Programs Fail: A Guide to Systematic 
Debugging. Burlington: Kaufmann, 2009. Print.
Demon dialing/war dialing
Fields of Study
Security
Abstract
War dialing is the practice of autodialing a large 
range of phone numbers to find computer modems. 
It involves using a software program to call all of the 
phone numbers within an area code to see which 
ones are set up to accept incoming connections. 
Demon dialing is a synonym for war dialing, though 
it has also been used to describe making repeated 
calls to a single modem in a brute-force attempt to 
guess its password.
Prinicipal Terms

 cracker: a criminal hacker; one who finds and ex-
ploits weak points in a computer’s security system 
to gain unauthorized access for malicious pur-
poses.

 hacking: the use of technical skill to gain unau-
thorized access to a computer system; also, any 
kind of advanced tinkering with computers to in-
crease their utility.

 port scanning: the use of software to probe a com-
puter server to see if any of its communication 
ports have been left open or vulnerable to an un-
authorized connection that could be used to gain 
control of the computer.

 voice over IP (VoIP): short for “voice over Internet 
Protocol”; a telephone service designed for trans-
mission over broadband Internet connections 
rather than telephone wires.

 wardriving: driving around with a device such as a 
laptop that can scan for wireless networks that may 
be vulnerable to hacking.
The Early Internet
Prior to the mid-1990s, the vast majority of people 
wishing to go online did so using dial-up modems. 
A dial-up modem connected a computer to the tele-
phone network via a telephone wall jack. It accessed 
the Internet by making a phone call to another com-
puter or a server and transmitting data over the tele-
phone line. Most early Internet connections were 
made to limited, self-contained services, such as 
local area networks (LANs) and bulletin-board sys-
tems (BBSs) hosted on computer systems running 
terminal programs. In order for a connection to be 

103
Principles of Computer Science
Demon dialing/war dialing
established, the computer user would have to have 
the phone number of the host computer, and the 
host computer would have to be available. If the host 
computer was not online or had too many connec-
tions already, it could not accept another connection. 
Eventually the first Internet service providers (ISPs) 
were established, allowing many computers to access 
the public Internet simultaneously.
As the number of computers with modems in-
creased, computer hackers began to try to discover 
and connect to them. For some, hacking became a 
hobby of sorts. It presented a challenge that was ex-
citing and intellectually stimulating. For crackers, or 
criminal hackers, it was a way to steal data or commit 
other malicious acts. Regardless of the hackers’ mo-
tives, they all needed some way to find the phone 
numbers of computers with modems.
One such way was war dialing. Hackers wrote soft-
ware that would use a computer’s modem to dial 
every phone number in a given area code. The soft-
ware ran through each number, dialing and then 
hanging up after two rings. Most modems were set up 
to pick up after one ring, so if a number rang twice 
it most likely did not have a modem. The war-dialing 
software recorded which numbers had modems so 
that hackers could try to connect to them later.
“Demon dialing” originally meant making re-
peated calls to a single modem in a brute-force at-
tempt to guess its password. The practice was named 
for the Demon Dialer, a telephone dialer once sold 
by Zoom Telephonics. This device could automati-
cally redial a busy phone number until the call went 
through. Over time, demon dialing became a syn-
onym for war dialing.
Modern Takes on War Dialing
There have been many changes in technology that 
make the old methods of war dialing obsolete. The 
number of computers still connecting to the Internet 
via dial-up modems decreased sharply after broad-
band Internet access and wireless networking be-
came mainstream in the mid-2000s. However, war 
dialing itself is still practiced; it simply requires dif-
ferent techniques. For example, the open-source soft-
ware WarVOX is a war-dialing tool that connects via 
voice over IP (VoIP) systems instead of landline tele-
phones. It uses signal-processing techniques to probe 
and analyze telephone systems. Some information 
technology (IT) security personnel use VoIP-based 
war dialers to find unauthorized modems and faxes 
on their organization’s computer networks.
Another modern technique similar to war dialing 
is called port scanning. Computers connect to the 
Internet using different ports, which are like vir-
tual connection points. Some ports are traditionally 
used for certain connections, such as printers or web 
browsing. Other ports are left open for whichever 
application needs to create a connection. When a 
computer has been secured, ports not in use are kept 
closed to prevent unauthorized connections. Port 
scanners bombard computers with connection at-
tempts on many different ports at once, then report 
vulnerable port numbers back to the hacker. To pro-
tect against port scanning, many companies now use 
intrusion-detection systems that can identify when 
a port scan is underway. This security measure has 
in turn motivated hackers to develop port-scanning 
methods that can gather information without openly 
trying to connect to each port.
Wi-Fi War Dialing
Some hackers target wireless networks instead of wired 
ones. One technique for doing so is wardriving, in 
which hackers drive around a neighborhood with a 
laptop running Wi-Fi scanning software. The software 
identifies wireless networks as it passes through them 
and collects information about the type of security each 
wireless access point is using. Hackers can then sort 
through this information to find vulnerable networks. 
A hacker may exploit the network to gain free wireless 
Internet access or to disguise their online identity.
—Scott Zimmer, JD
Bibliography
Coleman, E Gabriella. Coding Freedom: The Ethics and 
Aesthetics of Hacking. Princeton: Princeton UP, 
2013. Print.
Haerens, Margaret, and Lynn M. Zott, eds. Hacking 
and Hackers. Detroit: Greenhaven, 2014. Print.
Kizza, Joseph Migga. Guide to Computer Network 
Security. 3rd ed. London: Springer, 2015. Print.
Morselli, Carlo, ed. Crime and Networks. New York: 
Routledge, 2014. Print.
Naraine, Ryan. “Metasploit’s H. D. Moore Releases 
‘War Dialing’ Tools.” ZDNet. CBS Interactive, 6 
Mar. 2009. Web. 15 Mar. 2016.

104
Device drivers
Principles of Computer Science
Netzley, Patricia D. How Serious a Problem Is Computer 
Hacking? San Diego: ReferencePoint, 2014. Print.
Shakarian, Paulo, Jana Shakarian, and Andrew Ruef. 
Introduction to Cyber-Warfare: A Multidisciplinary 
Approach. Waltham: Syngress, 2013. Print.
Device drivers
Fields of Study
Computer Engineering; Software Engineering
Abstract
Device drivers are software interfaces that allow a 
computer’s central processing unit (CPU) to commu-
nicate with peripherals such as disk drives, printers, 
and scanners. Without device drivers, the computer’s 
operating system (OS) would have to come prein-
stalled with all of the information about all of the de-
vices it could ever need to communicate with. OSs 
contain some device drivers, but these can also be 
installed when new devices are added to a computer.
Prinicipal Terms

 device manager: an application that allows users 
of a computer to manipulate the device drivers 
installed on the computer, as well as adding and 
removing drivers.

 input/output instructions: instructions used by 
the central processing unit (CPU) of a computer 
when information is transferred between the CPU 
and a device such as a hard disk.

 interface: the function performed by the device 
driver, which mediates between the hardware of 
the peripheral and the hardware of the computer.

 peripheral: a device that is connected to a com-
puter and used by the computer but is not part of 
the computer, such as a printer.

 virtual device driver: a type of device driver used 
by the Windows operating system that handles 
communications between emulated hardware and 
other devices.
How Device Drivers Work
The main strength of device drivers is that they en-
able programmers to write software that will run on 
a computer regardless of the type of devices that are 
connected to that computer. Using device drivers al-
lows the program to simply command the computer to 
save data to a file on the hard drive. It needs no specific 
information about what type of hard drive is installed 
in the computer or connections the hard drive has to 
other hardware in the computer. The device driver 
acts as an interface between computer components.
When a program needs to send commands to a 
peripheral connected to the computer, the program 
communicates with the device driver. The device 
driver receives the information about the action that 
the device is being asked to perform. It translates this 
information into a format that can be input into the 
device. The device then performs the task or tasks re-
quested. When it finishes, it may generate output that 
is communicated to the driver, either as a message 
or as a simple indication that the task has been com-
pleted. The driver then translates this information 
into a form that the original program can understand. 
The device driver acts as a kind of translator between 
the computer and its peripherals, conveying input/
output instructions between the two. Thus, the com-
puter program does not need to include all of the low-
level commands needed to make the device function. 
The program only needs to be able to tell the device 
driver what it wants the device to do. The device driver 
takes care of translating this into concrete steps.
How Device Drivers Are Made
Writing device drivers is a highly technical under-
taking. It is made more challenging by the fact that 
device drivers can be unforgiving when a mistake is 
made in their creation. This is because higher-level 
applications do not often have unlimited access to 
all of the computer’s functionality. Issuing the wrong 
command with unrestricted privileges can cause se-
rious damage to the computer’s operating system 
(OS) and, in some cases, to the hardware. This is a 
real possibility with device drivers, which usually need 
to have unrestricted access to the computer.

105
Principles of Computer Science
Device drivers
Because writing a device driver requires a lot of 
specialized information, most device drivers are 
made by software engineers who specialize in driver 
development and work for hardware manufacturers. 
Usually the device manufacturer has the most infor-
mation about the device and what it needs in order 
to function properly. The exception to this trend is 
the impressive amount of driver development accom-
plished by the open-source movement. Programmers 
all over the world have volunteered their own time 
and talent to write drivers for the Linux OS.
Often development is separated into logical and 
physical device driver development. Logical device 
driver development tends to be done by the creator 
of the OS that the computer will use. Physical device 
driver development, meanwhile, is handled by the 
device manufacturer. This division of labor makes 
sense, but it does require coordination and a will-
ingness to share standards and practices among the 
various parties.
Virtual device drivers
Virtual device drivers are a variation on traditional 
device drivers. They are used when a computer needs 
to emulate a piece of hardware. This often occurs 
when an OS runs a program that was created for a 
different OS by emulating that operating environ-
ment. One example would be a Windows OS run-
ning a DOS program. If the DOS program needed 
to interface with an attached printer, the computer 
would use a virtual device driver.
device managers
Most OSs now include device managers that make 
it easier for the user to manage device drivers. They 
allow the user to diagnose problems with devices, 
troubleshoot issues, and update or install drivers. 
Using the graphical interface of a device manager 
is less intimidating than typing in text commands to 
perform driver-related tasks.
—Scott Zimmer, JD
bibliography
Corbet, Jonathan, Alessandro Rubini, and Greg 
Kroah-Hartman. Linux Device Drivers. 3rd ed. 
Cambridge: O’Reilly, 2005. Print.
McFedries, Paul. Fixing Your Computer: Absolute 
Beginner’s Guide. Indianapolis: Que, 2014. Print.
Device Controller / Bus Driver
Device 
Driver
Device 
Driver
Bus
Driver
...
...
Device 
Driver
Bus
Driver
Monitor
Mouse
Keyboard
Device
Controller
...
...
Device 
Driver
Bus
Driver
DVD 
Reader
CPU
Micro-controller
Device Controller
Device 
Controller
Device 
Controller
Each device connected to a CPU is controlled by a device driver, software that controls, manages, and monitors a specifi c device (e.g., 
keyboard, mouse, monitor, DVD reader). Device drivers may also drive other software that drives a device (e.g., system management 
bus, universal serial bus controller). EBSCO illustration.

106
Digital citizenship
Principles of Computer Science
Mueller, Scott. Upgrading and Repairing PCs. 22nd ed. 
Indianapolis: Que, 2015. Print.
Noergaard, Tammy. Embedded Systems Architecture: A 
Comprehensive Guide for Engineers and Programmers. 
2nd ed. Boston: Elsevier, 2012. Print.
Orwick, Penny, and Guy Smith. Developing Drivers with 
the Windows Driver Foundation. Redmond: Microsoft 
P, 2007. Print.
“What Is a Driver?” Microsoft Developer Network. 
Microsoft, n.d. Web. 10 Mar. 2016.
Digital citizenship
Fields of Study
Information Systems; Digital Media; Privacy
Abstract
Digital citizenship can be defined as the norms of ap-
propriate, legal, and ethical behavior with regard to 
the use of information technology in a person’s civic 
and social life. Digital citizenship is a unique phe-
nomenon of the digital age, reflecting the growing 
importance of digital literacy, digital commerce, and 
information technology in global culture.
Prinicipal Terms

 butterfly effect: an effect in which small changes 
in a system’s initial conditions lead to major, unex-
pected changes as the system develops.

 digital commerce: the purchase and sale of goods 
and services via online vendors or information 
technology systems.
 digital literacy: familiarity with the skills, behaviors, 
and language specific to using digital devices to ac-
cess, create, and share content through the Internet.

 digital native: an individual born during the dig-
ital age or raised using digital technology and 
communication.

 IRL relationships: relationships that occur “in real 
life,” meaning that the relationships are developed 
or sustained outside of digital communication.

 piracy: in the digital context, unauthorized repro-
duction or use of copyrighted media in digital form.
A New Form of Citizenship
Digital citizenship can be defined as the norms and 
rules of behavior for persons using digital technology 
in commerce, political activism, and social commu-
nication. A person’s digital citizenship begins when 
they engage with the digital domain, for instance, by 
beginning to use a smartphone or e-mail. However, 
digital citizenship exists on a spectrum based on an 
individual’s level of digital literacy. This can be de-
fined as their familiarity with the skills, jargon, and 
behaviors commonly used to communicate and con-
duct commerce with digital tools.
An Evolving Paradigm
Educational theorist Marc Prensky suggested that the 
modern human population can be divided into two 
groups. Digital natives were raised in the presence of 
digital technology. They learned how to use it in child-
hood. They absorbed the basics of digital citizenship 
during their early development. Digital immigrants 
were born before the digital age or have limited ac-
cess to technology. They adapt to digital technology 
and communication later in life. Given the growing 
importance of digital technology, educators and so-
cial scientists believe that teaching children and adults 
to use digital technology safely and ethically is among 
the most important goals facing modern society.
Digital communication enables people to have 
relationships online or on mobile devices. The de-
gree to which these digital relationships affect IRL 
relationships, or those that occur “in real life,” is an 
important facet of digital citizenship. For instance, 
research suggests that people who spend more time 
communicating through smartphones or who feel 
they need constant access to digital media have more 
difficulty forming and maintaining IRL relation-
ships. Good digital citizenship can help people use 
digital technology in positive ways that do not detract 
from their IRL relationships and well-being.
Digital technology has had a powerful, democ-
ratizing force on culture. Social media, for instance, 
has enabled small, local social movements to have na-
tional and even international impact. Even a simple 
tweet or viral video can quickly spread to millions of 
social media users. Small-scale behaviors can thus have 
larger, often unexpected consequences, both good 
and bad. This is sometimes called the butterfly effect. 

107
Principles of Computer Science
Digital citizenship
The potential impact of even a single user in the dig-
ital domain highlights the importance of learning and 
teaching effective digital behavior and ethics.
Ethics of Digital Citizenship
In Digital Citizenship in Schools, educational theorist 
Mike Ribble outlines nine core themes that charac-
terize digital citizenship. These themes are: digital ac-
cess, digital commerce, digital communication, digital 
literacy, digital etiquette, digital law, digital rights and 
responsibilities, digital health and wellness, and digital 
security. They address appropriate ways of interacting 
with the technology, information, government, com-
panies, and other citizens in the digital world. Ribble 
writes that well-adjusted digital citizens help others be-
come digitally literate. They strive to make the digital 
domain harmonious and culturally beneficial.
He also argues that digital citizens are responsible 
for learning about and following the laws and ethical 
implications of all digital activities. Piracy is the unau-
thorized digital reproduction of copyrighted media. 
It violates laws that protect creative property. It has 
Student learning
and
academic performance
Student environment
and
behavior
Digital
communication
Digital
literacy
Digital
access
Digital commerce
Digital law
Digital health
and wellness
Digital etiquette
Digital security
Digital rights
and responsibilities
Student life
outside of the
school environment
Good digital citizens know and understand the nine themes of digital citizenship. These themes address appropriate ways of interacting with 
the technology, information, government, companies, and other citizens that comprise the digital world. EBSCO illustration.

108
Digital forensics
Principles of Computer Science
become one of the most controversial issues of the 
twenty-first century. Other common digital crimes 
and forms of misconduct include:
Plagiarizing content from digital sources,
Hacking (gaining unauthorized access to com-
puter networks or systems),
Sending unwanted communications and spam 
messages, and
Creating and spreading destructive viruses, 
worms, and malware.
Such behaviors violate others’ rights and so are con-
sidered unethical, illegal, or both.
Digital Security and Responsibility
The rights and responsibilities of digital citizenship 
differ by political environment. In the United States, in-
dividuals have the right to free speech and expression. 
They also have a limited right to digital privacy. The own-
ership of digital data is an evolving subject in US law.
One responsibility of digital citizenship is to learn 
about potential dangers, both social and physical, and 
how to avoid them. These include identity theft, cy-
berstalking, and cyberbullying. Strong digital security 
can help protect one’s identity, data, equipment, and 
creative property. Surge protectors, antivirus software, 
and data backup systems are some of the tools digital 
citizens use to enhance their digital security. Though 
the digital world is a complex, rapidly evolving realm, 
advocates argue that the rules of digital citizenship 
can be reduced to a basic concept: respect oneself 
and others when engaging in digital life.
—Micah L. Issitt
Bibliography
“The Digital Millennium Copyright Act of 1998.” 
Copyright. US Copyright Office, 28 Oct 1998. Web. 
23 Jan 2016.
McNeill, Erin. “Even ‘Digital Natives’ Need Digital 
Training.” Education Week. Editorial Projects in 
Education, 20 Oct 2015. Web. 26 Jan. 2016.
Ribble, Mike. Digital Citizenship in Schools: Nine 
Elements All Students Should Know. Eugene: Intl. 
Soc. for Technology in Education, 2015. Print.
Saltman, Dave. “Tech Talk: Turning Digital Natives 
into Digital Citizens.” Harvard Education Letter 
27.5 (2011): n. pag. Harvard Graduate School of 
Education. Web. 27 Jan. 2016.
Wells, Chris. The Civic Organization and the Digital 
Citizen: Communicating Engagement in a Networked 
Age. New York: Oxford UP, 2015. Print.
Digital forensics
Fields of Study
Information Technology; System Analysis; Privacy
Abstract
Digital forensics is a branch of science that studies 
stored digital data. The field emerged in the 1990s 
but did not develop national standards until the 
2000s. Digital forensics techniques are changing rap-
idly due to the advances in digital technology.
Prinicipal Terms

 cybercrime: crime that involves targeting a com-
puter or using a computer or computer network 
to commit a crime, such as computer hacking, 
digital piracy, and the use of malware or spyware.

 Electronic Communications Privacy Act: a 1986 
law that extended restrictions on wiretapping to 
cover the retrieval or interception of information 
transmitted electronically between computers or 
through computer networks.

 logical copy: a copy of a hard drive or disk that cap-
tures active data and files in a different configu-
ration from the original, usually excluding free 
space and artifacts such as file remnants; contrasts 
with a physical copy, which is an exact copy with 
the same size and configuration as the original.
 metadata: data that contains information about other 
data, such as author information, organizational in-
formation, or how and when the data was created.

 Scientific Working Group on Digital Evidence 
(SWGDE): an American association of various aca-
demic and professional organizations interested 

109
Principles of Computer Science
Digital forensics
in the development of digital forensics systems, 
guidelines, techniques, and standards.
An Evolving Science
Digital forensics is the science of recovering and 
studying digital data, typically in the course of crim-
inal investigations. Digital forensic science is used to 
investigate cybercrimes. These crimes target or in-
volve the use of computer systems. Examples include 
identity theft, digital piracy, hacking, data theft, 
and cyberattacks. The Scientiﬁ c Working Group on 
Digital Evidence (SWGDE), formed in 1998, develops 
industry guidelines, techniques, and standards.
Digital Forensics Policy
Digital forensics emerged in the mid-1980s in re-
sponse to the growing importance of digital data 
in criminal investigations. The ﬁ rst cybercrimes oc-
curred in the early 1970s. This era saw the emergence 
of “hacking,” or gaining unauthorized access to com-
puter systems. Some of the ﬁ rst documented uses of 
digital forensics data were in hacking investigations.
Prior to the Electronic Communications Privacy Act 
(ECPA) of 1986, digital data or communications were 
not protected by law and could be collected or inter-
cepted by law enforcement. The ECPA was amended 
several times in the 1990s and 2000s to address the 
growing importance of digital data for private com-
munication. In 2014, the Supreme Court ruled that 
police must obtain a warrant before searching the cell 
phone of a suspect arrested for a crime.
Digital Forensics Techniques
Once forensic investigators have access to equipment 
that has been seized or otherwise legally obtained, 
they can begin forensic imaging. This process in-
volves making an unaltered copy, or forensic image, 
of the device’s hard drive. A forensic image records 
the drive’s structures, all of its contents, and meta-
data about the original ﬁ les.
A forensic image is also known as a “physical copy.” 
There are two main methods of copying computer data, 
physical copying and logical copying. A physical copy 
duplicates all of the data on a speciﬁ c drive, including 
empty, deleted, or fragmented data, and stores it in 
its original conﬁ guration. A logical copy, by contrast, 
copies active data but ignores deleted ﬁ les, fragments, 
and empty space. This makes the data easier to 
read and analyze. However, it may not provide a 
complete picture of the relevant data.
After imaging, forensics examiners analyze the 
imaged data. They may use specialized tools to 
recover deleted ﬁ les using fragments or backup 
data, which is stored on many digital devices to 
prevent accidental data loss. Automated pro-
grams can be used to search and sort through 
imaged data to ﬁ nd useful information. (Because 
searching and sorting are crucial to the forensic 
process, digital forensics organizations invest in 
research into better search and sort algorithms). 
Information of interest to examiners may include 
e-mails, text messages, chat records, ﬁ nancial ﬁ les, 
and various types of computer code. The tools 
and techniques used for analysis depend largely 
on the crime. These specialists may also be tasked 
with interpreting any data collected during an in-
vestigation. For instance, they may be called on to 
explain their ﬁ ndings to police or during a trial.
Challenges for the Future
Digital forensics is an emerging ﬁ eld that 
lags behind fast-changing digital technology. 
For instance, cloud computing is a fairly new 
COMPUTER 
NETWORK 
FORENSICS
COMPUTER 
FORENSICS
FORENSIC 
DATA 
ANALYSIS
MOBILE 
FORENSICS
DATABASE 
FORENSICS
SOCIAL 
NETWORK 
FORENSICS
COMPUTER 
NETWORK 
FORENSICS
COMPUTER 
FORENSICS
FORENSIC 
DATA 
ANALYSIS
MOBILE 
FORENSICS
DATABASE 
FORENSICS
SOCIAL 
NETWORK 
FORENSICS
Digital
Forensics
Digital forensics encompasses computer forensics, mobile forensics, 
computer network forensics, social networking forensics, database foren-
sics, and forensic data analysis or the forensic analysis of large-scale data 
EBSCO illustration.

110
Digital signal processors
Principles of Computer Science
technology in which data storage and processing is dis-
tributed across multiple computers or servers. In 2014, 
the National Institute of Standards and Technology 
identified sixty-five challenges that must be addressed 
regarding cloud computing. These challenges include 
both technical problems and legal issues.
The SWGDE works to create tools and standards 
that will allow investigators to effectively retrieve 
and analyze data while keeping pace with changing 
technology. It must also work with legal rights orga-
nizations to ensure that investigations remain within 
boundaries set to protect personal rights and privacy. 
Each forensic investigation may involve accessing 
personal communications and data that might be 
protected under laws that guarantee free speech and 
expression or prohibit unlawful search and seizure. 
The SWGDE and law enforcement agencies are de-
bating changes to existing privacy and surveillance 
laws to address these issues while enabling digital fo-
rensic science to continue developing.
—Micah L. Issitt
Bibliography
“Digital Evidence and Forensics.” National Institute of 
Justice. Office of Justice Programs, 28 Oct. 2015. 
Web. 12 Feb. 2016.
Gogolin, Greg. Digital Forensics Explained. Boca Raton: 
CRC, 2013. Print.
Holt, Thomas J., Adam M. Bossler, and Kathryn C. 
Seigfried-Spellar. Cybercrime and Digital Forensics: 
An Introduction. New York: Routledge, 2015. Print.
Pollitt, Mark. “A History of Digital Forensics.” 
Advances in Digital Forensics VI. Ed. Kam-Pui Chow 
and Sujeet Shenoi. Berlin: Springer, 2010. 3–15. 
Print.
Sammons, John. The Basics of Digital Forensics: 
The Primer for Getting Started in Digital Forensics. 
Waltham: Syngress, 2012. Print.
Shinder, Deb. “So You Want to Be a Computer 
Forensics Expert.” TechRepublic. CBS Interactive, 
27 Dec. 2010. Web. 2 Feb. 2016.
Digital signal processors
Fields of Study
Computer 
Science; 
Information 
Technology; 
Network Design
Abstract
Digital signal processors (DSPs) are microprocessors 
designed for a special function. DSPs are used with 
analog signals to continuously monitor their output, 
often performing additional functions such as fil-
tering or measuring the signal. One of the strengths 
of DSPs is that they can process more than one in-
struction or piece of data at a time.
Prinicipal Terms

 fixed-point arithmetic: a calculation involving 
numbers that have a defined number of digits be-
fore and after the decimal point.

 floating-point arithmetic: a calculation involving 
numbers that have a decimal point that can be 
placed anywhere through the use of exponents, as 
is done in scientific notation.

 Harvard architecture: a computer design that has 
physically distinct storage locations and signal 
routes for data and for instructions.

 multiplier-accumulator: a piece of computer hard-
ware that performs the mathematical operation of 
multiplying two numbers and then adding the re-
sult to an accumulator.

 pipelined architecture: a computer design where 
different processing elements are connected in a 
series, with the output of one operation being the 
input of the next.

 semiconductor intellectual property (SIP) block: a 
quantity of microchip layout design that is owned 
by a person or group; also known as an “IP core.”
Digital Signal Processing
Digital signal processors (DSPs) are microproces-
sors designed for a special function. Semiconductor 
intellectual property (SIP) blocks designed for use 

111
Principles of Computer Science
Digital signal processors
as DSPs generally have to work in a very low latency 
environment. They are constantly processing streams 
of video or audio. They also need to keep power con-
sumption to a minimum, particularly in the case of 
mobile devices, which rely heavily on DSPs. To make 
this possible, DSPs are designed to work efficiently 
on both fixed-point arithmetic and the more com-
putationally intensive floating-point arithmetic. The 
latter, however, is not needed in most DSP applica-
tions. DSPs tend to use chip architectures that allow 
them to fetch multiple instructions at once, such as 
the Harvard architecture. Many DSPs are required 
to accept analog data as input, convert this to digital 
data, perform some operation, and then convert the 
digital signals back to analog for output. This gives 
DSPs a pipelined architecture. They use a multistep 
process in which the output of one step is the input 
needed by the next step.
An example of the type of work performed by a 
DSP can be seen in a multiplier-accumulator. This is 
a piece of hardware that performs a two-step opera-
tion. First, it receives two values as inputs and mul-
tiplies one value by the other. Next, the multiplier-
accumulator takes the result of the first step and 
adds it to the value stored in the accumulator. At the 
end, the accumulator’s value can be passed along as 
output. Because DSPs rely heavily on multiplier-ac-
cumulator operations, these are part of the instruc-
tion set hardwired into such chips. DSPs must be 
able to carry out these operations quickly in order 
to keep up with the continuous stream of data that 
they receive.
The processing performed by DSPs can some-
times seem mysterious. However, in reality it often 
amounts to the performance of fairly straightfor-
ward mathematical operations on each value in the 
stream of data. Each piece of analog input is con-
verted to a digital value. This digital value is then 
added to, multiplied by, subtracted from, or divided 
by another value. The result is a modified data 
stream that can then be passed to another process or 
generated as output.
Applications of Digital Signal 
Processors
There are many applications in which DSPs have 
become an integral part of daily life. The basic pur-
pose of a DSP is to accept as input some form of 
analog information from the real world. This could 
include anything from an audible bird call to a 
live video-feed broadcast from the scene of a news 
event.
DSPs are also heavily relied upon in the field of 
medical imaging. Medical imaging uses ultrasound 
or other technologies to produce live imagery of 
what is occurring inside the human body. Ultrasound 
is often used to examine fetal developmental, from 
what position the fetus is in to how it is moving to how 
its heart is beating, and so on. DSPs receive the ul-
trasonic signals from the ultrasound equipment and 
convert it into digital data. This digital data can then 
be used to produce analog output in the form of a 
video display showing the fetus.
Digital signal processing is also critical to many 
military applications, particularly those that rely on 
the use of sonar or radar. As with ultrasound devices, 
radar and sonar send out analog signals in the form 
of energy waves. These waves bounce off features in 
the environment and back to the radar- or sonar-gen-
erating device. The device uses DSPs to receive this 
analog information and convert it into digital data. 
The data can then be analyzed and converted into 
graphical displays that humans can easily interpret. 
DSPs must be able to minimize delays in processing, 
because a submarine using sonar to navigate under-
water cannot afford to wait to find out whether ob-
stacles are in its path.
Analog 
INPUT 
Signal
Digital
INPUT 
Signal
Digital 
OUTPUT 
Signal
Analog 
OUTPUT 
Signal
A/D 
converter
Digital 
signal 
processor
D/A
converter
This is a block diagram for an analog-to-digital processing system. Digital signal processors are responsible for performing specified opera-
tions on digital signals. Signals that are initially analog must first be converted to digital signals in order for the programs in the digital 
processor to work correctly. The output signal may or may not have to be converted back to analog. EBSCO illustration.

112
Digital watermarking
Principles of Computer Science
Biometric Scanning
A type of digital signal processing that many 
people have encountered at one time or another 
in their lives is the fingerprint scanner used in 
many security situations to verify one’s identity. 
These scanners allow a person to place his or her 
finger on the scanner, and the scanner receives 
the analog input of the person’s fingerprint. This 
input is then converted to a digital format and 
compared to the digital data on file for the person 
to see whether they match. As biometric data be-
comes increasingly important for security applica-
tions, the importance of digital signal processing 
will likely grow.
—Scott Zimmer, JD
Bibliography
Binh, 
Le 
Nguyen. 
Digital 
Processing: 
Optical 
Transmission and Coherent Receiving Techniques. Boca 
Raton: CRC, 2013. Print.
Iniewski, Krzysztof. Embedded Systems: Hardware, Design, 
and Implementation. Hoboken: Wiley, 2013. Print.
Kuo, Sen M., Bob H. Lee, and Wenshun Tian. 
Real-Time Digital Signal Processing: Fundamentals, 
Implementations and Applications. 3rd ed. Hoboken: 
Wiley, 2013. Print.
Snoke, David W. Electronics: A Physical Approach. 
Boston: Addison, 2014. Print.
Sozan´ski , Krzysztof. Digital Signal Processing in Power 
Electronics Control Circuits. New York: Springer, 
2013. Print.
Tan, Li, and Jean Jiang. Digital Signal Processing: 
Fundamentals and Applications. 2nd ed. Boston: 
Academic, 2013. Print.
Digital watermarking
Fields of Study
Computer Science; Digital Media; Security
Abstract
Digital watermarking protects shared or distributed 
intellectual property by placing an additional signal 
within the file. This signal can be used to inform 
users of the copyright owner’s identity and to authen-
ticate the source of digital data. Digital watermarks 
may be visible or hidden.
Prinicipal Terms

 carrier signal: an electromagnetic frequency that 
has been modulated to carry analog or digital in-
formation.

 crippleware: software programs in which key 
features have been disabled and can only be 
activated after registration or with the use of a 
product key.

 multibit watermarking: a watermarking process 
that embeds multiple bits of data in the signal to 
be transmitted.

 noise-tolerant signal: a signal that can be easily dis-
tinguished from unwanted signal interruptions or 
fluctuations (i.e., noise).

 1-bit watermarking: a type of digital water-
mark that embeds one bit of binary data in the 
signal to be transmitted; also called “0-bit water-
marking.”

 reversible data hiding: techniques used to conceal 
data that allow the original data to be recovered in 
its exact form with no loss of quality.
Protecting Ownership and Security of 
Digital Data
Digital watermarking is a technique that embeds 
digital media files with a hidden digital code. It was 
first developed in the late twentieth century. These 
hidden codes can be used to record copyright data, 
track copying or alteration of a file, or prevent altera-
tion or unauthorized efforts to copy a copyrighted 
file. Digital watermarking is therefore commonly 
used for copyright-protected music, video, and soft-
ware downloads. Governments and banks also rely on 
it to ensure that sensitive documents and currency 
are protected from counterfeiting and fraud.

113
Principles of Computer Science
Digital watermarking
Watermark 
test: 
compares 
image to 
presumed 
watermark
overlay
General Image Watermarking Process
Image Watermark Testing to Identify Possible Forgery
Similar
Passes test
Fails test
Forgery
Different
Possibly forged image 
from network
Original image
Watermark
Jpeg compression
Final 
watermarked 
image
Authentic
Authentic
Watermark 
test: 
compares 
image to 
presumed 
watermark
overlay
Similar
Passes test
Fails test
Forgery
Forgery
Different
Possibly forged image 
from network
Authentic
c
p
w
c
p
w
c
e
co
s
om
im
:
mp
ima
pa
mag
pre
are
age
res
w
res
ge t
esu
wat
s 
e to
sum
ate
to 
ume
erm
ov
med
rm
ove
ed 
ma
verl
ark
erla
rk
lay
co
im
pr
w
o
test:
W
test:
Wateterm
te
ma
es
ark
st
rk 
:
k 
W
c
p
w
c
p
w
c
e
co
s
om
im
:
mp
ima
pa
mag
pre
are
age
res
w
res
ge t
esu
wat
s 
e to
sum
ate
to 
ume
erm
ov
med
rm
ove
ed 
ma
verl
ark
erla
rk
lay
co
im
pr
w
o
test:
W
test:
Wateterm
te
ma
es
ark
st
rk 
:
k 
W
+
Diagrams of the watermarking process and a process for testing watermarks. EBSCO illustration.

114
Digital watermarking
Principles of Computer Science
Basics of Watermarking
A paper watermark is an image embedded within 
another image or a piece of paper. It can be seen 
by shining light on the image. Watermarks are used 
on banknotes, passports, and other types of paper 
documents to verify their authenticity. Similarly, 
digital watermarking involves embedding data 
within a digital signal in order to verify the authen-
ticity of the signal or identify its owners. Digital wa-
termarking was invented in the late 1980s or early 
1990s. It uses techniques that are also used in steg-
anography (the concealment of messages, files, or 
other types of data within images, audio, video, or 
even text).
Most digital watermarks are not detectable without 
an algorithm that can search for the signal embedded 
in the carrier signal. In order for a carrier signal to 
be watermarked, it must be tolerant of noise. Noise-
tolerant signals are generally strong signals that re-
sist degradation or unwanted modulation. Typically, 
digital watermarks are embedded in data by using an 
algorithm to encode the original signal with a hidden 
signal. The embedding may be performed using ei-
ther public- or private-key encryption, depending on 
the level of security required.
Qualities of Digital Watermarks
One way to classify digital watermarks is by capacity, 
which measures how long and complex a water-
marking signal is. The simplest type is 1-bit water-
marking. This is used to encode a simple message 
that is meant only to be detected or not (a binary 
result of 1 or 0). In contrast, multibit watermarking 
embeds multiple bits of data in the original signal. 
Multibit systems may be more resistant to attack, as 
an attacker will not know how or where the water-
mark has been inserted.
Watermarks may also be classified as either ro-
bust or fragile. Robust watermarks resist most types 
of modification and therefore remain within the 
signal after any alterations, such as compression or 
cropping. These watermarks are often used to embed 
copyright information, as any copies of the file will 
also carry the watermark. Fragile watermarks cannot 
be detected if the signal is modified and are there-
fore used to determine if data has been altered.
In some cases, a digital watermark is designed so 
that users can easily detect it in the file. For instance, 
a video watermark may be a visible logo or text 
hovering onscreen during playback. In most cases, 
however, digital watermarks are hidden signals that 
can only be detected using an algorithm to retrieve 
the watermarking code. Reversible data hiding refers 
to cases in which the embedding of a watermark can 
be reversed by an algorithm to recover the original 
file.
Applications of Digital Watermarking
A primary function of digital watermarking is to pro-
tect copyrighted digital content. Audio and video 
players may search for a digital watermark contained 
in a copyrighted file and only play or copy the file if it 
contains the watermark. This essentially verifies that 
the content is legally owned.
Certain types of programs, known colloquially as 
crippleware, use visible digital watermarks to ensure 
that they are legally purchased after an initial free 
evaluation period. Programs used to produce digital 
media files, such as image- or video-editing software, 
can often be downloaded for free so users can try 
them out first. To encourage users to purchase the 
full program, these trial versions will output images 
or videos containing a visible watermark. Only when 
the program has been registered or a product key has 
been entered will this watermark be removed.
Some creators of digital content use digital water-
marking to embed their content with their identity 
and copyright information. They use robust water-
marks so that even altered copies of the file will retain 
them. This allows a content owner to claim their work 
even if it has been altered by another user. In some 
cases, watermarked data can be configured so that 
any copies can be traced back to individual users or 
distributors. This function can be useful for tracing il-
legal distribution of copyrighted material. It can also 
help investigations into the unauthorized leaking of 
sensitive or proprietary files.
More recently, digital watermarking has been 
used to create hidden watermarks on product pack-
aging. This is intended to make it easier for point-of-
sale equipment to find and scan tracking codes on 
a product. Digital watermarking is also increasingly 
being used alongside or instead of regular water-
marking to help prevent the counterfeiting of impor-
tant identification papers, such as driver’s licenses 
and passports.
—Micah L. Issitt

115
Principles of Computer Science
Dirty paper coding
Bibliography
Chao, Loretta. “Tech Partnership Looks beyond the 
Bar Code with Digital Watermarks.” Wall Street 
Journal. Dow Jones, 12 Jan. 2016. Web. 14 Mar. 
2016.
“Frequently Asked Questions.” Digital Watermarking 
Alliance. DWA, n.d. Web. 11 Mar. 2016.
Gupta, Siddarth, and Vagesh Porwal. “Recent 
Digital Watermarking Approaches, Protecting 
Multimedia Data Ownership.” Advances in Computer 
Science 4.2 (2015): 21–30. Web. 14 Mar. 2016.
Patel, Ruchika, and Parth Bhatt. “A Review Paper 
on Digital Watermarking and Its Techniques.” 
International Journal of Computer Applications 110.1 
(2015): 10–13. Web. 14 Mar. 2016.
Savage, Terry Michael, and Karla E. Vogel. An 
Introduction 
to 
Digital 
Multimedia. 
2nd 
ed. 
Burlington: Jones, 2014. 256–58. Print.
“Unretouched by Human Hand.” Economist. Economist 
Newspaper, 12 Dec. 2002. Web. 14 Mar. 2016.
Dirty paper coding
Field of Study
Computer Science
Abstract
Dirty paper coding is a technique that aims to maxi-
mize channel capacity. Communication channels 
experience a lot of interference. Through this tech-
nique, the receiver should receive the signal or mes-
sage with minimal distortion. In such cases, receivers 
are unaware of the interference. Adoption and im-
provement of the technique ensure efficient data 
transmission with minimal power requirements.
Prinicipal Terms

 Additive White Gaussian Noise (AWGN): a model 
used to represent imperfections in real communi-
cation channels.

 channel capacity: the upper limit for the rate at 
which information transfer can occur without 
error.

 interference: anything that disrupts a signal as it 
moves from source to receiver.

 noise: interferences or irregular fluctuations af-
fecting electrical signals during transmission.

 precoding: a technique that uses the diversity 
of a transmission by weighting an information 
channel.

 signal-to-noise ratio (SNR): the power ratio be-
tween meaningful information, referred to as 
“signal,” and background noise.
Effective Data Transmission
Data encounters interference during transmission 
from source to receiver on a channel. Dirty paper 
coding (DPC) is used in channels subjected to in-
terference. Using DPC on such channels helps 
achieve efficient data transmission by ensuring 
channel capacity. Efficient transmission is pos-
sible despite interference because DPC uses pre-
coding. This is a technique that minimizes data’s 
vulnerability to distortion before reaching the 
receiver. The idea for DPC originated with Max 
Costa in 1983. Costa compared data transmis-
sion to sending a message on paper. The paper 
gets dirtier along the way before it reaches the 
intended recipient, who cannot distinguish ink 
from dirt. Apart from achieving channel capacity, 
DPC works without additional power require-
ments and without the receiver being aware of the  
interference.
Eliminating Interference
Data transmission via communication systems always 
faces interference. Through DPC, it is possible to 
eliminate as much interference as possible. Noise is 
a type of electrical interference. Applying Additive 
White Gaussian Noise (AWGN) provides an effective 
way to develop channels with minimal distortion rates 
and corrupted signals. In some contexts, AWGN is re-
ferred to as a “noise removal algorithm.” The model 
is “additive” because it is added to noise affecting a 
channel. It is “white” to denote uniform power across 

116
Dirty paper coding
Principles of Computer Science
a channel’s frequency band. It is Gaussian because of 
its normal distribution within the time domain.
AWGN is used to simulate distortions facing a 
channel to make it efficient, a concept that DPC pro-
poses. Models like AWGN have helped developers 
create multiuser channels with multiple-antenna 
transmitters. Implementing DPC techniques ensures 
each user encounters no interference from others in 
such multiuser channels.
Information Integrity
In wireless infrastructures, DPC helps improve perfor-
mance. Improvements have contributed to the devel-
opment of multicarrier hybrid systems that combine 
unicast and broadcast connectivity. Implementing ar-
chitectures that use DPC allows reception of interfer-
ence-free signals. Thus, cellular, television, and radio 
signals that use unicast and broadcast connectivity 
are becoming clearer with each improvement.
DPC has also found its way into information 
hiding, or “digital watermarking.” In that process, an 
encoded message is sneaked into a waveform using 
an unknown signal. DPC ensures the receiver can 
decode the message. The technique also minimizes 
distortion to the original message and required 
power. Attackers with no knowledge of the en-
coding and signal parameters cannot decode 
the message. Removing the watermark also re-
quires the hidden parameters.
Military 
communications 
apply 
water-
marking. Parameters used in the process be-
come classified information and are only acces-
sible by authorized individuals. Watermarking 
helps safeguard information integrity.
Dirty Paper Coding’s Contribution
Despite challenges in the calculations required, 
DPC has formed the basis for the development 
of new techniques for achieving efficiency in 
data transmission and information systems. 
One example is zero-forcing dirty paper coding. 
Efficient channels have fewer power demands, 
minimal message distortion, secure transmis-
sions, and high signal-to-noise ratios (SNR). 
The combination of such advantages presages 
the development of affordable wireless infra-
structures with fast and reliable data transmis-
sion rates. With the growing number of mobile 
devices accessing wireless networks, such infra-
structure improvement might provide one of 
the solutions to the low battery life of the devices. 
Models on DPC, such as DPC with phase reshaping 
(PDC-PR), can help address the problem of resource 
allocation facing unicast and broadcast systems. 
However, performance comparisons between dif-
ferent DPC modifications can help establish the best 
technique to adopt.
—Melvin O
Bibliography
Cox, Ingemar J., Jessica Fridrich, Matthew L. Miller, 
Jeffrey A. Bloom, and Ton Kalker. “Practical 
Dirty-Paper Codes.” Digital Watermarking and 
Steganography. 2nd ed. Amsterdam: Elsevier, 2008. 
183–212. Digital file.
Devroye, Natasha, Patrick Mitran and Vahid Tarokh. 
On Cognitive Graphs: Decomposing Wireless Networks. 
New York: Wiley Interscience, 2006. Print.
Devroye, N., P. Mitran, and V. Tarokh. “Limits on 
Communications in a Cognitive Radio Channel.” 
IEEE Communication Magazine 44.6 (2006): 4449. 
Inspec. Web. 9 Mar. 2016.
Dirty Coded Data
Dirty Coding
(interference)
Original Data
DATA
DATA
DATA
Dirty Coded Data
Dirty Coding
(interference)
Original Data
DATA
Dirty paper coding is named for the conceptual analogy of deciphering 
writing on paper that has had known ink splotches added to it. In data 
transmission, the “ink splotches” are data interference, but someone who 
knows the interference is there can remove it to see the original data that 
was covered. EBSCO illustration.

117
Principles of Computer Science
DOS
Habiballah, N., M. Qjani, A. Arbaoui, and J. Dumas. 
“Effect of a Gaussian White Noise on the Charge 
Density Wave Dynamics in a One Dimensional 
Compound.” Journal of Physics and Chemistry of 
Solids 75.1 (2014): 153–56. Inspec. Web. 9 Mar. 
2016.
Kilper, Daniel C., and Tucker, Rodney S. “Energy-
Efficient 
Telecommunications.” 
Optical 
Fiber 
Telecommunications. 6th ed. N.p.: Elsevier, 2013. 
747–91. Digital file.
Savischenko, Nikolay V. Special Integral Functions Used 
in Wireless Communications Theory. N.p.: World 
Scientific, 2014. Digital file.
Sample Problem
Given a bandwidth of 10 megabytes per second 
(MBps) and using an acceptable signal-to-
noise ratio (SNR) of 25 decibels (dB), calcu-
late the channel capacity (C) in bits per second 
(bps), using the following formula:
C = B log2(1 + S/N)
where B is the bandwidth, S is the signal power, 
and N is the noise power.
Answer:
First, convert the SNR in decibels to power, 
using the following equation:
SNRdB = 10 log10(S/N)
25 dB = 10 log10(S/N)
2.5 = log10(S/N)
102.5 = S/N
316 = S/N
Next, convert the bandwidth from megabytes 
per second (MBps) to bits per second (bps). 
Recall that 1 byte is equal to 8 bits, and ignore 
the seconds for now:
1 MB = 16 B
= 8 b/B × 16 B
= 86 b
Then, plug in the found values for the band-
width and S/N into the given formula, and solve:
C = B log2(1 + S/N)
C = 86 bps ∙ log2(1 + 316)
C = 86 (log2(317))
C = 86 × 8.31
C = 6.648 × 107 bps
DOS
Fields of Study
Computer 
Science; 
Information 
Technology; 
Operating Systems
Abstract
The term DOS is an acronym for “disk operating 
system.” DOS refers to any of several text-based oper-
ating systems that share many similarities. Perhaps the 
best-known form of DOS is MS-DOS. MS-DOS is an op-
erating system developed by the technology company 
Microsoft. It is based heavily on an earlier operating 
system known as QDOS. DOS was largely replaced by 
operating systems featuring graphical user interfaces 
by the end of the twentieth century. It continues to be 
used in certain specialized contexts in the twenty-first 
century, however.
Prinicipal Terms

 command line: a text-based computer interface 
that allows the user to input simple commands via 
a keyboard.

 graphical user interface (GUI): an interface that 
allows users to control a computer or other device 
by interacting with graphical elements such as 
icons and windows.

 nongraphical: not featuring graphical elements.

 shell: an interface that allows a user to operate a 
computer or other device.
Background on DOS
“Disk operating system” (DOS) is a catchall term 
for a variety of early operating systems designed for 
personal computers (PCs). Operating systems have 
existed since the early days of computers. They 

118
DOS
Principles of Computer Science
became more important in the late 1970s and early 
1980s. At that time PCs became available to the 
public. An operating system allows users unfamiliar 
with computer programming to interface directly 
with these devices. Among the first companies to 
offer PCs for sale was the technology company 
IBM, a long-time leader in the field of computer 
technology.
In 1980 IBM sought to license an operating system 
for its new device, the IBM PC. IBM approached 
the company Digital Research to license its Control 
Program for Microcomputers (CP/M). However, 
the two companies were unable to come to an agree-
ment. IBM made a deal with the software company 
Microsoft instead. Microsoft had been founded in 
1975. It initially specialized in creating and licensing 
computer programs and programming languages. 
In order to supply IBM with an operating system, 
Microsoft licensed the Quick and Dirty Operating 
System (QDOS) from the company Seattle Computer 
Products. This system, later renamed 86-DOS, was 
based in part on CP/M and used similar commands 
but differed in several key ways. Notably, the oper-
ating system was designed to be used with computers, 
such as the IBM PC, that featured sixteen-bit mi-
croprocessors, which could process sixteen bits, or 
pieces of binary information, at a time. Microsoft also 
hired QDOS creator Tim Patterson. Microsoft asked 
Patterson to create a new version of QDOS to be li-
censed to IBM under the name PC-DOS. Microsoft 
went on to develop an essentially identical version 
called MS-DOS, which the company sold and licensed 
to computer manufacturers itself.
As IBM’s PCs became increasingly popular, com-
peting hardware manufacturers created similar 
computers. Many of these computers, commonly 
known as “IBM clones,” used MS-DOS. In addition to 
MS-DOS, PC-DOS, and 86-DOS, other DOS or DOS-
compatible systems entered the market over the de-
cades. However, MS-DOS dominated the market and 
was often referred to simply as DOS.
Understanding DOS
MS-DOS is the most popular and best-known form of 
DOS. MS-DOS consists of three key parts: the input/
output (I/O) handler, the command processor, 
and the auxiliary utility programs. The I/O handler 
manages data input and output and consists of two 
programs, IO.SYS and MSDOS.SYS. The command 
processor enables the computer to take commands 
from the user and carry them out. The most com-
monly used commands and associated routines are 
stored in the command processor. Others 
are stored on the system disk and loaded 
as needed. Those routines are known as 
“auxiliary utility programs.”
Like all operating systems, DOS func-
tions as a shell. A shell is an interface 
that allows a user to operate a computer 
without needing knowledge of computer 
programming. A DOS or DOS-compatible 
system does require users to enter text-
based commands. These are relatively 
simple and limited in number, however. 
As such, the public found PCs featuring 
the early forms of DOS fairly easy to use.
Using DOS
MS-DOS and later DOS and DOS-
compatible systems are in many ways 
quite similar to those systems that came 
before, such as QDOS and CP/M. In 
general, MS-DOS and similar systems are 
nongraphical systems. Thus, they do not 
contain graphical elements found in later 
The FreeDOS command line interface is based on the original DOS (disk oper-
ating system) command line interface to provide individuals with an alternative to 
the more prevalent graphical user interface available with most operating systems. 
Public domain, via Wikimedia Commons.

119
Principles of Computer Science
Drones
graphical user interfaces (GUIs), such as clickable 
icons and windows. Instead, nongraphical systems 
allow the user to operate the computer by entering 
commands into the command line. By entering basic 
commands, the user can instruct the computer to 
perform a wide range of functions, including run-
ning programs and opening or copying files.
Impact of DOS
Although DOS and other nongraphical systems were 
largely phased out by the mid-1990s as GUIs became 
more popular, they remained an influential part of 
the development of PC operating systems. Some 
graphical operating systems, such as Microsoft’s 
Windows 95, were based in part on DOS, and had 
the ability to run DOS programs when opened in 
a specialized mode. Despite advances in computer 
technology, MS-DOS and similar systems are still used 
in the twenty-first century for applications. In some 
cases, companies or institutions continue to use the 
operating system in order to maintain access to soft-
ware compatible only with DOS. Some individuals use 
DOS and DOS-compatible systems to play early games 
originally designed for those systems. Companies 
such as Microsoft no longer sell DOS to the public. 
However, a number of companies and organizations 
are devoted to providing DOS-compatible systems to 
users, often in the form of open-source freeware.
—Joy Crelin
Bibliography
Doeppner, Thomas W. Operating Systems in Depth. 
Hoboken: Wiley, 2011. Print.
Gallagher, Sean. “Though ‘Barely an Operating 
System,’ DOS Still Matters (to Some People).” Ars 
Technica. Condé Nast, 14 July 2014. Web. 31 Jan. 
2016.
McCracken, Harry. “Ten Momentous Moments in 
DOS History.” PCWorld. IDG Consumer, n.d. Web. 
31 Jan. 2016.
Miller, Michael J. “The Rise of DOS: How Microsoft 
Got the IBM PC OS Contract.” PCMag.com. PCMag 
Digital Group, 10 Aug. 2011. Web. 31 Jan. 2016.
“MS-DOS: A Brief Introduction.” Linux Information 
Project. Linux Information Project, 30 Sept. 2006. 
Web. 31 Jan. 2016.
“Part Two: Communicating with Computers—The 
Operating System.” Computer Programming for 
Scientists. Oregon State U, 2006. Web. 31 Jan. 2016.
Shustek, Len. “Microsoft MS-DOS Early Source 
Code.” Computer History Museum. Computer 
History Museum, 2013. Web. 31 Jan. 2016.
Drones
Fields of Study
Computer Engineering; Robotics; Privacy
Abstract
Drones are unmanned aerial vehicles (UAVs). Unlike 
other UAVs, drones are at least partly automated but 
may also respond to remote piloting. Drones are 
used in military operations. They are also used for 
commercial applications like aerial photography and 
filmmaking, search and rescue, and environmental 
research. The concept of unmanned aerial vehicles 
for combat use emerged in the 1800s. Advancements 
in computer technology and engineering have made 
drones commonplace in the 2010s for both military 
and civilian applications.
Prinicipal Terms

 actuators: motors designed to control the move-
ment of a device or machine by transforming po-
tential energy into kinetic energy.

 communication devices: devices that allow drones 
to communicate with users or engineers in remote 
locations.

 field programmable gate array: an integrated cir-
cuit that can be programmed in the field and can 
therefore allow engineers or users to alter a ma-
chine’s programming without returning it to the 
manufacturer.

 sensors: devices capable of detecting, measuring, 
or reacting to external physical properties.

120
Drones
Principles of Computer Science

 telemetry: automated communication process 
that allows a machine to identify its position rela-
tive to external environmental cues.

 unmanned aerial vehicle (UAV): an aircraft that 
does not have a pilot onboard but typically oper-
ates through remote control, automated flight sys-
tems, or preprogrammed computer instructions.
Drone Development
The term “drone” refers to any of a large number 
of semi-independent flying vehicles, or unmanned 
aerial vehicles (UAVs). Military organizations began 
using UAVs in the form of balloons and kites to 
conduct aerial surveillance and air strikes in the 
nineteenth century. Radio-controlled UAVs were 
used in World War II. In the 1990s advancements in 
computer technology, sensor capacity, and micro-
engineering led to the development of the first true 
drones. True drones are remote-controlled vehicles 
that have some level of automation or self-control. 
By 2016, nations around the world were using drones 
for military operations. Drone technology had also 
become popular for nonmilitary uses, including 
photography, filmmaking, ecological and wildlife re-
search, and search-and-rescue operations.
How Drones Work
While remote-controlled helicopters, planes, and 
other flying vehicles have been available to hobbyists 
since the mid-twentieth century, remote-controlled 
vehicles that qualify as drones differ by having some 
degree of automation. Most are partly automated 
and still need some control from a pilot. Drones may 
be piloted using web-based signals, radio waves, or 
Bluetooth connections. More advanced drones can 
automatically avoid obstacles, regulate flight path, 
and even plot new paths.
Drones typically have sensors that allow the 
drone to detect, record, transmit, and respond to 
environmental variables. For instance, drones with 
high-definition cameras can collect visual data 
about the environment. Some advanced drones can 
use this data to adjust their flight path, speed, and 
elevation. They may also navigate toward certain 
visual targets. Drones also typically have telemetry 
equipment. Such equipment collects and transmits 
data about the drone’s location relative to other en-
vironmental features. With telemetry and sensors, 
drone pilots can determine the drone’s location. 
They can then direct the drone toward a target lo-
cation. Drones may also have communications de-
vices. These devices transmit data back to users and 
allow them to adjust the drone’s activities. Some 
advanced drones have voice-command software 
that allows the drone to respond to a user’s verbal 
commands.
Most drones are powered by batteries. The bat-
teries send electrical energy to actuators, engines 
that use the energy to create movement. This move-
ment powers rotors, jet turbines, 
and flight control mechanisms. 
Battery life is one of the major 
challenges in drone engineering. 
Long-lasting batteries are also 
typically heavy and so can only be 
used in larger drones. Engineers 
are designing a new generation of 
solar-electric hybrid drones. These 
hybrid drones can get additional 
charge from sunlight, thus greatly 
increasing the duration of drone 
flights.
Drone Applications
Many 
companies 
producing 
drones for the commercial market 
use open-source software. This al-
lows the same basic software to be 
Unmanned aerial vehicles, more commonly known as drones, come in sizes ranging from a 
small toy to a large plane. They are used by the military for surveillance as well as by civilians 
for recreation. By Gerald L. Nino, public domain, via Wikimedia Commons.

121
Principles of Computer Science
Drones
altered for a variety of applications. Many companies 
also outfit drones with adjustable hardware, such as 
field programmable gate arrays. These arrays allow 
users to adjust hardware programming in the field 
without returning the drone to the manufacturer. 
This flexibility in hardware and software design 
means that drones can be altered for applications 
ranging from scientific research to art projects.
Drones have been used by law enforcement agen-
cies for aerial search and surveillance. They have also 
been used for search-and-rescue operations. Drones 
with high-definition video equipment onboard have 
become a popular tool in journalism and filmmaking. 
Drones can take aerial shots that once would have 
been impossible without helicopters. Wildlife and 
ecological researchers have also used UAVs to take 
aerial footage. Drones with specialized equipment 
can be used to collect detailed topographical and 
ecological data or to monitor weather patterns. Some 
engineers think that the development of solar-elec-
tric hybrid drones may be the key to future weather 
monitoring. Such hybrid drones may one day replace 
the use of low-Earth orbit satellites for signal trans-
mission and other applications.
Controversy and Regulation
The military use of drones, especially for lethal op-
erations, is among the most controversial issues of 
the twenty-first century. Drone strikes have been 
linked to civilian casualties and accidental deaths. 
Opponents of drone strikes argue that automating 
military operations is a violation of moral and ethical 
principles. Another controversy surrounding UAVs is 
the potential for the technology to be misused for un-
authorized or illegal surveillance. Such surveillance 
would be a threat to personal privacy. In the United 
States, the Federal Aviation Authority (FAA) is investi-
gating potential regulations for commercial and rec-
reational drone use. State legislators have passed laws 
that prohibit the use of drones over private property. 
Governmental regulation of private drone use is still 
in its infancy. The debate over the legality and poten-
tial for misuse has spread around the world, however.
—Micah Issitt
Bibliography
Couts, Andrew. “Drones 101: A Beginner’s Guide to 
Taking Flight, No License Needed.” Digital Trends. 
Designtechnica, 16 Nov. 2013. Web. 27 Jan. 2015.
Fowler, Geoffrey, A. “The Drones on Autopilot That 
Follow Your Lead (Usually).” Wall Street Journal. 
Dow Jones, 23 Dec. 2014. Web. 20 Jan. 2016.
Moynihan, Tim. “Things Will Get Messy If We Don’t 
Start Wrangling Drones Now.” Wired. Condé Nast, 
30 Jan. 2015. Web. 30 Jan. 2016.
Pullen, John Patrick. “This Is How Drones Work.” 
Time. Time, 3 Apr. 2015. Web. 27 Jan. 2016.
Stanley, Jay. “‘Drones’ vs ‘UAVs’—What’s behind a 
Name?” ACLU. ACLU, 20 May 2013. Web. 27 Jan. 
2016.
“Unmanned Aircraft Systems (UAS) Frequently 
Asked Questions.” Federal Aviation Administration. 
US Dept. of Transportation, 18 Dec. 2015. Web. 
11 Feb. 2016.

122
Electronic circuits
Field of Study
Computer Engineering
Abstract
Electronic circuits actively manipulate electric cur-
rents. For many years these circuits have been part 
of computer systems and important home appliances 
such as televisions. Innovation has helped develop 
effective circuits such as integrated circuits, which 
are power efficient, small, and powerfully capable. 
Integrated circuits form the basic operational units 
of countless everyday gadgets.
Prinicipal Terms

 BCD-to-seven-segment decoder/driver: a logic 
gate that converts a four-bit binary-coded decimal 
(BCD) input to decimal numerals that can be 
output to a seven-segment digital display.

 counter: a digital sequential logic gate that records 
how many times a certain event occurs in a given 
amount of time.

 inverter: a logic gate whose output is the inverse of 
the input; also called a NOT gate.

 negative-AND (NAND) gate: a logic gate that pro-
duces a false output only when both inputs are 
true

 programmable oscillator: an electronic device 
that fluctuates between two states that allows 
user modifications to determine mode of opera-
tion.

 retriggerable single shot: a monostable multi-
vibrator (MMV) electronic circuit that outputs a 
single pulse when triggered but can identify a new 
trigger during an output pulse, thus restarting its 
pulse time and extending its output.
Electric versus Electronic Circuits
Electrical circuits have developed over the years since 
the discovery of the Leyden jar in 1745. An electrical 
circuit is simply a path through which electric current 
can travel. Its primary components include resistors, 
inductors, and capacitors. The resistor controls the 
amount of current that flows through the circuit. It 
is so called because it provides electrical resistance. 
The inductor and the capacitor both store energy. 
Inductors store energy in a magnetic field, while ca-
pacitors store it in the electric field. The Leyden jar was 
the first capacitor, designed to store static electricity.
Electronic circuits are a type of electrical circuit. 
However, they are distinct from basic electrical cir-
cuits in one important respect. Electrical circuits pas-
sively conduct electric current, while electronic cir-
cuits actively manipulate it. In addition to the passive 
components of an electrical circuit, electronic cir-
cuits also contain active components such as transis-
tors and diodes. A transistor is a semiconductor that 
works like a switch. It can amplify an electronic signal 
or switch it on or off. A diode is a conductor with very 
low electrical resistance in one direction and very 
high resistance in the other. It is used to direct the 
flow of current.
Integrated Circuits
The most important advance in electronic circuits 
was the development of the integrated circuit (IC) 
in the mid-twentieth century. An IC is simply a semi-
conductor chip containing multiple electronic cir-
cuits. Its development was enabled by the invention 
of the transistor in 1947. Previously, electric current 
was switched or amplified through a vacuum tube. 
Vacuum tubes are much slower, bulkier, and less ef-
ficient than transistors. The first digital computer, 
E

123
Principles of Computer Science
Electronic circuits
ENIAC (Electronic Numerical Integrator and 
Computer), contained about eighteen thousand 
vacuum tubes and weighed more than thirty tons. 
Once the transistor replaced the vacuum tube, elec-
tronic circuits could be made much smaller.
By 1958, several scientists had already proposed 
ideas for constructing an IC. That year, Jack Kilby, a 
scientist at Texas Instruments, was the first to put the 
idea into practice. He designed a chip constructed en-
tirely from a single block of semiconductor material. 
Because there were no individual components, the 
circuits did not have to be large enough to assemble 
manually. The number of circuits in the chip was lim-
ited only by the number of transistors that could fit 
in it. Early ICs contained only a few transistors each. 
By the twenty-first century, the maximum possible 
number of transistors per IC was in the billions.
Logic Gates
Active manipulation of electric current is accomplished 
through logic gates. A logic gate is an electronic circuit 
that implements a Boolean function. Broadly speaking, 
a Boolean function is a function that produces one of 
two potential outputs—either 0 or 1—based on a given 
rule. Because transistors work as switches, which can 
take one of two values (e.g., “on” or “off”), they are 
ideal for implementing logic gates. Most logic gates 
take in two inputs and produce a single output.
There are seven basic types of logic gates: AND, 
OR, NOT, XOR, NAND, NOR, and XNOR. These 
logic gates only accept two input values, 0 and 1, 
which represent “false” and “true” respectively. They 
are distinguished from one another based on what 
output is produced by each combination of inputs:
AND gate: output is only true (1) if both inputs 
are true; otherwise, output is false (0).
OR gate: output is false (0) only if both outputs 
are false; otherwise, output is true (1).
NOT gate: output is true (1) if input is false (0), 
and vice versa. A NOT gate is also called an in-
verter, because it takes in only one input and 
outputs the inverse.
exclusive-OR (XOR) gate: output is true (1) 
if the inputs are different, that is, if only one 
input is true; if both inputs are the same, output 
is false (0).
negative-AND (NAND) gate: output is false (0) 
if all inputs are true (1); otherwise output is 
OR
NOT
AND
AND
AND
AND
AND
AND
AND
OR
OR
OR
1
1
0
2
3
5
+
NOT
AND
NOT
AND
AND
OR
AND
0
1
1
1
+
Electronic circuits are designed to use a series of logic gates to send a charge through the circuit in a particular manner. These logic gates 
control the charge output and thus the output of the circuits. In this example, the circuit is designed to add two binary numbers together 
using a series of AND, OR, and NOT commands to determine the route of the charge and the resulting output from each circuit compo-
nent. EBSCO illustration.

124
Electronic circuits
Principles of Computer Science
true. A NAND gate is essentially an AND gate 
followed by an inverter
NOR gate: output is true (1) only if both inputs 
are false (0); otherwise, output is false. A NOR 
gate is an OR gate followed by an inverter.
exclusive-NOR (XNOR) gate: output is true (1) 
if both inputs are the same and false (0) if they 
are different. An XNOR gate is a XOR gate fol-
lowed by an inverter.
Electronic circuits transmit binary data in the form 
of electric pulses, where, for example, 0 and 1 are rep-
resented by pulses of different voltages. These seven 
gates can be combined in different ways to complete 
more complex operations. For example, a BCD-to-
seven-segment decoder/driver is a logic gate that con-
verts binary data from a counter to a decimal number 
display. The “seven segment” refers to the number 
display system common in digital clocks and other 
devices, where each numeral is represented by up to 
seven short parallel or perpendicular line segments. 
Another complex circuit is a retriggerable single shot. 
This is a type of time-delay relay circuit that can gen-
erate an output pulse of a predetermined length and 
then extend the output indefinitely if the input is re-
peated. The purpose of this is to change the timing 
of another circuit, such as a programmable oscillator.
Life without Integrated Circuits
Whether in home appliances, computer systems, or 
mobile devices, electronic circuits make modern life 
possible. Without advanced electronic circuits such 
as ICs, personal computers and small, portable elec-
tronic devices could not exist. Despite improvements 
over the years, ICs have maintained their silicon-
based design. Scientists predict that the only thing to 
replace ICs would be a new kind of biologically based 
circuit technology.
—Melvin O
Bibliography
Frenzel, Louis E., Jr. Electronics Explained: The New 
Systems Approach to Learning Electronics. Burlington: 
Elsevier, 2010. Print.
Harris, David Money, and Sarah L. Harris. Digital 
Design and Computer Architecture. 2nd ed. Waltham: 
Morgan, 2013. Print.
“The History of the Integrated Circuit.” Nobelprize.org. 
Nobel Media, 2014. Web. 31 Mar. 2016.
Kosky, Philip, et al. Exploring Engineering: An 
Introduction to Engineering and Design. 4th ed. 
Waltham: Academic, 2016. Print.
Tooley, Mike. Electronic Circuits: Fundamentals and 
Applications. 4th ed. New York: Routledge, 2015. Print.
Wilson, Peter. The Circuit Designer’s Companion. 3rd 
ed. Waltham: Newnes, 2012. Print.
Sample Problem
Determine the output of a NAND logic gate 
for all possible combinations of two input 
values (0 and 1).
Answer:
The combination of an AND gate and a NOT 
gate forms a NAND logic gate. The output of 
each input combination is the inverse of the 
AND gate output. The NAND gate accepts 
four possible combinations of two inputs and 
produces outputs as shown:
0,0 = 1
0,1 = 1
1,0 = 1
1,1 = 0

125
Principles of Computer Science
Electronic communication software
Electronic communication software
Fields of Study
Information Systems; Information Technology
Abstract
Electronic communication software is used to 
transfer information via the Internet or other trans-
mission-and-reception technology. As technology 
has evolved, electronic communication software has 
taken on many new forms, from text-based instant 
messaging using computers to SMS messages sent 
between cell phones on opposite sides of the world. 
Electronic communication software allows people to 
communicate in real time using audio and video and 
to exchange digital files containing text, photos, and 
other data.
Prinicipal Terms

 Electronic Communications Privacy Act (ECPA): a 
regulation enacted in 1986 to limit the ability of 
the US government to intrude upon private com-
munications between computers.

 multicast: a network communications protocol in 
which a transmission is broadcast to multiple re-
cipients rather than to a single receiver.

 push technology: a communication protocol in 
which a messaging server notifies the recipient as 
soon as the server receives a message, instead of 
waiting for the user to check for new messages.

 Short Message Service (SMS): the technology un-
derlying text messaging used on cell phones.

 voice over Internet Protocol (VoIP): a set of pa-
rameters that make it possible for telephone calls 
to be transmitted digitally over the Internet, rather 
than as analog signals through telephone wires.
Asynchronous Communication
Many types of electronic communication software are 
asynchronous. This means that the message sender 
and the recipient communicate with one another at 
different times. The classic example of this type of 
electronic communication software is e-mail. E-mail 
is asynchronous because when a person sense a mes-
sage, it travels first to the server and then to the re-
cipient. The server may use push technology to notify 
the recipient that a message is waiting. It is then up to 
the recipient to decide when to retrieve the message 
from the server and read it.
E-mail evolved from an earlier form of asynchro-
nous electronic communication: the bulletin-board 
system. In the 1980s and earlier, before the Internet 
was widely available to users in their homes, most 
people went online using a dial-up modem. A dial-
up modem is a device that allows a computer to 
connect to another computer through a telephone 
line. To connect, the first computer dials the phone 
number assigned to the other computer’s phone 
line. Connecting in this way was slow and cumber-
some compared to the broadband Internet access 
common today. This was in part because phone 
lines could only be used for one purpose at a time, 
so a user could not receive phone calls while on-
line. Because users tended to be online only in short 
bursts, they would leave messages for each other on 
online bulletin-board systems (BBSs). Similar to e-
mail, the message would stay on the BBS until its re-
cipient logged on and saw that it was waiting.
Another popular method of asynchronous com-
munication is text messaging. Text messaging al-
lows short messages to be sent from one mobile 
phone to another. The communications protocol 
technology behind text messages is called Short 
Message Service (SMS). SMS messages are limited 
to 160 characters. They are widely used because 
they can be sent and received using any kind of cell 
phone.
Synchronous Communication
Other forms of electronic communication software 
allow for synchronous communication. This means 
that both the recipient and the sender interact 
through a communications medium at the same time. 
The most familiar example of synchronous commu-
nication is the telephone, and more recently the cell 
phone. Using either analog protocols or voice over 
Internet Protocol (VoIP), users speak into a device. 
Their speech is translated into electronic signals by 
the device’s communication software and then trans-
mitted to the recipient. There are sometimes minor 
delays due to network latency. However, most of the 
conversation happens in the same way it would if the 
parties were face to face.

126
Electronic communication software
Principles of Computer Science
Another form of electronic synchronous commu-
nication is instant messaging or chat. Instant mes-
saging occurs when multiple users use computers 
or mobile devices to type messages to one another. 
Each time a user sends a message, the recipient or 
recipients see it pop up on their screens. Chat can 
occur between two users, or it can take the form of 
a multicast in which one person types a message and 
multiple others receive it.
Multicast can also be delivered asynchronously. 
An example of this type of electronic communica-
tion is a performer who records a video of them-
selves using a digital camera and then posts the video 
on an online platform such as YouTube. The video 
would then stay online, available for others to watch 
at any time, until its creator decided to take it down. 
This type of electronic communication is extremely 
popular because viewers do not have to be online at 
a particular time in order to view the performance, 
as was the case with television broadcasts in the past.
Privacy Concerns
In some respects the rapid growth of electronic com-
munication software caught regulators off guard. 
There were many protections in place to prevent the 
government from eavesdropping 
on private communications using 
the telephone. However similar pro-
tections for electronic communica-
tions were lacking until the passage 
of the Electronic Communications 
Privacy Act (ECPA) in the late 1980s. 
This act extended many traditional 
communication protections to VoIP 
calls, e-mails, SMS messages, chat 
and instant messaging logs, and 
other types of communications.
Cutting Edge
Some of the newest forms of elec-
tronic communication software are 
pushing the boundaries of what 
is possible. One example of this is 
video calling using cell phones. 
This technology is available in 
many consumer devices, but in re-
ality its utility is often limited by the 
amount of bandwidth available in 
some locations. This causes poor video quality and 
noticeable delays in responses between users.
—Scott Zimmer, JD
Bibliography
Bucchi, Massimiano, and Brian Trench, eds. Routledge 
Handbook of Public Communication of Science and 
Technology. 2nd ed. New York: Routledge, 2014. Print.
Cline, Hugh F. Information Communication Technology 
and Social Transformation: A Social and Historical 
Perspective. New York: Routledge, 2014. Print.
Gibson, Jerry D., ed. Mobile Communications Handbook. 
3rd ed. Boca Raton: CRC, 2012. Print.
Gillespie, Tarleton, Pablo J. Boczkowski, and 
Kirsten A. Foot, eds. Media Technologies: Essays on 
Communication, Materiality, and Society. Cambridge: 
MIT P, 2014. Print.
Hart, Archibald D., and Sylvia Hart Frejd. The Digital 
Invasion: How Technology Is Shaping You and Your 
Relationships. Grand Rapids: Baker, 2013. Print.
Livingston, Steven, and Gregor Walter-Drop, eds. 
Bits and Atoms: Information and Communication 
Technology in Areas of Limited Statehood. New York: 
Oxford UP, 2014. Print.
Electronic communication software has many forms to satisfy many uses. The most pop-
ular social media and communication companies implement programming that provides 
users with attributes they deem important for electronic communication, such as private 
and/or public sharing and posting to a network, saving, contributing, subscribing, and 
commenting across a number of formats. EBSCO illustration.

127
Principles of Computer Science
Electronic waste
Electronic waste
Fields of Study
Computer 
Science; 
Computer 
Engineering; 
Information Technology
Abstract
This article discusses the growing problem of waste 
in society and goes into the specific issues and chal-
lenges surrounding the problem of discarded elec-
tronic devices. Electronic waste poses a significant 
threat to the environment, and various efforts to re-
mediate the issue have been proposed.
Prinicipal Terms

 cathode ray tube (CRT): a vacuum tube used to 
create images in devices such as older television 
and computer monitors.

 commodities: consumer products, physical arti-
cles of trade or commerce.

 Environmental Protection Agency (EPA): US gov-
ernment agency tasked with combating environ-
mental pollution.

 heavy metal: one of several toxic natural sub-
stances often used as components in electronic 
devices.

 Phonebloks: a concept devised by Dutch designer 
Dave Hakkens for a modular mobile phone in-
tended to reduce electronic waste.

 planned obsolescence: a design concept in which 
consumer products are given an artificially limited 
lifespan, therefore creating a perpetual market.
Technology and Waste
One of the defining aspects of the twentieth cen-
tury was technological advancement. Telephones, 
televisions, computers, and other electronic devices 
evolved rapidly and became common throughout 
the world. This growth continued and even acceler-
ated into the twenty-first century, allowing for easier 
communication and enabling humanity to see itself 
in a larger, more global context.
However, these advancements have come at a cost. 
New communication technologies produce an ever-
expanding mass of electronic waste, or e-waste, as 
electronics break or become obsolete. The immense 
volume of electronic devices compounds traditional 
waste-disposal challenges. Many such products 
present serious pollution problems, especially in the 
developing world. Though many groups have noted 
and taken action on the issue, e-waste is a global 
problem that requires widespread cooperation from 
the individual to the government level.
What Is E-waste?
Simply put, e-waste is anything electronic that has 
been discarded. This includes common consumer 
items such as televisions, computers, microwave 
ovens, and mobile phones. Some experts further cat-
egorize e-waste into three groups: unwanted items 
that still work or can be repaired, items that can be 
harvested for scrap materials, and true waste to be 
dumped. The first two types are considered com-
modities that may be bought and sold for recycling 
purposes.
E-waste is a major problem for several reasons. 
Many electronics use parts made from toxic sub-
stances, especially heavy metals. If items are disposed 
of improperly, these substances can leak into the 
environment, causing widespread damage. For ex-
ample, lead used in cathode ray tubes (CRTs) can 
contaminate water and poison people and animals. 
Even the recycling process itself creates pollution, 
as electronics are exposed to chemicals or burned 
in order to separate valuable elements such as gold 
and copper. For this reason, various environmental 
organizations consider some electronics, such as 
CRTs, to be household hazardous waste. The US 
Environmental Protection Agency (EPA) has spe-
cial regulations for disposing of such materials. 
However, most e-waste is shipped to recycling cen-
ters and dumps in developing nations. Many of these 
countries lack strong environmental regulations, 
and e-waste pollution has become a major problem 
for them.
One reason for the huge amount of e-waste is the 
popularity of planned obsolescence in the technology 
industry. In sales-driven industries, product durability 
eventually becomes a liability. Companies can earn 
greater profits by using the cheapest possible mate-
rials and enticing consumers to upgrade as often as 
possible, thus generating more and more e-waste. 
The rapid evolution and improvement of computer 

128
Electronic waste
Principles of Computer Science
technology also drives this process. Laptops are a 
good example of planned obsolescence. Laptops are 
not built as durably as is possible, and it is often easier 
to buy a new one than to fix a broken component. 
Even if an older laptop works fine, it soon becomes 
incompatible with the latest software.
Potential Solutions
Cell phones are a notable example of e-waste. They 
are increasingly vital to everyday life in most soci-
eties, yet models have relatively short lifespans. Like 
laptops, they have many parts that can easily break 
and are not easily replaced, requiring the purchase 
of a new device. Both production and dis-
posal of cell phones cause significant envi-
ronmental problems.
Various methods have been proposed 
to address the issue. Dutch designer Dave 
Hakkens created the Phonebloks con-
cept to combat planned obsolescence. 
Phonebloks features a modular design for 
mobile phones, with interlocking parts 
connected on a frame. If, for example, the 
camera breaks, a new camera block can be 
installed, rather than replacing the whole 
phone. Consumers can upgrade and cus-
tomize their phones as they choose. The 
theory behind Phonebloks attracted corpo-
rate and public interest, bringing attention 
to the problem of e-waste. However, some 
critics suggested the idea would be very dif-
ficult to implement. It could also potentially 
end up increasing e-waste through constant 
upgrading.
Public awareness is critical to reducing 
waste. Many people do not realize their 
electronics are such a problem after being 
thrown away. The United Nations (UN) 
founded the Step (Solving the E-waste 
Problem) Initiative to coordinate anti-waste 
efforts at all levels. Others have looked into 
ways to tackle e-waste already in the environ-
ment. Bioremediation encompasses several 
methods of using organisms to metabolize 
waste and transform it into less hazardous 
forms. One method, vermiremediation, 
uses earthworms to produce enzymes that 
can absorb and biodegrade hazardous ma-
terials. Phytoremediation uses plants to 
help break down toxins in various ways. 
Bioremediation can be effective, but it has limita-
tions, such as the amount of time required.
Impact of Electronic Waste
The most obvious impact of e-waste is due to its sheer 
volume. In 2014 alone roughly 41.8 million metric 
tons of e-waste were generated globally. The EPA es-
timates that the amount of e-waste grows by 5 to 10 
percent each year. This volume has turned e-waste 
disposal into a major industry, especially in the de-
veloping world. The city of Guiyu in southern China 
is a focal point for used mobile phones, computers, 
As technology speeds forward, older products become obsolete more quickly. 
Electronic waste (e-waste) materials are not easily degradable, and the amount 
of waste accumulating in landfills is increasing. By Volker Thies, CC-BY-SA-3.0 
(http://creativecommons.org/licenses/by-sa/3.0/), via Wikimedia Commons

129
Principles of Computer Science
Electronic waste
and anything else that contains semiconductors and 
microchips. Africa also has its share of digital land-
fills. One of these is on the outskirts of Accra, Ghana. 
The concentrated pollution in these areas is a global 
health threat.
One other complication that arises from e-waste 
is the problem of identity theft. Many discarded 
computers and phones still contain personal infor-
mation. This can be collected by criminals working 
in recycling centers. According to the US State 
Department, e-waste is a significant factor in the in-
crease in cybercrime.
—Andrew Farrell
Bibliography
Baldé, C. P., et al. E-waste Statistics: Guidelines on 
Classification, Reporting and Indicators, 2015. Bonn: 
United Nations U, 2015. United Nations University. 
Web. 9 Feb. 2016.
“Ghana: Digital Dumping Ground.” Frontline. PBS, 23 
June 2009. Web. 29 Jan. 2016.
Glaubitz, John Paul Adrian. “Modern Consumerism 
and the Waste Problem.” ArXiv.org. Cornell U, 4 
June 2012. Web. 9 Feb. 2016.
McNicoll, Arion. “Phonebloks: The Smartphone for 
the Rest of Your Life.” CNN. Cable News Network, 
19 Sept. 2013. Web. 29 Jan. 2016.
Mooallem, Jon. “The Afterlife of Cellphones.” New 
York Times Magazine. New York Times, 13 Jan. 2008. 
Web. 9 Feb. 2016.
Patel, Shuchi, and Avani Kasture. “E (Electronic) 
Waste Management Using Biological Systems—
Overview.” 
International 
Journal 
of 
Current 
Microbiology and Applied Sciences 3.7 (2014): 495–
504. Web. 9 Feb. 2016.
“Planned 
Obsolescence: 
A 
Weapon 
of 
Mass 
Discarding, or a Catalyst for Progress?” ParisTech 
Review. ParisTech Rev., 27 Sept. 2013. Web. 9 Feb. 
2016.
“What Is E-waste?” Step: Solving the E-waste Problem. 
United Nations U/Step Initiative, 2016. Web. 9 
Feb. 2016.

130
Encryption
Principles of Computer Science
Encryption
Fields of Study
Security; Privacy; Algorithms
Abstract
Encryption is the encoding of information so that 
only those who have access to a password or encryp-
tion key can access it. Encryption protects data con-
tent, rather than preventing unauthorized intercep-
tion of or access to data transmissions. It is used by 
intelligence and security organizations and in per-
sonal security software designed to protect user data.
Prinicipal Terms

 asymmetric-key encryption: a process in which data 
is encrypted using a public encryption key but can 
only be decrypted using a different, private key.

 authentication: the process by which the receiver 
of encrypted data can verify the identity of the 
sender or the authenticity of the data.

 hashing algorithm: a computing function that con-
verts a string of characters into a different, usu-
ally smaller string of characters of a given length, 
which is ideally impossible to replicate without 
knowing both the original data and the algorithm 
used.

 Pretty Good Privacy: a data encryption program 
created in 1991 that provides both encryption and 
authentication.
Cryptography and Encryption
Encryption is a process in which data is translated 
into code that can only by read by a person with the 
correct encryption key. It focuses on protecting data 
content rather than preventing unauthorized inter-
ception. Encryption is essential in intelligence and 
national security and is also common in commercial 
applications. Various software programs are available 
that allow users to encrypt personal data and digital 
messages.
The study of different encryption techniques is 
called “cryptography.” The original, unencrypted 
data is called the “plaintext.” Encryption uses an al-
gorithm called a “cipher” to convert plaintext into 
ciphertext. The ciphertext can then be deciphered 
by using another algorithm known as the “decryption 
key” or “cipher key.”
Types of Encryption
A key is a string of characters applied to the plaintext 
to convert it to ciphertext, or vice versa. Depending 
on the keys used, encryption may be either symmetric 
or asymmetric. Symmetric-key encryption uses the 
same key for both encoding and decoding. The key 
used to encode and decode the data must be kept 
secret, as anyone with access to the key can translate 
the ciphertext into plaintext. The oldest known cryp-
tography systems used alphanumeric substitution al-
gorithms, which are a type of symmetric encryption. 
Symmetric-key algorithms are simple to create but 
vulnerable to interception.
In asymmetric-key encryption, the sender and 
receiver use different but related keys. First, the re-
ceiver uses an algorithm to generate two keys, one 
to encrypt the data and another to decrypt it. The 
encryption key, also called the “public key,” is made 
available to anyone who wishes to send the receiver 
a message. (For this reason, asymmetric-key encryp-
tion is also known as “public-key encryption.”) The 
decryption key, or private key, remains known only to 
the receiver. It is also possible to encrypt data using 
the private key and decrypt it using the public key. 
However, the same key cannot be used to both en-
crypt and decrypt.
Asymmetric-key encryption works because the 
mathematical algorithms used to create the public 
and private keys are so complex that it is computa-
tionally impractical determine the private key based 
on the public key. This complexity also means that 
asymmetric encryption is slower and requires more 
processing power. First developed in the 1970s, asym-
metric encryption is the standard form of encryption 
used to protect Internet data transmission.
Authentication and Security
Authentication is the process of verifying the iden-
tity of a sender or the authenticity of the data sent. A 
common method of authentication is a hashing algo-
rithm, which translates a string of data into a fixed-
length number sequence known as a “hash value.” 
This value can be reverted to the original data using 

131
Principles of Computer Science
Encryption
This is clear
With no encryption,
this text can be read by
anyone who knows the
language.
By using an encryption code,
the text becomes ciphered and
can only be understood if you
know the code. If you apply
the code to the ciphertext,
you can decrypt it and read
the cleartext. But it is still
easier to crack than hashed
text because the ciphertext is
the same amount of data as
the cleartext.
Cleartext
Output same
size as input
Ziol ol estqk
Vozi fg tfeknhzogf,
ziol ztbz eqf wt ktqr wn
qfngft vig afgvl zit
sqfuxqut.
Ciphertext
Encrypt
Decrypt
Encrypt
Decrypt
Encrypt
Decrypt
Wn xlofu qf tfeknhzogf egrt,
zit ztbz wtegdtl eohitktr qfr
eqf gfsn wt xfrtklzggr oy ngx
afgv zit egrt. Oy ngx qhhsn
zit egrt zg zit eohitkztbz,
ngx egf rteknhz oz qfr ktqr
zit estqkztbz. Wxz oz ol lzoss
tqlotk zg ekqea ziqf iqlitr
ztbz wteqxlt zit eohitkztbz ol
zit lqdt qdgxfz gy rqzq ql
zit estqkztbz.
This is clear
With no encryption,
this text can be read by
anyone who knows the
language.
By using an encryption code,
the text becomes ciphered and
can only be understood if you
know the code. If you apply
the code to the ciphertext,
you can decrypt it and read
the cleartext. But it is still
easier to crack than hashed
text because the ciphertext is
the same amount of data as
the cleartext.
Cleartext
Encrypt
Decrypt
Hashed
imgr578m3drff09k76s3d76q1ai98ln6
pp0o87y5f433dvunt78ik3ws9i8u7n54
p00o9m7yfkr644dv87m4s9kii6g3a23g
This diagram illustrates the output of encrypted content versus hashed content. 
When text is encrypted, the output will be the same size as the input, and it can be 
decrypted to show the original input. When text is hashed, input of any size will 
shrink to an output of a predetermined size, commonly 128 bits. The output cannot 
be decrypted, only authenticated by comparing it with a known hash value. EBSCO 
illustration.

132
Encryption
Principles of Computer Science
the same algorithm. The mathematical complexity 
of hashing algorithms makes it extremely difficult to 
decrypt hashed data without knowing the exact algo-
rithm used. For example, a 128-bit hashing algorithm 
can generate 2128 different possible hash values.
For the purpose of authenticating sent data, such 
as a message, the sender may first convert the data 
into a hash value. This value, also called a “message 
digest,” may then be encrypted using a private key 
unique to the sender. This creates a digital signature 
that verifies the authenticity of the message and the 
identity of the sender. The original unhashed mes-
sage is then encrypted using the public key that cor-
responds to the receiver’s private key. Both the pri-
vately encrypted digest and the publicly encrypted 
message are sent to the receiver, who decrypts the 
original message using their private key and decrypts 
the message digest using the sender’s public key. 
The receiver then hashes the original message using 
the same algorithm as the sender. If the message is 
authentic, the decrypted digest and the new digest 
should match.
Encryption Systems in Practice
One of the most commonly used encryption pro-
grams is Pretty Good Privacy (PGP). It was developed 
in 1991 and combines symmetric- and asymmetric-
key encryption. The original message is encrypted 
using a unique one-time-only private key called a “ses-
sion key.” The session key is then encrypted using the 
receiver’s public key, so that it can only be decrypted 
using the receiver’s private key. This encrypted key 
is sent to the receiver along with the encrypted mes-
sage. The receiver uses their private key to decrypt 
the session key, which can then can be used to de-
crypt the message. For added security and authenti-
cation, PGP also uses a digital signature system that 
compares the decrypted message against a message 
digest. The PGP system is one of the standards in 
personal and corporate security and is highly resis-
tant to attack. The data security company Symantec 
acquired PGP in 2010 and has since incorporated the 
software into many of its encryption programs.
Encryption can be based on either hardware or 
software. Most modern encryption systems are based 
on software programs that can be installed on a 
system to protect data contained in or produced by a 
variety of other programs. Encryption based on hard-
ware is less vulnerable to outside attack. Some hard-
ware devices, such as self-encrypting drives (SEDs), 
come with built-in hardware encryption and are 
useful for high-security data. However, hardware en-
cryption is less flexible and can be prohibitively costly 
to implement on a wide scale. Essentially, software 
encryption tends to be more flexible and widely us-
able, while hardware encryption is more secure and 
may be more efficient for high-security systems.
—Micah L. Issitt
Bibliography
Bright, Peter. “Locking the Bad Guys Out with 
Asymmetric Encryption.” Ars Technica. Condé 
Nast, 12 Feb. 2013. Web. 23 Feb. 2016.
Delfs, Hans, and Helmut Knebl. Introduction to 
Cryptography: Principles and Applications. 3rd ed. 
Berlin: Springer, 2015. Print.
History of Cryptography: An Easy to Understand History 
of Cryptography. N.p.: Thawte, 2013. Thawte. Web. 
4 Feb. 2016.
“An Introduction to Public Key Cryptography 
and PGP.” Surveillance Self-Defense. Electronic 
Frontier Foundation, 7 Nov. 2014. Web. 4 Feb. 
2016.
Lackey, Ella Deon, et al. “Introduction to Public-Key 
Cryptography.” Mozilla Developer Network. Mozilla, 
21 Mar. 2015. Web. 4 Feb. 2016.
McDonald, Nicholas G. “Past, Present, and 
Future Methods of Cryptography and Data 
Encryption.” SpaceStation. U of Utah, 2009. 
Web. 4 Feb. 2016.

133
Firewalls
Fields of Study
Information Systems; Privacy; Security
Abstract
A firewall is a program designed to monitor the traffic 
entering and leaving a computer network or single 
device and prevent malicious programs or users from 
entering the protected system. Firewalls may protect 
a single device, such as a server or personal computer 
(PC), or even an entire computer network. They also 
differ in how they filter data. Firewalls are used along-
side other computer security measures to protect sen-
sitive data.
Prinicipal Terms

 application-level firewalls: firewalls that serve as 
proxy servers through which all traffic to and from 
applications must flow.

 host-based firewalls: firewalls that protect a spe-
cific device, such as a server or personal computer, 
rather than the network as a whole.

 network firewalls: firewalls that protect an entire 
network rather than a specific device.

 packet filters: filters that allow data packets to 
enter a network or block them on an individual 
basis.

 proxy server: a computer through which all traffic 
flows before reaching the user’s computer.

 stateful filters: filters that assess the state of a con-
nection and allow or disallow data transfers ac-
cordingly.
History of Firewalls
In the early twenty-first century, increasing cyber-
crime and cyberterrorism made computer security 
a serious concern for governments, businesses and 
organizations, and the public. Nearly any computer 
system connected to the Internet can be accessed by 
malicious users or infected by harmful programs such 
as viruses. Both large networks and single PCs face 
this risk. To prevent such security breaches, organi-
zations and individuals use various security technolo-
gies, particularly firewalls. Firewalls are programs or 
sometimes dedicated devices that monitor the data 
entering a system and prevent unwanted data from 
doing so. This protects the computer from both mali-
cious programs and unauthorized access.
The term “firewall” is borrowed from the field 
of building safety. In that field it refers to a wall spe-
cially built to stop the spread of fire within a struc-
ture. Computer firewalls fill a similar role, preventing 
harmful elements from entering the protected area. 
The idea of computer firewalls originated in the 
1980s. At that time, network administrators used 
routers, devices that transfer data between net-
works, to separate one network from another. This 
stopped problems in one network from spreading 
into others. By the early 1990s, the proliferation of 
computer viruses and increased risk of hacking made 
the widespread need for firewalls clear. Some of the 
advances in that era, such as increased access to the 
Internet and developments in operating systems, also 
introduced new vulnerabilities. Early firewalls relied 
heavily on the use of proxy servers. Proxy servers are 
servers through which all traffic flows before entering 
a user’s computer or network. In the twenty-first cen-
tury, firewalls can filter data according to varied cri-
teria and protect a network at multiple points.
Types of Firewalls
All firewalls work to prevent unwanted data from en-
tering a computer or network. However, they do so 
in different ways. Commonly used firewalls can be 
in various positions relative to the rest of the system. 
F

134
Firewalls
Principles of Computer Science
An individual computer may have its own personal 
fi rewall, such as Windows Firewall or Macintosh OS 
X’s built-in fi rewall. Other networked devices, such 
as servers, may also have personal fi rewalls. These are 
known as host-based fi rewalls because they protect a 
single host rather than the whole network. They pro-
tect computers and other devices not only from mali-
cious programs or users on the Internet but also from 
viruses and other threats that have already infi ltrated 
the internal network, such as a corporate intranet, to 
which they belong. Network fi rewalls, on the other 
hand, are positioned at the entrance to the internal 
network. All traffi c into or out of that network must 
fi lter through them. A network fi rewall may be a 
single device, such as a router or dedicated com-
puter, which serves as the entrance point for all data. 
It then blocks any data that is malicious or otherwise 
unwanted. Application-level fi rewalls, which monitor 
and allow or disallow data transfers from and to appli-
cations, may be host based or network based.
INTERNET
Router
Router
Router
Switch
Switch
Firewall
Firewall
Firewall
Server
Firewalls are one of many protective measures used to prevent hackers from accessing computers or networks. EBSCO illustration.

135
Principles of Computer Science
Firewalls
Firewalls also vary based on how they filter data. 
Packet filters examine incoming data packets indi-
vidually and determine whether to block or allow 
each one to proceed. They decide this based on 
factors such as the origin and destination of the 
packets. Stateful filters determine whether to admit 
or block incoming data based on the state of the 
connection. Firewalls that use stateful filtering can 
identify whether data packets trying to enter the 
computer system are part of an ongoing, active con-
nection and determine whether to let them in based 
on that context. This allows them to examine and 
filter incoming data more quickly than their stateless 
counterparts.
Firewalls and Computer Security
By preventing malicious programs or users from ac-
cessing systems, firewalls protect sensitive data stored 
in or transmitted via computers. They are used to 
protect personally identifying information, such as 
Social Security numbers, as well as proprietary trade 
or government information. Both the technology in-
dustry and the public have put increased emphasis 
on such protections in the early twenty-first century, 
as identity theft, fraud, and other cybercrimes have 
become major issues. In light of such threats, fire-
walls play an essential role in the field of computer 
security. However, experts caution that a firewall 
should not be the sole security measure used. Rather, 
firewalls should be used along with other computer 
security practices. These practices include using se-
cure passwords, regularly updating software to install 
patches and eliminate vulnerabilities, and avoiding 
accessing compromised websites or downloading 
files from suspicious sources.
—Joy Crelin
Bibliography
“How Firewalls Work.” Boston University Information 
Services and Technology. Boston U, n.d. Web. 28 Feb. 
2016.
Ingham, Kenneth, and Stephanie Forrest. A History 
and Survey of Network Firewalls. Albuquerque: U of 
New Mexico, 2002. PDF file.
Morreale, Patricia, and Kornel Terplan, eds. The CRC 
Handbook of Modern Telecommunications. 2nd ed. 
Boca Raton: CRC, 2009. Print.4 Science Reference 
Center™ Firewalls
Northrup, Tony. “Firewalls.” TechNet. Microsoft, n.d. 
Web. 28 Feb. 2016.
Stallings, William, and Lawrie Brown. Computer 
Security: Principles and Practice. 3rd ed. Boston: 
Pearson, 2015. Print.
Vacca, John, ed. Network and System Security. 2nd ed. 
Waltham: Elsevier, 2014. Print.

136
Firmware
Principles of Computer Science
Firmware
Fields of Study
Embedded Systems; Software Engineering; System-
Level Programming
Abstract
Firmware occupies a position in between hardware, 
which is fixed and physically unchanging, and soft-
ware, which has no physical form apart from the 
media it is stored on. Firmware is stored in nonvola-
tile memory in a computer or device so that it is al-
ways available when the device is powered on. An ex-
ample can be seen in the firmware of a digital watch, 
which remains in place even when the battery is re-
moved and later replaced.
Prinicipal Terms

 embedded systems: computer systems that are in-
corporated into larger devices or systems to mon-
itor performance or to regulate system functions.

 flashing: a process by which the flash memory on 
a motherboard or an embedded system is updated 
with a newer version of software.

 free software: software developed by program-
mers for their own use or for public use and dis-
tributed without charge; it usually has conditions 
attached that prevent others from acquiring it and 
then selling it for their own profit.

 homebrew: software that is developed for a device 
or platform by individuals not affiliated with the 
device manufacturer; it is an unofficial or “home-
made” version of the software that is developed to 
provide additional functionality not included or 
not permitted by the manufacturer.

 nonvolatile memory: computer storage that re-
tains its contents after power to the system is cut 
off, rather than memory that is erased at system 
shutdown.
Overview of Firmware
Many consumer devices have become so complex 
that they need a basic computer to operate them. 
However, they do not need a fully featured computer 
with an operating system (OS) and specially designed 
software. The answer to this need is to use embedded 
systems. These systems are installed on microchips 
inside devices as simple as children’s toys and as 
complex as medical devices such as digital thermom-
eters. The term “embedded” is used because the 
chips containing firmware are ordinarily not directly 
accessible to consumers. They are installed within the 
device or system and expected to work throughout its 
lifespan.
Computers also use firmware, which is called the 
“basic input/output system,” or BIOS. Even though 
the computer has its own OS installed and numerous 
programs to accomplish more specific tasks, there 
is still a need for firmware. This is because, when the 
computer is powered on, some part of it must be imme-
diately able to tell the system what to do in order to set 
itself up. The computer must be told to check the part 
of the hard drive that contains the start-up sequence, 
then to load the OS, and so on. The firmware serves 
this purpose because, as soon as electric current flows 
into the system, the information stored in the com-
puter’s nonvolatile memory is loaded and its instruc-
tions are executed. Firmware is usually unaffected even 
when a different OS is installed. However, the user can 
also configure the BIOS to some extent and can boot 
the computer into the BIOS to make changes when 
necessary. For example, a computer that is configured 
to boot from the CD-ROM drive first could have this 
changed in the BIOS so that it would first attempt to 
read information from an attached USB drive.
Modifying and Replacing Firmware
Sophisticated users of technology sometimes find 
that the firmware installed by a manufacturer does 
not meet all of their needs. When this occurs, it is 
possible to update the BIOS through a process 
known as flashing. When the firmware is flashed, it is 
replaced by a new version, usually with new capabili-
ties. In some cases, the firmware is flashed because 
the device manufacturer has updated it with a new 
version. This is rarely done, as firmware function-
ality is so basic to the operation of the device that it is 
thoroughly tested prior to release. From time to time, 
however, security vulnerabilities or other software 
bugs are found in firmware. Manufacturers helping 
customers with troubleshooting often recommend 
using the latest firmware to rule out such defects.
Some devices, especially gaming consoles, have 
user communities that can create their own versions 
of firmware. These user-developed firmware versions 

137
Principles of Computer Science
Firmware
are referred to as homebrew software, as they are 
produced by users rather than manufacturers. 
Homebrew firmware is usually distributed on the 
Internet as free software, or freeware, so that anyone 
can download it and flash their device. In the case of 
gaming consoles, this can open up new capabilities. 
Manufacturers tend to produce devices only for spe-
cialized functions. They exclude other functions be-
cause the functions would increase the cost or make 
it too easy to use the device for illegal or undesirable 
purposes. Flashing such devices with homebrew soft-
ware can make these functions available.
Automobile Software
One of the market segments that has become in-
creasingly reliant on firmware is automobile manu-
facturing. More and more functions in cars are now 
controlled by firmware. Not only are the speedom-
eter and fuel gauge computer displays driven by firm-
ware, but cars come with firmware applications for 
music players, real-time navigation and map display, 
and interfaces with passengers’ cell phones.
Firmware as Vulnerability
Although firmware is not very visible to users, it has 
still been a topic of concern for computer security 
professionals. With homebrew firmware distributed 
over the Internet, the concern is that the firmware 
may contain “backdoors.” A backdoor is a secret 
means of conveying the user’s personal information 
to unauthorized parties. Even with firmware from of-
ficial sources, some worry that it would be possible 
for the government or the device manufacturer to 
include security vulnerabilities, whether deliberate or 
inadvertent.
—Scott Zimmer, JD
Bibliography
Bembenik, Robert, Łukasz Skonieczny, Henryk 
Rybinn´ski, Marzena Kryszkiewicz, and Marek 
Niezgódka, eds. Intelligent Tools for Building a 
Scientific Information Platform: Advanced Architectures 
and Solutions. New York: Springer, 2013. Print.
Dice, Pete. Quick Boot: A Guide for Embedded Firmware 
Developers. Hillsboro: Intel, 2012. Print.
Iniewski, Krzysztof. Embedded Systems: Hardware, Design, 
and Implementation. Hoboken: Wiley, 2012. Print.
Khan, Gul N., and Krzysztof Iniewski, eds. Embedded 
and Networking Systems: Design, Software, and 
Implementation. Boca Raton: CRC, 2014. Print.
Noergaard, Tammy. Embedded Systems Architecture: A 
Comprehensive Guide for Engineers and Programmers. 
2nd ed. Boston: Elsevier, 2013. Print.
Sun, Jiming, Vincent Zimmer, Marc Jones, and Stefan 
Reinauer. Embedded Firmware Solutions: Development 
Best Practices for the Internet of Things. Berkeley: 
ApressOpen, 2015. Print.

138
Fitbit
Principles of Computer Science
Fitbit
Field of Study
Computer Science
Abstract
The term Fitbit refers to activity tracker devices 
manufactured and sold by Fitbit Incorporated. 
Designed to help users take control of their health 
and fitness, Fitbit devices track data such as step 
counts and calories burned, among other func-
tions. Following their introduction to the market 
in 2009, Fitbit devices popularized wearable ac-
tivity trackers among the public, prompting the 
creation of a wide variety of competing fitness 
devices.
Prinicipal Terms

 accelerometer IC: an integrated circuit that mea-
sures acceleration.

 low-energy connectivity IC: an integrated circuit 
that enables wireless Bluetooth connectivity while 
using little power.

 microcontroller: an integrated circuit that con-
tains a very small computer.

 near-field communications antenna: an antenna 
that enables a device to communicate wirelessly 
with a nearby compatible device.

 printed circuit board (PCB): a component of elec-
tronic devices that houses and connects many 
smaller components.

 vibrator: an electronic component that vibrates.
History of Fitbit
In the first decade of the twenty-first century, growing 
interest in health and fitness converged with signifi-
cant advances in consumer technology. This lead 
to the increasing popularity of wearable electronic 
devices. The personal activity tracker is essentially 
an upgraded version of earlier pedometers (step 
counters). Activity trackers record and measure fit-
ness data. Newer models can sync seamlessly with the 
user’s computer or smartphone. One such family of 
devices, sold under the brand name Fitbit, was intro-
duced in 2009. Different Fitbit products allow users 
to track various fitness-related data.
Fitbit Incorporated was founded in 2007 by James 
Park and Eric Friedman as Healthy Metrics Research 
Incorporated. The company’s initial goal was to in-
corporate accelerometers into wearable technology. 
Accelerometers are small sensors that measure ac-
celeration forces. The founders presented a pro-
totype consisting of a circuit board within a small 
balsawood box to investors in 2008. The company’s 
first device, known as the Fitbit Tracker, was released 
to the public in 2009. The original Fitbit clipped to 
the wearer’s clothing. The small device could count 
and track the number of steps taken and estimate re-
leased numerous additional Fitbit devices featuring 
different functions.
Types of Fitbit Devices
The Fitbit Tracker could track the number of 
steps the user took per day and estimate how many 
calories such activity burned. The device also 
tracked the user’s sleep duration and quality. The 
Fitbit Tracker could be worn clipped to the user’s 
clothing during the day. It also came with a wrist-
band with which to wear the device at night. The 
device was paired with a website where users could 
document their exercise routines and meals and re-
cord their weight. It also came with a charging sta-
tion that enabled users to wirelessly upload the data 
collected by their Fitbits.
Over the following years, Fitbit introduced more 
than ten other devices with a variety of different func-
tions. Its products are divided among three product 
lines: everyday, active, and performance fitness. 
Devices such as the Fitbit One and the Fitbit Charge be-
long to the everyday fitness category. They track the us-
er’s minutes of activity and number of floors climbed, 
among other data. Some devices, including the Fitbit 
Charge and the Fitbit Alta, feature clock displays and 
can connect to the user’s smartphone to display caller 
information. The active fitness product category in-
cludes the Fitbit Charge HR and Blaze. The Charge 
HR adds heart rate tracking to the Fitbit Charge’s fea-
tures. The Fitbit Blaze features the standard activity-
tracking features as well as music controls and phone 
notifications. The Fitbit Surge, a performance fitness 
device, can track the user’s location via GPS, among 
other features. Fitbit also manufactures the Aria smart 
scale. Aria tracks the user’s weight, body mass index, 
and other data and syncs with both the user’s activity 
tracker and the Fitbit website.

139
Principles of Computer Science
Fitbit
Inside Fitbit Devices
As multifunctional wearable devices, Fitbit activity 
trackers require that numerous components fit into 
a limited amount of space. The Fitbit Flex wristband-
style device, for example, contains a variety of small 
electronic components, including several integrated 
circuits (ICs). An IC is a silicon microchip that can 
perform a specialized task, such as handling memory. 
One core component of the Fitbit Flex is the three-
axis accelerometer IC, which measures acceleration 
forces to track the user’s movement. The device also 
contains a printed circuit board (PCB). The PCB 
houses and connects multiple smaller components. 
It is controlled by a microcontroller, a chip that is es-
sentially a tiny computer, complete with memory and 
a processor. As the device has a vibrating silent alarm 
function, the Fitbit Flex likewise has a vibrator com-
ponent. For connectivity purposes, the device contains 
a low-energy connectivity IC, which enables wireless 
Bluetooth connectivity. It also has a near-field commu-
nications (NFC) antenna. The NFC antenna enables 
the device to communicate wirelessly with other NFC-
compatible devices such as certain smartphones.
The specific components used in a 
Fitbit product vary based on the device 
in question. For example, the Fitbit 
Surge differs greatly from the clip- or 
wristband-based devices in terms of 
composition. However, the compo-
nents in use generally have a number 
of common characteristics, including 
small size and very low power usage.
Impact and Challenges
The popularity of the early Fitbit de-
vices spurred other companies’ ef-
forts to create similar devices. Some 
of these companies had been working 
on activity trackers prior to the ad-
vent of Fitbit. Some other activity-
tracking wearable devices include 
Nike+ products, Jawbone UP bands, 
and the Apple Watch, which has some 
activity-tracking capabilities.
However, this popularity has also 
raised concerns regarding user pri-
vacy. The activity data gathered by such 
devices, and particularly GPS data, 
could potentially be used for harm 
if accessed by individuals with mali-
cious intent. The safety and accuracy of Fitbit devices 
have also been called into question. A 2014 class-
action lawsuit alleged that the Fitbit Force’s wrist-
band caused users to develop rashes on their wrists. 
Another lawsuit filed in January 2016 argued that the 
company had misled consumers about the accuracy 
of the heart rate data gathered by the Fitbit Surge 
and Fitbit Charge HR.
—Joy Crelin
Bibliography
Case, Meredith A., et al. “Accuracy of Smartphone 
Applications and Wearable Devices for Tracking 
Physical Activity Data.” Journal of the American 
Medical Association 313.6 (2015): 625–26. Web. 2 
Mar. 2016.
Comstock, Jonah. “Eight Years of Fitbit News Leading 
Up to Its Planned IPO.” MobiHealthNews. HIMSS 
Media, 11 May 2015. Web. 28 Feb. 2016.
“Fitbit Flex Teardown.” iFixit. iFixit, 2013. Web. 28 
Feb. 2016.
Activity trackers include a number of sensors and outputs that allow users to mon-
itor health-related data such as steps taken, heart rate, sleeping patterns, and more. 
EBSCO illustration.

140
FORTRAN
Principles of Computer Science
Goode, Lauren. “Fitbit Hit with Class-Action Suit 
over Inaccurate Heart Rate Monitoring.” Verge. 
Vox Media, 6 Jan. 2016. Web. 28 Feb. 2016.
Hof, Robert. “How Fitbit Survived as a Hardware 
Startup.” Forbes. Forbes.com, 4 Feb. 2014. Web. 28 
Feb. 2016.
Mangan, Dan. “There’s a Hack for That: Fitbit User 
Accounts Attacked.” CNBC. CNBC, 8 Jan. 2016. 
Web. 28 Feb. 2016.
Marshall, Gary. “The Story of Fitbit: How a Wooden 
Box Became a $4 Billion Company.” Wareable. 
Wareable, 30 Dec. 2015. Web. 28 Feb. 2016.
Stevens, Tim. “Fitbit Review.” Engadget. AOL, 15 Oct. 
2009. Web. 28 Feb. 2016.
FORTRAN
Fields of Study
Programming Languages; Software Engineering;
Abstract
Fortran is the oldest programming still in common 
use, and is used primarily for scientific and engi-
neering computations requiring complex math-
ematical calculations. Fortran has been continually 
revised since its development at IBM in the 1950s, 
and is used presently on “supercomputers” with par-
allel architecture. It is a robust language that is rela-
tively easy to learn, and Fortran programs adhere to a 
strictly logical format.
Prinicipal Terms

 algorithm: a set of step-by-step instructions for per-
forming computations. 

 character: a unit of information that repre-
sents a single letter, number, punctuation mark, 
blank space, or other symbol used in written 
language. 

 function: instructions read by a computer’s pro-
cessor to execute specific events or operations. 

 object-oriented programming: a type of program-
ming in which the source code is organized into 
objects, which are elements with a unique iden-
tity that have a defined set of attributes and be-
haviors. 

 internal-use software: software developed by a 
company for its own use to support general and 
administrative functions, such as payroll, ac-
counting, or personnel management and mainte-
nance. 

 main loop: the overarching process being carried 
out by a computer program, which may then in-
voke subprocesses. 

 programming languages: sets of terms and rules of 
syntax used by computer programmers to create 
instructions for computers to follow. This code is 
then compiled into binary instructions for a com-
puter to execute. 

 syntax: rules that describe how to correctly struc-
ture the symbols that comprise a language.
FORTRAN History and Development
The FORTRAN programming language was devel-
oped by John Backus, working at IBM, between 1954 
and 1957. The name is a contraction taken from 
FORmula TRANslator, a reflection of the develop-
ment of the language for use in scientific and engi-
neering applications requiring complex mathemat-
ical functions. The BASIC programming language, 
developed in the 1960s, was strongly influenced by 
the Fortran programming language in regard to 
program structure, being essentially linear with the 
ability to use contained subroutines. Fortran has been 
developed through several versions into the 21st cen-
tury, including Fortran III, Fortran IV, Fortran 66, 
Fortran 77, Fortran 90, Fortran 95 and Fortran 2003, 
and is the oldest of programming languages still in 
common use. Fortran 90 and later versions are used 
on present-day advanced parallel computers, which 
are sometimes referred to as “supercomputers,” 
and it is notable that Fortran 2003 supports object-
oriented programming. The version numbers gener-
ally refer to the year in which the updated version 
of Fortran was released for use. Fortran is a versatile 
and powerful imperative programming language 

141
Principles of Computer Science
FORTRAN
that supports numerical analysis and scientific calcu-
lation, as well as structured, array, modular, generic 
and concurrent programming. Given the specialized 
fields of science and technology in which the Fortran 
language is applied, much of the programs written in 
that language are for internal-use software in which 
each function may be defined individually within a 
specific algorithm.
Program Structure
Like all programming languages, Fortran has its own 
syntax, or method in which program commands and 
statements must be written in order to be “under-
stood” and carried out. A Fortran program is made 
up of subroutines and function segments that can 
be compiled independently. Subroutines are termed 
“external procedures,” and it is characteristic of both 
Fortran and ALGOL that procedures are only in-
voked as a result of explicit calls from within the main 
loop of the program or from another procedure 
within it. Each segment of the program has local 
data written into the source code, but additional data 
can be made available to more than one segment by 
storing it in blocks of code labeled COMMON. Each 
Fortran program begins with a statement of the name 
of the program, followed by the statement “implicit 
none,” and ends with an “end program” statement 
for that program, as
program program_name
implicit none
.
.
.
end program program_name
Documentation phrases are added into a Fortran 
program using the ! character at the beginning of a 
line. Everything after the ! in that line is ignored by 
the Fortran compiler, allowing the programmer to 
describe what each program segment or line of code 
is intended to achieve. Following the “implicit none” 
statement, Fortran program have a block of type dec-
laration statements to identify the nature of the vari-
ables being used in the program. For example, the 
type declaration statement
real :: x, y, z
identifies the variables x, y and z as real numbers. 
A block of executable statements then follows. These 
are the functions and relations that the program is 
to calculate. The executable statement block may in-
clude, for example, statements such as
read (*,*) x 
read (*,*) y
z = (x + y)/2
print *, z
by which the user enters the two values for x and y 
from the keyboard. Their average value is calculated 
as the variable z, which is then printed to the default 
display device, usually the computer screen or an at-
tached printer.
The heading identifies the name of the program 
as “average_four.” The type declarations identify all 
of the five variables a, b, c, d and average as real num-
bers. The executable statements get the four values 
of a, b, c and d from keyboard input, calculates their 
Sample Problem
Write a Fortran program that asks for the 
input of four values and outputs their average.
Answer:
program average_four
implicit none
! this program gets four values and 
outputs their average
! type declarations
real :: a, b, c, d, average
! executable statements
	
read (*,*) a
	
read (*,*) b
	
read (*,*) c
	
read (*,*) d
	
average = (a + b + c + d)/4
	
print *, “The average value is,” 
average
end program average_four

142
FORTRAN
Principles of Computer Science
average value, and displays the result. The end of the 
program operation is identified by the “end program 
average_four” statement.
Fortran in the Present Day
The straightforward and strictly logical manner in 
which Fortran programs are written is very suitable 
for use in complex applications that call for close at-
tention to detail, such as scientific and engineering 
calculations. This would account for the longevity of 
the Fortran programming language in those areas, 
and it is used extensively on modern supercomputers 
for numerical analysis of complex systems. The ro-
bust character of the Fortran programming language 
ensures that it will continue to be used well into the 
future.
—Richard M. Renneboog M.Sc.
Bibliography
Chivers, Ian, and Jane Sleightholme. Introduction to 
Programming With Fortran, with Coverage of Fortran 
90, 95, 2003, 2008 and 77. 3rd ed., New York, NY: 
Springer, 2015. Print.
Counihan, Martin Fortran 95. Londion, UK: 
University College Press. 1996. Print.
Gehrke, Wilhelm. Fortran 90 Language Guide. New 
York, NY: Springer, 1995. Print.
Kupferschmid, Michael Classical Fortran Programming 
for Engineering and Scientific Applications. Boca 
Raton, FL: CRC Press, 2009. Print.
Metcalf, Michael, and John Reid. The F Programming 
Language. New York, NY: Oxford University Press, 
1996. Print.
O’Regan, Gerard. A Brief History of Computing. 2nd ed., 
New York, NY: Springer-Verlag. 2012. Print.
Rajaraman, V. Computer Programming in Fortran 77. 4th 
ed., New Delhi, IND: Prentice-Hall of India Pvt., 
2006. Print.
Scott, Michael L. Programming Language Pragmatics. 
4th ed., Waltham, MA: Morgan Kaufmann, 2016. 
Print.

143
Principles of Computer Science
Functional design
Functional design
Fields of Study
Software Engineering; System Analysis
Abstract
Functional design is a paradigm of computer pro-
gramming. Following functional design principles, 
computer programs are created using discrete mod-
ules that interact with one another only in very spe-
cific, limited ways. An individual module can be mod-
ified extensively with only minor impacts on other 
parts of the program.
Prinicipal Terms

 coupling: the degree to which different parts of a 
program are dependent upon one another.

 imperative programming: an approach to software 
development in which the programmer writes a 
specific sequence of commands for the computer 
to perform.

 inheritance: a technique that reuses and repur-
poses sections of code.

 interrupt vector table: a chart that lists the ad-
dresses of interrupt handlers.

 main loop: the overarching process being carried 
out by a computer program, which may then in-
voke subprocesses.

 polymorphism: the ability to maintain the same 
method name across subclasses even when the 
method functions differently depending on its 
class.

 subtyping: a relation between data types where 
one type is based on another, but with some limita-
tions imposed.
Benefits of Functional Design
Functional design is a concept most commonly as-
sociated with computer programming. However, it is 
also relevant to other fields, such as manufacturing 
and business management. At its core, the concepts 
underlying functional design are simple. Instead of 
designing a program in which the individual parts 
perform many different functions and are highly 
interconnected, modules should have low coupling. 
Modules—the individual parts of the program—
should be designed to have the simplest possible 
inputs and outputs. When possible, each module 
should only perform a single function. Low coupling 
ensures that each module has a high degree of in-
dependence from other parts of the program. This 
makes the overall program easier to debug and main-
tain. Most of the labor that goes into programming 
is related to these two activities. Therefore, it makes 
sense to use a design approach that makes both of 
these tasks easier.
Functional design is preferable because when a 
program has a bug, if the modules perform many 
different functions and are highly coupled, it is diffi-
cult to make adjustments to the malfunctioning part 
without causing unintended consequences in other 
parts of the program. Some parts of a program do not 
easily lend themselves to functional design because 
their very nature requires that they be connected to 
multiple parts of the overall program. One example 
of this is the main loop of the program. By necessity, 
the main loop interacts with and modifies different 
modules, variables, and functions. Similarly, the in-
terrupt vector table acts as a directory for interrupt 
handlers used throughout the program. Still, many 
other functions of a typical program can be designed 
in ways that minimize interdependencies.
Interdependent Complications
Functional programming is a form of declarative 
programming. Declarative programs simply specify 
the end result that a program should achieve. In 
contrast, imperative programming outlines the par-
ticular sequence of operations a program should 
perform. Programmers must carefully consider how 
they design variables and functions. While functional 
design emphasizes modularity and independence 
between different program components, a number 
of design techniques rely on interconnections. For 
example, inheritance draws on the properties of 
one class to create another. With polymorphism, the 
same methods and operations can be performed on 
a variety of elements but will have specific, custom-
ized behaviors depending on the elements’ class. 
With subtyping, a type is designed to contain one or 
more other variables types. While each of these tech-
niques has advantages, their interdependence can 
cause issues when a change is made to other parts of 
the program. For example, consider a variable called 

144
Functional design
Principles of Computer Science
NUM that can take on any numeric value. A second 
variable is defi ned as EVENNUM and can take on 
any NUM value divisible by 2 with no remainder. 
Because EVENNUM is defi ned in relation to NUM, 
if a programmer later makes a change to NUM, it 
could have unintended consequences on the variable 
EVENNUM. A program that is highly coupled could 
have a long chain reaction of bugs due to a change in 
one of its modules.
Functional design tries to avoid these issues in two 
ways. First, programmers specifi cally consider inter-
relations as they design a program and try to make 
design choices that will minimize complications. 
Second, once a program has been written, program-
mers will proofread the code to fi nd any elements 
that have unnecessary complications. Some clues are 
the presence of language such as “and” or “or” in the 
descriptions of variables, classes, and types. This can 
suggest that multiple functions are being combined 
into one module. The programmer may 
then separate these functions into sepa-
rate, less dependent modules if possible.
manufacturing Applications
Functional design also has applications 
in physical manufacturing. This has 
been facilitated by the advent of 3-D 
printing. 3-D printing can make it easier 
to produce parts, components, and even 
whole pieces of machinery that would 
be expensive or impossible to manufac-
ture using traditional methods. In this 
context, functional design conceptu-
ally separates a complex machine into 
simple sections that can be individually 
produced and then assembled.
Process and Function
Designers who approach a problem 
from the standpoint of functional de-
sign often observe that functional de-
sign is not simply an outcome but also 
a process. Functional design is a way of 
thinking about problems before any 
code is written. It asks how a design can 
be broken down into simpler steps and 
include only essential processes.
—Scott Zimmer, JD
bibliography
Bessiè re, Pierre, et al. Bayesian Programming. Boca 
Raton: CRC, 2014. Print.
Clarke, Dave, James Noble, and Tobias Wrigstad, 
eds. Aliasing in Object-Oriented Programming: Types, 
Analysis and Verifi cation. Berlin: Springer, 2013. 
Print.
Lee, Kent D. Foundations of Programming Languages. 
Cham: Springer, 2014. Print.
Neapolitan, Richard E. Foundations of Algorithms. 5th 
ed. Burlington: Jones, 2015. Print.
Streib, James T., and Takako Soma. Guide to Java: 
A Concise Introduction to Programming. New York: 
Springer, 2014. Print.
Wang, 
John, 
ed. 
Optimizing, 
Innovating, 
and 
Capitalizing on Information Systems for Operations. 
Hershey: Business Science Reference, 2013. Print.
Gather 
data
Identify 
design need
Avoid 
preconceptions 
& synthesize
(draw out)
Analyze 
implications
Functional design is a theory of design that focuses foremost on the function of 
the unit and then on aesthetics and/or economics. Functional design relies on 
identifying the current functional need, gathering data, and analyzing it to form a 
cohesive design that meets that function. The design must be tested to determine 
whether there are any further problems in the function, all while avoiding precon-
ceptions. EBSCO illustration.

145
Game programming
Fields of Study
Software Engineering; Programming Language
Abstract
Game programming is a type of software engi-
neering used to develop computer and video games. 
Depending on the type of game under develop-
ment, programmers may be required to specialize 
in areas of software development not normally re-
quired for computer programmers. These special-
ties include artificial intelligence, physics, audio, 
graphics, input, database management, and net-
work management.
Prinicipal Terms

 game loop: the main part of a game program that 
allows the game’s physics, artificial intelligence, 
and graphics to continue to run with or without 
user input.

 homebrew: a slang term for software made by pro-
grammers who create games in their spare time, 
rather than those who are employed by software 
companies.

 object-oriented programming: a type of program-
ming in which the source code is organized into 
objects, which are elements with a unique identity 
that have a defined set of attributes and behaviors.

 prototype: an early version of software that is still 
under development, used to demonstrate what 
the finished product will look like and what fea-
tures it will include.

 pseudocode: a combination of a programming 
language and a spoken language, such as English, 
that is used to outline a program’s code.

 source code: the set of instructions written in a 
programming language to create a program.
How Game Programming Works
While many video games are developed as home-
brew projects by individuals working in their spare 
time, most major games are developed by employees 
of large companies that specialize in video games. 
In such a company, the process of creating a video 
game begins with a basic idea of the game’s design. 
If enough people in the company feel the idea has 
merit, the company will create a prototype to develop 
the concept further. The prototype gives form to the 
game’s basic story line, graphics, and programming. 
It helps developers decide whether the game is likely 
to do well in the marketplace and what resources are 
needed for its development.
Games that move past the prototyping stage pro-
ceed along many of the same pathways as traditional 
software development. However, video games differ 
from most other software applications in that ele-
ments of the game must continue to run in the ab-
sence of user input. Programmers must therefore 
create a game loop, which continues to run the pro-
gram’s graphics, artificial intelligence, and audio 
when the user is not active.
Game programmers often use object-oriented 
programming (OOP). This is an approach to soft-
ware development that defines the essential objects 
of a program and how they can interact with each 
other. Each object shares attributes and behaviors 
with other objects in the same class. Because many 
video games run hundreds of thousands of lines of 
code, an object-oriented approach to game devel-
opment can help make the code more manageable. 
OOP makes programs easier to modify, maintain, 
and extend. This approach can make it easier to plan 
out and repurpose elements of the game.
Large numbers of people are involved in writing 
the source code of a game intended for wide distribu-
tion. They are typically organized in teams, with each 
G

146
Game programming
Principles of Computer Science
team specializing in certain aspects of the game’s me-
chanics or certain types of programming. To facilitate 
collaboration, detailed comments and pseudocode 
are used to document features of the program and 
explain choices about coding strategies. Pseudocode 
is a combination of a programming language and a 
natural language such as English. It is not executable, 
but it is much easier to read and understand than 
source code.
Game Programming Tools
Game programmers use the same basic tools as most 
software developers. These tools include text edi-
tors, debuggers, and compilers. The tools used are 
often determined by the type of device the game is in-
tended to run on. Games can be developed for a par-
ticular computer platform, such as Windows, Apple, 
or Linux. They can also be developed for a gaming 
console, such as the Wii, Xbox, or PlayStation. Finally, 
games may be developed for mobile platforms such 
as Apple’s iOS or Google’s Android operating system.
A wide variety of programming languages are used 
to create video games. Some of the most commonly 
used programming languages in game development 
are C++, C#, and Java. Many video games are pro-
grammed with a combination of several different 
languages. Assembly code or C may be used to write 
lower-level modules. The graphics of many games are 
programmed using a shading language such as Cg or 
HLSL.
Game developers often use application program-
ming interfaces (APIs) such as OpenGL and DirectX. 
They also rely heavily on the use of libraries and 
APIs to repurpose frequently used functions. Many 
of these resources are associated with animation, 
graphics, and the manipulation of 3-D elements. 
One driving force in game development has been 
the use of increasingly sophisticated visual elements. 
The video game industry has come to rival motion 
pictures in their use of complex plotlines, dazzling 
special effects, and fully developed characters.
A Popular but Demanding Career
Game programming has been a popular career 
choice among young people. Due to the popularity 
of video games, it is not surprising that large num-
bers of people are interested in a career that will 
allow them to get paid to work on games. Becoming 
Develop a concept
of the player’s experience
Follow up with maintenance
and updates to match
advancing technology
Produce digital game
for distribution
TEST
Test the physical prototype/board game
Game mechanics
Aesthetic design
Player
interaction design
Design game mechanics
that support the experience
or purpose of play
Create a physical
prototype
Develop a 
digital prototype
Test the digital
prototype/program/app
Rewrite code or
change design as needed
Write
code for:
The development of a quality game begins with defining the player’s experience and the game mechanics. Aesthetics and the digital proto-
types of game pieces and environments are actually determined much later in the process. EBSCO illustration.

147
Principles of Computer Science
Graphical user interface
a game programmer requires the ability to code and 
a familiarity with several different programming lan-
guages. Game programmers can specialize in devel-
oping game engines, artificial intelligence, audio, 
graphics, user interfaces, inputs, or gameplay.
Game programmers regularly report high levels of 
stress as they are pressured to produce increasingly 
impressive games every year. There is intense com-
petition to produce better special effects, more ex-
citing stories, and more realistic action with each new 
title. Some game companies have been criticized for 
the high demands and heavy workloads placed upon 
game developers.
—Scott Zimmer, JD
Bibliography
Harbour, Jonathan S. Beginning Game Programming. 
4th ed. Boston: Cengage, 2015. Print.
Kim, Chang-Hun, et al. Real-Time Visual Effects for Game 
Programming. Singapore: Springer, 2015. Print.
Madhav, Sanjay. Game Programming Algorithms and 
Techniques: A Platform-Agnostic Approach. Upper 
Saddle River: Addison, 2014. Print.
Marchant, Ben. “Game Programming in C and C++.” 
Cprogramming.com. 
Cprogramming.com, 
2011. 
Web. 16 Mar. 2016.
Nystrom, Robert. Game Programming Patterns. N.p.: 
Author, 2009–14. Web. 16 Mar. 2016.
Yamamoto, Jazon. The Black Art of Multiplatform Game 
Programming. Boston: Cengage, 2015. Print.
Graphical user interface
Fields of Study
Computer 
Science; 
Applications; 
System-Level 
Programming
Abstract
Graphical user interfaces (GUIs) are human-com-
puter interaction systems. In these systems, users in-
teract with the computer by manipulating visual rep-
resentations of objects or commands. GUIs are part 
of common operating systems like Windows and Mac 
OS. They are also used in other applications.
Prinicipal Terms

 application-specific GUI: a graphical interface de-
signed to be used for a specific application.

 command line: an interface that accepts text-based 
commands to navigate the computer system and 
access files and folders.

 direct manipulation interfaces: computer interac-
tion format that allows users to directly manipu-
late graphical objects or physical shapes that are 
automatically translated into coding.

 interface metaphors: linking computer com-
mands, actions, and processes with real-world ac-
tions, processes, or objects that have functional 
similarities.

 object-oriented user interface: an interface that 
allows users to interact with onscreen objects as 
they would in real-world situations, rather than 
selecting objects that are changed through a sepa-
rate control panel interface.

 user-centered design: design based on a perceived 
understanding of user preferences, needs, ten-
dencies, and capabilities.
Graphics and Interface Basics
A user interface is a system for human-computer in-
teraction. The interface determines the way that a 
user can access and work with data stored on a com-
puter or within a computer network. Interfaces can 
be either text based or graphics based. Text-based 
systems allow users to input commands. These com-
mands may be text strings or specific words that acti-
vate functions. By contrast, graphical user interfaces 
(GUIs) are designed so that computer functions are 
tied to graphic icons (like folders, files, and drives). 
Manipulating an icon causes the computer to per-
form certain functions.
History of Interface Design
The earliest computers used a text-based interface. 
Users entered text instructions into a command line. 
For instance, typing “run” in the command line would 

148
Graphical user interface
Principles of Computer Science
tell the computer to activate a program or process. 
One of the earliest text-based interfaces for consumer 
computer technology was known as a “disk operating 
system” (DOS). Using DOS-based systems required 
users to learn specifi c text commands, such as “del” 
for deleting or erasing fi les or “dir” for listing the con-
tents of a directory. The fi rst GUIs were created in the 
1970s as a visual “shell” built over DOS system.
GUIs transform the computer screen into a phys-
ical map on which graphics represent functions, pro-
grams, fi les, and directories. In GUIs, users control an 
onscreen pointer, usually an arrow or hand symbol, 
to navigate the computer screen. Users activate com-
puting functions by directing the pointer over an icon 
and “clicking” on it. For instance, GUI users can cause 
the computer to display the contents of a directory 
(the “dir” command in DOS) by clicking on a folder 
or directory icon on the screen. Modern GUIs com-
bine text-based icons, such as those found in menu 
bars and movable windows, with linked text icons that 
can be used to access programs and directories.
elements of Guis and other object 
interfaces
Computer programs are built using coded instruc-
tions that tell the computer how to behave when given 
inputs from a user. Many different programming 
languages can be used to create GUIs. These include 
C++, C#, JavaFX, XAML, XUL, among others. Each 
language offers different advantages and disadvan-
tages when used to create and modify GUIs.
User-centered design focuses on understanding 
and addressing user preferences, needs, capabili-
ties, and tendencies. According to these design prin-
ciples, interface metaphors help make GUIs user 
friendly. Interface metaphors are models that repre-
sent real-world objects or concepts to enhance user 
understanding of computer functions. For example, 
the desktop structure of a GUI is designed using the 
metaphor of a desk. Computer desktops, like actual 
desktops, might have stacks of documents (windows) 
and objects or tools for performing various func-
tions. Computer folders, trash cans, and recycle bins 
are icons whose functions mirror those of their real-
world counterparts.
Object-oriented user interfaces (OOUIs) allow a 
user to manipulate objects onscreen in intuitive ways 
based on the function that the user hopes to achieve. 
Most modern GUIs have some object-oriented func-
tionality. Icons that can be dragged, dropped, slid, 
toggled, pushed, and clicked are “objects.” Objects 
include folders, program shortcuts, drive icons, and 
trash or recycle bins. Interfaces that use icons can 
also be direct manipulation interfaces (DMI). These 
interfaces allow the user to adjust onscreen objects as 
though they were physical objects to get certain re-
sults. Resizing a window by dragging its corner is one 
example of direct manipulation used in many GUIs.
current and Future of interface design
GUIs have long been based on a model known as 
WIMP. WIMP stands for “windows, icons, menus, and 
pointer objects,” which describes the ways that users 
can interact with the interface. Modern GUIs are a 
blend of graphics-based and text-based functions, but 
this system is more diffi cult to implement on modern 
handheld computers, which have less space to hold 
icons and menus. Touch-screen interfaces represent 
the post-WIMP age of interface design. With touch 
screens, users more often interact directly with ob-
jects on the screen, rather than using menus and 
text-based instructions. Touch-screen design is im-
portant in a many application-specifi c GUIs. These 
interfaces are designed to handle a single process or 
application, such as self-checkout kiosks in grocery 
stores and point-of-sale retail software.
Graphical user interfaces (GUIs) became popular in the 
early 1980s. Early uses of a GUI included analog clocks, 
simple icons, charts, and menus. EBSCO illustration.

149
Principles of Computer Science
Graphics formats
Current computer interfaces typically require 
users to navigate through files, folders, and menus to 
locate functions, data, or programs. However, voice 
activation of programs or functions is now available 
on many computing devices. As this technology be-
comes more common and effective, verbal com-
mands may replace many functions that have been 
accessed by point-and-click or menu navigation.
—Micah L. Issitt
Bibliography
“Graphical User Interface (GUI).” Techopedia. 
Techopedia, n.d. Web. 5 Feb. 2016.
Johnson, Jeff. Designing with the Mind in Mind. 2nd ed. 
Waltham: Morgan, 2014. Print.
Lohr, Steve. “Humanizing Technology: A History of 
Human-Computer Interaction.” New York Times: 
Bits. New York Times, 7 Sept. 2015. Web. 31 Jan. 
2016.
Reimer, Jeremy. “A History of the GUI.” Ars Technica. 
Condé Nast, 5 May 2005. Web. 31 Jan. 2016.
“User Interface Design Basics.” Usability. US Dept. of 
Health and Human Services, 2 Feb. 2016. Web. 2 
Feb. 2016.
Wood, David. Interface Design: An Introduction to Visual 
Communication in UI Design. New York: Fairchild, 
2014. Print.
Graphics formats
Fields of Study
Information Systems; Digital Media; Graphic Design
Abstract
Graphics formats are standardized forms of com-
puter files used to transfer, display, store, or print 
reproductions of digital images. Digital image files 
are divided into two major families, vector and raster 
files. They can be compressed or uncompressed for 
storage. Each type of digital file has advantages and 
disadvantages when used for various applications.
Prinicipal Terms

 compressed data: data that has been encoded 
such that storing or transferring the data requires 
fewer bits of information.

 lossless compression: data compression that al-
lows the original data to be compressed and recon-
structed without any loss of accuracy.

 lossy compression: data compression that uses ap-
proximation to represent content and therefore 
reduces the accuracy of reconstructed data.

 LZW compression: a type of lossless compression 
that uses a table-based algorithm.

 RGB: a color model that uses red, green, and blue 
to form other colors through various combina-
tions.
Digital Imaging
A digital image is a mathematical representation of 
an image that can be displayed, manipulated, and 
modified with a computer or other digital device. 
It can also be compressed. Compression uses algo-
rithms to reduce the size of the image file to facilitate 
sharing, displaying, or storing images. Digital images 
may be stored and manipulated as raster or vector im-
ages. A third type of graphic file family, called “meta-
files,” uses both raster and vector elements.
The quality and resolution (clarity) of an image 
depend on the digital file’s size and complexity. In 
raster graphics, images are stored as a set of squares, 
called “pixels.” Each pixel has a color value and a 
color depth. This is defined by the number of “bits” 
allocated to each pixel. Pixels can range from 1 bit 
per pixel, which has a monochrome (two-color) 
depth, to 32-bit, or “true color.” 32-bit color allows for 
more than four billion colors through various com-
binations. Raster graphics have the highest level of 
color detail because each pixel in the image can have 
its own color depth. For this reason, raster formats 
are used for photographs and in image programs 
like Adobe Photoshop. However, the resolution of 
a raster image depends on size because the image 
has the same number of pixels at any magnification. 
For this reason, raster images cannot be magnified 
past a certain point without losing resolution. Vector 

150
Graphics formats
Principles of Computer Science
graphics store images as sets of polygons that are not 
size-dependent and look the same at any magnifica-
tion. For relatively simple graphics, like logos, vector 
files are smaller and more precise than raster images. 
However, vector files do not support complex colors 
or advanced effects, like blurring or drop shadows. 
Depending on the image format, data may be lost 
after compression and restoration. Loss of image 
data reduces the quality of the image.
Two basic color models are used to digitally dis-
play various colors. The RGB color model, also called 
“additive color,” combines red, green, and blue to 
create colors. The CMYK model, also called “subtrac-
tive color,” combines the subtractive primary colors 
cyan, magenta, yellow, and black to absorb certain 
wavelengths of light while reflecting others.
Image Compression
Image compression reduces the size of an image 
to enable easier storage and processing. Lossless 
compression uses a modeling algorithm that identi-
fies repeated or redundant information contained 
within an image. It stores this information as a set 
of instructions can be used to reconstruct the image 
without any loss of data or resolution. One form of 
lossless compression commonly used is the LZW com-
pression algorithm developed in the 1980s. The LZW 
algorithm uses a “code table” or “dictionary” for com-
pression. It scans data for repeated sequences and 
then adds these sequences to a “dictionary” within 
the compressed file. By replacing repeated data with 
references to the dictionary file, space is saved but no 
data is lost. Lossless compression is of benefit when 
image quality is essential but is less efficient at re-
ducing image size. Lossy compression algorithms re-
duce file size by removing less “valuable” information. 
However, images compressed with lossy algorithms 
continue to lose resolution each time the image is 
compressed and decompressed. Despite the loss of 
image quality, lossy compression creates smaller files 
Compressed
Original file
Restored
Original file
Compressed
LOSSY
LOSSLESS
Restored
Depending on the image format, data may be lost after compression and restoration. Loss of image 
data reduces the quality of the image. EBSCO illustration.

151
Principles of Computer Science
Graphics formats
and is useful when image quality is less important or 
when computing resources are in high demand.
Common Graphic Formats
JPEG is a type of lossy image compression format de-
veloped in the early 1990s. JPEGs support RGB and 
CMYK color and are most useful for small images, 
such as those used for display on websites. JPEGs are 
automatically compressed using a lossy algorithm. 
Thus, some image quality is lost each time the image 
is edited and saved as a new JPEG.
GIF (Graphics Interchange Format) files have a 
limited color palette and use LZW compression so that 
they can be compressed without losing quality. Unlike 
JPEG, GIF supports “transparency” within an image by 
ignoring certain colors when displaying or printing. 
GIF files are open source and can be used in a wide va-
riety of programs and applications. However, most GIF 
formats support only limited color because the em-
bedded LZW compression is most effective when an 
image contains a limited color palette. PNGs (Portable 
Network Graphics) are open-source alternatives to 
GIFs that support transparency and 24-bit color. This 
makes them better at complex colors than GIFs.
SVGs (Scalable Vector Graphics) are an open-
source format used to store and transfer vector im-
ages. SVG files lack built-in compression but can be 
compressed using external programs. In addition, 
there are “metafile” formats that can be used to share 
images combining both vector and raster elements. 
These include PDF (Portable Document Format) 
files, which are used to store and display documents, 
and the Encapsulated PostScript (EPS) format, 
which is typically used to transfer image files between 
programs.
—Micah L. Issitt
Bibliography
Brown, Adrian. Graphics File Formats. Kew: Natl. 
Archives, 2008. PDF file. Digital Preservation 
Guidance Note 4.
Celada, Laura. “What Are the Most Common 
Graphics File Formats.” FESPA. FESPA, 27 Mar. 
2015. Web. 11 Feb. 2016.
Costello, Vic, Susan Youngblood, and Norman E. 
Youngblood. Multimedia Foundations: Core Concepts 
for Digital Design. New York: Focal, 2012. Print.
Dale, Nell, and John Lewis. Computer Science 
Illuminated. 6th ed. Burlington: Jones, 2016. Print.
“Introduction to Image Files Tutorial.” Boston 
University Information Services and Technology. 
Boston U, n.d. Web. 11 Feb. 2016.
Stuart, Allison. “File Formats Explained: PDF, PNG 
and More.” 99Designs. 99Designs, 21 May 2015. 
Web. 11 Feb. 2016.

152
Green computing
Principles of Computer Science
Green computing
Fields of Study
Computer Engineering; Information Technology
Abstract
“Green computing” refers to efforts to manufacture, 
use, and dispose of computers and digital devices 
in eco-friendly ways. Concerned with the effects of 
computer-related pollution and energy use on the 
environment, various government bodies and in-
dustry organizations have sought to promote more 
sustainable computing practices and the recycling of 
electronic waste.
Prinicipal Terms

 e-waste: short for “electronic waste”; computers 
and other digital devices that have been discarded 
by their owners.

 Green Electronics Council: a US nonprofit organi-
zation dedicated to promoting green electronics.

 hibernation: a power-saving state in which a com-
puter shuts down but retains the contents of its 
random-access memory.

 TCO certification: a credential that affirms the 
sustainability of computers and related devices.

 undervolting: reducing the voltage of a computer 
system’s central processing unit to decrease power 
usage.
Background on Green Computing
As pollution and climate change have become more 
pressing concerns, the need to consider the ef-
fects of industry and daily life on the environment 
has become apparent. As an especially fast-growing 
field, technology attracted particular notice from 
lawmakers, nonprofit organizations, and members 
of the public concerned with sustainability. “Green 
computing” describes the attempts to address the 
environmental impacts of widespread digital tech-
nology use.
Proponents have highlighted the excessive energy 
use by large computer systems such as data centers 
as a particular concern. Centers in the United States 
consume large amounts of energy, due in part to inef-
ficient operating practices. Data centers used about 2 
percent of all electricity in the United States in 2010. 
On average, such centers use less than 15 percent of 
that power for computing data. The bulk of it is used 
to maintain idle servers in case of a sudden spike in 
activity. In addition, such data centers often rely on 
diesel generators for backup power. They thereby 
emit greenhouse gases that contribute to climate 
change.
Another concern is the effects of the computer 
manufacturing process. Digital devices contain min-
erals such as rare earth elements that must be mined 
from the earth and processed, which can create toxic 
waste. Waste is also a matter of concern when an ob-
solete or otherwise unwanted device is discarded. 
The resulting electronic waste, or e-waste, often 
contains materials that may contaminate the sur-
rounding area. Cathode-ray tube (CRT) monitors, 
for instance, contain lead, which can leach into soil 
and water from a landfill. Many computer parts are 
also not biodegradable. This means that they do not 
break down over time and will thus continue to be a 
cause of concern for decades.
Oversight and Certifications
A number of government and nonprofit organiza-
tions promote green computing and address issues 
such as environmentally responsible energy use and 
manufacturing. The US Environmental Protection 
Agency (EPA) launched the Energy Star program in 
1992 to reduce air pollution. In the 2000s and 2010s, 
Green computing, or sustainable computing, involves prac-
tices of environmental sustainability in information technology. 
Computer technology and components that reach a certain level 
of recyclability, biodegradability, and energy efficiency are la-
beled with the Energy Star logo. By United States Environmental 
Protection Agency, public domain, via Wikimedia Commons.

153
Principles of Computer Science
Green computing
it has focused on reducing greenhouse-gas emissions 
through energy efficiency. To earn the Energy Star 
certification for their products, computer manufac-
turers must demonstrate that their devices meet the 
EPA’s energy efficiency standards.
TCO Development, a Swedish nonprofit, offers 
TCO certification. This credential was originally de-
vised in 1992 and modernized in 2012. It is granted 
to computers, mobile phones, and peripherals that 
meet certain sustainability requirements in all stages 
of their life cycle, from manufacture to disposal.
The nonprofit Green Electronics Council manages 
the Electronic Product Environmental Assessment 
Tool (EPEAT), created in 2005. Manufacturers pro-
vide the council with product information covering 
an item’s life cycle. That information is then com-
pared against other products and listed on the public 
EPEAT registry, which buyers can access.
Green Computing Initiatives
Perhaps the most common green computing initia-
tives have been efforts to educate corporations and 
the public about the environmental effects of com-
puter use, particularly in regard to energy consump-
tion. Computer users can save energy through tech-
niques such as putting a computer in hibernation 
mode. When a computer enters this mode, it essen-
tially shuts down and stops using power. It still retains 
the contents of its random-access memory (RAM), 
allowing the user to resume their previous activities 
after exiting hibernation mode.
Another technique, called undervolting, lowers 
the voltage of the computer system’s central pro-
cessing unit (CPU). The CPU does need a minimum 
voltage to function, but decreasing the voltage by a 
small amount can improve the system’s energy ef-
ficiency. In a mobile device, heat is reduced while 
battery life is prolonged. Other measures including 
turning off devices when not in use and enabling 
power-saving settings when available.
Present and Future Concerns
E-waste is one of the most significant computer-­
related environmental issues. Advances in computer 
and mobile technology continue apace, and new de-
vices constantly replace obsolete or outmoded ones. 
Because of this, the disposal of discarded devices 
represents a serious and growing environmental 
problem, in large part because of parts that do not 
biodegrade and are environmentally hazardous. 
To prevent unwanted devices from entering land-
fills, major electronics companies such as Apple, 
Motorola, and Nintendo offer “take back” programs 
in which owners may return their devices to the 
manufacturers. Firms sometimes offer a payment 
or a discount on a new device in exchange. In some 
cases, the collected devices are refurbished and re-
sold. In others, they are recycled in an eco-friendly 
manner.
Another major concern is that as the amount of 
data being created, stored, and accessed continues to 
grow, more numerous and powerful data centers will 
be needed. Unless the issue is addressed, today’s en-
ergy inefficiency will lead to ever-greater amounts of 
electricity being wasted in the future.
—Joy Crelin
Bibliography
Dastbaz, Mohammad, Colin Pattinson, and Bakbak 
Akhgar, eds. Green Information Technology: A 
Sustainable Approach. Waltham: Elsevier, 2015. 
Print.
Feng, Wu-chun, ed. The Green Computing Book: 
Tackling Energy Efficiency at Large Scale. Boca Raton: 
CRC, 2014. Print.
Glanz, James. “Power, Pollution and the Internet.” 
New York Times. New York Times, 22 Sept. 2012. 
Web. 28 Feb. 2016.
Ives, Mike. “Boom in Mining Rare Earths Poses 
Mounting Toxic Risks.” Environment 360. Yale U, 
28 Jan. 2013. Web. 28 Feb. 2016.
Smith, Bud E. Green Computing: Tools and Techniques 
for Saving Energy, Money, and Resources. Boca Raton: 
CRC, 2014. Print.
“What You Can Do.” Green Computing. U of California, 
Berkeley, n.d. Web. 28 Feb. 2016.

154
Information technology
Fields of Study
Computer Science; Network Design
Abstract
Information technology (IT) is devoted to the cre-
ation and manipulation of information using com-
puters and other types of devices. It also involves the 
installation and use of software, the sets of instruc-
tions computers follow in order to function. At one 
time, most computers were designed with a single 
purpose in mind, but over time, IT has transformed 
into a discipline that can be used in almost any 
context.
Prinicipal Terms

 device: equipment designed to perform a specific 
function when attached to a computer, such as a 
scanner, printer, or projector.

 hardware: the physical parts that make up a com-
puter. These include the motherboard and pro-
cessor, as well as input and output devices such as 
monitors, keyboards, and mice.

 network: two or more computers being linked in a 
way that allows them to transmit information back 
and forth.

 software: the sets of instructions that a computer 
follows in order to carry out tasks. Software may be 
stored on physical media, but the media is not the 
software.

 system: either a single computer or, more gener-
ally, a collection of interconnected elements of 
technology that operate in conjunction with one 
another.

 telecom equipment: hardware that is intended 
for use in telecommunications, such as cables, 
switches, and routers.
History of Information Technology
Information technology (IT) encompasses a wide 
range of activities, from pure theory to hands-on 
jobs. At one end of the spectrum are IT professionals 
who design software and create system-level network 
designs. These help organizations to maximize their 
efficiency in handling data and processing informa-
tion. At the other end are positions in which phys-
ical hardware, from telecom equipment to devices 
such as routers and switches, are connected to one 
another to form networks. They are then tested to 
make sure that they are working correctly. This range 
includes many different types of employees. For 
example, there are computer technicians, system 
administrators, programmers at the system and ap-
plication levels, chief technology officers, and chief 
information officers.
One way of studying the history of IT is to focus on 
the ways that information has been stored. IT’s his-
tory can be divided into different eras based on what 
type of information storage was available. These eras 
include prehistoric, before information was written 
down, and early historical, when information started 
to be recorded on stone tablets. In the middle histor-
ical period, information was recorded on paper and 
stored in libraries and other archives. In the modern 
era, information has moved from physical storage to 
electronic storage. Over time, information storage 
has become less physical and more Abstract. IT 
now usually refers to the configuration of computer 
hardware in business networks. These allow for the 
manipulation and transfer of electronically stored 
information.
Dot-Com Bubble
IT gained prominence in the 1990s, as the Internet 
began to grow rapidly and become more user-friendly 
I

155
Principles of Computer Science
Information technology
than it had been in the past. Many companies arose 
to try to take advantage of the new business models 
that it made possible. Computer programmers and 
network technology experts found themselves in 
high demand. Startup companies tried to build on-
line services quickly and effectively. Investment in 
technology companies put hundreds of millions of 
dollars into IT research. Even established companies 
realized that they needed to invest in their IT infra-
structure and personnel if they wanted to stay com-
petitive. As the IT sector of the economy grew rapidly, 
financial experts began to worry that it was forming 
an economic bubble. An economic bubble occurs 
when a market grows rapidly and then that growth 
declines abruptly. The bubble eventually “pops,” and 
investors pull their money out. This did happen, and 
many Internet startups shut down.
While the dot-com bubble, as it came to be known, 
passed quickly, IT remained a central part of life. 
Simple tasks that used to be done without sophisti-
cated technology, such as banking, shopping, and 
even reading a book, now involve computers, mobile 
phones, tablets, or e-readers. This means that the av-
erage person must be more familiar with IT in the 
twenty-first century than in any previous era. Because 
of this, IT has become a topic of general interest. 
For example, an average person needs to know a 
bit about network configuration in order to set up a 
home system.
Data Production Growth
With IT, new information is constantly being created. 
Once it was possible for a single person to master all 
of society’s knowledge. In the modern world, more 
data is produced every year than a person could 
assimilate in lifetime. It is estimated that by 2020, 
there will be more than five thousand gigabytes (GB) 
of data for each and every person on earth.
The availability of IT is the factor most responsible 
for the explosion in data production. Most cell phone 
plans measure customer data in how many GB per 
month may be used, for example. This is because of 
the many photos, videos, and social media status up-
dates people create and share on the Internet every 
day. It is estimated that every two days, human beings 
create as much information as existed worldwide be-
fore 2003. The pace of this data explosion increases 
as time goes on.
—Scott Zimmer, JD
Bibliography
Black, Jeremy. The Power of Knowledge: How Information 
and Technology Made the Modern World. New Haven: 
Yale UP, 2014. Print.
Bwalya, Kelvin J., Nathan M. Mnjama, and Peter M. I. 
I. M. Sebina. Concepts and Advances in Information 
Knowledge Management: Studies from Developing and 
Emerging Economies. Boston: Elsevier, 2014. Print.
Campbell-Kelly, Martin, William Aspray, Nathan 
Ensmenger, and Jeffrey R. Yost. Computer: A History 
of the Information Machine. Boulder: Westview, 2014. 
Print.
Fox, Richard. Information Technology: An Introduction 
for Today’s Digital World. Boca Raton: CRC, 2013. 
Print.
Lee, Roger Y., ed. Applied Computing and Information 
Technology. New York: Springer, 2014. Print.
Marchewka, Jack T. Information Technology Project 
Management. 5th ed. Hoboken: Wiley, 2015. Print.

156
Integrated development environments
Principles of Computer Science
Integrated development environments
Fields of Study
Software Engineering; System-Level Programming
Abstract
An integrated development environment (IDE) is 
a computer program that brings together all of the 
tools needed for software development. These tools 
usually include a text editor for writing source code, a 
compiler for converting code into machine language, 
and a debugger to check programs for errors. The 
programmer writes the code, runs the code through 
the debugger, and then compiles it into a program.
Prinicipal Terms

 command line: a prompt where a user can type 
commands into a text-based interface.
 graphical user interface (GUI): an interface that allows 
a user to interact with pictures instead of requiring the 
user to type commands into a text-only interface.

 platform: the operating system a computer uses 
and with which all of that computer’s software must 
be compatible. The major platforms are Windows, 
Mac, and Linux.

 programming languages: sets of terms and rules of 
syntax used by computer programmers to create 
instructions for computers to follow. This code is 
then compiled into binary instructions for a com-
puter to execute.

 visual programming: a form of programming 
that allows a programmer to create a program 
by dragging and dropping visual elements with 
a mouse instead of having to type in text instruc-
tions.
Building and Running Code
Computers are not intelligent, even though they 
can process information much faster than humans. 
They only know what to do when a human tells them. 
One way of doing this is by typing instructions using 
the computer’s command line, but this has several 
drawbacks. First, it requires that a human be present 
whenever the program has to be run. Second, typing 
commands into a text-based interface may be un-
comfortable or even impossible for some users. They 
prefer to use a graphical user interface (GUI), which 
lets them click on icons to run applications (apps). 
To avoid the need to type commands, computer pro-
grammers write programs. These are sets of instruc-
tions for a computer to perform, like a recipe used 
for cooking. Instead of typing in dozens of commands 
to do each task, the user can simply run the program 
that contains all of the commands. The computer will 
follow the instructions just as a cook follows a recipe.
Programs are created using a programming lan-
guage, such as Java, C++, or Python. A programming 
language is not as complete as a traditional language 
such as English or Mandarin, but it contains enough 
words and rules of syntax to allow programmers to 
create instructions, or code. Once the source code 
has been written, the next step is to translate it into 
machine language that a computer can process. An 
app called a “compiler” performs this translation. The 
compiler produces an executable version of the pro-
gram. This version can be run by any user as long as it 
is executed on the computer platform it was designed 
for (such as Windows or Mac).
Programmers often use an integrated develop-
ment environment (IDE) when coding. An IDE is 
software that makes programming easier by providing 
many tools within a single program and automating 
some simple tasks. An IDE usually includes an editor 
to write source code, a debugger to fix errors, and 
build automation tools. Many also have intelligent 
code completion, which autocompletes commonly 
used programming expressions.
Visual Application Development
The first programmers did not use IDEs. They had 
to create programming languages before they could 
even write programs. They used many separate pro-
grams, including text editors, debuggers, and com-
pilers, to craft their software. This sometimes made 
the process of programming more complicated. 
Programmers had to learn how to use several distinct 
apps that had been designed separately. Thus, it took 
longer to learn how to use each one. The different 
tools sometimes conflicted or were incompatible. 
Tiring of this, some programmers began to develop 
IDEs made up of multiple programming tools, each 
designed to work with the others.
The first IDEs were largely text-based, because 
this is how programming was then done. However, 

157
Principles of Computer Science
Integrated development environments
Auto-
documentor
Text
Editor
Run-time
Environment
Source to
Object
Code Translator
Integrated
Development
Environment
Architecture
and Design
Development
Requirements
Business
Analysis
Maintenance
Deployment
Testing
and Evaluation 
Integrated development environments pull together the tools for multiple aspects of software development 
into a single program. Text editors (to write and edit source code), translators (to convert source code into 
object code), run-time environments (to allow editors to see the output of their code), debugging tools (to 
fix errors), and auto-documentation tools (to create the necessary software documentation) allow an editor 
to create software from design to release. EBSCO illustration.

158
Integrated development environments
Principles of Computer Science
programming gradually took a more visual approach, 
in the same way that other types of apps went from 
text-based interfaces to GUIs. Visual programming 
allows software developers to use a mouse to drag and 
drop pieces of code in order to design apps. This is 
often a much more intuitive method for designing 
an app and can be less time consuming than typing 
out lines of source code. Today, several IDEs support 
visual programming. Using these tools, programmers 
can create blocks of code in one window, then click to 
debug and compile the code.
Attitudes toward IDEs
The introduction of IDEs was not greeted with uni-
versal praise. Some programmers, even in the twenty-
first century, view such tools as limiting their own 
creativity as programmers. IDEs force programs to be 
written in certain ways in order to comply with the 
environment’s design. These programmers prefer to 
write code the old-fashioned way, even if doing so is 
more labor intensive and time consuming.
One consequence of the emergence of visual 
IDEs is that younger programmers may never be ex-
posed to the text-based development tools that their 
predecessors made and used. These contemporary 
programmers learn in visual environments where 
there is less need for memorizing programming 
commands. Veteran programmers have suggested 
that this is not necessarily a good thing. Not every 
app or programming task lends itself to visual de-
velopment, they argue, so programmers should be 
able to use text-based development tools as well as 
visual ones.
—Scott Zimmer, JD
Bibliography
Christiano, 
Marie. 
“What 
Are 
Integrated 
Development Environments?” All about Circuits. 
EETech Media, 3 Aug. 2015. Web. 23 Feb. 2016.
Freedman, Jeri. Software Development. New York: 
Cavendish Square, 2015. Print.
Mara, Wil. Software Development: Science, Technology, 
and Engineering. New York: Children’s, 2016. Print.
Patrizio, Andy. “The History of Visual Development 
Environments: Imagine There’s No IDEs. It’s 
Difficult If You Try.” Mendix. Mendix, 4 Feb. 2013. 
Web. 23 Feb. 2016.
Schmidt, Richard F. Software Engineering: Architecture-
driven Software Development. Waltham: Morgan, 
2013. Print.
Tucker, Allen B., Ralph Morelli, and Chamindra de 
Silva. Software Development: An Open Source Approach. 
Boca Raton: CRC, 2011. Print.

159
Principles of Computer Science
Intelligent tutoring system
Intelligent tutoring system
Field of study
Tutoring systems 
Abstract
An intelligent tutoring system is one that modifies 
its interaction with the person being tutored bases 
on the tutee’s performance. Just as an experienced 
teacher will adopt his information presentation to 
the characteristics of the learner so an intelligent tu-
toring system with adjust the rate of presentation to 
the learners prior knowledge, level of interest, and 
general intelligence.
Principal Terms

 Andes: An intelligent tutoring system for a math-
ematically based course in introductory physics 
developed for the U. S. Naval Academy.

 AutoTutor: a program, which tutors using ordi-
nary language and grades student responses using 
some form of latent semantic analysis.

 conceptual physics: Physics taught with min-
imum emphasis on mathematical methods. The 
physics teaching community was surprised by 
the discovery of Eric Mazur that students could 
score very well on course exams and yet have 
very little understanding or ability to explain 
their understanding of the problems they had 
just solved. 

 cognitive load (Sweller): the number of slots in 
short term memory required to solve problems. 
When then information presented in a text ex-
ceeds the cognitive capacity of the students, little 
learning will occur. 

 latent semantic analysis (LSA): a method to sum-
marize the content of a text corpus in terms of 
correlations between words, allowing for an evalu-
ation of a student’s answers to questions in com-
parison to a subject matter corpus. Generally LSA 
in involves finding a 300-500 dimensional vector 
space representation of the text.

 corpus: a body of text that the student is expected 
to be able to answer question about.

 user modeling: a mathematical model of student 
behavior while learning.
The Need for Intelligent Tutoring 
Systems (ITS’s)
While there is no restriction of intelligent tutoring 
systems to topics of military importance, probably the 
greatest stimulus to their development comes from 
the armed forces. Each year several hundred thou-
sand individuals begin advanced individual training 
as members of the United States’ Armed Forces. The 
majority of these will serve out their initial enlistment 
period then go on to alternative civilian employment. 
As a result the will be a tremendous need for basic 
technical training, probably a greater need than can 
be met with the available teaching manpower at any 
time in the near future.
Special Problem with Mathematics-
Based Subjects
Though traditional instruction in engineering, 
physics, and chemistry have focused on students’ 
problem solving ability, students can circumvent the 
process by memorizing a limited set of problem tem-
plates. Indeed, the need to confirm that students are 
actually developing their understanding of the mate-
rial is always a background consideration. 
Several computer programs have been devel-
oped in recent years as intelligent tutoring systems. 
Among the major sponsors of research in this area 
are the National Science Foundation, the Institute of 
Education Sciences and the Office of Naval Research. 
These researchers are slowly converging upon a gen-
eral framework for intelligent tutoring systems. 
Major Components of an Intelligent 
Tutoring System
Here we adopt the Generalized Intelligent Framework 
for Tutoring (GIFT), which has grown out of recent 
research at the U. S. Army Research Laboratory. It is 
of course highly desirable that ITS’s be modular in de-
sign. So that an ITS developed for say organic chem-
istry could have some interchangeable parts with ITS’s 
for Physical Chemistry or Physics, to minimize the total 
effort required in in research and development and 
to maximize transfer of learning from one ITS to an-
other. It is generally accepted that ITS’S will have four 
major components: (1) a domain model (2) a learner 
model (3) a pedagogical model and (4) a tutor-user 

160
Intelligent tutoring system
Principles of Computer Science
interface. The GIFT model assumes in addition a 
sensor module, which provides a non-verbal record of 
the student’s psychological state while learning.
The domain model involves the structure of the 
knowledge to be developed in the student. It nor-
mally contains the knowledge that experts bring to 
the subject along with an awareness of pitfalls and 
popular misconceptions.
The learner model consists of the succession of 
psychological states that the learner is expected to go 
through in the course of learning. It can be viewed as 
an overlay of the domain model, which will change 
over the course of learning.
The pedagogical model takes the domain and 
learner models as input and selects the next move 
that the tutor will take. In mixed initiative systems, 
learners may take actions, or ask for help as well.
The tutor – user interface records and interprets 
the learner’s responses obtained through various 
inputs: mouse clicks. typed-in answers, eye tracking, 
and possibly postural data and keeps a record as the 
tutoring progresses.
Tracking student progress
Here are several examples of learner modeling, used 
in contemporary ITS’s:
Knowledge tracing: If the knowledge to be con-
veyed is contained in a set of production rules then 
skillometer can be used to visually display the stu-
dents progress. Step by step knowledge tracing is in-
corporated in a number of tutoring programs devel-
oped in the Pittsburgh Science of Learning Center.
Constraint bases modeling: A relevant satisfied 
state constraint corresponds to an aspect of the 
problem solution. Learner modeling is tracked by 
which constraints are followed by students in solving 
problems. Successful constraint based tutors include 
the structured query language (SQL) tutor.
Knowledge space models underlies the very suc-
cessful Assessment and Learning in Knowledge 
Spaces (ALEKS) mathematics tutor now widely used 
on college campuses to deal with students have not 
studied mathematics in recent semesters. The knowl-
edge space is a large number of possible knowledge 
states of each mathematical field. As the topics are 
reviewed the student builds a coherent picture if the 
field as a whole. 
Expectation and misconception tailored dialog. 
These are particularly important in fields like physics 
where terms from common usage, like force and en-
ergy have specific restricted meanings which students 
must understand before progress is possible.
Donald Franceschetti, PhD
Bibliography
Mandl, H., and A. Lesgold, eds. Learning Issues for 
Intelligent Tutoring Systems. New York: Springer, 
1988. Print.
Stein, N. L., and S.W. Raudenbush, eds. Developmental 
Cognitive Science Goes to School. New York: Routledge, 
2011. Print
Sottilare, R., Graesser, A., Hu, X., and Holden, H. 
(Eds.). (2013). Design Recommendations for Intelligent 
Tutoring Systems: Volume 1 - Learner Modeling. 
Orlando, FL: U.S. Army Research Laboratory. 
ISBN 978-0-9893923-0-3. Available at: https://gift-
tutoring.org/documents/42

161
Principles of Computer Science
Internet privacy
Internet privacy
Fields of Study
Computer Science; Privacy; Security
Abstract
Internet privacy is an issue of concern in the early 
twenty-first century. With increasing Internet use 
for work, socializing, and daily tasks comes a cor-
responding increase in potential breaches of pri-
vacy. To address this issue, individuals may use a va-
riety of technologies and techniques to ensure the 
security of their personal information and online 
communications.
Prinicipal Terms

 behavioral marketing: advertising to users based 
on their habits and previous purchases.

 cookies: small data files that allow websites to track 
users.

 digital legacy (digital remains): the online ac-
counts and information left behind by a deceased 
person.

 device fingerprinting: the practice of collecting 
identifying information about a computer or 
other web-enabled device.

 personally identifiable information (PII): infor-
mation that can be used to identify a specific in-
dividual.

 Privacy Incorporated Software Agents (PISA): a 
project that sought to identify and resolve privacy 
problems related to intelligent software agents.
Internet Privacy Concerns
The Internet has enabled individuals the world 
over to communicate with one another and share 
information. Internet users can socialize, shop, 
conduct financial transactions, and carry out other 
tasks online. The privacy of their online actions 
and the data they share have become increasingly 
at risk, however. Breaches of online privacy can 
take many forms. It can be the inadvertent sharing 
of 
personally 
identifiable 
information 
(PII) 
through lax social media security settings. Or it can 
be the theft of banking or credit card information. 
Insufficient privacy practices can make it possible 
for criminals to gain access to individuals’ contact 
details, health data, financial information, or gov-
ernment identification information such as Social 
Security numbers. Such access can lead to identity 
theft or fraud.
Cybercrime is one of the major causes of concern 
in regard to Internet privacy. Privacy breaches by cor-
porations and marketers are also major concerns. 
Tracking technologies enable advertisers to market 
to individuals based on their browsing habits and pre-
vious purchases. Some users enjoy being served ads 
relevant to their shopping patterns, but others view 
that it as an invasion of privacy. Protecting one’s on-
line activity from government surveillance is likewise 
of concern to some Internet users. Their concern in-
creased after the secret US National Security Agency 
(NSA) Internet surveillance program PRISM was re-
vealed in 2013.
Research
A number of government and public-sector or-
ganizations have researched the privacy needs of 
Internet users. One such research initiative, the 
European-run Privacy Incorporated Software Agents 
(PISA), sought to identify and resolve privacy prob-
lems associated with intelligent software agents. 
These agents are computer systems that act on the 
behalf of the user in a semiautonomous manner. The 
researchers also examined existing privacy laws and 
best practices. They created a prototype interface de-
signed to meet usability requirements of users. The 
interface was also designed to gain users’ trust, so 
that they would feel comfortable entering their per-
sonal information. The researchers found that even 
when privacy options were provided, as in a settings 
or control panel interface, many users had difficulty 
understanding how to use those options to meet 
their privacy needs. The users also found it hard to 
understand how the privacy options were connected 
to the personal information they had entered.
Threats to Internet Privacy
Potential threats to Internet privacy take a number 
of forms. Perhaps the most high-profile threat is that 
of hackers who seek to steal individuals’ personal 
data. In some cases, criminals may steal data such as 
credit card numbers by hacking into the servers of 
online retailers. In other cases, a criminal may use 

162
Internet privacy
Principles of Computer Science
phishing e-mails to trick an individual into revealing 
personal information. Some criminals use surveil-
lance programs such as keyloggers. Keyloggers 
can record all keystrokes made on a computer and 
transmit that data to another user. Most threats to 
Internet privacy are digital in nature, but the con-
sequences of weak Internet privacy can also extend 
past the computer screen. A lack of privacy protec-
tions on social media can reveal an individual’s ad-
dress and the dates they will be on vacation to poten-
tial burglars, for instance.
Privacy concerns likewise extend to online mar-
keting. Many marketers use cookies, small data fi les 
that track individual users on the Internet. Cookies 
0
20
10
30
40
50
Yes
Unclear
No
Collects IP Address
Collects Contact Info
Ad Customization
Tracks Interaction
Third-party Tracking
User Access
Acquisition
Data Purchase
Data Retention
Personal Data Collected from the 50 Most-Popular Websites
Shares with
Affiliates
Shares with
Third Parties
Shares with
Contractors
Graph showing privacy policy practices of the 50 most visited websites, adapted from KnowPrivacy (knowprivacy.org) research conducted by 
UC Berkeley School of Information, 2009. Users’ personal data is often collected and shared with affi liates or contractors that the site does 
not consider “third party,” making them exempt from privacy policies stating the site does not share users’ information. In many cases, it is 
unclear what information is being collected. EBSCO illustration.

163
Principles of Computer Science
Internet privacy
allow marketers to send individuals targeted ads 
based on prior purchases and browsing habits. For 
example, an individual may view a pair of shoes on-
line but not immediately buy them. The shoe seller 
may use cookies to send ads for the shoes to the user 
as they keep browsing online. This process is called 
behavioral marketing.
In addition to criminals and ads, the government 
is sometimes perceived as a threat to online privacy. 
In the United States, government surveillance of on-
line activity greatly increased following the terrorist 
attacks of September 11, 2001, and the subsequent 
passage of the controversial PATRIOT Act. In 2013 
the US government confirmed the existence of the 
Internet surveillance program PRISM. The National 
Security Agency used PRISM to access data from 
companies such as Google and Microsoft. The gov-
ernment stated it needed this access to protect na-
tional security.
Digital Remains
Among the more unusual Internet privacy dilemmas 
is that of an individual’s digital legacy, or digital re-
mains. These are the online information, records of 
communication, and personal websites or profiles 
of a deceased person. Digital remains may continue 
to exist on the Internet long after the person has 
died. In certain cases, the deceased’s next of kin 
may leave online remains in place as a digital memo-
rial. Or, if allowed, they may delete them to protect 
the deceased person’s privacy. Some online services, 
such as the social network Facebook, offer users the 
option to name a contact who can gain access to 
the account after the user’s death. The contact can 
then determine whether to delete or preserve pro-
file information. Many legal issues remain around 
who can access and control an individual’s digital 
remains and to what extent this information may be 
controlled.
Protecting Internet Privacy
Those concerned about Internet privacy can make 
their personal data more secure. To obtain a basic 
level of privacy, individuals should observe standard 
online security procedures. These include using 
strong, unique passwords and two-factor authentica-
tion, in which a password is paired with another form 
of authentication, when available. Running antivirus 
or firewall software allows individuals to identify 
and remove programs such as keyloggers or prevent 
them from infecting the computer in the first place. 
Individuals can also protect the privacy of credit card 
or Social Security numbers by entering them only on 
trusted websites that encrypt such data. Most stan-
dard web browsers offer users the ability to delete 
cookies, which temporarily prevents such tracking 
from taking place. However, it is sometimes possible 
to track devices without the use of cookies through 
device fingerprinting. Thus, deleting cookies can 
provide only a limited level of privacy.
—Joy Crelin
Bibliography
Bernal, Paul. Internet Privacy Rights: Rights to Protect 
Autonomy. New York: Cambridge UP, 2014. Print.
“Internet Privacy.” ACLU. American Civil Liberties 
Union, 2015. Web. 28 Feb. 2016.
“Online Privacy: Using the Internet Safely.” Privacy 
Rights Clearinghouse. Privacy Rights Clearinghouse, 
Jan. 2016. Web. 28 Feb. 2016.
“Protect Your Privacy on the Internet.” Safety and 
Security Center. Microsoft, n. d. Web. 28 Feb. 2016.
“A Short History of US Internet Legislation: Privacy 
on the Internet.” ServInt. ServInt, 17 Sept. 2013. 
Web. 28 Feb. 2016.
“What Is Personally Identifiable Information (PII)?” 
U Health. U of Miami Health System, n.d. Web. 28 
Feb. 2016.

164
iOS
Principles of Computer Science
iOS
Fields of Study
Operating Systems; Software Engineering; Mobile 
Platforms
Abstract
Apple’s iOS is an operating system designed for mo-
bile computing. It is used on the company’s iPhone 
and iPad products. The system, which debuted in 
2007, is based on the company’s OS X. iOS was the 
first mobile operating system to incorporate ad-
vanced touch-screen controls.
Prinicipal Terms

 jailbreaking: the process of removing software re-
strictions within iOS that prevent a device from 
running certain kinds of software.

 multitasking: in computing, the process of com-
pleting multiple operations concurrently.

 multitouch gestures: combinations of finger move-
ments used to interact with touch-screen or other 
touch-sensitive displays in order to accomplish var-
ious tasks. Examples include double-tapping and 
swiping the finger along the screen.

 platform: the underlying computer system on 
which an application is designed to run.

 3D Touch: a feature that senses the pressure with 
which users exert upon Apple touch screens.

 widgets: small, self-contained applications that 
run continuously without being activated like a 
typical application.
A New Generation of Mobile Operating 
Systems
Apple’s iOS is an operating system (OS) designed for 
use on Apple’s mobile devices, including the iPhone, 
iPad, Apple TV, and iPod Touch. In 2016, iOS was 
the world’s second most popular mobile OS after the 
Android OS. Introduced in 2007, iOS was one of the 
first mobile OSs to incorporate a capacitive touch-
screen system. The touch screen allows users to acti-
vate functions by touching the screen with their fin-
gers. The Apple iOS was also among the first mobile 
OSs to give users the ability to download applications 
(apps) to their mobile devices. The iOS is therefore a 
platform for hundreds of thousands third-party apps.
The first iOS system and iPhone were unveiled at 
the 2007 Macworld Conference. The original iOS 
had a number of limitations. For example, it was un-
able to run third-party apps, had no copy and paste 
functions, and could not send e-mail attachments. It 
was also not designed for multitasking, forcing users 
to wait for each process to finish before beginning 
another. However, iOS introduced a sophisticated 
capacitive touch screen. The iOS touch features al-
lowed users to activate most functions with their fin-
gers rather than needing a stylus or buttons on the 
device. The original iPhone had only five physical 
buttons. All other functions, including the keyboard, 
were integrated into the device’s touch screen. In 
addition, the iOS system supports multitouch ges-
tures. This allows a user to use two or more fingers 
(pressure points) to activate additional functions. 
Examples include “pinching” and “stretching” to 
shrink or expand an image.
Jailbreaking
Computer hobbyists soon learned to modify the un-
derlying software restrictions built into iOS, a pro-
cess called jailbreaking. Modified devices allow users 
greater freedom to download and install apps. It also 
allows users to install iOS on devices other than Apple 
devices. Apple has not pursued legal action against 
those who jailbreak iPhones or other devices. In 2010, 
the US Copyright Office authorized an exception per-
mitting users to jailbreak their legally owned copies of 
iOS. However, jailbreaking iOS voids Apple warranties.
Version Updates
The second version of iOS was launched in July 2008. 
With iOS 2, Apple introduced the App Store, where 
users could download third-party apps and games. In 
2009, iOS 3 provided support for copy and paste func-
tions and multimedia messaging. A major advance-
ment came with the release of iOS 4 in 2010. This up-
date introduced the ability to multitask, allowing iOS 
to begin multiple tasks concurrently without waiting 
for one task to finish before initiating the next task 
in the queue. The iOS 4 release was also the first to 
feature a folder system in which similar apps could 
be grouped together on the device’s home screen 
(called the “springboard”). FaceTime video calls also 
became available with iOS 4.

165
Principles of Computer Science
iOS
The release of iOS 5 in 2011 integrated the voice-
activated virtual assistant Siri as a default app. Other 
iOS 5 updates include the introduction of iMessage, 
Reminders, and Newsstand. In 2012, iOS 6 replaced 
Google Maps with Apple Maps and redesigned the 
App Store, among other updates. Released in 2013, 
iOS 7 featured a new aesthetic and introduced the 
Control Center, AirDrop, and iTunes Radio.
New Innovations
With the release of iOS 8, Apple included third-party 
widget support for the first time in the company’s his-
tory. Widgets are small programs that do not need 
to be opened and continuously run on a device. 
Examples including stock tickers and weather wid-
gets that display current conditions based on data 
from the web. Widgets had been a feature of Android 
and Windows mobile OSs for years. However, iOS 8 
was the first iOS version to support widgets for Apple. 
Since their release, Apple has expanded the avail-
ability of widgets for users.
The release of iOS 9 in 2015 marked a visual de-
parture for Apple. This update debuted a new type-
face for iOS called San Francisco. This specially tai-
lored font replaced the former Helvetica Neue. The 
release of iOS 9 also improved the battery life of 
Apple devices. This update introduced a low-power 
mode that deactivates high-energy programs until 
the phone is fully charged. Low-power mode can ex-
tend battery life by as much as an hour on average.
Coinciding with the release of iOS 9, Apple also 
debuted the iPhone 6S and iPhone 6S Plus, which 
introduced 3D Touch. This new feature is built into 
the hardware of newer Apple devices and can sense 
how deeply a user is pressing on the touch screen. 
3D Touch is incorporated into iOS 9 and enables 
previews of various functions within apps without 
needing to fully activate or switch to a new app. For 
instance, within the camera app, lightly holding a 
finger over a photo icon will bring up an enlarged 
preview without needing to open the iPhoto app.
—Micah L. Issitt
iOS integrated Siri, a voice-activated search engine, for the iPhone. Through multiple updates, its capabilities 
have expanded to work extremely well with a number of popular websites with all Apple apps, as well as with 
many popular third-party apps. Vasile Cotovanu, CC BY 2.0 (http://creativecommons.org/licenses/by/2.0), 
via Wikimedia Commons.

166
iOS
Principles of Computer Science
Bibliography
Heisler, Yoni. “The History and Evolution of iOS, 
from the Original iPhone to iOS 9.” BGR. BGR 
Media, 12 Feb. 2016. Web. 26 Feb. 2016.
“iOS: A Visual History.” Verge. Vox Media, 16 Sept. 
2013. Web. 24 Feb. 2016.
Kelly, Gordon, “Apple iOS 9: 11 Important New 
Features.” Forbes. Forbes.com, 16 Sept. 2015. Web. 
28 Feb. 2016.
Parker, Jason, “The Continuing Evolution of iOS.” 
CNET. CBS Interactive, 7 May 2014. Web. 26 Feb. 
2016.
Williams, Rhiannon. “Apple iOS: A Brief History.” 
Telegraph. Telegraph Media Group, 17 Sept. 2015. 
Web. 25 Feb. 2016.
Williams, Rhiannon, “iOS 9: Should You Upgrade?” 
Telegraph. Telegraph Media Group, 16 Sept. 2015. 
Web. 25 Feb. 2016.

167
L
LISP
Fields of Study
Programming Languages; Software Engineering
Abstract
LISP is the second-oldest programming language 
still in common use, and is one of the preferred lan-
guages for research and development in the field of 
artificial intelligence. Numerous variations of the lan-
guage exists, but are becoming standardized in the 
variations called Common LISP and Scheme. LISP is 
designed for the manipulation of lists of data objects, 
rather than for “number crunching,” although it is 
founded on principles of mathematical logic. The 
language was developed in 1958.
Prinicipal Terms

 artificial intelligence: the intelligence exhibited by 
machines or computers, in contrast to human, or-
ganic, or animal intelligence. 

 character: a unit of information that represents 
a single letter, number, punctuation mark, blank 
space, or other symbol used in written language. 

 compliance: adherence to standards or specifica-
tions established by an official body to govern a 
particular industry, product, or activity. 

 declarative language: language that specifies the 
result desired but not the sequence of operations 
needed to achieve the desired result. 

 floating-point arithmetic: a calculation involving 
numbers that have a decimal point that can be 
placed anywhere through the use of exponents, as 
is done in scientific notation. 

 programming languages: sets of terms and rules of 
syntax used by computer programmers to create 
instructions for computers to follow. This code is 
then compiled into binary instructions for a com-
puter to execute. 

 syntax: rules that describe how to correctly struc-
ture the symbols that comprise a language. 
History and Characteristics of LISP
LISP is the second-oldest of programming languages 
that is still in common use. LISP, a contraction of the 
words LISt Processor, was described in 1958, by John 
McCarthy, at MIT. It has been most commonly used 
in university laboratories and only recently has it be-
come more generally utilized in specialized fields that 
rely of the manipulation of data lists. LISP is prefered 
for research and development in the field of artificial 
intelligence. It is a high-level declarative language de-
veloped for manipulating lists of data objects, rather 
than for numerical calculation. There are dozens of 
variations of the LISP language that have not been 
fully standardized. Efforts to bring LISP into compli-
ance with a standard definition of the language by 
ANSI and IEEE have focused on the Common LISP 
and Scheme variations of LISP. Symbols and lists 
are the essential data types for the theory behind 
the LISP language. Math data types are not central 
to LISP programming although LISP does support 
floating point arithmetic and integers of all sizes. A 
math calculation carried out in a LISP program will 
therefore be expected to return a complete numer-
ical value rather than an abbreviated, rounded value.
Program Features of LISP
The program structure and syntax of LISP are very 
unlike those of general purpose programming lan-
guages such as C, C++, Java and other languages that 
rely on statements and subroutines in an object-ori-
ented programming format. LISP programs use pri-
marily expressions and functions stated as a “form.” 
Every LISP expression returns a value, and every 
LISP procedure returns a data object. The differ-
ence in syntax and program execution does allow the 

168
LISP
Principles of Computer Science
translation of source code from another language 
such as FORTRAN into LISP source code, but the re-
sult obtained from such translated code will not be 
quite the same as that obtained from true LISP code. 
LISP does not rely on imperative programming, but 
uses function statements. The simple symbolic struc-
ture of such statements is very non-intuitive to hu-
mans, but is very amenable to computations carried 
out by the computer. For example, the operation of 
multiplying two pairs of variables and then adding 
their resultant values would normally be written in 
other languages as
(a*b) + (c*d)
but in LISP this is written as
(+ (* a b) (* c d))
When this is understood, the construction of com-
plex mathematical statements using LISP’s built-in 
assortment of functions becomes much easier. For 
example, writing a LISP expression to evaluate a 
relation that adds two numbers and multiplies the 
product by some multiple of a trigonometric func-
tion can be written simply in LISP as
(* 4.5 (sin 27.5) (+ 6 7))
The code for such an expression is both compact 
and efficient, and conducive to the use of very exten-
sive statements. All LISP procedures have the same 
value in syntax as a function, and each one returns 
a data object as its associated value when called. A 
data object may be anything from a single character 
to a long string of symbols or a numerical value, de-
pending on the contextual nature of the procedure. 
A list is a sequence of data objects delimited by paren-
theses. For example, a list of three data objects (DOn) 
would appear as
(DO1 DO2 DO3).
Lists can also contain lists. The structure
((DO1 DO2) (DO3 DO4))
is a list of two lists. Symbols are strings of let-
ters, digits and special characters, and have dif-
ferent contextual meanings in different state-
ments. They typically serve as identifiers in LISP 
programs, much like an assigned variable name in 
other languages. There is a special form in LISP, 
called the lambda form, that allows mentioning a 
procedure without a specified name. For example, 
to evaluate a function that accepts three numer-
ical values and calculates the solution of the cor-
responding quadratic equation y = ax2 + bx + c, 
one could write
<span class=”text-node”> (lambda (a b c)  
(- (* b b) (* 4.0 a c))) </span>
Sample Problem
Write a lambda form that accepts two 
numbers b and h, and evaluates the corre-
sponding area of a right triangle.
Answer:
The formula for the area of a right triangle is A 
= ½ bh. Therefore the corresponding lambda 
form would be
<span class=”text-node”>(lambda ( b h)  
(* 0.5 b h))</span>
Value of LISP Programming
The use of data objects and lists in LISP program-
ming and its basis  on logic principles have made it 
a language of choice for artificial intelligence studies 
and development. These capabilities also make it 
useful for manipulating the source code of programs 
written in other languages, and may be an essential 
feature in developing computer applications that can 
“learn” as they carry out their functions. 
—Richard M. Renneboog M.Sc.
Bibliography
Barski, Conrad. Land of Lisp. Learn to Program LISP 
One Game at a Time. San Francisco, CA: No Starch 
Press, 2011. Print.
Kramer, Bill. The Autocadet’s Guide to Visual LISP. 
Laurence, KS: CMP Books, 2002. Print.
Méndez, Luis Argüelles. A Practical Introduction to 
Fuzzy Logic Using LISP. New York, NY: Springer, 
2016. Print.
O’Regan, Gerard. A Brief History of Computing 2nd ed., 
New York, NY: Springer-Verlag, 2012. Print.

169
Principles of Computer Science
LISP
Rawls, Rod. R., Paul F. Richard and Mark A. Hagen. 
Visual LISP Programming: Principles and Techniques 
Tinley Park, IL: Goodheart-Willcox, 2007. Print.
Scott, Michael L. Programming Language Pragmatics 4th 
ed., Waltham, MA: Morgan Kaufmann. Print.
Seibel, Peter. Practical Common LISP New York, NY: 
Apress/Springer-Verlag. Print.
Touretzky, David S. Common LISP. A Gentle Introduction 
to Symbolic Computation Mineola, NY: Dover 
Publications. Print.

170
M
Malware
Fields of Study
Software Engineering; Security
Abstract
Malware, or malicious software, is a form of software 
designed to disrupt a computer or to take advan-
tage of computer users. Creating and distributing 
malware is a form of cybercrime. Criminals have fre-
quently used malware to conduct digital extortion.
Prinicipal Terms

 adware: software that generates advertisements to 
present to a computer user.

 ransomware: malware that encrypts or blocks ac-
cess to certain files or programs and then asks 
users to pay to have the encryption or other re-
strictions removed.

 scareware: malware that attempts to trick users 
into downloading or purchasing software or appli-
cations to address a computer problem.
 spyware: software installed on a computer that allows 
a third party to gain information about the computer 
user’s activity or the contents of the user’s hard drive.

 worm: a type of malware that can replicate itself 
and spread to other computers independently; 
unlike a computer virus, it does not have to be at-
tached to a specific program.

 zombie computer: a computer that is connected 
to the Internet or a local network and has been 
compromised such that it can be used to launch 
malware or virus attacks against other computers 
on the same network.
Malicious Programming
Malware, or malicious software, is a name given to any 
software program or computer code that is used for 
malicious, criminal, or unauthorized purposes. While 
there are many different types of malware, all malware 
acts against the interests of the computer user, either by 
damaging the user’s computer or extorting payment 
from the user. Most malware is made and spread for the 
purposes of extortion. Other malware programs de-
stroy or compromise a user’s data. In some cases, gov-
ernment defense agencies have developed and used 
malware. One example is the 2010 STUXNET virus, 
which attacked digital systems and damaged physical 
equipment operated by enemy states or organizations. 
The earliest forms of malware were viruses and worms. 
A virus is a self-replicating computer program that at-
taches itself to another program or file. It is transferred 
between computers when the infected file is sent to an-
other computer. A worm is similar to a virus, but it can 
replicate itself and send itself to another networked 
computer without being attached to another file. The 
first viruses and worms were experimental programs 
created by computer hobbyists in the 1980s. As soon as 
they were created, computer engineers began working 
on the first antivirus programs to remove viruses and 
worms from infected computers.
Public knowledge about malware expanded rap-
idly in the late 1990s and early 2000s due to several 
well-publicized computer viruses. These included the 
Happy99 worm in 1999 and the ILOVEYOU worm 
in May 2000, the latter of which infected nearly 50 
million computers within ten days. According to 
research from the antivirus company Symantec in 
2015, more than 317 million new malware programs 
were created in 2014. Yet despite public awareness of 
malware, many large organizations are less careful 
than they should be. In a 2015 study of seventy major 
companies worldwide, Verizon reported that almost 
90 percent of data breaches in 2014 exploited known 
vulnerabilities that were reported in 2002 but had 
not yet been patched.

171
Principles of Computer Science
Malware
types of malware
One of the most familiar types of malware is ad-
ware. This refers to programs that create and dis-
play unwanted advertisements to users, often in 
pop-ups or unclosable windows. Adware may be 
legal or illegal, depending on how the programs 
are used. Some Internet browsers use adware 
programs that analyze a user’s shopping or web 
browsing history in order to present targeted ad-
vertisements. A 2014 survey by Google and the 
University of California, Berkeley, showed that 
more than fi ve million computers in the United 
States were infected by adware.
Another type of malware is known as spyware. This 
is a program that is installed on a user’s computer to 
track the user’s activity or provide a third party with 
access to the computer system. Spyware programs can 
also be legal. Many can be unwittingly downloaded 
by users who visit certain sites or attempt to 
download other fi les.
One of the more common types of mal-
ware is scareware. Scareware tries to con-
vince users that their computer has been 
infected by a virus or has experienced 
another technical issue. Users are then 
prompted to purchase “antivirus” or “com-
puter cleaning” software to fi x the problem.
Although ransomware dates back as far 
as 1989, it gained new popularity in the 
2010s. Ransomware is a type of malware 
that encrypts or blocks access to certain fea-
tures of a computer or programs. Users with 
infected computers are then asked to pay a 
ransom to have the encryption removed.
Addressing the threat
Combating malware is diffi cult for various 
reasons. Launching malware attacks inter-
nationally makes it diffi cult for police or 
national security agencies to target those 
responsible. Cybercriminals may also use 
zombie computers to distribute malware. 
Zombie computers are computers that 
have been infected with a virus without the 
owner’s knowledge. Cybercriminals may 
use hundreds of zombie computers simul-
taneously. Investigators may therefore trace 
malware to a computer only to fi nd that it 
is a zombie distributor and that there are 
no links to the program’s originator. While 
malware is most common on personal computers, 
there are a number of malware programs that can be 
distributed through tablets and smartphones.
Often creators of malware try to trick users into 
downloading their programs. Adware may appear in 
the form of a message from a user’s computer saying 
that a “driver” or other downloadable “update” is 
needed. In other cases, malware can be hidden in 
social media functions, such as the Facebook “like” 
buttons found on many websites. The ransomware 
program Locky, which appeared in February 2016, 
used Microsoft Word to attack users’ computers. 
Users would receive an e-mail containing a docu-
ment that prompted them to enable “macros” to read 
the document. If the user followed the instructions, 
the Locky program would be installed on their com-
puter. Essentially, users infected by Locky made two 
MALWARE
Viruses
Trojans
Rootkits
Worms
Spam
Zombies
Spyware
Crimeware
Adware
Ransomware
Malware consists of any software designed to cause harm to a device, steal in-
formation, corrupt data, confi scate or overwhelm the processor, or delete fi les. 
Examples of malware include adware, viruses, worms, Trojans, spam, and zom-
bies. EBSCO illustration.

172
Medical technology
Principles of Computer Science
mistakes. First, they downloaded a Word document 
attachment from an unknown user. Then they fol-
lowed a prompt to enable macros within the docu-
ment—a feature that is automatically turned off in all 
versions of Microsoft Word. Many malware programs 
depend on users downloading or installing programs. 
Therefore, computer security experts warn that the 
best way to avoid contamination is to avoid opening 
e-mails, messages, and attachments from unknown or 
untrusted sources.
—Micah L. Issitt
Bibliography
Bradley, Tony. “Experts Pick the Top 5 Security 
Threats for 2015.” PCWorld. IDG Consumer & 
SMB, 14 Jan. 2015. Web. 12 Mar. 2016.
Brandom, Russell. “Google Survey Finds More than 
Five Million Users Infected with Adware.” The 
Verge. Vox Media, 6 May 2015. Web. 12 Mar. 2016.
Franceschi-Bicchierai, Lorenzo. “Love Bug: The Virus 
That Hit 50 Million People Turns 15.” Motherboard. 
Vice Media, 4 May 2015. Web. 16 Mar. 2016.
Gallagher, Sean. “‘Locky’ Crypto-Ransomware Rides 
In on Malicious Word Document Macro.” Ars 
Technica. Condé Nast, 17 Feb. 2016. Web. 16 Mar. 
2016.
Harrison, Virginia, and Jose Pagliery. “Nearly 1 
Million New Malware Threats Released Every 
Day.” CNNMoney. Cable News Network, 14 Apr. 
2015. Web. 16 Mar. 2016.
Spence, Ewan. “New Android Malware Strikes at 
Millions of Smartphones.” Forbes. Forbes.com, 4 
Feb. 2015. Web. 11 Mar. 2016.
“Spyware.” Secure Purdue. Purdue U, 2010. Web. 11 
Mar 2016.
Medical technology
Fields of Study
Applications; Information Systems
Abstract
The field of medical technology encompasses a 
wide range of computerized devices and systems 
that collect information about a patient’s health, 
process it, and produce reports that can be inter-
preted by doctors to help them decide what treat-
ment to recommend. Medical technology focuses 
on the diagnosis and treatment of medical condi-
tions and is considered part of the broader field of 
health technology.
Prinicipal Terms

 biosignal processing: the process of capturing 
the information the body produces, such as heart 
rate, blood pressure, or levels of electrolytes, and 
analyzing it to assess a patient’s status and to guide 
treatment decisions.

 clinical engineering: the design of medical devices 
to assist with the provision of care.

 Medical Device Innovation Consortium: a non-
profit organization established to work with the 
US Food and Drug Administration on behalf of 
medical device manufacturers to ensure that these 
devices are both safe and effective.

 medical imaging: the use of devices to scan a pa-
tient’s body and create images of the body’s in-
ternal structures to aid in diagnosis and treatment 
planning.

 telemedicine: the practice of doctors interacting 
with and treating patients long distance using 
telecommunications technology (such as tele-
phones or videoconferencing software) and the 
Internet.
Computerized Diagnostic Tools
Devices that help doctors diagnose health conditions 
are one very common form of medical technology. 
Some medical conditions are obvious to even an un-
trained observer, such as a gunshot wound or a sev-
ered limb. These conditions require little diagnosis 
because it is immediately apparent what is wrong. 
Other diseases and conditions, however, can be very 
difficult to detect and identify. Conditions such as low 

173
Principles of Computer Science
Medical technology
blood sugar, high cholesterol, or thyroid gland prob-
lems cannot be diagnosed visually. For such condi-
tions, specially designed devices or procedures must 
be used to collect information about the patient’s 
bodily processes. This is where medical technology 
comes in.
For doctors, everything the body does creates 
information. They must find ways to collect this in-
formation, analyze it, and use it to make educated 
guesses about the patient’s condition. Part of medical 
technology involves clinical engineering, in which 
devices are designed to collect information about the 
patient’s health. Some of these devices allow doctors 
to measure the signals created by the body, a process 
known as biosignal processing. For example, electro-
encephalograms (EEG) measure the electrical ac-
tivity of the brain. Detecting abnormalities in these 
signals helps doctors to diagnose epilepsy, sleep dis-
orders, and brain tumors, among other problems. 
Doctors also use other types of medical imaging, 
such as X-rays. A person with a painful hand injury 
might receive an X-ray to determine whether or not 
any bones have been broken. Today, these X-ray im-
ages may be taken and stored digitally, rather than 
on film.
Treatment Tools
Medical technology can also be used to help treat a 
condition that has been diagnosed. Some types of 
medical technology are simple to understand and 
use. Think of bandages, crutches, and wheelchairs. 
Other types of medical treatment devices rely on 
more advanced technology. For example, tiny moni-
tors can be implanted in the body to monitor heart 
rhythm.
The US Food and Drug Administration (FDA) 
must approve medical devices before they can be used 
in general practice. This requires extensive testing to 
show that the technology is unlikely to cause harm. 
Other bodies are also involved in this process, such as 
Medical technology is constantly evolving to provide medical practitioners with a better view of what is hap-
pening and more information to help determine problems and find solutions. By Tomáš Vendiš, CC BY 3.0 
(http://creativecommons.org/licenses/by/3.0), via Wikimedia Commons.

174
Medical technology
Principles of Computer Science
the Medical Device Innovation Consortium (MDIC). 
The nonprofit MDIC supports medical technology 
inventors and guides them through the FDA approval 
process. Groups such as this must balance the inter-
ests of doctors and the FDA with those of the medical 
technology industry in creating and marketing new 
devices. It is common for approval of medical devices 
to take several years. During this time, doctors and 
even some patients may be waiting for approval to 
use groundbreaking treatments. This places great 
pressure on the FDA to move the approval process 
forward both quickly and safely.
Medical Technology in the Twenty-First 
Century
The Internet has opened up new avenues of treat-
ment for medical technology. The field of telemedi-
cine, in which patients consult with doctors long 
distance, has grown greatly. It is particularly useful 
in bringing more treatment options to rural areas 
that may not have well-equipped hospitals or many 
medical specialists. In the past, telemedicine was re-
stricted to telephone consultations. Diagnostic data 
can now be collected from a patient and transmitted 
over the Internet to a specialist. The specialist then 
uses it to recommend a treatment for the patient.
Advances in nonmedical technology have also 
raised the possibility of devices “printing” new organs 
or tissue structures that are designed for a particular 
patient. Today, computer models tell 3-D printers 
how to build a 3-D object using layers of materials 
such as plastic or ceramic. Devices could one day use 
3-D printing to grow cellular structures according to 
dimensions defined by medical imaging scans. If a 
patient needed a healthy kidney, it would be possible 
to scan them to find out exactly what size and shape 
of kidney was needed and then 3-D print the kidney 
using tissue from the patient. This would minimize 
the risk of tissue rejection that comes along with tra-
ditional organ transplant.
—Scott Zimmer, JD
Bibliography
Cassell, Eric J. The Nature of Healing: The Modern 
Practice of Medicine. New York: Oxford UP, 2013. 
Print.
Liang, Hualou, Joseph D. Bronzino, and Donald R. 
Peterson, eds. Biosignal Processing: Principles and 
Practices. Boca Raton: CRC, 2012. Print.
Rasmussen, Nicolas. Gene Jockeys: Life Science and the 
Rise of Biotech Enterprise. Baltimore: Johns Hopkins 
UP, 2014. Print.
Topol, Eric J. The Creative Destruction of Medicine: How 
the Digital Revolution Will Create Better Health Care. 
New York: Basic, 2013. Print.
Tulchinsky, Theodore H., Elena Varavikova, Joan D. 
Bickford, and Jonathan E. Fielding. The New Public 
Health. New York: Academic, 2014. Print.
Wachter, Robert M. The Digital Doctor: Hope, Hype, and 
Harm at the Dawn of Medicine’s Computer Age. New 
York: McGraw, 2015. Print.

175
Principles of Computer Science
Mesh networking
Mesh networking
Fields of Study
Information Technology; Network Design
Abstract
Mesh networking is one way of organizing a network. 
It was designed for military applications requiring 
high performance and reliability. All of the nodes in 
a mesh network transmit data across the network. In 
some mesh networks, each node has a connection 
to every other node. These types of mesh networks 
are highly resilient, but they can become costly as the 
network grows in size.
Prinicipal Terms

 flooding: sending information to every other node 
in a network to get the data to its appropriate des-
tination.

 hopping: the jumping of a data packet from one 
device or node to another as it moves across the 
network. Most transmissions require each packet 
to make multiple hops.

 node: a point on a network where two or more 
links meet, or where one link terminates.

 peer-to-peer (P2P) network: a network in which all 
computers participate equally and share coordina-
tion of network operations, as opposed to a client-
server network, in which only certain computers 
coordinate network operations.
 routing: selecting the best path for a data packet to 
take in order to reach its destination on the network.

 topology: the way a network is organized, in-
cluding nodes and the links that connect nodes.
Benefits of Mesh Networking
Mesh networking is a type of network topology. It was 
developed because of the limitations of other types of 
network topologies, such as star networks, ring net-
works, and bus networks. While these designs have 
some advantages, experience showed that they are 
vulnerable to device failure. In some scenarios, the 
failure of a single node could prevent the entire net-
work from functioning. Furthermore, adding more 
nodes to these networks could be challenging, as it 
sometimes required changing the existing network 
to accommodate new equipment.
Mesh networks are designed to keep functioning 
even when one or more nodes fail. When this oc-
curs, unaffected nodes are able to compensate for 
the damage the network has suffered. The ability 
of mesh networks to respond to broken nodes 
has led some to describe them as “self-healing,” 
because they adapt their layouts in order to keep 
functioning.
Most mesh networks are wireless rather than 
wired. Wireless technology has advanced enough 
that different types of wireless antennas can be in-
corporated in a single device or node on the net-
work. This allows for greater versatility in the type 
and number of connections that nodes can make 
with one another. In addition, wireless mesh net-
works are easier to set up than wired ones, because 
there is no need to install physical cables connecting 
all of the nodes on the network.
How a Mesh Network Works
There are two main methods that a mesh network 
uses to convey information from one point to an-
other. The first of these methods is routing. To 
route data, the network first calculates the most ef-
ficient route for data packets to take. This may be 
done through the use of a routing table, which is a 
list of routes that can be used to reach various points 
on the network. This is analogous to looking up a 
person’s address before driving to their house for a 
visit. Using the information from the routing table, 
packets travel by hopping from node to node until 
they finally reach their destination. This is similar 
to a person jumping from stone to stone in order to 
cross a stream, where each stone represents a node 
on the network.
Another approach to network navigation is 
flooding. With this method, the network does not 
need to determine the best path a data packet to 
take. Instead, the node simply sends a copy of the 
packet to every other node it is attached to. Each of 
these nodes then does the same thing, and the pro-
cess is repeated. This is called flooding because the 
network is quickly flooded with copies of the packet 
being transmitted. While this may seem redundant, 
the redundancy creates greater certainty that the 
packet will reach its destination.

176
Mesh networking
Principles of Computer Science
Client 1
Wired
Internet
Client 2
Client 3
Client 7
Client 6
Client 4
Client 5
Router
Router
MN1
MN2
MN3
MN6
MN5
MN7
MN4
Wireless mesh networks make use of a spiderweb-like design that allows multiple pathways for data to travel from one client to an-
other. This allows data to move quickly by taking the path of least resistance at any given instant. Adapted from the WMN Topology 
diagram, Communications Network Research Institute.

177
Principles of Computer Science
Mesh networking
Peer-to-Peer Networks
A peer-to-peer (P2P) network is a type of mesh net-
work. P2P networks are used to share files. Hundreds 
of thousands of computers are joining and leaving 
the mesh network at any given moment. Each peer 
computer is able to share its locally stored files with 
other peers on the network.
P2P networks are legal, but they may be used for 
legal or illegal purposes. While software developers 
use P2P networks to share software with users legally, 
online pirates often use P2P networks to illegally 
share copyrighted music, movies, and other digital 
products without permission.
Increasing Internet Access
Mesh networks have useful applications in many dif-
ferent contexts. For example, they have often been 
used in underdeveloped regions to provide afford-
able and reliable Internet access to large numbers of 
people without having to run cables to each house-
hold. Instead, homes are provided with wireless 
Internet access points configured to connect to one 
another, so that an entire city can quickly be blan-
keted in Internet access.
—Scott Zimmer, JD
Bibliography
Basagni, Stefano, et al., eds. Mobile Ad Hoc Networking: 
Cutting Edge Directions. 2nd ed. Hoboken: Wiley, 
2013. Print.
Gibson, Jerry D., ed. Mobile Communications Handbook. 
3rd ed. Boca Raton: CRC, 2013. Print.
Khan, Shafiullah, and Al-Sakib Khan Pathan, eds. 
Wireless Networks and Security: Issues, Challenges and 
Research Trends. Berlin: Springer, 2013. Print.
Pathak, Parth H., and Rudra Dutta. Designing for 
Network and Service Continuity in Wireless Mesh 
Networks. New York: Springer, 2013. Print.
Wei, Hung-Yu, Jarogniew Rykowski, and Sudhir Dixit. 
WiFi, WiMAX, and LTE Multi- Hop Mesh Networks: 
Basic Communication Protocols and Application Areas. 
Hoboken: Wiley, 2013. Print.
Yu, F. Richard, Xi Zhang, and Victor C. M. Leung, 
eds. Green Communications and Networking. Boca 
Raton: CRC, 2013. Print.

178
Metacomputing
Principles of Computer Science
Metacomputing
Fields of Study
Computer Science; Information Systems
Abstract
Metacomputing is the use of computing to study 
and design solutions to complex problems. These 
problems can range from how best to design large-
scale computer networking systems to how to de-
termine the most efficient method for performing 
mathematical operations involving very large num-
bers. In essence, metacomputing is computing 
about computing. Metacomputing makes it possible 
to perform operations that individual computers, 
and even some supercomputers, could not handle 
alone.
Prinicipal Terms

 domain-dependent complexity: a complexity 
that results from factors specific to the context in 
which the computational problem is set.
 meta-complexity: a complexity that arises when the 
computational analysis of a problem is compounded 
by the complex nature of the problem itself.

 middle computing: computing that occurs at the 
application tier and involves intensive processing 
of data that will subsequently be presented to the 
user or another, intervening application.

 networking: the use of physical or wireless con-
nections to link together different computers and 
computer networks so that they can communicate 
with one another and collaborate on computa-
tionally intensive tasks.

 supercomputer: an extremely powerful com-
puter that far outpaces conventional desktop 
computers.

 ubiquitous computing: an approach to com-
puting in which computing activity is not iso-
lated in a desktop, laptop, or server, but can 
occur everywhere and at any time through the 
use of microprocessors embedded in everyday 
devices.
Reasons for Metacomputing
The field of metacomputing arose during the 1980s. 
Researchers began to realize that the rapid growth 
in networked computer systems would soon make it 
difficult to take advantage of all interconnected com-
puting resources. This could lead to wasted resources 
unless an additional layer of computing power were 
developed. This layer would not work on a computa-
tional problem itself; instead, it would determine the 
most efficient method of addressing the problem. In 
other words, researchers saw the potential for using 
computers in a manner so complicated that only a 
computer could manage it. This new metacomputing 
layer would rest atop the middle computing layer 
that works on research tasks. It would ensure that 
the middle layer makes the best use of its resources 
and that it approaches calculations as efficiently as 
possible.
One reason an application may need a meta-
computing layer is the presence of complexities. 
Complexities are elements of a computational 
problem that make it more difficult to solve. Domain-
dependent complexities arise due to the context of 
the computation. For example, when calculating the 
force and direction necessary for an arrow to strike 
its target, the effects of wind speed and direction 
would be a domain-dependent complexity. Meta-
complexities are those that arise due to the nature of 
the computing problem rather than its context. An 
example of a meta-complexity is a function that has 
more than one possible solution.
Metacomputing and the Internet
Metacomputing is frequently used to make it pos-
sible to solve complex calculations by networking 
between many different computers. The networked 
computers can combine their resources so that each 
one works on part of the problem. In this way, they 
become a virtual supercomputer with greater capa-
bilities than any individual machine. One successful 
example of this is a project carried out by biochem-
ists studying the way proteins fold and attach to one 
another. This subject is usually studied using com-
puter programs that model the proteins’ behavior. 
However, these programs consume a lot of time and 
computing power. Metacomputing allowed the scien-
tists to create a game that users all over the world can 
play that generates data about protein folding at the 
same time. Users try to fit shapes together in different 

179
Principles of Computer Science
Metacomputing
ways, contributing their time and computing power 
to the project in a fun and easy way.
ubiquitous metacomputing
Another trend with great potential for metacom-
puting is ubiquitous computing, meaning computing 
that is everywhere and in everything. As more and 
more mundane devices are equipped with Internet-
connected microprocessors, from coffee makers to 
cars to clothing, there is the potential to harness this 
computing power and data. For example, a person 
might use multiple devices to monitor different as-
pects of their health, such as activity level, water in-
take, and calories burned. Metacomputing could 
correlate all of this independently collected infor-
mation and analyze it. This data could then be used 
to diagnose potential diseases, recommend lifestyle 
changes, and so forth.
One form of ubiquitous metacomputing that al-
ready exists is the way that various smartphone ap-
plications use location data to describe and pre-
dict traffi c patterns. One person’s data cannot 
reveal much about traffi c. However, when many 
people’s location, speed, and direction are reported 
simultaneously, the data can be 
used to predict how long one 
person’s commute will be on a 
given morning.
mimicking the brain
Metacomputing is often used 
when there is a need for a com-
puter system that can “learn.” 
A system that can learn is one 
that can analyze its own per-
formance to make adjustments 
to its processes and even its 
architecture. These systems 
bear a strong resemblance to 
the operation of the human 
brain. In some cases they are 
intentionally 
designed 
to 
imitate the way the brain ap-
proaches problems and learns 
from its past performance. 
Metacomputing, in this sense, 
is not too dissimilar from 
metacognition.
Metacomputing 
sometimes 
conjures up fears of the dangers 
posed by artifi cial intelligence in science fi ction. In 
reality, metacomputing is just another category of 
computer problem to be solved, not the beginning 
of world domination by machines. Humans can con-
ceive of computer problems so complex that it is 
nearly impossible to solve them without the aid of an-
other computer. Metacomputing is simply the solu-
tion to this dilemma.
—Scott Zimmer, JD
bibliography
Loo, Alfred Waising, ed. Distributed Computing 
Innovations for Business, Engineering, and Science. 
Hershey: Information Science Reference, 2013. 
Print.
Mallick, Pradeep Kumar, ed. Research Advances in 
the Integration of Big Data and Smart Computing. 
Hershey: Information Science Reference, 2016. 
Print.
Mason, Paul. Understanding Computer Search and 
Research. Chicago: Heinemann, 2015. Print.
Request for computing
Distribution
of computing power
Client
Agent
Metacomputing is a system design concept that allows multiple computers to share the 
responsibility of computation to complete a request effi ciently. EBSCO illustration.

180
Microprocessors
Principles of Computer Science
Nayeem, Sk. Md. Abu, Jyotirmoy Mukhopadhyay, and 
S. B. Rao, eds. Mathematics and Computing: Current 
Research and Developments. New Delhi: Narosa, 
2013. Print.
Segall, Richard S., Jeffrey S. Cook, and Qingyu 
Zhang, eds. Research and Applications in Global 
Supercomputing. Hershey: Information Science 
Reference, 2015. Print.
Tripathy, B. K., and D. P. Acharjya, eds. Global Trends 
in Intelligent Computing Research and Development. 
Hershey: Information Science Reference, 2014. 
Print.
Microprocessors
Fields of Study
Computer Engineering; System-Level Programming
Abstract
Microprocessors are part of the hardware of a com-
puter. They consist of electronic circuitry that stores 
instructions for the basic operation of the computer 
and processes data from applications and programs. 
Microprocessor technology debuted in the 1970s and 
has advanced rapidly into the 2010s. Most modern mi-
croprocessors use multiple processing “cores.” These 
cores divide processing tasks between them, which al-
lows the computer to handle multiple tasks at a time.
Prinicipal Terms

 central processing unit (CPU): electronic circuitry 
that provides instructions for how a computer 
handles processes and manages data from applica-
tions and programs.

 clock speed: the speed at which a microprocessor 
can execute instructions; also called “clock rate.”

 data width: a measure of the amount of data that 
can be transmitted at one time through the com-
puter bus, the specific circuits and wires that carry 
data from one part of a computer to another.

 micron: a unit of measurement equaling one mil-
lionth of a meter; typically used to measure the 
width of a core in an optical figure or the line 
width on a microchip.

 million instructions per second (MIPS): a unit of 
measurement used to evaluate computer perfor-
mance or the cost of computing resources.

 transistor: a computing component generally 
made of silicon that can amplify electronic ­signals 
or work as a switch to direct electronic signals 
within a computer system.
Basics of Microprocessing
Microprocessors are computer chips that contain in-
structions and circuitry needed to power all the basic 
functions of a computer. Most modern micropro-
cessors consist of a single integrated circuit, which 
is a set of conducting materials (usually silicon) ar-
ranged on a plate. Microprocessors are designed to 
receive electronic signals and to perform processes 
on incoming data according to instructions pro-
grammed into the central processing unit (CPU) 
and contained in computer memory. They then to 
produce output that can direct other computing 
functions. In the 2010s, microprocessors are the 
standard for all computing, from handheld devices 
to supercomputers. Among the modern advance-
ments has been development of integrated circuits 
with more than one “core.” A core is the circuitry 
responsible for calculations and moving data. As of 
2016, microprocessors may have as many as eighteen 
cores. The technology for adding cores and for inte-
grating data shared by cores is a key area of develop-
ment in microprocessor engineering.
Microprocessor History and Capacity
Before the 1970s, and the invention of the first mi-
croprocessor, computer processing was handled by 
a set of individual computer chips and transistors. 
Transistors are electronic components that either 
amplify or help to direct electronic signals. The first 
microprocessor for the home computer market was 
the Intel 8080, an 8-bit microprocessor introduced in 
1974. The number of bits refers to the storage size of 
each unit of the computer’s memory. From the 1970s 

181
Principles of Computer Science
Microprocessors
to the 2010s, microprocessors have followed the 
same basic design and concept but have increased 
processing speed and capability. The standard for 
computing in the 1990s and 2000s was the 32-bit mi-
croprocessor. The first 64-bit processors were intro-
duced in the 1990s. They have been slow to spread, 
however, because most basic computing functions do 
not require 64-bit processing.
Computer performance can be measured in mil-
lion instructions per second (MIPS). The MIPS 
measurement has been largely replaced by mea-
surements using floating-point operations per 
second (FLOPS) or millions of FLOPS (MFLOPS). 
Floating-point operations are specific operations, 
such as performing a complete basic calculation. 
A processor with a 1 gigaFLOP (GFLOP) perfor-
mance rating can perform one billion FLOPS each 
second. Most modern microprocessors can perform 
10 GFLOPS per second. Specialized computers 
can perform in the quadrillions of operations per 
second (petaFLOPS) scale.
Characteristics of 
Microprocessors
The 
small 
components 
within 
modern microprocessors are often 
measured in microns or microme-
ters, a unit equaling one-millionth of 
a meter. Microprocessors are usually 
measured in line width, which mea-
sures the width of individual circuits. 
The earliest microprocessor, the 
1971 Intel 4004, had a minimum line 
width of 10 microns. Modern micro-
processors can have line width mea-
surements as low as 0.022 microns.
All microprocessors are created 
with a basic instruction set. This de-
fines the various instructions that can 
be processed within the unit. The 
Intel 4004 chip, which was installed 
in a basic calculator, provided instruc-
tions for basic addition and subtrac-
tion. Modern microprocessors can 
handle a wide variety of calculations.
Different brands and models 
of microprocessors differ in band-
width, which measures how many 
bits of data a processor can handle 
per second. Microprocessors also 
differ in data width, which measures 
the amount of data that can be transferred between 
two or more components per second. A computer’s 
bus describes the parts that link the processor to 
other parts and to the computer’s main memory. 
The size of a computer’s bus is known as the width. 
It determines how much data can be transferred 
each second. A computer’s bus has a clock speed, 
measured in megahertz (MHz) or gigahertz (GHz). 
All other factors being equal, computers with larger 
data width and a faster clock speed can transfer data 
faster and thus run faster when completing basic 
processes.
Microprocessor Development
Intel cofounder Gordon Moore noted that the ca-
pacity of computing hardware has doubled every 
two years since the 1970s, an observation now 
known as Moore’s law. Microprocessor advance-
ment is complicated by several factors, however. 
These factors include the rising cost of producing 
Microprocessors contain all the components of a CPU on a single chip; this allows new 
devices to have higher computing power in a smaller unit. By M.ollivander, CC BY-SA 
3.0 (http://creativecommons.org/licenses/by-sa/3.0), via Wikimedia Commons.

182
Microprocessors
Principles of Computer Science
microprocessors and the fact that the ability to 
reduce power needs has not grown at the same 
pace as processor capacity. Therefore, unless en-
gineers can reduce power usage, there is a limit to 
the size and processing speed of microprocessor 
technology. Data centers in the United States, 
for instance, used about 91 billion kilowatt-hours 
of electricity in 2013. This is equal to the amount 
of electricity generated each year by thirty-four 
large coal-burning power plants. Computer engi-
neers are exploring ways to address these issues, 
including alternatives for silicon in the form of 
carbon nanotubes, bioinformatics, and quantum 
computing processors.
—Micah L. Issitt
Bibliography
Ambinder, Marc. “What’s Really Limiting Advances 
in Computer Tech.” Week. The Week, 2 Sept. 2014. 
Web. 4 Mar. 2016.
Borkar, Shekhar, and Andrew A. Chien. “The Future 
of Microprocessors.” Communications of the ACM. 
ACM, May 2011. Web. 3 Mar. 2016.
Delforge, Pierre. “America’s Data Centers Consuming 
and Wasting Growing Amounts of Energy.” NRDC. 
Natural Resources Defense Council, 6 Feb. 2015. 
Web. 17 Mar. 2016.
McMillan, Robert. “IBM Bets $3B That the Silicon 
Microchip Is Becoming Obsolete.” Wired. Condé 
Nast, 9 July 2014. Web. 10 Mar. 2016.
“Microprocessors: Explore the Curriculum.” Intel. 
Intel Corp., 2015. Web. 11 Mar. 2016.
“Microprocessors.” MIT Technology Review. MIT 
Technology Review, 2016. Web. 11 Mar. 2016.
Wood, Lamont. “The 8080 Chip at 40: What’s Next 
for the Mighty Microprocessor?” Computerworld. 
Computerworld, 8 Jan. 2015. Web. 12 Mar. 2016.

183
Principles of Computer Science
Microscale 3-D printing
Microscale 3-D printing
Fields of Study
Information Technology; Computer Engineering
Abstract
Microscale 3-D printing is a type of 3-D printing that 
makes it possible to construct objects at an extremely 
small scale. Some processes can create objects as 
small as 100 micrometers. 3-D printing at this scale 
has a number of applications for computing and 
medicine. It makes it possible to produce micro-
scopic structures out of organic materials for bio-
medical applications.
Prinicipal Terms

 binder jetting: the use of a liquid binding agent to 
fuse layers of powder together.

 directed energy deposition: a process that de-
posits wire or powdered material onto an object 
and then melts it using a laser, electron beam, or 
plasma arc.

 material extrusion: a process in which heated fila-
ment is extruded through a nozzle and deposited 
in layers, usually around a removable support.

 material jetting: a process in which drops of liquid 
photopolymer are deposited through a printer 
head and heated to form a stable solid.

 powder bed fusion: the use of a laser to heat layers 
of powdered material in a movable powder bed.

 sheet lamination: a process in which thin layered 
sheets of material are adhered or fused together 
and then extra material is removed with cutting 
implements or lasers.

 vat photopolymerization: a process in which a laser 
hardens layers of light-sensitive material in a vat.
A Revolutionary Technology
3-D printing is a relatively new technology. However, 
it has already revolutionized manufacturing. It takes 
its name from traditional computer printers that pro-
duce pages of printed images. Regular printers op-
erate by depositing small amounts of ink at precise 
locations on a piece of paper. Instead of ink, a 3-D 
printer uses a material, such as a polymer, metallic 
powder, or even organic material. It builds an object by 
depositing small amounts of that material in successive 
layers. In some cases, 3-D printing fastens materials to 
a substrate using heat, adhesives, or other methods. 
3-D printing can produce incredibly intricate objects 
that would be difficult or impossible to create through 
traditional manufacturing methods.
Microscale 3-D printing advances the innova-
tion of standard 3-D printing to create microscopic 
structures. The potential applications for microscale 
3-D printing are still being explored. However, mi-
croscale 3-D printing presents the possibility of cre-
ating tissue for transplant. For example, full-scale 3-D 
printing has already produced some types of tissue, 
such as muscle, cartilage, and bones. One problem is 
the printed tissue sometimes did not survive because 
it had no circulatory system to bring blood and nutri-
ents to the new tissue. Microscale 3-D printing makes 
it possible to create the tiny blood vessels needed in 
living tissue, among other potential applications.
3-D Printing Methods
The basic approach used by 3-D printing is to build an 
object by attaching tiny amounts of material to each 
Material engineers can use microscale 3-D printing 
to develop unique materials for use in a wide range 
of fields, including bioengineering, architecture, 
and electronics. The combination and arrangement 
of particular molecules allows engineers to develop 
materials with the necessary characteristics to fulfill 
specific needs, such as the polymers used to build the 
entry heat shields for NASA. By Alexander Thompson 
and John Lawson, NASA Ames Research Center.

184
Microscale 3-D printing
Principles of Computer Science
other at precise locations. Some materials are melted 
before they are deposited. Material extrusion heats 
polymer filament. The melted plastic material is then 
extruded through nozzles and deposited in a layer. 
The materials then harden into place, and another 
layer is added. With vat photopolymerization, a light-
sensitive liquid polymer is printed onto a platform 
within a vat of liquid resin. An ultraviolet (UV) laser 
then hardens a layer of resin. More liquid polymer is 
then added, and the process is repeated. With mate-
rial jetting, a printer head deposits liquefied plastic 
or other light-sensitive material onto a platform. The 
material is then hardened with UV light.
Other methods melt or fuse materials after they 
have been deposited. In powder bed fusion, the 
printer heats a bed of powdered glass, metal, ce-
ramic, or plastic until the materials fuse together in 
the desired locations. Another layer of powder is then 
added and fused onto the first. Binder jetting uses a 
printer head to deposit drops of glue-like liquid into 
a powdered medium. The liquid soaks into and so-
lidifies the medium. Sheet lamination fuses together 
thin sheets of paper, metal, or plastic with an adhe-
sive. The layers are then cut with a laser into the de-
sired shape. In directed energy deposition, a metal 
wire or powder is deposited in thin layers before 
being melted with a laser.
The method used depends on the physical proper-
ties of the material being printed. Metal alloys, for ex-
ample, cannot easily be liquefied for vat polymeriza-
tion, material jetting, or material extrusion. Instead, 
they are printed using binder jetting, powder bed fu-
sion, or sheet lamination.
Microscale Methods
Microscale 3-D printing requires more exact methods 
to create objects that are just a few micrometers wide. 
Microscopic objects require tiny droplets of materials 
and precise locations of deposition. One microscale 
3-D printing technique is optical lithography. This 
technique uses light to create patterns in a photosen-
sitive resist, where material is then deposited. Optical 
transient liquid molding (TLM) uses UV light pat-
terns and a custom flow of liquid polymer to create 
objects that are smaller than the width of a human 
hair. Optical TLM combines a liquid polymer, which 
will form the structure of the printed object, with 
a liquid mold in a series of tiny pillars. The pillars 
are arranged based on software that determines the 
shape of the liquids’ flow. Patterned UV light then 
cuts into the liquids to further shape the stream. The 
combination of the liquid mold and the UV light pat-
tern allow the creation of highly complex structures 
that are just 100 micrometers in size.
Microscale 3-D printing makes it possible to create 
extremely small circuits. This will enable the creation 
of new devices, such as “smart clothing” that can 
sense the wearer’s body temperature and adjust its 
properties based on this information. Microscale 3-D 
printing may also revolutionize the creation of new 
medicines. Because drug uptake by cells is shape-
dependent, the precision of microscale 3-D printing 
may allow researchers to design custom drugs for 
specific brain receptors.
A New Type of “Ink”
Some refer to the build materials used in 3-D printing 
as “inks” because they take the place of ink as it is 
used in regular document printers. This can stretch 
the definition of “ink.” Most people think of ink as 
either the liquid in a pen or the toner of a computer 
printer. In microscale 3-D printing, the ink might 
be human cells used to create an organ or metallic 
powder that will be fused into tiny circuits.
—Scott Zimmer, JD
Bibliography
Bernier, Samuel N., Bertier Luyt, Tatiana Reinhard, 
and Carl Bass. Design for 3D Printing: Scanning, 
Creating, Editing, Remixing, and Making in Three 
Dimensions. San Francisco: Maker Media, 2014. 
Print.
France, Anna Kaziunas, comp. Make: 3D Printing—
The Essential Guide to 3D Printers. Sebastopol: Maker 
Media, 2013. Print.
Horvath, Joan. Mastering 3D Printing: Modeling, 
Printing, and Prototyping with Reprap- Style 3D 
Printers. Berkeley: Apress, 2014. Print.
Hoskins, Stephen. 3D Printing for Artists, Designers and 
Makers. London: Bloomsbury, 2013. Print.
Lipson, Hod, and Melba Kurman. Fabricated: The New 
World of 3D Printing. Indianapolis: Wiley, 2013. 
Print.

185
Principles of Computer Science
Mobile apps
Mobile apps
Fields of Study
Applications; Mobile Platforms
Abstract
Mobile apps are programs designed to run on smart-
phones, tablets, and other mobile devices. These apps 
usually perform a specific task, such as reporting the 
weather forecast or displaying maps for navigation. 
Mobile devices have special requirements because 
of their small screens and limited input options. 
Furthermore, a touch screen is typically the only way to 
enter information into a mobile device. Programmers 
need special knowledge to understand the mobile 
platform for which they wish to create apps.
Prinicipal Terms

 applications: programs that perform specific func-
tions that are not essential to the operation of the 
computer or mobile device; often called “apps.”

 emulators: programs that mimic the functionality 
of a mobile device and are used to test apps under 
development.

 mobile website: a website that has been opti-
mized for use on mobile devices, typically fea-
turing a simplified interface designed for touch 
screens.

 platform: the hardware and system software of a 
mobile device on which apps run.

 system software: the operating system that allows 
programs on a mobile device to function.

 utility programs: apps that perform basic func-
tions on a computer or mobile device such as dis-
playing the time or checking for available network 
connections.
Types of Mobile Software
Mobile applications, or apps, are computer programs 
designed specifically to run on smartphones, tablets, 
and other mobile devices. Apps must be designed for 
a specific platform. A mobile platform is the hard-
ware and system software on which a mobile device 
operates. Some of the most widely used mobile plat-
forms include Google’s Android, Apple’s iOS, and 
Microsoft’s Windows. Mobile devices support a va-
riety of software. At the most basic level, a platform’s 
system software includes its operating system (OS). 
The OS supports all other programs that run on 
that device, including apps. In addition to the OS, 
smartphones and tablets come with a variety of prein-
stalled utility programs, or utilities, that manage basic 
functions. Examples of utilities include a clock and a 
calendar, photo storage, security programs, and clip-
board managers. While utilities are not essential to 
the functionality of the OS, they perform key tasks 
that support other programs.
However, the real power of mobile devices has come 
from the huge assortment of mobile apps they can 
run. The app stores for various mobile devices con-
tain hundreds of thousands of different apps to down-
load. Each app has been designed with different user 
needs in mind. Many are games of one sort or another. 
There are also vast numbers of apps for every pursuit 
imaginable, including video chat, navigation, social 
networking, file sharing and storage, and banking.
Developers of mobile apps use various approaches 
to design their software. In some cases, an app is little 
more than a mobile website that has been optimized 
for use with small touch screens. For example, the 
Facebook app provides essentially the same func-
tionality as its website, although it can be integrated 
with the device’s photo storage for easier uploading. 
Other mobile apps are developed specifically for the 
mobile devices they run on. Programmers must pro-
gram their apps for a specific mobile platform. App 
developers usually use a special software develop-
ment kit and an emulator for testing the app on a vir-
tual version of a mobile device. Emulators provide a 
way of easily testing mobile apps. Emulators generate 
detailed output as the app runs, so the developer can 
use this data to diagnose problems with the app.
The App Economy
Mobile apps have evolved into a multibillion-dollar 
business. Before the advent of mobile devices, soft-
ware was developed for use on personal computers 
and a software package often sold for hundreds of 
dollars. The mobile app marketplace has adopted a 
very different model. Instead of creating apps that 
cost a large amount of money and try to provide a 
wide range of functions, the goal is to create an app 
that does one thing well and to charge a small amount 
of money. Many apps are free to download, and paid 

186
Mobile apps
Principles of Computer Science
apps are typically priced anywhere from ninety-nine 
cents to a few dollars. Despite such low price points, 
developers of successful apps can still earn large sums 
of money. This is in part due to the large numbers of 
smartphone and tablet users.
Mobile apps also have a low cost of distribution. 
In the past, software was sold on physical media such 
as floppy disks or CD-ROMs. These had to be pack-
aged and shipped to retailers at the software devel-
oper’s expense. With mobile apps, there are no such 
overhead costs because apps are stored online by the 
platform’s app store and then downloaded by users. 
Aside from the time and effort of developing an app, 
the only other financial cost to the developer is an an-
nual registration fee to the platform’s app store and a 
percentage of revenue claimed by the platform. App 
stores typically vet apps to make sure they do not con-
tain malware, violate intellectual property, or make 
false advertising claims. App developers can earn rev-
enue from the download fee, in-app advertisements, 
and in-app purchases. There have been several devel-
opers who have produced an unexpectedly popular 
app, catapulting them to fame and wealth.
Mobile Apps and Social Change
Mobile apps can do much more than entertain and 
distract. For example, Twitter is a microblogging app 
that allows users to post short messages and read up-
dates from others. Twitter has played a significant 
role in social movements such as the Arab Spring of 
2011. Because it relies on telecommunications tech-
nology that continues to function even when there 
NATIVE
HYBRID
Full device access
(Very fast processing speeds, 
available in app stores)
Partial device access
(Moderate processing speeds, 
not available in app stores)
Large platform 
requirements
(High development costs)
Few platform 
requirements
(Low development costs)
HTML5
Native mobile apps and hybrid apps offer some features and capabilities unavailable with standard web programming, and they offer 
faster connectivity than is available with traditional computer software. EBSCO illustration.

187
Principles of Computer Science
Mobile apps
are disruptions in other media, the Twitter app has 
allowed people to communicate even during major 
disasters and political upheavals. Other apps allow 
users to report and pinpoint environmental damage 
or potholes for government agencies to fix.
Platform Lock
Mobile apps must be designed for a particular mo-
bile platform. Sometimes, a developer will make a 
version of an app for each major platform. In other 
cases, however, developers only create an app to run 
on a single platform. This leaves users of other mo-
bile platforms unable to use the app. In the case of 
popular apps, this can cause frustration among those 
who want to be able to use whatever apps they want 
on their platform of choice.
—Scott Zimmer, JD
Bibliography
Banga, Cameron, and Josh Weinhold. Essential Mobile 
Interaction Design: Perfecting Interface Design in Mobile 
Apps. Upper Saddle River: Addison-Wesley, 2014. 
Print.
Glaser, J. D. Secure Development for Mobile Apps: How to 
Design and Code Secure Mobile Applications with PHP 
and JavaScript. Boca Raton: CRC, 2015. Print.
Iversen, Jakob, and Michael Eierman. Learning Mobile 
App Development: A Hands-On Guide to Building 
Apps with iOS and Android. Upper Saddle River: 
Addison-Wesley, 2014. Print.
Miller, Charles, and Aaron Doering. The New 
Landscape of Mobile Learning: Redesigning Education 
in an App-Based World. New York: Routledge, 2014. 
Print.
Salz, Peggy Anne, and Jennifer Moranz. The Everything 
Guide to Mobile Apps: A Practical Guide to Affordable 
Mobile App Development for Your Business. Avon: 
Adams Media, 2013. Print.
Takahashi, Dean. “The App Economy Could Double 
to $101 Billion by 2020.” VB. Venture Beat, 10 Feb. 
2016. Web. 11 Mar. 2016.

188
Mobile operating systems
Principles of Computer Science
mobIle operatIng systems
Fields oF study
Mobile Platforms; Operating Systems
AbstrAct
Mobile operating systems (OSs) are installed on mo-
bile devices such as smartphones, tablets, and por-
table media players. Mobile OSs differ from ordinary 
computer OSs in that they must manage cellular 
connections and are confi gured to support touch 
screens and simplifi ed input methods. Mobile OSs 
tend to have sophisticated power management fea-
tures as well, since they are usually not connected to a 
power source during use.
PriniciPAl terms
 
 android open source project: a project under-
taken by a coalition of mobile phone manufac-
turers and other interested parties, under the lead-
ership of Google. The purpose of the project is to 
develop the Android platform for mobile devices.
 
 ios: Apple’s proprietary mobile operating system, 
installed on Apple devices such as the iPhone, 
iPad, and iPod touch.
 
 jailbreak: the removal of restrictions placed on a 
mobile operating system to give the user greater 
control over the mobile device.
 
 near-fi eld communication: a method by which two 
devices can communicate wirelessly when in close 
proximity to one another.
 
 real-time operating system: an operating system 
that is designed to respond to input within a set 
amount of time without delays caused by buffering 
or other processing backlogs.
A brief History of mobile operating 
systems
The mobile computing market is one of the fastest-
growing sectors of the technology fi eld. Its growing 
popularity began in the late 1990s with the release of 
the Palm Pilot 1000, a personal digital assistant (PDA). 
The Pilot 1000 introduced Palm OS, an early mobile 
operating system (OS). The Palm OS was later ex-
tended to smartphones. Smartphones combined the 
features of PDAs, personal computers (PCs), and cell 
phones. Prior to smartphones, many people had cell 
phones but their functionality was extremely limited, 
and many people owned both a PDA and cell phone. 
The Ericsson R380, released in 2000, was the fi rst cell 
phone marketed as a smartphone. The Ericsson R380 
ran on Symbian. Symbian was the dominant mobile 
OS in the early smartphone market.
The arrival of the iPhone in 2007 changed this. 
The sleek, simple design of Apple’s mobile OS, 
called iOS, gave the device an intuitive user inter-
face. In addition to the iOS, the iPhone has a base-
band processor that runs a real-time operating 
system (RTOS). All smartphones have an RTOS 
in addition to the manufacturer’s mobile OS. In a 
smartphone, the RTOS communicates with the cel-
lular network, enabling the phone to exchange data 
with the network. Despite the success of the iPhone, 
some felt that Apple devices and its App Store were 
too locked down. Apple does not allow users to make 
certain changes to the iOS or to install apps from 
unoffi cial sources. In response, a community of 
iPhone hackers began releasing software that could 
be used to jailbreak Apple devices. Jailbreaking re-
moves some of the restrictions that are built into the 
Operating systems on mobile devices have multiple limitations; however, the data input hardware and 
software can vary greatly, even within one device. One requirement of a good mobile operating system 
is that it be able to effi ciently respond to a variety of data input methods. EBSCO illustration.

189
Principles of Computer Science
Mobile operating systems
iOS in order to give users root access to the iOS. 
Jailbreaking gives users greater control over their 
mobile devices.
Google released a mobile OS called Android in 
2008. Unlike Apple’s iOS, Android’s source code is 
open source. Open-source software is created using 
publicly available source code. The Android Open 
Source Project (AOSP) develops modified versions 
of the Android OS using the open-source code. 
Android’s status as an open system means there is no 
need to jailbreak. Android quickly became the domi-
nant OS worldwide. However, Apple’s iOS accounts 
for slightly more than half of mobile OS market in 
the United States and Canada.
Mobile Features
Mobile OSs share many similarities with desktop and 
laptop OSs. However, mobile OSs are more closely 
integrated with touch-screen technology. Mobile 
OSs also typically feature Bluetooth and Wi-Fi con-
nectivity, global positioning system (GPS) naviga-
tion, and speech recognition. Furthermore, many 
new smartphones are equipped with hardware that 
supports near-field communication (NFC). NFC al-
lows two devices to exchange information when they 
are placed close to one another. NFC uses radio fre-
quency identification (RFID) technology to enable 
wireless data transfers. A major benefit of NFC is 
its low power usage, which is particularly critical to 
mobile devices. Mobile technology such as NFC fa-
cilitates numerous business transactions, including 
mobile payment systems at the point of sale.
Although there remain some significant differ-
ences between desktop OSs and mobile OSs, they are 
rapidly converging. Cloud computing enables users to 
share and sync data across devices, further narrowing 
the differences between smartphones and PCs.
Security Concerns
Features such as NFC and Bluetooth come with privacy 
and security concerns. Many of the mobile OS fea-
tures that make smartphones so convenient also make 
them vulnerable to access by unauthorized users. For 
example, hackers have been able to sweep up many 
users’ private information by scanning large crowds for 
mobile devices with unsecured Bluetooth connections. 
The ability to capture private data so easily creates 
major vulnerabilities for identity theft. To combat this 
threat, mobile OSs have incorporated various forms of 
security to help make sure that the private data they 
contain can only be accessed by an authorized user. For 
example, Apple has integrated fingerprint recognition 
into its iOS and newer versions of the iPhone.
Both Android and iOS now include features that 
help a user to locate and recover a mobile device that 
has been lost or stolen. Mobile owners can go online 
to geographically locate the device using its GPS data. 
The owner can also remotely lock the device to prevent 
its use by anyone else. It can even cause the device to 
emit a loud alarm to alert those nearby to its presence.
Impact
Mobile OSs and the apps that run on them have revo-
lutionized the way in which people conduct their daily 
lives. Thanks to mobile OSs, users can track personal 
health data, transfer funds, connect with social media, 
receive GPS and weather data, and even produce and 
edit photo and audiovisual files, among other activities. 
As the popularity of mobile devices (and, by extension, 
mobile OSs) has increased, many have questioned the 
future of the PC. For now, PCs and desktop OSs con-
tinue to dominant the business sector, however.
—Scott Zimmer, JD
Bibliography
Collins, Lauren, and Scott Ellis. Mobile Devices: Tools 
and Technologies. Boca Raton: CRC, 2015. Print.
Drake, 
Joshua 
J. 
Android 
Hacker’s 
Handbook. 
Indianapolis: Wiley, 2014. Print.
Dutson, Phil. Responsive Mobile Design: Designing for 
Every Device. Upper Saddle River: Addison-Wesley, 
2015. Print.
Elenkov, Nikolay. Android Security Internals: An In-
Depth Guide to Android’s Security Architecture. San 
Francisco: No Starch, 2015. Print.
Firtman, Maximiliano R. Programming the Mobile Web. 
Sebastopol: O’Reilly Media, 2013. Print.
Neuburg, Matt. Programming iOS 8: Dive Deep into 
Views, View Controllers, and Frameworks. Sebastopol: 
O’Reilly Media, 2014. Print.
Ravindranath, Mohana. “PCs Lumber towards the 
Technological Graveyard.” Guardian. Guardian 
News and Media, 11 Feb. 2014. Web. 10 Mar. 2016.
Tabini, Marco. “Hidden Magic: A Look at the Secret 
Operating System inside the iPhone.” MacWorld. IDG 
Consumer & SMB, 20 Dec. 2013. Web. 9 Mar. 2016.

190
Molecular Computers
Principles of Computer Science
Molecular Computers
Fields of Study
Computer 
Science; 
Computer 
Engineering; 
Information Technology
Summary
The DNA structure is highly amenable to informa-
tion storage. A single strand of normally helical 
DNA, an oligomer, containing 20 base pairs, can 
exist in 420 or roughly 1.2 trillion, distinctly different 
arrangements. Synthesis of a given oligomer is not 
difficult and has been automated. While other mol-
ecules had been used as computational substrates, 
molecular biologist Leonard Adelman gained con-
siderable notoriety by demonstrating how a classic 
mathematical problem could be formulated as a 
bonding problem for a small set of DNA oligomers.  
Principal Terms

 Hamiltonian path problem: one of the classic NP-
complete problems for which the time of solu-
tion grows faster than any power of the number if 
points involved.

 ligase: an enzyme that facilitates bond formation 
between two molecular fragments. Also called a 
polymerase.

 NP-complete problem: The class of problems for 
which the solution time grows faster than a poly-
nomial of the number of points involved.

 Oligomer: amodest length polymer of DNA, about 
20 base pairs long.

 polymerase chain reaction: the Nobel-prize win-
ning discovery by Cary Mullis that thermal cycling 
can produce numerous copies of a single DNA 
oligomer; used in DNA fingerprinting as well as 
DNA computing.

 SAT problem: determining whether a single as-
signment of true and false values to the k Boolean 
values in a one or more statements yields a unique 
solution.

 thermal cycling: bringing a sample repeatedly 
above the dissociation temperature so that the 
strands dissociate, and then below that tempera-
ture so that replicates form 

 Watson-Crick base-pairing: the pairing of adenine 
and thymine and that of cytosine with guanine in 
H-bonded complexes in the DNA double helix
Adelman’s Experiment
In 1994, L. M. Adelman used standard molecular bi-
ology techniques to find a Hamiltonian path through 
a map of 7 interconnected vertices. A Hamiltonian 
path is one, which passes through each vertex only 
once and only once. While the actual problem solved 
might have required a few seconds thought by an in-
sightful high school student, the Hamiltonian path 
problem is one that mathematicians consider NP 
complete, one for which the solution time grows 
faster than any polynomial in the number of vertices. 
Given a map with a hundreds of cities connected by a 
network of roads, some one way, it could take years to 
determine if a Hamiltonian path actually exists.
Adelman’s solution involved treating the road map 
of one-way and two-way streets as a directed graph G 
of n vertices with a designated beginning and end. 
Adelman represented each vertex of his graph by a 
20-mer chosen at random. These could be synthe-
sized by well-established techniques and amplified 
using the polymerase chain reaction. Possible paths 
representing an allowed connection between vertex 
i and vertex j would be represented by the comple-
ment to the last 10 sites in i and the first 10 in j.
1.	 Generate a large number of paths through G at 
random. This involved synthesizing oligomers to rep-
resent all allowed vertices and allowing them to pair 
with each other
2.	 Remove all paths that do not begin at the des-
ignated beginning or end at the designated end. 
This was easily done using standard biochemical 
techniques.
3.	 Remove all paths that do not involve exactly n ver-
tices. This is easily accomplished using gel electro-
phoresis for which the mobility is inversely propor-
tional to the molecular mass.
4.	 For each vertex v, remove all paths that do not in-
clude it 
If any DNA is to be found remaining it represents 
a Hamiltonian path.
Adelman’s experiment as described in the journal 
Science in November 1994 drew a great deal of atten-
tion. A conference followed in April 1994 at which 
Leonard Adelman was the keynote speaker. One 
of the most significant immediate extensions of 
Adelman’s approach was proposed by R. J. Lipton 

191
Principles of Computer Science
Molecular Computers
at the same conference, who showed that the SAT 
problem could also be solved by manipulating DNA.
The SAT problem
The SAT problem involves finding a set of truth values 
that satisfies a formula in the propositional calculus, 
in which Boolean variables appear connected by ne-
gation, disjunction or conjunction. For instance.
a = (x1.OR..NOT.x2ORx3).AND.(x2.OR.x3).AND. 
(x1.OR.x3).AND..NOT.x3
is only satisfied if x1 =x2=true, and x3 is false.
For a propositional formula with k variables we 
construct a graph with 3k +1 points. The vk (k=0,…
,k) are assigned to points on the x axis, points to the 
right and below the axis are assigned to ak
1 and the 
points above the axis are assigned to the ak
o . Then a 
path from v0 to vk going above or below the like indi-
cates a truth assignment for all the k variables.
The first step in Lipton’s method consists in gener-
ating all paths from v1 to vk. 
In analogy to the previous section, assign each on 
axis vertex to a single stranded oligomer and to each 
ak
o and ak
1. Then a path vi-1ai
jvi encodes the truth value 
of xi. Lipton’s method begins with a test tube con-
taining all possible paths, then selectively removing 
all the molecules for which the first clause is not 
net, the selectively removing all molecules for which 
the second clause is not met, and so on. When the 
number of molecules remaining becomes two small 
the polymerase chain reaction can be run. If at the 
end of the sequence of reactions the tube becomes 
empty, there is no solution to the SAT problem.
We see thus that a great many graph theoretic 
questions can be answered, in principle at least by 
DNA computation. There will of course be practical 
difficulties as the size of problems increases, associ-
ated with the statistics of binding, but the true poten-
tial of DNA computing has yet to be reached.
—Donald Franceschetti, PhD
Bibliography
Adelman, L. M. Molecular computation of solutions 
to combinatorial problems, Science 226, 1021-1024 
(1994).
Calude, C. S., and Gheorge Paun, Computing with Cells 
and Atoms. (New York: Taylor & Francis, 2001. Print.
Lipton, R. J., and E. B. Baum, eds., DNA Based 
Computers, 
DIMACS 
Series 
in 
Discrete 
Mathematics, and Theoretical Computer Science, 
27, American -Mathematical Society (1995).

192
Motherboards
Principles of Computer Science
Motherboards
Fields of Study
Computer Engineering; Information Technology
Abstract
The motherboard is the main printed circuit board 
inside a computer. It has two main functions: to sup-
port other computer components, such as random 
access memory (RAM), video cards, sound cards, and 
other devices; and to allow these devices to communi-
cate with other parts of the computer by using the cir-
cuits etched into the motherboard, which are linked 
to the slots holding the various components.
Prinicipal Terms
 core voltage: the amount of power delivered to the 
processing unit of a computer from the power supply.

 crosstalk: interference of the signals on one cir-
cuit with the signals on another, caused by the two 
circuits being too close together.

 printed circuit board: a flat copper sheet shielded 
by fiberglass insulation in which numerous lines 
have been etched and holes have been punched, 
allowing various electronic components to be con-
nected and to communicate with one another and 
with external components via the exposed copper 
traces.

 trace impedance: a measure of the inherent re-
sistance to electrical signals passing through the 
traces etched on a circuit board.

 tuning: the process of making minute adjustments 
to a computer’s settings in order to improve its 
performance.
Evolution of Motherboards
The motherboard of a computer is a multilayered 
printed circuit board (PCB) that supports all of the 
computer’s other components, which are secondary 
to its functions. In other words, it is like the “mother” 
of other, lesser circuit boards. It is connected, either di-
rectly or indirectly, to every other part of the computer.
In the early days of computers, motherboards con-
sisted of several PCBs connected either by wires or 
by being plugged into a backplane (a set of intercon-
nected sockets mounted on a frame). Each necessary 
computer component, such as the central processing 
unit (CPU) and the system memory, required one or 
more PCBs to house its various parts. With the advent 
and refinement of microprocessors, computer com-
ponents rapidly shrank in size. While a CPU in the 
late 1960s consisted of numerous integrated circuit 
(IC) chips attached to PCBs, by 1971 Intel had pro-
duced a CPU that fit on a single chip. Other essential 
and peripheral components could also be housed in 
a single chip each. As a result, the motherboard could 
support a greater number of components, even as it 
too was reduced in size. Sockets were added to sup-
port more peripheral functions, such as mouse, key-
board, and audio support.
In addition to being more cost effective, this con-
solidation of functions helped make computers run 
faster. Sending information from point to point on a 
computer takes time. It is much faster to send infor-
mation directly from the motherboard to a periph-
eral device than it is to send it from the CPU PCB 
across the backplane to the memory PCB, and then 
from there to the device. 
Motherboard Design
Designing a motherboard is quite challenging. 
The main issues arise from the presence of a large 
number of very small circuits in a relatively small 
area. One of the first considerations is how best to 
arrange components on the motherboard’s various 
layers. A typical PCB consists of sheets of copper 
separated by sheets of fiberglass. The fiberglass 
insulates the copper layers from each other. Most 
motherboards consist of six to twelve layers, though 
more or fewer layers are also possible. Certain layers 
typically have specific functions. The outer layers 
are signal layers, while other layers carry voltage, 
ground returns, or carry memory, processor, and 
input/output data.
Lines etched in the fiberglass insulating each 
layer allow the familiar copper lines, or traces, to 
show through. These traces conduct the electrical 
signals. Most motherboard designers use computer 
simulations to determine the optimal length, width, 
and route of the individual traces. For example, a 
motherboard will have a target trace impedance 
value, often fifty or sixty ohms, which must be kept 
constant. Widening a trace will decrease impedance, 
while narrowing it will make it greater. Another issue 

193
Principles of Computer Science
Motherboards
is crosstalk resulting from the high level of circuit 
density. If this happens, traces must be either better 
insulated or moved farther apart so that interference 
will diminish.
Some sophisticated computer users may try 
tuning their systems by adding or removing moth-
erboard components, adjusting power settings, 
or “overclocking” the CPU to make it run faster. 
Overclocking can be risky, as it typically requires in-
creasing the core voltage. Most motherboards have 
a built-in voltage regulator to ensure that the core 
voltage does not exceed the recommended voltage 
for the CPU and other processors. However, some 
regulators allow users to adjust their settings. While 
there is usually a buffer zone between the 
recommended voltage and the maximum 
safe voltage, setting the voltage too high can 
still cause processors to overheat or even 
burn out.
Form Factor
Though somewhat standardized, moth-
erboards still come in different sizes and 
shapes. The main distinction is between 
motherboards for laptops and those for 
desktops. Motherboards designed for one 
of these categories generally will not fit into 
the other category. Most large desktop com-
puter cases have enough room inside for 
just about any model of desktop mother-
board, though smaller motherboards leave 
more space for peripherals to be added 
later.
BIOS
A motherboard will have some basic soft-
ware embedded in a read-only memory 
(ROM) chip. This software is called the 
BIOS, which stands for “basic input/output 
system.” When the power button is pressed, 
the BIOS tells the computer what devices 
to activate in order to locate the operating 
system and begin running it. If a computer 
malfunctions, it may be necessary to use the 
BIOS to change how the motherboard be-
haves while the system starts up.
—Scott Zimmer, JD
Bibliography
Andrews, Jean. A+ Guide to Hardware: Managing, 
Maintaining, and Troubleshooting. 6th ed. Boston: 
Course Tech., 2014. Print.
Andrews, Jean. A+ Guide to Managing and Maintaining 
Your PC. 8th ed. Boston: Course Tech., 2014. Print.
Cooper, Stephen. “Motherboard Design Process.” 
MBReview.com. Author, 4 Sept. 2009. Web. 14 Mar. 
2016.
Englander, Irv. The Architecture of Computer Hardware, 
Systems Software, & Networking: An Information 
Technology Approach. 5th ed. Hoboken: Wiley, 2014. 
Print.
A motherboard is the main printed circuit board (PCB) of a computer. Also 
known as a logic board or mainboard, it connects the CPU to memory and to pe-
ripherals. Often it includes hard drives, sound cards, network cards, video cards, 
and other components. By .kkursor, public domain, via Wikimedia Commons.

194
Multiprocessing operating systems
Principles of Computer Science
Mueller, Scott. Upgrading and Repairing PCs. 22nd ed. 
Indianapolis: Que, 2015. Print.
Roberts, Richard M. Computer Service and Repair. 4th 
ed. Tinley Park: Goodheart, 2015. Print.
White, Ron. How Computers Work: The Evolution of 
Technology. Illus. Tim Downs. 10th ed. Indianapolis: 
Que, 2015. Print.
Multiprocessing operating systems
Fields of Study
Computer Engineering; Operating Systems
Abstract
A multiprocessing operating system (OS) is one in 
which two or more central processing units (CPUs) 
control the functions of the computer. Each CPU 
contains a copy of the OS, and these copies commu-
nicate with one another to coordinate operations. 
The use of multiple processors allows the computer 
to perform calculations faster, since tasks can be di-
vided up between processors.
Prinicipal Terms

 central processing unit (CPU): sometimes de-
scribed as the “brain” of a computer, the col-
lection of circuits responsible for performing 
the main operations and calculations of a com-
puter.

 communication architecture: the design of com-
puter components and circuitry that facilitates the 
rapid and efficient transmission of signals between 
different parts of the computer.

 parallel processing: the division of a task among 
several processors working simultaneously, so that 
the task is completed more quickly.

 processor coupling: the linking of multiple pro-
cessors within a computer so that they can work to-
gether to perform calculations more rapidly. This 
can be characterized as loose or tight, depending 
on the degree to which processors rely on one an-
other.

 processor symmetry: multiple processors sharing 
access to input and output devices on an equal 
basis and being controlled by a single operating 
system.
Multiprocessing versus Single-
Processor Operating Systems
Multiprocessing operating systems (OSs) perform 
the same functions as single-processor OSs. They 
schedule and monitor operations and calculations 
in order to complete user-initiated tasks. The dif-
ference is that multiprocessing OSs divide the work 
up into various subtasks and then assign these sub-
tasks to different central processing units (CPUs). 
Multiprocessing uses a distinct communication ar-
chitecture to accomplish this. A multiprocessing OS 
needs a mechanism for the processors to interact 
with one another as they schedule tasks and coordi-
nate their completion. Because multiprocessing OSs 
rely on parallel processing, each processor involved 
in a task must be able to inform the others about 
how its task is progressing. This allows the work of 
the processors to be integrated when the calculations 
are done such that delays and other inefficiencies are 
minimized. Multiprocessing operating systems can 
handle tasks more quickly, as each CPU that becomes 
available can access the shared memory to complete 
the task at hand so all tasks can be completed the 
most efficiently.
For example, if a single-processor OS were running 
an application requiring three tasks to be performed, 
one taking five milliseconds, another taking eight 
milliseconds, and the last taking seven milliseconds, 
the processor would perform each task in order. The 
entire application would thus require twenty milli-
seconds. If a multiprocessing OS were running the 
same application, the three tasks would be assigned 
to separate processors. The first would complete the 
first task in five milliseconds, the second would do 
the second task in eight milliseconds, and the third 
would finish its task in seven milliseconds. Thus, the 
multiprocessing OS would complete the entire task 
in eight milliseconds. From this example, it is clear 
that multiprocessing OSs offer distinct advantages.

195
Principles of Computer Science
Multiprocessing operating systems
coupling
Multiprocessing OSs can be designed in a number 
of different ways. One main difference between 
designs is the degree to which the processors com-
municate and coordinate with one another. This is 
known as processor coupling. Coupling is classifi ed 
as either “tight” or “loose.” Loosely coupled multi-
processors mostly communicate with one another 
through shared devices rather than direct channels. 
For the most part, loosely coupled CPUs operate 
independently. Instead of coordinating their use of 
devices by directly communicating with other pro-
cessors, they share access to resources by queueing 
for them. In tightly coupled systems, each CPU is 
more closely bound to the others in the system. They 
coordinate operations and share a single queue for 
resources.
One type of tightly coupled multiprocessing 
system has processors share memory with each other. 
This is known as symmetric multiprocessing (SMP). 
Processor symmetry is present when the multipro-
cessing OS treats all processors equally, rather than 
prioritizing a particular one for certain operations. 
Multiprocessing OSs are designed with special fea-
tures that support SMP, because the OS must be able 
to take advantage of the presence of more than one 
processor. The OS has to “know” that it can divide up 
certain types of tasks among different processors. It 
must also be able to track the progress of each task so 
that the results of each operation can be combined 
CPU
Memory
Disks
Dis
Single
Processor
Symmetric
Multiprocessing
Memory
CPU
CPU
CPU
CPU
Disks
Dis
Clustered Symmetric Processing
Communications
Facility
Memory
CPU
CPU
CPU
CPU
Memory
CPU
CPU
CPU
CPU
Disks
Dis
Disks
Dis
Multiprocessing operating systems can handle tasks more quickly, as each CPU that becomes available can access the shared memory to 
complete the task at hand so all tasks can be completed the most effi ciently. EBSCO illustration.

196
Multitasking operating systems
Principles of Computer Science
once they conclude. In contrast, asymmetric multi-
processing occurs when a computer assigns system 
maintenance tasks to some types of processors and 
application tasks to others. Because the type of task 
assigned to each processor is not the same, they are 
out of symmetry. SMP has become more common-
place because it is usually more efficient.
Multitasking
The advent of multiprocessing OSs has had a major 
influence on how people perform their work. 
Multiprocessing OSs can execute more than one pro-
gram at a time. This enables computers to use more 
user-friendly interfaces based on graphical represen-
tations of input and output. It allows users with rela-
tively little training to perform computing tasks that 
once were highly complex. They can even perform 
many such tasks at once.
Multiprocessing OSs, though once a major inno-
vation, have become the norm rather than the ex-
ception. As each generation of computers must run 
more and more complex applications, the processing 
workload becomes greater and greater. Without the 
advantages offered by multiple processors and OSs 
tailored to take advantage of them, computers would 
not be able to keep up.
—Scott Zimmer, JD
Bibliography
Garrido, José M., Richard Schlesinger, and Kenneth 
E. Hoganson. Principles of Modern Operating Systems. 
2nd ed. Burlington: Jones, 2013. Print.
Gonzalez, Teofilo, and Jorge Díaz-Herrera, eds. 
Computing Handbook: Computer Science and Software 
Engineering. 3rd ed. Boca Raton: CRC, 2014. Print.
Sandberg, Bobbi. Networking: The Complete Reference. 
3rd ed. New York: McGraw, 2015. Print.
Silberschatz, Abraham, Peter B. Galvin, and 
Greg Gagne. Operating Systems Concepts. 9th ed. 
Hoboken: Wiley, 2012. Print.
Stallings, William. Operating Systems: Internals and 
Design Principles. 8th ed. Boston: Pearson, 2014. 
Print.
Tanenbaum, Andrew S., and Herbert Bos. Modern 
Operating Systems. 4th ed. Boston: Pearson, 2014. 
Print.
Multitasking operating systems
Fields of Study
Computer Science; Operating Systems
Abstract
A multitasking operating system (OS) is one that can 
work on more than one task at a time by switching be-
tween the tasks very rapidly. The tasks may all pertain 
to a single user or to multiple users. A multitasking 
OS can save the current state of each user and task 
so that it does not lose its place when it comes back 
to a task to resume its work. This allows the system to 
switch smoothly between tasks.
Prinicipal Terms

 context switch: a multitasking operating system 
shifting from one task to another; for example, 
after formatting a print job for one user, the com-
puter might switch to resizing a graphic for an-
other user.

 cooperative multitasking: an implementation of 
multitasking in which the operating system will 
not initiate a context switch while a process is 
running in order to allow the process to com-
plete.

 hardware interruption: a device attached to a com-
puter sending a message to the operating system to 
inform it that the device needs attention, thereby 
“interrupting” the other tasks that the operating 
system was performing.

 multiprocessing: the use of more than one cen-
tral processing unit to handle system tasks; this 
requires an operating system capable of dividing 
tasks between multiple processors.

 preemptive multitasking: an implementation of 
multitasking in which the operating system will ini-
tiate a context switch while a process is running, 

197
Principles of Computer Science
Multitasking operating systems
usually on a schedule so that switches between 
tasks occur at precise time intervals.
 
 time-sharing: the use of a single computing re-
source by multiple users at the same time, made 
possible by the computer’s ability to switch rapidly 
between users and their needs.
time-sharing and multitasking
Multitasking operating systems (OSs) are based on 
the idea of time-sharing. Multitasking OSs do not ac-
tually perform multiple tasks at the same exact time. 
A particular central processing unit (CPU) can only 
do one thing at any given moment. Instead, they give 
the appearance of multitasking by switching back 
and forth between separate tasks for intervals of time 
too small for humans to perceive. This makes it seem 
to the user that multiple tasks are being carried out 
at once.
In the early days of computer science, when com-
puters were huge machines requiring a lot of effort 
to maintain, researchers observed that it wasted time 
for a computer to be used by only one person at once. 
This was because the typical workfl ow of a computer 
operator was to submit a task requiring processing 
time, wait for the process to complete, and then re-
ceive the output. After receiving the output, the user 
evaluated it and prepared the next set of inputs. This 
time between tasks was wasted as the computer was 
idle instead of processing. The solution was to allow 
multiple users to interact with the computer. That 
way, while the computer was waiting for a response 
from one user, it could switch over to another user 
who was ready to proceed. This presented some prob-
lems in the early days of computing. Computers were 
not robust enough to easily store all of a user’s con-
text information (a description of the current state 
Operating System
Task 1
Task 2
Task 3
Task 4
CPU
3
Multitasking operating systems use a single CPU to work on a number of tasks. The operating system deter-
mines which task the CPU will work on at any given time, pausing tasks as needed, so that all tasks are com-
pleted as effi ciently as possible. EBSCO illustration.

198
Multitasking operating systems
Principles of Computer Science
of the user’s tasks and progress). However, with the 
passage of time and the use of multiprocessing com-
puters, this changed.
Cooperation and Preemption
Multitasking OSs can take a variety of forms. One 
difference is the way in which they handle the con-
text switch between different users and tasks. A mul-
titasking OS must have some way of deciding when 
is the appropriate time to make a context switch. 
The switch requires processing resources in order 
to be correctly executed, and switching contexts at 
random would reduce the system’s predictability 
and efficiency over time. One method of deciding 
the timing of context switching is cooperative multi-
tasking. With this method, the OS will not interrupt 
a process with a context switch while the process 
is running but will instead wait for the process to 
finish.
An alternative approach is preemptive multi-
tasking. Unlike cooperative multitasking, preemp-
tive multitasking does not wait for a process to finish 
before initiating a context switch. Instead, a mul-
titasking OS using preemption will automatically 
switch between processes at defined intervals. This 
allocates resources evenly among all processes. This 
can be more resource intensive because it requires 
the system to store information about each process 
state. That way, the process can smoothly resume 
when its turn comes round again.
Sometimes, other events can intrude on the OS’s 
time-sharing. An example is hardware interruption. 
This occurs when a device attached to the computer 
(such as a printer, scanner, or disk drive) requires the 
attention of the OS. It sends an interrupt signal to the 
CPU. This might occur when a printer experiences a 
paper jam, for instance. It would interrupt the com-
puter’s multitasking in order to signal the computer 
to display an error message to the user.
Prioritizing Processes
Multitasking OSs must often assign different pri-
ority levels to different processes. One reason for 
this is that some processes have more of an impact 
on the user’s real-time experience than others do. 
If a computer were busy keeping track of the time 
and preparing a system backup when a user closed 
an application window, the OS would prioritize the 
tasks associated with closing the window ahead of 
the others. Doing this gives the user the impres-
sion that the computer is operating smoothly. This 
would not be the case if the window being closed 
were to freeze in place until the conclusion of the 
backup process.
Multitasking places heavy demands on a com-
puter. The computer must store enough informa-
tion about each process to be able to quickly return 
to the task where it left off. As applications have 
grown more complex and more reliant on multi-
tasking, hardware has struggled to keep pace. Each 
year, computers are brought to market with more 
memory, faster processors, and other hardware up-
grades necessary for computers to do more work in 
less time.
—Scott Zimmer, JD
Bibliography
Ben-Ari, M. Principles of Concurrent and Distributed 
Programming. 2nd ed. New York: Addison, 2006. 
Print.
Hart, Archibald D., and Sylvia Hart Frejd. The Digital 
Invasion: How Technology Is Shaping You and Your 
Relationships. Grand Rapids: Baker, 2013. Print.
Kaptelinin, Victor, and Mary P. Czerwinski, eds. 
Beyond the Desktop Metaphor: Designing Integrated 
Digital Work Environments. Cambridge: MIT P, 
2007. Print.
Lee, John D., and Alex Kirlik, eds. The Oxford 
Handbook of Cognitive Engineering. New York: 
Oxford UP, 2013. Print.
Sinnen, Oliver. Task Scheduling for Parallel Systems. 
Hoboken: Wiley, 2007. Print.
Walker, Henry M. The Tao of Computing. 2nd ed. Boca 
Raton: CRC, 2013. Print.

199
Principles of Computer Science
Multithreading operating systems
Multithreading operating systems
Fields of Study
Computer Science; Operating Systems; Computer 
Engineering
Abstract
Multithreading operating systems (OSs) are OSs 
designed to increase the speed of computing by 
working on multiple threads, which are parts of a 
programming sequence. There are various types of 
multithreading processes, based on how various types 
of threads are handled by the OS. Multithreading 
is beneficial for increasing computing speed, but 
makes programming more complicated for software 
designers and creates additional potential for appli-
cation errors.
Prinicipal Terms

 address space: the amount of memory allocated 
for a file or process on a computer.

 context switching: pausing and recording the 
progress of a thread or process such that the pro-
cess or thread can be executed at a later time.

 fiber: a small thread of execution using coopera-
tive multitasking.

 multitasking: the process of executing multiple 
tasks concurrently on an operating system (OS).

 process: the execution of instructions in a com-
puter program.

 thread: the smallest part of a programmed se-
quence that can be managed by a scheduler in an 
OS.
The Basics of Multithreading
Multithreading is a way of increasing operating 
system (OS) efficiency. It does this by dividing the 
way that the OS handles processes, which occur when 
a computer executes instructions within a computer 
program. Multithreading OSs divide processes into 
subprocesses called threads. Threads are the smallest 
part of a programmed process that can be managed 
independently. In multithreading, the OS operates 
on multiple threads at the same time, or switches 
rapidly between threads. This essentially allows the 
OS to work on multiple processes simultaneously or 
to finish a process more efficiently. Multithreading 
is a form of multitasking that occurs within applica-
tions. Multitasking is the act of handling more than 
one task or application at the same time at the system 
level.
Memory and Context
Processes are the building blocks of applications and 
all computing functions. All processes within a com-
puter are managed by a scheduler. The scheduler is 
typically built into the OS and determines when and 
in what order various tasks should be completed. 
Each program requires a certain amount of memory, 
representing the resources available to complete the 
task. The amount of memory allocated to a process, 
application, or program is called the address space. 
To manage ongoing processes, OSs engage in con-
text switching. Context switching means that a pro-
cess or a thread within a process may be essentially 
put on hold while the computer switches between 
tasks.
While each process and each application requires 
its own address space, threads share the same address 
space. The way that OSs handle threads depends on 
the computer’s processor. The processor is the essen-
tial circuitry that processes tasks through a system of 
electrical capacitors and transistors. Single-processor 
computers switch back and forth between threads 
so rapidly that the threads appear to be processed 
at the same time. Multicore processors can process 
more than one thread at a time by dividing threads 
between processors. Because threads share the same 
resources and address space, switching between 
threads is much easier for most computers than 
switching between processes.
There are two basic types of threads, user 
threads and kernel threads. Threads can also be 
classified according to their complexity. The term 
fiber is used for extremely simple threads that re-
quire little processing. User threads are those that 
are created by user interaction or are built into a 
program while kernel threads are built into the 
OS. During processing each user thread is linked 
to a kernel thread. This provides a link between 
the computer’s central processing and the applica-
tion. 3 Science Reference Center™ Multithreading 
Operating Systems

200
Multithreading operating systems
Principles of Computer Science
multithreading os models
In the one-to-one model, each user thread is linked 
to a specifi c kernel thread. Older OSs, like Windows 
95 and XP, use a one-to-one model. This model 
places a limit on the number of threads that can be 
used to handle a process. In the many-to-one model, 
all application threads are mapped to a single kernel 
thread. This model is widely used in single-pro-
cessor computers because multiple threads cannot 
be scheduled simultaneously. In a many-to-many 
model, user threads can be mapped onto an equal 
or smaller number of kernel threads. The many-to-
many OS model is best for multiprocessor computers 
because it allows the computer to process multiple 
user threads at once.
benefits and drawbacks of 
multithreading
Multithreading allows a program to continue run-
ning in the background while still accepting new user 
input or while another task is being completed. This 
helps to make programs more responsive. Multicore 
processors have gradually become the norm in 
personal computing, so designing OSs to handle 
multithreading provides faster processing overall. 
Dividing processes into threads is also more effi cient 
than running multiple instances of an entire pro-
gram. Data from multiple users or different inputs 
can be integrated more effi ciently through threads 
than through separate processes. This is because 
threads already share programs, code, and address 
space, while processes are usually independent of 
one another.
Creating hardware and software for multi-
threading is diffi cult, however, and coding for mul-
tithreading applications requires specialized knowl-
edge. In some cases, it can be more diffi cult to 
locate and identify errors in multithreading systems. 
Problems in the code or errors in multithreading 
systems can also lead to system lag or can crash an 
entire process. Errors in the execution of any single 
thread can threaten the progress of an entire process. 
Therefore, multithreading essentially adds a variety 
of new variables that can lead to system-wide errors 
and processing issues.
—Micah L. Issitt
Register
Stack
CPU
Registers
Registers
Registers
Stack
Stack
Stack
CPU
Multi-threaded process
Single-threaded process
Multithreading operating systems allow more than one part of a process to be conducted simultaneously by a single CPU. 
Unlike multiprocessing, which uses multiple CPUs working on non- overlapping tasks, multithreading allows full resource 
sharing so each thread can be dispersed among multiple CPUs in a system EBSCO illustration.

201
Principles of Computer Science
Multitouch displays
Bibliography
“About Threads and Processes.” MSDN.Microsoft. 
Windows, n.d. Web. 15 Mar 2016.
Aravind, Alex A., and Sibsankar Haldar. Operating 
Systems. Upper Saddle River: Pearson, 2010. Print.
Ballew, Joli, and Ann McIver McHoes. Operating 
Systems DeMYSTiFieD. New York: McGraw, 2012. 
Print.
“Differences 
between 
Multithreading 
and 
Multitasking for Programmers.” NI. National 
Instruments, 20 Jan. 2014. Web. 15 Mar 2016.
Herlihy, Maurice, and Nir Shavit. The Art of 
Multiprocessor Programming. New York: Elsevier, 
2012. Print.
Tanenbaum, Andrew S., and Herbert Bos. Modern 
Operating Systems. 4th ed. New York: Pearson, 2014. 
Print.
Multitouch displays
Fields of Study
Software Engineering; Information Technology; 
Mobile Platforms
Abstract
Devices with multitouch displays allow a user to touch 
a screen to enter commands and data. Traditional 
touchscreens could only respond to a single touch, 
as when a user touches a button on the screen. 
Multitouch displays can interpret touches from more 
than one conductive surface, such as two or more 
fingers, allowing for a greater range of inputs and 
responses.
Prinicipal Terms

 force-sensing touch technology: touch display that 
can sense the location of the touch as well as the 
amount of pressure the user applies, allowing for a 
wider variety of system responses to the input.

 gestures: combinations of finger movements used 
to interact with multitouch displays in order to ac-
complish various tasks. Examples include tapping 
the finger on the screen, double-tapping, and 
swiping the finger along the screen.

 optical touchscreens: touchscreens that use op-
tical sensors to locate the point where the user 
touches before physical contact with the screen 
has been made.

 projected capacitive touch: technology that uses 
layers of glass etched with a grid of conductive 
material that allows for the distortion of voltage 
flowing through the grid when the user touches 
the surface; this distortion is measured and used 
to determine the location of the touch.

 resistive touchscreens: touchscreens that can lo-
cate the user’s touch because they are made of 
several layers of conductive material separated by 
small spaces; when the user touches the screen, 
the layers touch each other and complete a circuit.

 surface capacitive technology: a glass screen 
coated with an electrically conductive film that 
draws current across the screen when it is touched; 
the flow of current is measured in order to deter-
mine the location of the touch.
Innovative Touchscreen Technology
Multitouch displays are an innovation in computer 
input technology. Input technology takes several 
forms. Users can enter information into a computer 
using a keyboard. They type commands that the com-
puter executes. Users can also move a mouse to con-
trol the movement of a cursor on the screen. More re-
cently, touchscreens began to be used. Touchscreens 
display functions that users can activate by touching 
graphics on the screen. Multitouch displays take this 
capability a step further. They allow users to touch the 
screen with more than one finger at once. Users can 
combine touch and gestures to issue commands to 
the computer. For example, if a user wished to move a 
photo on a multitouch display screen, the user could 
touch the screen and slide the photo across it, just 
as if they were sliding a piece of paper across a table. 
A multitouch display senses the user’s touches and 
movement and determines how the image should ap-
pear to move.

202
Multitouch displays
Principles of Computer Science
From an engineering standpoint, there are several 
ways to design multitouch displays. Each has its own 
method for the computer to sense multiple touches 
and movements and then to use software to inter-
pret what these touches and movements mean. Some 
methods rely primarily on electrical conductivity 
between the user (a person’s fingers, most often) 
and the display. Surface capacitive technology uses 
a conductive film to coat the display. When the user 
touches the film, a circuit is completed and the dis-
play can detect where it was touched. Projected ca-
pacitive touch uses the same basic idea of detecting 
completed circuits, but instead of a conducting film 
coating the display, a grid of conductive material is 
etched into the display’s glass.
Optics and Pressure-Sensing
Some multitouch displays rely on optics instead of ca-
pacitive touch. Optical touchscreens use visual sensors, 
which are like tiny cameras. They detect when an object 
is about to touch the display and at what location. These 
systems can be preferable when touchscreen input 
from nonconductive objects may be necessary. For in-
stance, a major drawback of capacitive touchscreens is 
that one cannot use them while wearing gloves. An op-
tical touchscreen, on the other hand, will still respond 
to a touch from a gloved hand because it detects the 
touch without relying on conductivity. This means that 
multitouch displays located outdoors in cold 
climates will be much more user-friendly if 
they have optical touchscreens.
Another category of multitouch displays 
uses neither optics nor capacitive touch. 
Instead, it can sense the amount of force 
the user employs when touching the dis-
play. This category is described as force-
sensing touch technology. Within this cat-
egory, resistive touchscreens are the most 
common form. Resistive touchscreens can 
tell how hard a user is pressing because they 
contain several layers of material separated 
by tiny spaces. When a user touches the dis-
play lightly, only a few layers are compressed 
enough to come into contact with one an-
other. A touch with greater force behind it 
will cause more layers to compress. As layers 
touch one another, they complete a circuit 
and send a signal to the device about the lo-
cation and amount of force used.
Advantages
One main advantage to using multitouch displays is 
that they allow users with little or no training to input 
information that would otherwise be time-consuming 
to enter into a computer or that would require more 
technical knowledge than the average user has. For 
example, a user wishing to rotate an image to a more 
pleasing orientation would probably not know that 
they want to turn the image precisely 37 degrees to 
the right. Even if the user did know how many de-
grees to rotate the image, typing this into a computer 
or using a mouse to make the adjustment would take 
much more care than simply touching the corner of 
the image and sliding a finger in an arc to the right.
Pinch to Zoom
Not surprisingly, much of the growth in multitouch 
displays has been driven by the mobile computing in-
dustry. As ever more computing power is packed into 
ever smaller devices, consumers find themselves navi-
gating websites on ever smaller screens. One of the 
more frequent multitouch gestures used is the pinch-
to-zoom option. This feature lets one magnify a par-
ticular part of the information being displayed on the 
screen, as if one were zooming in with a microscope.
—Scott Zimmer, JD
Multitouch displays allow users to manipulate things onscreen using multiple 
fingers at once. Standard gestures cause specific actions for objects onscreen 
(e.g., touching at two points and turning to rotate), much as specific mouse 
gestures are expected to cause specific actions (e.g., click and hold to drag an 
object). Kenneth M Pennington, CC BY 3.0 (http://creativecommons.org/li-
censes/by/3.0), via Wikimedia Commons.

203
Principles of Computer Science
Multi-user operating systems
Bibliography
Horspool, Nigel, and Nikolai Tillmann. Touchdevelop: 
Programming on the Go. New York: Apress, 2013. 
Print.
Hughes, John F. Computer Graphics: Principles and 
Practice. Upper Saddle River: Addison, 2014. Print.
Moss, Frank. The Sorcerers and Their Apprentices: How the 
Digital Magicians of the MIT Media Lab Are Creating 
the Innovative Technologies That Will Transform Our 
Lives. New York: Crown Business, 2011. Print.
Rogers, Scott. Swipe This!: The Guide to Great Touchscreen 
Game Design. Chichester: Wiley, 2012. Print.
Saffer, Dan. Designing Gestural Interfaces. Beijing: 
O’Reilly, 2008. Print.
Soares, Marcelo M., and Francisco Rebelo. Advances 
in Usability Evaluation. Boca Raton: CRC, 2013. 
Print.
Multi-user operating systems
Fields of Study
Computer Science; Operating Systems
Abstract
A multi-user operating system (OS) is one that can 
be used by more than one person at a time while run-
ning on a single machine. Different users access the 
machine running the OS through networked ter-
minals. The OS can handle requests from users by 
taking turns among connected users. This capability 
is not available in a single-user OS, where one user 
interacts directly with a machine with a single-user 
operating system installed on it.
Prinicipal Terms

 multi-terminal configuration: a computer configu-
ration in which several terminals are connected to 
a single computer, allowing more than one person 
to use the computer.

 networking: connecting two or more computers to 
one another using physical wires or wireless con-
nections.

 resource allocation: a system for dividing com-
puting resources among multiple, competing re-
quests so that each request is eventually fulfilled.

 system: a computer’s combination of hardware 
and software resources that must be managed by 
the operating system.

 terminals: a set of basic input devices, such as a 
keyboard, mouse, and monitor, that are used to 
connect to a computer running a multi-user oper-
ating system.

 time-sharing: a strategy used by multi-user oper-
ating systems to work on multiple user requests by 
switching between tasks in very small intervals of 
time.
Multi-User Operating Systems
A computer’s operating system (OS) is its most fun-
damental type of software. It manages the computer 
system (its hardware and other installed software). 
An OS is often described as the computer’s “traffic 
cop.” It regulates how the different parts of the com-
puter can be used and which users or devices may use 
them. Many OS functions are invisible to the user of 
the computer. This is either because they occur auto-
matically or because they happen at such a low level, 
as with memory management and disk formatting.
A multi-user OS performs the same types of op-
erations as a single-user OS. However, it responds to 
requests from more than one user at a time. When 
computers were first developed, they were huge, 
complex machines that took up a great deal of phys-
ical space. Some of the first computers took up whole 
rooms and required several people to spend hours 
programming them to solve even simple calculations. 
These origins shaped the way that people thought 
about how a computer should work. Computers be-
came more powerful and able to handle more and 
more complex calculations in shorter time periods. 
However, computer scientists continued to think of 
a computer as a centralized machine usable by more 
than one person at a time through multiple terminals 
connected by networking. This is why some of the 
earliest OSs developed, such as UNIX, were designed 

204
Multi-user operating systems
Principles of Computer Science
with a multi-terminal configuration in mind. Given 
the nature of early computers, it made more sense 
to share access to a single computer. Only years later, 
when technology advanced and PCs became widely 
available and affordable, would the focus switch to 
single-user OSs. Multi-user operating systems are 
designed to have multiple terminals (monitor, key-
board, mouse, etc.) all connected to a single main-
frame (a powerful CPU with many microprocessors) 
that allocates time for each user’s processing de-
mands so that it appears to the users that they are all 
working simultaneously.
Shared Computing
In order for an OS to be able to serve multiple users 
at once, the system needs to have either multiple 
processors that can be devoted to different users or 
a mechanism for dividing its time between multiple 
users. Some multi-user OSs use both strategies, since 
there comes a point at which it becomes impractical 
to continue adding processors. A multi-user OS may 
appear to be responding to many different requests 
at once. However, what is actually happening inside 
the machine is that the computer 
is spending a very small amount 
of time on one task and then 
switching to another task. This 
is called time-sharing. This task 
switching continues at a speed 
that is too fast for the user to de-
tect. It therefore appears that sep-
arate tasks are being performed 
at once. Multi-user OSs function 
this way because if they handled 
one task at a time, users would 
have to wait in line for their re-
quests to be filled. This would be 
inefficient because users would 
be idle until their requests had 
been processed. By alternating 
between users, a multi-user OS 
reduces the amount of time spent 
waiting. Time-sharing is one as-
pect of resource allocation. This 
is how a system’s resources are di-
vided between the different tasks 
it must perform.
One example of a multi-user 
OS is the software used to run 
the servers that support most webmail applications. 
These systems have millions or even billions of users 
who continually log on to check their messages, so 
they require OSs that can handle large numbers of 
users at once. A typical webmail application might re-
quire hundreds of computers, each running a multi-
user OS capable of supporting thousands of users at 
once.
Economy of Scale
Many companies are moving back toward the use 
of multi-user OSs in an effort to contain costs. For 
a large organization to purchase and maintain full-
featured computers for each employee, there must 
be a sizable investment in personnel and computer 
hardware. Companies seeking to avoid such expense 
find that it can be much more cost effective to deploy 
minimally equipped terminals for most users. This al-
lows them to connect to servers running multi-user 
OSs. Backing up user data is also simpler with a multi-
user OS because all of the data to be backed up is 
on the same machine. Therefore, it is less likely that 
1
2
3
4
Mainframe
Computing by user over time
0.00 s
0.01 s
0.02 s
Time
1
1
2
3
4
3
2
Multi-user operating systems are designed to have multiple terminals (monitor, keyboard, 
mouse, etc.) all connected to a single mainframe (a powerful CPU with many microproces-
sors) that allocates time for each user’s processing demands so that it appears to the users 
that they are all working simultaneously. EBSCO illustration.

205
Principles of Computer Science
Multi-user operating systems
users will lose their data and it saves time and money 
for the organization.
—Scott Zimmer, JD
Bibliography
Anderson, Thomas, and Michael Dahlin. Operating 
Systems: Principles and Practice. West Lake Hills: 
Recursive, 2014. Print.
Garrido, José M, Richard Schlesinger, and Kenneth 
E. Hoganson. Principles of Modern Operating Systems. 
Burlington: Jones, 2013. Print.
Holcombe, Jane, and Charles Holcombe. Survey of 
Operating Systems. New York: McGraw, 2015. Print.
Silberschatz, Abraham, Peter B. Galvin, and Greg 
Gagne. Operating System Concepts Essentials. 2nd ed. 
Wiley, 2014. Print.
Stallings, William. Operating Systems: Internals and 
Design Principles. Boston: Pearson, 2015. Print.
Tanenbaum, Andrew S. Modern Operating Systems. 
Boston: Pearson, 2015. Print.

206
N
Natural language processing
Fields of Study
Algorithms, Biotechnology, Programming Language
Abstract
Natural language processing, or NLP, is a type of ar-
tificial intelligence that deals with analyzing, under-
standing, and generating natural human languages 
so that computers can process written and spoken 
human language without using computer-driven lan-
guage. Natural language processing, sometimes also 
called “computational linguistics,” uses both seman-
tics and syntax to help computers understand how 
humans talk or write and how to derive meaning from 
what they say. This field combines the power of artifi-
cial intelligence and computer programming into an 
understanding so powerful that programs can even 
translate one language into another reasonably accu-
rately. This field also includes voice recognition, the 
ability of a computer to understand what you say well 
enough to respond appropriately.
Prinicipal Terms

 algorithm: a set of step-by-step instructions for per-
forming computations.

 anthropomorphic: resembling a human in shape 
or behavior; from the Greek words anthropos 
(human) and morphe (form).

 artificial intelligence: the intelligence exhibited by 
machines or computers, in contrast to human, or-
ganic, or animal intelligence.

 neural network: in computing, a model of infor-
mation processing based on the structure and 
function of biological neural networks such as the 
human brain.

 process: the execution of instructions in a com-
puter program.

 semantics: a branch of linguistics that studies the 
meanings of words and phrases

 syntax: a branch of linguistics that studies how 
words and phrases are arranged in sentences to 
create meaning.
The Beginnings of Natural Language 
Processing
It has long been a dream of scientists, inventors, and 
computer programmers to make a robot, computer, 
or program, such as a voice response program, that 
can be mistaken for a human. Alan Turing once 
said, “ A computer would deserve to be called intel-
ligent if it could deceive a human into believing it 
was human.” One of the roadblocks to creating a 
machine like this is that human language has been 
nearly impossible for machines to understand and re-
spond to appropriately. 
That hasn’t stopped people from trying. Many 
early science fiction stories are based on the idea 
of a robot as a human. Early in the 1950s, program-
mers attempted to get computers to be able to un-
derstand language enough to be able to translate 
from one language to another. Success was lim-
ited. An undocumented (and probably not exactly 
true) story that is told about early attempts to have 
computers understand human language correctly 
goes like this: A programmer typed “The spirit is 
willing but the flesh is weak” into a computer pro-
gram that was supposed to translate the sentence 
into Russian, which it did. Then the programmer 
asked the computer to translate the Russian sen-
tence back into English. The result was “The vodka 
is good, but the meat is rotten.” As you can see, this 
makes some sense if you read the original sentence 
very literally, but the meaning of the sentence was 
completely lost.

207
Principles of Computer Science
Natural language processing
Why It’s So Hard For Computers to 
Understand Us
Humans learn and use language in a way that is difficult 
for computers to understand. As a simple example, here 
is a sentence that might mean different things: “Baby swal-
lows fly.” Is “baby” a noun or an adjective? Is “swallows” 
a verb or a noun? Is “fly” a noun or a verb? Depending 
on the context of the conversation, a human is likely to 
understand this ambiguous sentence. However, a com-
puter, without any anthropomorphic understanding, is 
not likely to understand the linguistic structure.
Computer programmers have made great strides 
in this field. They have combined the linguistic fields 
of semantics and syntax with powerful computer pro-
grams using neural network processes that “learn” 
how to look for the same kinds of signals humans look 
for to create meaning. What words are surrounding 
the words we want to understand? In our example 
above, if “birds” are mentioned anywhere around 
the phrase, the computer program can “understand” 
that this sentence is talking about small birds that fly. 
If “insect” or “child” is mentioned anywhere around 
the sentence, the computer can “understand” that 
this sentence means that a small child ate an insect. 
This is a very simple example that doesn’t really give 
the scope of the power behind computer programs 
that perform natural language processing but is easy 
to understand.
Ways Natural Language Processing 
Works
Programmers use a variety of techniques to help ma-
chines understand natural language. For example, 
automatic summarization consists of two techniques, 
extraction or abstraction. Extraction is a technique 
that attempts to extract the most important segments 
of the text and make a summary list of it. Abstraction, 
which is much more complex, involves writing a sum-
mary of the information. 
Sentiment analysis tries to identify the emotions 
conveyed in a text. For example, on a trip review site, a 
program would try to identify whether a review was pos-
itive or negative based on the words used in the review, 
such as “liked,” “enjoyed,” “unhappy,” or “problem.” 
Text classification is a way to assign predefined 
categories to a text. For example, your email spam 
detector determines whether a message is spam or 
something you actually want to see, with various de-
grees of success. Other types of text classification tech-
niques organize, for example, news stories by topics, 
such as sports or headlines. There are text classifica-
tion programs, called author attribution, that are so 
sophisticated that they can determine who wrote the 
text based on the style of writing, word frequency, 
vocabulary richness, phrase structure, and sentence 
length. Some people have used a program like this to 
determine, for example, whether Shakespeare really 
wrote all of the plays attributed to him.
Conversational agents are systems that attempt 
to have a conversation with a human. You may have 
seen this in customer service situations, where it is 
often called a “chatbot.” You may not recognize right 
at first that you are not talking to a human, but the 
program often gives itself away—eventually you re-
alize that you are not chatting with a human based on 
an unpredictable response in the dialogue.
Natural Language Processing in 
Everyday Applications
It is almost impossible to imagine a world without 
computers translating one language into another. 
Programs like Google Translate exist for translating 
nearly any language into nearly any other language. 
Even FaceBook has a “translate” feature where if your 
friends type in Spanish, you can read it in English. As 
you have probably noticed, it’s not perfect, but it’s 
generally pretty close. This kind of translation works 
best in fields or with text where the vocabulary is well 
known and there are few idioms used. For example, 
a machine translation of a technical manual could 
work well, but a translation of a novel or short story is 
often almost comical.
Natural language processing as voice recognition 
is also everywhere. The iPhone feature “Siri” takes a 
question that you ask and makes enough sense of it 
to be able to answer your question adequately most 
of the time. You can speak to your phone and ask it 
to call someone for you or give you directions to get 
somewhere. Other applications are when your email 
recognizes an event and suggests you add it to your 
calendar (called information extraction). These 
types of features seemed like science fiction at one 
time but are now part of our everyday lives.
—Marianne Moss Madsen, MS

208
Networking: routing and switches
Principles of Computer Science
Bibliography
Bird, Steven, Ewan Klein, and Edward Loper. Natural 
Language Processing with Python, 2nd ed. O’Reilly 
Media, 2017.
Clark, Alexander, Chris Fox, and Shalom Lappin 
(eds.) The Handbook of Computational Linguistics 
and Natural Language Processing. Wiley-Blackwell, 
2012.
Flach, Peter. Machine Learning: The Art and Science 
of Algorithms that Make Sense of Data. Cambridge 
University Press, 2012.
Ingersoll, Grant S., Thomas S. Morton, and Drew 
Farris. Taming Text: How to Find, Organize, and 
Manipulate It. Manning Publications, 2013.
Jurafsky, Daniel, and James H. Martin. Speech and 
Language Processing: An Introduction to Natural 
Language Processing, Computational Linguistics, and 
Speech Recognition, PEL, 2008.
Kumar, Ela. Natural Language Processing. I K 
International Publishing House, 2011.
Mihalcea, Rada and Radev Dragomir. Graph-based 
Natural 
Language 
Processing 
and 
Information 
Retrieval. Cambridge University Press, 2011.
Watanabe, Shinji, and Jen-Tzung Chien. Bayesian 
Speech 
and 
Language 
Processing. 
Cambridge 
University Press, 2015.
Networking: routing and switches
Fields of Study
Information Technology; Network Design
Abstract
Routing and switches are two vital parts of any 
computer network. A switch is a piece of hard-
ware that connects devices on a network, directing 
communications traffic within the network as ter-
minals and hardware devices send data to one an-
other. Routing is the part of the network protocol 
that directs individual packets of data to the right 
destination.
Prinicipal Terms

 bridge: a connection between two or more net-
works, or segments of a single network, that al-
lows the computers in each network or segment to 
communicate with one another.

 firewall: a virtual barrier that filters traffic as it en-
ters and leaves the internal network, protecting 
internal resources from attack by external sources.

 gateway: a device capable of joining one network 
to another that has different protocols.

 node: any point on a computer network where 
communication pathways intersect, are redistrib-
uted, or end (i.e., at a computer, terminal, or 
other device).

 packet forwarding: the transfer of a packet, or unit 
of data, from one network node to another until it 
reaches its destination.

 packet switching: a method of transmitting data 
over a network by breaking it up into units called 
packets, which are sent from node to node along 
the network until they reach their destination and 
are reassembled.
Routing
Routing is the method by which a packet of data 
is transmitted to its destination. In computer net-
works, routing is accomplished by packet switching. 
This term describes how packets of data are moved 
from one node to another in a network. A network 
is made up of multiple nodes, which can be devices 
such as printers or computers. Nodes can also be 
network hardware such as switches, which direct 
network traffic. These devices communicate with 
each other by sending messages over the network. 
Each message is divided into units of data called 
“packets,” which are sent across the network individ-
ually and then reassembled once they all reach their 
destination. Each packet may take a different path 
to the destination, because when a packet reaches 
a node, it is forwarded to another node based on 
the state of the network at that instant. This is called 
packet forwarding.

209
Principles of Computer Science
Networking: routing and switches
History of network routing
The reason that data is transmitted via packet 
switching is due to the origins of the Internet. The 
Internet began as a research project of the US 
Department of Defense, which wanted to design a 
communication network that could function even 
if some parts of the network failed. A direct line of 
communication, such as a telephone wire, could 
be easily disrupted by cutting the wire. However, a 
packet-switched network, with many possible path-
ways from origin to destination, would be less vul-
nerable. If one part of the network is damaged, the 
packets can be routed around the affected nodes and 
reach their destination by another path. This com-
munication method proved so successful that the re-
sulting networks continued to grow and connect with 
one another, network to network, until they formed 
the Internet as it is known today.
INTERNET
Router
Router
Router
Switch
Switch
Firewall
Firewall
Firewall
Server
Switches are built into computer circuitry and systems to augment the signal fl ow throughout the system. The arrangement of switches and 
the criteria that open and close them will determine what components of the system are used. EBSCO illustration.

210
Networking: routing and switches
Principles of Computer Science
Communication within and between 
Networks
The power of the Internet is due in part to the ro-
bust performance of packet-switched networks and 
in part to the fact that it is essentially a network made 
up of other networks. The fact that networks can be 
connected to each other has allowed the Internet to 
grow at a phenomenal rate. Several different types of 
hardware are used to make this happen.
When an organization decides to build its own 
network, it uses a switch to connect the different de-
vices that will use the network. In effect, the switch 
creates the network. In many cases, an organization 
then divides its internal network into smaller sub-
networks, or “subnets.” This is done when groups 
of computers need to communicate often with each 
other but only rarely with other computers in the 
organization. It is more efficient to set such groups 
apart so that the subnet’s communications do not 
compete with other traffic on the network. Different 
subnets are connected to one another through a net-
work bridge. A bridge can also be used to connect 
separate networks.
In order to communicate outside of the organiza-
tion, a local network must be connected to the rest 
of the Internet. This is done by configuring one or 
more gateways, typically using a piece of hardware 
called a “router.” Making this connection can leave 
the local network vulnerable to external attack. To 
prevent this, a common practice is to set up a firewall 
on the gateway to filter out suspicious activity, such as 
hackers probing the network to see if it is vulnerable.
Internet Access
When a user on a local network goes online to 
check the weather forecast, that person uses their 
computer to send a request to the server containing 
the weather data. The request is broken down into 
multiple packets of data. Packets are transmitted be-
tween nodes on the local network until they reach 
the gateway. They then travel out into the Internet, 
where they eventually reach their destination and are 
reassembled. The response is sent from the server 
back to the local user in the same way.
Understanding the nature and function of routing 
and switches is important for a number of reasons. 
Among the most important is that it helps people 
understand the mechanics of protecting their com-
puters from hackers, viruses, and other dangers 
found on the Internet.
—Scott Zimmer, JD
Bibliography
Agrawal, Manish. Business Data Communications. 
Hoboken: Wiley, 2011. Print.
Comer, Douglas E. Computer Networks and Internets. 
6th ed. Boston: Pearson, 2015. Print.
Elahi, Ata, and Mehran Elahi. Data, Network, and 
Internet Communications Technology. Clifton Park: 
Thomson, 2006. Print.
Kurose, James F., and Keith W. Ross. Computer 
Networking: A Top-Down Approach. 6th ed. Boston: 
Pearson, 2013. Print.
Mir, Nader F. Computer and Communication Networks. 
2nd ed. Upper Saddle River: Prentice, 2015. 
Print.
Mueller, Scott. Upgrading and Repairing PCs. 22nd ed. 
Indianapolis: Que, 2015. Print.
Serpanos, Dimitrios N., and Tilman Wolf. Architecture 
of Network Systems. Burlington: Morgan, 2011. 
Print.

211
Principles of Computer Science
Neural networks
Neural networks
Fields of Study
Network Design; Computer Engineering; Robotics 
Computer Science
Abstract
Neural networks, or artificial neural networks 
(ANNs) are modeled on the structure and intercon-
nectedness of neurons in biological brains. They have 
a three-layered structure consisting of an input layer, 
a hidden layer, and an output layer. Like a biological 
system, the input layer acquires data and delivers it 
to the hidden layer for processing. When a suitable 
solution to the input data has been determined, the 
hidden layer delivers it to the output layer. ANNs are 
controlled by the system architecture, the activity 
function and the learning function, enabling them 
to learn, or “train,” by adjusting to changing input in-
formation. Inputs are assigned different weights and 
activities, and computation is carried out using “fuzzy 
logic” as the key to artificial intelligence. ANNs learn 
and compute most commonly through the backprop-
agation algorithm, which is akin to negative feed-
back, to minimize error by successive approximation. 
When the ANN has determined the solution having 
the least error, this becomes the output of the system. 
Typically, an ANN of this type gives no indication of 
how the solution was obtained. Neural networks can 
be analog or digital, and can be constructed as elec-
tronic, optical, hydraulic and software-simulated sys-
tems. Parallel processing is the most compatible type 
of computer system for software-simulated ANNs. 
Several types of ANN can be defined according to the 
logic principles by which they function.
Prinicipal Terms

 artificial intelligence: the intelligence exhibited by 
machines or computers, in contrast to human, or-
ganic, or animal intelligence. 

 bridge: a connection between two or more net-
works, or segments of a single network, that al-
lows the computers in each network or segment to 
communicate with one another. 

 nervous (neural) system: the system of nerve path-
ways by which an organism senses changes in itself 
and its environment and transmits ­electrochemical 
signals describing these changes to the brain so 
that the brain can respond. 

 neural network: in computing, a model of infor-
mation processing based on the structure and 
function of biological neural networks such as the 
human brain. 

 processor coupling: the linking of multiple pro-
cessors within a computer so that they can work to-
gether to perform calculations more rapidly. This 
can be characterized as loose or tight, depending 
on the degree to which processors rely on one an-
other. 
Basic Concept
The concept of a neural network is founded in the 
multiple interconnections of neurons in the brain, 
which is the ultimate component of the nervous 
(neural) system. Individual brain cells are called 
“neurons,” and each has the ability to communicate 
with adjacent cells. The structure of each brain cell 
consists of a central body, from which a number of 
“axons” protrude. Each neuron can have several 
axons, that form connections with other axons. The 
“synapse” is the interface where axons from two dif-
ferent neurons come into contact with each other, 
and where the exchange of information takes place. 
Between actual brain neurons, this process involves 
the exchange of special compounds called “neu-
rotransmitters.” Each connection between neurons 
that is formed in this way represents a new access 
route to and from the knowledge that is stored in 
the brain, and it is this multiplicity of intercon-
nections that is the key to the processing capacity 
of the brain. This is analogous to processor cou-
pling in parallel computing systems, which use a 
network of central processing units (CPUs) each 
linked by a bridge to carry out a large number of 
computing processes at the same time rather than 
one after another in a single CPU. Data input to a 
biological brain comes from an “input layer” com-
posed of the physical senses. This “data” is pro-
cessed in the “hidden layer” of neurons within the 
brain. The result of the processing is then sent to an 
“output layer” of the muscles appropriate to effect 
the desired physical response. Processing within 
the brain is not a binary, deterministic process of 

212
Neural networks
Principles of Computer Science
computation with “on” and “off” signals. It is a prob-
abilistic process that uses many “partial values” of 
varying importance at the same time. This kind of 
processing is called “fuzzy logic,” a term coined by 
Lofti Zadeh in 1965. Fuzzy logic considers weighted 
values of data according to the degree to which they 
are true rather than as absolute “black or white” 
truths. Artificial neural networks are constructed 
on these same principles, and as an integral com-
ponent of machine learning and parallel processing 
are believed to be essential to the development of 
artificial intelligence.
Artificial Neural Networks
An artificial neural network can be analog or dig-
ital, and may be electronic, optical, hydraulic or 
software-simulated in nature. For the purposes of 
computer science, however, only electronic and 
optical constructs are considered. The principal 
electronic components are digital CPUs. Optical 
components that use light for data transmission 
and storage are still in early development, though 
they are expected to provide greatly enhanced 
computational and operational abilities in com-
parison to silicon-based processors. An artificial 
neural network (ANN) has a functional structure 
analogous to that of the brain, consisting of an 
input layer, a hidden or intermediate layer that 
carries out the computation and processing of 
data, and an output layer. In an ANN, the inter-
activity of the processor network is structured to 
mimic the interactivity of neurons in the brain and 
each of the processors in the ANN is effectively 
connected to all of the other processors in the net-
work. The interconnectivity may be electronic or 
optical in transistor-transistor based communica-
tion, or the entire network may be simulated by 
software. Currently, ANNs are physically limited to 
just a few thousands of concurrent processors in 
the largest interconnected systems, in what is ef-
fectively a two-dimensional array. By comparison, 
the human brain contains about 100 billion (1011) 
neurons, each with multiple interconnections in a 
three-dimensional array. Miniaturization of inte-
grated circuitry, the development of three-dimen-
sional integrated circuits, and the development of 
quantum computers is expected to produce a very 
large increase in the power of ANNs.
Specification of ANNs
Three factors are typically used to specify an ANN. 
The first is the architecture of the system. The ar-
chitecture specifies the variables involved in the 
composition and functioning of the ANN and 
typically assigns weights, or relative importance, 
to the elements in the input layer and assigns 
their respective activities. The second factor is 
the “activity rule,” which describes how the weight 
values of the inputs change as they respond to 
each other. The third factor is the “learning rule,” 
which specifies how the input weights change over 
time. The learning rule functions on a longer 
time scale than the activity rule and modifies the 
weights of the various interconnections according 
to the input patterns that are received. The most 
common learning rule used is based on the back-
propagation of error, or what might otherwise be 
termed negative feedback, for the adjustment of 
the weights of input signals. Through repetition 
of backpropagated error correction the variance 
of the computed result from its actual value is 
minimized, and triggers the output of the result. 
Effectively, the functioning of an ANN can be 
thought of as simply a repeated process of ap-
proximation to achieve the outcome that has the 
highest likelihood of being true.
Input Weighting
The weighting of inputs to an ANN is based on 
the logical principle of the “threshold logic unit” 
(TLU). The TLU is a processing unit for threshold 
values having n inputs but just one output. The 
TLU returns an output of 1 when the sum of the 
proportional, or weighted, values of all inputs is 
greater than or equal to the threshold value, and 
an output of 0 otherwise. In effect it is a “truth test” 
of the input conditions. For example, given the 
threshold value of 3 and the number of inputs as 
6, then the TLU will return a value of 1 if the total 
input over all six inputs is 3 or more. It will return 
a value of 0 if the total input over all 6 inputs is less 
than 3.

213
Principles of Computer Science
Neural networks
Sample Problem
A water tank releases water at a rate of 3 liters 
per minute. It is connected to 6 hoses that 
bring fresh water into the tank at different 
rates. To maintain a constant level of water in 
the tank, fresh water must be introduced at 
the same rate of 3 liters per minutes using all 
6 hoses. An indicator on the side of the tank 
points to 1 if the water level rises, or to 0 if the 
water level decreases. When fully open, each 
hose brings 1 liter of water per minute into 
the tank, and this rate is controlled by oper-
ating the hoses only fractionally. Determine 
whether the indicator will point to 1 or 0 when 
the 6 hoses are operated at 0.1, 0.75, 0.2, 0.5, 
0.33 and 0.25 of capacity, respectively
Answer:
The TLU for this system is that the sum of all 
6 hose inputs must be 3 liters per minute or 
greater for the indicator to point to 1, else it 
will point to 0. Therefore,
∑ (0.1 + 0.75 + 0.2 + 0.5 + 0.33 + 0.25) 
(1 liter per minute)
= 2.13 liters per minute
Since this value is less than the 3 liters per 
minute required to maintain the constant 
water level, the water level in the tank will de-
crease and the indicator will point to 0.
This is an example of a hydraulic neural 
network, and clearly demonstrates the con-
cept of weighted inputs as it applies to neural 
networks generally.
Uses and Limitations of ANNs
The development of ANNs is currently still very basic, 
although algorithms for the concept were first pro-
duced in 1947. The limiting factor since that time 
has always been the limitations inherent in available 
computing capabilities. General applications of the 
concept could not be produced until computational 
ability became sufficiently small and agile enough to 
be feasible. While it is tempting to think of neural 
networks in terms of supercomputers, the first gen-
erally available device to make use of an ANN for 
the control of its function was a household vacuum 
cleaner that could self-adjust its operational settings 
according to the floor-level conditions that it encoun-
tered. A similar system has since been incorporated 
into many other kinds of household appliances. The 
feature of interest in these, and in all ANNs, is that 
they have the ability to learn , or “train,” to carry out 
tasks. An exceptional example of this is the “Roomba” 
floor sweeping device and its competitors. The task 
the device is required to learn is the most efficient 
manner of sweeping a mapped space of the physical 
dimensions of the floor. Neural networks learn their 
tasks by capturing associations and regularities within 
patterns, just as a Roomba captures associations and 
regularities within its map of a floor area. They also 
are very effective in applications involving a large 
volume of diverse data that includes numerous vari-
ables for input, or when the relationships between 
variables are poorly understood. Similarly, ANNs are 
useful when the relationships between input variables 
are not described well or responsive to conventional 
computational approaches. The major problem with 
the functioning of ANNs that rely on backpropa-
gation is that their computation strategies are not 
transparent to the user. They function ultimately in 
a “black box” manner to produce an output from 
specified inputs, without providing any indication of 
how that output was achieved. In another context, 
a software-simulated ANN consisting of several pro-
cessor nodes can be slow to run on a single computer, 
since that computer consists of just a single processor 
node. Consequently, it must compute the result from 
each of the software-simulated nodes before the 
back-propagation of error can be investigated. This 
feature renders parallel processing computers more 
capable for the manipulation of software-simulated 
ANNs. Progress in the fields of parallel processing 
and quantum computing to increase the ability of 
neural networks to perform increasingly complex 
computations and “learn as they go” will bring with it 
corresponding advances in the development of artifi-
cial intelligence.
—Richard M. Renneboog M.Sc.

214
Neuromorphic chips
Principles of Computer Science
Bibliography
Galushkin, Alexander I. Neural Network Theory. New 
York, NY: Springer, 2007. Print.
Graupe, Daniel. Principles of Artificial Neural Networks. 
2nd ed. Hackensack, NJ: World Scientific, 2007. 
Print.
Hagan, Martin T., Howard B. Demuth, , Mark H. 
Beale, and Orlando de Jesús. Neural Network Design. 
2nd ed. Martin Hagan, 2014. Print.
Pandzu, Abhujit S. and Robert B. Macy. Pattern 
Recognition with Neural Networks in C++. Boca Raton, 
FL: CRC Press, 1996. Print.
Prasad, Bhanu, and S.R. Mahadeva Prasanna, eds. 
Speech, Audio, Image and Biomedical Signal Processing 
Using Neural Networks. New York, NY: Springer, 
2008. Print.
Priddy, Kevin L. and Keller, Paul E. Artificial Neural 
Networks, An Introduction. Bellingham, WA: SPIE 
Press, 2005. Print.
Rao, M. Ananda, and J. Srinavas. Neural Networks. 
Algorithms and Applications. Pangbourne, UK: 
Alpha Science International, 2003. Print.
Rogers, Joey. Object-Oriented Neural Networks in C++. 
New York, NY: Academic Press, 1997. Print.
Neuromorphic chips
Fields of Study
Computer Engineering; Information Systems
Abstract
Neuromorphic chips are a new generation of com-
puter processors being designed to emulate the way 
that the brain works. Instead of being locked into a 
single architecture of binary signals, neuromorphic 
chips can form and dissolve connections based on 
their environment, in effect “learning” from their 
surroundings. These chips are needed for complex 
tasks such as image recognition, navigation, and 
problem solving.
Prinicipal Terms

 autonomous: able to operate independently, 
without external or conscious control.

 Human Brain Project: a project launched in 2013 
in an effort at modeling a functioning brain by 
2023; also known as HBP.

 memristor: a memory resistor, a circuit that can 
change its own electrical resistance based on the 
resistance it has used in the past and can respond 
to familiar phenomena in a consistent way.

 nervous (neural) system: the system of nerve path-
ways by which an organism senses changes in itself 
and its environment and transmits electrochem-
ical signals describing these changes to the brain 
so that the brain can respond.

 neuroplasticity: the capacity of the brain to change 
as it acquires new information and forms new 
neural connections.
Brain-Based Design
Neuromorphic chips have much in common with 
traditional microprocessor chips. Both kinds of 
chip control how a computer receives an input, pro-
cesses that information, and then produces output 
either in the form of information, action, or both. 
The difference is that traditional chips consist of 
millions of tiny, integrated circuits. These circuits 
store and process information by alternating be-
tween binary “on” and “off” states. Neuromorphic 
chips are designed to mimic the way that the human 
body’s nervous (neural) system handles informa-
tion. They are designed not only to process infor-
mation but to learn along the way. A system that 
can learn may be more powerful than a system that 
must be programmed to respond in every possible 
situation. Systems that can learn can function au-
tonomously, that is, on their own without guidance. 
A good example of this is car navigation systems. 
These systems have to store or access detailed maps 
as well as satellite data about their current position. 
With this data, a navigation system can then plot 
a course. The neuromorphic chips may not need 
all of that background data. They may be better 
able to understand information about their imme-
diate surroundings and use it to predict outcomes 

215
Principles of Computer Science
Neuromorphic chips
based on past events. They would function much 
the way someone dropped off a few blocks from 
home could figure out a way to get there without 
consulting a map.
In order to design neuromorphic chips, engi-
neers draw upon scientific research about the brain 
and how it functions. One group doing such re-
search is the Human Brain Project (HBP). HBP is 
trying to build working models of a rodent brain. 
Eventually HBP will try to build a fully working 
model of a human brain. Having models like these 
will allow scientists to test hypotheses about how 
the brain works. Their research will aid in the de-
velopment of computer chips that can mimic such 
operations.
The Limits of Silicon
One reason researchers have begun to develop 
neuromorphic chips is that the designs of tradi-
tional chips are approaching the limits of their 
computational power. For many years, efforts were 
focused on developing computers capable of the 
type of learning and insights that the human brain 
can accomplish. These efforts were made on both 
the hardware and the software side. Programmers 
designed applications and operating systems to use 
data storage and access algorithms like those found 
in the neural networks of the brain. Chip designers 
found ways to make circuits smaller and smaller so 
they could be ever more densely packed onto con-
ventional chips. Unfortunately, both approaches 
have failed to produce machines that have either the 
brain’s information processing power or its neuro-
plasticity. Neuroplasticity is the brain’s ability to con-
tinuously change itself and improve at tasks through 
repetition.
Neuromorphic chips try to mimic the way the 
brain works. In the brain, instead of circuits, there 
are about 100 billion neurons. Each neuron is con-
nected to other neurons by synapses, which carry 
electrical impulses. The brain is such a powerful 
computing device because its huge amount of 
neurons and synapses allow it to take advantage of 
parallel processing. Parallel processing is when dif-
ferent parts of the brain work on a problem at the 
same time. Parallel processing allows the brain to 
form new connections between neurons when cer-
tain pathways have proven especially useful. This 
forming of new neural pathways is what happens 
when a person learns something new or how to do a 
Neuromorphic chips are designed to detect and predict patterns in data and processing pathways to improve 
future computing. They simulate the brain’s neuroplasticity, allowing for efficient abstraction and analysis of 
visual and auditory patterns. Each of the chips on this board has hundreds of millions of connections mim-
icking the synapses that connect neurons. By DARPA SyNAPSE, public domain, via Wikimedia Commons.

216
Neuromorphic chips
Principles of Computer Science
task more efficiently or effectively. The goal of neu-
romorphic chip designers is to develop chips that 
approach the brain’s level of computational density 
and neuroplasticity. For example, researchers have 
proposed the use of a memristor, a kind of learning 
circuit. To bring such ideas about, a chip architec-
ture completely different from the traditional bi-
nary chips is needed.
Complex Tasks
Neuromorphic chips are especially suitable for com-
puting tasks that have proven too intense for tradi-
tional chips to handle. These tasks include speech-
to-text translation, facial recognition, and so-called 
smart navigation. All of these applications require a 
computer with a large amount of processing power 
and the ability to make guesses about current and fu-
ture decisions based on past decisions. Because neu-
romorphic chips are still in the design and experi-
mentation phase, many more uses for them have yet 
to emerge.
The Way of the Future
Neuromorphic chips represent an answer to the 
computing questions that the future poses. Most 
of the computing applications being developed re-
quire more than the ability to process large amounts 
of data. This capability already exists. Instead, they 
require a device that can use data in many different 
forms to draw conclusions about the environment. 
The computers of the future will be expected to act 
more like human brains, so they will need to be de-
signed and built more like brains.
—Scott Zimmer, JD
Bibliography
Burger, John R. Brain Theory from a Circuits and 
Systems Perspective: How Electrical Science Explains 
Neuro-Circuits, Neuro-Systems, and Qubits. New York: 
Springer, 2013. Print.
Human Brain Project. Human Brain Project, 2013. 
Web. 16 Feb. 2016.
Lakhtakia, A., and R. J. Martín-Palma. Engineered 
Biomimicry. Amsterdam: Elsevier, 2013. Print.
Liu, Shih-Chii, Tobi Delbruck, Giacomo Indiveri, 
Adrian Whatley, and Rodney Douglas. Event-Based 
Neuromorphic Systems. Chichester: Wiley, 2015. 
Print.
Prokopenko, Mikhail. Advances in Applied Self-
Organizing Systems. London: Springer, 2013. Print.
Quian, Quiroga R., and Stefano Panzeri. Principles of 
Neural Coding. Boca Raton: CRC, 2013. Print.
Rice, Daniel M. Calculus of Thought: Neuromorphic 
Logistic Regression in Cognitive Machines. Waltham: 
Academic, 2014. Print.

217
O
Object-oriented design
Fields of Study
Software Engineering; Programming Language; 
Computer Science
Abstract
Object-oriented design (OOD) is an approach to 
software design that uses a process of defining ob-
jects and their interactions when planning code to 
develop a computer program. Programmers use con-
ceptual tools to transform a model into the specifica-
tions required to create the system. Programs created 
through OOD are typically more flexible and easier 
to write.
Prinicipal Terms

 attributes: the specific features that define an ob-
ject’s properties or characteristics.

 class-based inheritance: a form of code reuse in 
which attributes are drawn from a preexisting class 
to create a new class with additional attributes.

 class: a collection of independent objects that 
share similar properties and behaviors.

 method: a procedure that describes the behavior 
of an object and its interactions with other objects.

 object: an element with a unique identity and a de-
fined set of attributes and behaviors.

 prototypal inheritance: a form of code reuse in 
which existing objects are cloned to serve as pro-
totypes.
Advancing Software Development
Object-oriented design (OOD) was developed to 
improve the accuracy of code while reducing soft-
ware development time. Object-oriented (OO) sys-
tems are made up of objects that work together by 
sending messages to each other to tell a program 
how to behave. Objects in software design represent 
real-life objects and concerns. For example, in the 
code for a company’s human resources website, an 
object might represent an individual employee. An 
object is independent of all other objects and has its 
own status. However, all objects share attributes or 
features with other objects in the same class. A class 
describes the shared attributes of a group of related 
objects.
OOD typically follows object-oriented analysis 
(OOA), which is the first step in the software devel-
opment process when building an OO system. OOA 
involves planning out the features of a new program. 
OOD involves defining the specific objects and 
classes that will make up that program. The first OO 
language, Simula, was developed in the 1960s, fol-
lowed by Smalltalk in 1972. Examples of well-known 
OO programming languages include Ruby, C++, 
Java, PHP, and Smalltalk.
OOD consists of two main processes, system de-
sign and object design. First, the desired system’s 
architecture is mapped out. This involves defining 
the system’s classes. Using the example of a human 
resources website, a class might consist of employees, 
while an object in that class would be a specific em-
ployee. Programmers plan out the essential attributes 
of a class that are shared across all objects within the 
class. For example, all employees would have a name, 
a position, and a salary. The class defines these attri-
butes but does not specify their values. Thus, the class 
would have a field for the employee’s name, and the 
individual object for a specific employee might have 
the name Joe Smith. All the objects within a class also 
share methods, or specific behaviors. For example, 
employee objects could be terminated, given a raise, 
or promoted to a different position.
Object design is based on the classes that are 
mapped out in the system design phase. Once 
software engineers have identified the required 

218
Object-oriented design
Principles of Computer Science
classes, they are able to write code to create the 
attributes and methods that will define the nec-
essary objects. As development progresses, they 
may decide that new classes or subclasses need to 
be created. New classes can inherit features from 
existing classes through class-based inheritance. 
Inheritance is a form of code reuse. Class-based 
inheritance draws attributes from an existing class 
to create a new class. New attributes can then be 
added to the new class. This is distinct from proto-
typal inheritance, which involves cloning existing 
objects, called prototypes, instead of drawing from 
existing classes.
ood Principles and Applications
Software engineers often follow a set of OOD 
principles identifi ed by the acronym SOLID. 
These principles were fi rst compiled by software 
engineer and author Robert Cecil Martin. The 
SOLID principles provide guidance for better 
software development, maintenance, and ex-
pansion. These principles are:
Single responsibility principle: Each class 
should have only one job.
Open-closed principle: A class should be 
open for extension but closed to modifi cation 
(that is, it can be easily extended with new code 
without modifying the class itself).
Liskov substitution principle: Each subclass 
can be substituted for its parent class.
Interface segregation principle: Do not 
force clients to implement an interface (a list 
of methods) they do not use. Interfaces should 
be as small as possible. Having multiple smaller 
interfaces is better than having one large one.
Dependency inversion principle: High-level 
modules must not be dependent on low-level 
modules. Attempt to minimize dependencies 
between objects.
OOD is used to create software that solves real-
world problems, such as addressing a user’s 
needs when interfacing with an automatic teller 
machine (ATM). In this example, the software 
engineer fi rst defi nes the classes, such as the 
transaction, screen, keypad, cash dispenser, 
and bank database. Classes or objects can be 
thought of as nouns. Some classes, such as trans-
actions, might be further categorized into more 
specifi c subclasses, such as inquiry, withdrawal, 
deposit, or transfer.
Relationships between classes are then defi ned. 
For example, a withdrawal is related to the card 
reader, where the user inserts an ATM card, and the 
keypad, where the user enters their personal iden-
tifi cation number (PIN). Next, classes are assigned 
attributes. For a withdrawal, these might be the ac-
count number, the balance, and the amount of cash 
to withdraw.
Finally, methods or operations are assigned to 
each class, based on user needs identifi ed in the OOA 
phase. Methods can be thought of as verbs, such as 
Develop a new 
object/class/part
Test
application
Object-oriented 
design
Reusability survey 
of object/class/part 
library
Current
object/class/part 
library
Construct 
application
Object/ 
class/part 
available in 
library
Not available
Available
Include in 
library
The benefi t of object-oriented design lies in the reusability of the object 
data-base. Because each object has only one job, the code for an object 
remains applicable for any instance where the object is needed. EBSCO 
illustration.

219
Principles of Computer Science
Object-oriented design
execute, display, or withdraw. Detailed diagrams de-
scribe changes that happen within the objects, the 
activities they carry out, and how they interact with 
each other within the system.
Precautions and Drawbacks
Successful development with OOD requires realistic 
expectations. Possible problems with this approach 
include insufficient training, which can lead to the 
belief that OOD will clear up every development 
delay, resulting in missed deadlines. Further issues 
arise when timelines are shortened in expectation of 
OOD’s promise of speed. Thorough training is a pro-
grammer’s best course to understanding the limita-
tions and subtleties of OOD.
OOD has been a mainstay in software program-
ming and will be used for years to come, but advances 
in technology will require programming languages 
that can deliver faster search results, more accurate 
calculations, and better use of limited bandwidth 
for the applications users increasingly rely upon. In 
addition, OOD is not the best choice for every re-
quirement. It is appropriate for systems with a large 
amount of graphics, interfaces, and databases, but 
for simpler systems, task-oriented design (TOD) 
might be a better choice.
—Teresa E. Schmidt
Bibliography
Booch, Grady, et al. Object-Oriented Analysis and Design 
with Applications. 3rd ed. Upper Saddle River: 
Addison, 2007. Print.
Dennis, Alan, Barbara Haley Wixom, and David 
Tegarden. Systems Analysis and Design: An Object-
Oriented Approach with UML. 5th ed. Hoboken: 
Wiley, 2015. Print.
Garza, George. “Working with the Cons of Object 
Oriented Programming.” Ed. Linda Richter. Bright 
Hub. Bright Hub, 19 May 2011. Web. 6 Feb. 2016.
Harel, 
Jacob. 
“SynthOS 
and 
Task-Oriented 
Programming.” 
Embedded 
Computing 
Design. 
Embedded Computing Design, 2 Feb. 2016. Web. 
7 Feb. 2016.
Metz, Sandi. Practical Object-Oriented Design in Ruby: An 
Agile Primer. Upper Saddle River: Addison, 2012. 
Print.
Oloruntoba, Samuel. “SOLID: The First 5 Principles 
of Object Oriented Design.” Scotch. Scotch.io, 18 
Mar. 2015. Web. 1 Feb. 2016.
Puryear, Martin. “Programming Trends to Look for 
This Year.” TechCrunch. AOL, 13 Jan. 2016. Web. 7 
Feb. 2016.
Weisfeld, Matt. The Object-Oriented Thought Process. 4th 
ed. Upper Saddle River: Addison, 2013. Print.

220
Parallel processors
Fields of Study
Computer Engineering
Abstract
A single processor carries out program processes 
sequentially. With parallel processors, a program 
can be parallelized such that various independent 
processes in the program can be computed simul-
taneously, rather than sequentially. Large arrays of 
parallel processors, called supercomputers, can gen-
erally mitigate against the effects of excess heat gen-
eration and electrical noise that can interfere with 
successful computation. Amdahl’s Law describes the 
limit to which program execution can be sped up by 
parallelization.
Prinicipal Terms

 algorithm: a set of step-by-step instructions for per-
forming computations. 

 bridge: a connection between two or more net-
works, or segments of a single network, that al-
lows the computers in each network or segment to 
communicate with one another. 

 central processing unit (CPU): electronic circuitry 
that provides instructions for how a computer 
handles processes and manages data from applica-
tions and programs. 

 channel capacity: the upper limit for the rate at 
which information transfer can occur without 
error. 

 communication architecture: the design of com-
puter components and circuitry that facilitates the 
rapid and efficient transmission of signals between 
different parts of the computer. 

 processor coupling: the linking of multiple pro-
cessors within a computer so that they can work to-
gether to perform calculations more rapidly. This 
can be characterized as loose or tight, depending 
on the degree to which processors rely on one an-
other. 

 processor symmetry: multiple processors sharing 
access to input and output devices on an equal 
basis and being controlled by a single operating 
system. 

 supercomputer: an extremely powerful computer 
that far outpaces conventional desktop computers.
Parallel and Conventional Processors
A conventional computer is constructed around a 
single central processing unit (CPU) that normally 
carries out all computational activities, including 
those needed for active graphic displays such as those 
in simulation programs and video games, and com-
municates with a limited set of input/output (I/O) 
devices. The computing capability of the CPU de-
pends on the number of transistor structures that 
are etched onto the surface of the silicon chip that 
makes up the CPU. The architecture of the CPU 
chip includes registers for manipulating ‘bits’ of data 
through various Boolean calculation operations, and 
cache memory for the temporary storage of data be-
tween calculation operations and before writing to 
an I/O device. That  operations are carried out in 
a sequential manner limits the flow of operations, 
due to the size of the registers (the number of bits 
of data that can be held in a register at any one time) 
and the number of ‘clock cycles’ required for execu-
tion of an operation. This is alleviated by providing 
a ‘graphics processing unit’, or GPU, dedicated to 
the manipulation of graphics. The resultant sharing 
of computational duties between these separate pro-
cessing units is the essential feature of parallel pro-
cessing, although the CPU-GPU combination is not 
precisely what defines parallel processors and their 
P

221
Principles of Computer Science
Parallel processors
function. True parallel processors provide a means of 
by-passing the ‘bottleneck’ by carrying out sequential 
processes simultaneously in multiple registers across 
multiple CPUs.
Properties of Parallel Processors
Splitting computational processes across multiple 
processors not only by-passes the register size bot-
tleneck, it allows a much greater quantity of data to 
be processed. Just as the computational capacity of 
a single CPU is determined by the number of tran-
sistor structures that exist on that chip, the com-
putational capacity of a parallel processor system is 
determined by the number of individual CPUs that 
make up a particular system. The overall capacity 
of the resulting machine is such that it is often 
referred to as a supercomputer. Two major prob-
lems that have become increasingly important as 
transistor size has become smaller and CPUs more 
compact in size are heat and noise. The reason for 
the difficulties these two factors pose is the close 
proximity of transistor structures to each other. 
Heat generated by the flow of electrons through 
the material of the transistors must be dissipated ef-
fectively in order to prevent transistor failure due 
to overheating. Similarly, the closer in proximity 
transistor structures are to each other, the more 
readily do signals from one ‘leak’ into another, re-
sulting in scrambled data and inaccurate results. 
Electrical leakage also has the potential to short out 
and destroy a transistor structure, effectively de-
stroying the entire value of the CPU. Splitting pro-
cesses across multiple processors mitigates the heat 
generated by distributing it across the array so that 
it is rather less localized. In addition, the physical 
size of the array enables the incorporation of large-
scale cooling systems that would not be feasible for 
smaller or individual CPU systems. For example, 
supercomputers such as IBM’s ‘Blue’ series consist 
of CPU arrays housed in cabinetry large enough 
to occupy several hundred square meters of floor 
space. The cabinetry for such systems incorporates 
a refrigeration cooling system that maintains the 
entire array at its optimum operating temperature. 
Electrical noise can also be mitigated in supercom-
puter arrays of parallel processors by enabling use 
of CPUs having transistor structures etched on a 
larger scale, unlike the close-packed arrays of tran-
sistors on the CPU chips of stand-alone systems.
Parallelism and Concurrence
The structure of a parallel processor array requires 
a well-designed and implemented bridge system for 
processor coupling. Additionally, each implementa-
tion depends on a proper algorithm for distributing 
the computation task in the desired manner across 
the CPU array. This must take in the channel capacity 
of the system in order to avoid overloading and bot-
tlenecks in data flow. In most applications, the de-
sired function of the communication architecture is 
to produce process symmetry such that the computa-
tional load is evenly distributed among the CPUs in 
the parallel array. Two methodologies describe pro-
cess symmetry. One is concurrence, and the other is 
parallelism. Concurrent processes are different pro-
cesses being carried out on different processors at 
the same time. This is the condition achieved when 
processes ‘time-share’ on one or more CPUs. In ef-
fect, the CPU carries out a number of operations for 
one process, then switches over to carry out a number 
of operations for the other process. One might liken 
concurrence to working out two or more different 
crossword puzzles at the same time, filling in a few 
answers of one, then a few of another, until all are 
finished at roughly the same time. True parallelism, 
in comparison, can actually be carried out at bit level, 
but it is most often the case that the operational al-
gorithm divides the overall program into a number 
of different subtasks and program functions that can 
be processed independently. Upon completion, the 
individual results are combined to yield the solution 
or result of the computation. A third methodology 
called ‘distributed computing’ can utilize both con-
currency and parallelism, but typically requires that 
various CPUs communicate with each other for the 
completion of their respective tasks when the compu-
tation of one task relies on the result or a parameter 
being passed from another task on another CPU.
Amdahl’s Law
As a first approximation, increasing the number of 
processors in a parallel processor array would be ex-
pected to have a geometric effect on the performance 
of a parallel processing algorithm. That is to say, dou-
bling the number of processors would be expected to 
cut the processing time in half. Doubling the number 
of processors again would be expected to take up only 
one quarter of the original processing time. This is 
not observed, however, and while an increasing rate 

222
Parallel processors
Principles of Computer Science
relationship is observed for parallel processor sys-
tems having a fairly small number of processors, most 
processing rates tend to plateau rather than increase 
beyond the sixth or seventh processor doubling itera-
tion. The discrepancy is described as the latency of the 
system, and is described by Amdahl’s Law
Slatency(s) = (1 – p + p/s)-1
Latency is due to the fact that a program cannot 
be made completely parallel in execution; there is al-
ways some part of the program code that either does 
not translate to a parallel counterpart, or that must 
be run sequentially. Amdahl’s Law considers S. the 
‘speed up’ of the entire executable program, relative 
to s, the speed up of just the part of the program that 
can be made parallel, in terms of p, the percentage 
of the execution time of the program due to the part 
that can be parallelized before the parallelization oc-
curs. This fraction will always be detrimental to the 
extent to which the execution of program code can 
be sped up, regardless of the number of additional 
processors over which the program is spread. For ex-
ample, a program in which 10% of the code cannot 
be parallelized has a corresponding value of p = 0.9, 
and  cannot run any more than ten times faster. The 
addition of more processors than are required to 
achieve that result will have no further effect on the 
speed at which the program will run.
—Richard M. Renneboog M.Sc.
Bibliography
Fountain, T J. Parallel Computing: Principles and 
Practice. Cambridge: Cambridge University Press, 
1994. Print. 
Hughes, Cameron, and Tracey Hughes. Parallel 
and Distributed Programming Using C++. Boston: 
Addison-Wesley, 2004. Print. 
Jadhav, S.S. (2008) Advanced Computer Architecture & 
Computing. Pune, IND: Technical Publishers, 2008. 
Print.
Kirk, David B, and Wen-mei Hwu. Programming 
Massively Parallel Processors: A Hands-on Approach. 
Burlington, Massachusetts: Morgan Kaufmann 
Elsevier, 2013. Print. 
Lafferty, Edward L, Marion C. Michaud, and Myra 
J. Prelle. Parallel Computing: An Introduction. Park 
Ridge: Noyes Data Corporation, 1993. Print. 
Openshaw, Stan, and Ian Turton. High Performance 
Computing and the Art of Parallel Programming: An 
Introduction for Geographers, Social Scientists, and 
Engineers. London: Routledge, 2005. Print. 
Roosta, Seyed H. Parallel Processing and Parallel 
Algorithms: Theory and Computation. New York: 
Springer, 2013. Print. 
Tucker, Allen B, Teofilo F. Gonzalez, and Jorge L. 
Diaz-Herrera. Computing Handbook. Boca Raton, 
FL: CRC Press, 2014. Print. 

223
Principles of Computer Science
Personal health monitor technology
Personal health monitor technology
Fields of Study
Biotechnology
Abstract
Modern health care is largely based on the reliable 
and accurate gathering of patient information. 
By collecting and analyzing data over a period 
of time, health care providers can gain a deeper 
understanding of their patients and make better 
decisions. New technology has facilitated patients’ 
participation in their own health care in this re-
spect. However, the safeguarding of this informa-
tion has raised concerns about patient privacy and 
rights.
Prinicipal Terms

 biometrics: measurable physical or behavioral 
characteristics that can be used to describe, char-
acterize, or identify a given individual.

 data granularity: the level of detail with which data 
is collected and recorded.

 data integrity: the degree to which collected data 
is and will remain accurate and consistent.

 multimodal monitoring: the monitoring of sev-
eral physical parameters at once in order to better 
evaluate a patient’s overall condition, as well as 
how different parameters affect one another or 
respond to a given treatment.

 temporal synchronization: the alignment of sig-
nals from multiple devices to a single time stan-
dard, so that, for example, two different devices 
that record the same event will show the event 
happening at the exact same time.
Personal Health Data
Personal health monitoring involves gathering physi-
ological data and evaluating it by comparing it to a set 
of standard indicators. Digital technology has made 
this process much more efficient, both for individuals 
and in medical settings. New mobile applications, or 
“apps,” allow people to monitor different aspects of 
their health. A growing awareness of the importance 
of a healthy lifestyle has fueled the creation of new 
devices and applications (apps) for tracking exercise, 
nutrition, and other health indicators. Some of these 
devices are affordable and even free. However, con-
cerns have been raised about patient privacy rights 
and data accuracy and retention.
Tools for Monitoring Health
Ever more health-tracking devices are produced and 
purchased each year. A growing number of these 
are wearable tracking devices, such as fitness bands. 
Some companies participate in programs that make 
such tracking devices available to employees who 
wish to monitor their activity or fitness levels. These 
programs often provide incentives for employees to 
participate, such as prizes or cash rewards.
Other health-monitoring systems include apps 
that can be downloaded onto mobile devices. 
Advocates say that health apps encourage self-care 
and empower users by involving them more with 
their own health. Health care providers have become 
more supportive of quality wearable tracking devices 
because they reduce the need for appointments and 
make necessary appointments more efficient. In 
some cases, these devices can be connected to other 
forms of monitoring technology, providing useful 
data to medical personnel.
Multimodal monitoring is the monitoring of sev-
eral physical indicators at once. This typically involves 
connecting different monitoring tools to a central 
display. A growing number of health care providers 
are exploring the possible benefits of multimodal 
self-monitoring devices, particularly for chronic 
conditions. These systems may range from apps to 
remote monitoring and increasingly include bio-
metrics. Developers are creating devices that would 
connect with others in order to provide greater data 
granularity.
However, such devices require a greater level of pa-
tient involvement than regular trackers. Multimodal 
monitoring usually requires the support of techni-
cians to perform complex tasks such as temporal 
synchronization. The accuracy of the results can be 
skewed by even tiny differences in timekeeping.
Advantages and Disadvantages
The health data gathered by personal health-moni-
toring devices can be highly sensitive. Because data 
gathering and sharing methods vary from system 

224
Personal health monitor technology
Principles of Computer Science
to system, concerns have been raised about privacy 
and accuracy. Legally, the situation remains murky. 
Patient-rights regulations appear not to apply to in-
formation gathered by many of these devices. For 
example, the companies that provide device support 
are not bound by medical confidentiality require-
ments. It is important to ensure that these services 
have safeguards in place to protect patient informa-
tion and prevent the misuse of health data.
The benefits of innovative methods of self-mon-
itoring are undeniable. Devices have been devel-
oped that can warn of an impending heart attack or 
the possibility of an asthma event. However, some 
health experts are concerned that self-monitoring 
devices may also pose grave risks. Some argue that 
data integrity may be compromised. Others are con-
cerned about privacy issues, such as who has access 
to the information collected and how it will be used. 
Mental health experts have argued that excessive 
monitoring may even be harmful, as it tends to in-
crease anxiety and self-centeredness. Finally, some 
are worried because there is very little government 
regulation of these devices, particularly the nonin-
vasive type.
Sensors on health-monitoring devices are able to pick up heart-rate and blood pressure measurements. 
Advances in technology as a whole have improved the information gathered about a patient by making per-
sonal health-monitoring technology more portable and more accurate. By Bogdanradenkovic, CC BY-SA 4.0 
(http://creativecommons.org/licenses/by-sa/4.0), via Wikimedia Commons

225
Principles of Computer Science
Personal health monitor technology
Yet research has shown that self-monitoring can in 
fact help people who suffer from anxiety to become 
more aware of their triggers. Moreover, as users track 
their exercise and diet, they become better aware of 
how their daily actions affect their health. In other 
words, personal health monitoring encourages users 
to become more responsible for their well-being.
—Trudy Mercadal, PhD
Bibliography
Briassouli, 
Alexia, 
Jenny 
Benois-Pineau, 
and 
Alexander Hauptmann, eds. Health Monitoring and 
Personalized Feedback Using Multimedia Data. Cham: 
Springer, 2015. Print.
Cha, Ariana Eunjung. “Health and Data: Can Digital 
Fitness Monitors Revolutionise Our Lives?” 
Guardian. Guardian News and Media, 19 May 
2015. Web. 26 Feb. 2016.
Fasano, Philip. Transforming Health Care: The Financial 
Impact of Technology, Electronic Tools and Data Mining. 
Hoboken: Wiley, 2013. Print.
Gulchak, Daniel J. “Using a Mobile Handheld 
Computer to Teach a Student with an Emotional 
and 
Behavioral 
Disorder 
to 
Self-Monitor 
Attention.” Education and Treatment of Children 31.4 
(2008): 567–81. PDF file.
Havens, John C. Hacking Happiness: Why Your Personal 
Data Counts and How Tracking It Can Change the 
World. New York: Tarcher, 2014. Print.
McNeill, Dwight. Using Person-Centered Health Analytics 
to Live Longer: Leveraging Engagement, Behavior 
Change, and Technology for a Healthy Life. Upper 
Saddle River: Pearson, 2015. Print.
Paddock, Catharine. “How Self-Monitoring Is 
Transforming Health.” Medical News Today. 
MediLexicon Intl., 15 Aug. 2013. Web. 26 Feb. 
2016.
Schmidt, 
Silke, 
and 
Otto 
Rienhoff, 
eds. 
Interdisciplinary Assessment of Personal Health 
Monitoring. Amsterdam: IOS, 2013. Print.

226
Personalized medicine
Principles of Computer Science
Personalized medicine
Fields of Study
Applications; Information Systems
Abstract
Personalized medicine is the use of information 
about a patient’s biology—especially his or her per-
sonal genome—to provide custom tailored health 
care. Practitioners seek to use a patient’s genetic 
and biological characteristics to better under-
stand and predict his or her health risks and drug 
responses.
Prinicipal Terms

 biomarker: short for “biological marker”; a mea-
surable quality or quantity (e.g., internal tem-
perature, amount of iron dissolved in blood) that 
serves as an indicator of an organism’s health, or 
some other biological phenomenon or state.

 biometrics: measurements that can be used to 
distinguish individual humans, such as a person’s 
height, weight, fingerprints, retinal pattern, or ge-
netic makeup.

 genome-wide association study: a type of genetic 
study that compares the complete genomes of 
individuals within a population to find which ge-
netic markers, if any, are associated with various 
traits, most often diseases or other health prob-
lems.

 pharmacogenomics: the study of how an individ-
ual’s genome influences his or her response to 
drugs.

 toxgnostics: a subfield of personalized medicine 
and pharmacogenomics that is concerned with 
whether an individual patient is likely to suffer a 
toxic reaction to a specific medication.
Custom Health Care
No two people are identical. Individuals differ from 
one another in countless ways, some obvious (height, 
weight, sex) and some less so (hormone produc-
tion, brain chemistry, genetic makeup). Because of 
these differences, no two people get sick in exactly 
the same way. Personalized medicine seeks to use a 
patient’s biometrics to make predictions about his or 
her health and provide specialized treatment.
Personalized medicine researchers look for con-
nections between certain biomarkers and an individ-
ual’s health-related traits. Biomarkers are biometrics 
that correlate to important biological traits or condi-
tions, such as disease risk. For example, a low number 
of red blood cells in a patient’s blood is a biomarker 
for anemia.
Human Genomics
The backbone of personalized medicine is human ge-
nomics, the study of the human genome. “Genome” 
may refer to a species-wide or a personal genome. A 
species-wide genome is the order in which genes ap-
pear in the DNA of all members of one species. The 
particular alleles, or variants, of those genes differ be-
tween individuals. The Human Genome Project was 
a species-wide genome project, completed in 2003, 
that produced the first comprehensive map of the 
human genetic sequence. A personal genome is an 
individual’s unique DNA makeup, including both 
the genetic sequence and the specific alleles of each 
gene.
One powerful method of identifying potential ge-
netic biomarkers is a genome-wide association study 
(GWAS). This process involves studying the genomes 
of many individuals to search for correlations be-
tween certain genes and certain health traits. Once 
a potential genetic marker is identified, researchers 
can use more targeted methods to further explore 
the connection.
Examples of such biomarkers include mutations in 
the BRCA1 and BRCA2 genes, which produce tumor-
suppressing proteins in humans. Mutations in these 
genes are associated with a higher risk of cancer, 
particularly breast, ovarian, and prostate cancers. 
(The “BRCA” in the gene names stands for “breast 
cancer.”) This marker was discovered and described 
prior to GWAS technology, using a mix of patient 
health histories and more limited genetic analysis to 
identify the genes in question.
Personal genomics companies, such as 23 and 
Me, sequence individuals’ genomes to provide them 
with their genetic information. This includes infor-
mation about a person’s ancestry as well as health 
traits such as disease risk. Doctors use prenatal ge-
netic  screening to determine babies’ health risks 

227
Principles of Computer Science
Personalized medicine
even before birth. Personalized medicine may one 
day move from screening and preventive treatment 
to custom post-diagnosis treatment.
The Genomics of Drug Response
The vanguard of personalized medicine and human 
genomics is a subfield known as pharmacoge-
nomics. This field studies the influence of individ-
uals’ genomes on their responses to various drugs. 
Toxgnostics, a related subfield, focuses on whether a 
patient will have a toxic response to certain medica-
tions based on his or her personal genome. The goal 
of pharmacogenomics is to replace the “one size fits 
all” drug model with prescriptions tailored to each 
patient’s biology.
A Computation-Intensive Process
The amount of data contained in a single human ge-
nome is very large. When studying many individuals’ 
genomes, this information quickly 
grows unwieldy. Personalized medi-
cine, with its emphasis on genomics, 
is only made possible by extremely 
powerful computers and specialized 
software designed to analyze large 
amounts of data. Fortunately, tech-
nology has advanced very quickly in 
this field, so that functions that once 
took several years can now be done in 
days.
—Kenrick Vezina, MS
Bibliography
“A Catalog of Published Genome-
Wide Association Studies.” National 
Human Genome Research Institute. 
Natl. Insts. of Health, 16 Sept. 
2015. Web. 23 Dec. 2015.
Hamburg, Margaret A., and Francis S. 
Collins. “The Path to Personalized 
Medicine.” New England Journal of 
Medicine 363.4 (2010): 301–4. Web. 
23 Dec. 2015.
McMullan, Dawn. “What Is Personalized Medicine?” 
Genome Spring 2014: n. pag. Web. 23 Dec. 2015.
“Personalized Medicine and Pharmacogenomics.” 
Mayo Clinic. Mayo Foundation for Medical 
Education and Research, 5 June 2015. Web. 23 
Dec. 2015.
“Precision (Personalized) Medicine.” US Food and 
Drug Administration. Dept. of Health and Human 
Services, 18 Nov. 2015. Web. 23 Dec. 2015.
“What 
Is 
Pharmacogenomics?” 
Genetics 
Home 
Reference. Natl. Insts. of Health, 21 Dec. 2015. Web. 
23 Dec. 2015.
“When 
Healthcare 
and 
Computer 
Science 
Collide.” Health Informatics and Health Information 
Management. Pearson/U of Illinois at Chicago, 
n.d. Web. 23 Dec. 2015.
Personalized medicine is developed by analyzing the patient’s DNA and determining 
the ideal treatment based on his or her unique genetic makeup. By Christoph Bock, 
Max Planck Institute for Informatics, CC BY-SA 3.0 (http://creativecommons.org/­
licenses/by-sa/3.0), via Wikimedia Commons.

228
Privacy regulations
Principles of Computer Science
Privacy regulations
Fields of Study
Privacy; Information Systems
Abstract
Privacy regulations are laws and policies put in place 
to protect digital privacy and to regulate access to 
digital data and equipment. While US law has no gen-
eral consumer privacy protection laws, it does protect 
certain types of digital data, including medical and 
financial data.
Prinicipal Terms

 access level: in a computer security system, a des-
ignation assigned to a user or group of users that 
allows access a predetermined set of files or func-
tions.

 Computer Fraud and Abuse Act (CFAA): a 1986 
legislative amendment that made accessing a 
protected computer without authorization, or ex-
ceeding one’s authorized level of access, a federal 
offense.

 Health Insurance Portability and Accountability 
Act (HIPAA): a 1996 law that established national 
standards for protecting individuals’ medical re-
cords and other personal health information.

 PATRIOT Act: a 2001 law that expanded the 
powers of federal agencies to conduct surveillance 
and intercept digital information for the purpose 
of investigating or preventing terrorism.

 pen/trap: short for pen register or trap-and-trace 
device, devices used to record either all numbers 
called from a particular telephone (pen register) 
or all numbers making incoming calls to that 
phone (trap and trace); also refers to the court 
order that permits the use of such devices.
Privacy Protections
As of 2016, the United States has no general laws 
protecting computer privacy. However, access to 
computers and certain types of digital data are re-
stricted by various federal and state laws. While the 
US Constitution has no specific provision protecting 
the right to privacy, the Supreme Court has repeat-
edly interpreted several amendments to implicitly 
guarantee it. For example, the Fourth Amendment 
protects against unwarranted search and seizure. 
This has been taken to apply to an individual’s per-
sonal communications. With advances in digital tech-
nology, millions of Americans have begun lobbying 
for new protections specifically for digital communi-
cation and data.
General Federal Privacy Laws
Within an organization, permission to access digital 
data may be restricted according to a system of ac-
cess levels. In such a system, users are grouped into 
categories with varying levels of computer clearance. 
Network administrators usually have access to all data 
and operations. Users at other levels may have more 
limited access. In corporate and government systems, 
users are prohibited from accessing computers or 
data beyond their access level.
The Computer Fraud and Abuse Act (CFAA) of 
1986 amended the United States Code statutes on 
federal crimes and criminal procedures. This act 
made unauthorized access to computer systems in-
volved in interstate or foreign communications a fed-
eral offense. It allows for the prosecution of persons 
who attempt to gain unlawful computer access. The 
CFAA was specifically designed to protect govern-
ment and financial institutions.
Also in 1986, Congress passed the Electronic 
Communications Privacy Act (ECPA). This law ex-
tended wiretap restrictions to apply to electronic 
data transmissions as well as pen/trap devices. It 
also specified what information Internet service pro-
viders (ISPs) cannot disclose about their users. One 
category of protected information is electronic com-
munications, such as e-mails. However, the ECPA 
only protects e-mail stored on an ISP server for 180 
days. After that time, the government can compel the 
ISP to disclose it. The ECPA has been criticized for 
not keeping pace with Internet technology. When 
the ECPA was first passed, ISPs only stored a user’s 
e-mail for a short time, until it was downloaded to the 
user’s computer. This changed with the emergence 
of web-based e-mail services. E-mail ISPs store users’ 
e-mails on their servers indefinitely, often until the 
users delete them. Under the ECPA, all of these e-
mails can be freely accessed after 180 days. Had they 
been downloaded to a computer and deleted from 

229
Principles of Computer Science
Privacy regulations
the server instead, they could not be accessed without 
a warrant.
After the September 11, 2001, terrorist attacks, 
Congress passed the PATRIOT Act. This act gave 
federal agencies increased powers to monitor digital 
communications in order to prevent terrorism. It also 
specifi ed that pen/trap restrictions apply to routing 
information from electronic communications as well. 
This technically extends privacy protections, but also 
allows government agencies to compel ISPs to pro-
vide routing information instead of having to gather 
it themselves.
Provisional Privacy regulations
A number of US federal regulations protect cer-
tain types of consumer data. For instance, the 
Health Insurance Portability and Accountability Act 
(HIPAA) of 1996 regulates the collection and use 
of medical information. Organizations with access 
to someone’s health care data may not disclose the 
data without permission from that person. HIPAA 
mainly applies to health care providers and pharma-
cies. Similarly, the Fair Credit Reporting Act of 1970 
limited the use of individual personal and fi nancial 
information by consumer credit reporting agencies. 
Other such laws include the Privacy Act (1974), the 
Tax Reform Act (1976), and the Electronic Fund 
Transfer Act (1978). These laws were not necessarily 
designed to protect electronic data. Nevertheless, 
they form the basis of Internet privacy regulations. 
However, many Americans feel that more general pri-
vacy laws are necessary.
ownership of data and state laws
One recent controversy in digital privacy concerns the 
ownership of digital data. Data transmitted through 
cell phones and ISPs become partially the property of 
the service provider. ISPs and social media websites 
have mined user data to market products to users and, 
in some cases, to share their information with third 
parties. As there are no specifi c federal laws against 
this, several state legislatures have restricted corpo-
rate access to digital data. California, Connecticut, 
and Delaware have all passed laws requiring com-
mercial websites to clearly disclose corporate privacy 
policies and to comply with “Do Not Track” requests 
from users, especially when collecting personal in-
formation. In 2003, Minnesota prohibited ISPs from 
disclosing a user’s Internet habits or history without 
their permission. These statutes were the fi rst US 
state laws intended to protect individuals’ Internet 
privacy. In October 2015, California adopted the 
United 
States
Mexico
100
80
60
40
20
Percent
0
Germany
National
Regulations
International
Regulations
India
Consumer Preference
The Internet extends beyond national boundaries, making the governance and regulation of Internet privacy challenging. The majority 
of Internet users in countries around the world believe Internet privacy should be regulated globally, rather than locally. Adapted from 
data presented in “Market Research in the Mobile World” by Lightspeed, LLC.

230
Programming languages
Principles of Computer Science
California Electronic Communications Privacy Act, 
hailed as the nation’s most comprehensive digital pri-
vacy laws to date.
—Micah L. Issitt
Bibliography
“Computer Crime Laws.” Frontline. WGBH Educ. 
Foundation, 2014. Web. 28 Mar. 2016.
“Computer Fraud and Abuse Act (CFAA).” Internet 
Law Treatise. Electronic Frontier Foundation, 24 
Apr. 2013. Web. 31 Mar. 2016.
Duncan, Geoff. “Can the Government Regulate 
Internet Privacy?” Digital Trends. Designtechnica, 
21 Apr. 2014. Web. 28 Mar. 2016.
“Health Information Privacy.” HHS.gov. Dept. of 
Health and Human Services, n.d. Web. 28 Mar. 
2016. 4 Science Reference Center™ Privacy 
Regulations
“State Laws Related to Internet Privacy.” National 
Conference of State Legislatures. NCSL, 5 Jan. 2016. 
Web. 28 Mar. 2016.
“USA Patriot Act.” Electronic Privacy Information Center. 
EPIC, 31 May 2015. Web. 28 Mar. 2016.
Zetter, Kim. “California Now Has the Nation’s Best 
Digital Privacy Law.” Wired. Condé Nast, 8 Oct. 
2015. Web. 28 Mar. 2016.
Programming languages
Fields of Study
Computer 
Engineering; 
Software 
Engineering; 
Applications
Abstract
A programming language is a code used to control 
the operation of a computer and to create computer 
programs. Computer programs are created by sets of 
instructions that tell a computer how to do small but 
specific tasks, such as performing calculations or pro-
cessing data.
Prinicipal Terms

 Abstraction: a technique used to reduce the 
structural complexity of programs, making them 
easier to create, understand, maintain, and use.

 declarative language: language that specifies the 
result desired but not the sequence of operations 
needed to achieve the desired result.

 imperative language: language that instructs a 
computer to perform a particular sequence of op-
erations.
 semantics: rules that provide meaning to a language.

 syntax: rules that describe how to correctly struc-
ture the symbols that comprise a language.

 Turing complete: a programming language that 
can perform all possible computations.
What Are Programming Languages?
Programming languages are constructed languages 
that are used to create computer programs. They 
relay sets of instructions that control the operation of 
a computer. A computer’s central processing unit op-
erates through machine code. Machine code is based 
on numerical instructions that are incredibly difficult 
to read, write, or edit. Therefore, higher-level pro-
gramming languages were developed to simplify the 
creation of computer programs. Programming lan-
guages are used to write code that is then converted 
to machine code. The machine code is then executed 
by a computer, smartphone, or other machine.
There are many different types of programming 
languages. First-generation, or machine code, lan-
guages are processed by computers directly. Such 
languages are fast and efficient to execute. However, 
they are difficult for humans to read and require 
advanced knowledge of hardware to use. Second-
generation languages, or assembly languages, are 
more easily read by humans. However, they must be 
converted into machine code before being executed 
by a computer. Second-generation languages are 
used more often than first-generation ones because 
they are easier for humans to use while still inter-
acting quickly and efficiently with hardware.
Both first- and second-generation languages are 
low-level languages. Third-generation languages 

231
Principles of Computer Science
Programming languages
are the most widely used programming languages. 
Third-generation, or high-level programming, lan-
guages are easier to use than low-level languages. 
However, they are not as fast or efficient. Early ex-
amples of such languages include Fortran, COBOL, 
and ALGOL. Some of the most widely used program-
ming languages in the twenty-first century are third-
generation. These include C++, C#, Java, JavaScript, 
and BASIC. Programming languages with higher 
levels of Abstraction are sometimes called fourth-
generation languages. Higher levels of Abstraction 
increases platform independence. Examples include 
Ruby, Python, and Perl.
Programming languages can be based on different 
programming paradigms or styles. Imperative lan-
guages, such as COBOL, use statements to instruct 
the computer to perform a specific sequence of opera-
tions to achieve the desired outcome. Declarative lan-
guages specify the desired outcome but not the specific 
sequence of operations that will be used to achieve it. 
Structured Query Language (SQL) is one example.
Programming languages can also be classified by 
the number of different computations they can per-
form. Turing complete programming languages can 
perform all possible computations and algorithms. 
Most programming languages are Turing complete. 
However, some programming languages, such as 
Charity and Epigram, can only perform a limited 
number of computations and are therefore not 
Turing complete.
How Programming Languages Are 
Structured
The basic structural rules of a programming language 
are defined in its syntax and semantics. A program-
ming language’s syntax is the grammar that defines 
the rules for how its symbols, such as words, numbers, 
and punctuation marks, are used. A programming 
language’s semantics provide the rules used to inter-
pret the meaning of statements constructed using its 
syntax. For example, the statement 1 + pizza might 
comply with a programming language’s syntax. 
However, adding a number and a word together (as 
opposed to adding two numbers) might be semanti-
cally meaningless. Programs created with a program-
ming language must comply with the structural rules 
established by the language’s syntax and semantics if 
they are to execute correctly.
Abstraction reduces the structural complexity 
of programs. More Abstract languages are easier to 
On the TIOBE index for various programming languages, C and Java are the highest-rated languages. By 
TIOBE Software B.V., CC BY-SA 4.0 (http://creativecommons.org/licenses/by-sa/4.0), via Wikimedia 
Commons.

232
Programming languages
Principles of Computer Science
understand and use. Abstraction is based on the prin-
ciple that any piece of functionality that a program 
uses should be implemented only once and never 
duplicated. Abstraction focuses only on the essen-
tial requirements of a program. Abstraction can be 
implemented using subroutines. A subroutine is a 
sequence of statements that perform a specific task, 
such as checking to see if a customer’s name exists in 
a text file. If Abstraction is not used, the sequence of 
statements needed to check if a customer’s name ex-
ists in the file would need to be repeated every place 
in the program where such a check is needed. With 
subroutines, the sequence of statements exists in only 
one place, within the subroutine. Thus, it does not 
need to be duplicated.
Using Pseudocode
The programming language statements that com-
prise a program are called “code.” Code must comply 
with all of the programming language’s syntax and 
semantic rules. Pseudocode uses a combination of a 
programming language and a natural language such 
as English to simply describe a program or algorithm. 
Pseudocode is easier to understand than code and is 
often used in textbooks and scientific publications.
The Future of Programming Languages
Programming languages are growing in importance 
with the continued development of the Internet 
and with the introduction of new programmable 
machines including household appliances, driver-
less cars, and remotely controlled drones. Such sys-
tems will increase the demand for new programming 
languages and paradigms designed for larger, more 
complex, and highly interconnected programs.
—Maura Valentino, MSLIS
Bibliography
Belton, Padraig. “Coding the Future: What Will the 
Future of Computing Look Like?” BBC News. BBC, 
15 May 2015. Web. 24 Feb. 2016.
Friedman, Daniel P., and Mitchell Wand. Essentials of 
Programming Languages. Cambridge: MIT P, 2006. 
Print.
Harper, Robert. Practical Foundations for Programming 
Languages. Cambridge: Cambridge UP, 2013. Print.
MacLennan, Bruce J. Principles of Programming 
Languages: Design, Evaluation, and Implementation. 
Oxford: Oxford UP, 1999. Print.
Scott, Michael L. Programming Language Pragmatics. 
Burlington: Kaufmann, 2009. Print.
Van Roy, Peter. Concepts, Techniques, and Models of Computer 
Programming. Cambridge: MIT P, 2004. Print.
Watt, David A. Programming Language Design Concepts. 
West Sussex: Wiley, 2004. Print.
Woods, Dan. “Why Adopting the Declarative 
Programming Practices Will Improve Your Return 
from Technology.” Forbes. Forbes.com, 17 Apr. 
2013. Web. 2 Mar. 2016.
Sample Problem
The pseudocode below describes an algo-
rithm designed to count the number of words 
in a text file and to delete the file if no words 
are found:
Open the file
For each word in file
counter = counter + 1;
If counter = 0 Then
Delete file
Close the file
Use the pseudocode to describe an algo-
rithm that counts the number of words in a 
file and then prints the number of words if the 
number of words is greater than 300.
Answer:
Open the file
For each word in file
wordcount = wordcount + 1;
If wordcount > 300 Then
print wordcount
Close the file
The pseudocode above uses natural lan-
guage for statements such as Open the file and 
programming language for statements such 
as wordcount = wordcount + 1. Pseudocode 
cannot be executed by a computer. However, 
it is helpful for showing the outline of a pro-
gram or algorithm’s operating principles.

233
Principles of Computer Science
PROLOG
PROLOG
Fields of Study
Programming Languages
Abstract
The PROLOG language was released in 1971 and is 
a completely declarative language, although it is also 
capable of carrying out mathematical calculations. 
Numerous implementations of PROLOG have been 
produced, with two main variations. Programs are 
structured with only onr data type called the term, 
with four kinds of terms. Relations are structured on 
the pattern of “well-formed formulas” in logic, using 
rules and facts. A program is executed as a query run 
on a relation, and the result is evaluated as true or 
false by determining whether the query disagrees 
with any of the rules. PROLOG is used extensively in 
the field of artificial intelligence, and is more pop-
ular with European researchers, while the similar lan-
guage LISP is more popular witrh North American 
researchers.
Prinicipal Terms

 artificial intelligence: the intelligence exhibited by 
machines or computers, in contrast to human, or-
ganic, or animal intelligence. 

 character: a unit of information that represents 
a single letter, number, punctuation mark, blank 
space, or other symbol used in written language. 

 declarative language: language that specifies the 
result desired but not the sequence of operations 
needed to achieve the desired result. 

 programming languages: sets of terms and rules of 
syntax used by computer programmers to create 
instructions for computers to follow. This code is 
then compiled into binary instructions for a com-
puter to execute. 

 syntax: rules that describe how to correctly struc-
ture the symbols that comprise a language. 
History and Characteristics of PROLOG
PROLOG was released in 1971 after being developed 
at l’Université de Marseilles, in France. The name is a 
contraction of the words PROgrammation en LOGique 
(in English, PROgramming in LOGic). It is one of the 
first logic programming languages, and although not 
as old as LISP, it is one of the oldest still in common 
usage. PROLOG is more commonly used in Europe, 
while LISP is more popular with North American 
researchers and developers. Both PROLOG and 
LISP are used extensively for research and develop-
ment in the field of artificial intelligence. Numerous 
implementations of PROLOG have been produced 
, the two major variations being ISO Prolog and 
Edinburgh Prolog. PROLOG is geared to natural lan-
guage processing, or the use of computers to process 
natural language. PROLOG can be implemented in 
any programming language or environment that is 
capable of calling a dynamic linked library, or .dll, 
file. This includes languages such as C, C++, C#, PHP, 
Java, Visual BASIC, Delphi and .NET, as well as other 
similar languages. PROLOG is particularly useful for 
database manipulation, symbolic math and language 
parsing applications.
Program Features of PROLOG
PROLOG is strictly a declarative language rather 
than imperative. The syntax of the language is based 
on logic notation. Program logic is expressed as re-
lations in terms of facts and rules, and each process 
is initiated as a query. The execution of a PROLOG 
program uses an inverse logic approach to the resolu-
tion of a query. The program computes by searching 
for a resolution of the negated query, known as SLD 
resolution. The truth of a relation is not evaluated as 
to whether it is in agreement with the query. Rather, 
it is tested to determine if it agrees with any condi-
tion that counts as failure of the query. If accord with 
failure of the query is not found, then the relation 
is deemed to be true by default. PROLOG has just 
a single data type, called a term, with relations being 
defined as a clause. There are just four types of term 
in PROLOG, called atom, number, variable, or com-
pound_term. The atom is just a general purpose term 
used as any variable name would be in another lan-
guage. As in LISP, a number in PROLOG is either a 
float or an integer. The variable term always begins 
with an underscore or an upper case letter and is 
made up of a character string consisting of letters, 
numbers and underscores. The variable term func-
tions like the variable term in a logic statement. The 
compound_term has a form similar to a function call 

234
PROLOG
Principles of Computer Science
in other languages. It consists of an atom with a list of 
arguments in parentheses, as for example
atom (arg1, arg2,...)
A list can also be used as an argument, as can a string 
of characters enclosed in quotation marks, as for 
example
atom (arg1, [arg2, arg3], “some text”)
Clause Structure in PROLOG
Relations in PROLOG are stated as clauses, which are 
described as a fact or a rule. A rule essentially states a 
logical relationship of the type “this head statement 
is true if the following body conditions are true,” and 
has the form
head statement :- body condition statement(s)
The symbol “:-” is read as “is true if.” A simple log-
ical statement of this form might be something like 
“(a = 2) is true if (a = b) and (b = 2).” In this simple 
example, the conditions described by “(a = b) and (b 
= 2)” would correspond to the body statements, and 
would be written as facts in the form
a (b) :- true.
b (2) :- true.
Given these facts, one could then pose the query
?- b(2).
equivalent to the question “is b equal to 2?,” which 
would then return the value statement “yes” to indi-
cate the “true” condition.
Practice And Applicaton
Write a rule, facts, and a query statement to test the 
fact of whether a cat is green.
Answer
	
facts:	
cat (red).
	
	
cat (blue).
	
	
cat (brown).
	
	
cat (tabby).
	
	
cat (black)
	
rule:	
green :- (cat (green)).
	
query:	 ?- cat(green).
The facts specify that cats are red, blue, brown, 
tabby and black but no other colors are defined and 
therefore are not “true.” The rule states that cats are 
green only if they are defined to be green. The query 
asks “are cats green?,” and because comparisons with 
the facts all fail the program returns the value state-
ment “no.”
PROLOG and Logic
Scientific method relies strictly on the application 
of pure logic to achieve results in any field. Strictly 
speaking, scientific method, in any area of applica-
tion, seeks to eliminate as many unpredictable vari-
ables as possible in determining the answer to a 
specific question. In logic, this depends on the con-
struction of “well-formed formulas,” and such are the 
essential components of PROLOG programs. This is 
a central aspect in the development of artificial intel-
ligence programs.
—Richard M. Renneboog M.Sc.
Bibliography
Batchelor, Bruce. Intelligent Image Processing in 
PROLOG. London, UK: Springer-Verlag, 1991. 
Print.
Brauer, Max. Logic Programming With Prolog. New 
York, NY: Springer, 2005. Print.
Coelho, Helder, and José C. Cotta. Prolog by Example: 
How to Learn, Teach and Use It. New York, NY: 
Springer, 1996. Print.
Deransart, P., A. Ed-Dbali, and L. Cervoni. PROLOG: 
The Standard Reference Manual. New York, NY: 
Springer, 1996. Print.
Jones, M. Tim, Artificial Intelligence: A Systems Approach. 
Sudbury, MA: Jones and Bartlett, 2009. Print.
Mathews, Clive. An Introduction to Natural Language 
Processing Through Prolog New York, NY: Routledge, 
2014. Print.
Scott, Michael L. Programming Language Pragmatics. 
Waltham, MA: Morgan Kaufmann, 2016. Print. 
Sterling, Leon, ed. The Practice of Prolog. Boston, MA: 
MIT Press,1990. Print.

235
Quantum computers
Fields of study
Computer engineering
Abstract
Quantum computers are computing devices that can 
theoretically have computing power that is many or-
ders of magnitude greater than that of conventional 
computers. The basic unit of data in a quantum 
computer is the quantum bit, or qubit, that is the 
quantum state of electrons in an atom. Qubits can 
theoretically exist in several superposed states simul-
taneously, enabling them to carry far more informa-
tion than id available using conventional two-state 
bits. The mathematical basis of the proportionality 
of qubit states is similar to that of the input weights 
of neural networks. There has been some successful 
development of quantum computer technology, but 
a great deal of research and development remains 
to be done before quantum computers become vi-
able as a mainstream technology, and there are 
arguments as to why this eventuality can never be 
achieved.
Prinicipal terms

 Constraints: limitations on values in computer 
programming that collectively identify the solu-
tions to be produced by a programming problem. 

 Control unit design: describes the part of the cpu 
that tells the computer how to perform the in-
structions sent to it by a program. 

 Entanglement: the phenomenon in which two 
or more particles’ quantum states remain linked 
even if the particles are later separated and be-
come part of distinct systems. 

 Logic implementation: the way in which a cpu is 
designed to use the open or closed state of combi-
nations of circuits to represent information. 

 Quantum bit (qubit): a basic unit of quantum com-
putation that can exist in multiple states at the 
same time, and can therefore have multiple values 
simultaneously. 

 Quantum logic gate: a device that alters the be-
havior or state of a small number of qubits for the 
purpose of computation. 

 State: a technical term for all of the stored infor-
mation, and the configuration thereof, that a pro-
gram or circuit can access at a given time; a com-
plete description of a physical system at a specific 
point in time, including such factors as energy, 
momentum, position, and spin. 

 Superposition: the principle that two or more 
waves, including waves describing quantum states, 
can be combined to give rise to a new wave state 
with unique properties. This allows a qubit to po-
tentially be in two states at once. 
A controversial subject
The supposed difference in processing power be-
tween a semiconductor transistor-based computer 
and a quantum computer has been likened to 
reading all of the books in the library of congress one 
after the other versus reading them all at the same 
time. But quantum computers and their viability is 
probably the most controversial topic in computer 
science. Even while significant research is carried 
out to design and construct physical components 
such as a functioning quantum logic gate, there 
are those who offer valid conceptual reasons why 
quantum computers will never be either functional 
or viable. Indeed, the quantum computer currently 
exists only as a theoretically possible entity based 
solely on the mathematics of advanced quantum 
theory. However, history has amply demonstrated 
that theoretical possibility has become practical 
reality on many occasions. Accordingly, the ‘jury is 
Q

236
Quantum computers
Principles of Computer Science
still out’ on the potential future of quantum com-
puters. In both concept and eventual practicality, the 
quantum computer is an alien device in comparison 
to current semiconductor transistor-based computer 
technology. Comprehending the functionality of a 
quantum computer requires a complete paradigm 
shift for computer science. The quantum bit (qubit) 
is entirely unlike the bit of current computer tech-
nology. The control unit design of a quantum com-
puter can have little or no resemblance to current 
control unit design that is based on bit manipula-
tion. The constraints of programming are entirely 
different for quantum computer program opera-
tion than for current bit-based computer programs. 
The logic implementation of quantum computers, 
based on quantum theory and mathematical prin-
ciples, bears no resemblance to the boolean logic of 
transistor-based computers. Transistor logic as it is 
used in current computer technology uses just three 
kinds of transistor gate, called and, or, and not, that 
are effectively nothing more than electronic on-off 
switches for the flow of electrons through a semi-
conducting material. Accordingly, they have just one 
macroscopic state for each bit.
Bits and qubits
Bits are controlled by the ‘clock speed’ of the par-
ticular computer, and is defined as being either the 
high state or the low state for the finite period of time 
determined by one clock cycle. A quantum bit (a 
qubit), however, is defined by the particular quantum 
state of the atom and can have several values at the 
same time, determined by the superposition or linear 
combination of several different states of the same 
atom. The qubit therefore exists as a quantum elec-
tronic state rather than as a macroscopic property. A 
transistor gate functions simply by switching the flow 
of electrons to change the state of a bit according to 
boolean relationships. A quantum gate, however, uses 
the quantum property of entanglement to change 
the state of a qubit. This is a very strange action that 
is not explained outside of quantum mathematics. 
Quantum entanglement occurs when two particles 
stay connected in a way such that any action on one 
particle equally affects the other particle, even when 
they are separated by great distances. Digital com-
puter information is coded as strings of bits. In a 
quantum computer, the elements that carry the in-
formation are quantum states. This does not include 
just the ground and excited states, but also linear 
combinations and superpositions of states. This al-
lows making use of quantum parallelism techniques 
that would be far more powerful than even the mas-
sively parallel techniques of digital computing. A fair 
question at this point would be ‘what are quantum 
states?’. In 1897, electrons and protons were posi-
tively identified as component particles of atoms, 
and though it was theorized at the same time, the 
existence of the neutron was not conclusively dem-
onstrated until 1932. In the early 1900s, this model 
of atomic structure was refined using quantum me-
chanics to account for and describe the energies of 
electrons in atoms. Quantum mechanics describes 
the energy states that electrons in atoms are allowed 
to occupy. Each of these energy levels and the cor-
responding behavior and distribution of electrons is 
thus termed a ‘quantum state’. An electron can move 
from one quantum state to another by absorbing or 
emitting a ‘quantum’ of the appropriate energy. A 
quantum is the minimum ‘particle size’ of energy re-
quired for a specific change of quantum state, and 
each atom has numerous possible quantum states. By 
using quantum states instead of the binary states of 
transistors, the amount of information that can be 
carried through the system increases exponentially 
with each additional state.
Quantum computers and neural 
networks
A comparison can be made between the mathematics 
of qubits and the mathematics of neural networks. In 
particular, the functioning of an artificial neural net-
work is determined in part by the ‘weight’ assigned to 
each input in relation to the ‘threshold value’ of the 
output. The logic of an artificial neural network trig-
gers an output when a ‘true’ value (1) of the sum of 
the weighted inputs equal to or greater than the given 
threshold value is obtained. A false value (0) and no 
output is obtained otherwise. In a similar manner, 
the state of a quantum computer is described by a 
multidimensional vector in which each term has a 
complex coefficient. The state of a quantum com-
puter is described when the sum of the squared value 
of the vector coefficients is equal to 1, analogous to 
the threshold state of an artificial neural network. 
This similarity has led to some speculation that the 
brain, a true neural network, may also function using 

237
Principles of Computer Science
Quantum computers
quantum computing principles, though no evidence 
of this has been found.
Types of quantum computers and the 
progress so far
Research into quantum-scale phenomena is time-con-
suming and expensive. Nevertheless, many agencies 
in government and industry are actively pursuing re-
search into the development of quantum computers, 
for several reasons. The most important reason, of 
course, is the vastly superior speed of computation 
that quantum computers should exhibit, enabling 
them to solve computational problems that cannot 
be solved by conventional computers. Quantum com-
puters would be far more effective at dealing with 
and analyzing large quantities of data, as well as for 
service in the field of cryptanalysis and encryption. 
As might be expected, the different theoretical ave-
nues by which quantum states can be described have 
given rise to several different approaches to defining 
qubits and achieving quantum computing devices. 
These include
• Use of superconducting josephson junctions
• Use of the internal electronic state of trapped 
ions
• Use of the internal electronic state of neutral 
atoms in an optical lattice
• Use of quantum dots and either the spin state 
or position of trapped electrons
• Use of nuclear spin state in nuclear magnetic 
resonance 
• Use of electron spin
• Use of nuclear spin of phosphorus donors in 
silicon
• Use of bose-einstein condensates
• Use of light modes through various linear 
elements
• Use of carbon nanoparticles and fullerenes
Each of these methods has had some success, in 
accord with the principles of scientific investigation 
that seek to test s single hypothesis by the strict con-
trol of variables. None, however, have yet indicated a 
general or universal method of generating qubits for 
the creation of a quantum computer. In recent years, 
there has been some successful commercialization 
of quantum computer technology by the canadian 
company d-wave. The device does function using 
quantum properties, but has yet to achieve greater 
performance than is available using classical com-
puters. Nasa revealed the $15 million device publicly 
in december, 2015. More recently, in august, 2016, 
computer scientists at university of maryland con-
structed the first working quantum computer that is 
capable of being reprogrammed.
The major problem facing the 
development of quantum computers
That a quantum computer that functions at least 
partially as theorized has been built is perhaps veri-
fication of a paraphrased physical law that says ‘for 
every theory there is an equal and opposite set of 
empirical results’. On one hand, the strong invest-
ment into the development of quantum computers 
supports the view that quantum computer tech-
nology will eventually provide computing power 
that is “hundreds of orders of magnitude greater 
than that of conventional computers.” On the other 
hand, that such an increase has not been indicated 
by the performance of quantum technology so far 
supports the view that quantum computers will never 
be feasible. The principle argument for this latter 
view is based on the concept of ‘noise’. In conven-
tional computers, that function on a macroscopic 
scale, electronic interactions from both internal and 
external sources produce a constant background 
of low energy random signals occasionally punctu-
ated by individual events of higher energy. This is 
electrical ‘noise’. On the macroscopic scale, this in-
terference can be readily filtered our or minimized 
such that the desired signals for data manipulation 
are essentially clean signals. On the quantum scale, 
however, there is no way to ‘filter out’ the quantum 
effects that arise from noise. The other major argu-
ment against the increased power of quantum com-
puters is that even though the qubits of the process 
can be simultaneously in numerous states, their 
value can only be read as one of two states, making 
them no better than ordinary bits in terms of com-
puting power. The reality is that, at the present time, 
neither argument for or against quantum computers 
has been demonstrated successfully, and a great deal 
of research is still required to either bring quantum 
computers out of the realm of theory and into the 
mainstream of technology or relegate them to the 
realm of failed scientific concepts.
—richard m. Renneboog m.sc.

238
Quantum computing
Principles of Computer Science
Bibliography
Miszczak, jarosław adam (2012) high-level structures for 
quantum computing williston, vt: morgan and clay-
pool. Print.
Hirvensalo. Mike (2001) quantum computing new 
york, ny: springer. Print.
Rieffel, eleanor and polak, wolfgang (2011) quantum 
computing. A gentle introduction cambridge, ma: mit 
press. Print.
Berman, gennady p., doolen, gary d., mainieri, 
ronnie and tsifrinovich, vladimir i. (1998) introduc-
tion to quantum computers river edge, nj: world sci-
entific publishing. Print.
Kaye, phillip, laflamme, raymond and mosea, mi-
chele (2007) an introduction to quantum computing 
new york, ny: oxford university press. Print.
Mermin, n. David (2007) quantum computer science. 
An introduction new york, ny: cambridge university 
press. Print.
Quantum computing
Fields of Study
Computer Science; System-Level Programming; 
Computer Engineering
Abstract
Quantum computing is an emerging field of com-
puter engineering that uses charged particles rather 
than silicon electrical circuitry to process signals. 
Existing quantum computers are only in the experi-
mental stage. Engineers believe that quantum com-
puting has the potential to advance far beyond the 
limitations of traditional computing technology.
Prinicipal Terms

 entanglement: the phenomenon in which two 
or more particles’ quantum states remain linked 
even if the particles are later separated and be-
come part of distinct systems.

 quantum logic gate: a device that alters the be-
havior or state of a small number of qubits for the 
purpose of computation.

 quantum bit (qubit): a basic unit of quantum com-
putation that can exist in multiple states at the 
same time, and can therefore have multiple values 
simultaneously.

 state: a complete description of a physical system 
at a specific point in time, including such factors 
as energy, momentum, position, and spin.

 superposition: the principle that two or more 
waves, including waves describing quantum states, 
can be combined to give rise to a new wave state 
with unique properties. This allows a qubit to po-
tentially be in two states at once.
Subatomic Computation Theories
Quantum computing is an emerging field of com-
puting that uses subatomic particles rather than 
silicon circuitry to transmit signals and perform cal-
culations. Quantum physics studies particle behavior 
at the subatomic scale. At extremely small scales, sub-
atomic particles such as photons (the basic unit of 
light) exhibit properties of both particles and waves 
simultaneously. This phenomenon, called wave-par-
ticle duality, gives subatomic particles unique prop-
erties. Traditional computer algorithms are con-
strained by the physical properties of digital electrical 
signals. Engineers working on quantum computing 
hope that quantum algorithms, based on the unique 
properties of quantum mechanics, will be able to 
complete computations faster and more efficiently.
The Basics of Quantum Data
Digital computing uses electrical signals to create bi-
nary data. Binary digits, called bits, have two possible 
values: 0 or 1. Digital computers also use logic gates. 
These are electronic circuits that process bits of data 
by amplifying or changing signals. Logic gates in dig-
ital computers accept one or more inputs and pro-
duce only one output. In quantum computing, dig-
ital bits are replaced by quantum bits (qubits). Qubits 
are created by manipulating subatomic particles.
The value of a qubit represents its current 
quantum state. A quantum state is simply all known 
data about a particle, including its momentum, 

239
Principles of Computer Science
Quantum computing
physical location, and energetic properties. To be 
used as a qubit, a particle should have two distinct 
states, representing the binary 0 and 1. For example, 
if the qubit is a photon, the two states would be hori-
zontal polarization (0) and vertical polarization (1). 
However, a particle in a quantum system can exist in 
two or more states at the same time. This principle is 
called superposition. Thus, a qubit is not limited to 
binary values of either 0 or 1. Instead, it can have a 
value of 0, 1, or any superposition of both 0 and 1 at 
the same time.
Quantum particles also display a property known 
as entanglement. This is when two or more particles 
are linked in such a way that changing the state of one 
particle changes the state of the other(s), even after 
they are physically separated. Entanglement could 
potentially allow for the development of quantum 
computers that can instantly transmit information 
across great distances and physical barriers.
Practical design of Quantum computers
Current designs for quantum computers use ener-
getic particles such as electrons or photons as qu-
bits. The states of these particles are altered using 
quantum logic gates, much like digital logic gates 
alter electrical signals. A quantum gate may operate 
using energy from lasers, electromagnetic fi elds, or 
several other methods. These state changes can then 
be used to calculate data.
One avenue of research is the potential derivation 
of qubits from ion traps. Ions are atoms that have lost 
or gained one or more electrons. Ion traps use elec-
tric and magnetic fi elds to catch, keep, and arrange 
ions.
the Potential of Quantum computing
As of 2016, the practical value of quantum computing 
had only been demonstrated for a small set of po-
tential applications. One such application is Shor’s 
algorithm, created by mathematician Peter 
Shor, which involves the mathematical pro-
cess of factorization. Factorization is used 
to fi nd two unknown prime numbers that, 
when multiplied together, give a third known 
number. Shor’s algorithm uses the properties 
of quantum physics to speed up factorization. 
It can perform the calculation twice as fast 
as a standard algorithm. Researchers have 
also demonstrated that quantum algorithms 
might improve the speed and accuracy of 
search engines. However, research in this area is in-
complete, and the potential benefi ts remain unclear.
There are signifi cant challenges to overcome be-
fore quantum computing could become mainstream. 
Existing methods for controlling quantum states and 
manipulating particles require highly sensitive mate-
rials and equipment. Scientists working on quantum 
computers argue that they may make the biggest im-
pact in technical sciences, where certain math and 
physics problems require calculations so extensive 
that solutions could not be found even with all of the 
computer resources on the planet. Special quantum 
properties, such as entanglement and superposition, 
mean that qubits may be able to perform parallel 
computing processes that would be impractical or 
improbable with traditional computer technology.
—Micah L. Issitt
bibliography
Ambainis, Andris. “What Can We Do with a Quantum 
Computer?” Institute Letter Spring 2014: 6–7. 
Institute for Advanced Study. Web. 24 Mar. 2016.
Bone, Simon, and Matias Castro. “A Brief History 
of Quantum Computing.” SURPRISE May–June 
1997: n. pag. Department of Computing, Imperial 
College London. Web. 24 Mar. 2016.
Crothers, Brooke. “Microsoft Explains Quantum 
Computing So Even You Can Understand.” CNET. 
CBS Interactive, 25 July 2014. Web. 24 Mar. 2016.
Gaudin, Sharon. “Quantum Computing May Be 
Moving out of Science Fiction.” Computerworld. 
Computerworld, 15 Dec. 2015. Web. 24 Mar. 2016.
“The 
Mind-Blowing 
Possibilities 
of 
Quantum 
Computing.” TechRadar. Future, 17 Jan. 2010. 
Web. 26 Mar. 2016.
“A Quantum Leap in Computing.” NOVA. WGBH/
PBS Online, 21 July 2011. Web. 24 Mar. 2016.
Classic Bit
Quantum Bit (qubit)
0  or  1  or
0 1
0  
 1  
0  or  1
Quantum computing uses quantum bits (qubits). Classic bits can be in one of 
two states, 0 or 1, but qubits can be in state 0, state 1, or superstate 01. EBSCO 
illustration.

240
Random-access memory
Fields of Study
Computer Engineering; Information Technology
Abstract
Random-access memory (RAM) is a form of 
memory that allows the computer to retain and 
quickly access program and operating system data. 
RAM hardware consists of an integrated circuit chip 
containing numerous transistors. Most RAM is dy-
namic, meaning it needs to be refreshed regularly, 
and volatile, meaning that data is not retained if the 
RAM loses power. However, some RAM is static or 
nonvolatile.
Prinicipal Terms

 direct-access storage: a type of data storage in 
which the data has a dedicated address and loca-
tion on the storage device, allowing it to be ac-
cessed directly rather than sequentially.

 dynamic random-access memory (DRAM): a form 
of RAM in which the device’s memory must be re-
freshed on a regular basis, or else the data it con-
tains will disappear.

 nonvolatile random-access memory (NVRAM): a 
form of RAM in which data is retained even when 
the device loses access to power.

 read-only memory (ROM): a type of nonvolatile 
data storage that can be read by the computer 
system but cannot be modified.

 shadow RAM: a form of RAM that copies code 
stored in read-only memory into RAM so that it 
can be accessed more quickly.

 static random-access memory (SRAM): a form of 
RAM in which the device’s memory does not need 
to be regularly refreshed but data will still be lost if 
the device loses power.
History of RAM
The speed and efficiency of computer processes 
are among the most areas of greatest concern for 
computer users. Computers that run slowly (lag) or 
stop working altogether (hang or freeze) when one 
or more programs are initiated are frustrating to 
use. Lagging or freezing is often due to insufficient 
computer memory, typically random-access memory 
(RAM). RAM is an essential computer component 
that takes the form of small chips. It enables com-
puters to work faster by providing a temporary space 
in which to store and process data. Without RAM, this 
data would need to be retrieved from direct-access 
storage or read-only memory (ROM), which would 
take much longer.
Computer memory has taken different forms over 
the decades. Early memory technology was based 
on vacuum tubes and magnetic drums. Between 
the 1950s and the mid- 1970s, a form of memory 
called “magnetic-core memory” was most common. 
Although RAM chips were first developed during 
the same period, they were initially unable to replace 
core memory because they did not yet have enough 
memory capacity.
A major step forward in RAM technology came in 
1968, when IBM engineer Robert Dennard patented 
the first dynamic random-access memory (DRAM) 
chip. Dennard’s original chip featured a memory 
cell consisting of a paired transistor and capacitor. 
The capacitor stored a single bit of binary data as 
an electrical charge, and the transistor read and re-
freshed the charge thousands of times per second. 
Over the following years, semiconductor companies 
such as Fairchild and Intel produced DRAM chips 
of varying capacities, with increasing numbers of 
memory cells per chip. Intel also introduced DRAM 
with three transistors per cell, but over time the 
need for smaller and smaller computer components 
R

241
Principles of Computer Science
Random-access memory
made this design less practical. In the 2010s, com-
monly used RAM chips incorporate billions of 
memory cells.
Types of RAM
Although all RAM serves the same basic purpose, 
there are a number of different varieties. Each type 
has its own unique characteristics. The RAM most 
often used in personal computers is a direct descen-
dant of the DRAM invented by Dennard and popular-
ized by companies such as Intel. DRAM is dynamic, 
meaning that the electrical charge in the memory 
cells, and thus the stored data, will fade if it is not re-
freshed often. A common variant of DRAM is speed-
focused double data rate synchronous DRAM (DDR 
SDRAM), the fourth generation of which entered the 
market in 2014.
RAM that is not dynamic is known as static random-
access memory (SRAM). SRAM chips contain many 
more transistors than their DRAM counterparts. 
They use six transistors per cell: two to control access 
to the cell and four to store a single bit of data. As 
such, they are much more costly to produce. A small 
amount of SRAM is often used in a computer’s cen-
tral processing unit (CPU), while DRAM performs 
the typical RAM functions.
Just as the majority of RAM is dynamic, most RAM 
is also volatile. Thus, the data stored in the RAM will 
disappear if it is no longer being supplied with elec-
tricity—for instance, if the computer in which it is 
installed has been turned off. Some RAM, however, 
can retain data even after losing power. Such RAM 
is known as nonvolatile random-access memory 
(NVRAM).
Using RAM
RAM works with a computer’s other memory and 
storage components to enable the computer to 
run more quickly and efficiently, without lagging 
or freezing. Computer memory should not be 
confused with storage. Memory is where applica-
tion data is processed and stored. Storage houses 
files and programs. It takes a computer longer to 
access program data stored in ROM or in long-
term storage than to access data stored in RAM. 
Thus, using RAM enables a computer to retrieve 
data and perform requested functions faster. To 
improve a computer’s performance, particularly 
when running resource-intensive programs, a user 
may replace its RAM with a higher-capacity chip so 
the computer can store more data in its temporary 
memory.
RAM
SRAM
DRAM
ASRAM
SBSRAM
FPMDRAM
EDODRAM
BEDODRAM
SDRAM
There are two major categories of random-access memory: static RAM (SRAM) and dynamic RAM (DRAM). Static RAM may be 
asynchronous SRAM (ASRAM) or synchronous SRAM with a burst feature (SBSRAM). Dynamic RAM may come in one of four 
types: fast page mode DRAM (FPMDRAM), extended data out DRAM (EDODRAM), extended data out DRAM with a burst feature 
(BEDODRAM), or synchronous DRAM (SDRAM). EBSCO illustration.

242
Removable memory
Principles of Computer Science
Shadow RAM
While RAM typically is used to manage data related 
to the applications in use, at times it can be used to 
assist in performing functions that do not usually 
involve RAM. Certain code, such as a computer’s 
basic input/output system (BIOS), is typically stored 
within the computer’s ROM. However, accessing data 
saved in ROM can be time consuming. Some com-
puters can address this issue by copying data from the 
ROM and storing the copy in the RAM for ease of ac-
cess. RAM that contains code copied from the ROM 
is known as shadow RAM.
—Joy Crelin
Bibliography
Adee, Sally. “Thanks for the Memories.” IEEE 
Spectrum. IEEE, 1 May 2009. Web. 10 Mar. 2016.
Hey, Tony, and Gyuri Pápay. The Computing Universe: A 
Journey through a Revolution. New York: Cambridge 
UP, 2015. Print.
ITL Education Solutions. Introduction to Information 
Technology. 2nd ed. Delhi: Pearson, 2012. Print.
“Shadow RAM Basics.” Microsoft Support. Microsoft, 4 
Dec. 2015. Web. 10 Mar. 2016.
Stokes, Jon. “RAM Guide Part I: DRAM and SDRAM 
Basics.” Ars Technica. Condé Nast, 18 July 2000. 
Web. 10 Mar. 2016.
“Storage vs. Memory.” Computer Desktop Encyclopedia. 
Computer Lang., 1981–2016. Web. 10 Mar. 2016.
Removable memory
Fields of Study
Information Technology; Computer Engineering; 
Digital Media
Abstract
Removable memory systems allow the user to store 
digital data and transfer them between computer 
systems. Using magnetic, optical, and circuit-
based systems, removable memory has evolved 
throughout the digital age and continues to be 
widely used, though cloud storage may eventually 
take its place.
Prinicipal Terms

 magnetic storage: a device that stores information 
by magnetizing certain types of material.

 main memory: the primary memory system of a 
computer, often called “random access memory” 
(RAM), accessed by the computer’s central pro-
cessing unit (CPU).

 optical storage: storage of data by creating marks 
in a medium that can be read with the aid of a 
laser or other light source.

 read-only memory (ROM): type of computer data 
storage that is typically either impossible to erase 
and alter or requires special equipment to do so; 
it is usually used to store basic operating informa-
tion needed by a computer.

 solid-state storage: computer memory that stores 
information in the form of electronic circuits, 
without the use of disks or other read/write equip-
ment.

 volatile memory: memory that is created and 
maintained only while a computer is active and is 
generally erased when the computer is powered 
down.
Temporary and Permanent Memory
Computer memory, sometimes called “storage,” re-
fers to the hardware used to store both computer 
instructions and digital data for processing. There 
are two basic types of computer memory. Volatile 
memory, including a computer’s random access 
memory (RAM), is stored as electrical signals and 
so requires power. RAM, sometimes called main 
memory, is created each time a computer is powered 
on. It provides rapid access to instructions and data. 
These are already stored as electrical signals and so 
do not have to be “read” and converted from another 
storage medium. Nonvolatile memory is stored physi-
cally, such that powering down the computer does not 
erase the data. Removable memory refers to certain 

243
Principles of Computer Science
Removable memory
subsets of both of these types of memory. Nonvolatile 
removable memory involves methods of storage that 
can be removed or transferred from one computer 
to another. Volatile removable memory is expansion 
memory that can be installed on a computer to in-
crease temporary memory capacity. One of the most 
familiar examples of removable memory is the SIM 
cards used in some mobile phones. Until recently, 
most mobile devices used removable memory cards 
to store user data. The card could be removed and 
installed on another mobile device, allowing users to 
transfer personal data between mobile devices.
Magnetic and Optical Storage Media
The earliest forms of computer memory consisted of 
punch cards that represented digital data physically. 
The data could then be read and interpreted by a com-
puter. In the 1950s computer engineers began using 
magnetic storage. Magnetic storage uses electronic 
heads to write and read data on a tape or other ma-
terial coated with a magnetically sensitive substance. 
The floppy disks that were popular in the 1970s and 
1980s for storing data and applications were a form 
of magnetic storage. In the 1980s computer scientists 
developed the first optical storage systems. In these 
systems, data was stored by making physical marks 
on a medium coated with a light-sensitive material. 
A laser could imprint the storage medium with dig-
ital data. It could then read the data and translate it 
into electrical signals that could be interpreted by a 
computer. Commonly used optical storage media in-
cluded CD-ROMs and later DVD technology. Blu-ray 
discs are also examples of optical storage technology. 
Meanwhile, advances in magnetic storage led to the 
debut of shingled magnetic recording (SMR) de-
vices. SMR devices can create layers of magnetic data 
that partially overlap like roof shingles. This gives 
them vastly increased storage capacity.
Solid-State Storage
Reading and writing on magnetic or optical media 
requires complex moving parts. For instance, a mag-
netic or optical disk must be turned and a moving 
laser or electronic head must be used to read the 
Removable memory comes in many forms, partly because of changes in technology and partly because they 
solve different challenges for the user. Most external data storage devices have some balance of portability 
and durability. By avaragado from Cambridge, CC BY 2.0 (http://creativecommons.org/licenses/by/2.0), 
via Wikimedia Commons

244
Removable memory
Principles of Computer Science
encoded data. These delicate moving parts are prone 
to malfunction. To avoid this problem, engineers in 
the 1980s pioneered the use of self-contained solid-
state storage systems in which data could be encoded 
in the form of electrical signals and circuits.
Among the earliest types of solid-state storage 
was read-only memory (ROM). This typically re-
fers to a storage system that is built into a computer 
and cannot be erased or altered by a user. Some 
modern ROM systems, however, can be altered by 
the user with specialized tools. A computer’s ROM 
usually contains the basic instructions that the com-
puter needs to function. CD-ROMs are so-called 
because the information they hold also cannot be 
changed or erased, although they are not solid-state 
technology.
In the 1990s a variety of solid-state removable 
memory systems were introduced to the consumer 
market. These included flash memory chips and 
drives that store data as electrical signals. The popular 
thumb drives, USB drives, and other memory cards 
and drives are solid-state storage media. Engineers 
have also created multi-level cells (MLC), which can 
store more digital data in each electrical cell and so 
provide vast increases in storage capacity.
Clouds and Virtual Computing
Removable media, while still used for many applica-
tions, is rapidly changing due to growth of virtual 
data storage. Cloud networks let users save data to the 
Web and later retrieve the data using any Internet-
enabled device. They are rapidly replacing the use 
of removable storage and external hard drives for 
maintaining personal and organizational data. In ad-
dition, engineers have begun using virtual networks 
to increase processing capability by distributing tasks 
across several different computers. As cloud storage 
and virtual networks handle more of the storage and 
processing, removable storage may be used less and 
less by consumers while data centers have greater 
need of physical storage.
—Micah L. Issitt
Bibliography
Alcorn, Paul. “SMR (Shingled Magnetic Recording) 
101.” Tom’s IT Pro. Purch, 10 July 2015. Web. 7 Mar. 
2016.
Blanco, Xiomara. “Top Tablets with Expandable 
Storage.” CNET. CBS Interactive, 25 Jan. 2016. 
Web. 7 Mar. 2016.
Edwards, Benj. “From Paper Tape to Data Sticks: The 
Evolution of Removable Storage.” PCWorld. IDG 
Consumer & SMB, 7 Feb. 2010. Web. 7 Mar. 2016.
Goodwins, Rupert. “The Future of Storage: 2015 and 
Beyond.” ZDNet. CBS Interactive, 1 Jan. 2015. Web. 
7 Mar. 2016.
Mercer, Christina. “5 Best Storage Devices for 
Startups: Removable, SSD or Cloud, What Type 
of Storage Should Your Business Use?” Techworld. 
IDG UK, 22 Dec. 2015. Web. 7 Mar. 2016.
Taylor, Ben. “Cloud Storage vs. External Hard Drives: 
Which Really Offers the Best Bang for your Buck?” 
PCWorld. IDG Consumer and SMB, 10 July 2014. 
Web. 7 Mar. 2016.

245
Scaling systems
Fields of Study
Information Systems; Information Technology; 
System-Level Programming
Abstract
System scalability is the ability of a computer system 
or product to continue to function correctly when it 
is altered in size or volume due to user need. To be 
considered scalable, a system must be able not only 
to function properly while scaled but also to adapt to 
and take advantage of its new environment and in-
creased capacity.
Prinicipal Terms

 caching: the storage of data, such as a previously 
accessed web page, in order to load it faster upon 
future access.

 heterogeneous scalability: the ability of a multi-
processor system to scale up using parts from dif-
ferent vendors.

 horizontal scaling: the addition of nodes (e.g., 
computers or servers) to a distributed system.

 proxy: a dedicated computer server that functions 
as an intermediary between a user and another 
server.
 vertical scaling: the addition of resources to a single 
node (e.g., a computer or server) in a system.
Scalability
As computer technology rapidly advances, computer 
systems must handle increasingly large and complex 
demands. Rather than replacing an existing system 
each time these demands exceed its capacity, one can 
simply scale up or out.
Scalability is the ability of a computer system to 
adapt to and accommodate an increased workload. 
Horizontal scaling works by distributing the workload 
across multiple servers or systems. Vertical scaling in-
volves adding resources, such as hardware upgrades, 
to an existing server or system. Scalability allows data 
to be processed at a greater rate, decreasing load 
times and increasing productivity.
Horizontal versus Vertical Scaling
Both horizontal and vertical scaling play key roles in 
how networks and computers operate. How fast and 
how well a program or web application (app) func-
tions depends on the resources at its disposal. For ex-
ample, a new web app may have only a small number 
of users per day, who can easily be accommodated by 
a single server. However, as the number of users in-
creases, that server will at some point become unable 
to support them all. Users may find that their con-
nection is much slower, or they may be unable to con-
nect to the server at all. At this point, the server must 
be scaled either vertically, such as by adding more 
memory, or horizontally, by adding more servers.
Vertical scaling is limited by the physical size of 
the existing system and its components. Fortunately, 
most modern computers employ open architecture, 
which supports the addition of hardware from dif-
ferent vendors. This allows for greater flexibility 
when upgrading components or even rebuilding 
part of the system to accommodate more compo-
nents. One example of vertical scaling might be 
adding another processing unit to a single-processor 
system so that it can carry out parallel operations. An 
open-architecture system could use processors from 
different vendors or even different types of proces-
sors that perform different functions, such as a cen-
tral processing unit (CPU) and a graphic processing 
unit (GPU). A system that can be scaled using such 
heterogeneous components is said to have heteroge-
neous scalability.
S

246
Scaling systems
Principles of Computer Science
Ultimately, however, vertical scaling can only go 
so far. For larger-scale operations such as cloud com-
puting, horizontal scaling is more common. Cloud 
computing connects multiple hardware or software 
components, such as servers or networks, so they can 
work as a single unit. The downside of horizontal 
scaling is that it is not immediate. It requires careful 
planning and preparation to make the components 
work logically together. While adding more servers 
is a good way to deal with increased traffic, for in-
stance, if the system cannot properly balance the 
load across the available servers, a user may see little 
improvement.
Other Scaling Techniques
Scalability refers to more than just processing power. 
It also includes various techniques to improve system 
efficiency and provide the best performance even 
when usage levels are high. One way to reduce server 
load during periods of high usage is to implement 
caching. A cache is a block of memory in which 
recently accessed files are stored briefly for easy 
2 CPUs
8 CPUs
2 CPUs
2 CPUs
2 CPUs
2 CPUs
Vertical scaling
Baseline
Horizontal scaling
$
$ $ $ $ $
$ $ $ $
$
Vertical and horizontal scaling have their pros and cons. Vertical scaling increases computing power 
with a more powerful (and often more costly) computer. Horizontal scaling increases computing 
power with more computers of equal power (and equal price), but a more complex network and 
programming is required. EBSCO illustration.

247
Principles of Computer Science
Scaling systems
retrieval. If a user requests a file from the server that 
was accessed recently and is still in the cache, the 
system can provide the cached version instead of the 
original. This provides a faster response time and re-
duces the load on the server.
Another way is to use a proxy server. This is particu-
larly useful for a system with multiple servers. A proxy 
functions as an intermediary between the user and 
the main servers. It receives requests from users and 
directs them to the appropriate servers. A proxy can 
also combine requests to speed up processing. If mul-
tiple users request the same data, a system without a 
proxy has to perform multiple retrievals of that data. 
However, if the requests are filtered through a proxy 
server, the proxy can perform a single retrieval and 
then forward the data to each user.
Scaling for the Cloud
Unlike other computing systems, many cloud-com-
puting services automatically scale up or down to be 
more efficient. Extra computing resources, such as 
additional servers, are provided when usage is high 
and then removed when they are no longer needed. 
This is known as “auto-scaling.” It was first introduced 
by Amazon Web Services and has since been adopted 
by other cloud-computing services, such as Microsoft 
Azure and Google Cloud Platform.
For large Internet companies such as these, which 
support massive distributed storage and cloud-com-
puting systems, simply auto-scaling is not enough. 
They must be able to scale up from a handful of 
servers to thousands rapidly, efficiently, and resil-
iently, without server outages that might impact user 
experience. This type of architecture is described 
as “hyper-scale.” Hyper-scale data-center architec-
ture is mainly software-based, replacing much of the 
hardware of a traditional data center with virtual 
machines. In addition to supporting rapid, efficient 
auto-scaling, this greatly reduces infrastructure costs 
for both established and new companies. Large 
companies that support hundreds of millions of daily 
users do not have to pay to operate the massive data 
centers that traditional architecture would require. 
Meanwhile, new start-ups can launch with just a few 
servers and then easily scale up to a few thousand 
when necessary.
Receive-side scaling (RSS) is another way in which 
companies can scale up. This directs incoming net-
work traffic to various CPUs for processing, thereby 
speeding up the network. RSS has become a feature 
in some cloud-computing services, such as Microsoft 
Azure.
—Daniel Horowitz
Bibliography
De George, Andy. “How to Autoscale an Application.” 
Microsoft Azure. Microsoft, 7 Dec. 2015. Web. 17 
Feb. 2016.
El-Rewini, Hesham, and Mostafa Abd-El-Barr. 
Advanced 
Computer 
Architecture 
and 
Parallel 
Processing. Hoboken: Wiley, 2005. Print.
Evans, Kirk. “Autoscaling Azure—Virtual Machines.” 
.NET from a Markup Perspective. Microsoft, 20 Feb. 
2015. Web. 17 Feb. 2016.
Hill, Mark D. “What Is Scalability?” Scalable Shared 
Memory Multiprocessors. Ed. Michel Dubois and 
Shreekant Thakkar. New York: Springer, 1992. 
89–96. Print.
Matsudaira, Kate. “Scalable Web Architecture and 
Distributed Systems.” The Architecture of Open Source 
Applications. Ed. Amy Brown and Greg Wilson. Vol. 
2. N.p.: Lulu, 2012. PDF file.
Miniman, 
Stuart. 
“Hyperscale 
Invades 
the 
Enterprise.” Network Computing. UBM, 13 Jan. 
2014. Web. 8 Mar. 2016.
Peterson, Larry L., and Bruce S. Davie. Computer 
Networks: A Systems Approach. 5th ed. Burlington: 
Morgan, 2012. Print.

248
Signal processing
Principles of Computer Science
Signal processing
Fields of Study
Algorithms; Information Technology; Digital Media
Abstract
“Signal processing” refers to the various technolo-
gies by which analog or digital signals are received, 
modified, and interpreted. A signal, broadly defined, 
is data transmitted over time. Signals permeate ev-
eryday life, and many modern technologies operate 
by acquiring and processing these signals.
Prinicipal Terms

 analog signal: a continuous signal whose values or 
quantities vary over time.

 filter: in signal processing, a device or procedure 
that takes in a signal, removes certain unwanted 
elements, and outputs a processed signal.

 fixed point: a type of digital signal processing in 
which numerical data is stored and represented 
with a fixed number of digits after (and sometimes 
before) the decimal point, using a minimum of 
sixteen bits.

 floating point: a type of digital signal processing 
in which numerical data is stored and represented 
in the form of a number (called the mantissa, or 
significand) multiplied by a base number (such as 
base-2) raised to an exponent, using a minimum 
of thirty-two bits.

 Fourier transform: a mathematical operator that 
decomposes a single function into the sum of mul-
tiple sinusoidal (wave) functions.

 linear predictive coding: a popular tool for dig-
ital speech processing that uses both past speech 
samples and a mathematical approximation of a 
human vocal tract to predict and then eliminate 
certain vocal frequencies that do not contribute to 
meaning. This allows speech to be processed at a 
faster bit rate without significant loss in compre-
hensibility.
Digital and Analog Signals
Signals can be analog or digital. Analog signals are 
continuous, meaning they can be divided infinitely 
into different values, much like mathematical func-
tions. Digital signals, by contrast, are discrete. They 
consist of a set of values, and there is no informa-
tion “between” two adjacent values. A digital signal is 
more like a list of numbers.
Typically, computers represent signals digitally, 
while devices such as microphones represent signals 
in an analog fashion. It is possible to convert a signal 
from analog to digital by “sampling” the signal—that 
is, recording the value of the signal at regular inter-
vals. A digital signal can be converted to analog by 
generating an electrical signal that approximates the 
digital signal.
One example of analog-to-digital signal conver-
sion is the setup used to record a music perfor-
mance to CD. The sound waves produced by the 
musicians’ voices and instruments are picked up 
by microphones plugged into a computer. The mi-
crophones convert the sound waves to analog elec-
trical signals, which the computer then converts 
to digital ones. Each signal can then be processed 
separately. For example, the signal from the drum 
can be made quieter, or that from the guitar can be 
processed to add distortion or reverb. Once the in-
dividual signals are processed, they are combined 
into a single signal, which is then converted to the 
CD format.
Processing Signals
The core concept in signal processing is the Fourier 
transform. A transform is a mathematical oper-
ator that changes how data are expressed without 
changing the value of the data themselves. In signal 
processing, a transform changes how a signal is rep-
resented. The Fourier transform allows signals to be 
broken down into their constituent components. 
It turns a single signal into superimposed waves of 
different frequencies, just as a music chord consists 
of a set of superimposed musical notes. The signals 
can then be quickly and efficiently analyzed to sup-
press unwanted components (“noise”), extract fea-
tures contained in an image, or filter out certain 
frequencies.
In signal processing, a filter is a device or process 
that takes in a signal, performs a predetermined op-
eration on it (typically removing certain unwanted 
elements), and outputs a processed signal. For 
example, a low-pass filter removes high frequen-
cies from a signal, such as the sound produced by a 

249
Principles of Computer Science
Signal processing
piccolo, and leaves only the low frequencies intact, 
such as those from a bass guitar. A high-pass filter 
does the opposite. It removes low frequencies from 
signals, keeping only the high frequencies.
When processing digital signals, one important 
consideration is how the data should be stored. The 
discrete values that make up a digital signal may be 
stored as either fixed point or floating point. Both 
formats have advantages and disadvantages. A digital 
signal processor (DSP) is optimized for one or the 
other. Fixed-point DSPs are typically cheaper and 
require less processing power but support a much 
smaller range of values. Floating-point DSPs offer 
greater precision and a wider value range, but they 
are costlier and consume more memory and power. 
Floating-point DSPs can also handle fixed-point 
Signal recorder
Signal deconvolution
processing
1
Original audio spectrum
Amplitude
Frequency
2
Undesired convolution
Amplitude
Frequency
3
Restored audio spectrum
Amplitude
Frequency
5
Deconvolution filter
Amplitude
Frequency
4
Resonance peaks
Amplitude
Frequency
Signal processing involves sensors that receive a signal input (1). Programming to record the input may include deconvolution filters 
(5) to remove any undesired convolutions (2), such as harmonic resonance (4), so that the signal output (3) is restored. EBSCO 
illustration.

250
Signal processing
Principles of Computer Science
values, but because the system is optimized for 
floating point, the advantages of fixed point are lost.
Extracting Information from Signals
Sometimes a signal contains encoded information 
that must be extracted. For example, a bar-code 
reader extracts the bar code from an acquired image. 
In this case, the image is the signal. It varies in space 
rather than in time, but similar techniques can be ap-
plied. Filters can be used to remove high spatial fre-
quencies (such as sudden changes in pixel value) or 
low spatial frequencies (such as very gradual changes 
in pixel value). This allows the bar-code reader to 
find and process the bar code and determine the nu-
merical value that it represents.
Applications
Signal processing is everywhere. Modems take in-
coming analog signals from wires and turn them into 
digital signals that one computer can use to commu-
nicate with another. Cell phone towers work similarly 
to let cell phones communicate. Computer vision is 
used to automate manufacturing or tracking. Signals 
are stored in CDs, DVDs, and solid-state drives (SSDs) 
to represent audio, video, and text.
One particular application of signal processing is 
in speech synthesis and analysis. Speech signal pro-
cessing relies heavily on linear predictive coding, 
which is based on the source-filter model of speech 
production. This model posits that humans produce 
speech through a combination of a sound source 
(vocal cords) and a linear acoustic filter (throat and 
mouth). The filter, in the rough shape of a tube, 
modifies the signal produced by the sound source. 
This modification produces resonant frequencies 
called “formants,” which make speech sound nat-
ural but carry no inherent meaning. Linear predic-
tive coding uses a mathematical model of a human 
vocal tract to predict these formants. They can then 
be removed for faster analysis of human speech or 
produced to make synthesized speech sound more 
natural.
—Andrew Hoelscher, MEng, John Vines, and Daniel 
Horowitz
Bibliography
Boashash, Boualem, ed. Time-Frequency Signal Analysis 
and Processing: A Comprehensive Reference. 2nd ed. 
San Diego: Academic, 2016. Print.
Lathi, B. P. Linear Systems and Signals. 2nd rev. ed. New 
York: Oxford UP, 2010. Print.
Owen, Mark. Practical Signal Processing. New York: 
Cambridge UP, 2012. Print.
Prandoni, Paolo, and Martin Vetterli. Signal Processing 
for Communications. Boca Raton: CRC, 2008. Print.
Proakis, John G., and Dimitris G. Manolakis. Digital 
Signal 
Processing: 
Principles, 
Algorithms, 
and 
Applications. 4th ed. Upper Saddle River: Prentice, 
2007. Print.
Shenoi, Belle A. Introduction to Digital Signal Processing 
and Filter Design. Hoboken: Wiley, 2006. Print.
Vetterli, Martin, Jelena Kovacevic, and Vivek K. 
Goyal. Foundations of Signal Processing. Cambridge: 
Cambridge UP, 2014. Print.

251
Principles of Computer Science
Smart homes
Smart homes
Fields of Study
Network Design; Computer Engineering; Embedded 
Systems
Abstract
Smart-home technology encompasses a wide range 
of everyday household devices that can connect to 
one another and to the Web. This connectivity allows 
owners to program simple daily tasks and, in some 
cases, to control device operation from a distance. 
Designed for convenience, smart homes also hold 
the promise of improved independent living for el-
derly people and those with disabilities.
Prinicipal Terms

 dual-mesh network: a type of mesh network in 
which all nodes are connected both through 
wiring and wirelessly, thus increasing the reliability 
of the system. A node is any communication in-
tersection or endpoint in the network (e.g., com-
puter or terminal).

 electronic interference: the disturbance gener-
ated by a source of electrical signal that affects an 
electrical circuit, causing the circuit to degrade 
or malfunction. Examples include noise, electro-
static discharge, and near-field and far-field inter-
ference.

 Internet of things: a wireless network connecting 
devices, buildings, vehicles, and other items with 
network connectivity.

 mesh network: a type of network in which each 
node relays signal through the network. A node 
is any communication intersection or endpoint in 
the network (e.g., computer or terminal).

 ubiquitous computing: a trend of embedding mi-
croprocessors and transmitters in everyday ob-
jects, enabling the objects to communicate with 
other computing devices.
The Emerging Field of Home Automation
Smart homes, or automated homes, are houses in 
which household electronics, environmental con-
trols, and other appliances are connected in a net-
work. Home automation is a growing trend in the 
2010s that allows owners to monitor their homes from 
afar, automate basic home functions, and save money 
in the long run on utility payments and other costs. 
Creating a smart home generally involves buying 
a hub and then various smart devices, appliances, 
plugs, and sensors that can be linked through that 
hub to the home network. Setting up a smart home 
is costly and requires strong network infrastructure 
to work well.
Creating the Home Network
Not all smart devices are designed to connect in the 
same way. Wi-Fi enabled devices connect to Wi-Fi net-
works, which use super-high frequency (SHF) radio 
signals to link devices together and to the Internet 
via routers, modems, and range extenders. Other 
smart devices have Bluetooth connectivity. Bluetooth 
also uses SHF radio waves but operates over shorter 
ranges. Assorted Bluetooth devices may be con-
nected to a smart-home network for remote in-home 
control but cannot be operated outside the home. 
Essentially, a Wi-Fi network is necessary for Internet-
based control.
More advanced options for smart-home net-
working involve mesh networks. In these networks, 
each node can receive, repeat, and transmit signals 
to all others in the network. Mesh networks may 
be wired, with cables connecting the nodes. More 
often they are wireless, using Bluetooth or Wi-Fi to 
transmit radio signals between nodes. Smart-home 
network company Insteon has created products 
linked into dual-mesh networks. Such networks use 
a dual-band connection, with both wired and wire-
less connections between the nodes. Dual-mesh 
networks are designed to avoid electronic inter-
ference from outside signals coming from sources 
such as microwave ovens and televisions. In a dual-
mesh network, interruption to the power lines en-
tering the home or to the home’s wireless network 
will not prevent signal from reaching a networked 
device. Mesh networks also work with devices from 
different manufacturers, while other types of net-
works may not be compatible with all of an owner’s 
devices.
Connecting Smart Devices and Utilities
Different types of devices can be connected to smart-
home networks. Smart light switches and dimmers can 

252
Smart homes
Principles of Computer Science
Smoke sensor
Camera
Automatic curtain
Motion
detector lights
Smart
thermostat
Automatic air
conditioning switch
Automatic
window
Gas sensor
Intelligent switch
Magnetic
door sensor
REMOTE MONITORING
There are a number of devices and technologies available to improve a home’s effi ciency and sustainability, allowing individuals to cus-
tomize their environment at any time, anywhere. Some products, such as programmable thermostats, can be programmed to run only 
at the optimal time needed to reduce resource use. Others such as motion-sensored lights are designed to respond instantly to demand. 
There are even devices that sync to a smartphone and allow the homeowner to control security, lighting, and many other devices remotely. 
EBSCO illustration.

253
Principles of Computer Science
Smart homes
be used to control existing light fixtures, turning lights 
on or off or connecting them to a scheduler for timed 
activation. A smart-home owner might program their 
lights to coincide with the sunrise/sunset cycle, for ex-
ample. The use of smart plugs also allows for the control 
of existing, ordinary devices. Smart kitchen appliances 
and entertainment units are becoming available as well.
Other devices can be used to monitor home utili-
ties. For instance, smart water sensors can detect 
moisture or leaks. Smart locking devices placed on 
doors and windows can alert the owner when a door 
or window is unsecured and allow them to lock and 
unlock doors and windows remotely. Sensors and 
controls can measure heat within each room and ad-
just thermostat settings. Smart-home owners can use 
programmable thermostats to lower temperatures 
during sleep periods and to raise heat levels before 
they wake. While many thermostats have timers, au-
tomated systems also allow owners to adjust heat and 
energy consumption from their smartphone or com-
puter, even when away from the home. Some even 
feature machine learning, in which the device adjusts 
its behavior over time in response to repeated usage 
patterns. By shutting off air conditioning, heat, or 
electricity when not needed, automated monitoring 
and control can save owners large amounts of money 
in utility costs each year.
Goals and Challenges
Smart-home technology is part of a technological 
movement called the Internet of things (IoT). IoT 
is based on the idea that devices, vehicles, and build-
ings will all someday be linked into a wireless network 
and collect and communicate data about the envi-
ronment. This concept relies on ubiquitous com-
puting, the practice of adding microprocessors and 
Internet connectivity to basic devices and appliances. 
Ubiquitous computing aims to make data processing 
a constant background activity embedded in daily life.
While still in its infancy, smart-home technology 
is intended to enhance security and convenience 
for homeowners. A home could be configured, for 
instance, so that motion sensors detecting activity in 
the morning open window shades, turn on a coffee 
maker, and raise the heat. Although still largely a 
hobbyist industry, home automation is also begin-
ning to help elderly people and those with disabili-
ties. Features such as voice activation, mobile apps, 
and touch screens make it easier for those owners to 
control their environment and live independently.
Consumers have been slow to adopt smart-home 
technology for several reasons. One is fear that 
manufacturers will use such devices to collect and 
sell data about their personal habits. Another is fear 
that the security of such systems could be hacked 
and 4 Science Reference Center™ Smart Homes 
lead to safety issues such as burglary. The multiple 
users’ preferences, power dynamics, and rapid shifts 
in schedule that present in family life pose further 
challenges for smart-home systems. With the debut 
of wearable digital devices like smart watches, some 
companies are experimenting with linking wearable 
and smart-home devices. Wearable devices that can 
record biometric data could be used to verify a home-
owner’s identity for increased security and personal-
ization, for example.
—Micah L. Issitt
Bibliography
Clark, Don. “Smart-Home Gadgets Still a Hard Sell.” 
Wall Street Journal. Dow Jones, 5 Jan. 2016. Web. 12 
Mar. 2016.
Glink, Ilyce. “10 Smart Home Features Buyers 
Actually Want.” CBS News. CBS Interactive, 11 Apr. 
2015. Web. 12 Mar. 2016.
Gupta, Shalene. “For the Disabled, Smart Homes 
Are Home Sweet Home.” Fortune. Fortune, 1 Feb. 
2015. Web. 15 Mar. 2016.
Higginbotham, Stacey. “5 Reasons Why the ‘Smart 
Home’ Is Still Stupid.” Fortune. Fortune, 19 Aug. 
2015. Web. 12 Mar. 2016.
Taylor, Harriet. “How Your Home Will Know What 
You Need Before You Do.” CNBC. CNBC, Jan 6 
2016. Web. 11 Mar. 2016.
Vella, Matt. “Nest CEO Tony Fadell on the Future of 
the Smart Home.” Time. Time, 26 June 2014. Web. 
12 Mar. 2016.

254
Software architecture
Principles of Computer Science
Software architecture
Fields of Study
Applications; 
Software 
Engineering; 
Operating 
Systems
Abstract
Software architecture refers to the specific set of deci-
sions that software engineers make to organize the 
complex structure of a computer system under devel-
opment. Sound software architecture helps minimize 
the risk of the system faltering as well as optimizing its 
performance, durability, and reliability.
Prinicipal Terms

 agile software development: a method of software 
development that addresses changing require-
ments as they arise throughout the process.

 component-based development: an approach to 
software design that uses standardized software 
components to create new applications and new 
software.

 functional requirement: a specific function of a 
computer system, such as calculating or data pro-
cessing.

 nonfunctional requirement: also called extrafunc-
tional requirement; an attribute of a computer 
system, such as reliability, scalability, testability, 
security, or usability, that reflects the operational 
performance of the system.

 plug-in: an application that is easily installed 
(plugged in) to add a function to a computer 
system.

 separation of concerns: a principle of software 
engineering in which the designer separates the 
computer program’s functions into discrete ele-
ments.
Challenges of Software Architecture
Building architects deal with static structures that 
maintain their structural integrity over the long 
term. In contrast, the architecture of a computer 
program must be able to change, grow, and be modi-
fied. “Software architecture” refers to the internal 
operations of a system under development and how 
its elements will ultimately function together. It is a 
blueprint that helps software engineers avoid and 
troubleshoot potential problems. These problems 
are far easier to address while the system is in devel-
opment rather than after it is operational.
Software architects examine how a system’s func-
tional requirements and nonfunctional require-
ments relate to each other. Functional requirements 
control what processes a system is able to perform. 
Nonfunctional requirements control the overall op-
eration of a program rather than specific behaviors. 
They include performance metrics such as manage-
ability, security, reliability, maintainability, usability, 
adaptability, and resilience. Nonfunctional require-
ments place constraints on the system’s functional 
requirements.
The main challenge for the software architect is to 
determine which requirements should be optimized. 
If a client looking for a new software program is asked 
what requirements are most critical—usability, per-
formance, or security, for instance—they are most 
likely going to say all of them. Because this cannot 
be done without exorbitant costs, the software archi-
tect prioritizes the list of requirements, knowing that 
there must be trade-offs in the design of any applica-
tion. If, for instance, the project is a long-term home-
loan application program or a website for purchasing 
airplane tickets, the architect will most likely focus on 
security and modification. If the project is short term, 
such as a seasonal marketing campaign or a website 
for a political campaign, the architect will focus in-
stead on usability and performance.
Methodology
Software architecture design is an early-stage Abstract 
process that allows for the testing of an assortment of 
scenarios in order to maximize the functionality of 
a system’s most critical elements. Well-planned soft-
ware architecture helps developers ascertain how the 
system will operate and how best to minimize poten-
tial risks or system failures. In most cases, software 
architecture factors in modifiability, allowing the 
system to grow over time to prevent obsolescence and 
anticipate future user needs.
Software architecture projects can involve weeks, 
months, or even years of development. Software ar-
chitects often develop a basic skeletal system early 
in the development process. While this early system 
lacks the depth and reach of the desired program, 

255
Principles of Computer Science
Software architecture
it provides critical insights into the system’s devel-
oping functionality. Incremental modifi cations are 
then made and tested to ensure that the emerging 
system is performing properly. Multiple iterations 
of the system are made until full functionality is 
achieved. This approach is known as agile software 
development.
Software architects work to understand how the 
system will ultimately operate to meet its perfor-
mance requirements. Programs should be designed 
to be fl exible enough to grow into more sophisti-
cated and more specialized operations. Plug-ins 
can be used to add features to existing systems. By 
designing software functions as discrete elements, 
software architects can make the system easier to 
adapt. This design principle is known as separation 
of concerns. It allows one element of the program 
to be upgraded without dismantling the entire super-
structure, for example. This has the potential to save 
considerable time and money. Component-based 
development is a related idea that borrows from the 
industrial assembly-line model. It involves using and 
reusing standardized software components across 
different programs.
implications
Software architecture aims to balance the end user’s 
needs with the system’s behavioral infrastructure and 
the expectations of the company that will support the 
software. Software architects start the development 
process by looking at the broadest possible implica-
tions of a proposed system’s elements and their re-
lationships to one another. This distinguishes them 
from code developers, whose vision is often relatively 
narrow and specifi ed. Software architects evaluate 
the critical needs of the software, consider the needs 
of both the client and the end user, and draft system 
blueprints that maximize those requirements while 
managing the practical concerns of time and cost.
—Joseph Dewey
bibliography
Bass, Len, Paul Clements, and Rick Kazman. Software 
Architecture in Practice. 3rd ed. Upper Saddle River: 
Addison, 2012. Print.
Cervantes, Humberto, and Rick Kazman. Designing 
Software Architectures: A Practical Approach. Upper 
Saddle River: Addison, 2016. Print.
Clements, 
Paul, 
et 
al. 
Documenting 
Software 
Architectures: Views and Beyond. 2nd ed. Upper 
Saddle River: Addison, 2011. Print.
Langer, Arthur M. Guide to Software Development: 
Designing and Managing the Life Cycle. New York: 
Springer, 2012. Print.
Mitra, Tilak. Practical Software Architecture: Moving from 
System Context to Deployment. Indianapolis: IBM, 
2015. Print.
Build for adaptivity 
rather than longevity
Use computer 
modeling to analyze 
risk and dependencies
Use visualizations for 
communication and 
collaboration
Minimize complexity 
through key engineering 
decisions
SOFTWARE ARCHITECTURE DESIGN PRINCIPLES
Software architecture is the structural design of the software. When designing new software, it is important to build in fl exibility, use 
models to analyze the design and reduce risks, use visualizations to communicate and collaborate with stakeholders, and maintain a 
focus on reducing complexity. EBSCO illustration.

256
Software regulations
Principles of Computer Science
Rozanski, Nick, and Eoin Woods. Software Systems 
Architecture: 
Working 
with 
Stakeholders 
Using 
Viewpoints and Perspectives. 2nd ed. Upper Saddle 
River: Addison, 2012. Print.
Sonmez, John Z. Soft Skills: The Software Developer’s Life 
Manual. Shelter Island: Manning, 2015. Print.
Taylor, Richard N., Nenad Medvidović, and Eric M. 
Dashofy. Software Architecture: Foundations, Theory, 
and Practice. Hoboken: Wiley, 2010. Print
Software regulations
Fields of Study
Software Engineering
Abstract
Software developers must keep numerous legal and 
regulatory considerations in mind when creating 
software. Organizations that use the software should 
also be aware of these regulations in order to remain 
compliant, both in their record keeping and in their 
use of internal-use, proprietary, and open-source 
software.
Prinicipal Terms

 compliance: adherence to standards or specifica-
tions established by an official body to govern a 
particular industry, product, or activity.

 Health Insurance Portability and Accountability 
Act (HIPAA): a 1996 law that established national 
standards for protecting individuals’ medical re-
cords and other personal health information.

 internal-use software: software developed by a 
company for its own use to support general and 
administrative functions, such as payroll, ac-
counting, or personnel management and mainte-
nance.

 open-source software: software that makes its 
source code available to the public for free use, 
study, modification, and distribution.

 proprietary software: software owned by an indi-
vidual or company that places certain restrictions 
on its use, study, modification, or distribution and 
typically withholds its source code.

 Sarbanes-Oxley Act (SOX): a 2002 law that re-
quires all business records, including electronic 
records and electronic messages, to be retained 
for a certain period of time.
Software Regulations and Legal 
Standards
Until 1983, computer programs could not copy-
righted in the United States. A software developer 
could copyright source code, but not the binary pro-
gram produced when this code is compiled. This is 
because the compiled program was viewed as a “utili-
tarian good” generated from the code rather than a 
creative work. In order to assert a copyright, the de-
veloper had to make the source code available with 
the program. While publishing the source code gave 
a developer greater control, it also made it easier for 
others to copy and modify the program.
Copyright rules began to change with the US 
Court of Appeals’ decision in Apple Computer, Inc. 
v. Franklin Computer Corp. (1983). It was the first ap-
pellate court ruling to state that machine-readable 
code is subject to copyright. Prior to this, developers 
had no reason to withhold source code. Computers 
were not standardized enough to make large-scale 
development profitable, and software often had 
to be modified to run on different computers. 
Introducing software copyright allowed for greater 
profit potential and provided new incentives for 
standardization. It also gave developers a reason to 
keep source code private: now they had a copyright 
to protect.
Since then, several laws have been passed to reg-
ulate both software development and its end use. 
Because computer technology is constantly evolving, 
new regulations are often needed. One such law is 
the Health Insurance Portability and Accountability 
Act (HIPAA). HIPAA includes provisions designed 
to protect patients’ health information, particularly 
when using software developed for health-care pro-
viders. Any software or application (app) that collects 
or stores personally identifiable health information 
or shares it with certain covered medical entities, 

257
Principles of Computer Science
Software regulations
such as doctors and hospitals, must be in compliance 
with HIPAA.
Another law, the Sarbanes-Oxley Act (SOX), 
covers information retention. It states that all orga-
nizations, regardless of size, must retain certain busi-
ness records for at least five years. E-mails and elec-
tronic records are included in this category.
Types of Software
There are several types of software, distinguished by 
which license governs their use. A software license is 
a legal instrument that states how copyrighted soft-
ware can be used. Open-source software makes its 
source code available, with no restrictions on how it 
may be used. Its license gives users the right to modify 
the program, make copies, and distribute it to others. 
Open-source software is usually, but not always, free 
of charge.
Proprietary software is software on which the copy-
right holder has placed certain restrictions. It typi-
cally comes with a license agreement. This is an im-
plied contract between the copyright holder and the 
end user. The license agreement spells out what the 
user can and cannot to do with the software. It may 
also include a disclaimer of responsibility should the 
software damage the user’s computer in some way. As 
a legal contract, license agreements can, in theory, be 
enforced in court. In practice, enforceability may de-
pend on the terms of the agreement, how and when 
the user consented to it, and even which court has 
jurisdiction.
Other types of software include freeware, share-
ware, and internal-use software. Freeware can be 
freely used, copied, and distributed but does not 
permit modification of source code. Shareware 
is a type of proprietary software that is initially 
provided for no cost and can be freely copied 
and distributed, but continued use under certain 
conditions requires the purchase of a license. 
Internal-use software, or private software, is de-
veloped for a company’s own internal use but not 
made publicly available.
Mobile and Smartphone Software
In 2008, Google released the first version of Android, 
a smartphone operating system (OS) based on 
Linux. Android is open-source software, with source 
code available through the Android Open Source 
Project. As a result, a large community has formed 
in which developers modify and distribute their own 
Financial data in publicly traded corporations
Health care information
Credit card information stored by merchants
Personal financial information stored by financial institutions
Customers’ personal information stored by any organization that does business in 
the state of California
Personal financial information and transactions transmitted and stored by 
financial institutions
Sarbanes-Oxley
HIPAA
PCI
GLBA
SB 1386
BASEL II
Act
Maintaining privacy, confidentiality, and integrity of . . .
Without a single overarching regulatory organization for online personal information privacy and confidentiality, a number 
of acts have been adopted to protect consumers. Each act focuses on a different aspect of online privacy protection. EBSCO 
illustration.

258
Software regulations
Principles of Computer Science
versions of Android. These modified versions often 
provide updates and bug fixes ahead of official re-
leases. Others are designed to support older devices 
or to run on devices designed for other OSs.
The Food and Drug Administration (FDA) has 
said that it does not intend to regulate mobile med-
ical apps and consumer devices to the same extent as 
other medical software. Official guidelines state that 
unless an app or device makes disease-specific claims, 
it will receive no or low-level oversight, depending 
on how much risk it poses to patients. Any app that 
shares health information with covered medical enti-
ties must be HIPAA compliant.
The Value of Software Regulation
Software regulations and standards provide nu-
merous benefits, including limiting flaws in soft-
ware and lessening users’ exposure to viruses. They 
are also geared toward protecting users’ privacy. 
Regulation is about ensuring the confidentiality, ac-
cessibility, availability, and integrity of information. It 
is a form of accountability that will allow both propri-
etary and open-source software to improve as tech-
nology moves forward.
—Daniel Horowitz
Bibliography
Aziz, Scott. “With Regulation Looming, It’s Time for 
Industry to Raise the Bar for Software Quality.” 
Wired. Condé Nast, 28 Aug. 2014. Web. 31 Mar. 
2016.
Balovich, 
David. 
“Sarbanes-Oxley 
Document 
Retention and Best Practices.” Creditworthy News. 
3JM Company, 5 Sept. 2007. Web. 1 Apr. 2016.
“Categories of Free and Nonfree Software.” GNU 
Operating System. Free Software Foundation, 1 Jan. 
2016. Web. 31 Mar. 2016.
Gaffney, Alexander. “FDA Confirms It Won’t Regulate 
Apps or Devices Which Store Patient Data.” 
Regulatory Affairs Professionals Society. Regulatory 
Affairs Professional Soc., 6 Feb. 2015. Web. 31 Mar. 
2016.
Rouse, Margaret. “Sarbanes-Oxley Act (SOX).” 
TechTarget. TechTarget, June 2014. Web. 31 Mar. 
2014.
Wang, Jason. “HIPAA Compliance: What Every 
Developer Should Know.” Information Week. UBM, 
11 July 2014. Web. 26 Feb. 2016.

259
Principles of Computer Science
Software testing
Software testing
Fields of Study
Software 
Engineering; 
Applications; 
Computer 
Engineering
Abstract
Software testing ensures that software and applica-
tions perform well and meet users’ needs. It is in-
tended to expose problems such as defects (often 
referred to as bugs) or inefficient design. Software 
testing allows project managers to make informed 
decisions about moving forward or making changes 
on a product before releasing it.
Prinicipal Terms

 agile software development: an approach to soft-
ware development that addresses changing re-
quirements as they arise throughout the process, 
with programming, implementation, and testing 
occurring simultaneously.

 black-box testing: a testing technique in which 
function is analyzed based on output only, without 
knowledge of structure or coding.

 dynamic testing: a testing technique in which soft-
ware input is tested and output is analyzed by ex-
ecuting the code.

 phased software development: an approach to 
software development in which most testing oc-
curs after the system requirements have been im-
plemented.

 static testing: a testing technique in which software 
is tested by analyzing the program and associated 
documentation, without executing its code.

 white-box testing: a testing technique that in-
cludes complete knowledge of an application’s 
coding and structure.
The Importance of Software Testing
Software testing contributes to the development of 
programs and applications (apps) by detecting gaps 
between the software’s stated requirements and its 
actual performance. This testing informs software 
engineers about any poor functionality, missing re-
quirements, errors, and defects. Originally, program-
mers tested software at the end of the development 
process. However, the popularity of the graphical 
user interface (GUI) and other more user-friendly 
software highlighted the need for more thorough 
testing. Consumers expect programs to work prop-
erly from the start. Therefore, programmers started 
taking a more careful and systematic approach. Soon, 
testing throughout the development process became 
the industry standard.
Manufacturers value software testing because it 
prevents flawed products from entering the market. 
Delivering software or launching an application only 
to have customers discover bugs can have both short- 
and long-term negative effects. Poorly tested software 
can lead to declining sales, damage to the compa-
ny’s brand, and possibly even harm to a customer’s 
business operations. In addition, testing can save 
time and money. It is much easier to correct errors 
during the development process than after a release. 
Without testing, manufacturers risk lost opportuni-
ties, as users who have a bad experience often switch 
to a competing product.
Software Testing Methods
“Software testing” is broad term describing a va-
riety of specific methods. Programmers test dif-
ferent aspects of applications, including design, 
response to inputs, usability, stability, and results. 
The type of test they choose is based on factors 
such as the programming language and what they 
need to learn.
Agile software development is a development 
process that emphasizes teamwork, customer in-
volvement, and user testing of portions of the 
system. Agile practitioners follow the views of the 
“Agile Manifesto” (2001), written by a group of 
developers who wanted to change the software de-
velopment process. According to the agile method-
ology, testing is not a phase. Instead, tests are per-
formed continuously as the product is developed 
and requirements change. In contrast, phased 
software development follows the original require-
ments, includes the features that matter most, 
and tests when development is completed. New 
requirements can be established after the original 
software is in use.
White-box testing is also referred to as glass-box or 
clear-box testing. It produces test data by examining 
the program’s code. White-box testing is useful for 

260
Software testing
Principles of Computer Science
finding errors in hidden code. However, it can be ex-
pensive and requires in-depth knowledge of the pro-
gram or app’s programming language. The opposite 
of white-box testing is black-box testing, which tests 
functionality using only the specifications, not the 
code. Testers know what the app is supposed to do 
but not how it operates.
There are dozens of additional approaches to 
software testing. In static testing, code reviews and 
analyses are used to discover inconsistencies and 
variables in the code. In dynamic testing, the code is 
actually executed and its behavior is analyzed. Static 
testing is for verification, while dynamic testing is for 
validation.
PROJECT START
FINAL DEPLOYMENT
Acceptance test
Performance
test
System
test
Integration
test
Unit test
Detailed
technical design
High-level
technical design
Define user
requirements
Non-functional
system requirements
Functional
system requirements
CODE
Test for proper implementation
Software
testing
Software
design
Software testing using the V-model incorporates multiple tests for each stage of software development to ensure all levels of software 
design, from user requirements to low-level technical design requirements, are implemented properly. Image adapted from Infinite 
Computing Systems, Inc EBSCO illustration.

261
Principles of Computer Science
Software testing
Automating the Process
Because the testing process is critical and detail ori-
ented as well as repetitive, tools have been developed 
to assist in the process. These include:
test management tools
static analysis tools
test data preparation tools
test execution tools
performance testing tools
coverage measurement tools
incident management tools
Software testing tools can manage and schedule the 
testing process. These tools also log defects, track 
changes, monitor performance, and conduct static 
testing, analysis, and design. Specialized software 
performs repetitive tasks and tests that are difficult or 
time consuming to perform manually.
When Testing Is Insufficient
Releasing an app that does not function as expected 
can cause serious repercussions, even for a respected 
company. When Apple first introduced the Apple 
Maps app, users soon discovered that certain cities 
were mislabeled or had disappeared, familiar land-
marks had moved, and satellite images were obscured 
by clouds. As a result, the company faced ridicule, 
and Apple’s CEO was compelled to issue an apology.
To ensure proper testing and avoid such prob-
lems, programmers follow a series of steps, starting 
with reviewing software specifications and developing 
a test plan. They also 4 Science Reference Center™ 
Software Testing write test cases. Test cases are sets of 
actions that verify program functions. Writing effec-
tive test cases can save time and money throughout 
the testing process. As testing proceeds, bugs are dis-
covered, such as when an expected result is not the 
actual result. These bugs are logged and assigned to a 
developer for fixing.
Programmers approach testing from many direc-
tions. Some attempt to make an app perform func-
tions it should not do. Others imagine various sce-
narios, such as what would happen if someone tried 
to use the software maliciously or with no knowledge 
of the application. By putting themselves in the place 
of various users, testers can uncover defects and im-
prove the user experience.
Moving Software Testing to the Cloud
Information technology (IT) firms spend hundreds 
of billions every year on software testing because con-
sumers and businesses expect the software and apps 
they purchase to work as they expect. These firms in-
vest in servers, databases, storage, operating systems, 
and testing tools, all of which need to be upgraded 
and maintained. The cost is generally accepted be-
cause the risk of delivering a flawed product greatly 
outweighs the cost of thorough testing.
As IT companies face greater competition and 
pressures to earn profits, testing will require new 
solutions. Cloud-based systems provide speed and 
cost savings because service providers can maintain 
systems, tools, storage, and databases in a more cost-
effective manner.
—Teresa E. Schmidt
Bibliography
Holland, Bill. “Software Testing: A History.” SitePoint. 
SitePoint, 15 Feb. 2012. Web. 9 Feb. 2016.
Jorgensen, Paul C. Software Testing: A Craftsman’s 
Approach. 4th ed. Boca Raton: CRC, 2014. Print.
Mili, Ali, and Fairouz Tchier. Software Testing: Concepts 
and Operations. Hoboken: Wiley, 2015. Print.
Mitchell, Jamie L., and Rex Black. Advanced Software 
Testing. 2nd ed. 3 vols. Santa Barbara: Rocky Nook, 
2015. Print.
Murphy, Chris. “How to Save $150 Billion: Move All 
App Dev and Testing to the Cloud.” Forbes. Forbes.
com, 3 Feb. 2016. Web. 10 Feb. 2016.
Singh, Amandeep. “Top 13 Tips for Writing Effective 
Test Cases for Any Application.” Quick Software 
Testing. QuickSoftwareTesting, 23 Jan. 2014. Web. 
11 Feb. 2016.
Smith, Catharine. “Tim Cook Issues Apology for 
Apple Maps.” Huffpost Tech. TheHuffingtonPost.
com, 28 Sept. 2012. Web. 11 Feb. 2016.
“Why Do You Need to Test Software?” BBC Academy. 
BBC, 23 Feb. 2015. Web. 7 Feb. 2016.

262
Software-defined radio
Principles of Computer Science
Software-defined radio
Fields of Study
Digital Media; Software Engineering
Abstract
By modifying the hardware of a standard radio set 
with layers of operational software, a software-defined 
radio (SDR) system can greatly expand its versatility 
and power efficiently and cost-effectively. Digital and 
SDR technology provide radio reception a stronger, 
more durable signal. SDR may also allow for the cen-
tralization of common applications that rely on radio 
communication.
Prinicipal Terms

 amplifier: a device that strengthens the power, 
voltage, or current of a signal.

 converter: a device that expands a system’s range 
of reception by bringing in and adapting signals 
that the system was not originally designed to pro-
cess.

 mixer: a component that converts random input 
radio frequency (RF) signal into a known, fixed 
frequency to make processing the signal easier.

 modulator: a device used to regulate or adjust 
some aspect of an electromagnetic wave.

 software-defined antennas: reconfigurable an-
tennas that transmit or receive radio signals ac-
cording to software instructions.

 sound card: a peripheral circuit board that en-
ables a computer to accept audio input, convert 
signals between analog and digital formats, and 
produce sound.
How Radio Works
Radio remains the most useful and most widely 
used communication technology for sharing both 
entertainment and information. The basic tech-
nology of the conventional radio is simple: radio 
waves are enriched with information—music, talk, 
weather, news—and sent wirelessly from a trans-
mitter to a receiver. The receiver might be down 
the street, or halfway around the earth. The source 
emits a steady radio signal whose frequency or am-
plitude is modulated to add the message informa-
tion. Then, signal is beamed from large antennas 
to smaller ones that grab the signal and relay it to 
a receiver. The receiver uses a mixer to convert the 
carrier radio frequency (RF) to an intermediate 
one for easier processing. A filter removes distor-
tion, and the signal is then released into the sys-
tem’s amplifiers, thus recreating the original radio 
programming. The varieties of radio wave tech-
nology—the familiar AM/FM radio bands, walkie-
talkies, ham radio systems, emergency notification 
systems, transportation communication systems—
attest to its vital role in moving information effi-
ciently and effectively.
Although such radio systems revolutionized com-
munication, this analog system did present major 
problems. The signal (that is, the energy wave itself) 
was tied to the information being sent. If the wave is 
disrupted or compromised, the information itself is 
compromised. This has led to frustrations with recep-
tion, the clarity of a signal. Signals are vulnerable to 
distortions within the atmosphere, most notably from 
rain, fog, or solar activity. They can also be disrupted 
or even lost due to surfaces coming between trans-
mitter and receiver. Nearly any solid object, from 
trees to buildings, can destabilize a radio signal. And 
the strength of the original broadcast signal and the 
range of receiver also impact the reach (and quality) 
of the signal. Because the information is relayed by 
waves, the analog radio set relies on an array of hard-
ware components, each with a radio chip designed to 
perform one crucial function. The physical layout of 
each component—antenna, mixer, amplifier, modu-
lator, and sometimes, converters—can be consider-
able. Analog radio, although useful and remarkably 
versatile, is thus cumbersome, costly, and not entirely 
reliable.
Software-defined radio (SDR), like analog radio, 
transmits acoustic information from a source to a 
destination and uses similar equipment. However, 
SDR addresses several longstanding issues, from 
sound quality to cost.
Digital and Software-Defined Radio
In digital radio, information is encoded as binary 
numbers. These are then sent as on-off pulses along 
radio waves. The information is thus generated (or 
defined) by a computer and in turn, the transmis-
sion is decoded by another computing device. More 

263
Principles of Computer Science
Software-defi ned radio
importantly, the transmission is sent again and again 
in a kind of barrage of information. This guarantees 
a stronger, more consistent signal reception because 
the receiver can assemble the reproduced signal from 
the many information fragments recovered from the 
signal fi eld. That process takes fractions of seconds 
longer but greatly improves the sound quality and 
signal reliability.
The SDR process begins with the microphone 
where the initial audio data are created. As in analog 
radio, an amplifi er is fed the signal and strengthens 
it for transmission. That signal is sent through an an-
alog-to-digital converter that recasts the information 
as ASCII data. ASCII is an encoding system that con-
verts English language content into numbers. This 
makes the transfer of the signal to another computer 
possible. A modulator then presses that ASCII data 
onto the designated RF carrier. Amplifi ers strengthen 
the signal to the degree necessary to broadcast it 
from a software-defi ned antenna. A software-defi ned 
receiving antenna retrieves the signal, which is con-
verted to a stable frequency. Then, a demodulator 
separates the ASCII data from the RF carrier. A dig-
ital-to-analog converter in a sound card reproduces 
the information. An amplifi er then projects it into a 
speaker system, headphone, or earpiece. Software is 
used in as much of the signal processing as possible 
in SDR systems.
implications
SDR is perhaps best appreciated for its implications. 
A single outfi t might serve as a radio, certainly. But 
just by adding various software applications, it might 
also provide cell phone reception, fax and ham radio 
capabilities, weather and traffi c information, global 
positioning, and web access. In addition, SDR could 
greatly enhance business operations via videocon-
ferencing. All of those functions use different fre-
quencies in different bands of the electromagnetic 
spectrum. SDR expands the radio’s ability to receive 
Hardwired
Analog signal
Digital signal
Flexible
RF
Hardware
Smart
Antenna
Channelization
and
Sample Rate
Conversion
Analog-
Digital
Converter
Digital-
Analog
Converter
Hardware
• FPCAs
• DSPs   
• ASICs  
Software
• Algorithms
• Middleware  
• CORBA
• Virtual Radio Machine  
Processing Hardware & Software
Output
Input
Software defi ned radio replaces much of the hardware needed for standard analog radio processing with computer hardware and software. 
The software requires conversion of the signal from analog to digital before processing. Adapted from McSush derivation of Tarkvaralise 
raadio plokkskeem.

264
Speech-recognition software
Principles of Computer Science
and process those various frequencies. SDR is already 
used for military operations and cellular networks. 
Its versatility and flexibility may someday make SDR 
a cost-effective way for anyone to access virtually all 
communication systems without greatly altering a 
radio set’s physical parts.
—Joseph Dewey
Bibliography
Dillinger, Markus, Kambiz Madani, and Nancy 
Alonistioti. Software Defined Radio: Architectures, 
Systems, and Functions. Hoboken: Wiley, 2003. 
Print.
Ewing, Martin. The ABCs of Software Defined Radio. 
Hartford: Amer. Radio Relay League, 2012. Print.
Grayver, Eugene. Implementing Software Defined Radio. 
New York: Springer, 2012. Print.
Johnson, C. Richard, and William A. Sethares. 
Telecommunications 
Breakdown: 
Concepts 
of 
Communication Transmitted via Software-Defined 
Radio. New York: Prentice, 2003. Print.
Pu, Di, and Alexander M. Wyglinski. Digital 
Communication Systems Engineering with Software-
Defined Radio. London: Artech, 2013. Print.
Reed, Jeffrey H. Software Radio: A Modern Approach to 
Radio Engineering. New York: Prentice, 2002. Print.
Speech-recognition software
Fields of Study
Software Engineering; Applications; Algorithms
Abstract
Speech-recognition software records, analyzes, and 
responds to human speech. The earliest such systems 
were used for speech-to-text programs. Speech recog-
nition became commonplace in the 2010s through 
automated assistants. Speech recognition depends 
on complex algorithms that analyze speech pat-
terns and predict the most likely word from various 
possibilities.
Prinicipal Terms

 deep learning: an emerging field of artificial in-
telligence research that uses neural network algo-
rithms to improve machine learning.

 hidden Markov model: a type of model used to 
represent a dynamic system in which each state 
can only be partially seen.

 neural network: in computing, a model of infor-
mation processing based on the structure and 
function of biological neural networks such as the 
human brain.

 phoneme: a sound in a specified language or dia-
lect that is used to compose words or to distinguish 
between words.

 speaker independent: describes speech software 
that can be used by any speaker of a given lan-
guage.
The Basics of Speech Recognition
Speech-recognition software consists of computer 
programs that can recognize and respond to human 
speech. Applications include speech-to-text software 
that translates speech into digital text for text mes-
saging and document dictation. This software is also 
used by automated personal assistants such as Apple’s 
Siri and Microsoft’s Cortana, which can respond to 
spoken commands. Speech-recognition software de-
velopment draws on the fields of linguistics, machine 
learning, and software engineering. Researchers first 
began investigating the possibility of speech-recog-
nition software in the 1950s. However, the first such 
programs only became available to the public in the 
1990s.
Speech-recognition software works by recognizing 
the phonemes that make up words. Algorithms are 
used to identify the most likely word implied by the se-
quence of phonemes detected. The English language 
has forty-four phonemes, which can be combined to 
create tens of thousands of different words. A par-
ticularly difficult aspect of speech recognition is dis-
tinguishing between homonyms (or homophones). 
These are words that consist of the same phonemes 

265
Principles of Computer Science
Speech-recognition software
but are typically spelled differently. Examples in-
clude “addition” versus “edition” and “scent” versus 
“cent.” Distinguishing between homonyms requires 
an understanding of context. Speech-recognition 
software must be able to evaluate surrounding words 
to discern the most likely homonym intended by the 
speaker.
Some speech-recognition software uses training, 
in which the speaker fi rst reads text or a list of vo-
cabulary words to help the program learn particu-
larities of their voice. Training increases accuracy 
and decreases the error rate. Software that requires 
training is described as speaker dependent. Speaker-
independent software does not require training, but 
it may be less accurate. Speaker-adaptive systems can 
alter some operations in response to new users.
speech-recognition Algorithms
Research into speech-recognition software began 
in the 1950s. The fi rst functional speech-recogni-
tion programs were developed in the 1960s and 
1970s. The fi rst innovation in speech-recognition 
technology was the development of dynamic time 
warping (DTW). DTW is an algorithm that can ana-
lyze and compare two auditory sequences that occur 
at different rates.
Speech recognition advanced rapidly with the in-
vention of the hidden Markov model (HMM). The 
HMM is an algorithm that evaluates a series of poten-
tial outcomes to a problem and estimates the prob-
ability of each one. It is used to determine the “most 
likely explanation” of a sequence of phonemes, and 
thus the most likely word, given options taken from 
a speaker’s phonemes. Together, HMMs and DTW 
are used to predict the most likely word or words in-
tended by an utterance.
Speech recognition is based on predictive analysis. 
An important part of developing a predictive algo-
rithm is feature engineering. This is the process of 
teaching a computer to recognize features, or rele-
vant characteristics needed to solve a problem. Raw 
speech features are shown as waveforms. Speech 
waveforms are the 2-D representations of sonic sig-
nals produced when various phonemes are said.
An emerging feature in speech recognition is the 
use of neutral networks. These computing systems are 
designed to mimic the way that brains handle compu-
tations. Though only beginning to affect speech rec-
ognition, neural networks are being combined with 
deep learning algorithms, which make use of raw fea-
tures, to analyze data.
Applications and Future directions
Deep neural network algorithms and other ad-
vancements have made speech recognition soft-
ware more accurate and effi cient. The most familiar 
Spectral 
Analysis
Decode
Pronunciation
Models
Rules for 
Grammar
Probability 
Estimate
SPEECH
TRANSCRIPTION
Speech recognition software must perform a number of processes to convert the spoken word to the written word. A spoken sen-
tence must fi rst go through spectral analysis to identify the unique soundwave, then the soundwaves are decoded into potential 
words, and fi nally the words are run through probability algorithms that estimate their likelihood based on the rules of grammar and 
pronunciation models. The end result is the most likely transcription of what was spoken. EBSCO illustration.

266
Speech-recognition software
Principles of Computer Science
applications for speech-recognition technology 
include the voice-to-text and voice-to-type fea-
tures on many computers and smartphones. Such 
features automatically translate the user’s voice 
into text for sending text messages or composing 
documents or e-mails. Most speech-recognition 
programs rely on cloud computing, which is the 
collective data storage and processing capability of 
remote computer networks. The user’s speech is 
uploaded to the cloud, where computers equipped 
with complex algorithms analyze the speech be-
fore returning the data to the user. Automated as-
sistant programs such as Siri and Cortana can use 
an array of data collected from a user’s device to 
aid comprehension. For instance, if a user tells a 
speech-recognition program “bank,” the program 
uses the Internet and global positioning systems to 
return data on nearby banks or banks that the user 
has visited in the past.
Experts predict that speech-recognition apps and 
devices will likely become ubiquitous. Fast, accented, 
or impeded speech and slang words pose much less of 
a challenge than they once did. Speech-recognition 
software has become a basic feature in many new 
versions of the Mac and Windows operating systems. 
These programs also help make digital technology 
more accessible for people with disabilities. In fu-
ture, as voice recognition improves and becomes 
commonplace, a wider range of users will be able to 
use advanced computing features.
—Micah L. Issitt
Bibliography
Gallagher, Sean. “Cortana for All: Microsoft’s Plan 
to Put Voice Recognition behind Anything.” Ars 
Technica. Condé Nast, 15 May 2015. Web. 21 Mar. 
2016.
Information Resources Management Association, ed. 
Assistive Technologies: Concepts, Methodologies, Tools, 
and Applications. Vol. 1. Hershey: Information 
Science Reference, 2014. Print.
“How Speech-Recognition Software Got So Good.” 
Economist. Economist Newspaper, 22 Apr 2014. 
Web. 21 Mar. 2016.
Kay, Roger. “Behind Apple’s Siri Lies Nuance’s 
Speech Recognition.” Forbes. Forbes. com, 24 Mar. 
2014. Web. 21 Mar. 2016.
Manjoo, Farhad. “Now You’re Talking!” Slate. Slate 
Group, 6 Apr. 2011. Web. 21 Mar. 2016.
McMillan, Robert. “Siri Will Soon Understand You 
a Whole Lot Better.” Wired. Condé Nast, 30 June 
2014. Web. 21 Mar. 2016.
Pinola, Melanie. “Speech Recognition through the 
Decades: How We Ended Up with Siri.” PCWorld. 
IDG Consumer & SMB, 2 Nov. 2011. Web. 21 Mar. 
2016.

267
Turing machine
Fields of Study
Algorithms; computer science
Abstract
A Turing machine is a mathematical tool that is the 
equivalent of a digital computer. It is the most widely 
used model of computation in computability and 
complexity theory in the computer world today. In 
its simplest form, it consists of an input/output re-
lationship that the machine computes—the input is 
in binary format on a tape that the machine uses to 
compute the function, and the output is the contents 
of the tape when the machine stops. Studying this 
machine led to the study of classes of language and 
ultimately led to the development of computer pro-
gramming languages.
Prinicipal Terms

 algorithm: a set of step-by-step instructions for per-
forming computations.

 character: a unit of information that represents 
a single letter, number, punctuation mark, blank 
space, or other symbol used in written language.

 coding theory: the study of codes and their use in 
certain situations for various applications.

 function: instructions read by a computer’s pro-
cessor to execute specific events or operations.

 process: the execution of instructions in a com-
puter program.

 programming languages: sets of terms and rules of 
syntax used by computer programmers to create 
instructions for computers to follow. This code is 
then compiled into binary instructions for a com-
puter to execute.

 Turing complete: a programming language that 
can perform all possible computations.
Birth of the Turing Machine
In the 1930s, before digital computers were even 
thought of, several mathematicians began to think 
about what it means to compute a function. Alan 
Turing was one of these mathematicians, and his 
Turing machine became part of the definition of 
a computable function: “A function is computable 
if it can be computed by a Turing machine.” He 
is called the father of modern computing because 
the Turing machine is the precursor to all modern 
computers. 
He developed these ideas at King’s College, 
Cambridge, between the years 1932 to 1935, when 
many free-thinking mathematicians congregated 
there. His mentors include Christopher Morcom, 
Philip Hall, M.H.A. Newman, and Bertrand Russell. 
In 1936-37, he published a groundbreaking paper, 
On computable numbers, with an application to the 
Entscheidungsproblem, describing the theory of 
Turing machines and defining computability. One 
sentence reads “It is possible to invent a single ma-
chine which can be used to compute any comput-
able sequence.” He called this machine a “universal 
computing machine,” and defined it as a Turing 
machine that could read the description of any 
other Turing machine and to carry out what that 
Turing machine would have done. One might 
think that complex tasks need complex machines, 
but that is not true; a simple machine can perform 
extremely complex functions when given enough 
time. Computers are now programmed to perform 
extremely complicated operations very quickly, but 
none of them can outdo a basic Turing machine 
in regards computing a function. Now that we all 
have computers with us constantly, we can see what 
Turing only imagined: He came up with the idea of 
a Universal Turing Machine 10 years before it could 
even be implemented!
T

268
Turing machine
Principles of Computer Science
A model of a Turing Machine as seen at the Go Ask Alice exhibit at the Harvard Collection of Historical Scientific Instruments. By GabrielF 
(Own work) [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons
What Exactly is a Turing Machine?
This simple machine is made up of a tape (a ribbon 
of paper) containing an algorithm and a program-
mable read-write head (an instrument that can read 
the symbols on the tape, write a new symbol and 
move, for example, left/right or up/down). These 
terms may seem a bit archaic, but they give a sense 
of what kinds of machines existed when Turing in-
vented this machine. The input on the tape must 
consist of a finite number of symbols; however, the 
tape, ideally, is infinitely long. Turing imagined this 
type of input to show that there are tasks that these 
machines cannot perform, even with unlimited time 
and working memory.
The Turing machine starts out in a specific state, 
then a program is written, on the tape, consisting of a 
list of transitions, telling the head what the next state 
should be and where it should move. The transition 
tells the head to do three things: (1) print something 
on the tape, (2) move to the right or left by one cell, 
and (3) change to a new state. The machine can also 
stop if there is no unique transition, for example, 
there is nothing on the tape. 
Even Turing himself did not think that this ma-
chine would be the model one would use to actually 
build a practical computing machine. He called his 
idea a machine of the mind, a thought experiment, 
something that was created to explore problems in 

269
Principles of Computer Science
Turing machine
logic and discover the limits of the human mind as it 
applied to machines. In fact, he himself did not pro-
mote his ideas about Turing machines very much in 
the fields of computer science or mathematical logic. 
Others, such as Martin Davis and Marvin Minksy, saw 
the potential for this idea in mainstream logic and 
computer sciences. This idea gained further ground 
in the 1970s with complexity theory and in the 1980s 
with the development of quantum computing.
How does a Turing Machine Work?
There are really only six operations that a basic 
Turing machine can perform:
• Read the symbol that is under the head
• Write a symbol on the tape under the head
• Move the tape one square left
• Move the tape one square right
• Change its state
• Stop
How a Turing machine acts is completely depen-
dent on (1) the current state of the machine, (2) the 
symbol on the tape that is currently being read by the 
head, and (3) a table of transition rules, or the pro-
gram, for the machine. 
We could write this as Statecurrent, Symbol, Statenext, 
Action. If the machine is in a Current State and the 
tape contains the recognized Symbol, the machine 
will then move into the Next State and take Action.
The tape serves as the memory of the machine 
while the head is the mechanism by which data is ac-
cessed and updates the action of the machine.
Here is an example of a program for a Turing ma-
chine which starts with a blank, endless tape. This 
function tells the machine to perform the process 
of printing alternating characters, 0s and 1s, on the 
tape, leaving a blank space in between each numeral. 
The machine has four possible states, A, B, C, or D. 
State
Print
Move
Next State
A
0
R
B
B
R
C
C
1
R
D
D
R
A
With this simple programming, the Turing machine 
will print an endless tape full of alternating 1s and 0s, 
leaving a blank space in between each numeral.
Why is a Turing Machine so Important?
The advent of the Turing machine got people started 
thinking about coding theory for computers, what 
kinds of problems computers are able to solve, and 
what kinds of programming languages we can use to 
communicate with them, such as Turing complete. 
Any computer is, at its base, a Turing machine. It is 
a model of computation that captures the idea of 
what computability is very simply, without needing 
to think about all the parts your computer currently 
contains. According to the Church-Turing thesis, no 
computing device is more powerful than a Turing ma-
chine. Some scientists even find examples of Turing 
machines in nature; for example, our cells have ribo-
somes that translate RNA into proteins, using much 
the same process as a Turing machine.
Alan Turing was a large part of the Enigma project 
during World War II. This project was the subject of 
several movies such as The Imitation Game and Enigma.
—Marianne Moss Madsen, MS
Bibliography
Bernhardt, Chris. Turing’s Vision: The Birth of Computer 
Science. MIT Press, 2016.
Boyle, David. Alan Turing: Unlocking the Enigma. 
CreateSpace Independent Publishing Platform, 2014.
Dyson, George. Turing’s Cathedral: The Origins of the 
Digital Universe. Vintage, 2012.
Hodges, Andrew. Alan Turing: The Enigma: The 
Book That Inspired the Film “The Imitation Game.” 
Princeton University Press, 2014.
McKay, Sinclair. The Secret Lives of Codebreakers: The 
Men and Women Who Cracked the Enigma Code at 
Bletchley Park. Plume, 2012.
Nicolelis, Miguel A. and Ronald M. Cicurel. The 
Relativistic Brain: How it works and why it cannot 
be simulated by a Turing machine. CreateSpace 
Independent Publishing Platform, 2015.
Petzold, Charles. The Annotated Turing: A Guided Tour 
Through Alan Turing’s Historic Paper on Computability 
and the Turing Machine. Wiley, 2008.
Soare, Robert I. Turing Computability: Theory 
and Applications (Theory and Applications of 
Computability). Springer, 2016.

270
Turing test
Principles of Computer Science
tUrIng test
Fields oF study
Computer Science; Robotics
AbstrAct
The Turing test is a game proposed by computer 
scientist and mathematician Alan Turing in 1950 in 
order to determine whether machines can think. 
Despite criticism, the test has shaped the develop-
ment and study of artifi cial intelligence ever since.
PriniciPAl terms
 
 artifi cial intelligence: the intelligence exhibited by 
machines or computers, in contrast to human, or-
ganic, or animal intelligence.
 
 automaton: a machine that mimics a human but is 
generally considered to be unthinking.
 
 chatterbot: a computer program that mimics 
human conversation responses in order to interact 
with people through text; also called “talkbot,” 
“chatbot,” or simply “bot.”
 
 imitation game: Alan Turing’s name for his pro-
posed test, in which a machine would attempt to 
respond to questions in such a way as to fool a 
human judge into thinking it was human.
can machines think?
In 1950, British mathematician Alan Turing (1912–
54) wrote a paper titled “Computing Machinery 
and Intelligence” in which he asked, “Can machines 
think?” The question was too diffi cult to answer di-
rectly, so instead he thought up a simple game to 
compare a machine’s ability to respond in conversa-
tion to that of a human. If the machine could fool a 
human participant into believing it was human, then 
it could be considered functionally indistinguishable 
from a thinking entity (i.e., a person). This game 
later came to be known as the Turing test.
In Turing’s time, digital computers and automata 
already existed. However, the notion of developing 
machines with programming sophisticated enough 
to engender something like consciousness was still 
very new. What Turing envisioned would, within the 
decade, become the fi eld of artifi cial intelligence 
(AI): the quest for humanlike, or at least human-
equivalent, intelligence in a machine.
Person A
Machine
Person B
Is it a person or a machine?
I love blue
I love blue
The Turing test of artifi cial intelligence should allow person A to determine whether they are corre-
sponding via technology with a human or a machine entity. EBSCO illustration.

271
Principles of Computer Science
Turing test
The Imitation Game
To settle the question of whether or not machines can 
think, Turing proposed what he called an imitation 
game. He based it on a party game of the time, played 
by a man, a woman, and a third party (the judge). In 
the game, the judge stays in a separate room from the 
contestants and asks them questions, which they an-
swer via writing or an impartial intermediary. Based 
on their answers, the judge must determine which 
player is which. The man tries to fool the judge, while 
the woman attempts to help him or her.
In Turing’s game, instead of a man and a woman, 
the players would be a human and a machine. 
Computer terminals would be used to facilitate anon-
ymous text-based communication. If the machine 
could fool a human judge a sufficient percentage 
of the time, Turing argued, it would be functionally 
equivalent to a human in terms of conversational 
ability. Therefore, it must be considered intelligent.
Later versions of the game eliminated the need for 
a human to compete against the machine. Instead, 
the judge is presented with a text-based interface and 
told to ask it questions to determine if he or she is 
talking to a person or a machine. This version has 
been the basis of the Loebner Prize chatterbot com-
petition since 1995.
Natural Language Processing
Turing’s challenge spurred the development of a sub-
field of AI known as “natural language processing” 
(NLP), devoted to creating programs that can un-
derstand and produce humanlike speech. The 1966 
program ELIZA, written by German American com-
puter scientist Joseph Weizenbaum (1923–2008), was 
very convincing in its ability to imitate a Rogerian 
psychotherapist (“How does that make you feel?”). 
Programs like ELIZA are called “chatterbots,” or 
simply “bots.” These programs became a feature of 
early text-based Internet communication. With the 
advent of smartphones, both Apple and Google have 
developed advanced NLP to underpin the voice-
activated functions of their devices, most famously 
Apple’s AI assistant Siri.
Criticism of the Turing Test
The Turing test has been a point of contention among 
AI researchers and computer scientists for decades. 
Objections fall into two main camps. Some object to 
the idea that the Turing test is a reasonable test of the 
presence of intelligence, while others object to the 
idea that machines are capable of intelligence at all. 
With regard to the former, some critics have argued 
that a machine without intelligence could pass the 
test if it had sufficient processing power and memory 
to automatically respond to questions with prewritten 
answers. Others feel that the test is too difficult and 
too narrow a goal to help guide AI research.
Legacy of the Turing Test
Despite objections, the Turing test has endured as a 
concept in AI research and robotics. Many similar, 
though not identical, tests are also informally called 
Turing tests. In 2014, sixty years after Turing’s death, 
a program named Eugene Goostman made head-
lines for passing a version of the Turing test.
However, the Turing test’s lasting contribution to 
AI is not its utility as a test, but the context in which 
Turing proposed it. With his proposal, Turing be-
came the first major advocate for the idea that hu-
manlike machine intelligence was indeed possible. 
Though he died before AI emerged as a proper field 
of study, his ideas shaped its development for decades 
thereafter.
—Kenrick Vezina, MS
Bibliography
Ball, Phillip. “The Truth about the Turing Test.” 
Future. BBC, 24 July 2015. Web. 18 Dec. 2015.
Myers, Courtney Boyd, ed. The AI Report. Forbes. 
Forbes.com, 22 June 2009. Web. 18 Dec. 2015.
Olley, Allan. “Can Machines Think Yet? A Brief 
History of the Turing Test.” Bubble Chamber. U of 
Toronto’s Science Policy Working Group, 23 June 
2014. Web. 18 Dec. 2015.
Oppy, Graham, and David Dowe. “The Turing Test.” 
Stanford Encyclopedia of Philosophy (Spring 2011 
Edition). Ed. Edward N. Zalta. Stanford U, 26 Jan. 
2011. Web. 18 Dec. 2015.
Turing, Alan M. “Computing Machinery and 
Intelligence.” Mind 59.236 (1950): 433–60. Web. 
23 Dec. 2015.
“Turing Test.” Encyclopædia Britannica. Encyclopædia 
Britannica, 23 Sept. 2013. Web. 18 Dec. 2015.

272
Unicode
Fields of Study
Computer 
Science; 
Software 
Engineering; 
Information Technology
Abstract
Unicode is a character-encoding system used by 
computer systems worldwide. It contains numeric 
codes for more than 120,000 characters from 129 
languages. Unicode is designed for backward 
compatibility with older character-encoding stan-
dards, such as the American Standard Code for 
Information Interchange (ASCII). It is supported 
by most major web browsers, operating systems, and 
other software.
Prinicipal Terms

 glyph: a specific representation of a grapheme, 
such as the letter A rendered in a particular type-
face.

 grapheme: the smallest unit used by a writing 
system, such as alphabetic letters, punctuation 
marks, or Chinese characters.

 hexadecimal: a base-16 number system that uses 
the digits 0 through 9 and the letters A, B, C, D, E, 
and F as symbols to represent numbers.

 normalization: a process that ensures that dif-
ferent code points representing equivalent char-
acters will be recognized as equal when processing 
text.

 rendering: the process of selecting and displaying 
glyphs.

 script: a group of written signs, such as Latin or 
Chinese characters, used to represent textual in-
formation in a writing system.

 special character: a character such as a symbol, 
emoji, or control character.
Character-Encoding Systems
In order for computer systems to process text, the 
characters and other graphic symbols used in written 
languages must be converted to numbers that the 
computer can read. The process of converting these 
characters and symbols to numbers is called “char-
acter encoding.” As the use of computer systems in-
creased during the 1940s and 1950s, many different 
character encodings were developed.
To improve the ability of computer systems to inter-
operate, a standard encoding system was developed. 
Released in 1963 and revised in 1967, the American 
Standard Code for Information Interchange (ASCII) 
encoded ninety-five English language characters and 
thirty-three control characters into values ranging 
from 0 to 127. However, ASCII only provided sup-
port for the English language. Thus, there remained 
a need for a system that could encompass all of the 
world’s languages.
Unicode was developed to provide a character en-
coding system that could encompass all of the scripts 
used by current and historic written languages. By 
2016, Unicode provided character encoding for 129 
scripts and more than 120,000 characters. These in-
clude special characters, such as control characters, 
symbols, and emoji.
Understanding the Unicode Standard
The Unicode standard encodes graphemes and 
not glyphs. A grapheme is the smallest unit used 
by a writing system, such as an alphabetic letter or 
Chinese character. A glyph is specific representa-
tion of a grapheme, such as the letter A rendered 
in a particular typeface and font size. The Unicode 
standard provides a code point, or number, to rep-
resent each grapheme. However, Unicode leaves the 
rendering of the glyph that matches the grapheme to 
software programs. For example, the Unicode value 
U

273
Principles of Computer Science
Unicode
of U+0041 (which represents the grapheme for the 
letter A) might be provided to a web browser. The 
browser might then render the glyph of the letter A 
using the Times New Roman font.
Unicode defines 1,114,112 code points. Each code 
point is assigned a hexadecimal number ranging 
from 0 to 10FFFF. When written, these values are typi-
cally preceded by U+. For example, the letter J is as-
signed the hexadecimal number 004A and is written 
U+004A. The Unicode Consortium provides charts 
listing all defined graphemes and their associated 
code points. In order to allow organizations to define 
their own private characters without conflicting with 
assigned Unicode characters, ranges of code points 
are left undefined. One of these ranges includes all 
of the code points between U+E000 and U+F8FF. 
Organizations may assign undefined code points to 
their own private graphemes.
One inherent problem with Unicode is that cer-
tain graphemes have been assigned to multiple 
code points. In an ideal system, each grapheme 
would be assigned to a single code point to sim-
plify text processing. However, in order to en-
courage the adoption of the Unicode standard, 
character encodings such as ASCII were supported 
in Unicode. This resulted in certain graphemes 
being assigned to more than one code point in the 
Unicode standard.
0020
0021
0022
0023
0024
0025
0026
0027
0028
0029
002A
002B
002C
002D
002E
002F
0030
0031
0032
0033
0034
0035
0036
0037
0038
0039
003A
003B
003C
003D
003E
003F
0040
0041
0042
0043
0044
0045
0046
0047
0048
0049
004A
004B
004C
004D
004E
004F
0050
0051
0052
0053
0054
0055
0056
0057
0058
0059
005A
005B
005C
005D
005E
005F
0060
0061
0062
0063
0064
0065
0066
0067
0068
0069
006A
006B
006C
006D
006E
006F
0070
0071
0072
0073
0074
0075
0076
0077
0078
0079
007A
007B
007C
007D
007E
007F
00A0
00A1
00A2
00A3
00A4
00A5
00A6
00A7
00A8
00A9
00AA
00AB
00AC
00AD
00AE
00AF
00B0
00B1
00B2
00B3
00B4
00B5
00B6
00B7
00B8
00B9
00BA
00BB
00BC
00BD
00BE
00BF
00C0
00C1
00C2
00C3
00C4
00C5
00C6
00C7
00C8
00C9
00CA
00CB
00CC
00CD
00CE
00CF
00D0
00D1
00D2
00D3
00D4
00D5
00D6
00D7
00D8
00D9
00DA
00DB
00DC
00DD
00DE
00DF
00E0
00E1
00E2
00E3
00E4
00E5
00E6
00E7
00E8
00E9
00EA
00EB
00EC
00ED
00EE
00EF
00F0
00F1
00F2
00F3
00F4
00F5
00F6
00F7
00F8
00F9
00FA
00FB
00FC
00FD
00FE
00FF
>
Graphic character symbol
Hexadecimal character value
The Unicode Standard is a universally recognized coding system for more than 120,000 characters, using either 8-bit (UTF-8) or 16-bit 
(UTF-16) encoding. This chart shows the character symbol and the corresponding hexadecimal UTF-8 code. For the first 127 characters, 
UTF-8 and ASCII are identical. EBSCO illustration.

274
Unicode
Principles of Computer Science
Unicode also provides support for normaliza-
tion. Normalization ensures that different code 
points that represent equivalent characters will be 
recognized as equal when processing text. For ex-
ample, normalization ensures that the character 
é (U+00E9) and the combination of characters e 
(U+0065) and (U+0301) are treated as equivalent 
when processing text.
Sample Problem
Using a hexadecimal character chart as a ref-
erence, translate the following characters into 
their Unicode code point values: <, 9, ?, E, and 
@.
Then select an undefined code point to 
store a private grapheme.
Answer:
Unicode uses the hexadecimal character 
code preceded by a U+ to indicate that the 
hexadecimal value refers to a Unicode char-
acter. Using the chart, <, 9, ?, E, and @ are 
associated with the following hexadecimal 
values: 003C, 0039, 003F, 0045, and 0040. 
Their Unicode code point values are there-
fore: U+003C, U+0039, U+003F, U+0045, and 
U+0040.
A private grapheme may be assigned any 
code point value within the ranges U+E000 to 
U+F8FF, U+F0000 to U+FFFFF, and U+100000 
to U+10FFFD. These code points are left un-
defined by the Unicode standards.
Using Unicode to Connect Systems 
Worldwide
Since its introduction in 1991, Unicode has been 
widely adopted. Unicode is supported by major op-
erating systems and software companies including 
Microsoft and Apple. Unicode is also implemented 
on UNIX systems as well. Unicode has become an 
important encoding system for use on the Internet. 
It is widely supported by web browsers and other 
Internet-related technologies. While older systems 
such as ASCII are still used, Unicode’s support for 
multiple languages makes it the most important 
character-encoding system in use. New languages, 
pictographs, and symbols are added regularly. Thus, 
Unicode remains poised for significant growth in the 
decades to come.
—Maura Valentino, MSLIS
Bibliography
Berry, John D. Language Culture Type: International 
Type Design in the Age of Unicode. New York: Graphis, 
2002. Print.
Gillam, Richard. Unicode Demystified: A Practical 
Programmer’s Guide to the Encoding Standard. Boston: 
Addison-Wesley, 2002. Print.
Graham, Tony. Unicode: A Primer. Foster City: M&T, 
2000. Print.
Korpela, Jukka K. Unicode Explained. Sebastopol: 
O’Reilly Media, 2006. Print.
“The Unicode Standard: A Technical Introduction.” 
Unicode.org. Unicode, 25 June 2015. Web. 3 Mar. 
2016.
“Unicode 8.0.0.” Unicode.org. Unicode, 17 June 2015. 
Web. 3 Mar. 2016.
“What Is Unicode?” Unicode.org. Unicode, 1 Dec. 
2015. Web. 3 Mar. 2016.

275
Principles of Computer Science
UNIX
UNIX
Fields of Study
Operating Systems; Computer Science; Information 
Technology
Abstract
UNIX is a computer operating system originally de-
veloped by researchers at Bell Laboratories in 1969. 
The term is also used to refer to later operating sys-
tems based in part on its source code. Several of the 
original UNIX operating system’s features, such as 
its hierarchical file system and multiuser support, be-
came standard in later systems.
Prinicipal Terms

 command-line interpreter: an interface that inter-
prets and carries out commands entered by the 
user.

 hierarchical file system: a directory with a treelike 
structure in which files are organized.

 kernel: the central component of an operating 
system.

 multitasking: capable of carrying out multiple 
tasks at once.

 multiuser: capable of being used by multiple users 
at once.

 operating system (OS): a specialized program that 
manages a computer’s functions.
Origin of UNIX
As computer technology rapidly developed in the 
mid-twentieth century, programmers sought to 
create means of interfacing with computers and 
making use of their functions in a more straightfor-
ward, intuitive way. Chief among the goals of many 
programmers was the creation of an operating system 
(OS). OSs are specialized programs that manage all 
of computers’ processes and functions. Although 
many different OSs were created over the decades, 
UNIX proved to be one of the most influential. 
UNIX inspired numerous later OSs and continues to 
be used in various forms into the twenty-first century.
Development of the original UNIX OS began 
in 1969 at Bell Laboratories, a research facility 
then owned by AT&T. Researchers at Bell had 
been working on the Multiplexed Information and 
Computing Service (Multics) project in collabora-
tion with the Massachusetts Institute of Technology 
and General Electric. The group had focused on cre-
ating systems that allowed multiple users to access a 
computer at once. After AT&T left the project, Bell 
programmers Ken Thompson and Dennis Ritchie 
began work on an OS. Thompson coded the bulk of 
the system, which consisted of 4,200 lines of code, 
in the summer of 1969, running it on an outdated 
PDP-7 computer.
The OS was initially named the Unmultiplexed 
Information and Computing Service, a play on the 
name of the Multics project. That name was later 
shortened to UNIX. Thompson, Ritchie, and their 
colleagues continued to work on UNIX over the next 
several years. In 1973 they rewrote the OS in the new 
programming language C, created by Ritchie. UNIX 
gained popularity outside of Bell Laboratories in the 
mid-1970s. It subsequently inspired the creation of 
many UNIX variants and UNIX-like OSs.
Understanding UNIX
While different editions of UNIX vary, the majority 
of UNIX OSs have some common essential charac-
teristics. UNIX’s kernel is the core of the OS. The 
kernel is responsible for executing programs, allo-
cating memory, and otherwise running the system. 
The user interacts with what is known as the “shell.” 
The shell is an interface that transmits the user’s com-
mands to the kernel. The original UNIX shell was a 
command-line interpreter, a text-based interface into 
which the user types commands. Over time program-
mers developed a variety of different shells for UNIX 
OSs. Some of these shells were graphical user inter-
faces that enabled the user to operate the computer 
by interacting with icons and windows. Files saved to a 
computer running UNIX are stored in a hierarchical 
file system. This file system used a treelike structure 
that allowed folders to be saved within folders.
Using UNIX
In keeping with its origins as a project carried out by 
former Multics researchers, UNIX was designed to 
have multiuser capabilities. This enabled multiple 
people to use a single computer running UNIX at 
the same time. This was an especially important fea-
ture in the late 1960s and early 1970s. At this time, 

276
UNIX
Principles of Computer Science
computers were not personal computers but large, 
expensive mainframes that took up a significant 
amount of space and power. Multiuser capabilities 
made it possible for the organization using the UNIX 
OS to maximize the functions of their computers.
UNIX is also a multitasking system, meaning it can 
carry out multiple operations at once. One of the first 
programs designed for UNIX was a text-editing pro-
gram needed by the employees of Bell Laboratories. 
Over time, programmers wrote numerous programs 
compatible with the OS and its later variants, in-
cluding games, web browsers, and design software.
UNIX Variants and UNIX-Like Operating 
Systems
At the time that UNIX was first developed, AT&T was 
prohibited from selling products in fields other than 
telecommunications. The company was therefore 
unable to sell its researchers’ creation. Instead, the 
company licensed UNIX’s source code to various in-
stitutions. Programmers at those many institutions 
rewrote portions of the OS’s code, creating UNIX 
variants that suited their needs. Perhaps the most in-
fluential new form of UNIX was the Berkeley Software 
Distribution variant, developed at the University of 
California, Berkeley, in the late 1970s.
In the early twenty-first century, the multitude of 
UNIX variants and UNIX-derived OSs are divided 
into two main categories. Those that conform to 
standards established by an organization known as 
the Open Group, which holds the trademark to the 
UNIX name, may be referred to as “certified UNIX 
operating systems.” Systems that are similar to UNIX 
but do not adhere to the Open Group’s standards are 
typically known as “UNIX-like operating systems.” 
The latter category includes Apple’s OS X and the 
The creators of UNIX were Dennis Ritchie (standing) and Ken Thompson (sitting). By Peter Hamer, CC 
BY-SA 2.0 (http://creativecommons.org/licenses/by-sa/2.0), via Wikimedia Commons.

277
Principles of Computer Science
UNIX
free, open-source system Linux. The mobile OSs 
Android and Apple iOS also fall under this category 
and account for hundreds of millions of users, argu-
ably making UNIX-derived OSs the most widely used 
systems ever.
—Joy Crelin
Bibliography
Gancarz, Mike. The UNIX Philosophy. Woburn: 
Butterworth, 1995. Print.
“History and Timeline.” Open Group. Open Group, 
n.d. Web. 28 Feb. 2016.
Raymond, Eric S. “Origins and History of Unix, 
1969–1995.” The Art of UNIX Programming. Boston: 
Pearson Education, 2004. Print.
Stonebank, M. “UNIX Introduction.” University of 
Surrey. U of Surrey, 2000. Web. 28 Feb. 2016.
Toomey, Warren. “The Strange Birth and Long Life 
of Unix.” IEEE Spectrum. IEEE, 28 Nov. 2011. Web. 
28. Feb. 2016.
“What Is Unix?” Knowledge Base. Indiana U, 2015. 
Web. 28 Feb. 2016.
Worstall, Tim. “Is Unix Now the Most Successful 
Operating System of All Time?” Forbes. Forbes.
com, 7 May 2013. Web. 7 Mar. 2016.

278
Web design programming tools
Fields of Study
Information Technology; Graphic Design
Abstract
Web design refers to the visual design, layout, and 
coding of a website. It is a subset of the larger dis-
cipline of web development. Web designers are re-
sponsible for building effective websites that effec-
tively communicate ideas and provide users with the 
information they need. Web design programming 
tools help designers create attractive, well-organized 
websites.
Prinicipal Terms

 HTML editor: a computer program for editing 
web pages encoded in hypertext markup language 
(HTML).

 JavaScript: a flexible programming language that 
is commonly used in website design.

 jQuery: a free, open-source JavaScript library.

 screen-reading program: a computer program that 
converts text and graphics into a format accessible 
to visually impaired, blind, learning disabled, or 
illiterate users.
From Early HTML to Responsive Design
The Internet has come a long way from the time 
when black computer screens displayed plain text 
and a blinking cursor. Websites did not exist before 
1989, when English physicist Tim Berners-Lee began 
developing what would soon become the World Wide 
Web. In the process, he invented hypertext markup 
language (HTML), which is the code that describes 
the structure of a website. HTML was extremely 
simple at first, with fewer than twenty defined tags. 
However, it soon expanded as users’ needs grew.
While HTML is still the backbone of web design, 
other web markup languages have since been de-
veloped to supplement it. JavaScript, introduced in 
1995, resolved some of HTML’s limitations while in-
creasing interactivity between the site and the user. 
It became useful for game development, desktop ap-
plications, and animated and interactive web func-
tions. The development of JavaScript libraries such as 
jQuery eliminated the need for developers to create 
their own libraries, making coding much easier and 
faster.
Adobe Flash, previously known as Macromedia 
Flash from 1996 to 2005, also allowed designers to 
animate web graphics and improve user engage-
ment. However, Flash effects require a great deal of 
processing power and can take a long time to load, 
making the technology unsuitable for mobile devices 
and smartphones.
Cascading Style Sheets (CSS) was introduced in 
1998. CSS structures the design separately from the 
content, which is managed through HTML editors. 
This removed more limitations for designers, al-
lowing them have greater control over the appear-
ance of websites.
With the popularity of mobile devices came more 
challenges for web designers, from screen size to data 
load speed. Responsive web design solved these issues. 
Responsive design uses the same content across de-
vices, but with different layouts for each one. Usually, 
the width of the web browser window determines 
which layout is used. Other determining factors in-
clude whether or not JavaScript or certain HTML or 
CSS features are supported. JavaScript frameworks 
such as jQuery can be used to test for these features. 
Designers have also addressed the need for different 
layouts for each type of device by adopting flat de-
sign, a minimalist approach that simplifies visual 
W

279
Principles of Computer Science
Web design programming tools
elements and emphasizes the message itself. The lack 
of complex elements makes alternate layouts easier 
to design and allows them to load faster.
Why Web Design Matters
While it is important for websites to look appealing 
to users, web design includes more than just aes-
thetics. The designer’s role is to make a website as 
user-friendly as possible. The elements of good web 
design include:
• Navigation: how website information is located. 
Good navigation makes it easy for users to move 
through the site and find what they need.
• Organization: how information is pre-
sented. Ideally, information should be pre-
sented in order from most important to least 
important.
• Appearance. A site that looks appealing, with 
proper use of color, space, type, and images, 
builds trust and engagement.
Generate sitemap
Check competition
Usage analytics
Social media
optimization
Responsive check
Google SERP position
Get source code
Web design tools available through outside experts may provide the solutions needed to design a su-
perior website. Such tools include usage analytics, source-code generators, search engine optimiza-
tion, sitemap generators, competitive analysis, layout responsiveness, and social media optimization. 
EBSCO illustration.

280
Web design programming tools
Principles of Computer Science
Good web design creates an environment in which 
users are comfortable, can find what they need, and 
feel that their time was well spent.
Powerful Tools Make Website Design 
Easier
The Internet is where business is conducted, learning 
takes place, and connections are made. Businesses 
and organizations need to have a web presence so 
their customers can learn about their products and 
services. The development of web design program-
ming tools has made it possible for professional de-
signers to create attractive and user-friendly websites 
in much less time, and thus at a lower cost.
Tools such as website builders also allow amateurs 
with no knowledge of coding to successfully create 
and launch their own websites without having to learn 
HTML or CSS. As of 2016, popular cloud-based web-
site builders include Squarespace, Webflow, Weebly, 
Jimdo, WordPress, and Wix. Each of these tools uses 
HTML5, the fifth version of the HTML standard. 
Many feature a variety of predesigned, customizable 
templates. Templates simplify the entire process by 
allowing users to plug their content into a preexisting 
layout. Web designers have already completed both 
the visible design and the invisible coding that makes 
everything work. Using a website template may have 
some disadvantages, such as a limit on customization. 
However, the ease and cost savings of these tools and 
templates appeal to many consumers.
Making the Web More Accessible
Because the Internet is a primarily visual experi-
ence, it is important that web designers make web-
sites more accessible for people with impaired vision. 
Screen-reading programs translate on-screen text 
and output it to a text-to-speech system or a refresh-
able braille display. Text-to-speech programs can also 
improve accessibility for those who are illiterate or 
learning disabled.
In addition to textual content, screen-reading pro-
grams can convey visual elements as well. To facilitate 
this, web designers should label images and other 
decorative elements with “alt text”—alternative text 
that describes these elements for those who cannot 
see them. This text is also displayed on the screen if 
the element it describes fails to load.
The Future of Web Design
As web technology advances, web design tools will 
be developed to take advantage of new possibilities, 
as seen with wearable technology, cloud computing, 
and smartphone applications. Programming tools 
will use artificial intelligence to determine the pur-
pose of content and automatically alter the design for 
optimal results or for specific users’ needs. The way 
people interact with computers will continue to dras-
tically change, just as it has done from the beginning.
—Teresa E. Schmidt
Bibliography
Brownlee, John. “The History of Web Design 
Explained in 9 GIFs.” Co.Design. Fast Co., 5 Dec. 
2014. Web. 22 Feb. 2016.
“Choose a Website Builder: 14 Top Tools.” Creative 
Bloq. Future, 8 Feb. 2016. Web. 22 Feb. 2016.
“Designing for Screen Reader Compatibility.” 
WebAIM. Center for Persons with Disabilities, Utah 
State U, 19 Nov. 2014. Web. 22 Feb. 2016.
“HTML Tutorial.” Tutorials Point. Tutorials Point, 
2016. Web. 22 Feb. 2016.
Laszlo, Arp. “Why Is Website Design So Important?” 
Sunrise Pro Websites & SEO. Sunrise Pro Websites, 
22 Jan. 2016. Web. 22 Feb. 2016.
Luenendonk, Martin. “Top Programming Languages 
Used in Web Development.” Cleverism. Cleverism, 
21 June 2015. Web. 22 Feb. 2016.
“Pros & Cons of Website Templates.” Entheos. Entheos, 
n.d. Web. 22 Feb. 2016
Weller, Nathan. “A Look into the Future of Web 
Design: Where Will We Be in 20 Years?” Elegant 
Themes Blog. Elegant Themes, 9 May 2015. Web. 22 
Feb. 2016.

281
Principles of Computer Science
Web graphic design
Web graphic design
Fields of Study
Graphic Design; Digital Media; Programming 
Language
Abstract
Web graphic design is the use of graphic design tech-
niques in designing websites. Web graphic designers 
must balance the marketing aspects of a website with 
aesthetic design criteria. They also attempt to in-
crease the likelihood that the website will be found 
in search results and therefore be an effective adver-
tising tool.
Prinicipal Terms

 logotype: a company or brand name rendered in a 
unique, distinct style and font; also called a “word-
mark.”

 search engine optimization (SEO): techniques 
used to increase the likelihood that a website will 
appear among the top results in certain search en-
gines.

 tableless web design: the use of style sheets rather 
than HTML tables to control the layout of a web 
page.

 typography: the art and technique of arranging 
type to make language readable and appealing.

 wireframe: a schematic or blueprint that repre-
sents the visual layout of a web page, without any 
interactive elements.
Designing for the Web
Web graphic design is a subfield of graphic design 
that focuses on designing for the web. It typically 
involves a blend of graphic design techniques and 
computer programming. Many websites are used 
as marketing materials for businesses and organiza-
tions. Consequently, web designers often incorpo-
rate business logos and other promotional materials. 
They also use search engine optimization (SEO) 
techniques to increase the likelihood that the web 
page will be found by search engines.
Web Design Functionality
Most websites, whether business, advocacy, news, 
or personal sites, serve as both marketing and 
informational tools. A website that represents a spe-
cific brand will incorporate iconic logos, logotypes, 
or combination marks to aid brand recognition. An 
iconic logo is a symbol or emblem that represents 
a person, business, or organization. A logotype is a 
company or brand name rendered in a unique or 
proprietary font and style. Combination marks com-
bine icons with logotypes.
In addition to being informative, a website should 
be visually appealing and easy to understand. Skilled 
use of typography helps web designers achieve these 
goals. For example, while logotypes are designed 
to catch the eye, important information should be 
presented in a font that is aesthetically pleasing yet 
unobtrusive. A font that draws attention to itself will 
detract from the message of the text. Typography 
techniques such as this help web designers make web-
sites easy to read and navigate.
In addition, web designers must be familiar with 
SEO, which relies on elements such as keywords and 
links to ensure that users can find a website using a 
search engine. SEO techniques are most effective 
when incorporated into the overall design of a web-
site. As such, many web graphic designers also help 
users optimize their websites for better search results.
By the 2010s, web design had begun to focus on 
designing for the mobile web, creating websites and 
e-commerce sites that could be viewed and accessed 
using mobile devices. Many do-it-yourself (DIY) web-
site builders started offering mobile web design tem-
plates and conversions.
Development of Web Design Techniques
Modern web design began in the 1990s, with the 
creation and adoption of hypertext markup lan-
guage (HTML). This markup language specifies 
the location and appearance of objects displayed 
on a web page. In the mid-1990s, web designers 
began using HTML tables. These consist of static 
cells that can be arranged on a page to specify the 
location of text or objects. By placing tables within 
tables, designers could create a richer experience 
for websites.
The programming language JavaScript was de-
veloped in 1995 to code website behavior. This 
made individual web pages interactive for the first 
time. With JavaScript, web designers could embed 

282
Web graphic design
Principles of Computer Science
image galleries, add functions such as drop-down 
menus, and make websites respond visually when 
a user clicked on text or images. Flash, developed 
in 1996, allowed designers to add animated ele-
ments to websites. However, not all browsers sup-
ported Flash, so it was not as universally useful as 
JavaScript.
The next major step in web graphic design was 
the introduction of tableless web design. This ap-
proach to web design does not rely on HTML tables. 
Instead, it uses style sheets to format pages. The 
Cascading Style Sheets (CSS) language, fi rst intro-
duced in 1996, allows designers to separate the vi-
sual elements of the design from the content. CSS 
can determine the appearance and spacing, while 
individual elements are still described in HTML. 
Personal Home Page (PHP), another language used 
in web design, was developed in 1994 but gained in 
popularity during the late 1990s. It provides further 
options for making web pages interactive. When 
used with HTML, PHP allows websites to create new 
content based on user information and to collect in-
formation from visitors.
In the 2010s, most web design is still based on 
a combination of CSS and HTML. Programming 
in PHP and the database server software MySQL 
provides options for greater fl exibility and interac-
tivity. New versions of web design languages, such 
as CSS4 and HTML5, offer more ways of incorpo-
rating multimedia, better support for multilingual 
websites, and a wider array of aesthetic options for 
designers.
user-Generated Web design
The popular website WordPress is actually a con-
tent management system (CMS) built on PHP and 
FUNCTIONALITY
ACCESSIBILITY
VISUAL APPEAL
CONTENT
USABILITY
DESIGN
MESSAGE
A quality website design ensures that the message gets across. Good web design incorporates accessible con-
tent, provided in a visually appealing way, through strong website back-end functionality and front-end us-
ability. EBSCO illustration.

283
Principles of Computer Science
Web graphic design
MySQL. It debuted in 2003 as a blogging website, 
offering users templates and tools to design blogs. 
The site soon became popular as a basic web de-
sign system, allowing users with limited knowledge 
of HTML or other coding to build basic and func-
tional, if aesthetically rudimentary, websites. Soon 
web designers began tailoring WordPress sites for 
customers. This has essentially served as a shortcut 
for the web design industry. As of 2015, WordPress re-
mained among the most popular web design systems 
in the world, accounting for about half of all CMS-
based websites on the Internet.
The debut of WordPress launched a new era in 
DIY web graphic design. Other websites such as 
Squarespace, Wix, and Weebly soon began offering 
users the ability to design websites quickly and easily, 
without the need to understand programming lan-
guages. Most such sites provide users templates in the 
form of wireframes. Wireframes are structural layouts 
that specify the location of images, text, and interac-
tive elements on a web page but are not themselves 
interactive. Users can then insert their own images 
and text and, depending on the underlying program, 
also rearrange the layout of the wireframe. Like 
most elements of online commerce and marketing, 
web graphic design is moving toward user-generated 
and user-influenced design in order to open up to 
broader audiences.
—Micah L. Issitt
Bibliography
Allanwood, Gavin, and Peter Beare. User Experience 
Design: Creating Designs Users Really Love. New York: 
Fairchild, 2014. Print.
Cezzar, Juliette. “What Is Graphic Design?” AIGA. 
Amer. Inst. of Graphic Arts, 2016. Web. 16 Mar. 
2016.
Hagen, Rebecca, and Kim Golombisky. White Space Is 
Not Your Enemy: A Beginner’s Guide to Communicating 
Visually through Graphic, Web & Multimedia Design. 
2nd ed. Burlington: Focal, 2013. Print.
Malvik, Callie. “Graphic Design vs. Web Design: 
Which Career Is Right for You?” Rasmussen College. 
Rasmussen Coll., 25 July 2013. Web. 16 Mar. 2016.
Schmitt, Christopher. Designing Web & Mobile Graphics: 
Fundamental Concepts for Web and Interactive Projects. 
Berkeley: New Riders, 2013. Print.
Williams, Brad, David Damstra, and Hal Stern. 
Professional WordPress: Design and Development. 3rd 
ed. Indianapolis: Wiley, 2015. Print.

284
Windows operating system
Principles of Computer Science
Windows operating system
Fields of Study
Operating 
Systems; 
Information 
Technology; 
Computer Science
Abstract
Windows is an operating system (OS) developed by 
Microsoft. It has been one of the dominant OSs for 
personal computing since the 1990s. Windows is 
designed to operate across a variety of platforms, in-
cluding personal computers, smartphones, and the 
Xbox One video-gaming console.
Prinicipal Terms

 graphical user interface (GUI): a type of computer 
interface that uses graphic icons and a pointer to 
access system functions and files.

 language interface packs: programs that trans-
late interface elements such as menus and dialog 
boxes into different languages.

 multitasking: in computing, the ability to perform 
or plan multiple operations concurrently.

 multiuser: capable of being accessed or used by 
more than one user at once.

 operating system shell: a user interface used to 
access and control the functions of an operating 
system.

 platform: the underlying computing system on 
which applications can run, which may be the 
computer’s hardware or its operating system.
History of the Windows Operating 
System
Windows is an operating system (OS) developed 
by Microsoft Corporation for personal computers 
(PCs). Windows features a graphical user interface 
(GUI). A GUI allows users to interact with the OS 
using a pointer, dropdown menus, graphic icons, and 
movable windows representing folders and drives. 
Microsoft debuted Windows in the early 1980s. Since 
then, Windows has dominated the PC market. By 
some estimates, 90 percent of computers worldwide 
run some version of Windows.
The first PCs sold on the consumer market used a 
text-based disk operating system (DOS). Microsoft’s 
version of DOS, called MS-DOS, used a command-
line interface in which users typed text commands 
to activate functions. Microsoft debuted the first 
Win32 
Application
Win32 
Application
Win16 
Application
DOS 
Device Driver
DOS
Application
MS-DOS
MS-DOS
Kernel
User
GDI
DOS VM
Virtual Machine Manager
Installable File System Manager
VFAT
Input/Output Supervisor
SCSI Layer
Miniport 
Driver
Real Mode 
Mapper
Port 
Driver
CDFS
Configuration Manager
Registry
Enumerator
Arbitrator
Device 
Driver
System VM
Network
Redirector
Bus Driver
Architecture of Windows 95 as depicted by Ruud Koot. The development of Windows 95 improved upon MS-DOS by incorporating a 32-bit 
multitasking architecture. By Ruud Koot, CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0), via Wikimedia Commons

285
Principles of Computer Science
Windows operating system
version of Windows in 1985. Windows 1.0 was an op-
erating system shell that could be installed over the 
MS-DOS system. Windows featured one of the first 
GUIs. Windows was based on a model called WIMP 
(windows, icons, menus, pointer objects). GUIs allow 
users to navigate a virtual desktop with a mouse that 
controls a pointer icon. GUIs also enable users to 
click on text and graphic icons to activate programs. 
For instance, in Windows 1.0, clicking on a folder 
icon activated the underlying DOS command “dir” to 
display the contents of a directory.
A new version, Windows 95, introduced the 
company’s Internet Explorer browser. Windows 95 
also introduced the Start button and taskbar. The 
Windows 95 kernel (basic underlying programming) 
remained the standard for additional versions of 
Windows until 2001, with the release of Windows XP. 
Windows 7, released in 2009, was one of the compa-
ny’s most popular versions of the interface. Windows 
7 featured a new, more intuitive layout. Windows 8 
introduced a completely redesigned interface with 
tiles that are easier for users with touch-screen tab-
lets. In 2015, Microsoft debuted Windows 10, the first 
version of their OS to be offered as a free upgrade. 
Windows 10 also introduced a new service model in 
which the OS would continually receive updates to 
various features and functions. This model is similar 
to that employed by Microsoft’s competitor Apple. 
Windows 10 was also the first version of Windows that 
had a universal application architecture. This means 
apps on Windows 10 can be used on smartphones, 
tablets, and the Xbox One gaming system as well.
Features of Windows
In computing, a platform can be defined as the com-
puter system that a program uses. A platform can 
be the hardware architecture or an OS. The earliest 
versions of the Windows OS were cross-platform and 
could be used with different types of architecture. 
In 2016, the Windows OS is one of the primary plat-
forms for software. The Windows OS is designed to 
be compatible with the X86-64 computer architec-
ture, a 64-bit processor system created by AMD in 
2000. Windows 10 can also be implemented on older 
x86 (32-bit) computers.
Windows OS is designed to be a multiuser environ-
ment in which more than one users can use the same 
OS. The Remote Desktop Connection system allows 
multiple users to use Windows at once. The Windows 
OS also enables users to adjust OS settings for mul-
tiple accounts, essentially creating multiple versions 
of the OS on the same computer. It is also a multi-
tasking system in which more than one processing 
job can run at a time. The Windows 10 system intro-
duced a new automated assistant called Cortana in an 
effort to compete with the popularity of Apple’s Siri.
Dominance and Reinvention
More than 90 percent of PCs worldwide ran some ver-
sion of Windows in 2015. The majority (52 percent) 
run Windows 7, which, as of 2016, was the compa-
ny’s most popular product. The release of Windows 
10 in 2015 had little impact, accounting for 12 per-
cent of the market in February 2016. Windows is sold 
around the world and comes formatted for a variety 
of languages. In addition, language interface packs 
are available for free download and offer support for 
languages not found in full versions. Each pack re-
quires a base language that can be activated within 
the OS after installation.
Given the rising popularity of handheld com-
puting devices, such as tablets and smartphones, the 
global computer market is no longer based solely on 
the PC market. While Microsoft continues to domi-
nate the PC market, the company controls only 14 
percent of the global market across all computing 
devices. Looking at smartphone OSs specifically, 
Windows controls less than 3 percent of that market, 
falling far behind Android and Apple iOS. As a re-
sult, with the release of Windows 10, Microsoft has 
made changes to its basic strategy. The company has 
reduced its focus on proprietary software. Instead it is 
increasingly embracing the potential of open-source 
software. Future versions of Windows will likely show 
an increased focus on cloud-based computing, intui-
tive touch-screen technology, and alternative inter-
face controls such as voice activation and multitouch 
gestures.
—Micah L. Issitt
Bibliography
Bishop, Todd. “Microsoft Exec Admits New Reality: 
Market Share No Longer 90%— It’s 14%.” 
Geekwire. GeekWire, 14 July 2014. Web. 30 Jan. 
2015.

286
Wireframes
Principles of Computer Science
Gibbs, Samuel. “From Windows 1 to Windows 10: 29 
Years of Windows Evolution.” Guardian. Guardian 
News and Media, 2 Oct. 2014. Web. 2 Jan. 2016.
“A History of Windows.” Microsoft. Microsoft, 2016. 
Web. 2 Jan. 2016.
McLellan, Charles. “The History of Windows: A 
Timeline.” ZDNet. CBS Interactive, 14 Apr. 2014. 
Web. 15 Feb. 2016.
Protalinski, Emil. “Windows 10 Ends 2015 under 10% 
Market Share.” VentureBeat. VentureBeat, 1 Jan. 
2016. Web. 26 Feb. 2016.
Warren, Tom. “Windows Turns 30: A Visual History.” 
Verge. Vox Media, 19 Nov. 2015. Web. 26 Feb. 2016.
Wireframes
Fields of Study
Computer Science; Graphic Design; Digital Media
Abstract
A wireframe visually represents the basic elements 
of a proposed website before a web designer actually 
builds it. Creating a wireframe ensures that both the 
designer and the client can agree on the basic layout 
and functionality of the site. It also allows them to 
map out how the end user will ultimately interact 
with the site.
Prinicipal Terms

 high-fidelity wireframe: an image or series of im-
ages that represents the visual elements of the web-
site in great detail, as close to the final product as 
possible, but still does not permit user interaction.

 information hierarchy: the relative importance of 
the information presented on a web page.

 interaction design: the practice of designing a user 
interface, such as a website, with a focus on how to 
facilitate the user’s experience.

 portlet: an independently developed software 
component that can be plugged into a website to 
generate and link to external content.

 prototype: a simplified, but visually very similar, 
version of the final website that simulates how the 
user will interact with the site.

 typography: the style, arrangement, and overall 
appearance of text on a website.

 widget: an independently developed segment of 
code that can be plugged into a website to display 
information retrieved from other sites.
What Is a Wireframe?
A wireframe is a visual, noninteractive representation 
of a proposed website. A web designer may use a wire-
frame to determine how best to lay out the elements 
of the site. If the site is being designed for a client, 
the designer may show the client a wireframe to get 
approval for the final product.
A website is essentially a sophisticated informa-
tion storage bank. Ultimately, it is designed for the 
end users, who will need easy, efficient access to the 
information it contains. To that end, a web designer 
will block out the basic elements of the proposed 
website first, most often drawing them on paper. 
This drawing is the wireframe. Though it may be 
no more than a set of squares, circles, columns, and 
lines that represent the individual elements of the 
pages, it allows the designer and the client to re-
view the site concept together before it is built. Any 
changes in the website format can be easily made at 
this stage.
A wireframe also allows the designer to sort the 
content of the proposed website into an information 
hierarchy. The designer can then determine how 
best to arrange it on the page. Part of web design is 
knowing what information users expect to find and 
where they are most likely to look for it. Eye-tracking 
studies have determined that upon accessing a web-
site, a user will first look at the top-left corner of the 
page, scan across the top, then look down the left side 
and scan partway across the screen. This essentially 
forms an F-shape. Thus, crucial information should 
be concentrated mainly in the upper left portion of 
the page. Such insights into how users interact with 
and experience websites are an important part of the 
design field known as interaction design.

287
Principles of Computer Science
Wireframes
1. Header
1.1 Brand
1.2 Global Nav
1.3 Tools
2.1 Hero Feature
2.2 Hero Call
to Action
3. Ad
2. Hero
5.1 Sidebar
Feature
4.1 Article
Feature
4.2 Article Listing
4. Body
5. Sidebar
6. Footer
5. Sidebar
1. Header
1.1 Brand
1.2 Global Nav
1.3 Tools
2.1 Hero Feature
2.2 Hero Call to Action
3. Ad
5.1 Sidebar Feature
4.1 Article Feature
4.2 Article Listing
2. Hero
4. Body
6. Footer
2.2 Hero Call to Action
2. Hero
3. Ad
4.1 Article Feature
4.2 Article Listing
4. Body
6. Footer
1. Header
1.1 Brand
1.2 Global Nav
1.3 Tools
Bootstrap Thumb Smartphone
Bootstrap Thumb Portrait
Bootstrap Thumb Default
Wireframes of a website in three different screen layouts show how the same content can be presented in different ways to 
best fit the device. These high-fidelity wireframes indicate what type of content is presented in which locations as a first step 
in designing for responsiveness. EBSCO illustration.

288
Wireframes
Principles of Computer Science
Wireframing in Web Design
When designing a website, particularly for a business 
or other organization, certain elements are standard. 
These elements may include:
The client’s logo, to identify the business or orga-
nization in a recognizable way
A navigation bar, to provide access to the different 
pages of the site
A front-page image or slide show of images
A search box
A registration or login box, if the site allows (or 
requires) users to register
A header containing an index of the major ele-
ments of the website
A footer containing contact information, privacy 
guidelines, and other legal information
Many websites also include external content such 
as news headlines, weather reports, or social media 
feeds. In most cases, the creator of this external con-
tent generates a portlet or widget that designers can 
simply plug into the code of the site. While portlets 
and widgets work in similar ways, they differ mainly in 
their execution. A portlet is executed on the server, 
so that the code it generates is not visible to the end 
user. A widget is executed in the browser.
A wireframe should incorporate any and all of ele-
ments that will be included in the final website. In most 
cases, the wireframe serves as a blueprint, with no dis-
tracting aesthetic features such as font, color, or images. 
The emphasis is on functionality—that is, how the pro-
posed site will accommodate the end user. This type of 
wireframe is also called a “low-fidelity wireframe.”
Once the client is onboard with the concept, the 
designer may produce a more sophisticated, high-
fidelity wireframe. This type of wireframe is usually 
created using dedicated software. It represents the 
final website in much more detail. It incorporates 
such design elements as images, color schemes, and 
typography. Sequences of images are used to repre-
sent how users will interact with the site. However, the 
wireframe itself is still noninteractive.
Sometimes, between the high-fidelity wireframe 
and the final product, a designer will produce an 
interactive prototype. While a prototype will usually 
look very similar to the final product, its appearance 
matters less than its functionality. A prototype should 
simulate the user experience with the completed 
website. Prototypes are ideal for early user testing. 
Any problems with the interface can be addressed 
before the designer begins coding for the final site.
Impact of Wireframes
Much as an architect creates a blueprint for a skyscraper 
or a director storyboards a film, a web designer uses a 
wireframe to spot potential problems before building 
the final product. This saves both the designer’s time 
and the client’s money and, ultimately, shields the end 
user from frustration—all critical factors in the highly 
competitive world of Internet marketing.
—Joseph Dewey
Bibliography
Greenberg, Saul, et al. Sketching User Experiences: The 
Workbook. Waltham: Morgan, 2012. Print. 4 Science 
Reference Center™ Wireframes
Hamm, Matthew J. Wireframing Essentials: An 
Introduction to User Experience Design. Birmingham: 
Packt, 2014. Print.
Klimczak, Erik. Design for Software: A Playbook for 
Developers. Hoboken: Wiley, 2013. Print.
Krug, Steve. Don’t Make Me Think, Revisited: A Common 
Sense Approach to Web Usability. 3rd ed. Berkeley: 
New Riders, 2014. Print.
Marsh, Joel. UX for Beginners: A Crash Course in 100 
Short Lessons. Sebastopol: O’Reilly, 2016. Print.
Nielsen, Jakob. “F-Shaped Pattern for Reading Web 
Content.” Nielsen Norman Group. Nielsen Norman 
Group, 17 Apr. 2006. Web. 4 Mar. 2016.
Norman, Don. The Design of Everyday Things. Rev. and 
expanded ed. New York: Basic, 2013. Print.
Treder, Marcin. “Wireframes vs. Prototypes: What’s 
the Difference?” Six Revisions. Jacob Gube, 11 Apr. 
2014. Web. 4 Mar. 2016.

289
Principles of Computer Science
Wireless networks
Wireless networks
Fields of Study
Information Technology; Network Design
Abstract
Wireless networks allow computers and other Internet-
enabled devices to connect to the Internet without 
being wired directly to a modem or router. They 
transmit signals across radio waves instead. Wireless net-
works make communication services available almost 
anywhere, without the need for wired connections.
Prinicipal Terms

 electromagnetic spectrum: the complete range 
of electromagnetic radiation, from the longest 
wavelength and lowest frequency (radio waves) 
to the shortest wavelength and highest frequency 
(gamma rays).

 local area network (LAN): a network that con-
nects electronic devices within a limited physical 
area.

 microwaves: electromagnetic radiation with a fre-
quency higher than that of radio wave but lower 
than that of visible light.

 node: a point at which a communication network 
connects to another network, is redistributed, or 
terminates at a user interface.

 personal area network (PAN): a network gener-
ated for personal use that allows several devices 
to connect to one another and, in some cases, to 
other networks or to the Internet.

 radio waves: low-frequency electromagnetic ra-
diation, commonly used for communication and 
navigation.
How Wireless Networks Work
Computer networks allow multiple computers to ac-
cess information stored in other locations and to use 
common devices, such as printers and file servers. 
Setting up a traditional computer network requires 
cables, distributors, routers, and internal and ex-
ternal network cards.
As computers became smaller and therefore more 
mobile, cables and wires presented challenges. The 
development of wireless connectivity made it more 
convenient to use laptops, tablets, and other mobile 
devices in more places. Wireless networks provide 
the same connectivity and access as traditional net-
works without the need for physical cables.
Most wireless networks transmit signals using radio 
waves, the lowest-frequency waves in the electromag-
netic spectrum. A computer sends data across a wire-
less network through nodes, which are all the hubs, 
switches, and devices connected to the network. The 
data are translated into a radio signal and transmitted 
to the wireless router. The router then sends the in-
formation to the Internet through an Ethernet cable. 
Multiple devices can access the same wireless router 
to connect to the Internet.
Types of Wireless Networks
Broadly speaking, the different types of wireless net-
works can be divided into four main categories, based 
on range and how they transmit information.
A personal area network (PAN) covers the shortest 
range. PANs are typically used by one person to 
transmit information between two or more devices. 
The network is generated by one device, such as a cell 
phone. Any device within range of that device can 
then communicate with it, or with any other device 
within its range. The range of a wireless PAN (WPAN) 
is generally no more than a few meters.
The next-largest type of network is a local area 
network (LAN). The term was initially used to refer 
to devices within a limited area, such as a single 
building, that are connected via cables to a cen-
tral access point, such as a modem. A wireless LAN 
(WLAN) works much the same way. Instead of 
being connected to the individual devices by cables, 
however, the access point connects to a router that 
generates a wireless network. This network is omni-
directional, meaning that it is equally strong in all 
directions. As a result, the signal is weaker, and thus 
has a shorter range, than if it were focused in one 
direction.
A wireless wide area network (WWAN) can cover 
a range of several miles. To accomplish this, WWANs 
use directional antennas and typically transmit signals 
via microwaves. Microwaves have a higher frequency 
than radio waves, although there is some overlap. 
While microwaves can travel farther, they cannot pen-
etrate obstacles as well as radio waves. Thus, to use 
the network, a device must have a direct line of sight 

290
Wireless networks
Principles of Computer Science
Client 1
Wired
Internet
Client 2
Client 3
Client 7
Client 6
Client 4
Client 5
Router
Router
MN1
MN2
MN3
MN6
MN5
MN7
MN4
Wired connections
High-speed wireless connections
Low-speed wireless connections
Wireless networks allow multiple clients or terminals to connect with other terminals via wireless pathways. Depending on the type 
of router, the connection may be high speed or low speed. EBSCO illustration.

291
Principles of Computer Science
Wireless networks
to the antenna or be connected to another node that 
does. A WWAN may be either point-to-point (P2P) or 
point-to-multipoint (P2MP). P2P, the simplest type, 
connects one node to another. P2MP connects mul-
tiple nodes to one central access point.
Finally, a mesh network is a wireless network in 
which all devices connect to each other and relay sig-
nals from device to device, without a wired infrastruc-
ture. In a mesh network, all nodes—that is, all devices 
connected to the network—transmit signals.
Pros and Cons of Wireless
WLANs gained popularity over regular LANs because 
of their convenience and cost savings. In fact, wireless 
networks became so prolific that Apple phased out 
Ethernet connection ports on its MacBook laptops. 
By investing in wireless networking hardware, busi-
nesses can avoid the trouble and expense of installing 
cables throughout a building. They can quickly ex-
pand the network when needed. Consumers can 
easily set up home networks for use with laptops, tab-
lets, and smartphones. Users can be more productive 
by accessing the Internet wherever they are.
However, wireless networks have drawbacks as 
well. Their smaller range makes additional equip-
ment, such as repeaters, necessary for larger build-
ings. In addition, radio waves are prone to interfer-
ence, making reliability an issue. Security is also a 
concern. Since data are transmitted through the air, 
wireless networks involve greater risk.
A Big Impact from Invisible Radio Waves
Wireless networks have allowed more people around 
the world to access the Internet from more locations 
than ever. In business, wireless networks enable com-
panies to operate with more speed, flexibility, and 
connection. In health care, they allow physicians to 
consult with faraway specialists or to check in with pa-
tients in rural areas. For individuals, wireless networks 
bring friends and families closer, enabling them to 
keep in more frequent contact and allow users to 
stream entertainment or download books while on 
the go. News is delivered as it happens. People im-
mediately share their reactions, while families and 
friends can check on those affected.
Without wireless technology, there would be no 
smartphones and no texting, and there would be less 
access to social media. Social media enables people 
all over the world to express themselves, whether 
for personal or political reasons. From young 
voters in the United States to the political uprisings 
throughout the Middle East during the Arab Spring 
of 2011, social media has become an important force 
for political awareness and connection over shared 
interests.
The Evolution of Wireless Networks
Businesses are increasingly moving to cloud-based 
networks, which eliminate on-site hardware and 
move all infrastructure, administration, and pro-
cesses to remote servers. Communities are creating 
mesh networks with their own infrastructure to 
share resources and connect through their mobile 
devices. Such mesh networks provide more reliable 
communication and connection during emergencies 
or natural disasters. At home, consumers are taking 
advantage of new technology to make wireless con-
nection to the Internet easier and more convenient. 
It is clear that the proliferation of wireless networks 
has changed the way the world learns, communi-
cates, and conducts business. As the cost of wireless 
technology decreases, the benefits will spread even 
further.
—Teresa E. Schmidt
Bibliography
“Computer—Networking.” Tutorials Point. Tutorials 
Point, 2016. Web. 22 Feb. 2016.
“DCN—Computer Network Types.” Tutorials Point. 
Tutorials Point, 2016. Web. 22 Feb. 2016.
De Filippi, Primavera. “It’s Time to Take Mesh 
Networks Seriously (and Not Just for the Reasons 
You Think).” Wired. Condé Nast, 2 Jan. 2014. Web. 
22 Feb. 2016.
Lander, Steve. “Disadvantages or Problems for 
Implementing Wi-Fi Technology.” Small Business—
Chron.com. Hearst Newspapers, n.d. Web. 22 Feb. 
2016.
Smith, Matt. “Wi-Fi vs. Ethernet: Has Wireless Killed 
Wired?” Digital Trends. Designtechnica, 18 Jan. 
2013. Web. 22 Feb. 2016.
“Types of Wireless Networks.” Commotion. Open 
Technology Inst., n.d. Web. 3 Mar. 2016.
“Wireless 
History 
Timeline.” 
Wireless 
History 
Foundation. Wireless Hist. Foundation, 2016. Web. 
22 Feb. 2016.

292
Workplace monitoring
Principles of Computer Science
Workplace monitoring
Fields of Study
Information Technology
Abstract
Modern technology makes it possible for firms to 
monitor many aspects of the workplace, particu-
larly employee activities and performance. Most 
monitoring systems used are electronic, including 
computer terminals, e-mail, Internet, telephone 
and smartphone systems, GPS, drones, and others. 
Challenges to workplace monitoring appear in the 
arena of employee rights and privacy legislation.
Prinicipal Terms

 packet sniffers: a program that can intercept data 
or information as it moves through a network.

 real-time monitoring: a process that grants admin-
istrators access to metrics and usage data about a 
software program or database in real time.

 remote monitoring: a platform that reviews the 
activities on software or systems that are located 
off-site.

 silent monitoring: listening to the exchanges be-
tween an incoming caller and an employee, such 
as a customer service agent.

 transparent monitoring: a system that enables em-
ployees to see everything that the managers moni-
toring them can see.
Monitoring Technologies
The practice of monitoring employees at work is 
known as “workplace monitoring.” According to 
the US Office of Technology Assessment, work-
place monitoring is the collection, storage, anal-
ysis, and reporting of information about workers’ 
activities.
The introduction of computers and other tele-
communication technologies in the workplace 
brought great changes to monitoring practices. 
Workplace monitoring includes a wide variety of 
technologies, such as hidden or overt video cam-
eras, global positioning systems (GPS), landline 
telephones, cell phones and smartphones, com-
puter software and systems, the Internet, and 
drones. Many techniques have been developed 
to improve workplace monitoring. For example, 
packet sniffers are computer programs that can 
analyze communication flow across networks and 
intercept malicious files. Many businesses monitor 
all network traffic passing through their servers. 
Automatic programs can alert managers if a net-
worked computer connects to a malicious domain, 
block particular websites, and scan e-mails for 
spam.
Many organizations monitor their workers in 
order to protect their property and information, pro-
tect employees, and measure the quality of their work 
and productivity. However, employers must also be 
mindful of legal and ethical considerations of work-
place monitoring. Employers must work to maintain 
a sense of trust in their employees when engaging in 
monitoring. Monitoring practices can generate nega-
tive feeling among employees and create legal prob-
lems for the employer if they are not implemented 
carefully.
Further complicating the issue is the increasingly 
fluid nature of workplace boundaries. For instance, 
work is often spread across geographical spaces. 
Many employers have a large number of employees 
who work away from headquarters, either because 
they travel or work remotely. This has led to remote 
monitoring by way of varied technologies and de-
vices, including GPS and drones.
Moreover, a growing amount of work is comput-
erized, through intranet and Internet networks. 
As a result, employers face real risks of employee 
misuse of these systems. However, these same systems 
have led to improvements in workplace monitoring 
technology.
Benefits of Workplace Monitoring
Because of the growing instances of hacking and 
information theft, employers have legitimate con-
cerns about the security of workplace information. 
They may also want to conduct quality controls and 
monitor employee productivity. For example, it is 
a common practice for call centers to record cus-
tomer service calls, in a system commonly known as 
silent monitoring. Another type of system, real-time 
monitoring, allows managers to analyze usage on 
computer systems as it occurs. Real-time monitoring 

293
Principles of Computer Science
Workplace monitoring
helps track not only employee activity but also other 
firm processes, such as sales data and other trends. It 
is often used in conjunction with remote monitoring. 
Remote monitoring involves tracking the activity of 
employees who work off-site.
Hundreds of software programs and other technol-
ogies are available to monitor the workplace. Some 
of these programs are free. Companies can establish 
their own monitoring systems. Others prefer to hire 
firms that provide the technology and analyze the 
data. The data retrieved by monitoring technologies 
may be used to identify information leaks or theft, 
evaluate employee performance, detect malware, or 
analyze business trends. Among the latest monitoring 
technologies are drones. Drones are mainly used in 
real estate, construction, and agriculture.
There are many ways in which electronic work-
place monitoring is used. According to the American 
Management Association, 78 percent of major com-
panies in the United States reported monitoring 
employee use of e-mail, Internet, or phone in 2015. 
Some employers also use monitoring technology in 
order to measure employee performance by mea-
suring time spent at the computer or keystroke speed.
Legal and Ethical Considerations
The increasing breadth of workplace monitoring has 
raised ethical and legal concerns. On the balance are 
the privacy rights of workers. Electronic workplace 
monitoring has become so common that labor rights 
advocates have raised concerns about the abuse or 
inappropriate use of monitoring practices. On the 
other hand, employers are concerned about liability 
costs and legal consequences.
In the opinion of some experts, electronic work-
place monitoring is subject to insufficient govern-
ment regulation. This may be due to its constant in-
novations and its relatively new status. Technically, 
employers are legally allowed to listen to, watch over, 
record, and read all work-related forms of communi-
cation. However, federal law stipulates that personal 
calls cannot be monitored.
Experts recommend companies ensure their mon-
itoring practices are fair and consistent. It is crucial 
• Prevent Data Leakage
• Track Source of Leakages
• Avoid Cyber Threats
• Block Web Distractions
• Determine Employee Productivity
• Stays Effective When Offline
• Track Document Usage
• Track Device Usage
• Record Website Browsing
• Record Chat and Instant Message Usage
• Monitor Application Activity
• Block Incoming and Outgoing Email
• Record Screenshots
• Monitor Network and Bandwidth Usage
• Record File Printing Data
USE COMPUTER MONITORING SOFTWARE?
WHY
CAN PROGRAMS DO?
WHAT
Workplace monitoring software is designed to monitor employees’ computer use in order to protect against external threats, limit 
distractions, prevent and track information leaks, and support productivity online and offline. These objectives can be accom-
plished through a number of possible software configurations. EBSCO illustration.

294
Workplace monitoring
Principles of Computer Science
to implement measures to prevent managers from 
engaging in abuse of power and other illegal activi-
ties using monitoring technologies. Staff in charge of 
workplace monitoring must be adequately trained. 
Some organizations implement transparent moni-
toring in order to make employees feel more at ease 
with the process. Transparent monitoring welcomes 
the participation of employees in the process. Finally, 
when employers suspect employees of engaging in 
criminal action, experts recommend they alert the 
authorities rather than try to set up a sting operation 
on their own.
Debate continues about what employers should 
be allowed to monitor and to what extent employees 
have the right to know they are being monitored. 
Nevertheless, employees should always assume that 
they are being monitored and keep all private or 
personal communication in separate accounts and 
devices.
—Trudy Mercadal, PhD
Bibliography
Alton, Larry. “Email Security in 2016: What You Need 
to Know.” Inc. Mansueto Ventures, 18 Feb. 2016. 
21 Feb. 2016.
Plumb, Charles. “Drones in the Workplace.” 
EmployerLINC. McAfee and Taft, 14 Dec. 2015. 
Web. 21 Jan. 2016.
Smith, Eric N. Workplace Security Essentials: A Guide for 
Helping Organizations Create Safe Work Environments. 
Oxford: Butterworth-Heinemann, 2014. Print.
Stanton, Jeffrey, and Kathryn R. Slam. The Visible 
Employee: Using Workplace Monitoring and Surveillance 
to Protect Information Assets without Compromising 
Employee Privacy or Trust. Medford: Information 
Today, 2006. Print.
Tabak, Filiz, and William Smith. “Privacy and 
Electronic Monitoring in the Workplace: A Model 
of Managerial Cognition and Relational Trust 
Development.” Employee Responsibilities and Rights 
Journal 17.3 (2005): 173–89. Print.
Yakowitz, Will. “When Monitoring Your Employees 
Goes Horribly Wrong.” Inc. Mansueto Ventures, 14 
Feb. 2016. Web. 21 Feb. 2016.
Yerby, Jonathan. “Legal and Ethical Issues of 
Employee Monitoring.” Online Journal of Applied 
Knowledge Management 1.2 (2013): 44–54. Web. 21 
Jan. 2016.

295
circa 2400 bce
The abacus – the first known calculator, was probably 
invented by the Babylonians as an aid to simple arith-
metic around this time period.
circa 1115 bce
The south-pointing chariot was invented in ancient 
China. It was the first known geared mechanism to 
use a differential gear. 
circa 500 bce
First known use of 0 (by mathematicians in ancient 
India around this date.
circa 500 bce
Indian grammarian Pan.ini formulated the grammar 
of Sanskrit (in 3959 rules) known as the Ashtadhyayi. 
His grammar had the computing power equivalent 
to a Turing machine. The Panini-Backus form used 
to describe most modern programming languages is 
also significantly similar to Pan.ini’s grammar rules.
circa 300 bce
Indian mathematician/scholar/musician Pingala 
first described the binary number system which is 
now used in the design of essentially all modern com-
puting equipment.
circa 200 bce
The Chinese invented the suanpan (Chinese abacus) 
which was widely used until the invention of the 
modern calculator, and continues to be used in some 
cultures today.
circa 125 bce
The Antikythera mechanism: A clockwork, analog 
computer believed to have been designed and built 
in the Corinthian colony of Syracuse. The mecha-
nism contained a differential gear and was capable 
of tracking the relative positions of all then-known 
heavenly bodies. 
circa 100 bce
Chinese mathematicians first used negative numbers.
circa 60 bce
Heron of Alexandria made numerous inventions, in-
cluding “Sequence Control.” This was, essentially, the 
first computer program. He also made numerous in-
novations in the field of automata, which are impor-
tant steps in the development of robotics. 
circa 200
Jaina mathematicians invented logarithms.
circa 600
Indian mathematician Brahmagupta was the first to 
describe the modern place-value numeral system.
724
Chinese inventor Liang Lingzan built the world’s first 
fully mechanical clock; the earliest true computers, 
made a thousand years later, used technology based 
on that of clocks. 
820
Persian mathematician, Muh. ammad ibn Musa al-
Khwarizmı, described the rudiments of modern  
algebra. The word “algorithm” is derived from al-Kh-
warizmi’s Latinized name “Algoritmi”.
circa 850
Arab mathematician, Al-Kindi (Alkindus gave the 
first known recorded explanation of cryptanalysis 
in “A Manuscript on Deciphering Cryptographic 
Messages”. In particular, he is credited with devel-
oping the frequency analysis method whereby varia-
tions in the frequency of the occurrence of letters 
could be analyzed and exploited to break encryption 
ciphers.
850
The Banu– Mu–sa– brothers, in their “Book of Ingenious 
Devices”, invented an organ which played inter-
changeable cylinders automatically. They also in-
vented an automatic flute player which appears to 
have been the first programmable machine.
History of Events Leading up to the Development  
of Modern Computers

296
History of Events Leading up to the Development of Modern Computers 
Principles of Computer Science
996
Persian astronomer, Abu Rayhan al-Bırunı, invented 
the first geared mechanical astrolabe, featuring eight 
gear-wheels. 
circa 1000
Abu Rayhan al-Bırunı invented the Planisphere, an 
analog computer as well as the first mechanical lu-
nisolar calendar—an early example of a fixed-wired 
knowledge processing machine.
circa 1015
Arab astronomer, Abu Ishaq Ibrahım al-Zarqalı 
(Arzachel) of al-Andalus, invented the Equatorium, a 
mechanical analog computer device used for finding 
the longitudes and positions of the Moon, Sun and 
planets without calculation.
1020
The mechanical geared astrolabe earlier developed 
by Abu Rayhan al-Bırunı perfected by Ibn Samh. This 
can be considered an ancestor of the mechanical 
clock.
circa 1100
Arab astronomer, Jabir ibn Aflah (Geber), invented 
the Torquetum, an observational instrument and 
mechanical analog computer device used to trans-
form between spherical coordinate systems. It was 
designed to take and convert measurements made 
in three sets of coordinates: horizon, equatorial, and 
ecliptic.
1206
Arab engineer, Al-Jazari, designed a humanoid-
shaped mannequin which may have been the first sci-
entific plan for a robot. His “castle clock” considered 
one of the earliest programmable analog computers, 
displayed the zodiac and solar and lunar orbits. The 
length of day and night could be re-programmed 
every day in order to account for the changing 
lengths of day and night throughout the year.
1235
Persian astronomer Abi Bakr of Isfahan invented a 
brass astrolabe with a geared calendar movement 
based on the design of Abu Rayhan al-Bırunı’s me-
chanical calendar analog computer.
1300
Ramon Llull invented the Lullian Circle: a notional 
machine for calculating answers to philosophical 
questions via logical combinatorics. This idea was 
taken up by Leibniz centuries later, and is thus one 
of the founding elements in computing and informa-
tion science.
circa 1400
Kerala school of astronomy and mathematics in 
South India invented the floating point number 
system.
circa 1400
Jamshıd 
al-Kashı 
invented 
the 
“Plate 
of 
Conjunctions”,  an analog computer instrument 
used to determine the time of day at which planetary 
conjunctions will occur, and for performing linear 
interpolation. 
circa 1400
Ahmad al-Qalqashandi gives a list of ciphers in his 
“Subh al-a’sha” which include both substitution and 
transposition, and for the first time, a cipher with 
multiple substitutions for each plaintext letter. He 
also gives an exposition on and worked example of 
cryptanalysis, including the use of tables of letter fre-
quencies and sets of letters which cannot occur to-
gether in one word.
1492
Leonardo da Vinci produced drawings of a device 
consisting of interlocking cog wheels which can be 
interpreted as a mechanical calculator capable of ad-
dition and subtraction. Da Vinci also made plans for 
a mechanical man: an early design for a robot.
1588
Joost Buerghi discovered natural logarithms.
1614
Scotsman John Napier reinvented a form of loga-
rithms and an ingenious system of movable rods (re-
ferred to as Napier’s Rods or Napier’s bones). These 
rods were based on lattice or gelosia multiplication 
algorithm and allowed the operator to multiply, di-
vide and calculate square and cube roots by moving 

297
Principles of Computer Science
History of Events Leading up to the Development of Modern Computers 
the rods around and placing them in specially con-
structed boards.
1622
William Oughtred developed slide rules based on nat-
ural logarithms as developed by John Napier.
1623
German polymath Wilhelm Schickard drew a device 
that he called a “Calculating Clock” on two letters 
that he sent to Johannes Kepler; one in 1623 and the 
other in 1624. A fire later destroyed the machine as it 
was being built in 1624 and he decided to abandon 
his project. This machine became known to the 
world only in 1957 when the two letters were discov-
ered. Some replicas were built in 1961. This machine 
had no impact on the development of mechanical 
calculators.
1642
France
French polymath Blaise Pascal invented the me-
chanical calculator. Called “machine arithmétique”, 
Pascal’s calculator and eventually Pascaline, its public 
introduction in 1645 started the development of me-
chanical calculators first in Europe and then in the 
rest of the world. It was the first machine to have a 
controlled carry mechanism. Pascal built 50 proto-
types before releasing his first machine (eventually 
twenty machines were built). The Pascaline inspired 
the works of Gottfried Leibniz (1671), Thomas de 
Colmar (1820) and Dorr E. Felt (1887).
1668
United Kingdom
Sir Samuel Morland (1625–1695), of England, pro-
duced a non-decimal adding machine, suitable for use 
with English money. Instead of a carry mechanism, it 
registered carries on auxiliary dials, from which the 
user re-entered them as addends.
1671
Germany
German mathematician, Gottfried Leibniz designed 
a machine which multiplied, the ‘Stepped Reckoner’. 
It could multiply numbers of up to 5 and 12 digits to 
give a 16 digit result. Two machines were built, one in 
1694 (it was discovered in an attic in 1879), and one 
in 1706. 
1685
Germany
In an article titled “”Machina arithmetica in qua non 
additio tantum et subtractio sed et multiplicatio nullo, 
diviso vero paene nullo animi labore peragantur””, 
Gottfried Leibniz described a machine that used 
wheels with movable teeth which, when coupled to a 
Pascaline, could perform all four mathematical op-
erations. There is no evidence that Leibniz ever con-
structed this pinwheel machine.
1709
Italy
Giovanni Poleni was the first to build a calculator that 
used a pinwheel design. It was made of wood and was 
built in the shape of a “calculating clock”.
1726
United Kingdom
Jonathan Swift described (satirically) a machine (“en-
gine”) in his “Gulliver’s Travels”. The “engine” con-
sisted of a wooden frame with wooden blocks con-
taining parts of speech. When the engine’s 40 levers 
are simultaneously turned, the machine displayed 
grammatical sentence fragments.
1774
Germany
Philipp Matthäus Hahn, in what is now Germany, 
made a successful portable calculator able to perform 
all four mathematical operations.
1775
United Kingdom
Charles Stanhope, 3rd Earl Stanhope, of England, de-
signed and constructed a successful multiplying calcu-
lator similar to Leibniz’s.
1786
Germany
J. H. Müller, an engineer in the Hessian army, first 
conceived of the idea of a difference engine.

298
History of Events Leading up to the Development of Modern Computers 
Principles of Computer Science
1801
France
Joseph-Marie Jacquard developed an automatic loom 
controlled by punched cards.
1820
France
Charles Xavier Thomas de Colmar invented the 
‘Arithmometer’ which after thirty more years of de-
velopment became, in 1851, the first mass-produced 
mechanical calculator. An operator could perform 
long multiplications and divisions quickly and effec-
tively by using a movable accumulator for the result. 
This machine was based on the earlier works of Pascal 
and Leibniz. 
1822
United Kingdom
Charles Babbage designed his first mechanical com-
puter, the first prototype of the decimal difference 
engine for tabulating polynomials.
1832
Russia
Semen Korsakov proposed the usage of punched 
cards for information storage and search. He de-
signed several machines to demonstrate his ideas, in-
cluding the so-called “linear homeoscope”.
1832
United Kingdom
Babbage and Joseph Clement produced a prototype 
segment of his difference engine. The complete 
engine was planned to operate both on sixth-order 
differences with numbers of about 20 digits, and on 
third-order differences with numbers of 30 digits. 
Each addition would have been done in two phases, 
the second one taking care of any carries generated 
in the first. The output digits were to be punched 
into a soft metal plate, from which a printing plate 
might have been made. No more than this prototype 
piece was ever finished. 
1834
United Kingdom
Babbage conceived, and began to design, his decimal 
‘Analytical Engine’. A program for it was to be stored 
on read-only memory, in the form of punched cards. 
The machine envisioned would have been capable of 
an addition in 3 seconds and a multiplication or divi-
sion in 2–4 minutes. It was to be powered by a steam 
engine. In the end, no more than a few parts were 
actually built.
1835
United States
Joseph Henry invented the electromechanical relay.
1842
France
Timoleon Maurel patented the Arithmaurel, a me-
chanical calculator with a very intuitive user inter-
face, especially for multiplying and dividing numbers 
because the result was displayed as soon as the oper-
ands were entered. It received a gold medal at the 
French national show in Paris in 1849. Unfortunately 
its complexity and the fragility of its design prevented 
it from being manufactured. 
1842
United Kingdom
Construction of Babbage’s difference engine was 
cancelled as an official project. The cost overruns 
had been considerable (£17,470 was spent, which, in 
2004 money, would be about £1,000,000 ). 
1843
Sweden
Per Georg Scheutz and his son Edvard produced 
a third-order difference engine with printer; the 
Swedish government agrees to fund their next 
development.
1847
United Kingdom
Babbage designed an improved, simpler difference 
engine (the Difference Engine No.2), a project which 
took 2 years. The machine would have operated on 7th-
order differences and 31-digit numbers, but nobody 
was found to pay to have it built. In 1989-1991 a team 
at London’s Science Museum did build one from the 
surviving plans. They built components using modern 
methods, but with tolerances no better than Clement 
could have provided... and, after a bit of tinkering and 
detail-debugging, they found that the machine works 
properly. In 2000, the printer was also completed.

299
Principles of Computer Science
History of Events Leading up to the Development of Modern Computers 
1848
United Kingdom
British Mathematician George Boole developed 
binary algebra (Boolean algebra) which has been 
widely used in binary computer design and opera-
tion, beginning about a century later. 
1851
France
After 30 years of development, Thomas de Colmar 
launched the mechanical calculator industry by 
starting the manufacturing of a much simplified 
Arithmometer (invented in 1820). Aside from its 
clones, which started thirty years later, it was the only 
calculating machine available anywhere in the world 
for forty years (Dorr Felt|Dorr E. Felt only sold one 
hundred comptometers and a few comptographs 
from 1887 to 1890). Its simplicity made it the most 
reliable calculator to date. It was a big machine (a 20 
digit arithmometer was long enough to occupy most 
of a desktop). Even though the arithmometer was 
only manufactured until 1915, twenty European com-
panies manufactured improved clones of its design 
until the beginning of WWII ; they were Burkhardt, 
Layton, Saxonia, Gräber, Peerless, Mercedes-Euklid, 
XxX, Archimedes, etc...
1853
Sweden
To Babbage’s delight, the Scheutzes completed the 
first full-scale difference engine, which they called a 
Tabulating Machine. It operated on 15-digit numbers 
and 4th-order differences, and produced printed 
output just as Babbage’s would have. A second ma-
chine was later built to the same design by the firm of 
Bryan Donkin of London.
1858
United States
The first Tabulating Machine (see 1853) was bought 
by the Dudley Observatory in Albany, New York, and 
the second by the British government. The Albany 
machine was used to produce a set of astronomical ta-
bles; but the Observatory’s director was fired for this 
extravagant purchase, and the machine never seri-
ously used again, eventually ending up in a museum. 
The second machine had a long and useful life.
1869
United Kingdom
The first practical logic machine was built by William 
Stanley Jevons.
1871
United Kingdom
Babbage produced a prototype section of the 
Analytical Engine’s mill and printer.
1875
Sweden
Martin Wiberg produced a reworked difference-en-
gine-like machine intended to prepare logarithmic 
tables.
1878
Spain
Ramon Verea, living in New York City, invented a 
calculator with an internal multiplication table; this 
was much faster than the shifting carriage, or other 
digital methods of the time. He wasn’t interested in 
putting it into production, however; it seems he just 
wanted to show that a Spaniard could invent as well 
as an American.
1879
United Kingdom
A committee investigated the feasibility of com-
pleting the Analytical Engine, and concluded that it 
would be impossible now that Babbage was dead. The 
project was then largely forgotten, except by a very 
few; Howard Aiken was a notable exception.
1884
United States
Dorr Felt, of Chicago, developed his Comptometer. 
This was the first calculator in which operands are en-
tered by pressing keys rather than having to be, for 
example, dialled in. It was feasible because of Felt’s 
invention of a carry mechanism fast enough to act 
while the keys return from being pressed. Felt and 
Tarrant started a partnership to manufacture the 
comptometer in 1887. 

300
History of Events Leading up to the Development of Modern Computers 
Principles of Computer Science
1885
United States, Sweden, Russia
A multiplying calculator more compact than the 
Arithmometer entered mass production. The design 
was the independent, and more or less simultaneous, 
invention of Frank S. Baldwin, of the United States, 
and Willgodt Theophil Odhner, a Swede living in 
Russia. Fluted drums were replaced by a “variable-
toothed gear” design: a disk with radial pegs that 
could be made to protrude or retract from it.
1886
United States
Herman Hollerith developed the first version of his 
tabulating system in the Baltimore Department of 
Health.
1889
United States
Dorr Felt invented the first printing desk calculator.
1890
United States
The 1880 US census had taken 7 years to complete 
since all processing had been done by hand from 
journal sheets. The increasing population suggested 
that by the 1890 census, data processing would take 
longer than the 10 years before the next census—so 
a competition was held to find a better method. It 
was won by a Census Department employee, Herman 
Hollerith, who went on to found the Tabulating 
Machine Company, later to become IBM. He in-
vented the recording of data on a medium that could 
then be read by a machine. Prior uses of machine 
readable media had been for control (Automatons, 
Piano rolls, looms, ...), not data. “After some initial 
trials with paper tape, he settled on punched cards...” 
His machines used mechanical relays (and solenoids) 
to increment mechanical counters. This method was 
used in the 1890 census and the completed results 
(62,622,250 people) were “... finished months ahead 
of schedule and far under budget”. The inspiration 
for this invention was Hollerith’s observation of rail-
road conductors during a trip in the western US; they 
encoded a crude description of the passenger (tall, 
bald, male) in the way they punched the ticket.
1892
United States
William S. Burroughs of St. Louis, invented a ma-
chine similar to Felt’s (see 1884) in 1885 but unlike 
the comptometer it was a ‘key-set’ machine which 
only processed each number after a crank handle 
was pulled. The true manufacturing of this machine 
started in 1892 even though Burroughs had started 
his” American Arithmometer Company” in 1886 (it 
later became Burroughs Corporation and is now 
called Unisys).
1896
United States
Herman 
Hollerith introduced 
an 
Integrating 
Tabulator that could add numbers encoded on 
punched cards to one of several 7-digit counters. His 
earlier tabulators simply incremented counters based 
on whether a hole was punched or not.
1901
United States
The Standard Adding Machine Company released 
the first 10-key adding machine in between 1901 and 
1903. The inventor, William Hopkins, filed his first 
patent on October 4, 1892. The 10 keys were set on a 
single row.
1902
United States
Remington advertised the Dalton adding machine as 
the first 10-key printing adding machine. The 10 keys 
were set on two rows. Six machines had been manu-
factured by the end of 1906
1906
United Kingdom
Henry Babbage, Charles’s son, with the help of the 
firm of R. W. Munro, completed the ‘mill’ from his 
father’s Analytical Engine, to show that it would have 
worked. It does. The complete machine was not 
produced.
1906
United States
Vacuum tube (or thermionic valve) invented by Lee 
De Forest.

301
Principles of Computer Science
History of Events Leading up to the Development of Modern Computers 
1906
United States
Herman Hollerith introduces a tabulator with a plug-
board that can be rewired to adapt the machine for 
different applications. Plugboards were widely used 
to direct machine calculations until displaced by 
stored programs in the 1950s.
1919
United Kingdom
William Henry Eccles and F. W. Jordan published the 
first flip-flop circuit design.
1924
Germany
Walther Bothe built an “‘AND’” logic gate - the “co-
incidence circuit”, for use in physics experiments, for 
which he received the Nobel Prize in Physics 1954. 
Digital circuitries of all kinds make heavy use of this 
technique.
1926
United States
Westinghouse AC Calculating board. A Network ana-
lyzer (AC power) used for electrical transmission line 
simulations up until the 1960s. 
1928
United States
IBM standardizes on punched cards with 80 columns 
of data and rectangular holes. Widely known as IBM 
Cards, they dominate the data processing industry 
for almost half a century.
1930
United States
Vannevar Bush built a partly electronic difference en-
gine capable of solving differential equations.
1930
United Kingdom
Welsh physicist C. E. Wynn-Williams<!--- (1903-1979) 
--->, at Cambridge, England, used a ring of thyra-
tron tubes to construct a binary digital counter that 
counted emitted Alpha particles.
1931
Austria
Kurt Gödel of Vienna University, Austria, published 
a paper on a universal formal language based on 
arithmetic operations. He used it to encode arbi-
trary formal statements and proofs, and showed 
that formal systems such as traditional mathematics 
are either inconsistent in a certain sense, or contain 
unprovable but true statements. This result is often 
called the fundamental result of theoretical com-
puter science.
1931
United States
IBM introduced the IBM 601 Multiplying Punch, 
an electromechanical machine that could read two 
numbers, up to 8 digits long, from a card and punch 
their product onto the same card.
1934
United States
Wallace Eckert of Columbia University connects an 
IBM 285 Tabulator, an 016 Duplicating Punch and 
an IBM 601 Multiplying Punch with a cam-controlled 
sequencer switch that he designed. The combined 
system was used to automate the integration of dif-
ferential equations.
1936
United Kingdom
Alan Turing of Cambridge University, England, pub-
lished a paper on ‘computable numbers’ which re-
formulated Kurt Gödel’s results (see related work by 
Alonzo Church). His paper addressed the famous 
‘Entscheidungsproblem’ whose solution was sought 
in the paper by reasoning (as a mathematical device) 
about a simple and theoretical computer, known 
today as a Turing machine. In many ways, this device 
was more convenient than Gödel’s arithmetics-based 
universal formal system.
1937
United States
George Stibitz of the Bell Telephone Laboratories 
(Bell Labs), New York City, constructed a demon-
stration 1-bit binary adder using relays. This was 
one of the first binary computers, although at this 

302
History of Events Leading up to the Development of Modern Computers 
Principles of Computer Science
stage it was only a demonstration machine; improve-
ments continued leading to the “Complex Number 
Calculator” of January 1940.
1937
United States
Claude E. Shannon published a paper on the imple-
mentation of symbolic logic using relays as his MIT 
Master’s thesis.
1938
Nazi Germany
Konrad Zuse of Berlin, completed the ‘Z1’, the first 
mechanical binary programmable computer. It was 
based on Boolean Algebra and had some of the basic 
ingredients of modern machines, using the binary 
system and floating-point arithmetic. Zuse’s 1936 
patent application (Z23139/GMD Nr. 005/021) 
also suggested a ‘von Neumann’ architecture (re-
invented about 1945) with program and data modifi-
able in storage. Originally the machine was called the 
‘V1’ but retroactively renamed after the war, to avoid 
confusion with the V-1 flying bomb. It worked with 
floating point numbers (7-bit exponent, 16-bit man-
tissa, and sign bit). The memory used sliding metal 
parts to store 16 such numbers, and worked well; but 
the arithmetic unit was less successful, occasionally 
suffering from certain mechanical engineering prob-
lems. The program was read from holes punched in 
discarded 35 mm movie film. Data values could have 
been entered from a numeric keyboard, and outputs 
were displayed on electric lamps. The machine was 
not a general purpose computer (i.e., Turing com-
plete) because it lacked loop capabilities.
1939
United States
William Hewlett and David Packard established the 
Hewlett-Packard Company in Packard’s garage in 
Palo Alto, California with an initial investment of 
$538; this was considered to be the symbolic founding 
of Silicon Valley. HP would grow to become one of 
the largest technology companies in the world today. 
1939, Nov
United States
John Vincent Atanasoff and graduate student 
Clifford Berry of Iowa State College (now the Iowa 
State University), Ames, Iowa, completed a prototype 
16-bit adder. This was the first machine to calculate 
using vacuum tubes. 
1939
Nazi Germany
Konrad Zuse completed the ‘Z2’ (originally ‘V2’), 
which combined the Z1’s existing mechanical 
memory unit with a new arithmetic unit using relay 
logic. Like the Z1, the Z2 lacked loop capabilities. 
The project was interrupted for a year when Zuse was 
drafted, but continued after he was released.
1939
Nazi Germany
Helmut Schreyer completed a prototype 10-bit adder 
using vacuum tubes, and a prototype memory using 
neon lamps.
1940, Jan
United States
At Bell Labs, Samuel Williams and George Stibitz 
completed a calculator which could operate on com-
plex numbers, and named it the ‘Complex Number 
Calculator’; it was later known as the ‘Model I Relay 
Calculator’. It used telephone switching parts for 
logic: 450 relays and 10 crossbar switches. Numbers 
were represented in ‘plus 3 bceD’; that is, for each 
decimal digit, 0 is represented by binary 0011, 1 by 
0100, and so on up to 1100 for 9; this scheme re-
quires fewer relays than straight bceD. Rather than 
requiring users to come to the machine to use it, 
the calculator was provided with three remote key-
boards, at various places in the building, in the form 
of teletypes. Only one could be used at a time, and 
the output was automatically displayed on the same 
one. On 9 September 1940, a teletype was set up at 
a Dartmouth College in Hanover, New Hampshire, 
with a connection to New York, and those attending 
the conference could use the machine remotely.
1940, Apr 1 
GER|Nazi
In 1940 Zuse presented the Z2 to an audience 
of the {{lang|de|”Deutsche Versuchsanstalt für 
Luftfahrt”}} (“German Laboratory for Aviation”) in 
Berlin-Adlershof.
|}

303
Principles of Computer Science
History of Events Leading up to the Development of Modern Computers 
==1941–1949==
{| class=”wikitable sortable”
|-
! Date
! class=”unsortable” | Place
! class=”unsortable” | Event
1941, May 11
GER|Nazi
Now working with limited backing from the DVL 
(German Aeronautical Research Institute), Konrad 
Zuse completed the “Z3” (originally ‘V3’): the first 
operational programmable computer. One major 
improvement over Charles Babbage’s non-functional 
device is the use of Leibniz’s binary system (Babbage 
and others unsuccessfully tried to build decimal pro-
grammable computers). Zuse’s machine also fea-
tured floating point numbers with a 7-bit exponent, 
14-bit mantissa (with a ‘1’ bit automatically prefixed 
unless the number is 0), and a sign bit. The memory 
held 64 of these words and therefore required over 
1400 relays; there were 1200 more in the arithmetic 
and control units. It also featured parallel adders. 
The program, input, and output were implemented 
as described above for the Z1. Although conditional 
jumps were not available, it has been shown that 
Zuse’s Z3 is, in principle, capable of functioning as 
a universal computer. The machine could do 3-4 ad-
ditions per second, and took 3–5 seconds for a mul-
tiplication. The Z3 was destroyed in 1943 during an 
Allied bombardment of Berlin, and had no impact 
on computer technology in America and England.
1942, Summer
United States
Atanasoff and Berry completed a special-purpose 
calculator for solving systems of simultaneous linear 
equations, later called the ‘Abce’ (‘Atanasoff–Berry 
Computer’). This had 60 50-bit words of memory in 
the form of capacitors (with refresh circuits—the first 
regenerative memory) mounted on two revolving 
drums. The clock speed was 60 Hz, and an addi-
tion took 1 second. For secondary memory it used 
punched cards, moved around by the user. The holes 
were not actually punched in the cards, but burned. 
The punched card system’s error rate was never re-
duced beyond 0.001%, and this was inadequate. 
Atanasoff left Iowa State after the U.S. entered the 
war, ending his work on digital computing machines.
1942
GER|Nazi
Helmut Hölzer built an analog computer to calculate 
and simulateV-2 rocket trajectories.
1942
GER|Nazi
Konrad Zuse developed the S1, the world’s first pro-
cess computer, used by Henschel to measure the sur-
face of wings.
1943, Apr
United Kingdom
Max Newman, Wynn-Williams and their team at 
the secret Government Code and Cypher School 
(‘Station X’), Bletchley Park, Bletchley, England, 
completed the ‘Heath Robinson’. This was a special-
ized counting machine used for cipher-breaking, 
not a general-purpose calculator or computer, but a 
logic device using a combination of electronics and 
relay logic. It read data optically at 2000 characters 
per second from 2 closed loops of paper tape, each 
typically about 1000 characters long. It was signifi-
cant since it was the forerunner of Colossus. Newman 
knew Turing from Cambridge (Turing was a student 
of Newman’s), and had been the first person to see a 
draft of Turing’s 1936 paper. Heath Robinson is the 
name of a British cartoonist known for drawings of 
comical machines, like the American Rube Goldberg. 
Two later machines in the series will be named after 
London stores with ‘Robinson’ in their names.
1943, Sep
United States
Williams 
and 
Stibitz 
completed 
the 
‘Relay 
Interpolator’, later called the ‘Model II Relay 
Calculator’. This was a programmable calculator; 
again, the program and data were read from paper 
tapes. An innovative feature was that, for greater re-
liability, numbers were represented in a biquinary 
format using 7 relays for each digit, of which exactly 2 
should be “on”: 01 00001 for 0, 01 00010 for 1, and so 
on up to 10 10000 for 9. Some of the later machines 
in this series would use the biquinary notation for the 
digits of floating-point numbers.

304
History of Events Leading up to the Development of Modern Computers 
Principles of Computer Science
1943, Dec
United Kingdom
The Colossus was built, by Dr Thomas Flowers at 
The Post Office Research Laboratories in London, 
to crack the German Lorenz (SZ42) cipher. It con-
tained 2400 vacuum tubes for logic and applied a 
programmable logical function to a stream of input 
characters, read from punched tape at a rate of 5000 
characters a second. Colossus was used at Bletchley 
Park during World War II—as a successor to the unre-
liable Heath Robinson machines. Although 10 were 
eventually built, most were destroyed immediately 
after they had finished their work to maintain the se-
crecy of the work.
1944, Aug 7
United States
The 
IBM 
Automatic 
Sequence 
Controlled 
Calculator was turned over to Harvard University, 
which called it the Harvard Mark I. It was designed 
by Howard Aiken and his team, financed and built 
by IBM—it became the second program controlled 
machine (after Konrad Zuse’s). The whole machine 
was {{convert|51|ft|m}} long, weighed 5 (short) 
tons (4.5 tonnes), and incorporated 750,000 parts. 
It used 3304 electromechanical relays as on-off 
switches, had 72 accumulators (each with its own 
arithmetic unit), as well as a mechanical register 
with a capacity of 23 digits plus sign. The arithmetic 
was fixed-point and decimal, with a control panel 
setting determining the number of decimal places. 
Input-output facilities include card readers, a card 
punch, paper tape readers, and typewriters. There 
were 60 sets of rotary switches, each of which could 
be used as a constant register—sort of mechanical 
read-only memory. The program was read from one 
paper tape; data could be read from the other tapes, 
or the card readers, or from the constant registers. 
Conditional jumps were not available. However, in 
later years, the machine was modified to support 
multiple paper tape readers for the program, with 
the transfer from one to another being conditional, 
rather like a conditional subroutine call. Another 
addition allowed the provision of plug-board wired 
subroutines callable from the tape. Used to create 
ballistics tables for the US Navy.
1945
GER|Nazi
Konrad Zuse developed Plankalkül, the first higher-
level programming language. He also presented the 
Z4 in March. 
1945
United States
Vannevar Bush developed the theory of the memex, 
a hypertext device linked to a library of books and 
films.
1945
United States
John von Neumann drafted a report describing 
the future computer eventually built as the EDVAC 
(Electronic Discrete Variable Automatic Computer). 
“First Draft of a Report on the EDVAC” includes 
the first published description of the design of a 
stored-program computer, giving rise to the term 
von Neumann architecture. It directly or indirectly 
influenced nearly all subsequent projects, especially 
EDSAC. The design team included John W. Mauchly 
and J. Presper Eckert.
1946, Feb 14
United States
ENIAC (Electronic Numerical Integrator and 
Computer): One of the first totally electronic, valve 
driven, digital, program-controlled computers was 
unveiled although it was shut down on 9 November 
1946 for a refurbishment and a memory upgrade, 
and was transferred to Aberdeen Proving Ground, 
Maryland in 1947. Development had started in 1943 
at the Ballistic Research Laboratory, USA, by John W. 
Mauchly and J. Presper Eckert. It weighed 30 tonnes 
and contained 18,000 electronic valves, consuming 
around 160 kW of electrical power. It could do 50,000 
basic calculations a second. It was used for calculating 
ballistic trajectories and testing theories behind the 
hydrogen bomb.
1946, Feb 19
United Kingdom
ACE (Automatic Computing Engine): Alan Turing 
presented a detailed paper to the National Physical 
Laboratory (NPL) Executive Committee, giving the 

305
Principles of Computer Science
History of Events Leading up to the Development of Modern Computers 
first reasonably complete design of a stored-program 
computer. However, because of the strict and long-
lasting secrecy around his wartime work at Bletchley 
Park, he was prohibited (having signed the Official 
Secrets Act) from explaining that he knew that his 
ideas could be implemented in an electronic device.
1946
United Kingdom
The trackball was invented as part of a radar plotting 
system named Comprehensive Display System (CDS) 
by Ralph Benjamin when working for the British 
Royal Navy Scientific Service. Benjamin’s project 
used analog computers to calculate the future posi-
tion of target aircraft based on several initial input 
points provided by a user with a joystick. Benjamin 
felt that a more elegant input device was needed and 
invented a “ball tracker” system called the “roller 
ball” for this purpose in 1946. The device was pat-
ented in 1947 but only a prototype was ever built and 
the device was kept as a secret outside military.
1947, Dec 16
United States
Invention of the transistor at Bell Laboratories, USA, 
by William B. Shockley, John Bardeen and Walter 
Brattain.
1947
United States
Howard Aiken completed the Harvard Mark II.
1947
United States
The Association for Computing Machinery (ACM), 
was founded as the world’s first scientific and educa-
tional computing society. It remains to this day with a 
membership currently around 78,000. Its headquar-
ters are in New York City.
1948, Jan 27
United States
IBM finished the SSEC (Selective Sequence 
Electronic Calculator). It was the first computer to 
modify a stored program. “About 1300 vacuum tubes 
were used to construct the arithmetic unit and eight 
very high-speed registers, while 23000 relays were 
used in the control structure and 150 registers of 
slower memory.”
1948, Jul 21
United Kingdom
SSEM, Small-Scale Experimental Machine or ‘Baby’ 
was built at the University of Manchester. It ran its 
first program on this date. It was the first computer to 
store both its programs and data in RAM, as modern 
computers do. By 1949 the ‘Baby’ had grown, and 
acquired a magnetic drum for more permanent 
storage, and it became the Manchester Mark 1.
1948
United States
ANACOM from Westinghouse was an AC-energized 
electrical analog computer system used up until the 
early 1990s for problems in mechanical and struc-
tural design, fluidics, and various transient problems.
1948
United States
IBM introduced the ‘604’, the first machine to fea-
ture Field Replaceable Units (FRUs), which cut 
downtime as entire pluggable units can simply be re-
placed instead of troubleshot.
1948
The first Curta handheld mechanical calculator was 
sold. The Curta computed with 11 digits of decimal 
precision on input operands up to 8 decimal digits. 
The Curta was about the size of a handheld pepper 
grinder.
1949, Mar
United States
John 
Presper 
Eckert 
and 
John 
William 
Mauchly construct the BINAC for Northrop 
Corporation|Northrop.
1949, May 6
United Kingdom
This is considered the birthday of modern com-
puting. Maurice Wilkes and a team at Cambridge 
University executed the first stored program on the 
EDSAC computer, which used paper tape input-
output. Based on ideas from John von Neumann 

306
History of Events Leading up to the Development of Modern Computers 
Principles of Computer Science
about stored program computers, the EDSAC was the 
first complete, fully functional von Neumann archi-
tecture computer.
1949, Oct
United Kingdom
The Manchester Mark 1 final specification is com-
pleted; this machine was notably in being the first 
computer to use the equivalent of base/index regis-
ters, a feature not entering common computer archi-
tecture until the second generation around 1955.
1949
Australia
CSIR Mk I (later known as CSIRAC), Australia’s first 
computer, ran its first test program. It was a vacuum 
tube based electronic general purpose computer. 
Its main memory stored data as a series of acoustic 
pulses in 5-foot-long tubes filled with mercury.
1949
United Kingdom
MONIAC (Monetary National Income Analogue 
Computer) also known as the Phillips Hydraulic 
Computer, was created in 1949 to model the national 
economic processes of the United Kingdom. The 
MONIAC consisted of a series of transparent plastic 
tanks and pipes. It is thought that twelve to fourteen 
machines were built.
1949
United States
“Computers in the future may weigh no more than 
1.5 tons.” “Popular Mechanics”, forecasting the re-
lentless march of science.

307
Timeline of Microprocessors
Year
Microprocessors
1971
Intel 4004
1972
Fairchild PPS-25; Intel 8008; Rockwell PPS-4
1973
Burroughs Mini-D; National IMP-16; NEC µCOM
1974
General Instrument CP1600; Intel 4040, 8080; Mostek 5065; Motorola 6800; National 
IMP-4, IMP-8, ISP-8A/500, PACE; Texas Instruments TMS 1000; Toshiba TLCS-12
1975
Fairchild F-8; Hewlett Packard BPC; Intersil 6100; MOS Technology 6502; RCA CDP 1801; 
Rockwell PPS-8; Signetics 2650
1976
RCA CDP 1802; Signetics 8x300; Texas Instruments TMS9900; Zilog Z-80
1977
Intel 8085
1978
Intel 8086; Motorola 6801, 6809
1979
Intel 8088; Motorola 68000; Zilog Z8000
1980
National Semi 16032; Intel 8087
1981
DEC T-11; Harris 6120; IBM ROMP
1982
Hewlett Packard FOCUS; Intel 80186, 80188,; 80286; Berkeley RISC-I
1983
Stanford MIPS; UC Berkeley RISC-II
1984
Motorola 68020; National Semi 32032; NEC V20
1985
DEC MicroVax II; Harris Novix; Intel 80386; MIPS R2000
1986
NEC V60; Sun SPARC; Zilog Z80000
1987
Acorn ARM2; DEC CVAX 78034; Hitachi Gmicro/200; Motorola 68030; NEC V70
1988
Intel 80386SX, i960; MIPS R3000
1989
DEC VAX DC520 Rigel; Intel 80486, i860
1990
IBM POWER1; Motorola 68040
1991
DEC NVAX; IBM RSC; MIPS R4000
1992
DEC Alpha 21064; Hewlett Packard PA-7100; Sun microSPARC I
1993
IBM POWER2, PowerPC 601; Intel Pentium

308
Timeline of Microprocessors
Principles of Computer Science
1994
DEC Alpha 21064A; Hewlett Packard PA-7100LC, PA-7200; IBM PowerPC 603, PowerPC 
604; Motorola 68060; QED R4600
1995
DEC Alpha 21164; HAL Computer SPARC64; Intel Pentium Pro; Sun UltraSPARC
1996
AMD K5; DEC Alpha 21164A; HAL Computer SPARC64 II; Hewlett Packard PA-8000; IBM 
P2SC; MTI R10000; QED R5000
1997
AMD K6; IBM PowerPC 620, PowerPC 750,; RS64, ES/390 G4; Intel Pentium II; Sun 
UltraSPARC IIs
1998
DEC Alpha 21264; HAL Computer SPARC64 III; Hewlett Packard PA-8500; IBM POWER3, 
RS64-II; ES/390 G5; QED RM7000; SGI MIPS R12000
1999
AMD Athlon; IBM RS64-III; Intel Pentium III; Motorola PowerPC 7400
2000
AMD Athlon XP; Duron; Fujitsu SPARC64 IV; IBM RS64-IV; z900; Intel Pentium 4
2001
IBM POWER4; Intel Itanium; Motorola PowerPC 7450; SGI MIPS R14000; Sun  
UltraSPARC III
2002
Fujitsu SPARC64 V; Intel Itanium 2
2003
AMD Opteron; IBM PowerPC 970; Intel Pentium M
2004
IBM POWER5; PowerPC BGL
2005
AMD Athlon 64 X2; Opteron Athens; IBM PowerPC 970MP; Xenon; Intel Pentium D; Sun 
UltraSPARC IV; UltraSPARC T1
2006
IBM Cell/B.E.; Intel Core 2; Core Duo; Itanium Montecito
2007
AMD Opteron Barcelona; Fujitsu SPARC64 VI; IBM POWER, PowerPC BGP; Sun 
UltraSPARC T2; Tilera TILE64
2008
AMD Opteron Shanghai, Phenom; Fujitsu SPARC64 VII; IBM PowerXCell 8i; IBM z10; 
Intel Atom, Core i7; Tilera TILEPro64
2009
AMD Opteron Istanbul, Phenom II
2010
AMD Opteron Magny-cours; Fujitsu SPARC64 VII+; IBM POWER7; z196; Intel Itanium 
Tukwila, Westmere; Xeon, Nehalem-EX; Sun SPARC T3
2011
AMD FX Bulldozer, Interlagos, Llano; Fujitsu SPARC64 VIIIfx; Freescale PowerPC e6500; 
Intel Sandy Bridge, Xeon E7; Oracle SPARC T4
2012
Fujitsu SPARC64 IXfx; IBM POWER7+, zEC12; Intel Itanium Poulson
2013
Fujitsu SPARC64 X; Intel Haswell; Oracle SPARC T5
2014
IBM POWER8

309
The pioneers of computer science
al-Khwarizmı
The term “algorithm” is derived from the algorism, 
the technique of performing arithmetic with Hindu-
Arabic numerals developed by al-Khwarizmi. Both 
“algorithm” and “algorism” are derived from the 
Latinized forms of al-Khwarizmi’s name, Algoritmi 
and Algorismi, respectively.
c. 780 – c. 850
John Atanasoff
Built the first electronic digital computer, the 
Atanasoff–Berry Computer, though it was neither pro-
grammable nor Turing-complete (1939).
b. 1903
d. 1995
Charles Babbage
Originated the concept of a programmable general-
purpose computer. Designed the Analytical Engine 
and built a prototype for a less powerful mechanical 
calculator.
1822
1837
John Warner Backus
Invented 
FORTRAN 
(“For”mula 
“Tran”slation), 
the first practical high-level programming language 
(1954), and he formulated the Backus–Naur form 
that described the formal language syntax (1963).
b. 1924
d. 2007
Jean Bartik
One of the first computer programmers, on ENIAC 
(1946). Worked with John Mauchly toward BINAC 
(1949), EDVAC (1949), UNIVAC (1951) to develop 
early “stored program” computers.
b. 1924
d. 2011
Sir Timothy John 
Berners-Lee
Invented worldwide web. With Robert Cailliau, sent 
first HTTP communication between client and server 
(1989).
b. 1955
George Boole
Formalized Boolean algebra (1854) in The Laws of 
Thought, the basis for digital logic and computer 
science.
b. 1815
d. 1864
Per Brinch Hansen
|Developed the RC 4000 multiprogramming system in-
troducing the concept of an operating system kernel 
and the separation of policy and mechanism (1967). 
Co-developed the monitor with Tony Hoare (1973), 
and created the first monitor implementation (1975). 
Implemented the first form of remote procedure call 
in the RC 4000 (1978)
|1969
Nikolay Brusentsov
Built ternary computer Setun (1958).
b. 1925
d. 2014
Vannevar Bush
Originator of the Memex concept (1930), which led to 
the development of Hypertext.
b. 1890
d. 1974
David Caminer
Developed the LEO computer the first business com-
puter with John Pinkerton, for J. Lyons and Co. (1951)
b. 1915
d. 2008

310
The pioneers of computer science
Principles of Computer Science
Vinton Gray Cerf
With Bob Kahn, designed the Transmission Control 
Protocol and Internet Protocol (TCP/IP), the pri-
mary data communication protocols of the Internet 
and other computer networks (1978).
b. 1943
Avrom Noam Chomsky
Responsible for Chomsky hierarchy (1956), a dis-
covery which has directly impacted Programming lan-
guage theory and other branches of computer science.
b. 1928
Alonzo Church
Contributions to theoretical computer science, specif-
ically for the development of the lambda calculus and 
the discovery of the undecidability problem within it 
(1936).
B, 1903
d. 1995
Wesley Allison. Clark
Designed LINC, the first functional computer scaled 
down and priced for the individual user (1963) with 
many of its features prototypes of what became essen-
tial elements of personal computers.
b. 1927
d. 2016
Edmund Melson 
Clarke, Jr.
Developed model checking and formal verification of 
software and hardware with E. Allen Emerson (1981).
b. 1945
Edgar Frank “Ted” 
Codd
Proposed and formalized the relational model of data 
management, the theoretical basis of relational data-
bases (1970).
b. 1923
d. 2003
Stephen Cook
Formalized the notion of NP-completeness, inspiring 
a great deal of research in computational complexity 
theory (1971).
b. 1939
James William Cooley
Created the Fast Fourier Transform (FFT) with John 
W. Tukey (1965).
b. 1926
d. 2016
Ole-Johan Dahl
With Kristen Nygaard, invented the proto-object ori-
ented language SIMULA (1962).
b. 1931
d. 2002
Edsger Wybe Dijkstra
Made advances in algorithms (1965), the semaphore, 
rigor, and pedagogy (1968).
b. 1930
d. 2002
John Adam Presper 
“Pres” Eckert, Jr.
Designed and built the ENIAC (1943) with John 
Mauchly, the first all electronic, Turing-complete) 
computer, and the UNIVAC I, the first commercially 
available computer (1951).
b. 1919
d. 1995
Enest Allen Emerson II
Developed model checking and formal verification 
of software and hardware together with Edmund M. 
Clarke (1981).
b. 1954
Douglas Carl Engelbart
Invented the computer mouse with Bill English 
(1963); a pioneer of human-computer interaction 
whose Augment team developed hypertext, networked 
computers, and precursors to GUIs (1968).
b. 1925
d. 2013

311
Principles of Computer Science
The pioneers of computer science
Thomas “Tommy” 
Harold Flowers
Designed and built the Mark 1 (1943) and the im-
proved Mark 2 (1944) Colossus computers, the world’s 
first programmable, digital, electronic, computing 
devices.
1943
b. 1905
d. 1998
Friederich Ludwig 
Gottlob Frege
Developed first-order predicate calculus, a crucial 
precursor requirement to developing computation 
theory (1879).
b. 1848
d. 1935
Seymour Ginsburg
Proved “don’t-care” circuit minimization does not nec-
essarily yield optimal results (1958); ALGOL program-
ming language is context-free (thus linking formal 
language theory to the problem of compiler writing. 
1966); invented AFL Theory (1967).
b. 1927
d. 2004
Kurt Friedrich Gödel
Proved Peano axiomatized arithmetic could not be 
both logically consistent and complete in first-order 
predicate calculus (1931). Church, Kleene, and 
Turing developed the foundations of computation 
theory based on corollaries to Gödel’s work.
b. 1906
d. 1978
Lois Haibt
Was a member of the ten person team that invented 
Fortran (1954) and among the first women to play a 
crucial role in the development of computer science.
b. 1934
Margaret Heafield 
Hamilton
Credited with coining the phrase “Software engi-
neering”; developed the concepts of asynchronous 
software, priority scheduling, end-to-end testing, and 
human-in-the-loop decision capability which became 
the foundation for ultra reliable software design 
(1965).
b. 1936
Sir Charles Anthony 
Richard “C.A.R.” Hoare
Developed the formal language Communicating 
Sequential Processes (CSP) and Quicksort (1960).
b. 1934
Herman Hollerith
Widely regarded as the father of modern machine 
data processing. Invented punched card evaluating 
machine (1889) which began the era of automatic 
data processing systems.
b. 1860
d. 1929
Grace Brewster Murray 
Hopper
Pioneered work on the necessity for high-level pro-
gramming languages, which she termed “automatic 
programming”; her A-O compiler (written in 1952) 
influenced the COBOL language.
b. 1906
d. 1992
Cuthbert Corwin Hurd
Helped 
the 
International 
Business 
Machines 
Corporation develop its first general-purpose com-
puter, the IBM 701 (1952).
b. 1911
d. 1996
Kenneth Eugene 
Iverson
Assisted in establishing and taught the first graduate 
course in computer science at Harvard (1955); in-
vented the APL programming language (1962); made 
contributions to interactive computing.
b. 1920
d. 2004

312
The pioneers of computer science
Principles of Computer Science
Joseph Marie Jacquard
Built and demonstrated the Jacquard loom, a pro-
grammable mechanized loom controlled by punch 
cards (1801).
b. 1752
d. 1834
Maurice Karnaugh
Inventor of the Karnaugh map, used for logic function 
minimization (1953).
b. 1924
Jacek Karpin´ ski
Developed the first differential analyzer that used tran-
sistors; one of the first machine learning algorithms 
for character and image recognition; inventor of one 
of the first minicomputers, the K-202 (1973)
b. 1927
d. 2010
Alan Curtis Kay
Pioneered ideas at the root of object-oriented pro-
gramming languages (1966), led the team that devel-
oped Smalltalk (1976), and made fundamental contri-
butions to personal computing.
b. 1940
Stephen Cole Kleene
Pioneered work with Alonzo Church on the Lambda 
Calculus that first laid down the foundations of com-
putation theory (1935).
b. 1909
d. 1994
Donald Ervin Knuth
Wrote The Art of Computer Programming (1968) and cre-
ated TeX (1978).
1968
b. 1938
Leslie B. Lamport
Formulated algorithms to solve fundamental prob-
lems in distributed systems (e.g. the bakery algorithm. 
1974); logical clock (1978), enabling synchroniza-
tion between distributed entities based on the events 
through which they communicate; created LaTeX 
(1985)
b. 1941
Developed the concept of a.
1978
Sergei Alexeyevich 
Lebedev
Independently designed the first electronic computer 
in the Soviet Union, MESM, in Kiev, Ukraine (1951).
b. 1902
d. 1974
Gottfried Wilhelm 
Leibniz
Advances in symbolic logic, such as the Calculus ra-
tiocinator, heavily influential on Gottlob Frege. Made 
developments in first-order predicate calculus, crucial 
for the theoretical foundations of computer science. 
(1670s)
b. 1646
d. 1716
Joseph Carl Robnett  
“J. C. R.” Licklider
Began the investigation of human-computer interac-
tion (1960), leading to many advances in computer 
interfaces as well as in cybernetics and artificial 
intelligence. 
b. 1915
d. 1990
Ramon Llull
Pioneer of computational theory whose notions of 
symbolic representation and manipulation to produce 
knowledge were major influences on Leibniz.
b. c.1232
d. c. 1315-1316

313
Principles of Computer Science
The pioneers of computer science
Ada Lovelace (Augusta 
Ada King-Noel, 
Countess of Lovelace 
(née Byron)
Began the study of scientific computation, analyzing 
Babbage’s work in her “Sketch of the Analytical 
Engine” (1842). Namesake for the Ada programming 
language.
b. 1815
d. 1852
John William Mauchly
Designed and built the ENIAC (1946) with J. Presper 
Eckert (all electronic, Turing-complete) computer. 
Also worked on BINAC (1949), EDVAC (1949), 
UNIVAC (1951) with Grace Hopper and Jean Bartik, 
to develop early “Stored program” computers.
b. 1907
d. 1980
John McCarthy
Invented LISP, a functional programming language 
(1958).
b. 1927
d. 2011
Marvin Lee Minsky
Co-founder 
of 
Artificial 
Intelligence 
Lab 
at 
Massachusetts Institute of Technology (1963), author 
of several texts on AI and philosophy.
b. 1927
d. 2016
Peter Naur
Edited the ALGOL 60 Revised Report, introducing 
Backus-Naur form (1960).
b. 1928
d. 2016
Maxwell Herman 
Alexander “Max” 
Neumann
Instigated the production of the Colossus com-
puters at Bletchley Park (1941). Established the 
Computing Machine Laboratory at the University of 
Manchester (1946) where the Manchester Small-Scale 
Experimental Machine was invented (1948).
b. 1897
d. 1984
John von Neumann
Formulated the von Neumann architecture upon 
which most modern computers are based (1945).
b. 1903
d. 1957
Kristen Nygaard
With Ole-Johan Dahl, invented the proto-object ori-
ented language SIMULA (1962).
b. 1926
d. 2002
Blaise Pascal
Invented the mechanical calculator (1642).
b. 1623
d. 1662
Emil Leon Post
Developed the Post machine (1936). Known also for 
developing truth tables, the Post correspondence 
problem (1946) and Post’s theorem.
1936
b. 1897
d. 1954
Dennis MacAlistair 
Ritchie
Created the C programming language with Ken 
Thompson and the Unix computer operating system 
at Bell Labs (1970s).
b. 1941
d. 2011
Saul Rosen
Designed the software of the first transistor-based 
computer (1957). Also influenced the ALGOL pro-
gramming language (1958–1960).
b, 1922
d., 1991
Bertrand Arthur 
Russell
Worked on Mathematical logic (example: Truth 
function). Introduced the notion of Type theory. 
Introduced Type system with Alfred North Whitehead 
in Principia Mathematica (1910).
b. 1872
d. 1979

314
The pioneers of computer science
Principles of Computer Science
Gerard Salton
Pioneer of automatic information retrieval (1960s); 
proposed the vector space model and the inverted 
index (1975).
b. 1927
d. 1995
Claude Elwood 
Shannon
Founded practical digital circuit design (1937) and in-
formation theory (1948). 
b. 1916
d. 2001
Herbert Alexander 
Simon
A political scientist and economist who pioneered ar-
tificial intelligence. Co-creator of the Logic Theory 
Machine with Allen Newell (1956) and the General 
Problem Solver with J. C. Shaw and Allen Newell 
(1959).
1956
b. 1916
d. 2001
Ivan Edward 
Sutherland
Author of Sketchpad (1963), precursor to modern 
computer-aided drafting (CAD) programs and an 
early example of object-oriented programming.
b. 1938
John Wilder Tukey
Created the Fast Fourier Transform (FFT algorithm), 
with James Cooley (1965).
b. 1915
d. 2000
Alan MathisonTuring
Made several founding contributions to computer sci-
ence: the Turing machine and the high-speed ACE 
design (1936). Widely considered as the father of 
Computer Science and Artificial Intelligence.
b. 1912
d. 1954
Willis Howard Ware
Co-designer of JOHNNIAC(1955). Chaired committee 
that developed the Code of Fair Information Practice 
(1960s) and led to the Privacy Act of 1974. Vice-chair 
of the Privacy Protection Study Commission(1974).
b. 1920
d. 2013
Adriaan van 
Wijngaarden
Developer of the W-grammar first used in the defini-
tion of ALGOL 68(1968)
b. 1916
d. 1987
Sir Maurice Vincent 
Wilkes
Built the first practical stored program computer 
(EDSAC) to be completed (1949) and for being cred-
ited with the ideas of several high-level programming 
language constructs.
b. 1913
d. 2010
Sophie Wilson
Wrote BBC Basic programming language (1981). Also 
designed the ARM architecture (1985) and Firepath 
(2001) processsors
Niklaus Wirth
Designed the Pascal (1970), Modula-2 (1978) and 
Oberon (1986) programming languages.
b. 1934
Konrad Zuse
Built the first digital freely programmable computer, 
the Z1 (1938). Built the first functional tape-stored 
program-controlled computer, the Z3 (1941), proven 
to be Turing-complete in 1998. Produced the world’s 
first commercial computer, the Z4. Designed the 
first high-level programming language, Plankalkül 
(1943–1945).
b. 1910
d. 1995

315
Glossary
1-bit watermarking: a type of digital watermark that 
embeds one bit of binary data in the signal to be 
transmitted; also called “0-bit watermarking.” 
3-D rendering: the process of creating a 2-D anima-
tion using 3-D models. 
3D Touch: a feature that senses the pressure with 
which users exert upon Apple touch screens. 
abstraction: a technique used to reduce the struc-
tural complexity of programs, making them easier to 
create, understand, maintain, and use. 
accelerometer IC: an integrated circuit that mea-
sures acceleration. 
access level: in a computer security system, a designa-
tion assigned to a user or group of users that allows 
access a predetermined set of files or functions. 
actuator: a motor designed to control the movement 
of a device or machine by transforming potential en-
ergy into kinetic energy. 
Additive White Gaussian Noise (AWGN): a model 
used to represent imperfections in real communica-
tion channels. 
address space: the amount of memory allocated for a 
file or process on a computer. 
adware: software that generates advertisements to 
present to a computer user. 
affinity chromatography: a technique for separating 
a particular biochemical substance from a mix-
ture based on its specific interaction with another 
substance. 
agile software development: an approach to soft-
ware development that addresses changing require-
ments as they arise throughout the process, with pro-
gramming, implementation, and testing occurring 
simultaneously. 
algorithm: a set of step-by-step instructions for per-
forming computations. 
American National Standards Institute (ANSI): a 
nonprofit organization that oversees the creation 
and use of standards and certifications such as those 
offered by CompTIA. 
amplifier: a device that strengthens the power, 
voltage, or current of a signal. 
analog signal: a continuous signal whose values or 
quantities vary over time. 
analytic combinatorics: a method for creating precise 
quantitative predictions about large sets of objects. 
Android Open Source Project: a project undertaken 
by a coalition of mobile phone manufacturers and 
other interested parties, under the leadership of 
Google. The purpose of the project is to develop the 
Android platform for mobile devices. 
animation variables (avars): defined variables used in 
computer animation to control the movement of an 
animated figure or object. 
anthropomorphic: resembling a human in shape or 
behavior; from the Greek words anthropos (human) 
and morphe (form). 
app: an abbreviation for “application,” a program de-
signed to perform a particular task on a computer or 
mobile device. 
application program interface (API): the code that 
defines how two pieces of software interact, particu-
larly a software application and the operating system 
on which it runs. 
application suite: a set of programs designed to work 
closely together, such as an office suite that includes 
a word processor, spreadsheet, presentation creator, 
and database application. 

316
Glossary
Principles of Computer Science
application-level firewalls: firewalls that serve as 
proxy servers through which all traffic to and from 
applications must flow. 
application-specific GUI: a graphical interface de-
signed to be used for a specific application. 
artificial intelligence: the intelligence exhibited by 
machines or computers, in contrast to human, or-
ganic, or animal intelligence. 
asymmetric-key encryption: a process in which data is 
encrypted using a public encryption key but can only 
be decrypted using a different, private key. 
attenuation: the loss of intensity from a signal being 
transmitted through a medium. 
attributes: the specific features that define an object’s 
properties or characteristics. 
audio codec: a program that acts as a “coder-decoder” 
to allow an audio stream to be encoded for storage or 
transmission and later decoded for playback. 
authentication: the process by which the receiver of 
encrypted data can verify the identity of the sender 
or the authenticity of the data. 
automatic sequential control system: a mechanism 
that performs a multistep task by triggering a series 
of actuators in a particular sequence. 
automaton: a machine that mimics a human but is 
generally considered to be unthinking. 
autonomic components: self-contained software or 
hardware modules with an embedded capacity for 
self-management, connected via input/outputs to 
other components in the system. 
autonomous: able to operate independently, without 
external or conscious control. 
autonomous agent: a system that acts on behalf of an-
other entity without being directly controlled by that 
entity. 
backdoor: a hidden method of accessing a computer 
system that is placed there without the knowledge of 
the system’s regular user in order to make it easier to 
access the system secretly. 
base-16: a number system using sixteen symbols, 0 
through 9 and A through F. 
base-2 system: a number system using the digits 0  
and 1. 
BCD-to-seven-segment decoder/driver: a logic gate 
that converts a four-bit binary-coded decimal (BCD) 
input to decimal numerals that can be output to a 
seven-segment digital display. 
behavioral marketing: advertising to users based on 
their habits and previous purchases. 
binder jetting: the use of a liquid binding agent to 
fuse layers of powder together. 
bioinformatics: the scientific field focused on devel-
oping computer systems and software to analyze and 
examine biological data. 
bioinstrumentation: devices that combine biology 
and electronics in order to interface with a pa-
tient’s body and record or monitor various health 
parameters. 
biomarker: short for “biological marker”; a measur-
able quality or quantity (e.g., internal temperature, 
amount of iron dissolved in blood) that serves as an 
indicator of an organism’s health, or some other bio-
logical phenomenon or state. 
biomaterials: natural or synthetic materials that can 
be used to replace, repair, or modify organic tissues 
or systems. 
biomechanics: the various mechanical processes such 
as the structure, function, or activity of organisms. 
bioMEMS: short for “biomedical micro-electrome-
chanical system”; a microscale or nanoscale self- 
contained device used for various applications in 
health care. 

317
Principles of Computer Science
Glossary
biometrics: measurements that can be used to distin-
guish individual humans, such as a person’s height, 
weight, fingerprints, retinal pattern, or genetic 
makeup. 
bionics: the use of biologically based concepts and 
techniques to solve mechanical and technological 
problems. 
biosignal processing: the process of capturing the 
information the body produces, such as heart rate, 
blood pressure, or levels of electrolytes, and ana-
lyzing it to assess a patient’s status and to guide treat-
ment decisions. 
bit: a single binary digit that can have a value of ei-
ther 0 or 1. 
bit rate: the amount of data encoded for each second 
of video; often measured in kilobits per second 
(kbps) or kilobytes per second (Kbps). 
bit width: the number of bits used by a computer or 
other device to store integer values or other data. 
black-box testing: a testing technique in which func-
tion is analyzed based on output only, without knowl-
edge of structure or coding. 
blotting: a method of transferring RNA, DNA, and 
other proteins onto a substrate for analysis. 
bootstrapping: a self-starting process in a computer 
system, configured to automatically initiate other pro-
cesses after the booting process has been initiated. 
bridge: a connection between two or more networks, 
or segments of a single network, that allows the com-
puters in each network or segment to communicate 
with one another. 
broadcast: an audio or video transmission sent via a 
communications medium to anyone with the appro-
priate receiver. 
building information modeling (BIM): the creation 
of a model of a building or facility that accounts 
for its function, physical attributes, cost, and other 
characteristics. 
butterfly effect: an effect in which small changes in a 
system’s initial conditions lead to major, unexpected 
changes as the system develops. 
byte: a group of eight bits. 
caching: the storage of data, such as a previously ac-
cessed web page, in order to load it faster upon fu-
ture access. 
carrier signal: an electromagnetic frequency that 
has been modulated to carry analog or digital 
information. 
cathode ray tube (CRT): a vacuum tube used to 
create images in devices such as older television and 
computer monitors. 
central processing unit (CPU): electronic circuitry 
that provides instructions for how a computer han-
dles processes and manages data from applications 
and programs. 
channel capacity: the upper limit for the rate at which 
information transfer can occur without error. 
character: a unit of information that represents a 
single letter, number, punctuation mark, blank space, 
or other symbol used in written language. 
chatterbot: a computer program that mimics human 
conversation responses in order to interact with 
people through text; also called “talkbot,” “chatbot,” 
or simply “bot.” 
circular wait: a situation in which two or more pro-
cesses are running and each one is waiting for a re-
source that is being used by another; one of the nec-
essary conditions for deadlock. 
class: a collection of independent objects that share 
similar properties and behaviors. 
class-based inheritance: a form of code reuse in 
which attributes are drawn from a preexisting class to 
create a new class with additional attributes. 
clinical engineering: the design of medical devices to 
assist with the provision of care. 

318
Glossary
Principles of Computer Science
clock speed: the speed at which a microprocessor can 
execute instructions; also called “clock rate.” 
coding theory: the study of codes and their use in cer-
tain situations for various applications. 
combinatorial design: the study of the creation and 
properties of finite sets in certain types of designs. 
command line: a text-based computer interface 
that allows the user to input simple commands via a 
keyboard. 
command-line interpreter: an interface that inter-
prets and carries out commands entered by the user. 
commodities: consumer products, physical articles of 
trade or commerce. 
communication architecture: the design of computer 
components and circuitry that facilitates the rapid 
and efficient transmission of signals between dif-
ferent parts of the computer. 
communication devices: devices that allow drones 
to communicate with users or engineers in remote 
locations. 
compliance: adherence to standards or specifications 
established by an official body to govern a particular 
industry, product, or activity. 
component-based development: an approach to soft-
ware design that uses standardized software compo-
nents to create new applications and new software. 
compressed data: data that has been encoded such 
that storing or transferring the data requires fewer 
bits of information. 
computational linguistics: a branch of linguistics that 
uses computer science to analyze and model lan-
guage and speech. 
Computer Fraud and Abuse Act (CFAA): a 1986 leg-
islative amendment that made accessing a protected 
computer without authorization, or exceeding one’s 
authorized level of access, a federal offense. 
computer technician: a professional tasked with the 
installation, repair, and maintenance of computers 
and related technology. 
constraints: limitations on values in computer pro-
gramming that collectively identify the solutions to 
be produced by a programming problem. 
context switch: a multitasking operating system 
shifting from one task to another; for example, 
after formatting a print job for one user, the com-
puter might switch to resizing a graphic for an-
other user. 
context switching: pausing and recording the prog-
ress of a thread or process such that the process or 
thread can be executed at a later time. 
control characters: units of information used to con-
trol the manner in which computers and other de-
vices process text and other characters. 
control unit design: describes the part of the CPU 
that tells the computer how to perform the instruc-
tions sent to it by a program. 
converter: a device that expands a system’s range of 
reception by bringing in and adapting signals that 
the system was not originally designed to process. 
cookies: small data files that allow websites to track 
users. 
cooperative multitasking: an implementation of mul-
titasking in which the operating system will not ini-
tiate a context switch while a process is running in 
order to allow the process to complete. 
core voltage: the amount of power delivered to 
the processing unit of a computer from the power 
supply. 
counter: a digital sequential logic gate that records 
how many times a certain event occurs in a given 
amount of time. 
coupling: the degree to which different parts of a 
program are dependent upon one another. 

319
Principles of Computer Science
Glossary
cracker: a criminal hacker; one who finds and 
exploits weak points in a computer’s security 
system to gain unauthorized access for malicious 
purposes. 
crippleware: software programs in which key features 
have been disabled and can only be activated after 
registration or with the use of a product key. 
crosstalk: interference of the signals on one circuit 
with the signals on another, caused by the two circuits 
being too close together. 
cybercrime: crime that involves targeting a computer 
or using a computer or computer network to commit 
a crime, such as computer hacking, digital piracy, 
and the use of malware or spyware. 
data granularity: the level of detail with which data is 
collected and recorded. 
data integrity: the degree to which collected data is 
and will remain accurate and consistent. 
data source: the origin of the information used in a 
computer model or simulation, such as a database or 
spreadsheet. 
data width: a measure of the amount of data that can 
be transmitted at one time through the computer 
bus, the specific circuits and wires that carry data 
from one part of a computer to another. 
datapath design: describes how data flows through 
the CPU and at what points instructions will be de-
coded and executed. 
declarative language: language that specifies the 
result desired but not the sequence of operations 
needed to achieve the desired result. 
deep learning: an emerging field of artificial intelli-
gence research that uses neural network algorithms 
to improve machine learning. 
delta debugging: an automated method of debug-
ging intended to identify a bug’s root cause while 
eliminating irrelevant information. 
deterministic algorithm: an algorithm that when 
given a particular input will always produce the same 
output. 
device: equipment designed to perform a specific 
function when attached to a computer, such as a 
scanner, printer, or projector. 
device fingerprinting: information that uniquely 
identifies a particular computer, component, or 
piece of software installed on the computer. This can 
be used to find out precisely which device accessed a 
particular online resource. 
dexterity: finesse; skill at performing delicate or pre-
cise tasks. 
digital commerce: the purchase and sale of goods 
and services via online vendors or information tech-
nology systems. 
digital legacy (digital remains): the online accounts 
and information left behind by a deceased person. 
digital literacy: familiarity with the skills, behaviors, 
and language specific to using digital devices to ac-
cess, create, and share content through the Internet. 
digital native: an individual born during the dig-
ital age or raised using digital technology and 
communication. 
direct manipulation interfaces: computer interaction 
format that allows users to directly manipulate graph-
ical objects or physical shapes that are automatically 
translated into coding. 
direct-access storage: a type of data storage in which 
the data has a dedicated address and location on 
the storage device, allowing it to be accessed directly 
rather than sequentially. 
directed energy deposition: a process that deposits 
wire or powdered material onto an object and then 
melts it using a laser, electron beam, or plasma arc. 
distributed algorithm: an algorithm designed to run 
across multiple processing centers and so is capable 

320
Glossary
Principles of Computer Science
of directing a concentrated action between several 
computer systems. 
domain: the range of values that a variable may take 
on, such as any even number or all values less than 
−23.7. 
domain-dependent complexity: a complexity that re-
sults from factors specific to the context in which the 
computational problem is set. 
DRAKON chart: a flowchart used to model algo-
rithms and programmed in the hybrid DRAKON 
computer language. 
dual-mesh network: a type of mesh network in which 
all nodes are connected both through wiring and 
wirelessly, thus increasing the reliability of the system. 
A node is any communication intersection or end-
point in the network (e.g., computer or terminal). 
dynamic balance: the ability to maintain balance 
while in motion. 
dynamic random-access memory (DRAM): a form 
of RAM in which the device’s memory must be re-
freshed on a regular basis, or else the data it contains 
will disappear. 
dynamic testing: a testing technique in which soft-
ware input is tested and output is analyzed by exe-
cuting the code. 
edit decision list (EDL): a list that catalogs the reel or 
time code data of video frames so that the frames can 
be accessed during video editing. 
electromagnetic spectrum: the complete range of 
electromagnetic radiation, from the longest wave-
length and lowest frequency (radio waves) to the 
shortest wavelength and highest frequency (gamma 
rays). 
Electronic Communications Privacy Act: a 1986 law 
that extended restrictions on wiretapping to cover 
the retrieval or interception of information trans-
mitted electronically between computers or through 
computer networks. 
electronic interference: the disturbance generated by 
a source of electrical signal that affects an electrical 
circuit, causing the circuit to degrade or malfunc-
tion. Examples include noise, electrostatic discharge, 
and near-field and far-field interference. 
embedded systems: computer systems that are incor-
porated into larger devices or systems to monitor per-
formance or to regulate system functions. 
entanglement: the phenomenon in which two or 
more particles’ quantum states remain linked even if 
the particles are later separated and become part of 
distinct systems. 
enumerative combinatorics: a branch of combina-
torics that studies the number of ways that certain 
patterns can be formed using a set of objects. 
Environmental Protection Agency (EPA): US govern-
ment agency tasked with combating environmental 
pollution. 
e-waste: short for “electronic waste”; computers and 
other digital devices that have been discarded by 
their owners. 
false match rate: the probability that a biometric 
system incorrectly matches an input to a template 
contained within a database. 
fault detection: the monitoring of a system in order 
to identify when a fault occurs in its operation. 
fiber: a small thread of execution using cooperative 
multitasking. 
field programmable gate array: an integrated circuit 
that can be programmed in the field and can there-
fore allow engineers or users to alter a machine’s pro-
gramming without returning it to the manufacturer. 
filter: in signal processing, a device or procedure that 
takes in a signal, removes certain unwanted elements, 
and outputs a processed signal. 
firewall: a virtual barrier that filters traffic as it enters 
and leaves the internal network, protecting internal 
resources from attack by external sources. 

321
Principles of Computer Science
Glossary
fixed point: a type of digital signal processing in 
which numerical data is stored and represented with 
a fixed number of digits after (and sometimes be-
fore) the decimal point, using a minimum of sixteen 
bits. 
fixed-point arithmetic: a calculation involving num-
bers that have a defined number of digits before and 
after the decimal point. 
flash memory: nonvolatile computer memory that 
can be erased or overwritten solely through elec-
tronic signals, i.e. without physical manipulation of 
the device. 
flashing: a process by which the flash memory on 
a motherboard or an embedded system is updated 
with a newer version of software. 
floating point: a type of digital signal processing in 
which numerical data is stored and represented in 
the form of a number (called the mantissa, or signifi-
cand) multiplied by a base number (such as base-2) 
raised to an exponent, using a minimum of thirty-two 
bits. 
floating-point arithmetic: a calculation involving 
numbers that have a decimal point that can be placed 
anywhere through the use of exponents, as is done in 
scientific notation. 
flooding: sending information to every other node 
in a network to get the data to its appropriate 
destination. 
force-sensing touch technology: touch display that 
can sense the location of the touch as well as the 
amount of pressure the user applies, allowing for a 
wider variety of system responses to the input. 
four-dimensional building information modeling 
(4-D BIM): the process of creating a 3-D model that 
incorporates time-related information to guide the 
manufacturing process. 
Fourier transform: a mathematical operator that de-
composes a single function into the sum of multiple 
sinusoidal (wave) functions. 
frames per second (FPS): a measurement of the rate 
at which individual video frames are displayed. 
free software: software developed by programmers 
for their own use or for public use and distributed 
without charge; it usually has conditions attached 
that prevent others from acquiring it and then selling 
it for their own profit. 
function: instructions read by a computer’s processor 
to execute specific events or operations. 
functional programming: a theoretical approach to 
programming in which the emphasis is on applying 
mathematics to evaluate functional relationships that 
are, for the most part, static. 
functional requirement: a specific function of a com-
puter system, such as calculating or data processing. 
game loop: the main part of a game program that al-
lows the game’s physics, artificial intelligence, and 
graphics to continue to run with or without user 
input. 
gateway: a device capable of joining one network to 
another that has different protocols. 
genetic modification: direct manipulation of an 
organism’s genome, often for the purpose of engi-
neering useful microbes or correcting for genetic 
disease. 
genome-wide association study: a type of genetic 
study that compares the complete genomes of in-
dividuals within a population to find which genetic 
markers, if any, are associated with various traits, 
most often diseases or other health problems. 
gestures: combinations of finger movements used to 
interact with multitouch displays in order to accom-
plish various tasks. Examples include tapping the 
finger on the screen, double-tapping, and swiping 
the finger along the screen. 
glyph: a specific representation of a grapheme, such 
as the letter A rendered in a particular typeface. 

322
Glossary
Principles of Computer Science
graph theory: the study of graphs, which are diagrams 
used to model relationships between objects. 
grapheme: the smallest unit used by a writing system, 
such as alphabetic letters, punctuation marks, or 
Chinese characters. 
graphical user interface (GUI): an interface that al-
lows users to control a computer or other device by 
interacting with graphical elements such as icons and 
windows. 
graphical user interface (GUI): an interface that 
allows a user to interact with pictures instead of re-
quiring the user to type commands into a text-only 
interface. 
Green Electronics Council: a US nonprofit organiza-
tion dedicated to promoting green electronics. 
hacking: the use of technical skill to gain unauthor-
ized access to a computer system; also, any kind of 
advanced tinkering with computers to increase their 
utility. 
hamming distance: a measurement of the difference 
between two characters or control characters that ef-
fects character processing, error detection, and error 
correction. 
hardware: the physical parts that make up a com-
puter. These include the motherboard and processor, 
as well as input and output devices such as monitors, 
keyboards, and mice. 
hardware interruption: a device attached to a com-
puter sending a message to the operating system to 
inform it that the device needs attention, thereby “in-
terrupting” the other tasks that the operating system 
was performing. 
Harvard architecture: a computer design that has 
physically distinct storage locations and signal routes 
for data and for instructions. 
hash function: an algorithm that converts a string 
of characters into a different, usually smaller, fixed-
length string of characters that is ideally impossible 
either to invert or to replicate. 
hashing algorithm: a computing function that con-
verts a string of characters into a different, usually 
smaller string of characters of a given length, which is 
ideally impossible to replicate without knowing both 
the original data and the algorithm used. 
Health Insurance Portability and Accountability Act 
(HIPAA): a 1996 law that established national stan-
dards for protecting individuals’ medical records and 
other personal health information. 
heavy metal: one of several toxic natural substances 
often used as components in electronic devices. 
heterogeneous scalability: the ability of a multipro-
cessor system to scale up using parts from different 
vendors. 
hexadecimal: a base-16 number system that uses the 
digits 0 through 9 and the letters A, B, C, D, E, and F 
as symbols to represent numbers. 
hibernation: a power-saving state in which a computer 
shuts down but retains the contents of its random-
access memory. 
hidden Markov model: a type of model used to rep-
resent a dynamic system in which each state can only 
be partially seen. 
hierarchical file system: a directory with a treelike 
structure in which files are organized. 
high-fidelity wireframe: an image or series of images 
that represents the visual elements of the website in 
great detail, as close to the final product as possible, 
but still does not permit user interaction. 
homebrew: software that is developed for a device or 
platform by individuals not affiliated with the device 
manufacturer; it is an unofficial or “homemade” ver-
sion of the software that is developed to provide ad-
ditional functionality not included or not permitted 
by the manufacturer. 
hopping: the jumping of a data packet from one de-
vice or node to another as it moves across the net-
work. Most transmissions require each packet to 
make multiple hops. 

323
Principles of Computer Science
Glossary
horizontal scaling: the addition of nodes (e.g., com-
puters or servers) to a distributed system. 
host-based firewalls: firewalls that protect a specific 
device, such as a server or personal computer, rather 
than the network as a whole. 
HTML editor: a computer program for editing 
web pages encoded in hypertext markup language 
(HTML). 
Human Brain Project: a project launched in 2013 in 
an effort at modeling a functioning brain by 2023; 
also known as HBP. 
humanoid: resembling a human. 
hybrid cloud: a cloud computing model that com-
bines public cloud services with a private cloud plat-
form linked through an encrypted connection. 
identifiers: measurable characteristics used to iden-
tify individuals. 
imitation game: Alan Turing’s name for his proposed 
test, in which a machine would attempt to respond to 
questions in such a way as to fool a human judge into 
thinking it was human. 
immersive mode: a full-screen mode in which the 
status and navigation bars are hidden from view 
when not in use. 
imperative language: language that instructs a com-
puter to perform a particular sequence of operations. 
imperative programming: programming that pro-
duces code that consists largely of commands issued 
to the computer, instructing it to perform specific 
actions. 
in-circuit emulator: a device that enables the debug-
ging of a computer system embedded within a larger 
system. 
information hierarchy: the relative importance of the 
information presented on a web page. 
information technology: the use of computers and 
related equipment for the purpose of processing and 
storing data. 
infrastructure as a service: a cloud computing plat-
form that provides additional computing resources 
by linking hardware systems through the Internet; 
also called “hardware as a service.” 
inheritance: a technique that reuses and repurposes 
sections of code. 
Initiative for Software Choice (ISC): a consortium of 
technology companies founded by CompTIA, with 
the goal of encouraging governments to allow com-
petition among software manufacturers. 
input/output instructions: instructions used by the 
central processing unit (CPU) of a computer when 
information is transferred between the CPU and a 
device such as a hard disk. 
integration testing: a process in which multiple units 
are tested individually and when working in concert. 
interaction design: the practice of designing a user 
interface, such as a website, with a focus on how to 
facilitate the user’s experience. 
interface: the function performed by the device 
driver, which mediates between the hardware of the 
peripheral and the hardware of the computer. 
interface metaphors: linking computer commands, 
actions, and processes with real-world actions, pro-
cesses, or objects that have functional similarities. 
interference: anything that disrupts a signal as it 
moves from source to receiver. 
interferometry: a technique for studying biochem-
ical substances by superimposing light waves, typi-
cally one reflected from the substance and one re-
flected from a reference point, and analyzing the 
interference. 
internal-use software: software developed by a 
company for its own use to support general and 

324
Glossary
Principles of Computer Science
administrative functions, such as payroll, accounting, 
or personnel management and maintenance. 
Internet of things: a wireless network connecting de-
vices, buildings, vehicles, and other items with net-
work connectivity. 
interpolation: a process of estimating intermediate 
values when nearby values are known; used in image 
editing to “fill in” gaps by referring to numerical data 
associated with nearby points. 
interrupt vector table: a chart that lists the addresses 
of interrupt handlers. 
intrusion detection system: a system that uses hard-
ware, software, or both to monitor a computer or net-
work in order to determine when someone attempts 
to access the system without authorization. 
inverter: a logic gate whose output is the inverse of 
the input; also called a NOT gate. 
iOS: Apple’s proprietary mobile operating system, 
installed on Apple devices such as the iPhone, iPad, 
and iPod touch. 
IRL relationships: relationships that occur “in real 
life,” meaning that the relationships are developed 
or sustained outside of digital communication. 
jailbreak: the removal of restrictions placed on a mo-
bile operating system to give the user greater control 
over the mobile device. 
jailbreaking: the process of removing software restric-
tions within iOS that prevent a device from running 
certain kinds of software. 
JavaScript: a flexible programming language that is 
commonly used in website design. 
jQuery: a free, open-source JavaScript library. 
kernel: the central component of an operating 
system. 
keyframing: a part of the computer animation pro-
cess that shows, usually in the form of a drawing, the 
position and appearance of an object at the begin-
ning of a sequence and at the end. 
language interface packs: programs that translate 
interface elements such as menus and dialog boxes 
into different languages. 
learner-controlled program: software that allows a 
student to set the pace of instruction, choose which 
content areas to focus on, decide which areas to ex-
plore when, or determine the medium or difficulty 
level of instruction; also known as a “student-con-
trolled program.” 
learning strategy: a specific method for acquiring 
and retaining a particular type of knowledge, such 
as memorizing a list of concepts by setting the list to 
music. 
learning style: an individual’s preferred approach 
to acquiring knowledge, such as by viewing visual 
stimuli, reading, listening, or using one’s hands to 
practice what is being taught. 
lexicon: the total vocabulary of a person, language, 
or field of study. 
linear predictive coding: a popular tool for digital 
speech processing that uses both past speech samples 
and a mathematical approximation of a human vocal 
tract to predict and then eliminate certain vocal fre-
quencies that do not contribute to meaning. This al-
lows speech to be processed at a faster bit rate without 
significant loss in comprehensibility. 
livelock: a situation in which two or more processes 
constantly change their state in response to one an-
other in such a way that neither can complete. 
local area network (LAN): a network that connects 
electronic devices within a limited physical area. 
logic implementation: the way in which a CPU is de-
signed to use the open or closed state of combina-
tions of circuits to represent information. 
logical copy: a copy of a hard drive or disk that cap-
tures active data and files in a different configuration 
from the original, usually excluding free space and 

325
Principles of Computer Science
Glossary
artifacts such as file remnants; contrasts with a phys-
ical copy, which is an exact copy with the same size 
and configuration as the original. 
logotype: a company or brand name rendered 
in a unique, distinct style and font; also called a 
“wordmark.” 
lossless compression: data compression that allows 
the original data to be compressed and reconstructed 
without any loss of accuracy. 
lossy compression: a method of reducing the size of 
an audio file while sacrificing some of the quality of 
the original file. 
lossy compression: a method of decreasing image file 
size by discarding some data, resulting in some image 
quality being irreversibly sacrificed. 
low-energy connectivity IC: an integrated circuit that 
enables wireless Bluetooth connectivity while using 
little power. 
LZW compression: a type of lossless compression that 
uses a table-based algorithm. 
magnetic storage: a device that stores information by 
magnetizing certain types of material. 
main loop: the overarching process being carried 
out by a computer program, which may then invoke 
subprocesses. 
main memory: the primary memory system of a com-
puter, often called “random access memory” (RAM), 
accessed by the computer’s central processing unit 
(CPU). 
mastering: the creation of a master recording that 
can be used to make other copies for distribution. 
Material Design: a comprehensive guide for visual, 
motion, and interaction design across Google plat-
forms and devices. 
material extrusion: a process in which heated fila-
ment is extruded through a nozzle and deposited in 
layers, usually around a removable support. 
material jetting: a process in which drops of liquid 
photopolymer are deposited through a printer head 
and heated to form a dry, stable solid. 
Medical Device Innovation Consortium: a nonprofit 
organization established to work with the US Food 
and Drug Administration on behalf of medical de-
vice manufacturers to ensure that these devices are 
both safe and effective. 
medical imaging: the use of devices to scan a patient’s 
body and create images of the body’s internal struc-
tures to aid in diagnosis and treatment planning. 
memory dumps: computer memory records from 
when a particular program crashed, used to pinpoint 
and address the bug that caused the crash. 
memristor: a memory resistor, a circuit that can 
change its own electrical resistance based on the re-
sistance it has used in the past and can respond to 
familiar phenomena in a consistent way. 
mesh network: a type of network in which each node 
relays signal through the network. A node is any com-
munication intersection or endpoint in the network 
(e.g., computer or terminal). 
meta-complexity: a complexity that arises when the 
computational analysis of a problem is compounded 
by the complex nature of the problem itself. 
metadata: data that contains information about other 
data, such as author information, organizational in-
formation, or how and when the data was created. 
method: a procedure that describes the behavior of 
an object and its interactions with other objects. 
microcontroller: a tiny computer in which all of the 
essential parts of a computer are united on a single 
microchip—input and output channels, memory, 
and a processor. 
microcontroller: an integrated circuit that contains a 
very small computer. 
micron: a unit of measurement equaling one mil-
lionth of a meter; typically used to measure the width 

326
Glossary
Principles of Computer Science
of a core in an optical figure or the line width on a 
microchip. 
microwaves: electromagnetic radiation with a fre-
quency higher than that of radio wave but lower than 
that of visible light. 
middle computing: computing that occurs at the ap-
plication tier and involves intensive processing of 
data that will subsequently be presented to the user 
or another, intervening application. 
million instructions per second (MIPS): a unit of 
measurement used to evaluate computer perfor-
mance or the cost of computing resources. 
mixer: a component that converts random input 
radio frequency (RF) signal into a known, fixed fre-
quency to make processing the signal easier. 
mixing: the process of combining different sounds 
into a single audio recording. 
modeling: the process of creating a 2-D or 3-D repre-
sentation of the structure being designed. 
modulator: a device used to regulate or adjust some 
aspect of an electromagnetic wave. 
morphology: a branch of linguistics that studies the 
forms of words. 
multi-agent system: a system consisting of multiple 
separate agents, either software or hardware systems, 
that can cooperate and organize to solve problems. 
multibit watermarking: a watermarking process 
that embeds multiple bits of data in the signal to be 
transmitted. 
multicast: a network communications protocol in 
which a transmission is broadcast to multiple recipi-
ents rather than to a single receiver. 
multimodal monitoring: the monitoring of several 
physical parameters at once in order to better eval-
uate a patient’s overall condition, as well as how dif-
ferent parameters affect one another or respond to a 
given treatment. 
multiplexing: combining multiple data signals into 
one in order to transmit all signals simultaneously 
through the same medium. 
multiplier-accumulator: a piece of computer hard-
ware that performs the mathematical operation of 
multiplying two numbers and then adding the result 
to an accumulator. 
multiprocessing: the use of more than one central 
processing unit to handle system tasks; this requires 
an operating system capable of dividing tasks be-
tween multiple processors. 
multitasking: in the mobile phone environment, al-
lowing different apps to run concurrently, much like 
the ability to work in multiple open windows on a PC. 
multitasking: in computing, the process of executing 
multiple tasks concurrently on an operating system 
(OS). 
multitenancy: a software program that allows mul-
tiple users to access and use the software from dif-
ferent locations. 
multi-terminal configuration: a computer configura-
tion in which several terminals are connected to a 
single computer, allowing more than one person to 
use the computer. 
multitouch gestures: combinations of finger move-
ments used to interact with touch-screen or other 
touch-sensitive displays in order to accomplish var-
ious tasks. Examples include double-tapping and 
swiping the finger along the screen. 
multiuser: capable of being accessed or used by more 
than one user at once. 
mutual exclusion: a rule present in some database 
systems that prevents a resource from being accessed 
by more than one operation at a time; one of the nec-
essary conditions for deadlock. 
near-field communication: a method by which two 
devices can communicate wirelessly when in close 
proximity to one another. 

327
Principles of Computer Science
Glossary
near-field communications antenna: an antenna that 
enables a device to communicate wirelessly with a 
nearby compatible device. 
negative-AND (NAND) gate: a logic gate that pro-
duces a false output only when both inputs are true 
nervous (neural) system: the system of nerve path-
ways by which an organism senses changes in itself 
and its environment and transmits electrochemical 
signals describing these changes to the brain so that 
the brain can respond. 
network: two or more computers being linked in a 
way that allows them to transmit information back 
and forth. 
network firewalls: firewalls that protect an entire net-
work rather than a specific device. 
networking: the use of physical or wireless connec-
tions to link together different computers and com-
puter networks so that they can communicate with 
one another and collaborate on computationally in-
tensive tasks. 
neural network: in computing, a model of informa-
tion processing based on the structure and function 
of biological neural networks such as the human 
brain. 
neuroplasticity: the capacity of the brain to change 
as it acquires new information and forms new neural 
connections. 
nibble: a group of four bits. 
node: any point on a computer network where com-
munication pathways intersect, are redistributed, or 
end (i.e., at a computer, terminal, or other device). 
noise: interferences or irregular fluctuations af-
fecting electrical signals during transmission. 
noise-tolerant signal: a signal that can be easily distin-
guished from unwanted signal interruptions or fluc-
tuations (i.e., noise). 
nondestructive editing: a mode of image editing in 
which the original content of the image is not de-
stroyed because the edits are made only in the ed-
iting software. 
nonfunctional requirement: also called extrafunc-
tional requirement; an attribute of a computer 
system, such as reliability, scalability, testability, secu-
rity, or usability, that reflects the operational perfor-
mance of the system. 
nongraphical: not featuring graphical elements. 
nonlinear editing: a method of editing video in which 
each frame of video can be accessed, altered, moved, 
copied, or deleted regardless of the order in which 
the frames were originally recorded. 
nonvolatile memory: computer storage that retains 
its contents after power to the system is cut off, rather 
than memory that is erased at system shutdown. 
nonvolatile random-access memory (NVRAM): a 
form of RAM in which data is retained even when the 
device loses access to power. 
normalization: a process that ensures that different 
code points representing equivalent characters will 
be recognized as equal when processing text. 
nuclear magnetic resonance (NMR) spectroscopy: 
a technique for studying the properties of atoms or 
molecules by applying an external magnetic field to 
atomic nuclei and analyzing the resulting difference 
in energy levels. 
object: an element with a unique identity and a de-
fined set of attributes and behaviors. 
object-oriented programming: a type of program-
ming in which the source code is organized into ob-
jects, which are elements with a unique identity that 
have a defined set of attributes and behaviors. 
object-oriented user interface: an interface that al-
lows users to interact with onscreen objects as they 
would in real-world situations, rather than selecting 
objects that are changed through a separate control 
panel interface. 

328
Glossary
Principles of Computer Science
open-source software: software that makes its source 
code available to the public for free use, study, modi-
fication, and distribution. 
operating system (OS): a specialized program that 
manages a computer’s functions. 
operating system shell: a user interface used to access 
and control the functions of an operating system. 
optical storage: storage of data by creating marks in 
a medium that can be read with the aid of a laser or 
other light source. 
optical touchscreens: touchscreens that use optical 
sensors to locate the point where the user touches be-
fore physical contact with the screen has been made. 
packet filters: filters that allow data packets to enter a 
network or block them on an individual basis. 
packet forwarding: the transfer of a packet, or unit 
of data, from one network node to another until it 
reaches its destination. 
packet sniffers: a program that can intercept data or 
information as it moves through a network. 
packet switching: a method of transmitting data 
over a network by breaking it up into units called 
packets, which are sent from node to node along the 
network until they reach their destination and are 
reassembled. 
parallel processing: the division of a task among sev-
eral processors working simultaneously, so that the 
task is completed more quickly. 
parameter: a measurable element of a system that af-
fects the relationships between variables in the system. 
PATRIOT Act: a 2001 law that expanded the powers 
of federal agencies to conduct surveillance and in-
tercept digital information for the purpose of investi-
gating or preventing terrorism. 
pedagogy: a philosophy of teaching that addresses 
the purpose of instruction and the methods by which 
it can be achieved. 
peer-to-peer (P2P) network: a network in which all 
computers participate equally and share coordina-
tion of network operations, as opposed to a client-
server network, in which only certain computers co-
ordinate network operations. 
pen/trap: short for pen register or trap-and-trace de-
vice, devices used to record either all numbers called 
from a particular telephone (pen register) or all 
numbers making incoming calls to that phone (trap 
and trace); also refers to the court order that permits 
the use of such devices. 
peripheral: a device that is connected to a computer 
and used by the computer but is not part of the com-
puter, such as a printer, scanner, external storage de-
vice, and so forth. 
personal area network (PAN): a network generated 
for personal use that allows several devices to connect 
to one another and, in some cases, to other networks 
or to the Internet. 
personally identifiable information (PII): informa-
tion that can be used to identify a specific individual. 
pharmacogenomics: the study of how an individual’s 
genome influences his or her response to drugs. 
phased software development: an approach to 
software development in which most testing oc-
curs after the system requirements have been 
implemented. 
phishing: the use of online communications in order 
to trick a person into sharing sensitive personal infor-
mation, such as credit card numbers or social security 
numbers. 
Phonebloks: a concept devised by Dutch designer 
Dave Hakkens for a modular mobile phone intended 
to reduce electronic waste. 
phoneme: a sound in a specified language or dialect 
that is used to compose words or to distinguish be-
tween words. 
pipelined architecture: a computer design where dif-
ferent processing elements are connected in a series, 

329
Principles of Computer Science
Glossary
with the output of one operation being the input of 
the next. 
piracy: in the digital context, unauthorized repro-
duction or use of copyrighted media in digital form. 
planned obsolescence: a design concept in which 
consumer products are given an artificially limited 
lifespan, therefore creating a perpetual market. 
platform: the specific hardware or software infra-
structure that underlies a computer system; often 
refers to an operating system, such as Windows, Mac 
OS, or Linux. 
platform: the underlying computing system on which 
applications can run, which may be the computer’s 
hardware or its operating system. 
platform as a service: a category of cloud computing 
that provides a virtual machine for users to develop, 
run, and manage web applications. 
plug-in: an application that is easily installed (plugged 
in) to add a function to a computer system. 
polymerase chain reaction (PCR) machine: a ma-
chine that uses polymerase chain reaction to amplify 
segments of DNA for analysis; also called a thermal 
cycler. 
polymorphism: the ability to maintain the same 
method name across subclasses even when the 
method functions differently depending on its class. 
port scanning: the use of software to probe a com-
puter server to see if any of its communication ports 
have been left open or vulnerable to an unauthorized 
connection that could be used to gain control of the 
computer. 
portlet: an independently developed software com-
ponent that can be plugged into a website to gen-
erate and link to external content. 
postproduction: the period after a model has been 
designed and an image has been rendered, when 
the architect may manipulate the created image by 
adding effects or making other aesthetic changes. 
powder bed fusion: the use of a laser to heat layers of 
powdered material in a movable powder bed. 
precoding: a technique that uses the diversity of a 
transmission by weighting an information channel. 
preemptive multitasking: an implementation of mul-
titasking in which the operating system will initiate 
a context switch while a process is running, usually 
on a schedule so that switches between tasks occur at 
precise time intervals. 
Pretty Good Privacy: a data encryption program 
created in 1991 that provides both encryption and 
authentication. 
principle of least privilege: a philosophy of com-
puter security that mandates users of a computer or 
network be given, by default, the lowest level of privi-
leges that will allow them to perform their jobs. This 
way, if a user’s account is compromised, only a lim-
ited amount of data will be vulnerable. 
printable characters: characters that can be written, 
printed, or displayed in a manner that can be read 
by a human. 
printed circuit board: a flat copper sheet shielded 
by fiberglass insulation in which numerous lines 
have been etched and holes have been punched, 
allowing various electronic components to be con-
nected and to communicate with one another and 
with external components via the exposed copper 
traces. 
printed circuit board (PCB): a component of elec-
tronic devices that houses and connects many smaller 
components. 
Privacy Incorporated Software Agents (PISA): a 
project that sought to identify and resolve privacy 
problems related to intelligent software agents. 
process: the execution of instructions in a computer 
program. 
processor coupling: the linking of multiple proces-
sors within a computer so that they can work together 
to perform calculations more rapidly. This can be 

330
Glossary
Principles of Computer Science
characterized as loose or tight, depending on the de-
gree to which processors rely on one another. 
processor symmetry: multiple processors sharing ac-
cess to input and output devices on an equal basis 
and being controlled by a single operating system. 
programmable oscillator: an electronic device that 
fluctuates between two states that allows user modifi-
cations to determine mode of operation. 
programming languages: sets of terms and rules of 
syntax used by computer programmers to create in-
structions for computers to follow. This code is then 
compiled into binary instructions for a computer to 
execute. 
projected capacitive touch: technology that uses 
layers of glass etched with a grid of conductive mate-
rial that allows for the distortion of voltage flowing 
through the grid when the user touches the surface; 
this distortion is measured and used to determine 
the location of the touch. 
proprietary software: software owned by an indi-
vidual or company that places certain restrictions on 
its use, study, modification, or distribution and typi-
cally withholds its source code. 
protocol processor: a processor that acts in a 
secondary capacity to the CPU, relieving it from 
some of the work of managing communication 
protocols that are used to encode messages on the 
network. 
prototypal inheritance: a form of code reuse in which 
existing objects are cloned to serve as prototypes. 
prototype: an early version of software that is still 
under development, used to demonstrate what the 
finished product will look like and what features it 
will include. 
proxy: a dedicated computer server that functions as 
an intermediary between a user and another server. 
proxy server: a computer through which all traffic 
flows before reaching the user’s computer. 
pseudocode: a combination of a programming lan-
guage and a spoken language, such as English, that is 
used to outline a program’s code. 
public-key cryptography: a system of encryption that 
uses two keys, one public and one private, to encrypt 
and decrypt data. 
push technology: a communication protocol in which 
a messaging server notifies the recipient as soon as 
the server receives a message, instead of waiting for 
the user to check for new messages. 
quantum bit (qubit): a basic unit of quantum com-
putation that can exist in multiple states at the 
same time, and can therefore have multiple values 
simultaneously. 
quantum logic gate: a device that alters the behavior 
or state of a small number of qubits for the purpose 
of computation. 
radio waves: low-frequency electromagnetic ra-
diation, commonly used for communication and 
navigation. 
random access memory (RAM): memory that the 
computer can access very quickly, without regard to 
where in the storage media the relevant information 
is located. 
ransomware: malware that encrypts or blocks ac-
cess to certain files or programs and then asks users 
to pay to have the encryption or other restrictions 
removed. 
rapid prototyping: the process of creating physical 
prototype models that are then tested and evaluated. 
raster: a means of storing, displaying, and editing 
image data based on the use of individual pixels. 
read-only memory (ROM): type of nonvolatile data 
storage that is typically either impossible to erase and 
alter or requires special equipment to do so; it is usu-
ally used to store basic operating information needed 
by a computer. 

331
Principles of Computer Science
Glossary
real-time monitoring: a process that grants adminis-
trators access to metrics and usage data about a soft-
ware program or database in real time. 
real-time operating system: an operating system that 
is designed to respond to input within a set amount 
of time without delays caused by buffering or other 
processing backlogs. 
receiver: a device that reads a particular type of trans-
mission and translates it into audio or video. 
recursive: describes a method for problem solving 
that involves solving multiple smaller instances of the 
central problem. 
remote monitoring: a platform that reviews the activi-
ties on software or systems that are located off-site. 
render farm: a cluster of powerful computers that 
combine their efforts to render graphics for anima-
tion applications. 
rendering: the process of transforming one or more 
models into a single image; the production of a com-
puter image from a 2-D or 3-D computer model; the 
process of selecting and displaying glyphs.
resistive touchscreens: touchscreens that can locate 
the user’s touch because they are made of several 
layers of conductive material separated by small 
spaces; when the user touches the screen, the layers 
touch each other and complete a circuit. 
resource allocation: a system for dividing computing 
resources among multiple, competing requests so 
that each request is eventually fulfilled. 
resource distribution: the locations of resources avail-
able to a computing system through various software 
or hardware components or networked computer 
systems. 
resource holding: a situation in which one process is 
holding at least one resource and is requesting fur-
ther resources; one of the necessary conditions for 
deadlock. 
retriggerable single shot: a monostable multivibrator 
(MMV) electronic circuit that outputs a single pulse 
when triggered but can identify a new trigger during 
an output pulse, thus restarting its pulse time and ex-
tending its output. 
reversible data hiding: techniques used to conceal 
data that allow the original data to be recovered in its 
exact form with no loss of quality. 
RGB: a color model that uses red, green, and blue 
to form other colors through various combinations. 
routing: selecting the best path for a data packet to 
take in order to reach its destination on the network. 
Sarbanes-Oxley Act (SOX): a 2002 law that requires 
all business records, including electronic records and 
electronic messages, to be retained for a certain pe-
riod of time. 
scareware: malware that attempts to trick users into 
downloading or purchasing software or applications 
to address a computer problem. 
Scientific Working Group on Digital Evidence 
(SWGDE): an American association of various aca-
demic and professional organizations interested in 
the development of digital forensics systems, guide-
lines, techniques, and standards. 
screen-reading program: a computer program that 
converts text and graphics into a format accessible 
to visually impaired, blind, learning disabled, or il-
literate users. 
script: a group of written signs, such as Latin or 
Chinese characters, used to represent textual infor-
mation in a writing system. 
scrubbing: navigating through an audio recording 
repeatedly in order to locate a specific cue or word. 
search engine optimization (SEO): techniques used 
to increase the likelihood that a website will appear 
among the top results in certain search engines. 

332
Glossary
Principles of Computer Science
self-star properties: a list of component and system 
properties required for a computing system to be 
classified as an autonomic system. 
self-star properties: a list of component and system 
properties required for a computing system to be 
classified as an autonomic system. 
semantics: a branch of linguistics that studies the 
meanings of words and phrases. 
semiconductor intellectual property (SIP) block: a 
quantity of microchip layout design that is owned by 
a person or group; also known as an “IP core.” 
sensors: devices capable of detecting, measuring, or 
reacting to external physical properties. 
separation of concerns: a principle of software en-
gineering in which the designer separates the com-
puter program’s functions into discrete elements. 
shadow RAM: a form of RAM that copies code stored 
in read-only memory into RAM so that it can be ac-
cessed more quickly. 
sheet lamination: a process in which thin layered 
sheets of material are adhered or fused together and 
then extra material is removed with cutting imple-
ments or lasers. 
shell: an interface that allows a user to operate a com-
puter or other device. 
Short Message Service (SMS): the technology under-
lying text messaging used on cell phones. 
signal-to-noise ratio (SNR): the power ratio between 
meaningful information, referred to as “signal,” and 
background noise. 
silent monitoring: listening to the exchanges be-
tween an incoming caller and an employee, such as a 
customer service agent. 
simulation: a computer model executed by a com-
puter system. 
software: the sets of instructions that a computer 
follows in order to carry out tasks. Software may be 
stored on physical media, but the media is not the 
software. 
software as a service: a software service system in 
which software is stored at a provider’s data center 
and accessed by subscribers. 
software patches: updates to software that correct 
bugs or make other improvements. 
software-defined antennas: reconfigurable antennas 
that transmit or receive radio signals according to 
software instructions. 
solid modeling: the process of creating a 3-D repre-
sentation of a solid object. 
solid-state storage: computer memory that stores in-
formation in the form of electronic circuits, without 
the use of disks or other read/write equipment. 
sound card: a peripheral circuit board that enables 
a computer to accept audio input, convert signals 
between analog and digital formats, and produce 
sound. 
source code: the set of instructions written in a pro-
gramming language to create a program. 
speaker independent: describes speech software that 
can be used by any speaker of a given language. 
special character: a character such as a symbol, emoji, 
or control character. 
spyware: software installed on a computer that allows 
a third party to gain information about the computer 
user’s activity or the contents of the user’s hard drive. 
state: a technical term for all of the stored informa-
tion, and the configuration thereof, that a program 
or circuit can access at a given time; a complete de-
scription of a physical system at a specific point in 
time, including such factors as energy, momentum, 
position, and spin. 

333
Principles of Computer Science
Glossary
stateful filters: filters that assess the state of a connec-
tion and allow or disallow data transfers accordingly. 
static random-access memory (SRAM): a form of 
RAM in which the device’s memory does not need to 
be regularly refreshed but data will still be lost if the 
device loses power. 
static testing: a testing technique in which software is 
tested by analyzing the program and associated docu-
mentation, without executing its code. 
substitution cipher: a cipher that encodes a message 
by substituting one character for another. 
subtyping: a relation between data types where one 
type is based on another, but with some limitations 
imposed. 
supercomputer: an extremely powerful computer 
that far outpaces conventional desktop computers. 
superposition: the principle that two or more waves, 
including waves describing quantum states, can be 
combined to give rise to a new wave state with unique 
properties. This allows a qubit to potentially be in two 
states at once. 
surface capacitive technology: a glass screen coated 
with an electrically conductive film that draws cur-
rent across the screen when it is touched; the flow of 
current is measured in order to determine the loca-
tion of the touch. 
symmetric-key cryptography: a system of encryption 
that uses the same private key to encrypt and decrypt 
data. 
syntax: a branch of linguistics that studies how words 
and phrases are arranged in sentences to create 
meaning. 
syntax: rules that describe how to correctly structure 
the symbols that comprise a language. 
system: a set of interacting or interdependent com-
ponent parts that form a complex whole; a comput-
er’s combination of hardware and software resources 
that must be managed by the operating system.
system agility: the ability of a system to respond to 
changes in its environment or inputs without failing 
altogether. 
system identification: the study of a system’s inputs 
and outputs in order to develop a working model of 
it. 
system software: the basic software that manages the 
computer’s resources for use by hardware and other 
software. 
tableless web design: the use of style sheets rather 
than HTML tables to control the layout of a web page. 
TCO certification: a credential that affirms the sus-
tainability of computers and related devices. 
telecom equipment: hardware that is intended for 
use in telecommunications, such as cables, switches, 
and routers. 
telemedicine: health care provided from a distance 
using communications technology, such as video 
chats, networked medical equipment, smartphones, 
and so on. 
telemetry: automated communication process that 
allows a machine to identify its position relative to ex-
ternal environmental cues. 
temporal synchronization: the alignment of signals 
from multiple devices to a single time standard, so 
that, for example, two different devices that record 
the same event will show the event happening at the 
exact same time. 
terminals: a set of basic input devices, such as a key-
board, mouse, and monitor, that are used to con-
nect to a computer running a multi-user operating 
system. 
third-party data center: a data center service pro-
vided by a separate company that is responsible for 
maintaining its infrastructure. 
thread: the smallest part of a programmed sequence 
that can be managed by a scheduler in an OS. 

334
Glossary
Principles of Computer Science
time-sharing: a strategy used by multi-user operating 
systems to work on multiple user requests by switching 
between tasks in very small intervals of time. 
time-sharing: the use of a single computing resource 
by multiple users at the same time, made possible by 
the computer’s ability to switch rapidly between users 
and their needs. 
topology: the way a network is organized, including 
nodes and the links that connect nodes. 
toxgnostics: a subfield of personalized medicine and 
pharmacogenomics that is concerned with whether 
an individual patient is likely to suffer a toxic reaction 
to a specific medication. 
trace impedance: a measure of the inherent resis-
tance to electrical signals passing through the traces 
etched on a circuit board. 
transactional database: a database management system 
that allows a transaction— a sequence of operations to 
achieve a single, self-contained task—to be undone, or 
“rolled back,” if it fails to complete properly. 
transistor: a computing component generally made 
of silicon that can amplify electronic signals or work 
as a switch to direct electronic signals within a com-
puter system. 
transmission medium: the material through which a 
signal can travel. 
transmitter: a device that sends a signal through a 
medium to be picked up by a receiver. 
transparent monitoring: a system that enables em-
ployees to see everything that the managers moni-
toring them can see. 
transposition cipher: a cipher that encodes a message 
by changing the order of the characters within the 
message. 
trusted platform module (TPM): a standard used for 
designing cryptoprocessors, which are special chips 
that enable devices to translate plain text into cipher 
text and vice versa. 
tuning: the process of making minute adjustments 
to a computer’s settings in order to improve its 
performance. 
Turing complete: a programming language that can 
perform all possible computations. 
typography: the art and technique of arranging type 
to make language readable and appealing. 
ubiquitous computing: an approach to computing in 
which computing activity is not isolated in a desktop, 
laptop, or server, but can occur everywhere and at 
any time through the use of microprocessors em-
bedded in everyday devices. 
undervolting: reducing the voltage of a computer 
system’s central processing unit to decrease power 
usage. 
unmanned aerial vehicle (UAV): an aircraft that 
does not have a pilot onboard but typically operates 
through remote control, automated flight systems, or 
preprogrammed computer instructions. 
user-centered design: design based on a perceived 
understanding of user preferences, needs, tenden-
cies, and capabilities. 
utility program: a type of system software that per-
forms one or more routine functions, such as disk 
partitioning and maintenance, software installation 
and removal, or virus protection. 
variable: a symbol representing a quantity with no 
fixed value. 
vat photopolymerization: a process in which a laser 
hardens layers of light-sensitive material in a vat. 
vector: a means of storing, displaying, and editing 
image data based on the use of defined points and 
lines. 
vertical scaling: the addition of resources to a single 
node (e.g., a computer or server) in a system. 
vibrator: an electronic component that vibrates. 

335
Principles of Computer Science
Glossary
video scratching: a technique in which a video se-
quence is manipulated to match the rhythm of a 
piece of music. 
virtual device driver: a type of device driver used by 
the Windows operating system that handles com-
munications between emulated hardware and other 
devices. 
virtual memory: memory used when a computer con-
figures part of its physical storage (on a hard drive, 
for example) to be available for use as additional 
RAM. Information is copied from RAM and moved 
into virtual memory whenever memory resources are 
running low. 
virtual reality: the use of technology to create a sim-
ulated world into which a user may be immersed 
through visual and auditory input. 
vision mixing: the process of selecting and combining 
multiple video sources into a single video. 
visual programming: a form of programming that al-
lows a programmer to create a program by dragging 
and dropping visual elements with a mouse instead 
of having to type in text instructions. 
voice over Internet Protocol (VoIP): a set of param-
eters that make it possible for telephone calls to be 
transmitted digitally over the Internet, rather than as 
analog signals through telephone wires. 
volatile memory: memory that stores information 
in a computer only while the computer has power; 
when the computer shuts down or power is cut, the 
information is lost. 
wardriving: driving around with a device such as a 
laptop that can scan for wireless networks that may be 
vulnerable to hacking. 
web application: an application that is downloaded 
either wholly or in part from the Internet each time 
it is used. 
white-box testing: a testing technique that includes 
complete knowledge of an application’s coding and 
structure. 
widget: an independently developed segment of code 
that can be plugged into a website to display informa-
tion retrieved from other sites. 
widgets: small, self-contained applications that run 
continuously without being activated like a typical 
application. 
wireframe: a schematic or blueprint that represents 
the visual layout of a web page, without any interac-
tive elements. 
word prediction: a software feature that recognizes 
words that the user has typed previously and offers 
to automatically complete them each time the user 
begins typing. 
worm: a type of malware that can replicate itself and 
spread to other computers independently; unlike a 
computer virus, it does not have to be attached to a 
specific program. 
zombie computer: a computer that is connected to 
the Internet or a local network and has been com-
promised such that it can be used to launch malware 
or virus attacks against other computers on the same 
network. 


337
“3D Printing Processes: The Free Beginner’s Guide.” 
3D Printing Industry. 3D Printing Industry, 2015. 
Web. 6 Jan. 2016.
“About 
Additive 
Manufacturing.” 
Additive 
Manufacturing Research Group. Loughborough U, 
2015. Web. 6 Jan. 2016.
“About ANSI.” ANSI. American Natl. Standards Inst., 
2016. Web. 31 Jan. 2016.
“About Threads and Processes.” MSDN.Microsoft. 
Windows, n.d. Web. 15 Mar 2016.
Abramovich, Sergei, ed. Computers in Education. 2 
vols. New York: Nova, 2012. Print.
“A Catalog of Published Genome-Wide Association 
Studies.” National Human Genome Research Institute. 
Natl. Insts. of Health, 16 Sept. 2015. Web. 23 Dec. 
2015.
Adams, Ty, and Stephen A. Smith. Communication 
Shock: The Rhetoric of New Technology. Newcastle 
upon Tyne: Cambridge Scholars, 2015. Print.
Adee, Sally. “Thanks for the Memories.” IEEE 
Spectrum. IEEE, 1 May 2009. Web. 10 Mar. 2016.
Adelman, L. M. Molecular computation of solutions 
to combinatorial problems, Science 226, 1021-1024 
(1994).
Agrawal, Manindra, S. Barry Cooper, and Angsheng 
Li, eds. Theory and Applications of Models of 
Computation: 9th Annual Conference, TAMC 2012, 
Beijing, China, May 16–21, 2012. Berlin: Springer, 
2012. Print.
Agrawal, Manish. Business Data Communications. 
Hoboken: Wiley, 2011. Print.
“A History of Technology in the Architecture Office.” 
Architizer. Architizer, 23 Dec. 2014. Web. 31 Jan. 
2016.
“A History of Windows.” Microsoft. Microsoft, 2016. 
Web. 2 Jan. 2016.
Alcorn, Paul. “SMR (Shingled Magnetic Recording) 
101.” Tom’s IT Pro. Purch, 10 July 2015. Web. 7 Mar. 
2016.
Allanwood, Gavin, and Peter Beare. User Experience 
Design: Creating Designs Users Really Love. New York: 
Fairchild, 2014. Print.
Alton, Larry. “Email Security in 2016: What You Need 
to Know.” Inc. Mansueto Ventures, 18 Feb. 2016. 
21 Feb. 2016.
Amadeo, Ron. “The History of Android.” Ars Technica. 
Condé Nast, 15 June 2014. Web. 2 Jan. 2016.
Ambainis, Andris. “What Can We Do with a Quantum 
Computer?” Institute Letter Spring 2014: 6–7. 
Institute for Advanced Study. Web. 24 Mar. 2016.
Ambinder, Marc. “What’s Really Limiting Advances 
in Computer Tech.” Week. The Week, 2 Sept. 2014. 
Web. 4 Mar. 2016.
Ambrose, Gavin, Paul Harris, and Sally Stone. The 
Visual Dictionary of Architecture. Lausanne: AVA, 
2008. Print.
Amer. Standards Assn. American Standard Code for 
Information Interchange. Amer. Standards Assn., 17 
June 1963. Digital file.
“An Introduction to Public Key Cryptography and 
PGP.” Surveillance Self-Defense. Electronic Frontier 
Foundation, 7 Nov. 2014. Web. 4 Feb. 2016.
Anderson, Deborah. “Global Linguistic Diversity 
for the Internet.” Communications of the ACM Jan. 
2005: 27. PDF file.
Anderson, Gary H. Video Editing and Post Production: A 
Professional Guide. Woburn: Focal, 1999. Print.
Anderson, James A, Edward Rosenfeld, and Andras 
Pellionisz. Neurocomputing. Cambridge, Mass: MIT 
Press, 1990. Print.
Anderson, Nate. “CompTIA Backs Down; Past Certs 
Remain Valid for Life.” Ars Technica. Condé Nast, 
26 Jan. 2010. Web. 31 Jan. 2016.
Anderson, Thomas, and Michael Dahlin. Operating 
Systems: Principles and Practice. West Lake Hills: 
Recursive, 2014. Print.
Andrews, Jean. A+ Guide to Hardware: Managing, 
Maintaining, and Troubleshooting. 6th ed. Boston: 
Course Tech., 2014. Print.
Andrews, Jean. A+ Guide to Managing and Maintaining 
Your PC. 8th ed. Boston: Course Tech., 2014. Print.
“Android: A Visual History.” Verge. Vox Media, 7 Dec. 
2011. Web. 2 Jan. 2016.
“ANSI Accredits Four Personnel Certification 
Programs.” ANSI. American Natl. Standards Inst., 
8 Apr. 2008. Web. 31 Jan. 2016.
Anthes, 
Gary. 
“Back 
to 
Basics: 
Algorithms.” 
Computerworld. Computerworld, 24 Mar. 2008. 
Web. 19 Jan. 2016.
BIBLIOGRAPHY

338
Bibliography
Principles of Computer Science
Ao, Sio-Iong, and Len Gelman, eds. Electrical 
Engineering and Intelligent Systems. New York: 
Springer, 2013. Print.
“A Quantum Leap in Computing.” NOVA. WGBH/
PBS Online, 21 July 2011. Web. 24 Mar. 2016.
Aravind, Alex A., and Sibsankar Haldar. Operating 
Systems. Upper Saddle River: Pearson, 2010. Print.
Ascher, 
Steven. 
The 
Filmmaker’s 
Handbook: 
A 
Comprehensive Guide for the Digital Age. New York: 
Plume, 2012. Print.
“A Short History of US Internet Legislation: Privacy 
on the Internet.” ServInt. ServInt, 17 Sept. 2013. 
Web. 28 Feb. 2016.
“A Tutorial on Data Representation: Integers, 
Floating-Point Numbers, and Characters.” NTU.
edu. Nanyang Technological U, Jan. 2014. Web. 20 
Feb. 2016.
Australian National University. Binary Representation 
and Computer Arithmetic. Australian National U, 
n.d. Digital file. 
Aziz, Scott. “With Regulation Looming, It’s Time for 
Industry to Raise the Bar for Software Quality.” 
Wired. Condé Nast, 28 Aug. 2014. Web. 31 Mar. 
2016.
Badilescu, Simona, and Muthukumaran Packirisamy. 
BioMEMS: Science and Engineering Perspectives. Boca 
Raton: CRC, 2011. Print.
Bajarin, Tim. “Google Is at a Major Crossroads with 
Android and Chrome OS.” PCMag. Ziff Davis, 21 
Dec. 2015. Web. 4 Jan. 2016.
Bajo, Javier, et al., eds. Highlights of Practical Applications 
of Agents, Multi-Agent Systems, and Sustainability. 
Proc. of the International Workshops of PAAMS 
2015, June 3–4, 2015, Salamanca, Spain. Cham: 
Springer, 2015. Print.
Baldé, C. P., et al. E-waste Statistics: Guidelines on 
Classification, Reporting and Indicators, 2015. Bonn: 
United Nations U, 2015. United Nations University. 
Web. 9 Feb. 2016.
Ball, Phillip. “The Truth about the Turing Test.” 
Future. BBC, 24 July 2015. Web. 18 Dec. 2015.
Ballew, Joli, and Ann McIver McHoes. Operating 
Systems DeMYSTiFieD. New York: McGraw, 2012. 
Print.
Balovich, 
David. 
“Sarbanes-Oxley 
Document 
Retention and Best Practices.” Creditworthy News. 
3JM Company, 5 Sept. 2007. Web. 1 Apr. 2016.
Banga, Cameron, and Josh Weinhold. Essential Mobile 
Interaction Design: Perfecting Interface Design in Mobile 
Apps. Upper Saddle River: Addison-Wesley, 2014. 
Print.
Baptiste, Philippe, Claude Le Pape, and Wim Nuijten. 
Constraint-Based Scheduling: Applying Constraint 
Programming to Scheduling Problems. New York: 
Springer, 2013. Print.
Barski, Conrad. Land of Lisp. Learn to Program LISP 
One Game at a Time. San Francisco, CA: No Starch 
Press, 2011. Print.
Basagni, Stefano, et al., eds. Mobile Ad Hoc Networking: 
Cutting Edge Directions. 2nd ed. Hoboken: Wiley, 
2013. Print.
Bass, Len, Paul Clements, and Rick Kazman. Software 
Architecture in Practice. 3rd ed. Upper Saddle River: 
Addison, 2012. Print.
Batchelor, Bruce. Intelligent Image Processing in 
PROLOG. London, UK: Springer-Verlag, 1991. 
Print.
Beattie, Andrew. “Cloud Computing: Why the Buzz?” 
Techopedia. Techopedia, 30 Nov. 2011. Web. 21 Jan. 
2016.
Beeler, Robert A., How to Count: An Introduction to 
Combinatorics. New York: Springer, 2015. Print.
Bell, Tim, et al. “Algorithms.” Computer Science Field 
Guide. U of Canterbury, 3 Feb. 2015. Web. 19 Jan. 
2016.
Bell, Tom. Programming: A Primer; Coding for Beginners. 
London: Imperial Coll. P, 2016. Print.
Belton, Padraig. “Coding the Future: What Will the 
Future of Computing Look Like?” BBC News. BBC, 
15 May 2015. Web. 24 Feb. 2016.
Bembenik, Robert, Łukasz Skonieczny, Henryk 
Rybinn´ski, Marzena Kryszkiewicz, and Marek 
Niezgódka, eds. Intelligent Tools for Building a 
Scientific Information Platform: Advanced Architectures 
and Solutions. New York: Springer, 2013. Print.
Ben-Ari, M. Principles of Concurrent and Distributed 
Programming. 2nd ed. New York: Addison, 2006. 
Print.
Bergin, Michael S. “History of BIM.” Architecture 
Research Lab. Architecture Research Lab, 21 Aug. 
2011. Web. 31 Jan. 2016.
Berman, P. Gennady, Garu D. Doolen, Ronnie 
Mainieri, and Vladimir Tsifrinovich. (1998) 
Introduction to Quantum Computers. River Edge, NJ: 
World Scientific Publishing. Print.
Bernal, Paul. Internet Privacy Rights: Rights to Protect 
Autonomy. New York: Cambridge UP, 2014. 
Print.

339
Principles of Computer Science
Bibliography
Bernhardt, Chris. Turing’s Vision: The Birth of Computer 
Science. MIT Press, 2016.
Bernier, Samuel N., Bertier Luyt, Tatiana Reinhard, 
and Carl Bass. Design for 3D Printing: Scanning, 
Creating, Editing, Remixing, and Making in Three 
Dimensions. San Francisco: Maker Media, 2014. 
Print.
Berns, Andrew, and Sukumar Ghosh. “Dissecting Self-
Properties.” SASO 2009: Third IEEE International 
Conference on Self-Adaptive and Self-Organizing 
Systems. Los Alamitos: IEEE, 2009. 10–19. Andrew 
Berns: Homepage. Web. 20 Jan. 2016.
Berry, John D. Language Culture Type: International 
Type Design in the Age of Unicode. New York: Graphis, 
2002. Print.
Bessière, Pierre, et al. Bayesian Programming. Boca 
Raton: CRC, 2014. Print.
Bibby, Joe. “Robonaut: Home.” Robonaut. NASA, 31 
May 2013. Web. 21 Jan. 2016.
Biere, Armin, Amir Nahir, and Tanja Vos, eds. 
Hardware and Software: Verification and Testing. New 
York: Springer, 2013. Print.
Binh, 
Le 
Nguyen. 
Digital 
Processing: 
Optical 
Transmission and Coherent Receiving Techniques. Boca 
Raton: CRC, 2013. Print.
Biometric Center of Excellence. Federal Bureau of 
Investigation, 2016. Web. 21 Jan. 2016.
“Biomedical 
Engineers.” 
Occupational 
Outlook 
Handbook, 2016–2017 Edition. Bureau of Labor 
Statistics, US Dept. of Labor, 17 Dec. 2015. Web. 
23 Jan. 2016.
“Biotechnology.” ACS. Amer. Chemical Soc., n.d. 
Web. 27 Jan. 2016.
Bird, Steven, Ewan Klein, and Edward Loper. Natural 
Language Processing with Python, 2nd ed. O’Reilly 
Media, 2017.
Bishop, Owen (2011) Electronics. Circuits and Systems 
4th ed., New York, NY: E;sevier. Print.
Bishop, Todd. “Microsoft Exec Admits New Reality: 
Market Share No Longer 90%— It’s 14%.” 
Geekwire. GeekWire, 14 July 2014. Web. 30 Jan. 
2015.
Black, Jeremy. The Power of Knowledge: How Information 
and Technology Made the Modern World. New Haven: 
Yale UP, 2014. Print.
Blanco, Xiomara. “Top Tablets with Expandable 
Storage.” CNET. CBS Interactive, 25 Jan. 2016. 
Web. 7 Mar. 2016.
Boashash, Boualem, ed. Time-Frequency Signal Analysis 
and Processing: A Comprehensive Reference. 2nd ed. 
San Diego: Academic, 2016. Print.
Bone, Simon, and Matias Castro. “A Brief History 
of Quantum Computing.” SURPRISE May–June 
1997: n. pag. Department of Computing, Imperial 
College London. Web. 24 Mar. 2016.
Booch, Grady, et al. Object-Oriented Analysis and Design 
with Applications. 3rd ed. Upper Saddle River: 
Addison, 2007. Print.
“Book—Understanding Biometrics.” Griaule Biometrics. 
Griaule Biometrics, 2008. Web. 21 Jan. 2016.
Borkar, Shekhar, and Andrew A. Chien. “The Future 
of Microprocessors.” Communications of the ACM. 
ACM, May 2011. Web. 3 Mar. 2016.
Boyle, David. Alan Turing: Unlocking the Enigma. 
CreateSpace Independent Publishing Platform, 
2014.
Boyle, Randall, and Raymond R. Panko. Corporate 
Computer Security. 4th ed. Boston: Pearson, 2015. 
Print.4 Science Reference Center™ Computer 
Security
Bradley, Tony. “Experts Pick the Top 5 Security 
Threats for 2015.” PCWorld. IDG Consumer & 
SMB, 14 Jan. 2015. Web. 12 Mar. 2016.
Brandom, Russell. “Google Survey Finds More than 
Five Million Users Infected with Adware.” The 
Verge. Vox Media, 6 May 2015. Web. 12 Mar. 2016.
Brauer, Max. Logic Programming With Prolog. New 
York, NY: Springer, 2005. Print.
Briassouli, 
Alexia, 
Jenny 
Benois-Pineau, 
and 
Alexander Hauptmann, eds. Health Monitoring and 
Personalized Feedback Using Multimedia Data. Cham: 
Springer, 2015. Print.
Bright, Peter. “Locking the Bad Guys Out with 
Asymmetric Encryption.” Ars Technica. Condé 
Nast, 12 Feb. 2013. Web. 23 Feb. 2016.
Brindley, Keith (2011) Starting Electronics 4th ed., New 
York, NY: Elsevier. Print.
Brooks, R. R. Introduction to Computer and Network 
Security: Navigating Shades of Gray. Boca Raton: 
CRC, 2014. Digital file.
Brown, Adrian. Graphics File Formats. Kew: Natl. 
Archives, 2008. PDF file. Digital Preservation 
Guidance Note 4.
Brownlee, John. “The History of Web Design 
Explained in 9 GIFs.” Co. Design. Fast Co., 5 Dec. 
2014. Web. 22 Feb. 2016.

340
Bibliography
Principles of Computer Science
Bryden, Douglas. CAD and Rapid Prototyping for 
Product Design. London: King, 2014. Print.
Bucchi, Massimiano, and Brian Trench, eds. Routledge 
Handbook of Public Communication of Science and 
Technology. 2nd ed. New York: Routledge, 2014. 
Print.
Burger, John R. Brain Theory from a Circuits and 
Systems Perspective: How Electrical Science Explains 
Neuro-Circuits, Neuro-Systems, and Qubits. New York: 
Springer, 2013. Print.
Busch, David D. Mastering Digital SLR Photography. 
2nd ed. Boston: Thompson Learning, 2008. 
Print.
Bwalya, Kelvin J., Nathan M. Mnjama, and Peter M. I. 
I. M. Sebina. Concepts and Advances in Information 
Knowledge Management: Studies from Developing and 
Emerging Economies. Boston: Elsevier, 2014. Print.
Calude, C. S., and Gheorge Paun, Computing with 
Cells and Atoms. (New York: Taylor & Francis, 2001. 
Print.
Calude, Cristian S., ed. The Human Face of Computing. 
London: Imperial Coll. P, 2016. Print.
Campbell-Kelly, Martin, William Aspray, Nathan 
Ensmenger, and Jeffrey R. Yost. Computer: A History 
of the Information Machine. Boulder: Westview, 2014. 
Print.
Carlson, Wayne. “A Critical History of Computer 
Graphics and Animation.” Ohio State University. 
Ohio State U, 2003. Web. 31 Jan. 2016.
Case, Meredith A., et al. “Accuracy of Smartphone 
Applications and Wearable Devices for Tracking 
Physical Activity Data.” Journal of the American 
Medical Association 313.6 (2015): 625–26. Web. 2 
Mar. 2016.
Cassell, Eric J. The Nature of Healing: The Modern 
Practice of Medicine. New York: Oxford UP, 2013. 
Print.
“Categories of Free and Nonfree Software.” GNU 
Operating System. Free Software Foundation, 1 Jan. 
2016. Web. 31 Mar. 2016.
Cavanagh, Joseph (2013) X86 Assembly Language and 
C Fundamentals Boca Raton, FL: CRC Press. Print.
Ceberio, Martine, and Vladik Kreinovich. Constraint 
Programming and Decision Making. New York: 
Springer, 2014. Print.
Celada, Laura. “What Are the Most Common 
Graphics File Formats.” FESPA. FESPA, 27 Mar. 
2015. Web. 11 Feb. 2016.
Cervantes, Humberto, and Rick Kazman. Designing 
Software Architectures: A Practical Approach. Upper 
Saddle River: Addison, 2016. Print.
Cezzar, Juliette. “What Is Graphic Design?” AIGA. 
Amer. Inst. of Graphic Arts, 2016. Web. 16 Mar. 
2016.
Cha, Ariana Eunjung. “Health and Data: Can Digital 
Fitness Monitors Revolutionise Our Lives?” 
Guardian. Guardian News and Media, 19 May 
2015. Web. 26 Feb. 2016.
Chan, Melanie. Virtual Reality: Representations in 
Contemporary Media. New York: Bloomsbury, 2014. 
Print.
Chao, Loretta. “Tech Partnership Looks beyond the 
Bar Code with Digital Watermarks.” Wall Street 
Journal. Dow Jones, 12 Jan. 2016. Web. 14 Mar. 2016.
Cheever, 
Erik. 
“Representation 
of 
Numbers.” 
Swarthmore College. Swarthmore College, n.d. Web. 
20 Feb. 2016. 
Chen, Yufeng, and Zhiwu Li. Optimal Supervisory 
Control of Automated Manufacturing Systems. Boca 
Raton: CRC, 2013. Print.
Chivers, Ian, and Jane Sleightholme. Introduction to 
Programming With Fortran, with Coverage of Fortran 
90, 95, 2003, 2008 and 77. 3rd ed., New York, NY: 
Springer, 2015. Print.
“Choose a Website Builder: 14 Top Tools.” Creative 
Bloq. Future, 8 Feb. 2016. Web. 22 Feb. 2016.
Christiano, 
Marie. 
“What 
Are 
Integrated 
Development Environments?” All about Circuits. 
EETech Media, 3 Aug. 2015. Web. 23 Feb. 2016.
Chua, Chee Kai, Kah Fai Leong, and Chu Sing 
Lim. Rapid Prototyping: Principles and Applications. 
Hackensack: World Scientific, 2010. Print.
Clark, Alexander, Chris Fox, and Shalom Lappin 
(eds.) The Handbook of Computational Linguistics 
and Natural Language Processing. Wiley-Blackwell, 
2012.
Clark, Don. “Smart-Home Gadgets Still a Hard Sell.” 
Wall Street Journal. Dow Jones, 5 Jan. 2016. Web. 12 
Mar. 2016.
Clarke, Dave, James Noble, and Tobias Wrigstad, 
eds. Aliasing in Object-Oriented Programming: Types, 
Analysis and Verification. Berlin: Springer, 2013. 
Print.
Clements, Alan (2006) Principles of Computer Hardware 
4th ed., New York, NY: Oxford University Press. 
Print.

341
Principles of Computer Science
Bibliography
Clements, 
Paul, 
et 
al. 
Documenting 
Software 
Architectures: Views and Beyond. 2nd ed. Upper 
Saddle River: Addison, 2011. Print.
Cline, Hugh F. Information Communication Technology 
and Social Transformation: A Social and Historical 
Perspective. New York: Routledge, 2014. Print.
Coelho, Helder, and José C. Cotta. Prolog by Example: 
How to Learn, Teach and Use It. New York, NY: 
Springer, 1996. Print.
Coleman, E Gabriella. Coding Freedom: The Ethics and 
Aesthetics of Hacking. Princeton: Princeton UP, 
2013. Print.
Collins, Lauren, and Scott Ellis. Mobile Devices: Tools 
and Technologies. Boca Raton: CRC, 2015. Print.
Collins, Mike. Pro Tools 11: Music Production, Recording, 
Editing, and Mixing. Burlington: Focal, 2014. Print.
“Combinatorics.” Mathigon. Mathigon, 2015. Web. 10 
Feb. 2016.
Comer, Douglas E. Computer Networks and Internets. 
6th ed. Boston: Pearson, 2015. Print.
“CompTIA A+.” CompTIA. Computing Technology 
Industry Assn., 2015. Web. 31 Jan. 2016.
“Computer Crime Laws.” Frontline. WGBH Educ. 
Foundation, 2014. Web. 28 Mar. 2016.
“Computer-Aided Design (CAD) and Computer-
Aided Manufacturing (CAM).” Inc. Mansueto 
Ventures, n.d. Web. 31 Jan. 2016.
“Computer—Networking.” Tutorials Point. Tutorials 
Point, 2016. Web. 22 Feb. 2016.
“Computer Fraud and Abuse Act (CFAA).” Internet 
Law Treatise. Electronic Frontier Foundation, 24 
Apr. 2013. Web. 31 Mar. 2016.
Comstock, Jonah. “Eight Years of Fitbit News Leading 
Up to Its Planned IPO.” MobiHealthNews. HIMSS 
Media, 11 May 2015. Web. 28 Feb. 2016.
Connolly, Thomas M., and Carolyn E. Begg. 
Database Systems: A Practical Approach to Design, 
Implementation, and Management. 6th ed. Boston: 
Pearson, 2015. Print.
Cooper, Stephen. “Motherboard Design Process.” 
MBReview.com. Author, 4 Sept. 2009. Web. 14 Mar. 
2016.
Corbet, Jonathan, Alessandro Rubini, and Greg 
Kroah-Hartman. Linux Device Drivers. 3rd ed. 
Cambridge: O’Reilly, 2005. Print.
Cormen, Thomas H. Algorithms Unlocked. Cambridge: 
MIT P, 2013. Print.
Cormen, Thomas H., et al. Introduction to Algorithms. 
3rd ed. Cambridge: MIT P, 2009. Print.
Costello, Vic, Susan Youngblood, and Norman E. 
Youngblood. Multimedia Foundations: Core Concepts 
for Digital Design. New York: Focal, 2012. Print.
Counihan, Martin Fortran 95. Londion, UK: 
University College Press. 1996. Print.
Couts, Andrew. “Drones 101: A Beginner’s Guide to 
Taking Flight, No License Needed.” Digital Trends. 
Designtechnica, 16 Nov. 2013. Web. 27 Jan. 2015.
Cox, Ingemar J., Jessica Fridrich, Matthew L. Miller, 
Jeffrey A. Bloom, and Ton Kalker. “Practical 
Dirty-Paper Codes.” Digital Watermarking and 
Steganography. 2nd ed. Amsterdam: Elsevier, 2008. 
183–212. Digital file.
Cross, Mark. Audio Post Production for Film and 
Television. Boston: Berklee, 2013. Print.
Crothers, Brooke. “Microsoft Explains Quantum 
Computing So Even You Can Understand.” CNET. 
CBS Interactive, 25 July 2014. Web. 24 Mar. 2016.
Dale, Nell, and John Lewis. Computer Science 
Illuminated. 6th ed. Burlington: Jones, 2016. Print.
Dancyger, Ken. The Technique of Film and Video Editing: 
History, Theory, and Practice. Burlington: Focal, 
2013. Digital file.
Dastbaz, Mohammad, Colin Pattinson, and Bakbak 
Akhgar, eds. Green Information Technology: A 
Sustainable Approach. Waltham: Elsevier, 2015. 
Print.
Davies, Alan. An Introduction to Applied Linguistics: From 
Practice to Theory. 2nd ed. Edinburgh: Edinburgh 
UP, 2007. Print.
Davis, Stephen R. (2015) Beginning Programming with 
C++ for Dummies 2nd ed. Joboken, NJ: John Wiley & 
Sons. Print.
“DCN—Computer Network Types.” Tutorials Point. 
Tutorials Point, 2016. Web. 22 Feb. 2016.
De Filippi, Primavera. “It’s Time to Take Mesh 
Networks Seriously (and Not Just for the Reasons 
You Think).” Wired. Condé Nast, 2 Jan. 2014. Web. 
22 Feb. 2016.
De George, Andy. “How to Autoscale an Application.” 
Microsoft Azure. Microsoft, 7 Dec. 2015. Web. 17 
Feb. 2016.
Deitel, H.M. And Deitel, P.J. (2009) C++ for 
Programmers Upper Saddle River, NJ: Pearson 
Education Incorporated. Print.
Delforge, Pierre. “America’s Data Centers Consuming 
and Wasting Growing Amounts of Energy.” NRDC. 
Natural Resources Defense Council, 6 Feb. 2015. 
Web. 17 Mar. 2016.

342
Bibliography
Principles of Computer Science
Delfs, Hans, and Helmut Knebl. Introduction to 
Cryptography: Principles and Applications. 3rd ed. 
Berlin: Springer, 2015. Print.
Dennis, Alan, Barbara Haley Wixom, and David 
Tegarden. Systems Analysis and Design: An Object-
Oriented Approach with UML. 5th ed. Hoboken: 
Wiley, 2015. Print.
Deransart, P., A. Ed-Dbali, and L. Cervoni. PROLOG: 
The Standard Reference Manual. New York, NY: 
Springer, 1996. Print.
“Design and Technology: Manufacturing Processes.” 
GCSE Bitesize. BBC, 2014. Web. 31 Jan. 2016.
“Designing for Screen Reader Compatibility.” 
WebAIM. Center for Persons with Disabilities, Utah 
State U, 19 Nov. 2014. Web. 22 Feb. 2016.
Devroye, N., P. Mitran, and V. Tarokh. “Limits on 
Communications in a Cognitive Radio Channel.” 
IEEE Communication Magazine 44.6 (2006): 4449. 
Inspec. Web. 9 Mar. 2016.
Devroye, Natasha, Patrick Mitran and Vahid Tarokh. 
On Cognitive Graphs: Decomposing Wireless Networks. 
New York: Wiley Interscience, 2006. Print.
Dey, Pradip, and Manas Ghosh. Computer Fundamentals 
and Programming in C. 2nd ed. New Delhi: Oxford 
UP, 2013. Print.
Dice, Pete. Quick Boot: A Guide for Embedded Firmware 
Developers. Hillsboro: Intel, 2012. Print.
“Differences 
between 
Multithreading 
and 
Multitasking for Programmers.” NI. National 
Instruments, 20 Jan. 2014. Web. 15 Mar 2016.
“Digital Evidence and Forensics.” National Institute of 
Justice. Office of Justice Programs, 28 Oct. 2015. 
Web. 12 Feb. 2016.
Dillinger, Markus, Kambiz Madani, and Nancy 
Alonistioti. Software Defined Radio: Architectures, 
Systems, and Functions. Hoboken: Wiley, 2003. 
Print.
Doeppner, Thomas W. Operating Systems in Depth. 
Hoboken: Wiley, 2011. Print.
Dor, Daniel. The Instruction of Imagination: Language 
as a Social Communication Technology. New York: 
Oxford UP, 2015. Print.
Drake, 
Joshua 
J. 
Android 
Hacker’s 
Handbook. 
Indianapolis: Wiley, 2014. Print.
Duncan, Geoff. “Can the Government Regulate 
Internet Privacy?” Digital Trends. Designtechnica, 
21 Apr. 2014. Web. 28 Mar. 2016.
Duntemann, 
Jeff 
(2011) 
Assembly 
Language 
Programming Step-by-Step: Programming with Linux 3rd 
ed., Hoboken, NJ: John Wiley & Sons. Print.
Dutson, Phil. Responsive Mobile Design: Designing for 
Every Device. Upper Saddle River: Addison-Wesley, 
2015. Print.
Dyson, George. Turing’s Cathedral: The Origins of the 
Digital Universe. Vintage, 2012.
Edwards, Benj. “From Paper Tape to Data Sticks: The 
Evolution of Removable Storage.” PCWorld. IDG 
Consumer & SMB, 7 Feb. 2010. Web. 7 Mar. 2016.
Edwards, Jim. “Proof That Android Really Is for the 
Poor.” Business Insider. Business Insider, 27 June 
2014. Web. 4 Jan. 2016.
Edwards, Paul N. A Vast Machine: Computer Models, 
Climate Data, and the Politics of Global Warming. 
Cambridge: MIT P, 2010. Print.
El-Rewini, Hesham, and Mostafa Abd-El-Barr. 
Advanced 
Computer 
Architecture 
and 
Parallel 
Processing. Hoboken: Wiley, 2005. Print.
Elahi, Ata, and Mehran Elahi. Data, Network, and 
Internet Communications Technology. Clifton Park: 
Thomson, 2006. Print.
Elenkov, Nikolay. Android Security Internals: An In-
Depth Guide to Android’s Security Architecture. San 
Francisco: No Starch, 2015. Print.
Enderle, John Denis, and Joseph D. Brozino. 
Introduction to Biomedical Engineering. 3rd ed. 
Burlington: Elsevier, 2012. Print.
Englander, Irv. The Architecture of Computer Hardware, 
Systems Software, & Networking: An Information 
Technology Approach. 5th ed. Hoboken: Wiley, 2014. 
Print.
Erben, Tony, Ruth Ban, and Martha E. Castañeda. 
Teaching 
English 
Language 
Learners 
through 
Technology. New York: Routledge, 2009. Print.
Esslinger, Bernhard, et al. The CrypTool Script: 
Cryptography, Mathematics, and More. 11th ed. 
Frankfurt: CrypTool, 2013. CrypTool Portal. Web. 2 
Mar. 2016.
Evans, Kirk. “Autoscaling Azure—Virtual Machines.” 
.NET from a Markup Perspective. Microsoft, 20 Feb. 
2015. Web. 17 Feb. 2016.
Ewing, Martin. The ABCs of Software Defined Radio. 
Hartford: Amer. Radio Relay League, 2012. Print.
“Examples and Explanations of BME.” Biomedical 
Engineering Society. Biomedical Engineering Soc., 
2012–14. Web. 23 Jan. 2016.

343
Principles of Computer Science
Bibliography
Fasano, Philip. Transforming Health Care: The Financial 
Impact of Technology, Electronic Tools and Data Mining. 
Hoboken: Wiley, 2013. Print.
Faticoni, Theodore G., Combinatorics: An Introduction. 
New York: Wiley, 2014. Digital file.
Feng, Wu-chun, ed. The Green Computing Book: 
Tackling Energy Efficiency at Large Scale. Boca Raton: 
CRC, 2014. Print.
“Fitbit Flex Teardown.” iFixit. iFixit, 2013. Web. 28 
Feb. 2016.
Firtman, Maximiliano R. Programming the Mobile Web. 
Sebastopol: O’Reilly Media, 2013. Print.
Fischer, Eric. The Evolution of Character Codes, 1874–
1968. N.p.: Fischer, n.d. Trafficways.org. Web. 22 
Feb. 2016.
Flach, Peter. Machine Learning: The Art and Science 
of Algorithms that Make Sense of Data. Cambridge 
University Press, 2012.
Follin, Steve. “Preparing for IT Infrastructure 
Autonomics.” IndustryWeek. Penton, 19 Nov. 2015. 
Web. 20 Jan. 2016.
Foote, Steven. Learning to Program. Upper Saddle 
River: Pearson, 2015. Print.
Fountain, T J. Parallel Computing: Principles and 
Practice. Cambridge: Cambridge University Press, 
1994. Print. 
Fowler, Geoffrey, A. “The Drones on Autopilot That 
Follow Your Lead (Usually).” Wall Street Journal. 
Dow Jones, 23 Dec. 2014. Web. 20 Jan. 2016.
Fox, Richard. Information Technology: An Introduction 
for Today’s Digital World. Boca Raton: CRC, 2013. 
Print.
France, Anna Kaziunas, comp. Make: 3D Printing—
The Essential Guide to 3D Printers. Sebastopol: Maker 
Media, 2013. Print.
Franceschi-Bicchierai, Lorenzo. “Love Bug: The Virus 
That Hit 50 Million People Turns 15.” Motherboard. 
Vice Media, 4 May 2015. Web. 16 Mar. 2016.
Freedman, Jeri. Software Development. New York: 
Cavendish Square, 2015. Print.
Freeman, Michael. Digital Image Editing & Special 
Effects: Quickly Master the Key Techniques of Photoshop 
& Lightroom. New York: Focal, 2013. Print.
“Frequently Asked Questions.” Digital Watermarking 
Alliance. DWA, n.d. Web. 11 Mar. 2016.
Frenzel, Louis E., Jr. Electronics Explained: The New 
Systems Approach to Learning Electronics. Burlington: 
Elsevier, 2010. Print.
Friedman, Daniel P., and Mitchell Wand. Essentials of 
Programming Languages. Cambridge: MIT P, 2006. 
Print.
Fuchs, Christian, and Marisol Sandoval, eds. Critique, 
Social Media and the Information Society. New York: 
Routledge, 2014. Print.
Gaffney, Alexander. “FDA Confirms It Won’t Regulate 
Apps or Devices Which Store Patient Data.” 
Regulatory Affairs Professionals Society. Regulatory 
Affairs Professional Soc., 6 Feb. 2015. Web. 31 Mar. 
2016.
Galer, Mark, and Philip Andrews. Photoshop CC 
Essential Skills: A Guide to Creative Image Editing. 
New York: Focal, 2014. Print.
Gallagher, Sean. “Cortana for All: Microsoft’s Plan 
to Put Voice Recognition behind Anything.” Ars 
Technica. Condé Nast, 15 May 2015. Web. 21 Mar. 
2016.
Gallagher, Sean. “‘Locky’ Crypto-Ransomware Rides 
In on Malicious Word Document Macro.” Ars 
Technica. Condé Nast, 17 Feb. 2016. Web. 16 Mar. 
2016.
Gallagher, Sean. “Though ‘Barely an Operating 
System,’ DOS Still Matters (to Some People).” Ars 
Technica. Condé Nast, 14 July 2014. Web. 31 Jan. 
2016.
Galushkin, Alexander I. Neural Network Theory. New 
York, NY: Springer, 2007. Print.
Gancarz, Mike. The UNIX Philosophy. Woburn: 
Butterworth, 1995. Print.
Garrido, José M., Richard Schlesinger, and Kenneth 
E. Hoganson. Principles of Modern Operating Systems. 
2nd ed. Burlington: Jones, 2013. Print.
Garza, George. “Working with the Cons of Object 
Oriented Programming.” Ed. Linda Richter. 
Bright Hub. Bright Hub, 19 May 2011. Web. 6 Feb. 
2016.
Gaudin, Sharon. “Quantum Computing May Be 
Moving out of Science Fiction.” Computerworld. 
Computerworld, 15 Dec. 2015. Web. 24 Mar. 
2016.
Gee, James Paul. Unified Discourse Analysis: Language, 
Reality, Virtual Worlds, and Video Games. New York: 
Routledge, 2015. Print.
Gehrke, Wilhelm. Fortran 90 Language Guide. New 
York, NY: Springer, 1995. Print.
“Ghana: Digital Dumping Ground.” Frontline. PBS, 23 
June 2009. Web. 29 Jan. 2016.

344
Bibliography
Principles of Computer Science
Gibbs, Samuel. “From Windows 1 to Windows 10: 29 
Years of Windows Evolution.” Guardian. Guardian 
News and Media, 2 Oct. 2014. Web. 2 Jan. 2016.
Gibbs, Samuel. “Google’s Massive Humanoid Robot 
Can Now Walk and Move without Wires.” Guardian. 
Guardian News and Media, 21 Jan. 2015. Web. 21 
Jan. 2016.
Gibbs, W. Wayt. “Autonomic Computing.” Scientific 
American. Nature Amer., 6 May 2002. Web. 20 Jan. 
2016.
Gibson, J.R. (2011) Electronic Logic Circuits 3rd ed., 
New York, NY: Routledge. Print.
Gibson, Jerry D., ed. Mobile Communications Handbook. 
3rd ed. Boca Raton: CRC, 2013. Print.
Gilder, Jules H. (1986) Apple Iic and Iie Assembly 
Language New York, NY: Chapman and Hall. Print.
Gillam, Richard. Unicode Demystified: A Practical 
Programmer’s Guide to the Encoding Standard. Boston: 
Addison-Wesley, 2002. Print.
Gillespie, Tarleton, Pablo J. Boczkowski, and 
Kirsten A. Foot, eds. Media Technologies: Essays on 
Communication, Materiality, and Society. Cambridge: 
MIT P, 2014. Print.
Glanz, James. “Power, Pollution and the Internet.” 
New York Times. New York Times, 22 Sept. 2012. 
Web. 28 Feb. 2016.
Glaser, Anton. History of Binary and Other Nondecimal 
Numeration. Rev. ed. Los Angeles: Tomash, 1981. 
Print. 
Glaser, J. D. Secure Development for Mobile Apps: How to 
Design and Code Secure Mobile Applications with PHP 
and JavaScript. Boca Raton: CRC, 2015. Print.
Glaubitz, John Paul Adrian. “Modern Consumerism 
and the Waste Problem.” ArXiv.org. Cornell U, 4 
June 2012. Web. 9 Feb. 2016.
Glink, Ilyce. “10 Smart Home Features Buyers 
Actually Want.” CBS News. CBS Interactive, 11 Apr. 
2015. Web. 12 Mar. 2016.
Godbey, W. T. An Introduction to Biotechnology: 
The Science, Technology and Medical Applications. 
Waltham: Academic, 2014. Print.
Goelker, Klaus. Gimp 2.8 for Photographers: Image 
Editing with Open Source Software. Santa Barbara: 
Rocky Nook, 2013. Print.
Gogolin, Greg. Digital Forensics Explained. Boca Raton: 
CRC, 2013. Print.
Goldsborough, Reid. “Android on the Rise.” Tech 
Directions May 2014: 12. Academic Search Complete. 
Web. 2 Jan. 2016.
Gonzalez, Teofilo, and Jorge Díaz-Herrera, eds. 
Computing Handbook: Computer Science and Software 
Engineering. 3rd ed. Boca Raton: CRC, 2014. Print.
Goode, Lauren. “Fitbit Hit with Class-Action Suit 
over Inaccurate Heart Rate Monitoring.” Verge. 
Vox Media, 6 Jan. 2016. Web. 28 Feb. 2016.
Goodman, Robert, and Patrick McGrath. Editing 
Digital Video: The Complete Creative and Technical 
Guide. New York: McGraw, 2003. Print.
Goodwins, Rupert. “The Future of Storage: 2015 and 
Beyond.” ZDNet. CBS Interactive, 1 Jan. 2015. Web. 
7 Mar. 2016.
Goriunova, Olga, ed. Fun and Software: Exploring 
Pleasure, Paradox, and Pain in Computing. New York: 
Bloomsbury, 2014. Print.
Govindjee, S. Internal Representation of Numbers. Dept. 
of Civil and Environmental Engineering, U of 
California Berkeley, Spring 2013. Digital File. 
Graham W. Seed (2012) An Introduction to Object-
Oriented Programming in C++ with Applications 
in Computer Graphics New York, NY: Springer 
Science+Business Media. Print.
Graham, Tony. Unicode: A Primer. Foster City: M&T, 
2000. Print.
“Graphical User Interface (GUI).” Techopedia. 
Techopedia, n.d. Web. 5 Feb. 2016.
Graupe, Daniel. Principles of Artificial Neural Networks. 
2nd ed. Hackensack, NJ: World Scientific, 2007. Print.
Grayver, Eugene. Implementing Software Defined Radio. 
New York: Springer, 2012. Print.
Greenberg, Saul, et al. Sketching User Experiences: The 
Workbook. Waltham: Morgan, 2012. Print. 4 Science 
Reference Center™ Wireframes
Griffiths, Devin C. Virtual Ascendance: Video Games and 
the Remaking of Reality. Lanham: Rowman, 2013. 
Print.
Guichard, David. “An Introduction to Combinatorics 
and Graph Theory.” Whitman. Whitman Coll., 4 
Jan 2016. Web. 10 Feb. 2016.
Gulchak, Daniel J. “Using a Mobile Handheld 
Computer to Teach a Student with an Emotional 
and 
Behavioral 
Disorder 
to 
Self-Monitor 
Attention.” Education and Treatment of Children 31.4 
(2008): 567–81. PDF file.
Gupta, Shalene. “For the Disabled, Smart Homes 
Are Home Sweet Home.” Fortune. Fortune, 1 Feb. 
2015. Web. 15 Mar. 2016.
Gupta, Siddarth, and Vagesh Porwal. “Recent 
Digital Watermarking Approaches, Protecting 

345
Principles of Computer Science
Bibliography
Multimedia Data Ownership.” Advances in Computer 
Science 4.2 (2015): 21–30. Web. 14 Mar. 2016.
Habiballah, N., M. Qjani, A. Arbaoui, and J. Dumas. 
“Effect of a Gaussian White Noise on the Charge 
Density Wave Dynamics in a One Dimensional 
Compound.” Journal of Physics and Chemistry of Solids 
75.1 (2014): 153–56. Inspec. Web. 9 Mar. 2016.
Haerens, Margaret, and Lynn M. Zott, eds. Hacking 
and Hackers. Detroit: Greenhaven, 2014. Print.
Hagan, Martin T., Howard B. Demuth, , Mark H. 
Beale, and Orlando de Jesús. Neural Network Design. 
2nd ed. Martin Hagan, 2014. Print.
Hagen, Rebecca, and Kim Golombisky. White Space Is 
Not Your Enemy: A Beginner’s Guide to Communicating 
Visually through Graphic, Web & Multimedia Design. 
2nd ed. Burlington: Focal, 2013. Print.
Hamburg, Margaret A., and Francis S. Collins. “The 
Path to Personalized Medicine.” New England 
Journal of Medicine 363.4 (2010): 301–4. Web. 23 
Dec. 2015.
Hamm, Matthew J. Wireframing Essentials: An 
Introduction to User Experience Design. Birmingham: 
Packt, 2014. Print.
Harbour, Jonathan S. Beginning Game Programming. 
4th ed. Boston: Cengage, 2015. Print.
Harel, 
Jacob. 
“SynthOS 
and 
Task-Oriented 
Programming.” 
Embedded 
Computing 
Design. 
Embedded Computing Design, 2 Feb. 2016. Web. 
7 Feb. 2016.
Harper, Robert. Practical Foundations for Programming 
Languages. Cambridge: Cambridge UP, 2013. 
Print.
Harris, David Money, and Sarah L. Harris. Digital 
Design and Computer Architecture. 2nd ed. Waltham: 
Morgan, 2013. Print.
Harrison, Virginia, and Jose Pagliery. “Nearly 1 
Million New Malware Threats Released Every 
Day.” CNNMoney. Cable News Network, 14 Apr. 
2015. Web. 16 Mar. 2016.
Hart, Archibald D., and Sylvia Hart Frejd. The Digital 
Invasion: How Technology Is Shaping You and Your 
Relationships. Grand Rapids: Baker, 2013. Print.
Harth, Andreas, Katja Hose, and Ralf Schenkel, eds. 
Linked Data Management. Boca Raton: CRC, 2014. 
Print.
Haug, Hartmut, and Stephan W. Koch. Quantum 
Theory of the Optical and Electronic Properties of 
Semiconductors. New Jersey [u.a.]: World Scientific, 
2009. Print.
Havens, John C. Hacking Happiness: Why Your Personal 
Data Counts and How Tracking It Can Change the 
World. New York: Tarcher, 2014. Print.
“Health Information Privacy.” HHS.gov. Dept. of 
Health and Human Services, n.d. Web. 28 Mar. 
2016.
Heisler, Yoni. “The History and Evolution of iOS, 
from the Original iPhone to iOS 9.” BGR. BGR 
Media, 12 Feb. 2016. Web. 26 Feb. 2016.
Henz, Martin. Objects for Concurrent Constraint 
Programming. New York: Springer, 1998. Print.
Herlihy, Maurice, and Nir Shavit. The Art of 
Multiprocessor Programming. New York: Elsevier, 
2012. Print.
Herrman, John. “How to Get Started: 3D Modeling 
and Printing.” Popular Mechanics. Hearst Digital 
Media, 15 Mar. 2012. Web. 31 Jan. 2016.
Hey, Tony, and Gyuri Pápay. The Computing Universe: A 
Journey through a Revolution. New York: Cambridge 
UP, 2015. Print.
Higginbotham, Stacey. “5 Reasons Why the ‘Smart 
Home’ Is Still Stupid.” Fortune. Fortune, 19 Aug. 
2015. Web. 12 Mar. 2016.
Highfield, Roger. “Fast Forward to Cartoon Reality.” 
Telegraph. Telegraph Media Group, 13 June 2006. 
Web. 31 Jan. 2016.
Hill, Mark D. “What Is Scalability?” Scalable Shared 
Memory Multiprocessors. Ed. Michel Dubois and 
Shreekant Thakkar. New York: Springer, 1992. 
89–96. Print.
Hillis, W. Daniel. “Richard Feynman and the 
Connection Machine.” Phys. Today Physics Today 
42.2 (1989): 78. Web.
Hirvensalo, Mike. Quantum Computing. New York, 
NY: Springer, 2001. Print.
“History and Timeline.” Open Group. Open Group, 
n.d. Web. 28 Feb. 2016.
History of Cryptography: An Easy to Understand History 
of Cryptography. N.p.: Thawte, 2013. Thawte. Web. 
4 Feb. 2016.
Hodges, Andrew. Alan Turing: The Enigma: The 
Book That Inspired the Film “The Imitation Game.” 
Princeton University Press, 2014.
Hof, Robert. “How Fitbit Survived as a Hardware 
Startup.” Forbes. Forbes.com, 4 Feb. 2014. Web. 28 
Feb. 2016.
Hoffer, Jeffrey A., V. Ramesh, and Heikki Topi. 
Modern Database Management. 12th ed. Boston: 
Pearson, 2016. Print.

346
Bibliography
Principles of Computer Science
Hoffstein, Jeffrey, Jill Pipher, and Joseph H. Silverman. 
An Introduction to Mathematical Cryptography. 2nd 
ed. New York: Springer, 2014. Print.
Hofstedt, Petra. Multiparadigm Constraint Programming 
Languages. New York: Springer, 2013. Print.
Holcombe, Jane, and Charles Holcombe. Survey 
of Operating Systems. New York: McGraw, 2015. 
Print.
Holland, Bill. “Software Testing: A History.” SitePoint. 
SitePoint, 15 Feb. 2012. Web. 9 Feb. 2016.
Holleley, Douglas. Photo-Editing and Presentation: 
A Guide to Image Editing and Presentation for 
Photographers 
and 
Visual 
Artists. 
Rochester: 
Clarellen, 2009. Print.
Holt, Thomas J., Adam M. Bossler, and Kathryn C. 
Seigfried-Spellar. Cybercrime and Digital Forensics: 
An Introduction. New York: Routledge, 2015. Print.
Hopfield, J. J. “Neural Networks and Physical 
Systems with Emergent Collective Computational 
Abilities.” Proceedings of the National Academy of 
Sciences 79.8 (1982): 2554-558. Web.
Horspool, Nigel, and Nikolai Tillmann. Touchdevelop: 
Programming on the Go. New York: Apress, 2013. 
Print.
Horvath, Joan. Mastering 3D Printing: Modeling, 
Printing, and Prototyping with Reprap- Style 3D 
Printers. Berkeley: Apress, 2014. Print.
Hoskins, Stephen. 3D Printing for Artists, Designers and 
Makers. London: Bloomsbury, 2013. Print.
“How Firewalls Work.” Boston University Information 
Services and Technology. Boston U, n.d. Web. 28 Feb. 
2016.
“How Speech-Recognition Software Got So Good.” 
Economist. Economist Newspaper, 22 Apr 2014. 
Web. 21 Mar. 2016.
Hsu, John Y. (2002) Computer Logic Design Principles 
and Applications New York, NY: Springer. Print.
“HTML Tutorial.” Tutorials Point. Tutorials Point, 
2016. Web. 22 Feb. 2016.
Hughes, Cameron, and Tracey Hughes. Parallel 
and Distributed Programming Using C++. Boston: 
Addison-Wesley, 2004. Print. 
Hughes, John F. Computer Graphics: Principles and 
Practice. Upper Saddle River: Addison, 2014. Print.
Human Brain Project. Human Brain Project, 2013. 
Web. 16 Feb. 2016.
Hutchinson, Lee. “Home 3D Printers Take Us on a 
Maddening Journey into Another Dimension.” Ars 
Technica. Condé Nast, 27 Aug. 2013. Web. 6 Jan. 
2016.
Huth, Alexa, and James Cebula. The Basics of Cloud 
Computing. N.p.: Carnegie Mellon U and US 
Computer Emergency Readiness Team, 2011. PDF 
file.
Hyde, Randall (2010) The Art of Assembly Language 
2nd ed., San Francisco, CA: No Starch Press. 
Print.
Hyde, Randall. Write Great Code: Understanding the 
Machine. Vol. 1. San Francisco: No Starch, 2005. 
Print.
Information Resources Management Association, ed. 
Assistive Technologies: Concepts, Methodologies, Tools, 
and Applications. Vol. 1. Hershey: Information 
Science Reference, 2014. Print.
Ingersoll, Grant S., Thomas S. Morton, and Drew 
Farris. Taming Text: How to Find, Organize, and 
Manipulate It. Manning Publications, 2013.
Ingham, Kenneth, and Stephanie Forrest. A History 
and Survey of Network Firewalls. Albuquerque: U of 
New Mexico, 2002. PDF file.
Iniewski, Krzysztof. Embedded Systems: Hardware, 
Design, and Implementation. Hoboken: Wiley, 2013. 
Print.
“Internet Privacy.” ACLU. American Civil Liberties 
Union, 2015. Web. 28 Feb. 2016.
“Intro to Algorithms.” Khan Academy. Khan Acad., 
2015. Web. 19 Jan. 2016.
“Introduction 
to 
Biometrics.” 
Biometrics.gov. 
Biometrics.gov, 2006. Web. 21 Jan. 2016.
“Introduction to Biotechnology.” Center for Bioenergy 
and Photosynthesis. Arizona State U, 13 Feb. 2006. 
Web. 20 Jan 2016.
“Introduction to Image Files Tutorial.” Boston 
University Information Services and Technology. 
Boston U, n.d. Web. 11 Feb. 2016.
Intro to Biotechnology: Techniques and Applications. 
Cambridge: NPG Educ., 2010. Scitable. Web. 20 
Jan. 2016.
“iOS: A Visual History.” Verge. Vox Media, 16 Sept. 
2013. Web. 24 Feb. 2016.
ITL Education Solutions. Introduction to Information 
Technology. 2nd ed. Delhi: Pearson, 2012. Print.
Iversen, Jakob, and Michael Eierman. Learning Mobile 
App Development: A Hands-On Guide to Building 
Apps with iOS and Android. Upper Saddle River: 
Addison-Wesley, 2014. Print.

347
Principles of Computer Science
Bibliography
Ives, Mike. “Boom in Mining Rare Earths Poses 
Mounting Toxic Risks.” Environment 360. Yale U, 
28 Jan. 2013. Web. 28 Feb. 2016.
Jacobson, Douglas, and Joseph Idziorek. Computer 
Security Literacy: Staying Safe in a Digital World. Boca 
Raton: CRC, 2013. Print.
Jadhav, S.S. (2008) Advanced Computer Architecture & 
Computing. Pune, IND: Technical Publishers, 2008. 
Print.
Jain, Anil K., Arun A. Ross, and Karthik Nandakumar. 
Introduction to Biometrics. New York: Springer, 2011. 
Print.
Janert, Philipp K. Feedback Control for Computer Systems. 
Sebastopol: O’Reilly, 2014. Print.
Jeannot, Emmanuel, and J. Žilinskas. High Performance 
Computing on Complex Environments. Hoboken: 
Wiley, 2014. Print.
Jennings, Tom. “An Annotated History of Some 
Character Codes.” World Power Systems. Tom 
Jennings, 29 Oct. 2004. Web. 16 Feb. 2016.
Johnson, C. Richard, and William A. Sethares. 
Telecommunications 
Breakdown: 
Concepts 
of 
Communication Transmitted via Software-Defined 
Radio. New York: Prentice, 2003. Print.
Johnson, Jeff. Designing with the Mind in Mind. 2nd ed. 
Waltham: Morgan, 2014. Print.
Jones, M. Tim, Artificial Intelligence: A Systems Approach. 
Sudbury, MA: Jones and Bartlett, 2009. Print.
Jorgensen, Paul C. Software Testing: A Craftsman’s 
Approach. 4th ed. Boca Raton: CRC, 2014. Print.
Jurafsky, Daniel, and James H. Martin. Speech and 
Language Processing: An Introduction to Natural 
Language Processing, Computational Linguistics, and 
Speech Recognition, PEL, 2008.
Kale, Vivek. Guide to Cloud Computing for Business 
and Technology Managers. Boca Raton: CRC, 2015. 
Print.
Kaptelinin, Victor, and Mary P. Czerwinski, eds. 
Beyond the Desktop Metaphor: Designing Integrated 
Digital Work Environments. Cambridge: MIT P, 
2007. Print.
Katoh, Shigeo, Jun-ichi Horiuchi, and Fumitake 
Yoshida. Biochemical Engineering: A Textbook for 
Engineers, Chemists and Biologists. 2nd rev. and enl. 
ed. Weinheim: Wiley, 2015. Print.
Katz, Jonathan, and Yehuda Lindell. Introduction to 
Modern Cryptography. 2nd ed. Boca Raton: CRC, 
2015. Print.
Kay, Roger. “Behind Apple’s Siri Lies Nuance’s 
Speech Recognition.” Forbes. Forbes. com, 24 Mar. 
2014. Web. 21 Mar. 2016.
Kaye, Phillip, Raymond Laflamme, and Michele 
Mosea. An Introduction to Quantum Computing. 
New  York, NU: Oxford University Press, 2007. 
Print.
Kefauver, Alan P., and David Patschke. Fundamentals 
of Digital Audio. Middleton: A-R Editions, 2007. 
Print.
Kelly, Gordon, “Apple iOS 9: 11 Important New 
Features.” Forbes. Forbes.com, 16 Sept. 2015. Web. 
28 Feb. 2016.
Kemeny, John G. and  Thomas E. Kurtz. Back To 
BASIC: The History, Corruption, and Future of the 
Language. Boston, MA: Addison-Wesley, 1985. 
Print.
Kernighan, Brian W. and Dennis M. Ritchie. The 
C Programming Language. Englewood Cliffs, NJ: 
Prentice-Hall, 1978. Print.
Khan, Gul N., and Krzysztof Iniewski, eds. Embedded 
and Networking Systems: Design, Software, and 
Implementation. Boca Raton: CRC, 2014. Print.
Khan, Shafiullah, and Al-Sakib Khan Pathan, eds. 
Wireless Networks and Security: Issues, Challenges and 
Research Trends. Berlin: Springer, 2013. Print.
Kilkelly, Michael. “Which Architectural Software 
Should You Be Using?” ArchDaily. ArchDaily, 4 May 
2015. Web. 31 Jan. 2016.
Kilper, Daniel C., and Tucker, Rodney S. “Energy-
Efficient 
Telecommunications.” 
Optical 
Fiber 
Telecommunications. 6th ed. N.p.: Elsevier, 2013. 
747–91. Digital file.
Kim, Chang-Hun, et al. Real-Time Visual Effects for Game 
Programming. Singapore: Springer, 2015. Print.
Kirk, David B, and Wen-mei Hwu. Programming 
Massively Parallel Processors: A Hands-on Approach. 
Burlington, Massachusetts: Morgan Kaufmann 
Elsevier, 2013. Print. 
Kirkwood, Patricia Elaine, and Necia T. Parker-
Gibson. Informing Chemical Engineering Decisions 
with Data, Research, and Government Resources. San 
Rafael: Morgan, 2013. Digital file.
Kizza, Joseph Migga. Ethical and Social Issues in the 
Information Age. 5th ed. London: Springer, 2013. 
Print.
Kizza, Joseph Migga. Guide to Computer Network 
Security. 3rd ed. London: Springer, 2015. Print.

348
Bibliography
Principles of Computer Science
Klimczak, Erik. Design for Software: A Playbook for 
Developers. Hoboken: Wiley, 2013. Print.
“Knowledge Base: Technologies in 3D Printing.” 
DesignTech. DesignTech Systems, n.d. Web. 6 Jan. 
2016.
Köhler, Anna, and Heinz Bässler. Electronic Processes in 
Organic Semiconductors: An Introduction. Wiley-VHC 
Verlag, 2015. Print.
Kojic´, Miloš, et al. Computer Modeling in Bioengineering: 
Theoretical Background, Examples and Software. 
Hoboken: Wiley, 2008. Print.
Könenkamp, 
Rolf. 
Photoelectric 
Properties 
and 
Applications of Low-Mobility Semiconductors. Berlin: 
Springer, 2000. Print.
Korpela, Jukka K. Unicode Explained. Sebastopol: 
O’Reilly Media, 2006. Print.
Kosky, Philip, et al. Exploring Engineering: An 
Introduction to Engineering and Design. 4th ed. 
Waltham: Academic, 2016. Print.
Kramer, Bill. The Autocadet’s Guide to Visual LISP. 
Laurence, KS: CMP Books, 2002. Print.
Krar, Steve, Arthur Gill, and Peter Smid. Computer 
Numerical Control Simplified. New York: Industrial, 
2001. Print.
Krug, Steve. Don’t Make Me Think, Revisited: A Common 
Sense Approach to Web Usability. 3rd ed. Berkeley: 
New Riders, 2014. Print.
Kruk, Robert. “Public, Private and Hybrid Clouds: 
What’s the Difference?” Techopedia. Techopedia, 
18 May 2012. Web. 21 Jan. 2016.
Kshemkalyani, Ajay D., and Mukesh Singhal. 
Distributed Computing: Principles, Algorithms, and 
Systems. New York: Cambridge UP, 2008. Print.
Kulisch, Ulrich. Computer Arithmetic and Validity: 
Theory, Implementation, and Applications. 2nd ed. 
Boston: De Gruyter, 2013. Print.
Kumar, Ela. Natural Language Processing. I K 
International Publishing House, 2011.
Kumari, Ramesh (2005) Computers and Their 
Applications to Chemistry. 2nd ed. Oxford, UK: Alpha 
Science International. Print.
Kuo, Sen M., Bob H. Lee, and Wenshun Tian. 
Real-Time Digital Signal Processing: Fundamentals, 
Implementations and Applications. 3rd ed. Hoboken: 
Wiley, 2013. Print.
Kupferschmid, Michael. Classical Fortran Programming 
for Engineering and Scientific Applications. Boca 
Raton, FL: CRC Press, 2009. Print.
Kurose, James F., and Keith W. Ross. Computer 
Networking: A Top-Down Approach. 6th ed. Boston: 
Pearson, 2013. Print.
Lackey, Ella Deon, et al. “Introduction to Public-Key 
Cryptography.” Mozilla Developer Network. Mozilla, 
21 Mar. 2015. Web. 4 Feb. 2016.
Lafferty, Edward L, Marion C. Michaud, and Myra 
J. Prelle. Parallel Computing: An Introduction. 
Park Ridge: Noyes Data Corporation, 1993. 
Print. 
Lakhtakia, A., and R. J. Martín-Palma. Engineered 
Biomimicry. Amsterdam: Elsevier, 2013. Print.
Lalanda, Philippe, Julie A. McCann, and Ada 
Diaconescu, eds. Autonomic Computing: Principles, 
Design and Implementation. London: Springer, 2013. 
Print.
Lande, Daniel R. “Development of the Binary 
Number System and the Foundations of Computer 
Science.” Mathematics Enthusiast 1 Dec. 2014: 513–
40. Print. 
Lander, Steve. “Disadvantages or Problems for 
Implementing Wi-Fi Technology.” Small Business—
Chron.com. Hearst Newspapers, n.d. Web. 22 Feb. 
2016.
Langer, Arthur M. Guide to Software Development: 
Designing and Managing the Life Cycle. New York: 
Springer, 2012. Print.
Lardinois, Frederic. “Ukrainian Students Develop 
Gloves That Translate Sign Language into 
Speech.” TechCrunch. AOL, 9 July 2012. Web. 19 
Jan 2016.
Laszlo, Arp. “Why Is Website Design So Important?” 
Sunrise Pro Websites & SEO. Sunrise Pro Websites, 
22 Jan. 2016. Web. 22 Feb. 2016.
Lathi, B. P. Linear Systems and Signals. 2nd rev. ed. New 
York: Oxford UP, 2010. Print.
Law, Averill M. Simulation Modeling and Analysis. 5th 
ed. New York: McGraw, 2015. Print.
Lee, John D., and Alex Kirlik, eds. The Oxford 
Handbook of Cognitive Engineering. New York: 
Oxford UP, 2013. Print.
Lee, Kent D. Foundations of Programming Languages. 
Cham: Springer, 2014. Print.
Lee, Roger Y., ed. Applied Computing and Information 
Technology. New York: Springer, 2014. Print.
Li, Han-Xiong, and XinJiang Lu. System Design and 
Control Integration for Advanced Manufacturing. 
Hoboken: Wiley, 2015. Print.

349
Principles of Computer Science
Bibliography
Liang, Hualou, Joseph D. Bronzino, and Donald R. 
Peterson, eds. Biosignal Processing: Principles and 
Practices. Boca Raton: CRC, 2012. Print.
Lien, Tracey. “Virtual Reality Isn’t Just for Video 
Games.” Los Angeles Times. Tribune, 8 Jan. 2015. 
Web. 23 Mar. 2016.
Lipiansky, Ed. Electrical, Electronics, and Digital 
Hardware Essentials for Scientists and Engineers. 
Hoboken: Wiley, 2013. Print.
Lippman, Stanley B, Josee Lajoie, and Barbara E. 
Moo. C++ Primer 5th ed. Upper Saddle River, NJ: 
Addison-Wesley, 2003. Print.
Lipson, Hod, and Melba Kurman. Fabricated: The New 
World of 3D Printing. Indianapolis: Wiley, 2013. 
Print.
Lipton, R. J., and E. B. Baum, eds., DNA Based 
Computers, 
DIMACS 
Series 
in 
Discrete 
Mathematics, and Theoretical Computer Science, 
27, American -Mathematical Society (1995).
Liu, Shih-Chii, Tobi Delbruck, Giacomo Indiveri, 
Adrian Whatley, and Rodney Douglas. Event-Based 
Neuromorphic Systems. Chichester: Wiley, 2015. 
Print.
Livingston, Steven, and Gregor Walter-Drop, eds. 
Bits and Atoms: Information and Communication 
Technology in Areas of Limited Statehood. New York: 
Oxford UP, 2014. Print.
Lohr, Steve. “Humanizing Technology: A History of 
Human-Computer Interaction.” New York Times: 
Bits. New York Times, 7 Sept. 2015. Web. 31 Jan. 
2016.
Loo, Alfred Waising, ed. Distributed Computing 
Innovations for Business, Engineering, and Science. 
Hershey: Information Science Reference, 2013. 
Print.
Luenendonk, Martin. “Top Programming Languages 
Used in Web Development.” Cleverism. Cleverism, 
21 June 2015. Web. 22 Feb. 2016.
MacLennan, Bruce J. Principles of Programming 
Languages: Design, Evaluation, and Implementation. 
Oxford: Oxford UP, 1999. Print.
Madhav, Sanjay. Game Programming Algorithms and 
Techniques: A Platform-Agnostic Approach. Upper 
Saddle River: Addison, 2014. Print.
Malcolme-Lawes, D.J. (1969) Programming – ALGOL 
London, UK: Pergamon Press. Print.
Mallick, Pradeep Kumar, ed. Research Advances in 
the Integration of Big Data and Smart Computing. 
Hershey: Information Science Reference, 2016. 
Print.
Malvik, Callie. “Graphic Design vs. Web Design: 
Which Career Is Right for You?” Rasmussen College. 
Rasmussen Coll., 25 July 2013. Web. 16 Mar. 2016.
Mandl, H., and A. Lesgold, eds. Learning Issues for 
Intelligent Tutoring Systems. New York: Springer, 
1988. Print.
Mangan, Dan. “There’s a Hack for That: Fitbit User 
Accounts Attacked.” CNBC. CNBC, 8 Jan. 2016. 
Web. 28 Feb. 2016.
Manjoo, Farhad. “Now You’re Talking!” Slate. Slate 
Group, 6 Apr. 2011. Web. 21 Mar. 2016.
Manjoo, Farhad. “Planet Android’s Shaky Orbit.” New 
York Times 28 May 2015: B1. Print.
Mara, Wil. Software Development: Science, Technology, 
and Engineering. New York: Children’s, 2016. Print.
Marble, Scott, ed. Digital Workflows in Architecture. 
Basel: Birkhäuser, 2012. Print.
Marchant, Ben. “Game Programming in C and C++.” 
Cprogramming.com. 
Cprogramming.com, 
2011. 
Web. 16 Mar. 2016.
Marchewka, Jack T. Information Technology Project 
Management. 5th ed. Hoboken: Wiley, 2015. Print.
Margush, Timothy S. (2012) Some Assembly Required. 
Assembly Language Programming with the AVR 
Microcontroller Boca Raton, FL: CRC Press. Print.
Marsh, Joel. UX for Beginners: A Crash Course in 100 
Short Lessons. Sebastopol: O’Reilly, 2016. Print.
Marshall, Gary. “The Story of Fitbit: How a Wooden 
Box Became a $4 Billion Company.” Wareable. 
Wareable, 30 Dec. 2015. Web. 28 Feb. 2016.
Mason, Paul. Understanding Computer Search and 
Research. Chicago: Heinemann, 2015. Print.
Mathews, Clive. An Introduction to Natural Language 
Processing Through Prolog New York, NY: Routledge, 
2014. Print.
Matsudaira, Kate. “Scalable Web Architecture and 
Distributed Systems.” The Architecture of Open Source 
Applications. Ed. Amy Brown and Greg Wilson. Vol. 
2. N.p.: Lulu, 2012. PDF file.
Matulka, Rebecca. “How 3D Printers Work.” Energy.
gov. Dept. of Energy, 19 June 2014. Web. 6 Jan. 
2016.
McCauley, Renée, et al. “Debugging: A Review of 
the Literature from an Educational Perspective.” 
Computer Science Education 18.2 (2008): 67–92. 
Print.

350
Bibliography
Principles of Computer Science
McConnell, Robert, James Haynes, and Richard 
Warren. 
“Understanding 
ASCII 
Codes.” 
NADCOMM. NADCOMM, 14 May 2011. Web. 16 
Feb. 2016.
McCracken, Harry. “Ten Momentous Moments in 
DOS History.” PCWorld. IDG Consumer, n.d. Web. 
31 Jan. 2016.
McDonald, Nicholas G. “Past, Present, and Future 
Methods of Cryptography and Data Encryption.” 
SpaceStation. U of Utah, 2009. Web. 4 Feb. 2016.
McFedries, Paul. Fixing Your Computer: Absolute 
Beginner’s Guide. Indianapolis: Que, 2014. Print.
McGrath, Mike (2015) C++ Programming in Easy Steps 
4th ed. Leamington Spa, UK: Easy Steps Limited. 
Print.
McKay, Sinclair. The Secret Lives of Codebreakers: The 
Men and Women Who Cracked the Enigma Code at 
Bletchley Park. Plume, 2012.
McLellan, Charles. “The History of Windows: A 
Timeline.” ZDNet. CBS Interactive, 14 Apr. 2014. 
Web. 15 Feb. 2016.
McMillan, Robert. “IBM Bets $3B That the Silicon 
Microchip Is Becoming Obsolete.” Wired. Condé 
Nast, 9 July 2014. Web. 10 Mar. 2016.
McMillan, Robert. “Siri Will Soon Understand You 
a Whole Lot Better.” Wired. Condé Nast, 30 June 
2014. Web. 21 Mar. 2016.
McMullan, Dawn. “What Is Personalized Medicine?” 
Genome Spring 2014: n. pag. Web. 23 Dec. 2015.
McNeill, Dwight. Using Person-Centered Health Analytics 
to Live Longer: Leveraging Engagement, Behavior 
Change, and Technology for a Healthy Life. Upper 
Saddle River: Pearson, 2015. Print.
McNeill, Erin. “Even ‘Digital Natives’ Need Digital 
Training.” Education Week. Editorial Projects in 
Education, 20 Oct 2015. Web. 26 Jan. 2016.
McNicoll, Arion. “Phonebloks: The Smartphone for 
the Rest of Your Life.” CNN. Cable News Network, 
19 Sept. 2013. Web. 29 Jan. 2016.
Méndez, Luis Argüelles. A Practical Introduction to 
Fuzzy Logic Using LISP. New York, NY: Springer, 
2016. Print.
Menezes, Alfred J., Paul C. van Oorschot, and Scott 
A. Vanstone. Handbook of Applied Cryptography. 
Boca Raton: CRC, 1996. Print.
Mercer, Christina. “5 Best Storage Devices for 
Startups: Removable, SSD or Cloud, What Type 
of Storage Should Your Business Use?” Techworld. 
IDG UK, 22 Dec. 2015. Web. 7 Mar. 2016.
Mermin, David N. Quantum Computer Science: An in-
troduction. New York, NY: Cambridge University 
Press, 2007. Print.
Metcalf, Michael, and John Reid. The F Programming 
Language. New York, NY: Oxford University Press, 
1996. Print.
Metz, Sandi. Practical Object-Oriented Design in Ruby: An 
Agile Primer. Upper Saddle River: Addison, 2012. 
Print.
“Microprocessors: Explore the Curriculum.” Intel. 
Intel Corp., 2015. Web. 11 Mar. 2016.
“Microprocessors.” MIT Technology Review. MIT 
Technology Review, 2016. Web. 11 Mar. 2016.
Mihalcea, Rada and Radev Dragomir. Graph-based 
Natural 
Language 
Processing 
and 
Information 
Retrieval. Cambridge University Press, 2011.
“Milestones of Innovation.” American Institute for 
Medical and Biological Engineering. Amer. Inst. for 
Medical and Biological Engineering, 2016. Web. 
25 Jan. 2016.
Mili, Ali, and Fairouz Tchier. Software Testing: Concepts 
and Operations. Hoboken: Wiley, 2015. Print.
Miller, Charles, and Aaron Doering. The New 
Landscape of Mobile Learning: Redesigning Education 
in an App-Based World. New York: Routledge, 2014. 
Print.
Miller, Michael J. “The Rise of DOS: How Microsoft 
Got the IBM PC OS Contract.” PCMag.com. PCMag 
Digital Group, 10 Aug. 2011. Web. 31 Jan. 2016.
Miller, Michelle D. Minds Online: Teaching Effectively 
with Technology. Cambridge: Harvard UP, 2014. 
Print.
Miniman, 
Stuart. 
“Hyperscale 
Invades 
the 
Enterprise.” Network Computing. UBM, 13 Jan. 
2014. Web. 8 Mar. 2016.
Mir, Nader F. Computer and Communication Networks. 
2nd ed. Upper Saddle River: Prentice, 2015. Print.
Mishra, Umesh. Semiconductor Device Physics and 
Design. Place of publication not identified: 
Springer, 2014. Print.
Miszczak, Jarosław Adam. High-Level Structures for 
Quantum Computing. Williston, VT: Morgan and 
Claypool, 2012. Print.
Mitchell, Jamie L., and Rex Black. Advanced Software 
Testing. 2nd ed. 3 vols. Santa Barbara: Rocky Nook, 
2015. Print.
Mitra, Tilak. Practical Software Architecture: Moving from 
System Context to Deployment. Indianapolis: IBM, 
2015. Print.

351
Principles of Computer Science
Bibliography
Modi, Shimon K. Biometrics in Identity Management. 
Boston: Artech House, 2011. Print.
Mooallem, Jon. “The Afterlife of Cellphones.” New 
York Times Magazine. New York Times, 13 Jan. 2008. 
Web. 9 Feb. 2016.
Morreale, Patricia, and Kornel Terplan, eds. The CRC 
Handbook of Modern Telecommunications. 2nd ed. 
Boca Raton: CRC, 2009. Print.
Morrison, Foster. The Art of Modeling Dynamic Systems: 
Forecasting for Chaos, Randomness, and Determinism. 
1991. Mineola: Dover, 2008. Print.
Morselli, Carlo, ed. Crime and Networks. New York: 
Routledge, 2014. Print.
Moss, Frank. The Sorcerers and Their Apprentices: 
How the Digital Magicians of the MIT Media Lab 
Are Creating the Innovative Technologies That Will 
Transform Our Lives. New York: Crown Business, 
2011. Print.
Moynihan, Tim. “Things Will Get Messy If We Don’t 
Start Wrangling Drones Now.” Wired. Condé Nast, 
30 Jan. 2015. Web. 30 Jan. 2016.
“MS-DOS: A Brief Introduction.” Linux Information 
Project. Linux Information Project, 30 Sept. 2006. 
Web. 31 Jan. 2016.
Mueller, Scott. Upgrading and Repairing PCs. 22nd ed. 
Indianapolis: Que, 2015. Print.
Murphy, Chris. “How to Save $150 Billion: Move All 
App Dev and Testing to the Cloud.” Forbes. Forbes.
com, 3 Feb. 2016. Web. 10 Feb. 2016.
Murphy, Michael P., and Metin Sitti. “Waalbot: Agile 
Climbing with Synthetic Fibrillar Dry Adhesives.” 
2009 IEEE International Conference on Robotics and 
Automation. Piscataway: IEEE, 2009. IEEE Xplore. 
Web. 21 Jan. 2016.
Myers, Courtney Boyd, ed. The AI Report. Forbes. 
Forbes.com, 22 June 2009. Web. 18 Dec. 2015.
Myers, Glenford J., Tom Badgett, and Corey Sandler. 
The Art of Software Testing. Hoboken: Wiley, 2012. 
Print.
Naraine, Ryan. “Metasploit’s H. D. Moore Releases 
‘War Dialing’ Tools.” ZDNet. CBS Interactive, 6 
Mar. 2009. Web. 15 Mar. 2016.
Nayeem, Sk. Md. Abu, Jyotirmoy Mukhopadhyay, and 
S. B. Rao, eds. Mathematics and Computing: Current 
Research and Developments. New Delhi: Narosa, 
2013. Print.
Neapolitan, Richard E. Foundations of Algorithms. 5th 
ed. Burlington: Jones, 2015. Print.
Neiderreiter, Harald, and Chaoping Xing. Algebraic 
Geometry in Coding Theory and Cryptography. 
Princeton: Princeton UP, 2009. Print.
Netzley, Patricia D. How Serious a Problem Is Computer 
Hacking? San Diego: ReferencePoint, 2014. Print.
Neuburg, Matt. Programming iOS 8: Dive Deep into 
Views, View Controllers, and Frameworks. Sebastopol: 
O’Reilly Media, 2014. Print.
Newman, Jared. “Android Laptops: The $200 Price 
Is Right, but the OS May Not Be.” PCWorld. IDG 
Consumer & SMB, 26 Apr. 2013. Web. 27 Jan. 
2016.
Newman, Jared. “With Android Lollipop, Mobile 
Multitasking Takes a Great Leap Forward.” Fast 
Company. Mansueto Ventures, 6 Nov. 2014. Web. 
27 Jan. 2016.
Nicolelis, Miguel A. and Ronald M. Cicurel. The 
Relativistic Brain: How it works and why it cannot 
be simulated by a Turing machine. CreateSpace 
Independent Publishing Platform, 2015.
Nielsen, Jakob. “F-Shaped Pattern for Reading Web 
Content.” Nielsen Norman Group. Nielsen Norman 
Group, 17 Apr. 2006. Web. 4 Mar. 2016.
Noergaard, Tammy. Embedded Systems Architecture: 
A Comprehensive Guide for Engineers and Programmers. 
2nd ed. Boston: Elsevier, 2013. Print.
Norman, Don. The Design of Everyday Things. Rev. and 
expanded ed. New York: Basic, 2013. Print.
Northrup, Tony. “Firewalls.” TechNet. Microsoft, n.d. 
Web. 28 Feb. 2016.
Nystrom, Robert. Game Programming Patterns. N.p.: 
Author, 2009–14. Web. 16 Mar. 2016.
“Online Privacy: Using the Internet Safely.” Privacy 
Rights Clearinghouse. Privacy Rights Clearinghouse, 
Jan. 2016. Web. 28 Feb. 2016.
O’Regan, Gerard. A Brief History of Computing 2nd ed., 
New York, NY: Springer-Verlag, 2012. Print.
Ohanian, Thomas. Digital Nonlinear Editing: Editing 
Film and Video on the Desktop. Woburn: Focal, 1998. 
Print.
Olley, Allan. “Can Machines Think Yet? A Brief 
History of the Turing Test.” Bubble Chamber. U of 
Toronto’s Science Policy Working Group, 23 June 
2014. Web. 18 Dec. 2015.
Oloruntoba, Samuel. “SOLID: The First 5 Principles 
of Object Oriented Design.” Scotch. Scotch.io, 18 
Mar. 2015. Web. 1 Feb. 2016.
Openshaw, Stan, and Ian Turton. High Performance 
Computing and the Art of Parallel Programming: An 

352
Bibliography
Principles of Computer Science
Introduction for Geographers, Social Scientists, and 
Engineers. London: Routledge, 2005. Print. 
Oppy, Graham, and David Dowe. “The Turing Test.” 
Stanford Encyclopedia of Philosophy (Spring 2011 
Edition). Ed. Edward N. Zalta. Stanford U, 26 Jan. 
2011. Web. 18 Dec. 2015.
Organick, E.I., Forsythe, A.I. and Plummer, R.P. 
(1978) Programming Language Structures New York, 
NY: Academic Press. Print.
Orwick, Penny, and Guy Smith. Developing Drivers with 
the Windows Driver Foundation. Redmond: Microsoft 
P, 2007. Print.
“Our Story.” CompTIA. Computing Technology 
Industry Assn., n. d. Web. 31 Jan. 2016.
“Our Story.” Pixar. Pixar, 2016. Web. 31 Jan. 2016.
Owen, Mark. Practical Signal Processing. New York: 
Cambridge UP, 2012. Print.
Paar, 
Christof, 
and 
Jan 
Pelzi. 
Understanding 
Cryptography: A Textbook for Students and Practitioners. 
Heidelberg: Springer, 2010. Print.
Paddock, Catharine. “How Self-Monitoring Is 
Transforming Health.” Medical News Today. 
MediLexicon Intl., 15 Aug. 2013. Web. 26 Feb. 
2016.
Pandolfi, Luciano. Distributed Systems with Persistent 
Memory: Control and Moment Problems. New York: 
Springer, 2014. Print.
Pandzu, Abhujit S. and Robert B. Macy. Pattern 
Recognition with Neural Networks in C++. Boca Raton, 
FL: CRC Press, 1996. Print.
Parashar, Manish, and Salim Hariri, eds. Autonomic 
Computing: Concepts, Infrastructure, and Applications. 
Boca Raton: CRC, 2007. Print.
Parent, Rick. Computer Animation: Algorithms and 
Techniques. Waltham: Elsevier, 2012. Print.
Parisi, Tony. Learning Virtual Reality: Developing 
Immersive Experiences and Applications for Desktop, 
Web, and Mobile. Sebastopol: O’Reilly, 2015. Print.
Parker, Jason, “The Continuing Evolution of iOS.” 
CNET. CBS Interactive, 7 May 2014. Web. 26 Feb. 
2016.
“Part Two: Communicating with Computers—The 
Operating System.” Computer Programming for 
Scientists. Oregon State U, 2006. Web. 31 Jan. 2016.
Patel, Ruchika, and Parth Bhatt. “A Review Paper 
on Digital Watermarking and Its Techniques.” 
International Journal of Computer Applications 110.1 
(2015): 10–13. Web. 14 Mar. 2016.
Patel, Shuchi, and Avani Kasture. “E (Electronic) 
Waste Management Using Biological Systems—
Overview.” 
International 
Journal 
of 
Current 
Microbiology and Applied Sciences 3.7 (2014): 495–
504. Web. 9 Feb. 2016.
Pathak, Parth H., and Rudra Dutta. Designing for 
Network and Service Continuity in Wireless Mesh 
Networks. New York: Springer, 2013. Print.
Patrizio, Andy. “The History of Visual Development 
Environments: Imagine There’s No IDEs. It’s 
Difficult If You Try.” Mendix. Mendix, 4 Feb. 2013. 
Web. 23 Feb. 2016.
Patterson, David A., and John L. Hennessy. Computer 
Organization and Design: The Hardware/Software 
Interface. 5th ed. Waltham: Morgan, 2013. Print.
Pavel, M., et al. “The Role of Technology and 
Engineering Models in Transforming Healthcare.” 
IEEE Reviews in Biomedical Engineering. IEEE, 2013. 
Web. 25 Jan. 2016.
“Personalized Medicine and Pharmacogenomics.” 
Mayo Clinic. Mayo Foundation for Medical 
Education and Research, 5 June 2015. Web. 23 
Dec. 2015.
Pele, Maria, and Carmen Cimpeanu. Biotechnology: 
An Introduction. Billerica: WIT, 2012. Print.
Pelleau, Marie, and Narendra Jussien. Abstract 
Domains in Constraint Programming. London: ISTE, 
2015. Print.
Peterson, James L. Computer Organization and Assembly 
Language Programming New York, NY: Academic 
Press, 1978. Print.
Peterson, Larry L., and Bruce S. Davie. Computer 
Networks: A Systems Approach. 5th ed. Burlington: 
Morgan, 2012. Print.
Petzold, Charles. The Annotated Turing: A Guided Tour 
Through Alan Turing’s Historic Paper on Computability 
and the Turing Machine. Wiley, 2008.
Pinch, T. J., and Karin Bijsterveld. The Oxford 
Handbook of Sound Studies. New York: Oxford UP, 
2013. Print.
Pinola, Melanie. “Speech Recognition through the 
Decades: How We Ended Up with Siri.” PCWorld. 
IDG Consumer & SMB, 2 Nov. 2011. Web. 21 Mar. 
2016.
“Planned 
Obsolescence: 
A 
Weapon 
of 
Mass 
Discarding, or a Catalyst for Progress?” ParisTech 
Review. ParisTech Rev., 27 Sept. 2013. Web. 9 Feb. 
2016.

353
Principles of Computer Science
Bibliography
Plumb, Charles. “Drones in the Workplace.” 
EmployerLINC. McAfee and Taft, 14 Dec. 2015. 
Web. 21 Jan. 2016.
Pollitt, Mark. “A History of Digital Forensics.” 
Advances in Digital Forensics VI. Ed. Kam-Pui Chow 
and Sujeet Shenoi. Berlin: Springer, 2010. 3–15. 
Print.
Pourhashemi, Ali, ed. Chemical and Biochemical 
Engineering: New Materials and Developed Components. 
Rev. Gennady E. Zaikov and A. K. Haghi. Oakville: 
Apple Acad., 2015. Print.
Prandoni, Paolo, and Martin Vetterli. Signal Processing 
for Communications. Boca Raton: CRC, 2008. Print.
Prasad, Bhanu, and S.R. Mahadeva Prasanna, eds. 
Speech, Audio, Image and Biomedical Signal Processing 
Using Neural Networks. New York, NY: Springer, 
2008. Print.
“Precision (Personalized) Medicine.” US Food and 
Drug Administration. Dept. of Health and Human 
Services, 18 Nov. 2015. Web. 23 Dec. 2015.
Priddy, Kevin L. and Keller, Paul E. Artificial Neural 
Networks, An Introduction. Bellingham, WA: SPIE 
Press, 2005. Print.
Proakis, John G., and Dimitris G. Manolakis. Digital 
Signal 
Processing: 
Principles, 
Algorithms, 
and 
Applications. 4th ed. Upper Saddle River: Prentice, 
2007. Print.
Prokopenko, Mikhail. Advances in Applied Self-
Organizing Systems. London: Springer, 2013. Print.
“Pros & Cons of Website Templates.” Entheos. 
Entheos, n.d. Web. 22 Feb. 2016
“Protect Your Privacy on the Internet.” Safety and 
Security Center. Microsoft, n. d. Web. 28 Feb. 2016.
Protalinski, Emil. “Windows 10 Ends 2015 under 10% 
Market Share.” VentureBeat. VentureBeat, 1 Jan. 
2016. Web. 26 Feb. 2016.
Pu, Di, and Alexander M. Wyglinski. Digital 
Communication Systems Engineering with Software-
Defined Radio. London: Artech, 2013. Print.
Pullen, John Patrick. “This Is How Drones Work.” 
Time. Time, 3 Apr. 2015. Web. 27 Jan. 2016.
Puryear, Martin. “Programming Trends to Look for 
This Year.” TechCrunch. AOL, 13 Jan. 2016. Web. 7 
Feb. 2016.
Quian, Quiroga R., and Stefano Panzeri. Principles of 
Neural Coding. Boca Raton: CRC, 2013. Print.
Rahimi, Saeed K., and Frank S. Haug. Distributed 
Database Management Systems: A Practical Approach. 
Hoboken: Wiley, 2010. Print.
Rajaraman, V. Computer Programming in Fortran 77. 4th 
ed., New Delhi, IND: Prentice-Hall of India Pvt., 
2006. Print.
Rajasekaran, 
Sanguthevar. 
Multicore 
Computing: 
Algorithms, Architectures, and Applications. Boca 
Raton: CRC, 2013. Print.
Ramasubbu, 
Suren. 
“How 
Technology 
Can 
Help Language Learning.” Huffington Post. 
TheHuffingtonPost.com, 3 June 2015. Web. 19 
Jan. 2016.
Rao, M. Ananda, and J. Srinavas. Neural Networks. 
Algorithms and Applications. Pangbourne, UK: 
Alpha Science International, 2003. Print.
Rasmussen, Nicolas. Gene Jockeys: Life Science and the 
Rise of Biotech Enterprise. Baltimore: Johns Hopkins 
UP, 2014. Print.
Ravindranath, Mohana. “PCs Lumber towards the 
Technological Graveyard.” Guardian. Guardian 
News and Media, 11 Feb. 2014. Web. 10 Mar. 2016.
Rawls, Rod. R., Paul F. Richard and Mark A. Hagen. 
Visual LISP Programming: Principles and Techniques 
Tinley Park, IL: Goodheart-Willcox, 2007. Print.
Raymond, Eric S. “Origins and History of Unix, 
1969–1995.” The Art of UNIX Programming. Boston: 
Pearson Education, 2004. Print.
Reed, Jeffrey H. Software Radio: A Modern Approach to 
Radio Engineering. New York: Prentice, 2002. Print.
Reimer, Jeremy. “A History of the GUI.” Ars Technica. 
Condé Nast, 5 May 2005. Web. 31 Jan. 2016.
Ribble, Mike. Digital Citizenship in Schools: Nine 
Elements All Students Should Know. Eugene: Intl. 
Soc. for Technology in Education, 2015. Print.
Rice, Daniel M. Calculus of Thought: Neuromorphic 
Logistic Regression in Cognitive Machines. Waltham: 
Academic, 2014. Print.
Rieffel, Eleanor, and Wolfgang Polak. Quantum 
Computing: A Gentle Introduction. Cambridge, MA: 
MIT Press, 2011. Print.
Roberts, Fred S., and Barry Tesman. Applied 
Combinatorics. 2nd ed. Boca Raton: Chapman, 
2012. Print.
Roberts, Richard M. Computer Service and Repair. 4th 
ed. Tinley Park: Goodheart, 2015. Print.
Roblyer, M. D., and Aaron H. Doering. Integrating 
Educational Technology into Teaching. 6th ed. Boston: 
Pearson, 2013. Print.
Rockett, Angus. The Materials Science of Semiconductors. 
New York, NY: Springer, 2010. Print.

354
Bibliography
Principles of Computer Science
Rogers, Joey. Object-Oriented Neural Networks in C++. 
New York, NY: Academic Press, 1997. Print.
Rogers, Scott. Swipe This!: The Guide to Great Touchscreen 
Game Design. Chichester: Wiley, 2012. Print.
Roosta, Seyed H. Parallel Processing and Parallel 
Algorithms: Theory and Computation. New York: 
Springer, 2013. Print. 
Rountree, Derrick, and Ileana Castrillo. The Basics 
of Cloud Computing. Waltham: Elsevier, 2014. 
Print.
Rouse, Margaret. “Sarbanes-Oxley Act (SOX).” 
TechTarget. TechTarget, June 2014. Web. 31 Mar. 
2014.
Rozanski, Nick, and Eoin Woods. Software Systems 
Architecture: 
Working 
with 
Stakeholders 
Using 
Viewpoints and Perspectives. 2nd ed. Upper Saddle 
River: Addison, 2012. Print.
Rubin, Michael. Nonlinear—A Field Guide to Digital 
Video and Film Editing. Gainesville: Triad, 2000, 
Print.
Rutishauer, Heinz (1967) “Description of ALGOL-
60” Chapter in Bauer, F.L., Householder, I.S., 
Olver, F.W.J., Rutishauer, H., Samelson, K. and 
Stiefel, E., eds. Handbook for Automatic Computation 
Berlin, GER: Springer-Verlag. Print.
Ryan, Janel. “Five Basic Things You Should Know 
about Cloud Computing.” Forbes. Forbes.com, 30 
Oct. 2013. Web. 30 Oct. 2013.
Sabbatini, Renato M. E. “Imitation of Life: A History 
of the First Robots.” Brain & Mind 9 (1999): n. 
pag. Web. 21 Jan. 2016.
Saffer, Dan. Designing Gestural Interfaces. Beijing: 
O’Reilly, 2008. Print.
Saltman, Dave. “Tech Talk: Turning Digital Natives 
into Digital Citizens.” Harvard Education Letter 
27.5 (2011): n. pag. Harvard Graduate School of 
Education. Web. 27 Jan. 2016.
Saltzman, Steven. Music Editing for Film and Television: 
The Art and the Process. Burlington: Focal, 2015. 
Print.
Saltzman, W. Mark. “Lecture 1—What Is Biomedical 
Engineering?” BENG 100: Frontiers of Biomedical 
Engineering. Yale U, Spring 2008. Web. 23 Jan. 
2016.
Salz, Peggy Anne, and Jennifer Moranz. The Everything 
Guide to Mobile Apps: A Practical Guide to Affordable 
Mobile App Development for Your Business. Avon: 
Adams Media, 2013. Print.
Sammons, John. The Basics of Digital Forensics: 
The Primer for Getting Started in Digital Forensics. 
Waltham: Syngress, 2012. Print.
Sandberg, Bobbi. Networking: The Complete Reference. 
3rd ed. New York: McGraw, 2015. Print.
Sanders, James. “Hybrid Cloud: What It Is, Why It 
Matters.” ZDNet. CBS Interactive, 1 July 2014. Web. 
10 Jan. 2016.
Sarkar, Jayanta. Computer Aided Design: A Conceptual 
Approach. Boca Raton: CRC, 2015. Print.
Savage, Terry Michael, and Karla E. Vogel. An 
Introduction 
to 
Digital 
Multimedia. 
2nd 
ed. 
Burlington: Jones, 2014. 256–58. Print.
Savischenko, Nikolay V. Special Integral Functions Used 
in Wireless Communications Theory. N.p.: World 
Scientific, 2014. Digital file.
Schmidt, Richard F. Software Engineering: Architecture-
driven Software Development. Waltham: Morgan, 
2013. Print.
Schmidt, 
Silke, 
and 
Otto 
Rienhoff, 
eds. 
Interdisciplinary Assessment of Personal Health 
Monitoring. Amsterdam: IOS, 2013. Print.
Schmitt, Christopher. Designing Web & Mobile Graphics: 
Fundamental Concepts for Web and Interactive Projects. 
Berkeley: New Riders, 2013. Print.
Schou, Corey, and Steven Hernandez. Information 
Assurance Handbook: Effective Computer Security and 
Risk Management Strategies. New York: McGraw, 
2015. Print.
Schwartz, John. “In the Lab: Robots That Slink and 
Squirm.” New York Times. New York Times, 27 Mar. 
2007. Web. 21 Jan. 2016.
Scott, Michael L. Programming Language Pragmatics. 
4th ed., Waltham, MA: Morgan Kaufmann, 2016. 
Print.
Segall, Richard S., Jeffrey S. Cook, and Qingyu 
Zhang, eds. Research and Applications in Global 
Supercomputing. Hershey: Information Science 
Reference, 2015. Print.
Seibel, Peter. Practical Common LISP New York, NY: 
Apress/Springer-Verlag. Print.
Seidl, Martina, et al. UML@Classroom: An Introduction 
to Object-Oriented Modeling. Cham: Springer, 2015. 
Print.
Serpanos, Dimitrios N., and Tilman Wolf. Architecture 
of Network Systems. Burlington: Morgan, 2011. Print.
“Shadow RAM Basics.” Microsoft Support. Microsoft, 4 
Dec. 2015. Web. 10 Mar. 2016.

355
Principles of Computer Science
Bibliography
Shahani, Aarthi. “Biometrics May Ditch the Password, 
But Not the Hackers.” All Things Considered. NPR, 
26 Apr. 2015. Web. 21 Jan. 2016.
Shakarian, Paulo, Jana Shakarian, and Andrew Ruef. 
Introduction to Cyber-Warfare: A Multidisciplinary 
Approach. Waltham: Syngress, 2013. Print.
Shen, Jialie, John Shepherd, Bin Cui, and Ling 
Liu. Intelligent Music Information Systems: Tools and 
Methodologies. Hershey: IGI Global, 2008. Print.
Shenoi, Belle A. Introduction to Digital Signal Processing 
and Filter Design. Hoboken: Wiley, 2006. Print.
Shinder, Deb. “So You Want to Be a Computer 
Forensics Expert.” TechRepublic. CBS Interactive, 
27 Dec. 2010. Web. 2 Feb. 2016.
Shustek, Len. “Microsoft MS-DOS Early Source 
Code.” Computer History Museum. Computer 
History Museum, 2013. Web. 31 Jan. 2016.
Silberschatz, Abraham, Peter B. Galvin, and 
Greg Gagne. Operating Systems Concepts. 9th ed. 
Hoboken: Wiley, 2012. Print.
Silberschatz, Abraham, Peter B. Galvin, and Greg 
Gagne. Operating System Concepts Essentials. 2nd ed. 
Wiley, 2014. Print.
Silver, H. Ward. “Digital Code Basics.” Qst 98.8 
(2014): 58–59. PDF file.
Simpson, James, ed. The Routledge Handbook of Applied 
Linguistics. New York: Routledge, 2011. Print.
Singh, Amandeep. “Top 13 Tips for Writing Effective 
Test Cases for Any Application.” Quick Software 
Testing. QuickSoftwareTesting, 23 Jan. 2014. Web. 
11 Feb. 2016.
Sinnen, Oliver. Task Scheduling for Parallel Systems. 
Hoboken: Wiley, 2007. Print.
Sito, Tom. Moving Innovation: A History of Computer 
Animation. Cambridge: MIT P, 2013. Print.
Smith, Bud E. Green Computing: Tools and Techniques 
for Saving Energy, Money, and Resources. Boca Raton: 
CRC, 2014. Print.
Smith, Catharine. “Tim Cook Issues Apology for 
Apple Maps.” Huffpost Tech. TheHuffingtonPost.
com, 28 Sept. 2012. Web. 11 Feb. 2016.
Smith, Eric N. Workplace Security Essentials: A Guide for 
Helping Organizations Create Safe Work Environments. 
Oxford: Butterworth-Heinemann, 2014. Print.
Smith, Matt. “Wi-Fi vs. Ethernet: Has Wireless Killed 
Wired?” Digital Trends. Designtechnica, 18 Jan. 
2013. Web. 22 Feb. 2016.
Snoke, David W. Electronics: A Physical Approach. 
Boston: Addison, 2014. Print.
Soare, Robert I. Turing Computability: Theory 
and Applications (Theory and Applications of 
Computability). Springer, 2016.
Soares, Marcelo M., and Francisco Rebelo. Advances 
in Usability Evaluation. Boca Raton: CRC, 2013. 
Print.
Solnon, Christine. Ant Colony Optimization and 
Constraint Programming. Hoboken: Wiley, 2010. 
Print.
Solomon, Nancy B., ed. Architecture: Celebrating 
the Past, Designing the Future. New York: Visual 
Reference, 2008. Print.
Song, Dong-Ping. Optimal Control and Optimization of 
Stochastic Supply Chain Systems. London: Springer, 
2013. Print.
Sonmez, John Z. Soft Skills: The Software Developer’s Life 
Manual. Shelter Island: Manning, 2015. Print.
Sottilare, R., Graesser, A., Hu, X., and Holden, H. 
(Eds.). (2013). Design Recommendations for Intelligent 
Tutoring Systems: Volume 1 - Learner Modeling. 
Orlando, FL: U.S. Army Research Laboratory. 
ISBN 978-0-9893923-0-3. Available at: https://gift-
tutoring.org/documents/42
Sozan´ski, Krzysztof. Digital Signal Processing in Power 
Electronics Control Circuits. New York: Springer, 
2013. Print.
Spence, Ewan. “New Android Malware Strikes at 
Millions of Smartphones.” Forbes. Forbes.com,  
4 Feb. 2015. Web. 11 Mar. 2016.
“Spyware.” Secure Purdue. Purdue U, 2010. Web. 11 
Mar 2016.
St. Germain, H. James de. “Debugging Programs.” 
University of Utah. U of Utah, n.d. Web. 31 Jan. 
2016.
“State Laws Related to Internet Privacy.” National 
Conference of State Legislatures. NCSL, 5 Jan. 2016. 
Web. 28 Mar. 2016.
Stallings, William, and Lawrie Brown. Computer 
Security: Principles and Practice. 3rd ed. Boston: 
Pearson, 2015. Print.
Stallings, William. Operating Systems: Internals and 
Design Principles. Boston: Pearson, 2015. Print.
Stanley, Jay. “‘Drones’ vs ‘UAVs’—What’s behind a 
Name?” ACLU. ACLU, 20 May 2013. Web. 27 Jan. 
2016.
Stanton, Jeffrey, and Kathryn R. Slam. The Visible 
Employee: Using Workplace Monitoring and Surveillance 
to Protect Information Assets without Compromising 

356
Bibliography
Principles of Computer Science
Employee Privacy or Trust. Medford: Information 
Today, 2006. Print.
Stein, N. L., and S.W. Raudenbush, eds. Developmental 
Cognitive Science Goes to School. New York: Routledge, 
2011. Print
Steiner, Craig (1990) The 8051/8052 Microcontroller: 
Architecture, Assembly Language and Hardware 
Interfacing Boca Raton, FL: Universal Publishers. 
Print.
Sterling, Leon, ed. The Practice of Prolog. Boston, MA: 
MIT Press,1990. Print.
Stevens, Tim. “Fitbit Review.” Engadget. AOL, 15 Oct. 
2009. Web. 28 Feb. 2016.
“Storage vs. Memory.” Computer Desktop Encyclopedia. 
Computer Lang., 1981–2016. Web. 10 Mar. 2016.
Stokes, Jon. Inside the Machine: An Illustrated 
Introduction to Microprocessors and Computer 
Architecture. San Francisco: No Starch, 2015. 
Print.
Stokes, Jon. “RAM Guide Part I: DRAM and SDRAM 
Basics.” Ars Technica. Condé Nast, 18 July 2000. 
Web. 10 Mar. 2016.
Stonebank, M. “UNIX Introduction.” University of 
Surrey. U of Surrey, 2000. Web. 28 Feb. 2016.
Streib, James T. (2011) Guide to Assembly Language. 
A Concise Introduction New York, NY: Springer. 
Print.
Streib, James T., and Takako Soma. Guide to Java: 
A Concise Introduction to Programming. New York: 
Springer, 2014. Print.
Stroustrup, Bjarne. The C++ Programming Language 
4th  ed. Upper Saddle River, NJ: Addison-Wesley, 
2013. Print.
Stuart, Allison. “File Formats Explained: PDF, PNG 
and More.” 99Designs. 99Designs, 21 May 2015. 
Web. 11 Feb. 2016.
Sun, Jiming, Vincent Zimmer, Marc Jones, and Stefan 
Reinauer. Embedded Firmware Solutions: Development 
Best Practices for the Internet of Things. Berkeley: 
ApressOpen, 2015. Print.
Tabak, Filiz, and William Smith. “Privacy and 
Electronic Monitoring in the Workplace: A Model 
of Managerial Cognition and Relational Trust 
Development.” Employee Responsibilities and Rights 
Journal 17.3 (2005): 173–89. Print.
Tabini, Marco. “Hidden Magic: A Look at the Secret 
Operating System inside the iPhone.” MacWorld. 
IDG Consumer & SMB, 20 Dec. 2013. Web. 9 Mar. 
2016.
Takahashi, Dean. “The App Economy Could Double 
to $101 Billion by 2020.” VB. Venture Beat, 10 Feb. 
2016. Web. 11 Mar. 2016.
Talbot, James, and Justin McLean. Learning Android 
Application Programming: A Hands- On Guide to 
Building Android Applications. Upper Saddle River: 
Addison, 2014. Print.
Tan, Li, and Jean Jiang. Digital Signal Processing: 
Fundamentals and Applications. 2nd ed. Boston: 
Academic, 2013. Print.
Tanenbaum, Andrew S., and Herbert Bos. Modern 
Operating Systems. 4th ed. New York: Pearson, 2014. 
Print.
Tarantola, Andrew. “Why Frame Rate Matters.” 
Gizmodo. Gizmodo, 14 Jan. 2015. Web. 11 Mar. 
2016.
Tatnall, Arthur, and Bill Davey, eds. Reflections on 
the History of Computers in Education: Early Use of 
Computers and Teaching about Computing in Schools. 
Heidelberg: Springer, 2014. Print.
Taylor, Ben. “Cloud Storage vs. External Hard Drives: 
Which Really Offers the Best Bang for your Buck?” 
PCWorld. IDG Consumer and SMB, 10 July 2014. 
Web. 7 Mar. 2016.
Taylor, Harriet. “How Your Home Will Know What 
You Need Before You Do.” CNBC. CNBC, Jan 6 
2016. Web. 11 Mar. 2016.
Taylor, Richard N., Nenad Medvidovic´, and Eric M. 
Dashofy. Software Architecture: Foundations, Theory, 
and Practice. Hoboken: Wiley, 2010. Print
“The Digital Millennium Copyright Act of 1998.” 
Copyright. US Copyright Office, 28 Oct 1998. Web. 
23 Jan 2016.
“The History of the Integrated Circuit.” Nobelprize.org. 
Nobel Media, 2014. Web. 31 Mar. 2016.
“The 
Mind-Blowing 
Possibilities 
of 
Quantum 
Computing.” TechRadar. Future, 17 Jan. 2010. 
Web. 26 Mar. 2016.
“The 
Printed 
World.” 
Economist. 
Economist 
Newspaper, 10 Feb. 2011. Web. 6 Jan. 2016.
“The Unicode Standard: A Technical Introduction.” 
Unicode.org. Unicode, 25 June 2015. Web. 3 Mar. 
2016.
Thieman, William J., and Michael A. Palladino. 
Introduction to Biotechnology. 3rd ed. San Francisco: 
Benjamin, 2012. Print.
“Timeline of Computer History: 1963.” Computer 
History Museum. Computer History Museum,  
1 May 2015. Web. 23 Feb. 2016.

357
Principles of Computer Science
Bibliography
Toal, Ray. “Algorithms and Data Structures.” Ray 
Toal. Loyola Marymount U, n.d. Web. 19 Jan. 
2016.
Tomei, Lawrence A., ed. Encyclopedia of Information 
Technology Curriculum Integration. 2 vols. Hershey: 
Information Science Reference, 2008. Print.
Tomlinson, Brian, ed. Applied Linguistics and Materials 
Development. New York: Bloomsbury, 2013. Print.
Tooley, Mike. Electronic Circuits: Fundamentals and 
Applications. 4th ed. New York: Routledge, 2015. 
Print.
Toomey, Warren. “The Strange Birth and Long  
Life of Unix.” IEEE Spectrum. IEEE, 28 Nov. 2011. 
Web. 28. Feb. 2016.
Topol, Eric J. The Creative Destruction of Medicine: How 
the Digital Revolution Will Create Better Health Care. 
New York: Basic, 2013. Print.
Tosoni, Simone, Matteo Tarantino, and Chiara 
Giaccardi, eds. Media and the City: Urbanism, 
Technology and Communication. Newcastle upon 
Tyne: Cambridge Scholars, 2013. Print.
Touretzky, David S. Common LISP. A Gentle Introduction 
to Symbolic Computation Mineola, NY: Dover 
Publications. Print.
Treder, Marcin. “Wireframes vs. Prototypes: What’s 
the Difference?” Six Revisions. Jacob Gube, 11 Apr. 
2014. Web. 4 Mar. 2016.
Tripathy, B. K., and D. P. Acharjya, eds. Global Trends 
in Intelligent Computing Research and Development. 
Hershey: Information Science Reference, 2014. 
Print.
Tucker, Allen B., Ralph Morelli, and Chamindra de 
Silva. Software Development: An Open Source Approach. 
Boca Raton: CRC, 2011. Print.
Tucker, Allen B, Teofilo F. Gonzalez, and Jorge L. 
Diaz-Herrera. Computing Handbook. Boca Raton, 
FL: CRC Press, 2014. Print. 
Tulchinsky, Theodore H., Elena Varavikova, Joan D. 
Bickford, and Jonathan E. Fielding. The New Public 
Health. New York: Academic, 2014. Print.
Turing, Alan M. “Computing Machinery and 
Intelligence.” Mind 59.236 (1950): 433–60. Web. 
23 Dec. 2015.
“Turing Test.” Encyclopædia Britannica. Encyclopædia 
Britannica, 23 Sept. 2013. Web. 18 Dec. 2015.
“Types of Wireless Networks.” Commotion. Open 
Technology Inst., n.d. Web. 3 Mar. 2016.
“Unicode 8.0.0.” Unicode.org. Unicode, 17 June 2015. 
Web. 3 Mar. 2016.
“Unmanned Aircraft Systems (UAS) Frequently 
Asked Questions.” Federal Aviation Administration. 
US Dept. of Transportation, 18 Dec. 2015.  
Web. 11 Feb. 2016.
“Unretouched 
by 
Human 
Hand.” 
Economist. 
Economist Newspaper, 12 Dec. 2002. Web. 14 Mar. 
2016.
“USA Patriot Act.” Electronic Privacy Information Center. 
EPIC, 31 May 2015. Web. 28 Mar. 2016.
“User Interface Design Basics.” Usability. US Dept. of 
Health and Human Services, 2 Feb. 2016. Web. 2 
Feb. 2016.
Vacca, John R. Computer and Information Security 
Handbook. Amsterdam: Kaufmann, 2013. Print.
Vacca, John, ed. Network and System Security. 2nd ed. 
Waltham: Elsevier, 2014. Print.
Van den Broek, Egon L. “Beyond Biometrics.” Procedia 
Computer Science 1.1 (2010): 2511–19. Print.
Van Roy, Peter. Concepts, Techniques, and Models of 
Computer Programming. Cambridge: MIT P, 2004. 
Print.
Van Schuppen, Jan H., and Tiziano Villa, eds. 
Coordination Control of Distributed Systems. Cham: 
Springer, 2015. Print
Vella, Matt. “Nest CEO Tony Fadell on the Future of 
the Smart Home.” Time. Time, 26 June 2014. Web. 
12 Mar. 2016.
Vetterli, Martin, Jelena Kovacevic, and Vivek K. 
Goyal. Foundations of Signal Processing. Cambridge: 
Cambridge UP, 2014. Print.
Vick, Paul (2004) The Visual BASIC .NET Programming 
Language. Boston, MA: Addison-Wesley. Print.
Wachter, Robert M. The Digital Doctor: Hope, Hype, and 
Harm at the Dawn of Medicine’s Computer Age. New 
York: McGraw, 2015. Print.
Wagner, Carl. “Choice, Chance, and Inference.” 
Math.UTK.edu. U of Tennessee, Knoxville, 2015. 
Web. 10 Feb. 2016.
Walker, Henry M. The Tao of Computing. 2nd ed. Boca 
Raton: CRC, 2013. Print.
Wang, Jason. “HIPAA Compliance: What Every 
Developer Should Know.” Information Week. UBM, 
11 July 2014. Web. 26 Feb. 2016.
Wang, John, ed. Optimizing, Innovating, and 
Capitalizing on Information Systems for Operations. 
Hershey: Business Science Reference, 2013. 
Print.
Warren, Tom. “Windows Turns 30: A Visual History.” 
Verge. Vox Media, 19 Nov. 2015. Web. 26 Feb. 2016.

358
Bibliography
Principles of Computer Science
Watanabe, Shinji, and Jen-Tzung Chien. Bayesian 
Speech 
and 
Language 
Processing. 
Cambridge 
University Press, 2015.
Watt, David A. Programming Language Design Concepts. 
West Sussex: Wiley, 2004. Print.
Wei, Hung-Yu, Jarogniew Rykowski, and Sudhir Dixit. 
WiFi, WiMAX, and LTE Multi- Hop Mesh Networks: 
Basic Communication Protocols and Application Areas. 
Hoboken: Wiley, 2013. Print.
Weisfeld, Matt. The Object-Oriented Thought Process. 4th 
ed. Upper Saddle River: Addison, 2013. Print.
Weller, Nathan. “A Look into the Future of Web 
Design: Where Will We Be in 20 Years?” Elegant 
Themes Blog. Elegant Themes, 9 May 2015. Web. 22 
Feb. 2016.
Wells, Chris. The Civic Organization and the Digital 
Citizen: Communicating Engagement in a Networked 
Age. New York: Oxford UP, 2015. Print.
Wexelblat, Richard L. History of Programming 
Languages. New York, NY: Academic Press, 1981. 
Print.
“What Is a Driver?” Microsoft Developer Network. 
Microsoft, n.d. Web. 10 Mar. 2016.
“What Is E-waste?” Step: Solving the E-waste Problem. 
United Nations U/Step Initiative, 2016. Web. 9 
Feb. 2016.
“What Is Personally Identifiable Information (PII)?” 
U Health. U of Miami Health System, n.d. Web. 28 
Feb. 2016.
“What 
Is 
Pharmacogenomics?” 
Genetics 
Home 
Reference. Natl. Insts. of Health, 21 Dec. 2015. Web. 
23 Dec. 2015.
“What Is the CompTIA A+ Certification?” Knowledge 
Base. Indiana U, 15 Jan. 2015. Web. 31 Jan. 2016.
“What Is Unicode?” Unicode.org. Unicode, 1 Dec. 
2015. Web. 3 Mar. 2016.
“What Is Unix?” Knowledge Base. Indiana U, 2015. 
Web. 28 Feb. 2016.
“What Went Wrong? Finding and Fixing Errors 
through Debugging.” Microsoft Developer Network 
Library. Microsoft, 2016. Web. 31 Jan. 2016.
“What You Can Do.” Green Computing. U of California, 
Berkeley, n.d. Web. 28 Feb. 2016.
“When 
Healthcare 
and 
Computer 
Science 
Collide.” Health Informatics and Health Information 
Management. Pearson/U of Illinois at Chicago, 
n.d. Web. 23 Dec. 2015.
“Why Do You Need to Test Software?” BBC Academy. 
BBC, 23 Feb. 2015. Web. 7 Feb. 2016.
White, Ron. How Computers Work: The Evolution of 
Technology. Illus. Tim Downs. 10th ed. Indianapolis: 
Que, 2015. Print.
Wieber, Pierre-Brice, Russ Tedrake, and Scott 
Kuindersma. “Modeling and Control of Legged 
Robots.” Handbook of Robotics. Ed. Bruno Siciliano 
and Oussama Khatib. 2nd ed. N.p.: Springer, 
n.d. (forthcoming). Scott Kuindersma—Harvard 
University. Web. 6 Jan. 2016
Williams, Brad, David Damstra, and Hal Stern. 
Professional WordPress: Design and Development. 3rd 
ed. Indianapolis: Wiley, 2015. Print.
Williams, Rhiannon. “Apple iOS: A Brief History.” 
Telegraph. Telegraph Media Group, 17 Sept. 2015. 
Web. 25 Feb. 2016.
Williams, Rhiannon, “iOS 9: Should You Upgrade?” 
Telegraph. Telegraph Media Group, 16 Sept. 2015. 
Web. 25 Feb. 2016.
Williams, Richard N. Internet Security Made Easy: Take 
Control of Your Computer. London: Flame Tree, 
2015. Print.
Wills, Craig E. “Process Synchronization and 
Interprocess 
Communication.” 
Computing 
Handbook: Computer Science and Software Engineering. 
Ed. Teofilo Gonzalez and Jorge Díaz-Herrera. 3rd 
ed. Boca Raton: CRC, 2014. 52-1–21. Print.
Wilson, Peter. The Circuit Designer’s Companion. 3rd 
ed. Waltham: Newnes, 2012. Print.
Winder, Catherine, and Zahra Dowlatabadi. Producing 
Animation. Waltham: Focal, 2011. Print.
“Wireless 
History 
Timeline.” 
Wireless 
History 
Foundation. Wireless Hist. Foundation, 2016. Web. 
22 Feb. 2016.
Wolf, Marilyn. High Performance Embedded Computing: 
Architectures, 
Applications, 
and 
Methodologies. 
Amsterdam: Elsevier, 2014. Print.
Wood, David. Interface Design: An Introduction to Visual 
Communication in UI Design. New York: Fairchild, 
2014. Print.
Wood, Lamont. “The 8080 Chip at 40: What’s Next 
for the Mighty Microprocessor?” Computerworld. 
Computerworld, 8 Jan. 2015. Web. 12 Mar. 2016.
Woods, Dan. “Why Adopting the Declarative 
Programming Practices Will Improve Your Return 
from Technology.” Forbes. Forbes.com, 17 Apr. 
2013. Web. 2 Mar. 2016.
Worstall, Tim. “Is Unix Now the Most Successful 
Operating System of All Time?” Forbes. Forbes.
com, 7 May 2013. Web. 7 Mar. 2016.

359
Principles of Computer Science
Bibliography
Xue, Su. Data-Driven Image Editing for Perceptual 
Effectiveness. New Haven: Yale U, 2013. Print.
Yakowitz, Will. “When Monitoring Your Employees 
Goes Horribly Wrong.” Inc. Mansueto Ventures, 14 
Feb. 2016. Web. 21 Feb. 2016.
Yamamoto, Jazon. The Black Art of Multiplatform Game 
Programming. Boston: Cengage, 2015. Print.
Yerby, Jonathan. “Legal and Ethical Issues of 
Employee Monitoring.” Online Journal of Applied 
Knowledge Management 1.2 (2013): 44–54. Web. 21 
Jan. 2016.
Yu, F. Richard, Xi Zhang, and Victor C. M. Leung, 
eds. Green Communications and Networking. Boca 
Raton: CRC, 2013. Print.
Yu, P.Y, and M Cardona. Fundamentals of Semiconductors: 
Physics and Materials Properties. Berlin: Springer, 
2010. Print. 
Zeller, Andreas. Why Programs Fail: A Guide to Systematic 
Debugging. Burlington: Kaufmann, 2009. Print.
Zeng, An-Ping, ed. Fundamentals and Application of 
New Bioproduction Systems. Berlin: Springer, 2013. 
Print.
Zetter, Kim. “California Now Has the Nation’s Best 
Digital Privacy Law.” Wired. Condé Nast, 8 Oct. 
2015. Web. 28 Mar. 2016.
Zhong, Jian-Jiang, ed. Future Trends in Biotechnology. 
Berlin: Springer, 2013. Print.
Zhou, Weichang, and Anne Kantardjieff, eds. 
Mammalian Cell Cultures for Biologics Manufacturing. 
Berlin: Springer, 2014. Print


361
0-bit watermarking, 112
1-bit watermarking, 112, 114
3-D computer graphics, 63
3-D printing, 1–3
extrusion printing, 3
liquid, 1–3
powder-based, 3
software and modeling, 1
3-D rendering, 62, 64
3D touch, 164–165
64-bit processor system, 285
86-DOS, 118
A
abstraction, 207, 230, 232
Accelerometer IC, 138–139
access level, 228
activity
rule, 212
trackers, 138, 139
actuators, 90, 119
adaptive instruction, 85
additive color model, 150
additive manufacturing (AM), 1
additive white Gaussian noise (AWGN), 115
address space, 199
Adelman, Leonard, 190
Adelman’s experiment, 190–191
adhesive technology, 46
Adobe Creative Cloud, 14, 55
Adobe Flash, 278
Adobe Photoshop, 20, 75
Adobe Premiere, 80
advancing software development, 217–218
adware, 170–171
affinity chromatography, 38, 39
agile manifesto, 259
agile robotics, 4–6
agile software development, 254, 259
ALGOL, 6–7
history and development, 7
legacy, 8
program characteristics, 7–8
algorithm, 6, 9–10, 31, 56, 71, 206, 220, 267
applications, 10
basics, 9–10
types, 10
ALGOrithmic Language, 7
al-Khwarizmi, Muhammad ibn Musa, 9
Amdahl’s Law, 220–222
American Management Association, 293
American National Standards Institute (ANSI), 
60–61
American Standard Code for Information 
Interchange (ASCII), 21, 272
American Standards Association (ASA), 21
amplifier, 262, 263
analog radio, 262
analog signals, 248
analog-to-digital converter, 263
analog-to-digital signal, 248
analytic combinatorics, 56
Andes, tutoring system, 159
AND gate, 65
Android, 257, 276
changes and grows, 12
OS, 11–13
takes off, 11–12
Android Open Source Project (AOSP), 188, 189
animation. See Computer(s), animation
animators, 64
anthropomorphic, 4, 206
antigen, 39
appearance, web, 279
Apple, 75, 274
Apple Corporation’s Macintosh, 32
Apple iOS, 11, 276
Apple Maps app, 261
Apple’s Siri, 264, 285
Apple’s Touch ID system, 45
applications (apps), 13–16, 185
careers, 14–16
context, 13–14
designers, 13
of digital signal processors, 111
of digital watermarking, 114
evolution, 14
of Flip-Flops, 66
suite, 13
types, 14
INDEX

362
Index
Principles of Computer Science
application-level firewalls, 133, 134
application programming interfaces (APIs), 11–12, 
146
application-specific GUI, 147
applied biometrics, 44–45
applied linguistics, 16
computer science applications, 17–18
development, 16–17
technology, 16
Apps Runtime on Chrome (ARC), 12
architecture software, 18–19
advantages and disadvantages, 20
features, 20
artificial intelligence (AI), 25, 27, 167, 206, 211, 233, 
270
artificial neural networks (ANNs), 211–212
ASCII, 21–23
data, 263
standard, 21–22
table, 22
assembly language, 23, 25, 31, 230
CPU architecture, 23–24
programming format, 24–25
assembly-line robot, 90
Assessment and Learning in Knowledge Spaces 
(ALEKS) mathematics tutor, 160
Association of Better Computer Dealers, 61
asymmetric-key cryptography, 96
asymmetric-key encryption, 130
asynchronous communication, 125
AT&T, 275
attackers, 82
attenuation, 58–59
attributes, 217
audio codec, 76
authentication, 130
and security, 130, 131, 132
author attribution, 207
AutoCAD Architecture, 18
autodesk, 18
automated assistant programs, 265
automatic programs, 292
automatic sequential control system, 90
automatic summarization, 207
automatic teller machine (ATM), 218
automaton, 270
automobile software, 137
autonomic components (ACs), 25, 27
autonomic computing, 25
design, 27
self-managing systems, 25–26
self-star systems, 26–27
autonomic nervous system, 26–27
autonomous, 4, 214
autonomous agent, 90
auto-scaling, 247
AutoTutor, 159
auxiliary utility programs, 118
avatars and simulation
educational applications, 29
mechanics of animation, 29
VR, 28–29
VR in 3-D, 29
axons, 211
B
Babylonia, 43
backdoor, 81, 137
backpropagation algorithm, 211
Backus, John, 140
band gap, 67
bar-codes, 250
base-2 number system, 34
base-10 system, 34
base-16 number system, 34, 36, 272
BASIC, 31–32
background, 31–32
program structure, 32
subroutines vs functions, 32–33
weaknesses, 33
basic input/output system (BIOS), 136, 242
BCD-to-seven-segment decoder/driver, 122, 124
BCPL language, 49
bears, 5
Beauty and the Beast (film), 63
Beginners All-Purpose Symbolic Instruction Code 
(BASIC). See BASIC
behavioral marketing, 161, 163
Bell Laboratories, 275–276
Berkeley Software, 276
Berners-Lee, Tim, 278
binary/hexadecimal representations, 34–37
binary number(s)
system, 34
using hexadecimal to simplify, 36
binder jetting, 1, 183–184

363
Principles of Computer Science
Index
biochemical engineering, 38–40
applications, 39
engineers, 39
methods of analysis, 38–39
bioinformatics, 43, 46
bioinstrumentation, 40, 46
biological marker, 226
biological system, 211
biological technology, 46
biomaterials, 46
biomechanics, 4, 40, 46
biomedical engineering (BME), 40
computers in, 42
defined, 40
engineering biology, 42
tools of medicine, 40–42
“biomedical microelectromechanical system,” 40
bioMEMS, 40
biometrics, 43–45, 223, 226
applied, 44–45
basics, 43
devices, 82
sampling and matching, 43–44
scanning, 112
bionics, 46
Bioremediation, 128
biosignal processing, 43, 172
biotechnology, 46–48
bistable multivibrator devices, 65–66
bit(s), 34, 78
and qubits, 236
rate, 78
width, 21
Black box
manner, 213
testing, 259–260
B language, 49
block delimiters, 8
blotting, 38
Bluetooth, 251
Bluetooth-enabled glove, 17
Blu-ray discs, 243
Boolean calculation operations, 220
Boolean function, 123
bootstrapping, 25, 27
Boston Dynamics, 4
Brain-Based design, 214–215
brain cells, 211
breast cancer (BRCA) genes, 226
bridge, 208, 211, 220
broadcast, 58–59
bugs, 100. See also Debugging
identifying and addressing, 101
building and running code, 156
building information modeling (BIM), 18, 20
bulletin-board systems (BBSs), 102
butterfly effect, 106
byte, 34
C
C++
classes, functions and objects, 49–50
historical aspects and characteristics, 49
program, 50–51
cache, 245, 246
memory, 220
CAD/CAM, 52–54
applications, 52–53
California Electronic Communications Privacy Act, 
229–230
Carrier signal, 112
Cascading Style Sheets (CSS), 278, 280–281
cathode ray tube (CRT), 31–32, 127
Catmull, Ed, 63
cel animation, 62
cell phone towers, 250
central processing unit (CPU), 23, 31–32, 67–68, 
86, 92, 180, 192, 194, 197–198, 211, 220, 
241, 245
architecture and addressing, 23–24
design, 92–94
instruction sets, 93–94
Moore’s Law, 94
RISC, 94
certifying IT, 60–61
channel capacity, 115, 220
character, 6, 21, 233, 267
encoding, 21, 272
standards, 272
system, 272
Fortran, 140
LISP, 167
charge carriers, 67–68
charity, 231
chatbot, 207, 270
chatterbot, 270–271

364
Index
Principles of Computer Science
Cheetah (prototype robot), 4
chip architectures, 111
chip designers, 215
chromatography, 39
chromebooks, 12
Church-Turing thesis, 269
cipher, 95–96
algorithm, 130
ciphertext, 96, 130
circular wait, 98–99
class, 49, 217
functions and objects, 49–50
class-based inheritance, 217–218
clause structure in PROLOG, 234
client’s logo, 288
clinical engineering, 172–173
clock
rate, 65
signal, 65
speed, 65, 180
closed-loop system, 90–91
cloud-based graphic design, 14
cloud computing, 54–56, 109–110, 246–247, 265, 
285
advantages and disadvantages, 55
network design, 54
as service, 55
systems, 261
types, 54–55
websites, 280
cloud network, 244
design, 54
“cloud rot,” 12
clouds and virtual computing, 244
CMYK (cyan, magenta, yellow, and black) model, 
150
code, 232
“code indirect” addressing, 24
“coder-decoder,” 76
coding theory, 56–57, 267
Coffman conditions, 99
cognitive load (Sweller), 159
CollageIt, 75
combinatorial design, 56
combinatorics, 56–58
in algorithm design, 57
application, 57
basics, 56
in coding, 57–58
combining transmissions, 59
command line, 117, 147, 156
interpreter, 275
comment lines, 7
commodities, 127
communication
architecture, 194, 220–221
devices, 119
method, 209
within and between Networks, 209
technology, 58–59
internet communications, 59–60
communities, 291
compilation errors, 101
compiler, 156
complexities, metacomputing, 178
complex tasks, 216
compliance, 167, 256
component-based development, 254–255
compressed data, 149
compression and transmission, 77
CompTIA A+ Certification, 60–63, 61
computational linguistics, 16, 206
computation-intensive process, 227
computer(s), 203
animation, 62–64
2-D, 64
3-D, 64
history, 62–64
mechanics, 29
tools, 64
variables (avars), 28, 62, 64
in biomedical engineering, 42
circuitry
flip flops, 65–66
semiconductors, 67
codes, 101
fan, 91
memory, 69–70, 240–242
flash and solid state memory, 70
random and virtual memory, 70
modeling, 71–73
networks, 208, 289
platform for games, 146
programs, 230
science, 197, 212
scientists, 203
security, 81–82
hardware, 82

365
Principles of Computer Science
Index
network, 82
software, 82–83
system, 228
systems, 98, 100
technicians, 60
viruses, 170
vision, 250
computer-aided design (CAD), 1, 18, 52–53
computer-aided manufacturing (CAM), 18,  
52–53
computer-assisted instruction (CAI), 83–85,  
84–85
adaptive instruction, 85
advantages and disadvantages, 84–85
vs traditional instruction, 83–84
computer-assisted language learning (CALL), 16–17
computer-assisted vs. traditional instruction, 83–84
computer-compatible machinery, 52
computer-driven language, 206
Computer Fraud and Abuse Act (CFAA), 228
computerized diagnostic tools, 172–173
computer numerical control (CNC), 53
computer programming
image editing, 73–75
compression, 74–75
mobile, 75
music editing, 76–78
compression and transmission, 77
culture of remixing, 77
mixing and mastering, 77
motion picture, 77
video editing
defined, 78
in digital world, 79
in practice, 78–79
computer’s bus, 181
computer science, 56–57
applications, 17–18
computer’s processor, 199
computing machinery and intelligence, 270
Computing Technology Industry Association 
(CompTIA), 60
conceptual physics, 159
concurrent processes, 221
conduction band, 67
conductivity and electrical current, 67
connecting smart devices and utilities, 251–253
connection machine, 86–87
considerations of communication technology, 59
constraint, 235
bases modeling, 160
programming
feasibility vs. optimization, 89
models of, 87–88
content management system (CMS), 282
context switch, 196, 198–199
continuous model, 71
control characters, 21
control program for microcomputers  
(CP/M), 118
control systems, 90
linear, 91
performance, 90–91
control unit design, 92, 235
conventional computer, 220
converter, 262
cookies, internet privacy, 161
cooperation and preemption, 198
cooperative multitasking, 196
copyright rules, 256
core
microprocessor, 180
themes, digital citizenship, 107
voltage, 192
Corel’s Pinnacle Studio Pro, 80
Corpus, 159
Cortana, Microsoft, 264, 285
Costa, Max, 115
counter, 65, 122
counting methods, 56
coupling, 143
Cracker, 102
Creative Cloud, Adobe, 14
crippleware, 112
cross-disciplinary subfield, 4
cross-platform application, 13
crosstalk, 192
crossword puzzles, 221
crutches, 40
cryptographic algorithm, 96
cryptography, 95, 130
in computer age, 96
decrypting cipher, 96–97
defined, 95–96
and encryption, 130
culture of remixing, 77
custom health care, 226
cybercrime, 108

366
Index
Principles of Computer Science
D
data
granularity, 223
integrity, 223
production growth, 155
security, 55
source, 71
width, 180
database, 98
datapath design, 92
“data pointers,” 24
Davis, Martin, 269
deadlock, 98
holding resources, 98–99
responses, 99–100
standoff, 99
debate, 294
debuggers, 101–102
debugging, 100–101. See also Bugs
bugs, 101
in practice, 101–102
decimal system, 34
declarative language, 167, 230–231, 233
declarative programs, 143
decryption key/cipher key, 130
deep learning, 264
deep neural network algorithms, 265
delta debugging, 100, 102
demodulator, 263
demon dialing, 102–103
Wi-Fi, 103
Dennard, Robert, 240–241
dependency inversion principle, 218
designing C++ Program, 50–51
design of autonomic systems, 27
desktop computers, 75
destructive editing, 74
detection algorithm, 100
deterministic algorithm, 9–10
Development of Web Design Techniques, 281–282
device, 154
drivers, 104
making of, 104–105
virtual, 105
work, 104
fingerprinting, 81–82, 161
managers, 104, 105
dexterity, 4
diagrammatic method, 87
dial-up modem, 102
digital and analog Signals, 248
digital and software-defined radio, 262–263
digital citizenship, 106, 107
defined, 106
digital security and responsibility, 108
ethics of, 107–108
digital commerce, 106
digital computer, 96, 267
information, 236
digital computing, 238
digital crimes, 108
digital data, 228
digital electronic device, 65
Digital Equipment Corporation’s DEC PDP-11, 31
digital forensics, 108–109
policy, 109
techniques, 109
digital imaging, 149–150
digital legacy, 161
digital literacy, 106
digital native, 106
digital radio, 262
digital remains, 163
digital signal, 248
digital signal processor (DSP), 110–111, 248, 249
applications, 111
biometric scanning, 111
digital signatures, 96
digital technology, 106–107, 223
digital-to-analog converter, 263
digital watermarking, 112, 114
applications, 114
data, ownership and security, 112–113
qualities, 114
diode, 122
direct-access storage, 240
“direct addressing,” 24
directed energy deposition (DED), 1, 3, 183
direct manipulation interfaces (DMI), 147, 148
dirty paper coding (DPC), 115
contribution, 116
data transmission, 115
information integrity, 116
interface, eliminating, 115–116
discrete model, 71
disk operating system (DOS). See DOS
distance education, 85
distributed algorithm, 9–10

367
Principles of Computer Science
Index
‘distributed computing,’ 221
“divide and conquer,” 10
DNA, 226
analysis, 45
strands, 38
documentation phrase, 24
do-it-yourself (DIY), 281
domain, 87
model, 160
domain-dependent complexity, 178
dopant, 68
DOS, 117–118, 284
impact of, 119
using, 118–119
dot-com bubble, 154–155
double data rate synchronous DRAM (DDRS 
DRAM), 241
DPC. See Dirty paper coding (DPC)
DPTR (data pointer) register, 24
drafting, 53
drakon chart, 9, 10
DRAKON computer language, 10
drones, 119–120
applications, 120–121
controversy and regulation, 121
development, 120
strikes, 121
work, 119
dual-mesh network, 251
duties of biochemical engineer, 39
dynamic balance, 4
dynamic models, 71
dynamic random-access memory (DRAM), 240–241
dynamic testing, 259
dynamic time warping (DTW), 265
E
e-commerce sites, 281
economic bubble, 155
economy of scale, 204–205
Edinburgh Prolog, 233
edit decision list (EDL), 78–80
educational applications, 29
effective data transmission, 115
electrical circuit, 122
electrical current, 67
electrical leakage, 221
electrical noise, 221
electric versus electronic circuits, 122
electrocardiograph (EKG), 42
electro-encephalograms (EEG), 173
electromagnetic radiation, 289
electromagnetic spectrum, 289
electron, 236
electronic circuits, 122
ICs, 122–123
logic gates, 123–124
vs. electric circuit, 122
electronic communication software,  
125, 126
asynchronous communication, 125
privacy concerns, 126
synchronous communication, 125–126
Electronic Communications Privacy Act (ECPA), 
108, 109, 125–126, 228
Electronic Fund Transfer Act (1978), 229
electronic interference, 251
Electronic Product Environmental Assessment  
Tool (EPEAT), 153
electronic tape editing technology, 79
electronic wastes. See E-waste (electronic waste)
electronic workplace monitoring, 293
elegant algorithm, 9
elements of GUIs and other object interfaces,  
148
eliminating interference, 115–116
ELIZA, 271
e-mail, 257
ISPs, 228
embedded systems, 23, 136
emerging and alternative methods, 3
emulation, 93
emulators, 185
enable talk glove, 17
Encapsulated PostScript (EPS) format, 151
encrypting messages, 95
encryption, 55, 95, 130, 131
authentication and security, 130–132
cryptography and, 130
systems, 132
types of, 130
endangered languages, 17
energy consumption, data centers, 152
Energy Star program, 152–153
engineering biology, 42
English language, 264
ENIAC (Electronic Numerical Integrator and 
Computer), 122–123

368
Index
Principles of Computer Science
Enigma
machine, 96
project, 269
enrollment, 44
entanglement, 235, 238–239
enumeration, 56
enumerative combinatorics, 56
Environmental Protection Agency (EPA),  
127, 152
epigram, 231
errors, 100–101
Esperanto, 17
ethernet cable, 289
evolutionary algorithms, 10
e-waste (electronic waste), 127, 152
defined, 127
impact of, 128–129
solutions, 128
technology and, 127
“external direct” addressing, 24
extracting information from signals, 250
extraction, 207
extrafunctional requirement, 254
extrusion printing, 3
eye-tracking studies, 286
F
Facebook, 207
factorization, 239
Fairchild, 240
Fair Credit Reporting Act of 1970, 229
false match rate, 43
farming model, 71
fault detection, 90
feasibility vs. optimization, 89
Federal Aviation Authority (FAA), 121
federal law, 293
feedback control system, 27
Feynman, Richard, 87
fiber, 199
fiberglass, 192
fiber-optic cable, 58
field programmable gate array,  
119, 121
filtering algorithm, 75
filters, 248, 250
fingerprint
identification, 43
scanner, 112
firewall, 55, 133, 134, 208
and computer security, 135
types of, 133–135
firmware, 136
automobile software, 137
modifying and replacing, 136–137
as vulnerability, 137
first-generation languages, 230
Fitbit, 138, 139
history of, 138
impact and challenges, 139
inside Fitbit devices, 139
tracker, 138
types, 138
fixed point, 248
arithmetic, 110–111
DSPs, 249
flash and solid state memory, 70
flashing, 70, 136
flash memory, 69
flip-flops, 65
floating point, 248
arithmetic, 110, 167
floating-point operations per second (FLOPS), 181
flooding, networking, 175
flowcharts, 10
Food and Drug Administration (FDA), 173, 258
footer, 288
force-sensing touch technology, 201
forensic image, 109
“forking,” 12
form, LISP, 167
“formants,” 250
FORTRAN, 7–8, 140
history and development, 140–141
in present day, 142
program structure, 141–142
versions, 140
four-dimensional building information modeling 
(4-D BIM), 52, 53
Fourier transform, 248
fourth-generation languages, 231
“frames,” 78
frames per second (FPS), 78
free software, 136
freeware, 27
frequency-division multiplexing, 59
Friedman, Eric, 138
front-page image, 288

369
Principles of Computer Science
Index
full-screen immersive mode, 12
function, 6, 9, 49, 140, 267
functional design, 143, 144
benefits, 143
interdependent complications, 143–144
manufacturing applications, 144
process and function, 144
functional programming, 87
functional requirement, 254
fur-covered avatar, 29
fused deposition modeling (FDM), 3
Futureworld (feature film), 63
fuzzy logic, 211–212
G
game loop, 145
game programming, 145–146
career in, 146–147
requirements, 146–147
gates, 65
gateway, 208
General Federal Privacy Laws, 228–229
Generalized Intelligent Framework for Tutoring 
(GIFT), 159
genetic engineering, 42
genetic modification, 40
genome, 226
genome-wide association study (GWAS), 226
gestures, 201
GIF (Graphics Interchange Format), 151
gigaFLOP (GFLOP), 181
glass-box/clear-box testing, 259
global positioning system (GPS), 189, 292
glyph, 272
Google App Engine, 55
Google search engine, 10
Google’s operating systems, 12
Google Translate, 207
Goostman, Eugene, 271
Go Pro Hero 4, 78
graph, 57
theory, 56–57
grapheme, 272
graphical interface, 105
graphical user interface (GUI), 117, 147, 148, 156, 
259, 284–285
elements of, 148
graphics and interface basics, 147
interface design, 147–149
graphic design software, 20
graphic processing unit (GPU), 220, 245
graphics formats, 149
common, 151
formats, 151
digital imaging, 149–150
image compression, 150–151
green computing, 152
concerns, 153
initiatives, 153
oversight and certifications, 152–153
green electronics council, 152
GUI. See Graphical user interface (GUI)
H
hackers, 82–83
hacking, 102
Hakkens, Dave, 127
Hamiltonian path problem, 190
hamming distance, 21–22, 43–44
hardware, 23, 65, 154
interruption, 65, 196, 198
security, 82
as service, 54
Harvard architecture, 110
hash function, 95–96
hashing algorithm, 130
hash value, 130, 132
head, 7
statement, 234
header, 288
health care, 291
modern, 223
Health Insurance Portability and Accountability Act 
(HIPAA), 228–229, 256
health-monitoring systems, 223
health-tracking devices, 223
heat energy, 67
heavy metal, 127
Hebbian learning, 86
heterogeneous scalability, 245
hexadecimal, 23, 272
number system, 34, 36
hibernation, 152–153
hidden Markov model (HMM), 264–265
hidden watermarks, 114
hierarchical file system, 275
higher-level programming languages, 230
high-fidelity wireframe, 286

370
Index
Principles of Computer Science
high-pass filter, 249
“hold and wait,” 98
Holding Resources, 98–99
hole, 67
home automation, 251
homebrew software, 136–137, 145
home network, 251
Hopfield, John, 86
Hopfield neural network, 86
Hopfield’s method, 86
hopping, networking, 175
horizontal scaling, 245–246
horizontal versus vertical scaling, 245–246
host-based firewalls, 133
“host” language, 87
HTC Dream, 12
HTML editor, 278
Hull, Chuck, 1
human
brain, 206
coders, 28
genomics, 226–227
resources website, 217
Human Brain Project (HBP), 214–215
Human Genome Project, 226
humanoid, 4
robots, 5–6
Huntington’s disease, 42
hybrid cloud, 54
hydrogen bonding, 46
hyper-scale data-center architecture, 247
hypertext markup language (HTML), 278,  
280–281
I
IBM clones, 118
IBM PC, 22
IC. See Integrated circuit (IC)
iconic logo, 281
IDE. See Integrated development environment (IDE)
ideal system, 273
identifiers, 43
identity theft, 129
IDE tools, 156
image compression, 150–151
algorithms, 74
image editing, 73, 75
mobile, 75
imitation game, 270
“immediate addressing,” 24
immersive mode, 11
immune system, 39
imperative language, 230, 231, 233
imperative programming, 23, 31, 87, 143
in-betweening/tweening, 29
in-circuit emulator, 100, 102
“include” statements, 50
increasing internet access, 177
“indirect addressing,” 24
information
extraction, 207
hierarchy, 286
integrity, 116
information technology (IT), 60, 154, 261
data production growth, 155
dot-com, 154–155
history, 154
infrastructure as a service (IaaS), 54–55
inheritance, 143, 218
Initiative for Software Choice (ISC), 60–61
inkjet printing technique, 3
“input layer,” 211
input/output (I/O) devices, 220
input weighting, 212
inside fitbit devices, 139
instant messaging, 126
instruction sets, 93–94
integrated circuit (IC), 68, 122–124
integrated development environment (IDE),  
156, 157
building and running code, 156
visual application development, 156–158
integration testing, 100, 102
Intel, 240–241
intelligent tutoring system (ITS), 159
components, 159–160
needs, 159
problem with mathematics-based subjects,  
159
tracking student progress, 160
interaction design, 286
interdependent complications, 143–144, 144
interface design, 148–149
history of, 147–148
interface metaphors, 147, 148
interface segregation principle, 218
interference, 115
interferometry, 38–39

371
Principles of Computer Science
Index
internal-use software, 140, 256–257
internet, 59–60, 209–210, 278
access, 210
communication, 59–60
protocols, 93
companies, 247
networking, 210
privacy, 161, 162
concerns, 161
protecting, 163
research, 161
threats, 161–163
Internet of Things (IoT), 251, 253
internet service providers (ISPs), 103, 228
internet surveillance program PRISM, 163
interpolation, 73
interrupt vector table, 143
intrusion-detection systems, 82, 103
intuitive touch-screen technology, 285
inverter, 65, 122
ion traps, 239
iOS, Apple, 11
iOS operating system, 164–165, 188
“iostream,” 49, 51
IP core, 67, 110
iPhone feature, 207
IRL (in real life) relationship, 106
ISO Prolog, 233
IT and communication overlap, 60
iterative algorithm, 10
iterative calculations/operations, 32
J
jailbreaking, 164, 188
Java programming language, 51
JavaScript, 278, 281
J-K flip-flops, 65
JPEG, 75
image compression format, 151
jQuery, 278
K
kernel, 275
threads, 199
keyframing, 28–29, 62
keyloggers, surveillance program, 161
keystroke, 33
Kilby, Jack, 123
KitKat, 12
knowledge
space, 160
tracing, 160
L
“labs-on-a-chip,” 42
Labyrinth (film), 63
lagging/freezing, 240
lambda form, LISP, 168
language interface packs, 284
larger-scale operations, 246
laser, 3, 243
latency, 222
latent semantic analysis (LSA), 159
learner-controlled program, 83–84
learner model, 160
learning strategy, 83
learning style, 83
lexicon, 16
Leyden jar, capacitor, 122
ligase, 190
limiting factor, 213
linear acoustic filter, 250
Linear Control Systems, 91
linear predictive coding, 248, 250
linear video editing, 78–79
linguistics
analysis, 17
subfield, 16
and technology, 16
Linux, 257
Lipton, R. J., 190–191
liquid 3-D printing, 1–3
liquid resin, 1
Liskov substitution principle, 218
LISP (LISt Processor), 167, 233
history and characteristics, 167
program features, 167–168
value of programming, 168
livelock, 98
local area network (LAN), 102, 289
Locky, ransomware program,  
171–172
logic
errors, 101
gates, 123
implementation, 92, 235
logical copy, 108, 109
logical device river development, 105

372
Index
Principles of Computer Science
logotype, 281
Lollipop, 12
lossless compression, 73, 149, 150
lossy compression, 73, 76, 149, 150
low-energy connectivity IC, 138
lowest-frequency waves, 289
low-fidelity wireframe, 288
low-level languages, 231
low-pass filter, 248–249
LS3, 4
LZW compression, 149–150
M
MacBook laptops, 291
machine code, 230
machine language/code, 31
machine-readable code, 256
machine translation, 207
machining, 1
Macintosh, Apple, 32
Macromedia Flash, 278
macroscopic scale, 237
“magic square” diagram, 57
magnetic and optical storage media, 243
magnetic-core memory, 240
magnetic resonance imaging (MRI), 41–42
magnetic storage, 242–243
main() function, 51
main loop, 7, 23, 31, 140, 143
main memory, 242
main program, 50
mainstream technology, 235
malicious programming, 170
malware, 170, 171
combating, 171–172
malicious programming, 170
types, 171
Manhattan Project, 71
manufacturing applications, 144
many-to-many model, 200
many-to-one model, 200
map makers, 89
Martin, Robert Cecil, 218
Massachusetts Institute of Technology (MIT),  
4, 86, 275
mass communication, 59
massive open online course (MOOC), 85
mastering, 76
“master-slave,” 66
material
design, 11–12
extrusion, 1, 183
jettin, 1
jetting, 183–184
mathematical number system, 34
Mazur, Eric, 159
McCarthy, John, 167
Medical Device Innovation Consortium (MDIC), 
172, 174
medical imaging, 111, 172
medical technology, 172, 173
computerized diagnostic tools, 172–173
treatment tools, 173–174
in twenty-first century, 174
memory dumps, 100–101
memristor, 214
mesh network, 175, 176, 251, 291
benefits, 175
increasing internet access, 177
peer-to-peer networks, 177
work, 175–176
message, 208
digest, 132
meta-complexity, 178
metacomputing, 178, 179
and Internet, 178–179
layer, 178
mimicking brain, 179
reasons, 178
ubiquitous, 179
metadata, 108
metafiles, 149
method, 217
methodology, 254–255
microcode, 93–94
microcontroller, 92, 138
micron, 180
microprocessing, 180
microprocessor(s), 90, 180, 181
characteristics, 181
chips, 214
development, 181–182
history and capacity, 180–181
microscale 3-D printing, 183–184, 183
methods, 183–184
microscale methods, 184
Microsoft Azure, 247
Microsoft Corporation, 274, 284

373
Principles of Computer Science
Index
Microsoft Office, 14
Microsoft’s Cortana, 264
microwave, 289
ovens, 251
middle computing, 178
million instructions per second (MIPS), 180–181
millions of FLOPS (MFLOPS), 181
mimicking the brain, 179
mimics biological systems, 46
miniaturization, 212
Minksy, Marvin, 269
Minnesota, 229
MITS Altair 8800, 31
mixer, 262
mixing, 76
and mastering, 76
mobile
apps, 185, 186
economy, 185–186
and social change, 186–187
types, 185
computing, 11
devices, 75
features, 189
image editing, 75
operating systems, 189
features, 189
history, 188–189
impact, 189
security concerns, 189
platforms for games, 146
and smartphone software, 257–258
website, 185
modeling, 18, 28
modern computers, 245
modulator, 262
modules, 143
molecular computers, 190
Adelman’s experiment, 190–191
SAT problem, 191
monitoring technology, 292, 293
monostable multivibrator (MMV) electronic circuit, 
122
Moore, Gordon, 94, 181
Moore’s Law, 68, 94
Morse code, 21
motherboards, 192
BIOS, 193
design, 192–193
evolution, 192
form factor, 193
motion picture music editing, 77
MP3 compression algorithm, 77
MPEG-4 international standard, 29
MS-DOS, 284
Mullis, Cary, 190
multi-agent system, 25
multibit watermarking, 112, 114
multicarrier hybrid systems, 116
multicast, 125–126
multicore processors, 199–200
multi-level cells (MLC), 244
multimodal monitoring, 223
multiple processing cores, 180
multiplexing, 58–59
multiplier-accumulator, 110, 111
multiprocessing, 196
multiprocessing operating systems, 194, 195
coupling, 195–196
multitasking, 196
vs single processing operating systems, 194
multistep process, 111
multitasking, 11, 164, 199, 275, 284
multitasking operating systems (OS), 196–198
cooperation and preemption, 199
prioritizing processes, 198
time-sharing, 197–198
multitasking system, 276. See also Multitasking 
­operating systems (OS)
multitenancy, 54
multi-terminal configuration, 203
multithreading operating systems (OS), 199
benefits and drawbacks, 200
memory and context, 199–200
as model, 200
multitouch displays, 201
advantages, 202
innovative touchscreen technology,  
201–202
optics and pressure-sensing, 202
pinch to zoom, 202
multi-touch gestures, 11, 164
multiuser, 275, 284
multi-user operating systems (OS),  
203–204
economy of scale, 204–205
shared computing, 204
music editing software, 77

374
Index
Principles of Computer Science
mutations, 226
mutual exclusion, 98–99
MySQL, 282–283
N
NASA, 5
National Institute of Standards and Technology, 110
natural language processing (NLP), 206–207, 271
in everyday applications, 207
natural languages, 17
navigation, 279
bar, 288
near-field communications (NFC), 188–189
antenna, 138, 139
negative-AND (NAND) gate, 65, 122
negative-OR (NOR) gate, 65
nervous (neural) system, 211, 214
network, 154
bridge, 210
firewalls, 133, 134
hardware, 208
security, 82
networking, 178, 203, 208
communication within and between, 210
Internet access, 210
model, 54
routing, 208–209
neural networks, 206, 211–212, 264
artificial neural network, 212
specifications, 212
uses and limitations, 212
input weighting, 212–213
processes, 207
quantum computers and, 236–237
neuromorphic chips
brain-based design, 214–215
silicon, limits of, 215–216
neurons, 211
neuroplasticity, 214–215
neurotransmitters, 211
nibble, 34
node, 175, 208, 289
noise, 115
removal algorithm, 115
tolerant signals, 112, 114
Nokia, 75
nondestructive editing, 73, 74, 76
nondeterministic algorithms, 10
nonfunctional requirement, 254
nongraphical, 117
nonlinear editing, 78
nonvolatile memory, 69, 136, 242
nonvolatile random-access memory (NVRAM), 
240–241
nonvolatile removable memory, 243
normalization, 272, 274
NOT gate, 65
NP-complete problem, 190
nuclear magnetic resonance (NMR) spectroscopy, 38
O
object, 49, 217
object-oriented (OO)
programming, 6, 49, 140, 145
systems, 217
object-oriented analysis (OOA), 217
object-oriented design (OOD), 217
precautions and drawbacks, 219
principles and applications, 218–219
software development, 217–218
object-oriented user interfaces (OOUIs), 147, 148
Oculus Rift, 29
oligomer, 190
omnidirectional networks, 289
one-to-many basis, 59
one-to-one model, 200
“op-code,” 24
open-closed principle, 218
open group, 276
open-loop system, 90
open-source software, 12, 256–257, 285
operating system (OS), 13–14, 104, 196–199, 257, 
275
shell, 284
optical storage media, 243
optical touchscreens, 201–202
optical transient liquid molding (TLM), 184
optics and pressure-sensing, 202
optimization, 89
organization, 279
OR gate, 65
OS Linux, 12
“owning,” 83
P
pacemaker, 47
packet(s), 208, 210
filters, 133, 135

375
Principles of Computer Science
Index
forwarding, 208
sniffers, 292
switching, 208
packet-switched network, 209–210
Palm OS, 188
paper watermark, 114
parallel and conventional processors, 220–221
parallel processing, 86, 194, 211, 215
parallel processors, 220–222
Amdahl’s law, 221–222
parallelism and concurrence, 221
properties, 221
parameter, 71
Park, James, 138
passwords limit, 82
patient-rights regulations, 224
PATRIOT Act, 228–229
Patterson, Tim, 118
PDF (portable document format), 151
pedagogical model, 160
pedagogy, 83
peer-to-peer (P2P) network, 175, 177
peer-to-peer (P2P) systems, 27
Penicillium mold, 40
pen/trap, 228–229
peripherals, 92
permutations, 56
Persian mathematician, 9
personal area network (PAN), 289
personal computers (PCs), 284
personal digital assistant (PDA), 188
personal genome, 226
personal health
data, 223
monitor technology, 223
advantages and disadvantages, 223–225
health data, 223
tools, 223
personal home page (PHP), 282
personal identification number (PIN), 218
personalized medicine, 226
computation-intensive process, 227
custom health care, 226
genomics of drug response, 227
human genomics, 226–227
personally identifiable information (PII), 161
perturbation model, 87–88
pharmaceutical manufacturing processes, 39
pharmacogenomics, 226–227
phased software development, 259
phishing, 82
phonebloks, 127–128
phoneme, 264
photofabrication, 3
photograph, 73
photons, 238
photoshop
Adobe, 20, 75
CC, 75
photo-solidification, 3
physical copy, 109
physical device river development, 105
physical model, 71
phytoremediation, 128
Pinch to Zoom, 202
pipelined architecture, 110
piracy, 106–108
“pivot,” 10
pixels, 149
plaintext, 96, 130
planned obsolescence, 127
platform, 13, 156, 164, 185, 284
lock, 187
platform as a service (PaaS), 54–55
plug-ins, 254, 255
PNGs (Portable Network Graphics), 151
point-to-multipoint (P2MP), 291
point-to-point (P2P), 291
polymerase chain reaction (PCR), 190
machine, 38
polymer filament, 3
polymorphism, 143
portlet, 286, 288
port scanning, 102–103
positron emission tomography (PET), 42
post-diagnosis treatment, 227
postproduction, 18
powder-based 3-D printing, 3
powder bed fusion, 1, 183
practical design of quantum computers, 239
precoding, 115
predictive analysis, 56
preemption, 99
Premiere, Adobe, 80
Prensky, Marc, 106
present and future concerns, 153
pretty good privacy (PGP), 130, 132
primates, 5

376
Index
Principles of Computer Science
principle of least privilege, 82
printable characters, 21
printed circuit board (PCB), 138–139, 192
Privacy Act (1974), 229
privacy concerns, 126
Privacy Incorporated Software Agents (PISA), 161
privacy protections, 228
privacy regulations, 228
data and state laws, ownership, 229
general federal privacy laws, 228–229
protection, 228
provisional privacy regulations, 229
private key, 130
probability theory, 56
procedures, 7
process, 23, 206, 267
processing signals, 248–250
processor
coupling, 194, 211, 220
symmetry, 194, 220
program characteristics, 7
“program counter,” 24
programmable oscillator, 122
PROgrammation en LOGique, 233
programmers plan, 217
programming languages, 7, 31, 140, 156, 167, 230, 
233, 267
defined, 230–231
future, 232
for games, 146
pseudocode, 232
structured, 231–232
programming tools, 280
program structure, 141
projected capacitive touch, 201
PROLOG, 233
clause structure, 234
history and characteristics, 233
and logic, 234
practice and application, 234
program features, 233–234
PROLOG and Logic, 234
proprietary software, 256–257
pros and cons of wireless, 291
protecting internet privacy, 163
protecting ownership and security of digital data, 
112
protocol processor, 92–93
prototypal inheritance, 217
prototype, 145, 218, 286, 288
robot, 4
provisional privacy regulations, 229
proxy, 245
server, 133, 247
pseudocode, 49–51, 145–146, 232
psychological, 5
public-cloud service, 54
public-key, 130
cryptography, 95–96
encryption, 96
push technology, 125
Q
qualities of digital watermarks, 114
quality assurance, 14
quantitative analysis, 56
quantum
bit (qubit), 235–236, 238
computers, 235–236
bits and qubits, 236
and neural networks, 236–237
practical design, 239
problems, development, 237
types, 237
computing, 238
quantum data, basics of, 238–239
subatomic computation theories, 238
entanglement, 236
gate, 236
logic gate, 235, 238
parallelism technique, 236
state, 238–239
quick and dirty operating system (QDOS), 118
quicksort algorithm, 10
QWERTY keyboard, 12
R
radio frequency (RF), 262
radio frequency identification (RFID) technology, 
189
radio waves, 262, 289, 291
“ramping” effect, 65
random access memory (RAM), 69–70, 240–242
history, 240–241
shadow, 242
types, 241
using, 241
random and virtual memory, 70

377
Principles of Computer Science
Index
random logic, 93–94
ransomware, 170–171
“rapid-iteration, web-style update cycle,” 12
rapid prototyping, 52
raste
graphics, 149–150
raster, 18, 52
images, 149
read-only memory (ROM), 240, 242, 244
chip, 193
real-time monitoring, 292–293
real-time operating system (RTOS), 188
receiver, 58
receive-side scaling (RSS), 247
recursive, 9
algorithm, 10
red blood cells, 226
Reduced Instruction Set Computer (RISC), 94
refinement model, 88
refrigeration cooling system, 221
“registering,” data values, 24
registration/login box, 288
remixing, 77
remote desktop connection system, 285
remote monitoring, 292–293
removable memory, 242
clouds and virtual computing, 244
magnetic and optical storage media, 243
solid-state storage, 243–244
temporary and permanent memory, 242–243
render farm, 28, 62
rendering, 18, 20, 64, 73, 272
and postproduction, 20
“reprogrammed,” 38
research, internet privacy, 161
resistive touchscreens, 201–202
resource
allocation, 203
distribution, 25
holding, 98–99
resource-intensive programs, 241
responsive web design, 278
retriggerable single shot, 122, 124
reversible data hiding, 112, 114
RGB (red, green, and blue) color model, 149
Ribble, Mike, 107
Ritchie, Dennis, 275
robonauts, 5
robots, 4–5
robust watermarks, 114
Roomba floor sweeping device, 213
“rotor machines,” 96
router, 210, 289
routing, 175, 208
table, 175
“R registers,” 24
R-S flip-flops, 65
run-time errors, 101
S
safety field, 133
sampling and matching, 43–44
Samuel Morse’s telegraph system, 21
Sarbanes-Oxley Act (SOX), 256–257
SAT problem, 190–191
scalability, 245–246
scaling systems, 245
for cloud, 247
horizontal versus vertical scaling, 245–246
scalability, 245
techniques, 246–247
scareware, 170–171
scheduler, 199
The science of counting and combinations, 56
The science of identifying individuals, 43
Scientific Working Group on Digital Evidence 
(SWGDE), 108–109
screen-reading programs, 278, 280
script, 272
scrubbing, 76–77
search engine optimization (SEO), 281
second-generation languages, 230
security
concerns, 189
implications, 14
selective layer sintering (SLS), 3
self-awareness, 27
self-encrypting drives (SEDs), 132
self-healing, 27
self-learning, 27
self-managing systems, 25
self-monitoring device, 224
self-optimization, 27
self-organization, 27
self-protection, 27
self-regulatory systems, 27
self-*/self-star, 25
self-stabilization, 27

378
Index
Principles of Computer Science
self-star properties, 25–26
semantics, 16–17, 206, 230
semi-autonomic software and hardware systems, 27
semiconductor
materials, 67
transistor-based computer, 235–236
semiconductor intellectual property (SIP), 67–68
block, 110–111
sending and receiving signals, 58
sensitive equipment, 82
sensors, 42, 90, 119, 253
sentiment analysis, 207
shading language, 146
shadow RAM, 240, 242
shared computing, 204
shareware, 257
sheet lamination, 1, 183
shell, 117, 275
shingled magnetic recording (SMR), 243
Shor’s algorithm, 239
short message service (SMS), 125
signal processing
applications, 250
digital and analog, 248
extracting information, 250
processing signals, 248–250
signal-to-noise ratio (SNR), 115
silent monitoring, 292
silicon-based processors, 212
simulation, 28–29, 71
single-processor computers, 199
single-processor system, 245
single responsibility principle, 218
single-user OS, 203
Siri, voice-activated virtual assistant, 165
skeletal system, 254
skyscraper, 288
SLA printer, 1
SLD resolution, 233
slithering motion, 5
Smalltalk, 217
smart clothing device, 184
smart homes, 251
automation, 251
goals and challenges, 253
home network, 251
smart devices and utilities, connecting, 251–253
smart locking devices, 253
smart navigation, 216
Smith, Joe, 217
social media, 291
soft robotic fish, 4
software, 49, 154
applications, See Applications (Apps)
architecture, 254
challenges of, 254
implications, 255
methodology, 254–255
engineer, 218
and modeling, 1
patches, 100, 101
programs, 13
regulations, 256
and legal standards, 256–257
mobile and smartphone software, 257–258
types of software, 257
value, 258
security, 82–83
testing, 259
automating the process, 261
cloud, moving to, 261
insufficient, 261
methods, 259–260
software as a service (SaaS), 54–55
software-based language learning programs, 17
software-defined antennas, 262–263
software-defined radio (SDR), 262
digital and, 262–263
implications, 263–264
working, 262
software-defined receiving antenna, 263
solar-electric hybrid drones, 120
solid modeling, 52–53
SOLID principles, 218
solid state disks (SSDs), 70
solid-state drives (SSDs), 250
solid-state storage, 67, 242–244
sorting algorithms, 57
sound card, 262
sound software architecture, 254
source code, 31, 49, 145, 256
source-filter model, 250
speaker-adaptive systems, 265
speaker independent, 264
special characters, 272
specialized applications, 53
special problem with mathematics- based subjects, 159
species-wide genome, 226

379
Principles of Computer Science
Index
speech-recognition, 84
algorithms, 265
software
algorithms, 265
applications, 265–266
speech signal processing, 250
speech-to-text program, 264
speech waveforms, 265
splitting process, 221
spyware, 170–171
stairs, 5
stand-alone system, 221
standard input, 51
standard output, 51
standoff, 99
state, 9, 235, 238
stateful filters, 133, 135
static models, 71
static random-access memory (SRAM), 240–241
static testing, 259–260
steganography, 114
stereolithography, 1
storage, 242
storyboards, 64
‘string,’ 33
Stroustrup, Bjarne, 49
Structured Query Language (SQL), 231
tutor, 160
“student-controlled program,” 83
subatomic computation theories, 238
subnets, 210
subroutine, 232
versus functions, 32–33
substitution cipher, 95–96
subtractive color model, 150
subtyping, 143
Sudoku puzzles, 57
Sun Microsystems, 86
supercomputer, 178, 220–221
super-high frequency (SHF), 251
superimposed waves, 248
superposition, 235, 238–239
surface capacitive technology, 201–202
surface tessellation language (STL), 1
SVGs (Scalable Vector Graphics), 151
switch, 208
Symantec, antivirus company, 170
symmetric-key cryptography, 95, 96
symmetric-key encryption, 130
symmetric multiprocessing (SMP), 195
“synapse,” 211
synchronous communication, 125–126
syntax, 7, 16–17, 23, 49, 140, 167, 206, 230, 233
errors, 101
system, 71, 154, 203
agility, 90
identification, 90
software, 13, 185
T
tableless web design, 281
take back programs, 153
“talkbot,” 270
task-oriented design (TOD), 219
Tax Reform Act (1976), 229
TCO certification, 152–153
technical support, 14–16
technical writers, 16
technology and waste, 127
telecom equipment, 154
telemedicine, 40, 42, 172
telemetry, 119–120
telephone wire, 209
templates, 280
temporal synchronization, 223
temporary and permanent memory, 242–243
terminals, 203
tessellate, 1
text-based interface, 271
text-based internet communication, 271
text-based systems, 147
text classification technique, 207
text messaging, 125
text-to-speech programs, 280
thermal cycling, 190
thermostats, 253
Thinking Machines Corporation, 86
third-generation languages, 230–231
third-party data center, 54
Thompson, Ken, 275
thread, 199
three-dimensional computer animation, 64
threshold logic unit (TLU), 212
threshold state, 236
time-division multiplexing, 59
time-intensive process, 64
time-sharing, 197, 203–204
and multitasking, 197–198

380
Index
Principles of Computer Science
T-Mobile G1, 12
tools, 64
topology, 175
Torvalds, Linus, 12
Touch ID system, Apple, 45
touch-screen interfaces, 148
toxgnostics, 226–227
Toy Story (film), 29, 63
trace impedance, 192
tracking student progress, 160
traditional animation, 62
transactional database, 98
transistor, 65, 67, 122, 180, 240–241
gate, 65, 236, See also Logic, gates
structures, 68
transistor-transistor based communication, 212
transistor-transistor logic, 65
transitions, 268
transmission medium, 58
transmitter, 58
transparent monitoring, 292, 294
transposition cipher, 95, 96
treatment tools, 173–174
true parallelism, 221
true parallel processors, 221
trusted platform module (TPM), 82
truth test, 212
tuning, 192
Turing, Alan, 206, 267, 269–270
Turing complete, 267
Turing complete programming languages, 230–231
Turing machine, 267
birth of, 267
definition, 268–269
work, 269
Turing’s time, 270
Turing test, 270
criticism, 271
imitation game, 271
legacy, 271
natural language processing (NLP), 271
two-dimensional computer animation, 64
two-legged robot, 6
typography, 281, 286
U
ubiquitous computing, 178–179, 251, 253
ultrasound, 111
ultraviolet (UV) light, 1, 3
understanding UNIX, 275
undervolting, 152
Unicode, 272
character-encoding systems, 272
connect systems worldwide, 274
consortium, 273
standard, 272–274
universal computing machine, 267
UNIX, 203, 275
origin, 275
UNIX-like operating systems, 276–277
using, 275–276
variants, 276–277
unmanned aerial vehicle (UAV), 119–120
user-centered design, 147, 148
user-Generated Web design, 282–283
user modeling, 159
user threads, 199
US National Security Agency (NSA), 161
utility programs, 13, 185
V
variables, 31, 49, 71
vat photopolymerization, 1, 3, 183–184
vector, 18, 52
graphics, 149–150
images, 20
Velcro, 46
vermiremediation, 128
vertical scaling, 245
vibrator, 65, 138
video editing, 78
in digital world, 80
in practice, 78–80
video games, creation, 145
video scratching, 78
virtual device drivers, 105
virtual memory, 69–70
virtual networks, 54
virtual reality (VR), 28, 62
virtual worlds, 28
viruses, 170
malware, 170, 171
vision mixing, 78
visual application development, 156, 158
visual design, 278
visual programming, 156, 158
voice activation of programs, interfaces, 149
voice over Internet Protocol (VoIP), 102, 125

381
Principles of Computer Science
Index
volatile memory, 69, 242
volatile removable memory, 243
Von Neumann Architecture, 86
VR in 3-D, 29
W
war dialing. See Demon dialing
wardriving, 102
WarVOX, open-source software, 103
watermarking, 113, 114
Watson-Crick base-pairing, 190
wearable devices, 253
wearable tracking devices, 223
web
application, 13
apps, 245
browsing, 13
designers, 281, 286
design functionality, 281
design programming tools, 278–280
future of, 280
HTML to responsive design, 278
graphic design, 281
techniques, 281–282
user-generated web design, 282–283
web-based linguistic applications, 17
web-hosting companies, 14
webmail application, 204
website, 286
Weizenbaum, Joseph, 271
well-defined scheme, 96
well-formed formulas, 233–234
white-box testing, 259–260
widget, 164–165, 286
Wi-Fi network, 251
Wi-Fi war dialing, 103
WIMP (windows, icons, menus, and pointer objects), 
148
Windows, 285
Windows 1.0, 285
Windows 7, 285
Windows 8, 285
Windows 10, 285
Windows 95, 285
Windows operating system, 284
dominance and reinvention, 285
future, 285
history, 284–285
“wire-frame model,” 53
wireframes, 281, 283, 286
defined, 286–287
impact, 288
in Web Design, 288
wireless LAN (WLAN), 289
wireless networks, 289
evolution of, 291
pros and cons, 291
types, 289–291
works, 289
wireless wide area network (WWAN), 289, 291
withdrawal process, 98
word prediction, 83
word processing, 13
workplace monitoring, 292
benefits, 292–293
legal and ethical considerations, 293–294
technologies, 292
worm, malware, 170
writing system, 272
X
X86-64 computer architecture, 285
Xbox One gaming system, 285
Y
Y2K bug, 101
Z
zero-forcing dirty paper coding, 116
zombie computers, 170–171


Matrices:
Theory and Applications
Denis Serre
Springer

Graduate Texts in Mathematics 216
Editorial Board
S. Axler
F.W. Gehring
K.A. Ribet

This page intentionally left blank 

Denis Serre
Matrices
Theory and Applications

Denis Serre
Ecole Normale SupeÂ´rieure de Lyon
UMPA
Lyon Cedex 07, F-69364
France
Denis.SERRE@umpa.ens-lyon.fr
Editorial Board:
S. Axler
F.W. Gehring
K.A. Ribet
Mathematics Department
Mathematics Department
Mathematics Department
San Francisco State
East Hall
University of California,
University
University of Michigan
Berkeley
San Francisco, CA 94132
Ann Arbor, MI 48109
Berkeley, CA 94720-3840
USA
USA
USA
axler@sfsu.edu
fgehring@math.lsa.umich.edu
ribet@math.berkeley.edu
Mathematics Subject Classification (2000): 15-01
Library of Congress Cataloging-in-Publication Data
Serre, D. (Denis)
[Matrices. English.]
Matrices : theory and applications / Denis Serre.
p. cm.â€”(Graduate texts in mathematics ; 216)
Includes bibliographical references and index.
ISBN 0-387-95460-0 (alk. paper)
1. Matrices
I. Title.
II. Series.
QA188 .S4713 2002
512.9â€²434â€”dc21
2002022926
ISBN 0-387-95460-0
Printed on acid-free paper.
Translated from Les Matrices: TheÂ´orie et pratique, published by Dunod (Paris), 2001.
ï›™2002 Springer-Verlag New York, Inc.
All rights reserved. This work may not be translated or copied in whole or in part without the
written permission of the publisher (Springer-Verlag New York, Inc., 175 Fifth Avenue, New York,
NY 10010, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use
in connection with any form of information storage and retrieval, electronic adaptation, computer
software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.
The use in this publication of trade names, trademarks, service marks, and similar terms, even if
they are not identified as such, is not to be taken as an expression of opinion as to whether or not
they are subject to proprietary rights.
Printed in the United States of America.
9 8 7 6 5 4 3 2 1
SPIN 10869456
Typesetting: Pages created by the author in LaTeX2e.
www.springer-ny.com
Springer-Verlag
New York Berlin Heidelberg
A member of BertelsmannSpringer Science+Business Media GmbH

To Pascale and Joachim

This page intentionally left blank 

Preface
The study of matrices occupies a singular place within mathematics. It
is still an area of active research, and it is used by every mathematician
and by many scientists working in various specialities. Several examples
illustrate its versatility:
â€¢ Scientiï¬c computing libraries began growing around matrix calculus.
As a matter of fact, the discretization of partial diï¬€erential operators
is an endless source of linear ï¬nite-dimensional problems.
â€¢ At a discrete level, the maximum principle is related to nonnegative
matrices.
â€¢ Control theory and stabilization of systems with ï¬nitely many degrees
of freedom involve spectral analysis of matrices.
â€¢ The discrete Fourier transform, including the fast Fourier transform,
makes use of Toeplitz matrices.
â€¢ Statistics is widely based on correlation matrices.
â€¢ The generalized inverse is involved in least-squares approximation.
â€¢ Symmetric matrices are inertia, deformation, or viscous tensors in
continuum mechanics.
â€¢ Markov processes involve stochastic or bistochastic matrices.
â€¢ Graphs can be described in a useful way by square matrices.

viii
Preface
â€¢ Quantum chemistry is intimately related to matrix groups and their
representations.
â€¢ The case of quantum mechanics is especially interesting: Observables
are Hermitian operators, their eigenvalues are energy levels. In the
early years, quantum mechanics was called â€œmechanics of matrices,â€
and it has now given rise to the development of the theory of large
random matrices. See [23] for a thorough account of this fashionable
topic.
This text was conceived during the years 1998â€“2001, on the occasion of
a course that I taught at the Â´Ecole Normale SupÂ´erieure de Lyon. As such,
every result is accompanied by a detailed proof. During this course I tried
to investigate all the principal mathematical aspects of matrices: algebraic,
geometric, and analytic.
In some sense, this is not a specialized book. For instance, it is not as
detailed as [19] concerning numerics, or as [35] on eigenvalue problems,
or as [21] about Weyl-type inequalities. But it covers, at a slightly higher
than basic level, all these aspects, and is therefore well suited for a gradu-
ate program. Students attracted by more advanced material will ï¬nd one
or two deeper results in each chapter but the ï¬rst one, given with full
proofs. They will also ï¬nd further information in about the half of the
170 exercises. The solutions for exercises are available on the authorâ€™s site
http://www.umpa.ens-lyon.fr/ Ëœserre/exercises.pdf.
This book is organized into ten chapters. The ï¬rst three contain the
basics of matrix theory and should be known by almost every graduate
student in any mathematical ï¬eld. The other parts can be read more or
less independently of each other. However, exercises in a given chapter
sometimes refer to the material introduced in another one.
This text was ï¬rst published in French by Masson (Paris) in 2000, under
the title Les Matrices: thÂ´eorie et pratique. I have taken the opportunity
during the translation process to correct typos and errors, to index a list
of symbols, to rewrite some unclear paragraphs, and to add a modest
amount of material and exercises. In particular, I added three sections,
concerning alternate matrices, the singular value decomposition, and the
Mooreâ€“Penrose generalized inverse. Therefore, this edition diï¬€ers from the
French one by about 10 percent of the contents.
Acknowledgments. Many thanks to the Ecole Normale SupÂ´erieure de Lyon
and to my colleagues who have had to put up with my talking to them
so often about matrices. Special thanks to Sylvie Benzoni for her constant
interest and useful comments.
Lyon, France
Denis Serre
December 2001

Contents
Preface
vii
List of Symbols
xiii
1
Elementary Theory
1
1.1
Basics
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Change of Basis . . . . . . . . . . . . . . . . . . . . . . .
8
1.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
2
Square Matrices
15
2.1
Determinants and Minors . . . . . . . . . . . . . . . . . .
15
2.2
Invertibility
. . . . . . . . . . . . . . . . . . . . . . . . .
19
2.3
Alternate Matrices and the Pfaï¬ƒan . . . . . . . . . . . .
21
2.4
Eigenvalues and Eigenvectors
. . . . . . . . . . . . . . .
23
2.5
The Characteristic Polynomial . . . . . . . . . . . . . . .
24
2.6
Diagonalization . . . . . . . . . . . . . . . . . . . . . . .
28
2.7
Trigonalization . . . . . . . . . . . . . . . . . . . . . . . .
29
2.8
Irreducibility . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
3
Matrices with Real or Complex Entries
40
3.1
Eigenvalues of Real- and Complex-Valued Matrices . . .
43
3.2
Spectral Decomposition of Normal Matrices
. . . . . . .
45
3.3
Normal and Symmetric Real-Valued Matrices
. . . . . .
47

x
Contents
3.4
The Spectrum and the Diagonal of Hermitian Matrices .
51
3.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
4
Norms
61
4.1
A Brief Review
. . . . . . . . . . . . . . . . . . . . . . .
61
4.2
Householderâ€™s Theorem . . . . . . . . . . . . . . . . . . .
66
4.3
An Interpolation Inequality
. . . . . . . . . . . . . . . .
67
4.4
A Lemma about Banach Algebras . . . . . . . . . . . . .
70
4.5
The Gershgorin Domain
. . . . . . . . . . . . . . . . . .
71
4.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
5
Nonnegative Matrices
80
5.1
Nonnegative Vectors and Matrices . . . . . . . . . . . . .
80
5.2
The Perronâ€“Frobenius Theorem: Weak Form . . . . . . .
81
5.3
The Perronâ€“Frobenius Theorem: Strong Form . . . . . .
82
5.4
Cyclic Matrices
. . . . . . . . . . . . . . . . . . . . . . .
85
5.5
Stochastic Matrices . . . . . . . . . . . . . . . . . . . . .
87
5.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
6
Matrices with Entries in a Principal Ideal Domain;
Jordan Reduction
97
6.1
Rings, Principal Ideal Domains
. . . . . . . . . . . . . .
97
6.2
Invariant Factors of a Matrix . . . . . . . . . . . . . . . .
101
6.3
Similarity Invariants and Jordan Reduction
. . . . . . .
104
6.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
111
7
Exponential of a Matrix, Polar
Decomposition, and
Classical Groups
114
7.1
The Polar Decomposition . . . . . . . . . . . . . . . . . .
114
7.2
Exponential of a Matrix
. . . . . . . . . . . . . . . . . .
116
7.3
Structure of Classical Groups
. . . . . . . . . . . . . . .
120
7.4
The Groups U(p, q) . . . . . . . . . . . . . . . . . . . . .
122
7.5
The Orthogonal Groups O(p, q) . . . . . . . . . . . . . .
123
7.6
The Symplectic Group Spn
. . . . . . . . . . . . . . . .
127
7.7
Singular Value Decomposition . . . . . . . . . . . . . . .
128
7.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
8
Matrix Factorizations
136
8.1
The LU Factorization . . . . . . . . . . . . . . . . . . . .
137
8.2
Choleski Factorization
. . . . . . . . . . . . . . . . . . .
142
8.3
The QR Factorization . . . . . . . . . . . . . . . . . . . .
143
8.4
The Mooreâ€“Penrose Generalized Inverse
. . . . . . . . .
145
8.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
9
Iterative Methods for Linear Problems
149

Contents
xi
9.1
A Convergence Criterion . . . . . . . . . . . . . . . . . .
150
9.2
Basic Methods . . . . . . . . . . . . . . . . . . . . . . . .
151
9.3
Two Cases of Convergence . . . . . . . . . . . . . . . . .
153
9.4
The Tridiagonal Case . . . . . . . . . . . . . . . . . . . .
155
9.5
The Method of the Conjugate Gradient . . . . . . . . . .
159
9.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
10 Approximation of Eigenvalues
168
10.1
Hessenberg Matrices
. . . . . . . . . . . . . . . . . . . .
169
10.2
The QR Method . . . . . . . . . . . . . . . . . . . . . . .
173
10.3
The Jacobi Method . . . . . . . . . . . . . . . . . . . . .
180
10.4
The Power Methods . . . . . . . . . . . . . . . . . . . . .
184
10.5
Leverrierâ€™s Method
. . . . . . . . . . . . . . . . . . . . .
188
10.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
References
195
Index
199

This page intentionally left blank 

List of Symbols
|A|, 80
a|b, 97
A â—¦B, 59
Aâ€ , 145
A â‰¥0, 80
a â‰ºb, 52
a âˆ¼b, 97
Aâˆ—, 15, 97
B âŠ—C, 13
(b), 97
BP , 106
Cn, 33
Cr, 83
âˆ†n, 87
Î´j
i , 5
det M, 16
Di, 71
diag(d1, . . . , dn), 5
dim E, 3
dimK F, 3
Dk(N), 102
e, 87
ei, 3
EK(Î»), 28
EÎ», 29
End(E), 7
Ïµ(Ïƒ), 16
exp A, 116
F + G, 2
F âŠ•G, 3
F âŠ•âŠ¥G, 12
F âŠ¥, 11
G, 152
G, 121
G(A), 71
GÎ±, 125
GCC, 3
gcd, 98
GLn(A), 20
G0, 126
H â‰¥h, 42
H â‰¥0n, 42
Hn, 41
HPDn, 42
âˆš
H, 115
â„‘, imaginary part, 56

xiv
List of Symbols
In, 5
J, 151
J(a; r), 110
Jik, 100
J2, 132
J3, 132
J4, 132
K(A), 162
K, 4
ker M, 7
ker u, 7
KI, 2
K(M), 6
Kn, 57
K[X], 15
k[X, Y ], 99
Î»k(A), 57
L(E, F), 7
LÏ‰, 152
adj M, 17
Â¯
M, 40
Ë†
M, 17
M
 i1
i2
Â· Â· Â·
ip
j1
j2
Â· Â· Â·
jp

, 17
M k, 6
M âˆ’1, 20
M âˆ’k, 20
M âˆ’T , 20
[M, N], 6
Mn(K), 5
MnÃ—m(K), 5
M âˆ—, 40
M âˆ’âˆ—, 40
M T , 10
âˆ¥A âˆ¥, 64
âˆ¥A âˆ¥p, 65
âˆ¥x âˆ¥p, 61
âˆ¥x âˆ¥A, 154
âˆ¥x âˆ¥âˆ, 61
âˆ¥Â· âˆ¥â€², 64
||| Â· |||, 65
Ï‰J, 158
0n, 5
On(K), 20
0nm, 5
Oâˆ’
n , 123
O(p, q), 120
âŠ¥A, 160
Pf, 22
PG, 156
Ï€0, 125
PJ, 156
PM, 24
PÏ‰, 156
pâ€², 62
PSL2(IR), 56
RA(F), 57
rk M, 5
â„œ, real part, 63
R(h; A), 92
Ï(A), 61
R(M), 8
r(x), 70, 160
âŸ¨x, yâŸ©, 11, 41
Sâˆ†n, 90
Ïƒr, 188
sj(A), 75
sk(a), 52
SLn(A), 20
sm, 189
Sn, 15
SOn(K), 20
S1, 86
Sp(M), 24
SpK(M), 24
SPDn, 42
Spm, 120
Spm, 120
S2, 56, 126
SUn, 41
Symn(K), 10
Ï„, 151
Ï„CG, 164
Tk, 162
Tr M, 25
Un, 41
U p, 85

List of Symbols
xv
U(p, q), 120
uâˆ—, 42
uT , 11
V (a), 173
|x|, 80
x â‰¤y, 80
x > 0, 80
x â‰¥0, 80

This page intentionally left blank 

1
Elementary Theory
1.1
Basics
1.1.1
Vectors and Scalars
Fields. Let (K, +, Â·) be a ï¬eld. It could be IR, the ï¬eld of real numbers, CC
(complex numbers), or, more rarely, QQ (rational numbers). Other choices
are possible, of course. The elements of K are called scalars.
Given a ï¬eld k, one may build larger ï¬elds containing k: algebraic ex-
tensions k(Î±1, . . . , Î±n), ï¬elds of rational fractions k(X1, . . . , Xn), ï¬elds of
formal power series k[[X1, . . . , Xn]]. Since they are rarely used in this book,
we do not deï¬ne them and let the reader consult his or her favorite textbook
on abstract algebra.
The digits 0 and 1 have the usual meaning in a ï¬eld K, with 0 + x =
1 Â· x = x. Let us consider the subring ZZ1, composed of all sums (possibly
empty) of the form Â±(1 + Â· Â· Â· + 1). Then ZZ1 is isomorphic to either ZZ or
to a ï¬eld ZZ/pZZ. In the latter case, p is a prime number, and we call it the
characteristic of K. In the former case, K is said to have characteristic 0.
Vector spaces. Let (E, +) be a commutative group. Since E is usually
not a subset of K, it is an abuse of notation that we use + for the additive
laws of both E and K. Finally, let
(a, x)
â†’
ax,
K Ã— E
â†’
E,

2
1. Elementary Theory
be a map such that
(a + b)x = ax + bx,
a(x + y) = ax + ay.
One says that E is a vector space over K (one often speaks of a K-vector
space) if moreover,
a(bx) = (ab)x,
1x = x,
hold for all a, b âˆˆK and x âˆˆE. The elements of E are called vectors. In a
vector space one always has 0x = 0 (more precisely, 0Kx = 0E).
When P, Q âŠ‚K and F, G âŠ‚E, one denotes by PQ (respectively P +
Q, F +G, PF) the set of products pq as (p, q) ranges over P Ã—Q (respectively
p+q, f+g, pf as p, q, f, g range over P, Q, F, G). A subgroup (F, +) of (E, +)
that is stable under multiplication by scalars, i.e., such that KF âŠ‚F, is
again a K-vector space. One says that it is a linear subspace of E, or just a
subspace. Observe that F, as a subgroup, is nonempty, since it contains 0E.
The intersection of any family of linear subspaces is a linear subspace. The
sum F + G of two linear subspaces is again a linear subspace. The trivial
formula (F + G) + H = F + (G + H) allows us to deï¬ne unambiguously
F + G + H and, by induction, the sum of any ï¬nite family of subsets of E.
When these subsets are linear subspaces, their sum is also a linear subspace.
Let I be a set. One denotes by KI the set of maps a = (ai)iâˆˆI : I â†’K
where only ï¬nitely many of the aiâ€™s are nonzero. This set is naturally
endowed with a K-vector space structure, by the addition and product
laws
(a + b)i := ai + bi,
(Î»a)i := Î»ai.
Let E be a vector space and let i â†’fi be a map from I to E. A linear
combination of (fi)iâˆˆI is a sum

iâˆˆI
aifi,
where the aiâ€™s are scalars, only ï¬nitely many of which are nonzero (in other
words, (ai)iâˆˆI âˆˆKI). This sum involves only ï¬nitely many terms. It is a
vector of E. The family (fi)iâˆˆI is free if every linear combination but the
trivial one (when all coeï¬ƒcients are zero) is nonzero. It is a generating
family if every vector of E is a linear combination of its elements. In other
words, (fi)iâˆˆI is free (respectively generating) if the map
KI
â†’
E,
(ai)iâˆˆI
â†’

iâˆˆI
aifi,
is injective (respectively onto). Last, one says that (fi)iâˆˆI is a basis of E if
it is free and generating. In that case, the above map is bijective, and it is
actually an isomorphism between vector spaces.

1.1. Basics
3
If G âŠ‚E, one often identiï¬es G and the associated family (g)gâˆˆG. The set
G of linear combinations of elements of G is a linear subspace E, called the
linear subspace spanned by G. It is the smallest linear subspace E containing
G, equal to the intersection of all linear subspaces containing G. The subset
G is generating when G = E.
One can prove that every K-vector space admits at least one basis. In
the most general setting, this is a consequence of the axiom of choice.
All the bases of E have the same cardinality, which is therefore called the
dimension of E, denoted by dim E. The dimension is an upper (respectively
a lower) bound for the cardinality of free (respectively generating) families.
In this book we shall only use ï¬nite-dimensional vector spaces. If F, G are
two linear subspaces of E, the following formula holds:
dim F + dim G = dim F âˆ©G + dim(F + G).
If F âˆ©G = {0}, one writes F âŠ•G instead of F + G, and one says that F
and G are in direct sum. One has then
dim F âŠ•G = dim F + dim G.
Given a set I, the family (ei)iâˆˆI, deï¬ned by
(ei)j =

0,
j Ì¸= i,
1,
j = i,
is a basis of KI, called the canonical basis. The dimension of KI is therefore
equal to the cardinality of I.
In a vector space, every generating family contains at least one basis of
E. Similarly, given a free family, it is contained in at least one basis of E.
This is the incomplete basis theorem.
Let L be a ï¬eld and K a subï¬eld of L. If F is an L-vector space, then F
is also a K-vector space. As a matter of fact, L is itself a K-vector space,
and one has
dimK F = dimL F Â· dimK L.
The most common example (the only one that we shall consider) is K = IR,
L = CC, for which we have
dimIR F = 2 dimCC F.
Conversely, if G is an IR-vector space, one builds its complexiï¬cation GCC
as follows:
GCC = G Ã— G,
with the induced structure of an additive group. An element (x, y) of GCC
is also denoted x + iy. One deï¬nes multiplication by a complex number by
(Î» = a + ib, z = x + iy) â†’Î»z := (ax âˆ’by, ay + bx).

4
1. Elementary Theory
One veriï¬es easily that GCC is a CC-vector space, with
dimCC GCC = dimIR G.
Furthermore, G may be identiï¬ed with an IR-linear subspace of GCC by
x â†’(x, 0).
Under this identiï¬cation, one has GCC = G + iG. In a more general setting,
one may consider two ï¬elds K and L with K âŠ‚L, instead of IR and CC, but
the construction of GL is more delicate and involves the notion of tensor
product. We shall not use it in this book.
One says that a polynomial P âˆˆL[X] splits over L if it can be written
as a product of the form
a
r

i=1
(X âˆ’ai)ni,
a, ai âˆˆL,
r âˆˆIN, ni âˆˆIN âˆ—.
Such a factorization is unique, up to the order of the factors. A ï¬eld L in
which every nonconstant polynomial P âˆˆL[X] admits a root, or equiva-
lently in which every polynomial P âˆˆL[X] splits, is algebraically closed. If
the ï¬eld Kâ€² contains the ï¬eld K and if every polynomial P âˆˆK[X] admits
a root in Kâ€², then the set of roots in Kâ€² of polynomials in K[X] is an alge-
braically closed ï¬eld that contains K, and it is the smallest such ï¬eld. One
calls Kâ€² the algebraic closure of K. Every ï¬eld K admits an algebraic clo-
sure, unique up to isomorphism, denoted by K. The fundamental theorem
of algebra asserts that IR = CC. The algebraic closure of QQ, for instance,
is the set of algebraic complex numbers, meaning that they are roots of
polynomials P âˆˆZZ[X].
1.1.2
Matrices
Let K be a ï¬eld. If n, m â‰¥1, a matrix of size n Ã— m with entries in K is a
map from {1, . . . , n} Ã— {1, . . . , m} with values in K. One represents it as
an array with n rows and m columns, an element of K (an entry) at each
point of intersection of a row an a column. In general, if M is the name of
the matrix, one denotes by mij the element at the intersection of the ith
row and the jth column. One has therefore
M =
ï£«
ï£¬
ï£­
m11
. . .
m1m
...
...
...
mn1
. . .
mnm
ï£¶
ï£·
ï£¸,
which one also writes
M = (mij)1â‰¤iâ‰¤n,1â‰¤jâ‰¤m.
In particular circumstances (extraction of matrices or minors, for example)
the rows and the columns can be numbered in a diï¬€erent way, using non-

1.1. Basics
5
consecutive numbers. One needs only two ï¬nite sets, one for indexing the
rows, the other for indexing the columns.
The set of matrices of size n Ã— m with entries in K is denoted by
MnÃ—m(K). It is an additive group, where M + M â€² denotes the matrix M â€²â€²
whose entries are given by mâ€²â€²
ij = mij + mâ€²
ij. One deï¬nes likewise multipli-
cation by a scalar a âˆˆK. The matrix M â€² := aM is deï¬ned by mâ€²
ij = amij.
One has the formulas a(bM) = (ab)M, a(M + M â€²) = (aM) + (aM â€²), and
(a + b)M = (aM) + (bM), which endow MnÃ—m(K) with a K-vector space
structure. The zero matrix is denoted by 0, or 0nm when one needs to avoid
ambiguity.
When m = n, one writes simply Mn(K) instead of MnÃ—n(K), and 0n
instead of 0nn. The matrices of sizes n Ã— n are called square matrices. One
writes In for the identity matrix, deï¬ned by
mij = Î´j
i =
 0,
if i Ì¸= j,
1,
if i = j.
In other words,
In =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
0
Â· Â· Â·
0
0
...
...
...
...
...
...
0
0
Â· Â· Â·
0
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
The identity matrix is a special case of a permutation matrix, which are
square matrices having exactly one nonzero entry in each row and each
column, that entry being a 1. In other words, a permutation matrix M
reads
mij = Î´Ïƒ(j)
i
for some permutation Ïƒ âˆˆSn.
A square matrix for which i < j implies mij = 0 is called a lower
triangular matrix. It is upper triangular if i > j implies mij = 0. It is
strictly upper triangular if i â‰¥j implies mij = 0. Last, it is diagonal if mij
vanishes for every pair (i, j) such that i Ì¸= j. In particular, given n scalars
d1, . . . , dn âˆˆK, one denotes by diag(d1, . . . , dn) the diagonal matrix whose
diagonal term mii equals di for every index i.
When m = 1, a matrix M of size n Ã— 1 is called a column vector. One
identiï¬es it with the vector of Kn whose ith coordinate in the canonical
basis is mi1. This identiï¬cation is an isomorphism between MnÃ—1(K) and
Kn. Likewise, the matrices of size 1 Ã— m are called row vectors.
A matrix M âˆˆMnÃ—m(K) may be viewed as the ordered list of its
columns M (j) (1 â‰¤j â‰¤m). The dimension of the linear subspace spanned
by the M (j) in Kn is called the rank of M and denoted by rk M.

6
1. Elementary Theory
1.1.3
Product of Matrices
Let n, m, p â‰¥1 be three positive integers. We deï¬ne a (noncommutative)
multiplication law
MnÃ—m(K) Ã— MmÃ—p(K)
â†’
MnÃ—p(K),
(M, M â€²)
â†’
MM â€²,
which we call the product of M and M â€². The matrix M â€²â€² = MM â€² is given
by the formula
mâ€²â€²
ij =
m

k=1
mikmâ€²
kj,
1 â‰¤i â‰¤n, 1 â‰¤j â‰¤p.
We check easily that this law is associative: if M, M â€², and M â€²â€² have
respective sizes n Ã— m, m Ã— p, p Ã— q, one has
(MM â€²)M â€²â€² = M(M â€²M â€²â€²).
The product is distributive with respect to addition:
M(M â€² + M â€²â€²) = MM â€² + MM â€²â€²,
(M + M â€²)M â€²â€² = MM â€²â€² + M â€²M â€²â€².
It also satisï¬es
a(MM â€²) = (aM)M â€² = M(aM â€²),
âˆ€a âˆˆK.
Last, if m = n, then InM â€² = M â€². Similarly, if m = p, then MIm = M.
The product is an internal composition law in Mn(K), which endows
this space with a structure of a unitary K-algebra. It is noncommutative
in general. For this reason, we deï¬ne the commutator of M and N by
[M, N] := MN âˆ’NM. For a square matrix M âˆˆMn(K), one deï¬nes
M 2 = MM, M 3 = MM 2 = M 2M (from associativity), ..., M k+1 = M kM.
One completes this notation by M 1 = M and M 0 = In. One has M jM k =
M j+k for all j, k âˆˆIN. If M k = 0 for some integer k âˆˆIN, one says that
M is nilpotent. One says that M is idempotent if In âˆ’M is nilpotent.
One says that two matrices M, N âˆˆMn(K) commute with each other
if MN = NM. The powers of a square matrix M commute pairwise. In
particular , the set K(M) formed by polynomials in M, which cinsists of
matrices of the form
a0In + a1M + Â· Â· Â· + arM r,
a0, . . . , ar âˆˆK,
r âˆˆIN,
is a commutative algebra.
One also has the formula (see Exercise 2)
rk(MM â€²) â‰¤min{rk M, rk M â€²}.
1.1.4
Matrices as Linear Maps
Let E, F be two K-vector spaces. A map u : E â†’F is linear (one also
speaks of a homomorphism) if u(x + y) = u(x) + u(y) and u(ax) = au(x)

1.1. Basics
7
for every x, y âˆˆE and a âˆˆK. One then has u(0) = 0. The preimage
uâˆ’1(0), denoted by ker u, is the kernel of u. It is a linear subspace of E.
The range u(E) is also a linear subspace of F. The set of homomorphisms
of E into F is a K-vector space, denoted by L(E, F). If F = E, one deï¬nes
End(E) := L(E, F); its elements are the endomorphisms of E.
The identiï¬cation of MnÃ—1(K) with Kn allows us to consider the matri-
ces of size n Ã— m as linear maps from Km to Kn. If M âˆˆMnÃ—m(K), one
proceeds as in the following diagram:
Km
â†’
MmÃ—1(K)
â†’
MnÃ—1(K)
â†’
Kn,
x
â†’
X
â†’
Y = MX
â†’
y.
Namely, the image of the vector x with coordinates x1, . . . , xm is the vector
y with coordinates y1, . . . , yn given by
yi =
m

j=1
mijxj.
(1.1)
One thus obtains an isomorphism between MnÃ—m(K) and L(Km; Kn),
which we shall use frequently in studying matrix properties.
More generally, if E, F are K-vector spaces of respective dimensions m
and n, in which one chooses bases Î² = {e1, . . . , em} and Î³ = {f1, . . . , fn},
one may construct the linear map u : E â†’F by
u(x1e1 + Â· Â· Â· + xmem) = y1f1 + Â· Â· Â· + ynfn,
via the formulas (1.1). One says that M is the matrix of u in the bases Î²,
Î³.
Let E, F, G be three K-vector spaces of dimensions p, m, n. Let us
choose respective bases Î±, Î², Î³. Given two matrices M, M â€² of sizes n Ã— m
and m Ã— p, corresponding to linear maps u : F â†’G and uâ€² : E â†’F, the
product MM â€² is the matrix of the linear map u â—¦uâ€² : E â†’G. Here lies
the origin of the deï¬nition of the product of matrices. The associativity
of the product expresses that of the composition of maps. One will note,
however, that the isomorphism between MnÃ—m(K) and L(E, F) is by no
means canonical, since the correspondence M â†’u always depends on an
arbitrary choice of two bases. One thus cannot reduce the entire theory of
matrices to that of linear maps, and vice versa.
When E = F is a K-vector space of dimension n, it is often worth
choosing a single basis (Î³ = Î² with the previous notation). One then has
an algebra isomorphism M â†’u between Mn(K) and End(E), the algebra
of endomorphisms of E. Again, this isomorphism depends on an arbitrary
choice of basis.
If M is the matrix of u âˆˆL(E, F) in the bases Î±, Î², the linear subspace
u(E) is spanned by the vectors of F whose representations in the basis Î²
are the columns M (j) of M. Its dimension thus equals rkM.
If M âˆˆMnÃ—m(K), one deï¬nes the kernel of M to be the set ker M of
those X âˆˆMmÃ—1(K) such that MX = 0n. The image of Km under M is

8
1. Elementary Theory
called the range of M, sometimes denoted by R(M). The kernel and the
range of M are linear subspaces of Km and Kn, respectively. The range is
spanned by the columns of M and therefore has dimension rk M.
Proposition 1.1.1 Let K be a ï¬eld. If M âˆˆMnÃ—m(K), then
m = dim ker M + rk M.
Proof
Let {f1, . . . , fr} be a basis of R(M). By construction, there exist vectors
{e1, . . . , er} of Km such that Mej = fj. Let E be the linear subspace
spanned by the ej. If e = 
j ajej âˆˆker M, then 
j ajfj = 0, and thus the
aj vanish. It follows that the restriction M : E â†’R(M) is an isomorphism,
so that dim E = rk M.
If e âˆˆKm, then Me âˆˆR(M), and there exists eâ€² âˆˆE such that Meâ€² =
Me. Therefore, e = eâ€² + (e âˆ’eâ€²) âˆˆE + ker M, so that Km = E + ker M.
Since E âˆ©ker M = {0}, one has m = dim E + dim ker M.
1.2
Change of Basis
Let E be a K-vector space, in which one chooses a basis Î² = {e1, . . . , en}.
Let P âˆˆMn(K) be an invertible matrix.1 The set Î²â€² = {eâ€²
1, . . . , eâ€²
n} deï¬ned
by
eâ€²
i =
n

j=1
pjiej
is a basis of E. One says that P is the matrix of the change of basis Î² â†’Î²â€²,
or the change-of-basis matrix. If x âˆˆE has coordinates (x1, . . . , xn) in the
basis Î² and (xâ€²
1, . . . , xâ€²
n) in the basis Î²â€², one then has the formulas
xj =
n

i=1
pjixâ€²
i.
If u : E â†’F is a linear map, one may compare the matrices of u for
diï¬€erent choices of the bases of E and F. Let Î², Î²â€² be bases of E and let
Î³, Î³â€² be bases of F. Let us denote by P, Q the change-of-basis matrices of
Î² â†’Î²â€² and Î³ â†’Î³â€². Finally, let M, M â€² be the matrices of u in the bases
Î², Î³ and Î²â€², Î³â€², respectively. Then
MP = QM â€²,
or M â€² = Qâˆ’1MP, where Qâˆ’1 denotes the inverse of Q. One says that M
and M â€² are equivalent. Two equivalent matrices have same rank.
1See Section 2.2 for the meaning of this notion.

1.2. Change of Basis
9
If E = F and u âˆˆEnd(E), one may compare the matrices M, M â€² of u
in two diï¬€erent bases Î², Î²â€² (here Î³ = Î² and Î³â€² = Î²â€²). The above formula
becomes
M â€² = P âˆ’1MP.
One says that M and M â€² are similar, or that they are conjugate (the latter
term comes from group theory). One also says that M â€² is the conjugate of
M by P.
The equivalence and the similarity of matrices are two equivalence
relations. They will be studied in Chapter 6.
1.2.1
Block Decomposition
Considering matrices with entries in a ring A does not cause diï¬ƒculties, as
long as one limits oneself to addition and multiplication. However, when A
is not commutative, it is important to choose the formula
m

j=1
MijM â€²
jk
when computing (MM â€²)ik, since this one corresponds to the composition
law when one identiï¬es matrices with A-linear maps from Am to An.
When m = n, the product is a composition law in Mn(K). This space
is thus a K-algebra. In particular, it is a ring, and one may consider the
matrices with entries in B = Mn(K). Let M âˆˆMpÃ—q(B) have entries Mij
(one chooses uppercase letters in order to keep in mind that the entries
are themselves matrices). One naturally identiï¬es M with the matrix M â€² âˆˆ
MpnÃ—qn(K), whose entry of indices ((i âˆ’1)n + k, (j âˆ’1)n + l), for i â‰¤p,
j â‰¤q, and k, l â‰¤n, is nothing but
(Mij)kl.
One veriï¬es easily that this identiï¬cation is an isomorphism between
MpÃ—q(B) and MpnÃ—qn(K) as K-vector spaces.
More generally, choosing decompositions n = n1+Â· Â· Â·+nr, m = m1+Â· Â· Â·+
ms with nk, ml â‰¥1, one may associate to every matrix M âˆˆMnÃ—m(K)
an array Ëœ
M with r rows and s columns whose element of index (k, l) is a
matrix Ëœ
Mkl âˆˆMnkÃ—ml(K). Deï¬ning
Î½k =

t<k
nt,
Âµl =

t<l
mt
(Î½1 = Âµ1 = 0),
one has by deï¬nition
( Ëœ
Mkl)ij = mÎ½k+i,Âµl+j,
1 â‰¤i â‰¤nk, 1 â‰¤j â‰¤ml.
This procedure, which depends on the choice of nk, ml, is called block
decomposition.

10
1. Elementary Theory
Though Ëœ
M is not strictly speaking a matrix (except in the case studied
previously where the nk, ml are all equal to each other), one still may deï¬ne
the sum and the product of such objects. Concerning the product of Ëœ
M and
Ëœ
M â€², we must of course be able to compute the products Ëœ
Mjk Ëœ
M â€²
kl, and thus
the sizes of blocks must be compatible. One veriï¬es easily that the block
decomposition behaves well with respect to the addition and the product.
For instance, if n = n1 + n2, m = m1 + m2 and p = p1 + p2, two matrices
M, M â€² of sizes n Ã— m and m Ã— p, with block decomposition Mij, M â€²
kl, have
a product M â€²â€² = MM â€² âˆˆMnÃ—p(K), whose block decomposition M â€²â€²
ij is
given by
M â€²â€²
ij = Mi1M â€²
1j + Mi2M â€²
2j.
A square matrix M, whose block decomposition is the same according to
rows and columns (that is mk = nk, in particular the diagonal blocks are
square matrices) is said lower block-triangular if the blocks Mkl with k < l
are null blocks. One deï¬nes similarly the upper block-triangular matrices or
the block-diagonal matrices.
1.2.2
Transposition
If M âˆˆMnÃ—m(K), one deï¬nes the transposed matrix of M (or simply the
transpose of M) by
M T = (mji)1â‰¤iâ‰¤m,1â‰¤jâ‰¤n.
The transposed matrix has size m Ã— n, and its entries Ë†mij are given by
Ë†mij = mji. When the product MM â€² makes sense, one has (MM â€²)T =
(M â€²)T M T (note that the orders in the two products are reversed). For two
matrices of the same size, (M + M â€²)T = M T + (M â€²)T . Finally, if a âˆˆK,
then (aM)T = a(M T ). The map M â†’M â€² deï¬ned on Mn(K) is thus linear,
but it is not an algebra endomorphism.
A matrix and its transpose have the same rank. A proof of this fact is
given at the end of this section.
For every matrix M âˆˆMnÃ—m(K), the products M T M and MM T always
make sense. These products are square matrices of sizes m Ã— m and n Ã— n,
respectively.
A square matrix is said to be symmetric if M T = M, and skew-symmetric
if M T = âˆ’M (notice that these two notions coincide when K has char-
acteristic 2). When M âˆˆMnÃ—m(K), the matrices M TM and MM T are
symmetric. We denote by Symn(K) the subset of symmetric matrices in
Mn(K). It is a linear subspace of Mn(K). The product of two symmetric
matrices need not be symmetric.
A square matrix is called orthogonal if M T M = In. We shall see in
Section 2.2 that this condition is equivalent to MM T = In.
If M âˆˆMnÃ—m(K), y âˆˆKm, and x âˆˆKn, then the product xT My
belongs to M1(K) and is therefore a scalar, equal to yT M Tx. Saying that

1.2. Change of Basis
11
M = 0 amounts to writing xT My = 0 for every x and y. If m = n and
xT Mx = 0 for every x, one says that M is alternate. An alternate matrix
is skew-symmetric, since
xT (M +M T)y = xT My+yTMx = (x+y)T M(x+y)âˆ’xTMxâˆ’yTMy = 0.
The converse holds whenever the characteristic of K is not 2, since
2xT Mx = xT (M + M T )x = 0.
However, in characteristic 2 there exist matrices that are skew-symmetric
but not alternate. As a matter of fact, the diagonal of an alternate matrix
must vanish, though this need not be the case for a skew-symmetric matrix
in characteristic 2.
The interpretation of transposition in terms of linear maps is the
following. One provides Kn with the bilinear form
âŸ¨x, yâŸ©:= xT y = yT x = x1y1 + Â· Â· Â· + xnyn,
called the canonical scalar product; one proceeds similarly in Km. If M âˆˆ
MnÃ—m(K), there exists a unique matrix N âˆˆMmÃ—n(K) satisfying
âŸ¨Mx, yâŸ©= âŸ¨x, NyâŸ©,
for all x âˆˆKm and y âˆˆKn (notice that the scalar products are deï¬ned on
distinct vector spaces). One checks easily that N = M T . More generally, if
E, F are K-vector spaces endowed with nondegenerate symmetric bilinear
forms, and if u âˆˆL(E, F), then one can deï¬ne a unique uT âˆˆL(F, E) from
the identity
âŸ¨u(x), yâŸ©F = âŸ¨x, uT (y)âŸ©E,
âˆ€x âˆˆE, y âˆˆF.
When E = Km and F = Kn are endowed with their canonical bases and
canonical scalar products, the matrix associated to uT is the transpose of
the matrix associated to u.
Let K be a ï¬eld. Let us endow Km with its canonical scalar product. If
F is a linear subspace of Km, one deï¬nes the orthogonal subspace of F by
F âŠ¥:= {x âˆˆKm; âŸ¨x, yâŸ©= 0, âˆ€y âˆˆF}.
It is a linear subspace of Km. We observe that for a general ï¬eld, the
intersection F âˆ©F âŠ¥can be nontrivial, and Km may diï¬€er from F + F âŠ¥.
One has nevertheless
dim F + dim F âŠ¥= m.
Actually, F âŠ¥is the kernel of the linear map T : Km â†’L(F; K) =: F âˆ—,
deï¬ned by T (x)(y) = âŸ¨x, yâŸ©for x âˆˆKm, y âˆˆF. Let us show that T is onto.
If {f1, . . . , fr} is a basis of F, then every linear form l on F is a map
f =

j
zjfj â†’l(f) =

j
l(fj)zj.

12
1. Elementary Theory
Completing the basis of F as a basis of Km, one sees that l is the restric-
tion of a linear form L on Km. Let us deï¬ne the vector x âˆˆKm by its
coordinates in the canonical basis: xj = L(ej). One has L(y) = âŸ¨x, yâŸ©for
every y âˆˆKm; that is, l = T (x). Finally, we obtain
m = dim kerT + rk T = dim F âŠ¥+ dim F âˆ—.
The dual formulas between kernels and ranges are frequently used. If
M âˆˆMnÃ—m(K), one has
Km = ker M âŠ•âŠ¥R(M T ),
Kn = ker(M T ) âŠ•âŠ¥R(M),
where âŠ•âŠ¥means a direct sum of orthogonal subspaces. We conclude that
rk M T = dim R(M T ) = m âˆ’dim R(M T )âŠ¥= m âˆ’dim ker M,
and ï¬nally, that
rk M T = rk M.
1.2.3
Matrices and Bilinear Forms
Let E, F be two K-vector spaces. One chooses two respective bases Î² =
{e1, . . . , en} and Î³ = {f1, . . . , fm}. If B : E Ã— F â†’K is a bilinear form,
then
B(x, y) =

i,j
B(ei, fj)xiyj,
where the xi, yj are the coordinates of x, y. One can deï¬ne a matrix M âˆˆ
MnÃ—m(K) by mij = B(ei, fj). Conversely, if M âˆˆMnÃ—m(K) is given, one
can construct a bilinear form on E Ã— F by the formula
B(x, y) := xT My =

i,j
mijxiyj.
Therefore, there is an isomorphism between MnÃ—m(K) and the set of bi-
linear forms on E Ã— F. One says that M is the matrix of B with respect
to the bases Î², Î³. This isomorphism depends on the choice of the bases.
A particular case arises when E = Kn and F = Km are endowed with
canonical bases.
If M is associated to B, it is clear that M T is associated to the bilinear
form deï¬ned on F Ã— E by
(y, x) â†’B(x, y).
When M is a square matrix, one may take F = E and Î³ = Î². In that
case, M is symmetric if and only if B is symmetric: B(x, y) = B(y, x).
Likewise, one says that B is alternate if B(x, x) â‰¡0, that is if M itself is
an alternate matrix.

1.3. Exercises
13
If B : E Ã— F â†’K is bilinear, one can compare the matrices M and
M â€² of B with respect to the bases Î², Î³ and Î²â€², Î³â€². Denoting by P, Q the
change-of-basis matrices of Î² â†’Î²â€² and Î³ â†’Î³â€², one has
mâ€²
ij = B(eâ€²
i, f â€²
j) =

k,l
pkiqljB(ek, fl) =

k,l
pkiqljmkl.
Therefore,
M â€² = P T MQ.
When F = E and Î³ = Î², Î³â€² = Î²â€², the change of basis has the eï¬€ect of
replacing M by M â€² = P T MP. In general, M â€² is not similar to M, though
it is so if P is orthogonal. If M is symmetric, then M â€² is too. This was
expected, since one expresses the symmetry of the underlying bilinear form
B.
If the characteristic of K is distinct from 2, there is an isomorphism
between Symn(K) and the set of quadratic forms on Kn. This isomorphism
is given by the formula
Q(ei + ej) âˆ’Q(ei) âˆ’Q(ej) = 2mij.
In particular, Q(ei) = mii.
1.3
Exercises
1. Let G be an IR-vector space. Verify that its complexiï¬cation GCC is a
CC-vector space and that dimCC GCC = dimIR G.
2. Let M âˆˆMnÃ—m(K) and M â€² âˆˆMmÃ—p(K) be given. Show that
rk(MM â€²) â‰¤min{rk M, rk M â€²}.
First show that rk(MM â€²) â‰¤rk M, and then apply this result to the
transpose matrix.
3. Let K be a ï¬eld and let A, B, C be matrices with entries in K, of
respective sizes n Ã— m, m Ã— p, and p Ã— q.
(a) Show that rk A + rk B â‰¤m + rk AB. It is suï¬ƒcient to consider
the case where B is onto, by considering the restriction of A to
the range of B.
(b) Show that rk AB + rk BC â‰¤rk B + rk ABC. One may use
the vector spaces Kp/ ker B and R(B), and construct three
homomorphisms u, v, w, with v being onto.
4.
(a) Let n, nâ€², m, mâ€² âˆˆIN âˆ—and let K be a ï¬eld. If B âˆˆMnÃ—m(K) and
C âˆˆMnâ€²Ã—mâ€²(K), one deï¬nes a matrix B âŠ—C âˆˆMnnâ€²Ã—mmâ€²(K),

14
1. Elementary Theory
the tensor product, whose block form is
B âŠ—C =
ï£«
ï£¬
ï£­
b11C
Â· Â· Â·
b1mC
...
...
bn1C
Â· Â· Â·
bnmC
ï£¶
ï£·
ï£¸.
Show that (B, C) â†’B âŠ—C is a bilinear map and that its range
spans Mnnâ€²Ã—mmâ€²(K). Is this map onto?
(b) If p, pâ€² âˆˆIN âˆ—and D âˆˆMmÃ—p(K), E âˆˆMmâ€²Ã—pâ€²(K), then
compute (B âŠ—C)(D âŠ—E).
(c) Show that for every bilinear form Ï† : MnÃ—m(K)Ã—Mnâ€²Ã—mâ€²(K) â†’
K, there exists one and only one linear form
L : Mnnâ€²Ã—mmâ€²(K) â†’K
such that L(B âŠ—C) = Ï†(B, C).

2
Square Matrices
The essential ingredient for the study of square matrices is the determinant.
For reasons that will be given in Section 2.5, as well as in Chapter 6, it
is useful to consider matrices with entries in a ring. This allows us to
consider matrices with entries in ZZ (rational integers) as well as in K[X]
(polynomials with coeï¬ƒcients in K). We shall assume that the ring A of
scalars is a commutative (meaning that the multiplication is commutative)
integral domain (meaning that it does not have zero divisors: ab = 0 implies
either a = 0 or b = 0), with a unit denoted by 1, that is, an element
satisfying 1x = x1 = x for every x âˆˆA. Observe that the ring Mn(A) is
not commutative if n â‰¥2. For instance,
 0
1
0
0
  0
0
1
0

=
 1
0
0
0

Ì¸=
 0
0
0
1

=
 0
0
1
0
  0
1
0
0

.
An element a of A is invertible if there exists b âˆˆA such that ab = 1.
The element b is unique (because A is an integral domain), and one calls it
the inverse of a, with the notation b = aâˆ’1. The set of invertible elements
of A is a multiplicative group, denoted by Aâˆ—. One has
(ab)âˆ’1 = bâˆ’1aâˆ’1 = aâˆ’1bâˆ’1.
2.1
Determinants and Minors
We recall that Sn, the symmetric group, denotes the group of permutations
over the set {1, . . . , n}.

16
2. Square Matrices
Let M âˆˆMn(A) be a square matrix. Its determinant is deï¬ned by
det M :=

ÏƒâˆˆSn
Ïµ(Ïƒ)m1Ïƒ(1) Â· Â· Â· mnÏƒ(n),
where the sum ranges over all the permutations of the integers 1, . . . , n.
We denote by Ïµ(Ïƒ) = Â±1 the signature of Ïƒ, equal to +1 if Ïƒ is the product
an even number of transpositions, and âˆ’1 otherwise. Recall that Ïµ(ÏƒÏƒâ€²) =
Ïµ(Ïƒ)Ïµ(Ïƒâ€²).
If M is triangular, then all the products vanish other than the one
associated with the identity (that is, Ïƒ(j) = j). The determinant of a
triangular M is thus equal to the product of diagonal entries mii. In par-
ticular, det In = 1 and det 0n = 0. An analogous calculation shows that
the determinant of a block triangular matrix is equal to the product of the
determinants of the diagonal blocks Mjj.
Since Ïµ(Ïƒâˆ’1) = Ïµ(Ïƒ), one has
det M T = det M.
Looking at M as a row matrix with entries in An, one may view the
determinant as a multilinear form of the n columns of M:
det M = det

M (1), . . . , M (n)
.
This form is alternate: If two columns are equal, the determinant vanishes.
As a matter of fact, if the ith and the jth columns are equal, one groups the
permutations pairwise (Ïƒ, Ï„Ïƒ), where Ï„ is the transposition (i, j). For each
pair, both products are equal, up to the signatures, which are opposite;
their sum is thus zero. Likewise, if two rows are equal, the determinant is
zero.
More generally, if the columns of M satisfy a non trivial linear relation
(a1, . . . , an not all zero) of linear dependence
a1M1 + Â· Â· Â· + anMn = 0
(that is, if rk M < n), then det M is zero. Let us assume, for instance, that
a1 is nonzero. For j â‰¥2, one has
det

M (j), M (2), . . . , M (n)
= 0.
Using the multilinearity, one has thus
a1 det M
=
det

a1M (1) + Â· Â· Â· + anM (n), M (2), . . . , M (n)
=
det

0, M (2), . . .

= 0.
Since A is an integral domain, we conclude that det M = 0.
For a matrix M âˆˆMnÃ—m(A), not necessarily square, and p â‰¥1 an integer
with p â‰¤m, n, one may extract a p Ã— p matrix M â€² âˆˆMp(A) by retaining
only p rows and p columns of M. The determinant of such a matrix M â€² is

2.1. Determinants and Minors
17
called a minor of order p. Once the choice of the row indices i1 < Â· Â· Â· < ip
and column indices j1 < Â· Â· Â· < jp has been made, one denotes by
M
 i1
i2
Â· Â· Â·
ip
j1
j2
Â· Â· Â·
jp

the corresponding minor. A principal minor is a minor with equal row and
column indices, that is, of the form
M
 i1
i2
Â· Â· Â·
ip
i1
i2
Â· Â· Â·
ip

.
In particular, the leading principal minor of order p is
M

1
2
Â· Â· Â·
p
1
2
Â· Â· Â·
p

.
Given a matrix M âˆˆMn(A), one associates the matrix Ë†
M of cofactors,
deï¬ned as follows: its (i, j)-th entry Ë†mij is the minor of order nâˆ’1 obtained
by removing the ith row and the jth column multiplied by (âˆ’1)i+j. It is
also the factor of mij in the formula for the determinant of M. Finally, we
deï¬ne the adjoint matrix adj M by
adj M := Ë†
M T .
Proposition 2.1.1 If M âˆˆMn(A), one has
M(adj M) = (adj M)M = det M Â· In.
(2.1)
Proof
The identity is clear as far as diagonal terms are concerned; it amounts to
the deï¬nition of the determinant (see also below). The oï¬€-diagonal terms
mâ€²
ij of M(adj M) are sums involving on the one hand an index, and on
the other hand a permutation Ïƒ âˆˆSn. One groups the terms pairwise,
corresponding to permutations Ïƒ and ÏƒÏ„, where Ï„ is the tranposition (i, j).
The sum of two such terms is zero, so that mâ€²
ij = 0.
Proposition 2.1.1 contains the well-known and important expansion for-
mula for the determinant with respect to either a row or a column. The
expansion with respect to the ith row is written
det M = (âˆ’1)i+1mi1 Ë†mi1 + Â· Â· Â· + (âˆ’1)i+nmin Ë†min,
while the expansion with respect to the ith column is
det M = (âˆ’1)i+1m1i Ë†m1i + Â· Â· Â· + (âˆ’1)i+nmni Ë†mni.
2.1.1
Irreducibility of the Determinant
By deï¬nition, the determinant is a polynomial function, in the sense that
det M is the value taken by a polynomial DetA âˆˆA[x11, . . . , xnn] when the

18
2. Square Matrices
xijâ€™s are replaced by the scalars mij. We observe that DetA does not really
depend on the ring A, in the sense that it is the image of DetZZ through
the canonical ring homomorphism ZZ â†’A. For this reason, we shall simply
write Det. The polynomial Det may be viewed as the determinant of the
matrix X = (xij)1â‰¤i,jâ‰¤n âˆˆMn(A[x11, . . . , xnn]).
Theorem 2.1.1 The polynomial Det is irreducible in A[x11, . . . , xnn].
Proof
We shall proceed by induction on the size n. If n = 1, there is nothing
to prove. Thus let us assume that n â‰¥2. We denote by D the ring of
polynomials in the xij with (i, j) Ì¸= (1, 1), so that A[x11, . . . , xnn] = D[x11].
From the expansion with respect to the ï¬rst row, we see that Det = x11P +
Q, with P, Q âˆˆD. Since Det is of degree one as a polynomial in x11,
any factorization must be of the form (x11R + S)T , with R, S, T âˆˆD. In
particular, RT = P.
By induction, and since P is the polynomial Det of (n âˆ’1) Ã— (n âˆ’1)
matrices, it is irreducible in E, the ring of polynomials in the xijâ€™s with
i, j > 1. Therefore, it is also irreducible in D, since D is the polynomial
ring E[x12, . . . , x1n, x21, . . . , xn1]. Therefore, we may assume that either R
or T equals 1.
If the factorization is nontrivial, then R = 1 and T = P. It follows that
P divides Det. An expansion with respect to various rows shows similarly
that every minor of size nâˆ’1, considered as an element of A[x11, . . . , xnn],
divides Det. However, each such minor is irreducible, and they are pairwise
distinct, since they do not depend on the same set of xijâ€™s. We conclude
that the product of all minors of size n âˆ’1 divides Det. In particular, the
degree n of Det is greater than or equal to the degree n2(n âˆ’1) of this
product, an obvious contradiction.
2.1.2
The Cauchyâ€“Binet Formula
In the sequel, we shall use also the following result.
Proposition 2.1.2 Let B âˆˆMnÃ—m(A), C âˆˆMmÃ—l(A), and an integer
p â‰¤n, l be given. Let 1 â‰¤i1 < Â· Â· Â· < ip â‰¤n and 1 â‰¤k1 < Â· Â· Â· < kp â‰¤l be
indices. Then the minor
(BC)
 i1
i2
Â· Â· Â·
ip
k1
k2
Â· Â· Â·
kp

is given by the formula

1â‰¤j1<j2<Â·Â·Â·<jpâ‰¤m
B
 i1
i2
Â· Â· Â·
ip
j1
j2
Â· Â· Â·
jp

Â· C
 j1
j2
Â· Â· Â·
jp
k1
k2
Â· Â· Â·
kp

.

2.2. Invertibility
19
Corollary 2.1.1 Let b, c âˆˆA. If b divides every minor of order p of B
and if c divides every minor of order p of C, then bc divides every minor
of order p of BC.
The particular case l = m = n is fundamental:
Theorem 2.1.2 If B, C âˆˆMn(A), then det(BC) = det B Â· det C.
In other words, the determinant is a multiplicative homomorphism from
Mn(A) to A.
Proof
The corollaries are trivial. We only prove the Cauchyâ€“Binet formula.
Since the calculation of the ith row (respectively the jth column) of BC
involves only the ith row of B (respectively the jth column of C), one
may assume that p = n = l. The minor to be evaluated is then det BC. If
m < n, there is nothing to prove, since on the one hand the rank of BC
is less than or equal to m, thus det BC is zero, and on the other hand the
left-hand side sum in the formula is empty.
There remains the case m â‰¥n. Let us write the determinant of a ma-
trix P as that of its columns Pj and let us use the multilinearity of the
determinant:
det BC
=
det
ï£«
ï£­
n

j1=1
cj11Bj1, (BC)2, . . . , (BC)n
ï£¶
ï£¸
=
n

j1=1
cj11 det
ï£«
ï£­Bj1,
n

j2=1
cj22Bj2, (BC)3, . . . , (BC)n
ï£¶
ï£¸
=
Â· Â· Â· =

1â‰¤j1,... ,jnâ‰¤n
cj11 Â· Â· Â· cjnn det(Bj1, . . . , Bjn).
In the sum the determinant is zero as soon as f â†’jf is not injective,
since then there are two identical columns. If on the contrary j is injective,
this determinant is a minor of B, up to the sign. This sign is that of the
permutation that puts j1, . . . , jp in increasing order. Grouping in the sum
the terms corresponding to the same minor, we ï¬nd that det BC equals

1â‰¤k1<Â·Â·Â·<knâ‰¤m,

ÏƒâˆˆSn
Ïµ(Ïƒ)ck1Ïƒ(1) Â· Â· Â· cknÏƒ(n)B

1
2
Â· Â· Â·
n
k1
k2
Â· Â· Â·
kn

,
which is the required formula.
2.2
Invertibility
Since Mn(A) is not an integral domain, the notion of invertible elements
of Mn(A) needs an auxiliary result, presented below.

20
2. Square Matrices
Proposition 2.2.1 Given M
âˆˆMn(A), the following assertions are
equivalent:
1. There exists N âˆˆMn(A) such that MN = In.
2. There exists N â€² âˆˆMn(A) such that N â€²M = In.
3. det M is invertible.
If M satisï¬es one of these equivalent conditions, then the matrices N, N â€²
are unique and one has N = N â€².
Deï¬nition 2.2.1 One then says that M is invertible. One also says some-
times that M is nonsingular, or regular. One calls the matrix N = N â€² the
inverse of M, and one denotes it by M âˆ’1. If M is not invertible, one says
that M is singular.
Proof
Let us show that (1) is equivalent to (3). If MN = In, then det M Â·
det N
=
1; hence det M
âˆˆ
Aâˆ—. Conversely, if det M is invertible,
(det M)âˆ’1 Ë†
M T is an inverse of M by (2.1). Analogously, (2) is equivalent
to (3). The three assertions are thus equivalent.
If MN = N â€²M = In, one has N = (N â€²M)N = N â€²(MN) = N â€². This
equality between the left and right inverses shows that these are unique.
The set of the invertible elements of Mn(A) is denoted by GLn(A) (for
â€œgeneral linear groupâ€). It is a multiplicative group, and one has
(MN)âˆ’1 = N âˆ’1M âˆ’1,
(M k)âˆ’1 = (M âˆ’1)k,
(M T )âˆ’1 = (M âˆ’1)T .
The matrix (M T )âˆ’1 is also written M âˆ’T . If k âˆˆIN, one writes M âˆ’k =
(M k)âˆ’1 and one has M jM k = M j+k for every j, k âˆˆZZ.
The set of the matrices of determinant one is a normal subgroup of
GLn(A), since it is the kernel of the homomorphism M â†’det M. It is
called the special linear group and is denoted by SLn(A).
The orthogonal matrices are invertible, and they satisfy the relation
M âˆ’1 = M T . In particular, orthogonality is equivalent to MM T = In.
The set of orthogonal matrices with entries in a ï¬eld K is obviously a
multiplicative group, and is denoted by On(K). It is called the orthogonal
group. The determinant of an orthogonal matrix equals Â±1, since
1 = det M Â· det M T = (det M)2.
The set SOn(K) of orthogonal matrices with determinant equal to 1 is
obviously a normal subgroup of the orthogonal group. It is called the special
orthogonal group. It is simply the intersection of On(K) with SLn(K).
A triangular matrix is invertible if and only if its diagonal entries are
invertible; its inverse is then triangular of the same type, upper or lower.
The proposition below is an immediate application of Theorem 2.1.2.

2.3. Alternate Matrices and the Pfaï¬ƒan
21
Proposition 2.2.2 If M, M â€²
âˆˆ
Mn(A) are similar (that is, M â€²
=
P âˆ’1MP with P âˆˆGLn(A)), then
det M â€² = det M.
2.3
Alternate Matrices and the Pfaï¬ƒan
The very simple structure of alternate forms is described in the following
statement.
Proposition 2.3.1 Let B be an alternate bilinear form on a vector space
E, of dimension n. Then there exists a basis
{x1, y1, . . . , xk, yk, z1, . . . , znâˆ’2k}
such that the matrix of B in this basis is block-diagonal, equal to
diag(J, . . . , J, 0, . . . , 0), with k blocks J deï¬ned by
J =

0
1
âˆ’1
0

.
Proof
We proceed by induction on the dimension n. If B = 0, there is nothing to
prove. If B is nonzero, there exist two vectors x1, y1 such that B(x1, y1) Ì¸= 0.
Multiplying one of them by B(x1, y1)âˆ’1, one may assume that B(x1, y1) =
1. Since B is alternate, {x1, y1} is free. Let N be the plane spanned by x1, y1.
The set of vectors x satisfying B(x, v) = 0 (or equivalently B(v, x) = 0,
since B must be skew-symmetric) for every v in N is denoted by N âŠ¥. The
formulas
B(ax1 + by1, x1) = âˆ’b,
B(ax1 + by1, y1) = a
show that N âˆ©N âŠ¥= {0}. Additionally, every vector x âˆˆE can be written
as x = y + n, where n âˆˆN and y âˆˆN âŠ¥are given by
n = B(x, y1)x1 âˆ’B(x, x1)y1,
y := x âˆ’n.
Therefore, E = N âŠ•N âŠ¥. We now consider the restriction of B to the
subspace N âŠ¥and apply the induction hypothesis. There exists a basis
{x2, y2, . . . , xk, yk, z1, . . . , znâˆ’2k} such that the matrix of the restriction of
B in this basis is block-diagonal, equal to diag(J, . . . , J, 0, . . . , 0), with kâˆ’1
blocks J, which means that B(xj, yj) = 1 = âˆ’B(yj, xj) and B(u, v) = 0
for every other choice of u, v in the basis. Obviously, this property extends
to the form B itself and the basis {x1, y1, . . . , xk, yk, z1, . . . , znâˆ’2k}.
We now choose an alternate matrix M âˆˆMn(K) and apply Proposition
2.3.1 to the form deï¬ned by M. In view of Section 1.2.3, we have the
following.

22
2. Square Matrices
Corollary 2.3.1 Given an alternate matrix M âˆˆMn(K), there exists a
matrix Q âˆˆGLn(K) such that
M = QT diag(J, . . . , J, 0, . . . , 0)Q.
(2.2)
Obviously, the rank of M, being the same as that of the block-diagonal
matrix, equals twice the number of J blocks. Finally, since det J = 1, we
have det M = Ïµ(det Q)2, where Ïµ = 0 if there is a zero diagonal block in the
decomposition, and Ïµ = 1 otherwise. Thus we have proved the following
result.
Proposition 2.3.2 The rank of an alternate matrix M is even. The num-
ber of J blocks in the identity (2.2) is the half of that rank. In particular,
it does not depend on the decomposition. Finally, the determinant of an
alternate matrix is a square in K.
A very important application of Proposition 2.3.2 concerns the Pfaï¬ƒan,
whose crude deï¬nition is a polynomial whose square is the determinant of
the general alternate matrix. First of all, since the rank of an alternate
matrix is even, det M = 0 whenever n is odd. Therefore, we restrict our
attention from now on to the even-dimensional case n = 2m. Let us consider
the ï¬eld F = QQ(xij) of rational functions with rational coeï¬ƒcients, in
n(n âˆ’1)/2 indeterminates xij, i < j. We apply the proposition to the
alternate matrix X whose (i, i)-entry is 0 and (i, j)-entry (respectively (j, i)-
entry) is xij (respectively âˆ’xij). Its determinant, a polynomial in ZZ[xij], is
the square of some irreducible rational function f/g, where f and g belong
to ZZ[xij]. From g2 det X = f 2, we see that g divides f in ZZ[xij]. But since
f and g are coprime, one ï¬nds that g is invertible; in other words g = Â±1.
Thus
det X = f 2.
(2.3)
Now let k be a ï¬eld and let M âˆˆMn(k) be alternate. There exists
a unique homomorphism from ZZ[xij] into k sending xij to mij. From
equation (2.3) we obtain
det M = (f(m12, . . . , mnâˆ’1,n))2.
(2.4)
In particular, if k = QQ and M = diag(J, . . . , J), one has f 2 = 1. Up
to multiplication by Â±1, which leaves unchanged the identity (2.3), we
may assume that f = 1 for this special case. This determination of the
polynomial f is called the Pfaï¬ƒan and is denoted by Pf. It may be viewed
as a polynomial function on the vector space of alternate matrices with
entries in a given ï¬eld k. equation (2.4) now reads
det M = (Pf(M))2.
(2.5)
Given an alternate matrix M âˆˆMn(k) and a matrix Q âˆˆMn(k), we
consider the Pfaï¬ƒan of the alternate matrix QT MQ. We ï¬rst consider the
case of the ï¬eld of fractions QQ(xij, yij) in the n2+n(nâˆ’1)/2 indeterminates

2.4. Eigenvalues and Eigenvectors
23
xij (1 â‰¤i < j â‰¤n) and yij (1 â‰¤i, j â‰¤n). Let Y be the matrix whose
(i, j)-entry is yij. Then, with X as above,
(Pf(Y T XY ))2 = det Y T XY = (det Y )2 det X = (Pf(X) det Y )2.
Since ZZ[xij, yij] is an integral domain, we have the polynomial identity
Pf

Y T XY

= Ïµ Pf(X) det Y,
Ïµ = Â±1.
As above, one infers that Pf(QT MQ) = Â± Pf(M) det Q for every ï¬eld k,
matrix Q âˆˆMn(k), and alternate matrix M âˆˆMn(k). Inspection of the
particular case Q = In yields Ïµ = 1. We summarize these results now.
Theorem 2.3.1 Let n = 2m be an even integer. There exists a unique
polynomial Pf in the indeterminates xij (1 â‰¤i < j â‰¤n) with integer
coeï¬ƒcients such that:
â€¢ For every ï¬eld k and every alternate matrix M âˆˆMn(k), one has
det M = Pf(M)2.
â€¢ If M = diag(J, . . . , J), then Pf(M) = 1.
Moreover, if Q âˆˆMn(k) is given, then Pf

QT MQ

= Pf(M) det Q.
We warn the reader that if m > 1, there does not exist a matrix Z âˆˆQQ[xij]
such that X = ZT diag(J, . . . , J)Z. The factorization of the polynomial
det X does not correspond to a similar factorization of X itself. In other
words, the decomposition X = QT diag(J, . . . , J)Q in Mn(QQ(xij)) cannot
be written within Mn(QQ[xij]).
The Pfaï¬ƒan is computed easily for small values of n. For instance,
Pf(X) = x12 if n = 2, and Pf = x12x34 âˆ’x13x24 + x14x23 if n = 4.
2.4
Eigenvalues and Eigenvectors
Let K be a ï¬eld and E, F two vector spaces of ï¬nite dimension. Let us
recall that if u : E â†’F is a linear map, then
dim E = dim ker u + rk u,
where rk u denotes the dimension of u(E) (the rank of u). In particular, if
u âˆˆEnd(E), then
u is bijective â‡â‡’u is injective â‡â‡’u is surjective.
However, u is bijective, that is invertible, in End(E), if and only if its
matrix M in some basis Î² is invertible, that is if its determinant is nonzero.
As a matter of fact, the matrix of uâˆ’1 is M âˆ’1; the existence of an inverse
(either that of M or that of u) implies that of the other one. Finally, if
M âˆˆMn(K), then det M Ì¸= 0 is equivalent to
âˆ€X âˆˆKn,
MX = 0 =â‡’X = 0.

24
2. Square Matrices
In other words,
det M = 0 â‡â‡’(âˆƒX âˆˆKn, X Ì¸= 0, MX = 0).
More generally, since MX = Î»X (Î» âˆˆK) can also be written (Î»Inâˆ’M)X =
0, one sees that det(Î»In âˆ’M) is zero if and only if there exists a nonzero
vector in Kn such that MX = Î»X. One then says that Î» is an eigenvalue
of M in K, and that X is an eigenvector associated to Î». An eigenvector
is thus always a nonzero vector. The set of the eigenvalues of M in K is
called the spectrum of M and is denoted by SpK(M).
A matrix in Mn(K) may have no eigenvalues in K, as the following
example demonstrates, with K = IR:

0
1
âˆ’1
0

.
In order to understand in detail in the structure of a square matrix M âˆˆ
Mn(K), one is thus led to consider M as a matrix with entries in K. One
then writes Sp(M) instead of SpK(M), and one has SpK(M) = K âˆ©Sp(M),
since the eigenvalues are characterized by det(Î»In âˆ’M) = 0, and this
equality has the same meaning in K as in K when Î» âˆˆK.
2.5
The Characteristic Polynomial
The previous calculations show that the eigenvalues of M âˆˆMn(K) are
the roots of the polynomial
PM(X) := det(XIn âˆ’M).
Let us observe in passing that if X is an indeterminate, then XIn âˆ’M âˆˆ
Mn(K(X)). Its determinant PM is thus well-deï¬ned, since K(X) is a
commutative integral domain with a unit element. One calls PM the charac-
teristic polynomial of M. Substituting 0 for X, one sees that the constant
term in PM is simply (âˆ’1)n det M. Since the term corresponding to the
permutation Ïƒ = id in the computation of the determinant is of degree
n (it is 
i(X âˆ’mii)) and since the products corresponding to the other
permutations are of degree less than or equal to n âˆ’2, one sees that PM is
of degree n, with
PM(X) = Xn âˆ’
 n

i=1
mii

Xnâˆ’1 + Â· Â· Â· + (âˆ’1)n det M.
The coeï¬ƒcient
n

i=1
mii

2.5. The Characteristic Polynomial
25
is called the trace of M and is denoted by Tr M. One has the trivial formula
that if N âˆˆMnÃ—m(K) and P âˆˆMmÃ—n(K), then
Tr(NP) = Tr(PN).
For square matrices, this identity also becomes
Tr[N, P] = 0.
Since PM possesses n roots in K, counting multiplicities, one sees that
a square matrix has always at least one eigenvalue, which, however, does
not necessarily belong to K. The multiplicity of Î» as a root of PM is called
algebraic multiplicity of the eigenvalue Î». The geometric multiplicity of Î» is
the dimension of ker(Î»Inâˆ’M) in Kn. The sum of the algebraic multiplicities
of the eigenvalues of M (considered in K) is n, the size of the matrix. An
eigenvalue of algebraic multiplicity one (that is, a simple root of PM) is
called simple. It is geometrically simple if its geometric multiplicity equals
one.
The characteristic polynomial is a similarity invariant, in the following
sense:
Proposition 2.5.1 If M and M â€² are similar, then PM
= PMâ€². In
particular, det M = det M â€² and Tr M = Tr M â€².
The proof is immediate. One deduces that the eigenvalues and their
algebraic multiplicities are similarity invariants. This is also true for the
geometric multiplicities, by a direct comparison of the kernel of Î»In âˆ’
M and of Î»In âˆ’M â€². Furthermore, the expression obtained above for the
characteristic polynomial provides the following result.
Proposition 2.5.2 The product of the eigenvalues of M (considered in
K), counted with their algebraic multiplicities, is det M. Their sum is Tr M.
Let Âµ be the geometric multiplicity of an eigenvalue Î» of M. Let us choose
a basis Î³ of ker(Î»In âˆ’M), and then a basis of Î² of K
n that completes Î³.
Using the change-of-basis matrix from the canonical basis to Î², one sees
that M is similar to a matrix M â€² = P âˆ’1MP, whose Âµ ï¬rst columns have
the form

Î»IÂµ
0nâˆ’Âµ,Âµ

.
A direct calculation shows then that (Xâˆ’Î»)Âµ divides PMâ€², that is, PM. The
geometric multiplicity is thus less than or equal to the algebraic multiplicity.
The characteristic polynomials of M and M T are equal. Thus, M and
M T have the same eigenvalues. We shall show in Chapter 6 a much deeper
result, namely M and M T are similar.
The main result concerning the characteristic polynomial is the Cayleyâ€“
Hamilton theorem:

26
2. Square Matrices
Theorem 2.5.1 Let M âˆˆMn(K). Let
PM(X) = Xn + a1Xnâˆ’1 + Â· Â· Â· + an
be its characteristic polynomial. Then the matrix
M n + a1M nâˆ’1 + Â· Â· Â· + anIn
equals 0n.
One also writes PM(M) = 0. Though this formula looks trivial (obviously,
det(MIn âˆ’M) = 0), it is not. Actually, it must be understood in the
following way. Let us consider the expression XIn âˆ’M as a matrix with
entries in K[X]. When one substitutes a matrix N for the indeterminate
X in XIn âˆ’M, one obtains a matrix of Mn(A), where A is the subring
of Mn(K) spanned by In and N (one denotes it by K(N)). The ring A is
commutative (but is not an integral domain in general), since it is the set
of the q(N) for q âˆˆK[X]. Therefore,
PM(N) =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
N âˆ’m11In
...
âˆ’mijIn
...
N âˆ’mnnIn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
The Cayleyâ€“Hamilton theorem expresses that the determinant (which is
an element of Mn(K), rather than of K) of this matrix is zero.
Proof
Let R âˆˆMn(K(X)) be the matrix XIn âˆ’M, and let S be the adjoint of
R. Each sij is a polynomial of degree less than or equal to n âˆ’1, because
the products arising in the calculation of the cofactors involve n âˆ’1 linear
or constant terms. Thus we may write
S = S0Xnâˆ’1 + Â· Â· Â· + Snâˆ’1,
where Sj âˆˆMn(K). Let us now write RS = (det R)In = PM(X)In:
(XIn âˆ’M)(S0Xnâˆ’1 + Â· Â· Â· + Snâˆ’1) = (Xn + a1Xnâˆ’1 + Â· Â· Â· + an)In.
Identifying the powers of X, we obtain
S0
=
In,
S1 âˆ’MS0
=
a1In,
...
Sj âˆ’MSjâˆ’1
=
ajIn,
...
Snâˆ’1 âˆ’MSnâˆ’2
=
anâˆ’1In,
âˆ’MSnâˆ’1
=
anIn.

2.5. The Characteristic Polynomial
27
Let us multiply these rows by the powers of M, beginning with M n and
ending with M 0 = In. Summing the obtained equalities, we obtain the
expected formula.
For example, every 2 Ã— 2 matrix satisï¬es the identity
M 2 âˆ’(Tr M)M + (det M)I2 = 0.
2.5.1
The Minimal Polynomial
For a square matrix M âˆˆMn(K), let us denote by JM the set of polyno-
mials Q âˆˆK[X] such that Q(M) = 0. It is clearly an ideal of K[X]. Since
K[X] is Euclidean, hence principal (see Sections 6.1.1 and 6.1.2), there ex-
ists a polynomial QM such that JM = K[X]QM. In other words, Q(M) = 0
and Q âˆˆK[X] imply QM|Q. Theorem 2.5.1 shows that the ideal JM does
not reduce to {0}, because it contains the characteristic polynomial. Hence,
QM Ì¸= 0 and one may choose it monic. This choice determines QM in a
unique way, and one calls it the minimal polynomial of M. It divides the
characteristic polynomial.
Contrary to the case of the characteristic polynomial, it is not immedi-
ate that the minimal polynomial is independent of the ï¬eld in which one
considers JM (note that we consider only ï¬elds that contain the entries of
M). We shall see in Section 6.3.2 that if L is a ï¬eld containing K, then the
minimal polynomials of M in K[X] and L[X] are the same. This explains
the terminology.
Two similar matrices obviously have the same minimal polynomial, since
Q(P âˆ’1MP) = P âˆ’1Q(M)P.
If Î» is an eigenvalue of M, associated to an eigenvector X, and if q âˆˆ
K[X], then q(Î»)X = q(M)X. Applied to the minimal polynomial, this
equality shows that the minimal polynomial is divisible by X âˆ’Î». Hence,
if PM splits over Â¯K in the form
r

j=1
(X âˆ’Î»j)nj,
the Î»j all being distinct, then the minimal polynomial can be written as
r

j=1
(X âˆ’Î»j)mj,
with 1 â‰¤mj â‰¤nj. In particular, if every eigenvalue of M is simple, the
minimal polynomial and the characteristic polynomial are equal.
An eigenvalue is called semi-simple if it is a simple root of the minimal
polynomial.

28
2. Square Matrices
2.6
Diagonalization
If Î» âˆˆK is an eigenvalue of M, one calls the linear subspace EK(Î») =
ker(M âˆ’Î»In) in Kn the eigenspace associated to Î». It is formed of eigen-
vectors associated to Î» on the one hand, and of the zero vector on the other
hand. Its dimension is nonzero. If L is a ï¬eld containing K (an â€œextensionâ€
of K), then dimK EK(Î») = dimL EL(Î»). This equality is not obvious. It
follows from the third canonical form with Jordan blocks, which we shall
see in Section 6.3.3.
If Î»1, . . . , Î»r are distinct eigenvalues, then the eigenspaces are in direct
sum. That is,
(x1 âˆˆEK(Î»1), . . . , xr âˆˆEK(Î»r), x1+Â· Â· Â·+xr = 0) =â‡’(x1 = Â· Â· Â· = xr = 0).
As a matter of fact, if there existed a relation x1 + Â· Â· Â· + xs = 0 where
x1, . . . , xs did not vanish simultaneously (we say that it has length s), one
could choose such a relation of minimal length r. One then would have
r â‰¥2. Multiplying this relation by M âˆ’Î»rIn, one would obtain
(Î»1 âˆ’Î»r)x1 + Â· Â· Â· + (Î»râˆ’1 âˆ’Î»r)xrâˆ’1 = 0,
which is a nontrivial relation of length r âˆ’1 for the vectors (Î»j âˆ’Î»r)xj âˆˆ
EK(Î»j). This contradicts the minimality of r.
If all the eigenvalues of M are in K and if the algebraic and geometric
multiplicities coincide for each eigenvalue of M, the sum of the dimensions
of the eigenspaces equals n. Since these linear subspaces are in direct sum,
one deduces that
Kn = E(Î»1) âŠ•Â· Â· Â· âŠ•E(Î»r).
Thus one may choose a basis of Kn formed of eigenvectors. If P is the
change-of-basis matrix from the canonical basis to the new one, then
M â€² = P âˆ’1MP is diagonal, and its diagonal terms are the eigenvalues,
repeted with their multiplicities. One says that M is diagonalizable in K.
A particular case is that in which the eigenvalues of M are in K and are
simple.
Conversely, if M is similar, in Mn(K), to a diagonal matrix M â€² =
P âˆ’1MP, then P is a change-of-basis matrix from the canonical basis to
an eigenbasis (that is, a basis composed of eigenvectors) of M. Hence, M
is diagonalizable if and only if the algebraic and geometric multiplicities of
each eigenvalue coincide.
Two obstacles could prevent M from being diagonalizable in K. The
ï¬rst one is that an eigenvalue of M does not belong to K. One can always
overcome this diï¬ƒculty by moving towards Mn(K). The second one is more
serious: In K, the geometric multiplicity of an eigenvalue can be strictly
less than its algebraic multiplicity. For instance, a triangular matrix whose
diagonal vanishes has only one eigenvalue, zero, of algebraic multiplicity
n. Such a matrix is nilpotent. However it is diagonalizable only if it is 0n,

2.7. Trigonalization
29
since M = PM â€²P âˆ’1 and M â€² = 0 imply M = 0. Hence,

0
1
0
0

is not diagonalizable.
2.7
Trigonalization
Let us begin with an application of the Cayleyâ€“Hamilton theorem.
Proposition 2.7.1 Let M âˆˆMn(K) and let PM be its characteristic poly-
nomial. If PM = QR with coprime factors Q, R âˆˆK[X], then Kn = E âŠ•F,
where E, F are the ranges of Q(M) and R(M), respectively. Moreover, one
has E = ker R(M), F = ker Q(M).
More generally, if PM = R1 Â· Â· Â· Rs, where the Rs are coprime, one has
Kn = E1 âŠ•Â· Â· Â· âŠ•Es with Ej = ker Rj(M).
Proof
It is suï¬ƒcient to prove the ï¬rst assertion. From BÂ´ezoutâ€™s theorem, there
exists R1, Q1 âˆˆK[X] such that RR1 + QQ1 = 1. Hence, every x âˆˆKn can
be written as a sum y + z with y = Q(M)(Q1(M)x) âˆˆE, and similarly
z = R(M)(R1(M)x) âˆˆF. Hence Kn = E + F.
Furthermore, for every y âˆˆE, the Cayleyâ€“Hamilton theorem says that
R(M)y = 0. Likewise, z âˆˆF implies Q(M)z = 0. If x âˆˆE âˆ©F, one has
thus R(M)x = Q(M)x = 0. Again using BÂ´ezoutâ€™s theorem, one obtains
x = 0. This proves Kn = E âŠ•F.
Finally, E âŠ‚ker R(M). Since these two vector spaces have the same
dimension (namely n âˆ’dim F), they are equal.
If K is algebraically closed, we can split PM in the form
PM(X) =

Î»âˆˆSp(M)
(X âˆ’Î»)nÎ».
From Proposition 2.7.1 one has Kn = âŠ•Î»EÎ», where EÎ» = ker(M âˆ’Î»I)nÎ»
is called a generalized eigenspace. Choosing a basis in each EÎ», we obtain a
new basis B of Kn. If P is the matrix of the linear transformation from the
canonical basis to B, the matrix PMP âˆ’1 is block-diagonal, because each
EÎ» is stable under the action of M:
PMP âˆ’1 = diag(. . . , MÎ», . . . ).
The matrix MÎ» is that of the restriction of M to EÎ». Since EÎ» = ker(M âˆ’
Î»I)nÎ», one has (MÎ» âˆ’Î»I)nÎ» = 0, so that Î» is the unique eigenvalue of MÎ».

30
2. Square Matrices
Let us deï¬ne NÎ» = MÎ» âˆ’Î»InÎ», which is nilpotent. Let us also write
Dâ€²
=
diag(. . . , Î»InÎ», . . . ),
N â€²
=
diag(. . . , NÎ», . . . ),
and then D = P âˆ’1Dâ€²P, N = P âˆ’1N â€²P. The matrices Dâ€², N â€² are respec-
tively diagonal and nilpotent. Moreover, they commute with each other:
Dâ€²N â€² = N â€²Dâ€². One deduces the following result.
Proposition 2.7.2 If K is algebraically closed, every matrix M âˆˆMn(K)
decomposes as a sum M = D+N, where D is diagonalizable, N is nilpotent,
DN = ND, and Sp(D) = Sp(M).
Let us continue this analysis.
Lemma 2.7.1 Every nilpotent matrix is similar to a strictly upper
triangular matrix (and also to a strictly lower triangular one).
Proof
Let us consider the nondecreasing sequence of linear subspaces Ek =
ker N k. Since E0 = {0} and Er = Kn for a suitable r, one can ï¬nd a basis
{x1, . . . , xn} of Kn such that {x1, . . . , xj} is a basis of Ek if j = dim Ek
(use the theorem that any linearly independent set can be enlarged to
a basis). Since N(Ek+1) = Ek, Nxj âˆˆEk. If P is the change-of-basis
matrix from this basis to the canonical one, then PNP âˆ’1 is strictly upper
triangular.
Let us return to the decomposition PMP âˆ’1 = Dâ€² + N â€² above. Each NÎ»
can be written, from the lemma, in the form Râˆ’1
Î» TÎ»RÎ», where TÎ» is strictly
upper triangular. Then RÎ»(DÎ» + NÎ»)Râˆ’1
Î»
= DÎ» + TÎ» is triangular. Let us
set
R = diag(. . . , RÎ», . . . ).
Then (RP)M(RP)âˆ’1 is block-diagonal, with the diagonal blocks upper
triangular, and hence this matrix is itself upper triangular.
Theorem 2.7.1 If K is algebraically closed, then every square matrix is
similar to a triangular matrix (one says that it is trigonalizable).
More generally, if the characteristic polynomial of M âˆˆMn(K) splits as
the product of linear factors, then M is trigonalizable.
A direct proof of this theorem that does not use the three previous
statements is possible. Its strategy is used in the proof of Theorem 3.1.3
2.8
Irreducibility
A square matrix A is said reducible if there exists a nontrivial partition
{1, . . . , n} = I âˆªJ such that (i, j) âˆˆI Ã— J implies aij = 0. It is irreducible

2.9. Exercises
31
otherwise. Saying that a matrix is reducible is equivalent to saying that
there exists a permutation matrix P such that PAP âˆ’1 is of block-triangular
form

B
C
0p,nâˆ’p
D

,
with 1 â‰¤p â‰¤n âˆ’1. As a matter of fact, P is the matrix of the transforma-
tion from a basis Î³ to the canonical one, Î³ being obtained by ï¬rst writing
the vectors ej with j âˆˆJ, and then those with j âˆˆI. Working in the new
basis amounts to decomposing the linear system Ax = b into two subsys-
tems Dz = d and By = c âˆ’Cz, which are to be solved successively. The
spectrum of A is the union of those of B and D, so that many interesting
questions concerning square matrices reduce to questions about irreducible
matrices.
We shall see in the exercises a characterization of irreducible matrices in
terms of graphs. Here is a useful consequence of irreducibility.
Proposition 2.8.1 Let M âˆˆMn(K) be an irreducible matrix such that
i â‰¥j + 2 implies mij = 0. Then the eigenvalues of M are geometrically
simple.
Proof
The hypothesis implies that all entries mi+1,i are nonzero. If Î» is an eigen-
value, let us consider the matrix N âˆˆMnâˆ’1( Â¯K), obtained from M âˆ’Î»In
by deleting the ï¬rst row and the last column. It is a triangular matrix,
whose diagonal terms are nonzero. It is thus invertible, which implies
rk(M âˆ’Î»In) = n âˆ’1. Hence ker(M âˆ’Î»In) is of dimension one.
2.9
Exercises
1. Verify that the product of two triangular matrices of the same type
(upper or lower) is triangular, of the same type.
2. Prove in full detail that the determinant of a triangular matrix (re-
spectively a block-triangular one) equals the product of its diagonal
terms (respectively the product of the determinants of its diagonal
blocks).
3. Find matrices M, N âˆˆM2(K) such that MN = 02 and NM Ì¸= 02.
Such an example shows that MN and NM are not necessarily similar,
though they would be in the case where M or N is invertible.
4. Characterize the square matrices that are simultaneously orthogonal
and triangular.

32
2. Square Matrices
5. One calls any square matrix M satisfying M 2 = M a projection
matrix, or projector.
(a) Let P âˆˆMn(K) be a projector, and let E = ker P, F = ker(Inâˆ’
P). Show that Kn = E âŠ•F.
(b) Let P, Q be two projectors. Show that (P âˆ’Q)2 commute with
P and with Q. Also, prove the identity
(P âˆ’Q)2 + (In âˆ’P âˆ’Q)2 = In.
6. Let M be a square matrix over a ï¬eld K, which we write blockwise
as
M =

A
B
C
D

.
The formula det M = det(ADâˆ’BC) is meaningless in general, except
when A, B, C, D have the same size. In that case the formula is false,
with the exception of scalar blocks. Compare with Schurâ€™s formula
(Proposition 8.1.2).
7. If A, B, C, D âˆˆMm(K) and if AC = CA, show that the determinant
of
M =

A
B
C
D

equals det(AD âˆ’CB). Begin with the case where A is invertible, by
computing the product

Im
0m
âˆ’C
A

M.
Then apply this intermediate result to the matrix Aâˆ’zIn, with z âˆˆÂ¯K
a suitable scalar.
Compare with the previous exercise.
8. Verify that the inverse of a triangular matrix, whenever it exists, is
triangular of the same type.
9. Show that the eigenvalues of a triangular matrix are its diagonal
entries. What are their algebraic multiplicities?
10. Let A âˆˆMn(K) be given. One says that a list (a1Ïƒ(1), . . . , anÏƒ(n))
is a diagonal of A if Ïƒ is a permutation (in that case, the diagonal
given by the identity is the main diagonal). Show the equivalence of
the following properties.
â€¢ Every diagonal of A contains a zero element.
â€¢ There exists a null matrix extracted from A of size k Ã— l with
k + l > n.

2.9. Exercises
33
11. Compute the number of elements in the group GL2(ZZ/2ZZ). Show
that it is not commutative. Show that it is isomorphic to the
symmetric group Sm, for a suitable integer m.
12. If (a0, . . . , anâˆ’1) âˆˆCCn is given, one deï¬nes the circulant matrix
circ(a0, . . . , anâˆ’1) âˆˆMn(CC) by
circ(a0, . . . , anâˆ’1) :=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a0
a1
. . .
anâˆ’1
anâˆ’1
a0
...
...
...
...
...
a1
a1
. . .
anâˆ’1
a0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
We denote by Cn the set of circulant matrices. Obviously, the ma-
trix circ(1, 0, 0, . . . , 0) is the identity. The matrix circ(0, 1, 0, . . . , 0)
is denoted by Ï€.
(a) Show that Cn is a subalgebra of Mn(CC), equal to CC[Ï€]. Deduce
that it is isomorphic to the quotient ring CC[X]/(Xn âˆ’1).
(b) Let C be a circulant matrix. Show that Câˆ—, as well as P(C), is
circulant for every polynomial P. If C is nonsingular, show that
Câˆ’1 is circulant.
(c) Show that the elements of Cn are diagonalizable in a common
eigenbasis.
(d) Replace CC by any ï¬eld K. If K contains a primitive nth root Ï‰
of unity (that is, Ï‰n = 1, and Ï‰m = 1 implies m âˆˆnZZ), show
that the elements of Cn are diagonalizable.
Note: A thorough presentation of circulant matrices and
applications is given in Davisâ€™s book [12].
(e) One assumes that the characteristic of K divides n. Show that
Cn contains matrices that are not diagonalizable.
13. Show that the Pfaï¬ƒan is linear with respect to any row or column
of an alternate matrix. Deduce that the Pfaï¬ƒan is an irreducible
polynomial in ZZ[xij].
14. (Schurâ€™s Lemma).
Let k be an algebraically closed ï¬eld and S a subset of Mn(k). As-
sume that the only linear subspaces of kn that are stable under every
element of S are {0} and kn itself. Let A âˆˆMn(k) be a matrix that
commutes with every element of S. Show that there exists c âˆˆk such
that A = cIn.
15.
(a) Show that A âˆˆMn(K) is irreducible if and only if for every pair
(j, k) with 1 â‰¤j, k â‰¤n, there exists a ï¬nite sequence of indices
j = l1, . . . , lr = k such that alp,lp+1 Ì¸= 0.
(b) Show that a tridiagonal matrix A âˆˆMn(K), for which none of
the aj,j+1â€™s and aj+1,jâ€™s vanish, is irreducible.

34
2. Square Matrices
16. Let A âˆˆMn(k) (k = IR or CC) be given, with minimal polynomial q.
If x âˆˆkn, the set
Ix := {p âˆˆk[X] | p(A)x = 0}
is an ideal of k[X], which is therefore principal.
(a) Show that Ix Ì¸= (0) and that its monic generator, denoted by
px, divides q.
(b) One writes rj instead of px when x = ej. Show that q is the
least common multiple of r1, . . . , rn.
(c) If p âˆˆk[X], show that the set
Vp := {x âˆˆkn | px âˆˆ(p)}
(the vectors x such that p divides px) is open.
(d) Let x âˆˆkn be an element for which px is of maximal degree.
Show that px = q. Note: In fact, the existence of an element x
such that px equals the minimal polynomial holds true for every
ï¬eld k.
17. Let k be a ï¬eld and A âˆˆMnÃ—m(k), B âˆˆMmÃ—n(k) be given.
(a) Let us deï¬ne
M =
 XIn
A
B
XIm

.
Show that Xm det M = Xn det(X2Im âˆ’BA) (search for a lower
triangular matrix M â€² such that M â€²M is upper triangular).
(b) Find an analogous relation between det(X2In âˆ’AB) and det M.
Deduce that XnPBA(X) = XmPAB(X).
(c) What do you deduce about the eigenvalues of A and of B?
18. Let k be a ï¬eld and Î¸ : Mn(k) â†’k a linear form satisfying Î¸(AB) =
Î¸(BA) for every A, B âˆˆMn(k).
(a) Show that there exists Î± âˆˆk such that for all X, Y âˆˆkn, one
has Î¸(XY T ) = Î± 
j xjyj.
(b) Deduce that Î¸ = Î± Tr.
19. Let An be the ring K[X1, . . . , Xn] of polynomials in n variables.
Consider the matrix M âˆˆMn(An) deï¬ned by
M =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
Â· Â· Â·
1
X1
Â· Â· Â·
Xn
X2
1
Â· Â· Â·
X2
n
...
...
Xnâˆ’1
1
Â· Â· Â·
Xnâˆ’1
n
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Let us denote by âˆ†(X1, . . . , Xn) the determinant of M.

2.9. Exercises
35
(a) Show that for every i Ì¸= j, the polynomial Xj âˆ’Xi divides âˆ†.
(b) Deduce that
âˆ†= a

i<j
(Xj âˆ’Xi),
where a âˆˆK.
(c) Determine the value of a by considering the monomial
n

j=1
Xj
j .
(d) Redo this analysis for the matrix
ï£«
ï£¬
ï£­
Xp1
1
Â· Â· Â·
Xp1
n
...
...
Xpn
1
Â· Â· Â·
Xpn
n
ï£¶
ï£·
ï£¸,
where p1, . . . , pn are nonnegative integers.
20. Deduce from the previous exercise that the determinant of the
Vandermonde matrix
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
Â· Â· Â·
1
a1
Â· Â· Â·
an
a2
1
Â· Â· Â·
a2
n
...
...
anâˆ’1
1
Â· Â· Â·
anâˆ’1
n
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
a1, . . . , an âˆˆK,
is zero if and only if at least two of the ajâ€™s coincide.
21. A matrix A âˆˆMn(IR) is called a totally positive matrix when all
minors
A

i1
i2
Â· Â· Â·
ip
j1
j2
Â· Â· Â·
jp

with 1 â‰¤p â‰¤n, 1 â‰¤i1 < Â· Â· Â· < ip â‰¤n and 1 â‰¤j1 < Â· Â· Â· < jp â‰¤n are
positive.
(a) Prove that the product of totally positive matrices is totally
positive.
(b) Prove that a totally positive matrix admits an LU factorization
(see Chapter 8), and that every â€œnontrivialâ€ minor of L and U
is positive. Here, â€œnontrivialâ€ means
L

i1
i2
Â· Â· Â·
ip
j1
j2
Â· Â· Â·
jp

with 1 â‰¤p â‰¤n, 1 â‰¤i1 < Â· Â· Â· < ip â‰¤n, 1 â‰¤j1 < Â· Â· Â· < jp â‰¤l,
and is â‰¥js for every s. For U, read is â‰¤js instead. Note: One
says that L and U are triangular totally positive.

36
2. Square Matrices
(c) Show that a Vandermonde matrix (see the previous exercise) is
totally positive whenever 0 < a1 < Â· Â· Â· < an.
22. Multiplying a Vandermonde matrix by its transpose, show that
det
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
n
s1
Â· Â· Â·
snâˆ’1
s1
s2
...
...
...
...
...
snâˆ’1
Â· Â· Â·
Â· Â· Â·
s2nâˆ’2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
=

i<j
(aj âˆ’ai)2,
where sq := aq
1 + Â· Â· Â· + aq
n.
23. The discriminant of a matrix A âˆˆMn(k) is the number
d(A) :=

i<j
(Î»j âˆ’Î»i)2,
where Î»1, . . . , Î»n are the eigenvalues of A, counted with multiplicity.
(a) Verify that the polynomial
âˆ†(X1, . . . , Xn) :=

i<j
(Xj âˆ’Xi)2
is symmetric. Therefore, there exists a unique polynomial Q âˆˆ
ZZ[Y1, . . . , Yn] such that
âˆ†= Q(Ïƒ1, . . . , Ïƒn),
where the Ïƒjâ€™s are the elementary symmetric polynomials
Ïƒ1 = X1 + Â· Â· Â· + Xn, . . . , Ïƒn = X1 Â· Â· Â· Xn.
(b) Deduce that there exits a polynomial D âˆˆZZ[xij] in the indeter-
minates xij, 1 â‰¤i, j â‰¤n, such that for every k and every square
matrix A,
d(A) = D(a11, a12, . . . , ann).
(c) Consider the restriction DS of the discriminant to symmetric
matrices, where xji is replaced by xij whenever i < j. Prove that
DS takes only nonnegative values on IRn(n+1)/2. Show, however,
that DS is not the square of a polynomial if n â‰¥2 (consider ï¬rst
the case n = 2).

2.9. Exercises
37
24. Let P âˆˆk[X] be a polynomial of degree n that splits completely in
k. Let BP be the companion matrix
BP :=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
Â· Â· Â·
Â· Â· Â·
0
âˆ’an
1
...
...
...
0
...
...
...
...
...
...
...
0
...
0
Â· Â· Â·
0
1
âˆ’a1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Find a matrix H âˆˆMn(k), whose transpose is of Vandermonde type,
such that
HBP = diag(Î»1, . . . , Î»n)H.
This furnishes a direct proof of the fact that when the roots of P are
simple, BP is diagonalizable.
25. (E. Formanek [14])
Let k be a ï¬eld of characteristic 0.
(a) Show that for every A, B, C âˆˆM2(k),

[A, B]2, C

= 0.
Hint: use the Cayleyâ€“Hamilton theorem.
(b) Show that for every M, N âˆˆM2(k),
MN + NM âˆ’Tr(M)N âˆ’Tr(N)M+
(Tr(M) Tr(N) âˆ’Tr(MN))I2
=
0.
One may begin with the case M = N and recognize a classical
theorem, then â€œbilinearizeâ€ the formula.
(c) If Ï€ âˆˆSr (Sr is the symmetric group over {1, . . . , r}), one
deï¬nes a map TÏ€ : M2(k)r â†’k in the following way. One de-
composes Ï€ as a product of disjoint cycles, including the cycles
of order one, which are the ï¬xed points of Ï€:
Ï€ = (a1, . . . , ak1)(b1, . . . , bk2) Â· Â· Â· .
One sets then
TÏ€(N1, . . . , Nr) = Tr(Na1 Â· Â· Â· Nak1) Tr(Nb1 Â· Â· Â· Nbk2 ) Â· Â· Â·
(note that the right-hand side depends neither on the order of
the cycles in the product nor on the choice of the ï¬rst index
inside each cycle, because of the formula Tr(AB) = Tr(BA)).
Show that for every N1, N2, N3 âˆˆM2(k), one has

Ï€âˆˆS3
Ïµ(Ï€)TÏ€(N1, N2, N3) = 0.

38
2. Square Matrices
(d) Generalize this result to Mn(k): for every N1, . . . , Nn+1 âˆˆ
Mn(k), one has

Ï€âˆˆSn+1
Ïµ(Ï€)TÏ€(N1, . . . , Nn+1) = 0.
Note: Polynomial identities satisï¬ed by every nÃ—n matrix have
been studied for decades. See [15] for a thorough account. One
should at least mention the theorem of Amitsur and Levitzki:
Theorem 2.9.1 Consider the free algebra ZZ[x1, . . . , xr] (where
x1, . . . , xr are noncommuting indeterminates) deï¬ne the stan-
dard polynomial Sr by
Sr(x1, . . . , xr) =

Ï€âˆˆSr
Ïµ(Ï€)xÏ€(1) Â· Â· Â· xÏ€(r).
Then, given a commutative ring A, one has the polynomial
identity
S2n(Q1, . . . , Q2n) = 0n,
âˆ€Q1, . . . , Q2n âˆˆMn(A).
26. Let k be a ï¬eld and let A âˆˆMn(k) be given. For every set J âŠ‚
{1, . . . , n}, denote by AJ the matrix extracted from A by keeping
only the indices i, j âˆˆJ. Hence, AJ âˆˆMp(k) for p = cardJ. Let
Î» âˆˆk.
(a) Assume that for every J whose cardinality is greater than or
equal to n âˆ’p, Î» is an eigenvalue of AJ. Show that Î» is an
eigenvalue of A, of algebraic multiplicity greater than or equal
to p+1 (express the derivatives of the characteristic polynomial).
(b) Conversely, let q be the geometric multiplicity of Î» as an eigen-
value of A. Show that if card J > n âˆ’q, then Î» is an eigenvalue
of AJ.
27. Let A âˆˆMn(k) and l âˆˆIN be given. Show that there exists a poly-
nomial ql âˆˆk[X], of degree at most n âˆ’1, such that Al = ql(A). If
A is invertible, show that there exists rl âˆˆk[X], of degree at most
n âˆ’1, such that Aâˆ’l = rl(A).
28. Let k be a ï¬eld and A, B âˆˆMn(k). Assume that Î» Ì¸= Âµ for every
Î» âˆˆSp A, Âµ âˆˆSp B. Show, using the Cayleyâ€“Hamilton theorem,
that the linear map M â†’AM âˆ’MB is an automorphism of Mn(k).
29. Let k be a ï¬eld and (Mjk)1â‰¤j,kâ‰¤n a set of matrices of Mn(k), at
least one of which is nonzero, such that MijMkl = Î´k
j Mil for all
1 â‰¤i, j, k, l â‰¤n.
(a) Show that none of the matrices Mjk vanishes.
(b) Verify that each Mii is a projector. Denote its range by Ei.

2.9. Exercises
39
(c) Show that E1, . . . , En are in direct sum. Deduce that each Ej
is a line.
(d) Show that there exist generators ej of each Ej such that Mjkel =
Î´l
kej.
(e) Deduce that every algebra automorphism of Mn(k) is interior:
For every Ïƒ âˆˆAutMn(k), there exists P âˆˆGLn(k) such that
Ïƒ(M) = P âˆ’1MP For every M âˆˆMn(k).

3
Matrices with Real or Complex Entries
Deï¬nitions
A square matrix M âˆˆMn(IR) is said to be normal if M and M T commute:
M T M = MM T. The real symmetric, skew-symmetric, and orthogonal
matrices are normal.
In considering matrices with complex entries, a useful operation is com-
plex conjugation z â†’Â¯z. One denotes by Â¯
M the matrix obtained from M
by conjugating the entries. We then deï¬ne the Hermitian adjoint matrix1
M âˆ—by
M âˆ—:= ( Â¯
M)T = M T.
One therefore has mâˆ—
ij = mji and det M âˆ—= det M. The map M â†’M âˆ—
is an anti-isomorphism, which means that it is antilinear (meaning that
(Î»M)âˆ—= Â¯Î»M âˆ—) and satisï¬es, moreover, the product formula
(MN)âˆ—= N âˆ—M âˆ—.
When a square matrix M âˆˆMn(CC) is invertible, then (M âˆ—)âˆ’1 = (M âˆ’1)âˆ—.
This matrix is sometimes denoted by M âˆ’âˆ—.
One says that a square matrix M âˆˆMn(CC) is Hermitian if M âˆ—= M and
skew-Hermitian if M âˆ—= âˆ’M. If M âˆˆMnÃ—m(CC), the matrices MM âˆ—and
1We warn the reader about the possible confusion between the adjoint and the Her-
mitian adjoint of a matrix. One may remark that the Hermitian adjoint is deï¬ned for
every rectangular matrix with complex entries, while the adjoint is deï¬ned for every
square matrix with entries in a commutative ring.

3. Matrices with Real or Complex Entries
41
M âˆ—M are Hermitian. We denote by Hn the set of Hermitian matrices in
Mn(CC). It is an IR-linear subspace of Mn(CC), though it is not a CC-linear
subspace, since iM is skew-Hermitian when M is Hermitian.
A square matrix M âˆˆMn(CC) is said to be unitary if M âˆ—M = In. Since
this means that M is invertible, with inverse M âˆ—, and since the left and
the right inverses are equal, an equivalent criterion is MM âˆ—= In. The
set of unitary matrices in Mn(CC) forms a multiplicative group, denoted
by Un. Unitary matrices satisfy | det M| = 1, since det M âˆ—M = | det M|2
for every matrix M. The set of unitary matrices whose determinant equals
1, denoted by SUn is obviously a normal subgroup of Un. Finally, M is
said to be normal if M and M âˆ—commute: MM âˆ—= M âˆ—M. The Hermitian,
skew-Hermitian, and unitary matrices are normal.
Observe that the real orthogonal (respectively symmetric, skew-sym-
metric) matrices are unitary (respectively Hermitian, skew-Hermitian).
Conversely, if M is real and either unitary, symmetric, or skew-symmetric,
then M is either orthogonal, Hermitian, or skew-Hermitian.
A sesquilinear form on a complex vector space is a map
(x, y) â†’âŸ¨x, yâŸ©,
linear in x and satisfying
âŸ¨y, xâŸ©= âŸ¨x, yâŸ©.
It is thus antilinear in y:
âŸ¨x, Î»yâŸ©= Â¯Î»âŸ¨x, yâŸ©.
When y = x, âŸ¨x, yâŸ©= âŸ¨x, xâŸ©is a real number. The map x â†’âŸ¨x, xâŸ©is called
a Hermitian form. The correspondence between sesquilinear and Hermitian
forms is one-to-one.
Given a matrix M âˆˆMn(CC), the form
(x, y) â†’

j,k
mjkxjyk,
deï¬ned on CCn Ã— CCn, is sesquilinear if and only if M is Hermitian. It fol-
lows that there is an isomorphism between the sets of Hermitian matrices,
Hermitian, and sesquilinear forms on CCn. As a matter of fact, a Hermitian
form can be written in the form
x â†’

j,k
mjkxj Â¯xk.
The kernel of a Hermitian or a sesquilinear form is the set of vectors
x âˆˆE such that âŸ¨x, yâŸ©= 0 for every y âˆˆE. It equals the set of vectors
y âˆˆE such that âŸ¨x, yâŸ©= 0 for every x âˆˆE. If E = CCn, it is also the kernel
of M T , where M is the (Hermitian) matrix associated to the Hermitian
form. One says that the Hermitian form is degenerate if its kernel does not

42
3. Matrices with Real or Complex Entries
reduce to {0}. When E = CCn, this amounts to det M = 0. One says that
the form is nondegenerate otherwise.
If both E and F are endowed with nondegenerate sequilinear forms âŸ¨Â·, Â·âŸ©E
and âŸ¨Â·, Â·âŸ©F , respectively, and if u âˆˆL(E, F), one deï¬nes uâˆ—by the formula
âŸ¨uâˆ—(x), yâŸ©E = âŸ¨x, u(y)âŸ©F ,
âˆ€x âˆˆF, y âˆˆE.
The map u â†’uâˆ—is an IR-isomorphism from L(E, F) onto L(F, E), and
one has (Î»u)âˆ—= Â¯Î»uâˆ—, (uâˆ—)âˆ—= u. When E = CCn and F = CCm are endowed
with the canonical sesquilinear forms x1y1 + Â· Â· Â· , the matrix associated
to uâˆ—is simply the Hermitian adjoint of the matrix associated to u. The
canonical Hermitian form over CCn is positive deï¬nite: âŸ¨x, xâŸ©> 0 if x Ì¸= 0. It
allows us to deï¬ne a norm by âˆ¥xâˆ¥=

âŸ¨x, xâŸ©. Identifying CCn with column
vectors, one also deï¬nes âˆ¥Xâˆ¥=
âˆš
Xâˆ—X if X âˆˆMnÃ—1(CC). This norm will
be denoted by âˆ¥Â· âˆ¥2 in Chapter 4. A matrix is unitary if and only if it is
associated with an isometry of CCn:
âˆ¥u(x)âˆ¥= âˆ¥xâˆ¥,
âˆ€x âˆˆCCn.
More generally, let M be a Hermitian matrix and âŸ¨Â·, Â·âŸ©the form that it
deï¬nes on CCn. One says that M is positive deï¬nite if âŸ¨x, xâŸ©> 0 for ev-
ery x Ì¸= 0. Again,

âŸ¨x, xâŸ©is a norm on CCn. We shall denote by HPDn
the set of the positive deï¬nite Hermitian matrices; it is an open cone in
Hn. Its closure consists of the Hermitian matrices M that deï¬ne a posi-
tive semideï¬nite Hermitian form over CCn (âŸ¨x, xâŸ©â‰¥0 for every x). They
are called positive semideï¬nite Hermitian matrices. One deï¬nes similarly,
among the real symmetric matrices, those that are positive deï¬nite, respec-
tively positive semideï¬nite. The positive deï¬nite real symmetric matrices
form an open cone in Symn(IR), denoted by SPDn.
The natural ordering on Hermitian forms induces an ordering on Hermi-
tian matrices. One writes H â‰¥0n when the Hermitian form associated to H
takes nonnegative values. More generally, one writes H â‰¥h if H âˆ’h â‰¥0n.
We likewise deï¬ne an ordering on real-valued symmetric matrices, referring
to the ordering on real-valued quadratic forms.2
If U is unitary, the matrix U âˆ—MU is similar to M. If M is Hermitian,
skew-Hermitian, normal, or unitary and if U is unitary, then U âˆ—MU is still
Hermitian, skew-Hermitian, normal, or unitary.
2We warn the reader that another, completely diï¬€erent, order still denoted by the
symbol â‰¥will be deï¬ned in Chapter 5. This one will concern real-valued matrices that
are neither symmetric nor even square. One expects that the context is never ambiguous.

3.1. Eigenvalues of Real- and Complex-Valued Matrices
43
3.1
Eigenvalues of Real- and Complex-Valued
Matrices
Since CC is algebraically closed, every complex-valued square matrix, and
every endomorphism of a CC-vector space of dimension n â‰¥1, possesses
eigenvalues. As a matter of fact, the characteristic polynomial has roots.
A real-valued square matrix may not have eigenvalues in IR, but it has at
least one in CC. If n is odd, M âˆˆMn(IR) has at least a real eigenvalue,
because PM is real of odd degree.
Proposition 3.1.1 The eigenvalues of Hermitian matrices, as well as
those of real symmetric matrices, are real.
Proof
Let M âˆˆMn(CC) be a Hermitian matrix and let Î» be one of its eigen-
values. Let us choose an eigenvector X: MX = Î»X. Taking the Hermitian
adjoint, we obtain Xâˆ—M = Â¯Î»X. Hence,
Î»Xâˆ—X = Xâˆ—(MX) = (Xâˆ—M)X = Â¯Î»Xâˆ—X,
or
(Î» âˆ’Â¯Î»)Xâˆ—X = 0.
However Xâˆ—X = 
j |xj|2 > 0. Therefore, we are left with Â¯Î»âˆ’Î» = 0. Hence
Î» is real.
We leave it to the reader to show, as an exercise, that the eigenvalues of
skew-Hermitian matrices are purely imaginary.
Proposition 3.1.2 The eigenvalues of the unitary matrices, as well as
those of real orthogonal matrices, are complex numbers of modulus one.
Proof
As before, if X is an eigenvector associated to Î», one has
|Î»|2âˆ¥Xâˆ¥2 = (Î»X)âˆ—(Î»X) = (MX)âˆ—MX = Xâˆ—M âˆ—MX = Xâˆ—X = âˆ¥Xâˆ¥2,
and therefore |Î»|2 = 1.
3.1.1
Continuity of Eigenvalues
One of the more delicate statements in the elementary theory of matrices
concerns the continuity of the eigenvalues. Though a proof might be pro-
vided througth explicit bounds, it is easier to use RouchÂ´eâ€™s theorem about
holomorphic functions. We begin with a statement concerning polynomials,
that is a bit less precise than RouchÂ´eâ€™s theorem.

44
3. Matrices with Real or Complex Entries
Theorem 3.1.1 Let n âˆˆIN and let P âˆˆCC[X] be a polynomial of degree
n,
P(X) = p0 + p1X + Â· Â· Â· + pnXn.
Let x be a root of P, with multiplicity Âµ, and let d be the distance from x to
the other roots of P. Let D be an open disk, D = D(x; Ï), with 0 < Ï < d.
Then there exists a number Ïµ > 0 such that if Q âˆˆCC[X] has degree n,
Q(X) = q0 + q1X + Â· Â· Â· + qnXn,
and if
max
j
|qj âˆ’pj| < Ïµ,
then D contains exactly Âµ roots of Q, counting multiplicities.
Let us apply this result to the characteristic polynomial of a given matrix.
Since the coeï¬ƒcients of the characteristic polynomial pM are polynomial
functions of the entries of M, the map M â†’pM is continuous from Mn(CC)
to the set of polynomials of degree n. From RouchÂ´eâ€™s theorem, we have the
following result.
Theorem 3.1.2 Let M âˆˆMn(CC), and let Î» be one of its eigenvalues, with
multiplicity Âµ, and let d be the distance from Î» to the other eigenvalues of
M. Let D be an open disk, D = D(Î»; Ï), with 0 < Ï < d. Let us ï¬x a norm
on Mn(CC).
There exists an Ïµ > 0 such that if A âˆˆMn(CC) and âˆ¥Aâˆ¥< Ïµ, the sum of
algebraic multiplicities of the eigenvalues of M + A in D equals Âµ.
Let us remark that this statement becomes false if one considers the
geometric multiplicities.
One often invokes this theorem by saying that the eigenvalues of a ma-
trix are continuous functions of its entries. Here is an interpretation. One
adapts the Hausdorï¬€distance between compact sets so as to take into ac-
count the multiplicity of the eigenvalues. If M, N âˆˆMn(CC), let us denote
by (Î»1, . . . , Î»n) and (Î¸1, . . . , Î¸n) their eigenvalues, repeated according to
their multiplicities. One then deï¬nes
d(Sp M, Sp N) :=
inf
ÏƒâˆˆSn
max
j
|Î»j âˆ’Î¸Ïƒ(j)|,
where Sn is the group of permutations of the indices {1, . . . , n}. This num-
ber is called the distance between the spectra of M and N. With this
notation, one may rewrite Theorem 3.1.2 in the following form.
Proposition 3.1.3 If M âˆˆMn(CC) and Î± > 0, there exists Ïµ > 0 such
that âˆ¥N âˆ’Mâˆ¥< Ïµ implies d(Sp M, Sp N) < Î±.
A useful consequence of Theorem 3.1.2 is the following.

3.2. Spectral Decomposition of Normal Matrices
45
Corollary 3.1.1 In Mn(k) (k = IR or CC) the set of diagonalizable
matrices is an open subset.
3.1.2
Trigonalization in an Orthonormal Basis
From now on we say that two matrices are unitarily similar if they are
similar through a unitary transformation. Two real matrices are unitarily
similar if they are similar through an orthogonal transformation.
If K = CC, one may sharpen Theorem 2.7.1:
Theorem 3.1.3 (Schur) If M âˆˆMn(CC), there exists a unitary matrix
U such that U âˆ—MU is upper triangular.
One also says that every matrix with complex entries is unitarily
trigonalizable.
Proof
We proceed by induction on the size n of the matrices. The statement is
trivial if n = 1. Let us assume that it is true in Mnâˆ’1(CC), with n â‰¥2. Let
M âˆˆMn(CC) be a matrix. Since CC is algebraically closed, M has at least
one eigenvalue Î». Let X be an eigenvector associated to Î». By dividing X
by âˆ¥Xâˆ¥, one can assume that X is a unit vector. One can then ï¬nd an
orthonormal basis {X1, X2, . . . , Xn} of CCn whose ï¬rst element is X. Let
us consider the matrix V := (X1 = X, X2, . . . , Xn), which is unitary, and
let us form the matrix M â€² := V âˆ—MV . Since
V M â€²e1 = MV e1 = MX = Î»X = Î»V e1,
one obtains M â€²e1 = Î»e1. In other words, M â€² has the block-triangular form:
M â€² =

Î»
Â· Â· Â·
0nâˆ’1
N

,
where N âˆˆMnâˆ’1(CC). Applying the induction hypothesis, there exists
W âˆˆUnâˆ’1 such that W âˆ—NW is upper triangular. Let us denote by Ë†W
the (block-diagonal) matrix diag(1, W) âˆˆUn. Then Ë†
W âˆ—M â€² Ë†
W is upper
triangular. Hence, U = V Ë†W satisï¬es the conditions of the theorem.
3.2
Spectral Decomposition of Normal Matrices
We recall that a matrix M is normal if M âˆ—commutes with M. For real
matrices, this amounts to saying that M T commutes with M. Since it is
equivalent for a Hermitian matrix H to be zero or to satisfy xâˆ—Hx = 0 for
every vector x, we see that M is normal if and only if âˆ¥Axâˆ¥2 = âˆ¥Aâˆ—xâˆ¥2
for every vector, where âˆ¥xâˆ¥2 denotes the standard Hermitian (Euclidean)
norm (take H = AAâˆ—âˆ’Aâˆ—A).

46
3. Matrices with Real or Complex Entries
Theorem 3.2.1 If K = CC, the normal matrices are diagonalizable, using
unitary matrices:
(M âˆ—M = MM âˆ—) =â‡’(âˆƒU âˆˆUn;
M = U âˆ’1 diag(d1, . . . , dn)U).
Again, one says that normal matrices are unitarily diagonalizable. This
theorem contains the following properties.
Corollary 3.2.1 Unitary, Hermitian, and skew-Hermitian matrices are
unitarily diagonalizable.
Observe that among normal matrices one distinguishes each of the above
families by the nature of their eigenvalues. Those of unitary matrices have
modulus one, while those of Hermitian matrices are real. Finally, those of
skew-Hermitian matrices are purely imaginary.
Proof
We proceed by induction on the size n of the matrix M. If n = 0, there
is nothing to prove. Otherwise, if n â‰¥1, there exists an eigenpair (Î», x):
Mx = Î»x,
âˆ¥xâˆ¥2 = 1.
Since M is normal, Mâˆ’Î»In is, too. From above, we see that âˆ¥(M âˆ—âˆ’Â¯Î»)xâˆ¥2 =
âˆ¥(M âˆ’Î»)xâˆ¥2 = 0, and hence M âˆ—x = Â¯Î»x. Let V be a unitary matrix such
that V e1 = x. Then the matrix M1 := V âˆ—MV is normal and satisï¬es
M1e1 = Î»e1. Hence it satisï¬es M âˆ—
1 e1 = Â¯Î»e1. This amounts to saying that
M1 is block-diagonal, of the form M1 = diag(Î», M â€²). Obviously, M â€² inherits
the normality of M1. From the induction hypothesis, M â€², and therefore M1
and M, are unitarily diagonalizable.
One observes that the same matrix U diagonalizes M âˆ—, because M =
U âˆ’1DU implies M âˆ—= U âˆ—Dâˆ—U âˆ’1âˆ—= U âˆ’1Dâˆ—U, since U is unitary.
Let us consider the case of a positive semideï¬nite Hermitian matrix H. If
HX = Î»X, then 0 â‰¤Xâˆ—HX = Î»âˆ¥Xâˆ¥2. The eigenvalues are thus nonnega-
tive. Let Î»1, . . . , Î»p be the nonzero eigenvalues of H. Then H is unitarily
similar to
D := diag(Î»1, . . . , Î»p, 0, . . . , 0).
From this, we conclude that rk H = p. Let U âˆˆUn be such that H =
UDU âˆ—. Deï¬ning the vectors XÎ± = âˆšÎ»Î±UÎ±, where the UÎ± are the columns
of U, we obtain the following statement.
Proposition 3.2.1 Let H âˆˆMn(CC) be a positive semideï¬nite Hermitian
matrix. Let p be its rank. Then H has p real, positive eigenvalues, while the
eigenvalue Î» = 0 has multiplicity n âˆ’p. There exist p column vectors XÎ±,
pairwise orthogonal, such that
H = X1Xâˆ—
1 + Â· Â· Â· + XpXâˆ—
p.
Finally, H is positive deï¬nite if and only if p = n (in which case, Î» = 0 is
not an eigenvalue).

3.3. Normal and Symmetric Real-Valued Matrices
47
3.3
Normal and Symmetric Real-Valued Matrices
The situation is a bit more involved if M, a normal matrix, has real en-
tries. Of course, one can consider M as a matrix with complex entries and
diagonalize it in an orthonormal basis, but we quit in general the ï¬eld of
real numbers when doing so. We prefer to allow bases consisting of only
real vectors. Since some of the eigenvalues might be nonreal, one cannot in
general diagonalize M. The statement is thus the following.
Theorem 3.3.1 Let M âˆˆMn(IR) be a normal matrix. There exists an or-
thogonal matrix O such that OMOâˆ’1 be block-diagonal, the diagonal blocks
being 1 Ã— 1 (those corresponding to the real eigenvalues of M) or 2 Ã— 2, the
latter being matrices of direct similitude:3

a
b
âˆ’b
a

(b Ì¸= 0).
Similarly, OM T Oâˆ’1 is block-diagonal, the diagonal blocks being eigen-
values or matrices of direct similitude.
Proof
One again proceeds by induction on n. When n â‰¥1, the proof is the same
as in the previous section whenever M has at least one real eigenvalue.
If this is not the case, then n is even. Let us ï¬rst consider the case n = 2.
Then
M =
 a
b
c
d

.
Since M is normal, we have b2 = c2 and (a âˆ’d)(b âˆ’c) = 0. However,
b Ì¸= c, since otherwise M would be symmetric, hence would have two real
eigenvalues. Hence b = âˆ’c and a = d.
Now let us consider the general case, with n â‰¥4. We know that M has
an eigenpair (Î», z), where Î» is not real. If the real and imaginary parts of
z were colinear, M would have a real eigenvector, hence a real eigenvalue,
a contradiction. In other words, the real and imaginary parts of z span a
plane P in IRn. As before, Mz = Î»z implies M T z = Â¯Î»z. Hence we have
MP âŠ‚P and M T P âŠ‚P. Now let V be an orthogonal matrix that maps
the plane P0 := IRe1 âŠ•IRe2 onto P. Then the matrix M1 := V T MV is
normal and satisï¬es
M1P0 âŠ‚P0,
M T
1 P0 âŠ‚P0.
This means that M1 is block-diagonal. Of course, each diagonal block (of
sizes 2Ã—2 and (nâˆ’2)Ã—(nâˆ’2)) inherits the normality of M1. Applying the
induction hypothesis, we know that these blocks are unitarily similar to a
3A similitude is an endomorphism of a Euclidean space that preserves angles. It splits
as aR, where R is orthogonal and a is a scalar. It is direct if its determinant is positive.

48
3. Matrices with Real or Complex Entries
block-diagonal matrix whose diagonal blocks are direct similitudes. Hence
M1 and M are unitarily similar to such a matrix.
Corollary 3.3.1 Real symmetric matrices are diagonalizable over IR,
through orthogonal conjugation. In other words, given M âˆˆSymn(IR),
there exists an O âˆˆOn(IR) such that OMOâˆ’1 is diagonal.
In fact, since the eigenvalues of M are real, OMOâˆ’1 has only 1 Ã— 1 blocks.
We say that real symmetric matrices are orthogonally diagonalizable.
The interpretation of this statement in terms of quadratic forms is the
following. For every quadratic form Q on IRn, there exists an orthonor-
mal basis {e1, . . . , en} in which this form can be written with at most n
squares:4
Q(x) =
n

i=1
aix2
i .
Replacing the basis vector ej by |aj|1/2ej, one sees that there also exists
an orthogonal basis in which the quadratic form can be written
Q(x) =
r

i=1
x2
i âˆ’
s

j=1
x2
j+r,
with r+s â‰¤n. This quadratic form is nondegenerate if and only if r+s = n.
The pair (r, s) is unique and called the signature or the Sylvester index of
the quadratic form. In such a basis, the matrix associated to Q is
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
...
0
1
âˆ’1
...
âˆ’1
0
0
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
3.3.1
Rayleigh Quotients
Let M be a real n Ã— n symmetric matrix, and let Î»1 â‰¤Â· Â· Â· â‰¤Î»n be its
eigenvalues arranged in increasing order and counted with multiplicity. Let
4In solid mechanics, when Q is the matrix of inertia, the vectors of this basis are
along the inertia axes, and the aj, which then are positive, are the momenta of inertia.

3.3. Normal and Symmetric Real-Valued Matrices
49
us denote by B = {v1, . . . , vn} an orthonormal eigenbasis (Mvj = Î»jvj).
If x âˆˆIRn, let us denote by y1, . . . , yn the coordinates of x in the basis B.
Finally, let us denote by âˆ¥Â· âˆ¥2 the usual Euclidean norm on IRn. Then
xT Mx =

j
Î»jy2
j â‰¤Î»n

j
y2
j = Î»nâˆ¥xâˆ¥2
2.
Since vT
n Mvn = Î»nâˆ¥vnâˆ¥2
2, we deduce the value of the largest eigenvalue of
M:
Î»n = max
xÌ¸=0
xT Mx
âˆ¥xâˆ¥2
2
= max

xT Mx | âˆ¥xâˆ¥2
2 = 1

.
(3.1)
Similarly, the smallest eigenvalue of a real symmetric matrix is given by
Î»1 = min
xÌ¸=0
xT Mx
âˆ¥xâˆ¥2
2
= min{xT Mx | âˆ¥xâˆ¥2
2 = 1}.
(3.2)
For a Hermitian matrix, the formulas (3.1,3.2) remain valid when we replace
xT by xâˆ—.
We evaluate the other eigenvalues of M âˆˆSymn(IR) in the following
way. For every linear subspace F of IRn of dimension k, let us deï¬ne
R(F) =
max
xâˆˆF \{0}
xT Mx
âˆ¥xâˆ¥2
2
= max

xT Mx | x âˆˆF, âˆ¥xâˆ¥2
2 = 1

.
The intersection of F with the linear subspace spanned by {vk, . . . , vn} is
of dimension greater than or equal to one. There exists, therefore, a nonzero
vector x âˆˆF such that y1 = Â· Â· Â· = ykâˆ’1 = 0. One has then
xT Mx =
n

j=k
Î»jy2
j â‰¥Î»k

j
y2
j = Î»kâˆ¥xâˆ¥2
2.
Hence, R(F) â‰¥Î»k. Furthermore, if G is the space spanned by {v1, . . . , vk},
one has R(G) = Î»k. Thus, we have
Î»k = min{R(F) | dim F = k}.
Finally, we may state the following theorem.
Theorem 3.3.2 Let M be an n Ã— n real symmetric matrix and Î»1, . . . , Î»n
its eigenvalues arranged in increasing order, counted with multiplicity. Then
Î»k =
min
dim F =k
max
xâˆˆF \{0}
xT Mx
âˆ¥xâˆ¥2
2
.
If M is complex Hermitian, one has similarly
Î»k =
min
dim F =k
max
xâˆˆF \{0}
xâˆ—Mx
âˆ¥xâˆ¥2
2
.
This formula generalizes (3.1, 3.2).

50
3. Matrices with Real or Complex Entries
3.3.2
Applications
Theorem 3.3.3 Let H âˆˆHnâˆ’1, x âˆˆCCnâˆ’1, and a âˆˆIR be given. Let
Î»1 â‰¤Â· Â· Â· â‰¤Î»nâˆ’1 be the eigenvalues of H and Âµ1 â‰¤Â· Â· Â· â‰¤Âµn those of the
Hermitian matrix
Hâ€² =

H
x
xâˆ—
a

.
One has then Âµ1 â‰¤Î»1 â‰¤Â· Â· Â· â‰¤Âµj â‰¤Î»j â‰¤Âµj+1 â‰¤Â· Â· Â· .
Proof
By Theorem 3.3.2, the inequality Âµj â‰¤Î»j is obvious, because the inï¬mum
is taken over a smaller set.
Conversely, let Ï€ : x â†’(x1, . . . , xnâˆ’1)T be the projection from CCn on
CCnâˆ’1. If F is a linear subspace of CCn of dimension j + 1, its image under
Ï€ contains a linear subspace G of dimension j (it will often be exactly of
dimension j). By Theorem 3.3.2, applied to H, one therefore has
Râ€²(F) â‰¥R(G) â‰¥Î»j.
Taking the inï¬mum, we obtain Âµj+1 â‰¥Î»j.
The previous theorem is optimal, in the following sense.
Theorem 3.3.4 Let Î»1 â‰¤Â· Â· Â· â‰¤Î»nâˆ’1 and Âµ1 â‰¤Â· Â· Â· â‰¤Âµn be real numbers
satisfying Âµ1 â‰¤Î»1 â‰¤Â· Â· Â· â‰¤Âµj â‰¤Î»j â‰¤Âµj+1 â‰¤Â· Â· Â· . Then there exist a vector
x âˆˆIRn and a âˆˆIR such that the real symmetric matrix
H =
 Î›
x
xT
a

,
where Î› = diag(Î»1, . . . , Î»nâˆ’1), has the eigenvalues Âµj.
Proof
Let us compute the characteristic polynomial of H from Schurâ€™s
complement formula5 (see Proposition 8.1.2):
pn(X)
=

X âˆ’a âˆ’xT (XInâˆ’1 âˆ’Î›)âˆ’1x

det(XInâˆ’1 âˆ’Î›)
=
ï£«
ï£­X âˆ’a âˆ’

j
x2
j
X âˆ’Î»j
ï£¶
ï£¸
j
(X âˆ’Î»j).
Let us assume for the moment that all the inequalities Âµj â‰¤Î»j â‰¤Âµj+1
hold strictly. In particular, the Î»jâ€™s are distinct. Let us consider the partial
fraction decomposition of the rational function

l(X âˆ’Âµl)

j(X âˆ’Î»j) = X âˆ’a âˆ’

j
cj
X âˆ’Î»j
.
5One may equally (exercise) compute it by induction on n.

3.4. The Spectrum and the Diagonal of Hermitian Matrices
51
One thus obtains
a =

l
Âµl âˆ’

j
Î»j,
a formula that could also have been found by comparing the traces of Î›
and of H. The inequalities Î»jâˆ’1 < Âµj < Î»j ensure that each cj is positive,
because
cj = âˆ’

l(Î»j âˆ’Âµl)

kÌ¸=j(Î»j âˆ’Î»k).
Let us put, then, xj = âˆšcj (or âˆ’xj = âˆšcj). We obtain, as announced,
pn(X) =

l
(X âˆ’Âµl).
In the general case one may choose sequences Âµ(m)
l
and Î»(m)
j
that con-
verge to the Âµlâ€™s and the Î»jâ€™s as m â†’+âˆand that satisfy the inequalities
in the hypothesis strictly. The ï¬rst part of the proof (case with strict in-
equalities) provides matrices H(m). Since the spectral radius is a norm over
Symn(IR) (the spectral radius is deï¬ned in the next Chapter), the sequence
(H(m))mâˆˆIN is bounded. In other words, (a(m), x(m)) remains bounded. Let
us extract a subsequence that converges to a pair (a, x) âˆˆIR Ã— IRnâˆ’1. The
matrix H associated to (a, x) solves our problem, since the eigenvalues
depend continuously on the entries of the matrix.
Corollary 3.3.2 Let H âˆˆSymnâˆ’1(IR) with eigenvalues Î»1 â‰¤Â· Â· Â· â‰¤Î»nâˆ’1.
Let Âµ1, . . . , Âµn be real numbers satisfying Âµ1 â‰¤Î»1 â‰¤Â· Â· Â· â‰¤Âµj â‰¤Î»j â‰¤
Âµj+1 â‰¤Â· Â· Â· . Then there exist a vector x âˆˆIRn and a âˆˆIR such that the real
symmetric matrix
Hâ€² =
 H
x
xT
a

has the eigenvalues Âµj.
The proof consists in diagonalizing H through an orthogonal conjugation,
then applying the theorem, and ï¬nally performing the inverse conjugation.
3.4
The Spectrum and the Diagonal of Hermitian
Matrices
Let us begin with an order relation between ï¬nite sequences of real num-
bers. If a = (a1, . . . , an) is a sequence of n real numbers, and if 1 â‰¤l â‰¤n,

52
3. Matrices with Real or Complex Entries
we denote by sk(a) the number
min
ï£±
ï£²
ï£³

jâˆˆJ
aj | card J = k
ï£¼
ï£½
ï£¾.
Deï¬nition 3.4.1 Let a = (a1, . . . , an) and b = (b1, . . . , bn) be two se-
quences of n real numbers. One says that b majorizes a, and one writes
a â‰ºb, if
sk(a) â‰¤sk(b),
âˆ€1 â‰¤k â‰¤n,
sn(a) = sn(b).
The functions sk are symmetric:
sk(a) = sk(aÏƒ(1), . . . , aÏƒ(n))
for every permutation Ïƒ. One thus may always restrict attention to the
case of nondecreasing sequences a1 â‰¤Â· Â· Â· â‰¤an. One has then sk(a) =
a1 + Â· Â· Â· + ak. The relation a â‰ºb for nondecreasing sequences, can now be
written as
a1 + Â· Â· Â· + ak
â‰¤
b1 + Â· Â· Â· + bk,
k = 1, . . . , n âˆ’1,
a1 + Â· Â· Â· + an
=
b1 + Â· Â· Â· + bn.
The latter equality plays a crucial role in the analysis below. The relation
â‰ºis a partial ordering.
Proposition 3.4.1 Let x, y âˆˆIRn. Then x â‰ºy if and only if for every
real number t,
n

j=1
|xj âˆ’t| â‰¥
n

j=1
|yj âˆ’t|.
(3.3)
Proof
We may assume that x and y are nondecreasing. If the inequality (3.3)
holds, we write it ï¬rst for t outside the interval I containing the xjâ€™s and
the yjâ€™s. This gives sn(x) = sn(y). Then we write it for t = xk. Using
sn(x) = sn(y), we obtain

j
|xj âˆ’xk|
=
k

1
(xk âˆ’yj) +
n

k+1
(yj âˆ’xk) + 2(sk(y) âˆ’sk(x))
â‰¤

j
|yj âˆ’xk| + 2(sk(y) âˆ’sk(x)),
which with (3.3) gives sk(x) â‰¤sk(y).
Conversely, let us assume that x â‰ºy. Let us deï¬ne Ï†(t) := 
j |xj âˆ’
t| âˆ’
j |yj âˆ’t|. This is a piecewise linear function, zero outside I. Its
derivative, integer-valued, is piecewise constant. It increases at the points
xjâ€™s and decreases at the points yjâ€™s only. If min{Ï†(t); t âˆˆIR} < 0, this
minimum will thus be reached at some xk, with Ï†â€²(xk âˆ’0) â‰¤0 â‰¤Ï†â€²(xk +0),

3.4. The Spectrum and the Diagonal of Hermitian Matrices
53
from which one obtains ykâˆ’1 â‰¤xk â‰¤yk+1. Therefore, there are two cases,
depending on the position of yx with respect to xk. For example, if yk â‰¤xk,
we compute

j
|xj âˆ’xk| =
n

k+1
(xj âˆ’xk) +
k

1
(xk âˆ’xj).
From the assumption, it follows that

j
|xj âˆ’xk| â‰¥
n

k+1
(yj âˆ’xk) +
k

1
(xk âˆ’yj) =

jÌ¸=k
|yj âˆ’xk|,
which means that Ï†(xk) â‰¥0, which contradicts the hypothesis. Hence, Ï† is
a nonnegative function.
Our ï¬rst statement expresses an order between the diagonal and the
spectrum of a Hermitian matrix.
Theorem 3.4.1 (Schur) Let H be a Hermitian matrix with diagonal a
and spectrum Î». Then a â‰»Î».
Proof
Let n be the size of H. We argue by induction on n. We may assume that
an is the largest component of a. Since sn(Î») = Tr A, one has sn(Î») = sn(a).
In particular, the theorem holds true for order 1. Let us assume that it holds
for order n âˆ’1. Let A be the matrix obtained from H by deleting the nth
row and the nth column. Let Âµ = (Âµ1, . . . , Âµnâˆ’1) be the spectrum of A.
Let us arrange Î» and Âµ in increasing order. From Theorem 3.3.3, one has
Î»1 â‰¤Âµ1 â‰¤Î»2 â‰¤Â· Â· Â· â‰¤Âµnâˆ’1 â‰¤Î»n. It follows that sk(Âµ) â‰¥sk(Î») for
every k < n. The induction hypothesis tells us that sk(Âµ) â‰¤sk(aâ€²), where
aâ€² = (a1, . . . , anâˆ’1). Finally, we have sk(aâ€²) = sk(a), and sk(Î») â‰¤sk(a) for
every k < n, which ends the induction.
.
Here is the converse.
Theorem 3.4.2 Let a and Î» be two sequences of n real numbers such that
a â‰»Î». Then there exists a real symmetric matrix of size n Ã— n whose
diagonal is a and spectrum is Î».
Proof
We proceed by induction on n. The statement is trivial if n = 1. If n â‰¥2,
we use the following lemma, which will be proved afterwards.
Lemma 3.4.1 Let n â‰¥2 and Î±, Î² two nondecreasing sequences of n real
numbers, satisfying Î± â‰ºÎ². Then there exists a sequence Î³ of n âˆ’1 real
numbers such that
Î±1 â‰¤Î³1 â‰¤Î±2 â‰¤Â· Â· Â· â‰¤Î³nâˆ’1 â‰¤Î±n

54
3. Matrices with Real or Complex Entries
and Î³ â‰ºÎ²â€² = (Î²1, . . . , Î²nâˆ’1).
We apply the lemma to the sequences Î± = Î», Î² = a. Since Î³ â‰ºaâ€², the
induction hypothesis tells us that there exists a real symmetric matrix S
of size (n âˆ’1) Ã— (n âˆ’1) with diagonal aâ€² and spectrum Î³. From Corollary
3.3.2, there exist a vector y âˆˆIRn and b âˆˆIR such that the matrix
Î£ =
 S
yT
y
b

has spectrum Î». Since sn(a) = sn(Î») = Tr Î£ = Tr S + b = snâˆ’1(aâ€²) + b, we
have b = an. Hence, a is the diagonal of Î£.
We prove now Lemma 3.4.1. Let âˆ†be the set of sequences Î´ of nâˆ’1 real
numbers satisfying
Î±1 â‰¤Î´1 â‰¤Î±2 â‰¤Â· Â· Â· â‰¤Î´nâˆ’1 â‰¤Î±n
(3.4)
together with
k

j=1
Î´j â‰¤
k

j=1
Î²j,
âˆ€k â‰¤n âˆ’2.
(3.5)
We must show that there exists Î´ âˆˆâˆ†such that snâˆ’1(Î´) = snâˆ’1(Î²â€²). Since
âˆ†is convex and compact (it is closed and bounded in IRn), it is enough to
show that
inf
Î´âˆˆâˆ†snâˆ’1(Î´) â‰¤snâˆ’1(Î²â€²) â‰¤sup
Î´âˆˆâˆ†
snâˆ’1(Î´).
(3.6)
On the one hand, Î±â€² = (Î±1, . . . , Î±nâˆ’1) belongs to âˆ†and snâˆ’1(Î±â€²) â‰¤
snâˆ’1(Î²â€²) from the hypothesis, which proves the ï¬rst inequality in (3.6).
Let us now choose a Î´ that achieves the supremum of snâˆ’1 over âˆ†. Let r
be the largest index less than or equal to nâˆ’2 such that sr(Î´) = sr(Î²â€²), with
r = 0 if all the inequalities are strict. From sj(Î´) < sj(Î²â€²) for r < j < nâˆ’1,
one has Î´j = Î±j+1, since otherwise, there would exist Ïµ > 0 such that
Ë†Î´ := Î´ + Ïµej belong to âˆ†, and one would have snâˆ’1(Ë†Î´) = snâˆ’1(Î´) + Ïµ,
contrary to the maximality of Î´. Now let us compute
snâˆ’1(Î´) âˆ’snâˆ’1(Î²â€²)
=
sr(Î²) âˆ’snâˆ’1(Î²) + Î±r+2 + Â· Â· Â· + Î±n
=
sr(Î²) âˆ’snâˆ’1(Î²) + sn(Î±) âˆ’sr+1(Î±)
â‰¥
sr(Î²) âˆ’snâˆ’1(Î²) + sn(Î²) âˆ’sr+1(Î²)
=
Î²n âˆ’Î²r+1 â‰¥0.
This proves (3.6) and completes the proof of the lemma.

3.5. Exercises
55
3.4.1
Hadamardâ€™s Inequality
Proposition 3.4.2 Let H âˆˆHn be a positive semideï¬nite Hermitian
matrix. Then
det H â‰¤
n

j=1
hjj.
If H âˆˆHPDn, the equality holds only if H is diagonal.
Proof
If det H = 0, there is nothing to prove, because the hjj are nonnegative
(these are numbers (ej)âˆ—Hej). Otherwise, H is positive deï¬nite and one
has hjj > 0. We restrict attention to the case with a constant diagonal
by letting D := diag(hâˆ’1/2
11
, . . . , hâˆ’1/2
nn
) and writing (det H)/(
j hjj) =
det DHD = det Hâ€², where the diagonal entries of Hâ€² equal one. There
remains to prove that det Hâ€² â‰¤1. However, the eigenvalues Âµ1, . . . , Âµn of
Hâ€² are strictly positive, of sum n. Since the logarithm is concave, one has
1
n log det Hâ€² = 1
n

j
log Âµj â‰¤log 1
n

Âµj = log 1 = 0,
which proves the inequality. Since the concavity is strict, the equality holds
only if Âµ1 = Â· Â· Â· = Âµn = 1, but then Hâ€² is similar, thus equal to In. In that
case, H is diagonal.
Applying proposition 3.4.2 to matrices of the form M âˆ—M or MM âˆ—, one
obtains the following result.
Theorem 3.4.3 For M âˆˆMn(CC), one has
| det M| â‰¤
n

i=1
ï£«
ï£­
n

j=1
|mij|2
ï£¶
ï£¸
1/2
,
| det M| â‰¤
n

j=1
 n

i=1
|mij|2
1/2
.
When M âˆˆGLn(CC), the ï¬rst (respectively the second) inequality is an
equality only if the rows (respectively the columns) of M are pairwise
orthogonal.
3.5
Exercises
1. Show that the eigenvalues of skew-Hermitian matrices, or as well
those of real skew-symmetric matrices, are pure imaginary.
2. Let P, Q âˆˆMn(IR) be given. Assume that P + iQ âˆˆGLn(CC). Show
that there exist a, b âˆˆIR such that aP + bQ âˆˆGLn(IR). Deduce that
if M, N âˆˆMn(IR) are similar in Mn(CC), then these matrices are
similar in Mn(IR).

56
3. Matrices with Real or Complex Entries
3. Show that a triangular and normal matrix is diagonal. Deduce that
if U âˆ—TU is a unitary trigonalization of M, and if M is normal, then
T is diagonal.
4. For A âˆˆMn(IR), symmetric positive deï¬nite, show that
max
i,jâ‰¤n |aij| = max
iâ‰¤n aii.
5. Given an invertible matrix
M =

a
b
c
d

âˆˆGL2(IR),
deï¬ne a map hM from S2 := CC âˆª{âˆ} into itself by
hM(z) := az + b
cz + d.
(a) Show that hM is a bijection.
(b) Show that h : M â†’hM is a group homomorphism. Compute its
kernel.
(c) Let H be the upper half-plane, consisting on those z âˆˆCC with
â„‘z > 0. Compute â„‘hM(z) in terms of â„‘z and deduce that the
subgroup
GL+
2 (IR) := {M âˆˆGL2(IR) | det M > 0}
acts on H.
(d) Conclude that the group PSL2(IR) := SL2(IR)/{Â±I2}, called
the modular group, acts on H.
(e) Let M âˆˆSL2(IR) be given. Determine, in terms of Tr M, the
number of ï¬xed points of hM on H.
6. Show that the supremum of a family of convex functions on IRN is
convex. Deduce that the map M â†’Î»n (largest eigenvalue of M)
deï¬ned on Hn is convex.
7. Show that M âˆˆMn(CC) is normal if and only if there exists a unitary
matrix U such that M âˆ—= MU.
8. Show that in Mn(CC) the set of diagonalizable matrices is dense. Hint:
Use Theorem 3.1.3.
9. Let (a1, . . . , an) and (b1, . . . , bn) be two sequences of real numbers.
Find the supremum and the inï¬mum of Tr(AB) as A (respectively B)
runs over the Hermitian matrices with spectrum equal to (a1, . . . , an)
(respectively (b1, . . . , bn)).
10. (Kantorovich inequality)

3.5. Exercises
57
(a) Let a1 â‰¤Â· Â· Â· â‰¤an be a list of real numbers, with aâˆ’1
n
= a1 > 0.
Deï¬ne
l(u) :=
n

j=1
ajuj,
L(u) :=
n

j=1
uj
aj
.
Let Kn be the simplex of IRn deï¬ned by the constraints uj â‰¥0
for every j = 1, . . . , n, and 
j uj = 1. Show that there exists
an element v âˆˆKn that maximizes l + L and minimizes |L âˆ’l|
on Kn simultaneously.
(b) Deduce that
max
uâˆˆKn l(u)L(u) =
a1 + an
2
2
.
(c) Let A âˆˆHPDn and let a1, an be the smallest and largest
eigenvalues of A. Show that for every x âˆˆCn,
(xâˆ—Ax)(xâˆ—Aâˆ’1x) â‰¤(a1 + an)2
4a1an
âˆ¥xâˆ¥4.
11. (Weylâ€™s inequalities)
Let A, B be two Hermitian matrices of size n Ã— n whose respective
eigenvalues are Î±1 â‰¤Â· Â· Â· â‰¤Î±n and Î²1 â‰¤Â· Â· Â· â‰¤Î²n. Deï¬ne C = A + B
and let Î³1 â‰¤Â· Â· Â· â‰¤Î³n be its eigenvalues.
(a) Show that Î±j + Î²1 â‰¤Î³j â‰¤Î±j + Î²n.
(b) Let us recall that if F is a linear subspace of CCn, one writes
RA(F) = max{xâˆ—Ax | x âˆˆF, âˆ¥xâˆ¥2 = 1}.
Show that if G, H are two linear subspaces of CCn, then RC(G âˆ©
H) â‰¤RA(G) + RB(H).
(c) Deduce that if l, m â‰¥1 and l +m = k +n (hence l +m â‰¥n+1),
then
Î³k â‰¤Î±l + Î²m.
(d) Similarly, show that l + m = k + 1 implies
Î³k â‰¥Î±l + Î²m.
(e) Conclude that the function A â†’Î»k(A) that associates to a Her-
mitian matrix its kth eigenvalue (in increasing order) is Lipschitz
with ratio 1, meaning that
|Î»k(B) âˆ’Î»k(A)| â‰¤âˆ¥B âˆ’Aâˆ¥2 = Ï(B âˆ’A)
(see the next chapter for the meaning of the norm âˆ¥Mâˆ¥2 and for
the spectral radius Ï(M)).
Remark: The description of the set of the 3n-tuplets (âƒ—Î±, âƒ—Î²,âƒ—Î³) as A
and B run over Hn is especially delicate. For a complete historical

58
3. Matrices with Real or Complex Entries
account of this question, one may read the ï¬rst section of Fultonâ€™s
and Bhatiaâ€™s articles [16, 6]. For another partial result, see Exercise
19 of Chapter 5 (theorem of Lidskii).
12. Let A be a Hermitian matrix of size n Ã— n whose eigenvalues are
Î±1 â‰¤Â· Â· Â· â‰¤Î±n. Let B be a Hermitian positive semideï¬nite matrix.
Let Î³1 â‰¤Â· Â· Â· â‰¤Î³n be the eigenvalues of A + B. Show that Î³k â‰¥Î±k.
13. Let M, N be two Hermitian matrices such that N and M âˆ’N are
positive semideï¬nite. Show that det N â‰¤det M.
14. Let A âˆˆMp(CC), C âˆˆMq(CC) be given with p, q â‰¥1. Assume that
M :=

A
B
Bâˆ—
C

is Hermitian positive deï¬nite. Show that det M â‰¤(det A)(det C). Use
the previous exercise and Proposition 8.1.2.
15. For M âˆˆHPDn, we denote by Pk(M) the product of all the principal
minors of order k of M. There are
 n
k

such minors.
Applying Proposition 3.4.2 to the matrix M âˆ’1, show that
Pn(M)nâˆ’1 â‰¤Pnâˆ’1(M),
and then in general that
Pk+1(M)k â‰¤Pk(M)nâˆ’k.
16. Let d : Mn(IR) â†’IR+ be a multiplicative function; that is,
d(MN) = d(M)d(N)
for every M, N âˆˆMn(IR). If Î± âˆˆIR, deï¬ne Î´(Î±) := d(Î±In)1/n.
Assume that d is not constant.
(a) Show that d(0n) = 0 and d(In) = 1. Deduce that P âˆˆGLn(IR)
implies d(P) Ì¸= 0 and d(P âˆ’1) = 1/d(P). Show, ï¬nally, that if M
and N are similar, then d(M) = d(N).
(b) Let D âˆˆMn(IR) be diagonal. Find matrices D1, . . . , Dnâˆ’1, sim-
ilar to D, such that DD1 Â· Â· Â· Dnâˆ’1 = (det D)In. Deduce that
d(D) = Î´(det D).
(c) Let M âˆˆMn(IR) be a diagonalizable matrix. Show that d(M) =
Î´(det M).
(d) Using the fact that M T is similar to M, show that d(M) =
Î´(det M) for every M âˆˆMn(IR).

3.5. Exercises
59
17. Let B âˆˆGLn(CC). Verify that the inverse and the Hermitian adjoint
of Bâˆ’1Bâˆ—are similar. Conversely, let A âˆˆGLn(CC) be a matrix whose
inverse and the Hermitian adjoint are similar: Aâˆ—= PAâˆ’1P âˆ’1.
(a) Show that there exists an invertible Hermitian matrix H such
that H = Aâˆ—HA. Look for an H as a linear combination of P
and of P âˆ—.
(b) Show that there exists a matrix B âˆˆGLn(CC) such that A =
Bâˆ’1Bâˆ—. Look for a B of the form (aIn + bAâˆ—)H.
18. Let A âˆˆMn(CC) be given, and let Î»1, . . . , Î»n be its eigenvalues. Show,
by induction on n, that A is normal if and only if

i,j
|aij|2 =
n

1
|Î»l|2.
Hint: The left-hand side (whose square root is called Schurâ€™s norm)
is invariant under conjugation by a unitary matrix. It is then enough
to restrict attention to the case of a triangular matrix.
19.
(a) Show that | det(In + A)| â‰¥1 for every skew-Hermitian matrix
A, and that equality holds only if A = 0n.
(b) Deduce that for every M âˆˆMn(CC) such that H := (M +M âˆ—)/2
is positive deï¬nite,
det H â‰¤| det M|
by showing that Hâˆ’1(M âˆ’M âˆ—) is similar to a skew-Hermitian
matrix. You may use the square root deï¬ned at Chapter 7.
20. Describe every positive semideï¬nite matrix M âˆˆSymn(IR) such that
mjj = 1 for every j and possessing the eigenvalue Î» = n (ï¬rst show
that M has rank one).
21. If A, B âˆˆMnÃ—m(CC), deï¬ne the Hadamard product of A and B by
A â—¦B := (aijbij)1â‰¤iâ‰¤n,1â‰¤jâ‰¤m.
(a) Let A, B be two Hermitian matrices. Verify that A â—¦B is
Hermitian.
(b) Assume that A and B are positive semideï¬nite, of respective
ranks p and q. Using Proposition 3.2.1, show that there exist pq
vectors zÎ±Î² such that
A â—¦B =

Î±,Î²
zÎ±Î²zâˆ—
Î±Î².
Deduce that A â—¦B is positive semi-deï¬nite.
(c) If A and B are positive deï¬nite, show that A â—¦B also is positive
deï¬nite.

60
3. Matrices with Real or Complex Entries
(d) Construct an example for which p, q < n, but A â—¦B is positive
deï¬nite.
22. (Fiedler and PtÂ´ak [13]) Given a matrix A âˆˆMn(IR), we wish to prove
the equivalence of the following properties:
P1 For every vector x Ì¸= 0 there exists an index k such that
xk(Ax)k > 0.
P2 For every vector x Ì¸= 0 there exists a diagonal matrix D with
positive diagonal elements such that the scalar product (Ax, Dx)
is positive.
P3 For every vector x Ì¸= 0 there exists a diagonal matrix D with
nonnegative diagonal elements such that the scalar product
(Ax, Dx) is positive.
P4 The real eigenvalues of all principal submatrices of A are positive.
P5 All principal minors of A are positive.
We shall use the following notation: if x âˆˆIRn and if J is the index set
of its nonzero components, then xJ denotes the vector in IRk, and k
the cardinality of J, where one retains only the nonzero components
of x. To the set J one also associates the matrix AJ, retaining only
the indices in J.
(a) Prove that Pj implies P(j+1) for every j = 1, . . . , 4.
(b) Assume P5. Show that for every diagonal matrix D with
nonnegative entries, one has det(A + D) > 0.
(c) Then prove that P5 implies P1.

4
Norms
4.1
A Brief Review
In this Chapter, the ï¬eld K will always be IR or CC and E will denote Kn.
If A âˆˆMn(K), the spectral radius of A, denoted by Ï(A), is deï¬ned as
the largest modulus of the eigenvalues of A:
Ï(A) = max{|Î»|; Î» âˆˆSp(A)}.
When K = IR, one takes into account the complex eigenvalues when
computing Ï(A).
The scalar (if K = IR) or Hermitian (if K = CC) product on E is denoted
by (x, y) := 
j xj Â¯yj. The vector space E is endowed with various norms,
pairwise equivalent since E has ï¬nite dimension (Proposition 4.1.3 below).
Among these, the most used norms are the lp norms:
âˆ¥xâˆ¥p =
ï£«
ï£­
j
|xj|p
ï£¶
ï£¸
1/p
,
âˆ¥xâˆ¥âˆ= max
j
|xj|.
Proposition 4.1.1 For 1 â‰¤p â‰¤âˆ, the map x â†’âˆ¥xâˆ¥p is a norm on E.
In particular, one has Minkowskiâ€™s inequality
âˆ¥x + yâˆ¥p â‰¤âˆ¥xâˆ¥p + âˆ¥yâˆ¥p.
(4.1)

62
4. Norms
Furthermore, one has HÂ¨olderâ€™s inequality
|(x, y)| â‰¤âˆ¥xâˆ¥pâˆ¥yâˆ¥pâ€²,
1
p + 1
pâ€² = 1.
(4.2)
The numbers p, pâ€² are called conjugate exponents.
Proof
Everything except the HÂ¨older and Minkowski inequalities is obvious.
When p = 1 or p = âˆ, these inequalities are trivial. We thus assume
that 1 < p < âˆ.
Let us begin with (4.2). If x or y is null, it is obvious. Indeed, one can
even assume, by decreasing the value of n, that none of the xj, yjâ€™s is null.
Likewise, since |(x, y)| â‰¤
j |xj||yj|, one can also assume that the xj, yj are
real and positive. Dividing by âˆ¥xâˆ¥p and by âˆ¥yâˆ¥pâ€², one may restrict attention
to the case where âˆ¥xâˆ¥p = âˆ¥yâˆ¥pâ€² = 1. Hence, xj, yj âˆˆ(0, 1] for every j. Let
us deï¬ne
aj = p log xj,
bj = pâ€² log yj.
Since the exponential function is convex,
eaj/p+bj/pâ€² â‰¤1
peaj + 1
pâ€² ebj,
that is,
xjyj â‰¤1
pxp
j + 1
pâ€² ypâ€²
j .
Summing over j, we obtain
(x, y) â‰¤1
pâˆ¥xâˆ¥p
p + 1
pâ€² âˆ¥yâˆ¥pâ€²
pâ€² = 1
p + 1
pâ€² = 1,
which proves (4.2).
We now turn to (4.1). First, we have
âˆ¥x + yâˆ¥p
p =

k
|xk + yk|p â‰¤

k
|xk||xk + yk|pâˆ’1 +

k
|yk||xk + yk|pâˆ’1.
Let us apply HÂ¨olderâ€™s inequality to each of the two terms of the right-hand
side. For example,

k
|xk||xk + yk|pâˆ’1 â‰¤âˆ¥xâˆ¥p

k
|xk + yk|(pâˆ’1)pâ€²
1/pâ€²
,
which amounts to

k
|xk||xk + yk|pâˆ’1 â‰¤âˆ¥xâˆ¥pâˆ¥x + yâˆ¥pâˆ’1
p
.
Finally,
âˆ¥x + yâˆ¥p
p â‰¤(âˆ¥xâˆ¥p + âˆ¥yâˆ¥p)âˆ¥x + yâˆ¥pâˆ’1
p
,

4.1. A Brief Review
63
which gives (4.1).
For p = 2, the norm âˆ¥Â·âˆ¥2 is given by a Hermitian form and thus satisï¬es
the Cauchyâ€“Schwarz inequality:
|(x, y)| â‰¤âˆ¥xâˆ¥2âˆ¥yâˆ¥2.
This is a particular case of HÂ¨olderâ€™s inequality.
Proposition 4.1.2 For conjugate exponents p, pâ€², one has
âˆ¥xâˆ¥p = sup
yÌ¸=0
â„œ(x, y)
âˆ¥yâˆ¥pâ€² .
Proof
The inequality â‰¥is a consequence of HÂ¨olderâ€™s. The reverse inequality is
obtained by taking yj = Â¯xj|xj|pâˆ’2 if p < âˆ. If p = âˆ, choose yj = Â¯xj for
an index j such that |xj| = âˆ¥xâˆ¥âˆ. For k Ì¸= j, take yk = 0.
Deï¬nition 4.1.1 Two norms N and N â€² on a (real or complex) vector
space are said to be equivalent if there exist two numbers c, câ€² âˆˆIR such
that
N â‰¤cN â€²,
N â€² â‰¤câ€²N.
The equivalence between norms is obviously an equivalence relation, as
its name implies. As announced above, we have the following result.
Proposition 4.1.3 All norms on E = Kn are equivalent. For example,
âˆ¥xâˆ¥âˆâ‰¤âˆ¥xâˆ¥p â‰¤n1/pâˆ¥xâˆ¥âˆ.
Proof
It is suï¬ƒcient to show that every norm is equivalent to âˆ¥Â· âˆ¥1.
Let N be a norm on E. If x âˆˆE, the triangle inequality gives
N(x) â‰¤

i
|xi|N(ei),
where (e1, . . . , en) is the canonical basis. One thus has N â‰¤câˆ¥Â· âˆ¥1 for
c := maxi N(ei). Observe that this ï¬rst inequality expresses the fact that
N is Lipschitz (hence continuous) on the metric space X = (E, âˆ¥Â· âˆ¥1).
For the reverse inequality, we reduce ad absurdum: Let us assume that
the supremum of âˆ¥xâˆ¥1/N(x) is inï¬nite for x Ì¸= 0. By homogeneity, there
would then exist a sequence of vectors (xm)mâˆˆIN such that âˆ¥xmâˆ¥1 = 1 and
N(xm) â†’0 when m â†’+âˆ. Since the unit sphere of X is compact, one
may assume (up to the extraction of a subsequence) that xm converges to
a vector x such that âˆ¥xâˆ¥1 = 1. In particular, x Ì¸= 0. Since N is continuous
on X, one has also N(x) = limmâ†’+âˆN(xm) = 0. Since N is a norm, we
deduce x = 0, a contradiction.

64
4. Norms
4.1.1
Duality
Deï¬nition 4.1.2 Given a norm âˆ¥Â· âˆ¥on IRn, its dual norm on IRn is
deï¬ned by
âˆ¥xâˆ¥â€² := sup
yÌ¸=0
yT x
âˆ¥yâˆ¥.
The fact that âˆ¥Â· âˆ¥â€² is a norm is obvious. The dual of a norm on CCn is
deï¬ned in a similar way, with â„œyâˆ—x instead of yT x. For every x, y âˆˆCCn,
one has
â„œyâˆ—x â‰¤âˆ¥xâˆ¥Â· âˆ¥yâˆ¥â€².
(4.3)
Proposition 4.1.2 shows that the dual norm of âˆ¥Â·âˆ¥p is âˆ¥Â·âˆ¥q for 1/p+1/q = 1.
This suggests the following property.
Proposition 4.1.4 The bidual (dual of the dual norm) of a norm is this
norm itself:
(âˆ¥Â· âˆ¥â€²)â€² = âˆ¥Â· âˆ¥.
Proof
From (4.3), one has (âˆ¥Â· âˆ¥â€²)â€² â‰¤âˆ¥Â· âˆ¥. The converse is a consequence of
the Hahnâ€“Banach theorem: the unit ball B of âˆ¥Â· âˆ¥is convex and compact.
If x is a point of its boundary (that is, âˆ¥xâˆ¥= 1), there exists an IR-
aï¬ƒne (that is, of the form constant plus IR-linear) function that is zero
at x and nonpositive on B. Such a function can be written in the form
z â†’â„œzâˆ—y + c, where c is a constant, necessarily equal to âˆ’â„œzâˆ—x. Without
loss of generality, one may assume that zâˆ—x is real. Hence
âˆ¥yâˆ¥â€² = sup
âˆ¥zâˆ¥=1
â„œyâˆ—z = yâˆ—x.
One deduces
(âˆ¥xâˆ¥â€²)â€² â‰¥yâˆ—x
âˆ¥yâˆ¥â€² = 1 = âˆ¥xâˆ¥.
By homogeneity, this is true for every x âˆˆCCn.
4.1.2
Matrix Norms
Let us recall that Mn(K) can be identiï¬ed with the set of endomorphisms
of E = Kn by
A â†’(x â†’Ax).
Deï¬nition 4.1.3 If âˆ¥Â· âˆ¥is a norm on E and if A âˆˆMn(K), we deï¬ne
âˆ¥Aâˆ¥:= sup
xÌ¸=0
âˆ¥Axâˆ¥
âˆ¥xâˆ¥.

4.1. A Brief Review
65
Equivalently,
âˆ¥Aâˆ¥= sup
âˆ¥xâˆ¥â‰¤1
âˆ¥Axâˆ¥= max
âˆ¥xâˆ¥â‰¤1 âˆ¥Axâˆ¥.
One veriï¬es easily that A â†’âˆ¥Aâˆ¥is a norm on Mn(K). It is called the norm
induced by that of E, or the norm subordinated to that of E. Though we
adopted the same notation âˆ¥Â· âˆ¥for the two norms, that on E and that on
Mn(K), these are, of course, distinct objects. In many places, one ï¬nds the
notation ||| Â· ||| for the induced norm. When one does not wish to mention
from which norm on E a given norm on Mn(K) is induced, one says that
A â†’âˆ¥Aâˆ¥is a matrix norm. The main properties of matrix norms are
âˆ¥ABâˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥Bâˆ¥,
âˆ¥Inâˆ¥= 1.
These properties are those of any algebra norm (otherwise called norm of
algebra, see Section 4.4). In particular, one has âˆ¥Akâˆ¥â‰¤âˆ¥Aâˆ¥k for every
k âˆˆIN.
Here are a few examples induced by the norms lp:
âˆ¥Aâˆ¥1
=
max
1â‰¤jâ‰¤n
i=n

i=1
|aij|,
âˆ¥Aâˆ¥âˆ
=
max
1â‰¤iâ‰¤n
j=n

j=1
|aij|,
âˆ¥Aâˆ¥2
=
Ï(Aâˆ—A)1/2.
To prove these formulas, we begin by proving the inequalities â‰¥, selecting
a suitable vector x, and writing âˆ¥Aâˆ¥p â‰¥âˆ¥Axâˆ¥p/âˆ¥xâˆ¥p. For p = 1 we choose
an index j such that the maximum in the above formula is achieved. Then
we let xj = 1, while xk = 0 otherwise. For p = âˆ, we let xj = Â¯ai0j/|ai0j|,
where i0 achieves the maximum in the above formula; For p = 2 we choose
an eigenvector of Aâˆ—A associated to an eigenvalue of maximal modulus.
We thus obtain three inequalities. The reverse inequalities are direct con-
sequences of the deï¬nitions. The values of âˆ¥Aâˆ¥1 and âˆ¥Aâˆ¥âˆillustrate a
particular case of the general formula
âˆ¥Aâˆ—âˆ¥â€² = âˆ¥Aâˆ¥= sup
xÌ¸=0
sup
yÌ¸=0
â„œ(yâˆ—Ax)
âˆ¥xâˆ¥Â· âˆ¥yâˆ¥â€² .
Proposition 4.1.5 For an induced norm, the condition âˆ¥Bâˆ¥< 1 implies
that In âˆ’B is invertible, with inverse given by the sum of the series
âˆ

k=0
Bk.
Proof
The series 
k Bk is normally convergent, since 
k âˆ¥Bkâˆ¥â‰¤
k âˆ¥Bâˆ¥k,
where the latter series converges because âˆ¥Bâˆ¥< 1. Since Mn(K) is com-

66
4. Norms
plete, the series 
k Bk converges. Furthermore, (In âˆ’B) 
kâ‰¤N Bk =
In âˆ’BN+1, which tends to In. The sum of the series is thus the inverse of
In âˆ’B. One has, moreover,
âˆ¥(In âˆ’B)âˆ’1âˆ¥â‰¤

k
âˆ¥Bâˆ¥k =
1
1 âˆ’âˆ¥Bâˆ¥.
One can also deduce Proposition 4.1.5 from the following statement.
Proposition 4.1.6 For every induced norm, one has
Ï(A) â‰¤âˆ¥Aâˆ¥.
Proof
The case K = CC is easy, because there exists an eigenvector X âˆˆE
associated to an eigenvalue of modulus Ï(A):
Ï(A)âˆ¥Xâˆ¥= âˆ¥Î»Xâˆ¥= âˆ¥AXâˆ¥â‰¤âˆ¥Aâˆ¥âˆ¥Xâˆ¥.
If K = IR, one needs a more involved trick.
Let us choose a norm on CCn and let us denote by N the induced norm
on Mn(CC). We still denote by N its restriction to Mn(IR); it is a norm.
Since this space has ï¬nite dimension, any two norms are equivalent: There
exists C > 0 such that N(B) â‰¤Câˆ¥Bâˆ¥for every B in Mn(IR). Using the
result already proved in the complex case, one has for every m âˆˆIN that
Ï(A)m = Ï(Am) â‰¤N(Am) â‰¤Câˆ¥Amâˆ¥â‰¤Câˆ¥Aâˆ¥m.
Taking the mth root and letting m tend to inï¬nity, and noticing that C1/m
tends to 1, one obtains the announced inequality.
In general, the equality does not hold. For example, if A is nilpotent
though nonzero, one has Ï(A) = 0 < âˆ¥Aâˆ¥for every matrix norm.
Proposition 4.1.7 Let âˆ¥Â· âˆ¥be a norm on Kn and P âˆˆGLn(K). Hence,
N(x) := âˆ¥Pxâˆ¥deï¬nes a norm on Kn. Denoting still by âˆ¥Â· âˆ¥and N the
induced norms on Kn, one has N(A) = âˆ¥PAP âˆ’1âˆ¥.
Proof
Using the change of dummy variable y = Px, we have
N(A) = sup
xÌ¸=0
âˆ¥PAxâˆ¥
âˆ¥Pxâˆ¥= sup
yÌ¸=0
âˆ¥PAP âˆ’1yâˆ¥
âˆ¥yâˆ¥
= âˆ¥PAP âˆ’1âˆ¥.
4.2
Householderâ€™s Theorem
Householderâ€™s theorem is a kind of converse of the inequality Ï(B) â‰¤âˆ¥Bâˆ¥.

4.3. An Interpolation Inequality
67
Theorem 4.2.1 For every B âˆˆMn(CC) and all Ïµ > 0, there exists a norm
on CCn such that for the induced norm
âˆ¥Bâˆ¥â‰¤Ï(B) + Ïµ.
In other words, Ï(B) is the inï¬mum of âˆ¥Bâˆ¥, as âˆ¥Â· âˆ¥ranges over the set
of matrix norms.
Proof
From Theorem 2.7.1 there exists P âˆˆGLn(CC) such that T := PBP âˆ’1
is upper triangular. From Proposition 4.1.7, one has
inf âˆ¥Bâˆ¥= inf âˆ¥PBP âˆ’1âˆ¥= inf âˆ¥T âˆ¥,
where the inï¬mum is taken over the set of induced norms. Since B and
T have the same spectra, hence the same spectral radius, it is enough to
prove the theorem for upper triangular matrices.
For such a matrix T , Proposition 4.1.7 still gives
inf âˆ¥T âˆ¥â‰¤inf{âˆ¥QTQâˆ’1âˆ¥2; Q âˆˆGLn(CC)}.
Let us now take Q(Âµ) = diag(1, Âµ, Âµ2, . . . , Âµnâˆ’1). The matrix Q(Âµ)T Q(Âµ)âˆ’1
is upper triangular, with the same diagonal as that of T . Indeed, the entry
with indices (i, j) becomes Âµiâˆ’jtij. Hence,
lim
Âµâ†’âˆQ(Âµ)T Q(Âµ)âˆ’1
is simply the matrix D = diag(t11, . . . , tnn). Since âˆ¥Â· âˆ¥2 is continuous (as
is every norm), one deduces
inf âˆ¥T âˆ¥â‰¤lim
Âµâ†’âˆâˆ¥Q(Âµ)TQ(Âµ)âˆ’1âˆ¥2 = âˆ¥Dâˆ¥2 =

Ï(Dâˆ—D) = max |tjj| = Ï(T ).
Remark: The theorem tells us that Ï(A) = Î›(A), where
Î›(A) := inf âˆ¥Aâˆ¥,
the inï¬mum being taken over the set of matrix norms. The ï¬rst part of the
proof tells us that Ï and Î› coincide on the set of diagonalizable matrices,
which is a dense subset of Mn(CC). But this is insuï¬ƒcient to conclude,
since Î› is a priori only upper semicontinuous, as the inï¬mum of continuous
functions. The continuity of Î› is actually a consequence of the theorem.
4.3
An Interpolation Inequality
Theorem 4.3.1 (case K = CC) Let âˆ¥Â·âˆ¥p be the norm on Mn(CC) induced
by the norm lp on CCn. The function
1/p
â†’
log âˆ¥Aâˆ¥p,
[0, 1]
â†’
IR,

68
4. Norms
is convex. In other words, if 1/r = Î¸/p + (1 âˆ’Î¸)/q with Î¸ âˆˆ(0, 1), then
âˆ¥Aâˆ¥r â‰¤âˆ¥Aâˆ¥Î¸
pâˆ¥Aâˆ¥1âˆ’Î¸
q
.
Remark:
1. The proof uses the fact that K = CC. However, the norms induced
by the âˆ¥Â· âˆ¥pâ€™s on Mn(IR) and Mn(CC) take the same values on real
matrices, even though their deï¬nitions are diï¬€erent (see Exercise 6).
The statement is thus still true in Mn(IR).
2. The case (p, q, r) = (1, âˆ, 2) admits a direct proof. See the exercises.
3. The result still holds true in inï¬nite dimension, at the expense of
some functional analysis. One even can take diï¬€erent Lp norms at
the source and target spaces. Here is an example:
Theorem 4.3.2 (Rieszâ€“Thorin) Let â„¦be an open set in IRD and
Ï‰ an open set in IRd. Let p0, p1, q0, q1 be four numbers in [1, +âˆ].
Let Î¸ âˆˆ[0, 1] and p, q be deï¬ned by
1
p = 1 âˆ’Î¸
p0
+ Î¸
p1
,
1
q = 1 âˆ’Î¸
q0
+ Î¸
q1
.
Consider a linear operator T deï¬ned on Lp0 âˆ©Lp1(â„¦), taking values in
Lq0 âˆ©Lq1(Ï‰). Assume that T can be extended as a continuous operator
from Lpj(â„¦) to Lqj(Ï‰), with norm Mj, j = 1, 2 :
Mj := sup
fÌ¸=0
âˆ¥T fâˆ¥qj
âˆ¥fâˆ¥pj
.
Then T can be extended as a continuous operator from Lp(â„¦) to
Lq(Ï‰), and its norm is bounded above by
M 1âˆ’Î¸
0
M Î¸
1 .
4. A fundamental application is the continuity of the Fourier transform
from Lp(IRd) into its dual Lpâ€²(IRd) when 1 â‰¤p â‰¤2. We have only
to observe that (p0, p1, q0, q1) = (1, 2, +âˆ, 2) is suitable. It can be
proved by inspection that every pair (p, q) such that the Fourier trans-
form is continuous from Lp(IRd) into Lq(IRd) has the form (p, pâ€²) with
1 â‰¤p â‰¤2.
5. One has analogous results for Fourier series. There lies the origin of
Rieszâ€“Thorin theorem.
Proof (due to F. Riesz)
Let us ï¬x x and y in Kn. We have to bound
|(Ax, y)| =


j,k
ajkxj Â¯yk

.

4.3. An Interpolation Inequality
69
Let B be the strip in the complex plane deï¬ned by â„œz âˆˆ[0, 1]. Given z âˆˆB,
deï¬ne (conjugate) exponents r(z) and râ€²(z) by
1
r(z) = z
p + 1 âˆ’z
q
,
1
râ€²(z) = z
pâ€² + 1 âˆ’z
qâ€²
.
Set
Xj(z)
:=
|xj|âˆ’1+r/r(z)xj = xj exp
 r
r(z) âˆ’1

log |xj|

,
Yj(z)
:=
|yj|âˆ’1+râ€²/râ€²(Â¯z)yj.
We then have
âˆ¥X(z)âˆ¥r(â„œz) = âˆ¥xâˆ¥r/r(â„œz)
r
,
âˆ¥Y (z)âˆ¥râ€²(â„œz) = âˆ¥yâˆ¥râ€²/râ€²(â„œz)
râ€²
.
Next, deï¬ne a holomorphic map in the strip B by f(z) := (AX(z), Y (z)).
It is bounded, because the numbers Xj(z) and Yk(z) are. For example,
|Xj(z)| = |xj|r/r(â„œz)
lies between |xj|r/p and |xj|r/q.
Let us set M(Î¸) = sup{|f(z)|; â„œz = Î¸}. Hadamardâ€™s three lines lemma
(see [29], Chapter 12, exercise 8) expresses that
Î¸ â†’log M(Î¸)
is convex on (0, 1). However, r(0) = q, r(1) = p, râ€²(0) = qâ€², râ€²(1) = pâ€²,
r(Î¸) = r, râ€²(Î¸) = râ€², X(Î¸) = x, and Y (Î¸) = y. Hence
|(Ax, y)| = |f(Î¸)| â‰¤M(Î¸) â‰¤M(1)Î¸M(0)1âˆ’Î¸.
Now we have
M(1)
=
sup{|f(z)|; â„œz = 1}
â‰¤
sup{âˆ¥AX(z)âˆ¥r(1)âˆ¥Y (z)âˆ¥r(1)â€²; â„œz = 1}
=
sup{âˆ¥AX(z)âˆ¥pâˆ¥Y (z)âˆ¥pâ€²; â„œz = 1}
â‰¤
âˆ¥Aâˆ¥p sup{âˆ¥X(z)âˆ¥pâˆ¥Y (z)âˆ¥pâ€²; â„œz = 1}
=
âˆ¥Aâˆ¥pâˆ¥xâˆ¥r/p
r
âˆ¥yâˆ¥râ€²/pâ€²
râ€²
.
Likewise, M(0) â‰¤âˆ¥Aâˆ¥qâˆ¥xâˆ¥r/q
r
âˆ¥yâˆ¥râ€²/qâ€²
râ€²
. Hence
|(Ax, y)|
â‰¤
âˆ¥Aâˆ¥Î¸
pâˆ¥Aâˆ¥1âˆ’Î¸
q
âˆ¥xâˆ¥r(Î¸/p+(1âˆ’Î¸)/q)
r
âˆ¥yâˆ¥râ€²(Î¸/pâ€²+(1âˆ’Î¸)/qâ€²)
râ€²
= âˆ¥Aâˆ¥Î¸
pâˆ¥Aâˆ¥1âˆ’Î¸
q
âˆ¥xâˆ¥râˆ¥yâˆ¥râ€².
Finally,
âˆ¥Axâˆ¥r = sup
yÌ¸=0
|(Ax, y)|
âˆ¥yâˆ¥râ€²
â‰¤âˆ¥Aâˆ¥Î¸
pâˆ¥Aâˆ¥1âˆ’Î¸
q
âˆ¥xâˆ¥r,
which proves the theorem.

70
4. Norms
4.4
A Lemma about Banach Algebras
Deï¬nition 4.4.1 A normed algebra is a K-algebra endowed with a norm
satisfying âˆ¥xyâˆ¥â‰¤âˆ¥xâˆ¥âˆ¥yâˆ¥. Such a norm is called an algebra norm. When
a normed algebra is complete (which is always true in ï¬nite dimension), it
is called a Banach algebra.
Lemma 4.4.1 Let A be a normed algebra and let x âˆˆA. The sequence
um := âˆ¥xmâˆ¥1/m converges to its inï¬mum, denoted by r(x). Additionally,
if K = CC, and if A has a unit element and is complete, then 1/r(x) is
the radius of the largest open ball B(0; R) such that e âˆ’zx is invertible for
every z âˆˆB(0; R).
Of course, one may apply the lemma to A = Mn(CC) endowed with
a matrix norm. One then has r(x) = Ï(x), because e âˆ’zx = I âˆ’zA is
invertible, provided that z is not the inverse of an eigenvalue. In the case
K = IR, one uses an auxiliary norm N that is the restriction to Mn(IR) of
an induced norm on Mn(CC). Since âˆ¥Â· âˆ¥and N are equivalent, one simply
writes
Ï(A) = Ï(Am)1/m â‰¤âˆ¥Amâˆ¥1/m â‰¤C1/mN(Am)1/m.
The latter sequence converges to Ï(A) from the lemma, which implies the
convergence of the former. We thus have the following result.
Proposition 4.4.1 If A âˆˆMn(K), then
Ï(A) = lim
mâ†’âˆâˆ¥Amâˆ¥1/m
for every matrix norm.
Proof
Convergence. The result is trivial if xm = 0 for some exponent. In the
opposite case, we use the following inequalities, which come directly
from the deï¬nition:
âˆ¥xap+râˆ¥â‰¤âˆ¥xpâˆ¥aâˆ¥xrâˆ¥,
âˆ€a, p, r âˆˆIN.
We then deï¬ne
vm = 1
m log âˆ¥xmâˆ¥= log um.
Let us ï¬x an integer p and perform Euclidean division of m by p:
m = ap + r with 0 â‰¤r â‰¤p âˆ’1. This yields
vap+r â‰¤apvp + rvr
ap + r
.
As m, hence a, tends to inï¬nity, the right-hand side converges,
because rvr remains bounded:
lim sup vm â‰¤vp.

4.5. The Gershgorin Domain
71
Since this holds true for every p, we conclude that
lim sup vm â‰¤inf vp â‰¤lim inf vp,
which proves the convergence to the inï¬mum.
Characterization (complex case). If R < 1/r(x), the Taylor series

mâˆˆIN
zmxm,
z âˆˆCC,
converges in norm in the ball B(0; R). Its sum equals (e âˆ’zx)âˆ’1 (see
the proof of Proposition 4.1.5).
The domain of the map z â†’(e âˆ’zx)âˆ’1 is open, since if it contains a
point z0, the previous paragraph shows that e âˆ’(z âˆ’z0)(e âˆ’z0x)âˆ’1x
is invertible for every z satisfying
|z âˆ’z0|r

(e âˆ’z0x)âˆ’1x

< 1.
Denoting by Xz the inverse, we see that Xz(e âˆ’z0x)âˆ’1 is an inverse
of e âˆ’zx. In particular, f : z â†’(e âˆ’z)âˆ’1 is holomorphic.
If f is deï¬ned on a ball B(0; s), Cauchyâ€™s formula
xm = 1
m!f (m)(0) =
1
2iÏ€
 
B(0;s)
f(z)
zm+1 dz
shows that âˆ¥xmâˆ¥= O(sâˆ’m). Hence, 1/r(x) â‰¥s.
Corollary 4.4.1 Let B âˆˆMn(K) be given. Then Bm mâ†’+âˆ
â†’
0 if and only
if Ï(B) < 1.
Indeed, Ï(B) â‰¥1 implies âˆ¥Bmâˆ¥â‰¥Ï(Bm) â‰¥1 for every m. Conversely,
Ï(B) < 1 implies âˆ¥Bmâˆ¥< rm for m large enough, where r is selected in
(Ï(B), 1).
We observe that this result is also a consequence of Householderâ€™s
theorem.
4.5
The Gershgorin Domain
Let A âˆˆMn(CC), and let Î» be an eigenvalue and x an associated eigenvector.
Let i be an index such that |xi| = âˆ¥xâˆ¥âˆ. Then xi Ì¸= 0 and
|aii âˆ’Î»| =


jÌ¸=i
aij
xj
xi

â‰¤

jÌ¸=i
|aij|.
Proposition 4.5.1 (Gershgorin) The spectrum of A is included in the
Gershgorin domain G(A), deï¬ned as the union of the Gershgorin disks
Di := D(aii; 
jÌ¸=i |aij|).

72
4. Norms
This result can also be deduced from Proposition 4.1.5: Let us decompose
A = D + C, where D is the diagonal part of A. If Î» Ì¸= aii for every i, then
Î»In âˆ’A = (Î»In âˆ’D)(In âˆ’B) with B = (Î»In âˆ’D)âˆ’1C. Hence, if Î» is an
eigenvalue, then either Î» is an aii, or âˆ¥Bâˆ¥âˆâ‰¥1.
One may improve this result by considering the connected components
of G. Let G be one of them. It is the union of the Dkâ€™s that meet it. Let
p be the number of such disks. One then has G = âˆªiâˆˆIDi where I has
cardinality p.
Theorem 4.5.1 There are exactly p eigenvalues of A in G, counted with
their multiplicities.
Proof
For r âˆˆ[0, 1], we deï¬ne a matrix A(r) by the formula
aij(r) :=
 aii,
j = i,
raij,
j Ì¸= i.
It is clear that the Gershgorin domain Gr of A(r) is included in G. We
observe that A(1) = A, and that r â†’A(r) is continuous. Let us denote by
m(r) the number of eigenvalues (counted with multiplicity) of A(r) that
belong to G.
Since G and G \ G are compact, one can ï¬nd a Jordan curve, oriented in
the trigonometric sense, that separates G from G\G. Let Î“ be such a curve.
Since Gr is included in G, the residue formula expresses m(r) in terms of
the characteristic polynomial of A(r):
m(r) =
1
2iÏ€
 
Î“
P â€²
r(z)
Pr(z) dz.
Since Pr does not vanish on Î“ and r â†’Pr, P â€²
r are continuous, we deduce
that r â†’m(r) is continuous. Since m(r) is an integer and [0, 1] is connected,
m(r) remains constant. In particular, m(0) = m(1).
Finally, m(0) is the number of entries ajj (eigenvalues of A(0)) that
belong to G. But ajj is in G if and only if Dj âŠ‚G. Hence m(0) = p, which
implies m(1) = p, the desired result.
An improvement of Gershgorinâ€™s theorem concerns irreducible matrices.
Proposition 4.5.2 Let A be an irreducible matrix. If an eigenvalue of A
does not belong to the interior of any Gershgorin disk, then it belongs to
all the circles S(aii; 
jÌ¸=i |aij|).
Proof
Let Î» be such an eigenvalue and x an associated eigenvector. By assump-
tion, one has |Î» âˆ’aii| â‰¥
jÌ¸=i |aij| for every i. Let I be the set of indices

4.6. Exercises
73
for which |xi| = âˆ¥xâˆ¥âˆand let J be its complement. If i âˆˆI, then
âˆ¥xâˆ¥âˆ

jÌ¸=i
|aij| â‰¤|Î» âˆ’aii| âˆ¥xâˆ¥âˆ=


jÌ¸=i
aijxj

â‰¤

jÌ¸=i
|aij| |xj|.
It follows that 
jÌ¸=i(âˆ¥xâˆ¥âˆâˆ’|xj|)|aij| â‰¤0, where all the terms in the sum
are nonnegative. Each term is thus zero, so that aij = 0 for j âˆˆJ. Since A
is irreducible, J is empty. One has thus |xj| = âˆ¥xâˆ¥âˆfor every j, and the
previous inequalities show that Î» belongs to every circle.
Deï¬nition 4.5.1 A square matrix A âˆˆMn(CC) is said to be
1. diagonally dominant if
|aii| â‰¥

jÌ¸=i
|aij|,
1 â‰¤i â‰¤n;
2. strongly diagonally dominant if in addition at least one of these n
inequalities is strict;
3. strictly diagonally dominant if the inequality is strict for every index
i.
Corollary 4.5.1 Let A be a square matrix. If A is strictly diagonally dom-
inant, or if A is irreducible and strongly diagonally dominant, then A is
invertible.
In fact, either zero does not belong to the Gershgorin domain, or it is
not interior to the disks. In the latter case, A is assumed to be irreducible,
and there exists a disk Dj that does not contain zero.
4.6
Exercises
1. Under what conditions on the vectors a, b âˆˆCCn does the matrix M
deï¬ned by mij = aibj satisfy âˆ¥Mâˆ¥p = 1 for every p âˆˆ[1, âˆ]?
2. Under what conditions on x, y, and p does the equality in (4.2) or
(4.1) hold?
3. Show that
lim
pâ†’+âˆâˆ¥xâˆ¥p = âˆ¥xâˆ¥âˆ,
âˆ€x âˆˆE.
4. A norm on Kn is a strictly convex norm if âˆ¥xâˆ¥= âˆ¥yâˆ¥= 1, x Ì¸= y, and
0 < Î¸ < 1 imply âˆ¥Î¸x + (1 âˆ’Î¸)yâˆ¥< 1.
(a) Show that âˆ¥Â· âˆ¥p is strictly convex for 1 < p < âˆ, but is not so
for p = 1, âˆ.

74
4. Norms
(b) Deduce from Corollary 5.5.1 that the induced norm âˆ¥Â· âˆ¥p is not
strictly convex on Mn(IR).
5. Let N be a norm on IRn.
(a) For x âˆˆCCn, deï¬ne
N1(x) := inf
!
l
|Î±l|N(xl)
"
,
where the inï¬mum is taken over the set of decompositions x =

l Î±lxl with Î±l âˆˆCC and xl âˆˆIRn. Show that N1 is a norm on
CCn (as a CC-vector space) whose restriction to IRn is N. Note:
N1 is called the complexiï¬cation of N.
(b) Same question as above for N2, deï¬ned by
N2(x) := 1
2Ï€
 2Ï€
0
[eiÎ¸x]dÎ¸,
where
[x] :=

N(â„œx)2 + N(â„‘x)2.
(c) Show that N2 â‰¤N1.
(d) If N(x) = âˆ¥xâˆ¥1, show that N1(x) = âˆ¥xâˆ¥1. Considering then the
vector
x =
 1
i

,
show that N2 Ì¸= N1.
6. (continuation of exercise 5)
The norms N (on IRn) and N1 (on CCn) lead to induced norms on
Mn(IR) and Mn(CC), respectively. Show that if M âˆˆMn(IR), then
N(M) = N1(M). Deduce that Theorem 4.3.1 holds true in Mn(IR).
7. Let âˆ¥Â· âˆ¥be an algebra norm on Mn(K) (K = IR or CC), that is, a
norm satisfying âˆ¥ABâˆ¥â‰¤âˆ¥Aâˆ¥Â· âˆ¥Bâˆ¥. Show that Ï(A) â‰¤âˆ¥Aâˆ¥for every
A âˆˆMn(K).
8. In Mn(CC), let D be a diagonalizable matrix and N a nilpotent matrix
that commutes with D. Show that Ï(D) = Ï(D + N).
9. Let B âˆˆMn(CC) be given. Assume that there exists an induced norm
such that âˆ¥Bâˆ¥= Ï(B). Let Î» be an eigenvalue of maximal modulus
and X a corresponding eigenvector. Show that X does not belong to
the range of B âˆ’Î»In. Deduce that the Jordan block associated to Î»
is diagonal (Jordan reduction is presented in Chapter 6).
10. (continuation of exercise 9)

4.6. Exercises
75
Conversely, show that if the Jordan blocks of B associated to the
eigenvalues of maximal modulus of B are diagonal, then there exists
a norm on CCn such that, using the induced norm, Ï(B) = âˆ¥Bâˆ¥.
11. Here is another proof of Theorem 4.2.1. Let K = IR or CC, A âˆˆ
Mn(K), and let N be a norm on Kn. If Ïµ > 0, we deï¬ne for all
x âˆˆKn
âˆ¥xâˆ¥:=

kâˆˆIN
(Ï(A) + Ïµ)âˆ’kN(Akx).
(a) Show that this series is convergent (use Corollary 4.4.1).
(b) Show that âˆ¥Â· âˆ¥is a norm on Kn.
(c) Show that for the induced norm, âˆ¥Aâˆ¥â‰¤Ï(A) + Ïµ.
12. A matrix norm âˆ¥Â· âˆ¥on Mn(CC) is said to be unitarily invariant if
âˆ¥UAV âˆ¥= âˆ¥Aâˆ¥for every A âˆˆMn(CC) and all unitary matrices U, V .
(a) Find, among the most classical norms, two examples of unitarily
invariant norms.
(b) Given a unitarily invariant norm, show that there exists a norm
N on IRn such that
âˆ¥Aâˆ¥= N(s1(A), . . . , sn(A)),
where the sj(A)â€™s, the eigenvalues of H in the polar decompo-
sition A = QH (see Chapter 7 for this notion), are called the
singular values of A.
13. (R. Bhatia [5]) Suppose we are given a norm âˆ¥Â· âˆ¥on Mn(CC) that
is unitarily invariant (see the previous exercise). If A âˆˆMn(CC), we
denote by D(A) the diagonal matrix obtained by keeping only the
ajj and setting all the other entries to zero. If Ïƒ is a permutation,
we denote by AÏƒ the matrix whose entry of index (j, k) equals ajk if
k = Ïƒ(j), and zero otherwise. For example, Aid = D(A), where id is
the identity permutation. If r is an integer between 1 âˆ’n and n âˆ’1,
we denote by Dr(A) the matrix whose entry of index (j, k) equals ajk
if k âˆ’j = r, and zero otherwise. For example, D0(A) = D(A).
(a) Let Ï‰ = exp(2iÏ€/n) and let U be the diagonal matrix whose
diagonal entries are the roots of unity 1, Ï‰, . . . , Ï‰nâˆ’1. Show that
D(A) = 1
n
nâˆ’1

j=0
U âˆ—jAU j.
Deduce that âˆ¥D(A)âˆ¥â‰¤âˆ¥Aâˆ¥.
(b) Show that âˆ¥AÏƒâˆ¥â‰¤âˆ¥Aâˆ¥for every Ïƒ âˆˆSn. Observe that âˆ¥Pâˆ¥=
âˆ¥Inâˆ¥for every permutation matrix P. Show that âˆ¥Mâˆ¥â‰¤âˆ¥Inâˆ¥
for every bistochastic matrix M (see Section 5.5 for this notion).

76
4. Norms
(c) If Î¸ âˆˆIR, let us denote by UÎ¸ the diagonal matrix, whose kth
diagonal term equals exp(ikÎ¸). Show that
Dr(A) = 1
2Ï€
 2Ï€
0
eirÎ¸UÎ¸AU âˆ—
Î¸ dÎ¸.
(d) Deduce that âˆ¥Dr(A)âˆ¥â‰¤âˆ¥Aâˆ¥.
(e) Let p be an integer between zero and n âˆ’1 and r = 2p + 1. Let
us denote by Tr(A) the matrix whose entry of index (j, k) equals
ajk if |k âˆ’j| â‰¤p, and zero otherwise. For example, T3(A) is a
tridiagonal matrix. Show that
Tr(A) = 1
2Ï€
 2Ï€
0
dp(Î¸)UÎ¸AU âˆ—
Î¸ dÎ¸,
where
dp(Î¸) =
p

âˆ’p
eikÎ¸
is the Dirichlet kernel.
(f) Deduce that âˆ¥Tr(A)âˆ¥â‰¤Lpâˆ¥Aâˆ¥, where
Lp = 1
2Ï€
 2Ï€
0
|dp(Î¸)|dÎ¸
is the Lebesgue constant (note: Lp = 4Ï€âˆ’2 log p + O(1)).
(g) Let âˆ†(A) be the upper triangular matrix whose entries above
the diagonal coincide with those of A. Using the matrix
B =

0
âˆ†(A)âˆ—
âˆ†(A)
0

,
show that âˆ¥âˆ†(A)âˆ¥2 â‰¤Lnâˆ¥Aâˆ¥2 (observe that âˆ¥Bâˆ¥2 = âˆ¥âˆ†(A)âˆ¥2).
(h) What inequality do we obtain for âˆ†0(A), the strictly upper tri-
angular matrix whose entries lying strictly above the diagonal
coincide with those of A?
14. We endow CCn with the usual Hermitian structure, so that Mn(CC) is
equipped with the norm âˆ¥Aâˆ¥= Ï(Aâˆ—A)1/2.
Suppose we are given a sequence of matrices (Aj)jâˆˆZZ in Mn(CC) and
a summable sequence Î³ âˆˆl1(ZZ) of positive real numbers. Assume,
ï¬nally, that for every pair (j, k) âˆˆZZ Ã— ZZ,
âˆ¥Aâˆ—
jAkâˆ¥â‰¤Î³(j âˆ’k)2,
âˆ¥AjAâˆ—
kâˆ¥â‰¤Î³(j âˆ’k)2.
(a) Let F be a ï¬nite subset of ZZ. Let BF denote the sum of the
Ajâ€™s as j runs over F. Show that
âˆ¥(Bâˆ—
F BF )2mâˆ¥â‰¤card F âˆ¥Î³âˆ¥2m
1 ,
âˆ€m âˆˆIN.
(b) Deduce that âˆ¥BF âˆ¥â‰¤âˆ¥Î³âˆ¥1.

4.6. Exercises
77
(c) Show (Cotlarâ€™s lemma) that for every x, y âˆˆCCn, the series
yT 
jâˆˆZZ
Ajx
is convergent, and that its sum yTAx deï¬nes a matrix A âˆˆ
Mn(CC) that satisï¬es
âˆ¥Aâˆ¥â‰¤

jâˆˆZZ
Î³(j).
Hint: For a sequence (uj)jâˆˆZZ of real numbers, the series 
j uj
is absolutely convergent if and only if there exists M < +âˆ
such that 
jâˆˆF |uj| â‰¤M for every ï¬nite subset F.
(d) Deduce that the series 
j Aj converges in Mn(CC). May one
conclude that it converges normally?
15. Let âˆ¥Â· âˆ¥be an induced norm on Mn(IR). We wish to characterize the
matrices B âˆˆMn(IR) such that there exist Ïµ0 > 0 and Ï‰ > 0 with
(0 < Ïµ < Ïµ0) =â‡’(âˆ¥In âˆ’ÏµBâˆ¥â‰¤1 âˆ’Ï‰Ïµ).
(a) For the norm âˆ¥Â·âˆ¥âˆ, it is equivalent that B be strictly diagonally
dominant.
(b) What is the characterization for the norm âˆ¥Â· âˆ¥1?
(c) For the norm âˆ¥Â· âˆ¥2, it is equivalent that BT + B be positive
deï¬nite.
16. If A âˆˆMn(CC) and j = 1, . . . , n are given, we deï¬ne rj(A) :=

kÌ¸=j |ajk|. For i Ì¸= j, deï¬ne
Bij(A) = {z âˆˆCC ; |(z âˆ’aii)(z âˆ’ajj)| â‰¤ri(A)rj(A)}.
These sets are Cassini ovals. Finally, let
B(A) := âˆª1â‰¤i<jâ‰¤nBij(A).
(a) Show that Sp A âŠ‚B(A).
(b) Show that this result is sharper than Proposition 4.5.1.
(c) When n = 2, show that in fact Sp A is included in the boundary
of B(A).
17. Let B âˆˆMn(CC).
(a) Returning to the proof of Theorem 4.2.1, show that for every
Ïµ > 0 there exists on CCn a Hermitian norm âˆ¥Â· âˆ¥such that for
the induced norm âˆ¥Bâˆ¥â‰¤Ï(B) + Ïµ.
(b) Deduce that Ï(B) < 1 holds if and only if there exists a matrix
A âˆˆHPDn such that A âˆ’Bâˆ—AB âˆˆHPDn.
18. For A âˆˆMn(CC), deï¬ne
Ïµ := max
iÌ¸=j |aij|,
Î´ := min
iÌ¸=j |aii âˆ’ajj|.

78
4. Norms
We assume in this exercise that Î´ > 0 and Ïµ â‰¤Î´/4n.
(a) Show that each Gershgorin disk Dj
contains exactly one
eigenvalue of A.
(b) Let Ï > 0 be a real number. Show that AÏ, obtained by multi-
plying the ith row of A by Ï and the ith column by 1/Ï, has the
same eigenvalues as A.
(c) Choose Ï = 2Ïµ/Î´. Show that the ith Gershgorin disk of AÏ con-
tains exactly one eigenvalue. Deduce that the eigenvalues of A
are simple and that
d(Sp(A), diag(A)) â‰¤2nÏµ2
Î´
,
where diag(A) = {a11, . . . , ann}.
19. Let A âˆˆMn(CC) be a diagonalizable matrix:
A = S diag(d1, . . . , dn)Sâˆ’1.
Let âˆ¥Â· âˆ¥be an induced norm for which âˆ¥Dâˆ¥= maxj |dj| holds, where
D := diag(d1, . . . , dn). Show that for every E âˆˆMn(CC) and for every
eigenvalue Î» of A + E, there exists an index j such that
|Î» âˆ’dj| â‰¤âˆ¥Sâˆ¥Â· âˆ¥Sâˆ’1âˆ¥Â· âˆ¥Eâˆ¥.
20. Let A âˆˆMn(K), with K = IR or CC. Give another proof, using
the Cauchyâ€“Schwarz inequality, of the following particular case of
Theorem 4.3.1:
âˆ¥Aâˆ¥2 â‰¤âˆ¥Aâˆ¥1/2
1
âˆ¥Aâˆ¥1/2
âˆ.
21. Show that if A âˆˆMn(CC) is normal, then Ï(A) = âˆ¥Aâˆ¥2. Deduce that
if A and B are normal, Ï(AB) â‰¤Ï(A)Ï(B).
22. Let N1 and N2 be two norms on CCn. Denote by N1 and N2 the
induced norms on Mn(CC). Let us deï¬ne
R := max
xÌ¸=0
N1(x)
N2(x),
S := max
xÌ¸=0
N2(x)
N1(x).
(a) Show that
max
AÌ¸=0
N1(A)
N2(A) = RS = max
AÌ¸=0
N2(A)
N1(A).
(b) Deduce that if N1 = N2, then N2/N1 is constant.
(c) Show that if N1 â‰¤N2, then N2/N1 is constant and therefore
N2 = N1.
23. (continuation of exercise 22)
Let âˆ¥Â· âˆ¥be an algebra norm on Mn(CC). If y âˆˆCCn is nonzero, we
deï¬ne âˆ¥xâˆ¥y := âˆ¥xyâˆ—âˆ¥.

4.6. Exercises
79
(a) Show that âˆ¥Â· âˆ¥y is a norm on CCn for every y Ì¸= 0.
(b) Let Ny be the norm induced by âˆ¥Â· âˆ¥y. Show that Ny â‰¤âˆ¥Â· âˆ¥.
(c) We say that âˆ¥Â·âˆ¥is minimal if there exists no other algebra norm
less than or equal to âˆ¥Â·âˆ¥. Show that the following assertions are
equivalent:
i. âˆ¥Â· âˆ¥is an induced norm on Mn(CC).
ii. âˆ¥Â· âˆ¥is a minimal norm on Mn(CC).
iii. For all y Ì¸= 0, one has âˆ¥Â· âˆ¥= Ny.
24. (continuation of exercise 23)
Let âˆ¥Â· âˆ¥be an induced norm on Mn(CC).
(a) Let y, z Ì¸= 0 be two vectors in CCn. Show that (with the notation
of the previous exercise) âˆ¥Â· âˆ¥y/âˆ¥Â· âˆ¥z is constant.
(b) Prove the equality
âˆ¥xyâˆ—âˆ¥Â· âˆ¥ztâˆ—âˆ¥= âˆ¥xtâˆ—âˆ¥Â· âˆ¥zyâˆ—âˆ¥.
25. Let M âˆˆMn(CC) and H âˆˆHPDn be given. Show that
âˆ¥HMHâˆ¥2 â‰¤1
2âˆ¥H2M + MH2âˆ¥2.
26. We endow IR2 with the Euclidean norm âˆ¥Â· âˆ¥2, and M2(IR) with the
induced norm, denoted also by âˆ¥Â·âˆ¥2. We denote by Î£ the unit sphere of
M2(IR): M âˆˆÎ£ is equivalent to âˆ¥Mâˆ¥2 = 1, that is, to Ï(M T M) = 1.
Similarly, B denotes the unit ball of M2(IR).
Recall that if C is a convex set and if P âˆˆC, then P is called an
extremal point if P âˆˆ[Q, R] and Q, R âˆˆC imply Q = R = P.
(a) Show that the set of extremal points of B is equal to O2(IR).
(b) Show that M âˆˆÎ£ if and only if there exist two matrices P, Q âˆˆ
O2(IR) and a number a âˆˆ[0, 1] such that
M = P
 a
0
0
1

Q.
(c) We denote by R = SO2(IR) the set of rotation matrices, and
by S that of matrices of planar symmetry. Recall that O2(IR) is
the disjoint union of R and S. Show that Î£ is the union of the
segments [r, s] as r runs over R and s runs over S.
(d) Show that two such â€œopenâ€ segments (r, s) and (râ€², sâ€²) are either
disjoint or equal.
(e) Let M, N âˆˆÎ£. Show that âˆ¥M âˆ’Nâˆ¥2 = 2 (that is, (M, N) is a
diameter of B) if and only if there exists a segment [r, s] (r âˆˆR
and s âˆˆS) such that M âˆˆ[r, s] and N âˆˆ[âˆ’r, âˆ’s].

5
Nonnegative Matrices
In this chapter matrices have real entries in general. In a few speciï¬ed cases,
entries might be complex.
5.1
Nonnegative Vectors and Matrices
Deï¬nition 5.1.1 A vector x âˆˆIRn is nonnegative, and we write x â‰¥0,
if its coordinates are nonnegative. It is positive, and we write x > 0, if its
coordinates are (strictly) positive. Furthermore, a matrix A âˆˆMnÃ—m(IR)
(not necessarily square) is nonnegative (respectively positive) if its entries
are nonnegative (respectively positive); we again write A â‰¥0 (respectively
A > 0). More generally, we deï¬ne an order relationship x â‰¤y whose
meaning is y âˆ’x â‰¥0.
Deï¬nition 5.1.2 Given x âˆˆCCn, we let |x| denote the nonnegative vector
whose coordinates are the numbers |xj|. Similarly, if A âˆˆMn(CC), the
matrix |A| has entries |aij|.
Observe that given a matrix and a vector (or two matrices), the triangle
inequality implies
|Ax| â‰¤|A| Â· |x|.
Proposition 5.1.1 A matrix is nonnegative if and only if x â‰¥0 implies
Ax â‰¥0. It is positive if and only if x â‰¥0 and x Ì¸= 0 imply Ax > 0.
Proof

5.2. The Perronâ€“Frobenius Theorem: Weak Form
81
Let us assume that Ax â‰¥0 (respectively > 0) for every x â‰¥0 (respec-
tively â‰¥0 and Ì¸= 0). Then the ith column A(i) is nonnegative (respectively
positive), since it is the image of the ith vector of the canonical basis. Hence
A â‰¥0 (respectively > 0).
Conversely, A â‰¥0 and x â‰¥0 imply trivially Ax â‰¥0. If A > 0, x â‰¥0,
and x Ì¸= 0, there exists an index l such that xl > 0. Then
(Ax)i =

j
aijxj â‰¥ailxl > 0,
and hence Ax > 0.
An important point is the following:
Proposition 5.1.2 If A âˆˆMn(IR) is nonnegative and irreducible, then
(I + A)nâˆ’1 > 0.
Proof
Let x be a nonnegative, nonzero vector and deï¬ne xm = (I + A)mx,
which is nonnegative. Let us denote by Pm the set of indices of the nonzero
components of xm: P0 is nonempty. Since xm+1
i
â‰¥xm
i , one has Pm âŠ‚
Pm+1. Let us assume that the cardinality |Pm| of Pm is strictly less than
n. There are thus one or more zero components, whose indices form a
nonempty subset I, complement of Pm. Since A is irreducible, there exists
some nonzero entry aij, with i âˆˆI and j âˆˆPm. Then xm+1
i
â‰¥aijxm
j > 0,
which shows that Pm+1 is not equal to Pm, and thus |Pm+1| > |Pm|. By
induction, we deduce that |Pm| â‰¥min{m + 1, n}. Hence |Pnâˆ’1| = n.
5.2
The Perronâ€“Frobenius Theorem: Weak Form
Theorem 5.2.1 Let A âˆˆMn(IR) be a nonnegative matrix. Then Ï(A) is
an eigenvalue of A associated to a nonnegative eigenvector.
Proof
Let Î» be an eigenvalue of maximal modulus and v an eigenvector,
normalized by âˆ¥vâˆ¥1 = 1. Then
Ï(A)|v| = |Î»v| = |Av| â‰¤A|v|.
Let us denote by C the subset of IRn (actually a subset of the unit simplex
Kn) deï¬ned by the (in)equalities 
i xi = 1, x â‰¥0, and Ax â‰¥Ï(A)x. This
is a closed convex set, nonempty, since it contains |v|. Finally, it is bounded,
because x âˆˆC implies 0 â‰¤xj â‰¤1 for every j; thus it is compact. Let us
distinguish two cases:
1. There exists x âˆˆC such that Ax = 0. Then Ï(A)x â‰¤0 furnishes
Ï(A) = 0. The theorem is thus proved in this case.

82
5. Nonnegative Matrices
2. For every x in C, Ax Ì¸= 0. Then let us deï¬ne on C a continuous map
f by
f(x) =
1
âˆ¥Axâˆ¥1
Ax.
It is clear that f(x) â‰¥0 and that âˆ¥f(x)âˆ¥1 = 1. Finally,
Af(x) =
1
âˆ¥Axâˆ¥1
AAx â‰¥
1
âˆ¥Axâˆ¥1
AÏ(A)x = Ï(A)f(x),
so that f(C) âŠ‚C. Then Brouwerâ€™s theorem (see [3], p. 217) asserts
that a continuous function from a compact convex subset of IRN
into itself has a ï¬xed point. Thus let y be a ï¬xed point of f. It is
a nonnegative eigenvector, associated to the eigenvalue r = âˆ¥Ayâˆ¥1.
Since y âˆˆC, we have ry = Ay â‰¥Ï(A)y and thus r â‰¥Ï(A), which
implies r = Ï(A).
That proof can be adapted to the case where a real number r and a
nonzero vector y are given satisfying y â‰¥0 and Ay â‰¥ry. Just take for C
the set of vectors x such that 
i xi = 1, x â‰¥0, and Ax â‰¥rx. We then
conclude that Ï(A) â‰¥r.
5.3
The Perronâ€“Frobenius Theorem: Strong Form
Theorem 5.3.1 Let A âˆˆMn(IR) be a nonnegative irreducible matrix.
Then Ï(A) is a simple eigenvalue of A, associated to a positive eigenvector.
Moreover, Ï(A) > 0.
5.3.1
Remarks
1. Though the Perronâ€“Frobenius theorem says that Ï(A) is a simple
eigenvalue, it does not tell anything about the other eigenvalues
of maximal modulus. The following example shows that such other
eigenvalues may exist:
 0
1
1
0

.
The existence of several eigenvalues of maximal modulus will be
studied in Section 5.4.
2. One obtains another proof of the weak form of the Perronâ€“Frobenius
theorem by applying the strong form to A + Î±J, where J > 0 and
Î± > 0, then letting Î± tend to zero.

5.3. The Perronâ€“Frobenius Theorem: Strong Form
83
3. Without the irreducibility assumption, Ï(A) may be a multiple eigen-
value, and a nonnegative eigenvector may not be positive. This holds
for a matrix of size n = 2m that reads blockwise
A =
 B
0m
Im
B

.
Here, Ï(A) = Ï(B), and every eigenvalue has an even algebraic mul-
tiplicity. Moreover, if Ï(B) is a simple eigenvalue of B, associated to
the eigenvector Z â‰¥0, then the kernel of A âˆ’Ï(A)In is spanned by
X =
 0m
Z

,
which is not positive.
Proof
For r â‰¥0, we denote by Cr the set of vectors of IRn deï¬ned by the
(in)equalities
x â‰¥0,
âˆ¥xâˆ¥1 = 1,
Ax â‰¥rx.
Each Cr is a convex compact set. We saw in the previous section that if Î»
is an eigenvalue associated to an eigenvector x of unit norm âˆ¥xâˆ¥1 = 1, then
|x| âˆˆC|Î»|. In particular, CÏ(A) is nonempty. Conversely, if Cr is nonempty,
then for x âˆˆCr,
r = râˆ¥xâˆ¥1 â‰¤âˆ¥Axâˆ¥1 â‰¤âˆ¥Aâˆ¥1âˆ¥xâˆ¥1 = âˆ¥Aâˆ¥1,
and therefore r â‰¤âˆ¥Aâˆ¥1. Furthermore, the map r â†’Cr is nonincreasing
with respect to inclusion, and is â€œleft continuousâ€ in the following sense. If
r > 0, one has
Cr = âˆ©s<rCs.
Let us then deï¬ne
R = sup{r | Cr Ì¸= âˆ…},
so that R âˆˆ[Ï(A), âˆ¥Aâˆ¥1]. The monotonicity with respect to inclusion shows
that r < R implies Cr Ì¸= âˆ….
If x > 0 and âˆ¥xâˆ¥1 = 1, then Ax â‰¥0 and Ax Ì¸= 0, since A is nonnegative
and irreducible. From Lemma 5.3.1 it follows that R > 0. The set CR, being
the intersection of a totally ordered family of nonempty compacts sets, is
nonempty.
Let x âˆˆCR. Lemma 5.3.1 below shows that x is an eigenvector of A
associated to the eigenvalue R. We observe that this eigenvalue is not less
than Ï(A) and infer that Ï(A) = R. Hence Ï(A) is an eigenvalue associated
to the eigenvector x, and Ï(A) > 0. Lemma 5.3.2 below ensures that x > 0.
The proof of the simplicity of the eigenvalue Ï(A) will be given in Section
5.3.3.

84
5. Nonnegative Matrices
5.3.2
A Few Lemmas
Lemma 5.3.1 Let r â‰¥0 and x â‰¥0 such that Ax â‰¥rx and Ax Ì¸= rx.
Then there exists râ€² > r such that Crâ€² is nonempty.
Proof
Let y := (In + A)nâˆ’1x. Since A is irreducible and x â‰¥0 is nonzero, one
has y > 0. Similarly, Ay âˆ’ry = (In + A)nâˆ’1(Ax âˆ’rx) > 0. Let us deï¬ne
râ€² := minj(Ay)j/yj, which is strictly larger than r. We then have Ay â‰¥râ€²y,
so that Crâ€² contains the vector y/âˆ¥yâˆ¥1.
Lemma 5.3.2 The nonnegative eigenvectors of A are positive.
Proof
Given such a vector x with Ax = Î»x, we observe that Î» âˆˆIR+. Then
x =
1
(1 + Î»)nâˆ’1 (In + A)nâˆ’1x,
and the right-hand side is strictly positive, from Proposition 5.1.2.
Finally, we can state the following result.
Lemma 5.3.3 Let A, B âˆˆMn(CC) be matrices, with A irreducible and
|B| â‰¤A. Then Ï(B) â‰¤Ï(A).
In case of equality (Ï(B) = Ï(A)), the following hold:
â€¢ |B| = A;
â€¢ for every eigenvector x of B associated to an eigenvalue of modulus
Ï(A), |x| is an eigenvector of A associated to Ï(A).
Proof
In order to establish the inequality, we proceed as above. If Î» is an
eigenvalue of B, of modulus Ï(B), and if x is a normalized eigenvector,
then Ï(B)|x| â‰¤|B| Â· |x| â‰¤A|x|, so that CÏ(B) is nonempty. Hence Ï(B) â‰¤
R = Ï(A).
Let us investigate the case of equality. If Ï(B) = Ï(A), then |x| âˆˆCÏ(A),
and therefore |x| is an eigenvector: A|x| = Ï(A)|x| = Ï(B)|x| â‰¤|B| Â· |x|.
Hence, (Aâˆ’|B|)|x| â‰¤0. Since |x| > 0 (from Lemma 5.3.2) and Aâˆ’|B| â‰¥0,
this gives |B| = A.
5.3.3
The Eigenvalue Ï(A) Is Simple
Let PA(X) be the characteristic polynomial of A. It is given as the compo-
sition of an n-linear form (the determinant) with polynomial vector-valued
functions (the columns of XInâˆ’A). If Ï† is p-linear and if V1(X), . . . , Vp(X)

5.4. Cyclic Matrices
85
are polynomial vector-valued functions, then the polynomial P(X) :=
Ï†(V1(X), . . . , Vp(X)) has the derivative
P â€²(X) = Ï†(V â€²
1, V2, . . . , Vp) + Ï†(V1, V â€²
2, . . . , Vp) + Â· Â· Â· + Ï†(V1, . . . , Vpâˆ’1, V â€²
p).
One therefore has
P â€²
A(X)
=
det(e1, a2, . . . , an) + det(a1, e2, . . . , an) + Â· Â· Â·
+ Â· Â· Â· + det(a1, . . . , anâˆ’1, en),
where aj is the jth column of XIn âˆ’A and {e1, . . . , en} is the canonical
basis of IRn. Developping the jth determinant with respect to the jth
column, one obtains
P â€²
A(X) =
n

j=1
PAj(X),
(5.1)
where Aj âˆˆMnâˆ’1(IR) is obtained from A by deleting the jth row and
the jth column. Let us now denote by Bj âˆˆMn(IR) the matrix obtained
from A by replacing the entries of the jth row and column by zeroes. This
matrix is block-diagonal, the two diagonal blocks being Aj âˆˆMnâˆ’1(IR) and
0 âˆˆM1(IR). Hence, the eigenvalues of Bj are those of Aj, together with
zero, and therefore Ï(Bj) = Ï(Aj). Furthermore, |Bj| â‰¤A, but |Bj| Ì¸= A
because A is irreducible and Bj is block-diagonal, hence reducible. It follows
(Lemma 5.3.3) that Ï(Bj) < Ï(A). Hence PAj(Ï(A)) is nonzero, with the
same sign as PAj in a neighborhood of +âˆ, which is positive. Finally,
P â€²
A(Ï(A)) is positive and Ï(A) is a simple root.
This completes the proof of Theorem 5.3.1. A diï¬€erent proof of the sim-
plicity and another proof of the Perronâ€“Frobenius theorem are given in
Exercises 2 and 4.
5.4
Cyclic Matrices
The following statement completes Theorem 5.3.1.
Theorem 5.4.1 Under the assumptions of Theorem 5.3.1, the set R(A) of
eigenvalues of A of maximal modulus Ï(A) is of the form R(A) = Ï(A)U p,
where Up is the group of pth roots of unity, where p is the cardinality of
R(A). Every such eigenvalue is simple. The spectrum of A is invariant un-
der multiplication by Up. Finally, A is similar, by means of a permutation
of coordinates in IRn, to the following cyclic form. In this cyclic matrix each
element is a block, and the diagonal blocks (which all vanish) are square

86
5. Nonnegative Matrices
with nonzero sizes:
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
M1
0
Â· Â· Â·
0
...
...
...
...
...
...
...
...
0
0
...
Mpâˆ’1
Mp
0
Â· Â· Â·
Â· Â· Â·
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Remarks:
â€¢ The converse is true. For example, the spectrum of a cyclic matrix is
stable under multiplication by exp(2iÏ€/p).
â€¢ One may show that p divides n âˆ’n0, where n0 is the multiplicity of
the zero eigenvalue.
â€¢ The nonzero eigenvalues of A are the pth roots of those of the matrix
M1M2 Â· Â· Â· Mp, which is square, though its factors might not be square.
Proof
Let us denote by X the unique nonnegative eigenvector of A normalized
by âˆ¥Xâˆ¥1 = 1. If Y is a unitary eigenvector, associated to an eigenvalue Âµ
of maximal modulus Ï(A), the inequality Ï(A)|Y | = |AY | â‰¤A|Y | im-
plies (Lemma 5.3.3) |Y | = X. Hence there is a diagonal matrix D =
diag(eiÎ±1, . . . , eiÎ±n) such that Y = DX. Let us deï¬ne a unimodular com-
plex number eiÎ³ = Âµ/Ï(A) and let B be the matrix eâˆ’iÎ³Dâˆ’1AD. One has
|B| = A and BX = X. For every j, one therefore has

n

k=1
bjkxk
 =
n

k=1
|bjk|xk.
Since X > 0, one deduces that B is real-valued and nonnegative; that is,
B = A. Hence Dâˆ’1AD = eiÎ³A. The spectrum of A is thus invariant under
multiplication by eiÎ³.
Let U = Ï(A)âˆ’1R(A), which is included in S1, the unit circle. The previ-
ous discussion shows that U is stable under multiplication. Since U is ï¬nite,
it follows that its elements are roots of unity. Since the inverse of a dth root
of unity is its own (d âˆ’1)th power, U is stable under inversion. Hence it is
a ï¬nite subgroup of S1; that is, it is Up, for a suitable p.
Let PA be the characteristic polynomial and let Ï‰ = exp(2iÏ€/p). One
may apply the ï¬rst part of the proof to Âµ = Ï‰Ï(A). One has thus Dâˆ’1AD =
Ï‰A, and it follows that PA(X) = Ï‰nPA(X/Ï‰). Therefore, multiplication by
Ï‰ sends eigenvalues to eigenvalues of the same multiplicities. In particular,
the eigenvalues of maximal modulus are simple.
Iterating the conjugation, one obtains Dâˆ’pADp = A. Let us set
Dp = diag(d1, . . . , dn).

5.5. Stochastic Matrices
87
One has thus dj = dk, provided that ajk Ì¸= 0. Since A is irreducible, one
can link any two indices j and k by a chain j0 = j, . . . , jr = k such that
ajsâˆ’1,js Ì¸= 0 for every s. It follows that dj = dk for every j, k. But since one
may choose Y1 = X1, that is, Î±1 = 0, one also has d1 = 1 and hence Dp =
In. The Î±j are thus pth roots of unity. With a conjugation by a permutation
matrix we may limit ourselves to the case where D has the block-diagonal
form diag(J0, Ï‰J1, . . . , Ï‰pâˆ’1Jpâˆ’1), where the Jl are identity matrices of
respective sizes n0, . . . , npâˆ’1. Decomposing A into blocks Alm of sizes nl Ã—
nm, one obtains Ï‰kAjk = Ï‰j+1Ajk directly from the conjugation identity.
Hence Ajk = 0 except for the pairs (j, k) of the form (0, 1), (1, 2), . . . , (p âˆ’
2, p âˆ’1), (p âˆ’1, 0). This is the announced cyclic form.
5.5
Stochastic Matrices
Deï¬nition 5.5.1 A matrix M âˆˆMn(IR) is said to be stochastic if M â‰¥0
and if for every i = 1, . . . , n, one has
n

j=1
mij = 1.
One says that M is bistochastic (or doubly stochastic) if both M and M T
are stochastic.
Denoting by e âˆˆIRn the vector all of whose coordinates equal one, one
sees that M is stochastic if and only if M â‰¥0 and Me = e. Moreover, M
is bistochastic if M â‰¥0, Me = e, and eT M = eT . If M is stochastic, one
has âˆ¥Mxâˆ¥âˆâ‰¤âˆ¥xâˆ¥âˆfor every x âˆˆCCn, and therefore Ï(M) â‰¤1. But since
Me = e, one has in fact Ï(M) = 1.
The stochastic matrices play an important role in the study of Markov
chains. A special case of a bistochastic matrix is a permutation matrix P(Ïƒ)
(Ïƒ âˆˆSn), whose entries are
pij = Î´j
Ïƒ(i).
The following theorem explains the role of permutation matrices.
Theorem 5.5.1 (Birkhoï¬€) A matrix M âˆˆMn(IR) is bistochastic if and
only if it is a center of mass (that is, a barycenter with nonnegative weights)
of permutation matrices.
The fact that a center of mass of permutation matrices is a doubly stochas-
tic matrix is obvious, since the set âˆ†n of doubly stochastic matrices is
convex. The interest of the theorem lies in the statement that if M âˆˆâˆ†n,
there exist permutation matrices P1, . . . , Pr and positive real numbers
Î±1, . . . , Î±r with Î±1 + Â· Â· Â· + Î±r = 1 such that M = Î±1P1 + Â· Â· Â· + Î±rPr.

88
5. Nonnegative Matrices
Let us recall that a point a of a convex set C is an extreme point if the
equality x = Î¸y + (1 âˆ’Î¸)z, with y, z âˆˆC and Î¸ âˆˆ(0, 1) implies y = z = x.
The Kreinâ€“Milman theorem (see [30], Theorem 3.23) says that a convex
compact subset of IRn is the convex hull, that is, the set of centers of mass,
of its extreme points. Since âˆ†n is closed and bounded, hence compact, it
is permissible to apply the Kreinâ€“Milman theorem.
Proof
To begin with, it is immediate that the permutation matrices are ex-
treme points of âˆ†n. From the Kreinâ€“Milman theorem, the proof amounts
to showing that there is no other extreme point in âˆ†n.
Let M âˆˆâˆ†n be given. If M is not a permutation matrix, there exists
an entry mi1j1 âˆˆ(0, 1). Since M is stochastic, there also exists j2 Ì¸= j1
such that mi1j2 âˆˆ(0, 1). Since M T is stochastic, there exists i2 Ì¸= i1
such that mi2j2 âˆˆ(0, 1). By this procedure one constructs a sequence
(j1, i1, j2, i2, . . . ) such that miljl âˆˆ(0, 1) and milâˆ’1jl âˆˆ(0, 1). Since the
set of indices is ï¬nite, it eventually happens that one of the indices (a row
index or a column index) is repeated.
Therefore, one can assume that the sequence (j1, i1, . . . , jr, ir, jr+1 = j1)
has the above property. Let us deï¬ne a matrix B âˆˆMn(IR) by biljl = 1,
biljl+1 = âˆ’1, bij = 0 otherwise. By construction, Be = 0 and eT B = 0. If
Î± âˆˆIR, one therefore has (M Â± Î±B)e = e and eT (M Â± Î±B) = eT . If Î± > 0
is small enough, M Â± Î±B turns out to be nonnegative. Finally, M + Î±B
and M âˆ’Î±B are bistochastic, and
M = 1
2(M âˆ’Î±B) + 1
2(M + Î±B).
Hence M is not an extreme point of âˆ†n.
Here is a nontrivial consequence (Stoer and Witzgall [32]):
Corollary 5.5.1 Let âˆ¥Â· âˆ¥be a norm on IRn, invariant under permutation
of the coordinates. Then âˆ¥Mâˆ¥= 1 for every bistochastic matrix (where by
abuse of notation we have used âˆ¥Â· âˆ¥for the induced norm on Mn(IR)).
Proof
To begin with, âˆ¥Pâˆ¥= 1 for every permutation matrix, by assumption.
Since the induced norm is convex (true for every norm), one deduces
from Birkhoï¬€â€™s theorem that âˆ¥Mâˆ¥â‰¤1 for every bistochastic matrix.
Furthermore, Me = e implies âˆ¥Mâˆ¥â‰¥âˆ¥Meâˆ¥/âˆ¥eâˆ¥= 1.
This result applies, for instance, to the norm âˆ¥Â·âˆ¥p, providing a nontrivial
convex set on which the map 1/p â†’log âˆ¥Mâˆ¥p is constant (compare with
Theorem 4.3.1).
The bistochastic matrices are intimately related to the relation â‰º(see
Section 3.4). In fact, we have the following theorem.

5.5. Stochastic Matrices
89
Theorem 5.5.2 A matrix A is bistochastic if and only if Ax â‰»x for every
x âˆˆIRn.
Proof
If A is bistochastic, then
âˆ¥Axâˆ¥1 â‰¤âˆ¥Aâˆ¥1âˆ¥xâˆ¥1 = âˆ¥xâˆ¥1,
since AT is stochastic. Since A is stochastic, Ae = e. Applying this inequal-
ity to x âˆ’te, one therefore has âˆ¥Ax âˆ’teâˆ¥1 â‰¤âˆ¥x âˆ’teâˆ¥1. Proposition 3.4.1
then shows that x â‰ºAx.
Conversely, let us assume that x â‰ºAx for every x âˆˆIRn. Choosing x as
the jth vector of the canonical basis, ej, the inequality s1(ej) â‰¤s1(Aej)
expresses that A is a nonnegative matrix, while sn(ej) = sn(Aej) yields
n

i=1
aij = 1.
(5.2)
One then chooses x = e. The inequality s1(e) â‰¤s1(Ae) expresses1 that
Ae â‰¥e. Finally, sn(e) = sn(Ae) and Ae â‰¥e give Ae = e. Hence, A is
bistochastic.
This statement is completed by the following.
Theorem 5.5.3 Let x, y âˆˆIRn. Then x â‰ºy if and only if there exists a
bistochastic matrix A such that y = Ax.
Proof
From the previous theorem, it is enough to show that if x â‰ºy, there exists
A, a bistochastic matrix, such that y = Ax. To do so, one applies Theorem
3.4.2: There exists a Hermitian matrix H whose diagonal and spectrum
are y and x, respectively. Let us diagonalize H by a unitary conjugation:
H = U âˆ—DU, with D = diag(x1, . . . , xn). Then y = Ax, where aij = |uij|2.
Since U is unitary, A is bistochastic.2
An important aspect of stochastic matrices is their action on the simplex
Kn :=
!
x âˆˆIRn ; x â‰¥0 and

i
xi = 1
"
.
It is clear that M T is stochastic if and only if M(Kn) is contained in Kn;
M is bistochastic if, moreover, Me = e.
Considered as a part of the aï¬ƒne subspace whose equation is 
i xi = 1,
Kn is a convex set with a nonempty interior. Its interior comprises those
points that satisfy x > 0. One denotes âˆ‚Kn the boundary of Kn. If x âˆˆKn,
1For another vector y, s1(y) â‰¤s1(Ay) does not imply Ay â‰¥y.
2This kind of bistochastic matrix is called orthostochastic.

90
5. Nonnegative Matrices
we denote by O(x) the set of indices i such that xi = 0, and by o(x)
its cardinality, in such a way that âˆ‚Kn comprises those points satisfying
o(x) â‰¥1. One always has mij = 0 for (i, j) âˆˆO(Mx) Ã— O(x)c, where Ic
denotes the complement of I in {1, . . . , n}.
Proposition 5.5.1 Let x âˆˆKn and M âˆˆâˆ†n be given. Then one has
o(Mx) â‰¤o(x).
Moreover, if o(Mx) = o(x), one has mij = 0 for every (i, j) âˆˆO(Mx)c Ã—
O(x).
Proof
Let us compute
o(x) âˆ’o(Mx) =
n

i=1

O(x)
mij âˆ’

O(Mx)
n

j=1
mij =

O(Mx)cÃ—O(x)
mij â‰¥0.
The case of equality is immediate.
We could have obtained the ï¬rst part of the proposition by applying
Theorem 5.5.2.
Corollary 5.5.2 Let I and J be two subsets of {1, . . . , n} and let M âˆˆâˆ†n
be a matrix satisfying mij = 0 for every (i, j) âˆˆI Ã— Jc. Then one has
|J| â‰¥|I|. If, moreover, |I| = |J|, then mij also vanishes for (i, j) âˆˆIc Ã— J.
Proof
It is suï¬ƒcient to choose x âˆˆKn with Jc = O(x) if J is nonempty. If J
is empty, the statement is obvious.
We shall denote by Sâˆ†n (S for strict) the set of doubly stochastic matri-
ces M for which the conditions |I| = |J| and mij = 0 for every (i, j) âˆˆIÃ—Jc
imply either I = âˆ…or I = {1, . . . , n}. These are also the matrices for which
x âˆˆâˆ‚Kn implies o(Mx) < o(x). This set does not contain permutation
matrices P, since these satisfy o(Px) = o(x) for every x âˆˆKn.
Let M âˆˆâˆ†n be given. A decomposition of M consists of two partitions
I1 âˆªÂ· Â· Â· âˆªIr and J1 âˆªÂ· Â· Â· âˆªJr of the set {1, . . . , n} such that
(i âˆˆIl, j âˆˆJm, l Ì¸= m) =â‡’mij = 0.
From Corollary 5.5.2, we have |Il| = |Jl| for every l. Eliminating empty
parts if necessary, we can always assume that none of the Ilâ€™s or Jlâ€™s is
empty. A decomposition of M furnishes a block structure, in which each
row-block has only one nonzero block, and the same for the column-blocks.
The blocks of indices Il Ã— Jl are themselves stochastic matrices. A matrix
of Sâˆ†n admits only the trivial decomposition r = 1, I1 = J1 = {1, . . . , n}.
If M admits two decompositions, one with the sets Il, Jl, 1 â‰¤l â‰¤r, the
other one with Iâ€²
l, Jâ€²
l, 1 â‰¤l â‰¤s, let us form the partitions âˆªl,mIâ€²â€²
lm and

5.6. Exercises
91
âˆªl,mJâ€²â€²
lm, with Iâ€²â€²
lm := Il âˆ©Iâ€²
m and Jâ€²â€²
lm := Jl âˆ©Jâ€²
m. If i âˆˆIâ€²â€²
lm and j âˆˆJâ€²â€²
pq,
with (l, m) Ì¸= (p, q), we have mij = 0. From Corollary 5.5.2, applied to
M and to its transposition, we have |Iâ€²â€²
lm| = |Jâ€²â€²
lm|. Eliminating the empty
parts, we obtain therefore a decomposition of M that is ï¬ner than the ï¬rst
two, in the sense of inclusion order: Each Il (or Iâ€²
l) is a union of some parts
of the form Iâ€²â€²
p .
Since the set of decompositions of M is ï¬nite, the previous argument
shows that there exists a ï¬nest one. We shall call it the canonical decom-
position of M. It is the only decomposition for which the blocks of indices
Il Ã— Jl are themselves of class Sâˆ†.
5.6
Exercises
1. We consider the following three properties for a matrix M âˆˆMn(IR).
P1 M is nonnegative.
P2 M T e = e, where e = (1, . . . , 1)T .
P3 âˆ¥Mâˆ¥1 â‰¤1.
(a) Show that P2 and P3 imply P1.
(b) Show that P2 and P1 imply P3.
(c) Does P1 and P3 imply P2?
2. Here is another proof of the simplicity of Ï(A) in the Perronâ€“
Frobenius theorem, which does not require Lemma 5.3.3.
(a) We assume that A is irreducible and nonnegative, and we denote
by x a positive eigenvector associated to the eigenvalue Ï(A). Let
K be the set of nonnegative eigenvectors y associated to Ï(A)
such that âˆ¥yâˆ¥1 = 1. Show that K is compact and convex.
(b) Show that the geometric multiplicity of Ï(A) equals 1 (Hint:
Otherwise, K would contain a vector with at least one zero
component.)
(c) Show that the algebraic multiplicity of Ï(A) equals 1 (Hint:
Otherwise, there would be a nonnegative vector y such that Ayâˆ’
Ï(A)y = x > 0.)
3. Let M âˆˆMn(IR) be either a strictly diagonally dominant, or an
irreducible strongly diagonally dominant, matrix. Assume that mjj >
0 for every j = 1, . . . , n and mij â‰¤0 otherwise. Show that M is
invertible and that the solution of Mx = b, when b â‰¥0, satisï¬es
x â‰¥0. Deduce that M âˆ’1 â‰¥0.
4. Here is another proof of Theorem 5.3.1, due to Perron himself. We
proceed by induction on the size n of the matrix. The statement is
obvious if n = 1. We therefore assume that it holds for matrices
of size n. We give ourselves an irreducible nonnegative matrix A âˆˆ

92
5. Nonnegative Matrices
Mn+1(IR), which we decompose blockwise as
A =

a
Î¾T
Î·
B

,
a âˆˆIR,
Î¾, Î· âˆˆIRn,
B âˆˆMn(IR).
(a) Applying the induction hypothesis to the matrix B + ÏµJ, where
Ïµ > 0 and J > 0 is a matrix, then letting Ïµ go to zero, show
that Ï(B) is an eigenvalue of B, associated to a nonnegative
eigenvector (this avoids the use of Theorem 5.2.1).
(b) Using the formula
(Î»In âˆ’B)âˆ’1 =
âˆ

k=1
Î»âˆ’kBkâˆ’1,
valid for Î» âˆˆ(Ï(B), +âˆ), deduce that the function h(Î») :=
Î»âˆ’aâˆ’Î¾T (Î»In âˆ’B)âˆ’1Î· is strictly increasing on this interval and
that on the same interval the vector x(Î») := (Î»In âˆ’B)âˆ’1Î· is
positive.
(c) Prove the relation PA(Î») = PB(Î»)h(Î») between the characteris-
tic polynomials.
(d) Deduce that the matrix A has one and only one eigenvalue in
(Ï(B), +âˆ), and that it is a simple one, associated to a positive
eigenvector. One denotes this eigenvalue by Î»0.
(e) Applying the previous results to AT , show that there exists â„“âˆˆ
IRn such that â„“> 0 and â„“T (A âˆ’Î»0In) = 0.
(f) Let Âµ be an eigenvalue of A, associated to an eigenvector X.
Show that (Î»0 âˆ’|Âµ|)â„“T |X| â‰¥0. Conclusion?
5. Let A âˆˆMn(IR) be a matrix satisfying aij â‰¥0 for every pair (i, j) of
distinct indices.
(a) Using the Exercise 3, show that
R(h; A) := (In âˆ’hA)âˆ’1 â‰¥0,
for h > 0 small enough.
(b) Deduce that exp(tA) â‰¥0 for every t > 0 (the exponential of
matrices is presented in Chapter 7). Consider Trotterâ€™s formula
exp tA =
lim
mâ†’+âˆR(t/m; A)m,
where exp is the exponential of square matrices, deï¬ned in
Chapter 7. Trotterâ€™s formula is justiï¬ed by the convergence (see
Exercise 10 in Chapter 7) of the implicit Euler method for the
diï¬€erential equation
dx
dt = Ax.
(5.3)
(c) Deduce that if x(0) â‰¥0, then the solution of (5.3) is nonnegative
for every nonnegative t.

5.6. Exercises
93
(d) Deduce also that
Ïƒ := sup{â„œÎ»; Î» âˆˆSp A}
is an eigenvalue of A.
6. Let A âˆˆMn(IR) be a matrix satisfying aij â‰¥0 for every pair (i, j) of
distinct indices.
(a) Let us deï¬ne
Ïƒ := sup{â„œÎ»; Î» âˆˆSp A}.
Among the eigenvalues of A whose real parts equal Ïƒ, let us
denote by Âµ the one with the largest imaginary part. Show that
for every positive large enough real number Ï„, Ï(A + Ï„In) =
|Âµ + Ï„|.
(b) Deduce that Âµ = Ïƒ = Ï(A) (apply Theorem 5.2.1).
7. Let B âˆˆMn(IR) be a matrix whose oï¬€-diagonal entries are positive
and such that the eigenvalues have strictly negative real parts. Show
that there exists a nonnegative diagonal matrix D such that Bâ€² :=
Dâˆ’1BD is strictly diagonally dominant, namely,
bâ€²
ii < âˆ’

jÌ¸=i
bâ€²
ij.
8. Let B âˆˆMn(IR) be a nonnegative matrix and
A :=
 B
0m
Im
B

.
(a) If an eigenvalue Î» of A is associated to a positive eigenvector,
show that there exists Âµ > Î» and Z > 0 such that BZ â‰¥ÂµZ.
Deduce that Î» < Ï(B).
(b) Deduce that A admits no strictly positive eigenvector (ï¬rst of
all, apply Theorem 5.2.1 to the matrix AT ).
9.
(a) Let B âˆˆMn(IR) be given, with Ï(B) = 1. Assume that the
eigenvalues of B of modulus one are (algebraically) simple. Show
that the sequence (Bm)mâ‰¥1 is bounded.
(b) Let M âˆˆMn(IR) be a nonnegative irreducible matrix, with
Ï(M) = 1. We denote by x and yT the left and right eigenvectors
for the eigenvalue 1 (Mx = x and yT M = yT ), normalized by
yT x = 1. We deï¬ne L := xyT and B = M âˆ’L.
i. Verify that Bâˆ’In is invertible. Determine the spectrum and
the invariant subspaces of B by means of those of M.
ii. Show that the sequence (Bm)mâ‰¥1 is bounded. Express M m
in terms of Bm.

94
5. Nonnegative Matrices
iii. Deduce that
lim
Nâ†’+âˆ
1
N
Nâˆ’1

m=0
M m = L.
iv. Under what additional assumption do we have the stronger
convergence
lim
Nâ†’+âˆM N = L?
10. Let B âˆˆMn(IR) be a nonnegative irreducible matrix and let C âˆˆ
Mn(IR) be a nonzero nonnegative matrix. For t > 0, we deï¬ne rt :=
Ï(B + tC) and we let Xt denote the nonnegative unitary eigenvector
associated to the eigenvalue rt.
(a) Show that t â†’rt is strictly increasing.
Deï¬ne r := limtâ†’+âˆrt. We wish to show that r = +âˆ. Let X
be a cluster point of the sequence Xt. We may assume, up to a
permutation of the indices, that
X =
 Y
0

,
Y > 0.
(b) Suppose that in fact, r < +âˆ. Show that BX â‰¤rX. Deduce
that Bâ€²Y = 0, where Bâ€² is a matrix extracted from B.
(c) Deduce that X = Y ; that is, X > 0.
(d) Show, ï¬nally, that CX = 0. Conclude that r = +âˆ.
(e) Assume, moreover, that Ï(B) < 1. Show that there exists one
and only one t âˆˆIR such that Ï(B + tC) = 1.
11. Show that âˆ†is stable under multiplication. In particular, if M is
bistochastic, the sequence (M m)mâ‰¥1 is bounded.
12. Let M âˆˆMn(IR) be a bistochastic irreducible matrix. Show that
lim
Nâ†’+âˆ
1
N
Nâˆ’1

m=0
M m = 1
n
ï£«
ï£¬
ï£­
1
. . .
1
...
...
1
. . .
1
ï£¶
ï£·
ï£¸=: Jn
(use Exercise 9). Show by an example that the sequence (M m)mâ‰¥1
may or may not converge.
13. Show directly that for every p âˆˆ[1, âˆ], âˆ¥Jnâˆ¥p = 1, where Jn was
deï¬ned in the previous exercise.
14. Let P âˆˆGLn(IR) be given such that P, P âˆ’1 âˆˆâˆ†n. Show that P is a
permutation matrix.
15. If M âˆˆâˆ†n is given, we deï¬ne an equivalence relation between in-
dices in the following way: iâ€²Riâ€²â€² if there exists a sequence i1 =
iâ€², j1, i2, j2, . . . , ip = iâ€²â€² such that mij > 0 each time that (i, j) is

5.6. Exercises
95
of the form (il, jl) or (il+1, jl) (compare with the proof of Theorem
5.5.1). Show that in the canonical decomposition of M, the Il are the
equivalence classes of R.
Deduce that the following matrix belongs to Sâˆ†n:
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1/2
1/2
0
Â· Â· Â·
0
1/2
0
1/2
...
...
0
1/2
...
...
0
...
...
...
0
1/2
0
Â· Â· Â·
0
1/2
1/2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
16. Let M âˆˆSâˆ†n and M â€² âˆˆâˆ†n be given. Show that MM â€², M â€²M âˆˆSâˆ†n.
17. If M âˆˆSâˆ†n, show that limNâ†’+âˆM N exists.
18. Consider the induced norm âˆ¥Â·âˆ¥p on Mn(CC). Let M be a bistochastic
matrix.
(a) Compute âˆ¥Mâˆ¥1 and âˆ¥Mâˆ¥âˆ.
(b) Show that âˆ¥Mâˆ¥â‰¥1 for every induced norm.
(c) Deduce from Theorem 4.3.1 that âˆ¥Mâˆ¥p = 1. To what extent is
this result diï¬€erent from Corollary 5.5.1?
19. Suppose that we are given three real symmetric matrices (or
Hermitian matrices) A, B, C = A + B.
(a) If t âˆˆ[0, 1] consider the matrix S(t) := A+tB, so that S(0) = A
and S(1) = C. Arrange the eigenvalues of S(t) in increasing
order Î»1(t) â‰¤Â· Â· Â· â‰¤Î»n(t). For each value of t there exists an
orthonormal eigenbasis {X1(t), . . . , Xn(t)}. We admit the fact
that it can be chosen continuously with respect to t, so that
t â†’Xj(t) is continuous with a piecewise continuous derivative.
Show that Î»â€²
j(t) = (BXj(t), Xj(t)).
(b) Let Î±j, Î²j, Î³j (j = 1, . . . , n) be the eigenvalues of A, B, C,
respectively. Deduce from part (a) that
Î³j âˆ’Î±j =
 1
0
(BXj(t), Xj(t)) dt.
(c) Let {Y1, . . . , Yn} be an orthonormal eigenbasis, relative to B.
Deï¬ne
Ïƒjk :=
 1
0
|(Xj(t), Yk)|2dt.
Show that the matrix Î£ := (Ïƒjk)1â‰¤j,kâ‰¤n is bistochastic.
(d) Show that Î³j âˆ’Î±j = 
k ÏƒjkÎ²k. Deduce (Lidskiiâ€™s theorem) that
the vector (Î³1 âˆ’Î±1, . . . , Î³n âˆ’Î±n) belongs to the convex hull of
the vectors obtained from the vector (Î²1, . . . , Î²n) by all possible
permutations of the coordinates.

96
5. Nonnegative Matrices
20. Let a âˆˆIRn be given, a = (a1, . . . , an).
(a) Show that
C(a) := {b âˆˆIRn | b â‰»a}
is a convex compact set. Characterize its extremal points.
(b) Show that
Y (a) := {M âˆˆSymn(IR) | Sp M â‰»a}
is a convex compact set. Characterize its extremal points.
(c) Deduce that Y (a) is the closed convex hull (actually the convex
hull) of the set
X(a) := {M âˆˆSymn(IR) | Sp M = a}.
(d) Set Î± = sn(a)/n and aâ€² := (Î±, . . . , Î±). Show that aâ€² âˆˆC(a), and
that b âˆˆC(a) =â‡’b â‰ºaâ€².
(e) Characterize the set
{M âˆˆSymn(IR) | Sp M â‰ºaâ€²}.

6
Matrices with Entries in a Principal
Ideal Domain; Jordan Reduction
6.1
Rings, Principal Ideal Domains
In this Chapter we consider commutative integral domains A (see Chapter
2). In particular, such a ring A can be embeded in its ï¬eld of fractions, which
is the quotient of A Ã— (A \ {0}) by the equivalence relation (a, b)R(c, d) â‡”
ad = bc. The embedding is the map a â†’(a, 1). In a ring A the set of
invertible elements is denoted by Aâˆ—. If a, b âˆˆA are such that b = ua with
u âˆˆAâˆ—, we say that a and b are associated, and we write a âˆ¼b, which
amounts to saying that aA = bA. If there exists c âˆˆA such that ac = b,
we say that a divides b and write a|b. Then the quotient c is unique and
is denoted by b/a. We say that b is a prime, or irreducible, element if the
equality b = ac implies that one of the factors is invertible.
An ideal I in a ring A is an additive subgroup of A such that A Â· I âŠ‚I:
a âˆˆA, x âˆˆI imply ax âˆˆI. For example, if b âˆˆA, the subset bA is an ideal,
denoted by (b). Ideals of the form (b) are called principal ideals.
6.1.1
Facts About Principal Ideal Domains
Deï¬nition 6.1.1 A commutative ring A is a principal ideal domain if
every ideal in A is principal: For every ideal I there exists a âˆˆA such that
I = (a).
A ï¬eld is a principal ideal domain that has only two ideals, (0) and (1).
The set ZZ of rational integers and the polynomial algebra over a ï¬eld k,

98
6. Matrices with Entries in a Principal Ideal Domain; Jordan Reduction
denoted by k[X], are also principal ideal domains. More generally, every
Euclidean domain is a principal ideal domain (see Proposition 6.1.3 below).
In a commutative integral domain one says that d is a greatest common
divisor (gcd) of a and b if d divides a and b, and if every common divisor
of a and b divides d. In other words, the set of common divisors of a and
b admits d as a greatest element. The gcd of a and b, whenever it exists,
is unique up to multiplication by an invertible element. We say that a
and b are coprime if all their common divisors are invertible; in that case,
gcd(a, b) = 1.
Proposition 6.1.1 In a principal ideal domain, every pair of elements has
a greatest common divisor. The gcd satisï¬es the BÂ´ezout identity: For every
a, b âˆˆA, there exist u, v âˆˆA such that
gcd(a, b) = ua + vb.
Such u and v are coprime.
Proof
Let A be a principal ideal domain. If a, b âˆˆA, the ideal I =: (a, b)
spanned by a and b, which is the set of elements of the form xa + yb,
x, y âˆˆA, is principal: I = (d), where d = gcd(a, b). Since a, b âˆˆI, d divides
a and b. Furthermore, d = ua + vb because d âˆˆI. If c divides a and b, then
c divides ua + vb; hence divides d, which happens to be a gcd of a and b.
If m divides u and v, then md|ua + vb; hence d = smd. If d Ì¸= 0, one has
sm = 1, which means that m âˆˆAâˆ—. Thus u and v are coprime. If d = 0,
then a = b = 0, and one may take u = v = 1, which are coprime.
Let us remark that a gcd of a and b is a generator of the ideal aA+bA. It
is thus nonunique. Every element associated to a gcd of a and b is another
gcd. In certain rings one can choose the gcd in a canonical way, such as
being positive in ZZ, or monic in k[X].
The gcd is associative: gcd(a, gcd(b, c)) = gcd(gcd(a, b), c). It is therefore
possible to speak of the gcd of an arbitrary ï¬nite subset of A. In the above
example we denote it by gcd(a, b, c). At our disposal is a generalized BÂ´ezout
formula: There exist elements u1, . . . , ur âˆˆA such that
gcd(a1, . . . , ar) = a1u1 + Â· Â· Â· + arur.
Deï¬nition 6.1.2 A ring A is Noetherian if every nondecreasing (for in-
clusion) sequence of ideals is constant beyond some index: I0 âŠ‚I1 âŠ‚Â· Â· Â· âŠ‚
Im âŠ‚Â· Â· Â· implies that there is an l such that Il = Il+1 = Â· Â· Â· .
Proposition 6.1.2 The principal ideal domains are Noetherian.
Observe that in the case of principal ideal domains the Noetherian property
means exactly that if a sequence a1, . . . of elements of A is such that every
element is divisible by the next one, then there exists an index J such that
the ajâ€™s are pairwise associated for every j â‰¥J.

6.1. Rings, Principal Ideal Domains
99
This property seems natural because it is shared by all the rings encoun-
tered in number theory. But the ring of entire holomorphic functions is not
Noetherian: Just take for an the function
z â†’
 n

k=1
(z âˆ’k)âˆ’1

sin 2Ï€z.
Proof
Let A be a principal ideal domain and let (Ij)jâ‰¥0 be a nondecreasing
sequence of ideals in A. Let I be their union. This sequence is nondecreasing
under inclusion, so that I is an ideal. Let a be a generator: I = (a). Then
a belongs to one of the ideals, say a âˆˆIk. Hence I âŠ‚Ik, which implies
Ij = I for j â‰¥k.
We remark that the proof works with slight changes if we know that
every ideal in A is spanned by a ï¬nite set. For example, the ring of poly-
nomials over a Noetherian ring is itself Noetherian: ZZ[X] and k[X, Y ] are
Noetherian rings.
The principal ideal domains are also factorial (a short term for unique
factorization domain): Every element of A admits a factorization consist-
ing of prime factors. This factorization is unique up to ambiguities, which
may be of three types: the order of factors, the presence of invertible ele-
ments, and the replacement of factors by associated ones. This property is
fundamental to the arithmetic in A.
6.1.2
Euclidean Domains
Deï¬nition 6.1.3 A Euclidean domain is a ring A endowed with a map
N : A â†’IN such that for every a, b âˆˆA with b Ì¸= 0, there exists a unique
pair (q, r) âˆˆA Ã— A such that a = qb + r with N(r) < N(b) (Euclidean
division).
A special case of Euclidean division occurs when b divides a. Then r = 0
and we conclude that N(b) > N(0) for every b Ì¸= 0.
Classical examples of Euclidean domains are the ring of the rational
integers ZZ, with N(a) = |a|, the ring k[X] of polynomials over a ï¬eld
k, with N(P) = 2deg P ,1 and the ring of Gaussian integers ZZ[âˆšâˆ’1], with
N(z) = |z|2. Observe that if b is nonzero, the Euclidean division of b by
itself shows that N(b) is positive. The function N is often called a norm,
though it does not resemble the norm on a real or complex vector space. In
practice, one may deï¬ne N(0) in a consistent way by 0 if b Ì¸= 0 =â‡’N(b) > 0
(case of ZZ and ZZ[âˆšâˆ’1]), and by âˆ’âˆotherwise (case of k[X]). With that
1One may take either N(P ) = 1 + deg P if P is nonzero, and N(0) = 0.

100
6. Matrices with Entries in a Principal Ideal Domain; Jordan Reduction
extension, the pair (q, r) in the deï¬nition is uniquely deï¬ned by a = bq + r
and N(r) < N(b).
Proposition 6.1.3 Euclidean domains are principal ideal domains.
Proof
Let I be an ideal of a Euclidean domain A. If I = (0), there is nothing
to show. Otherwise, let us select in I \ {0} an element a of minimal norm.
If b âˆˆI, the remainder r of the Euclidean division of b by a is an element
of I and satisï¬es N(r) < N(a). The minimality of N(a) implies r = 0, that
is, a|b. Finally, I = (a).
The converse of Proposition 6.1.3 is not true. For example, the quadratic
ring ZZ[
âˆš
14] is Euclidean, though not a principal ideal domain. More infor-
mation about rings of quadratic integers can be found in Cohnâ€™s monograph
[10].
6.1.3
Elementary Matrices
An elementary matrix of order n is a matrix of one of the following forms:
â€¢ The transposition matrices: If Ïƒ âˆˆSn, the matrix PÏƒ has entries
pij = Î´j
Ïƒ(i), where Î´ is the Kronecker symbol.
â€¢ The matrices In + aJik, for a âˆˆA and 1 â‰¤i Ì¸= k â‰¤n, with
(Jik)lm = Î´l
iÎ´m
k .
â€¢ The diagonal invertible matrices, that is, those whose diagonal entries
are invertible in A.
We observe that the inverse of an elementary matrix is again elementary.
For example, (In + aJik)(In âˆ’aJik) = In.
Theorem 6.1.1 A square invertible matrix of size n with entries in a
Euclidean domain A is a product of elementary matrices with entries in
A.
Proof
We shall prove the theorem for n = 2. The general case will be deduced
from that particular one and from the proof of Theorem 6.2.1 below, since
the matrices used in that proof are block-diagonal with 1 Ã— 1 and 2 Ã— 2
diagonal blocks.
Let
M =

a
a1
c
d


6.2. Invariant Factors of a Matrix
101
be given in SL2(A): we have ad âˆ’a1c âˆˆAâˆ—. If N(a) < N(a1), we multiply
M on the right by

0
1
1
0

.
We are now in the case N(a1) â‰¤N(a). Let a = a1q + a2 be the Euclidean
division of a by a1. Then
M

1
0
âˆ’q
1

=: M â€² =

a2
a1
Â·
d

.
Next, we have
M â€²

0
1
1
0

=: M1 =

a1
a2
Â·
Â·

,
with N(a2) < N(a1). We thus construct a sequence of matrices Mk of the
form

akâˆ’1
ak
Â·
Â·

,
with akâˆ’1 Ì¸= 0, each one the product of the previous one by elementary
matrices. Furthermore, N(ak) < N(akâˆ’1). From Proposition 6.1.2, this
sequence is ï¬nite, and there is a step for which ak = 0. The matrix Mk,
being triangular and invertible, has an invertible diagonal D. Then MkDâˆ’1
has the form
 1
0
Â·
1

,
which is an elementary matrix.
Again, the statement is false in a general principal ideal domain. Whether
GLn(A) equals the group spanned by elementary matrices is a diï¬ƒcult
question of Ktheory.
6.2
Invariant Factors of a Matrix
Theorem 6.2.1 Let M âˆˆMnÃ—m(A) be a matrix with entries in a principal
ideal domain. Then there exist two invertible matrices P âˆˆGLn(A), Q âˆˆ
GLm(A) and a quasi-diagonal matrix D âˆˆMnÃ—m(A) (that is, dij = 0 for
i Ì¸= j) such that:
â€¢ on the one hand, M = PDQ,
â€¢ on the other hand, d1|d2, . . . , di|di+1, . . . , where the dj are the
diagonal entries of D.

102
6. Matrices with Entries in a Principal Ideal Domain; Jordan Reduction
Furthermore, if M = P â€²Dâ€²Qâ€² is another decomposition with these two prop-
erties, the scalars dj and dâ€²
j are associated. Up to invertible elements, they
are thus unique.
Deï¬nition 6.2.1 For this reason, the scalars d1, . . . , dr (r = min(n, m))
are called the invariant factors of M.
Proof
Uniqueness: for k â‰¤r, let us denote by Dk(N) the gcd of minors of order
k of the matrix N. From Corollary 2.1.1, we have Dk(M) = Dk(D) =
Dk(Dâ€²). It is immediate that Dk(D) = d1 Â· Â· Â· dk (because the minors
of order k are either null, or products of k terms dj with distinct
subscripts), so that
d1 Â· Â· Â· dk = ukdâ€²
1 Â· Â· Â· dâ€²
k,
1 â‰¤k â‰¤r,
for some uk âˆˆAâˆ—. Hence, d1 and dâ€²
1 are associated. Since A is an
integral domain, we also have dâ€²
k = uâˆ’1
k ukâˆ’1dk. In other words, dk
and dâ€²
k are associated.
Existence: We see from the above that the djâ€™s are determined by the
equalities d1 Â· Â· Â· dj = Dj(M). In particular, d1 is the gcd of the entries
of M. Hence the ï¬rst step consists in ï¬nding a matrix M â€², equivalent
to M, such that mâ€²
11 is equal to this gcd.
To do so, we construct a sequence of equivalent matrices M (p), with M (0) =
M, such that m(p)
11 divides m(pâˆ’1)
11
. Given the matrix N := M (pâˆ’1), we
distinguish four cases:
1. n11 divides n11, . . . , n1,jâˆ’1, but does not divide n1j. Then d :=
gcd(n11, n1j) reads d = un11 + vn1j. Let us deï¬ne w := âˆ’n1j/d
and z := n11/d and let us deï¬ne a matrix Q âˆˆGLm(A) by:
â€¢ q11 = u, qj1 = v, q1j = w, qjj = z,
â€¢ qkl = Î´l
k, otherwise.
Then M (p) := M (pâˆ’1)Q is suitable, because m(p)
11 = d|n11 = m(pâˆ’1)
11
.
2. n11 divides each n1j, as well as n11, . . . , niâˆ’1,1, but does not divide
ni1. This case is symmetric to the previous one. Multiplication on
the right by a suitable P âˆˆGLn(A) furnishes M (p), with m(p)
11 =
gcd(n11, ni1)|m(pâˆ’1)
11
.
3. n11 divides each n1j and each ni1, but does not divide some nij with
i, j â‰¥2. Then ni1 = an11. Let us deï¬ne a matrix P âˆˆGLn(A) by
â€¢ p11 = a + 1, pi1 = 1, p1i = âˆ’1, pii = 0;
â€¢ pkl = Î´l
k, otherwise;
If we then set N â€² = PN, we have nâ€²
11 = n11 and nâ€²
1j = (a+1)n1jâˆ’nij.
We have thus returned to the ï¬rst case, and there exists an equiv-

6.2. Invariant Factors of a Matrix
103
alent matrix M (p), with m(p)
11 = gcd(nâ€²
11, nâ€²
1j) = gcd(n11, nij)|n11 =
m(pâˆ’1)
11
.
4. n11 divides all the entries of the matrix N. In that case, M (p) :=
M (pâˆ’1).
It is essential to observe that in the ï¬rst three cases, m(p)
11 is not associated
to m(pâˆ’1)
11
, though it divides it.
From Proposition 6.1.2, the elements of the sequence

m(p)
11

pâ‰¥0 are pair-
wise associated, once p is large enough. We are then in the last of the
four cases above: m(q)
11 divides all the m(q)
ij â€™s. We have m(q)
i1 = aim(q)
11 and
m(q)
1j = bjm(q)
11 . Then let P âˆˆGLn(A) and Q âˆˆGLm(A) be the matrices
deï¬ned by:
â€¢ pii = 1, pi1 = âˆ’ai if i â‰¥2, pij = 0 otherwise,
â€¢ qjj = 1, q1j = âˆ’bj if j â‰¥2, qij = 0 otherwise.
The matrix M â€² := PM (q)Q is equivalent to M (q), hence to M. It has the
form
M â€² =
ï£«
ï£¬
ï£¬
ï£¬
ï£­
m
0
Â· Â· Â·
0
0
...
M â€²â€²
0
ï£¶
ï£·
ï£·
ï£·
ï£¸,
where m divides all the entries of M â€²â€². Obviously, m = D1(M â€²) = D1(M).
Having shown that every matrix M is equivalent to a matrix of the form
described above, one may argue by induction on the size of M (that is,
on the integer r = min(n, m)). If r = 1, we have just proved the claim.
If r â‰¥2 and if the claim is true up to the order r âˆ’1, we apply the
induction hypothesis to the factor M â€²â€² âˆˆM(nâˆ’1)Ã—(mâˆ’1)(A) in the above
reduction: there exist P â€²â€² âˆˆGLnâˆ’1(A) and Qâ€²â€² âˆˆGLmâˆ’1(A) such that
P â€²â€²M â€²â€²Qâ€²â€² is quasi-diagonal, with diagonal entries d2, . . . , dr ordered by
dl|dl+1 for l â‰¥2. From the uniqueness step, d2 = D1(M â€²â€²). Since m divides
the entries of M â€²â€², we have m|d2. Let us then deï¬ne P â€² = diag(1, P â€²â€²)
and Qâ€² = diag(1, Qâ€²â€²), which are invertible: P â€²M â€²Qâ€² is quasi-diagonal, with
diagonal entries d1 = m, d2, . . . , a nondecreasing sequence (according to
the division in A). Since M is equivalent to M â€², this proves the existence
part of the theorem.
6.2.1
Comments
In the list of invariant factors of a matrix some djâ€™s may equal zero. In
that case, dj = 0 implies dj+1 = Â· Â· Â· = dr = 0. Moreover, some invariant

104
6. Matrices with Entries in a Principal Ideal Domain; Jordan Reduction
factor may occur several times in the list d1, . . . , dr, up to association. The
number of times that a factor d or its associates occur is its multiplicity.
If m = n and if the invariant factors of a matrix M are (1, Â· Â· Â· , 1), then
D = In, and M = PQ is invertible. Conversely, if M is invertible, then the
decomposition M = MInIn shows that d1 = Â· Â· Â· = dn = 1.
If A is a ï¬eld, then there are only two ideals: A = (1) itself and (0). The
list of invariant factors of a matrix is thus of the form (1, . . . , 1, 0, . . . , 0).
Of course, there may be no 1â€™s (for the matrix 0mÃ—n), or no 0â€™s. There are
thus exactly min(n, m) + 1 classes of equivalent matrices in Mn(A), two
matrices being equivalent if and only if they have the same rank q. The rank
is then the number of 1â€™s among the invariant factors. The decomposition
M = PDQ is then called the rank decomposition.
Theorem 6.2.2 Let k be a ï¬eld and M âˆˆMnÃ—m(k) a matrix. Let q be
the rank of M, that is, the dimension of the linear subspace of kn spanned
by the columns of M. Then there exist two square invertible matrices P, Q
such that M = PDQ with dii = 1 if i â‰¤q and dij = 0 in all other cases.
6.3
Similarity Invariants and Jordan Reduction
From now on, k will denote a ï¬eld and A = k[X] the ring of polynomi-
als over k. This ring is Euclidean, hence a principal ideal domain. In the
sequel, the results are eï¬€ective, in the sense that the normal forms that
we deï¬ne will be obtained by means of an algorithm that uses right or left
multiplications by elementary matrices of Mn(A), the computations being
based upon the Euclidean division of polynomials.
Given a matrix B âˆˆMn(k) (a square matrix with constant entries, in the
sense that they are not polynomials), we consider the matrix XIn âˆ’B âˆˆ
Mn(A), where X is the indeterminate in A.
Deï¬nition 6.3.1 If B âˆˆMn(k), the invariant factors of M := XIn âˆ’B
are called invariant polynomials of B, or similarity invariants of B.
This deï¬nition is justiï¬ed by the following statement.
Theorem 6.3.1 Two matrices in Mn(k) are similar if and only if
they have the same list of invariant polynomials (counted with their
multiplicities).
This theorem is a particular case of a more general one:
Theorem 6.3.2 Let A0, A1, B0, B1 be matrices in Mn(k), with A0, A1.
Then the matrices XA0 + B0 and XA1 + B1 are equivalent (in Mn(A)) if
and only if there exist G, H âˆˆGLn(k) such that
GA0 = A1H,
GB0 = B1H.

6.3. Similarity Invariants and Jordan Reduction
105
When A0 = A1 = In, Theorem 6.3.2 tells that XIn âˆ’B0 and XIn âˆ’B1
are equivalent, namely that they have the same invariant polynomials, if
there exists P âˆˆGLn(k) such that PB0 = B1P, which is the criterion
given by Theorem 6.3.1.
Proof
We prove Theorem 6.3.2. The condition is clearly suï¬ƒcient.
Conversely, if XA0+B0 and XA1+B1 are equivalent, there exist matrices
P, Q âˆˆGLn(A), such that P(XA0 + B0) = (XA1 + B1)Q. Since A1 is
invertible, one may perform Euclidean division2 of P by XA1 + B1 on the
right:
P = (XA1 + B1)P1 + G,
where G is a matrix whose entries are constant polynomials. We warn the
reader that since Mn(k) is not commutative, Euclidean division may be
done either on the right or on the left, with distinct quotients and distinct
remainders. Likewise, we have Q = Q1(XA0 + B0) + H with H âˆˆMn(k).
Let us write, then,
(XA1 + B1)(P1 âˆ’Q1)(XA0 + B0) = (XA1 + B1)H âˆ’G(XA0 + B0).
The left-hand side of this equality has degree (the degree is deï¬ned as the
supremum of the degrees of the entries of the matrix) 2 + deg(P1 âˆ’Q1),
while the right-hand side has degree less than or equal to one. The two
sides, being equal, must vanish, and we conclude that
GA0 = A1H,
GB0 = B1H.
There remains to show that G and H are invertible. To do so, let us deï¬ne
R âˆˆMn(A) as the inverse matrix of P (which exists by assumption). We
still have
R = (XA0 + B0)R1 + K,
K âˆˆMn(k).
Combining the equalities stated above, we obtain
In âˆ’GK = (XA1 + B1)(QR1 + P1K).
Since the left-hand side is constant and the right-hand side has degree
1 + deg(QR1 + P1K), we must have In = GK, so that G is invertible.
Likewise, H is invertible.
We conclude this paragraph with a remarkable statement:
Theorem 6.3.3 If B âˆˆMn(k), then B and BT are similar.
Indeed, XIn âˆ’B and XIn âˆ’BT are transposes of each other, and hence
have the same list of minors, hence the same invariant factors.
2The fact that A1 is invertible is essential, since the ring Mn(A) is not an integral
domain.

106
6. Matrices with Entries in a Principal Ideal Domain; Jordan Reduction
6.3.1
Example: The Companion Matrix of a Polynomial
Given a polynomial
P(X) = Xn + a1Xnâˆ’1 + Â· Â· Â· + an,
there exists a matrix B âˆˆMn(k) such that the list of invariant factors of
the matrix XIn âˆ’B is (1, . . . , 1, P). We may take the companion matrix
associated to P to be
BP :=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
Â· Â· Â·
Â· Â· Â·
0
âˆ’an
1
...
...
...
0
...
...
...
...
...
...
...
0
...
0
Â· Â· Â·
0
1
âˆ’a1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Naturally, any matrix similar to BP would do as well, because if B =
Qâˆ’1BP Q, then XInâˆ’B is similar, hence equivalent, to XIn âˆ’BP . In order
to show that the invariant factors of BP are the polynomials (1, . . . , 1, P),
we observe that XInâˆ’BP possesses a minor of order nâˆ’1 that is invertible,
namely, the determinant of the submatrix
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
X
0
Â· Â· Â·
0
0
...
...
...
...
...
...
...
...
0
...
...
...
X
0
Â· Â· Â·
Â· Â· Â·
0
âˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
We thus have Dnâˆ’1(XIn âˆ’BP ) = 1, so that the invariant factors
d1, . . . , dnâˆ’1 are all equal to 1. Hence dn = Dn(XIn âˆ’BP ) = det(XIn âˆ’
BP ), the characteristic polynomial of BP , namely P.
In this example P is also the minimal polynomial of BP . In fact, if Q is
a polynomial of degree less than or equal to n âˆ’1,
Q(X) = b0Xnâˆ’1 + Â· Â· Â· + bnâˆ’1,
the vector Q(A)e1 reads
b0en + Â· Â· Â· + bnâˆ’1e1.
Hence Q(A) = 0 and deg Q â‰¤n âˆ’1 imply Q = 0. The minimal polynomial
is thus of degree at least n. It is thus equal to the characteristic polynomial.
6.3.2
First Canonical Form of a Square Matrix
Let M âˆˆMn(k) be a square matrix and P1, . . . , Pn âˆˆk[X] its similarity
invariants. The sum of their degrees nj (1 â‰¤j â‰¤n) is n. Let us denote

6.3. Similarity Invariants and Jordan Reduction
107
by M (j) âˆˆMnj(k) the companion matrix of the polynomial Pj. Let us
form the matrix M â€², block-diagonal, whose diagonal blocks are the M (j)â€™s.
The few ï¬rst polynomials Pj are generally constant (we shall see below
that the only case where P1 is not constant corresponds to M = Î±In), and
the corresponding blocks are empty, as are the corresponding rows and
columns. To be precise, the actual number m of diagonal blocks is equal to
the nuber of nonconstant similarity invariants.
Since the matrix XInj âˆ’M (j) is equivalent to the matrix N (j) =
diag(1, . . . , 1, Pj), we have
XInj âˆ’M (j) = P (j)N (j)Q(j),
where P (j), Q(j) âˆˆGLnj(k[X]). Let us form matrices P, Q âˆˆGLn(k[X])
by
P = diag(P (1), . . . , P (n)),
Q = diag(Q(1), . . . , Q(n)).
We obtain
XIn âˆ’M â€² = PNQ,
N = diag(N (1), . . . , N (n)).
Here N is a diagonal matrix, whose diagonal entries are the similarity
invariants of M, up to the order. In fact, each nonconstant Pj appears
in the associated block N (j). The other diagonal terms are the constant
1, which occurs n âˆ’m times; these are the polynomials P1, . . . , Pnâˆ’m, as
expected. Conjugating by a permutation matrix, we obtain that XIn âˆ’M â€²
is equivalent to the matrix diag(P1, . . . , Pn). Hence XIn âˆ’M â€² is equivalent
to XIn âˆ’M. From Theorem 6.3.1, M and M â€² are similar.
Theorem 6.3.4 Let k be a ï¬eld, M âˆˆMn(k) a square matrix, and
P1, . . . , Pn its similarity invariants. Then M is similar to the block-
diagonal matrix M â€² whose jth diagonal block is the companion matrix of
Pj.
The matrix M â€² is called the ï¬rst canonical form of M, or the Frobenius
canonical form of M.
Remark: If L is an extension of k (namely, a ï¬eld containing k) and M âˆˆ
Mn(k), then M âˆˆMn(L). Let P1, . . . , Pn be the similarity invariants of M
as a matrix with entries in k. Then XInâˆ’M = P diag(P1, . . . , Pn)Q, where
P, Q âˆˆGLn(k[X]). Since P, Q, their inverses, and the diagonal matrix also
belong to Mn(L[X]), P1, . . . , Pn are the similarity invariants of M as a
matrix with entries in L. In other words, the similarity invariants depend
on M but not on the ï¬eld k. To compute them, it is enough to place
ourselves in the smallest possible ï¬eld, namely that spanned by the entries
of M. The same remark holds true for the ï¬rst canonical form. As we shall
see in the next section, it is no longer true for the second canonical form,
which is therefore less canonical.
We end this paragraph with a characterization of the minimal polyno-
mial.

108
6. Matrices with Entries in a Principal Ideal Domain; Jordan Reduction
Theorem 6.3.5 Let k be a ï¬eld, M âˆˆMn(k) a square matrix, and
P1, . . . , Pn its similarity invariants. Then Pn is the minimal polynomial
of M. In particular, the minimal polynomial does not depend on the ï¬eld
under consideration, as long as it contains the entries of M.
Proof
We use the ï¬rst canonical form M â€² of M. Since M â€² and M are similar,
they have the same minimal polynomial. One thus can assume that M is
in the canonical form M = diag(M1, . . . , Mn), where Mj is the companion
matrix of Pj. Since Pj(Mj) = 0 (Cayleyâ€“Hamilton, theorem 2.5.1) and
Pj|Pn, we have Pn(Mj) = 0 and thus Pn(M) = 0n. Hence, the minimal
polynomial QM divides Pn. Conversely, Q(M) = 0n implies Q(Mn) = 0.
Since Pn is the minimal polynomial of Mn, Pn divides Q. Finally, Pn = QM.
Finally, since the similarity invariants do not depend on the choice of the
ï¬eld, Pn also does not depend on this choice.
Warning: One may draw an incorrect conclusion if one applies Theorem
6.3.5 carelessly. Given a matrix M âˆˆMn(ZZ), one can deï¬ne a matrix
M(p) in Mn(ZZ/pZZ) by reduction modulo p (p a prime number). But the
minimal polynomial of M(p) is not necessarily the reduction modulo p of
QM. Here is an example: Let us take n = 2 and
M =

2
2
0
2

.
Then QM divides PM = (X âˆ’2)2, but QM Ì¸= X âˆ’2, since M Ì¸= 2I2. Hence
QM = (X âˆ’2)2. On the other hand, M(2) = 02, whose minimal polynomial
is X, which is diï¬€erent from X2, the reduction modulo 2 of QM.
The explanation of this phenomenon is the following. The matrices M
and M(2) are composed of scalars of diï¬€erent natures. There is no ï¬eld
L containing simultaneously ZZ and ZZ/2ZZ. There is thus no context in
which Theorem 6.3.5 could be applied.
6.3.3
Second Canonical Form of a Square Matrix
We now decompose the similarity invariants of M into products of irre-
ducible polynomials. This decomposition depends, of course, on the choice
of the ï¬eld of scalars. Denoting by p1, . . . , pt the list of distinct irreducible
(in k[X]) factors of Pn, we have
Pj =
t
k=1
pÎ±(j,k)
k
,
1 â‰¤j â‰¤n
(because Pj divides Pn), where the Î±(j, k) are nondecreasing with respect
to j, since Pj divides Pj+1.

6.3. Similarity Invariants and Jordan Reduction
109
Deï¬nition 6.3.2 The elementary divisors of the matrix M âˆˆMn(k) are
the polynomials pÎ±(j,k)
k
for which the exponent Î±(j, k) is nonzero. The mul-
tiplicity of an elementary divisor pm
k is the number of solutions j of the
equation Î±(j, k) = m. The list of elementary divisors is the sequence of
these polynomials, repeated with their multiplicities.
Let us begin with the case of the companion matrix N of some polynomial
P. Its similarity invariants are (1, . . . , 1, P) (see above). Let Q1, . . . , Qt be
its elementary divisors (we observe that each has multiplicity one). We then
have P = Q1 Â· Â· Â· Qt, while the Qlâ€™s are pairwise coprime. To each Ql we
associate its companion matrix Nl, and we form a block-diagonal matrix
N â€² := diag(N1, . . . , Nt). Since each Nl âˆ’XIl is equivalent to a diagonal
matrix
ï£«
ï£¬
ï£¬
ï£¬
ï£­
1
...
1
Ql
ï£¶
ï£·
ï£·
ï£·
ï£¸
in Mn(l)(k[X]), the whole matrix N â€² âˆ’XIn is equivalent to
Q :=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
...
O
1
Q1
O
...
Qt
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Let us now compute the similarity invariants of N â€², that is, the invariant
factors of Q. It will be enough to compute the greatest common divisor
Dnâˆ’1 of the minors of size n âˆ’1. Taking into account the principal minors
of Q, we see that Dnâˆ’1 must divide every product of the form

lÌ¸=k
Ql,
1 â‰¤k â‰¤t.
Since the Qlâ€™s are pairwise coprime, this implies that Dnâˆ’1 = 1. This
means that the list of similarity invariants of N â€² has the form (1, . . . , 1, Â·),
where the last polynomial must be the characteristic polynomial of N â€².
This polynomial is the product of the characteristic polynomials of the
Nlâ€™s. These being equal to the Qlâ€™s, the characteristic polynomial of N â€² is
P. Finally, N and N â€² have the same similarity invariants and are therefore
similar.
Now let M be a general matrix in Mn(k). We apply the former reduction
to every diagonal block Mj of its Frobenius canonical form. Each Mj is
similar to a block-diagonal matrix whose diagonal blocks are companion
matrices corresponding to the elementary divisors of M entering into the

110
6. Matrices with Entries in a Principal Ideal Domain; Jordan Reduction
factorization of the jth invariant polynomial of M. We have thus proved
the following statement.
Theorem 6.3.6 Let Q1, . . . , Qs be the elementary divisors of M
âˆˆ
Mn(k). Then M is similar to a block-diagonal matrix M â€² whose diagonal
blocks are companion matrices of the Qlâ€™s.
The matrix M â€² is called the second canonical form of M.
Remark: The exact computation of the second canonical form of a given
matrix is impossible in general, in contrast to the case of the ï¬rst form.
Indeed, if there were an algorithmic construction, it would provide an algo-
rithm for factorizing polynomials into irreducible factors via the formation
of the companion matrix, a task known to be impossible if k = IR or CC.
Recall that one of the most important results in Galois theory, known as
Abelâ€™s theorem, states the impossibility of solving a general polynomial
equation of degree at least ï¬ve with complex coeï¬ƒcients, using only the
basic operations and the extraction of roots of any order.
6.3.4
Jordan Form of a Matrix
When the characteristic polynomial splits over k, which holds, for instance,
if the ï¬eld k is algebraically closed, the elementary divisors have the form
(X âˆ’a)r for a âˆˆk and r â‰¥1. In that case, the second canonical form can
be greatly simpliï¬ed by replacing the companion matrix of the monomial
(X âˆ’a)r by its Jordan block
J(a; r) :=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a
1
0
Â· Â· Â·
0
0
...
...
...
...
...
...
...
...
0
...
...
...
1
0
Â· Â· Â·
Â· Â· Â·
0
a
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
In fact, the characteristic polynomial of J(a; r) (of size r Ã— r) is (X âˆ’a)r,
while the matrix XIr âˆ’J(a; r) possesses an invertible minor of order r âˆ’1,
namely
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ’1
0
Â· Â· Â·
0
X âˆ’a
...
...
...
...
...
0
X âˆ’a
âˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
,
which is obtained by deleting the ï¬rst column and the last row. Again, this
shows that Dnâˆ’1(XIr âˆ’J) = 1, so that the invariant factors d1, . . . , drâˆ’1
are equal to 1. Hence dr = Dr(XIr âˆ’J) = det(XIr âˆ’J) = (X âˆ’a)r. Its

6.4. Exercises
111
invariant factors are thus 1, . . . , 1, (X âˆ’a)r. Hence we have the following
theorem.
Theorem 6.3.7 When an elementary divisor of M is (X âˆ’a)r, one may,
in the second canonical form of M, replace its companion matrix by the
Jordan block J(a; r).
Corollary 6.3.1 If the characteristic polynomial of M splits over k, then
M is similar to a block-diagonal matrix whose jth diagonal block is a Jordan
block J(aj; rj). This form is unique, up to the order of blocks.
Corollary 6.3.2 If k is algebraically closed (for example if k = CC), then
every square matrix M is similar to a block-diagonal matrix whose jth
diagonal block is a Jordan block J(aj; rj). This form is unique, up to the
order of blocks.
6.4
Exercises
See also the exercise 12 in Chapter 7.
1. Show that every principal ideal domain is a unique factorization
domain.
2. Verify that the characteristic polynomial of the companion matrix of
a polynomial P is equal to P.
3. Let k be a ï¬eld and M âˆˆMn(k). Show that M, M T have the same
rank and that in general, the rank of M T M is less than or equal
to that of M. Show that the equality of these ranks always holds if
k = IR, but that strict inequality is possible, for example with k = CC.
4. Compute the elementary divisors of the matrices
ï£«
ï£¬
ï£¬
ï£­
22
23
10
âˆ’98
12
18
16
âˆ’38
âˆ’15
âˆ’19
âˆ’13
58
6
7
4
âˆ’25
ï£¶
ï£·
ï£·
ï£¸,
ï£«
ï£¬
ï£¬
ï£­
0
âˆ’21
âˆ’56
âˆ’96
18
36
52
âˆ’8
âˆ’12
âˆ’17
âˆ’16
38
3
2
âˆ’2
âˆ’20
ï£¶
ï£·
ï£·
ï£¸
and
ï£«
ï£¬
ï£¬
ï£­
44
89
120
âˆ’32
0
âˆ’12
âˆ’32
âˆ’56
âˆ’14
âˆ’20
âˆ’16
49
8
14
16
âˆ’16
ï£¶
ï£·
ï£·
ï£¸
in Mn(CC). What are their Jordan reductions?
5. (Lagrangeâ€™s theorem)

112
6. Matrices with Entries in a Principal Ideal Domain; Jordan Reduction
Let K be a ï¬eld and A âˆˆMn(K). Let X, Y âˆˆKn be vectors such
that XT AY Ì¸= 0. We normalize by XT AY = 1 and deï¬ne
B := A âˆ’(AY )(XT A).
Show that in the factorization
PAQ =
 Ir
0
0
0nâˆ’r

,
P, Q âˆˆGLn(K),
one can choose Y as the ï¬rst column of Q and XT as the ï¬rst row of
P. Deduce that rk B = rk A âˆ’1.
More generally, show that if X, Y âˆˆMnÃ—m(K), XTAY âˆˆGLm(K),
and if
B := A âˆ’(AY )(XT AY )âˆ’1(XT A),
then rk B = rk A âˆ’m.
If A âˆˆSymn(IR) and if A is positive semideï¬nite, and if X = Y ,
show that B is also positive semideï¬nite.
6. For A âˆˆMn(CC), consider the linear diï¬€erential equation in CCn
dx
dt = Ax.
(6.1)
(a) Let P âˆˆGLn(CC) and let t â†’x(t) be a solution of (6.1). What
is the diï¬€erential equation satisï¬ed by t â†’Px(t)?
(b) Let (X âˆ’a)m be an elementary divisor of A. Show that for every
k = 0, . . . , m âˆ’1, (6.1) possesses solutions of the form eatQk(t),
where Qk is a complex-valued polynomial map of degree k.
7. Consider the following diï¬€erential equation of order n in CC:
x(n)(t) + a1x(nâˆ’1)(t) + Â· Â· Â· + anx(t) = 0.
(6.2)
(a) Deï¬ne P(X) = Xn + a1Xnâˆ’1 + Â· Â· Â· + an and let M be the
companion matrix of P. Let
P(X) =

aâˆˆA
(X âˆ’a)na
be the factorization of P into irreducible factors. Compute the
Jordan form of M.
(b) Using either the previous exercise or arguing directly, show that
the set of solutions of (6.2) is spanned by the solutions of the
form
t â†’eatR(t),
R âˆˆCC[X],
deg R < na.
8. Consider a linear recursion of order n in a ï¬eld K
um+n + a1um+nâˆ’1 + Â· Â· Â· + anum = 0,
m âˆˆIN.
(6.3)

6.4. Exercises
113
With the notation of the previous exercise, show that the set of
solutions of (6.3) is spanned by the solutions of the form
(amR(m))mâˆˆIN,
R âˆˆCC[X],
deg R < na.
9. Let n â‰¥2 and let M âˆˆMn(ZZ) be the matrix deï¬ned by mij =
i + j âˆ’1:
M =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
1
2
Â· Â· Â·
n
2
...
...
...
...
...
...
...
n
Â· Â· Â·
Â· Â· Â·
2n âˆ’1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
(a) Show that M has rank 2 (you may look for two vectors x, y âˆˆZZn
such that mij = xixj âˆ’yiyj).
(b) Compute the invariant factors of M in Mn(ZZ) (the equivalent
diagonal form is obtained after ï¬ve elementary operations).
10. The ground ï¬eld is CC.
(a) Deï¬ne
N = J(0; n),
B =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
. . .
0
1
...
...
...
0
0
...
...
...
1
0
. . .
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Compute NB, BN, and BNB. Show that S :=
1
âˆš
2(I + iB) is
unitary.
(b) Deduce that N is similar to
1
2
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
1
0
. . .
0
1
...
...
...
...
0
...
...
...
0
...
...
...
...
1
0
. . .
0
1
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
+ i
2
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
0
. . .
0
âˆ’1
0
...
...
...
...
1
0
...
...
...
0
âˆ’1
...
...
...
...
0
1
0
. . .
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
(c) Deduce that every matrix M âˆˆMn(CC) is similar to a complex
symmetric matrix. Compare with the real case.

7
Exponential of a Matrix, Polar
Decomposition, and Classical Groups
7.1
The Polar Decomposition
The polar decomposition of matrices is deï¬ned by analogy with that in the
complex plane: If z âˆˆCCâˆ—, there exists a unique pair (r, q) âˆˆ(0, +âˆ) Ã— S1
(S1 denotes the unit circle, the set of complex numbers of modulus 1) such
that z = rq. If z acts on CC (or on CCâˆ—) by multiplication, this action can
be decomposed as the product of a rotation of angle Î¸ (where q = exp(iÎ¸))
with a homothety of ratio r > 0. The fact that these two actions commute
is a consequence of the commutativity of the multiplicative group CCâˆ—; this
property does not hold for the polar decomposition in GLn(k), k = IR or
CC, because the general linear group is not commutative.
Let us recall that HPDn denotes the (open) cone of matrices of Mn(CC)
that are Hermitian positive deï¬nite, while Un denotes the group of unitary
matrices. In Mn(IR), SPDn is the set of symmetric positive deï¬nite ma-
trices, and On is the orthogonal group. The group Un is compact, since it
is closed and bounded in Mn(CC). Indeed, the columns of unitary matrices
are unit vectors, so that Un is bounded. On the other hand, Un is deï¬ned
by an equation U âˆ—U = In, where the map U â†’U âˆ—U is continuous; hence
Un is closed. By the same arguments, On is compact.
Polar decomposition is a fundamental tool in the theory of ï¬nite-
dimensional Lie groups and Lie algebras. For this reason, it is intimately
related to the exponential map. We shall not consider these two notions
here in their full generality, but we shall restrict attention to their matricial
aspects.

7.1. The Polar Decomposition
115
Theorem 7.1.1 For every M âˆˆGLn(CC), there exists a unique pair
(H, Q) âˆˆHPDn Ã— Un
such that M = HQ. If M âˆˆGLn(IR), then (H, Q) âˆˆSPDn Ã— On.
The map M â†’(H, Q), called the polar decomposition of M, is a
homeomorphism between GLn(CC) and HPDn Ã— Un (respectively between
GLn(IR) and SPDn Ã— On).
Theorem 7.1.2 Let H be a positive deï¬nite Hermitian matrix. There ex-
ists a unique positive deï¬nite Hermitian matrix h such that h2 = H. If H
is real-valued, then so is h. The matrix h is called the square root of H,
and is denoted by h =
âˆš
H.
Proof
We prove Theorem 7.1.1 and obtain Theorem 7.1.2 as a by-product.
Existence. Since MM âˆ—âˆˆHPDn, we can diagonalize MM âˆ—by a
unitary matrix
MM âˆ—= U âˆ—DU,
D = diag(d1, . . . , dn),
where dj âˆˆ(0, +âˆ). The matrix H := U âˆ—diag(âˆšd1, . . . , âˆšdn)U is Hermi-
tian positive deï¬nite and satisï¬es H2 = HHâˆ—= MM âˆ—. Then Q := Hâˆ’1M
satisï¬es Qâˆ—Q = M âˆ—Hâˆ’2M = M âˆ—(MM âˆ—)âˆ’1M = In, hence Q âˆˆUn. If
M âˆˆMn(IR), then clearly MM âˆ—is real symmetric. In fact, U is orthogo-
nal and H is real symmetric. Hence Q is real orthogonal. Note: H is called
the square root of MM âˆ—.
Uniqueness. Let M = Hâ€²Qâ€² be another suitable decomposition. Then
N := Hâˆ’1Hâ€² = Q(Qâ€²)âˆ’1 is unitary, so that Sp(N) âŠ‚S1. Let S âˆˆHPDn
be a positive deï¬nite Hermitian square root of Hâ€² (we shall prove below
that it is unique). Then N is similar to N â€² := SHâˆ’1S. However, N â€² âˆˆ
HPDn. Hence N is diagonalizable, with real positive eigenvalues. Hence
Sp(N) = {1}, and N is therefore similar, and thus equal, to In.
This proves that the positive deï¬nite Hermitian square root of a matrix
of HPDn is unique in HPDn, since otherwise, our construction would
provide several polar decompositions. We have thus proved Theorem 7.1.2
in passing.
Smoothness. The map (H, Q) â†’HQ is polynomial, hence continuous.
Conversely,, it is enough to prove that M â†’(H, Q) is sequentially con-
tinuous, since GLn(CC) is a metric space. Let (Mk)kâˆˆIN be a convergent
sequence in GLn(CC) and let M be its limit. Let us denote by Mk = HkQk
and M = HQ their respective polar decompositions. Let R be a cluster
point of the sequence (Qk)kâˆˆIN, that is, the limit of some subsequence
(Qkl)lâˆˆIN, with kl â†’+âˆ. Then Hkl = MklQâˆ—
kl converges to S := MRâˆ—.
The matrix S is Hermitian positive semideï¬nite (because it is the limit
of the Hklâ€™s) and invertible (because it is the product of M and Râˆ—). It
is thus positive deï¬nite. Hence, SR is a polar decomposition of M. The
uniqueness part ensures that R = Q and S = H. The sequence (Qk)kâˆˆIN,

116
7. Exponential of a Matrix, Polar Decomposition, and Classical Groups
which is relatively compact and has at most one cluster point (namely Q),
converges to Q. Finally, Hk = MkQâˆ—
k converges to MQâˆ—= H.
Remark: There is as well a polar decomposition M = QH with the same
properties. We shall use one or the other depending on the context. We
warn the reader, however, that for a given matrix, the two decompositions
do not coincide. For example, in M = HQ, H is the square root of MM âˆ—,
though in M = QH, it is the square root of M âˆ—M.
7.2
Exponential of a Matrix
The ground ï¬eld is here k = CC. By restriction, we can also treat the case
k = IR.
For A in Mn(CC), the series
âˆ

k=0
1
k!Ak
converges normally (which means that the series of norms is convergent),
since for any matrix norm, we have
âˆ

k=0
####
1
k!Ak
#### â‰¤
âˆ

k=0
1
k! âˆ¥Aâˆ¥k = exp âˆ¥Aâˆ¥.
Since Mn(CC) is complete, the series is convergent, and the estimation above
shows that it converges uniformly on every compact set. Its sum, denoted
by exp A, thus deï¬nes a continuous map exp : Mn(CC) â†’Mn(CC), called
the exponential. When A âˆˆMn(IR), we have exp A âˆˆMn(IR).
Given two matrices A and B in general position, the binomial formula
is not valid: (A + B)k does not necessarily coincide with
j=k

j=0
 k
j

AjBkâˆ’j.
It thus follows that exp(A + B) diï¬€ers in general from exp A Â· exp B. A
correct statement is the following.
Proposition 7.2.1 Let A, B âˆˆMn(CC) be commuting matrices; that is,
AB = BA. Then exp(A + B) = (exp A)(exp B).
Proof
The proof proceeds in exactly the same way as for the exponential of
complex numbers. We observe that since the series deï¬ning the expo-
nential of a matrix is normally convergent, we may compute the product

7.2. Exponential of a Matrix
117
(exp A)(exp B) by multiplying term by term the series
(exp A)(exp B) =
âˆ

j,k=0
1
j!k!AjBk.
In other words,
(exp A)(exp B) =
âˆ

l=0
1
l!Cl,
where
Cl :=

j+k=l
l!
j!k!AjBk.
From the assumption AB = BA, we know that the binomial formula holds.
Therefore, Cl = (A + B)l, which proves the proposition.
Noting that exp 0n = In and that A and âˆ’A commute, we derive the
following corollary.
Corollary 7.2.1 For every A âˆˆMn(CC), exp A is invertible, and its
inverse is exp(âˆ’A).
Given two conjugate matrices B = P âˆ’1AP, we have Bk = P âˆ’1AkP for
each integer k and thus
exp(P âˆ’1AP) = P âˆ’1(exp A)P.
(7.1)
If D = diag(d1, . . . , dn) is diagonal, we have
exp D = diag(exp d1, . . . , exp dn).
Of course, this formula, or more generally (7.1), can be combined with
Jordan reduction in order to compute the exponential of a given matrix.
Let us keep in mind, however, that Jordan reduction cannot be carried out
explicitly.
Let us introduce a real parameter t and let us deï¬ne a function g by
g(t) = exp tA. From Proposition 7.2.1, we see that g satisï¬es the functional
equation
g(s + t) = g(s)g(t).
(7.2)
On the other hand, g(0) = In, and we have
g(t) âˆ’g(0)
t
âˆ’A =
âˆ

k=2
tkâˆ’1
k! Ak.
Using any matrix norm, we deduce that
####
g(t) âˆ’g(0)
t
âˆ’A
#### â‰¤eâˆ¥tAâˆ¥âˆ’1 âˆ’âˆ¥tAâˆ¥
|t|
,

118
7. Exponential of a Matrix, Polar Decomposition, and Classical Groups
from which we obtain
lim
tâ†’0
g(t) âˆ’g(0)
t
= A.
We conclude that g has a derivative at t = 0, with gâ€²(0) = A. Using the
functional equation (7.2), we then obtain that g is diï¬€erentiable everywhere,
with
gâ€²(t) = lim
sâ†’0
g(t)g(s) âˆ’g(t)
s
= g(t)A.
We observe that we also have
gâ€²(t) = lim
sâ†’0
g(s)g(t) âˆ’g(t)
s
= Ag(t).
From either of these diï¬€erential equations we see that g is actually inï¬nitely
diï¬€erentiable. We shall retain the formula
d
dt exp tA = A exp tA = (exp tA)A.
(7.3)
This diï¬€erential equation is sometimes the most practical way to compute
the exponential of a matrix. This is of particular relevance when A has real
entries but has at least one nonreal eigenvalue if one wishes to avoid the
use of complex numbers.
Proposition 7.2.2 For every A âˆˆMn(CC),
det exp A = exp Tr A.
(7.4)
Proof
We could deduce (7.4) directly from (7.3). Here is a more elementary
proof. We begin with a reduction of A of the form A = P âˆ’1T P, where T
is upper triangular. Since T k is still triangular, with diagonal entries equal
to tk
jj, exp T is triangular too, with diagonal entries equal to exptjj. Hence
det exp T =

j
exp tjj = exp

j
tjj = exp Tr T.
This is the expected formula, since exp A = P âˆ’1(exp T )P.
Since (M âˆ—)k = (M k)âˆ—, we see easily that (exp M)âˆ—= exp(M âˆ—). In
particular, the exponential of a skew-Hermitian matrix is unitary, for then
(exp M)âˆ—exp M = exp(M âˆ—) exp M = exp(âˆ’M) exp M = In.
Similarly, the exponential of a Hermitian matrix is Hermitian positive
deï¬nite, because
exp M =

exp 1
2M
âˆ—
exp 1
2M.

7.2. Exponential of a Matrix
119
This calculation also shows that if M is Hermitian, then

exp M = exp 1
2M.
We shall use the following more precise statement:
Proposition 7.2.3 The map exp : Hn â†’HPDn is a homeomorphism
(that is, a bicontinuous bijection).
Proof
Injectivity: Let A, B âˆˆHn with exp A = exp B =: H. Then
exp 1
2A =
âˆš
H = exp 1
2B.
By induction, we have
exp 2âˆ’mA = exp 2âˆ’mB,
m âˆˆZZ.
Substracting In, multiplying by 2m, and passing to the limit as m â†’
+âˆ, we obtain
d
dt

t=0
exp tA = d
dt

t=0
exp tB;
that is, A = B.
Surjectivity: Let H âˆˆHPDn be given. Then H = U âˆ—diag(d1, . . . , dn)U,
where U is unitary and dj âˆˆ(0, +âˆ). From above, we know that
H = exp M for
M := U âˆ—diag(log d1, . . . , log dn)U,
which is Hermitian.
Continuity: The continuity of exp has already been proved. Let us in-
vestigate the continuity of the reciprocal map. Let (Hl)lâˆˆIN be a
sequence in HPDn that converges to H âˆˆHPDn. We denote by
M l, M âˆˆHn, the Hermitian matrices whose exponentials are Hl and
H. The continuity of the spectral radius gives
lim
lâ†’+âˆÏ(Hl) = Ï(H),
lim
lâ†’+âˆÏ

(Hl)âˆ’1
= Ï

(H)âˆ’1
.
(7.5)
Since Sp(M l) = log Sp(M l), we have
Ï(M l) = log max

Ï(Hl), Ï

(Hl)âˆ’1
.
(7.6)
Keeping in mind that the restriction to Hn of the induced norm âˆ¥Â·âˆ¥2
coincides with that of the spectral radius Ï, we deduce from (7.5, 7.6)
that the sequence (M l)lâˆˆIN is bounded. If N is a cluster point of the
sequence, the continuity of the exponential implies exp N = H. But
the injectivity shown above implies N = M. The sequence (M l)lâˆˆIN,
bounded with a unique cluster point, is convergent.

120
7. Exponential of a Matrix, Polar Decomposition, and Classical Groups
7.3
Structure of Classical Groups
Proposition 7.3.1 Let G be a subgroup of GLn(CC). We assume that G
is stable under the map M â†’M âˆ—and that for every M âˆˆG âˆ©HPDn, the
square root
âˆš
M is an element of G. Then G is stable under polar decompo-
sition. Furthermore, polar decomposition is a homeomorphism between G
and
(G âˆ©Un) Ã— (G âˆ©HPDn).
This proposition applies in particular to subgroups of GLn(IR) that are
stable under transposition and under extraction of square roots in SPDn.
One has then
G
homeo
âˆ¼
(G âˆ©On) Ã— (G âˆ©SPDn).
Proof
Let M âˆˆG be given and let HQ be its polar decomposition. Since
MM âˆ—âˆˆG, we have H2 âˆˆG, that is, H âˆˆG, by assumption. Finally, we
have Q = Hâˆ’1M âˆˆG. An application of Theorem 7.1.1 ï¬nishes the proof.
We apply this general result to the classical groups U(p, q), O(p, q)
(where n = p+q) and Spm (where n = 2m). These are respectively the uni-
tary group of the Hermitian form |z1|2+Â· Â· Â·+|zp|2âˆ’|zp+1|2âˆ’Â· Â· Â·âˆ’|zn|2, the
orthogonal group of the quadratic form x2
1 + Â· Â· Â·+ x2
p âˆ’x2
p+1 âˆ’Â· Â· Â·âˆ’x2
n, and
the symplectic group. They are deï¬ned by G = {M âˆˆMn(k)|M âˆ—JM = J},
with k = CC for U(p, q), k = IR otherwise. The matrix J equals

Ip
0pÃ—q
0qÃ—p
âˆ’Iq

,
for U(p, q) and O(p, q), and

0m
Im
âˆ’Im
0m

,
for Spm. In each case, J2 = Â±In.
Proposition 7.3.2 Let J be a complex n Ã— n matrix satisfying J2 = Â±In.
The subgroup G of Mn(CC) deï¬ned by the equation M âˆ—JM = J is invariant
under polar decomposition. If M âˆˆG, then | det M| = 1.
Proof
The fact that G is a group is immediate. Let M âˆˆG. Then det J =
det M âˆ—det M det J; that is, | det M|2 = 1. Furthermore, M âˆ—JM(JM âˆ—) =
J2M âˆ—= Â±M âˆ—= M âˆ—J2. Simplifying by M âˆ—J on the left, there remains
MJM âˆ—= J, that is, M âˆ—âˆˆG.

7.3. Structure of Classical Groups
121
Observe that, since G is a group, M âˆˆG implies (M âˆ—)kJ = JM âˆ’k for
every k âˆˆIN. By linearity, it follows that p(M âˆ—)J = Jp(M âˆ’1) holds for
every polynomial p âˆˆIR[X].
Let us now assume that M
âˆˆG âˆ©HPDn. We then have M
=
U âˆ—diag(d1, . . . , dn)U, where U is unitary and the djâ€™s are positive real
numbers. Let A be the set formed by the numbers dj and 1/dj. There ex-
ists a polynomial p with real entries such that p(a) = âˆša for every a âˆˆA.
Then we have p(M) =
âˆš
M and p(M âˆ’1) =
âˆš
M
âˆ’1. Since M âˆ—= M, we
have also p(M)J = Jp(M âˆ’1); that is,
âˆš
MJ = J
âˆš
M
âˆ’1. Hence
âˆš
M âˆˆG.
From Proposition 7.3.1, G is stable under polar decomposition.
The main result of this section is the following:
Theorem 7.3.1 Under the hypotheses of Proposition 7.3.2, the group G
is homeomorphic to (G âˆ©Un) Ã— IRd, for a suitable integer d.
Of course, if G = O(p, q) or Spm, the subgroup Gâˆ©Un can also be written
as G âˆ©On. We call G âˆ©Un a maximal compact subgroup of G, because one
can prove that it is not a proper subgroup of a compact subgroup of G.
Another deep result, which is beyond the scope of this book, is that every
maximal compact subgroup of G is a conjugate of G âˆ©Un. In the sequel,
when speaking about the maximal compact subgroup of G, we shall always
have in mind G âˆ©Un.
Proof
The proof amounts to showing that Gâˆ©HPDn is homeomorphic to some
IRd. To do this, we deï¬ne
G := {N âˆˆMn(k)| exp tN âˆˆG, âˆ€t âˆˆIR}.
Lemma 7.3.1 The set G deï¬ned above satiï¬es
G = {N âˆˆMn(k)|N âˆ—J + JN = 0n}.
Proof
If N âˆ—J + JN = 0n, let us set M(t) = exp tN. Then M(0) = In and
d
dtM(t)âˆ—JM(t) = M âˆ—(t)(N âˆ—J + JN)M(t) = 0n,
so that M(t)âˆ—JM(t) â‰¡J. We thus have N âˆˆG. Conversely,, if M(t) :=
exp tN âˆˆG for every t, then the derivative at t = 0 of M âˆ—(t)JM(t) = J
gives N âˆ—J + JN = 0n.
Lemma 7.3.2 The map exp : G âˆ©Hn â†’Gâˆ©HPDn is a homeomorphism.
Proof
We must show that exp : G âˆ©Hn â†’G âˆ©HPDn is onto. Let M âˆˆ
G âˆ©HPDn and let N be the Hermitian matrix such that exp N = M.
Let p âˆˆIR[X] be a polynomial with real entries such that for every Î» âˆˆ

122
7. Exponential of a Matrix, Polar Decomposition, and Classical Groups
Sp M âˆªSp M âˆ’1, we have p(Î») = log Î». Such a polynomial exists, since the
numbers Î» are real and positive.
Let N = U âˆ—DU be a unitary diagonalization of N. Then M = exp N =
U âˆ—(exp D)U and M âˆ’1 = exp(âˆ’N) = U âˆ—exp(âˆ’D)U. Hence, p(M) = N
and p(M âˆ’1) = âˆ’N. However, M âˆˆG implies MJ = JM âˆ’1, and therefore
q(M)J = Jq(M âˆ’1) for every q âˆˆIR[X]. With q = p, we obtain NJ = âˆ’JN.
These two lemmas complete the proof of the theorem, since G âˆ©Hn is an
IR-vector space. The integer d mentionned in the theorem is its dimension.
We wish to warn the reader that neither G, nor Hn is a CC-vector space.
We shall see examples in the next section that show that G âˆ©Hn can be
naturally IR-isomorphic to a CC-vector space, which is a source of confusion.
One therefore must be cautious when computing d.
The reader eager to learn more about the theory of classical groups is
advised to have a look at the book of R. MneimnÂ´e and F. Testard [28] or
the one by A. W. Knapp [24].
7.4
The Groups U(p, q)
Let us begin with the study of the maximal compact subgroup of U(p, q).
If M âˆˆU(p, q) âˆ©Un, let us write M blockwise:
M =

A
B
C
D

,
where A âˆˆMp(CC), etc. The following equations express that M belongs
to Un:
Aâˆ—A + Câˆ—C = Ip,
Bâˆ—B + Dâˆ—D = Iq,
Aâˆ—B + Câˆ—D = 0pq.
Similarly, writing that M âˆˆU(p, q),
Aâˆ—A âˆ’Câˆ—C = Ip,
Dâˆ—D âˆ’Bâˆ—B = Iq,
Aâˆ—B âˆ’Câˆ—D = 0pq.
Combining these equations, we obtain ï¬rst Câˆ—C = 0p and Bâˆ—B = 0q. For
every vector X âˆˆCCn, we have âˆ¥CXâˆ¥2
2 = Xâˆ—Câˆ—CX = 0; hence CX = 0.
Finally, C = 0 and similarly B = 0. There remains A âˆˆUp and D âˆˆUq.
The maximal compact subgroup of U(p, q) is thus isomorphic (not only
homeomorphic) to Up Ã— Uq.
Furthermore, G âˆ©Hn is the set of matrices
N =
 A
B
Bâˆ—
D

,
where A âˆˆHp, D âˆˆHq, which satisfy NJ + JN = 0n; that is, A = 0p,
D = 0q. Hence G âˆ©Hn is isomorphic to MpÃ—q(CC). One therefore has
d = 2pq.

7.5. The Orthogonal Groups O(p, q)
123
Proposition 7.4.1 The unitary group U(p, q) is homeomorphic to Up Ã—
Uq Ã— IR2pq. In particular, U(p, q) is connected.
There remains to show connectivity. It is a straightforward consequence of
the following lemma.
Lemma 7.4.1 The unitary group Un is connected.
Since GLn(CC) is homeomorphic to UnÃ—HPDn (via polar decomposition),
hence to Un Ã— Hn (via the exponential), it is equivalent to the following
statement.
Lemma 7.4.2 The linear group GLn(CC) is connected.
Proof
Let M âˆˆGLn(CC) be given. Deï¬ne A := CC \ {(1 âˆ’Î»)âˆ’1|Î» âˆˆSp(M)}.
The arcwise-connected set A does not contain the origin, nor the point
z = 1, since 0 Ì¸âˆˆSp(M). There thus exists a path Î³ joining 0 to 1 in A:
Î³ âˆˆC([0, 1]; A), Î³(0) = 0 and Î³(1) = 1. Let us deï¬ne M(t) := Î³(t)M +(1âˆ’
Î³(t))In. By construction, M(t) is invertible for every t, and M(0) = In,
M(1) = M. The connected component of In is thus all of GLn(CC).
7.5
The Orthogonal Groups O(p, q)
The analysis of the maximal compact subgroup and of Gâˆ©Hn for the group
O(p, q) is identical to that in the previous paragraph. On the one hand,
O(p, q) âˆ©On is isomorphic to Op Ã— Oq. On the other hand, G âˆ©Hn is
isomorphic to MpÃ—q(IR), which is of dimension d = pq.
Proposition 7.5.1 Let n â‰¥1. The group O(p, q) is homeomorphic to OpÃ—
Oq Ã—IRpq. The number of its connected components is two if p or q is zero,
four otherwise.
Proof
We must show that On has two connected components. However, On is
the disjoint union of SOn (matrices of determinant +1) and of Oâˆ’
n (matri-
ces of determinant âˆ’1). Since Oâˆ’
n = M Â· SOn for any matrix M âˆˆOâˆ’
n (for
example a hyperplane symmetry), there remains to show that the special
orthogonal group SOn is connected, in fact arcwise connected. We use the
following property:

124
7. Exponential of a Matrix, Polar Decomposition, and Classical Groups
Lemma 7.5.1 Given M âˆˆOn, there exists Q âˆˆOn such that the matrix
Qâˆ’1MQ has the form
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
(Â·)
0
Â· Â· Â·
0
0
...
...
...
...
...
...
0
0
Â· Â· Â·
0
(Â·)
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
,
(7.7)
where the diagonal blocks are of size 1Ã—1 or 2Ã—2 and are orthogonal, those
of size 2 Ã— 2 being rotations matrices:

cos Î¸
sin Î¸
âˆ’sin Î¸
cos Î¸

.
(7.8)
Let us apply Lemma 7.5.1 to M âˆˆSOn. The determinant of M, which
is the product of the determinants of the diagonal blocks, equals (âˆ’1)m,
m being the multiplicity of the eigenvalue âˆ’1. Since det M = 1, m is even,
and we can gather the diagonal âˆ’1â€™s pairwise in order to form matrices of
the form (7.8), with Î¸ = Ï€. Finally, there exists Q âˆˆOn such that
M = QT
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
R1
0
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
0
0
...
...
...
...
...
Rr
...
0
...
...
1
...
...
...
...
...
0
0
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
0
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
Q,
where each diagonal block Rj is a matrix of planar rotation:
Rj =

cos Î¸j
sin Î¸j
âˆ’sin Î¸j
cos Î¸j

.
Let us now deï¬ne a matrix M(t) as above, in which we replace the angles
Î¸j by tÎ¸j. We thus obtain a path in SOn, from M(0) = In to M(1) = M.
The connected component of In is thus the whole of SOn.
We now prove Lemma 7.5.1: As an orthogonal matrix, M is normal.
From Theorem 3.3.1, it decomposes into a matrix of the form (7.7), the
1 Ã— 1 diagonal blocks being the real eigenvalues. These eigenvalues are Â±1,
since Qâˆ’1MQ is orthogonal. The diagonal blocks 2Ã—2 are direct similitude
matrices. However, they are isometries, since Qâˆ’1MQ is orthogonal. Hence
they are rotation matrices.

7.5. The Orthogonal Groups O(p, q)
125
7.5.1
Notable Subgroups of O(p, q)
We assume here that p, q
â‰¥1, so that O(p, q) has four connected
components. We ï¬rst describe them.
Let us recall that if M âˆˆO(p, q) reads blockwise
M =

A
B
C
D

,
where A âˆˆMp(IR), etc. Then AT A = CT C + Ip is larger than Ip as a
symmetric matrix, so that det A cannot vanish. Similarly, DT D = BT B+Iq
shows that det D does not vanish. The continuous map M â†’(det A, det D)
thus sends O(p, q) to IRâˆ—Ã— IRâˆ—(in fact, to (IR \ (âˆ’1, 1))2). Since the sign
map from IRâˆ—to {âˆ’, +} is continuous, we may thus deï¬ne a continuous
function
O(p, q)
Ïƒâ†’
{âˆ’, +}2 âˆ¼(ZZ/2ZZ)2 ,
M
â†’
(sgn det A, sgn det D).
The diagonal matrices whose diagonal entries are Â±1 belong to O(p, q). It
follows that Ïƒ is onto. Since Ïƒ is continuous, the preimage GÎ± of an element
Î± of {âˆ’, +}2 is the union of some connected components of O(p, q); let n(Î±)
be the number of these components. Then n(Î±) â‰¥1 (Ïƒ being onto), and

Î± n(Î±) equals 4, the number of connected components of O(p, q). Since
there are four terms in this sum, we obtain n(Î±) = 1 for every Î±. Finally,
the connected components of O(p, q) are the GÎ±â€™s, where Î± âˆˆ{âˆ’, +}2.
The left multiplication by an element M of O(p, q) is continuous, bijec-
tive, whose inverse (another multiplication) is continuous. It thus induces a
permutation of the set Ï€0 of connected components of O(p, q). Since Ïƒ in-
duces a bijection between Ï€0 and {âˆ’, +}2, there exists thus a permutation
qM of {âˆ’, +}2 such that Ïƒ(MM â€²) = qM(Ïƒ(M â€²)). Similarly, the multiplica-
tion at right by M â€² is an homeomorphism, allowing to deï¬ne a permutation
pMâ€² of{âˆ’, +}2 such that Ïƒ(MM â€²) = pMâ€²(Ïƒ(M)). The equality
pMâ€²(Ïƒ(M)) = qM(Ïƒ(M â€²))
shows that pM and qM actually depend only on Ïƒ(M). In other words,
Ïƒ(MM â€²) depends only on Ïƒ(M) and Ïƒ(M â€²). A direct evaluation in the
special case of matrices in O(p, q)âˆ©On(IR) leads to the following conclusion.
Proposition 7.5.2 (p, q â‰¥1) The connected components of G = O(p, q)
are the sets GÎ± := Ïƒâˆ’1(Î±), deï¬ned by Î±1 det A > 0 and Î±2 det D > 0, when
a matrix M is written blockwise as above. The map Ïƒ : O(p, q) â†’{âˆ’, +}2
is a surjective group homomorphism; that is, Ïƒ(MM â€²) = Ïƒ(M)Ïƒ(M â€²). In
particular:
1. Gâˆ’1
Î±
= GÎ±;
2. GÎ± Â· GÎ±â€² = GÎ±Î±â€².

126
7. Exponential of a Matrix, Polar Decomposition, and Classical Groups
Remark: Ïƒ admits a right inverse, namely
Î± â†’M Î± := diag(Î±11, 1, . . . , 1, Î±21).
The group O(p, q) appears, therefore, as the semidirect product of G++
with (ZZ/2ZZ)2.
We deduce immediately from the proposition that O(p, q) possesses ï¬ve
open and closed normal subgroups, the preimages of the ï¬ve subgroups of
(ZZ/2ZZ)2:
â€¢ O(p, q) itself;
â€¢ G++, which we also denote by G0 (see Exercise 21), the connected
component of the unit element In,
â€¢ G++ âˆªGÎ±, for the three other choices of an element Î±.
One of these groups, namely G++ âˆªGâˆ’âˆ’is equal to the kernel SO(p, q)
of the homomorphism M â†’det M. In fact, this kernel is open and closed,
thus is the union of connected components of O(p, q). However the sign of
det M for M âˆˆGÎ± is that of Î±1Î±2, which can be seen directly from the
case of diagonal matrices M Î±.
7.5.2
The Lorentz Group O(1, 3)
If p = 1 and q = 3, the group O(1, 3) is isomorphic to the orthogonal
group of the Lorentz quadratic form dt2 âˆ’dx2
1 âˆ’dx2
2 âˆ’dx2
3, which deï¬nes
the space-time distance in special relativity.1 Each element M of O(1, 3)
corresponds to the transformation
 t,
x

â†’M
 t,
x

,
which we still denote by M, by abuse of notation. This transformation
preserve the light cone of equation t2 âˆ’x2
1 âˆ’x2
2 âˆ’x2
3 = 0. Since it is
a homeomorphism of IR4, it permutes the connected components of the
complement C of that cone. There are three such components (see Figure
7.1):
â€¢ the convex set C+ := {(t, x) | âˆ¥xâˆ¥< t};
â€¢ the convex set Câˆ’:= {(t, x) | âˆ¥xâˆ¥< âˆ’t};
â€¢ the â€œringâ€ A := {(t, x) | |t| < âˆ¥xâˆ¥}.
Clearly, C+ and Câˆ’are homeomorphic. For example, they are so via the
time reversal t â†’âˆ’t. However, they are not homeomorphic to A, because
the latter is homeomorphic to S2 Ã— IR2 (here, S2 denotes the unit sphere),
1We have selected a system of units in which the speed of light equals one.

7.6. The Symplectic Group Spn
127
x1
Câˆ’
x2
A
C+
t
Figure 7.1. The Lorentz cone.
which is not contractible, while a convex set is always contractible. Since
M is a homeomorphism, one deduces that necessarily, MA = A, while
MC+ = CÂ±, MCâˆ’= Câˆ“.
The transformations that preserve C+, and therefore every connected
component of C, form the orthochronous Lorentz group. Its elements are
those that send the vector e0 := (1, 0, 0, 0)T to C+; that is, those for which
the ï¬rst component of Me0 is positive. Since this component is A (here it
is nothing but a scalar), this group must be G++ âˆªG+âˆ’.
7.6
The Symplectic Group Spn
Let us study ï¬rst of all the maximal compact subgroup Spn âˆ©O2n. If
M =

A
B
C
D

,
with blocks of size n Ã— n, then M âˆˆSpn means that
AT C = CT A,
AT D âˆ’CT B = In,
BT D = DT B,
while M âˆˆO2n yields
AT A + CT C = In,
BT B + DT D = In,
BT A + DT C = 0n.

128
7. Exponential of a Matrix, Polar Decomposition, and Classical Groups
But since M T âˆˆSpn, we also have
ABT = BAT ,
ADT âˆ’BCT = In,
CDT = DCT .
Let us combine these equations:
B = B(AT A+CT C) = ABT A+(ADT âˆ’In)C = A(BT A+DT C)âˆ’C = âˆ’C.
Similarly,
D = D(AT A+CT C) = (In+CBT )A+CDT C = A+C(BT A+DT C) = A.
Hence
M =

A
B
âˆ’B
A

.
The remaining conditions are
AT A + BT B = In,
AT B = BT A.
This amounts to saying that A + iB is unitary. One immediately checks
that the map M â†’A + iB is an isomorphism from Spn onto Un.
Finally, if
N =

A
B
BT
D

is symmetric and NJ + JN = 02n, we have, in fact,
N =

A
B
B
âˆ’A

,
where A and B are symmetric. Hence G âˆ©Sym2n is homeomorphic to
Symn Ã— Symn, that is, to IRn(n+1).
Proposition 7.6.1 The symplectic group Spn is homeomorphic to Un Ã—
IRn(n+1).
Corollary 7.6.1 In particular, every symplectic matrix has determinant
+1.
Indeed, Proposition 7.6.1 shows that Spn is connected. Since the de-
terminant is continuous, with values in {âˆ’1, 1}, it is constant, equal to
+1.
7.7
Singular Value Decomposition
As we shall see in Exercise 8 (see also Exercise 12 in Chapter 4), the
eigenvalues of the matrix H in the polar decomposition of a given matrix
M are of some importance. They are called the singular values of M. Since
these are the square roots of the eigenvalues of M âˆ—M, one may even speak

7.7. Singular Value Decomposition
129
of the singular values of an arbitrary matrix, not necessarily invertible.
Recalling that (see Exercise 17 in Chapter 2) when M is n Ã— m, M âˆ—M and
MM âˆ—have the same nonzero eigenvalues, counting with multiplicities, one
may even speak of the singular values of a rectangular matrix, up to an
ambiguity concerning the multiplicity of the eigenvalue 0.
The main result of the section is the following.
Theorem 7.7.1 Let M
âˆˆMnÃ—m(CC) be given. Then there exist two
unitary matrices U âˆˆUn, V âˆˆUm and a quasi-diagonal matrix
D =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
s1
...
sr
0
...
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
with s1, . . . , sr âˆˆ(0, +âˆ), such that M = UDV . The numbers s1, . . . , sr
are uniquely deï¬ned up to permutation; they are the nonzero singular values
of M. In particular, r is the rank of M.
If M âˆˆMnÃ—m(IR), then one may choose U, V to be real orthogonal.
Remark: The factorization given in the theorem is far from being unique,
even for invertible square matrices. In fact, the number of real degrees of
freedom in that factorization is n2+m2+min(n, m), which is always greater
than the dimension 2nm of MnÃ—m(CC) as an IR-vector space.
Proof
Since MM âˆ—is positive semideï¬nite, we may write its eigenvalues as
s2
1, . . . , s2
r, 0, . . . , where the sjâ€™s, the singular values of M, are positive
real numbers. The spectrum of M âˆ—M has the same form, except for the
multiplicity of 0. Indeed, the multiplicities of 0 as an eigenvalue of MM âˆ—
and MM âˆ—, respectively, diï¬€er by n âˆ’m, while the multiplicities of other
eigenvalues are the same for both matrices. We set S = diag(s1, . . . , sr).
Since M and MM âˆ—have the same rank, and since R(MM âˆ—) âŠ‚R(M), we
have R(MM âˆ—) = R(M). Since MM âˆ—is Hermitian, its kernel is R(M)âŠ¥,
where orthogonality is relative to the canonical scalar product; with the
duality formula, we conclude that ker MM âˆ—= ker M âˆ—. Now we are in
position to state that
CCn = R(MM âˆ—) âŠ•âŠ¥ker M âˆ—.
Therefore, there exists an orthonormal basis {u1, . . . , un} of CCn consist-
ing of eigenvectors of MM âˆ—, associated to the s2
jâ€™s, followed by vectors of
ker M âˆ—. Let us form the unitary matrix
U = (u1, . . . , un).
Written blockwise, we have U = (UR, UK), where
MM âˆ—UR = URS2,
M âˆ—UK = 0.

130
7. Exponential of a Matrix, Polar Decomposition, and Classical Groups
Let now deï¬ne VR := M âˆ—URSâˆ’1. From above, we have
V âˆ—
RVR = Sâˆ’1U âˆ—
RMM âˆ—URSâˆ’1 = Ir.
This means that the columns v1, . . . , vr of VR constitute an orthonormal
family.
Noting that these column vectors belong to R(M âˆ—), that is, to (ker M)âŠ¥,
a subspace of codimension r, we see that {v1, . . . , vr} can be extended to
an orthonormal basis {v1, . . . , vm} of CCm, where vr+1, . . . belong to ker M.
Let V =: (VR, VK) be the unitary matrix whose columns are v1, . . .
We now compute blockwise the product U âˆ—MV . From MVK = 0 and
M âˆ—U âˆ—
K = 0, we get
U âˆ—MV =

U âˆ—
RMVR
0
0
0

.
Finally, we obtain
U âˆ—
RMVR = U âˆ—
RMM âˆ—URSâˆ’1 = U âˆ—
RURS = S.
7.8
Exercises
1. Show that the square root map from HPDn into itself is continuous.
2. Let M âˆˆMn(k) be given, with k = IR or CC. Show that there ex-
ists a polynomial P âˆˆk(X), of degree at most n âˆ’1, such that
P(M) = exp M. However, show that this polynomial cannot be
chosen independently of the matrix.
Compute this polynomial when M is nilpotent.
3. For t âˆˆIR, deï¬ne Pascalâ€™s matrix P(t) by pij(t) = 0 if i < j (the
matrix is lower triangular) and
pij(t) = tiâˆ’j
 i âˆ’1
j âˆ’1

otherwise. Let us emphasize that for just this once in this book, P
is an inï¬nite matrix, meaning that its indices range over the inï¬nite
set IN âˆ—. Compute P â€²(t) and deduce that there exists a matrix L such
that P(t) = exp(tL). Compute L explicitly.
4. Let I be an interval of IR and t â†’P(t) be a map of class C1 with
values in Mn(IR) such that for each t, P(t) is a projector: P(t)2 =
P(t).
(a) Show that the rank of P(t) is constant.
(b) Show that P(t)P â€²(t)P(t) = 0n.

7.8. Exercises
131
(c) Let us deï¬ne Q(t)
:=
[P â€²(t), P(t)]. Show that P â€²(t)
=
[Q(t), P(t)].
(d) Let t0 âˆˆI be given. Show that the diï¬€erential equation U â€² = QU
possesses a unique solution in I such that U(t0) = In. Show that
P(t) = U(t)P(t0)U(t)âˆ’1.
5. Show that the set of projectors of given rank p is a connected subset
in Mn(CC).
6.
(a) Let A âˆˆHPDn and B âˆˆHn be given. Show that AB is di-
agonalizable with real eigenvalues (though it is not necessarily
Hermitian). Show also that the sum of the multiplicities of the
positive eigenvalues (respectively zero, respectively negative) is
the same for AB as for B.
(b) Let A, B, C be three Hermitian matrices such that ABC âˆˆHn.
Show that if three of the matrices A, B, C, ABC are positive
deï¬nite, then the fourth is positive deï¬nite too.
7. Let M âˆˆGLn(CC) be given and M = HQ be its polar decomposition.
Show that M is normal if and only if HQ = QH.
8. The deformation of an elastic body is represented at each point by a
square matrix F âˆˆGL+
3 (IR) (the sign + expresses that det F > 0).
More generally, F âˆˆGL+
n (IR) in other space dimensions. The density
of elastic energy is given by a function F â†’W(F) âˆˆIR+.
(a) The principle of frame indiï¬€erence says that W(QF) = W(F)
for every F âˆˆGL+
n (IR) and every rotation Q. Show that there
exists a map w : SPDn â†’IR+ such that W(F) = w(H), where
F = QH is the polar decomposition.
(b) When the body is isotropic, we also have W(FQ) = W(F), for
every F âˆˆGL+
n (IR) and every rotation Q. Show that there exists
a map Ï† : IRn â†’IR+ such that W(F) = Ï†(h1, . . . , hn), where
the hj are the entries of the characteristic polynomial of H. In
other words, W(F) depends only on the singular values of F.
9. We use Schurâ€™s norm âˆ¥Aâˆ¥= (Tr Aâˆ—A)1/2.
(a) If A âˆˆMn(CC), show that there exists Q âˆˆUn such that âˆ¥A âˆ’
Qâˆ¥â‰¤âˆ¥A âˆ’Uâˆ¥for every U âˆˆUn. We shall deï¬ne S := Qâˆ’1A.
We therefore have âˆ¥S âˆ’Inâˆ¥â‰¤âˆ¥S âˆ’Uâˆ¥for every U âˆˆUn.
(b) Let H âˆˆHn be a Hermitian matrix. Show that exp(itH) âˆˆUn
for every t âˆˆIR. Compute the derivative at t = 0 of
t â†’âˆ¥S âˆ’exp(itH)âˆ¥2
and deduce that S âˆˆHn.
(c) Let D be a diagonal matrix, unitarily similar to S. Show that
âˆ¥Dâˆ’Inâˆ¥â‰¤âˆ¥DU âˆ’Inâˆ¥for every U âˆˆUn. By selecting a suitable
U, deduce that S â‰¥0n.

132
7. Exponential of a Matrix, Polar Decomposition, and Classical Groups
(d) If A âˆˆGLn(CC), show that QS is the polar decomposition of A.
(e) Deduce that if H âˆˆHPDn and if U âˆˆUn, U Ì¸= In, then
âˆ¥H âˆ’Inâˆ¥< âˆ¥H âˆ’Uâˆ¥.
(f) Finally, show that if H âˆˆHn, H â‰¥0n and U âˆˆUn, then
âˆ¥H âˆ’Inâˆ¥â‰¤âˆ¥H âˆ’Uâˆ¥.
10. Let A âˆˆMn(CC) and h âˆˆCC. Show that In âˆ’hA is invertible as soon
as |h| < 1/Ï(A). One then denotes its inverse by R(h; A).
(a) Let r âˆˆ(0, 1/Ï(A)). Show that there exists a c0 > 0 such that
for every h âˆˆCC with |h| â‰¤r, we have
âˆ¥R(h; A) âˆ’ehAâˆ¥â‰¤c0|h|2.
(b) Verify the formula
Cm âˆ’Bm
=
(C âˆ’B)Cmâˆ’1 + Â· Â· Â· + Blâˆ’1(C âˆ’B)Cmâˆ’l + Â· Â· Â·
+ Â· Â· Â· + Bmâˆ’1(C âˆ’B),
and deduce the bound
âˆ¥R(h; A)m âˆ’emhAâˆ¥â‰¤c0m|h|2ec2m|h|,
when |h| â‰¤r and m âˆˆIN.
(c) Show that for every t âˆˆCC,
lim
mâ†’+âˆR(t/m; A)m = etA.
11.
(a) Let J(a; r) be a Jordan block of size r, with a âˆˆCCâˆ—. Let b âˆˆCC be
such that a = eb. Show that there exists a nilpotent N âˆˆMr(CC)
such that J(a; r) = exp(bIr + N).
(b) Show that exp : Mn(CC) â†’GLn(CC) is onto, but that it is not
one-to-one. Deduce that X â†’X2 is onto GLn(CC). Verify that
it is not onto Mn(CC).
12.
(a) Show that the matrix
J2 =
 âˆ’1
1
0
âˆ’1

is not the square of any matrix of M2(IR).
(b) Show, however, that the matrix J4 := diag(J2, J2) is the square
of a matrix of M4(IR).
Show also that the matrix
J3 =
 J2
I2
02
J2

is not the square of a matrix of M4(IR).
(c) Show that J2 is not the exponential of any matrix of M2(IR).
Compare with the previous exercise.

7.8. Exercises
133
(d) Show that J4 is the exponential of a matrix of M4(IR), but that
J3 is not.
13. Let An(CC) be the set of skew-Hermitian matrices of size n. Show
that exp : An(CC) â†’Un is onto. Hint: If U is unitary, diagonalize it.
14.
(a) Let Î¸ âˆˆIR be given. Compute exp B, where
B =

0
Î¸
âˆ’Î¸
0

.
(b) Let An(IR) be the set of real skew-symmetric matrices of size n.
Show that exp : An(IR) â†’SOn is onto. Hint: Use the reduction
of direct orthogonal matrices.
15. Let Ï† : Mn(IR) â†’IR be a nonnull map satisfying Ï†(AB) = Ï†(A)Ï†(B)
for every A, B âˆˆMn(IR). If Î± âˆˆIR, we set Î´(Î±) = |Ï†(Î±In)|1/n. We
have seen, in Exercise 16 of Chapter 3, that |Ï†(M)| = Î´(det M) for
every M âˆˆMn(IR).
(a) Show that on the range of M â†’M 2 and on that of M â†’exp M,
Ï† â‰¡Î´ â—¦det.
(b) Deduce that Ï† â‰¡Î´â—¦det on SOn (use Exercise 14) and on SPDn.
(c) Show that either Ï† â‰¡Î´ â—¦det or Ï† â‰¡(sgn(det))Î´ â—¦det.
16. Let A be a K-Banach algebra (K = IR or CC) with a unit denoted by
e. If x âˆˆA, deï¬ne x0 := e.
(a) Given x âˆˆA, show that the series

mâˆˆIN
1
m!xm
converges normally, hence converges in A. Its sum is denoted by
exp x.
(b) If x, y âˆˆA, [x, y] = xy âˆ’yx is called the â€œcommutatorâ€ of x and
y. Show that [x, y] = 0 implies
exp(x + y) = (exp x)(exp y),
[x, exp y] = 0.
(c) Show that the map t â†’exp tx is diï¬€erentiable on IR, with
d
dt exp tx = x exp tx = (exp tx)x.
(d) Let x, y âˆˆA be given. Assume that [x, y] commutes with x and
y.
i. Show that (exp âˆ’tx)xy(exp tx) = xy + t[y, x]x.
ii. Deduce that [exp âˆ’tx, y] = t[y, x] exp âˆ’tx.

134
7. Exponential of a Matrix, Polar Decomposition, and Classical Groups
iii. Compute the derivative of t â†’(exp âˆ’ty)(exp âˆ’tx) exp t(x+
y). Finally, prove the Campbellâ€“Hausdorï¬€formula
exp(x + y) = (exp x)(exp y)

exp 1
2[y, x]

.
(e) In A = M3(IR), construct an example that satisï¬es the above
hypothesis ([x, y] commutes with x and y), where [x, y] is
nonzero.
17. Show that the map
H â†’f(H) := (iIn + H)(iIn âˆ’H)âˆ’1
induces a homeomorphism from Hn onto the set of matrices of Un
whose spectrum does not contain âˆ’1. Find an equivalent of f(tH) âˆ’
exp(âˆ’2itH) as t â†’0.
18. Let G be a group satisfying the hypotheses of Proposition 7.3.2.
(a) Show that G is a Lie algebra, meaning that it is stable under the
bilinear map (A, B) â†’[A, B] := AB âˆ’BA.
(b) Show that for t â†’0+,
exp(tA) exp(tB) exp(âˆ’tA) exp(âˆ’tB) = In + t2[A, B] + O(t3).
Deduce another proof of the stability of G by [Â·, Â·].
(c) Show that the map M â†’[A, M] is a derivation, meaning that
the Jacobi identity
[A, [B, C]] = [[A, B], C] + [B, [A, C]]
holds.
19. In the case p = 1, q â‰¥1, show that G++ âˆªG+âˆ’is the set of matrices
M âˆˆO(p, q) such that the image under M of the â€œtimeâ€ vector
(1, 0, . . . , 0)T belongs to the convex cone whose equation is
x1 >
$
x2
2 + Â· Â· Â· + x2n.
20. Assume that p, q â‰¥1 and consider the group O(p, q). Deï¬ne G0 :=
G++. Since âˆ’In âˆˆO(p, q), we denote by (Âµ, Î²) the indices for which
âˆ’In âˆˆGÂµ,Î².
If H âˆˆGLn(IR), denote by ÏƒH the conjugation M â†’Hâˆ’1MH.
(a) Let H âˆˆG be given. Show that ÏƒH (or rather its restriction to
G0) is an automorphism of G0.
(b) Let H âˆˆMn(IR) be such that HM = MH for every M âˆˆG0.
Show that HN = NH for every N âˆˆG. Deduce that H is a
homothety.
(c) Let H âˆˆG. Show that there exists K âˆˆG0 such that ÏƒH = ÏƒK
if and only if H âˆˆG0 âˆªGÂµ,Î².

7.8. Exercises
135
21. A topological group is a group G endowed with a topology for which
the maps (g, h) â†’gh and g â†’gâˆ’1 are continuous. Show that in a
topological group, the connected component of the unit element is
a normal subgroup. Show also that the open subgroups are closed.
Give an example of a closed subgroup that is not open.
22. One identiï¬es IR2n with CCn by the map
 x
y

â†’x + iy.
Therefore, every matrix M âˆˆM2n(IR) deï¬nes an IR-linear map Ëœ
M
form CCn into itself.
(a) Let
M =
 A
B
C
D

âˆˆM2n(IR)
be given. Under what condition on the blocks A, B, C, D is the
map Ëœ
M CC-linear?
(b) Show that M â†’Ëœ
M is an isomorphism from Spn âˆ©O2n onto Un.

8
Matrix Factorizations
The direct solution (by Cramerâ€™s method) of a linear system Mx = b,
where M âˆˆGLn(k) (b âˆˆkn) is computationally expensive, especially if
one wishes to solve the system many times with various values of b. In the
next chapter we shall study iterative methods for the case k = IR or CC.
Here we concentrate on a simple idea: To decompose M as a product PQ
in such a way that the resolution of the intermediate systems Py = b and
Qx = y is â€œcheap.â€ In general, at least one of the matrices is triangular.
For example, if P is lower triangular (pij = 0 if i < j), then its diagonal
entries pii are nonzero, and one may solve the system Py = b step by step:
y1
=
b1
p11
,
...
yi
=
bi âˆ’pi1y1 âˆ’Â· Â· Â· âˆ’pi,iâˆ’1yiâˆ’1
pii
,
...
yn
=
bn âˆ’pn1y1 âˆ’Â· Â· Â· âˆ’pn,nâˆ’1ynâˆ’1
pnn
.
The computation of yi needs 2iâˆ’1 operations and the ï¬nal result is obtained
in n2 operations. This is not expensive if one notes that computing the
product x = M âˆ’1b (assuming that M âˆ’1 is computed once and for all, an
expensive task) needs 2n2 âˆ’n operations.

8.1. The LU Factorization
137
Another example of easily invertible matrices is the orthogonal matrices:
If Q âˆˆOn (or Q âˆˆUn), then Qx = y amounts to x = QTy (or x = Qâˆ—y),
which is computed in O(n2) operations.
The techniques described below are often called direct solving methods.
8.1
The LU Factorization
Deï¬nition 8.1.1 Let M âˆˆGLn(k), where k is a ï¬eld. We say that M
admits an LU factorization if there exist in GLn(k) two matrices L (lower
triangular with 1â€™s on the diagonal) and U (upper triangular) such that
M = LU.
Remarks:
â€¢ The diagonal entries of U are not equal to 1 in general. The LU
factorization is thus asymmetric with respect to L and U.
â€¢ The letters L and U recall the shape of the matrices: L for lower and
U for upper.
â€¢ If there exists an LU factorization (which is unique, as we shall see
below), then it can be computed by induction on the size of the
matrix. The algorithm is provided in the proof of the next theorem.
Indeed, if N (p) denotes the matrix extracted from N by keeping only
the ï¬rst p rows and columns, we have easily
M (p) = L(p)U (p),
where the matrices L(p) and U (p) have the required properties.
Deï¬nition 8.1.2 The leading principal minors of M are the determinants
of the matrices M (p), for 1 â‰¤p â‰¤n.
Theorem 8.1.1 The matrix M âˆˆGLn(k) admits an LU factorization if
and only if its leading principal minors are nonzero. When this condition
is fulï¬lled, the LU factorization is unique.
Proof
Let us begin with uniqueness: If LU = Lâ€²U â€², then (Lâ€²)âˆ’1L = U â€²U âˆ’1,
which reads Lâ€²â€² = U â€²â€², where Lâ€²â€² and U â€²â€² are triangular of opposite types,
the diagonal entries of Lâ€²â€² being 1â€™s. We deduce Lâ€²â€² = U â€²â€² = In; that is,
Lâ€² = L, U â€² = U.
We next assume that M admits an LU factorization. Then det M (p) =
det L(p) det U (p) = 
1â‰¤jâ‰¤p ujj, which is nonzero because U is invertible.
We prove the converse (the existence of an LU factorization) by induction
on the size of the matrices. It is clear if n = 1. Otherwise, let us assume
that the statement is true up to the order n âˆ’1 and let M âˆˆGLn(k) be

138
8. Matrix Factorizations
given, with nonzero leading principal minors. We look for L and U in the
blockwise form
L =
 Lâ€²
0
XT
1

,
U =
 U â€²
Y
0
u

,
with Lâ€², U â€² âˆˆMnâˆ’1(k), etc. We likewise obtain the description
M =

M â€²
R
ST
m

.
Multiplying blockwise, we obtain the equations
Lâ€²U â€² = M â€²,
Lâ€²Y = R,
(U â€²)T X = S,
u = m âˆ’XT Y.
By assumption, the leading principal minors of M â€² are nonzero. The induc-
tion hypothesis guarantees the existence of the factorization M â€² = Lâ€²U â€².
Then Y and X are the unique solutions of (triangular) Cramer systems.
Finally, u is explicitly given.
Let us now compute the number of operations needed in the computation
of L and U. We pass from a factorization in GLnâˆ’1(k) to a factorization in
GLn(k) by means of the computations of X ((n âˆ’1)(n âˆ’2) operations), Y
((nâˆ’1)2 operations) and u (2(nâˆ’1) operations), for a total of (nâˆ’1)(2nâˆ’1)
operations. Finally, the computation ex nihilo of an LU factorization costs
P(n) operations, where P is a polynomial of degree three, with P(X) =
2X3/3 + Â· Â· Â· .
Proposition 8.1.1 The LU factorization is computable in
2
3n3 + O(n2)
operations.
One says that the complexity of the LU factorization is 2
3n3.
Remark: When all leading principal minors but the last (det M) are
nonzero, the proof above furnishes a factorization M = LU, in which U is
not invertible; that is, unn = 0.
8.1.1
Block Factorization
One can likewise perform a blockwise LU factorization. If n = p1 + Â· Â· Â· + pr
with pj â‰¥1, the matrices L and U will be block-triangular. The diagonal
blocks are square, of respective sizes p1, . . . , pr. Those of L are of the form
Ipj, while those of U are invertible. A necessary and suï¬ƒcient condition
for such a factorization to exist is that the leading principal minors of M,
of orders p1 + Â· Â· Â· + pj (j â‰¤r), be nonzero. As above, it is not necessary
that the last minor det M be nonzero. Such a factorization is useful for the
resolution of the linear system MX = b if the diagonal blocks of U are
easily inverted, for instance if their sizes are small enough (say pj â‰ˆâˆšn).
An interesting application of block factorization is the computation of
the determinant by the Schur complement formula:

8.1. The LU Factorization
139
Proposition 8.1.2 Let M âˆˆMn(k) read blockwise
M =
 A
B
C
D

,
where the diagonal blocks are square and A is invertible. Then
det M = det A det(D âˆ’CAâˆ’1B).
Of course, this formula generalizes det M = ad âˆ’bc, which is valid only for
2 Ã— 2 matrices. The matrix D âˆ’CAâˆ’1B is called the Schur complement of
A in M.
Proof
Since A is invertible, M admits a blockwise LU factorization, with the
same subdivision. We easily compute
L =

I
0
CAâˆ’1
I

,
U =
 A
B
0
D âˆ’CAâˆ’1B

.
Then det M = det L det U furnishes the expected formula.
Corollary 8.1.1 Let M âˆˆGLn(k), with n = 2m, read blockwise
M =

A
B
C
D

,
A, B, C, D âˆˆGLm(k).
Then
M âˆ’1 =
 (A âˆ’BDâˆ’1C)âˆ’1
(C âˆ’DBâˆ’1A)âˆ’1
(B âˆ’ACâˆ’1D)âˆ’1
(D âˆ’CAâˆ’1B)âˆ’1

.
Proof
We can verify the formula by multiplying by M. The only point to show is
that the inverses are meaningful, that is, that Aâˆ’BDâˆ’1C, . . . are invertible.
Because of the symmetry of the formulas, it is enough to check it for a single
term, namely D âˆ’CAâˆ’1B. However, det(D âˆ’CAâˆ’1B) = det M/ det A,
which is nonzero by assumption.
We might add that as soon as M âˆˆGLn(k) and A âˆˆGLp(k) (even if
p Ì¸= n/2), then
M âˆ’1 =

Â·
Â·
Â·
(D âˆ’CAâˆ’1B)âˆ’1

,
because M admits the blockwise LU factorization and
M âˆ’1 = U âˆ’1Lâˆ’1 =
 Aâˆ’1
Â·
0
(D âˆ’CAâˆ’1B)âˆ’1

Â·
 I
0
Â·
I

.

140
8. Matrix Factorizations
8.1.2
Complexity of Matrix Inversion
We can now show that the complexity of inverting a matrix is not higher
than that of matrix multiplication, at equivalent sizes. We assume here that
k = IR or CC.
Notation 8.1.1 We denote by Jn the number of operations in k used in
the inversion of an n Ã— n matrix, and by Pn the number of operations (in
k) used in the product of two n Ã— n matrices.
Of course, the number Jn must be understood for generic matrices, that
is, for matrices within a dense open subset of Mn(k). More important,
Jn, Pn also depend on the algorithm chosen for inversion or for multipli-
cation. In the sequel we wish to adapt the inversion algorithm to the one
used for multiplication.
Let us examine ï¬rst of all the matrices whose size n has the form 2k.
We decompose the matrices M âˆˆGLn(k) blockwise, with blocks of size
n/2 Ã— n/2. The condition A âˆˆGLn/2(k) deï¬nes a dense open set, since
M â†’det A is a nonzero polynomial. Suppose that we are given an inver-
sion algorithm for generic matrices in GLn/2(k) in jkâˆ’1 operations. Then
blockwise LU factorization and the formula M âˆ’1 = U âˆ’1Lâˆ’1 furnish an
inversion algorithm for generic matrices in GLn(k). We can then bound
jk by means of jkâˆ’1 and the number Ï€kâˆ’1 = P2kâˆ’1 of operations used in
the computation of the product of two matrices of size 2kâˆ’1 Ã— 2kâˆ’1. We
shall denote also by Ïƒk = 22k the number of operations involved in the
computation of the sum of matrices in M2k(k).
To compute M âˆ’1, we ï¬rst compute Aâˆ’1, then CAâˆ’1, which gives us
Lâˆ’1 in jkâˆ’1 + Ï€kâˆ’1 operations. Then we compute (D âˆ’CAâˆ’1B)âˆ’1 (this
amounts to Ïƒkâˆ’1 + Ï€kâˆ’1 + jkâˆ’1 operations) and Aâˆ’1B(D âˆ’CAâˆ’1B)âˆ’1
(cost: 2Ï€kâˆ’1), which furnishes U âˆ’1. The computation of U âˆ’1Lâˆ’1 is done
at the cost Ïƒkâˆ’1 + 2Ï€kâˆ’1. Finally,
jk â‰¤2jkâˆ’1 + 2Ïƒkâˆ’1 + 6Ï€kâˆ’1.
In other words,
2âˆ’kjk âˆ’21âˆ’kjkâˆ’1 â‰¤2kâˆ’1 + 3 Â· 21âˆ’kÏ€kâˆ’1.
(8.1)
The complexity of the product in Mn(k) obeys the inequalities
n2 â‰¤Pn â‰¤n2(2n âˆ’1).
The ï¬rst inequality is due to the number of data (2n2) and the fact that
each operation involves only two of them. The second is given by the naive
algorithm that consists in computing n2 scalar products.
Lemma 8.1.1 If Pn â‰¤cÎ±nÎ± (with 2 â‰¤Î± â‰¤3), then jl â‰¤CÎ±Ï€l, where
CÎ± = 1 + 3cÎ±/(2Î±âˆ’1 âˆ’1).
It is enough to sum (8.1) from k = 1 to l and use the inequality 1 + q +
Â· Â· Â· + qlâˆ’1 â‰¤ql/(q âˆ’1) for q > 1.

8.1. The LU Factorization
141
When n is not a power of 2, we obtain M âˆ’1 by computing the inverse of a
block-diagonal matrix diag(M, I), whose size N satisï¬es n â‰¤N = 2l < 2n.
We obtain Jn â‰¤jl â‰¤CÎ±Ï€l. Finally, we have the following result.
Proposition 8.1.3 If the complexity Pn of the product in Mn(CC) is
bounded by cÎ±nÎ±, then the complexity Jn of inversion in GLn(CC) is
bounded by dÎ±nÎ±, where
dÎ± =

1 +
3cÎ±
2Î±âˆ’1 âˆ’1

2Î±.
That can be summarized as follows:
Those who know how to multiply know also how to invert.
8.1.3
Complexity of the Matrix Product
The ideas that follow apply to the product of rectangular matrices, but for
the sake of simplicity, we present only the case of square matrices.
As we have seen above, the complexity Pn of matrix multiplication in
Mn(k) is O(n3). However, better algorithms will allow us to improve the
exponent 3. The simplest and oldest one is Strassenâ€™s algorithm, which
uses a recursion. We note ï¬rst that there exists a way of computing the
product of two 2 Ã— 2 matrices by means of 7 multiplications and 18 addi-
tions. Two features of Strassenâ€™s formula are essential. First, the number of
multiplications that it involves is stricly less than that (eight) of the naive
algorithm. The second is that the method is valid when the matrices have
entries in a noncommutative ring, and so it can be employed for two matri-
ces M, N âˆˆMn(k), considered as elements of M2(A), with A := Mn/2(k).
This trick yields
Pn â‰¤7Pn/2 + 9n2/2.
For n = 2l, we then have
7âˆ’lÏ€l âˆ’71âˆ’lÏ€lâˆ’1 â‰¤9
2
4
7
l
,
which, after summation from k = 1 to l, gives
7âˆ’lÏ€l â‰¤9
2
1
1 âˆ’4/7,
because of 4
7 < 1. Finally,
Ï€l â‰¤21
2 7l.
When n is not a power of two, one chooses l such that n â‰¤2l < 2n and we
obtain the following result.

142
8. Matrix Factorizations
Proposition 8.1.4 The complexity of the multiplication of nÃ—n matrices
is O(nÎ±), with Î± = log 7/ log 2 = 2.807 . . . More precisely,
Pn â‰¤147
2 n
log 7
log 2 .
The exponent Î± can be improved, at the cost of greater complication and a
larger constant cÎ±. The best exponent known in 1997, due to Coppersmith
and Winograd [11], is Î± = 2.376 . . . A rather complete analysis can be
found in the book by P. BÂ¨urgisser, M. Clausen, and M. A. Shokrollahi [7].
Here is Strassenâ€™s formula [33]. Let M, N âˆˆM2(A), with
M =

a
b
c
d

,
N =

x
y
z
t

.
One ï¬rst forms the expressions x1 = (a + d)(x + t), x2 = (c + d)x, x3 =
a(yâˆ’t), x4 = d(zâˆ’x), x5 = (a+b)t, x6 = (câˆ’a)(x+y), x7 = (bâˆ’d)(z+t).
Then one computes the product
MN =

x1 + x4 âˆ’x5 + x7
x3 + x5
x2 + x4
x1 âˆ’x2 + x3 + x6

.
8.2
Choleski Factorization
In this section k = IR, and we consider symmetric positive deï¬nite matrices.
Theorem 8.2.1 Let M âˆˆSPDn. Then there exists a unique lower trian-
gular matrix L âˆˆMn(IR), with strictly positive diagonal entries, satisfying
M = LLT.
Proof
Let us begin with uniqueness. If L1 and L2 have the properties stated
above, then In = LLT , for L = Lâˆ’1
2 L1, which still has the same form. In
other words, L = Lâˆ’T, where both sides are triangular matrices, but of
opposite types (lower and upper). The equality shows that L is actually
diagonal, with L2 = In. Since its diagonal is positive, we obtain L = In;
that is, L2 = L1.
We shall give two constructions of L.
First method. The matrix M (p) is positive deï¬nite (test the quadratic
form induced by M on the linear subspace IRp Ã— {0}). The lead-
ing principal minors of M are thus nonzero and there exists an LU
factorization M = L0U0. Let D be the diagonal of U0, which is in-
vertible. Then U0 = DU1, where the diagonal entries of U1 equal
1. By transposition, we have M = U T
1 D0LT
0 . From uniqueness of
the LU factorization, we deduce U1 = LT
0 and M = L0DLT
0 . Then
L =
âˆš
DL0 satisï¬es the conditions of the theorem. Observe that
D > 0 because D = PMP T , with P = Lâˆ’1
0 .

8.3. The QR Factorization
143
Second method. We proceed by induction on n. The statement is clear
if n = 1. Otherwise, we seek an L of the form
L =

Lâ€²
0
XT
l

,
knowing that
M =
 M â€²
R
RT
m

.
The matrix Lâ€² is obtained by Choleski factorization of M â€², which
belongs to SPDnâˆ’1. Then X is obtained by solving Lâ€²X = R. Finally,
l is a square root of m âˆ’âˆ¥Xâˆ¥2. Since 0 < det M = (l det Lâ€²)2, we see
that m âˆ’âˆ¥Xâˆ¥2 > 0; we thus choose l =

m âˆ’âˆ¥Xâˆ¥2. This method
again shows uniqueness.
Remark: Choleski factorization extends to Hermitian positive deï¬nite ma-
trices. In that case, L has complex entries, but its diagonal entries are still
real and positive.
8.3
The QR Factorization
In this section k = IR or CC, the real case being a particular case of the
complex one.
Proposition 8.3.1 Let M âˆˆGLn(CC) be given. Then there exist a unitary
matrix Q and an upper triangular matrix R, whose diagonal entries are real
positive, such that M = QR. This factorization is unique.
We observe that the condition on the numbers rjj is essential for unique-
ness. In fact, if D is diagonal with |djj| = 1 for every j, then Qâ€² := Q Â¯D is
unitary, Râ€² := DR is upper triangular, and M = Qâ€²Râ€², which gives an in-
ï¬nity of factorizations â€œQU.â€ Even in the real case, where Q is orthogonal,
there are 2n â€œQUâ€ factorizations.
Proof
We ï¬rst prove uniqueness. If (Q1, R1) and (Q2, R2) give two factoriza-
tions, then Q = R, with Q := Qâˆ’1
2 Q1 and R := R2Râˆ’1
1 . Since Q is unitary,
we deduce Qâˆ—= Râˆ’1, or Q = Râˆ’âˆ—. This shows (recall that the inverse of a
triangular matrix is a triangular matrix of same type) that Q is simultane-
ously upper and lower triangular, and is therefore diagonal. Additionally,
its diagonal part is strictly positive. Then Q2 = Qâˆ—Q = In gives Q = In.
Finally, Q2 = Q1 and consequently, R2 = R1.
The existence follows from that of Choleski factorization. If M
âˆˆ
GLn(CC), the matrix M âˆ—M is Hermitian positive deï¬nite, hence admits a
Choleski factorization Râˆ—R, where R is upper triangular with real positive

144
8. Matrix Factorizations
diagonal entries. Deï¬ning Q := MRâˆ’1, we have
Qâˆ—Q = Râˆ’âˆ—M âˆ—MRâˆ’1 = Râˆ’âˆ—Râˆ—RRâˆ’1 = In;
hence Q is unitary. Finally, M = QR by construction.
The method used above is unsatisfactory from a practical point of view,
because one can compute Q and R directly, at a lower cost, without com-
puting M âˆ—M or its Choleski factorization. Moreover, the direct method,
which we shall present now, is based on a theoretical observation: The QR
factorization is nothing but the Gramâ€“Schmidt orthonormalization proce-
dure in CCn, endowed with the canonical scalar product âŸ¨Â·, Â·âŸ©. In fact, if
V 1, . . . , V n denote the column vectors of M, then giving M in GLn(CC)
amounts to giving a basis of CCn. If Y 1, . . . , Y n denote the column vectors
of Q, then {Y 1, . . . , Y n} is an orthonormal basis. Moreover, if M = QR,
then
V k =
k

j=1
rjkY j.
Denoting by Ek the linear subspace spanned by Y 1, . . . , Y k, of dimension
k, one sees that V 1, . . . , V k are in Ek; that is, {V 1, . . . , V k} is a basis of
Ek. Hence, the columns of Q are obtained by the Gramâ€“Schmidt procedure,
applied to the columns of M.
The practical computation of Q and R is done by induction on k. If
k = 1, then
r11 = âˆ¥V 1âˆ¥,
Y 1 = 1
r11
V 1.
If k > 1, and if Y 1, . . . , Y kâˆ’1 are already known, one looks for Y k and the
entries rjk (j â‰¤k). For j < k, we have
rjk = âŸ¨V k, Y jâŸ©.
Then
rkk = âˆ¥Zkâˆ¥,
Y k =
1
rkk
Zk,
where
Zk := V k âˆ’
kâˆ’1

j=1
rjkY j.
Let us examine the complexity of the procedure described above. To
pass from the step k âˆ’1 to the step k, one computes k âˆ’1 scalar products,
then Zk, its norm, and ï¬nally Y k. This requires (4n âˆ’1)k + 3n operations.
Summing from k = 1 to n yields 2n3 + O(n2) operations. This method is
not optimal, as we shall see in Section 10.2.3.

8.4. The Mooreâ€“Penrose Generalized Inverse
145
The interest of this construction lies also in giving a more complete
statement than Proposition 8.3.1:
Theorem 8.3.1 Let M âˆˆMn(CC) be a matrix of rank p. There exists
Q âˆˆUn and an upper triangular matrix R, with rll âˆˆIR+ for every l and
rjk = 0 for j > p, such that M = QR.
Remarks: The QR factorization of a singular matrix (i.e., a noninvertible
one) is not unique. There exists, in fact, a QR factorization for rectangular
matrices, in which R is a â€œquasi-triangularâ€ matrix.
8.4
The Mooreâ€“Penrose Generalized Inverse
The resolution of a general linear system Ax = b, where A may be singular
and may even not be square, is a delicate question, whose treatment is
made much simpler by the use of the Mooreâ€“Penrose generalized inverse.
We begin with the fundamental theorem.
Theorem 8.4.1 Let A âˆˆMnÃ—m(CC) be given. There exists a unique matrix
Aâ€  âˆˆMmÃ—n(CC), called the Mooreâ€“Penrose generalized inverse, satisfying
the following four properties:
1. AAâ€ A = A;
2. Aâ€ AAâ€  = Aâ€ ;
3. AAâ€  âˆˆHn;
4. Aâ€ A âˆˆHm.
Finally, if A has real entries, then so has Aâ€ .
When A âˆˆGLn(CC), Aâ€  coincides with the standard inverse Aâˆ’1, since
the latter obviously satisï¬es the four properties. More generaly, if A is
onto, then property 1 shows that AAâ€  = In; i.e., Aâ€  is a right inverse of A.
Likewise, if A is one-to-one, then Aâ€ A = Im; i.e., Aâ€  is a left inverse of A.
Proof
We ï¬rst remark that if X is a generalized inverse of A, that is, it satisï¬es
these four properties, and if U âˆˆUn, V âˆˆUm, then V âˆ—XU âˆ—is a generalized
inverse of UAV . Therefore, existence and uniqueness need to be proved
for only a single representative D of the equivalence class of A modulo
unitary multiplications on the right and the left. From Theorem 7.7.1, we
may choose D = diag(s1, . . . , sr, 0, . . . ), where s1, . . . , sr are the nonzero
singular values of A.
We are thus concerned only with quasi-diagonal matrices D. Let Dâ€  be
any generalized inverse of D, which we write blockwise as
Dâ€  =

G
H
J
K

.

146
8. Matrix Factorizations
We use the notation of Theorem 7.7.1. From property 1, we obtain S =
SGS, where S := diag(s1, . . . , sr). Since S is nonsingular, we obtain G =
Sâˆ’1. Next, property 3 implies SH = 0, that is, H = 0. Likewise, property
4 gives JS = 0, that is, J = 0. Finally, property 2 yields K = JSH = 0.
We see, then, that Dâ€  must equal (uniqueness)
 Sâˆ’1
0
0
0

.
One easily checks that this matrix solves our problem (existence).
Some obvious properties are stated in the following proposition. We warn
the reader that, contrary to what happens for the standard inverse, the
generalized inverse of AB does not need to be equal to Bâ€ Aâ€ .
Proposition 8.4.1 The following equalities hold for the generalized in-
verse:
(Î»A)â€  = 1
Î»Aâ€ 
(Î» Ì¸= 0),

Aâ€ â€  = A,

Aâ€ âˆ—= (Aâˆ—)â€  .
If A âˆˆGLn(CC), then Aâ€  = Aâˆ’1.
Since (AAâ€ )2 = AAâ€ , the matrix AAâ€  is a projector, which can therefore
be described in terms of its range and kernel. Since AAâ€  is Hermitian, these
subspaces are orthogonal to each other. Obviously, R(AAâ€ ) âŠ‚R(A). But
since AAâ€ A = A, the reverse inclusion holds too. Finally, we have
R(AAâ€ ) = R(A),
and AAâ€  is the orthogonal projector onto R(A). Likewise, Aâ€ A is an orthog-
onal projector. Obviously, ker A âŠ‚ker Aâ€ A, while the identity AAâ€ A = A
implies the reverse inclusion, so that
ker Aâ€ A = ker A.
Finally, Aâ€ A is the orthogonal projector onto (ker A)âŠ¥.
8.4.1
Solutions of the General Linear System
Given a matrix M âˆˆMnÃ—m(CC) and a vector b âˆˆCCn, let us consider the
linear system
Mx = b.
(8.2)
In (8.2), the matrix M need not be square, even not of full rank. From
property 1, a necessary condition for the solvability of (8.2) is MM â€ b =
b. Obviously, this is also suï¬ƒcient, since it ensures that x0 := M â€ b is
a solution. Hence, the generalized inverse plays one of the roles of the
standard inverse, namely to provide one solution of (8.2) when it is solvable.
To catch every solution of that system, it remains to solve the homogeneous

8.5. Exercises
147
problem My = 0. From the analysis done in the previous section, ker M is
nothing but the range of Im âˆ’M â€ M. Therefore, we may state the following
proposition:
Proposition 8.4.2 The system (8.2) is solvable if and only if b = MM â€ b.
When it is solvable, its general solution is x = M â€ b+(Im âˆ’M â€ M)z, where
z ranges over CCm. Finally, the special solution x0 := M â€ b is the one of
least Hermitian norm.
There remains to prove that x0 has the smallest norm among the solu-
tions. That comes from the Pythagorean theorem and from the fact that
R(M â€ ) = R(M â€ M) = (ker M)âŠ¥.
8.5
Exercises
1. Assume that there exists an algorithm for multiplying two N Ã— N
matrices with entries in a noncommutative ring by means of K
multiplications and L additions. Show that the complexity of the
multiplication in Mn(k) is O(nÎ±), with Î± = log K/ log N.
2. What is the complexity of Choleski factorization?
3. Let M âˆˆSPDn be also tridiagonal. What is the structure of L in
the Choleski factorization? More generally, what is the structure of
L when mij = 0 for |i âˆ’j| > r?
4. (continuation of exercise 3)
For i â‰¤n, denote by Ï†(i) the smallest index j such that mij Ì¸= 0.
In Choleski factorization, show that lij = 0 for every pair (i, j) such
that j < Ï†(i).
5. In the QR factorization, show that the map M â†’(Q, R) is continuous
on GLn(CC).
6. Let H be an n Ã— n Hermitian matrix, that blockwise reads
H =
 A
Bâˆ—
B
C

.
Assume that A âˆˆHPDnâˆ’k (1 â‰¤k â‰¤n âˆ’1).
Find a matrix T of the form
T =

Inâˆ’k
0
Â·
Ik

such that THT âˆ—is block-diagonal. Deduce that if W âˆˆHk, then
H âˆ’

0
0
0
W


148
8. Matrix Factorizations
is positive (semi)deï¬nite if and only if S âˆ’W is, where S is the Schur
complement of A in H.
7. (continuation of exercise 6)
Fix the size k and denote by S(H) the Schur complement in the
Hermitian matrix H when A âˆˆHPDnâˆ’k. Using the previous exercise,
show that:
(a) S(H + Hâ€²) âˆ’S(H) âˆ’S(Hâ€²) is positive semideï¬nite.
(b) If H âˆ’Hâ€² is positive semideï¬nite, then so is S(H) âˆ’S(Hâ€²).
In other words, H â†’S is â€œconcave nondecreasingâ€ on the convex
set formed of those matrices of Hn such that A âˆˆHPDnâˆ’k into the
ordered set Hk. The article [26] gives a review of the properties of
the map H â†’S(H).
8. In Proposition 8.3.1, ï¬nd an alternative proof of the uniqueness part,
by inspection of the spectrum of the matrix Q := Qâˆ’1
2 Q1 = R2Râˆ’1
1 .
9. Identify the generalized inverse of row matrices and column matrices.
10. What is the generalized inverse of an orthogonal projector, that is, a
Hermitian matrix P satisfying P 2 = P? Deduce that the description
of AAâ€  and Aâ€ A as orthogonal projectors does not characterize Aâ€ 
uniquely.
11. Given a matrix B âˆˆMpÃ—q(CC) and a vector a âˆˆCCp, let us form the
matrix A := (B, a) âˆˆMpÃ—(q+1)(CC).
(a) Let us deï¬ne d := Bâ€ a, c := a âˆ’Bd, and
b :=
 câ€ ,
if c Ì¸= 0,
(1 + |d|2)âˆ’1dâˆ—Bâ€ ,
if c = 0.
Prove that
Aâ€  =

Bâ€  âˆ’db
b

.
(b) Deduce an algorithm (Grevilleâ€™s algorithm in O(pq2) operations
for the computation of the generalized inverse of a p Ã— q matrix.
Hint: To get started with the algorithm, use Exercise 9.

9
Iterative Methods for Linear Problems
In this chapter the ï¬eld of scalars is K = IR or CC.
We have seen in the previous Chapter a few direct methods for solving
a linear system Ax = b, when A âˆˆMn(K) is invertible. For example, if A
admits an LU factorization, the successive resolution of Ly = b, Ux = y
is called the Gauss method. When a leading principal minor of A van-
ishes, a permutation of the columns allows us to return to the generic case.
More generally, the Gauss method with pivoting consists in permuting the
columns at each step of the factorization in such a way as to limit the
magnitude of round-oï¬€errors and that of the conditioning number of the
matrices L, U.
The direct computation of the solution of a Cramerâ€™s linear system Ax =
b, by the Gauss method or by any other direct method, is rather costly,
on the order of n3 operations. It also presents several inconveniences. On
the one hand, it does not exploit completely the sparse shape of many
matrices A; in numerical analysis it happens frequently that an n Ã— n
matrix has only O(n) nonzero entries, instead of O(n2). On the other hand,
the computation of an LU factorization is rather unstable, because the
round-oï¬€errors produced by the computer are ampliï¬ed at each step of
the computation.
For these reasons, one often uses an iterative method to compute an ap-
proximate solution xm, instead of an exact solution. The iterative methods
fully exploit the sparse structure of A. The number of operations is O(am),
where a is the number nonzero entries in A. The choice of m depends on
the accuracy that one requires a priori. It is, however, modest, because
the error âˆ¥xm âˆ’Â¯xâˆ¥from the exact solution Â¯x is of order constant Ã— km,

150
9. Iterative Methods for Linear Problems
where k < 1 whenever the method converges. Typically, a dozen iterations
give a rather good result, and then O(10a) â‰ªO(n3). another advantage of
the iterative methods is that the round-oï¬€errors are damped during the
computation, instead of being ampliï¬ed.
General principle: Choose a decomposition of A of the form M âˆ’N and
rewrite the system, assuming that M is invertible:
x = M âˆ’1(Nx + b).
Then choosing a starting vector x0 âˆˆKn, which may be a rather coarse
approximation of the solution, one constructs a sequence (xm)mâˆˆIN by
induction:
xm+1 = M âˆ’1(Nxm + b).
(9.1)
In practice, one does not compute M âˆ’1 explicitly but one solves the linear
systems Mxm+1 = Â· Â· Â· . It is thus important that this resolution be cheap.
This will be the case when M is triangular. In that case, the invertibility of
M can be read from its diagonal, since it occurs precisely when the diagonal
entries are nonzero.
9.1
A Convergence Criterion
Deï¬nition 9.1.1 Let us assume that A and M are invertible, A = M âˆ’N.
We say that an iterative method is convergent if for every pair (x0, b) âˆˆ
Kn Ã— Kn, we have
lim
mâ†’+âˆxm = Aâˆ’1b.
Proposition 9.1.1 An iterative method is convergent if and only if
Ï(M âˆ’1N) < 1.
Proof
If the method is convergent, then for b = 0,
lim
mâ†’+âˆ(M âˆ’1N)mx0 = 0,
for every x0 âˆˆKn. In other words,
lim
mâ†’+âˆ(M âˆ’1N)m = 0.
From Corollary 4.4.1, this implies Ï(M âˆ’1N) < 1.
Conversely, if Ï(M âˆ’1N) < 1, then by Proposition 4.4.1,
lim
mâ†’+âˆ(M âˆ’1N)m = 0,
and hence
xm âˆ’Aâˆ’1b = (M âˆ’1N)m(x0 âˆ’Aâˆ’1b) â†’0.

9.2. Basic Methods
151
To be more precise, if âˆ¥Â· âˆ¥is a norm on Kn, then
âˆ¥xm âˆ’Aâˆ’1bâˆ¥â‰¤âˆ¥(M âˆ’1N)mâˆ¥âˆ¥x0 âˆ’Aâˆ’1bâˆ¥.
From Householderâ€™s theorem (Theorem 4.2.1), there exists for every Ïµ > 0
a constant C(Ïµ) < âˆsuch that
âˆ¥xm âˆ’Aâˆ’1bâˆ¥â‰¤C(Ïµ)âˆ¥x0 âˆ’Â¯xâˆ¥(Ï(M âˆ’1N) + Ïµ)m.
In most cases (in fact, when there exists an induced norm satisfying
âˆ¥M âˆ’1Nâˆ¥= Ï(M âˆ’1N)), one can choose Ïµ = 0 in this inequality such that
âˆ¥xm âˆ’Aâˆ’1bâˆ¥= O(Ï(M âˆ’1N)m).
The choice of a vector x0 such that x0 âˆ’Aâˆ’1b is an eigenvector associated
to an eigenvalue of maximal modulus shows that this inequality cannot be
improved in general. For this reason, we call the positive number
Ï„ := âˆ’log Ï(M âˆ’1N)
the convergence ratio of the method. Given two convergent methods, we
say that the ï¬rst one converges faster than the second one if Ï„1 > Ï„2. For
example, we say that it converges twice as fast if Ï„1 = 2Ï„2. In fact, with
an error of order Ï(M âˆ’1N)m = exp(âˆ’mÏ„), we see that the faster method
needs only half as many iterations to obtain the same accuracy.
9.2
Basic Methods
There are three basic iterative methods, of which the ï¬rst has only a his-
torical or theoretical interest. Each uses the decomposition of A into three
parts, a diagonal one D, a lower triangular âˆ’E, and an upper triangular
one âˆ’F:
A = D âˆ’E âˆ’F =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
d1
...
âˆ’F
âˆ’E
...
dn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
In all cases, one assumes that D is invertible: The diagonal entries of A are
nonzero.
Jacobi method: One chooses M = D; thus N = E + F. The iteration
matrix is J := Dâˆ’1(E + F). Knowing the vector xm, one computes
the components of the vector xm+1 by the formula
xm+1
i
= 1
aii
ï£«
ï£­bi âˆ’

jÌ¸=i
aijxm
j
ï£¶
ï£¸.

152
9. Iterative Methods for Linear Problems
Gaussâ€“Seidel method: One chooses M = D âˆ’E, and thus N = F. The
iteration matrix is G := (Dâˆ’E)âˆ’1F. As we shall see below, one never
computes G explicitly. One computes the approximate solutions by
a double induction, on m on the one hand, and on i âˆˆ{1, . . . , n} on
the other hand:
xm+1
i
= 1
aii
ï£«
ï£­bi âˆ’
iâˆ’1

j=1
aijxm+1
j
âˆ’
j=n

j=i+1
aijxm
j
ï£¶
ï£¸.
The diï¬€erence between the two methods is that in Gaussâ€“Seidel one
always uses the most recently computed values of each coordinate.
Relaxation method: It often happens that the Gaussâ€“Seidel method
converges exceedingly slowly. We thus wish to improve the Gaussâ€“
Seidel method by looking for a â€œbestâ€ approximated value of the xj
(with j < i) when computing xm+1
i
. Instead of being simply xm
j , as
in the Jacobi method, or xm+1
j
, as in that of Gaussâ€“Seidel, this best
value will be an interpolation of both (we shall see that it is merely
an extrapolation). This justiï¬es the choice of
M = 1
Ï‰ D âˆ’E,
N =
 1
Ï‰ âˆ’1

D + F,
where Ï‰ âˆˆCC is a parameter. This parameter remains, in general,
constant throughout the calculations. The method is called successive
relaxation. When Ï‰ > 1, it bears the name successive overrelaxation
(SOR). The iteration matrix is
LÏ‰ := (D âˆ’Ï‰E)âˆ’1((1 âˆ’Ï‰)D + Ï‰F).
The Gaussâ€“Seidel method is a particular case of the relaxation
method, with Ï‰ = 1: L1 = G. Special attention is given to the choice
of Ï‰, in order to reach the minimum of Ï(LÏ‰). The computation of
the approximate solutions is done through a double induction:
xm+1
i
= Ï‰
aii
ï£«
ï£­bi âˆ’
iâˆ’1

j=1
aijxm+1
j
âˆ’
j=n

j=i+1
aijxm
j +
 1
Ï‰ âˆ’1

aiixm
i
ï£¶
ï£¸.
Without additional assumptions relative to the matrix A, the only result
concerning the convergence is the following:
Proposition 9.2.1 We have Ï(LÏ‰) â‰¥|Ï‰ âˆ’1|. In particular, if the relax-
ation method converges for a matrix A âˆˆMn(CC) and a parameter Ï‰ âˆˆCC,
then
|Ï‰ âˆ’1| < 1.
In other words, it is necessary that Ï‰ belong to the disk for which (0, 2) is
a diameter.

9.3. Two Cases of Convergence
153
Proof
If the method is convergent, we have Ï(LÏ‰) < 1. However,
det LÏ‰ = det((1 âˆ’Ï‰)D + Ï‰F)
det(D âˆ’Ï‰E)
= det((1 âˆ’Ï‰)D)
det D
= (1 âˆ’Ï‰)n.
Hence
Ï(LÏ‰) â‰¥| det LÏ‰|1/n = |1 âˆ’Ï‰|.
9.3
Two Cases of Convergence
In this section and the following one we show that simple and natural
hypotheses on A imply the convergence of the classical methods. We also
compare their eï¬ƒciencies.
9.3.1
The Diagonally Dominant Case
We assume here that one of the following two properties is satisï¬ed:
1. A is strictly diagonally dominant,
2. A is irreducible and strongly diagonally dominant.
Proposition 9.3.1 Under one or the other of the hypotheses (1) and (2),
the Jacobi method converges, as well as the relaxation method, with Ï‰ âˆˆ
(0, 1].
Proof
Jacobi method: The matrix J = Dâˆ’1(E + F) is clearly irreducible if A
is. Furthermore,
n

j=1
|Jij| â‰¤1,
i = 1, . . . , n,
in which all inequalities are strict if (1) holds, and at least one in-
equality is strict under the hypothesis (2). Then either Gershgorinâ€™s
theorem (Theorem 4.5.1) or its improvement, Proposition 4.5.2 for
irreducible matrices, yields Ï(J) < 1.
Relaxation method: We assume that Ï‰ âˆˆ(0, 1]. Let Î» âˆˆCC be a nonzero
eigenvalue of LÏ‰. It is a root of
det((1 âˆ’Ï‰ âˆ’Î»)D + Î»Ï‰E + Ï‰F) = 0.
Hence, Î»+Ï‰âˆ’1 is an eigenvalue of Aâ€² := Ï‰Dâˆ’1(Î»E+F). This matrix
is irreducible when A is. Then Gershgorinâ€™s theorem (Theorem 4.5.1)

154
9. Iterative Methods for Linear Problems
shows that
|Î» + Ï‰ âˆ’1| â‰¤max
ï£±
ï£²
ï£³
Ï‰
|aii|
ï£«
ï£­|Î»|

j<i
|aij| +

j>i
|aij|
ï£¶
ï£¸; 1 â‰¤i â‰¤n
ï£¼
ï£½
ï£¾.
(9.2)
If |Î»| â‰¥1, we deduce that
|Î» + Ï‰ âˆ’1| â‰¤max
ï£±
ï£²
ï£³
Ï‰|Î»|
|aii|

jÌ¸=i
|aij| ; 1 â‰¤i â‰¤n
ï£¼
ï£½
ï£¾.
In case (1), this yields
|Î» + Ï‰ âˆ’1| < Ï‰|Î»|,
so that |Î»| â‰¤|Î»+Ï‰âˆ’1|+|1âˆ’Ï‰| < |Î»|Ï‰+1âˆ’Ï‰; that is, (|Î»|âˆ’1)(1âˆ’Ï‰) <
0, which is a contradiction. In case (2), Proposition 4.5.2 says that
inequality (9.2) is strict. One concludes the proof the same way as in
case (1).
Of course, this result is not fully satisfactory, since Ï‰ â‰¤1 is not the
hypothesis that we should consider. Recall that in practice, one uses over-
relaxation (that is, Ï‰ > 1), which turns out to be much more eï¬ƒcient than
the Gaussâ€“Seidel method for an appropriate choice of the parameter.
9.3.2
The Case of a Hermitian Positive Deï¬nite Matrix
Let us begin with an intermediate result.
Lemma 9.3.1 If A and M âˆ—+ N are Hermitian positive deï¬nite (in a
decomposition A = M âˆ’N), then Ï(M âˆ’1N) < 1.
Proof
Let us remark ï¬rst that M âˆ—+N = M âˆ—+M âˆ’A is necessarily Hermitian
when A is.
It is therefore enough to show that âˆ¥M âˆ’1Nxâˆ¥A < âˆ¥xâˆ¥A for every nonzero
x âˆˆCCn, where âˆ¥Â· âˆ¥A denotes the norm associated to A:
âˆ¥xâˆ¥A =
âˆš
xâˆ—Ax.
We have M âˆ’1Nx = x âˆ’y with y = M âˆ’1Ax. Hence,
âˆ¥M âˆ’1Nxâˆ¥2
A
=
âˆ¥xâˆ¥2
A âˆ’yâˆ—Ax âˆ’xâˆ—Ay + yâˆ—Ay
=
âˆ¥xâˆ¥2
A âˆ’yâˆ—(M âˆ—+ N)y.
We conclude by observing that y is not zero; hence yâˆ—(M âˆ—+ N)y > 0.

9.4. The Tridiagonal Case
155
This proof gives a slightly more precise result than what was claimed: By
taking the supremum of âˆ¥M âˆ’1Nxâˆ¥A on the unit ball, which is compact,
we obtain âˆ¥M âˆ’1Nâˆ¥< 1 for the matrix norm induced by âˆ¥Â· âˆ¥A.
The main application of this lemma is the following theorem.
Theorem 9.3.1 If A is Hermitian positive deï¬nite, then the relaxation
method converges if and only if |Ï‰ âˆ’1| < 1.
Proof
We have seen in Proposition 9.2.1 that the convergence implies |Ï‰ âˆ’1| <
1. Let us see the converse. We have Eâˆ—= F and Dâˆ—= D. Thus
M âˆ—+ N =
 1
Ï‰ + 1
Â¯Ï‰ âˆ’1

D = 1 âˆ’|Ï‰ âˆ’1|2
|Ï‰|2
D.
Since D is positive deï¬nite, M âˆ—+ N is positive deï¬nite if and only if
|Ï‰ âˆ’1| < 1.
However, Lemma 9.3.1 does not apply to the Jacobi method, since the
hypothesis (A positive deï¬nite) does not imply that M âˆ—+ N = D + E + F
must be positive deï¬nite. We shall see in an exercise that this method
diverges for certain matrices A âˆˆHPDn, though it converges when A âˆˆ
HPDn is tridiagonal.
9.4
The Tridiagonal Case
We consider here the case of tridiagonal matrices A, frequently encountered
in the approximation of partial diï¬€erential equations by ï¬nite diï¬€erences
or ï¬nite elements. The general structure of A is the following:
A =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x
xâ€²
0
Â· Â· Â·
0
xâ€²â€²
...
...
...
...
0
...
...
...
0
...
...
...
...
yâ€²
0
Â· Â· Â·
0
yâ€²â€²
y
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
In other words, the entries aij are zero as soon as |j âˆ’i| â‰¥2.
In many cases, these matrices are blockwise tridiagonal, meaning that
the aij are matrices, the diagonal blocks aii being square matrices. In that
case, the iterative methods also read blockwise, the decomposition A =
D âˆ’E âˆ’F being done blockwise. The corresponding iterative methods
need the inversion of matrices of smaller sizes, namely the aii, usually done
by a direct method. We shall not detail here this extension of the classical
methods.
The structure of the matrix allows us to write a useful algebraic relation:

156
9. Iterative Methods for Linear Problems
Lemma 9.4.1 Let Âµ be a nonzero complex number and C a tridiagonal
matrix, of diagonal C0, of upper triangular part C+ and lower triangular
part Câˆ’. Then
det C = det

C0 + 1
ÂµCâˆ’+ ÂµC+

.
Proof
It is enough to observe that the matrix C is conjugate to
C0 + 1
ÂµCâˆ’+ ÂµC+,
through the linear transformation matrix
QÂµ =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Âµ
Âµ2
0
...
0
...
Âµn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
Let us apply the lemma to the computation of the characteristic
polynomial PÏ‰ of LÏ‰. We have
(det D)PÏ‰(Î»)
=
det((D âˆ’Ï‰E)(Î»In âˆ’LÏ‰))
=
det((Ï‰ + Î» âˆ’1)D âˆ’Ï‰F âˆ’Î»Ï‰E)
=
det

(Ï‰ + Î» âˆ’1)D âˆ’ÂµÏ‰F âˆ’Î»Ï‰
Âµ E

,
for every nonzero Âµ. Let us choose for Âµ any square root of Î». We then have
(det D)PÏ‰(Âµ2)
=
det((Ï‰ + Âµ2 âˆ’1)D âˆ’ÂµÏ‰(E + F))
=
(det D) det((Ï‰ + Âµ2 âˆ’1)In âˆ’ÂµÏ‰J).
Finally, we have the following lemma.
Lemma 9.4.2 If A is tridiagonal and D invertible, then
PÏ‰(Âµ2) = (ÂµÏ‰)nPJ
Âµ2 + Ï‰ âˆ’1
ÂµÏ‰

,
where PJ is the characteristic polynomial of the Jacobi matrix J.
Let us begin with the analysis of a simple case, that of the Gaussâ€“Seidel
method, for which G = L1.
Proposition 9.4.1 If A is tridiagonal and D invertible, then:
1. PG(X2) = XnPJ(X), where PG is the characteristic polynomial of
the Gaussâ€“Seidel matrix G,

9.4. The Tridiagonal Case
157
2. Ï(G) = Ï(J)2,
3. the Gaussâ€“Seidel method converges if and only if the Jacobi method
converges; moreover, in case of convergence, the Gaussâ€“Seidel method
converges twice as fast as the Jacobi method;
4. the spectrum of J is even: Sp J = âˆ’Sp J.
Proof
Formula (1) comes from Lemma 9.4.2. The spectrum of G is thus formed
of Î» = 0 (which is of multiplicity [(n + 1)/2] at least) and of squares of the
eigenvalues of J, which proves 2). Point 3 follows immediately. Finally, if
Âµ âˆˆSp J, then PJ(Âµ) = 0, and also PG(Âµ2) = 0, so that (âˆ’Âµ)nPJ(âˆ’Âµ) = 0.
Finally, either PJ(âˆ’Âµ) = 0, or Âµ = 0 = âˆ’Âµ, in which case PJ(âˆ’Âµ) also
vanishes.
In fact, the comparison given in point 3 of the proposition holds under
various assumptions. For example (see Exercises 3 and 8), it holds true
when D is positive and E, F are nonnegative.
We go back to the SOR, with an additional hypothesis: The spectrum of
J is real, and the Jacobi method converges. This property is satisï¬ed, for
instance, when A is Hermitian positive deï¬nite, since Theorem 9.3.1 and
Proposition 9.4.1 ensure the convergence of the Jacobi method, and since
J is similar to the Hermitian matrix Dâˆ’1/2(E + F)Dâˆ’1/2.
We also select a real Ï‰, that is, Ï‰ âˆˆ(0, 2), taking into account Proposition
9.2.1. The spectrum of J is thus formed of the eigenvalues
âˆ’Î»r < Â· Â· Â· < âˆ’Î»1 â‰¤Î»1 < Â· Â· Â· < Î»r = Ï(J) < 1,
from Proposition 9.4.1. This notation does not mean that n be even: If n is
odd, Î»1 = 0. Aside from the zero eigenvalue, which does not enter into the
computation of the spectral radius, the eigenvalues of LÏ‰ are the squares
of the roots of
Âµ2 + Ï‰ âˆ’1 = ÂµÏ‰Î»a,
(9.3)
for 1 â‰¤a â‰¤r. Indeed, taking âˆ’Î»a instead of Î»a furnishes the sames squares.
Let us deï¬ne âˆ†(Î») := Ï‰2Î»2 +4(1âˆ’Ï‰), the discriminant of (9.3). If âˆ†(Î»a)
is negative, both roots of (9.3) are complex conjugate, hence have modulus
|Ï‰ âˆ’1|1/2. The case Î» = 0 furnishes the same modulus. If that discriminant
is strictly positive, the roots are real and of distinct modulus. One of them,
denoted by Âµa, satisï¬es Âµ2
a > |Ï‰ âˆ’1|, the other one satisfying the opposite
inequality.
From Proposition 9.2.1, Ï(LÏ‰) is thus equal to one of the following:
â€¢ |Ï‰ âˆ’1|, if âˆ†(Î»a) â‰¤0 for every a, that is, if âˆ†(Ï(J)) â‰¤0;
â€¢ the maximum of the Âµ2
aâ€™s deï¬ned above, otherwise.

158
9. Iterative Methods for Linear Problems
The ï¬rst case corresponds to the choice Ï‰ âˆˆ[Ï‰J, 2), where
Ï‰J = 21 âˆ’

1 âˆ’Ï(J)2
Ï(J)2
=
2
1 +

1 âˆ’Ï(J)2 âˆˆ[1, 2).
Then Ï(LÏ‰) = Ï‰ âˆ’1.
The second case is Ï‰ âˆˆ(0, Ï‰J). If âˆ†(Î»a) > 0, let us denote by Qa(X)
the polynomial X2 + Ï‰ âˆ’1 âˆ’XÏ‰Î»a. The sum of its roots being positive,
Âµa is the largest one; it is thus positive. Moreover, Qa(1) = Ï‰(1 âˆ’Î»a) > 0
shows that both roots belong to the same half-line of IR \ {1}. Since their
product has modulus less than or equal to one, they are less than or equal
to one. In particular,
|Ï‰ âˆ’1|1/2 < Âµa < 1.
This shows that Ï(LÏ‰) < 1 holds for every Ï‰ âˆˆ
(0, 2). Under our
hypotheses, the relaxation method is convergent.
If Î»a Ì¸= Ï(J), we have Qr(Âµa) = ÂµaÏ‰(Î»a âˆ’Ï(J)) < 0. Hence, Âµa lies
between both roots of Qr, so that Âµa < Âµr. Finally, the case âˆ†(Ï(J)) â‰¤0
furnishes Ï(LÏ‰) = Âµ2
r. We then have
(2Âµr âˆ’Ï‰Ï(J))dÂµr
dÏ‰ + 1 âˆ’ÂµrÏ(J) = 0.
Since 2Âµr is larger than the sum Ï‰Ï(J) of the roots and since Âµr, Ï(J) âˆˆ
[0, 1), one deduces that Ï‰ â†’Ï(LÏ‰) is nonincreasing over (0, Ï‰J).
We conclude that Ï(LÏ‰) reaches its minimum at Ï‰J, that minimum being
Ï‰J âˆ’1 = 1 âˆ’

1 âˆ’Ï(J)2
1 +

1 âˆ’Ï(J)2 .
Theorem 9.4.1 [See Figure 9.1] Suppose that A is tridiagonal, D is in-
vertible, and that the eigenvalues of J are real and belong to (âˆ’1, 1). Assume
also that Ï‰ âˆˆIR.
Then the relaxation method converges if and only if Ï‰
âˆˆ
(0, 2).
Furthermore, the convergence ratio is optimal for the parameter
Ï‰J :=
2
1 +

1 âˆ’Ï(J)2 âˆˆ[1, 2),
where the spectral radius of LÏ‰J is
(Ï‰J âˆ’1 =)
1 âˆ’

1 âˆ’Ï(J)2
1 +

1 âˆ’Ï(J)2 =

1 âˆ’

1 âˆ’Ï(J)2
Ï(J)
2
.
Remarks:
â€¢ We shall see in Exercise 7 that Theorem 9.4.1 extends to complex
values of Ï‰: Under the same assumptions, Ï(LÏ‰) is minimal at Ï‰J,
and the relaxation method converges if and only if |Ï‰ âˆ’1| < 1.

9.5. The Method of the Conjugate Gradient
159
2
Ï‰J
1
1
Ï(LÏ‰)
Ï‰J âˆ’1
Ï‰
Figure 9.1. Ï(LÏ‰) in the tridiagonal case.
â€¢ The Gaussâ€“Seidel method is not optimal in general; Ï‰J = 1 holds
only when Ï(J) = 0, though in practice Ï(J) is close to 1. A typical
example is the resolution of an elliptic PDE by the ï¬nite element
method.
For values of Ï(J) that are not too close to 1, the relaxation method
with optimal parameter Ï‰J, though improving the convergence ratio,
is not overwhelmingly more eï¬ƒcient than Gaussâ€“Seidel. In fact,
Ï(G)/Ï (LÏ‰J) =

1 +

1 âˆ’Ï(J)2
2
lies between 1 (for Ï(J) close to 1) and 4 (for Ï(J) = 0), so that the
ratio
log Ï(LÏ‰J )/ log Ï(G)
remains moderate, as long as Ï(J) keeps away from 1. However, in
the realistic case where Ï(J) is close to 1, we have
log Ï(G)/ log Ï(LÏ‰J) âˆ¼
%
1 âˆ’Ï(J)
2
,
which is very small. The number of iterations needed for a prescribed
accuracy is multiplied by that ratio when one replaces the Gaussâ€“
Seidel method by the relaxation method with the optimal parameter.
9.5
The Method of the Conjugate Gradient
We present here the conjugate gradient method in the most appropriate
framework, namely that of systems Ax = b where A is real symmetric
positive deï¬nite (A âˆˆSPDn). As we shall see below, it is a direct method,
in the sense that it furnishes the solution Â¯x after a ï¬nite number of iterations

160
9. Iterative Methods for Linear Problems
(at most n). However, the round-oï¬€errors pollute the ï¬nal result, and we
would prefer to consider the conjugate gradient as an iterative method in
which the number N of iterations, much less than n, gives a rather good
approximation of Â¯x. We shall see that the choice of N is linked to the
condition number of the matrix A.
We denote by âŸ¨Â·, Â·âŸ©the canonical scalar product on IRn. When A âˆˆSPDn
and b âˆˆIRn, the function
x â†’J(x) := 1
2âŸ¨Ax, xâŸ©âˆ’âŸ¨b, xâŸ©
is strictly convex and tends to inï¬nity as âˆ¥xâˆ¥â†’+âˆ. It thus reaches
its inï¬mum at a unique point Â¯x, which is the unique vector where the
gradient of J vanishes. We shall denote by r (for residue) the gradient of
J: r(x) = Ax âˆ’b. Hence Â¯x is the solution of the linear system Ax = b.
If AÂ¯x = b and x âˆˆIRn, x Ì¸= Â¯x, then
J(x) = J(Â¯x) + 1
2âŸ¨A(x âˆ’Â¯x), x âˆ’Â¯xâŸ©> J(Â¯x).
(9.4)
The conjugate gradient is thus a descent method.
We shall denote by E the quadratic form associated to A: E(x) :=
âŸ¨Ax, xâŸ©. It is the square of a norm of IRn. The character âŠ¥A indicates
the orthogonality with respect to the scalar product deï¬ned by A.
9.5.1
A Theoretical Analysis
Let x0 âˆˆIRn be given. We deï¬ne e0 = x0 âˆ’Â¯x, r0 = r(x0) = Ae0. We may
assume that e0 Ì¸= 0; otherwise, we would already have the solution. For
k â‰¥1, let us deï¬ne the vector space
Hk := {P(A)r0 | P âˆˆIR[X], deg P â‰¤k âˆ’1},
H0 = {0}.
In Hk+1, the linear subspace Hk is of codimension 0 or 1. In the ï¬rst case,
Hk+1 = Hk, and it follows that Hk+2 = AHk+1 + Hk+1 = AHk + Hk =
Hk+1 = Hk and thus by induction, Hk = Hm for every m > k. Let us
denote by l the smallest index such that Hl = Hl+1. For k < l, Hk is thus
of codimension one in Hk+1, while if k â‰¥l, then Hk = Hk+1. It follows
that dim Hk = k if k â‰¤l. In particular, l â‰¤n.
One can always ï¬nd, by Gramâ€“Schmidt orthonormalization, an A-
orthogonal1 basis (that is, such that âŸ¨Apj, piâŸ©= 0 if i Ì¸= j) {p0, . . . , plâˆ’1}
of Hl such that {p0, . . . , pkâˆ’1} is a basis of Hk when k â‰¤l. The vectors pj,
which are not necessarily unit vectors, are deï¬ned, up to a scalar multiple,
by
pk âˆˆHk+1,
pkâŠ¥AHk.
1One must distinguish in this section between the two scalar products, namely âŸ¨Â·, Â·âŸ©
and âŸ¨AÂ·, Â·âŸ©.

9.5. The Method of the Conjugate Gradient
161
One says that the vectors pj are pairwise conjugate. Of course, conjugation
means A-orthogonality. This explains the name of the method.
The quadratic function J, strictly convex, reaches its inï¬mum on the
aï¬ƒne subspace x0 + Hk at a unique vector, which we denote by xk. This
notation makes sense for k = 0. If x = y+Î³pk âˆˆx0+Hk+1 with y âˆˆx0+Hk,
then
J(x)
=
J(Â¯x) + 1
2E(x âˆ’Â¯x)
=
J(Â¯x) + 1
2E(y âˆ’Â¯x) + 1
2Î³2E(pk) + Î³âŸ¨Apk, y âˆ’Â¯xâŸ©
=
J(y) + 1
2Î³2E(pk) âˆ’Î³âŸ¨Apk, e0âŸ©,
since âŸ¨Apk, y âˆ’x0âŸ©= 0. Hence, minimizing J over x0 + Hk+1 amounts to
minimizing J over x0 + Hk, together with minimizing Î³ â†’1
2Î³2E(pk) âˆ’
Î³âŸ¨pk, r0âŸ©over IR. We therefore have
xk+1 âˆ’xk âˆˆIRpk.
(9.5)
By deï¬nition of l there exists a nonzero polynomial P of degree l such
that P(A)r0 = 0, that is, AP(A)e0 = 0. Since A is invertible, P(A)e0 = 0.
Let us assume that P(0) vanishes. Then P(X) = XQ(X) with deg Q = lâˆ’1.
Therefore, Q(A)r0 = 0: The map S â†’S(A)r0 is not one-to-one over the
polynomials of degree less than or equal to l âˆ’1. Hence dim Hl < l, a
contradiction. Hence P(0) Ì¸= 1, and we may assume that P(0) = 1. Then
P(X) = 1 âˆ’XR(X), where deg R = l âˆ’1. Thus e0 = R(A)r0 âˆˆHl or,
equivalently, Â¯x âˆˆx0 + Hl. Conversely, if k â‰¤l and Â¯x âˆˆx0 + Hk, then
e0 âˆˆHk; that is, e0 = Q(A)r0, where deg Q â‰¤k âˆ’1. Then Q1(A)e0 = 0,
because Q1(X) = 1 âˆ’XQ(X). Therefore, Q1(A)r0 = 0, Q1(0) Ì¸= 0, and
deg Q1 â‰¤k. Hence k â‰¥l; that is, k = l. Summing up, we have Â¯x âˆˆx0 + Hl
but Â¯x Ì¸âˆˆx0 + Hlâˆ’1. Therefore, xl = Â¯x and xk Ì¸= Â¯x if k < l.
Lemma 9.5.1 Let us denote by Î»n â‰¥Â· Â· Â· â‰¥Î»1(> 0) the eigenvalues of A.
If k â‰¤l, then
E(xk âˆ’Â¯x) â‰¤E(e0) Â·
min
deg Qâ‰¤kâˆ’1 max
j
|1 + Î»jQ(Î»j)|2.
Proof
Let us compute
E(xk âˆ’Â¯x)
=
min{E(x âˆ’Â¯x) | x âˆˆx0 + Hk}
=
min{E(e0 + y) | y âˆˆHk}
=
min{E((In + AQ(A))e0) | deg Q â‰¤k âˆ’1}
=
min{âˆ¥(In + AQ(A))A1/2e0âˆ¥2
2 | deg Q â‰¤k âˆ’1},

162
9. Iterative Methods for Linear Problems
where we have used the equality âŸ¨Aw, wâŸ©= âˆ¥A1/2wâˆ¥2
2. Hence
E(xk âˆ’Â¯x)
â‰¤
min{âˆ¥In + AQ(A)âˆ¥2
2âˆ¥A1/2e0âˆ¥2
2 | deg Q â‰¤k âˆ’1}
=
E(e0) min{Ï(In + AQ(A))2 | deg Q â‰¤k âˆ’1},
since Ï(S) = âˆ¥Sâˆ¥2 holds for every real symmetric matrix.
From Lemma 9.5.1, we deduce an estimate of the error E(xk âˆ’Â¯x) by
bounding the right-hand side by
min
deg Qâ‰¤kâˆ’1
max
tâˆˆ[Î»1,Î»n] |1 + tQ(t)|2.
Classically, the minimum is reached for
1 + XQ(X) = Ï‰kTk
2X âˆ’Î»1 âˆ’Î»n
Î»n âˆ’Î»1

,
where Tk is a Chebyshev polynomial:
Tk(t) =
ï£±
ï£²
ï£³
cos k arccost
if
|t| â‰¤1,
cosh k arcosht
if
t â‰¥1,
(âˆ’1)k cosh k arcosh |t|
if
t â‰¤âˆ’1.
The number Ï‰k is the number that furnishes the value 1 at X = 0, namely
Ï‰k =
(âˆ’1)k
Tk

Î»n+Î»1
Î»nâˆ’Î»1
.
Then
max
[Î»1,Î»n] |1 + tQ(t)| = |Ï‰k| =
1
cosh k arcosh Î»n+Î»1
Î»nâˆ’Î»1
.
Hence E(xk âˆ’Â¯x) â‰¤|Ï‰k|2E(e0). However, if
Î¸ := arrcoshÎ»n + Î»1
Î»n âˆ’Î»1
,
then |Ï‰k| = (cosh kÎ¸)âˆ’1 â‰¤2 exp(âˆ’kÎ¸), while exp(âˆ’Î¸) is the root, less than
one, of the quadratic polynomial
T 2 âˆ’2Î»n + Î»1
Î»n âˆ’Î»1
T + 1.
Setting K(A) := âˆ¥Aâˆ¥2âˆ¥Aâˆ’1âˆ¥2 = Î»n/Î»1 the condition number of A, we
obtain
eâˆ’Î¸ = Î»n + Î»1
Î»n âˆ’Î»1
âˆ’
&Î»n + Î»1
Î»n âˆ’Î»1
2
âˆ’1 =
âˆšÎ»n âˆ’âˆšÎ»1
âˆšÎ»n + âˆšÎ»1
=

K(A) âˆ’1

K(A) + 1
.
The ï¬nal result is the following.

9.5. The Method of the Conjugate Gradient
163
Theorem 9.5.1 If k â‰¤l, then
E(xk âˆ’Â¯x) â‰¤4E(x0 âˆ’Â¯x)

K(A) âˆ’1

K(A) + 1
2k
.
(9.6)
We now set rk = r(xk) = A(xk âˆ’Â¯x). We have seen that rl = 0 and that
rk Ì¸= 0 if k < l. In fact, rk is the gradient of J at xk. The minimality of J at
xk over x0 +Hk thus implies that rkâŠ¥Hk (for the usual scalar product). In
other words, we have âŸ¨rk, pjâŸ©= 0 if j < k. However, xkâˆ’Â¯x âˆˆe0+Hk can also
be written as xk âˆ’Â¯x = Q(A)e0 with deg Q â‰¤k, which implies rk = Q(A)r0,
so that rk âˆˆHk+1. If k < l, one therefore has Hk+1 = Hk âŠ•IRrk.
We now normalize pk (which was not done up to now) by
pk âˆ’rk âˆˆHk.
In other words, pk is the A-orthogonal projection of rk = r(xk), parallel to
Hk. It is actually an element of Hk+1, since rk âˆˆHk+1. It is also nonzero
since rk Ì¸âˆˆHk. We note that rk is orthogonal to Hk with respect to the
usual scalar product, though pk is orthogonal to Hk with respect to the
A-scalar product; this explains why pk and rk are generally diï¬€erent.
If j â‰¤kâˆ’2, we compute âŸ¨A(pk âˆ’rk), pjâŸ©= âˆ’âŸ¨Ark, pjâŸ©= âˆ’âŸ¨rk, ApjâŸ©= 0.
We have used successively the conjugation of the pk, the symmetry of A,
the fact that Apj âˆˆHj+2, and the orthogonality of rk and Hk. We have
therefore pk âˆ’rkâŠ¥AHkâˆ’1, so that
pk = rk + Î´kpkâˆ’1
(9.7)
for a suitable number Î´k.
9.5.2
Implementing the Conjugate Gradient
The main feature of the conjugate gradient is the simplicity of the com-
putation of the vectors xk, which is done by induction. To begin with, we
have p0 = r0 = Ax0 âˆ’b, where x0 is at our disposal. Let us assume now
that xk and pkâˆ’1 are known. Then rk = Axk âˆ’b. If rk = 0, we already
have the solution. Otherwise, the formulas (9.5, 9.7) show that in fact,
xk+1 minimizes J over the plane xk + IRrk âŠ•IRpkâˆ’1. We therefore have
xk+1 = xk+Î±krk+Î²kpkâˆ’1, where the entries Î±k, Î²k are obtained by solving
the linear system of two equations

Î±kâŸ¨Ark, rkâŸ©+ Î²kâŸ¨Ark, pkâˆ’1âŸ©+ âˆ¥rkâˆ¥2 = 0,
Î±kâŸ¨Ark, pkâˆ’1âŸ©+ Î²kâŸ¨Apkâˆ’1, pkâˆ’1âŸ©= 0
(we have used âŸ¨rk, pkâˆ’1âŸ©= 0). Then we have Î´k = Î²k/Î±k. Observe that Î±k
is nonzero, because otherwise Î²k would vanish and rk would too.
Summing up, the algorithm reads as follows
â€¢ Choose x0; then deï¬ne p0 = r0 = r(x0) := Ax0 âˆ’b.

164
9. Iterative Methods for Linear Problems
â€¢ For k â‰¥0 with unit increment, do
â€“ Compute rk = r(xk) = Axk âˆ’b. If rk = 0, then Â¯x = xk.
â€“ Otherwise, minimize J(xk + Î±rk + Î²pkâˆ’1), by computing Î±k, Î²k
as above.
â€“ Deï¬ne
pk+1 = rk + (Î²k/Î±k)pkâˆ’1,
xk+1 = xk + Î±kpk.
A priori, this computation furnishes the exact solution Â¯x in l iterations.
However, l equals n in general, and the cost of each iteration is O(n2).
The conjugate gradient, viewed as a direct method, is thus rather slow.
One often uses this method for sparse matrices, whose maximal number
of nonzero elements m per rows is small compared to n. The complexity
of an iteration is then O(mn). However, that is still rather costly as a
direct method (O(mn2) operations in all), since the complexity of iterative
methods is also reduced for sparse matrices.
This explains why one prefers to consider the conjugate gradient as an
iterative method, in which one makes only a few iterations N â‰ªn. Strictly
speaking, Theorem 9.5.1 does not deï¬ne a convergence rate Ï„, since one
does not have, in general, an inequality of the form
âˆ¥xk+1 âˆ’Â¯xâˆ¥â‰¤eâˆ’Ï„âˆ¥xk âˆ’Â¯xâˆ¥.
In particular, one is not certain that âˆ¥x1 âˆ’Â¯xâˆ¥is smaller than âˆ¥x0 âˆ’Â¯xâˆ¥.
However, the inequality (9.6) is analogous to what we have for a classi-
cal iterative method, up to the factor 4. We shall therefore say that the
conjugate gradient admits a convergence rate Ï„CG that satisï¬es
Ï„CG â‰¤Î¸ = âˆ’log

K(A) âˆ’1

K(A) + 1
.
(9.8)
This rate is equivalent to 2K(A)âˆ’1/2 when K(A) is large. This method
can be considered as an iterative method when nÏ„CG â‰ª1 since then it is
possible to choose N â‰ªn. Obviously, a suï¬ƒcient condition is K(A) â‰ªn2.
Application: Let us consider the resolution of the Laplace equation in an
open bounded set â„¦of IRd, with a Dirichlet boundary condition, by the
ï¬nite elements method:
âˆ†u = f in â„¦,
u = 0 on âˆ‚â„¦.
The matrix A is symmetric, reï¬‚ecting the symmetry of the variational
formulation
 
â„¦
(âˆ‡u Â· âˆ‡v + fv) dx = 0,
âˆ€v âˆˆH1
0(â„¦).
If the diameter of the grid is h with 0 < h â‰ª1, and if that grid is regular
enough, the number of degrees of freedom (the size of the matrix) n is of
order C/hd, where C is a constant. The matrix is sparse with m = O(1).

9.6. Exercises
165
Each iteration thus needs O(n) operations. Finally, the condition number of
A is of order c/h2. Hence, a number of iterations N â‰«1/h is appropriate.
This is worthwhile as soon as d â‰¥2. The method becomes more useful as
d grows larger and the threshold 1/h is independent of the dimension.
Preconditioning: In practice, the performance of the method is improved
by preconditioning the matrix A. The idea is to replace the system Ax = b
by BT ABy = BT b, where the inversion of B is easy, for example B is block-
triangular or block-diagonal with small blocks. If BBT is close enough to
Aâˆ’1, the condition number of the new matrix is smaller, and the number
of iterations is reduced. Actually, when the condition number reaches its
inï¬mum K = 1, we have A = In, and the solution Â¯x = b is obvious. The
simplest preconditioning consists in choosing B = Dâˆ’1/2. Its eï¬ƒciency is
clear in the (trivial) case where A is diagonal, because the matrix of the
new system is In, and the condition number is lowered to 1. Observe that
preconditioning is also used with SOR, because it allows us to diminish the
value of Ï(J), hence also the convergence rate. We shall see in Exercise 5
that, if A âˆˆSPDn is tridiagonal and if D = dIn (which corresponds to the
preconditioning described above), the conjugate gradient method is twice
as slow as the relaxation method with optimal parameter; that is,
Î¸ = 1
2Ï„RL.
This equality is obtained by computing Î¸ and the optimal convergence
rate Ï„RL of the relaxation method in terms of Ï(J). In the real world, in
which A might not be tridiagonal, or be only blockwise tridiagonal, the
map Ï(J) â†’Î¸ remains the same, while Ï„RL deteriorates. The conjugate
gradient method becomes more eï¬ƒcient than the relaxation method. It has
also the advantage that it does not need the preliminary computation of
Ï(J), in contrast to the relaxation method with optimal parameter.
The reader will ï¬nd a deeper analysis of the method of the conjugate
gradient in the article of J.-F. MaË†Ä±tre in [1].
9.6
Exercises
1. Let A be a tridiagonal matrix with an invertible diagonal and let J
be its Jacobi matrix. Show that J is conjugate to âˆ’J. Compare with
Proposition 9.4.1.
2. We ï¬x n â‰¥2. Use Theorem 3.4.2 to construct a matrix A âˆˆSPDn
for which the Jacobi method does not converge. Show in particular
that
sup{Ï(J) | A âˆˆSPDn, D = In} = n âˆ’1.
3. Let A âˆˆMn(IR) satisfy aii > 0 for every index i, and aij â‰¤0
whenever j Ì¸= i. Using (several times) the weak form of the Perronâ€“

166
9. Iterative Methods for Linear Problems
Frobenius theorem, prove that either 1 â‰¤Ï(J) â‰¤Ï(G) or Ï(G) â‰¤
Ï(J) â‰¤1. In particular, as in point 3 of Proposition 9.4.1, the Jacobi
and Gaussâ€“Seidel methods converge or diverge simultaneously, and
Gaussâ€“Seidel is faster in the former case. Hint: Prove that
(Ï(G) â‰¥1) =â‡’(Ï(J) â‰¥1) =â‡’(Ï(G) â‰¥Ï(J))
and
(Ï(G) â‰¤1) =â‡’(Ï(J) â‰¥Ï(G)).
4. Let n â‰¥2 and A âˆˆHPDn be given. Assume that A is tridiagonal.
(a) Verify that the spectrum of J is real and even.
(b) Show that the eigenvalues of J satisfy Î» < 1.
(c) Deduce that the Jacobi method is convergent.
5. Let A âˆˆHPDn, A = D âˆ’E âˆ’Eâˆ—. Use the Hermitian norm âˆ¥Â· âˆ¥2.
(a) Show that |((E + Eâˆ—)v, v)| â‰¤Ï(J)âˆ¥D1/2vâˆ¥2 for every v âˆˆCCn.
Deduce that
K(A) â‰¤1 + Ï(J)
1 âˆ’Ï(J)K(D).
(b) Let us deï¬ne a function by
g(x) :=
âˆšx âˆ’1
âˆšx + 1.
Verify that
g
1 + Ï(J)
1 âˆ’Ï(J)

= 1 âˆ’

1 âˆ’Ï(J)2
Ï(J)
.
(c) Deduce that if A is tridiagonal and if D = dIn, then the con-
vergence ratio Î¸ of the conjugate gradient is the half of that of
SOR with optimal parameter.
6. Here is another proof of Theorem 9.3.1, when Ï‰ is real. Let A âˆˆ
HPDn.
(a) Suppose we are given Ï‰ âˆˆ(0, 2).
i. Assume that Î» = e2iÎ¸ (Î¸ real) is an eigenvalue of LÏ‰. Show
that (1 âˆ’Ï‰ âˆ’Î»)eâˆ’iÎ¸ âˆˆIR.
ii. Deduce that Î» = 1, then show that this case is impossible
too.
iii. Let m(Ï‰) be the number of eigenvalues of LÏ‰ of modulus
less than or equal to one (counted with multiplicities). Show
that m is constant on (0, 2).
(b)
i. Compute
lim
Ï‰â†’0
1
Ï‰(LÏ‰ âˆ’In).

9.6. Exercises
167
ii. Deduce that m = n, hence that the SOR converges for every
Ï‰ âˆˆ(0, 2).
7. (Extension of Theorem 9.4.1 to complex values of Ï‰). We still assume
that A is tridiagonal, that the Jacobi method converges, and that the
spectrum of J is real. We retain the notation of Section 9.4.
(a) Given an index a such that Î»a > 0, verify that âˆ†(Î»a) vanishes for
two real values of Ï‰, of which only one, denoted by Ï‰a, belongs
to the open disk D = D(1; 1). Show that 1 < Ï‰a < 2.
(b) Show that if Ï‰ âˆˆD\[Ï‰a, 2), then the roots of X2+Ï‰âˆ’1âˆ’Ï‰Î»aX
have distinct moduli, with one and only one of them, denoted
by Âµa(Ï‰), of modulus larger than |Ï‰ âˆ’1|1/2.
(c) Show that Ï‰ â†’Âµa is holomorphic on its domain, and that
lim
|Ï‰âˆ’1|â†’1 |Âµa(Ï‰)|2
=
1,
lim
Ï‰â†’Î³ |Âµa(Ï‰)|2
=
Î³ âˆ’1
if Î³ âˆˆ[Ï‰a, 2).
(d) Deduce that |Âµa(Ï‰)| < 1 (use the maximum principle), then that
the relaxation method converges for every Ï‰ âˆˆD.
(e) Show, ï¬nally, that the spectral radius of LÏ‰ is minimal for Ï‰ =
Ï‰r, which previously was denoted by Ï‰J.
8. Let B be a cyclic matrix of order three. With square diagonal blocks,
it reads blockwise as
B =
ï£«
ï£­
0
0
M1
M2
0
0
0
M3
0
ï£¶
ï£¸.
We wish to compare the Jacobi and Gaussâ€“Seidel methods for the
matrix A := I âˆ’B. Compute the matrix G. Show that Ï(G) = Ï(J)3.
Deduce that both methods converge or diverge simultaneously and
that, in case of convergence, Gaussâ€“Seidel is three times faster than
Jacobi. Show that for AT , the convergence or the divergence still
holds simultaneously, but that Gaussâ€“Seidel is only one and a half
times faster. Generalize to cyclic matrices of any order p.

10
Approximation of Eigenvalues
The computation of the eigenvalues of a square matrix is a problem of
considerable diï¬ƒculty. The naive idea, according to which it is enough to
compute the characteristic polynomial and then ï¬nd its roots, turns out
to be hopeless because of Abelâ€™s theorem, which states that the general
equation P(x) = 0, where P is a polynomial of degree d â‰¥5, is not solvable
using algebraic operations and roots of any order. For this reason, there
exists no direct method, even an expensive one, for the computation of
Sp(M).
Dropping half of that program, one could compute the characteristic
polynomial exactly, then compute an approximation of its roots. But the
cost and the instability of the computation are prohibitive. Amazingly, the
opposite strategy is often used: A standard algorithm for computing the
roots of a polynomial of high degree consists in forming its companion
matrix1 and then applying to this matrix the QR algorithm to compute its
eigenvalues with good accuracy.
Hence, all the methods are iterative. In particular, we shall limit ourselves
to the cases K = IR or CC. The general strategy consists in constructing a
sequence of matrices
M (0), M (1), . . . , M (m), . . . ,
1Fortunately, the companion matrix is a Hessenberg matrix; see below for this notion
and its practical aspects.

10.1. Hessenberg Matrices
169
pairwise similar, whose structure has some convergence property. Each
method is conceived in such a way that the sequence converges to a simple
form, triangular or diagonal, since then the eigenvalues can be read on the
diagonal. Such convergence is not always possible. For example, an algo-
rithm in Mn(IR) cannot converge to a triangular form when the matrix
under consideration possesses a pair of nonreal eigenvalues.
There are two strategies for the choice of M (0). One can naively take
M (0) = M. But since an iteration on a generic matrix is rather costly,
one often uses a preliminary reduction to a simple form (for example the
Hessenberg form, in the QR algorithm), which is preserved throughout the
iterations. With a few such tricks, certain methods can be astonishingly
eï¬ƒcient. The danger of iterative methods is the possible growth of round-
oï¬€errors and errors in the data. Typically, a procedure that doubles the
errors at each step transforms an initial error of size 10âˆ’3 into an O(1)
after ten iterations, which is by no means acceptable. For this reason, it
is important that the passage of M (m) to M (m+1) be contracting, that is,
that the errors be damped, or at worst not be ampliï¬ed. Since M (m+1) is
conjugate to M (m) by some matrix P (which in fact depends on m), the
growth rate is approximately the number K(P) := âˆ¥Pâˆ¥Â· âˆ¥P âˆ’1âˆ¥, called the
condition number, which is always greater than or equal to one. Using the
induced norm âˆ¥Â· âˆ¥2, it equals 1 if and only if P is a similitude matrix; that
is, P âˆˆCC Â· Un. For this reason, each iterative method builds sequences
of unitarily similar matrices: The conjugation matrices P (m) are unitary
(orthogonal if the ground ï¬eld is IR).
10.1
Hessenberg Matrices
Deï¬nition 10.1.1 A square matrix M âˆˆMn(K) is called upper Hessen-
berg (one speaks simply of a Hessenberg matrix) if mjk = 0 for every pair
(j, k) such that j âˆ’k â‰¥2.
A Hessenberg matrix thus has the form
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
x
Â· Â· Â·
Â· Â· Â·
y
...
0
...
...
...
...
...
...
...
...
0
Â· Â· Â·
0
z
t
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
In particular, an upper triangular matrix is a Hessenberg matrix.
When computing the spectrum of a given matrix, we may always restrict
ourselves to the case of an irreducible matrix, using a conjugation by a
permutation matrix: If M is reducible, we may limit ourselves to a block-
triangular matrix whose diagonal blocks are irreducible. It is enough then

170
10. Approximation of Eigenvalues
to compute the spectrum of each diagonal block. This principle applies
as well to a Hessenberg matrix. Hence one may always assume that M is
Hessenberg and that the mj+1,jâ€™s are nonzero. In that case, the eigenspaces
have dimension one. In fact, if Î» âˆˆÂ¯K, let L be the matrix extracted from
M âˆ’Î»In by deletion of the ï¬rst row and the last column. It is a triangular
matrix of Mnâˆ’1( Â¯K), invertible because its diagonal entries, the mj+1,jâ€™s,
are nonzero. Hence, M âˆ’Î»In is of rank at least equal to nâˆ’1, which implies
that the dimension of ker(M âˆ’Î»In) equals at most one.
Proposition 10.1.1 If M âˆˆMn(K) is a Hessenberg matrix with mj+1,j Ì¸=
0 for every j, in particular if this matrix is irreducible, then the eigenvalues
of M are geometrically simple.
The example
M =

1
1
âˆ’1
âˆ’1

shows that the eigenvalues of an irreducible Hessenberg matrix are not
necessarily algebraically simple.
From the point of view of matrix reduction by conjugation, one can at-
tribute two advantages to the Hessenberg class, compared with the class of
triangular matrices. First of all, if K = IR, many matrices are not trigonal-
izable in IR, though all are trigonalizable in CC. Of course, computing with
complex numbers is more expensive than computing with real numbers.
But we shall see that every square matrix with real entries is similar to a
Hessenberg matrix over the real numbers. Next, if K is algebraically closed,
the trigonalization of M needs the eï¬€ective computation of the eigenvalues,
which is impossible in view of Abelâ€™s theorem. However, the computation of
a similar Hessenberg matrix is obtained after a ï¬nite number of operations.
Let us observe, ï¬nally, that as the trigonalization (see Theorem 3.1.3),
the Hessenberg form is obtained through unitary transformations, a well-
conditionned process. When K = IR, these transformation are obviously
real orthogonal.
Theorem 10.1.1 For every matrix M âˆˆMn(CC) there exists a unitary
transformation U such that U âˆ’1MU is a Hessenberg matrix. If M âˆˆ
Mn(IR), one may take U âˆˆOn.
Moreover, the matrix U is computable in 5n3/3 + O(n2) multiplications
and 4n3/3 + O(n2) additions.
Proof
Let X âˆˆCCm be a unit vector: Xâˆ—X = 1. The matrix of the unitary
(orthogonal) symmetry with respect to the hyperplane XâŠ¥is S = Im âˆ’
2XXâˆ—. In fact, SX = X âˆ’2X = âˆ’X, while Y âˆˆXâŠ¥; that is Xâˆ—Y = 0,
implies SY = Y .

10.1. Hessenberg Matrices
171
We construct a sequence M1
= M, . . . , Mnâˆ’1 of unitarily similar
matrices. The matrix Mnâˆ’r will be of the form

H
B
0r,nâˆ’râˆ’1
Z
N

,
where H âˆˆMnâˆ’r(CC) is Hessenberg and Z is a vector in CCr. Hence, Mnâˆ’1
will be suitable.
One passes from Mnâˆ’r to Mnâˆ’r+1, that is, from r to râˆ’1, in the following
way. Let e1 be the ï¬rst vector of the canonical basis of CCr. If Z is colinear
to e1, one does nothing besides deï¬ning Mnâˆ’r+1 = Mnâˆ’r. Otherwise, one
chooses X âˆˆCCr so that SZ is parallel to e1 (we discuss below the possible
choices for X). Then one sets
V =

Inâˆ’r
0nâˆ’r,r
0r,nâˆ’r
S

,
which is a unitary matrix, with V âˆ—= V âˆ’1 = V (such a matrix is called a
Householder matrix). We then have
V âˆ’1Mnâˆ’rV =

H
BS
0n,nâˆ’râˆ’1
SZ
SNS

.
We thus deï¬ne Mnâˆ’r+1 = V âˆ’1Mnâˆ’rV .
There are two possible choices for S, given by
XÂ± :=
1
âˆ¥Z Â± âˆ¥Zâˆ¥2âƒ—qâˆ¥2
(Z Â± âˆ¥Zâˆ¥2q),
q = z1
|z1|e1.
It is always advantageous to choose the sign that gives the largest denom-
inator, namely the positive sign. One thus optimizes the round-oï¬€errors
when Z is almost aligned with e1.
Let us consider now the complexity of the (nâˆ’r)th step. Only the terms
of order r2 and r(n âˆ’r) are meaningful. The computation of X, in O(r)
operations, is thus negligible, like that of Xâˆ—and of 2X. The computation
of BS = B âˆ’(BX)(2Xâˆ—) needs about 4r(n âˆ’r) operations. Then 2NX
needs 2r2 operations, as does 2Xâˆ—N. We next compute 4Xâˆ—NX, and then
form the vector T := 4(Xâˆ—NX)X âˆ’2NX at the cost O(r). The product
TXâˆ—takes r2 operations, as 2X(Xâˆ—N). Then N + T Xâˆ—âˆ’X(2Xâˆ—N) needs
2r2 additions. The complete step is thus accomplished in 7r2 + 4r(n âˆ’
r) + O(n) operations. A sum from r = 1 to n âˆ’2 yields a complexity
of 3n3 + O(n2), in which one recognizes 5n3/3 + O(n2) multiplications,
4n3/3 + O(n2) additions, and O(n) square roots.
When M is Hermitian, the matrix U âˆ’1MU is still Hermitian. Since it
is Hessenberg, it is tridiagonal, with aj,j+1 = Â¯aj+1,j and ajj âˆˆIR. The
symmetry reduces the complexity to 2n3/3 + O(n2) multiplications. One
can then use the Hessenberg form of M in order to localize its eigenvalues.

172
10. Approximation of Eigenvalues
Proposition 10.1.2 If M is tridiagonal Hermitian and if the entries
mj+1,j are nonzero (that is, if M is irreducible), then the eigenvalues of
M are real and simple. Furthermore, if Mj is the (Hermitian, tridiagonal,
irreducible) matrix obtained by keeping only the j last rows and columns of
M, the eigenvalues of Mj strictly separate those of Mj+1.
The separation, not necessarily strict, of the eigenvalues of Mj+1 by those
of Mj has already been proved, in a more general framework, in Theorem
3.3.3.
Proof
The eigenvalues of a Hermitian matrix are real. Since this matrix is
diagonalizable, Proposition 10.1.1 shows that the eigenvalues are simple.
Both properties can be deduced from the following analysis.
We proceed by induction on j. If j â‰¥1, we decompose the matrix Mj+1
blockwise:
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
m
Â¯a
0
Â· Â· Â·
0
a
0
Mj
...
0
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
where a Ì¸= 0 and m âˆˆIR, m > 0. Let Pl be the characteristic polynomial
of Ml. We compute that of Mj+1 by expanding according to the elements
of the ï¬rst column:
Pj+1(X) = mPj(X) âˆ’|a|2Pjâˆ’1(X),
(10.1)
where P0 â‰¡1 by convention.
The induction hypothesis is as follows: Pj and Pjâˆ’1 have real entries and
have respectively j and j âˆ’1 real roots Âµ1, . . . , Âµj and Ïƒ1, . . . , Ïƒjâˆ’1, with
Âµ1 < Ïƒ1 < Âµ2 < Â· Â· Â· < Ïƒjâˆ’1 < Âµj.
In particular, they have no other roots, and their roots are simple. The signs
of the values of Pjâˆ’1 at points Âµj thus alternate. Since Pjâˆ’1 is positive over
(Ïƒjâˆ’1, +âˆ), we have (âˆ’1)jâˆ’kPjâˆ’1(Âµk) > 0.
This hypothesis clearly holds at step j = 1. If j â‰¥2 and if it holds at
step j, then (10.1) shows that Pj+1 âˆˆIR[X]. Furthermore,
(âˆ’1)jâˆ’kPj+1(Âµk) = âˆ’|a|2(âˆ’1)jâˆ’kPjâˆ’1(Âµk) < 0.
From the intermediate value theorem, Pj+1 possesses a root Î»k in
(Âµkâˆ’1, Âµk). Furthermore, Pj+1(Âµj) < 0, and Pj+1(x) is positive for x â‰«1;
hence there is also a root in (Âµj, +âˆ). Likewise, Pj+1 has a root in
(âˆ’âˆ, Âµ1). Hence, Pj+1 possesses j + 1 distinct real roots Î»k, with
Î»1 < Âµ1 < Î»2 < Â· Â· Â· < Âµj < Î»j+1.
Since Pj+1 has degree j + 1, there is no root other than the Î»kâ€™s, and these
are simple.

10.2. The QR Method
173
The sequence of polynomials Pj is a Sturm sequence, which allows
us to compute the number of roots of Pn in a given interval (a, b). A
Sturm sequence is a ï¬nite sequence of real polynomials Q0, . . . , Qn, with
Q0 a nonzero constant such that Qj(x) = 0 and 0 < j < n imply
Qj+1(x)Qjâˆ’1(x) < 0. In particular, Qj and Qj+1 do not share a com-
mon root. If a âˆˆIR is not a root of Qn, we denote by V (a) the number of
sign changes in the sequence (Q0(a), . . . , Qn(a)), with the zeros playing no
role.
Proposition 10.1.3 If Qn(a) Ì¸= 0 and Qn(b) Ì¸= 0, and if a < b, then the
number of roots of Qn in (a, b) is equal to V (a) âˆ’V (b).
Let us remark that it is not necessary of compute the polynomials Pj to
apply them to this proposition. Given a âˆˆIR, it is enough to compute the
sequence of values Pj(a).
Once an interval (a, b) is known to contain an eigenvalue Î» and only that
one (by means of Proposition 10.1.3 or Theorem 4.5.1), one can compute an
approximate value of Î», either by dichotomy, or by computing the numbers
V ((a+b)/2), . . ., or by the secant or Newton method. In the latter case, one
must compute Pn itself. The last two methods are convergent, provided that
we have a good initial approximation at our disposal, because P â€²
n(Î») Ì¸= 0.
We end this section with an obvious but nevertheless useful remark.
If M is Hessenberg and T upper triangular, the products T M and MT
are still Hessenberg (that would not be true if both matrices were Hessen-
berg). For example, if M admits an LU factorization, then L is Hessenberg,
and thus has only two nonzero diagonals, because L = MU âˆ’1. Similarly, if
M âˆˆGLn(CC), then the factor Q of the factorization M = QR is again Hes-
senberg, because Q = MRâˆ’1. An elementary compactness and continuity
argument shows that the same fact holds true for every M âˆˆMn(CC).
10.2
The QR Method
The QR method is considered the most eï¬ƒcient one for the approximate
computation of the whole spectrum of a square matrix M âˆˆMn(CC). One
employs it only after having reduced M to Hessenberg form, because this
form is preserved throughout the algorithm, while each iteration is much
cheaper than it is for a generic matrix.
10.2.1
Description of the QR Method
Let A âˆˆMn(K) be given, with K = IR or CC. We construct a sequence
of matrices (Aj)jâˆˆIN, with A1 = A. The induction Aj â†’Aj+1 consists
in performing the QR factorization of Aj, Aj = QjRj, and then deï¬ning

174
10. Approximation of Eigenvalues
Aj+1 := RjQj. We then have
Aj+1 = Qâˆ’1
j AjQj,
which shows that Aj+1 is unitarily similar to Aj. Hence,
Aj = (Q0 Â· Â· Â· Qjâˆ’1)âˆ’1A(Q0 Â· Â· Â· Qjâˆ’1)
(10.2)
is conjugate to A by a unitary transformation.
Let Pj := Q0 Â· Â· Â· Qjâˆ’1, which is unitary. Since Un is compact, the se-
quence (Pj)jâˆˆIN possesses cluster values. Let P be one of them. Then
Aâ€² := P âˆ’1AP = P âˆ—AP is a cluster point of (Aj)jâˆˆIN. Hence, if the se-
quence (Aj)j converges, its limit is unitarily similar to A, hence has the
same spectrum.
This argument shows that in general, the sequence (Aj)j does not con-
verge to a diagonal matrix, because then the eigenvectors of A would be
the columns of P. In other words, A would have an orthonormal eigenba-
sis. Namely, A would be normal. Except in this special case, one expects
merely that the sequence (Aj)j converges to a triangular matrix, an expec-
tation that is compatible with Theorem 3.1.3. But even this hope is too
optimistic in general. For example, if A is unitary, then Aj = A for every j,
with Qj = A and Rj = In; in that case, the convergence is useless, since the
limit A is not simpler than the data. We shall see later on that the reason
for this bad behavior is that the eigenvalues of a unitary matrix have the
same modulus: The QR method does not do a good job of separating the
eigenvalues of close modulus.
An important case in which a matrix has at least two eigenvalues of
the same modulus is that of matrices with real entries. If A âˆˆMn(IR),
then each Qj is real orthogonal, Rj is real, and Aj is real. This is seen by
induction on j. A limit Aâ€² will not be triangular if some eigenvalues of A
are nonreal, that is, if A possesses a pair of complex conjugate eigenvalues.
Let us sum up what can be expected in a brave new world. If all the
eigenvalues of A âˆˆMn(CC) have distinct moduli, the sequence (Aj)j might
converge to a triangular matrix, or at least its lower triangular part might
converge to
ï£«
ï£¬
ï£¬
ï£¬
ï£­
Î»1
0
Î»2
...
...
...
0
Â· Â· Â·
0
Î»n
ï£¶
ï£·
ï£·
ï£·
ï£¸.
When A âˆˆMn(IR), one makes the following assumption. Let p be the
number of real eigenvalues and 2q that of nonreal eigenvalues; then there
are p + q distinct eigenvalue moduli. In that case, (Aj)j might converge to
a block-triangular form, the diagonal blocks being 2Ã—2 or 1Ã—1. The limits
of the diagonal blocks provide trivially the eigenvalues of A.

10.2. The QR Method
175
The assertions made above have never been proved in full generality,
to our knowledge. We shall give below a rather satisfactory result in the
complex case.
10.2.2
The Case of a Singular Matrix
When A is not invertible, the QR factorization is not unique, raising a
diï¬ƒculty in the deï¬nition of the algorithm. The computation of the deter-
minant would detect immediately the case of noninvertibility, but would
not provide any solution. However, if the matrix has been ï¬rst reduced to
the Hessenberg form, then a single QR iteration detects the case and does
provide a solution. Indeed, if A is Hessenberg but not invertible, and if
A = QR, then Q is Hessenberg and R is not invertible. If a21 = 0, the
matrix A is block-triangular and we are reduced to the case of a matrix of
size (n âˆ’1) Ã— (n âˆ’1) by deleting the ï¬rst row and the ï¬rst column. Oth-
erwise, there exists j â‰¥2 such that rjj = 0. The matrix A1 = RQ is then
block-triangular, because it is Hessenberg and (A1)j,jâˆ’1 = rjjqj,jâˆ’1 = 0.
We are thus reduced to the computation of the spectra of two matrices of
sizes j Ã— j and (n âˆ’j) Ã— (n âˆ’j), the diagonal blocks of A1. After ï¬nitely
many such steps (not larger than the multiplicity of the null eigenvalue),
there remain only Hessenberg invertible matrices to deal with. We shall
assume therefore from now one that A âˆˆGLn(K).
10.2.3
Complexity of an Iteration
An iteration of the QR method requires the factorization Aj = QjRj and
the computation of Aj+1 = RjQj. Each part costs O(n3) operations if it
is done on a generic matrix (using the naive way of multiplying matrices).
Since the reduction to the Hessenberg form has a comparable cost, we loose
nothing by reducing A to this form. Actually, we make considerable gains
in two aspects. First, the cost of the QR iterations is reduced to O(n2).
Second, the cluster values of the sequence (Aj)j must have the Hessenberg
form too.
Let us examine ï¬rst the Householder method of QR factorization for a
generic matrix A. In practice, one computes only the factor R and matri-
ces of unitary symmetries whose product is Q. One then multiplies these
unitary matrices by R on the left to obtain Aâ€² = RQ.
Let a1 âˆˆCCn be the ï¬rst column vector of A. We begin by determining a
unit vector v1 âˆˆCCn such that the hyperplane symmetry H1 := In âˆ’2v1vâˆ—
1

176
10. Approximation of Eigenvalues
sends a1 to âˆ¥a1âˆ¥2e1. The matrix H1A has the form
ËœA =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
âˆ¥a1âˆ¥2
x
Â· Â· Â·
0
...
...
...
0
y
Â· Â· Â·
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£¸
.
We then perform these operations again on the matrix extracted from ËœA
by deleting the ï¬rst rows and columns, and so on. At the kth step, Hk is a
matrix of the form

Ik
0
0
Inâˆ’k âˆ’2vkvâˆ—
k

,
where vk âˆˆCCnâˆ’k is a unit vector. The computation of vk requires O(nâˆ’k)
operations. The product HkA(k), where A(k) is block-triangular, amounts
to that of two square matrices of size nâˆ’k, one of them I âˆ’2vkvâˆ—
k. We thus
compute a matrix N âˆ’2vvâˆ—N from v and N, which costs about 4(n âˆ’k)2
operations. Summing from k = 1 to k = n âˆ’1, we ï¬nd that the complexity
of the computation of R alone is 4n3/3 + O(n2). As indicated above, we
do not compute the factor Q, but compute all the matrices RHnâˆ’1 Â· Â· Â· Hk.
That necessitates 2n3 + O(n) operations (check this!). The complexity of
one step of the QR method on a generic matrix is thus 10n3/3 + O(n2).
Let us now analyze the situation when A is a Hessenberg matrix. By
induction on k, we see that vk belongs to the plane spanned by ek and
ek+1. Its computation needs O(1) operations. Then the product of Hk and
A(k) can be obtained by simply recomputing the rows of indices k and k+1,
about 6(n âˆ’k) operations. Summing from k = 1 to n âˆ’1, we ï¬nd that the
complexity of the computation of R alone is 3n2 + O(n). The computation
of the product (RHnâˆ’1 Â· Â· Â· Hk+1)Hk needs about 6k operations. Finally, the
complexity of the QR factorization of a Hessenberg matrix is 6n2 + O(n),
in which there are 4n2 + O(n) multiplications.
To sum up, the cost of the preliminary reduction of a matrix to Hessen-
berg form is less than or equal to what is saved during the ï¬rst iteration
of the QR method.
10.2.4
Convergence of the QR Method
As explained above, the best convergence statement assumes that the
eigenvalues have distinct moduli.
Let us recall that the sequence Ak is not always convergent. For example,
if A is already triangular, its QR factorization is Q = D, R = Dâˆ’1A, with
dj = ajj/|ajj|. Hence, A1 = Dâˆ’1AD is triangular, with the same diagonal
as that of A. By induction, Ak is triangular, with the same diagonal as
that of A. We have thus Qk = D for every k, so that Ak = Dâˆ’kADk. The

10.2. The QR Method
177
entry of index (l, m) is thus multiplied at each step by a unit number zlm,
which is not necessarily equal to one if l < m. Hence, the part above the
diagonal of Ak does not converge.
Summing up, a convergence theorem may concern only the diagonal of
Ak and what is below it.
Lemma 10.2.1 Let A âˆˆGLn(K) be given, with K = IR or CC. Let
Ak = QkRk be the sequence of matrices given by the QR algorithm. Let
us deï¬ne Pk = Q0 Â· Â· Â· Qkâˆ’1 and Uk = Rkâˆ’1 Â· Â· Â· R0. Then PkUk is the QR
factorization of the kth power of A:
Ak = PkUk.
Proof
From (10.2), we have Ak = P âˆ’1
k APk; that is, PkAk = APk. Then
Pk+1Uk+1 = PkQkRkUk = PkAkUk = APkUk.
By induction, PkUk = Ak. However, Pk âˆˆUn and Uk is triangular, with a
positive real diagonal, as a product of such matrices.
Theorem 10.2.1 Let A âˆˆGLn(CC) be given. Assume that the moduli of
the eigenvalues of A are distinct:
|Î»1| > |Î»2| > Â· Â· Â· > |Î»n|
(> 0).
In particular, the eigenvalues are simple, and thus A is diagonalizable:
A = Y âˆ’1 diag(Î»1, . . . , Î»n)Y.
Assume also that Y admits an LU factorization. Then the strictly lower
triangular part of Ak converges to zero, and the diagonal of Ak converges
to D := diag(Î»1, . . . , Î»n).
Proof
Let Y = LU be the factorization of Y . We also make use of the QR
factorization of Y âˆ’1: Y âˆ’1 = QR. Since Ak = Y âˆ’1DkY , we have PkUk =
Y âˆ’1DkY = QRDkLU.
The matrix DkLDâˆ’k is lower triangular with unit numbers on its di-
agonal. By assumption, its strictly lower part tends to zero (because
each term is multiplied by (Î»i/Î»j)k, where |Î»i/Î»j| < 1). Therefore,
DkLDâˆ’k = In + Ek with Ek â†’0n as k â†’+âˆ. Hence, PkUk =
QR(In + Ek)DkU = Q(In + REkRâˆ’1)RDkU = Q(In + Fk)RDkU, where
Fk â†’0n. Let OkTk = In + Fk be the QR factorization of In + Fk. By
continuity, Ok and Tk both tend to In. Then
PkUk = (QOk)(TkRDkU).
The ï¬rst product is a unitary matrix, while the second is a triangular
one. Let |D| be the â€œmodulusâ€ matrix of D (whose entries are the moduli

178
10. Approximation of Eigenvalues
of those of D), and let D1 be |D|âˆ’1D, which is unitary. We also deï¬ne
D2 = diag(ujj/|ujj|) and U â€² = Dâˆ’1
2 U. Then D2 is unitary and the diagonal
of U â€² is positive real. From the uniqueness of the QR factorization of an
invertible matrix we obtain
Pk = QOkDk
1D2,
Uk = (Dk
1D2)âˆ’1TkRDk
1D2|D|kU â€²,
which yields
Qk
=
P âˆ’1
k Pk+1 = Dâˆ’1
2 Dâˆ’k
1 Oâˆ’1
k Ok+1Dk+1
1
D2,
Rk
=
Uk+1U âˆ’1
k
= Dâˆ’1
2 Dâˆ’kâˆ’1
1
Tk+1RDRâˆ’1T âˆ’1
k Dk
1D2.
Since Dâˆ’k
1
and Dk+1
1
are bounded, we deduce that Qk converges, to D1.
Similarly, Rk âˆ’Râ€²
k â†’0n, where
Râ€²
k = Dâˆ’1
2 Dâˆ’k
1 RDRâˆ’1Dkâˆ’1
1
D2.
(10.3)
The fact that the matrix Râ€²
k is upper triangular shows that the strict lower
triangular part of Ak = QkRk tends to zero (observe that the sequence
(Rk)kâˆˆIN is bounded, because the set of unitary matrices conjugate to A
is bounded). Similarly, the diagonal of Râ€²
k is |D|, which shows that the
diagonal of Ak converges to D1|D| = D.
Remark: Formula (10.3) shows that the sequence Ak does not converge,
at least when the eigenvalues have distinct complex arguments. However,
if the eigenvalues have equal complex arguments, for example if they are
real and positive, then D1 = Î±In and Rk â†’T := Dâˆ’1
2 R|D|Râˆ’1D2; hence
Ak converges to Î±T. Note that this limit is not diagonal in this case.
The situation is especially favorable for tridiagonal Hermitian matrices.
To begin with, we may assume that A is positive deï¬nite, up to the change
of A into A + ÂµIn with Âµ > âˆ’Ï(A). Next, we can write A in block-diagonal
form, where the diagonal blocks are tridiagonal irreducible Hermitian ma-
trices. The QR method then treats each block separately. We are thus
reduced to the case of a Hermitian positive deï¬nite, tridiagonal and irre-
ducible matrix. Its eigenvalues are real, strictly positive, and simple, from
Proposition 10.1.2: we have Î»1 > Â· Â· Â· > Î»n > 0. We can then use the
following statement.
Theorem 10.2.2 Let A âˆˆGLn(CC) be an irreducible Hessenberg matrix
whose eigenvalues are of distinct moduli:
|Î»1| > Â· Â· Â· > |Î»n|
(> 0).

10.2. The QR Method
179
Then the QR method converges; that is, the lower triangular part of Ak
converges to
ï£«
ï£¬
ï£¬
ï£¬
ï£­
Î»1
0
Î»2
...
...
...
0
Â· Â· Â·
0
Î»n
ï£¶
ï£·
ï£·
ï£·
ï£¸.
Proof
In the light of Theorem 10.2.1, it is enough to show that the matrix
Y in the previous proof admits an LU factorization. We have Y A =
diag(Î»1, . . . , Î»n)Y . The rows of Y are thus the left eigenvectors: ljA = Î»jlj.
If x âˆˆCCn is nonzero, there exists a unique index r such that xr Ì¸= 0,
while j > r implies xj = 0. By induction, quoting the Hessenberg form and
the irreducibility of A, we obtain (Amx)r+m Ì¸= 0, while j > r + m implies
(Amx)j = 0. Hence, the vectors x, Ax, . . . , Anâˆ’rx are linearly independent.
A linear subspace, stable under A and containing x, is thus of dimension
greater than or equal to n âˆ’r + 1.
Let F be a linear subspace, stable under A, of dimension p â‰¥1. Let
r be the smallest integer such that F contains a nonzero vector x with
xr+1 = Â· Â· Â· = xn = 0. The minimality of r implies that xr Ì¸= 0. Hence, we
have p â‰¥n âˆ’r + 1. By construction, the intersection of F and of linear
subspace [e1, . . . , enâˆ’p] spanned by e1, . . . , erâˆ’1 reduces to {0}. Thus we
also have p + (r âˆ’1) â‰¤n. Finally, r = n âˆ’p + 1, and we see that
F âŠ•[e1, . . . , enâˆ’p] = CCn.
Let us choose F = [l1, . . . , lq]âŠ¥, which is stable under A. Then p = nâˆ’q,
and we have
[l1, . . . , lq]âŠ¥âŠ•[e1, . . . , eq] = CCn.
This amounts to saying that det(ljek)1â‰¤j,kâ‰¤q Ì¸= 0. In other words, the
leading principal minor of order q of Y is nonzero. From Theorem 8.1.1, Y
admits an LU factorization.
Corollary 10.2.1 If A âˆˆHPDn and if A0 is a Hessenberg matrix, unitar-
ily similar to A (for example, a matrix obtained by Householderâ€™s method),
then the sequence Ak deï¬ned by the QR method converges to a diagonal
matrix whose diagonal entries are the eigenvalues of A.
Indeed, A0 is block-diagonal with irreducible diagonal blocks. We are thus
reduced to the case of a Hermitian positive deï¬nite tridiagonal irreducible
matrix. Such a matrix satisï¬es the hypotheses of Theorem 10.2.2. The lower
triangular part converges, hence the whole matrix, since it is Hermitian.
Implementing the QR method: The QR method converges faster as
Î»n, or merely Î»n/Î»nâˆ’1, becomes smaller. We can obtain this situation

180
10. Approximation of Eigenvalues
by translating Ak â†’Ak âˆ’Î±kIn. The strategies for the choice of Î±k are
described in [25]. This procedure is called Rayleigh translation. It allows
for a observeable improvement of the convergence of the QR method. If
the eigenvalues of A are simple, a suitable translation allows us to restrict
ourselves to the case of distinct moduli. But this trick has a nonnegligible
cost if A is a real matrix with a pair of complex conjugate eigenvalues,
since it requires a translation by a nonreal number Î±. As mentioned above,
the computations become much more costly than they are in the domain
of real numbers.
As k increases, the triangular form of Ak appears ï¬rst at the last row.
In other words, the sequence (Ak)nn converges more rapidly thanother
sequences (Ak)jj. When the last row is suï¬ƒciently close to (0, . . . , 0, Î»n),
the Rayleigh translation must be selected in such a way as to bring Î»nâˆ’1,
instead of Î»n, to the origin; and so on.
With a clever choice of Rayleigh translations, the QR method, when it
converges, is of order two for a generic matrix, and is of order three for a
Hermitian matrix.
10.3
The Jacobi Method
The Jacobi method allows for the approximate computation of the whole
spectrum of a real symmetric matrix A âˆˆSymn. As in the QR method,
one constructs a sequence of matrices, unitarily similar to A. In particular,
the round-oï¬€errors are not ampliï¬ed. Each iteration is cheap (O(n) opera-
tions), and the convergence is quadratic when the eigenvalues are distinct.
It is thus a rather eï¬ƒcient method.
10.3.1
Conjugating by a Rotation Matrix
Let 1 â‰¤p, q â‰¤n be two distinct indices and Î¸ âˆˆ[âˆ’Ï€, Ï€) an angle. We
denote by Rp,q(Î¸) the rotation matrix through the angle Î¸ in the plane
spanned by ep and eq. For example, if p < q, then
R = Rp,q(Î¸) :=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Ipâˆ’1
...
0
...
0
Â· Â· Â·
cos Î¸
Â· Â· Â·
sin Î¸
Â· Â· Â·
0
...
Iqâˆ’pâˆ’1
...
0
Â· Â· Â·
âˆ’sin Î¸
Â· Â· Â·
cos Î¸
Â· Â· Â·
0
...
0
...
Inâˆ’q
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.

10.3. The Jacobi Method
181
If H is a symmetric matrix, we compute K := Râˆ’1HR = RT HR, which is
also symmetric. Setting c = cos Î¸, s = sin Î¸ the following formulas hold:
kij
=
hij
if i, j Ì¸= p, q,
kip
=
chip âˆ’shiq
if i Ì¸= p, q,
kiq
=
chiq + ship
if i Ì¸= p, q,
kpp
=
c2hpp + s2hqq âˆ’2cshpq,
kqq
=
c2hqq + s2hpp + 2cshpq,
kpq
=
cs(hpp âˆ’hqq) + (c2 âˆ’s2)hpq.
The cost of the computation of entries kij for i, j Ì¸= p, q is zero; that of
kpp, kqq, and kpq is O(1). The cost of this conjugation is thus 6n + O(1)
operations, keeping in mind the symmetry KT = K.
Let us remark that the conjugation by the rotation through the angle
Î¸Â±Ï€ yields the same matrix K. For this reason, we limit ourselves to angles
Î¸ âˆˆ[âˆ’Ï€/2, Ï€/2).
10.3.2
Description of the Method
One constructs a sequence A(0) = A, A(1), . . . of symmetric matrices,
each one conjugate to the previous one by a rotation as above: A(k+1) =
(R(k))T A(k)R(k). At step k, we choose two distinct indices p and q (in fact,
pk, qk) in such a way that a(k)
pq Ì¸= 0 (if it is not possible, A(k) is already a
diagonal matrix similar to A). We then choose Î¸ (in fact Î¸k) in such a way
that a(k+1)
pq
= 0. From the formulas above, this is equivalent to
cs(a(k)
pp âˆ’a(k)
qq ) + (c2 âˆ’s2)a(k)
pq = 0.
This amounts to solving the equation
cot 2Î¸ = a(k)
qq âˆ’a(k)
pp
2a(k)
pq
=: Ïƒk.
(10.4)
This equation possesses two solutions in [âˆ’Ï€/2, Ï€/2), namely Î¸k
âˆˆ
[âˆ’Ï€/4, Ï€/4) and Î¸k Â± Ï€/2. There are thus two possible rotation matri-
ces, which yield to two distinct results. Once the angle has been selected,
its computation is useless (it would be actually rather expensive). In fact,
t := tan Î¸k solves
2t
1 âˆ’t2 = tan 2Î¸;
that is,
t2 + 2tÏƒk âˆ’1 = 0.

182
10. Approximation of Eigenvalues
The two angles correspond to the two possible roots of this quadratic
equation. We then obtain
c =
1
âˆš
1 + t2 ,
s = tc.
We shall see below that the best choice is the angle Î¸k âˆˆ[âˆ’Ï€/4, Ï€/4), which
corresponds to the unique root t in [âˆ’1, 1).
The computation of c, s needs only O(1) operations, so that the cost of
an iteration of the Jacobi method is 6n + O(1). Observe that an entry that
has vanished at an iteration becomes in general nonzero after a few more
iterations.
10.3.3
Convergence of the Jacobi Method
We use here the Schur norm âˆ¥Mâˆ¥= (Tr M T M)1/2, also called the Frobe-
nius norm, denoted elsewhere by âˆ¥Mâˆ¥2. Since it amounts to showing that
A(k) converges to a diagonal matrix, we decompose this matrix in the form
A(k) = Dk + Ek, where Dk = diag(a(k)
11 , . . . , a(k)
nn). To begin with, since the
sequence is formed of unitarily similar matrices, we have âˆ¥A(k)âˆ¥= âˆ¥Aâˆ¥.
Lemma 10.3.1 We have
âˆ¥Ek+1âˆ¥2 = âˆ¥Ekâˆ¥2 âˆ’2

a(k)
pq
2
.
Proof
It is suï¬ƒcient to redo the calculations of Section 10.3.1, noting that
k2
ip + k2
iq = h2
ip + h2
iq
whenever i Ì¸= p, q, while k2
pq = 0.
We deduce from the lemma that âˆ¥Dk+1âˆ¥2 = âˆ¥Dkâˆ¥2 + 2

a(k)
pq
2
. The
convergence of the Jacobi method depends, then, on the choice of the pair
(p, q) at each step. For example, the choice of the same pair at two consec-
utive iterations is stupid, since it yields A(k+1) = A(k). A ï¬rst strategy (the
so-called optimal choice) consists in taking the pair (p, q) that optimizes the
instantaneous decay of âˆ¥Ekâˆ¥, that is, maximizes the number |a(k)
pq |. Since
this method involves the sorting of n(nâˆ’1)/2 entries, it is rather expensive.
Other strategies are available. One can, for instance, range over every pair
(p, q) with p < q, or choose a (p, q) for which |a(k)
pq | is larger than some
threshold. Here we shall study only the method with optimal choice.
Theorem 10.3.1 With the â€œoptimal choiceâ€ of (pk, qk) and with the choice
Î¸k âˆˆ[âˆ’Ï€/4, Ï€/4), the Jacobi method converges in the following sense. There
exists a diagonal matrix D such that
âˆ¥A(k) âˆ’Dâˆ¥â‰¤
âˆš
2âˆ¥E0âˆ¥
1 âˆ’Ï Ïk,
Ï :=
%
1 âˆ’
2
n2 âˆ’n.

10.3. The Jacobi Method
183
In particular, the spectrum of A consists of the diagonal terms of D, and
the Jacobi method is of order one at least.
Proof
With the optimal choice of (p, q), we have
(n2 âˆ’n)

a(k)
pq
2
â‰¥âˆ¥Ekâˆ¥2.
Hence,
âˆ¥Ek+1âˆ¥2 â‰¤

1 âˆ’
2
n2 âˆ’n

âˆ¥Ekâˆ¥2.
It follows that âˆ¥Ekâˆ¥â‰¤Ïkâˆ¥E0âˆ¥. In particular, Ek tends to zero as k â†’+âˆ.
It remains to show that Dk converges too. A calculation using the
notation of Section 10.3.1 and the fact that kpq = 0 yield
kpp âˆ’hpp = thpq.
Since |Î¸k| â‰¤Ï€/4, we have |t| â‰¤1, so that |a(k+1)
pp
âˆ’a(k)
pp | â‰¤|a(k)
pq |. Likewise,
|a(k+1)
qq
âˆ’a(k)
qq | â‰¤|a(k)
pq |. Since the other diagonal entries are unchanged, we
have âˆ¥Dk+1 âˆ’Dkâˆ¥â‰¤âˆ¥Ekâˆ¥.
We have seen that âˆ¥Ekâˆ¥â‰¤Ïkâˆ¥E0âˆ¥. Therefore,
âˆ¥Dl âˆ’Dkâˆ¥â‰¤âˆ¥E0âˆ¥Ïk
1 âˆ’Ï,
l > k.
The sequence (Dk)kâˆˆIN is thus Cauchy, hence convergent. Since Ek tends
to zero, Ak converges to the same limit D. This matrix is diagonal, with
the same spectrum as A, since this is true for each Ak. Finally, we obtain
âˆ¥A(k) âˆ’Dâˆ¥2 = âˆ¥Dk âˆ’Dâˆ¥2 + âˆ¥Ekâˆ¥2 â‰¤
2
(1 âˆ’Ï)2 âˆ¥Ekâˆ¥2.
10.3.4
Quadratic Convergence
The following statement shows that the Jacobi method compares rather
well with other methods.
Theorem 10.3.2 The Jacobi method with optimal choice of (p, q) is of
order two when the eigenvalues of A are simple, in the following sense. Let
N = n(n âˆ’1)/2 be the number of elements under the diagonal. Then there
exists a number c > 0 such that
âˆ¥Ek+Nâˆ¥â‰¤câˆ¥Ekâˆ¥2,
for every k âˆˆIN.
Proof

184
10. Approximation of Eigenvalues
We ï¬rst remark that if i Ì¸= j with {i, j} Ì¸= {pl, ql}, then
|a(l+1)
ij
âˆ’a(l)
ij | â‰¤|tl|
âˆš
2âˆ¥Elâˆ¥,
(10.5)
where tl = tan Î¸l. To see this, observe that 1 âˆ’c â‰¤t and |s| â‰¤t
whenever |t| â‰¤1. However, Theorem 10.3.1 ensures that Dk converges
to diag(Î»1, . . . , Î»n), where the Î»jâ€™s are the eigenvalues of A. Since these
are distinct, there exist K âˆˆIN and Î´ > 0 such that, if k â‰¥K, then
min
iÌ¸=j |a(k)
ii âˆ’a(k)
jj | â‰¥Î´
for k â‰¥K. We have therefore
|Ïƒk| â‰¥
Î´
âˆš
2âˆ¥Ekâˆ¥
kâ†’+âˆ
âˆ’â†’+âˆ.
It follows that tk tends to zero and, more precisely, that
tk âˆ¼âˆ’1
2Ïƒk
.
Finally, there exists a constant c1 such that
|tk| â‰¤c1âˆ¥Ekâˆ¥.
Let us ï¬x then k larger than K, and let us denote by J the set of pairs
(pl, ql) when k â‰¤l â‰¤k + N âˆ’1. For such an index, we have âˆ¥Elâˆ¥â‰¤
Ïlâˆ’kâˆ¥Ekâˆ¥â‰¤âˆ¥Ekâˆ¥. In particular, |tl| â‰¤c1âˆ¥Ekâˆ¥.
If (p, q) âˆˆJ and if l < k+N is the largest index such that (p, q) = (pl, ql),
a repeated application of (10.5) shows that
|a(k+N)
pq
| â‰¤c1N
âˆš
2âˆ¥Ekâˆ¥2.
If J is equal to the set of pairs (i, j) such that i < j, these inequalities
ensure that âˆ¥Ek+Nâˆ¥â‰¤c2âˆ¥Ekâˆ¥2. Otherwise, there exists a pair (p, q) that
one twice sets to zero: (p, q) = (pl, ql) = (pm, qm) with k â‰¤l < m < k + N.
In that case, the same argument as above shows that
âˆ¥Ek+Nâˆ¥â‰¤âˆ¥Emâˆ¥â‰¤
âˆš
2N|a(m)
pq | â‰¤2
âˆš
Nc1(m âˆ’l)âˆ¥Ekâˆ¥2.
Remarks: Exercise 18 shows that the distance between the diagonal and
the spectrum of A is O(âˆ¥Ekâˆ¥2), and not O(âˆ¥Ekâˆ¥) as naively expected. We
shall also analyze, in Exercise 10, the (bad) behavior of Dk when we make
the opposite choice Ï€/4 â‰¤|Î¸k| â‰¤Ï€/2.
10.4
The Power Methods
The power methods allow only for the approximation of a single eigenvalue.
Of course, their cost is signiï¬cantly lower than that of the previous ones.

10.4. The Power Methods
185
The standard method is especially designed for the search for the optimal
parameter in the SOR method for a tridiagonal matrix, where we have to
compute the spectral radius of the Jacobi iteration matrix (Theorem 9.4.1).
10.4.1
The Standard Method
Let M âˆˆMn(CC) be a matrix. We search for an approximation of its eigen-
value of maximum modulus, whenever only one such exists. The standard
method consists in choosing a norm on CCn, a unit vector x0 âˆˆCCn, and
then computing successively the vectors xk by the formula
xk+1 :=
1
âˆ¥Mxkâˆ¥Mxk.
The justiï¬cation of this method is given in the following theorem.
Theorem 10.4.1 One assumes that Sp M contains only one element Î» of
maximal modulus (that modulus is thus equal to Ï(M)).
If Ï(M) = 0, the method stops because Mxk = 0 for some k < n.
Otherwise, let CCn = E âŠ•F be the decomposition of CCn, where E, F are
stable linear subspaces under M, with Sp(M|E) = {Î»} and Î» Ì¸âˆˆSp(M|F ).
Assume that x0 Ì¸âˆˆF. Then Mxk Ì¸= 0 for every k âˆˆIN and:
1.
lim
kâ†’+âˆâˆ¥Mxkâˆ¥= Ï(M).
(10.6)
2.
V :=
lim
kâ†’+âˆ

Â¯Î»
Ï(M)
k
xk
is a unit eigenvector of M, associated to the eigenvalue Î».
3. If Vj Ì¸= 0, then
lim
kâ†’+âˆ
(Mxk)j
xk
j
= Î».
Proof
The case Ï(M) = 0 is obvious because M is then nilpotent. We may thus
assume that Ï(M) > 0.
Let x0 = y0 + z0 be the decomposition of x0 with y0 âˆˆE and z0 âˆˆF.
By assumption, y0 Ì¸= 0. Since M|E is invertible, M ky0 Ì¸= 0. Since M kx0 =
M ky0 + M kz0, M ky0 âˆˆE, and M kz0 âˆˆF, we conclude that M kx0 Ì¸= 0.

186
10. Approximation of Eigenvalues
The algorithm may be rewritten as2
xk =
1
âˆ¥M kx0âˆ¥M kx0.
We therefore have xk Ì¸= 0.
If F Ì¸= {0}, then Ï(M|F ) < Ï(M) by construction. Hence there exist
(from Theorem 4.2.1) Î· < Ï(M) and C > 0 such that âˆ¥(M|F )kâˆ¥â‰¤CÎ·k for
every k. Then âˆ¥(M|F)kz0âˆ¥â‰¤C1Î·k. On the other hand, Ï((M|E)âˆ’1) =
1/Ï(M), and the same argument as above ensures that âˆ¥(M|E)âˆ’kâˆ¥â‰¤
1/C2Âµk, for some Âµ âˆˆ(Î·, Ï(M)), so that âˆ¥M ky0âˆ¥â‰¥C3Âµk. Hence,
âˆ¥M kz0âˆ¥â‰ªâˆ¥M ky0âˆ¥,
so that
xk âˆ¼
1
âˆ¥M ky0âˆ¥M ky0.
We are thus reduced to the case where z0 = 0, that is, where M has no
eigenvalue but Î». That will be assumed from now on.
Let r be the degree of the minimal polynomial of M. The vector space
spanned by the vectors x0, Mx0, . . . , M râˆ’1x0 contains all the xkâ€™s. Up to
the replacement of CCn by this linear subspace, one may assume that it
equals CCn. Then we have r = n. Furthermore, since ker(M âˆ’Î»)nâˆ’1, a
nontrivial linear subspace, is stable under A, we see that x0 Ì¸âˆˆker(M âˆ’
Î»)nâˆ’1.
The vector space CCn then admits the basis
{v1 = x0, v2 = (M âˆ’Î»)x0, . . . , vn = (M âˆ’Î»)nâˆ’1x0}.
With respect to this basis, M becomes the Jordan matrix
Ëœ
M =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
Î»
0
. . .
. . .
1
...
...
...
0
...
...
...
...
...
...
...
...
0
. . .
0
1
Î»
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
The matrix Î»âˆ’k Ëœ
M k depends polynomially on k. The coeï¬ƒcient of highest
degree, as k â†’+âˆ, is at the intersection of the ï¬rst column and the last
row. It equals

k
n âˆ’1

Î»1âˆ’n,
2One could normalize xk at the end of the computation, but we prefer doing it at
each step in order to avoid overï¬‚ows, and also to ensure (10.6).

10.4. The Power Methods
187
which is equivalent to (k/Î»)nâˆ’1/(n âˆ’1)!. We deduce that
M kx0 âˆ¼knâˆ’1Î»kâˆ’n+1
(n âˆ’1)!
vn.
Hence,
xk âˆ¼
 Î»
|Î»|
kâˆ’n+1
vn
âˆ¥vnâˆ¥.
Since vn is an eigenvector of M, the claims of the theorem have been proved.
The case where the algebraic and geometric multiplicities of Î» are equal
(that is, M|E = Î»IE), for example if Î» is a simple eigenvalue, is especially
favorable. Indeed, M ky0 = Î»ky0, and therefore
xk =
1
âˆ¥y0âˆ¥y0 + O
âˆ¥M kz0âˆ¥
|Î»|k

.
Theorem 4.2.1 thus shows that the error
xk âˆ’
1
âˆ¥y0âˆ¥y0
tends to zero faster than
Ï(M|F ) + Ïµ
Ï(M)
k
,
for every Ïµ > 0. The convergence is thus of order one, and becomes faster
as the ratio |Î»2|/|Î»1| becomes smaller (arranging the eigenvalues by nonin-
creasing moduli). However, the convergence is much slower when the Jordan
blocks of M relative to Î» are nontrivial. The error decays then like 1/k in
general.
The situation is more delicate when Ï(M) is the modulus of several
distinct eigenvalues. The vector xk, suitably normalized, does not converge
in general but â€œspinsâ€ closer and closer to the sum of the corresponding
eigenspaces. The observation of the asymptotic behavior of xk allows us
to identify the eigendirections associated to the eigenvalues of maximal
modulus. The sequence âˆ¥Mxkâˆ¥does not converge and depends strongly on
the choice of the norm. However, log âˆ¥Mxkâˆ¥converges in the Cesaro sense,
that is, in the mean, to log Ï(M) (Exercise 12).
Remark: The hypothesis on x0 is generic, in the sense that it is satisï¬ed for
every choice of x0 in an open dense subset of CCn. If by chance x0 belongs to
F, the power method furnishes theoretically another eigenvalue, of smaller
modulus. In practice, a large enough number of iterations always allows for
the convergence to Î». In fact, the number Î» is rarely exactly representable in
a computer. If it is not, the linear subspace F does not contain any nonzero
representable vector. Thus the vector x0, or its computer representation,
does not belong to F, and Theorem 10.4.1 applies.

188
10. Approximation of Eigenvalues
10.4.2
The Inverse Power Method
Let us assume that M is invertible. The standard power method, applied
to M âˆ’1, furnishes the eigenvalue of least modulus, whenever it is simple, or
at least its modulus in the general case. Since the inversion of a matrix is a
costly operation, we involve ourselves with that idea only if M has already
been inverted, for example if we had previously had to make an LU or a QR
factorization. That is typically the situation when one begins to implement
the QR algorithm for M. It might look strange to involve a method giving
only one eigenvalue in the course of a method that is expected to compute
the whole spectrum.
The inverse power method is thus subtle. Here is the idea. One begins
by implementing the QR method, until one gets coarse approximations
Âµ1, . . . , Âµn of the eigenvalues Î»1, . . . , Î»n. If one persists in the QR method,
the proof of Theorem 10.2.1 shows that the error is at best of order Ïƒk
with Ïƒ = maxj |Î»j+1/Î»j|. When n is large, Ïƒ is in general close to 1 and
this convergence is rather slow. Similarly, the method with Rayleigh trans-
lations, for which Ïƒ is replaced by Ïƒ(Î·) := maxj |(Î»j+1 âˆ’Î·)/(Î»j âˆ’Î·)|, is not
satisfactory. However, if one wishes to compute a single eigenvalue, say Î»p,
with full accuracy, the power method, applied to M âˆ’ÂµpIn, produces an
error on the order of Î¸k, where Î¸ := |Î»p âˆ’Âµp|/ minjÌ¸=p |Î»j âˆ’Âµp| is a small
number, since Î»p âˆ’Âµp is small.
In practice, the inverse power method is used mainly to compute an
approximate eigenvector, associated to an eigenvalue for which one already
has a good approximate value.
10.5
Leverrierâ€™s Method
The method of Leverrier allows for the computation of the characteris-
tic polynomial of a square matrix. Though inserted in this Chapter, this
method is not suitable for computing approximate values of the eigenval-
ues of a matrix. First of all, it furnishes only the characteristic polynomial
which, as mentioned at the opening if this chapter, is not a good technique
for computing the eigenvalues. Its interest is purely academic. Observe,
however, that it is of great generality, applying to matrices with entries in
any ï¬eld of characteristic 0.
10.5.1
Description of the Method
Let K be a ï¬eld of characteristic 0 and M âˆˆMn(K) be given. Let us
denote by Î»1, . . . , Î»n the eigenvalues of M, counted with multiplicity. Let
us deï¬ne the two following lists of n numbers:

10.5. Leverrierâ€™s Method
189
Elementary symmetric polynomials
Ïƒ1
:=
Î»1 + Â· Â· Â· + Î»n = Tr M,
Ïƒ2
:=

j<k
Î»jÎ»k,
...
Ïƒr
:=

j1<Â·Â·Â·<jr
Î»j1 Â· Â· Â· Î»jr,
...
Ïƒn
:=

j
Î»j = det M.
Newton sums
sm :=

j
Î»m
j ,
1 â‰¤m â‰¤n.
The numbers (âˆ’1)jÏƒj are the coeï¬ƒcients of the characteristic polynomial
of M:
PM(X) = Xn âˆ’Ïƒ1Xnâˆ’1 + Ïƒ2Xnâˆ’2 âˆ’Â· Â· Â· + (âˆ’1)nÏƒn.
Furthermore, the sm are the traces of the powers M m. One can obtain
them by computing M 2, . . . , M n. Each of these matrices is obtained in
O(nÎ±) operations, with 2 â‰¤Î± â‰¤3 (Î± = 3, using the naive method for
the product of two matrices). In all, the computation of s1, . . . , sn needs
O(nÎ±+1) operations, which is a lot, compared to iterative methods (QR ,
Jacobi), for which each iteration is made in O(n2) operations at worst.
The passage from Newton sums to elementary symmetric polynomials is
done through Newtonâ€™s formulas. If Î£j = (âˆ’1)jÏƒj and Î£0 := 1, we have
mÎ£m +
m

k=1
skÎ£mâˆ’k = 0,
1 â‰¤n.
One uses these formulas in increasing order, beginning with Î£1 = âˆ’s1.
When Î£1, . . . , Î£mâˆ’1 are known, one computes
Î£m = âˆ’1
m(s1Î£mâˆ’1 + Â· Â· Â· + smÎ£0).
This computation, which needs only O(n2) operations, has a negligible cost.
Besides the high cost of this method, its instability is unfortunate when
k = IR or k = CC: when n is large, sk increases like Ï(M)k, thus much
more rapidly than Ïƒk. The eigenvalues of smaller modulus are thus much
perturbed by the round-oï¬€errors, and this is reinforced by the large number
of operations.

190
10. Approximation of Eigenvalues
When the ï¬eld is of nonzero characteristic p, the Leverrier method may
be employed only if n < p. Since sp = Ïƒp
1, the computation of the smâ€™s for
m â‰¥p does not bring any new information about the Ïƒjâ€™s.
10.6
Exercises
1. Given a polynomial P âˆˆIR[X], use the Euclidean division in order to
deï¬ne a sequence of nonzero polynomials Pj in the following way. Set
P0 = P, P1 = P â€². If Pj is not constant, âˆ’Pj+1 is the remainder of
the division of Pjâˆ’1 by Pj: Pjâˆ’1 = QjPj âˆ’Pj+1, deg Pj+1 < deg Pj.
(a) Assume that P has only simple roots. Show that the sequence
(Pj)j is well-deï¬ned, that it has only ï¬nitely many terms, and
that it is a Sturm sequence.
(b) Use Proposition 10.1.3 to compute the number of real roots of
the real polynomials X2 + aX + b or X3 + pX + q in terms of
their discriminants.
2. (J. Wilkinson [35], Section 5.45) Let n = 2p âˆ’1 be an odd number
and Wn âˆˆMn(IR) be the symmetric tridiagonal matrix
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
p
1
1
...
...
...
1
...
...
...
1
1
p
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
.
The diagonal entries are thus p, pâˆ’1, . . . , 2, 1, 2, . . . , pâˆ’1, p, and the
subdiagonal entries are equal to 1.
(a) Show that the linear subspace
Eâ€² = {X âˆˆIRn | xp+j = xpâˆ’j, 1 â‰¤j < p}
is stable under Wn. Similarly, show that the linear subspace
Eâ€²â€² = {X âˆˆIRn | xp+j = âˆ’xpâˆ’j, 0 â‰¤j < p}
is stable under Wn.
(b) Deduce that the spectrum of Wn is the union of the spectra of
the matrices
W â€²
n =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
p
1
1
...
...
...
...
...
1
2
1
2
1
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
(âˆˆMp(IR))

10.6. Exercises
191
and
W â€²â€²
n =
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
p
1
1
...
...
...
...
...
1
3
1
1
2
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
(âˆˆMpâˆ’1(IR)).
(c) Show that the eigenvalues of W â€²â€²
n separate strictly those of W â€²
n.
3. For a1, . . . , an âˆˆIR, with 
j aj = 1, form the matrix
M(a) :=
ï£«
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£¬
ï£­
a1
a2
a3
a4
an
a2
b2
a3
...
...
...
a3
a3
b3
...
...
a4
Â· Â· Â·
...
Â· Â· Â·
Â· Â· Â·
an
an
Â· Â· Â·
Â· Â· Â·
Â· Â· Â·
an
bn
ï£¶
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£·
ï£¸
,
where bj := a1 + Â· Â· Â· + ajâˆ’1 âˆ’(j âˆ’2)aj.
(a) Compute the eigenvalues and the eigenvectors of M(a).
(b) We limit ourselves to n-uplets a that belong to the simplex S
deï¬ned by 0 â‰¤an â‰¤Â· Â· Â· â‰¤a1 and 
j aj = 1. Show that for
a âˆˆS M(a) is bistochastic and b2 âˆ’a2 â‰¤Â· Â· Â· â‰¤bn âˆ’an â‰¤1.
(c) Let Âµ1, . . . , Âµn be an n-uplet of elements in [0, 1] with Âµn = 1.
Show that there exists a unique a in S such that {Âµ1, . . . , Âµn}
is equal to the spectrum of M(a) (counting with multiplicity).
(d) Consider the unit sphere Î£ of Mn(IR), when this space is en-
dowed with the norm âˆ¥Mâˆ¥2 =

Ï(M T M). Show that if P âˆˆÎ£,
then there exists a convex polytope T , of dimension (n âˆ’1)2,
included in Î£ and containing P. Hint: Use Corollary 5.5.1, with
unitary invariance of the norm âˆ¥Â· âˆ¥2.
4. Show that the cost of an iteration of the QR method for a Hermitian
tridiagonal matrix is 20n + O(1).
5. Show that the reduction to the Hessenberg form (in this case,
tridiagonal form) of a Hermitian matrix costs 7n3/6 + O(n2)
operations.
6. (Invariants of the algorithm QR ) For M âˆˆMn(IR) and 1 â‰¤k â‰¤nâˆ’1,
let us denote by (M)k the matrix of size (nâˆ’k)Ã—(nâˆ’k) obtained by
deleting the ï¬rst k rows and the last k columns. For example, (I)1 is
the Jordan matrix J(0; n âˆ’1). We shall denote also by K âˆˆMn(IR)
the matrix deï¬ned by k1n = 1 and kij = 0 otherwise.

192
10. Approximation of Eigenvalues
(a) For an upper triangular matrix T , compute explicitly KT and
TK.
(b) Let M âˆˆMn(IR). Prove the equality
det(M âˆ’Î»I âˆ’ÂµK) = (âˆ’1)nÂµ det(M âˆ’Î»I)1 + det(M âˆ’Î»I).
(c) Let A âˆˆGLn(IR) be given, with factorization A = QR. Prove
that
det(A âˆ’Î»I)1 = det R
rnn
det(Q âˆ’Î»Râˆ’1)1.
(d) Let Aâ€² = RQ. Show that
rnn det(Aâ€² âˆ’Î»I)1 = r11 det(A âˆ’Î»I)1.
(e) Generalize the previous calculation by replacing the index 1 by
k. Deduce that the roots of the polynomial det(A âˆ’Î»I)k are
conserved throughout the QR algorithm. How many such roots
do we have for a general matrix? How many for a Hessenberg
matrix?
7. (Invariants; continuing) For M âˆˆMn(IR), let us deï¬ne PM(h; z) :=
det((1 âˆ’h)M + hM T âˆ’zIn).
(a) Show that PM(h; z) = PM(1 âˆ’h; z). Deduce that there exists a
polynomial QM such that PM(h; z) = QM(h(1 âˆ’h); z).
(b) Show that QM remains constant throughout the QR algorithm:
If Q âˆˆOn(IR), R is upper triangular, and M = QR, N = RQ,
then QM = QN.
(c) Deduce that there exist polynomial functions Jrk on Mn(IR),
deï¬ned by
PM(h; z) =
n

r=0
[r/2]

k=0
(h(1 âˆ’h))kznâˆ’rJrk(M),
that are invariant throughout the QR algorithm. Verify that the
Jr0â€™s can be expressed in terms of invariants that we already
know.
(d) Compute explicitly J21 when n = 2. Deduce that in the case
where Theorem 10.2.1 applies and det A > 0, the matrix Ak
converges.
(e) Show that for n â‰¥2,
J21(M) = âˆ’1
2 Tr

(M âˆ’M T)2
.
Deduce that if Ak converges to a diagonal matrix, then A is
symmetric.

10.6. Exercises
193
8. In the Jacobi method, show that if the eigenvalues are simple, then
the product R1 Â· Â· Â· Rm converges, to an orthogonal matrix R such
that Râˆ—AR is diagonal.
9. Extend the Jacobi method to Hermitian matrices. Hint: Replace the
rotation matrices

cos Î¸
sin Î¸
âˆ’sin Î¸
cos Î¸

by unitary matrices

z1
z2
z3
z4

.
10. Let A âˆˆSymn(IR) be a matrix whose eigenvalues, of course real, are
simple. Apply the Jacobi method, but selecting the angle Î¸k so that
Ï€/4 â‰¤|Î¸k| â‰¤Ï€/2.
(a) Show that Ek tends to zero, that the sequence Dk is relatively
compact, and that its cluster values are diagonal matrices whose
diagonal terms are the eigenvalues of A.
(b) Show that an iteration has the eï¬€ect of permuting, asymp-
totically, a(k)
pp
and a(k)
qq , where (p, q)
=
(pk, qk). In other
words
lim
kâ†’+âˆ|a(k+1)
pp
âˆ’a(k)
qq | = 0,
and vice versa, permuting p and q.
11. The Bernoulli method computes an approximation of the root of
largest modulus for a polynomial a0Xn + Â· Â· Â· + an, when that root
is unique. To do so, one deï¬nes a sequence by a linear induction of
order n:
zk = âˆ’1
a0
(a1zkâˆ’1 + Â· Â· Â· + anzkâˆ’n).
Compare this method with the power method for a suitable matrix.
12. Consider the power method for a matrix M âˆˆMn(CC) of which several
eigenvalues are of modulus Ï(M) Ì¸= 0. Again, CCn = E âŠ•F is the
decomposition of CCn into linear subspaces stable under M, such that
Ï(M|F ) < Ï(M) and Î» âˆˆSp(M|E) =â‡’|Î»| = Ï(M). Finally, x0 =
y0 + z0 with y0 âˆˆE, z0 âˆˆF, and y0 Ì¸= 0.
(a) Express
1
m
mâˆ’1

k=0
log âˆ¥Mxkâˆ¥
in terms of âˆ¥M mx0âˆ¥.

194
10. Approximation of Eigenvalues
(b) Show that if 0 < Âµ < Ï(M) < Î·, then there exist constants C, Câ€²
such that
CÂµk â‰¤âˆ¥M kx0âˆ¥â‰¤Câ€²Î·k,
âˆ€k âˆˆIN.
(c) Deduce that log âˆ¥Mxkâˆ¥converges in the mean to log Ï(M).
13. Let M âˆˆMn(CC) be given. Assume that the Gershgorin disk Dl is
disjoint from the other disks Dm, m Ì¸= l. Show that the inverse power
method, applied to M âˆ’mllIn, provides an approximate computation
of the unique eigenvalue of M that belongs to Dl.

References
[1] Jacques Baranger. Analyse numÂ´erique. Hermann, Paris, 1991.
[2] G. R. Belitskii and Yurii. I. Lyubich. Matrix norms and their applications,
volume 36 of Operator theory : advances and applications. Birkhauser, BË†ale,
1988.
[3] M. Berger and B. Gostiaux. Diï¬€erential geometry : manifold, curves and
surfaces, volume 115 of Graduate text in Mathematics. Springer-Verlag, New
York, 1988.
[4] Rajendra Bhatia. Matrix Analysis, volume 169 of Graduate text in Mathe-
matics. Springer-Verlag, Heidelberg, 1996.
[5] Rajendra Bhatia. Pinching, trimming, truncating, and averaging of matrices.
Amer. Math. Monthly, 107(7):602â€“608, 2000.
[6] Rajendra Bhatia. Linear algebra to quantum cohomology : the story of
Alfred Hornâ€™s inequalities. Amer. Math. Monthly, 108(4):289â€“318, 2001.
[7] Peter BÂ¨urgisser, Michael Clausen, and M. Amin Shokrollahi. Algebraic
complexity theory. Springer-Verlag, Berlin, 1997. With the collaboration of
Thomas Lickteig.
[8] Philippe Ciarlet. Introduction to numerical linear algebra and optimisa-
tion. Cambridge texts in Applied Mathematics. Cambridge University Press,
Cambridge, 1989.
[9] Philippe Ciarlet and Jean-Marie Thomas. Exercices dâ€™analyse numÂ´erique
matricielle et dâ€™optimisation. MathÂ´ematiques appliquÂ´ees pour la maË†Ä±trise.
Masson, Paris, 1982.
[10] Harvey Cohn. Advanced number theory. Dover Publications Inc., New York,
1980. Reprint of A second course in number theory, 1962, Dover Books on
Advanced Mathematics.

196
References
[11] Don Coppersmith and Shmuel Winograd. Matrix multiplication via arith-
metic progressions. J. Symbolic Comput., 9(3):251â€“280, 1990.
[12] J. Davis, Philip. Circulant matrices. Chelsea Publishing, New York, 1979.
[13] M. Fiedler and V. PtÂ´ak. On matrices with non-positive oï¬€-diagonal elements
and positive principal minors. Czech. Math. Journal, 12:382â€“400, 1962.
[14] Edward Formanek. Polynomial identities and the Cayley-Hamilton theorem.
Math. Intelligencer, 11(1):37â€“39, 1989.
[15] Edward Formanek. The polynomial identities and invariants of n Ã— n ma-
trices. Number 78 in CBMS Regional Conf. Ser. Math. Amer. Math. Soc.,
Providence, RI, 1991.
[16] William Fulton. Eigenvalues, invariant factors, highest weights, and Schubert
calculus. Bull. Amer. Math. Soc. (N.S.), 37(3):209â€“249 (electronic), 2000.
[17] F. R. Gantmacher. The theory of matrices. Vol. 1. Chelsea Publish. Co.,
New York, 1959.
[18] F. R. Gantmacher. The theory of matrices. Vol. 2. Chelsea Publish. Co.,
New York, 1959.
[19] Gene H. Golub and Charles F. Van Loan. Matrix computations, volume 3
of Series in the mathematical sciences. John Hopkins University Press,
Baltimore, 1983.
[20] Nicholas Higham. Accuracy and stability of numerical algorithms. SIAM,
Philadelphia, PA, 1996.
[21] Roger A. Horn and Charles R. Johnson. Matrix analysis. Cambridge
University Press, Cambridge, 1985.
[22] Alston S. Householder. The theory of matrices in numerical analysis. Dover,
New York, 1975.
[23] Nicholas M. Katz and Peter Sarnak. Random matrices, Frobenius eigenval-
ues and monodromy. Number 45 in Colloquium publ. Amer. Math. Soc.,
Providence, RI, 1999.
[24] Anthony W. Knapp. Representation of semisimple groups. An overview based
on examples. Princeton Mathematical Series. Princeton University Press,
Princeton, NJ, 1986.
[25] P. Lascaux and R. ThÂ´eodor. Analyse numÂ´erique matricielle appliquÂ´ee `a lâ€™art
de lâ€™ingÂ´enieur. Masson, Paris, 1987.
[26] Chi-Wang Li and Roy Mathias. Extremal characterization of the Schur
complement and resulting inequalities. SIAM Review, 42:233â€“246, 2000.
[27] Helmut LÂ¨utkepohl. Handbook of matrices. J. Wiley & Sons, New York, 1996.
[28] MneimnÂ´e, Rached and Testard, FrÂ´edÂ´eric. Introduction `a la thÂ´eorie des
groupes de Lie classiques. Hermann, Paris, 1986.
[29] Walter Rudin. Real and complex analysis. McGraw-Hill Book co, NY, third
edition, 1987.
[30] Walter Rudin. Functional analysis. McGraw-Hill Book Co, NY, second
edition, 1991.
[31] E. Seneta. Non-negative matrices and Markov chains. Springer series in
statistics. Springer-Verlag, New York-Berlin, 1981.

References
197
[32] Joseph Stoer and Christoph Witzgall. Transformations by diagonal matrices
in a normed space. Numer. Math., 4:158â€“171, 1962.
[33] Volker Strassen. Gaussian elimination is not optimal. Numer. Math., 13:354â€“
356, 1969.
[34] J. H. M. Wedderburn. Lectures on matrices, volume XVII of Colloquium
publications. American Math. Society, New York, 1934.
[35] J. H. Wilkinson. The algebraic eigenvalue problem. Oxford Science Publica-
tions, Oxford, 1965.

This page intentionally left blank 

Index
QR
factorization, 143
method, 173
algebra
Banach, 70
Lie, 134
algebraically closed ï¬eld, 4
Abel
theorem, 110, 168
algebra
normed, 70
alternate
form, 12
matrix, 11
Amitsur & Levitzki
theorem, 38
basis, 2
Campbellâ€“Hausdorï¬€
formula, 134
canonical form, 107, 110
Cauchyâ€“Binet
formula, 18
Cayleyâ€“Hamilton
theorem, 26
characteristic
of a ï¬eld, 1
characteristic polynomial, 24
Choleski
factorization, 142
cofactor, 17
commutator, 6
condition number, 162, 169
conjugate
exponents, 62
gradient, 159
matrices, 9
convergence rate, 164
convergence ratio, 151
convergent
method, 150
Cotlar
lemma, 77
determinant, 16
diagonalizable
orthogonally, 48
diagonally
dominant, 73
strictly dominant, 73
strongly dominant, 73

200
Index
domain
Euclidean, 99
principal ideal, 97
eigenbasis, 28
eigenspace, 28
eigenvalue, 24
multiplicity
algebraic, 25
geometric, 25
semi-simple, 27
simple, 25
eigenvector, 24
elementary divisor, 109
equivalent
matrices, 8
norms, 63
exponential, 116
extremal point, 79
extreme point, 88
form
Hermitian, 41
sesquilinear, 41
Frobenius
norm, 182
Gauss
method, 149
Gaussâ€“Seidel
method, 152
gcd, 98
Gershgorin
disk, 71, 78
domain, 71
Greville
algorithm, 148
group
linear, 20
modular, 56
orthochronous Lorentz, 127
orthogonal, 20, 120
special, 20
special linear, 20
special orthogonal, 123
symmetric, 15
symplectic, 120
topological, 135
unitary, 120
Hessenberg, 169
Householder
matrix, 171
method, 175
theorem, 67
ideal, 97
principal, 97
inequality
Cauchyâ€“Schwarz, 63
HÂ¨older, 62
Minkowski, 61
integral domain, 15
invariant factor, 102
inverse
generalized, 145
left, 145
right, 145
irreducibility, 30
Jacobi
identity, 134
method, 151, 181
Jordan
block, 110
decomposition, 111
kernel, 7
Lp(â„¦), 68
matrices
commuting, 6
conjugate, 9
equivalent, 8
product of, 6
similar, 9
matrix
Householder, 171
Jordan, 110
Pascalâ€™s, 130
adjoint, 17
alternate, 11
companion, 37, 106
cyclic, 85
diagonal, 5
block-, 10
diagonalizable, 28
elementary, 100

Index
201
Hermitian, 40
positive deï¬nite, 42
Hermitian adjoint, 40
Hessenberg, 169
idempotent, 6
identity, 5
inverse, 20
invertible, 20
nilpotent, 6
nonnegative, 80
nonsingular, 20
normal, 40
orthogonal, 10
orthostochastic, 89
permutation, 5
projection, 32
regular, 20
singular, 20
skew-Hermitian, 40
skew-symmetric, 10
square, 5
stochastic, 87
bi-, 87
symmetric, 10
positive deï¬nite, 42
totally positive, 35
transposed, 10
triangular, 5
block-, 10
strictly, 5
tridiagonal, 155
unitary, 41
method
QR , 173
power
inverse, 188
conjugate gradient, 159
Gaussâ€“Seidel, 152
Jacobi, 151, 181
Leverrier, 188
power, 185
relaxation, 152
minimal polynomial, 27
minor, 17
leading principal, 17
principal, 17, 137
Mooreâ€“Penrose
inverse, 145
norm
lp, 61
algebra, 65, 70
Frobenius, 182
induced, 65
matrix, 65
Schur, 131
Schurâ€™s, 59, 182
subordinated, 65
norms
equivalent, 63
orthogonal
group, 20, 120
subspace, 11
orthogonally
diagonalizable, 48
Perronâ€“Frobenius
theorem, 81, 82
Pfaï¬ƒan, 22
polar decomposition, 115
polynomial
invariant, 104
standard, 38
preconditioning, 165
product
Hadamard, 59
of matrices, 6
scalar, 11
projector, 32
range, 7, 8
rank, 5
decomposition, 104
Rayleigh
ratio, 48
translation, 180
reductibility, 30
relaxation
method, 152
residue, 160
Rieszthorin
theorem, 68
ring
factorial, 99
Noetherian, 98
principal ideal domain, 97

202
Index
Schur
complement, 50, 139
lemma, 33
norm, 182
theorem, 45
signature, 48
similar
matrices, 9
unitarily, 45
similarity invariant, 25, 104
singular value, 75, 128
spectral radius, 61
spectrum, 24
square root, 115
Strassen
algorithm, 142
Sturm
sequence, 173
Sylvester index, 48
symmetric group, 15
symplectic
group, 120
trace, 25
unitarily
similar, 45
unitary
diagonalization, 46
group, 120
trigonalization, 45
Vandermonde
matrix, 35
vector
column, 5
nonnegative, 80
positive, 80
row, 5
vector space, 2


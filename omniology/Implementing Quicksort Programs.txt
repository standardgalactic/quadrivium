Programming 
Techniques 
S. L. Graham, R. L. Rivest 
Editors 
Implementing 
Quicksort Programs 
Robert Sedgewick 
Brown University 
This paper is a practical study of how to implement 
the Quicksort sorting algorithm and its best variants on 
real computers, including how to apply various code 
optimization techniques. A detailed implementation 
combining the most effective improvements to 
Quicksort is given, along with a discussion of how to 
implement it in assembly language. Analytic results 
describing the performance of the programs are 
summarized. A variety of special situations are 
considered from a practical standpoint to illustrate 
Quicksort's wide applicability as an internal sorting 
method which requires negligible extra storage. 
Key Words and Phrases: Quicksort, analysis of 
algorithms, code optimization, sorting 
CR Categories: 4.0, 4.6, 5.25, 5.31, 5.5 
Introduction 
One of the most widely studied practical problems in 
computer science is sorting: the use of a computer to put 
files in order. A person wishing to use a computer to sort 
is faced with the problem of determining which of the 
many available algorithms is best suited for his purpose. 
This task is becoming less difficult than it once was for 
three reasons. First, sorting is an area in which the 
mathematical analysis of algorithms has been particu- 
larly successful: we can predict the performance of many 
sorting methods and compare them intelligently. Second, 
we have a great deal of experience using sorting algo- 
Permission to copy without fee all or part of this material is 
granted provided that the copies are not made or distributed for direct 
commercial advantage, the ACM copyright notice and the title of the 
publication and its date appear, and notice is given that copying is by 
permission of the Association for Computing Machinery. To copy 
otherwise, or to republish, requires a fee and/or specific permission. 
This work was supported in part by the Fannie and John Hertz 
Foundation and in part by NSF Grants. No. GJ-28074 and MCS75- 
23738. 
Author's address: Division of Applied Mathematics and Computer 
Science Program, Brown University, Providence, RI 02912. 
Â© 1978 ACM 0001-0782/78/1000-0847 $00.75 
847 
rithms, and we can learn from that experience to separate 
good algorithms from bad ones. Third, if the tile fits into 
the memory of the computer, there is one algorithm, 
called Quicksort, which has been shown to perform well 
in a variety of situations. Not only is this algorithm 
simpler than many other sorting algorithms, but empir- 
ical [2, ll, 13, 21] and analytic [9] studies show that 
Quicksort can be expected to be up to twice as fast as its 
nearest competitors. The method is simple enough to be 
learned by programmers who have no previous experi- 
ence with sorting, and those who do know other sorting 
methods should also find it profitable to learn about 
Quicksort. 
Because of its prominence, it is appropriate to study 
how Quicksort might be improved. This subject has 
received considerable attention (see, for example, [1, 4, 
11, 13, 14, 18, 20]), but few real improvements have been 
suggested beyond those described by C.A.R. Hoare, the 
inventor of Quicksort, in his original papers [5, 6]. Hoare 
also showed how to analyze Quicksort and predict its 
running time. The analysis has since been extended to 
the improvements that he suggested, and used to indicate 
how they may best be implemented [9, 15, 17]. The 
subject of the careful implementation of Quicksort has 
not been studied as widely as global improvements to 
the algorithm, but the savings to be realized are as 
significant. The history of Quicksort is quite complex, 
and [15] contains a full survey of the many variants 
which, have been proposed. 
The purpose of this paper is to describe in detail how 
Quicksort can best be implemented to handle actual 
applications on real computers. A general description of 
the algorithm is followed by descriptions of the most 
effective improvements that have been proposed (as 
demonstrated in [15]). Next, an implementation of 
Quicksort in a typical high level language is presented, 
and assembly language implementation issues are con- 
sidered. This discussion should easily translate to real 
languages on real machines. Finally, a number of special 
issues are considered which may be of importance in 
particular sorting applications. 
This paper is intended to be a self-contained overview 
of the properties of Quicksort for use by those who need 
to actually implement and use the algorithm. A compan- 
ion paper [17] provides the analytical results which su- 
port much of the discussion presented here. 
The Algofithm 
Quicksort is a recursive method for sorting an array 
A[1], A[2] ..... A[N] by first "partitioning" it so that the 
following conditions hold: 
(i) Some key v is in its final position in the array. (If it 
is thejth smallest, it is in position A[j].) 
(ii) All elements to the left of A[j] are less than or equal 
to it. (These elements A [ 1 ], A [2] ..... A [j - 1 ] are 
called the "left subtile.") 
Communications 
October 1978 
of 
Volume 21 
the ACM 
Number 10 

(iii) All elements to the right of A[j] are greater than or 
equal to it. (These elements A [j + 1 ] ..... A IN] are 
called the "right subtile."] 
After partitioning, the original problem of sorting the 
entire array is reduced to the problem of sorting the left 
and right subfiles independently. The following program 
is a recursive implementation of this method, with the 
partitioning process spelled out explicitly. 
Program 1 
procedure quicksort (integer value/, r); 
comment Sort All : r] where A[r + 1] _> A[/] ..... Air]; 
if r > I then 
i:=/;j:= 
r+ 1; v := A[/]; 
loop: 
loop: i := i + 1; while A[i] < v repeat; 
Ioop:j :=j - 1; while A[]] > v repeat; 
until j < i: 
A[i] :=: A[JI; 
repeat; 
Alt] :=: A[/3; 
quicksort(l, j- 1); 
quicksort(i, r); 
endif; 
(This program uses an exchange (or swap) operator :=:, 
and the control constructs loop ... repeat and if ... endif, 
which are like those described by D.E. Knuth in [10]. 
Statements between loop and repeat are iterated: when 
the while condition fails (or the until condition is satis- 
fied) the loop is exited immediately. The keyword repeat 
may be thought of as meaning "execute the code starting 
at loop again," and, for example, "until j < i" may be 
read as "ifj < i then leave the loop".) 
The partitioning process may be most easily under- 
stood by first assuming that the keys A [ 1 ] ..... A [N] are 
distinct. The program starts by taking the leftmost ele- 
ment as the partitioning element. Then the rest of the 
array is divided by scanning from the left to fred an 
element > v, scanning from the fight to find an element 
< v, exchanging them, and continuing the process until 
the pointers cross. The loop terminates withj + 1 = i, at 
which point it is known that A[l + 1] ..... A[j] are < v 
and A[j + 1] ..... A[r] are > v, so that the exchange A[l] 
.=: A[j] completes the job of partitioning A[l] ..... Air]. 
The condition that Air + 1] must be greater than or 
equal to all of the keys All] ..... A[r] is included to stop 
the i pointer in the case that v is the largest of the keys. 
The procedure call quicksort (1, N) will therefore sort 
A[I] ..... A[N] ifA[N + 1] is initialized to some value 
at least as large as the other keys. (This is normally 
specified by the notation A[N + 1] := oo.) 
If equal keys are present among A [ 1 ], ..., A [N], then 
Program 1 still operates properly and efficiently, but not 
exactly as described above. If some key equal to v is 
already in position in the file, then the pointer scans 
could both stop with i = j, so that, after one more time 
through the loop, it terminates with j + 2 = i. But at this 
point it is known not only that A[I + 1] ..... A[j] are _ 
v and A[j + 2] ..... A[r] are _ v but also that A[j + 1] 
848 
= v. After the exchange A[I] ~: A[j], we have two 
elements in their final place in the array (A [j] and A [j 
+ 1]), and the subfiles are recursively sorted. 
Figures 1 and 2 show the operation of Program 1 on 
the first 16 digits of ~r. In Figure 1, elements marked by 
arrows are those pointed to by i and j, and each line is 
the result of a pointer increment or an exchange. In 
Figure 2, each line is the result of one "partitioning 
stage," and boldface elements are those put into position 
by partitioning. 
The differences between the implementation of par- 
titioning given in Program 1 and the many other parti- 
tioning methods which have been proposed are subtle, 
but they can have a significant effect on the performance 
of Quicksort. The issues involved are treated fully in 
[15]. By using this particular method, we have already 
begun to "optimize" Quicksort, for it has three main 
advantages over alternative methods. 
First, as we shall see in much more detail later, the 
inner loops are efficiently coded. Most of the running 
time of the program is spent executing the statements 
loop: i := i + 1; while A[Q < v repeat; 
loop: j ~j - 1; while A[J1 > v repeat; 
each of which can be implemented in machine language 
with a pointer increment, a compare, and a conditional 
branch. More naive implementations of partitioning in- 
clude other tests, for the pointers crossing or exceeding 
the array bounds, within these loops. For example, rather 
than using the "sentinel" A[N + 1] = ~ we could use 
loop: i ~i + 1; while i _< N and A[i] < v repeat; 
for the i pointer increment, but this would be far less 
efficient. 
Second, when equal keys are present, there is the 
question of how keys equal to the partitioning element 
should be treated. It might seem better to scan over such 
keys (by using the conditions A [i] _< v and A [j] _> v in 
the scanning loops), but careful analysis shows that it is 
always better to stop the scanning pointers on keys equal 
to the partitioning element, as in Program 1. (This idea 
was suggested in 1969 by R.C. Singleton [18].) In this 
paper, we will adopt this strategy for all of our programs, 
but in the analysis we will assume that all of the keys 
being sorted are distinct. Justification for doing so may 
be found in [16], where the subject of Quicksort with 
equal keys is studied in considerable detail. 
Third, the partitioning method used in Program 1 
does not impose a bias upon the subfiles. That is, if we 
start with a random arrangement of A[l] ..... A[N], then, 
after partitioning, the left subtile is a random arrange- 
ment of its elements and the right subtile is a random 
permutation of its elements. This fact is crucial to the 
analysis of the program, and it also seems to be a 
requirement for efficient operation. It is conceivable that 
a method could be devised which imparts a favorable 
bias to the subfiles, but the creation of nonrandom 
subfiles is usually done inadvertently. No method which 
Communications 
October 1978 
of 
Volume 21 
the ACM 
Number 10 

Fig. 1. Partitioning ~r (Program 1). 
3 
1 
4 
1 
5 
9 
2 
6 
5 
1 
4 
3 
1 
3 
1 
5 
3 
1 
3 
1 
3 
9 
5 
Â¢e-- 
6 
2 
Â¢e-- 
3 
1 
3 
1 
3 
2 
9 
6 
5 
9 
2 
--' 
2 
1 
3 
1 
3 
3 
9 
6 
5 
5 
3 
5 
5 
5 
8 
9 
7 
9 
3 
8 
7 
<__ 
9 
<..- 
9 
Â¢e-- 
8 
9 
7 
9 
4 
5 
5 
8 
9 
7 
9 
4 
5 
5 
8 
9 
7 
9 
4 
3 
Â¢e-- 
4 
Fig. 2. Quicksorting ~r (Program l). 
3 
1 
4 
1 
5 
9 
2 
6 
5 
2 
1 
3 
1 
3 
3 
9 
6 
5 
1 
1 
2 
3 
3 
1 
1 
3 
3 
3 
5 
8 
9 
7 
9 
3 
5 
5 
8 
9 
7 
9 
4 
7 
6 
5 
5 
5 
4 
6 
5 
5 
5 
4 
6 
5 
5 
5 
5 
5 
5 
6 
5 
5 
5 
5 
8 
4 
9 
9 
9 
7 
8 
8 
9 
1 
1 
2 
3 
3 
3 
4 
5 
5 
5 
6 
7 
8 
9 
9 
9 
produces nonrandom subfiles has yet been successfully 
analyzed, but empirical tests show that such methods 
slow down Quicksort by up to 20 percent (see [10, 15]). 
Improvements 
Program 1 is, then, an easily understandable descrip- 
tion of an efficient sorting algorithm. It can be a perfectly 
acceptable sorting program in many practical situations. 
However, if efficiency is a primary concern, there are a 
number of ways in which the program can be improved. 
This will be the subject of the remainder of this paper. 
Each improvement requires some effort to implement, 
which it rewards with a corresponding increase in effi- 
ciency. To illustrate the effectiveness of the various 
modifications, we shall make use of the analytic results 
849 
given in [17], where exact formulas are derived for the 
total average running time of realistic implementations 
of different versions of Quicksort on a typical computer. 
Removing Recursion 
The most serious problem with Program 1 is that it 
can consume unacceptable amounts of space in the 
implicit stack needed for the recursion. For example, if 
the file A[1] ..... A[N] is already in order, then the 
program will invoke itself to recursive depth N, and it 
will thus implicitly require extra storage proportional to 
N. Hoare pointed out that this is easily corrected by 
changing the recursive calls so that the shorter of the two 
subfiles is sorted first. The recursive depth is then limited 
to log2N [6]. Care must be exercised in implementing 
this change, since many compilers will not recognize that 
the second recursive call is not really recursive. When- 
ever a procedure ends with a call on another procedure, 
the stack space used for the first call may be reclaimed 
before the second call is made (see [10]). Rather than 
expose ourselves to the whims of compilers we will 
remove the recursion and use an explicit stack. This will 
also eliminate some overhead, and it is a straightforward 
transformation on Program 1. 
When implemented in assembly language with re- 
cursion removed in this way, the expected running time 
of Program 1 is shown in [17] to be about 11.6667N In N 
+ 12.312N time units. The "time unit" used is the time 
required for one memory reference (i.e. count one for 
each instruction, plus one more if the instruction refer- 
ences data in memory). The model is similar to Knuth's 
MIX [7]--we shall see it in more detail below when we 
examine assembly language implementation. The for- 
mulas derived in [17] are exact, but rather complicated: 
the simple formula above is accurate to within 0.1 percent 
for N > 1000, 1 percent for N > 100, and 2 percent for 
N > 20. Similar formulas with this accuracy are derived 
in [17] for all the improvements described below, and 
these are quite sufficient for comparing the methods and 
predicting their performance. 
Small Subfiles 
Another major difficulty with Program 1 is that it 
simply is not very efficient for small subfiles. This is 
especially unfortunate because the recursive nature of 
the program guarantees that it will always be used for 
many small subfiles. Therefore Hoare suggested that a 
more efficient method be used when r - I is small [6]. A 
method which is known to be very efficient for small 
files is insertion sorting. This is the method of scanning 
through the file and inserting each element into place 
among those previously considered, by successively mov- 
ing smaller elements up to make room. It may be imple- 
mented as follows: 
procedure insertionsort(l, r); 
comment Sort A [l : r] where A [r + 1 ] > A [1 ] ..... 
A [r]; 
loop forr- 
l>--i>--l: 
Communications 
October 1978 
of 
Volume 21 
the ACM 
Number 10 

ifA[i] > A[i + 1] then 
v .--- A[i];j := i + 1; 
loop: A[j - 1] ~ A[j];j :=j + 1; while A[j] < v repeat; 
A[j- 
1] .'= v; 
endif; 
(Just as there are many different implementations of 
Quicksort, so there are a variety of ways to implement 
Insertionsort. This subject is treated in detail in [9] and 
[15].) Now, the obvious way to improve Program 1 is to 
change the first if statement to 
if r - l --< M then insertionsort(l, r) else ... 
where M is some threshold value above which Quicksort 
is faster than Insertionsort. 
It is shown in [15] that there is an even better way to 
proceed. Suppose that small subfiles are simply ignored 
during partitioning, e.g. by changing the first if statement 
in Program 1 to "if r - l > M then .... " Then, after the 
entire tile has been partitioned, it has all the elements 
which were used as partitioning elements in place, with 
unsorted subtiles of length M or less between them. A 
single Insertionsort of the entire file will quite efficiently 
complete the job of sorting the file. 
Analysis shows that it takes Insertionsort only slightly 
longer to sort the whole tile than it would to sort all of 
the subtiles, but all of the overhead of invoking Inser- 
tionsort during partitioning is eliminated. For example, 
subfiles with M or fewer elements never need be put on 
the stack, since they are ignored during partitioning. It 
3 
turns out that this eliminates z of the stack pushes used, 
on the average. This makes the method preferable to the 
scheme of sorting the small subtiles during partitioning 
(even in an "optimal" manner). 
For most implementations, the best value of M is 
about 9 or 10, though the exact value is not highly 
critical: Any value between 6 and 15 would do about as 
well. Figure 3 shows the total running time on the 
machine in [17] for N = 10,000 for various values of M. 
The best value is M = 9, and the total running time for 
this value is about 11.6667N In N - 1.743N time units. 
Figure 4 is a graph of the function 14.055N/(11.6667N 
In N + 12.312N), which shows the percentage improve- 
ment for this optimum choice M = 9 over the naive 
choice M = 1 (Program 1). 
Worst Case 
A third main flaw of Program 1 is that there are some 
files which are likely to occur in practice for which it 
will perform badly. For example, suppose that the num- 
bers A[1], A[2] ..... A[N] are in order already when 
Program 2 is invoked. Then A [ 1 ] will be the first parti- 
tioning element, and the first partition will produce an 
empty left subtile and a right subtile consisting of A[2], 
.... A[N]. Then the same thing will happen to that 
subtile, and so on. The program has to deal with files of 
size N, N-l, N-2 .... and its total running time is obviously 
proportional to N 2. The same problem arises with a tile 
in reverse order. This O(N 2) worst case is inherent in 
850 
Fig. 3. Total running time of Quick sort for N = 10,000. 
1,3~,000 
1,2~,000 
1.1~,00~ 
I,O00,(X)O 
Cutoff for small subfiles (M). 
Fig. 4. Improvement due to sorting small subfiles on a separate pass. 
25 
20 
_E 
5 
<_ 
--1000 
2060 
3000 4000 5000 6000 
7000 8060 9000 10000 
File Size ~N) 
Quicksort: it is especially unfortunate if it occurs on files 
so likely to occur in practice. 
There are many ways to make such anomalies very 
unlikely in practical situations. Rather than using the 
first element in the file as the partitioning element, we 
might try to use some other fLxed element, like the middle 
element. This helps some, but simple anomalies still can 
occur. Hoare suggested a method which does work: 
choose a random element as the partitioning element 
[6]. As remarked above, care must be taken when imple- 
menting these simple changes to ensure that the parti- 
tioning method still produces random subfiles. The safest 
method, ifA[p] is to be used as the partitioning element 
(where, for example, p is computed to be a pseudoran- 
dom number between l and r), is to simply precede the 
statement v .--- A[I] by the statement A[p] .=: A[I] in 
Program 1. 
Using a random partitioning element will virtually 
ensure that anomalous cases for Program 2 will not occur 
in practical sorting situations, but it has the disadvantage 
that random number generation can be relatively expen- 
sive. We are probably being overcautious to slow down 
the program for all tiles, just to avoid a few anomalies. 
The next method that we will examine actually improves 
the average performance of the program while at the 
same time making the worst case unlikely to occur in 
practice. 
Communications 
October 1978 
of 
Volume 21 
the ACM 
Number 10 

Median-of-Three Modification 
The method is based on the observation that Quick- 
sort performs best when the partitioning element turns 
out to be near the center of the file. Therefore choosing 
a good partitioning element is akin to estimating the 
median of the file. The statistically sound method for 
doing this is to choose a sample from the file, find the 
median, and use that value as the estimate for the median 
of the whole file. This idea was suggested by Hoare in 
his original paper, but he didn't pursue it because he 
found it "very difficult to estimate the saving." It turns 
out that most of the savings to be had from this idea 
come when samples of size three are used at each parti- 
tioning stage. Larger sample sizes give better estimates 
of the median, of course, but they do not impro~,e the 
running time significantly. Primarily, sampling provides 
insurance that the partitioning elements don't consist- 
ently fall near the ends of the sub files, and three elements 
are sufficient for this purpose. (See [15] and [17] for 
analytic results confirming these conclusions.) The av- 
erage performance would be improved if we used any 
three elements for the sample, but to make the worst case 
unlikely we shall use the first, middle, and last elements 
as the sample, and the median of those three as the 
partitioning element. The use of these three particular 
elements was suggested by Singleton in 1969 [18]. Again, 
care must be taken not to disturb the partitioning process. 
The method can be implemented by inserting the state- 
ments 
A[(I + r) + 2] ~: A[I + 1]; 
if All + 1] >A[r] thenA[/+ 
1] .----: A[r] endif; 
ifA[l] > A[r] then A[I] ~: A[r] endif; 
if All + 1] > A [1] then All + 1] ~: A[/] endif; 
before partitioning (after "if r > l then" in Program 1. 
This change makes A [1] the median of the three elements 
originally at A[I], A[(I + r) + 2], and A[r] before 
partitioning. Furthermore, it makes A[I + 1] _< A[I] and 
A [r] _> A [l ], so the pointer initializations can be changed 
to "i .--- l + 1;j .--- r". This method preserves randomness 
in the subfiles. 
Median-of-three partitioning reduces the number of 
comparisons by about 14 percent, but it increases the 
number of exchanges slightly and requires the added 
overhead of finding the median at each stage. The total 
expected running time for the machine in [17] (with the 
optimum value M - 9) is about 10.6286N In N + 2.116N 
time units, and Figure 5 shows the percentage savings. 
Implementation 
Combining all of the improvements described above, 
we have Program 2, which has no recursion, which 
ignores small subfiles during partitioning, and which 
partitions according to the median-of-three modification. 
For clarity, the details of stack manipulation and selectÂ° 
ing the smaller of the two subfiles are omitted. Also, 
851 
Fig. 5. Improvement due to median-of-three partitioning. 
25 
20 
J5 
E 
5 
f 
3000 
4000 5000 6000 7000 8000 
Fiie Size (N) 
90o0 t0ood 
since recursion is no longer involved, we will deal with 
an in-line program to sort All] ..... A[N]. 
Program 2 
integer l, r, i, j; 
integer array stack[l : 2 XflN)]; 
boolean done; 
arbmode array A[I : N + 1]; 
arbmode v; 
l.---- l; r .--- N; done ~ N <- M; 
loop until done: 
A[(I + r) + 2] ~: A[I + 1]; 
if Air + 1] > A[r] then A[l + 1] .---: A[r] endif; 
if All] > A[r] then A[I] ~: A[r] endif; 
if A[/+ 1] > A[I] then A[I + 1] .---: A[I] endif; 
i.---- l+ l;jm r; v .--- A[I]; 
loop: 
loop: i ~ i + 1; while A[i] < v repeat; 
loop: j .---j - 1; while A[j] > v repeat; 
until j < i: 
A[i] ~: A[J]; 
repeat; 
All] :=: Aft]; 
if max(./" - l, r - i + 1) --< M 
then if stack empty 
then done .--- true 
else (1, r) .~ popstack 
endif; 
else ifmin(j-l,r-i+ 
l)<_M 
then (1, r) := large subtile; 
else pushstack (large subtile); 
(1, r) := small subtile 
endif; 
endif; 
repeat; 
A[N + 1] .--- oo; 
loop for N- 
1 >_i-- 
> 1: 
ifA[i] >A[i + 1] then 
v .= A[i];j :--- i + 1; 
loop: A[j - 1] ~ A[j]; j ~ j + l; while A[j] < v repeat; 
Ab- 1] .-- v; 
endif; 
repeat; 
In the logic for manipulating the stack after parti- 
tioning, (/, j - 1) is the "large subtile" and (i, r) is the 
"small subtile" if max (j -/, r - i + 1) =j -/, and vice 
versa if r - i + 1 > j - l. This may be implemented 
Communications 
October 1978 
of 
Volume 21 
the ACM 
Number 10 

simply and efficiently by making one copy of the code 
for each of the two outcomes of comparing j - l with r 
-i+1. 
Note that the condition A[N + 1] = oo is now only 
needed for the insertionsort. This could be eliminated, if 
desired, at only slight loss by changing the conditional in 
the inner loop of Insertionsort to "while A [j'] < v and j 
_< N". 
Left unspecified in Program 2 are the values of M, 
the threshold for small subfiles, andf(N), the maximum 
stack depth. These are implementation parameters which 
should be specified as constants at compile time. As 
mentioned above, the best value of M for most imple- 
mentations is 9 or 10, although any value from 6 to 15 
will do nearly as well. (Of course, we must have M ___ 2, 
since the partitioning method needs at least three ele- 
ments to find the median of.) The maximum stack depth 
turns out to be always less than logz (N + 1)/(M + 2) so 
(for M = 9) a stack withf(N) = 20 will handle files of up 
to about ten million elements. (See the analysis in [11, 
15, 171.) 
Figure 6 diagrams the operation of Program 2 upon 
the digits of ~r. Note that after partitioning all that is left 
for the insertionsort is the subtile 5 5 5 4, and the 
insertion sort simply scans over the other keys. 
The total average running time of a program is 
determined by first finding analytically the average fre- 
quency of execution of each of the instructions, then 
multiplying by the time per instruction and summing 
over all instructions. It turns out that the total expected 
running time of Program 2 can be determined from the 
six quantities: 
As 
the number of partitioning stages, 
Bu 
the number of exchanges during partitioning, 
CN the number of comparisons during partitioning, 
SN the number of stack pushes (and pops), 
DN 
the number of insertions, and 
E2v the number of keys moved during insertion. 
In Program 2, CN is the number of times i .--- i + 1 is 
executed plus the number of times j .'= j + 1 is executed 
within the scanning loops; BN is the number of times 
A[i] ".= A[j] is executed in the partitioning loop; AN is 
the number of times the main loop is iterated; DN is the 
number of times v is changed in the insertionsort; and 
EN is the number of times A[j - 1] .--- A[j] is executed 
Fig. 6. Quicksorting ~r--improved method (Program 2, M 
Quicksort: 
3
1
4
1
5
9
2
6
5
3
5
8
9
 
2
3
3
1
1
3
9
5
5
4
5
8
9
 
1 1 2 3 3  
5 5 5 4 6 8 9  
78 
Insertion- 
1
1
2
3
3
3
5
5
5
4
6
7
8
 
so~: 
4 6 7 8  
45 
1
1
2
3
3
3
4
5
5
 
1
1
2
3
3
3
4
5
5
5
6
7
8
 
852 
= 4). 
7 
9 
3 
7 
9 
6 
7 
9 
9 
999 
999 
999 
9 
9 
9 
in the insertionsort. Each instruction in an assembly 
language implementation can be labeled with its fre- 
quency in terms of these quantities and N. (There m~ty 
be a few other quantities involved: if they do not relate 
simply to the main quantities or cancel out when the 
total running time is computed, then they generally can 
be analyzed in the same way as the other quantities 
[17].) The analysis in [17] yields exact values for these 
quantities, from which the total running time can be 
computed and the best value of M chosen. For M = 9 it 
turns out that 
CN --~ 1.714N In N - 3.74N, 
B2v -~ .343N In N - .84N 
E2v = 1.14N, 
DN "~ .60N, 
AN M.16N, 
SN "" .05N. 
From these equations, the total running time of any 
particular implementation of Program 2 (with M = 9) 
can easily be estimated. For the model in [9, 15, 17], the 
total expected running time is 53Â½AN + 11B2v + 4CN + 
3DN + 8E2v + 9SN + 7N, which leads to the equation 
10.6286N In N + 2.116N given above. 
Assembly Language 
Program 2 is an extremely efficient sorting method, 
but it will not run efficiently on any particular computer 
unless it is translated into an efficient program in that 
computer's machine language. If large tiles are to be 
sorted or if the program is to be used often, this task 
should not be entrusted to any compiler. We shall now 
turn from methods of improving the algorithm to meth- 
ods of coding the program for a machine. 
Of most interest is the "inner loop" of the program, 
those statements whose execution frequencies are pro- 
portional to N In N. We shall therefore concern ourselves 
with the translation of the statements 
loop: 
loop: i ~ i + 1; while A[i] < v repeat; 
Ioop:j .'=-j - 1; while A[j] > v repeat; 
untilj < i: 
A[i] ~: A []']; 
repeat; 
Assembly-language implementations of the rest of the 
programs may be found in [9] or [15]. Rather than use 
any particular assembly-language or deal with any par- 
ticular machine, we shall use a mythical set of instruc- 
tions similar to those in Knuth's MIX [7]. Only simple 
machine-language capabilities are used, and the pro- 
grams and results that we shall derive may easily be 
translated to apply to most real machines. 
To begin, a direct translation of the inner loop of 
Programs 1 and 2 is given below. The comments on each 
line explain what the instructions are intended to do. 
The mnemonics I, V, J, X, and Y are symbolic register 
names, and the notation A(I) means the contents of the 
memory location whose address is A plus the contents of 
index register/, or A[i]. Readers unfamiliar with assem- 
bly language programming should consult [7]. 
Communications 
October 1978 
of 
Volume 21 
the ACM 
Number 10 

LOOP INC 
I, 1 
CMP V, A(I) 
JG 
* - 2 
DEC J, 1 
CMP V, A(J) 
JL 
* - 2 
CMP J, I 
JL 
OUT 
LD 
X, A(I) 
LD 
Y, A(J) 
ST 
X, A(J) 
ST 
Y, A(I) 
JMP 
LOOP 
OUT 
Increment register I by l. 
Compare v with A[i]. 
Go back two instructions if v > A[i]. 
Decrement register J by 1. 
Compare v with A [j]. 
Go back two instructions if v < A[j]. 
Compare J with I. 
Leave loop ifj < i. 
Load A[i] into register X. 
Load A[j] into register Y. 
Store register X into A[j]. 
Store register Y into A[0. 
Unconditional jump to LOOP. 
This direct translation of the inner loop of Programs 1 
and 2 is much more efficient than the code that most 
compilers would produce, and there is still room for 
improvement. 
First, no inner loop should ever end with an uncon- 
ditional jump. Any such loop must contain a conditional 
jump somewhere, and it can always be "rotated" to end 
with the conditional jump, as follows: 
JMP 
INTO 
LOOP LD 
X, A(I) 
LD 
Y, A(J) 
ST 
X, A(J) 
ST 
Y, A(I) 
INTO 
INC 
I, 1 
CMP V, A(I) 
JG 
â¢ - 2 
DEC J, 1 
CMP V, A(J) 
JL 
* - 2 
CMP J, I 
JGE 
LOOP 
OUT 
This sequence contains exactly the same number of 
instructions as the above, and they are identical when 
executed; but the unconditional jump has been moved 
out of the inner loop. (If the initialization of I were 
changed, a further savings could be achieved by moving 
INTO down one instruction.) This simple change re- 
duces the running time of the program by about 3 
percent. 
The coefficients I l and 4 for BN and CN in the 
expression given above for the total running time can be 
verified by counting two time units for instructions which 
reference memory and one time unit for those which do 
not. It is this low amount of overhead that makes Quick- 
sort stand out among sorting algorithms. In fact, the true 
"inner loop" is even tighter, because we have two loops 
within the inner loop here: the pointer scanning instruc- 
tions 
INC I, 1 
DEC J, 1 
CMP V, A(I) 
CMP V, A(J) 
JG 
* - 2 
JL 
* - 2 
are executed, on the average, three times more often than 
the others for Program 1. (The factor is 2Â½ for Program 
2.) It is hard to imagine a simpler sequence on which to 
base an algorithm: pointer increment, compare, and 
conditional jump. The fact that these loops are so small 
853 
makes the proper implementation and translation
Quicksort critical. If we had a translation of loop: i 
+ 1; while A[i] < v repeat which used only three su
fluous instructions, or if we had checked for the poin
crossing or going outside the array bounds within t
loops, then the running time of the whole program c
be doubled! 
Loop Unwrapping 
On the other hand, with our attention focused
these two pairs of three instructions, we can fur
improve the efficiency of the programs. The only 
overhead within these inner loops is the pointer ar
metic, INC I, 1 and DEC J, 1. We shall use a techni
called "loop unwrapping" (or "loop unrolling"-
[3]) which uses the addressing hardware to reduce 
overhead. The idea is to make two copies of the l
one for A[i] and one for A[i + 1], then increment
pointer once by 2 each time through. Of course, the c
coming into and going out of the loop has to be app
priately modified. 
Loop unwrapping is a well-known technique, bu
is not well understood, and it will be instructive
examine its application to Quicksort in detail. 
straightforward way to proceed would be to replace
instructions 
INC 
I, 1 
CMP V, A(I) 
JG 
* - 2 
by one of the equivalent code sequences 
JMP INTO 
LOOP CMP V, A + 1(I) 
LOOP INC I, 1 
JLE OUT1 
CMP V, A(I) 
INC I, 2 
JLE OUT 
CMP V, A(I) 
INTO CMP V, A + 1(I) 
JG 
LOOP 
JG 
LOOP 
JMP OUT 
INC I, 1 
OUT1 INC I, 1 
OUT ~ 
OUT 
We can measure the relative efficiency of these alte
tives by considering how many memory reference t
involve, assuming that the loop iterates s times. 
original code uses 4s memory references (three for
structions, one for data). For the unwrapped program
the left above, the number of memory references ta
for s = 1, 2, 3, 4, 5 .... is 5, 8, 12, 15, 19 ..... and a g
eral formula for the number of references saved is [
2)/2J. For the program on the right, the values are 4
11, 15, 18 .... and the savings are LÂ½(s - l)J. In both c
about V2s increments are saved, but the program on
right is slightly better. 
However, both sequences contain unnecessary 
conditional jumps, and both can be removed, altho
with quite different techniques. In the second progr
the code at OUT could be duplicated and a copy sub
tuted for JMP OUT. This technique is cumbersom
this code contains branches, and for Quicksort it e
contains another loop to be unwrapped. Despite s
complications, this will increase the savings to L
Communications 
October 1978 
of 
Volume 21 
the ACM 
Number 10 

when the loop is iterated s times. Fortunately, this same 
efficiency can be achieved by repairing the jump into the 
loop in the program on the left. The code is exactly 
equivalent to 
CMP V, A + l(I) 
JLE OUT1 
LOOP INC I, 1 
CMP V, A(I) 
JLE OUT 
CMP V, A + l(I) 
JG 
LOOP 
OUT1 INC I, 1 
OUT 
i 
and this code saves /s/2J memory references over the 
original when the loop is iterated s times. Thej loop can 
obviously be unwrapped in the same way, and these 
transformations give us a more efficient program in 
which the I and J pointers are altered much less often. 
Note that since the inner loops of Quicksort are 
iterated only a few times on the average, it is very 
important that loop unwrapping be carefully imple- 
mented. The first implementation above is slower than 
the original loop if it is iterated just once, and actually 
increases the total running time of the program. 
The analysis of the effect of loop unwrapping turns 
out to be much more difficult than the other variants 
that we have seen. The results in [17] show that unwrap- 
ping the loops of Program 2 once reduces its running 
time to about 10.0038N In N + 3.530N, time units, and 
that it is not worthwhile to unwrap further. Figure 7 
shows the percentage improvement when this technique 
is applied to Program 2. 
Perspective 
By describing algorithms to sort randomly ordered 
and distinct single-word keys in a high level language, 
and using performance statistics from low level imple- 
mentations on a mythical machine, we have avoided a 
number of complicated practical issues. For example, a 
real application might involve writing a program in a 
high level language to sort a large file of multiword keys 
on a virtual memory system. While other sorting methods 
may be appropriate for some particular applications, 
Quicksort is a very flexible algorithm, and the programs 
described above can be adapted to run efficiently in 
many special situations that arise in practice. We shall 
examine, in turn, ramifications of the analysis, special 
characteristics of applications, software considerations, 
and hardware considerations. 
Analysis 
In a practical situation, we might not expect to have 
randomly ordered files of distinct keys, so the relevance 
of the analytic results might be questioned. Fortunately, 
we know that the standard deviation is low (for Program 
1 the standard deviation has been shown to be about 
0.648N [11, 17]), so we can expect the average running 
854 
Fig. 7. Improvement due to loop unwrapping. 
[ 
E 
Â°'i[ 
h 
I~ 
2000 3000 4000 5()00 6000 7~ 
80b0 9000 10000 
File Size (N) 
time to be reasonably close to the formulas given (for 
example, we can be 99 percent sure that the formula for 
Program 1 is accurate to within 2N). It is shown in [16] 
that the assumption that the keys are distinct is justified 
and that Program 2 performs well when equal keys are 
present. Furthermore the technique of partitioning on 
the median of the first, middle, and last elements of the 
file ensures that Program 2 will work well on files that 
are almost in order, which do occur in practice. If other 
biases are suspected, the use of a random element for 
partitioning will lead to acceptable performance. 
All of the Quicksort programs do have an O(N z) 
worst case. One can always "work backwards" to fred a 
file which will require time proportional to N 2 to sort. 
This fact often dissuades people from using Quicksort, 
but it should not. The low standard deviation says that 
the worst case is extremely unlikely to occur in a prob- 
abilistic sense. This provides little consolation if it does 
occur in a practical file, and this is possible for Program 
1 since files already in order and other simple files will 
lead to the worst case. This does not seem to be the case 
for Program 2. Hoare's technique of using a random 
partitioning element makes it extremely unlikely that the 
running time will be far from the predicted averages. 
(The analysis is entirely valid in this case, no matter 
what the input is.) However, this is more expensive than 
the method of Program 2, which appears to offer suffi- 
cient protection against the worst case. 
Applications 
We have implicitly assumed throughout that all of 
the records to be sorted fit into memory--Quicksort is 
an "internal" sorting method. The issues involved in 
sorting very, very large files in external storage are very 
different. Most "external" sorting methods for doing so 
are based on sorting small subfiles on one pass through 
the data, then merging these on subsequent passes. The 
time taken by such methods is dependent on physical 
device characteristics and hardware configurations. Such 
methods have been studied extensively, but they are not 
comparable to internal methods like Quicksort because 
they are solving a different problem. 
Communications 
October 1978 
of 
Volume 21 
the ACM 
Number 10 

It is common in practical situations to have multi- 
word keys and even larger records in the fields to be 
sorted. If records are more than a few words long, it is 
best to keep a table of pointers and refer to the records 
indirectly, so only one-word pointers need be exchanged, 
not long records. The records can be rearranged after the 
pointers have been "sorted." This is called a "pointer" 
or "address table" sort (see [11]). The main effect of 
multiword keys to be considered is that there is more 
overhead associated with each comparison and ex- 
change. The results given above and in [17] make it 
possible to compare various alternatives and determine 
the total expected running time for particular applica- 
tions. For large records, the improvement due to loop 
unwrapping becomes unimportant. If the keys are very 
long, it may pay to save extra information on the stack 
indicating how many words in the keys are known to be 
equal (see [6]). Our conclusions comparing methods are 
still valid, because the extra overhead associated with 
large keys and records is present in all the sorting meth- 
ods. 
When we say that Quicksort is a good "general 
purpose" method, one imphcation is that not much 
information is available on the keys to be sorted or their 
distribution. If such information is available, then more 
efficient methods can be devised. For example, if the 
keys are the numbers 1, 2, ..., N, and an extra table of 
size N is available for output, they can be sorted by 
scanning through the file sequentially, putting the key 
with value i into the/th position in the table. (This kind 
of sorting, called "address calculation," can be extended 
to handle more general situations.) As another example, 
suppose that the N elements to be sorted have only 2 t + 
1 distinct values, all of which are known. Then we can 
partition the array on the median value, and apply the 
same procedure to the sub files, in total time proportional 
to (t + 1)(N + 1). It is shown in [16] that Program 1 will 
take on the order of (2 In 2)tN comparisons on such files, 
so Quicksort does not perform badly. Other special- 
purpose methods can be adapted to other special situa- 
tions, but Program 2 can be recommended as a general 
purpose sorting method because it handles many of these 
situations adequately. 
Software 
Modern compilers have not progressed to the point 
where they can produce the best possible (or even very 
good) assembly-language translations of high level pro- 
grams, so we have dealt with "ideal" assembly-language 
implementations. Standard compilers produce code for 
Quicksort that is 300-400 percent slower than the assem- 
bly-language implementation (see [15]). It is not unrea- 
sonable to expect that compilers may someday produce 
programs close to the ideal, since some of the improve- 
ments that we made could be done mechanically and are 
used in so-called "optimizing" compilers. Quicksort's 
partitioning loop, because of its structure, is actually a 
good test case for optimizing compilers--one well-known 
855 
compiler actually makes the inner loop longer when its 
optimizing feature is invoked [15]. 
If a sorting program must run efficiently, it should be 
implemented in assembly language, and we have shown 
a good way to do so. It is interesting to note that on 
many computers an implementation of Quicksort in 
Fortran (for example) will require about as many source 
statements as an assembly-language implementation (see 
[15], but it will of course produce a much less efficient 
program. 
If one is willing to pay for the extra overhead of 
implementing his sorting program in a high level lan- 
guage, then Quicksort should still be used because it will 
incur relatively less overhead than other methods. Pro- 
gram 2 can be used as it stands, although any effort 
spent trying to "optimize" it (such as choosing the very 
best value of M) would be better spent simply imple- 
menting it in assembly language. If a sorting program is 
to be used only a few times on files which are not large, 
then Program 1 (possibly with "A[/] .---: A[(I + r) + 2]" 
inserted before partitioning to make the worst case un- 
likely) will do quite nicely. The only danger is that the 
stack for recursion might consume excessive space, but 
this is very unlikely (it will require less than 30 entries, 
on the average, for files of 10,000 elements [15]) and it 
provides a ~onvenient "alarm" that the worst case is 
happening. Program 1 is a simple program whose aver- 
age running time is low--it will sort thousands of ele- 
ments in only a few seconds on most modern computer 
systems. 
Hardware 
Particular characteristics of particular real computers 
might allow for further improvements to Quicksort. For 
example, some computers have "compare and skip" and 
"increment and test" instructions which allow the inner 
loops to be implemented in two instructions, thus elimi- 
nating the need for loop unwrapping. Similar "local" 
improvements may be possible in other parts of the 
programs. 
The hardware feature on modern computers that has 
the most drastic effect on the performance of algorithms 
is paging. Quicksort actually does not perform badly in 
a virtual memory situation (see [2]) because it has two 
slowly changing "localities" around the scanning 
pointers. In some situations, it will be wise to minimize 
page faults by performing the extra processing necessary 
to split the array into many partitions (instead of only 
two) on the first partitioning stage. Of course, the pro- 
grams should be changed so that small subfiles are 
"insertionsorted" as they are encountered, because oth- 
erwise the last scan over the whole file will involve 
unnecessary page faults. Many internal sorting methods 
do not work well at all under paging, but Quicksort can 
be adapted to run quite efficiently. 
Another hardware feature of interest is parallelism. 
Quicksort does not take good advantage of the parallel- 
ism in large scientific computers, and there are methods 
Communications 
October 1978 
of 
Volume 21 
the ACM 
Number 10 

which should do better if parallel computations are 
involved. However, Quicksort has been shown to per- 
form quite well on one such computer [19]. Of course, if 
true parallelism is available then subfiles can be sorted 
independently by different processors. 
Many modern computers have hardware features 
such as instruction stacks, pipelined execution, caches, 
and interleaved storage which can improve performance 
greatly. Knuth [9] concludes that radix sorting might be 
preferred on "number-crunching" computers with pipe- 
lining. Loop unwrapping could be disastrous on com- 
puters with small instructions stacks, and the other fea- 
tures mentioned above will very often hide the time used 
for pointer arithmetic behind the time used for other 
instructions. The analysis of the effect of such hardware 
features can be very difficult, but again Quicksort makes 
a good test case for such studies because its inner loop is 
so small and its analysis is so well understood (see the 
analysis of loop unwrapping in [17]). However, there will 
probably always remain a role for empirical testing of 
alternatives in superoptimized implementations on ad- 
vanced machines. 
It is often the case that advanced hardware features 
allow the implementation of very fast routines for sorting 
small files. Using such a routine instead of Insertionsorts 
can lead to substantial improvements for Quicksort on 
some computers. To develop a good implementation of 
Quicksort on a new computer, one should first pay 
careful attention to the partitioning loops, then deal with 
the problem of sorting small subfiles efficiently. 
Conclusion 
Our goal in this paper has been to illustrate methods 
by which a typical computer can be made to sort a file 
as quickly and conveniently as possible. The algorithm, 
improvements, and implementation techniques de- 
scribed here should make it possible for readers to im- 
plement useful, efficient programs to solve specific sort- 
ing problems. 
Economic issues surrounding modern computer sys- 
tems are very complex, and it is necessary always to be 
sure that it will be worthwhile to implement projected 
improvements to programs. Many simple applications 
can be handled perfectly adequately with simple pro- 
grams such as program 1. However, sorting is a task 
which is performed frequently enough that most com- 
puter installations have "utility" programs for the pur- 
pose. Such programs should use the best techniques 
available, so something on the order of an assembly- 
language implementation of Program 2 is called for. 
Sorting small subfiles on a separate pass, partitioning 
on the median of three elements, and unwrapping the 
inner loops reduces the expected running time on a 
typical computer from about 11.6667N In N + 12.312N 
to about 10.0038N In N + 3.530N time units. Figure 8 
shows the total percentage improvement for these im- 
provements together. 
Many of the issues raised above relating to other 
sorting programs are treated fully in [9], and the issues 
specific to Quicksort are also dealt with in [15]. We have 
not described here the countless other variants of Quick- 
sort which have been proposed to improve the algorithm 
or to deal with the various problems outlines above [1, 
4, 13, 14, 20]. Many of these turn out not to be improve- 
ments at all: see [15] for complete descriptions. For 
example, nearly every published implementation of 
Quicksort uses a different partitioning method. The var- 
ious methods seem to differ only slightly, but actually 
their performance characteristics can differ greatly. Cau- 
tion should be exercised before a partitioning method 
which differs from those above is used. 
Program 2 is the method of choice in many practical 
sorting situations and will be very quick if properly 
implemented. Quicksort is an interesting algorithm 
which combines utility, elegance, and efficiency. 
Received May 1976; revised February 1978. 
Fig. 8. Cumulative improvement due to sorting small subfiles on a 
separate pass, median-of-three partitioning, and loop unwrapping. 
25 
2o 
E 
~. 15 
856 
1060 2~ 
30b0 4~oo 5~ 
6~oo 7~ 
so~ 
9o6o iooo6 
File Size (N) 
References 
!. 
Boothroyd, J. Sort of a section of the elements of an array by 
determining the rank of each element: Algorithm 25; and Ordering 
the subscripts of an array section according to the magnitudes of the 
elements: Algorithm 26. Comptr. Â£ 10 (Nov. 1967), 308-310. (See 
notes by R.S. Scowen in Comptr. J. 12 (Nov. 1969), 408-409, and by 
A.D. Woodall in Comptr. J. 13 (Aug. 1970.) 
2. 
Brawn, B.S., Gustavson, F.G., and Mankin, E. Sorting in a 
paging environment. Comm. ACM 13, 8 (Aug. 1970), 483-494. 
3. 
Cocke, J., and Schwartz, J.T. Programming languages and their 
compilers. Preliminary Notes. Courant Inst. of Math. Sciences, New 
York U., New York, 1970. 
4. 
Frazer, W.D., and McKellar, A.C. Samplesort: A sampling 
approach to minimal storage tree sorting. Â£ ACM 17, 3 (July 1970), 
496-507. 
5. 
Hoare, C.A.R. Partition: Algorithm 63; Quicksort: Algorithm 64; 
and Find: Algorithm 65. Comm. ACM 4, 7 (July 1961), 321-322. (See 
also certification by J.S. Hillmore in Comm. ACM 5, 8 (Aug. 1962), 
439, and B. Randell and L.J. Russell in Comm. A CM 6, 8 (Aug. 
1963), 446.) 
6. 
Hoare, C.A.R. Quicksort. Computer J. 5, 4 (April 1962), 10-15. 
7. 
Knuth, D.E. The Art of Computer Programming, VoL 1: 
Communications 
October 1978 
of 
Volume 21 
the ACM 
Number 10 

Fundamental Algorithms. Addison-Wesley, Mass., 1968. 
8. Knuth, D.E. The Art of Computer Programming, Vol. 2: 
Seminumerical Algorithms. Addison-Wesley, Mass., 1969. 
9. 
Knuth, D.E. The Art of Computer Programming, Vol. 3: Sorting 
and Searching. Addison-Wesley, Mass., 1972. 
10. Knuth, D.E. Structured programming with go to statements. 
Computing Surveys 6, 4 (Dec. 1974), 261-301. 
11. Loeser, R. Some performance tests of "quicksort" and 
descendants. Comm. ACM 17, 3 (March 1974), 143-152. 
12. Morris, R. Some theorems on sorting. SlAM J. Appl. Math. 17, 1 
(Jan. 1969), I-6. 
13. Rich, R.P. Internal Sorting Methods Illustrated with PL/I 
Progams. Prentice-Hall, Englewood Cliffs, N.J., 1972. 
14. Scowen, R.S. Quickersort: Algorithm 271. Comm. ACM 8, 11 
(Nov. 1965), 669-670. (See also certification by C.R. Blair in Comm. 
ACM 9, 5 (May 1966), 354.) 
15. Sedgewick, R. Quicksort. Ph.D. Th. Stanford Comptr. Sci. Rep. 
STAN-CS-75-492, Stanford U., Stanford, Calif., May 1975. 
16. Sedgewick, R. Quicksort with equal keys. Siam J. Comput. 6, 2 
(June 1977), 240-287. 
17. Sedgewick, R. The analysis of Quicksort programs. Acta 
Informatica 7 (1977), 327-355. 
18. Singleton, R.C. An efficient algorithm for sorting with minimal 
storage: Algorithm 347. Comm. ACM 12, 3 (March 1969), 185-187. 
(See also remarks by R. Griffin and K.A. Redish in Comm. ACM 13, 
l (Jan. 1970), 54 and by R. Peto, Comm. ACM 13, l0 (Oct. 1970), 
624.) 
19. Stone, H.S. Sorting on STAR. IEEE Trans. Software Eng. SE-4, 
2 (Mar. 1978), 138-146. 
20. van Emaen, M.N. Increasing the efficiency of quicksort: 
Algorithm 402. Comm. ACM 13, 11 (Nov. 1970), 693-694. (See also 
the article by the same name in Comm. ACM 13, 9 (Sept. 1970), 
563-567.) 
21. Wirth, N. Algorithms + Data Structures = Programs. Prentice- 
Hall, Englewood Cliffs, N.J., 1976. 
Programming 
Techniques 
S.L. Graham, R.L. Rivest 
Editors 
Packed Scatter Tables 
Gordon Lyon 
National Bureau of Standards 
Scatter tables for open addressing benefit from 
reeursive entry displacements, cutoffs for unsuccessful 
searches, and auxiliary cost functions. Compared with 
conventional methods, the new techniques provide 
substantially improved tables that resemble exact- 
solution optimal packings. The displacements are 
depth-limited approximations to an enumerative 
(exhaustive) optimization, although packing costs 
remain linear--O(n)--with table size n. The techniques 
are primarily suited for important fixed (but possibly 
quite large) tables for which reference frequencies may 
be known: op-code tables, spelling dictionaries, access 
arrays. Introduction of frequency weights further 
improves retrievals, but the enhancement may degrade 
cutoffs. 
Key Words and Phrases: assignment problem, 
backtrack programming, hashing, open addressing, 
recursion, scatter table rearrangements 
CR Categories: 3.74, 4.0 
857 
Permission to copy without fee all or part of this material is 
granted provided that the copies are not made or distributed for direct 
commercial advantage, the ACM copyright notice and the title of the 
publication and its date appear, and notice is given that copying is by 
permission of the Association for Computing Machinery. To copy 
otherwise, or to republish, requires a fee and/or specific permission. 
Author's address: U.S. Department of Commerce, National Bu- 
reau of Standards, Computer Science Section, A367-Tech, Washington, 
D.C. 20234. 
Â© 1978 ACM 0001-0782/78/1000-0857 $00.75 
Communications 
October 1978 
of 
Volume 21 
the ACM 
Number 10 


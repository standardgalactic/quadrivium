

COMPUTER SCIENCE, TECHNOLOGY AND APPLICATIONS 
 
 
 
 
 
 
 
 
 
RECENT ADVANCES IN ARTIFICIAL 
INTELLIGENCE RESEARCH 
 
 
No part of this digital document may be reproduced, stored in a retrieval system or transmitted in any form or
by any means. The publisher has taken reasonable care in the preparation of this digital document, but makes no
expressed or implied warranty of any kind and assumes no responsibility for any errors or omissions. No
liability is assumed for incidental or consequential damages in connection with or arising out of information
contained herein. This digital document is sold with the clear understanding that the publisher is not engaged in
rendering legal, medical or any other professional services. 

COMPUTER SCIENCE, TECHNOLOGY  
AND APPLICATIONS 
 
 
Additional books in this series can be found on Nova‟s website  
under the Series tab. 
 
 
Additional e-books in this series can be found on Nova‟s website  
under the e-book tab. 
 

COMPUTER SCIENCE, TECHNOLOGY AND APPLICATIONS 
 
 
 
 
 
 
 
 
RECENT ADVANCES IN ARTIFICIAL 
INTELLIGENCE RESEARCH 
 
 
 
 
 
 
 
AMBROGIO BACCIGA 
AND 
RENATO NALIATO 
EDITORS 
 
 
 
 
 
 
 
 
 
 
 
New York 
 

Copyright © 2013 by Nova Science Publishers, Inc. 
 
All rights reserved. No part of this book may be reproduced, stored in a retrieval system or 
transmitted in any form or by any means: electronic, electrostatic, magnetic, tape, mechanical 
photocopying, recording or otherwise without the written permission of the Publisher. 
 
For permission to use material from this book please contact us: 
Telephone 631-231-7269; Fax 631-231-8175 
Web Site: http://www.novapublishers.com 
 
NOTICE TO THE READER 
 
The Publisher has taken reasonable care in the preparation of this book, but makes no expressed or 
implied warranty of any kind and assumes no responsibility for any errors or omissions. No 
liability is assumed for incidental or consequential damages in connection with or arising out of 
information contained in this book. The Publisher shall not be liable for any special, 
consequential, or exemplary damages resulting, in whole or in part, from the readers’ use of, or 
reliance upon, this material. Any parts of this book based on government reports are so indicated 
and copyright is claimed for those parts to the extent applicable to compilations of such works. 
 
Independent verification should be sought for any data, advice or recommendations contained in 
this book. In addition, no responsibility is assumed by the publisher for any injury and/or damage 
to persons or property arising from any methods, products, instructions, ideas or otherwise 
contained in this publication. 
 
This publication is designed to provide accurate and authoritative information with regard to the 
subject matter covered herein. It is sold with the clear understanding that the Publisher is not 
engaged in rendering legal or any other professional services. If legal or any other expert 
assistance is required, the services of a competent person should be sought. FROM A 
DECLARATION OF PARTICIPANTS JOINTLY ADOPTED BY A COMMITTEE OF THE 
AMERICAN BAR ASSOCIATION AND A COMMITTEE OF PUBLISHERS. 
 
Additional color graphics may be available in the e-book version of this book. 
 
Library of Congress Cataloging-in-Publication Data 
 
Recent advances in artificial intelligence research / editors, Ambrogio Bacciga and Renato 
Naliato. 
       pages cm 
  Includes index. 
 1.  Neural networks (Computer science) 2.  Artificial intelligence.  I. Bacciga, Ambrogio, editor 
of compilation. II. Naliato, Renato, editor of compilation.  
  QA76.87.R425 2011 
  006.3--dc23 
                                                            2013030607 
 
Published by Nova Science Publishers, Inc. † New York 
ISBN: 978-1-62808-808-3 (eBook)

 
 
 
 
 
 
 
 
 
CONTENTS 
 
 
Preface 
 
vii 
Chapter 1 
Twitter Specific Lexicon for Sentiment Analysis 
1 
J. Skinner, M. Ghiassi and D. Zimbra 
Chapter 2 
Hybrid Unsupervised-Supervised Artificial 
Neural Networks for Modelling Activated 
Sludge Wastewater Treatment Plants 
31 
Rabee Rustum and Adebayo J. Adeloye 
Chapter 3 
Fast Visible Trajectory Planning in 3D 
Urban Environments Based on Local 
Point Clouds Data 
59 
Oren Gal and Yerach Doytsher 
Chapter 4 
“Smart Brakes”– A Neuro-Genetic Optimization 
of Brake Actuation Pressure 
85 
Dragan Aleksendrić and Velimir Ćirović 
Index 
 
103 
 
 
 
 


 
 
 
 
 
 
 
 
 
PREFACE 
 
 
Artificial intelligence (AI) is "the study and design of intelligent agents", where an 
intelligent agent is a system that perceives its environment and takes actions that maximize its 
chances of success. In this book, the authors present recent advances in the study of artificial 
intelligence with topics that include a Twitter specific lexicon for sentiment analysis; hybrid 
unsupervised-supervised artificial neural networks for modeling activated sludge wastewater 
treatment plants; fast and visible trajectory planning in 3D urban environments based on local 
point clouds data; and “smart brakes” - the neuro-genetic optimization of brake actuation 
pressure. 
Chapter 1 – Twitter messages are increasingly used to determine consumer sentiment 
towards a brand. The existing literature on Twitter sentiment analysis uses various feature sets 
and methods, many of which are adapted from more traditional text classification problems. 
In this chapter, the authors introduce an approach to supervised feature reduction using n-
grams and statistical analysis to develop a Twitter specific lexicon for sentiment analysis. The 
authors augment this reduced Twitter specific lexicon with brand specific terms for brand-
related tweets. The authors show that the reduced lexicon set, while significantly smaller, 
reduces modeling complexity, maintains a high degree of coverage over the Twitter corpus, 
and yields improved sentiment classification accuracy. To demonstrate the effectiveness of 
the devised Twitter specific lexicon compared to a traditional sentiment lexicon, the authors 
develop comparable sentiment classification models using SVM. The authors show that the 
Twitter specific lexicon is significantly more effective in terms of classification recall and 
accuracy metrics. 
Chapter 2 – Mathematical modelling of wastewater treatment plant process is important 
for improving its treatment efficiency and thus the quality of the effluent released into the 
receiving water body. However, due to the highly complex and non-linear characteristics of 
this biological system, traditional mathematical modelling of the treatment process has 
remained a challenge. This work presents a hybrid modelling strategy based on the Kohonen 
Self Organising Map (KSOM) and feed-forward, back-propagation artificial neural networks 
(BP-ANN) for modelling the activated sludge wastewater treatment plant. The hybrid 
approach involved a 2-stage process: firstly, the KSOM was used for data preparation, 
visualisation of high dimensional data and features extraction; and secondly, these features 
were then used for the training and validation of the BP-ANN. Comparison of this hybrid 
modelling approach against the straight modelling of the original raw data using BP-ANN 
showed that the hybrid approach resulted in much more improved model performance. 

Ambrogio Bacciga and Renato Naliato 
viii
The study demonstrated that the hybrid modelling strategy offers viable, flexible and 
robust modelling methodology for effectively handling noisy data for environmental systems 
modelling. 
Chapter 3 – In this paper the authors present an efficient and fast visible trajectory 
planning for unmanned vehicles in a 3D urban environment based on local point clouds data. 
The authors trajectory planning method is based on a two-step visibility analysis in 3D urban 
environments using predicted visibility from point clouds data. The first step in the authors 
unique concept is to extract basic geometric shapes. The authors focus on three basic 
geometric shapes from point clouds in urban scenes: planes, cylinders and spheres, extracting 
these geometric shapes using efficient RANSAC algorithms with a high success rate of 
detection. The second step is a prediction of these geometric entities in the next time step, 
formulated as states vectors in a dynamic system using Kalman Filter (KF). The authors 
planner is based on the optimal time horizon concept as a leading feature for their greedy 
search method for making their local planner safer. The authors demonstrate their visibility 
and trajectory planning method in simulations, showing predicted trajectory planning in 3D 
urban environments based on real LiDAR point clouds data. 
Chapter 4 – Many systems today that need modeling and optimization are non-linear 
systems or systems whose behavior is strongly influenced by their previous and current state. 
The most important automotive system having these characteristics is a braking system. The 
main purpose of braking systems is to control braking torques, allowing a vehicle to 
decelerate in an optimum manner while maintaining directional stability. The demands 
imposed on a braking system, over a wide range of operating conditions, are complex. It is 
especially related to the brakes. The brakes are supposed to provide high and stable values of 
braking torque over different operating conditions determined by synergistic influence of the 
actuation pressure and/or sliding speed and/or the brake interface temperature for the specific 
friction pair characteristics. Since the driver obtains an important feedback of vehicle 
dynamics and its braking capabilities depending on a brake performance change, it represents 
an important aspect of a vehicle performance and its quality of use. Sensitivity of braking 
torque versus influence of the friction couple interaction, under different braking conditions, 
is one of the most important properties of the disc brake. In this chapter, possibilities for an 
intelligent dynamic optimization of the brake performance have been investigated. The hybrid 
neuro-genetic optimization model has been developed for dynamic control and optimization 
of the disc brake actuation pressure during a braking cycle. This model provided “smart 
brake” abilities by optimization the value of the brake actuation pressure according to the 
pressure selected by a driver. The important property of the future smart brakes is related to 
stabilization and maximization of the brake performance versus the brake pedal travel 
selected by a driver and current braking conditions. In this chapter, influence of the 
interrelated parameters, such as a vehicle speed, the brake actuation pressure, and 
temperature, has been analyzed during a braking cycle in the case of the disc brake. The new 
optimization model of the brake actuation pressure of a passenger car has been developed. 
The model provided realization of different braking strategies according to the wanted brake 
performance. The model is able to optimize the value of the disc brake actuation pressure in a 
braking cycle versus the brake pedal travel selected by a driver and current the brake 
tribological behavior. The pressure was adjusted on the level which simultaneously providing 
stable and, for current braking regimes, maximum braking torque values. 
 

In: Recent Advances in Artificial Intelligence Research  
ISBN: 978-1-62808-807-6 
Editors: Ambrogio Bacciga and Renato Naliato 
© 2013 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 1 
 
 
 
TWITTER SPECIFIC LEXICON FOR SENTIMENT 
ANALYSIS 
 
 
J. Skinner1, M. Ghiassi2 and D. Zimbra2 
1Amazon.com, Inc. US 
2Santa Clara University, Santa Clara, CA, US 
 
 
ABSTRACT 
 
Twitter messages are increasingly used to determine consumer sentiment towards a 
brand. The existing literature on Twitter sentiment analysis uses various feature sets and 
methods, many of which are adapted from more traditional text classification problems. 
In this chapter, we introduce an approach to supervised feature reduction using n-grams 
and statistical analysis to develop a Twitter specific lexicon for sentiment analysis. We 
augment this reduced Twitter specific lexicon with brand specific terms for brand-related 
tweets. We show that the reduced lexicon set, while significantly smaller, reduces 
modeling complexity, maintains a high degree of coverage over the Twitter corpus, and 
yields improved sentiment classification accuracy. To demonstrate the effectiveness of 
the devised Twitter specific lexicon compared to a traditional sentiment lexicon, we 
develop comparable sentiment classification models using SVM. We show that the 
Twitter specific lexicon is significantly more effective in terms of classification recall and 
accuracy metrics. 
 
Keywords: Twitter, Sentiment Analysis, feature engineering, n-gram analysis, machine 
learning, SVM 
 
 
INTRODUCTION 
 
Twitter offers a unique dataset in the world of brand sentiment. Public figures and brands 
receive sentiment messages directly from consumers in real time in a public forum. Both the 
targeted and competing brands have the opportunity to dissect these messages to determine 
changes in consumer sentiment. Taking advantage of this data, however, requires researchers 

J. Skinner, M. Ghiassi and D. Zimbra 
2
to deal with analyzing an immense amount of data produced by Twitter each day, referred to 
as the Twitter fire hose. As noted by Twitter employee Taylor Singletary on the Twitter.com 
developer discussion site, “Firehose access is very hard to come by and potentially very 
expensive to realistically consume.” The most prolific account on Twitter receives over 
300,000 Twitter messages a day (e.g. Justin Bieber). Services that allow marketers to analyze 
a dataset of this magnitude (NetBase Solutions, Lexalytics, Converseon, Summize [24]) are 
becoming increasingly important in the world of brand management. Modeling and analyzing 
such large datasets often introduce a large number of variables or features. Go, et al. [23] 
report using a unigram model with more than 40,000 features for twitter sentiment analysis. 
Twitter‟s own Summize [24] uses a lexicon of 200,000 uni-grams and bi-grams. These 
models use a large set of features but have resulted in low prediction accuracy of tweet 
sentiment. 
The language found in Twitter messages has often stymied researchers‟ ability to 
determine sentiment. Using term pairs and a mutual information approach, Jansen et al. [24] 
found that “more than 80% of the tweets that mentioned one of these brands expressed no 
sentiment.” Bollen et al. [10] dropped 88.6% of all tweets gathered and reported, “we only 
retain tweets that match the following regular expressions “feel", “I'm", “Im", “am", “^am", 
“being", and “be".” Pak and Paroubek‟s [37] n-gram approach was intolerant of conflicting 
emoticons, “we assume that an emoticon within a message represents an emotion for the 
whole message and all the words of the message are related to this emotion”. And in a 
separate project involving distance supervision, Go, et al. [23] had to completely remove 
emoticons from their dataset, because they found that “if we leave the emoticons in, there is a 
negative impact on the accuracies of the MaxEnt and SVM classifier”. 
Many researchers have been challenged by the Twitter API, as well (the interface used to 
access public Twitter data). Go, et al. [23] reported that “The Twitter API has a limit of 100 
tweets in a response for any request” even though two separate mechanisms exist which allow 
queries to exceed the 100 response limit (API users may page through a result set or may set 
the ending Message ID for a given query). Because of this perceived limitation they requested 
only 100 records every 2 minutes, most likely resulting in a non-continuous dataset. 
Some researchers have limited the amount of data in their investigations as well: Pak and 
Paroubek [37] analyzed 300,000 records, but employed a training set of only 216 records. 
Jansen, et al. [24] analyzed 149,472 tweets, but relied on a third party service, Summize, to 
determine sentiment, which as the authors noted “Summize uses only the most recent 125 
tweets for the time period to determine overall sentiment”. It should be noted that [24] 
manually coded 2610 tweets to determine if the Summize service was accurate. They found 
that “Using the general linear model, we identified no difference between the sentiment 
distributions between the first 125 and second 125 tweets by brand. Therefore we are 
reasonably assured that the most recent 125 tweets are a representative sample of the tweets 
for given weeks.” Finally, Go, et al. [23] manually marked 359 tweets for sentiment. 
Rui et al. [42] summarized the problem best in their conclusion, “On the one hand we are 
happy to see large volumes of WOM (word of mouth) data because it reduces the sample bias; 
on the other hand, analyzing people‟s attitudes becomes a challenge because manually 
checking each WOM message is obviously not feasible.” 
We identify four distinct problem areas associating with measuring Twitter sentiment that 
need further investigation: data gathering, determination of a sentiment scale to apply to the 

Twitter Specific Lexicon for Sentiment Analysis 
3
data, feature engineering, and evaluation and classification of the Twitter messages. The 
following sections will discuss the considerations associated with each problem area. 
The remainder of this chapter is as follows: the next section presents a summary of the 
existing Twitter modeling approaches and introduces the approach used in this paper. Then 
we discuss data gathering and Twitter specific sentiment topics. We next present our feature 
engineering approach and introduce how we tailor feature engineering tools for feature 
selection, reduction and customizations. Then we discuss how the performance of the reduced 
lexicon is compared to a standard set from Opinion Finder to show the effectiveness of the 
reduced lexicon set. Finally, we present our conclusion. 
 
 
TWITTER SENTIMENT ANALYSIS LITERATURE REVIEW AND 
MODELING APPROACH 
 
Twitter is a popular and rapidly growing computer-mediated communication platform. 
Twitter users create micro-blog status update messages called tweets to communicate with 
other users for various reasons on a wide variety of topics. These tweets often contain 
valuable information and the perspectives and opinions of users on issues related to business 
and society [24]. Researchers have developed various approaches to monitor Twitter in real-
time for the occurrences of major events, the outbreak of news stories, and the reactions of the 
users to events [40, 8, 33, 39, 35, 47]. Researchers have also studied the dynamics of the 
diffusion of information throughout the Twitter user population [30, 41]. Social network 
analysis researchers have examined the influence of users in the Twitter network [12, 4]. 
Twitter has also been used by researchers as a source of information to explain various major 
events or indicators of business and society, demonstrating the value of these 
communications. For example, researchers have explained and predicted the outcomes of 
major political elections through the analysis of tweets on the candidates and political issues 
[36, 48, 6, 13]. Researchers have also utilized Twitter as a source of information to explain 
and predict movements in stock markets and other socioeconomic indicators [10, 11]. The 
majority of studies in these areas of Twitter research employ sentiment analysis approaches to 
identify and evaluate the opinions of users expressed in their tweets. 
Sentiment analysis is a prominent and active area of research, spurred particularly by the 
rapid growth of web social media and the opportunity to access the valuable opinions of 
numerous participants on various business and social issues. Sentiment analysis has been 
performed in a wide variety of genres of communication, including news articles [46], 
product reviews [38, 15, 18], and web forums [14, 1]. Approaches to sentiment analysis aim 
to identify and evaluate the opinions expressed in writing, a task considered orthogonal to 
topical analysis. 
Techniques for sentiment analysis can be broadly categorized into two classes of 
approaches. The first class involves the application of a sentiment lexicon of opinion-related 
positive or negative terms to evaluate text in an unsupervised fashion [49]. The second class 
of approaches utilize a textual feature representation coupled with machine learning 
algorithms to derive the relationship between features of the text segment and the opinions 
expressed in the writing in a supervised fashion [38]. Models based upon such supervised 
techniques require a large training set of instances complete with class labels, to calibrate the 

J. Skinner, M. Ghiassi and D. Zimbra 
4
models and tune the relevant parameters. Labels can be assigned to training instances 
manually through human evaluation of the text, or resources with explicitly defined ratings 
(such as the number of stars assigned in movie and product reviews) can be leveraged. 
Much of the existing research in sentiment analysis examines two major issues: the 
features utilized to represent the unit of writing [1], and the techniques applied to perform the 
sentiment analysis [38]. Several classes of writing features have been applied in the literature, 
including semantic [49], syntactic [38], and stylistic features [1]. Semantic features include 
manually or semi-automatically generated sentiment lexicons of specific terms categorized as 
expressing positive or negative opinion in a particular domain [49, 14, 46]. In addition to 
semantic features, syntactic features are most commonly applied, particularly word n-grams 
and part-of-speech n-grams [38, 15, 18]. 
Distinctive writing style is prevalent in web discourse, and stylistic features often 
incorporated in stylometry studies have also shown to be effective in sentiment analysis [1]. 
The inclusion of numerous writing features in the representation of a unit of text often results 
in an expansive feature space that is typically reduced through feature selection prior to 
classifier calibration, to identify and apply the most significant discriminators of opinion 
expression in the domain [18, 1]. 
To utilize these writing features in the sentiment analysis, various techniques have been 
proposed in the literature. Score-based methods are typically used in conjunction with 
sentiment lexicons to analyze a unit of writing [49, 28]. 
In the score-based approach the frequencies of occurrences of positive and negative terms 
in the lexicon are compared to produce an overall sentiment score and classification. Many 
prominent approaches to sentiment analysis utilize supervised machine learning techniques to 
develop classifiers [38, 15, 18, 1]. Among the various machine learning techniques, support-
vector machines and naïve Bayes are most commonly applied [38], although artificial neural 
networks and maximum entropy models have also demonstrated success in sentiment analysis 
applications. 
Another major area of Twitter research focuses on developing sentiment analysis 
approaches specifically designed for tweets. Tweets are a unique genre of communication, 
and its unique features and properties have brought into question the applicability and 
effectiveness of the more traditional approaches to sentiment analysis. Tweets are very short 
units of text, at maximum 140 characters long, and characterized by casual, compact language 
with extensive usage of slang, abbreviations, acronyms, and emoticons. Tweets also contain 
hashtags, user references, and embedded links to other websites containing additional 
referenced information, further complicating the sentiment analysis. 
To perform sentiment analysis on tweets, researchers have applied the more traditional 
approaches that have demonstrated success in other genres of communication, as well as 
devised improved methods that leverage the unique features of Twitter. 
Many researchers have utilized sentiment lexicons developed for traditional genres of 
communication with score-based methods to perform unsupervised sentiment analysis on 
tweets [23, 6, 36, 48, 10, 11]. Researchers have also utilized various textual feature 
representations of tweets coupled with machine learning algorithms to derive the relationship 
between features of the tweet and the opinion expressed in a supervised fashion. 
Similar to the approaches to sentiment analysis applied to other genres of 
communication, semantic, syntactic, and stylistic textual features have been examined, as 
well as features that are more specific to tweets. Researchers have utilized semantic measures 

Twitter Specific Lexicon for Sentiment Analysis 
5
in the tweet feature representations by incorporating sentiment lexicon terms [5, 2, 25, 29, 
44]. Syntactic measures such as word n-grams and part-of-speech n-grams have also been 
applied in tweet feature representations [23, 7, 16, 37, 25, 29]. Features designed to capture 
expressions of writing style in tweets have also been examined [5, 16, 2, 25]. Due to the 
casual, compact language utilized in Twitter, researchers have also evaluated features that are 
more specific to the domain, including acronyms [5, 2, 29] and emoticons [2, 25, 29, 52]. In 
utilizing these tweet feature representations in the sentiment analysis various machine 
learning techniques been evaluated by researchers, although support vector machines [23, 5, 
6, 2, 25, 52] and naïve Bayes models are most often applied [23, 6, 37, 43, 44]. 
While some researchers have recognized the unique features and properties of the 
language used in Twitter and attempted to incorporate some of these aspects in their textual 
feature representations through, for example, the inclusion of acronyms and emoticons, few 
have addressed this uniqueness directly in a more comprehensive fashion. The majority of 
studies in the area continue to rely upon semantic features and sentiment lexicons developed 
for other domains and more traditional genres of communication. Furthermore, efforts to 
expand these sentiment lexicons to form more comprehensive feature representations for 
application on tweets have leveraged traditional lexical resources like WordNet [2]. 
However, sentiment analysis research has shown such features and models are domain-
specific, and analytic performance degrades significantly when these are applied in other 
domains (known as the domain transfer problem) [3, 9, 45]. In this research, we present a 
supervised approach to the development and application of a Twitter-specific sentiment 
lexicon, recognizing and capturing the unique characteristics of the Twitter language. We 
extend this Twitter-specific sentiment lexicon to include brand-specific terms, and show an 
improvement in sentiment analysis performance through the inclusion of such features. 
Finally, we apply support vector machines to develop sentiment classifiers using both the 
reduced Twitter-specific lexicon and a traditional lexicon (Opinion Finder‟s set) and show 
that the reduced lexicon set provides more accurate results. 
In this chapter, we introduce a hybrid approach that uses n-gram analysis for feature 
extraction and the SVM as machine learning approach for twitter sentiment analysis. We view 
the twitter sentiment analysis as a text classification problem. The objective in this analysis, 
therefore, is to classify each tweet into a sentiment class, often three or five. Collectively, 
tweets‟ corpus sentiment represents the perspective of the dataset toward a subject. 
Traditionally, tweets may be analyzed to produce a sentiment score. The scoring process may 
discriminate among the features of a tweet by assigning different weights to features or by 
using a data analysis approach to obtain the weights. Once a tweet is scored, it can be mapped 
into one of the classes. The sentiment score continuum is partitioned so as to represent the 
three or five sentiment classes. Once a set of boundary values for each partition is identified, 
tweets are assigned to the appropriate class. Clearly, identifying a set of weights to achieve 
consistent sentiment scoring is a modeling challenge. The sentiment scoring process uses 
feature engineering methods to first identify relevant terms/features for tweets. This 
identification uses vocabulary (unigrams) or partial or full semantics (n-grams). In this 
chapter we use unigrams, bi-grams, and tri-grams for feature identification. 
The twitter corpus feature identification often results in a very large set, numbering in 
tens of thousands of features. Modeling any system with such a large number of features 
(variables) is often complex. Therefore, once features of a twitter corpus are identified 

J. Skinner, M. Ghiassi and D. Zimbra 
6
additional feature engineering activities can be used to reduce the number of features to a 
manageable set. 
We view the sentiment of a tweet as a function of its features. Most studies use some pre-
existing functional form for this representation, such as applying a sentiment lexicon and a 
score-based method to evaluate a tweet [10, 11]. In this chapter we do not assume existence 
of a specific functional form a priori and use the machine learning modeling approach for 
sentiment representation. 
This approach is data driven in which tweets are presented as vectors of zeros and ones. 
The twitter corpus, thus, is represented in vector space for analysis. The generated input 
matrix for this model is a sparse matrix in which a one represents the presence of the feature 
in the tweet and a zero its absence. The dependent variable in this model is the class 
designation of the tweet. 
In order to assess the sentiment of a tweet (and the corpus) toward a subject, the model 
needs to be trained. The training process requires generation of the vector representation for 
each tweet and the sentiment class it belongs to. Once the feature set is determined, creation 
of the input matrix is easily achieved. The sentiment class designation for the training dataset, 
however, is a manual operation. We developed automated tools to assist human evaluators to 
classify tweets for the training of the model. Similar to other text classification problems, we 
will use a training dataset and a separate testing dataset to train the model and to assess its 
accuracy. 
 
 
DATA COLLECTION AND PREPARATION 
 
We use the Twitter API for our data collection. The APIs offered by the Twitter service 
are a moving target, and have changed several times even over the course of our 
investigation. The most common and consistent method for gathering data is to request a 
paged set of data for a given query. The subject (brand) selected for this chapter is Justin 
Bieber. 
At the time of this research, his Twitter account was the largest Twitter account receiving 
more than 300,000 tweets daily, eclipsing the next three largest accounts together. This 
selection allows us to collect and process a very large dataset. 
In our investigation we developed a set of data gathering tools that both continually 
monitored the Twitter service for @justinbieber mentions (using a paging query) and also 
audited periods of time for any missed @justinbieber references. Auditing turned out to be a 
critical function as the Twitter API was frequently offline during the period of investigation. 
The data gathering tools were used to capture a total of 10,345,184 tweets from May 6th at 
7am to June 8th at 7am, 2012. 
 
 
SENTIMENT SCALE 
 
Literature defines various sentiment scales. Sentiment scales include mood states, tones, 
and simple positive to negative ranges (-1 to +1, or -100 to +100). The key considerations in 
selecting a sentiment scale are: 1) the ability to develop an association between message 

Twitter Specific Lexicon for Sentiment Analysis 
7
features and steps on the scale, and 2) the ability to develop a mechanism for human 
supervision of the results. 
 
 
Relationship between Message Features and Scale Steps 
 
It must be possible to relate message features to steps on the sentiment scale. Natural 
language processing (NLP) models analyze sentence parts that can be valued with different 
weights, resulting in a fine grained scale. 
A commercial sentiment analysis product based on this approach (Lexalytics) offers a 
200 point scale (from -1.00 to +1.00). Go, et al. [23] on the other hand, employed a three 
point scale (“positive”, “neutral” and “negative”) that included the following definition for 
neutral, “If the tweet could ever appear as a newspaper headline or as a sentence in 
Wikipedia, then it belongs in the neutral class.” The authors also categorize all factual 
statements as neutral (e.g. “just landed at San Francisco”). 
The Lexalytics approach, however, is fine grained enough to judge the degree of 
sentiment that exists in any statement. The Lexalytics wiki states, “We consider neutral to be 
a range where there might be some measurable sentiment within a piece of content, but not 
enough that you could firmly state the document is positively or negatively toned.” Bollen, et 
al. [10] introduced a six point sentiment scale. In their paper “Twitter Mood Predicts the 
Stock Market” they use the Google-Profile of Mood States (GPOMS) with six mood 
dimensions (Calm, Alert, Sure, Vital, Kind, and Happy). 
Twitter‟s own Summize engine uses the five Likert scale classes (wretched, bad, so-so, 
swell, and great) [24] for its classification. We follow a similar scale and define five 
sentiment states for our analysis (Strongly Positive, Mildly Positive, Neutral, Mildly 
Negative, and Strongly Negative). Finally, we acknowledge that the number of steps on the 
sentiment scale can determine the attempted precision. 
 
 
Ability to Supervise the Results 
 
The key problem with a fine grained scale is that the result is difficult to validate against 
a human-produced score. Even with detailed guidelines, it would be challenging to find two 
different evaluators that could produce identical 200 point values for a large set of Twitter 
messages. 
This problem is even further exacerbated when the scale attempts to depict something 
more than a negative to positive range (such as mood). Clearly scales with fewer steps are 
easier to validate (between a programmatic score and a human produced score). 
As the steps on the scale are reduced, however, a problem begins to arise with the middle, 
(neutral) category. Go, et al. [23] gave his evaluators a clear guideline for the neutral 
category: facts were considered neutral. Their approach, however, does not take into account 
the promotional impact that is intended when an author broadcasts engagement with a brand 
(e.g., “Listening to Justin Bieber!”). In their model all statements describing the use of a 
product fall into the neutral category. We consider this to be a flaw in a large, coarse neutral 
bucket; this approach will result in wide sentiment buckets that are easier to supervise, but 
sacrifice precision. 

J. Skinner, M. Ghiassi and D. Zimbra 
8
FEATURE ENGINEERING 
 
The twitter sentiment analysis is a special case of the general category of text 
classification. Text classification problems are complex in nature and are always 
characterized by high dimensionality [51]. To reduce this complexity researchers begin by 
applying preprocessing techniques to the original documents in order to produce a more 
simplified text. 
We use standard preprocessing activities in our feature engineering stage. These are: (1) 
removing stop words, (2) stemming, (3) transforming the data into the vector space, (4) term 
weighting, (5) feature selection, and (6) splitting the data into training and test sets. These 
preprocessing steps transform the documents into a simpler form and a vector representation 
of the documents is produced (stages 1- 4 of standard steps). This vector is called the bag-of-
words. Literature on information retrieval lists approaches commonly used for preprocessing 
activities and the steps necessary to produce such a vector [17, 32]. The transformed 
document‟s value for each term is defined using various term weighting methods. Common 
term weighting methods are binary weighting, the term frequency approach (TF), and the 
term-frequency/inverse document frequency (TF/IDF) method [32]. Automated software 
tools are available to perform some or all of these steps. Even after preprocessing, for large 
scale documents, the size of the resulting matrix is still very large and further activities are 
required to reduce its size. Feature engineering methods are introduced to reduce the size of 
such matrices. In text classification models, a feature represents a variable. Most real life text 
classification problems, including the Twitter dataset used in this chapter, often involve 1000s 
or 10,000s of features. Feature engineering is applied in order to reduce the number of 
features. This is a common step for classification methods in order to reduce complexity [51]. 
Additionally, feature engineering serves as a means to remove noise from the input 
documents [32]. The most widely used feature engineering approaches are the information 
gain (IG), document frequency thresholding (DF), mutual information (MI), term strength 
(TS), latent semantic indexing (LSI), and the χ2 statistic (CHI) [32, 51]. We also use feature 
engineering to reduce complexity in our analysis. We next examine applicability of traditional 
feature engineering tools and introduce a new measure specific for Twitter feature selection. 
There are specific challenges associated with feature engineering as it pertains to twitter 
sentiment analysis. One of the most widely used method for feature selection for text 
classification systems rely on TF/IDF weighting [26, 20]. This approach places value on 
highly discriminating terms in the document corpus. Manning, et al. [32] defines the 
calculation TF/IDFt,d as an assignment to term t a weight in document d that is: 
 
1. Highest when t occurs many times within a small number of documents (thus lending 
high discriminating power to those documents); 
2. Lower when the term occurs fewer times in a document, or occurs in many 
documents (thus offering a less pronounced relevance signal); 
3. Lowest when the term occurs in virtually all documents. 
 
However, sentiment within the Twitter corpus poses four key challenges that cannot be 
answered by measures such as TF/IDF. We next discuss these challenges and propose four 
properties of a feature engineering approach specific to twitter sentiment analysis. 

Twitter Specific Lexicon for Sentiment Analysis 
9
STRONG SENTIMENT TERMS ARE PERVASIVE 
 
First, there are strong sentiment terms that are pervasive throughout the Twitter corpus. 
Any approach that discounts these widely used terms due to lack of uniqueness will 
misrepresent the sentiment associated with the Tweets in which they occur. 
Table 1 captures word frequency for a variety of key sentiment terms across the 
10,345,184 tweets from the Justin Bieber dataset. 
With a TF/IDF based weighting approach, the feature “Love” would be underweighted 
(due to its pervasive use) while the feature “Luv” would be more strongly weighted. The two 
terms are used interchangeably in the Twitter corpus, however (i.e. they are semantically 
identical), and thus no discount should be applied to one term over the other (Table 1). 
Similarly, the smiley-face emoticon “:)” occurs 4.5 times more often in the left to right 
orientation “:)” than in the right to left orientation “(:”. The two emoticons are semantically 
identical, with the disparity attributed mainly to personal aesthetics and the orientation 
difference between Latin and ideographic (Chinese, Korean) scripts. 
Were a TF/IDF approach to be used to determining the feature weights of the smiley-face 
variants, it would improperly penalize the widely used “:)” and reward the lesser used “(:” 
variant (Table 1). 
An even more significant variation can be found in the set of sad-face emoticons. The 
sad-face emoticon “:(“ appears 11.3 times more often in the left to right orientation “:(“ than 
in the right to left orientation “):“. A reasonable conclusion to this problem would be to 
combine identical terms, and apply weights to the set of like terms. Combining terms, 
however, makes the set of terms less unique (than any individual member), more pervasive 
and results in even a lower TF/IDF weight. 
Property 1: In Twitter analysis, the pervasive use of a few strong sentiment terms, and 
the light use of analogous terms, must be tolerated by the feature construction and feature 
weighting approach. 
 
 
SYNTACTICAL CONTEXT MUST DETERMINE FEATURE BOUNDARY 
 
The next key challenge in weighing a set of features for Twitter sentiment classification 
is the manner in which boundary conditions are determined for each feature. 
 
Table 1. Frequency of common sentiment terms 
 
Feature 
Frequency 
Feature 
Frequency 
Feature 
Frequency 
Love 
(stemmed) 
23.65% 
Hate 
(stemmed) 
1.30% 
Please 
17.28% 
Luv 
(stemmed) 
0.47% 
Hate (exact 
match) 
0.53% 
Please follow 
7.76% 
:) 
14.42% 
Hater 
(stemmed) 
0.60% 
Follow 
34.66% 
(: 
3.22% 
 
Follow me 
23.03% 
:( 
3.63% 
 

J. Skinner, M. Ghiassi and D. Zimbra 
10
A stemmed query against the corpus for the term “hate” (i.e. “hate%”) results in a record 
set somewhat evenly divided between authors that “hate” Justin Bieber and authors that want 
to defend Justin from the “haters.” This causes “hate%” to be highly indiscriminate when it 
comes to sentiment. An example of one of these problematic Tweets is as follows: 
 
“@justinbieber Ignore all the haters their just prejudice idiots.” 
 
If a feature was allowed to include the full set of Tweets found with the stemmed query 
(“hate%”) that feature would not be able to clearly describe sentiment in one direction or 
another (due to the inclusion of both “hate” and “hater”). Table 1 denotes the frequency of 
“hate” (the stemmed version), “hate” (the exact match, a better candidate for negative 
sentiment) and “hater” (the stemmed version, which is an excellent candidate for positive 
sentiment). The catch-all query for “hate” includes more references to “hater” (0.602% of the 
corpus) than to “hate” itself (0.529% of the corpus). Thus the stemmed “hate” query cannot 
be used to create a feature with a meaningful syntactical boundary. The system used for 
weighing sentiment terms must be able to identify that the stemmed variation of “hate” 
includes a significant syntax variation and is therefore deficient in identifying sentiment. 
Property 2: The ability to determine precise feature boundaries must be included in the 
feature construction approach. 
 
 
THE “PLEASE FOLLOW ME” PHENOMENON – UNAVOIDABLY BROAD 
CLASSES 
 
Another distinct problem with Twitter sentiment categorization is that some of the classes 
into which messages are categorized are unavoidably large. This is a result of both the way in 
which Twitter users choose to employ the messaging system, and the nature of sentiment 
itself. 
For example, in the case of the Twitter network, a common activity is to expand one‟s set 
of followers by asking a high profile individual to “follow me.” This activity is so common 
that over a third of messages (34.7%) directed to Justin Bieber include the term “follow.” 
Table 1 denotes the term frequency for common unigrams and bi-grams associated with 
follow requests (“please”, “please follow”, “follow”, “follow me”). 
Term frequency analysis reveals that follow requests are so pervasive on the Twitter 
network that were a single class defined to hold the requests, it would contain 34.7% of all 
Tweets. 
Furthermore, the sentiment term “please” cannot be effectively used to recognize 
sentiment outside of follow requests because follow requests account for 45% of all uses of 
the term “please” on the Twitter service (e.g. 7.763 / 17.281 = 44.92%, Table (1)). Put 
another way, any algorithm that attempts to judge sentiment using the term “please” as a 
stand-alone feature will pick up “follow” sentiment 45% of the time. 
Thus an approach such as TF/IDF that values clear, distinct bursts of a term (or cluster of 
terms) in a statistically small number of documents (that correspond to a narrow category) 
will not be able to optimally select features for the wide sentiment buckets that are required to 
hold Twitter sentiment. 

Twitter Specific Lexicon for Sentiment Analysis 
11
Property 3: The feature weighting approach must be able to weigh the performance of 
features that are associated with wide sentiment classes. Furthermore, to increase the 
precision of the wide sentiment classes, the affinity that unigram terms show for one another 
must be considered at the time of feature engineering. 
 
 
REAL WORLD BIAS TOWARD POSITIVE SENTIMENT 
 
A key problem with Twitter sentiment is that there is considerably less positive feedback 
provided to the author after publishing a negative Tweet: 
 
 
Fewer re-tweets by the subject – The subject of a negative tweet is, obviously, 
predisposed against re-tweeting the message. 
 
Fewer re-tweets by readers – The Twitter user that reads a negative Tweet on a given 
subject either 1) made a conscious decision to follow the author or 2) made a 
conscious decision to follow the subject. In the case of a reader following the subject, 
there is some degree of effort involved in following the full set of messages 
pertaining to the subject. Thus it is likely that the reader will not further promulgate 
negative sentiment toward something they have invested time and effort into. 
 
Fewer follows – Both the subject of the negative tweet, and those disposed positively 
toward the subject will naturally exhibit a bias against following the author of a 
negative tweet. 
 
Wrath of the mob – The more widely beloved a subject is, the more likely the author 
of a negative message is to be inundated with responses defending the subject. 
 
This results in a corpus that is biased toward positive sentiment. In the 2918 rows 
randomly selected for this investigation approximately 85.4% were found to have either mild 
or strong positive sentiment. 
Lexalytics confirms a positive slant in the full Twitter corpus, though not to the degree 
we find in the Justin Bieber corpus, “We find that overall document content is slanted to be 
slightly positive by default”. 
Evaluating the term frequency of sentiment terms in the Twitter corpus with a TF/IDF-
based approach would result in a feature set that does not value commonly found, positive 
terms (as these terms are pervasive throughout the document set, which TF/IDF equates to a 
less pronounced relevance signal). 
Property 4: The unbalanced classes found in the Twitter corpus require the feature 
engineering approach to normalize for the larger positive sentiment class when measuring 
term frequency. Unbalanced classes must also be considered when determining the affinity a 
term has for a sentiment class (a key element of feature boundary engineering, described 
further). 
Our approach will engineer features in a manner that addresses problems described in this 
section and will rely on a sentiment scale that is both precise and free of interpretation bias as 
discussed earlier. 
In the following sections we introduce our approach for Twitter specific feature 
engineering and Twitter sentiment analysis. 

J. Skinner, M. Ghiassi and D. Zimbra 
12
EVALUATION OF TWITTER MESSAGES 
 
Twitter messages use some specialized terms and are often short. Applying feature 
engineering techniques to twitter messages, thus, require special considerations. The first 
consideration is the supervised training required for such analysis. In order to train a model, 
humans need to be able to evaluate and produce consistent score for like messages. Similarly, 
when using n-gram analysis the explanatory power of the feature set and the interaction 
between features must be considered. 
Our approach attempts to address both considerations, by starting with a large feature set 
which can also be reduced to a supervised score which maps to a step on a precise sentiment 
scale in real-time. 
Another feature of twitter messages is the offsetting nature of sentiment terms. Features 
that describe sentiment offer a distinct problem in text classification in that they can negate 
one another when found alongside one another in a given message. For example, if a message 
contains a feature that captures the term “not” and a feature that captures the term “happy”, 
and these features are in close proximity in the message, it is very likely that the author of the 
message was in fact “not happy” about something. An evaluation method that fails to take the 
offsetting nature of sentiment features into consideration will not accurately portray 
sentiment. 
Here is a sample score from Lexalytics, an NLP based Twitter sentiment service, which 
uses a -1.00 to +1.00 sentiment scale. 
Lexalytics correctly identifies the object of the sentence, but the negative scores for the 
words “sick”, “fever” and "cure" result in a negative score for the sentence as a whole, which 
ultimately fails to capture the author's intent. Lexalytics also misses one of the most common 
positive sentiment features in the Twitter-sphere, the emoticon (e.g. “.D”). In this example a 
strongly positive message is coded erroneously as negative by the engine. 
The lack of context associated with a unigram based approach is resolved to some degree 
by moving to n-gram analysis. Practitioners employing an n-gram approach to sentiment 
analysis have found greater accuracy because the n-gram structure stores a greater degree of 
context and language complexity than a single word. 
 
 

Twitter Specific Lexicon for Sentiment Analysis 
13
At the 2011 Twitter DevNest conference, Jeffrey Kinsey, Co-Founder at DataMinr, 
reported that “the larger the n-gram the more accurate it is in determining sentiment”. 
For example, if the tweet had been scored by comparing the word pairs found in the tweet 
to a set of known bi-grams that included the strongly positively bi-gram “Bieber fever”, the 
scoring process would have assigned a strongly positive weight to the occurrence of “Bieber 
fever” in the tweet (instead of the negative score assigned to the “fever” singleton) and as a 
result the entire sentence would be classified as having positive sentiment. If a 5 member n-
gram were to be used (e.g. a reference set that included the n-gram “I‟m sick with Bieber 
fever”) the analysis would yield an even more accurate determination of sentiment, however 
the larger the n-gram the fewer the number of sentence matches. Therefore, when applying n-
gram analysis in a relatively short twitter messages, the size of n in n-gram needs to be small. 
Kinsey reports that for his DataMinr service the optimal n-gram length is 3 (e.g. tri-grams). 
We also use n=3 in our chapter. 
 
 
TWITTER SPECIFIC LEXICON SET 
 
We offer a feature engineering method for twitter feature selection and reduction. Most 
existing Twitter sentiment analysis approaches use a very large set of features. Applying a 
large feature set results in (a) a very sparse feature-tweet input matrix, and (b) less significant 
features contribute to the model noise level. These factors often reduce model accuracies in 
machine learning approach to sentiment analysis. We, therefore, offer a feature engineering 
approach that (a) reduces the feature set, (b) creates a denser feature-tweet input matrix, and 
(c) ensures that no significant feature is excluded from the final feature set. Our approach 
adheres to the feature engineering guidelines suggested by Mitchell (2005), who suggests that 
to maximize the predictive ability of a reduced set of features; one should consider the 
following six attributes: Information loss, Bias, Noise, Collision of negative and positive 
semantic vectors, Differences in Scale, and Overfitting. 
We manually derived the most impactful terms from a large body of twitter messages 
concerning a single subject (107 tweets related to Justin Bieber). We find there are a distinct 
set of features that have a greater sentiment value (i.e. “love”, “hate”) than others (i.e. “if”, 
“but”). We also find that sentiment terms are not well distributed throughout the dataset. 
Strongly negative terms are found approximately 1/8th as often as strongly positive terms. 
Any feature reduction approach that requires well distributed data will struggle to maximize 
the predictive ability of a set of features drawn from social media messages. 
We have developed a Twitter specific approach for feature selection for our analysis. The 
process uses linguistic analysis to determine the n-grams of interest, and validates the 
discovered n-grams using frequency and affinity metrics (defined below). As stated earlier we 
used the Twitter API to collect our dataset. The dataset included all tweets between May 6th, 
2012, 7am and June 8th, 7am, 2012 (that mentioned @justinbieber). A total of 10,345,184 
tweets were gathered. We then divided the tweets into groups based on the last two digits of 
the tweet ID. This was done in order to select random groups of tweets that spanned the entire 
investigation period and to limit the influence of “spam” tweets. A “spammer” will send the 
same tweet as many as 100 times in a row, thus an algorithm that only takes one row out of 

J. Skinner, M. Ghiassi and D. Zimbra 
14
every 100 sequential tweets will reduce the impact of a “spammer” (to that of a normal 
follower). 
Ten groups were selected as candidate groups: candidate groups ranged in size from 
1,364 to 57,319 tweets. Tweet distribution was then evaluated across the 10 groups (based on 
time of day and day of month). The goal was to find a candidate group that most closely 
resembled the distribution of tweets in the greater dataset. A group of tweets with a random 
ID was selected (e.g. Tweet ID 66978297401196546 which reads,“@justinbieber if u follow 
me this will b the best day of my life!!!” was included in the dataset). The model group was 
initially composed of 12505 tweets. Re-tweets of Justin‟s messages to his followers were first 
removed from this set so that the sentiment Justin directly expressed (in his tweets) would not 
be reflected back onto him. This resulted in a model group of 9749 tweets. To provide 
sufficient instances to train our classification models, we aimed to score approximately 3000 
tweets of the 9749 by our human evaluators. 
Next, in order to train the sentiment engine, training and testing datasets are generated. 
The sets include representative tweets from each class, while accounting for unbalanced 
nature of tweet sentiment classes. Finally, these tweets are manually scored and classified. 
Three Information Systems graduate student evaluators independently scored 3000 tweets for 
sentiment on a 5 step positive to negative scale. Scale steps were “Strongly Positive”, “Mildly 
positive”, “Neutral”, “Mildly Negative”, and “Strongly Negative”. Sentiment evaluations that 
did not agree between the evaluators were discarded, resulting in 2618 tweets incorporated 
into the final dataset. Evaluators also marked the most meaningful bi-grams and tri-grams in 
each tweet. This step was performed in order to build the lexicon that would become the basis 
of the feature set. 
A statistically significant number of tweets in each sentiment category were required in 
order to fully evaluate the performance of the SVM models. However, the “Strongly Positive” 
bucket was found to so thoroughly dominate the 2618 tweets that additional refinement in the 
final dataset was required to account for the unbalanced categories. We required each 
category to have at minimum 500 representative instances in the final datasets. To reach this 
threshold, a supplementary set of 300 negative tweets were also included in the final dataset. 
To acquire these additional negative instances, we extracted another set of tweets from the 
9749 model group not already included in the dataset to be coded by our team of human 
evaluators, and incorporated 300 negative tweets with agreement among the evaluators into 
the final datasets. Therefore the final, refined model was composed of 2918 tweets with 
sentiment (2618 random tweets with sentiment across all five categories + 300 negative 
tweets filtered from another set). This group was used as the basis for our analysis. 
We next evaluated the bi-grams and tri-grams that were identified as most meaningful, in 
the previous stage, and decomposed them into unigrams and at this point unigram term 
frequency was calculated across the refined dataset. For example, Tweet ID 
66978297401196546 was determined to include the meaningful bi-grams and tri-grams “best 
day ever”, which was then decomposed into “best”, “day” and “ever”. After application of 
term frequency across the set of unigrams, the most frequently occurring terms (terms with 
frequency >= 0.033%) were selected as features (standard stop words, pronouns and quantity 
determiners were excluded). The goal of term frequency evaluation was to arrive at a reduced 
lexicon that could be generically applied to twitter messages by future investigators. This 
approach is in sharp contrast to Go, et al. [23] in which no consideration was given to the 
ability to supervise the lexicon that was employed. 

Twitter Specific Lexicon for Sentiment Analysis 
15
Since the term frequency parameter (0.033%) is experimentally defined, and to ensure 
that the resulting reduced feature set has general applicability, additional terms were added to 
feature set (in order of decreasing term frequency) until nearly all tweets in the full 
10,345,184 row corpus contained one or more of the featured terms. The specific goal was 
97% of all tweets sent to Justin Bieber during the period in question had to contain one or 
more of the features in the feature set. This resulted in a feature set containing approximately 
700 of the most frequently occurring terms and emoticons that were required in order to meet 
the coverage metric. 
At this point the feature set had not been cleansed in any way, stemming had not 
occurred, and the precision of the original bi-grams and tri-grams had been traded for the 
unigrams‟ higher term frequency. We next developed a feature boundary for each term. To 
determine the appropriate boundary, the affinity of each term to each sentiment class was 
examined. Any term that had a strong affinity for one side of the sentiment scale (e.g. 
frequently showing up in positive sentences and rarely showing up in negative sentences) was 
kept. Any term that did not clearly indicate sentiment was either set aside for later n-gram 
engineering or dropped altogether. For the terms that remained, synonyms were found and 
added to the feature definition. The feature boundary work resulted in features that were both 
highly explanatory and combinable with other features of similar sentiment affinity. At this 
point the feature set fell below our target 97% coverage goal due to the removal of ambiguous 
terms. 
We then examined contradictory terms and did not allow them to co-exist in the same 
feature, as this would severely impact the feature‟s explanatory ability. A good example of 
this problem is the term “hate”. A stemmed hate (i.e. “hate%”) picks up all occurrences of 
“hate” and “haters”. In the Justin Bieber corpus, the term “haters” is almost universally used 
to describe people that are seen in a negative light because they do not like Justin (i.e. they 
hate Justin, but the author loves Justin, so the author describes the group with the pejorative 
term “haters”). As a result messages regarding “haters” predominantly hold positive 
sentiment toward Justin. Because of this contradiction the “hate” feature includes a “hate” 
search that is not stemmed (i.e. “hate “), and a different, opposing feature called “haters” 
searches for the stemmed term “haters” (i.e. “haters%”). 
Another issue that arose once the selected bi-grams and tri-grams were abandoned 
involved negations. Terms that are negated are difficult to handle with pure unigram analysis. 
At the minimum the sentence “I‟m not happy” requires a “not happy” feature to be properly 
scored (as either a bi-gram or two related unigrams). Go et al. (2009) attempted to deal with 
negation using bi-grams to no avail. We successfully included negation in our model by 
employing frequently occurring negations (in bi-gram form) and frequently occurring 
negative terms in the singular (in unigram form). We created specific features for the most 
common sentiment negations (“not happy”, “not good”, “not great”, “not impressed”, “not 
nice”, “not like”, “not disappoint”, “not enjoy”, “can‟t wait”, “can‟t stand”, “don‟t want”, “no 
reason” and “no chance”) as well as features for each not variant itself (“not”, “can‟t”, 
“cannot”, “isn‟t”, “doesn‟t”, “don‟t”, and “won‟t”). This approach follows the method used 
by Pak and Paroubek [37] who proposed, “A negation (such as “no” and “not”) is attached to 
a word which precedes it or follows it. For example, a sentence “I do not like fish” will form 
two bi-grams: “I do+not”, “do+not like”, “not+like fish.” Such a procedure allows improving 
the accuracy of the classification since the negation plays a special role in an opinion and 
sentiment expression.” 

J. Skinner, M. Ghiassi and D. Zimbra 
16
Once the original bi-grams and tri-grams were decomposed, and a set of unigrams were 
selected based on term frequency, the most frequently occurring bi-grams and tri-grams could 
then be “safely” introduced back into the model (without risk of over fitting) using the 
affinity method, championed by [27]. They define the Affinity of a word phrase P as 
 
            
    
                
 
Where f(P) is the frequency of phrase P in a corpus, and min(f(wi)) is the minimum 
frequency across the words in a phrase P  in the corpus. The affinity approach adds back bi-
grams and tri-grams that have “higher collocation frequencies relative to individual 
occurrence frequencies of the constituent unigrams” ensuring that the additions are driven by 
n-gram frequency and meaningfulness. To further manage bias in the feature set, bi-grams 
and tri-grams were only added to features that were still lacking in precision after the initial 
refinement. For example, the stemmed “hope%” search term was not as precise as the 
following set of n-grams { “%hopeful%”, “%I hope u%”, “%I hope you%”, “%I hope he%”, 
“% hoping%”, “%do not lose hope%”, “%don‟t lose hope%”, “%don‟t lose hope%”} and was 
replaced with the n-gram set. 
Meaningful bi-grams and tri-grams were added to the feature set in order to once again 
regain its 97% coverage rate. This resulted in a total feature set of 755 n-grams (n=1, 2, 3). 
After the feature engineering phase, we then combined like features to arrive at 187 feature 
“groups” that each had a high degree of affinity for either positive or negative sentiment. 
Feature groups were formed around groups of n-grams with similar meaning. For example, 
the POS_LOVE feature group is composed of the n-grams “love”, “lovin” and “luv”. The 
POS_THANK feature group is composed of the n-grams “thank”, “thx” and “thks”. The 
POS_HI feature group includes 9 variants of “hi”. In a limited set of cases the grouping 
includes n-grams that have some minor degree of difference in terms of meaning, but were 
found to be used interchangeably on the Twitter service. For example the POS_AMAZIN 
feature group is composed of “amazin”, “amaze” and “brilliant”. At this juncture the reduced 
feature set was able to characterize 97.3% of the 10,345,184 messages in the greater Twitter 
corpus (i.e. one or more of the 187 feature “groups” appeared in 97.3% of the tweets). This 
indicates that no explanatory power was lost in the reduction to the 187 features (as the 
groups were simply logical partitioning of the 755 features). This produced a significantly 
smaller and denser input matrix for our analysis. Features were considered present or not 
present in the matrix (0 or 1). This process is an “equivalence partitioning” approach in which 
a representative feature is used as a proxy for a set of features with similar sentiment effect. 
Multiple presences of such features in a message are still presented with a single attribute of 
one in the input matrix. We also note that repetition of a single feature within a message did 
not add explanatory power (e.g. one happy face emoticon was grammatically identical to 
many happy face emoticons), thus repetition was not considered useful and was ignored. For 
example, the positive emoticon feature (“happy_emoticon”) was based on the 46 emoticons 
that were most frequently found in the positively-scored tweets. A tweet with the following 
text “@justinbieber       ” resulted in the same feature score as the tweet 
“@justinbieber ” (both resulted in the feature happy_emoticon = 1). 

Twitter Specific Lexicon for Sentiment Analysis 
17
Our approach minimizes information loss and noise by employing a model with a 
supervised feature set (187 unigrams, bi-grams and tri-grams). 
 
 
FEATURE SELECTION WALK THROUGH 
 
Our feature selection process chooses high frequency terms that have a clear affinity for 
either positive or negative sentiment (e.g. “love”, “hate”) and discards terms that are 
ambiguous (e.g. “from”, “again”). Further refinement of the feature set combines like terms, 
adds synonyms, and solves for the cases of contradiction and negation. Finally, related bi-
grams and tri-grams are then found using Kajanan‟s [27] Affinity algorithm (meaningful n-
grams that exhibited higher collocation frequencies relative to individual occurrence 
frequencies of the constituent unigrams). These feature engineering steps have resulted in 187 
features listed in Table 2. The prefix of “POS_” or “NEG_” indicates the sentiment affinity of 
the feature. The prefix of “NOTTEST” indicates a contradiction (these terms are used as 
modifiers of sentiment). The prefix of “MIXED” indicates a feature that does not have strong 
affinity in either direction (no clear boundary could be arrived at for this feature but it is 
instrumental in describing sentiment, often negating a sentence or adding a condition to it). 
The prefix of “is” denotes information about the Tweet itself (e.g. it is written in Spanish, or it 
does include an HTTP link). And finally there are nine features that occur with such high 
frequency they are not assigned a sentiment prefix (though they are still considered to contain 
sentiment). The frequently occurring features include positive emoticons, negative emoticons, 
“please” and “follow”. 
 
 
Re-Use of the lexicon 
 
One of the goals of our research was to arrive at a lexicon that would enable ongoing 
investigation of sentiment on the Twitter service, regardless of message domain. In the 
lexicon set we developed only six features were specific to Justin Beiber. These features 
(POS_FEVER, POS_NEVERSAYNEVER, POS_OTHER, POS_OTHER2, NEG_OTHER, 
and NEG_OTHER2) contained terms that we would not expect to find in other brands‟ 
Twitter feeds. The remaining 181 features are highly descriptive of sentiment throughout the 
Twitter-verse; however it will be left to future investigations to quantify the usefulness of this 
lexicon. 
 
 
CUSTOMIZATION SPECIFIC TO THE TWITTER SERVICE 
 
The messages on the Twitter service often involve functional elements of the Twitter 
service itself. A successful n-gram model, then, must include proper handling for terms that 
describe Twitter-specific behaviors: 
Re-tweets – The act of repeating an author‟s message to others 
Follow requests – The request for an author to add a user to a privileged list (i.e. a list of 
users the author follows) 

 
Table 2. Final Twitter Feature List 
 
POS_ADORE  
POS_AMAZIN  
POS_AMEN  
POS_AWARD 
POS_APPRECIATE  
POS_AWESOME  
POS_BABY  
POS_BEAUTIFUL  
POS_BELIEF  
POS_BEST  
POS_BETTER  
POS_BIGGEST  
POS_BLESS  
POS_BRIGHT  
POS_CANT_WAIT  
POS_CHECK_IT_OUT  
POS_CONGRAT  
POS_COURAGE  
POS_CRUSH 
POS_CUTE 
POS_DREAM  
POS_DYING 
POS_EXCITE  
POS_FEEL  
POS_FEVER  
POS_FAVORITE  
POS_FRIEND  
POS_FUNNY  
POS_GREET  
POS_HAHA  
POS_HATERS  
POS_HEART  
POS_HELL_YES 
POS_HELLO  
POS_HERO 
POS_HI 
POS_HILARIOUS  
POS_HONEST  
POS_HOPE  
POS_HOT 
POS_INSPIRE  
POS_KILL  
POS_LAUGH  
POS_LIKE  
POS_LOL 
POS_LOVE  
POS_LUCK 
POS_MAKE_MY  
POS_MISS 
POS_MISSING  
POS_MYSELF  
POS_NO_MATTER_WHAT  
POS_NOT_DISAPPOINT  
POS_NEVERSAYNEVER  
POS_PARTY  
POS_PEACE  
POS_PERFECT  
POS_POSITIVE  
POS_PRAY  
POS_PRETTY  
POS_PROPS  
POS_PROUD  
POS_RESPECT  
POS_RUMOR  
POS_SEX  
POS_SMILE  
POS_SORRY  
POS_SPECIAL  
POS_SUCCESS  
POS_SUPER  
POS_SUPPORT  
POS_SURE  
POS_SWAG  
POS_TALENT  
POS_THANK  
POS_TREND  
POS_TRUTH  
POS_WAITING  
POS_WANT  
POS_WATCH  
POS_WELCOME  
POS_WINNER  
POS_WISH  
POS_WITHOUT_YOU  
POS_WONDERFUL 
POS_WOW  
POS_YES  
POS_OTHER  
POS_OTHER2  
POS_NOTTEST_ENJOY  
POS_NOTTEST_GOOD  
POS_NOTTEST_GREAT  
POS_NOTTEST_HAPPY  
POS_NOTTEST_IMPRESSED  POS_NOTTEST_NICE  
NEG_NOT_GOOD  
NEG_NOT_HAPPY  
NEG_NOT_LIKE  
NEG_ANNOY 
NEG_BITCH  
NEG_CANT_STAND  
NEG_CONFUSE  
NEG_COMPLAIN  
NEG_CRAP  
NEG_CRY  
NEG_DAMMIT  
NEG_DISAPPOINT 
NEG_DISLIKE 
NEG_FAIL 
NEG_FAKE 
NEG_FUCK  
NEG_GONE  

 
NEG_GRR 
NEG_HORRIBLE 
NEG_HURT 
NEG_I_HATE 
NEG_I_DONT_WANT 
NEG_IDIOT 
NEG_IGNORE 
NEG_IMPOSSIBLE 
NEG_JUST_SAYING 
NEG_LAME 
NEG_LET_DOWN 
NEG_LIAR 
NEG_LOSE 
NEG_NEGATIVE 
NEG_NO_REASON 
NEG_NO_CHANCE 
NEG_NOT_NICE 
NEG_RAIN 
NEG_RUDE 
NEG_RIDICULOUS 
NEG_SAD 
NEG_SCREW 
NEG_SELFISH 
NEG_SHAME 
NEG_SHIT 
NEG_SLUT 
NEG_STUPID 
NEG_SUCK 
NEG_TERRIBLE 
NEG_TOO_BAD 
NEG_TRANSACTION_TERMS 
NEG_UGLY 
NEG_USELESS 
NEG_WEIRD 
NEG_WHY_WONT 
NEG_WORST 
NEG_WTF 
NEG_YOU_WONT 
NEG_OTHER 
NEG_OTHER2 
NEG_NOTTEST_ANGRY 
NEG_NOTTEST_UPSET 
NEG_NOTTEST_PISSED 
NEG_NOTTEST_MAD 
MIXED_BUT 
MIXED_DEPRESSED  
MIXED_DEATH  
MIXED_NOT_FEELING_WELL 
MIXED_IF 
MIXED_DAMN 
MIXED_CRAZY 
MIXED_JEALOUS 
MIXED_OMG 
NOTTEST_NOT 
NOTTEST_ISNT 
NOTTEST_ISN_T 
NOTTEST_DOESNT 
NOTTEST_DOESN_T  
NOTTEST_DONT 
NOTTEST_DON_T  
NOTTEST_WONT 
NOTTEST_WON_T  
NOTTEST_CANT  
NOTTEST_CAN_T 
NO 
QUESTION 
PLEASE 
EXCLAIM 
REALLY 
FOLLOW 
MESSAGING 
HAPPY_EMOTICON 
SAD_EMOTICON 
isHTTP 
isSPANISH 
 
 
 
 
 

J. Skinner, M. Ghiassi and D. Zimbra 
20
In the analysis of the Justin Bieber‟s Twitter feed we found that 30.31% of all messages 
pertained to follow requests, thus the explanatory value of the term “follow” cannot be 
ignored. We also found that 18.93% of all messages that mentioned @JustinBieber were re-
tweets (of an original message authored by Justin). The impact of Justin‟s own sentiment in 
re-tweet message must be handled appropriately as well. Identifying these elements in the 
model is surprisingly quite simple. To identify re-tweets the model simply has to look for 
“RT”, the acronym that appears before a re-tweeted sentence. To identify follow requests the 
model simply must search for occurrences of “follow” and the misspelled variant “folllow”. 
Analysis of the messages revealed that re-tweets of Justin Bieber‟s words did not have 
significant new information (apart from the original message). What little information was 
added to the original message had a strong positive bias as well. It is a trivial act to predict 
that these messages will be positive, and as such no further investigation of these messages 
was required. As previously mentioned, the corpus that was at the heart of our investigation 
subsequently had re-tweets filtered out. 
 
 
INCLUSION OF EMOTICONS 
 
The model includes the character combinations that are used to express sentiment on the 
service, commonly referred to as emoticons. While some researchers have dismissed 
emoticons as noisy and not containing adequate information, our analysis demonstrated that 
the model benefits from the information held in the emoticon data. 
The commonly employed happy and sad emoticons are presented in Table 3. Our feature 
set includes these commonly used emoticons. Each group of emoticons is represented by a 
single, distinct feature in the feature set. Emoticons are represented in the model by the 
features HAPPY_EMOTICON and SAD_EMOTICON. 
We offer a note on the neutral category. Using our strict interpretation of neutral 
sentiment (“unclear how the author feels about the brand”) our team of evaluators found that 
only 10.2% of Twitter messages referring to Justin Bieber were neutral. 
 
Table 3. Happy and Sad emoticons 
 
Happy Emoticons 
Sad emoticons 
:P 
:D 
:d 
:p 
:( 
;( 
:'( 
;'( 
;P 
;D 
;d 
;p 
=( 
={ 
): 
); 
:-) 
;-) 
:~) 
;~) 
)': 
)'; 
)= 
}= 
:<) 
:>) 
;>) 
=) 
;-{{ 
;-{ 
:-{{ 
:-{ 
=} 
:) 
(: 
;) 
:-( 
;-( 
:~-( 
;~-( 
(; 
:} 
{: 
;} 
:,( 
:'{ 
 
 
{; 
:] 
[: 
;] 
 
[; 
:') 
;') 
:-3 
;-3 
:-x 
;-x 
:-X 
;-X 
:-} 
;-} 
:-] 
;-] 
:-.) 
♥ 
☺ 
^_^ 
^-^ 
 
 

Twitter Specific Lexicon for Sentiment Analysis 
21
Since our use case focused on identifying the differences between strongly positive, 
mildly positive, mildly negative and strongly negative messages we decided to limit the SVM 
models to these four sentiment classifications. Tweets that SVM could not fit into one of the 
four sentiment classifications were considered neutral. 
Once the tweets are fully preprocessed and feature engineering techniques have been 
applied, the tweets are randomly divided into training and testing datasets and the 
classification process can begin. The classification strategies used for analysis are binary and 
multi-class categorization. In binary classification only two categories are present: either the 
tweet belongs to the class or it does not. This strategy is called “one vs. all” method and is 
used in this chapter. Most multi-class classification strategies are multiple iterations of binary 
classification. 
Finally, we note that the feature engineering approach introduced in this section has 
produced a reduced (general) Twitter specific and some brand (Bieber) specific lexicon. The 
reduction of features is substantial and will be shown to be still very effective. The reduced 
lexicon set offers the following benefits: 
 
1. The specialized set simplifies the modeling task by reducing complexity of the 
models (model size reduction). 
2. The reduced lexicon set still produces excellent coverage with very accurate results. 
3. Although, we only report results based on one dataset, the effectiveness measures 
(recall and accuracy values) are better than others reported in the literature for similar 
datasets/analysis. 
 
To show the effectiveness of the reduced lexicon set, we use the standard lexicon set 
traditionally employed by other researchers with SVM for our dataset and compare its 
effectiveness against the reduced lexicon set. Our results show that the reduced lexicon set is 
more effective while offering the benefits listed above. This conclusion is consistent with 
studies in other fields such as financial and accounting analysis that also relies on a reduced 
lexicon set [14, 31]. 
 
 
SENTIMENT SCALE 
 
We developed a scale that while precise it is still easy to supervise. The sentiment scale 
has sufficient steps such that a brand marketer can identify extremely positive and extremely 
negative statements regarding his or her product. It is expected that the extremely positive 
messages would be promoted by the brand, and the extremely negative messages would be 
referred to customer service to resolve. In light of this expected use case a broad neutral 
category is therefore not deemed desirable. 
In our scale we represent sentiment with a set of labels. We chose to use labels that 
traverse a linear scale (negative to positive) as opposed to labels that categorizes tweets into 
multiple moods. We find that a "positive to negative" scale is less hampered by interpretation 
bias. 

J. Skinner, M. Ghiassi and D. Zimbra 
22
We employ a scale that is more precise than the three step scale and easier to supervise 
than Lexalytics‟ 200 step scale. Our scale uses five steps [-2, -1, 0, 1, 2] to categorize 
sentiment. The definition of each step is as follows: 
 
2 = author clearly loves the brand (Strongly Positive) 
1 = author likes the brand (Mildly Positive) 
0 = unclear how author feels about the brand (Neutral) 
-1 = author dislikes the brand (Mildly Negative) 
-2 = author clearly hates the brand (Strongly Negative) 
 
The simplicity of this scale was of great benefit during the manual scoring process. We 
find that the easier it is to score a sentence the less chance a sentence will receive different 
scores by different human evaluators. 
 
 
TRAINING AND TESTING DATASETS AND ACCURACY METRICS 
 
We generated a manually classified dataset for this analysis. The dataset was randomly 
selected from the Twitter corpus. A total of 3440 tweets were manually classified by the three 
graduate students. We only used tweets that were similarly classified by all three. This 
resulted 2918 manually scored tweets. We next randomly partitioned this dataset into training 
(51%) and testing (49%) datasets. The training dataset was used to train the models and the 
testing dataset was kept out for independent testing of the trained model. As stated earlier, we 
used a “one vs. all” approach for classification. Therefore, we developed four separate 
models: one each for strongly positive, mildly positive, strongly negative, and mildly negative 
classes. The training dataset was used to calculate the starting point and training of each 
model. 
The training dataset is used to develop the four different models for twitter sentiment 
analysis. To gauge the effectiveness of the trained models the recall and accuracy for the 
prediction of each sentiment value was measured. The overall accuracy (across all sentiment 
buckets) was measured as well. The recall statistic for a given sentiment value X is defined 
as: 
(Number of tweets correctly predicted to have given sentiment value X) / (Total number 
of tweets with given sentiment value X) 
While the accuracy statistics for a given sentiment value X is defined as: 
(Number of tweets correctly predicted for X + number of tweets correctly predicted to not 
be in X)/ (Total number of tweets) 
We note that precision is not a valid metric to apply to this approach, as 100% of rows 
(Tweets) are categorized (every single row falls into a sentiment value). 
 
 
AUTOMATED, SUPERVISED SENTIMENT ANALYSIS 
 
As stated earlier, we consider Twitter sentiment analysis as a text classification problem. 
We selected the SVM method for this analysis. SVM is a supervised machine learning 

Twitter Specific Lexicon for Sentiment Analysis 
23
approach that requires a training dataset for its learning stage. Once each model is trained, 
they can be used to automatically provide sentiment associated with previously unseen input 
(tweets). 
Once the feature set is defined, in order to assess a tweet‟s or a corpus‟s sentiment, a 
functional form and a set of associated weights needs to be computed. Determining the 
functional form and such a set of weights is challenging. Generally, we define the twitter 
sentiment as: 
 
TSi = f ( Wj * Featurej ) 
 
where i = Tweet i, and j = feature, j = 1, …, 187 
In previous sections we introduced a feature engineering approach for term selection and 
a five point sentiment scale for tweets. To use SVM we needed to create training and testing 
datasets. Creation of these datasets requires human classification of tweets. We developed a 
tool that allowed three graduate students to manually classify tweets into one of the five 
classes by reading a tweet and using the guidelines and the tool workflow to classify it. The 
tool used by the graduate students for classification presented a single tweet at a time for 
review. The tweet was displayed in two different sections of the screen, at the same time, in 
two modes: 1) without emoticons and 2) with emoticons. This allowed the students to review 
the language of the tweet separate from the tweet‟s use of emoticons. This was done in order 
to prevent a quick, automatic classification on the part of the student, based solely on 
recognition of an emoticon. The student then had to answer 2 questions regarding the tweet 
(“is it English?” and “Does it contain sentiment?”) before the sentiment scale was presented 
for coding. Again, these workflow steps were introduced in order to force the classifier to 
seriously consider the content of the tweet and prevent a quick classification. 
An identical set of 3440 tweets were scored by each student. The guidelines for 
categorization were as follows: 
 
 
Strongly Positive – the author loves Justin Bieber or his music 
 
Mildly Positive – the author likes Justin Beiber or his music 
 
Neutral – the author has no opinion of Justin Bieber 
 
Mildly Negative – the author dislikes Justin Bieber or his music 
 
Strongly Negative – the author hates Justin Bieber or his music 
 
This experiment produced 2918 manually classified tweets for which the classifiers were 
in agreement. There was classification disagreement on 522 tweets (15.2%). 
We note that the human classification of tweets did not rely on nor had knowledge of the 
187 features introduced in the feature engineering section. 
 
 
SVM SENTIMENT ANALYSIS RESULTS 
 
This section presents the results of using SVM classification approach for Twitter 
sentiment analysis. The input to SVM includes the vector representation of the tweets from 
the training dataset and their corresponding manual classification. SVM uses the “one vs. all” 

J. Skinner, M. Ghiassi and D. Zimbra 
24
classification approach for each given sentiment class. We, therefore, developed four separate 
models; one for each sentiment class (Tweets with sentiment that do not belong to any of the 
four classes are considered as neutral). The models are allowed to train on 1450 records in the 
case of the Strongly Negative category and 1500 records in the case of the other categories. 
In order to ensure an even distribution between training and testing datasets, the Tweets 
are ordered by sentiment and then evenly divided between training and testing datasets (i.e. 1st 
row to training, 2nd row to testing, etc.). 
This left an unbalanced set of Strongly Negative tweets at the tail end of the record set. In 
order to better balance the test set for the Strongly Negative model, 50 records are moved 
from the training dataset to the test dataset prior to building the Strongly Negative model. 
This resulted in a Strongly Negative training dataset with 50 fewer tweets and a Strongly 
Negative test set with 50 additional tweets. 
The best SVM parameter values for each model are determined experimentally. Models‟ 
performances are then evaluated using the recall (defined as the number of records in the 
category that were correctly identified) and overall accuracy metrics (defined as the number 
of records in the category that were correctly identified + the number of records outside the 
category that were correctly identified as not belonging to the category). The distribution of 
the sentiment categories in the training and testing datasets are shown in Table 4. 
The SVM models were prepared using Thorsten Joachims‟ SVM Light application [26]. 
The SVM Light application ran in linear kernel mode with a 10 QP-sub problem setting. 
Models were constructed for each individual sentiment class (using “one vs. all” approach). 
The 187 individual features were used as the input to the SVM model. The format of the 
training data provided to SVM was ([positive example] [feature number]:[value] [feature 
number]:[value]…). An example of a training row associated with a mildly positive tweet is 
as follows: 
1 1:1 46:1 53:1 107:1 166:1 176:1 180:1 
The SVM model is trained on 1500 records and tested against 1418 records 
 
Table 4. Distribution of categories in the full dataset 
 
Category 
 
Category Distribution 
Strongly Positive 
56.0% 
Mildly Positive 
26.7% 
Mildly Negative 
11.7% 
Strongly Negative 
5.6% 
 
Table 5. Recall by Category 
 
Category 
SVM TEST 
SVM-OF TEST 
Strongly Positive 
94.1%* 
70.3% 
Mildly Positive 
0.0% 
0.0% 
 Mildly Negative 
25.1%* 
0.0% 
Strongly Negative 
19.3%* 
0.0% 
 SVM vs. SVM-OF *p<0.01 
 

Twitter Specific Lexicon for Sentiment Analysis 
25
Table 6. Accuracy by Category 
 
Category 
SVM TEST 
SVM-OF TEST 
Strongly Positive 
71.3%* 
67.4% 
Mildly Positive 
63.9% 
64.0% 
Mildly Negative 
88.3% 
87.3% 
Strongly Negative 
94.6%** 
93.7% 
 SVM vs. SVM-OF *p<0.05; **p<0.10 
 
The unbalanced distribution of categories in the dataset has a material impact on the 
ability of all models; especially in identifying tweets in the less frequently occurring 
categories. Table 5 presents SVM results using the recall metric for both the training and 
testing dataset. 
SVM exhibited relatively good recall values for three classes with the “Strongly Positive” 
and “Mildly Negative” classes showing the best results for the testing dataset (94.1 and 25.1% 
respectively). Table 6 presents results of SVM‟s performance using the accuracy metric. As 
indicated in Table 6, SVM performs well for all categories with the two “Negative” classes 
showing the best results for the testing dataset (88.3% and 94.6% respectively). 
The SVM model appears to be the most susceptible to the similarities between the 
overlapping mildly positive and strongly positive features (exhibiting the least accuracy in 
selecting mildly positive records). The greater number of strongly positive records influenced 
the model to more frequently choose to place borderline records into the strongly positive 
category (hurting its accuracy in the mildly positive bucket). 
Analysis of the results show that the SVM model struggles to identify the tweets 
associated with less frequently occurring categories (i.e. they could not find the needle in the 
haystack). The SVM models more often predicted that the tweet did not fit into the less 
frequently occurring category (i.e. they choose “haystack”). This approach, however, helped 
overall accuracy as more often than not the “haystack” was the correct choice. 
 
 
TWITTER SPECIFIC LEXICON EFFECTIVENESS 
 
The feature engineering approach introduced earlier resulted in a reduced Twitter specific 
lexicon with only 187 features. Of the 187, six were found to be brand specific (Bieber 
specific) while the remaining 181 were generic to all Twitter messages. We also showed that 
one or more of the terms in the reduced lexicon set were still present in more than 97.3% of 
the entire Twitter corpus used in this chapter (10,345,184 Tweets). To show the effectiveness 
of the reduced Twitter specific features for sentiment analysis, we developed SVM models 
using a traditional sentiment lexicon from the Opinion Finder system [50] to define the input 
feature space. We selected Opinion Finder and SVM because these are widely used sentiment 
analysis tools applied by researchers and are readily available. The Opinion Finder lexicon is 
comprised of 7630 terms expressing positive or negative sentiments. Tables 5 and 6 present 
the results of running the SVM model with Opinion Finder‟s lexicon and the associated recall 
and accuracy values (columns SVM-OF Test). The results show that the reduced lexicon set 
prepared using our approach indeed offers more favorable values for recall and accuracy in 

J. Skinner, M. Ghiassi and D. Zimbra 
26
three of the four sentiment classes. In particular the recall values for the Opinion Finder 
lexicon models are significantly lower than the corresponding values for the SVM models 
based upon the reduced lexicon set. We note that the SVM model based on the Opinion 
Finder‟s lexicon set was only effective in recall for the “Strongly Positive” category. Even for 
this category, the SVM model based on the Twitter specific lexicon performance shows 
improvement of more than 33.8% in recall over its corresponding Opinion Finder value. 
Clearly, the ineffectiveness of the Opinion Finder‟s model for the two “Negative” categories 
is an additional evidence for the development of the Twitter specific lexicon set. While the 
performances of the two lexicons are closer when comparing the results using the accuracy 
metric, our reduced lexicon produced improvements in three of the four sentiment classes, 
with the “Strongly Positive” category showing the most improvement (5.79%) over its 
corresponding Opinion Finder‟s model. To assess the statistical significance of the 
performance improvements associated with application of the Twitter specific lexicon to 
define the feature space for the SVM models in comparison with using the benchmark 
Opinion Finder lexicon, pair-wise t-tests were conducted (n=1418; alpha=0.05; two-tailed 
test). The results of these pair-wise t-tests are indicated in Tables 5 and 6 using asterisks (*). 
In three of the four sentiment classes, the improvements in recall provided by the devised 
lexicon were highly significant with p<0.01. While the performances of the two lexicons were 
closer in terms of accuracy, in the “Strongly Positive” and “Strongly Negative” sentiment 
classes the improvement in accuracy were statistically significant with p<0.05 and p<0.01 
respectively. Overall, the results show that the reduced Twitter specific lexicon set is very 
effective for Twitter sentiment analysis as measured in this experiment. Finally, the choice of 
using widely available modeling tools (e.g. SVM and Opinion Finder) should enable other 
researchers to use the reduced feature set to further validate its effectiveness. 
 
 
CONCLUSION 
 
This chapter makes several contributions to twitter sentiment analysis. Earlier research on 
Twitter classification has classified factual sounding tweets as a neutral tweet [23]. Using this 
approach, they state that “more than 80%” of tweets contain no sentiment. Our approach to 
sentiment analysis has increased sensitivity, accounting for tweets with mild sentiment 
(positive and negative), resulting in a more accurate identification of the neutral category. 
Using our approach, we find the number of neutral cases to be closer to 10%. Additionally, 
where some previous investigations have limited or removed emoticons from consideration, 
this research finds emoticons to have high explanatory power. And finally, we find the “re-
tweet” class to be a distinct class, separate from direct sentiment and much more easily 
characterized in terms of sentiment. 
Our process resulted in a more accurate estimation of sentiment. To begin with, data 
gathering tools were developed that allowed for an extremely large corpus, while scoring 
tools allowed for a large training and testing datasets. Next, our feature engineering approach 
resulted in a Twitter specific lexicon with a reduced a feature set with the ability to still 
characterize 97.3% of all messages in the 10,345,184 tweet corpus. The smaller, Twitter 
specific feature set reduces problem complexity (model size reduction), maintains a high 

Twitter Specific Lexicon for Sentiment Analysis 
27
degree of coverage over the Twitter corpus, and yields improved sentiment classification 
accuracy. 
The use of a highly explanatory n-gram feature set in this chapter offers additional tools 
for brands to recognize emerging issues with their brand identity. The tools allow brands to 
monitor key sentiment indicators on the Twitter service (i.e. follow request sentiment, re-
tweet sentiment and message sentiment). These metrics, when used to influence brand 
decisions (brand offerings, frequency of brand messaging, timing of messaging, type of brand 
messaging, brand reactions to external factors) will allow brand managers to make better use 
of the Twitter service and best influence public perception. 
 
 
REFERENCES 
 
[1] 
Abbasi, A., Chen, H., and Salem, A. (2008). Sentiment analysis in multiple languages: 
feature selection for opinion classification in web forums. ACM Transactions on 
Information Systems, 26(3), 12, 1 - 34. 
[2] 
Agarwal, A., Xie, B., Vovsha, I., Rambow, O., and Passonneau, R. (2011). Sentiment 
Analysis of Twitter Data. Proceeding of ACL HLT Conference, 30 - 38. 
[3] 
Aue, A., and Gamon, M. (2005). Customizing sentiment classifiers to new domains: a 
case study. Proceeding of the Intl. Conference on Recent Advances in Natural 
Language Processing. Borovets, BG. 
[4] 
Bakshy, E., Hofman, J., Mason, W., and Watts, D. (2011). Everyone‟s an Influencer: 
Quantifying Influence on Twitter. Proceeding of ACM WSDM Conference Hong 
Kong, China. 
[5] 
Barbosa, L., and Feng, J. (2010). Robust Sentiment Detection on Twitter from Biased 
and Noisy Data. Proceedings of the 23rd International Conference on Computational 
Linguistics (COLING‟10), 36 – 44. 
[6] 
Bermingham, A., and Smeaton, A. (2010). Classifying Sentiment in Microblogs: Is 
Brevity an Advantage? Proceeding of ACM CIKM Conference, Toronto, Ontario, 
Canada, 1833 – 1836. 
[7] 
Bermingham, A., and Smeaton, A. (2011). On Using Twitter to Monitor Political 
Sentiment and Predict Election Results. Proceeding of IJCNLP Conference, Chiang 
Mai, Thailand. 
[8] 
Bifet, A., and Frank, E. (2010). Sentiment Knowledge Discovery in Twitter Streaming 
Data. Proceeding of 13th international conference on Discovery Science Conference,  
1 - 15. 
[9] 
Blitzer, J., Dredze, M., and Pereira, F. (2007). Biographies, bollywood, boom-boxes 
and blenders: domain adaptation for sentiment classification. Proceeding of the Annual 
Meetings of the Association for Computational Linguistics, 440 - 447. 
[10] Bollen, J., Mao, H., and Zeng, X. (2011a). Twitter Mood Predicts the Stock Market. 
Journal of Computational Science, 2(1), 1-8. 
[11] Bollen, J., Pepe, A., and Mao, H. (2011b). Modeling public mood and emotion: Twitter 
sentiment and socio-economic phenomena. Proceedings of the Fifth International AAAI 
Conference on Weblogs and Social Media (ICWSM 2011), July, Barcelona, Spain,  
1-10. 

J. Skinner, M. Ghiassi and D. Zimbra 
28
[12] Cha, M., Haddadi, H., Benevenuto, F., and Gummadi, K. (2010). Measuring User 
Influence in Twitter: The Million Follower Fallacy. Proceeding of 4th AAAI 
Conference on Weblogs and Social Media, Washington DC, 10 - 17. 
[13] Chung, J. and Mustafaraj, E. (2011). Can Collective Sentiment Expressed on Twitter 
Predict Political Elections? Proceedings of the Twenty-Fifth AAAI Conference on 
Artificial Intelligence, 1770 - 1771. 
[14] Das, S., and Chen, M. (2007). Yahoo! for Amazon: sentiment extraction from small talk 
on the web. Management Science, 53(9), 1375 - 1388. 
[15] Dave, K., Lawrence, S., and Pennock, D. (2003). Mining the peanut gallery: opinion 
extraction and semantic classification of product reviews. Proceeding of 12th Intl. 
Conference on the WWW, 519 - 528. 
[16] Davidov, D., Tsur, O., and Rappoport, A. (2010). Enhanced Sentiment Learning Using 
Twitter Hashtags and Smileys. Proceeding of COLING Conference'10, Beijing, China, 
241 -249. 
[17] Frakes, W. B., and Baeza-Yates, R. (1992). Information Retrieval: Data Structures and 
Algorithms. Prentice Hall. 
[18] Gamon, M. (2004). Sentiment classification on customer feedback data: noisy data, 
large feature vectors, and the role of linguistic analysis. Proceeding of the 20th Intl. 
Conference on Computational Linguistics, 84. 
[19] Ghiassi, M., and Burnley, C. (2010). Measuring Effectiveness of a Dynamic Artificial 
Neural Network Algorithm for Classification Problems. Expert Systems with 
Applications, 37(4), 3118–3128. 
[20] Ghiassi, M., Olschimke, M., Moon, B., and Arnaudo, P. (2012). Automated Text 
Classification using a Dynamic Artificial Neural Network Model. Expert Systems with 
Applications, 39(12), 10967-10976. 
[21] Ghiassi, M., and Saidane, H. (2005). A dynamic architecture for artificial neural 
network. Neurocomputing, 63, 397–413. 
[22] Ghiassi, M., Saidane, H., and Zimbra, D. K. (2005). A dynamic artificial neural 
network model for forecasting time series events. International Journal of Forecasting, 
21, 241–362. 
[23] Go, A., Bhayani, R., and Huang, L. (2009). Twitter sentiment classification using 
distant supervision. Technical report, Stanford Digital Library Technologies Project.  
1-6. 
[24] Jansen, B., Zhang, M., Sobel, K., and Chowdury, A. (2009). Twitter Power: Tweets as 
Electronic Word of Mouth. Journal of the American Society for Information Science 
and Technology, 60(11), 2169-2188. 
[25] Jiang, L., Yu, M., Zhou, M., Liu, X., and Zhao, T. (2011). Target-dependent Twitter 
Sentiment Classification. Proceeding of ACL Conference , 151 - 160. 
[26] Joachims, T. (1999). Making large-scale SVM learning practical. In B. Schölkopf and 
C. Burges and A. Smola (ed.), Advances in Kernel Methods - Support Vector Learning, 
MIT-Press, 41-56. 
[27] Kajanan, S., Shafeeq Bin Mohd Shariff, A., Datta, A., Dutta, K., and Paul, D. (2011). 
Twitter Post filter for Mobile Applications. Proceedings of the 21st Workshop on 
Information Technology and Systems, 1-6. 
[28] Kim, S., and Hovy, E. (2004). Determining the sentiment of opinions. Proceeding of the 
Intl. Conference on Computational Linguistics, Geneva, Switzerland, 1 - 8. 

Twitter Specific Lexicon for Sentiment Analysis 
29
[29] Kouloumpis, E., Wilson, T., and Moore, J. (2011). Twitter Sentiment Analysis: The 
Good the Bad and the OMG! Proceeding of AAAI Conference on Weblogs and Social 
Media, 538 - 541. 
[30] Lerman, K., and Ghosh, R. (2010). Information Contagion: An Empirical Study of the 
Spread of News on Digg and Twitter Social Networks. Proceeding of 4th International 
AAAI Conference on Weblogs and Social Media, Washington DC, 90 - 97. 
[31] Loughran, T. and McDonald, B. (2011).When is a Liability Not a Liability? Textual 
Analysis, Dictionaries, and 10-Ks. The Journal of Finance, 66(1), 35-65. 
[32] Manning, C. D., Raghavan, P., and Schütze, H. (2008). Introduction to Information 
Retrieval, Cambridge University Press 
[33] Mathioudakis, M., and Koudas, N. (2010). TwitterMonitor: Trend Detection over the 
Twitter Stream. Proceeding of ACM SIGMOD Conference, 1155 -1158. 
[34] Mitchell, T. (2005). Reducing Data Dimension. Machine Learning 10-701, Carnegie 
Mellon 
University, 
http://www.cs.cmu.edu/~guestrin/Class/10701-S05/slides/ 
dimensionality.pdf, 1 - 40. 
[35] Naveed, N., Gottron, T., Kunegis, J., and Alhadi, A. (2011). Bad News Travel Fast: A 
Content-based Analysis of Interestingness on Twitter. Proceeding of 3rd ACM WebSci 
Conference, Koblenz, Germany. 
[36] O‟Connor, B., Balasubramanyan, R., Routledge, B., and Smith, N. (2010). From 
Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series. Proceeding of 
4th AAAI Conference on Weblogs and Social Media, Washington DC, 122 - 129. 
[37] Pak, A., and Paroubek, P. (2010). Twitter as a Corpus for Sentiment Analysis and 
Opinion Mining. Proceedings of the Seventh International Conference on Language 
Resources and Evaluation (LREC'10), 1320-1326. 
[38] Pang, B., Lee, L., and Vaithyanathan, S. (2002). Thumbs up? Sentiment classification 
using machine learning techniques. Proceedings of the Conference on Empirical 
Methods in Natural Language Processing, 79-86. 
[39] Petrovic, S., Osborne, M., and Lavrenko, V. (2010). Streaming First Story Detection 
with Application to Twitter. Proceeding of NAACL Conference, 181 - 189. 
[40] Phelan, O., McCarthy, K., and Smyth, B. (2009). Using Twitter to Recommend Real-
Time Topical News. Proceeding of ACM RecSys Conference, 385 - 388. 
[41] Romero, D., Meeder, B., and Kleinberg, J. (2011). Differences in the Mechanics of 
Information Diffusion Across Topics: Idioms, Political Hashtags, and Complex 
Contagion on Twitter. Proceeding of the 20th ACM International World Wide Web 
Conference on, Hyderabad, India. 
[42] Rui, H., Liu, Y., and Whinston, A. B. (2011). Whose and What Chatter Matters? The 
Impact of Tweets on Movie Sales. NET Institute Working Paper No. 11-27, October,  
1-30. 
[43] Saif, H., He, Y., and Alani, H. (2011). Semantic Smoothing for Twitter Sentiment 
Analysis. Proceeding of International Semantic Web Conference (ISWC). 
[44] Saif, H., He, Y., and Alani, H. (2012). Alleviating Data Sparsity for Twitter Sentiment 
Analysis. Proceeding of the 21st ACM International World Wide Web Conference, 
Lyon, France, 2 - 9. 
[45] Tan, S., Wu, G., Tang, H., and Cheng, X. (2007). A novel scheme for domain-transfer 
problem in the context of sentiment analysis. Proceeding of the 16th ACM conference 
on information and knowledge management (CIKM), 979 – 98. 

J. Skinner, M. Ghiassi and D. Zimbra 
30
[46] Tetlock, P. (2007). Giving content to investor sentiment: the role of the media in the 
stock market. Journal of Finance, 62(3), 1139 – 1168. 
[47] Thelwall, M., Buckley, K., and Paltoglou, G. (2011). Sentiment in Twitter Events. 
Journal of the American Society for Information Science and Technology, 62(2), 406-
418. 
[48] Tumasjan, A., Sprenger, T., Sandner, P., and Welpe, I. (2010). Predicting Elections 
with Twitter: What 140 Characters Reveal about Political Sentiment. Proceedings of the 
Fourth International AAAI Conference on Weblogs and Social Media, 178 - 185. 
[49] Turney, P. (2002). Thumbs up or thumbs down? Semantic orientation applied to 
unsupervised classification of reviews. Proceedings of the 40th Annual Meeting on 
Association for Computational Linguistics, 417 – 424. 
[50] Wilson, T., Hoffman, P., Somasundaran, S., Kessler, J., Wiebe, J., Choi, Y., Cardie, C., 
Riloff, E., and Patwardhan, S. 2005. OpinionFinder: A System for Subjectivity 
Analysis. Proceedings of Conference on Human Language Technology and Empirical 
Methods in Natural Language Processing, 34-35. 
[51] Yang, Y., and Pedersen, J. O. (1997). A comparative study on feature selection in text 
categorization. Proceedings of ICML-97, 14th international conference on machine 
learning, 412–420. 
[52] Zhang, L., Ghosh, R., Dekhil, M., Hsu, M. and Liu, B. (2011). Combining Lexicon-
based and Learning-based Methods for Twitter Sentiment Analysis. Hewlett-Packard 
Labs Technical Report HPL-2011-89. 
 

In: Recent Advances in Artificial Intelligence Research  
ISBN: 978-1-62808-807-6 
Editors: Ambrogio Bacciga and Renato Naliato 
© 2013 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 2 
 
 
 
HYBRID UNSUPERVISED-SUPERVISED ARTIFICIAL 
NEURAL NETWORKS FOR MODELLING ACTIVATED 
SLUDGE WASTEWATER TREATMENT PLANTS 
 
 
Rabee Rustum1, and Adebayo J. Adeloye2,† 
1School of the Built Environment, Heriot-Watt University (Dubai Campus),  
Dubai, UAE 
2School of the Built Environment, Heriot-Watt University, Edinburgh,  
Scotland, United Kingdom 
 
 
ABSTRACT 
 
Mathematical modelling of wastewater treatment plant process is important for 
improving its treatment efficiency and thus the quality of the effluent released into the 
receiving water body. However, due to the highly complex and non-linear characteristics 
of this biological system, traditional mathematical modelling of the treatment process has 
remained a challenge. This work presents a hybrid modelling strategy based on the 
Kohonen Self Organising Map (KSOM) and feed-forward, back-propagation artificial 
neural networks (BP-ANN) for modelling the activated sludge wastewater treatment 
plant. The hybrid approach involved a 2-stage process: firstly, the KSOM was used for 
data preparation, visualisation of high dimensional data and features extraction; and 
secondly, these features were then used for the training and validation of the BP-ANN. 
Comparison of this hybrid modelling approach against the straight modelling of the 
original raw data using BP-ANN showed that the hybrid approach resulted in much more 
improved model performance. 
The study demonstrated that the hybrid modelling strategy offers viable, flexible and 
robust modelling methodology for effectively handling noisy data for environmental 
systems modelling. 
 
Keywords: KSOM, ANN, modelling of activated sludge systems 
 
                                                        
 E-mail: r.rustum@hw.ac.uk. 
† E-mail: a.j.adeloye@hw.ac.uk. Tel. +44 131 4518236; Fax. +44 1314514617. 

Rabee Rustum and Adebayo J. Adeloye 
32
1. INTRODUCTION 
 
Increased regulations, through such as the EU urban wastewater treatment directive 
(EEC, 1991), to protect the environment and water bodies have led to growing demands to 
reduce point source pollution impacts on the quality of receiving water ecosystems. 
Compounding the problems is increasing plant loadings due to the growth of urban areas 
which means that existing facilities are now operating close to the limit of their design 
capacity. 
In addition, the privatisation of water industry in the UK, for example, has led to 
increased pressures for efficient design and operation of wastewater treatment plants, and 
other cost saving initiatives. 
Achieving the desired protection and/or enhancement of receiving water quality will be 
either by improving the performance of existing wastewater treatment plants or the 
construction of new facilities. The latter approach is costly, as the capital expenditure 
required for the construction of new wastewater treatment facilities is very high and the 
required land may not be available due to planning and environmental constraints. The time 
scale involved is also such that this option is often not feasible in the short-to-medium term. 
Hence, the former approach, which if properly done, can improve effluent water quality, 
reduce the need of chemicals and save energy and operational costs (Olsson et al., 2005; 
Mjalli et al., 2007). 
Therefore, sustainable solution to the problems of wastewater treatment will require the 
development of adequate information system for control and supervision of the process. 
However, because of variations in raw wastewater composition, as well as the changing and 
complex nature of the biological system of the activated sludge process, the operation and 
control of activated sludge wastewater treatment plants are quite complicated (Pu and Hung, 
1995). 
This reality has encouraged environmental engineers to use and develop new modelling 
techniques to improve plant operation and control by designing operational control systems 
for qualitative and quantitative description of the dynamic behaviour of treatment plants. 
Such systems help the process engineer to convert unsatisfactory dynamic behaviours into 
satisfactory behaviours, thus reducing operational costs for meeting the requirement of 
regulatory agencies and minimising any adverse effects on the environment. 
Indeed, the mathematical modelling of the activated sludge process is a useful tool for its 
optimal control, mainly because the effects of adjusting the operating variables can be studied 
far more quickly on a computer than by doing experiments. Hence, many alternative designs 
and operational strategies can be compared without the need for physical trials of each 
scenario (Olsson, 2005; Rustum and Adeloye, 2007; Rustum et al., 2008). By using these 
models to simulate the effect of different correction actions, it is possible to rapidly respond 
to any change in the process, and to devise an operational strategy, which can move the plant 
to new operating condition that improves its stability, the quality of the effluent and thus 
achieving significant reduction in the running costs. In this way, optimum process 
configurations, which meet given effluent quality standards at least cost, can be achieved 
(Olsson, 2005; Rivas et al., 2008). 
However, modelling the wastewater treatment process is not without its problems, 
namely that: 

Hybrid Unsupervised-Supervised Artificial Neural Networks … 
33
 
The process is time varying, consisting of many sub-processes with strong dynamics 
of various scales. 
 
The process has a complex multivariable nature, resulting in large amount of data 
collected by the measurement system. This high dimensionality makes it difficult to 
decide what should be considered as inputs and outputs. 
 
As a biological process, there is a lack of reliable on-line measurement instruments 
for some of the important variables such as the biochemical oxygen demand (BOD), 
which are difficult to measure in real time. In addition, many sensors are not reliable 
because they are noisy, have long response time, require frequent maintenance and 
can drift (Steyer et al., 1999; Olsson, 2005). Consequently, most of the available data 
records have lots of missing and/or erroneous values (Rustum and Adeloye, 2007). 
 
Many factors that affect the process are not routinely monitored in most wastewater 
treatment plants. The process is subject to large unpredictable conditions such as the 
effect of toxic industrial materials, mechanical breakdowns (Manfred et al., 2002), 
some of which are quite difficult to formulate mathematically. 
 
In general, no two wastewater treatment works are the same, with variations existing 
in both the size and circumstances. The nature of the industrial waste inputs is also 
site-specific and climate differences are also to be expected. All these warrant 
considering the specific features and circumstances of each wastewater treatment 
works that is to be simulated using mathematical models. 
 
Despite all of the above, several efforts have been devoted to the modelling of the 
activated sludge process using mechanistic models as summarised by Manfred et al. (2002). 
However, most of the models have been proposed to simulate the dynamic behaviour of the 
biological reactor and the secondary settler as if they were separable, independent units; very 
few models have looked at the interconnection between these two units (Hamed et al., 2004). 
Thus, while models are vital for the effective control of wastewater treatment plants, the 
limitations of the currently available models necessitate more investigations in this field. One 
possibility, which has received increased attention recently, is the use of artificial intelligence 
(AI) modelling techniques. AI approaches are suitable for modelling the complex activated 
sludge process due to their learning ability to construct nonlinear relationships that can 
explain the complex relationships within the data without the difficult task of dealing with 
deterministic non-linear mathematics (Hamed et al., 2004). In addition, AI techniques can 
deal with the complexity and uncertainty of the system in a manner similar to the human way 
of thinking and reasoning. AI models also have the ability to generalize the input-output 
relationship and thus to produce reliable estimates of the output variable when presented with 
previously unseen inputs. Although that might be possible with properly calibrated and 
validated mechanistic models, mechanistic models require so much data that their effective 
calibration and validation are always difficult, if not impossible. There is also the uncertainty 
associated with model identification for mechanistic models, i.e. the exact form of the 
functional relationship is unknown and so whatever mathematical expression is postulated for 
the unknown relationship, is bound to be a mere approximation. AI techniques on the other 
hand are data-driven techniques and there is no requirement to specify the mathematical form 
of the relationship being modelled a priori. 

Rabee Rustum and Adebayo J. Adeloye 
34
Among the commonly used AI tools and techniques are those belonging to the broad 
group known as artificial neural networks, ANNs. The two most commonly used in the group, 
based on their learning mechanism, are the Kohonen Self-Organising Map (KSOM), which 
uses unsupervised learning, and the back-propagation, multilayer perceptron artificial neural 
networks which use supervised learning. Both of these have played an important role in the 
development of models for complex environmental systems (Kohonen, 2001; Cinar, 2005), 
including the planning of water resources systems, as for example, in predicting reservoir 
storage-yield-reliability relationships (Adeloye and DeMunari, 2006); for predicting the 
monthly values of water quality parameters in rivers (Diamantopoulou et al., 2005); and for 
modelling complex wastewater treatment systems (e.g. Mjalli et al. (2007), Hamed et al. 
(2004), Du et al. (1999), Raduly et al. (2007), Chen et al. (2003), Gamal-El-Din and Smith 
(2002)). However, for this last group of applications, attention has focused on supervised 
neural networks and there has been less interest in exploring the unsupervised Kohonen self-
organising map or (KSOM) in the field of wastewater treatment, apart from some few some 
notable exceptions (e.g. Hong et al., 2003; Garcia and Gonzalez, 2004; Cinar, 2005; Gonzalez 
and Garcia, 2006). 
The aim of this work is to provide a systematic and thorough approach to the 
development of artificial intelligence technique in modelling and monitoring the activated 
sludge wastewater treatment plants and to show the potential of hybrid, integrated 
unsupervised-supervised ANN modelling approach. In the next Section, further elaboration of 
ANNs as a modelling tool is given, focusing on supervised and un-supervised learning for the 
purpose of training. Then the application of the modelling paradigm to a wastewater treatment 
plant in Edinburgh as case study is described. The results are then presented and discussed, 
followed by the conclusion. 
 
 
2. ARTIFICIAL NEURAL NETWORKS 
 
An artificial neural network (ANN) is a mathematical model or a form of computing 
algorithm inspired by the functioning of the biological nervous system. In mathematical 
terms, neural networks are non-linear statistical data modelling tools used to model complex 
relationships between inputs and outputs or to find patterns in data. In most cases, an ANN is 
an adaptive system that changes its structure based on external or internal information that 
flows through the network during the learning phase. In other words, knowledge is acquired 
by the network through a learning process and the inter connections between the elements of 
the network store the knowledge (Arbib, 2003). 
ANN is inspired by knowledge from neuroscience but it draws its methods from 
statistical physics (Arbib, 2003). The fundamental aspects of ANNs are the use of simple 
processing elements, which are models of the neurons in the brain. These elements (neurons) 
are then connected together in a well-structured way. The network then is taught, to achieve a 
particular task or function of interest, by patterns of data presented, such that it can 
subsequently not only recognise such patterns when they occur again, but also recognise 
similar patterns by generalisation (Abrahart et al., 2004). 
In other words, an ANN is a parallel-distributed information processing system that has 
certain performance characteristics resembling biological neural networks of the human brain 

Hybrid Unsupervised-Supervised Artificial Neural Networks … 
35
(Abrahart et al., 2004). The mathematical development of ANN is based on the following 
rules: 
 
1. Information processing occurs at many single elements called nodes, also referred to 
as units, cells, or neurons. 
2. Signals are passed between nodes through connection links. 
3. Each connection link has an associated weight that represents its connection strength. 
4. Each node typically applies a nonlinear transformation called an activation function 
to its net input to determine its output signal. 
 
Thus, a neural network is characterized by its architecture that represents the pattern of 
connection between nodes, its method of determining the connection weights, and the 
activation function. A typical ANN consists of a number of nodes that are organized 
according to a particular arrangement as shown in Figure 1. 
One way of classifying neural networks is by the number of layers: single and multilayer. 
In a feed-forward network, the nodes are generally arranged in layers, starting from a first 
input layer and ending at the final output layer. There can be several hidden layers, with each 
layer having one or more nodes. Information passes from the input to the output side. The 
nodes in one layer are connected to those in the next, but not to those in the same layer. 
Thus, the output of a node in a layer is only dependent on the inputs it receives from 
previous layers and the corresponding weights. 
 
 
Figure 1. Feedforward Three Layer ANN. 

Rabee Rustum and Adebayo J. Adeloye 
36
In most networks, the input (first) layer receives the input variables for the problem at 
hand. This input later thus consists of all quantities that can influence the output. The last or 
output layer consists of values predicted by the network and thus represents model output. 
The number of hidden layers and the number of nodes in each hidden layer are usually 
determined by a trial-and-error procedure. The schematic in Figure 1 has a single hidden 
layer. 
 
 
2.1. Mathematical Aspects of ANN 
 
A schematic diagram of a typical j-th node is shown in Figure 2. The inputs to such a 
node may come from system causal variables or outputs of other nodes, depending on the 
layer that the node is located in. These inputs form an input vector X = (x1, . . . , xi, . . . , xn). 
The sequence of weights leading to the node form a weight vector Wj = (w1j, . . . , wij,. . . , 
wnj), where wij represents the connection weight from the i-th node in the preceding layer to 
this node. 
The output of node j, 
'
j
y , is obtained by computing the value of the activation function 
with respect to the inner product of vector X and Wj minus bj, where bj is the threshold value, 
also called the bias, associated with this node, i.e.: 
 
 
(1) 
 
where function f (.) is called an activation function, whose functional form determines the 
response of a node to the total input signal it receives. 
 
 
Figure 2. Schematic Diagram of Node j. 


j
j
j
b
W
X
f
y


.
'

Hybrid Unsupervised-Supervised Artificial Neural Networks … 
37
The most commonly used form of f (.) is the sigmoid function, given as: 
 
 
(2) 
 
The sigmoid function is a bounded, monotonic, non-decreasing function that provides a 
graded, nonlinear response. This function enables a network to map any nonlinear process. 
The popularity of the sigmoid function is partially attributed to the simplicity of its derivative 
that will be used during the training process. 
 
 
2.2. Network Training 
 
In order for an ANN to generate an output vector 
)
,...,
,
(
'
,
'
2
,
'
1,
'
p
i
i
i
y
y
y
Y 
 that is as 
close as possible to the target vector 
)
,...,
,
(
,
2
,
1,
p
i
i
i
y
y
y
Y 
, a training process, also called 
learning, is employed to find optimal weight matrices W and bias vectors b, that minimize a 
predetermined error function that usually has the form: 
 
 
(3) 
 
In equation (3), 
j
iy , is the observed (i.e. the target) i-th element of the j-th output 
variable, 
'
, j
iy
 is the corresponding value predicted by the model, p is the number of output 
variables (i.e. the number of output nodes) and n is the number of observations for each 
output variable. In general, there are two types of training: the supervised and un-supervised. 
 
2.2.1. Supervised Training Algorithm 
A supervised training algorithm requires an external teacher to guide the training process. 
This typically implies that a large number of examples (or patterns) of inputs and outputs are 
required for training. The inputs are cause variables of a system and the outputs are the effect 
variables. This training procedure involves the iterative adjustment and optimization of 
connection weights and threshold values for each of nodes until the error function in Equation 
(3) is minimised by searching for a set of connection strengths and threshold values that cause 
the ANN to produce outputs that are equal or close to targets. After training has been 
accomplished, it is hoped the ANN is then capable of generating reasonable results given new 
inputs. 
Back-propagation is perhaps the most popular algorithm for training ANNs (Abrahart et 
al., 2004). It is essentially a gradient descent technique that minimizes the network error 
function in Equation (3). Each input pattern of the training data set is passed through the 
network from the input layer to the output layer. The network output is compared with the 
t
e
t
f


1
1
)
(







p
j
n
i
j
i
j
i
y
y
E
1
1
2
'
,
,

Rabee Rustum and Adebayo J. Adeloye 
38
desired target output, and an error is computed based on Equation (3). This error is 
propagated backward through the network to each node, and correspondingly the connection 
weights are adjusted based on Equation (4): 
 
 
(4) 
 
where ∆wij (n) and ∆wij (n - 1) = weight increments between node i and j during the nth and  
(n -1)-th pass, or epoch. ε and α are called learning rate and momentum, respectively. 
A similar equation is written for correction of bias values. The momentum factor can 
speed up training in very flat regions of the error surface and help prevent oscillations in the 
weights. The learning rate is used to increase the chance of avoiding the training process 
being trapped in a local minimum instead of the global minima. The above assumes that the 
architecture of the network, i.e. the number of hidden layers and nodes/layer, is known. 
However, this is not true and the best architecture has to be established as part of the training 
process. In general, it has been shown that one hidden layer such as that in Figure 1, having 
sufficient number of neurons, can approximate any complex relationships and thus most 
ANNs comprise one hidden layer. The number of neurons in this single hidden layer is 
determined by trial and error (Adeloye and DeMunari, 2006). 
 
2.2.2. Unsupervised Training Algorithm 
In contrast to supervised training algorithms, an unsupervised training algorithm does not 
involve a teacher. During training, only an input data set is provided to the ANN that 
automatically adapts its connection weights to cluster those input patterns into classes with 
similar properties. The most commonly used unsupervised ANN is the Kohonen self-
organising map shown in Figure 3 (Kohonen et al. 1996). It is usually presented as a 
dimensional grid or map whose units (i.e. nodes or neurons) become tuned to different input 
data patterns. 
The principal goal of the KSOM is to transform an incoming signal pattern of arbitrary 
dimension into a two-dimensional discrete map in such a way that similar patterns are 
represented by the same output neurons, or by one of its neighbours (Back et al., 1998). In 
this way, the KSOM can be viewed as a tool for reducing the amount of data by clustering, 
thus converting complex, nonlinear statistical relationship between high dimensional data into 
simple relationship on low dimensional display (Kohonen et al., 1996; Zhang, 2009). This 
mapping roughly preserves the most important topological and metric relationship of the 
original data elements, implying that the KSOM translates the statistical dependences 
between the data into geometric relationships, whilst maintaining the most important 
topological and metric information contained in the original data. Hence, not much 
information is lost during the mapping. In addition, similarities in relationship within the data 
and clusters can be visualised in a way that enables the user to explore and interpret the 
complex relationship within the data set. 
KSOM algorithms are based on unsupervised competitive learning, which means that 
training is entirely data driven and the neurons or nodes on the map compete with each other. 
In contrast to supervised neural networks, which require that target values corresponding to 
)1
(
.
)
(








n
w
w
E
n
w
ij
ij
ij



Hybrid Unsupervised-Supervised Artificial Neural Networks … 
39
input vectors are known, KSOM does not require the desired output to be known, hence, no 
comparisons are done to predetermine the ideal responses. During training, only input 
patterns are presented to the network which automatically adapts the weights of its 
connections to cluster the input patterns into groups with similar features (Obu-Can, 2001; 
Astel et al., 2007). 
Thus, the KSOM consists of two layers: the multi-dimensional input layer and the 
competitive or output layer; both of these layers are fully interconnected as illustrated in 
Figure 3. The output layer consists of M neurons arranged in a two-dimensional grid of 
nodes. Each node or neuron i (i = 1,2,…,M) is represented by an n-dimensional weight or 
reference vector Wi= [wi1,….,win]. The weight vectors of the KSOM form a codebook. The M 
nodes can be ordered so that similar neurons are located together and dissimilar neurons are 
remotely located on the map. 
To train the KSOM, the multi-dimensional input data are first standardized by deducting 
the mean and subsequently dividing the results by the corresponding standard deviation. Each 
of the M nodes of the map is seeded with a vector of randomly generated standardised values; 
the dimension of these weight vectors is equal to that of each of the input vector. Then a 
standardized input vector is chosen at random and presented to each of the individual map 
nodes or neurons for comparison with their code vectors in order to identify the code vector 
most similar to the presented input vector. The identification uses the Euclidian distance, 
which is defined in Equation (5): 
 
 
(5) 
 
 
Figure 3. Illustration of the winning node and its neighbourhood in the Kohonen Self-organizing Map. 


M
k
w
x
D
m
v
k
v
k
,...,
2,1
;
1
2
,






Rabee Rustum and Adebayo J. Adeloye 
40
where Dk is the Euclidian distance between the current input vector and the weight vector k; 
m is the dimensionality of the input vector; xv is the v-th element of the current input vector; 
wk,v is its corresponding value in weight vector k; and M is the total number of neurons in the 
KSOM. 
The neuron whose vector most closely matches the input data vector (i.e. for which the 
Dk is a minimum) is chosen as a winning node or the best matching unit (BMU) as illustrated 
in Figure 3. The vector weights of this winning neuron and those of its adjacent neurons are 
then adjusted to match the input vector using an appropriate algorithm, thus bringing the two 
vectors further into agreement. It follows that each neuron in the map internally develops the 
ability to recognize input vectors similar to itself. This characteristic is referred to as self-
organizing, because no external information is supplied to lead to a classification (Penn, 
2005). 
The process of comparison and adjustment continues until some specified error criteria 
are attained when the training stops. The following two error criteria are normally used: the 
quantization error and the topological error (Garcia and Gonzalez, 2004) defined by 
Equations (6) and (7), respectively. 
Hong et al. (2003), and Garcia and Gonzalez (2004) provided further details regarding 
the step-by-step procedure of the KSOM algorithms, including particular descriptions of the 
weight updating and neighbourhood functions. 
 
 
(6) 
 
 
(7) 
 
where qe is the quantization error; N is the number of samples; Xλ is the λ-th data sample or 
vector; Wc is the prototype vector of the best matching unit Xλ ...  denotes Euclidian 
distance; te is the topological error; and u is a binary integer, which is equal to 1, if the first 
and second best matching units for the argument of u are not adjacent units on the map, 
otherwise u is zero. 
The application of the KSOM for prediction purposes is illustrated in Figure 4 (see 
Rustum and Adeloye, 2007). First, the model is trained using the training data set. Then to 
predict a set of variables as part of an input vector, they are first removed from the vector and 
the depleted vector is subsequently presented to the KSOM to identify its BMU using the 
Euclidian distance calculated with the depleted input vector, i.e. any missing variable is just 
omitted from the calculation of the relevant Dk. However, since the code vector of the 
identified BMU is complete, the values for the missing variables in the input vector are then 
obtained by their corresponding values in the BMU. 
The KSOM can be used for many practical tasks, such as the reduction of the amount of 
training data for model identification, nonlinear interpolation and extrapolation, 
generalisation and compression of information for easy transmission (Kohonen et al., 1996; 
Kangas and Simula, 1995). 




N
c
e
W
x
N
q
1
1





N
e
x
u
N
t
1
)
(
1



Hybrid Unsupervised-Supervised Artificial Neural Networks … 
41
 
Figure 4. Diagrammatic representation of the integrated KSOM-ANN modelling strategy. 
Indeed, the KSOM has been used for a wide variety of applications, mostly for 
engineering problems but also for data analysis (Badekas and Papamarkos, 2007). However, 
the most important applications of the KSOM have been in the visualisation of high-
dimensional systems and process data and the discovery of categories and features in raw 
data. This application is called the exploratory data analysis or data mining (Kohonen et al., 
1996; Kangas and Simula, 1995). 
 
 
2.3. Hybrid Modelling Strategy 
 
The formulation of the BP-ANN presented earlier assumes that all the input and output 
exemplars used for the training are complete, i.e. there are no missing values. If any of the 
vectors are incomplete, then that vector will have to be removed from the training data set 
thus depleting the amount of experience available to the ANN to learn from. Apart from 
missing values, data on environmental systems can also contain outliers, which are 
observations that do not belong to the population being analysed either because they are 
abnormally too high or abnormally too low. Where there is evidence that certain observations 
are outliers, such data must be removed from the record so as not to distort the subsequent 
analysis and the interpretation of the ensuing results. Removing outliers in this way further 
depletes the available data for model training. However, the KSOM could be used to pre-
process the incoming information in order to remove any noise in the data record caused by 
missing values and outliers. Because the KSOM clusters the incoming vectors, the resulting 
features are complete with no missing values. Additionally, since the input vectors clustered 
in a given KSOM node are represented by the weight vector of that node, outliers and other 
unrepresentative measurements are no longer present. In this way, the features should perform 
better when fed as input into a BP-ANN than using the raw input vectors. The combination of 
KSOM and BP-ANN is what has been presented as the hybrid modelling approach in this 
work and is illustrated in Figure 4. 

Rabee Rustum and Adebayo J. Adeloye 
42
3. METHODS AND MATERIALS 
 
3.1. Case study 
 
The methodology of this research work was applied to data from Seafield wastewater 
treatment plant in Edinburgh, UK for the purpose of predicting the effluent 5-day biochemical 
oxygen demand (BOD5) and suspended solids concentrations, the two most commonly used 
water quality parameters for assessing the performance of wastewater treatment works and 
their compliance with discharge consent conditions. In the application, two situations were 
investigated: using the MLP-ANN on raw data; and using MLP-ANN on features extracted 
with the KSOM (i.e. the hybrid KSOM-ANN). The Seafield plant is part of the Almond 
Valley and Seafield project, an environmental regenerating initiative by Scottish Water for 
Edinburgh city and Lothian regions. The plant is operated by Veolia Water under a private 
finance initiative. The population equivalent (pe) of the treatment plant is currently 480, 000 
but is predicted to increase to 520, 000 by 2023 (Hill and Hare, 1999). The catchment served 
by Seafield treatment works contain both separate and combined sewerage systems with a 
number of combined storm overflows discharging to local watercourses. The main outfall, 
however, is situated adjacent to the proposed multimillion pound housing, leisure, business 
and continental ferry development at Leith Docks, and to Portebello beach in the east of the 
city, which has been designated a bathing beach. For the latter reason, part of the effluent of 
the works is subject to UV disinfection during the summer bathing season prior to being 
discharged. The plant comprises 8 circular sedimentation tanks, 4 rectangular non-nitrifying 
aeration lanes, and 8 circular final settlement tanks. The main treatment is preceded by six 
screens (spacing: 6 mm), four Detritor grit removal units and four storm tanks; the storm 
tanks come into operation when the flow reaching the works exceeds the 3DWF (Dry weather 
flow) design standard. When the storm tanks fill up, they spill and this spilled flow is 
discharged directly into the receiving environment without any further treatment; anything 
left in the storm tank at the end of the storm is returned to the head of the primary tanks for 
treatment. The average final effluent BOD5 concentration was around 10 mg/l with respect to 
the provided example data record, which is considerably much lower than the design consent 
of 25 mg/l, as taken from an internal report commissioned by the treatment plant operator. A 
plan view of the Seafield works is shown in Figure 5. 
 
 
3.2. Data and Preliminary Analyses 
 
Historical daily database describing the operation of the Seafield activated sludge 
treatment plant in Edinburgh (Scotland, UK) for a period of approximately three years with a 
total of 1066 data vectors were obtained from Thames Water (plant operator). These data 
come from different sources. On-line data were gained directly from sensors and these 
include flow, temperature, and pH. Off-line data or manual samples were derived variables 
involving several intermediate steps before being presented in the record sheet. These 
variables include Specific Sludge Volume Index (SSVI), Biochemical Oxygen Demand 
(BOD), Chemical Oxygen Demand (COD), Suspended Solids (SS), and Ammonia Nitrogen 
(NH4). 

Hybrid Unsupervised-Supervised Artificial Neural Networks … 
43
 
Figure 5. Layout of Seafield wastewater treatment plant, 1 is the screen house, 2 are the detritors, 3 Grit 
washing mechanism, 4 sedimentation tanks, 5 storm tanks, 6 aeration tanks, 7 final settling tanks, 8 UV 
treatment unit, 9 outfall tunnel. 
The third category of data are those calculated based on a combination of the above 
measurements and include input BOD-load, food to micro-organisms ratio (F/M), and Sludge 
Age. All the available and derived are summarised in Table 1. As seen in Table 1, the raw 
data comprise many gaps and data that were statistically proved to be outliers using visual 
inspection, the Z-score and modified Z-score approaches (Rustum and Adeloye, 2007; 
McBean and Rovers, 1998). 
As noted before, these missing data will make using the associated input vectors for BP-
ANN training impossible. For both modelling paradigms, the choice of input variables was 
based on examining of the correlation matrix. A correlation analysis was thus performed 
between the variables to establish, at a preliminary stage, which of the inputs has the most 
influence for the performance of ASP. 
Only the complete vectors were used for the correlation analysis, i.e. any vector with 
incomplete data was omitted. The results show that the highest 5 correlated variables with the 
effluent BOD5 and effluent SS, apart from effluent COD which is a quality output like 
effluent BOD5 and effluent SS, were found to be influent BOD5 load, which has a correlation 
coefficient of 0.34 and 0.21 with the effluent concentrations BOD5 and SS respectively; the 
dissolved oxygen (DO) concentration in the aerator has a correlation coefficient of -0.27 and -
0.21 with the effluent BOD5 and SS respectively; RAS-MLSS has a correlation coefficient of 
-0.34 and -0.47 with effluent BOD5 and SS respectively; F/M has a correlation coefficient of 
0.44 and 0.33 with effluent BOD5 and SS respectively, and the temperature (T) has a 
correlation coefficient of 0.20 and 0.33 with effluent BOD5 and SS respectively. 

Rabee Rustum and Adebayo J. Adeloye 
44
Table 1. Summary statistics of the measured variables at Seafield Treatment plant 
 
Variables 
Unit 
Number 
of 
missing 
values 
Number of outliers 
Average 
Minimum 
Maximum 
Visual 
Inspection 
Z-
score 
Modified 
Z-score 
Flow to ASP 
m3/d 
19 
23 
18 
54 
259427 
171367 
466486 
Influent BOD5 
mg/l 
105 
1 
5 
22 
65 
15 
180 
Influent SS 
mg/l 
87 
7 
22 
41 
68 
3 
268 
WAS Rate 
m3/d 
146 
15 
9 
20 
3822 
802 
6016 
Measured 
MLSS 
mg/l 
246 
16 
5 
25 
2240 
1126 
4180 
RAS MLSS 
mg/l 
303 
15 
7 
28 
4984 
1748 
1014 
SSVI 
mg/l 
310 
7 
4 
14 
92 
31 
165 
Sludge Age 
Days 
225 
13 
11 
30 
5 
1 
32 
Actual F/M 
 
292 
23 
4 
12 
0.15 
0.015 
0.43 
Final Effluent 
Flow 
m3/d 
1 
17 
16 
58 
250174 
65000 
461926 
Final Effluent 
SS 
mg/l 
14 
24 
24 
29 
28 
3 
190 
Final Effluent 
COD 
mg/l 
15 
48 
18 
42 
50 
15 
173 
Final Effluent 
BOD5 
mg/l 
8 
19 
5 
12 
9 
2 
351 
ASP = Activated Sludge Process; BOD5 = Biochemical Oxygen Demand; SS = Suspended Solids; 
WAS = Waste Activated Sludge; MLSS = Mixed Liquor Suspended Solids; RAS MLSS = Return 
Activated Sludge Mixed Liquor Suspended Solids; SSVI= Stirred Sludge Volume Index; F/M = 
Food to Microorganisms Ratio; 
 
 
3.3. Computer Software 
 
The developed models were implemented using MATLAB programming language with 
Neural Networks toolboxes. Kohonen Self Organizing Maps were built and visualized using 
SOM Toolbox for MATLAB, developed at the Laboratory of Computer and Information 
Science (CIS) at Helsinki University of Technology. Supporting statistical analysis was 
conducted using Statistical Toolbox and various functions in MATLAB. The MATLAB 
programming language was chosen for model development because NN requires intensive 
matrix computations. 
To overcome the over-fitting problem for the MLP-ANN, the early-stop rule was used 
which necessitated dividing the Seafield data into three subsets for training (500 data points), 
validation (200 data points) and testing (366 data points). The validation data set was used to 
stop the training when the errors in this set begin to increase during the training. The testing 
set was used to assess the ability of the ANN to generalise. The input and target data were 
normalised in order to have zero mean and unit standard deviation. The outputs of the trained 

Hybrid Unsupervised-Supervised Artificial Neural Networks … 
45
networks were post-processed, using the inverse of the pre-processing transformation, to be 
compared with the desired outputs. 
Although the correlation analysis carried out gave an initial idea of the relative 
importance of the different variables that affect effluent BOD5 and SS concentrations, the best 
way to select the ideal input variables for a neural network is to train several models with 
different combinations of inputs and choose the one with best generalisation performance. 
This approach is time consuming but was used in this study. Therefore, several network 
architectures, in terms of the number of input neurons, were trained and tested. All of these 
architectures have one hidden layer. The output variables were the effluent BOD5 and SS 
concentrations. The final models were evaluated using three criteria namely, the average 
absolute error (AAE), the mean square error (MSE), and the correlation coefficient (R): 
 
 
(8) 
 
 
(9) 
 
 
(10) 
 
where y and y‟ are the observed and model-predicted output variables respectively. 
To reach the suitable network architecture for the MLP-ANN, simulations were run for 
several of inputs and several of assumed numbers of hidden neurons. All the networks share 
the same specifications: 3-layer (input, hidden and output layers) feedforward neural 
networks, backpropagation learning algorithm, with Levenberg-Marquard optimization 
technique, tan-sigmoid transfer function is used in the hidden layer and the linear transfer 
function is used in the output layer (Haykin, 2008). The number of hidden neurons was set to 
range from 5 to 40. The number of learning iterations was set to be 100 epochs or learning 
cycles. 
A total of 72 BP-ANN models were thus trained and tested with different number of 
input variables and different number of neurons in the hidden layer. These comprise 36 
models which use the raw data and 36 models which use the KSOM features, i.e. the hybrid 
modelling. 
 
 
4. RESULTS AND DISCUSSION 
 
4.1. KSOM Component Planes 
 
The KSOM component planes of the features are illustrated in Figure 6. These 
visualizations present large amounts of detailed information in a concise and summary 
manner in order to give a qualitative idea of the properties of the data. Additionally such 
)
(
1
1
'




N
i
i
i
y
y
N
AAE
2
1
' )
(
1 



N
i
i
i
y
y
N
MSE










]
)'
(
'
][
)
(
[
'
'
2
2
2
2
y
y
N
y
y
N
y
y
yy
N
R

Rabee Rustum and Adebayo J. Adeloye 
46
visualizations enable the human visual system to detect the complex relationships in a 
multidimensional data (in this case 16 variables). Each component plane shows the 
distribution or spread of values of one weight vector component. 
 
 
Figure 6. Visualisation of the component planes of KSOM for operational process variables. 
By comparing patterns in identical positions of the component planes, it is possible to 
reveal correlations, e.g. if large (or small) values of one variable are associated with large (or 
small) values of another, then there is positive correlation between the two. 
The component planes can also be used to explain possible causes of elevated effluent 
BOD, chemical oxygen demand (COD) and SS upset conditions. For example, by comparing 
the dissolved oxygen component plane with each of the effluent BOD, COD and SS 
component planes, it can be readily seen that upset situations resulting in increased effluent 
concentrations of these latter variables are associated with a low dissolved oxygen level. 
Low dissolved oxygen level can cause the rapid proliferation of filamentous organisms. 
Although the growth of floc-forming bacteria and filamentous organisms is desired and 
necessary in the activated sludge aerator basin, any rapid growth, particularly of the latter, is 
undesirable. 
The rapid growth results in increased buoyancy of floc particles causing them to settle 
poorly in the secondary clarifier and cause settleability problems and additionally the loss of 
solids. 
Another observation evident from Figure 5 is that high effluent concentrations of BOD, 
COD, and SS are associated with relatively low sludge age. That might be because when the 
activated sludge is young, many weak and buoyant floc particles are produced. 
During these situations, fine solids are lost from the activated sludge process through 
shearing of the weak floc particles and the lack of an adequate population of ciliated protozoa 

Hybrid Unsupervised-Supervised Artificial Neural Networks … 
47
and metazoan. It is clear from the waste activated sludge rate component planes that the low 
sludge age is associated with the increased sludge wastage rate. In other words, the excess 
sludge wasting produces a young sludge age. 
By comparing the effluent BOD, COD and SS component planes with the mixed liquor 
suspended solids (MLSS) component plane, it can be seen that the high effluent 
concentrations of these parameters are associated with low MLSS values. An increase in the 
MLSS is due to an increase in the population size of the bacteria and vice versa. The return 
activated sludge (RAS) determines whether the MLSS is low or high. Low values of MLSS 
are associated with the low RAS-MLSS, which means that fewer solids (bacteria) are present 
in the biological reactor, leading to a decrease in biological activities and ultimately elevated 
effluent BOD and COD concentrations. 
In general as temperature becomes colder the wastewater becomes denser and the settling 
rate of secondary solids should decrease, implying increase in effluent concentrations of SS 
and BOD. 
However, this fact is not borne out by the component planes in Figure 5, which suggests 
the presence of other factors that negate the expected physical effect of temperature changes 
on the effluent concentrations of BOD, COD and SS. Indeed, much more important for the 
upset condition in the BOD is the effect of temperature on the solubility of gases, notably 
oxygen in water. 
At elevated temperatures, the solubility of oxygen decreases, causing a reduction in the 
aerobic oxidation of the BOD and other biodegradable materials. The elevated temperature 
also means that any available oxygen is used up much more rapidly. The reduction of 
dissolved oxygen is also a perfect condition for the preponderance of filamentous bacteria 
over floc-formers, leading to increased production and accumulation of insoluble biological 
secretions such as lipids which are adsorbed or entrapped in the solids. 
This entrapment in the solids renders the solids buoyant and results in poor compaction of 
solids and loss of settleability. The combination of these factors may have neutralised the 
expected impact of temperature increase to produce the results revealed by the component 
planes. 
The expected negative correlation between the stirred specific volume index (SSVI) and 
the effluent SS is not borne out by the component planes. Also revealed by the component 
planes is that a good (i.e. low) SSVI does not always imply a well thickened sludge in the 
secondary tank and that excessive effluent SS concentrations caused by the loss of fine 
biological particles from the secondary tank may still occur even when the SSVI is below the 
recommended value of 80 (Spellman, 2003). 
Several reasons could be responsible for this: the quiescent conditions in the settling 
column used in the laboratory for SSVI determination compared with the turbulence in a real 
settling tank; the effect of the flow rate on the thickening performance in a real tank; 
denitrification in the settling tank and the evolving nitrogen gas that can cause the sludge 
blanket to rise; and the possible re-suspension caused by bottom sludge scrapers. Other 
operational causes of SS upset conditions include bulking sludge, the presence of soap and 
detergents that favour the production of dispersed floc particles, and the presence of heavy 
metals that favour the production of congealed floc particles. All of this calls for caution in 
the use of the established wisdom about the SSVI in regulating the RAS. 
 
 

Rabee Rustum and Adebayo J. Adeloye 
48
4.2. Artificial Neural Networks Models Using Raw Data 
 
As noted previously, 36 models were trained and tested using the raw data with different 
numbers of inputs and different number of neurons in the single hidden layer. In the first 12 
models, there were 5 inputs (BOD-Load, DO, RAS-MLSS, F/M, T) and 2 outputs (BOD5 and 
SS) with different number of neurons in the hidden layer between 5 and 40. As stated 
previously, the early stopping technique was employed in which the training process was 
stopped when the validation error started to increase. This ensures that over-fitting does not 
occur. 
 
 
Figure 7. Variation of model performance with number of hidden neurons using raw data (effluent 
BOD5). 
5 7 10 13 16 18 20 23 26 30 35 40
15
20
25
30
35
40
Number of hidden  Neurons (N)
MSE (mg/l)2
5 7 10 13 16 18 20 23 26 30 35 40
2
3
4
5
6
7
Number of hidden  Neurons (N)
AAE (mg/l)
5 7 10 13 16 18 20 23 26 30 35 40
0
20
40
60
80
Number of hidden  Neurons (N)
R (%)
5 7 10 13 16 18 20 23 26 30 35 40
20
30
40
50
60
70
Number of hidden  Neurons (N)
MSE (mg/l)2
5 7 10 13 16 18 20 23 26 30 35 40
3
4
5
6
Number of hidden  Neurons (N)
AAE (mg/l)
5 7 10 13 16 18 20 23 26 30 35 40
0
20
40
60
Number of hidden  Neurons (N)
R (%)
5 7 10 13 16 18 20 23 26 30 35 40
20
25
30
35
Number of hidden  Neurons (N)
MSE (mg/l)2
5 7 10 13 16 18 20 23 26 30 35 40
3.5
4
4.5
5
5.5
Number of hidden  Neurons (N)
AAE (mg/l)
5 7 10 13 16 18 20 23 26 30 35 40
0
10
20
30
40
50
Number of hidden  Neurons (N)
R (%)
R (Training)
R (Validation)
R (Testing)
AAE (Training)
AAE (Validation)
AAE (Testing)
MSE (Training)
MSE (Validation)
MSE (Testing)
5-N-2
Raw
data
4-N-2
Raw
data
3-N-2
Raw
data

Hybrid Unsupervised-Supervised Artificial Neural Networks … 
49
The next set of 12 models had 4 inputs (BOD-Load, RAS-MLSS, F/M, and 
Temperature), 2 outputs (BOD5 and SS) with different numbers of neurons in the hidden 
layer. The last set of 12 models have 3 inputs (BOD-Load, RAS-MLSS, F/M, and 
Temperature), 2 outputs (BOD5 and SS) with different numbers of neurons in the hidden 
layer. Figures 7 and 8 show the comparison of the error criteria of the models developed in 
the three cases during training, validation and testing for effluent BOD5 and effluent SS 
respectively. 
It can be seen from the results that using the raw data produced unacceptably poor 
performance. For example, using the correlation coefficient R to illustrate, the lowest order 
(in terms of the number of input neurons) models in Figures 7 and 8 have R < 0.2 during the 
important testing phase of the model development. 
 
 
Figure 8. Variation of model performance with number of hidden neurons using raw data (effluent SS). 
While the correlation coefficient did increase as the number of input neurons increased, 
the resulting R was at best 0.4 for the 5-input neurons architecture. Since the performance 
during testing is an indication of the ability of the ANN to generalise, these performances are 
not re-assuring at all. However, the poor performances are not un-expected because of the low 
correlations between the various variables as reported earlier, an inevitable consequence of 
the noise in the raw data which the hybrid model is expected to redress. For the best 
performing 5-node input layer architecture, it would seem that a 7-node hidden layer is the 
5 7 10 13 16 18 20 23 26 30 35 40
100
150
200
250
300
350
Number of hidden Neurons (N)
MSE (mg/l)2
5 7 10 13 16 18 20 23 26 30 35 40
8
10
12
14
16
Number of hidden Neurons (N)
AAE (mg/l)
5 7 10 13 16 18 20 23 26 30 35 40
0
20
40
60
80
Number of hidden Neurons (N)
R (%)
5 7 10 13 16 18 20 23 26 30 35 40
100
200
300
400
500
600
Number of hidden Neurons (N)
MSE (mg/l)2
5 7 10 13 16 18 20 23 26 30 35 40
10
12
14
16
18
20
Number of hidden Neurons (N)
AAE (mg/l)
5 7 10 13 16 18 20 23 26 30 35 40
0
20
40
60
Number of hidden Neurons (N)
R (%)
5 7 10 13 16 18 20 23 26 30 35 40
100
200
300
400
500
Number of hidden Neurons (N)
MSE (mg/l)2
5 7 10 13 16 18 20 23 26 30 35 40
10
12
14
16
18
Number of hidden Neurons (N)
AAE (mg/l)
5 7 10 13 16 18 20 23 26 30 35 40
0
10
20
30
40
50
Number of hidden Neurons (N)
R (%)
AAE (Training)
AAE (Validation)
AAE (Testing)
MSE (Training)
MSE (Validation)
MSE (Testing)
R (Training)
R (Validation)
R (Testing)
5-N-2
Raw
data
4-N-2
Raw
data
3-N-2
Raw
data

Rabee Rustum and Adebayo J. Adeloye 
50
best because all the three performance metrics evaluated were superior for this architecture 
than for any of the other number of hidden nodes tested. 
 
 
4.3. Hybrid Modelling: Artificial Neural Networks Using the Features of the 
Data 
 
Another 36 models were trained and tested using the features of the data with different 
numbers of inputs and different number of hidden neurons. In the first 12 models there were 5 
inputs (BOD-Load, DO, RAS-MLSS, F/M, T) and 2 outputs (BOD5 and SS) with different 
number of neurons in the hidden layer were trained and tested. The next set of 12 models had 
4 inputs (BOD-Load, RAS-MLSS, F/M, and Temperature), 2 outputs (BOD5 and SS) with 
different numbers of neurons in the hidden layer between 5 and 40. The last set of 12 models 
have 3 inputs (BOD-Load, RAS-MLSS, F/M,), 2 outputs (BOD5 and SS) with different 
numbers of neurons in the hidden layer between 5 and 40. 
Figure 9 and Figure 10 show the comparison of the error criteria of the models developed 
in the three cases during training, validation and testing for effluent BOD5 and Effluent SS 
respectively. Juxtaposed with Figures 7 and 8, Figures 9 and 10 show a significant 
improvement over the use of the raw data for training the BP-ANN. 
 
 
Figure 9. Variation of model performance with number of hidden neurons using KSOM features 
(effluent BOD5). 
5 7 10 13 16 18 20 23 26 30 35 40
10
11
12
13
14
15
Number of hidden Neurons (N)
MSE (mg/l)2
5 7 10 13 16 18 20 23 26 30 35 40
2.3
2.4
2.5
2.6
2.7
2.8
Number of hidden Neurons (N)
AAE (mg/l)
5 7 10 13 16 18 20 23 26 30 35 40
50
60
70
80
90
Number of hidden Neurons (N)
R (%)
5 7 10 13 16 18 20 23 26 30 35 40
10
12
14
16
18
Number of hidden Neurons (N)
MSE (mg/l)2
5 7 10 13 16 18 20 23 26 30 35 40
2.2
2.4
2.6
2.8
3
Number of hidden Neurons (N)
AAE (mg/l)
5 7 10 13 16 18 20 23 26 30 35 40
40
50
60
70
80
90
Number of hidden Neurons (N)
R (%)
5 7 10 13 16 18 20 23 26 30 35 40
10
15
20
25
Number of hidden Neurons (N)
MSE (mg/l)2
5 7 10 13 16 18 20 23 26 30 35 40
2.5
3
3.5
4
Number of hidden Neurons (N)
AAE (mg/l)
5 7 10 13 16 18 20 23 26 30 35 40
40
50
60
70
80
Number of hidden Neurons (N)
R (%)
R (Training)
R (Validation)
R (Testing)
AAE (Training)
AAE (Validation)
AAE (Testing)
MSE (Training)
MSE (Validation)
MSE (Testing)
5-N-2
Features
4-N-2
Features
3-N-2
Features

Hybrid Unsupervised-Supervised Artificial Neural Networks … 
51
 
Figure 10. Variation of model performance with number of hidden neurons using KSOM features 
(effluent SS). 
 
 
Figure 11. Comparison of model performance for the different optimal hidden neurons architecture (F 
denotes KSOM features). 
1
2
3
4(F) 5(F) 6(F)
2
2.5
3
3.5
4
4.5
Model Number
AAE (mg/l)
1
2
3
4(F) 5(F) 6(F)
10
20
30
40
50
60
70
80
90
Model Number
R (%)
1
2
3
4(F) 5(F) 6(F)
100
150
200
250
300
350
Model Number
MSE (mg/l)2
1
2
3
4(F) 5(F) 6(F)
7
8
9
10
11
12
13
14
15
Model Number
AAE (mg/l)
1
2
3
4(F) 5(F) 6(F)
10
20
30
40
50
60
70
80
Model Number
R (%)
1
2
3
4(F) 5(F) 6(F)
10
12
14
16
18
20
22
24
26
Model Number
MSE (mg/l)2
Training
(Validation)
(Testing)
BOD
SS

Rabee Rustum and Adebayo J. Adeloye 
52
 
Figure 12. Comparison of the observed (doted) and predicted (solid) effluent BOD5 values for model 
M4(F) during training, validation and testing. 
 
 
Figure 13. Comparison of the observed (doted) and predicted (solid) effluent SS values for model 
M4(F) during training, validation and testing. 
0
50
100
150
200
250
300
350
400
450
500
0
10
20
30
Data Number (Training)
Effluent BOD
 (mg/l)     
500
520
540
560
580
600
620
640
660
680
700
0
5
10
15
20
25
Data Number(Validation)
Effluent BOD
 (mg/l)     
700
750
800
850
900
950
1000
1050
0
5
10
15
20
25
Data Number(Testing)
Effluent BOD 
(mg/l)       
0
50
100
150
200
250
300
350
400
450
500
0
20
40
60
80
100
Data Number (Training)
Effluent SS 
(mg/l)      
500
520
540
560
580
600
620
640
660
680
700
0
20
40
60
80
Data Number (Validation)
Effluent SS 
(mg/l)      
700
750
800
850
900
950
1000
1050
0
20
40
60
80
Data Number (Testing)
Effluent SS 
(mg/l)      

Hybrid Unsupervised-Supervised Artificial Neural Networks … 
53
 
Figure 14. X-Y scatter plots of observed and M4(F) model-predicted output variables. 
 
Table 2. Optimal architectures of neural network models 
 
Model 
Architecture 
Number of epochs 
M1 
5-7-2 
11 
M2 
4-5-2 
12 
M3 
3-10-2 
9 
M4 (F) 
5-18-2 
22 
M5 (F) 
4-23-2 
33 
M6 (F) 
3-35-2 
23 
 
Table 3. Comparison of model performance for the effluent BOD5 for the best model  
(in terms of the number of hidden neurons) in each category of models 
 
Model Number 
Architecture 
MSE (mg/l)2 
AAE (mg/l) 
Correlation (R) % 
Training 
Validation 
Testing 
Training 
Validation 
Testing 
Training 
Validation 
Testing 
M1 
5-7-2 
22.73 
17.01 
17.64 
3.60 
2.88 
3.09 
49.54 
51.71 
32.02 
M2 
4-5-2 
24.70 
21.01 
23.34 
3.91 
3.64 
4.08 
34.33 
28.76 
21.75 
M3 
3-10-2 
25.36 
20.36 
23.47 
4.08 
3.55 
4.15 
30.44 
32.00 
15.48 
M4 (F) 
5-18-2 
10.34 
11.54 
11.37 
2.32 
2.41 
2.51 
81.80 
68.31 
65.49 
M5 (F) 
4-23-2 
10.76 
12.68 
12.55 
2.37 
2.54 
2.55 
79.47 
65.26 
60.66 
M6 (F) 
3-35-2 
13.26 
13.64 
12.74 
2.65 
2.62 
2.58 
73.34 
63.10 
59.01 
 
 
 
 

Rabee Rustum and Adebayo J. Adeloye 
54
Table 4. Comparison of model performance for effluent SS for the best model (in terms 
of the number of hidden neurons) in each category of models 
 
Model Number 
Architecture 
MSE (mg/l)2 
AAE (mg/l) 
Correlation (R) % 
Training 
Validation 
Testing 
Training 
Validation 
Testing 
Training 
Validation 
Testing 
M1 
5-7-2 
230.48 
134.09 
222.94 
11.31 
8.67 
11.51 
45.68 
58.53 
42.25 
M 2 
4-5-2 
276.66 
179.95 
252.63 
13.01 
10.91 
13.14 
32.92 
30.24 
34.56 
M3 
3-10-2 278.81 
190.99 
303.83 
13.23 
11.54 
14.89 
31.81 
30.84 
18.50 
M4 (F) 
5-18-2 121.04 
106.20 
162.73 
7.62 
7.51 
8.49 
78.32 
67.53 
65.04 
M5 (F) 
4-23-2 129.55 
114.77 
168.12 
8.09 
7.76 
8.84 
76.69 
64.30 
62.91 
M6 (F) 
3-35-2 154.49 
114.66 
160.24 
9.03 
8.06 
9.05 
70.39 
58.06 
61.40 
 
 
4.4. Comparison between the Models 
 
The evaluation criteria (i.e. MSE, AAE, and R) were calculated for each architecture and 
the number of hidden neurons corresponding to the best performance during testing data set 
for each case was selected. Since the training was stopped according to the validation error, 
the number of epochs varied for each architecture. The result is summarised in Table 2, with 
the hybrid models being differentiated using (F), signifying the use of KSOM features. The 
associated values of the performance metrics for the best architecture models are shown in 
Tables 3 and 4 for the BOD5 and SS, respectively. 
Figure 11 further compares the models in Table 1 based on the same set of evaluation 
criteria from which it is clear that using the features of the data produced better performance 
than using the raw data itself to train the models. However, of all the hybrid models, the 
model with 5 input nodes (M4 (F)) has the best performance. The 18-node architecture for 
this model can thus be taken as a compromise best structure since no significant improvement 
in all the three performance criteria occurs when the number of neurons is increased beyond 
18. Because the ANN model with 18 neurons in the hidden layer using the features method, 
Model M4 (F), has the best performance, further analysis was only done with this model. 
Figures 12 and 13 show the time series plots that compare the model predictions with the 
targets for BOD5 and SS during training, validation and Testing. In general, the performance 
is good. The relatively uniform scatter of the plotted points around the line of equality as 
shown in Figure 14 is further confirmation that the model residuals are random and 
homoscedastic. 
 
 
 
 
 
 

Hybrid Unsupervised-Supervised Artificial Neural Networks … 
55
CONCLUSION 
 
As stated previously, with tighter regulations on the receiving water quality, it is 
important to limit point source pollution by improving the performance of wastewater 
treatment plants. Controlling treatment plants through modelling is technically the most 
feasible and maybe least costly way of achieving a sustainable improvement in performance. 
This is because modelling the wastewater treatment units can help the operator to test some 
corrective actions on the computer and, in this way, identify the corrective actions that give 
better performance. However, modelling the activated sludge process (ASP) has many 
problems; all these problems give the ASP its nonlinear characteristics and time-varying 
parameters. Thus, most approaches to modelling the ASP using mechanistic paradigms have 
relied on numerous simplifying, albeit often unrealistic, assumptions in order to make the 
problem tractable. 
In this work, an alternative approach involving neural computing has been applied to 
model the ASP. Artificial neural networks (ANNs) can be used to model any complex, 
nonlinear and dynamic systems without the need to specify the functional form of the 
governing relationship a priori. However, basic multi-layered perceptron (MLP-ANNs) are 
affected by the quality of the data such as noise and missing values, which can make effective 
training difficult. To solve this problem, a model based on the hybrid Kohonen self-
organising map (KSOM) and multi-layered perceptron artificial neural networks (MLP-ANN) 
was used. The best map units of the measurement vectors over the KSOM were used as inputs 
to the MLP-ANN to reduce the effects of noise and uncertainty in the measurement data, and 
to replace the missing elements. 
The current study has confirmed that a hybrid unsupervised-supervised artificial neural 
network to improve the performance of the basic backpropagation neural network method in 
modelling the activated sludge wastewater treatment plant. The method, applied to data 
collected from the Seafield wastewater treatment plant in Edinburgh, UK, during a period of 
about three years successfully improved the prediction of the effluent BOD5 and SS 
concentrations when compared to the use of the raw data set for the same purpose. 
Although the above hybrid methodology has been applied to only two treatment plant 
water quality parameters, i.e. the effluent BOD5 and SS concentrations, it could also be 
readily applied to the prediction of other water quality variables. For example, as the consent 
conditions for wastewater treatment plant effluent discharges become more stringent and 
comprehensive by including other water quality variables such as heavy metals, nutrients and 
bacteriological concentrations, the use of the modelling approach described in this work will 
make the associated enforcement and compliance tasks much easier and quicker to 
implement. Much wider applications in other areas of environmental engineering for the 
purpose of enhancing the performance of AI modelling is also possible as demonstrated 
recently by Adeloye et al. (2012) for the prediction of the reference crop evapotranspiration 
and multivariate rainfall-runoff modelling for the purpose of runoff record in-filling and 
extension in poorly gauged basins (Adeloye and Rustum, 2012). 
 
 
 
 

Rabee Rustum and Adebayo J. Adeloye 
56
REFERENCES 
 
Abrahart R. J., Kneale P. E., See L. M., 2004. Neural networks for hydrological modelling. 
Publisher: Balkema. 
Adeloye A. J., De Munari A., 2006.Artificial neural network based generalized storage–
yield–reliability models using the Levenberg–Marquardt algorithm. Journal of 
Hydrology, 326(1-4): 215-230. 
Adeloye, A. and Rustum R., 2011. Self-Organising Map rainfall-runoff multivariate 
modelling for runoff reconstruction in inadequately gauged basins. Hydrology Research, 
in press. 
Adeloye, A., Rustum R., and Kariyama I., 2011. Neural Computing Modelling of the 
Reference Crop Evapotranspiration. Environmental Modelling and Software, Elsevier, 
Volume 29,Issue 1, March, 2012 Pages 61-73. 
Arbib M.A, 2003. The handbook of brain theory and neural networks. 2nd ed. MIT. 
Astel A., Tsakovski S., Barbieri P., Simeonov V., 2007. Comparison of self-organizing maps 
classification approach with cluster and principal components analysis for large 
environmental data sets. Water Research, 41(19): 4566-4578. 
Back B., Sere K. and Hanna V., 1998. Managing complexity in large database using self 
organising map. Accounting Management and Information Technologies, 8:191-210. 
Badekas E., Papamarkos N., 2007.Optimal combination of document binarization techniques 
using a self-organizing map neural network. Engineering Applications of Artificial 
Intelligence, 20(1): 11-24. 
Chen J. C., Chang N. B., Shieh W. K., 2003. Assessing Wastewater reclamation Potential by 
neural network model. Engineering Applications of Artificial Intelligence, 16, 149-157. 
Cinar O., 2005. New tool for evaluation of performance of wastewater treatment plant: 
Artificial neural network. Process Biochemistry, 40 pp. 2980-2984. 
Diamantopoulou M., Papamichail D., Antonopoulos V., 2005. The Use of a Neural Network 
Technique for the Prediction of Water Quality Parameters. International Journal of 
Operational Research, 5 (1),paper No.9. 
Du Y. G. Tyagi R. D. and Bhamidimarri R., 1999. Use of fuzzy neural-net model for rule 
generation of activated sludge process. Process Biochemistry, 35(1): 77-83. 
EEC, 1991. Directive concerning urban wastewater treatment (91/271/EEC). European 
Economic Community (EEC); Official Journal L135/40; http://eur-lex.europa.eu/ 
LexUriServ/LexUriServ.do?uri=CELEX:31991L0271:EN:HTML (last accessed May 22, 
2012). 
Gamal El-Din A., and Smith D. W., 2002. A neural network model to predict the wastewater 
inflow incorporating rainfall events. Water Research, 36 , 1115 – 1126. 
Garcia, H. andGonzalez, L. 2004. Self-organizing map and clustering for wastewater 
treatment monitoring. Eng. Appl. Artific. Intellig., 17(3), 215–225. 
Gonzalez I.M., Garcia H.L., 2006. End-point detection of the aerobic phase in a biological 
reactor using SOM and clustering algorithms. Engineering Applications of Artificial 
Intelligence, Volume 19, Issue 1, pp. 19-28. 
Hamed M ., Khalafallah M., and Hassanien E., 2004. Prediction Of Wastewater Treatment 
Plant Performance using artificial neural networks. Environment Modelling andSoftware, 
19 ,919-928. 

Hybrid Unsupervised-Supervised Artificial Neural Networks … 
57
Haykin S. , 2008. Neural Networks and Learning Machines, Publisher: Prentice Hall; 3 editio. 
Hill A. and Hare G, 1999. The almond valley and Seafield PFI project risk issues associated 
with flow prediction. WaPUG Autumn Meeting, 1999. 
Hong, Y., Rosen, M., Bhamidimarri, R., 2003. Analysis of a municipal wastewater treatment 
plant using a neural network-based pattern analysis. Water research 37, 1608-1618. 
Kangas. J. and Simula, O., 1995. Process monitoring and visualization using self organising 
map. Chapter 14 in (ed. Bulsari, A. B.) Neural Networks for Chemical Engineers, 
Elsevier Science Publishers. 
Kohonen T., 2001. Self-organizing maps, 3rd ed. Berlin, New York: Springer, 2001. 
Kohonen, T., Oja, E., Simula, O., Visa, A. and Kangas, J., 1996. Engineering applications of 
the self-organizing map. Proceedings of the IEEE 84, (10), pp. 1358–1384. 
Manfred, S., Butler, D. and Beck, M. B., 2002. Modelling, Simulation and Control of Urban 
Wastewater Systems, 27–29. Springer-Verlag London Limited, 357. 
McBean, E. A. and Rovers, F. A., 1998. Statistical procedures for analysis of environmental 
monitoring data and risk assessment. Prentice Hall PTA, New Jersey, USA. 
Mjalli, F., Alasheh S., and Alfadala .H. E. 2007. Use of artificial neural network black- box 
modelling for the prediction of wastewater treatment plants performance Journal Of 
Environment Management, 83 (2007) 329-338. 
Obu-can, K., Fujimura, K., Tokutaka, H., Ohkita, M., Inui, M. and Ikeda, Y., 2001. Data 
mining of power transformer database using self-organising map. Tottori University, 
Dept. 
Of 
Electrical 
and 
Electronics 
Engineering, 
Koyama-Minami, 
Japan. 
Www.ele.tottori-u.ac.jp. 
Olsson, G., Nielsen, M., Yuan, Z., Jensen, A. L. and J-P Steyer, 2005. Instrumentation, 
Control and Automation in Wastewater Systems. IWA Publishing, ISBN 1900222833. 
Penn B. S., 2005. Using Self Organizing Maps to Visualize high dimensional Data. Computer 
and Geoscience 31, pp. 531-544. 
Pu, H. and Hung , Y. 1995. Artificial Neural Networks for Predicting Municipal Activated 
Sludge Wastewater Treatment Plant Performance. Intl. J. Environmental Studies, 48, 97 – 
116. 
Raduly B., Gernaey K. V., Capodaglio A. G., Mikkelsen P. S., and Henze M., 2007. Artificial 
neural networks for rapid WWTP performance evaluation. Environment Modelling and 
Software, 22,1208- 1216. 
Rivas A., Irizar I., Ayesa E., 2008. Model-based optimisation of Wastewater Treatment Plants 
design. Environmental Modelling and Software, 23(4): 435-450. 
Rustum R and A. J.Adeloye, 2007. Replacing outliers and missing values from activated 
sludge data using Kohonen Self Organizing Map. Journal of Environmental Engineering, 
133 (9): 909-916. 
Rustum R., Adeloye A.J., and Scholz M., 2008. Applying Kohonen Self-organizing Map as a 
Software Sensor to Predict the Biochemical Oxygen Demand. Water Environment 
Research, 80 (1): 32-40. 
Spellman, F. R., 2003. “Hand book of water and wastewater treatment plant operation”. 
Lewise publishers, USA. 
Steyer, J. P., Buffiere, P, Rolland , D. and Moletta , R., 1999. Advanced control of anaerobic 
digestion processes through disturbances monitoring. Wat. Res., 33(9), 2059 – 2068. 

Rabee Rustum and Adebayo J. Adeloye 
58
Zhang L. , Scholz M. , Mustafa A., Harrington R, 2009. Application of the self-organizing 
map as a prediction tool for an integrated constructed wetland agroecosystem treating 
agricultural runoff, Bioresource Technology, 100(2): 559-565. 
 
 
 
 
 

In: Recent Advances in Artificial Intelligence Research  
ISBN: 978-1-62808-807-6 
Editors: Ambrogio Bacciga and Renato Naliato 
© 2013 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 3 
 
 
 
FAST VISIBLE TRAJECTORY PLANNING  
IN 3D URBAN ENVIRONMENTS BASED ON LOCAL 
POINT CLOUDS DATA 
 
 
Oren Gal and Yerach Doytsher 
Mapping and Geo-information 
Technion - Israel Institute of Technology  
 
 
ABSTRACT 
 
In this paper we present an efficient and fast visible trajectory planning for 
unmanned vehicles in a 3D urban environment based on local point clouds data. Our 
trajectory planning method is based on a two-step visibility analysis in 3D urban 
environments using predicted visibility from point clouds data. The first step in our unique 
concept is to extract basic geometric shapes. We focus on three basic geometric shapes 
from point clouds in urban scenes: planes, cylinders and spheres, extracting these 
geometric shapes using efficient RANSAC algorithms with a high success rate of 
detection. The second step is a prediction of these geometric entities in the next time step, 
formulated as states vectors in a dynamic system using Kalman Filter (KF). Our planner 
is based on the optimal time horizon concept as a leading feature for our greedy search 
method for making our local planner safer. We demonstrate our visibility and trajectory 
planning method in simulations, showing predicted trajectory planning in 3D urban 
environments based on real LiDAR point clouds data. 
 
 
1. INTRODUCTION AND RELATED WORK 
 
In this paper we study an efficient and fast visible trajectory planning for unmanned 
vehicles in a 3D urban environment, based on local point clouds data. Recently, urban scene 
modeling has become more and more precise, using Terrestrial/ground-based LiDAR on 
unmanned vehicles for generating point clouds data for modeling roads, signs, lamp posts, 
                                                        
 Corresponding author, email: orengal@technion.ac.il. 

Oren Gal and Yerach Doytsher 
60
buildings, trees and cars. Visibility analysis in complex urban scenes is commonly treated as 
an approximated feature due to computational complexity.  
Our trajectory planning method is based on a two-step visibility analysis in 3D urban 
environments using predicted visibility from point clouds data. The first step in our unique 
concept is to extract basic geometric shapes. We focus on three basic geometric shapes from 
point clouds in urban scenes: planes, cylinders and spheres, extracting these geometric shapes 
using efficient RANSAC algorithms with a high success rate of detection. The second step is 
a prediction of these geometric entities in the next time step, formulated as states vectors in a 
dynamic system using Kalman Filter (KF). 
Visibility analysis based on this approximated scene prediction is done efficiently, based 
on our analytic solutions for visibility boundaries. Based on this capability, we present a local 
on-line planner generating visible trajectories, exploring the most visible and safe node in the 
next time step, using our predicted visibility analysis which is based on local point clouds 
data from the unmanned LiDAR vehicle. Our planner is based on the optimal time horizon 
concept as a leading feature for our greedy search method for making our local planner safer. 
For the first time, we propose a solution to the basic limitation of the Velocity Obstacle 
(VO) search and planning method, i.e. when all the dynamic available velocities for the next 
time step are blocked in the velocity space and there is no legal node at the next time step of 
the greedy search. The computation of the minimum time horizon is formulated as a 
minimum time problem that generates optimal trajectories in near-time time to the goal, 
exploring the most visible and safest node in the next time step. We demonstrate our visibility 
and trajectory planning method in simulations showing predicted trajectory planning in 3D 
urban environments using real LiDAR data from Ford Campus Project [1]. 
 
 
1.1. Related Work 
 
1.1.1. Safe and Visible Trajectory Planning 
The main challenge in motion planning is reaching the goal while searching and selecting 
only safe maneuvers. While reaching the goal cannot be guaranteed with an on-line planner, 
one can reduce the state space search to only safe states, i.e. states outside obstacles from 
which at least one other safe state is reachable. 
Generally, we distinguish between local and global planners. The local planner generates 
one step, or a few steps, at every time step, whereas the global planner uses a global search 
toward the goal over a time-spanned tree. We can divide this work into global and local 
(reactive) planners. The global planners generate complete trajectories to the goal in static [2] 
and dynamic [3, 4] environments. 
Examples of local (reactive) planners are [5 ,6, 7], but most do not guarantee safety as 
their ability to look ahead and avoid states of inevitable collision is very limited in a dense 
and fast changing environment, and in narrow passages such as indoor environments. 
Recently, iterative planners [8-13] have been developed that compute several steps at a time, 
subject to the available computation time. The trajectory is generated incrementally by 
exploring a search-tree and choosing the best branch. These planners also do not address the 
issue of safety and completeness. 
Only a few works have addressed the safety issue in dynamic environments, which is 
crucial for partial (local) planning. One approach to safe planning is to use braking policies 

Fast Visible Trajectory Planning in 3D Urban Environments … 
61
[14]; another is to ensure local avoidance for a limited time [12]. However, neither considers 
the dynamic of the moving robot. A promising approach to safe motion planning in a 
dynamic environment is the consideration of ”Regions of Inevitable Collision” (RIC) first 
introduced in [15] and later extended to Inevitable Collision States (ICS) in [16-19]. 
However, most of the effort focused on robot trajectory planning is related to obstacle 
avoidance with kinodynamic constraints, without taking into account visibility analysis as 
part of the nature of a trajectory in urban environments or considering point clouds data sets. 
We address the issue of searching for safe maneuvers for an on-line local planner 
generating visible trajectory based on local point clouds data sets. For the trajectory planning 
part, we use an improved unified Velocity Obstacles (VO) method. Safety is guaranteed by 
searching robots velocity which does not penetrate the VO, which is generated for a carefully 
selected time horizon. The analytic time horizon is calculated at each time step, allowing the 
robot to escape unsafe states in a blocked velocity space and to explore and search for new 
states, a limitation that has not yet been treated when using VO-based search planners. 
Computation of the analytic optimal time horizon, which is obstacle specific, is 
formulated as a minimum time problem that minimizes the time for the robot velocity to exit 
the velocity obstacle, including penetration states. The searching solution for this minimum 
time problem was formulated for omni-directional robots, satisfying Pontryagin‟s Maximum 
Principle providing necessary conditions for optimality. Determining the safe time horizon is 
computationally efficient and it does not require a prior mapping of Inevitable Collision 
States (ICS). Thus, an efficient and safe search can be implemented in very complicated 
environments which cannot be done using the traditional VO planner search [20, 21]. 
 
1.1.2. Visibility in 3D Urban Scenes 
The visibility problem has been extensively studied over the last twenty years, due to the 
importance of visibility in GIS and Geomatics, computer graphics and computer vision, and 
robotics. Accurate visibility computation in 3D environments is a very complicated task 
demanding a high computational effort, which could hardly have been done in a very short 
time using traditional well-known visibility methods [22]. The exact visibility methods are 
highly complex, and cannot be used for fast applications due to their long computation time. 
Previous research in visibility computation has been devoted to open environments using 
DEM models, representing raster data in 2.5D (Polyhedral model), and do not address, or 
suggest solutions for, dense built-up areas. Most of these works have focused on approximate 
visibility computation, enabling fast results using interpolations of visibility values between 
points, calculating point visibility with the Line of Sight (LOS) method [23]. Other fast 
algorithms are based on the conservative Potentially Visible Set (PVS) [24]. These methods 
are not always completely accurate, as they may render hidden objects' parts as visible due to 
various simplifications and heuristics. 
A vast number of algorithms have been suggested for speeding up the process and 
reducing computation time. Franklin [25] evaluates and approximates visibility for each cell 
in a DEM model based on greedy algorithms. Wang et al. [26] introduced a Grid-based DEM 
method using viewshed horizon, saving computation time based on relations between surfaces 
and the line of sight (LOS method). Later, an extended method for viewshed computation was 
presented, using reference planes rather than sightlines [27].  
One of the most efficient methods for DEM visibility computation is based on a shadow-
casting routine. The routine cast shadowed volumes in the DEM, like a light bubble [28]. 

Oren Gal and Yerach Doytsher 
62
Extensive research treated Digital Terrain Models (DTM) in open terrains, mainly 
Triangulated Irregular Network (TIN) and Regular Square Grid (RSG) structures. Visibility 
analysis in terrain was classified into point, line and region visibility, and several algorithms 
were introduced, based on horizon computation describing visibility boundary [29]. 
Only a few works have treated visibility analysis in urban environments. A mathematical 
model of an urban scene, calculating probabilistic visibility for a given object from a specific 
viewcell in the scene, has been presented by [30]. This is a very interesting concept, which 
extends the traditional deterministic visibility concept. Nevertheless, the buildings are 
modeled as cylinders, and the main challenges of spatial analysis and building model were 
not tackled. Other methods were developed, subject to computer graphics and vision fields, 
dealing with exact visibility in 3D scenes, without considering environmental constraints. 
Plantinga and Dyer [22] used the aspect graph – a graph with all the different views of an 
object. Due to their computational complexity, all of these works are not applicable to a large 
scene with near real-time demands, such as visible trajectory planning.  
Visibility from point clouds data was first presented by [31] using visibility cones. 
Visibility from point clouds is commonly treated as a surfaces reconstruction problem [32], 
computed off-line using a computer graphics application. In this research we present 
approximated visibility computation by predicting the scene using incomplete point clouds 
data set.  
 
 
2. VISIBILITY ANALYSIS FROM POINT CLOUDS DATA 
 
2.1. Overview 
 
As we mentioned, visibility analysis in complex urban scenes is commonly treated as an 
approximated feature due to computational complexity. Recently, urban scene modeling has 
become more and more exact, using Terrestrial/ground-based LiDAR generating dense point 
clouds data for modeling roads, signs, lamp posts, buildings, trees and cars. Automatic 
algorithms detecting basic shapes and extraction have been studied extensively, and are still a 
very active research field [33]. 
In this part, we present a unique concept for predicted and approximated visibility 
analysis in the next attainable vehicle's state at a one-time step ahead in time, based on local 
point clouds data which is a partial data set. 
We focus on three basic geometric shapes in urban scenes: planes, cylinders and spheres, 
which are very common and can be used for the majority of urban entities in modeling 
scenarios. Based on point clouds data generated from the current vehicle's position in state k-
1, we extract these geometric shapes using efficient RANSAC algorithms [34] with high 
success rate detection tested in real point cloud data. 
After extraction of these basic geometric shapes from local point clouds data, our unified 
concept, and our main contribution, focus on the ability to predict and approximate urban 
scene modeling at the next view point 
, i.e. attainable location of the vehicle in the next 
time step. Scene prediction is based on the geometric entities and Kalman Filter (KF) which is 
commonly used in dynamic systems for tracking target systems [35, 36]. We formulate the 
k
V

Fast Visible Trajectory Planning in 3D Urban Environments … 
63
geometric shapes as states vectors in a dynamic system and predict the scene structure the in 
the next time step, k. 
Based on the predicted scene in the next time step, visibility analysis is carried out from 
the next view point model [37], which is, of course, an approximated one. As the vehicle 
reaches the next viewpoint 
, point clouds data are measured and scene modeling and states 
vectors are updated, which is an essential procedure for reliable KF prediction. 
Our concept is based on RANSAC and KF, both real-time algorithms which can be 
integrated into autonomous mapping vehicles that have become very popular. This concept 
can be applicable for robot trajectory planning generating visible paths, by analyzing local 
point clouds data and predicting the most visible viewpoint in the next time step from among 
several options.  
In the next sub-section we introduce the main stages of our concept and algorithm. Basic 
geometric shapes and RANSAC detection method from point clouds data are presented in 
sub-section 2.2.A. Discrete dynamic model state for basic geometric shapes and prediction 
using KF technique consist of Predict, Measure, and Update stages, is presented in sub-
section 2.2.B. Predicted visibility analysis is discussed in sub-section 2.3. 
 
Concept's Stages 
Our methodology can be divided into three main sub-problems: 
 
1) Extract basic geometric shapes from point clouds data (using RANSAC algorithms) 
2) Predict scene modeling in the next viewpoint (using KF) 
3) Approximated visibility analysis of a predicted scene 
 
Each of the following stages is done after the other, where the last stage also includes 
updated measurement of point clouds data validating KF for the next viewpoint analysis. 
 
 
2.2. Shapes Extraction 
 
1) Geometric Shapes: 
 
The urban scene is a very complex one in the matter of modeling applications using 
ground LiDAR, and the generated point clouds is very dense. Due to these inherited 
complications, feature extraction can be made very efficient by using basic geometric shapes. 
We define three kinds of geometric shapes planes, cylinders and spheres, with a minimal 
number of parameters for efficient time computation. 
Plane: center point (x,y,z) and unit direction vector from center point.  
Cylinder: center point (x,y,z), radius and unit direction vector of the cylinder axis. 
Sphere: center point (x,y,z), radius and unit direction vector from center point. 
 
2) RANSAC: 
 
The RANSAC [38] paradigm is a well-known one, extracting shapes from point clouds 
using a minimal set of shape's primitives generated by random drawing in point clouds set. 
k
V

Oren Gal and Yerach Doytsher 
64
Minimal set is defined as the smallest number of points required to uniquely define a given 
type of geometric primitive.  
For each of the geometric shapes, points are tested and approximate the primitive of the 
shape (also known as "score of the shape"). At the end of this iterative process, extracted 
shapes are generated from the current point clouds data. 
Based on the RANSAC concept, the geometric shapes detailed above can be extract from 
a given point clouds data set. In order to improve the extraction process and reduce the 
number of points validating shape detection, we compute the approximated surface normal 
for each point and test the relevant shapes.  
Given a point-clouds 
with associated normals 
, the output of the 
RANSAC algorithm is a set of primitive shapes and a set of remaining points 
. 
In this part we briefly introduce the main idea of plane, sphere and cylinder extraction 
from point clouds data. An extended study of RANSAC capabilities can be found in [34]. 
Plane: A minimal set in the case of a plane, can be found by just three points 
, 
without considering normals in the points. Final validation of the candidate plane is computed 
from the deviation of the plane‟s normal from 
. A plane is extracted only in cases 
where all deviations are less than the predefined angle 
. 
Sphere: A sphere is fully defined by two points with corresponding normal vectors. The 
sphere center is defined from the midpoint of the shortest line segment between the two lines 
given by the points and their normals.  
A sphere counts as a detected shape in cases where all three points are within a distance 
of from the sphere and their normals do not deviate by more than degrees. 
Cylinder: A cylinder is set by two points and their normals, where the cylinder axis 
direction is the projected cross product of the normals, and a center point is calculated as the 
intersection of parametric lines generated from points and points' normal. A cylinder is 
verified by applying the thresholds and to distance and to normal deviation of the samples. 
 
Predicted Scene – Kalman Filter 
In this part, we present the global Kalman Filter approach for our discrete dynamic 
system at the estimated state, k, based on the defined geometric shapes formulation defined in 
the previous sub-section. 
Generally, the Kalman Filter can be described as a filter that consists of three major 
stages: Predict, Measure, and Update the state vector. The state vector contains different state 
parameters, and provides an optimal solution for the whole dynamic system [35]. We model 
our system as a linear one, with discrete dynamic model: 
 
 
(1) 
 
where is the state vector, F is the transition matrix and k is the state.  
The state parameters for all of the geometric shapes are defined with shape center 
, and 
unit direction vector 
, of the geometric shape, from the current time step and viewpoint to 
the predicted one. 
1
{
..
}
N
P
p p

1
{ ..
}
N
n n
1
\{
..
}
N
R
P
p
p



1
2
3
{
,
,
}
p p
p
1
2
3
{ ,
,
}
n n n

,
1
1
k
k k
k
x
F
x



s
d

Fast Visible Trajectory Planning in 3D Urban Environments … 
65
In each of the current states k, geometric shape center 
, is estimated based on the 
previous update of shape center location 
, and the previous updated unit direction vector 
, multiplied by small arbitrary scalar factor c: 
 
 
(2) 
 
Direction vector 
can be efficiently estimated extracting the rotation matrix T, between 
the last two states k, k-1. In case of an inertial system fixed on the vehicle, a rotation matrix 
can be simply found from the last two states of the vehicle translations: 
 
 
(3) 
 
The 3D rotation matrix T tracks the continuous extracted plans and surfaces to the next 
viewpoint 
, making it possible to predict a scene model where one or more of the 
geometric shapes are cut from current point clouds data in state k-1. The discrete dynamic 
system can be written as: 
 
 
(4) 
 
where the state vector is vector, and the transition squared matrix is 
. The dynamic 
system can be extended to additional state variables representing part of the geometric shape 
parameters such as radius, length etc. We define the dynamic system as the basic one for 
generic shapes that can be simply modeled with center and direction vector. The sphere radius 
and cylinder Z boundaries are defined in additional data structure of the scene entities. 
 
 
2.3. Fast and Approximated Visibility Analysis  
 
In this section, we present an analytic analysis of visibility boundaries of planes, 
cylinders and spheres for the predicted scene presented in the previous sub-section, which 
leads to an approximated visibility. For the plane surface, fast and efficient visibility analysis 
was already presented in [37]. 
In this part, we extend the previous visibility analysis concept [37] and include cylinders 
as continuous curves parameterization 
. 
Cylinder parameterization can be described as: 
 
ks
1
ks 
1
k
d 
1
1
k
k
k
s
s
cd




k
d
1
k
k
d
Td 

k
V
1
1
1
1
1
1
11
12
13
21
22
23
31
32
33
1
0
0
0
0
0
1
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
k
k
k
k
k
k
k
k
k
k
k
k
x
x
y
y
z
z
x
x
y
y
z
z
s
s
c
s
s
c
c
s
s
T
T
T
d
d
T
T
T
d
d
T
T
T
d
d






































































,
1
k k
F

ln ( , , )
c
d
C
x y z

Oren Gal and Yerach Doytsher 
66
 
 
 
(5) 
 
We define the visibility problem in a 3D environment for more complex objects as: 
 
 
(6) 
 
where 3D model parameterization is 
, and the viewpoint is given as 
. 
Extending the 3D cubic parameterization, we also consider the cylinder case. Integrating 
equation (5) to (6) yields: 
 
 
(7) 
 
 
(8) 
 
As can be noted, these equations are not related to Z axis, and the visibility boundary 
points are the same for each x-y cylinder profile. 
The visibility statement leads to complex equation, which does not appear to be a simple 
computational task. This equation can be efficiently solved by finding where the equation 
changes its sign and crosses zero value; we used analytic solution to speed up computation 
time and to avoid numeric approximations. We generate two values of generating two 
silhouette points in a very short time computation. Based on an analytic solution to the 
cylinder case, a fast and exact analytic solution can be found for the visibility problem from a 
viewpoint. 
We define the solution presented in equation (8) as x-y-z coordinates values for the 
cylinder case as Cylinder Boundary Points (CBP). CBP are the set of visible silhouette points 
for a 3D cylinder, as presented in Figure 1: 
 
 
(9) 
ln
sin( )
( , , )
cos( )
C
d
r const
r
C
x y z
r
c













_ max
0
2
1
0
peds
c
c
c
h








co s
co s
0
0
0
'( , )
( ( , )
(
,
,
))
0
n t
n t
z
z
C x y
C x y
V x
y z



( , )z const
C x y

0
0
0
(
,
,
)
V x y z
sin
cos
sin
cos
0
0
x
y
z
r
V
r
r
r
V
c
V



























_
_
_
_
1
1
1
1..
2
0
0
0
,
,
(
,
,
)
,
,
PBP
bound
PBP
bound
PBP
bound
PBP
bound
i
N
N
N
N
x y z
CBP
x
y z
x
y
z











Fast Visible Trajectory Planning in 3D Urban Environments … 
67
 
(a) 
 
 
(b) 
Figure 1. Cylinder Boundary Points (CBP) using Analytic Solution marked as blue points, Viewpoint 
Marked in Red: (a) 3D View (Visible Boundaries Marked with Red Arrows); (b) Topside View. 
In the same way, sphere parameterization can be described as: 
 
 
(10) 
 
We define the visibility problem in a 3D environment for this object as: 
 
 
(11) 
 
where the 3D model parameterization is 
, and the viewpoint is given as 
. 
Integrating eq. (10) to (11) yields: 
sin
cos
( , , )
sin sin
cos
0
0
2
Sphere
r const
r
C
x y z
r
r
























0
0
0
'( , , )
( ( , , )
(
,
,
))
0
C x y z
C x y z
V x
y z



( , , )
C x y z
0
0
0
(
,
,
)
V x
y z

Oren Gal and Yerach Doytsher 
68
 
(12) 
 
where r is set from sphere parameter, and is changes from visibility point along Z axis. The 
visibility boundary points for a sphere, together with the analytic solutions for planes and 
cylinders, allow us to compute fast and efficient visibility in a predicted scene from local 
point cloud data, that being updated in the next state. 
This extended visibility analysis concept, integrated with a well-known predicted filter 
and extraction method, can be implemented in real time applications with point clouds data. 
 
 
3. FAST VISIBLE TRAJECTORY PLANNING 
 
Our planner is a local one, generating one step ahead at every time step reaching toward 
the goal, which is a depth first A* search over a tree. We extend previous planners which take 
into account kinematic and dynamic constraints [15] and present a local planner for an omni-
directional robot, with these constraints mounted with LiDAR in a constant Z point. As far as 
we know, for the first time this planner generates fast and exact visible trajectories based on 
an optimal analytic time horizon solution handling blocked states where all future states are 
inside VO, and approximates visibility based on local point clouds data for the next time step 
based on incomplete data. The fast and efficient visibility analysis of our method [37], 
extended in section 2 for spheres and cylinders, allows us to generate the most visible 
trajectory from a starting state to the goal state in 3D urban environments, based on local 
decision-making capabilities, and demonstrates our capability, which can be extended to real 
performances in the future. 
We assume incomplete data of the 3D urban environment model as mentioned in Section 
2, and use an extended Velocity Obstacles (VO) method with analytic optimal time horizon.  
In this section, we first describe Velocity Obstacle and the Non Linear Velocity Obstacle 
methods. The following sub-sections focus on an analytic optimal time horizon for safe 
trajectories, and planner description based on predicted visibility computation at each time 
step based on incomplete point clouds data set; an object's prediction performed by RANSAC 
and KF is presented in Section 2. 
 
 
3.1. The Velocity Obstacle 
 
The velocity obstacle represents the set of all velocities of the robot colliding with 
neighboring obstacles. It essentially maps static and moving obstacles into the robot‟s 

Fast Visible Trajectory Planning in 3D Urban Environments … 
69
velocity space. The velocity obstacle (VO) of a planar circular obstacle, B, that is moving at a 
constant velocity   , is a cone in the velocity space at point A, as shown in Figure 2. In 
Figure 2, the position space and velocity space of A are overlaid to illustrate the relationship 
between the two spaces. The VO is generated by first constructing the Relative Velocity Cone 
(RVC) from A to the boundaries of B, then translating RVC by   . 
Each point in VO represents a velocity vector that originates at A. Any velocity of A that 
penetrates VO is a colliding velocity that would result in a collision between A and B at some 
future time. Figure 2 shows two velocities of A: one that penetrates VO, hence a colliding 
velocity, and one that does not. All velocities of A that are outside of VO are safe as long as B 
stays on its current course. The velocity obstacle thus allows determining if a given velocity is 
potentially dangerous, and suggesting possible changes to this velocity to avert collision. If B 
is known to move along a curved trajectory or at varying speeds, it would be best represented 
by the nonlinear velocity obstacle discussed next. 
 
 
Figure 2. A Linear Velocity Obstacle. 
 
3.2. Nonlinear Velocity Obstacle 
 
The nonlinear velocity obstacle (NLVO) accounts for a general trajectory of the obstacle, 
while assuming a constant velocity of the robot [21]. It applies to the scenario shown in 
Figure 3, where, at time   , a point A attempts to avoid an obstacle, B, that is following a 
general known trajectory, c(t), and at time    is located at c(  ). B represents the set of points 
that define the geometry of the obstacle, grown by the radius of the robot. If B is a circle, then 
c(t) represents the trajectory followed by its center. 
The NLVO consists of all velocities of A at    that would result in collision with the 
obstacle at any time t>  . Selecting a single velocity,   , at time t =    outside the NLVO thus 
guarantees avoiding collision at all times. It is constructed as a union of its temporal elements, 
NLVO(t), which is the set of all absolute velocities of A,   , that would result in collision at a 
specific time t. 
 
 
 
 

Oren Gal and Yerach Doytsher 
70
 
Figure 3. A Non-Linear Velocity Obstacle. 
Referring to Figure 3,    that would result in collision with point p in B at time t >   , 
expressed in a frame centered at A(  ), is simply: 
 
   
      
      
(13) 
 
where r is the vector to point p in the obstacle‟s fixed frame. The set, N LVO(t) of all 
absolute velocities of A that would result in collision with any point in B at time t >    is 
thus: 
 
 
(14) 
 
Clearly, NLVO(t) is a scaled B, located at a distance from A that is inversely 
proportional to the collision time t. The entire N LVO is the union of its temporal subsets 
from   , the current time, to some set time horizon   : 
 
     ⋃
      
    
  
    
  
(15) 
 
The smallest safe time horizon is one that allows sufficient time to avoid or escape 
collision, as detailed in the next section. The non-linear velocity obstacle is a warped cone. 
If c(t) is bounded over t = (  , ∞), then the apex of this cone is at A(  ). The boundaries of 
the N LVO represent velocities that would result in A grazing B. 
 
 
 
0
)
(
)
(
t
t
B
t
c
t
NLVO




Fast Visible Trajectory Planning in 3D Urban Environments … 
71
3.3. Analytic Optimal Time Horizon – Escaping Mode 
 
The time horizon plays an important role in selecting feasible avoidance maneuvers. It allows 
considering only those maneuvers that would result in a collision within a specified time interval 
and efficiently searching for safe maneuvers in the velocity space. Setting the time horizon too 
high would be too prohibitive, as it would mark as dangerous maneuvers resulting in collision 
at a distant time; selecting a too-small time horizon would permit dangerous maneuvers that are 
too close and at too high speeds to avoid the obstacle. 
It is essential that the proper time horizon ensures that a safe maneuver, even if temporarily 
pointing toward the obstacle, is selected.  
The main significance of the time horizon parameter using VO was first introduced 
in [20]. For each obstacle, time horizon is calculated as the minimum between stopping 
and passing time, as approximations to the exact optimization problem. Numeric 
solutions of the optimal time horizon for point mass model with cubic control constraints 
were presented in [20], based on external trajectories generated from the boundary of the 
control effort. This formulation of time horizon defines approximation of VO as the 
boundary of ICS without analytic solution escaping VO, in a case of bounded velocity 
space. 
In this part, we define a solution for some cases where all the attainable velocities in 
the next time step are bounded inside VO, where the classic VO does not include a 
solution for these scenarios. We set the smallest safe time horizon as the minimum time 
needed to exit the velocity obstacle from a given state, named as escaping mode, subject 
to robot dynamics and actuator constraints. This case can be formulated as the solution of 
the minimization problem satisfying system dynamic and control constrains: 
 
   ∫
   
  
  
  
(16) 
 
with the initial condition: 
 
x(  ), x˙ (  ) 
(17) 
 
The final condition: 
 
  ̇     ∑
   
 
   
    
(18) 
 
We address the time-optimal control problem, seeking the best maneuver in case of 
blocked nodes, i.e. when all of the possible nodes in the next time step are located inside 
VO. This problem is formulated as Pontryagin‟s Maximum Principle for analytic optimal 
solution. First, we introduce the robot dynamic model of our planner. 
 
3.3.1. Dynamic Model 
For our visibility analysis and trajectory planning, we refer to a robot equipped with 
LiDAR for point clouds data, for visibility analysis and Omni-directional dynamic model. 
One of the most famous robots compatible with our demands is the Segway robot shown in 
Figure 4, previously used for DARPA Mars program.  

Oren Gal and Yerach Doytsher 
72
 
Figure 4. Segway Robot used for DARPA Mars Program. 
We simplify robot dynamic model to:  
 
x¨ =   ; 
 
y¨ =   ; 
(19) 
 
and control constraints: 
 
{(  ,    ∈ U )|   2+   2 ≤   }  
(20) 
 
where (x, y)T∈R2 and (  ,   )T∈U represent the joint variables and actuator efforts, 
respectively. Viewpoint consists of z-axis value which is constant, where the LiDAR 
is mounted on a fixed point on the robot. 
 
3.3.2. Pontryagin’s Maximum Principle Analysis 
Pontryagin‟s Maximum Principle is a powerful method for the computation of optimal 
controls, which has the crucial advantage of not requiring prior evaluation of the informal 
cost function. 
We define the state vector as x=(  ,         )T = ( ,  ̇     ̇ )T and f = (  ,   ,   ,   ). 
For such control constraints and system dynamics, we introduce an analytic solution to the 
optimal time horizon problem. The meaning of the boundary condition ψ is that the control 
signal (  ,   )T at    is tangent to the closest boundary of the infinite VO from the current 
position of the robot. 
For our case, based on Pontryagin‟s Maximum Principle, the necessary condition for 
optimality can be written as: 
 

Fast Visible Trajectory Planning in 3D Urban Environments … 
73
                       +    
            
            
               
(21) 
 
where    are the coefficients of the closest boundary of the infinite VO, as seen in Figure 5: 
 
 
Figure 5. The Velocity Obstacles boundaries. 
where          ) is one of the tangent points between the obstacle B and its apex at   , 
separated into     and    . The slope of VO boundary is - 
  
    as can be shown in Figure 5. 
   depends on time, but can be computed as constant for a limited one-time step. The 
Hamiltonian function is expressed by: 
 
                ∑
         
 
   
  
(22) 
 
In our case, for time optimal trajectory: 
 
         
(23) 
 
The control signal can be written in polar coordinates: 
 
                               
(24) 
 
or 
 
                                         
(25) 
 
We can see that the signs of      determine the value of the optimal control solution: 
 

Oren Gal and Yerach Doytsher 
74
                                    
(26) 
 
Adjoins variables defined as: 
 
 ̇   
  
     
(27) 
 
Therefore: 
 
         
 
             
 
         
 
             
(28) 
 
The coefficients also satisfying the boundary condition: 
 
*       
   +
  
    
(29) 
 
According to that, there is no switching along the trajectories and the initial values 
determine the optimal solution: 
 
              
 
               
 
               
(30) 
 
The optimal angular from Omni-directional model is: 
 
   
                         
 
          (
  
  )         (
  
  )  
(31) 
 
The geometric interpretation for the optimal angular from the Omni-directional model 
reveals that the robot should move vertically to the boundary line of the infinite VO. This 
solution determines the optimal time horizon for each obstacle until its escape from VO with 
very low computation effort in case of deadlock inside the velocity space. 
 
3.3.3. Analytic Optimal Time Horizon - Examples 
In this part, we focus on the efficiency of our analytic time horizon solution via classic 
VO demonstrated in simulations. The analytic solution extends the traditional VO planner 

Fast Visible Trajectory Planning in 3D Urban Environments … 
75
search method and defines the strategy search in cases of blocked attainable velocity space for 
the next time step in velocity space. 
We use a planner similar to the one presented by [20] with the same cost function, and 
the Omni-directional robot model mentioned above. The search is guided by a cost function 
planner applying the safest maneuver at every time step. Unsafe states ahead in time are 
recognized before the robot enters into unsafe states, also called ICS. For one obstacle, our 
planner can ensure safety, but the planner is not a complete one. By using an analytic search, 
the planner computes near-time optimal and safe trajectory to the goal.  
 
 
Figure 6. Avoiding Two Obstacles Using Analytic Time Horizon. 
 
Figure 7. Blocked Velocity Space Avoiding Two Obstacles. 

Oren Gal and Yerach Doytsher 
76
 
Figure 8. Final Trajectory Avoiding Two Obstacles Using Analytic Time Horizon. 
 
 
Figure 9. Escaping Blocked Velocity Space Using Analytic Time Horizon. 
The main contribution of this section is to demonstrate cases of blocked nodes in the 
velocity space in the search tree for the next time step. In cases of blocked nodes, i.e. all of 
the nodes located inside the VO, the planner choose the node that leads outside VO as soon as 
possible, avoiding collision and formulated as analytic time horizon based search. Without 
using analytic time horizon formulation, there is no safe and legitimate option for the next 
node to be explored. As a result, conservative trajectories are computed, and in some cases 
safe trajectory to the goal cannot be found and collision eventually occurs. 
In a two-obstacles case shown in Figure 6, the robot, represented by a point, starts near 
point (0,-4) at zero speed, attempting to reach the goal at point (0,4) (marked by a yellow 

Fast Visible Trajectory Planning in 3D Urban Environments … 
77
triangle) at zero speed, while avoiding two static obstacles. The trajectory is dotted with a red 
dot representing the current position of the robot. The bounded velocity space, representing 
velocity obstacles as yellow cycles and velocity vector (with green triangles), can be seen in 
Figure 7, relating to the state space position as shown in Figure 6. 
 
 
Figure 10. Conservative Solution of Avoiding Two Obstacles Using Constant Time Horizon: Blocked 
Velocity Space Caused to Conservative Trajectory Turning Left vs. Sliding on their Edges and Passing 
Between them. 
Clearly, there is no gap to enter between VO's in Figure 7 and the velocity vector is 
bounded in the velocity space. The trivial VO, with a conservative and constant time horizon, 
cannot find the ultimate solution in such a case, and as a result, a conservative trajectory will 
be computed. The robot avoids the obstacles to the left with high time horizon values, as 
shown in Figure 10. Moreover, in some other cases of dense and bounded velocity space, no 
solution will be available at all. By using an analytic time horizon, the robot escapes velocity 
obstacles and searches for a safe maneuver in state space, as shown in Figure 8, and velocity 
space, respectively, as shown in Figure 9. 
 
 
3.4. The Planner 
 
Our planner, is a local one, generating one step ahead at every time step reaching toward 
the goal, which is a depth first A* search over a tree. We extend previous planners which take 
into account kinematic and dynamic constraints [20] and present a local planner for an Omni-
directional model, predicting visibility in the next time step, which can also avoid unsafe 
states in bounded velocity space by using an analytic optimal time horizon. 
By using RANSAC algorithm, at each time step point clouds data are extracted into three 
possible objects: plane, cylinder and sphere. The scene is formulated as a dynamic system 

Oren Gal and Yerach Doytsher 
78
using KF analysis for objects' prediction. The objects are approximated for the next time step, 
and each safe attainable state that can be explored is set as candidate viewpoint. The cost for 
each node is set as the total visible surfaces, based on the analytic visibility boundary, where 
the optimal and safe node is explored for the next time step. 
At each time step, the planner computes the next Attainable Velocities (AV), as detailed 
in section 3.4.1. We refer to the same dynamic model presented in section 3.3.1. The safe 
nodes not colliding with objects such as cubes, cylinders and spheres, i.e. nodes outside 
Velocity Obstacles are explored. Where all nodes are inside VO, a unified analytic solution 
for time horizon is presented in sub-section 3.3, generating an escape option for these radical 
cases without considering visibility analysis. The planner computes the cost for these safe 
nodes based on predicted visibility and chooses the node with the optimal cost for the next 
time step, as described in section 3.4.2. We repeat this procedure while generating the most 
visible trajectory, as described in section 3.4.3. 
 
3.4.1. Attainable Velocities 
The set of maneuvers that are dynamically feasible over a time step is represented by 
Attainable Velocities (AV). At each time step during the trajectory planning, we map the 
attainable velocities that the robot can choose under the effort control envelope. 
The Attainable Velocities,         , are integrated from the current state (  ,  ) by 
applying all admissible controls       . The geometric shape of AV depends on system 
dynamics. In our case,  
 
  ̇      
 
  ̇    
(32) 
 
where        . 
 
                            
(33) 
 
The attainable velocities at time      apply to the position          Thus, the 
attainable velocities, when intersected with VO that correspond to the same position, would 
indicate those velocities that are safe if selected at time     . 
 
3.4.2. Cost Function 
Our search is guided by minimum invisible parts from viewpoint V to the approximated 
3D urban environment model in the next time step,       set by KF after extracting objects 
from point clouds data using RANSAC algorithm, as presented in section 2. The cost function 
for each node is a combination of IRV and ISV, with different weights as functions of the 
required task.  
The cost function is computed for each safe node, i.e. node outside VO, considering the 
robot's future location at the next time step (        ,        ) as viewpoint: 
 
 (       )               ) +              ) 
(34) 
 

Fast Visible Trajectory Planning in 3D Urban Environments … 
79
where     are coefficients, affecting the trajectory's character. The cost function       
    produces the total sum of invisible parts from the viewpoint to the 3D urban environment, 
meaning that the velocity at the next time step with the minimum cost function value is the 
most visible node in our local search, based on our approximation. 
We divide point invisibility value into Invisible Surfaces Value (ISV) and Invisible Roofs 
Value (IRV). This classification allows us to plan delicate and accurate trajectory upon 
demand. We define ISV and IRS as the total sum of the invisible roofs and surfaces 
(respectively). Invisible Surfaces Value (ISV) of a viewpoint is defined as the total sum of the 
invisible surfaces of all the objects in a 3D environment, as described in equation (35): 
 
 
(35) 
 
In the same way, we define Invisible Roofs Value (IRV) value as the total sum of all the 
invisible roofs surfaces:  
 
 
(36) 
 
Extended analysis of analytic solution for visibility analysis for known 3D urban 
environments can be found in [37]. 
 
3.4.3. Planner Pseudo-Code 
The Pseudo-Code of the UAV Planner is as follows: 
 
 
 
 
0
t
t

.   𝑏𝑒  =        
1. While  𝑏𝑒  ≠       do: 
   1.1. Calculate AV nodes from  𝑏𝑒  . 
   1.2. For each node        check: 
1.2.1.     if   (̇  ℎ)  ∑
   
 
 =1
=   
              ( +   ) is illegal. 
                    Else  
                                  1.2.1.1. Extract Features – RANSAC 
                                  1.2.1.2. Scene Prediction - KF             
                                  1.2.1.3. Calculate Objects Visibility Boundaries    
                                  1.2.1.4. Calculate node visibility cost,  (   +
   ) 
    1.3. If all nodes are illegal  
                                   1.3.1. Calculate Analytic Optimal Time Horizon 
                                   1.3.2. Generate Escaping Mode  
     Else   
             1.3.1. Find node with minimal cost     = {  |min w(  )}. 
             1.3.2. Update  𝑏𝑒  =      
             1.3.3.  =  +    
     End 
 
 
1..
1
1..
1
0
0
0
1
(
,
,
)
obj
j
Nbound
i
j
Nbound
i
N
VP
VP
i
ISV x
y
z
IS






0
0
0
1
(
,
,
)
obj
j Nbound
i
j Nbound
i
N
VP
VP
i
IRV x
y z
IS





Oren Gal and Yerach Doytsher 
80
3.4.4. Simulations 
We have implemented the presented algorithm and tested some urban environments on a 
1.8GHz Intel Core CPU with Matlab. We computed the visible trajectories using our planner, 
with real raw data records from LiDAR as part of the Ford Campus Project. 
Point clouds data are generated by Velodyne HDL-64E LiDAR [39]. Velodyne HDL-64E 
LiDAR has two blocks of lasers, each consisting of 32 laser diodes aligned vertically, 
resulting in an effective 26:8 Vertical Field Of View (FOV). The entire unit can spin about its 
vertical axis at speeds up to 900 rpm (15 Hz) to provide a full 360 degree azimuthal ﬁeld of 
view. The maximum range of the sensor is 120 m and it captures about 1 million range points 
per second. We captured our data set with the laser spinning at 10 Hz. 
Due to these huge amounts of data, we planned a limited trajectory in this urban 
environment for a limited distance. In Figure 11, point clouds data from the start point can be 
seen, also marked as start point "S" in Figure 14. Planes extracted by RANSAC can be 
recognized. As part of the Ford Project, these point clouds are also projected to the panoramic 
camera's systems, making it easier to understand the scene, as seen in Figure 12.  
 
 
Figure 11. Point Clouds Data set at Start Point. 
 
Figure 12. Point Clouds Data Projected to Panoramic Camera Set at Start Point. 

Fast Visible Trajectory Planning in 3D Urban Environments … 
81
 
(a) 
 
 
(b) 
Figure 13. (a) Objects in point clouds data set. (b) Predicted objects using KF in the next time step. 
 
 
Figure 14. Vehicle Planned Trajectory Colored in Purple. 

Oren Gal and Yerach Doytsher 
82
As described earlier, at each time step the planner predicts the objects in the scene using 
KF. In Figure 13(a), objects in the scene are presented from a point clouds data set. These 
point clouds predicted using KF, and predicted to the next time step in Figure 13(b).  
The planned trajectory is presented in Figure 14 with a purple line. The starting point, 
marked as "S", is presented in Figure 14, where the cloud points in this state are presented in 
Figure 11. An arbitrary state during the planned trajectory, which is marked with an arrow, is 
also presented in Figure 14, where point clouds prediction using KF in this state are presented 
in Figure 13. For this trajectory,        , robot velocity is set to      *
  
  +  In this 
case, the robot avoided two other cars, without handling cases of analytic optimal time 
solution for deadlocks with bounded velocity space.  
 
 
CONCLUSION AND FUTURE WORK 
 
In this research, we have presented an efficient trajectory planning algorithm for visible 
trajectories in a 3D urban environment for an Omni-directional model, based on an 
incomplete data set from LiDAR, predicting the scene at the next time step and 
approximating visibility. 
Our planner is based on two steps visibility analysis in 3D urban environments using 
predicted visibility from point clouds data. The first step is to extract the basic geometric 
shapes: planes, cylinders and spheres, using RANSAC algorithms. The second step is a 
prediction of these geometric entities in the next time step, formulated as states vectors in a 
dynamic system using the Kalman Filter (KF).  
We extend our analytic visibility analysis method to cylinders and spheres, which allows 
us to efficiently set the visibility boundary of predicted objects in the next time step, 
generated by KF and RANSAC methods. Based on these fast computation capabilities, the 
on-line planner can approximate the most visible state as part of a greedy search method. 
As part of our planner, we extended the classical VO method, where the velocity space is 
bounded and the robot velocity cannot escape from the velocity obstacles in the current state. 
We presented an escape mode based on an analytic time-optimal minimization problem 
which, for the first time, defines time horizon for these cases. 
The visible trajectory is an approximated one, allowing us to configure the type of visible 
objects, i.e. roof or surfaces visibility of the trajectory, and can be used for different kinds of 
applications.  
Further research will focus on advanced geometric shapes, which will allow precise 
urban environment modeling, facing real-time implementation with on-line data processing 
from LiDAR.  
 
 
REFERENCES 
 
[1] 
G. Pandey, J.R. McBride and R.M. Eustice, Ford campus vision and lidar data set. 
International Journal of Robotics Research, 30(13):1543-1552, November 2011. 
[2] 
J.-C. Latombe, Robot Motion Planning. Kluwer Academic Publishers, 1990. 

Fast Visible Trajectory Planning in 3D Urban Environments … 
83
[3] 
M. Erdman and T. Lozano-Perez, On multiple moving objects, Algorithmica, vol. 2, pp. 
447–521, 1987. 
[4] 
K. Fugimura and H. Samet, A hierarchical strategy for path planning among moving 
obstacles, IEEE Transactions on Robotics and Automation, vol. 5, pp. 61–69, 1989. 
[5] 
L. Ulrich and J. Borenstien, Vfh+: Reliable obstacle avoidance for fast mobile robots, in 
Proceedings of the IEEE International Conference on Robotics and Automation, 1998, 
pp. 1572–1577. 
[6] 
N. Ko and R. Simmons, The lane-curvature method for local obstacle avoidance, in 
International Conference on Intelligence Robots and Systems, 1998, pp. 1615–1621. 
[7] 
J. Minguez and L. Montano, Nearest diagram navigation. a new real-time collision 
avoidance approach, in International Conference on Intelligence Robots and Systems, 
2000, pp. 2094–2100. 
[8] 
T. Fraichard, Planning in dynamic workspace: a state-time space approach, Advanced 
Robotics, vol. 13, pp. 75–94, 1999. 
[9] 
H. R. K. J.-C. Latombe and S. Rock, Randomized kinodynamic motion planning with 
moving obstacles, Algorithmics and Computational Robotics, vol. 4, pp. 247–264, 
2000. 
[10] O. Brock and O. Khatib, Real time replanning in high- dimensional configuration 
spaces using sets of homotopic paths, in Proceedings of the IEEE International 
Conference on Robotics and Automation, 2000, pp. 550–555. 
[11] N. S. J. Minguez L. Montano and R. Alami, Global nearest diagram navigation, in 
Proceedings of the IEEE International Conference on Robotics and Automation, 2001, 
pp. 33–39. 
[12] Feron and M. D. E. Frazzoli, Real time motion planning for agile autonomous vehicles, 
AIAA Journal of Guidance Control and Dynamics, vol. 25, pp. 116–129, 2002. 
[13] Fox, W. Burgard, and S. Thrun, The dynamic window approach to collision avoidance, 
IEEE Robotics and Automation Magazine, vol. 4, pp. 23–33, 1997. 
[14] T. Wikman and W. N. M.S. Branicky, Reflexive collision avoidance: a generalized 
approach, in Proceedings of the IEEE International Conference on Robotics and 
Automation, 1993, pp. 31–36. 
[15] S. Lavalle. J. Kuffner, Randomized kinodynamic planning, International Journal of 
Robotics Research, vol. 20, pp. 378–400, 2001. 
[16] T. Fraichard, A short paper about safety, in Proceedings of the IEEE International 
Conference on Robotics and Automation, 2007, pp. 1140–1145. 
[17] S. P. T. Fraichard, Safe motion planning in dynamic environment, in International 
Conference on Intelligence Robots and Systems, 2005, pp. 885–897. 
[18] T. Fraichard and H. Asama, Inevitable collision state-a step towards safer robots? 
Advanced Robotics, vol. 18, pp. 1001–1024, 2004. 
[19] N. Chan and M. Z. J. Kuffner, Improved motion planning speed and safety using region 
of in- evitable collision, in ROMANSY, July 2008, pp. 103–114. 
[20] O. Gal, Z. Shiller, and E. Rimon, Efficient and safe on-line motion planning in dynamic 
environment, in Proceedings of the IEEE International Conference on Robotics and 
Automation, 2009, pp. 88–93. 
[21] Z. Shiller. F. Large and S. Sekhavat, Motion planning in dynamic environments: 
Obstacle moving along arbitrary trajectories, in Proceedings of the IEEE International 
Conference on Robotics and Automation, 2001, pp. 3716–3721. 

Oren Gal and Yerach Doytsher 
84
[22] H. Plantinga, and R. Dyer, Visibility, Occlusion, and Aspect Graph, The International 
Journal of Computer Vision, 1990, vol. 5, pp. 137-160.  
[23] Y. Doytsher, and B. Shmutter, Digital Elevation Model of Dead Ground, Symposium 
on Mapping and Geographic Information Systems (Commission IV of the International 
Society for Photogrammetry and Remote Sensing), Athens, Georgia, USA, 1994. 
[24] F. Durand, 3D Visibility: Analytical Study and Applications, PhD thesis, Universite 
Joseph Fourier, Grenoble, France, 1999. 
[25] W.R. Franklin, Siting Observers on Terrain, in Proc. of 10th International Symposium 
on Spatial Data Handling. Springer-Verlag, 2002, pp. 109–120. 
[26] J. Wang, G.J. Robinson, and K. White, A Fast Solution to Local Viewshed 
Computation 
Using Grid-based 
Digital 
Elevation 
Models, 
Photogrammetric 
Engineering & Remote Sensing, 1996, vol. 62, pp. 1157-1164. 
[27] J. Wang, G.J. Robinson, and K. White, Generating Viewsheds without Using 
Sightlines, Photogrammetric Engineering & Remote Sensing, 2000, vol. 66, pp. 87-90. 
[28] C. Ratti, The Lineage of Line: Space Syntax Parameters from the Analysis of Urban 
DEMs', Environment and Planning B: Planning and Design, 2005, vol. 32, pp. 547-
566. 
[29] L. De Floriani, and P. Magillo, Visibility Algorithms on Triangulated Terrain Models, 
International Journal of Geographic Information Systems, 1994, vol. 8, pp.13-41. 
[30] B. Nadler, G. Fibich, S. Lev-Yehudi, and D. Cohen-Or, A Qualitative and Quantitative 
Visibility Analysis in Urban Scenes, Computers & Graphics, 1999, vol. 5, pp. 655-666. 
[31] Mederos, B., Amenta, N., Velho, L., de Figueiredo, L.H.: Surface reconstruction from 
noisy point clouds. In: Euro- graphics Symposium on Geometry Processing, 2005, pp. 
53-62. 
[32] Grossman J. P., D.W.J.: Point sample rendering. In: Rendering Techniques, 1998, pp. 
181-192. 
[33] G. Vosselman, B. Gorte, G. Sithole, and T. Rabbani. Recognizing structure in laser 
scanner point clouds. The International Archives of the Photogrammetry Remote 
Sensing and Spatial Information Sciences (IAPRS), 2004, vol. 36, pp. 33–38. 
[34] R. Schnabel, R. Wahl, R. Klein, Efficient RANSAC for Point-Cloud Shape Detection, 
Computer Graphics Forum, 2007, vol. 26, no.2, pp. 214-226.  
[35] R. Kalman. A new approach to linear filtering and prediction problems. Transactions of 
the ASME-Journal of Basic Engineering, 1960, vol. 82, no. 1, pp:35–45. 
[36] J. Lee, M. Kim, and I. Kweon. A kalman filter based visual tracking algorithm for an 
object moving, In IEEE/RSJ Intelligent Robots and Systems, 1995, pp. 342–347. 
[37] O. Gal, and Y. Doytsher, Fast Visibility Analysis in 3D Procedural Modeling 
Environments, in Proc. of the, 3rd International Conference on Computing for 
Geospatial Research and Applications, Washington DC, USA, 2012. 
[38] H. Boulaassal, T. Landes, P. Grussenmeyer, and F. Tarsha- Kurdi. Automatic 
segmentation of building facades using terrestrial laser data. The International Archives 
of the Photogrammetry Remote Sensing and Spatial Information Sciences (IAPRS), 
2007, vol. 36, no. 3. 
[39] Velodyne 2007: Velodyne HDL-64E: A high deﬁnition LIDAR sensor for 3D 
applications. Available at: http://www.velodyne.com/lidar/products/white paper. 
 
 

In: Recent Advances in Artificial Intelligence Research  
ISBN: 978-1-62808-807-6 
Editors: Ambrogio Bacciga and Renato Naliato 
© 2013 Nova Science Publishers, Inc. 
 
 
 
 
 
 
Chapter 4 
 
 
 
“SMART BRAKES”– A NEURO-GENETIC 
OPTIMIZATION OF BRAKE ACTUATION PRESSURE 
 
 
Dragan Aleksendrić and Velimir Ćirović 
University of Belgrade, Faculty of Mechanical Engineering,  
Kraljice Marije, Belgrade, Serbia 
 
 
ABSTRACT  
 
Many systems today that need modeling and optimization are non-linear systems or 
systems whose behavior is strongly influenced by their previous and current state. The 
most important automotive system having these characteristics is a braking system. The 
main purpose of braking systems is to control braking torques, allowing a vehicle to 
decelerate in an optimum manner while maintaining directional stability. The demands 
imposed on a braking system, over a wide range of operating conditions, are complex. It 
is especially related to the brakes. The brakes are supposed to provide high and stable 
values of braking torque over different operating conditions determined by synergistic 
influence of the actuation pressure and/or sliding speed and/or the brake interface 
temperature for the specific friction pair characteristics. Since the driver obtains an 
important feedback of vehicle dynamics and its braking capabilities depending on a brake 
performance change, it represents an important aspect of a vehicle performance and its 
quality of use. Sensitivity of braking torque versus influence of the friction couple 
interaction, under different braking conditions, is one of the most important properties of 
the disc brake. In this chapter, possibilities for an intelligent dynamic optimization of the 
brake performance have been investigated. The hybrid neuro-genetic optimization model 
has been developed for dynamic control and optimization of the disc brake actuation 
pressure during a braking cycle. This model provided “smart brake” abilities by 
optimization the value of the brake actuation pressure according to the pressure selected 
by a driver. The important property of the future smart brakes is related to stabilization 
and maximization of the brake performance versus the brake pedal travel selected by a 
driver and current braking conditions. In this chapter, influence of the interrelated 
parameters, such as a vehicle speed, the brake actuation pressure, and temperature, has 
been analyzed during a braking cycle in the case of the disc brake. The new optimization 
                                                        
 Corresponding author: E-mail: daleksendric@mas.bg.ac.rs. 

Dragan Aleksendrić and Velimir Ćirović 
86
model of the brake actuation pressure of a passenger car has been developed. The model 
provided realization of different braking strategies according to the wanted brake 
performance. The model is able to optimize the value of the disc brake actuation pressure 
in a braking cycle versus the brake pedal travel selected by a driver and current the brake 
tribological behavior. The pressure was adjusted on the level which simultaneously 
providing stable and, for current braking regimes, maximum braking torque values.  
 
Keywords: Optimization, disc brake sensitivity, neural networks, genetic algorithms 
 
 
INTRODUCTION 
 
It is well known that the motor vehicle brakes are supposed to provide high and stable 
values of braking torque in different operating conditions. The brake operation conditions are 
determined by synergistic influence of the brake actuation pressure and/or sliding speed 
and/or the brake interface temperature [1-4], for current the brake characteristics. Disc brakes 
are safety critical automotive components that also must satisfy tough cost, and 
environmental requirements [5]. According to [6], the driver in terms of the brake pedal „feel‟ 
often evaluates the performance of an automotive braking system. It is one of the first 
customer touch points during a driving experience, and as such can be an important 
contributor to quality perception and customer appeal [7]. Brake pedal feel depends on 
synergistic influence of a braking system performance and a driver subjective perception 
related to the quality of a brake pedal feel. The brake pedal feel gives a driver perception of 
the vehicles braking dynamics and braking performance. The control of brake performance 
and its sensitivity to synergistic change of speed-pressure-temperature during braking is an 
issue that should be better resolve. Nowadays, automotive braking systems should offer stable 
and at the same time maximum performance in all braking conditions according to a driver 
demands. Motor vehicle brakes operation need to be intelligently supported in order to 
optimize their non-linear behavior. Due to strong non-linear nature of a braking process, 
demands imposed on a braking system and especially its brakes are very complex over a wide 
range of operating conditions [8, 9]. 
Regarding modern automotive brakes, the extreme demands have been imposed on the 
friction couple and its tribological performance [10-13]. The braking torque sensitivity 
against synergistic change of the brake actuation pressure, sliding speed and temperature, is 
an important issue for the specific type of a brake. This issue should be resolved in order to 
provide better braking performance and adjust it to the driver demands. The most important 
influence on the brake performance sensitivity to the different braking regimes has the 
friction couple interaction. The brake friction material has significant influence on the friction 
pair contact situation [14-20]. Friction material properties play often a crucial role in a driver 
perception of braking performance, especially at elevated brake interface temperatures [14, 
17, 18]. Properties of a brake friction material influence the disc brake performance i.e. its 
sensitivity during a braking cycle on different ways. The brake friction material often 
critically influences the friction level and its variation during braking, in-stop friction rise, 
ambient and hot compressibility [6], etc. On the other hand, drivers expect relatively constant 
level of brakes performance at various braking regimes i.e. the highest level of the brakes 
reliability. The braking torque level in a braking cycle is very important issue during vehicle 

“Smart Brakes” – A Neuro-Genetic Optimization of Brake Actuation Pressure 
87
exploitation. It should remain on the stable level in a braking cycle and at the same time to be 
maximized. Due to often and high fluctuation of the brake performance, especially in-stop 
braking torque rising, stability of the brake performance is not provided. It causes poor brake 
pedal feel. Accordingly, the driver obtains a confusing feedback of vehicle dynamics and the 
braking system performance. That is why, optimization and dynamic control of the brakes 
performance should be provided in a braking cycle. It needs that a functional relationship 
between the brake pedal travel and the brakes performance should be established and 
optimized. It is required because this functional relationship should be used for a dynamic 
control and optimization of the disc brake performance versus a driver demands.  
Taking into consideration that very complex and highly non-linear phenomena are 
involved into the braking process, precise an analytical model of the brake performance is 
almost impossible to be obtained [21, 22]. Reason for that lies in the fact that the brake‟s 
friction pair coefficient of the friction has to be considered as a non-linear parameter. It 
significantly complicates the process of automotive brakes performance control [23-28]. That 
is why, intelligent methods should be introduced in modeling, control, and optimization of 
highly non-linear processes [29, 30] such as a braking process. An implementation of 
artificial intelligence should provide development of intelligent methods that incorporate 
adaptation and learning [31]. In this chapter, an intelligent approach to optimization of the 
brake performance of a passenger car was proposed based on synergy of artificial neural 
networks and genetic algorithms. Investigation presented in this chapter contributes to the 
efforts for an intelligent optimization of automotive braking systems performance and 
creating “smart brakes” abilities. In order to provide the optimization process, the hybrid 
neuro-genetic optimization model has been developed. This dynamic model has been used for 
control, and optimization of braking torque values by adjusting of the brake actuation 
pressure according to the influence of previous and current values of sliding speed, pressure, 
and temperature. Recurrent neural networks have dynamically modeled these synergistic 
influences. Furthermore, genetic algorithms have used this model for optimizing and 
adjusting of the disc brake actuation pressure on the level which providing stable and at the 
same time the maximum performance. The optimization process has been done according to 
the brake pedal travel selected by a driver and the brake inherent capabilities for generation of 
braking torque under specific braking conditions. It could be considered as a new intelligent 
way for development of the advance brake assistance system for correcting the current brake 
performance according to its previous behavior and selected the brake pedal travel. 
 
 
Figure 1. Single-end full-scale inertial dynamometer. 

Dragan Aleksendrić and Velimir Ćirović 
88
EXPERIMENTAL PROCEDURES 
 
Development of the neuro-genetic optimization model comprises resolving very 
important issues. The main step is related to development of the dynamic model of the 
braking torque change versus influence the speed, the brake actuation pressure, and the brake 
interface temperature for different the brake pedal travels. It is important in order to create 
possibilities for adjusting the brake actuation pressure to the level which providing stable but 
at the same time maximum brake performance. In order to find and establish that correlation, 
the appropriate data has to be provided. That is why, the disc brake has been previously tested 
using single-end full-scale inertial dynamometer (see Figure 1). The tested disc brake belongs 
to the front axle of passenger car with static load of 730 kg, effective disc radius of 101 mm, 
floating caliper (piston diameter 48 mm), and disc pads friction surface area of 32.4 cm2. The 
single-end full-scale inertial dynamometer has been used in order to provide data in the 
strictly controlled test conditions in accordance with the selected testing methodology. It is 
very important because friction material characteristics critically affecting overall brake 
performance and its change versus influence of speed, pressure and temperature in a braking 
cycle. This effect is primarily caused by a friction material compressibility and its in-stop 
friction behavior. Accordingly, the brake actuation pressure needs to be precisely controlled 
in order to investigate its influence on braking torque in synergy with different the brake 
operation conditions. 
The methodology used for testing of the disc brake is designed that the brake actuation 
pressure has been changed between 20 and 100 bar, while initial speed has been varied 
between 20 and 100 km/h. Brake interface temperature corresponds to more common braking 
conditions known as cold brake i.e. brake interface temperature has been kept between 25 and 
100°C. Dynamic neural network in synergy with genetic algorithms was used for mining the 
data obtained during the brake testing in order to learn and generalize how the disc brake 
performance was affected by the brake actuation pressure, the speed, and the brake interface 
temperature. Previously, the functional relationship between the brake pedal travel and the 
brake actuation pressure was established. Learned and generalized interrelated influences on 
the brake performance, versus different the brake pedal travels, is further used for 
optimization of the brake actuation pressure on the level which providing wanted braking 
torque.  
 
 
AN INTEGRATED NEURO – GENETIC APPROACH 
 
Artificial neural networks have shown to be an effective method for prediction of time 
series events. Neural networks can be classified into dynamic and static categories. Static 
(feed-forward) networks have no feedback elements and contain no delays – the output is 
calculated directly from the input through feed-forward connections [25]. Dynamic neural 
networks are generally more powerful than static networks since they have memory that can 
remember the past values and states of the network [25, 31]. The output of the dynamic 
network depends on the current input values as well as on the previous inputs, outputs or 
states of the network. That is why, the gradient must be computed in a more complex way. 
Dynamic neural networks can be trained using the same learning algorithms that are used for 

“Smart Brakes” – A Neuro-Genetic Optimization of Brake Actuation Pressure 
89
static networks. Traditional research in this area uses a network with a sequential iterative 
learning process based on the feedforward, back-propagation approach [25]. In general, 
dynamic neural networks can be divided into two types: (1) Feed-forward Time-Delay 
Networks, and (2) Feedback or Recurrent Networks. Recurrent neural networks are built in 
such way that the outputs of some neurons are fed back to the same neurons or to neurons in 
the preceding layers. They have an intrinsic dynamic memory: their outputs at a given instant 
reflect the current input as well as previous inputs and outputs that are gradually quenched. 
The network function is largely determined by the interconnections between neurons 
(connection weights). According to [25, 31], a dynamic network output is differently affected 
by weights. Two different effects could be considered: (i) direct effect (a change in the weight 
causes an immediate change in the output at the current time step), and (ii) indirect effect 
(implies using of dynamic backpropagation to compute the gradients, which is more 
computationally intensive). As a subcategory of dynamic neural networks, the Layer-
Recurrent network has been employed in this paper for dynamic modeling and optimization 
of the brake performance versus the brake pedal feel. Original simplified version of this 
network, introduced by Elman [33] is shown in Figure 2. This recurrent neural network is 
characterized by a feedback loop, with a single delay, around each layer of the network 
except for the last (output) layer (see Figure 2). It has found application in many filtering and 
modeling tasks. 
Because neural network models have excellent approximation abilities, they are able to 
approximate precisely nonlinear behavior of dynamic processes [34]. Very important feature 
of these networks is their adaptive nature, where „learning by example‟ replaces 
„programming‟ in solving the problems [35]. They can be trained to perform a particular 
function by adjusting the values of weight factors between the neurons, either from the 
information of outside the network or by the neurons themselves in response to the input. The 
learning and recognizing patterns in large data sets is the key ability of neural networks to 
achieve learning and memory. According to the objectives of this chapter, the input 
parameters in the dynamic model are defined by parameters, which defined the disc brake 
operation conditions (speed, pressure, and the brake interface temperature). A synergy of 
these influences is related to braking torque change. The pattern of influence of the brake 
actuation pressure should be recognized versus dynamic change of the others braking regimes 
and braking torque. That is why, braking torque has been taken as an output parameter, 
representing the brake performance (see Figure 3). As it can be seen, the influence of 
previous braking torque values on its current value is represented through feedback 
connections after each of hidden layers, known as Layer delay states. 
 
 
Figure 2. A typical structure of Layer-Recurrent neural network. 

Dragan Aleksendrić and Velimir Ćirović 
90
 
Figure 3. Dynamic neural model of disc brake operation based on Layer-Recurrent network. 
In this chapter, fifty different layer-recurrent neural networks have been investigated 
against influence of three training algorithms: Levenberg-Marquardt (LM), Bayesian 
Regularization (BR), and Resilient Backpropagation (RP), respectively. The tansig activation 
function has been used between the input and the first hidden layer, as well as between the 
two hidden layers. The purelin activation function has been used between the last hidden and 
the output layer. Between the total number of trained and tested models, the dynamic model 
based on the layer-recurrent neural network architecture with two hidden layers has been 
chosen as the best one for further analysis. The best model, with 4 neurons in the first, and 2 
neurons in the second hidden layer, trained with Bayesian learning algorithm, was employed 
for modeling and optimization of the brake performance sensitivity versus different braking 
regimes.  
 
 
Figure 4. The basic cycle of genetic algorithms. 

“Smart Brakes” – A Neuro-Genetic Optimization of Brake Actuation Pressure 
91
In order to provide optimization capabilities of the dynamic model, genetic algorithms 
have been used. Genetic algorithms represent search algorithms based on the mechanics of 
natural selection and natural genetics [35, 36]. They have been used for many optimization 
tasks, in particular by combining with other techniques of artificial intelligence, as it is shown 
in [37-40]. The basic GA optimization cycle is shown in Figure 4. Genetic algorithms start 
with initial population of random chosen individuals (chromosomes) from the design space 
and search the input range effectively for required output variables by means of reproduction, 
crossover, and mutation [41-44]. A population of chromosomes (individuals) represents 
potential solutions to the problem to be solved, that is, the optimization of a function, 
generally very complex [45]. In every evolutionary step, known as a generation, the 
individuals in the current population are decoded (evaluated) according to some predefined 
quality criterion, referred to as fitness function, i.e. objective function [35]. That means 
genetic algorithm repeatedly modifies a population of individual solutions. Therefore, each 
individual in the population has an associated fitness, indicating the utility or adaptation of 
the solution that it represents [45]. The most common representation in GA is binary. The 
chromosomes consist of a set of genes, which are generally characters belonging to an 
alphabet {0,1}. In optimization problems of parameters with variables in continuous domains, 
it is more natural to represent the genes directly as real numbers since the representations of 
the solutions are very close to the natural formulation, i.e. there are no differences between 
the genotype (coding) and the phenotype (search space) [43]. The genetic algorithm uses 
three main types of rules at each step to create the next generation from the current 
population: (i) Selection to select the individuals, called parents, that contribute to the 
population at the next generation, (ii) Crossover to combine two parents to form children for 
the next generation, and (iii) Mutation to apply random changes to individual parents in order 
to form children. At each step, the genetic algorithm selects individuals at random from the 
current population to be parents and uses them to produce the children for the next generation. 
Over successive generations, the population “evolves” towards an optimal solution.  
In this paper, genetic algorithms were used in combination with developed dynamic 
neural model in order to find the value of applied brake pressure, versus driver demands, for 
dynamic controlling of braking torque value. An implementation of genetic algorithms was 
provided using Genetic Algorithm solver in program package Matlab 7.11. Figure 5 shows 
the flow chart of the hybrid ANN-GA optimization model developed for that purpose. This 
integrated approach to optimization could be summarized into five main steps: (1) The GA 
generates an initial population of individuals for applied brake pressure; (2) After that, the 
fitness of each chromosome is evaluated. Also, a fitness value for each chromosome by the 
developed ANN model is assigned; (3) Creating a population for the next generation by the 
genetic operations (selection, crossover and mutation) with the probabilities based on fitness 
of the each chromosome is following afterwards; (4) The GA then sends the new child 
generation to ANN model as new input parameter; (5) Finally, calculating of the fitness by 
developed ANN model is performed. The last four steps are repeated until the terminating 
criterion has been met. Obviously, thus conceived optimization process exploits ability of 
genetic algorithm to optimize neural network inputs using the trained dynamic neural model 
as its fitness function. Regarding Figure 5, fitness of each individual is calculated by dynamic 
neural model in the each evolutionary step. By simulating the neural model for different 
speeds and temperatures in a braking cycle, the applied brake pressure was optimized in a 
dynamic manner taking into consideration dynamic changes of the real and wanted braking 

Dragan Aleksendrić and Velimir Ćirović 
92
torque values, and such defined nonlinear constraints. These constraints for fitness function 
value were formed based on the simulation model and the wanted value of braking torque. 
The constraint is defined as a difference between predicted and wanted braking torque values. 
Wanted braking torque could be differently defined, as a value which provides stable and/or 
maximum braking torque during a braking cycle. Minimization of a difference between these 
real and wanted values of braking torque, genetic algorithm searches the optimal value of the 
brake applied pressure during a braking cycle. The main goal is that the difference between 
real and wanted braking torque converges to zero during a series of successive generations of 
individuals. As the fitness function value, over successive generations, approaches towards 
the criteria being set, the applied pressure converges to optimal. 
 
 
RESULTS AND DISCUSSION 
 
The diversity and substantial variations of the braking torque versus synergistic influence 
of the pressure, the sliding speed, and brake interface temperature should be better controlled 
in order to provide a stable and at the same time maximum braking performance. High 
diversity of temperature at the contact of the friction couple, change coefficient of the friction, 
noise, and vibrations are the main negative issues of the brake friction surfaces interaction, 
particularly at high sliding speed. These negative influences could be partially reduced 
through an optimization of the brake actuation pressure, as it is proposed here. Introduction of 
the brake performance optimization through adjusting the brake actuation pressure could 
provide different braking strategies regarding the wanted level of brake performance in a 
braking cycle. Using here proposed approach, wanted brake performance could be differently 
set out. Two different ways for selection of wanted brake performance has been investigated 
and proposed in this chapter. The first approach was to find the value of brake actuation 
pressure, in a braking cycle, which provides the brake maximum performance. This 
maximum performance has been obtained according to the brake real tribo behavior 
determined by an interaction between the friction material and a brake disc, for selected the 
brake pedal travel. This approach means that a function goal was only related to 
maximization of the brake performance without braking torque stabilization in a braking 
cycle. The second approach was to optimize the brake actuation pressure providing not only 
maximum performance of the brake but also stabilization of a braking torque. Stabilization of 
a braking torque has been done by correcting the current brake tribo behavior i.e. correcting 
the brake performance sensitivity versus current braking regimes. It is important because the 
second approach could eliminate increasing of braking torque with speed decreasing in a 
braking cycle, for example. That negative effect of the brake operation could be avoided or 
suppressed by adjusting of the brake actuation pressure during a braking cycle. 
Figure 6 shows the process of the brake actuation pressure optimization versus real 
pressure selected by a driver. According to Figure 6, the brake operation conditions, related to 
synergistic influence of pressure – speed – temperature in a braking cycle, caused variations 
of braking torque in the wide range. Obviously, for the same brake pedal travel of 40% and 
the real brake actuation pressure of 22 bar (see Figure 6), the minimum performance were 
obtained for initial speed of 18 km/h. Maximum performance was reached for initial speed of 
88 km/h. The maximum braking torque has been changed between 300 Nm (18 km/h) and 

“Smart Brakes” – A Neuro-Genetic Optimization of Brake Actuation Pressure 
93
550 Nm (88 km/h). Accordingly, the brake has shown serious sensitivity to the sliding speed 
change in a braking cycle. Consequently, if the driver wants the same braking performance 
for different initial speeds, the brake pedal travel should be always changed (see Figure 6). 
Due to the friction couple speed sensitivity, it provokes number negative effects during 
braking. It happened for relatively low brake interface temperature between 70 and 95°C. 
This situation could be drastically changed if the braking was happened at elevated the brake 
interface temperatures [14, 17]. 
In the case of very high initial braking torque values (friction coefficient), the drivers 
often feel that brakes are so “grabby”. On the other hand, drivers can feel absence of braking 
in the case of low initial braking torque. That is why, the brake pedal feel assessing should 
consider so called in-stop friction performance because a significant fluctuation of braking 
torque could be occurred. These fluctuations of braking torque reflect a complex friction 
pair‟s contact situation. Figure 6 shows that in-stop friction performance of the brake is not 
good. According to [6], optimal in-stop performance could be defined as 10-20% increasing 
of friction, i.e. braking torque. According to Figure 6, 45% in-stop friction performance was 
obtained for initial speed of 88 km/h. High in-stop friction performance result in a grabby 
pedal feel at the end of braking and a soft pedal feel at the start of braking. It causing 
unpleasant feel of the vehicle braking performance and often the brake is prone to the noise 
generation at the end of braking. Moreover, the braking distance could be drastically 
increased, which is affecting the active safety of a vehicle. That is why, an initial value of 
braking torque as well as its value at the end of braking should be corrected to obtain 
relatively stable the brake performance regardless the vehicle initial speed and a brake 
actuation pressure change. 
 
 
Figure 5. A hybrid ANN - GA optimization model. 

 
 
Figure 6. Optimization of the brake actuation pressure versus braking torque (pedal travel 40%). 

“Smart Brakes” – A Neuro-Genetic Optimization of Brake Actuation Pressure 
95
It was mentioned that stabilization and maximization of braking torque could be done on 
different ways. The first approach was to provide the maximum brake performance, 
regardless of the vehicle speed. This approach preserves the “nature” of the brake 
performance i.e. its inherent in-stop friction performance (see line Braking torque 1 in Figure 
6). It means that the brake is forced to reach its maximum performance for selected value of 
the brake actuation pressure, but the way of the brake performance reaching is not controlled 
during a braking cycle. The second approach was related to the correction of the brake start-
stop performance during a braking cycle, providing stable (relatively constant) and at the 
same time not so grabby performance (see line Braking torque 2 in Figure 6), for the specific 
brake pedal travel. These two changes of the braking torque have been selected as wanted 
ones for which the brake actuation pressure was separately optimized versus the same 
pressure selected by the driver. It is important to emphasize that the dynamic adaptation of the 
brake performance has been done in a braking cycle. Based on this, the wanted brake 
performance could be differently defined. It is important particularly in the case when the 
braking torque has to be adapted to the adhesion tire-road condition [26, 27]. 
Results of the optimization process are shown in Figure 6. Lines denoted as Optimized 
pressure 1 and Optimized pressure 2 represent optimized change of the brake actuation 
pressure that should be used for the brake actuation in order to provide changes of braking 
torque denoted as Braking torque (1) and Braking torque (2), respectively. It is evident in 
Figure 6 that, for the selected brake pedal travel of 40% and generated real brake applied 
pressure of 22 bar, the brake should be activated with the higher pressure. This change, in 
both cases, was shown as Optimized pressure (1) and Optimized pressure (2). It means that in 
the first case the mean maximum value of the brake applied pressure should be increased on 
29 bar. Otherwise, for wanted braking torque change according to the line Braking torque (2), 
the mean maximum value of the brake actuation pressure should be increased on 32 bar (see 
Figure 6). According to Figure 6, the neuro – genetic optimization model, based on the 
dynamic model of brake performance, was found the optimum change of the brake actuation 
pressure during a braking cycle which generated the braking torque on the wanted level.  
The previous braking situation is related to the most common so called light braking 
condition with relatively low value of the brake actuation pressure. As it was shown, the 
brake performance in such braking conditions was very sensitive to the sliding speed change. 
In order to analyze the braking situation, which is more critical from the braking distance 
point of view and the brake pedal feel, the brake pedal travel was selected on 62% (see Figure 
7). According to Figure 7, the brake actuation pressure generated by the driver was 42 bar 
(Real pressure). The maximum range of real braking torque variation, for different initial 
speeds, was around 400 Nm. This common braking situation should be avoided because it 
provokes poor brake pedal feel and a longer braking distance. The minimum braking torque 
was obtained for initial speed of 88 km/h, contrary to the previous case, and maximum 
braking torque was obtained for initial speed of 36 km/h. As it was explained in the case of 
the brake pedal travel of 40% (see Figure 6), the brake pressure has been optimized for 
known two cases. Slight changes in the brake actuation pressure at the start of the braking 
cycle could partially suppress or eliminate a difference in braking torque caused by the 
friction pair speed sensitivity. Increasing of the brake applied pressure to 47 bar at the start of 
braking, the brake responsiveness has been improved and braking torque change has been 
relatively stabilized (Figure 7, see line Braking torque 2). It is evident from Figure 7 that the 
brake pedal feel has been changed regarding the previous situation shown in Figure 6. It can 

Dragan Aleksendrić and Velimir Ćirović 
96
be seen that for the brake pedal travels until 40%, the brakes generated low braking torque 
vales (see Figure 7). For the brake pedal travels between 40 and 50%, the brake 
responsiveness has been substantial increased. In the range between 60 and 80% of the brake 
pedal travels, the brake responsiveness has been deceased but for such selected the brake 
actuation pressure the maximum braking torque could be reached.  
In order to correct the brake performance at the end of braking cycle in the sense of its 
stabilization, the brake pressure has being slightly decreased during (see Optimized pressure 
2; Figure 7). Furthermore, braking time and braking distance would be shorted. According to 
Figures 6 and 7, the brake performance could be controlled by correcting the brake actuation 
pressure selected by a driver. It is important to note that the brake performance could be 
better stabilized for the brake actuation pressures more than 45 bar. It could be explained as 
an effect of the friction material compressibility. As it is shown in Figure 8, the brake 
performance has been stronger influenced by synergy of the brake actuation pressure 
increasing combined with the speed change. 
In the case of braking with almost full brake pedal travel, it is very important to have a 
stable and maximum performance at the start of a braking cycle. This braking situation, for 
the brake pedal travel of 78%, caused generation of the brake actuation pressure of 59 bar. It 
means that in braking situation which needs maximum brake performance, the front brakes 
would be actuated by high pressure value. Using the neuro- genetic optimization of the brake 
actuation pressure, the pressure could not be only increased, as it is a case with traditional 
brake assistance system, but also the brake performance were better controlled during a 
braking cycle. According to Figure 8, the neuro genetic optimization model suggested that the 
brake pressure should be increased on 80 bar at the start of braking, providing not only the 
brake maximum performance but also stable performance during a braking cycle (see Figure 
8, lines Optimized pressure 2 and Predicted braking torque 2). It needed, see Figure 8, 
significant increasing of braking torque in the first 2 sec of a braking cycle. Regarding the 
line Braking torque (2) in Figure 8, the maximum value of braking torque (1000 Nm) was 
obtained after 0.7 sec from the start of braking. Of course, the braking system transmission 
capabilities to generate such pressure and to increase the pressure on that level in very short 
period of time should be taken consideration. Obviously, the current brake conditions and its 
performance were on the level that required high increasing of the actuation pressure in order 
to correct the brake performance. However, the brakes should be actuated by the maximum 
pressure at the start of braking. It shows that the brake performance could be corrected in 
some level but the space for brake performance correction is limited by the real brake 
characteristics and the braking system transmission capabilities. This approach could be used 
for the brake assist proposes. From Figure 8, it can be again seen that the brake performance 
was oscillated versus change of an initial speed. The maximum performance was reached for 
speed of 36 km/h and minimum for 87 km/h, which is an unwanted situation.  
 

 
 
Figure 7. Optimization of the brake actuation pressure versus braking torque (pedal travel 62%). 

 
 
Figure 8. Optimization of the brake actuation pressure versus braking torque (pedal travel 78%). 

“Smart Brakes” – A Neuro-Genetic Optimization of Brake Actuation Pressure 
99
CONCLUSION 
 
The possibilities for optimization and improvement of the brake performance have been 
investigated using a neuro – genetic approach. The neuro – genetic optimization model of the 
brake performance has been developed able to correct the brake performance versus influence 
of the speed-pressure-temperature change during a braking cycle for the specific type of a 
brake and selected the brake pedal travels. The model has shown capabilities to recognize 
influences of the pressure, speed and the brake interface temperature on a dynamic change of 
braking torque and to optimize the brake actuation pressure. The variation of brake actuation 
pressure has been used for correction of braking torque in a braking cycle. It was shown that 
maximum and constant the brake performance could be provided by a dynamic adaptation of 
the brake actuation pressure regardless a vehicle speed and previous value of the brake 
actuation pressure. The model has been developed to cover a wide range of pressure–speed– 
temperature change. It was shown that through optimization of the brake performance, the 
maximum and stable performance of the disc brake could be provided. This approach gives 
opportunities for customization of the brake performance in terms of selection of wanted 
braking toque change and its responsiveness against different braking situation (gentle, 
normal or emergency braking). 
 
 
REFERENCES 
 
[1] 
Aleksendrić, D. & Ćirović, V. (2012). Dynamic control of disc brake performance. SAE 
Technical Paper, 2012-01-1837. 
[2] 
Ćirović V. & Aleksendrić, D. (2011). Dynamic modelling of disc brake contact 
phenomena. FME Transactions, Vol. 39, 177-183. 
[3] 
Aleksendrić, D., Duboka, Ĉ. & Ćirović, V. (2008). Intelligent control of disc brake 
operation. SAE Technical Paper, 2008-01-2570. 
[4] 
Ćirović, V. & Aleksendrić, D. (2008). Intelligent control of automotive braking system. 
FISITA 2008 World Automotive Congress, F2008-SC-046. 
[5] 
Sellgren, U. & Söderberg, A. (2009). Robust Brake-Feel Design. Proceedings NAFEMS 
World Congress, Greece. 
[6] 
Day, A. J., Ho, H. P., Hussain, K. & Johnstone, A. (2009). Brake system simulation to 
predict brake pedal feel in a passenger car. SAE Technical Paper, 2009-01-3043.  
[7] 
Hecht, R., Sanders, P., Hartsock, D. & Evans, C. (2002). Correlation of lining 
properties with brake pedal feel. SAE Technical Paper, 2002-01-2602, 155-161.  
[8] 
Aleksendrić, D., Duboka, Ĉ. & Ćirović, V. (2008). Intelligent Control of disc brake 
operation. SAE Technical Paper, 2008-01-2570.  
[9] 
Ćirović, V. & Aleksendrić, D. (2010). Development of neural network model of disc 
brake operation. FME Transactions, Vol. 38, 29-38.  
[10] Ericsson, M. (2000). Friction and Contact Phenomena of Disc Brakes Related to 
Squeal. Comprehensive summaries of Uppsala dissertation from Faculty of Science and 
Technology.  
[11] Xiao, G. & Zhu, Z. (2010). Friction materials development by using DOE/RSM and 
artificial neural network. Tribology International, Vol. 43, 218-227.  

Dragan Aleksendrić and Velimir Ćirović 
100
[12] Talib, R. J., Muchtar, A. & Azhari, C. H. (2003). Microstructural characteristics on the 
surface and subsurface of semimetallic automotive friction materials during braking 
process. Journal of Materials Processing Technology, Vol. 140, 694–699.  
[13] Rukiye, E. & Nurettin, Y. (2010). An experimental study of the effects of 
manufacturing parameters on the tribological properties of brake lining. Wear, Vol. 
268, 1524-1532.  
[14] Aleksendrić, D. & Duboka, C. (2007). Fade performance prediction of automotive 
friction materials by means of artificial neural networks. Wear, Vol. 262, 778-790.  
[15] Aleksendrić, D. & Duboka, C. (2006). Prediction of automotive friction material 
characteristics using artificial neural networks-cold performance, Wear, Vol. 261, 269-
282.  
[16] Aleksendrić, D. & Ćirović, V. (2010). Effect of friction material manufacturing 
conditions on its wear. SAE Technical Paper, 2010-01-1679.  
[17] Aleksendrić, D. An inverse neural network model of disc brake performance at elevated 
temperatures. In Editor: John A. Flores. Focus on Artificial Neural Networks, Nova 
Science Publishers, Inc., New York, 2011, 151-170.  
[18] Aleksendrić, D. Manufacturing of Brake Friction Materials. In Editor: Anthony B. 
Savarese. Manufacturing Engineering, Nova Science Publishers, Inc., New York, 2013, 
89-108.  
[19] Aleksendrić, D. & Senatore, A. (2012). Optimization of manufacturing process effects 
on brake friction material wear, Journal of Composite Material, 46 (22), 2777-2791.  
[20] Aleksendrić, D., Duboka, Ĉ. & Mariotti, G. V. (2008). Neural modelling of friction 
material cold performance, Proc. IMechE Part D: J. Automobile Engineering, Vol. 222 
No. 7, 1021-1029.  
[21] Aleksendrić, D. & Duboka, Ĉ. (2008). Artificial technologies in sustainable braking 
system development. International Journal of Vehicle Design, Vol. 46, 237–249.  
[22] Aleksendrić, D., Barton, D. C. & Vasić, B. (2010). Prediction of brake friction 
materials recovery performance using artificial neural networks. Tribology 
International, Vol. 43, 2092-2099.  
[23] Aleksendrić, D. & Barton, D. C. (2009). Neural network prediction of disc brake 
performance. Tribology International, Vol. 42, 1074-1080.  
[24] Aleksendrić, D., Jakovljević, Ž. & Ćirović V. (2012). Intelligent control of braking 
process. Expert Systems with Applications, Vol. 39, 11758-11765.  
[25] Ćirović, V., Aleksendrić, D. & Mladenović, D. (2012). Braking torque control using 
recurrent neural networks. Proceedings of the Institution of Mechanical Engineers, Part 
D: Journal of Automobile Engineering, Vol. 226, 754-766.  
[26] Ćirović, V. & Aleksendrić, D. (2013). Adaptive neuro-fuzzy wheel slip control. Expert 
Systems with Applications, Vol. 40/13, 5197-5209.  
[27] Ćirović, V., Aleksendrić, D. & Smiljanić, D. (2013). Longitudinal wheel slip control 
using dynamic neural networks, Mechatronics, Vol. 23, 135-146.  
[28] Aleksendrić D. (2013). Intelligent Braking – Technology, Performance and Economic 
Challenge. In Editors: Michal Zajac and Roman Nowaczek. Airports and the 
Automotive Industry: Security Issues, Economic Efficiency and Environmental Impact, 
Nova Science Publishers, Inc., New York, 33-64.  
[29] Simon, J. R. & Cole, D. (2010). Modelling nonlinear vehicle dynamics with neural 
networks. International Journal of Vehicle Design, Vol. 53, 260-287.  

“Smart Brakes” – A Neuro-Genetic Optimization of Brake Actuation Pressure 101
[30] Sinha, K., Krishnan, R. & Raghavendra, D. (2007). Multi-objective robust optimization 
for crashworthiness during side impact. International Journal of Vehicle Design, Vol. 
43, 116-135.  
[31] Masten, K. M. (1998). Electronics: The intelligence and intelligent control. Annual 
Reviews, Vol. 22, 1-11.  
[32] Terzic, E., Nagarajah, C. R. & Alamgir, M. (2010). Capacitive sensor-based fluid level 
measurement in a dynamic environment using neural network. Engineering 
Applications of Artificial Intelligence, Vol. 23, 614-619.  
[33] Beale, M. H., Hagan, M. T. & Demuth, H. B. (2010). Neural Network Toolbox™ 7, 
Users guide Version 7. 0 (Release 2010b), The Math Works, Inc.  
[34] Lawrynczuk, M. (2011). Accuracy and computational efficiency of suboptimal 
nonlinear predictive control based on neural models. Applied Soft Computing, Vol. 11, 
2202-2215.  
[35] Kesgin, U. (2004). Genetic algorithm and artificial neural network for engine 
optimization of efficiency and NOx emission. Fuel, Vol. 83, 885-895.  
[36] Weise, T. (2009). Global Optimization Algorithms – Theory and Application. http:// 
www. it-weise. de/.  
[37] Wu, M. H., Lin, W. & Duan, S. Y. (2006). Developing a neural network and real 
genetic algorithm combined tool for an engine test bed. Proceedings of the Institution of 
Mechanical Engineers, Part D: Journal of Automobile Engineering, Vol. 220, 1737-
1753.  
[38] Lee, K. M. et al. (2010). Improved genetic algorithm for mixed-discrete-continuous 
design optimization problems, Engineering Optimization, Vol. 42, 927-941.  
[39] Thompson, G. J., Atkinson, C. M., Clark, N. N., Long, T. W. & Hanzevack, E. (2000). 
Technical Note: Neural network modelling of the emissions and performance of a 
heavy-duty diesel engine, Proceedings of the Institution of Mechanical Engineers, Part 
D: Journal of Automobile Engineering, Vol. 214, 111-126.  
[40] Manan, A. et al. (2009). Optimization of aeroelastic composite structures using 
evolutionary algorithms, Engineering Optimization, Vol. 42, 171-184.  
[41] Mashinchi, M. R. & Selamat, A. (2009). An improvement on genetic-based learning 
method for fuzzy artificial neural networks. Applied Soft Computing, Vol. 9, 1208-
1216.  
[42] Ko, Y. D., Moon, P., Kim, C. E., Ham, M. H., Myoung, J. M. & Yun, I. (2009). 
Modelling and optimization of the growth rate for ZnO thin films using neural networks 
and genetic algorithms. Expert Systems with Applications, Vol. 36, 4061-4066.  
[43] Sahoo, B. & Maity, D. (2007). Damage assessment of structures using hybrid neuro-
genetic algorithm. Applied Soft Computing, Vol. 7, 89-104.  
[44] Srinivasu, D. S. & Babu, N. R. (2008). A neuro-genetic approach for selection of 
process parameters in abrasive water jet cutting considering variation in diameter of 
focusing nozzle. Applied Soft Computing, Vol. 8, 809-819.  
[45] Blanco, A., Delgado, M. & Pegalajar, M. C. (2001). A real-coded genetic algorithm for 
training recurrent neural networks. Neural Networks, Vol. 14, 93-105.  


 
 
 
 
 
 
 
 
 
 
INDEX 
 
 
A 
access, 2, 3 
accounting, 14, 21, 26 
ACL, 27, 28 
actuation, vii, viii, 85, 86, 87, 88, 89, 92, 93, 94, 95, 
96, 97, 98, 99 
actuation pressure, vii, viii, 85, 86, 87, 88, 89, 92, 93, 
94, 95, 96, 97, 98, 99 
adaptation, 27, 87, 91, 95, 99 
adhesion, 95 
adjustment, 37, 40 
adverse effects, 32 
aesthetics, 9 
age, 46, 47 
algorithm(s), viii, 3, 4, 10, 13, 17, 34, 37, 38, 40, 45, 
56, 59, 60, 61, 62, 63, 64, 77, 78, 80, 82, 84, 86, 
87, 88, 90, 91, 92, 101 
anaerobic digestion, 57 
apex, 70, 73 
artificial intelligence, vii, 33, 34, 87, 91 
Artificial Neural Networks, v, 31, 34, 48, 50, 57, 100 
assessment, 101 
attitudes, 2 
Automobile, 100, 101 
avoidance, 61, 71, 83 
B 
bacteria, 46, 47 
base, 57 
behaviors, 17 
Beijing, 28 
benefits, 20, 21 
bias, 2, 11, 16, 20, 21, 36, 37, 38 
biodegradable materials, 47 
biological activities, 47 
biological system, vii, 31, 32 
brain, 34, 56 
braking system, viii, 85, 86, 87, 96, 99, 100 
C 
calibration, 4, 33 
candidates, 3 
capital expenditure, 32 
case study, 27, 34 
casting, 61 
categorization, 10, 21, 23, 30 
category a, 24 
CBP, 66, 67 
challenges, 8, 62 
changing environment, 60 
chemical(s), 32, 46 
children, 91 
China, 27, 28 
chromosome, 91 
CIS, 44 
classes, 3, 4, 5, 7, 10, 11, 14, 22, 23, 24, 25, 26, 38 
classification, vii, 1, 3, 4, 5, 6, 7, 8, 9, 12, 14, 15, 21, 
22, 23, 26, 27, 28, 29, 30, 40, 56, 79 
climate, 33 
clustering, 38, 56 
clusters, 38, 41 
coding, 23, 91 
commercial, 7 
communication, 3, 4, 5 
compaction, 47 
complexity, vii, 1, 8, 12, 21, 26, 33, 56, 60, 62 
compliance, 42, 55 
complications, 63 
composition, 32 
compressibility, 86, 88, 96 
compression, 40 
computation, 60, 61, 62, 63, 66, 68, 72, 74, 82 
computer, 3, 32, 55, 61, 62 

Index 
104
computing, 34, 36, 55 
conference, 13, 27, 29, 30 
configuration, 83 
Congress, 99 
consent, 42, 55 
construction, 9, 10, 32 
consumers, 1 
continuous data, 2 
contradiction, 15, 17 
correlation(s), 43, 45, 46, 47, 49, 88 
correlation analysis, 43, 45 
correlation coefficient, 43, 45, 49 
cost, 32, 72, 75, 78, 79, 86 
cost saving, 32 
CPU, 80 
crop, 55 
cure, 12 
customer service, 21 
cycles, 45, 77 
D 
data analysis, 5, 41 
data collection, 6 
data gathering, 2, 3, 6, 26 
data mining, 41 
data processing, 82 
data set, 37, 38, 40, 41, 44, 54, 55, 56, 61, 62, 64, 68, 
80, 81, 82, 89 
data structure, 65 
database, 42, 56, 57 
decelerate, viii, 85 
denitrification, 47 
dependent variable, 6 
depth, 68, 77 
detection, viii, 56, 59, 60, 62, 63, 64 
detergents, 47 
deviation, 64 
diffusion, 3 
dimensionality, 8, 29, 33, 40 
diodes, 80 
disc brake, viii, 85, 86, 87, 88, 89, 90, 99, 100 
discharges, 55 
disinfection, 42 
dissolved oxygen, 43, 46, 47 
distribution, 14, 24, 25, 46 
diversity, 92 
drawing, 63 
dynamic control, viii, 85, 87, 91 
dynamic systems, 55, 62 
E 
effluent, vii, 31, 32, 42, 43, 45, 46, 47, 48, 49, 50, 
51, 52, 53, 54, 55 
elaboration, 34 
emergency, 99 
emission, 101 
emotion, 2, 27 
energy, 32 
enforcement, 55 
engineering, 1, 3, 5, 6, 8, 11, 12, 13, 15, 16, 17, 21, 
23, 25, 26, 41, 55 
entrapment, 47 
entropy model, 4 
environment(s), vii, viii, 32, 42, 59, 60, 61, 62, 66, 
67, 68, 78, 79, 80, 82, 83, 101 
equality, 54 
EU, 32 
evapotranspiration, 55 
evidence, 26, 41 
exploitation, 87 
extraction, 5, 28, 62, 63, 64, 68 
F 
feature selection, 3, 4, 8, 13, 17, 27, 30 
features extraction, vii, 31 
fever, 12, 13 
Field of View (FOV), 80 
financial, 21 
fish, 15 
fitness, 91, 92 
fluctuations, 93 
fluid, 101 
food, 43 
force, 23 
Ford, 60, 80, 82 
forecasting, 28 
France, 29, 84 
friction, viii, 85, 86, 87, 88, 92, 93, 95, 96, 100 
G 
genes, 91 
genetics, 91 
genotype, 91 
genre, 4 
Geographic Information System, 84 
geometric shapes, viii, 59, 60, 62, 63, 64, 65, 82 
geometry, 69 
Georgia, 84 
Germany, 29 

Index 
105
GIS, 61 
graduate students, 22, 23 
graph, 62 
grazing, 70 
Greece, 99 
grouping, 16 
growth, 3, 32, 46, 101 
growth rate, 101 
guidelines, 7, 13, 23 
H 
Hamiltonian, 73 
heavy metals, 47, 55 
Hong Kong, 27 
housing, 42 
human, 4, 6, 7, 14, 22, 23, 33, 34, 46 
human brain, 34 
hybrid, vii, viii, 5, 31, 34, 41, 42, 45, 49, 54, 55, 85, 
87, 91, 93, 101 
I 
ICS, 61, 71, 75 
ID, 2, 13, 14 
ideal, 39, 45 
identification, 5, 26, 33, 39, 40 
identity, 27 
improvements, 26 
indexing, 8 
India, 29 
indirect effect, 89 
individuals, 91, 92 
industry, 32 
ineffectiveness, 26 
information processing, 34 
information retrieval, 8 
input signal, 36 
intelligence, vii, 101 
interface, viii, 2, 85, 86, 88, 89, 92, 93, 99 
Israel, 59 
issues, 3, 27, 57, 88, 92 
J 
Japan, 57 
K 
Kinsey, 13 
L 
language processing, 7 
languages, 27 
lasers, 80 
lead, 40 
learning, 4, 23, 28, 33, 34, 37, 38, 45, 87, 88, 89, 90, 
101 
learning process, 34, 89 
leisure, 42 
lending, 8 
lidar, 82, 84 
light, 9, 15, 21, 61, 95 
Likert scale, 7 
linear model, 2 
linear systems, viii, 85 
lipids, 47 
love, 13, 16, 17 
M 
machine learning, 1, 3, 4, 5, 6, 13, 22, 29, 30 
magnitude, 2 
major issues, 4 
majority, 3, 5, 62 
management, 2, 29 
manufacturing, 100 
map unit, 55 
mapping, 38, 61, 63 
Mars, 71, 72 
mass, 71 
materials, 33, 99, 100 
mathematics, 33 
matrix, 6, 8, 13, 16, 43, 44, 64, 65 
matter, 63 
measurement(s) 33, 41, 43, 55, 63, 101 
media, 3, 13, 30 
media messages, 13 
memory, 88, 89 
messages, vii, 1, 2, 3, 7, 10, 11, 12, 13, 14, 15, 16, 
17, 20, 21, 25, 26 
methodology, viii, 31, 42, 55, 63, 88 
mobile robots, 83 
modelling, vii, viii, 31, 32, 33, 34, 41, 43, 45, 55, 56, 
57, 99, 100, 101 
models, vii, 1, 2, 4, 5, 7, 8, 14, 21, 22, 24, 25, 32, 33, 
34, 44, 45, 48, 49, 50, 53, 54, 56, 61, 89, 90, 101 
momentum, 38 
mood states, 6 
Moon, 28, 101 
multidimensional, 46 
music, 23 

Index 
106
mutation, 91 
N 
natural selection, 91 
negative effects, 93 
negative influences, 92 
nervous system, 34 
neural network(s), vii, 4, 28, 31, 34, 35, 38, 45, 53, 
55, 56, 57, 86, 87, 88, 89, 90, 91, 99, 100, 101 
Neural Network Model, 28 
neurons, 34, 35, 38, 39, 40, 45, 48, 49, 50, 51, 53, 
54, 89, 90 
neuroscience, 34 
neutral, 7, 20, 21, 24, 26 
next generation, 91 
nitrogen gas, 47 
nodes, 35, 36, 37, 38, 39, 50, 54, 71, 76, 78 
nutrients, 55 
O 
obstacles, 60, 68, 76, 77, 82, 83 
operations, 91 
opportunities, 99 
optimal time horizon, viii, 59, 60, 61, 68, 71, 72, 74, 
77 
optimization, vii, viii, 37, 45, 71, 85, 86, 87, 88, 89, 
90, 91, 92, 93, 95, 96, 99, 101 
oxidation, 47 
oxygen, 33, 42, 46, 47 
P 
parallel, 34 
parents, 91 
participants, 3 
partition, 5 
path planning, 83 
pedal, viii, 85, 86, 87, 88, 89, 92, 93, 94, 95, 96, 97, 
98, 99 
permit, 71 
pH, 42 
phenotype, 91 
physics, 34 
plants, vii, 32, 33, 34, 55, 57 
platform, 3 
polar, 73 
pollution, 32, 55 
poor performance, 49 
population, 3, 41, 42, 46, 47, 91 
population size, 47 
positive correlation, 46 
positive feedback, 11 
prejudice, 10 
preparation, vii, 31 
private finance initiative, 42 
programming, 44, 89 
project, 2, 42, 57 
proliferation, 46 
propagation, vii, 31, 34, 37, 89 
protection, 32 
prototype, 40 
publishing, 11 
PVS, 61 
Q 
quality standards, 32 
quantization, 40 
query, 2, 6, 10 
R 
radius, 63, 65, 69, 88 
rainfall, 55, 56 
reactions, 3, 27 
reading, 23 
real numbers, 91 
real time, 1, 33, 68 
reality, 32 
reasoning, 33 
recall, vii, 1, 21, 22, 24, 25 
recognition, 23 
reconstruction, 56, 62, 84 
recovery, 100 
regulations, 32, 55 
regulatory agencies, 32 
relevance, 8, 11 
reliability, 34, 56, 86 
reproduction, 91 
requirements, 86 
researchers, 2, 3, 4, 5, 8, 20, 21, 25 
residuals, 54 
resources, 4, 5 
response, 2, 33, 36, 37, 89 
response time, 33 
responsiveness, 95, 99 
risk, 16, 57 
risk assessment, 57 
robotics, 61 
rules, 35, 91 
runoff, 55, 56, 58 

Index 
107
S 
safety, 60, 75, 83, 86, 93 
scatter, 53, 54 
scatter plot, 53 
scripts, 9 
search space, 91 
sedimentation, 42, 43 
semantics, 5 
sensitivity, 26, 86, 90, 92, 93, 95 
sensors, 33, 42 
sentiment lexicon, vii, 1, 3, 4, 5, 6, 25 
Serbia, 85 
shape, 63, 64, 65, 78 
showing, viii, 15, 25, 26, 59, 60 
signs, 59, 62, 73 
silhouette, 66 
simulation(s), viii, 45, 59, 60, 74, 92, 99 
sludge, vii, 31, 32, 33, 34, 42, 46, 47, 55, 56, 57 
smart brake, vii, viii, 85, 87 
society, 3 
software, 8 
solubility, 47 
solution, 32, 60, 61, 64, 66, 68, 71, 72, 73, 74, 77, 
78, 79, 82, 91 
Spain, 27 
spam, 13 
specifications, 45 
speech, 4, 5 
spin, 80 
SS, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 54, 55 
stability, viii, 32, 85, 87 
stabilization, viii, 85, 92, 95, 96 
standard deviation, 39, 44 
stars, 4 
state(s), viii, 7, 26, 59, 60, 61, 62, 63, 64, 65, 68, 71, 
72, 75, 77, 78, 82, 83, 85, 88, 89 
statistics, 22, 44 
stock, 3, 30 
stock markets, 3 
storage, 34, 56 
structure, 12, 34, 54, 63, 84, 89 
style, 4, 5 
success rate, viii, 59, 60, 62 
supervision, 2, 7, 28, 32 
surface area, 88 
Switzerland, 28 
synergistic, viii, 85, 86, 87, 92 
T 
tanks, 42, 43 
target, 6, 15, 37, 38, 44, 62 
techniques, 3, 4, 5, 8, 12, 21, 29, 32, 33, 34, 56, 91 
technologies, 100 
temperature, viii, 42, 43, 47, 85, 86, 87, 88, 89, 92, 
99 
test data, 24 
testing, 6, 14, 21, 22, 23, 24, 25, 26, 44, 49, 50, 52, 
54, 88 
Thailand, 27 
thin films, 101 
time series, 28, 54, 88 
tones, 6 
torque, viii, 85, 86, 87, 88, 89, 91, 92, 93, 94, 95, 96, 
97, 98, 99, 100 
tracks, 65 
training, vii, 2, 3, 6, 8, 12, 14, 21, 22, 23, 24, 25, 26, 
31, 34, 37, 38, 40, 41, 43, 44, 48, 49, 50, 52, 54, 
55, 90, 101 
trajectory, vii, viii, 59, 60, 61, 62, 63, 68, 69, 71, 73, 
75, 76, 77, 78, 79, 80, 82 
transformation, 35, 45 
transmission, 40, 96 
treatment, vii, 31, 32, 33, 34, 42, 43, 55, 56, 57 
trial, 36, 38 
tribological behavior, viii, 86 
turbulence, 47 
U 
unique features, 4, 5 
United Kingdom (UK), 31, 32, 42, 55 
updating, 40 
urban, vii, viii, 32, 56, 59, 60, 61, 62, 63, 68, 78, 79, 
80, 82 
urban areas, 32 
USA, 57, 84 
UV, 42, 43 
V 
validation, vii, 31, 33, 44, 48, 49, 50, 52, 54, 64 
variables, 2, 5, 32, 33, 36, 37, 40, 42, 43, 44, 45, 46, 
49, 53, 55, 65, 72, 74, 91 
variations, 32, 33, 92 
vector, 4, 5, 6, 8, 23, 36, 37, 39, 40, 41, 43, 46, 63, 
64, 65, 69, 70, 72, 77 
vehicles, viii, 59, 63, 83, 86 
velocity, 60, 61, 68, 69, 70, 71, 74, 75, 76, 77, 79, 82 
vision, 61, 62, 82 
visual system, 46 
visualization, 57 
vocabulary, 5 

Index 
108
W 
Washington, 28, 29, 84 
waste, 33, 47 
wastewater, vii, 31, 32, 33, 34, 42, 43, 47, 55, 56, 57 
water, vii, 31, 32, 34, 42, 47, 55, 57, 101 
water ecosystems, 32 
water quality, 32, 34, 42, 55 
water resources, 34 
web, 3, 4, 27, 28 
websites, 4 
word frequency, 9 
workflow, 23 
World Wide Web, 29 
WWW, 28 
Y 
yield, 13, 34, 56 
Z 
ZnO, 101 
 


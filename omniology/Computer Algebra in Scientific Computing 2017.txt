Vladimir P. Gerdt · Wolfram Koepf
Werner M. Seiler · Evgenii V. Vorozhtsov (Eds.)
 123
LNCS 10490
19th International Workshop, CASC 2017
Beijing, China, September 18–22, 2017
Proceedings
Computer Algebra 
in Scientific Computing

Lecture Notes in Computer Science
10490
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, Lancaster, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Friedemann Mattern
ETH Zurich, Zurich, Switzerland
John C. Mitchell
Stanford University, Stanford, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Dortmund, Germany
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max Planck Institute for Informatics, Saarbrücken, Germany

More information about this series at http://www.springer.com/series/7407

Vladimir P. Gerdt
• Wolfram Koepf
Werner M. Seiler
• Evgenii V. Vorozhtsov (Eds.)
Computer Algebra
in Scientiﬁc Computing
19th International Workshop, CASC 2017
Beijing, China, September 18–22, 2017
Proceedings
123

Editors
Vladimir P. Gerdt
Joint Institute of Nuclear Research
Dubna
Russia
Wolfram Koepf
Universität Kassel
Kassel
Germany
Werner M. Seiler
Universität Kassel
Kassel
Germany
Evgenii V. Vorozhtsov
Russian Academy of Sciences
Novosibirsk
Russia
ISSN 0302-9743
ISSN 1611-3349
(electronic)
Lecture Notes in Computer Science
ISBN 978-3-319-66319-7
ISBN 978-3-319-66320-3
(eBook)
DOI 10.1007/978-3-319-66320-3
Library of Congress Control Number: 2017950084
LNCS Sublibrary: SL1 – Theoretical Computer Science and General Issues
© Springer International Publishing AG 2017
The chapter ‘Symbolic Versus Numerical Computation and Visualization of Parameter Regions for
Multistationarity of Biological Networks’ is licensed under the terms of the Creative Commons Attribution
4.0 International License (http://creativecommons.org/licenses/by/4.0/). For further details see license infor-
mation in the chapter.
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
The International Workshop on Computer Algebra in Scientiﬁc Computing (CASC) is
a leading conference which provides the opportunity for all researchers from home and
abroad to present their research results annually. CASC is the forum of excellence for
the exploration of the frontiers in the ﬁeld of computer algebra and its applications in
scientiﬁc computing. It brings together scholars, engineers, and scientists from various
disciplines including computer algebra. This workshop provides a platform for the
delegates to exchange new ideas and application experiences, share research results,
discuss existing issues and challenges, and explore international cooperation in
cutting-edge technology face to face.
This year, the 19th CASC conference was held in Beijing (China). Study on
computer algebra in China started with the work of Prof. Wen-Tsun Wu on automated
geometry theorem proving and characteristic set methods for polynomial equation
solving in the late 1970s. In 1990, the Research Center of Mathematics Mechanization
(MMRC) was established in the Chinese Academy of Sciences, and the center runs a
series of academic programs on computer algebra and related areas.
In particular, jointly with the Japanese Society for Symbolic and Algebraic Com-
putation, the Asian Symposium on Computer Mathematics was started in 1995 and
held every two years. MMRC also organized The International Symposium on Sym-
bolic and Algebraic Computation (ISSAC) in 2005. Other major research groups
include the Laboratory of Automated Reasoning at the Chongqing Branch of the
Chinese Academy of Sciences led by Prof. Jingzhong Zhang, and the research group at
Beihang University led by Prof. Dongming Wang. In 2007, the Computer Mathematics
Society of China was established, and Prof. Xiao-Shan Gao was the founding president
of the society. The society runs an annual conference with approximately 100–150
participants.
Prof. Xiao-Shan Gao has kindly agreed to be one of the General Chairs of the CASC
2017 workshop. This has affected the choice of Beijing as a venue for CASC 2017.
This volume contains 26 full papers submitted to the workshop by the participants
and accepted by the Program Committee after a thorough reviewing process with
usually three independent referee reports. Additionally, the volume includes two
contributions corresponding to the invited talks.
Polynomial algebra, which is at the core of computer algebra, is represented by
contributions devoted to the convergence conditions of interval Newton’s method
applied to the solution of a nonlinear system; certifying the simple real zeros of
overdetermined polynomial systems with interval methods; decomposition of poly-
nomial sets into lexicographic Gröbner bases and into normal triangular sets; com-
putation of all the isolated solutions to a special class of polynomial systems with the
aid of a special homotopy continuation method; computing real witness points of
general polynomial systems with the aid of the penalty function based critical point
approach; algorithms for zero-dimensional ideals using linear recurrent sequences;

ﬁnding quasihomogeneous isolated hypersurface singularities with the aid of an
interface of the computer algebra system (CAS) POLYMAKE in the CAS SINGULAR; and
full rank representation of real algebraic sets with applications in visualizing plane and
space curves with singularities. Two papers deal with the problems arising in poly-
nomial interpolation: one focussing on the optimal knots selection for spline interpo-
lation in the case of sparse reduced data, and one focussing on sparse interpolation
algorithms for black box univariate or multivariate polynomials whose coefﬁcients are
from a ﬁnite set.
The invited talk of Lihong Zhi is devoted to computing multiple zeros of polynomial
systems. It shows how to compute the multiplicity structure of each multiple zero and
the lower bound on the minimal distance between the multiple zero and other zeros
of the system. The developed algorithms were implemented in the CAS Maple.
Several papers deal with using computer algebra for the investigation of various
mathematical and applied topics related to ordinary differential equations (ODEs),
focussing on, for example, the introduction of the concept of a Laurent Gröbner basis
for the investigation of the Laurent (differential) polynomial systems, and the study of
local integrability of an autonomous system of ODEs with the aid of an approach based
on power geometry.
The invited talk by S. Abramov handles the problem of the solvability of linear
systems of ordinary differential equations whose coefﬁcients have the form of inﬁnite
formal power series. The problem is to decide whether the system has non-zero Laurent
series, regular, or formal exponential-logarithmic solutions, and to ﬁnd all such solu-
tions if they exist. Maple-based procedures are presented for constructing local
solutions.
Four papers deal with applications of symbolic and symbolic-numeric computations
for investigating and solving partial differential equations (PDEs) and ODEs in
mathematical
physics
and
ﬂuid
mechanics,
focussing
on,
for
example,
the
symbolic-numeric integration of the dynamical Cosserat partial differential equations
describing the mechanical behavior of elastic rods; the symbolic-numeric solution with
Maple of the parametric self-adjoint 2D elliptic boundary-value problem with the aid of
a high-accuracy ﬁnite element method; and a new symbolic-numeric preconditioned
solver for incompressible Navier–Stokes equations using the integral form of collo-
cation equations.
Applications of CASs in mechanics, physics, and biology are represented by the
following themes: investigation of the asymptotic stability of a satellite with a gravi-
tational stabilizer; satellite dynamics subject to damping torques; Mathematica-based
analysis of the relative equilibria stability in a problem of celestial mechanics; sta-
tionary motions of the generalized Kowalewski gyrostat and their stability; and sym-
bolic versus numerical computation and visualization of parameter regions for
multistationarity of biological networks.
The remaining topics include the computation of some integer sequences in Maple;
algorithms for computing the integer points of a polyhedron; a divide and conquer
algorithm for sparse nonlinear interpolation; and normalization of indexed differentials
based on function distance invariants.
The CASC 2017 workshop was supported ﬁnancially by the National Center for
Mathematics and Interdisciplinary Sciences of the Chinese Academy of Sciences, the
VI
Preface

Academy of Mathematics and Systems Science of the Chinese Academy of Sciences,
and the Key Laboratory of Mathematics Mechanization of the Chinese Academy of
Sciences. We appreciate that they provided free accommodation for a number of
participants.
Our particular thanks are due to the members of the CASC 2017 local organizing
committee at the Key Laboratory of Mathematics Mechanization, Chinese Academy of
Sciences, Jin-San Cheng, Changbo Chen, Ruyong Feng, and Zhikun She, who ably
handled all the local arrangements in Beijing. In addition, Profs. Xiao-Shan Gao and
Jin-San Cheng provided us with the history of computer algebra activities in China.
Furthermore, we want to thank all the members of the Program Committee for their
thorough work. We are grateful to Matthias Orth (Universität Kassel) for his technical
help in the preparation of the camera-ready manuscript for this volume. Finally, we are
grateful to the CASC publicity chair, Andreas Weber (Rheinische Friedrich-Wilhelms-
Universität Bonn), and his assistant, Hassan Errami, for the design of the conference
poster and the management of the conference web page http://www.casc.cs.uni-bonn.de.
July 2017
Vladimir P. Gerdt
Wolfram Koepf
Werner M. Seiler
Evgenii V. Vorozhtsov
Preface
VII

Organization
CASC 2017 was organized jointly by the Institute of Mathematics at Kassel University
and the Key Laboratory of Mathematics Mechanization, Academy of Mathematics and
System Sciences, Chinese Academy of Science, Beijing, China.
Workshop General Chairs
Xiao-Shan Gao, Beijing
Vladimir P. Gerdt, Dubna
Werner M. Seiler, Kassel
Program Committee Chairs
Wolfram Koepf, Kassel
Evgenii V. Vorozhtsov, Novosibirsk
Program Committee
Moulay Barkatou, Limoges
François Boulier, Lille
Jin-San Cheng, Beijing
Victor F. Edneral, Moscow
Matthew England, Coventry
Jaime Gutierrez, Santander
Sergey A. Gutnik, Moscow
Thomas Hahn, Munich
Jeremy Johnson, Philadelphia
Victor Levandovskyy, Aachen
Marc Moreno Maza, London, Canada
Veronika Pillwein, Linz
Alexander Prokopenya, Warsaw
Georg Regensburger, Linz
Eugenio Roanes-Lozano, Madrid
Valery Romanovski, Maribor
Doru Stefanescu, Bucharest
Thomas Sturm, Saarbrücken
Akira Terui, Tsukuba
Elias Tsigaridas, Paris
Jan Verschelde, Chicago
Stephen M. Watt, W. Ontario, Canada

Additional Reviewers
Alexander Bathkin
Russell Bradford
Martin Bromberger
Alexander Bruno
Xiaojie Dou
Bruno Grenet
Qiaolong Huang
Manuel Kauers
Denis Khmelnov
Thomas Peter
Hamid Rahkooy
Daniel Robertz
Olivier Ruatta
Vasily Shapeev
Yilei Tang
Francis Valiquette
Nathalie Verdière
Andreas Weber
Zafeirakis Zafeirakopoulos
Local Organization
Jin-San Cheng, Beijing (Chair)
Changbo Chen, Chongqing
Ruyong Feng, Beijing
Zhikun She, Beijing
Publicity Chair
Andreas Weber, Bonn
Website
http://www.casc.cs.uni-bonn.de/2017
(Webmaster: Hassan Errami)
X
Organization

Contents
Linear Differential Systems with Infinite Power Series Coefficients
(Invited Talk) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
S.A. Abramov
On the Asymptotic Stability of a Satellite with a Gravitational Stabilizer . . . .
16
Andrei V. Banshchikov
Sparse Interpolation, the FFT Algorithm and FIR Filters . . . . . . . . . . . . . . .
27
Matteo Briani, Annie Cuyt, and Wen-shin Lee
On New Integrals of the Algaba-Gamero-Garcia System . . . . . . . . . . . . . . .
40
Alexander D. Bruno, Victor F. Edneral, and Valery G. Romanovski
Full Rank Representation of Real Algebraic Sets and Applications . . . . . . . .
51
Changbo Chen, Wenyuan Wu, and Yong Feng
Certifying Simple Zeros of Over-Determined Polynomial Systems. . . . . . . . .
66
Jin-San Cheng and Xiaojie Dou
Decomposing Polynomial Sets Simultaneously into Gröbner Bases
and Normal Triangular Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
Rina Dong and Chenqi Mou
Symbolic Versus Numerical Computation and Visualization of Parameter
Regions for Multistationarity of Biological Networks. . . . . . . . . . . . . . . . . .
93
Matthew England, Hassan Errami, Dima Grigoriev, Ovidiu Radulescu,
Thomas Sturm, and Andreas Weber
The Polymake Interface in Singular and Its Applications . . . . . . . . . . . . . . .
109
Raul Epure, Yue Ren, and Hans Schönemann
Computation of Some Integer Sequences in Maple . . . . . . . . . . . . . . . . . . .
118
W.L. Fan, D.J. Jeffrey, and Erik Postma
Symbolic-Numerical Algorithm for Generating Interpolation Multivariate
Hermite Polynomials of High-Accuracy Finite Element Method . . . . . . . . . .
134
A.A. Gusev, V.P. Gerdt, O. Chuluunbaatar, G. Chuluunbaatar,
S.I. Vinitsky, V.L. Derbov, and A. Góźdź

Symbolic-Numerical Algorithms for Solving the Parametric Self-adjoint 2D
Elliptic Boundary-Value Problem Using High-Accuracy Finite
Element Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
A.A. Gusev, V.P. Gerdt, O. Chuluunbaatar, G. Chuluunbaatar,
S.I. Vinitsky, V.L. Derbov, and A. Góźdź
A Symbolic Study of the Satellite Dynamics Subject to Damping Torques . . .
167
Sergey A. Gutnik and Vasily A. Sarychev
Characteristic Set Method for Laurent Differential Polynomial Systems . . . . .
183
Youren Hu and Xiao-Shan Gao
Sparse Polynomial Interpolation with Finitely Many Values
for the Coefficients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
196
Qiao-Long Huang and Xiao-Shan Gao
On Stationary Motions of the Generalized Kowalewski Gyrostat
and Their Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
210
Valentin Irtegov and Tatyana Titorenko
Computing the Integer Points of a Polyhedron, I: Algorithm. . . . . . . . . . . . .
225
Rui-Juan Jing and Marc Moreno Maza
Computing the Integer Points of a Polyhedron, II: Complexity Estimates . . . .
242
Rui-Juan Jing and Marc Moreno Maza
Non-linearity and Non-convexity in Optimal Knots Selection for Sparse
Reduced Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
257
Ryszard Kozera and Lyle Noakes
The Convergence Conditions of Interval Newton’s Method
Based on Point Estimates. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
272
Zhe Li, Baocheng Wan, and Shugong Zhang
Normalization of Indexed Differentials Based on Function
Distance Invariants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
285
Jiang Liu
Symbolic-Numeric Integration of the Dynamical Cosserat Equations . . . . . . .
301
Dmitry A. Lyakhov, Vladimir P. Gerdt, Andreas G. Weber,
and Dominik L. Michels
Algorithms for Zero-Dimensional Ideals Using Linear Recurrent Sequences . . .
313
Vincent Neiger, Hamid Rahkooy, and Éric Schost
Symbolic-Numerical Analysis of the Relative Equilibria Stability
in the Planar Circular Restricted Four-Body Problem. . . . . . . . . . . . . . . . . .
329
Alexander N. Prokopenya
XII
Contents

The Method of Collocations and Least Residuals Combining the Integral
Form of Collocation Equations and the Matching Differential Relations
at the Solution of PDEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
346
Vasily P. Shapeev and Evgenii V. Vorozhtsov
A Special Homotopy Continuation Method for a Class
of Polynomial Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
362
Yu Wang, Wenyuan Wu, and Bican Xia
Penalty Function Based Critical Point Approach to Compute Real Witness
Solution Points of Polynomial Systems . . . . . . . . . . . . . . . . . . . . . . . . . . .
377
Wenyuan Wu, Changbo Chen, and Greg Reid
Computing Multiple Zeros of Polynomial Systems: Case of Breadth One
(Invited Talk) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
392
Lihong Zhi
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
Contents
XIII

Linear Diﬀerential Systems with Inﬁnite Power
Series Coeﬃcients (Invited Talk)
S.A. Abramov(B)
Dorodnitsyn Computing Centre, Federal Research Center Computer Science
and Control of Russian Academy of Sciences, Vavilova, 40, Moscow 119333, Russia
sergeyabramov@mail.ru
Abstract. Inﬁnite power series may appear as inputs for certain math-
ematical problems. This paper examines two possible solutions to the
problem of representation of inﬁnite power series: the algorithmic rep-
resentation (for each series, an algorithm is speciﬁed that, given an
integer i, ﬁnds the coeﬃcient of xi, — any such algorithm deﬁnes a
so called computable, or constructive, series) and a representation in an
approximate form, namely, in a truncated form.
1
Introduction
Inﬁnite power series play an important role in mathematical studies. Those series
may appear as inputs for certain mathematical problems. In order to be able to
discuss the corresponding algorithms, we must agree on representation of the
inﬁnite series (algorithm inputs are always objects represented by speciﬁc ﬁnite
words in some alphabet). This paper examines two possible solutions to the
problem of representation of power series.
In Sect. 2, we consider the algorithmic representation. For each series in x,
an algorithm is speciﬁed that, given an integer i, ﬁnds the coeﬃcient of xi.
Any deterministic algorithms are allowed (any such algorithm deﬁnes a so called
computable, or constructive series). Here there is a dissimilarity with the publi-
cations [14], [15, Chap. 10], where some speciﬁc case of input (mainly the hyper-
geometric type) is considered, and the coeﬃcients of the power series which are
returned by the corresponding algorithms can be given “in closed form”.
For example, suppose that a linear ordinary diﬀerential system S of arbitrary
order with inﬁnite formal power series coeﬃcients is given, decide whether the
system has non-zero Laurent series, regular, or formal exponential-logarithmic
solutions, and ﬁnd all such solutions if they exist. If the coeﬃcients of the origi-
nal systems are arbitrary formal power series represented algorithmically (thus,
we are not able, in general, to recognize whether a given series is equal to zero
or not) then these three problems are algorithmically undecidable, and this can
be deduced from the classical results of Turing [21]. But, it turns out that the
S.A. Abramov—Supported in part by the Russian Foundation for Basic Research,
project No. 16-01-00174.
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 1–15, 2017.
DOI: 10.1007/978-3-319-66320-3 1

2
S.A. Abramov
ﬁrst two problems are decidable in the case when we know in advance that
a given system S is of full rank [5]. However, the third problem (ﬁnding for-
mal exponential-logarithmic solutions) is not decidable even in this case [3].
It is shown that, despite the fact that such a system has a basis of formal
exponential-logarithmic solutions involving only computable (i.e., algorithmi-
cally represented) series, there is no algorithm to construct such a basis. But, it
is possible to specify a limited version of the third problem, for which there is
an algorithm of the desired type: namely, if S and a positive integer d are such
that for the system S the existence of at least d linearly independent solutions
is guaranteed, then we can construct such d solutions [20].
It is shown also that the algorithmic problems connected with the ramiﬁca-
tion indices of irregular formal solutions of a given system are mostly undecidable
even if we ﬁx a conjectural value ρ of the ramiﬁcation index [2]. However, there
is nearby an algorithmically decidable problem: if a system S of full rank and
positive integers ρ, d are such that for S the existence at least of d linearly
independent formal solutions of ramiﬁcation index ρ is guaranteed then one can
compute such d solutions of S.
Thus, when we use the algorithmic way of power series representation, a
neighborhood of algorithmically solvable and unsolvable problems is observed.
For the solvable problems mentioned above, a Maple implementation is pro-
posed [9]. In Sect. 2.2, we report some experiments.
Note that the ring of computable formal power series is smaller than the
ring of all formal power series because not every sequence of coeﬃcients can
be represented algorithmically. Indeed, the set of elements of the constructive
formal power series is countable (each of the algorithms is a ﬁnite word in some
ﬁxed alphabet) while the set of all power series is uncountable.
In Sect. 3, we consider an “approximate” representation. A well-known exam-
ple is the results [16] related to the number of terms of entries in A that can
inﬂuence some components of formal exponential-logarithmic solutions of a dif-
ferential system xsy′ = Ay, where s is a given non-negative integer, A is a matrix
whose entries are power series. As a further example we consider matrices with
inﬁnite power series entries and suppose that those series are represented in an
approximate form, namely, in a truncated form. Thus, it is assumed that a poly-
nomial matrix P which is the l-truncation (l is a non-negative integer, deg P = l)
of a power series matrix A is given, and P is non-singular, i.e., det P ̸= 0. In [4], it
is proven that the question of strong non-singularity, i.e., the question whether
P is not the l-truncation of a singular matrix having power series entries, is
algorithmically decidable. Assuming that a non-singular power series matrix A
(which is not known to us) is represented by a strongly non-singular polynomial
matrix P, we give a tight lower bound for the number of initial terms of A−1
which can be determined from P −1.
We discuss the possibility of applying the proposed approach to “approxi-
mate” linear higher-order diﬀerential systems: if a system is given in the approx-
imate truncated form and the leading matrix is strongly non-singular then the
results [16,18] and their generalization can be used, and the number of reliable
terms of Laurent series solution can be estimated by the algorithm proposed in [6].

Inﬁnite Power Series Coeﬃcients
3
Theorems are known that if a system has a solution in the form of a series,
then this system also has a solution in the form of a series with some speciﬁc
properties such that the initial terms of these series coincide (and estimates of the
number of coinciding terms are given), see, e.g., [13]. To avoid misunderstandings,
note that this is a diﬀerent type of task. We are considering a situation where a
truncated system is initially given, and we do not know the original system. We
are trying to establish, whether it is possible to get from the solutions of this
system an information on solutions of any system obtained from this system by
a prolongation of the polynomial coeﬃcients to series.
The information that can be extracted from truncated series, matrices, sys-
tems, etc. may be suﬃcient to obtain certain characteristics of the original
(untruncated) objects. Naturally, these characteristics are incomplete, but may
suﬃce for some purposes.
In Sect. 4, we discuss the fact that the width of a given full-rank system S
with computable formal power series coeﬃcients can be found, where the width
of S is the smallest non-negative integer w such that any l-truncation of S with
l ⩾w is a full-rank system. It is shown also that the above-mentioned value w
exists for any full-rank system [5]. We introduce also the notion of the s-width.
This is done on the base of the notion of the strong non-singularity.
2
Algorithmic Representation
Deﬁnition 1. We suppose that for each series a(x) = ∞
i=0 aixi under consid-
eration, an algorithm Ξa (a procedure, terminating in ﬁnitely many steps) such
that a(x) = ∞
i=0 Ξa(i)xi, i.e., such that ai = Ξa(i) ∀i is given. We will call
such series computable (or constructive).
2.1
Computable Inﬁnite Power Series in the Role of Coeﬃcients of
Linear Diﬀerential Systems
Let K be a ﬁeld of characteristic 0. We will use the standard notation K[x] for
the ring of polynomials in x and K(x) for the ﬁeld of rational functions of x with
coeﬃcients in K. Similarly, we denote by K[[x]] the ring of formal power series
and K((x)) = K[[x]][x−1] its quotient ﬁeld (the ﬁeld of formal Laurent series)
with coeﬃcients in K. The ring of n × n-matrices with entries belonging to a
ring (a ﬁeld) R is denoted by Mat n(R).
Deﬁnition 2. A ring (ﬁeld) is said to be constructive if there exist algorithms
for performing the ring (ﬁeld) operations and an algorithm for zero testing in
the ring (ﬁeld).
We suppose that the ground ﬁeld K is a constructive ﬁeld of characteristic 0.
We write θ for x d
dx and consider diﬀerential systems of the form
Ar(x)θry + Ar−1(x)θr−1y + · · · + A0(x)y = 0
(1)

4
S.A. Abramov
where y = (y1, . . . , ym)T is a column vector of unknown functions, and y1, . . . , ym
are the components of y.
For the matrices
A0(x), A1(x), . . . , Ar(x)
(2)
we have Ai(x) ∈Mat m(K[[x]]), i = 0, 1, . . . , r, and Ar(x) (the leading matrix of
the system) is non-zero.
We call elements of the matrices Ai(x) system coeﬃcients. As the system
coeﬃcients will appear computable series.
It can be deduced from the classical results of Turing [21] that
We are not able, in general, to test whether a given computable series is
equal to zero or not; for a square matrix whose entries are computable
series - to test, whether this matrix is non-singular or not.
However, it turns out that the problems of ﬁnding solutions of some types are
decidable in the case when we know in advance that a given diﬀerential system
S is of full rank, i.e., that the equations of the system are linearly independent
over K[θ]. Algorithms for constructing local solutions of certain types can be
proposed (the components of local solutions either are series in x, or contain
such series as constituents). All the involved series are supposed to be formal.
Deﬁnition 3. The solutions whose components are formal Laurent series are
Laurent solutions. The components of a regular solution are of the form
yi(x) =
u

i=1
xλi
ki

s=0
gi,s(x)lns x
s! ,
(3)
where u, ki ∈N, λi ∈¯K and gi,s(x) ∈¯K((x))m ( ¯K denotes the algebraic closure
for K.)
Deﬁnition 4. A proper formal (exponential-logarithmic) solution of a system is
a solution of the form
eQ( 1
t)tλΦ(t), x = tρ,
(4)
where
λ ∈¯K;
Q( 1
t ) is a polynomial in 1
t over ¯K and the constant term of this polynomial
is equal to zero;
ρ is a positive integer;
Φ(t) is a column vector with components in the form k
i=0 gi(t) logi(t), and
all gi(t) are power series over ¯K.
If ρ has the minimal possible value in representation (4) of a proper formal
solution then ρ is the ramiﬁcation index of that solution.
A formal (exponential-logarithmic) solution is a ﬁnite linear combination with
coeﬃcients from ¯K of proper formal solutions.

Inﬁnite Power Series Coeﬃcients
5
Formal exponential-logarithmicis solutions are of a special interest since, e.g.,
any system of the form y′ = Ay, where A is an m × m-matrix whose entries are
formal Laurent series, has m linearly independent (over ¯K) formal solutions [19].
The main problems which are considered in this section are the following.
Suppose that a linear ordinary diﬀerential system S of arbitrary order having
the form (1) with computable formal power series coeﬃcients (entries of the
matrices Ai(x)) is given, test whether the system has
(1) non-zero Laurent series,
(2) regular, or
(3) formal exponential-logarithmic solutions,
and ﬁnd all such solutions if they exist.
Theorem 1. (i) [5,8] The ﬁrst two problems are decidable in the case when we
know in advance that a given system S is of full rank, i.e., in the case where the
equations of the given system are linearly independent over the ring K[θ].
(ii) [3] Despite the fact that such a system has a basis of formal exponential-
logarithmic solutions involving only computable series, there is no algorithm to
construct such a basis.
However, it is possible to specify a limited version of the third problem, for
which there is an algorithm of the desired type:
Theorem 2 [20]. If S and a positive integer d are such that for the system S
the existence of at least d linearly independent solutions is guaranteed, we can
construct such d solutions.
It is shown also that the algorithmic problems connected with the ramiﬁca-
tion indices of irregular formal solutions of a given system are mostly undecidable
even if we ﬁx a conjectural value of the ramiﬁcation index:
Theorem 3 [2]. There exists no algorithm which, given a system S with com-
putable power series coeﬃcients and a positive integer ρ, tests the existence of a
proper formal solution of ramiﬁcation index ρ for the system S.
Thus,
When we use the algorithmic way of power series representation, a neigh-
borhood of algorithmically solvable and unsolvable problems is observed.
2.2
Procedures for Constructing Local Solutions
For the solvable problems mentioned above, a Maple [17] implementation as
procedures of the package EG was proposed [9]. The package is available from
http://www.ccas.ru/ca/eg.
We report some experiments (Figs. 1, 2 and 3). The degree of the truncation
of the series involved in the solutions returned by our procedures is not less

6
S.A. Abramov
Fig. 1. An example of a system (f(k) is not yet deﬁned).
than it is required by the user. That degree can be even bigger: in any case,
it is big enough to represent the dimension of the space of the solutions under
consideration.
Let m = 3 and the system be of the form presented in Fig. 1.
Suppose that we deﬁne the procedure for computing coeﬃcients of the series
∞
k=0 f(k)xk as presented in Fig. 2.
Fig. 2. f(k) is deﬁned.
(Thus, ∞
k=0 f(k)xk = −1 −x + x2 + ∞
k=3(−k2 + k)xk.) The results of the
search for Laurent, regular and formal solutions are presented on Fig. 3.
The procedure of the construction of all formal solutions constructs also
all regular, and in particular, all Laurent solutions. Actually, one procedure
EG[FormalSolution] is suﬃcient in order to obtain solutions of all three types.
However, if it is required to construct, say, only Laurent solutions, then it is
advantageous to use procedure EG[LaurentSolution], because it will construct
them considerably faster, even if the original system has no formal solutions but
the Laurent ones. For this reason, we propose three procedures for searching
solutions of various types.
In conclusion of this section note that the ring of computable formal power
series is smaller than the ring of all formal power series because not every
sequence of coeﬃcients can be represented algorithmically. Indeed, the set of

Inﬁnite Power Series Coeﬃcients
7
Fig. 3. Laurent, regular and formal solutions of the system.
elements of the computable formal power series is countable (each of the algo-
rithms is a ﬁnite word in some ﬁxed alphabet) while the set of all power series
is uncountable.
3
Approximate (Truncated) Representation
Now, we consider an “approximate” representation of series.
A well-known example [16] is the result by Lutz and Sch¨afke. It is related
to the number of terms of entries of a power series matrix A that can inﬂuence
initial terms of some constituents of formal exponential-logarithmic solutions of
a diﬀerential system xsy′ = Ay, where s is a non-negative integer.
As a further example [4], we consider matrices with inﬁnite power series
entries and suppose that those series are represented in an approximate form,
namely, in a truncated form.
We start with introducing some notions.
If l ∈Z, a ∈K((x)) then we deﬁne the l-truncation a⟨l⟩which is obtained
by omitting all the terms of degree larger than l in a. For a non-zero element
a =  aixi of K((x)), we denote by val a the valuation of a deﬁned by val a =
min {i such that ai ̸= 0}; by convention, val 0 = ∞.
For A ∈Mat n(K((x))), we deﬁne val A as the minimum of the valuations
of the entries of A. We deﬁne the leading coeﬃcient of a non-zero matrix A ∈
Mat n(K((x))) as lc A = (x−val AA)|x=0. For A ∈Mat n(K[x]), we deﬁne deg A
as the maximum of the degrees of the entries of A.
The notation AT is used for the transpose of a matrix (vector) A. In is the
identity n × n-matrix.
Given A ∈Mat n(K((x))), we deﬁne the matrix A⟨l⟩∈Mat n(K[x, x−1])
obtained by replacing the entries of A by their l-truncations (if A ∈Mat n(K[[x]])
then A⟨l⟩∈Mat n(K[x])).

8
S.A. Abramov
If P ∈Mat n(K[x]) then any ˆP ∈Mat n(K[[x]]) such that ( ˆP)⟨deg P ⟩= P is
a prolongation of P.
3.1
Strongly Non-singular Matrices
Deﬁnition 5. A polynomial matrix P which is non-singular, i.e., det P ̸= 0,
is strongly non-singular if P is not the l-truncation (l = deg P) of a singular
matrix having power series entries; in other words, P is strongly non-singular if
det ˆP ̸= 0 for any prolongation ˆP of P.
It is proven that the question of strong non-singularity is algorithmically
decidable. For the answer to this question, the number
h = deg P + val P −1
(5)
plays the key role.
Theorem 4 [4]. P is strongly non-singular if and only if
deg P + val P −1 ⩾0,
(6)
i.e., h ⩾0.
Example 1. If P is a non-singular constant matrix then P is a strongly non-
singular due to the latter proposition. However, the matrix
x 0
1 x

,
(7)
is not strongly non-singular:
det
x x2
1 x

= 0.
(8)
This could be recognized in advance: for (7) we have deg P = 1, val P −1 = −2
(since det P = x2), and the inequality h ⩾0 does not hold: 1 −2 = −1.
⊓⊔
Assuming that a non-singular power series matrix A (which is not known to
us) is represented by a strongly non-singular polynomial matrix P, we give a
tight lower bound for the number of initial terms of entries of A−1 which can be
determined from P −1.
Theorem 5 [4].
Let P be a polynomial matrix. If the inequality h ⩾0 holds
then ﬁrst, for any prolongation ˆP, the valuations of the determinant and the
inverse matrix of the approximate matrix and, resp., of the determinant and
the inverse of the prolonged matrix coincide. Second, in the determinants of the
approximate and prolonged matrices, the coeﬃcients coincide for xval det P , as
well as h subsequent coeﬃcients (for larger degrees of x). A similar statement
holds for the inverse matrix. The bound h is tight.

Inﬁnite Power Series Coeﬃcients
9
Example 2. Let
P =
1 + x
0
1
1 −x

.
Here h = 1. The matrix P is strongly non-singular.
Let
ˆP =
1 + x + x2 + . . .
0
1
1 −x

.
We have
det P = 1 −x2 = 1 + 0 · x −1 · x2, det ˆP = 1 + 0 · x + 0 · x2 + . . .
We have also:
P −1 =

1/(1 + x)
0
−1/(1 −x2) 1/(1 −x)

=

1 −x + x2 + . . .
0 + 0 · x
−1 + 0 · x −x2 −. . . 1 + x + x2 + . . .

,
ˆP −1 =

1 −x
0
−1 1/1 −x

=

1 −x
0 + 0 · x
−1 + 0 · x
1 + x + x2 + . . .

.
⊓⊔
As a consequence of Theorem 5, if val det P = e then val det ˆP = e and
det P −det ˆP = O(xe+h+1).
Similarly, if val P −1 = e then val ( ˆP)−1 = e and
P −1 −ˆP −1 = O(xe+h+1).
3.2
When only a Truncated System Is Known
In this section, we are interested in the following question. Suppose that for
a system S of the form (1) only a ﬁnite number of terms of the entries of
A0(x), A1(x), . . . , Ar(x) is known, i.e., we know not the system S itself but the
system S⟨l⟩for some non-negative integer l. Suppose that we also know that
– ordS⟨l⟩= ordS,
– Ar(x) is invertible.
How many terms of Laurent series solutions of S can be determined from the
given “approximate” system S⟨l⟩?
We ﬁrst recall the following result:

10
S.A. Abramov
Proposition 1 [6, Proposition 6]. Let S be a system of the form (1) and
γ = min
i
val

A−1
r (x)Ai(x)

, q = max{−γ, 0}.
There exists an algorithm that uses only the terms of degree less than
rmq + γ + val det Ar(x) + 1
(9)
of the entries of the matrices A0(x), A1(x), . . . , Ar(x), and computes a non-zero
polynomial (the so called indicial polynomial [12, Chap. 4, Sect. 8], [10, Deﬁnition
2.1], [6, Sect. 3.2]) I(λ) such that:
– if I(λ) has no integer root then (1) has no solution in K((x))m \ {0},
– otherwise, let e∗, e∗be the minimal and maximal integer roots of I(λ); then
the sequence
ak = rmq + γ + val det Ar(x) + max{e∗−e∗+ 1, k + (rm −1)q},
(10)
k = 1, 2, . . . , is such that for any e ∈Z, k ∈Z
+ and column vectors
ce, ce+1, . . . , ce+k−1 ∈Km,
the system S possesses a solution y(x) ∈K((x))m of the form
y(x) = cexe + ce+1xe+1 + · · · + ce+k−1xe+k−1 + O(xe+k),
if and only if, the system S⟨al⟩possesses a solution ˜y(x) ∈K((x))m such that
˜y(x) −y(x) = O(xe+k).
Using the latter proposition we prove
Theorem 6 [4]. Let Σ be a system of the form
Pr(x)θry + Pr−1(x)θr−1y + · · · + P0(x)y = 0
with polynomial matrices P0(x), P1(x), . . . , Pr(x). Let its leading matrix Pr(x)
be strongly non-singular. Let
d = deg Pr, p = −val P −1
r
, h = d −p, γ =
min
0⩽i⩽r−1 (val (P −1
r
Pi))
be such that the inequality
h −p −γ ⩾0
holds. Let I(λ) be the indicial polynomial of Σ. Let the set of integer roots of
I(λ) be non-empty, and e∗, e∗be the minimal and maximal integer roots of I(λ).
Let a non-negative integer k satisfy the equality
max{e∗−e∗+ 1, k + (rm −1)q} = l −rmq −γ −val det Pr(x).
(11)

Inﬁnite Power Series Coeﬃcients
11
Let ˆΣ be an arbitrary system of the form (1) such that ˆΣ⟨l⟩= Σ for l = deg Σ
(i.e., ˆΣ is an arbitrary prolongation of Σ). Then for any e ∈Z, the system ˆΣ
possesses a solution
ˆy(x) ∈K((x))m, val ˆy(x) = e,
if and only if, the system Σ possesses a solution y(x) ∈K((x))m such that
y(x) −ˆy(x) = O(xe+k+1)
(12)
(evidently, the equalities val ˆy(x) = e and (12) imply that val y(x) = e).
Example 3. Let
P1 =

1
0
0 1 −x

, P0 =

0
−1
−x + 2x2 + 2x3 + 2x4 −2 + 4x

.
For the ﬁrst-order diﬀerential system Σ
P1(x)θy + P0(x)y = 0
we have
d = 1, p = 0, h = 1, γ = 0, I(λ) = λ(λ −2), e∗−e∗+ 1 = 3.
The conditions of Theorem 6 are satisﬁed.
The general solution of Σ is
y1 = C1 −C1x + C2x2 −C2x3 + 0x4 + 2C1
15 x5 + C1
30 x6 +
 C1
210 + 2C2
35

x7 + . . . ,
y2 =
−C1x + 2C2x2 −3C2x3 + 0x4 + 2C1
3 x5 + C1
5 x6 +
C1
30 + 2C2
5

x7 + . . . ,
where C1 and C2 are arbitrary constants.
Equation (11) has the form max{3, k} = 4, thus
k = 4.
This means that all Laurent series solutions of any system ˆΣ of the form
A1(x)θy + A0(x)y = 0
(13)
with non-singular matrix A1 and such that ˆΣ⟨4⟩= Σ (we have deg Σ = 4) are
power series solutions having the form
ˆy1 = C1 −C1x + C2x2 −C2x3 + O(x5),
ˆy2 =
−C1x + 2C2x2 −3C2x3 + O(x5),
where C1, C2 are arbitrary constants. Consider, e.g., the ﬁrst-order diﬀerential
system ˆΣ of the form (13) with
A1 =

1
0
0
1 −x

,

12
S.A. Abramov
A0 =

0
−1
−x + 2x2 + 2x3 + 2x4 + 2x5 + 2x6 + x7 + x8 + . . . −2 + 4x

.
Its general solution is
ˆy1 = C1 −C1x + C2x2 −C2x3 + 0x4 + 0x5 + 0x6 + C1
35 x7 + . . . ,
ˆy2 =
−C1x + 2C2x2 −3C2x3 + 0x4 + 0x5 + 0x6 + C1
5 x7 + . . . ,
what corresponds to the forecast and expectations.
⊓⊔
Remark 1. The latter example shows that Theorem 6 gives a tight bound for
possible value of k: in that example that we cannot take k + 1 instead of k.
Indeed, y1 contains the term 2C1
15 x5, while ˆy1 has factually no term of degree 5.
We see that the information that can be extracted from truncated series,
matrices, systems, etc. may be suﬃcient to obtain certain characteristics
of the original (untruncated) objects. Naturally, these characteristics are
incomplete, but may suﬃce for some purposes.
In the context of truncated systems we considered only the problem of testing
the existence and constructing Laurent series solutions, but we did not discuss
similar problems related to regular and formal exponential-logarithmic solutions.
We will continue to investigate this line of enquiry.
4
The Width
In conclusion, we discuss a plot which connects both thematic lines of the paper.
Deﬁnition 6 [4,5]. Let S be a system of full rank over K[[x]][θ]. The minimal
integer w such that S⟨l⟩is of full rank for all l ⩾w is called the width of S The
minimal integer ws such that any system S1 having power series coeﬃcients and
satisfying the condition S⟨ws⟩
1
= S⟨ws⟩, is of full rank, is called the s-width (the
strong width) of S.
We will use the notations w(S), ws(S) when it is convenient.
Any linear algebraic system can be considered as a linear diﬀerential system
of zero order. This lets us state using the following example that for an arbitrary
diﬀerential system S we have ws(M) ̸= w(M) in general, however, the inequality
ws(S) ⩾w(S)
holds.

Inﬁnite Power Series Coeﬃcients
13
Example 4. Let A be
x x3
1 x

,
(14)
then
w(A) = 1,
since det A⟨0⟩= 0 and
A⟨1⟩= A⟨2⟩=

x 0
1 x

, det

x 0
1 x

̸= 0,
and A⟨l⟩= A when l ⩾3, det A ̸= 0. However, ws(A) > 1, due to det
x x2
1 x

=
0. It is easy to check that ws(A) = 2.
⊓⊔
It was proven in [5, Theorem 2] that if a system S of the form (1) is of full
rank then there exists the width w of S. The value w may be computed if the
coeﬃcients of S are represented algorithmically.
As for the idea of the proof from [5], it is shown that the rank-preserving
EG-eliminations [1,7] give a conﬁrmation for the fact that S is of full rank. That
conﬁrmation uses only a ﬁnite number of the terms of power series which are
coeﬃcients of S. For this, the induced recurrent system R is considered (such R
is a speciﬁc recurrent system for the coeﬃcients of Laurent series solutions of S).
This system has polynomial coeﬃcients of degree less than or equal to r = ord S.
The system S is of full rank if and only if R is of full rank as a recurrent system.
A recurrent system of this kind can be transformed by a special version of the
EG-eliminations [5, Sect. 3] into a recurrent system ˜R whose leading matrix is
non-singular. This gives the conﬁrmation mentioned above. It is important that
only a ﬁnite number of the coeﬃcients of R are involved in the obtained leading
matrix of ˜R (due to some characteristic properties of the used version of the EG-
eliminations). Each of polynomial coeﬃcients of R is determined from a ﬁnite
number (bounded by a non-negative integer N) of the coeﬃcients of the power
series involved in S. This proves the existence of the width and of the s-width
as well. The mentioned number N can be computed algorithmically when all
power series are represented algorithmically; thus, in this case we can compute
the width of S since we can test [1,7,11] whether a ﬁnite order diﬀerential system
with polynomial coeﬃcients is of full rank or not. From this point we can consider
step-by-step S⟨N−1⟩, S⟨N−2⟩, . . . , S⟨1⟩, S⟨0⟩until there appears the ﬁrst which is
not of full rank. If all the truncated systems are of full rank then w = 0.
Concerning the s-width, we get the following theorem
Theorem 7 [4]. Let S be a full rank system of the form (1). Then the s-width
ws(S) is deﬁned. If the power series coeﬃcients of S are represented algorith-
mically then we can compute algorithmically a non-negative integer N such that
ws(S) ⩽N.

14
S.A. Abramov
However, it is not exactly clear how to ﬁnd the minimal value N, i.e., ws(S). Is
this problem algorithmically solvable? The question is still open.
Acknowledgments. The author is thankful to M. Barkatou, D. Khmelnov, M.
Petkovˇsek, E. Pﬂ¨ugel, A. Ryabenko and M. Singer for valuable discussions.
References
1. Abramov, S.: EG-eliminations. J. Diﬀer. Eqn. Appl. 5, 393–433 (1999)
2. Abramov, S.: On ramiﬁcation indices of formal solutions of constructive linear
ordinary diﬀerential systems. J. Symbolic Comput. 79, 475–481 (2017)
3. Abramov, S.A., Barkatou, M.A.: Computable inﬁnite power series in the role of
coeﬃcients of linear diﬀerential systems. In: Gerdt, V.P., Koepf, W., Seiler, W.M.,
Vorozhtsov, E.V. (eds.) CASC 2014. LNCS, vol. 8660, pp. 1–12. Springer, Cham
(2014). doi:10.1007/978-3-319-10515-4 1
4. Abramov, S., Barkatou, M.: On strongly non-singular polynomial matrices. In:
Schneider, C., Zima, E. (eds.) Advances in Computer Algebra: Proceedings of
the Waterloo Workshop in Computer Algebra 2016. Springer, Heidelberg (2017,
accepted)
5. Abramov, S., Barkatou, M., Khmelnov, D.: On full rank diﬀerential systems with
power series coeﬃcients. J. Symbolic Comput. 68, 120–137 (2015)
6. Abramov, S.A., Barkatou, M.A., Pﬂ¨ugel, E.: Higher-order linear diﬀerential
systems with truncated coeﬃcients. In: Gerdt, V.P., Koepf, W., Mayr, E.W.,
Vorozhtsov, E.V. (eds.) CASC 2011. LNCS, vol. 6885, pp. 10–24. Springer, Heidel-
berg (2011). doi:10.1007/978-3-642-23568-9 2
7. Abramov, S., Bronstein, M.: Linear algebra for skew-polynomial matrices. Rapport
de Recherche INRIA RR-4420 (2002). http://www.inria.fr/RRRT/RR-4420.html
8. Abramov, S.A., Khmelnov, D.E.: Regular solutions of linear diﬀerential systems
with power series coeﬃcients. Program. Comput. Softw. 40(2), 98–106 (2014)
9. Abramov, S.A., Ryabenko, A.A., Khmelnov, D.E.: Procedures for searching local
solutions of linear diﬀerential systems with inﬁnite power series in the role of
coeﬃcients. Program. Comput. Softw. 42(2), 55–64 (2016)
10. Barkatou, M., Pﬂ¨ugel, E.: An algorithm computing the regular formal solutions of
a system of linear diﬀerential equations. J. Symbolic Comput. 28, 569–588 (1999)
11. Beckermann, B., Cheng, H., Labahn, G.: Fraction-free row reduction of matrices of
skew polynomials. In: Proceedings of the ISSAC 2002, pp. 8–15. ACM, New York
(2002)
12. Coddington, E., Levinson, N.: Theory of Ordinary Diﬀerential Equations. McGraw-
Hill, New York (1955)
13. Denef, J., Lipshitz, L.: Power series solutions of algebraic diﬀerential equations.
Math. Ann. 267, 213–238 (1984)
14. Koepf, W.: Power series in computer algebra. J. Symbolic Comput. 13, 581–603
(1992)
15. Koepf, W.: Computeralgebra. Eine algorithmisch orientierte Einf¨uhrung. Springer,
Heidelberg (2006)
16. Lutz, D.A., Sch¨afke, R.: On the identiﬁcation and stability of formal invariants for
singular diﬀerential equations. Linear Algebra Appl. 72, 1–46 (1985)
17. Maple online help. http://www.maplesoft.com/support/help/

Inﬁnite Power Series Coeﬃcients
15
18. Pﬂ¨ugel, E.: Eﬀective formal reduction of linear diﬀerential systems. Appl. Algebra
Eng. Commun. Comput. 10(2), 153–187 (2000)
19. van
der
Put,
M.,
Singer,
M.F.:
Galois
Theory of
Diﬀerential
Equations.
Grundlehren der mathematischen Wissenschaften, vol. 328. Springer, Heidelberg
(2003)
20. Ryabenko, A.A.: On exponential-logarithmic solutions of linear diﬀerential systems
with power series coeﬃcients. Program. Comput. Softw. 41(2), 112–118 (2015)
21. Turing, A.: On computable numbers, with an application to the Entscheidungs-
problem. Proc. Lond. Math. Soc. Ser. 2 42, 230–265 (1936)

On the Asymptotic Stability of a Satellite
with a Gravitational Stabilizer
Andrei V. Banshchikov(B)
Matrosov Institute for System Dynamics and Control Theory
of Siberian Branch of Russian Academy of Sciences,
PO Box 292, 134, Lermontov Str., Irkutsk 664033, Russia
bav@icc.ru
Abstract. The problem of the inﬂuence of the structure of forces on
the stability of the relative equilibrium of a controlled satellite with a
gravitational stabilizer on the circular orbit is studied. In the space of
entered parameters, the regions with diﬀerent degrees of instability by
Poincar´e are found. Assuming an instability of a potential system, the
problem of the possibility of its stabilization up to asymptotic stability
is considered. A parametric analysis of the obtained inequalities with the
help of “Mathematica” built-in tools for symbolic-numerical modelling
is carried out.
1
Introduction
Investigation of stability and stabilization of nonlinear or linearized models of
mechanical systems often leads to the problem of “parametric analysis” of the
conditions (inequalities) obtained. In the case of parametric analysis, it is impor-
tant to have a possibility to estimate the domain of values of the parameters
under which a desired system’s state is provided. Naturally, it is hard to hope
for obtaining any readable analytical results for the models which have high
dimensions and contain many parameters. At this stage, one can eﬃciently use
software packages of computer algebra (SPCA) as well as the corresponding
software elaborated on the basis of these software packages.
The paper considers a problem of stability of the position of relative equilib-
rium in the orbital coordinate system of a controlled satellite with a gravitational
stabilizer. The mechanical system in question is a well-studied model (see, for
example, the review [1]). To obtain suﬃcient stability conditions, the second Lya-
punov method and the Barbashin–Krasovskii theorem were applied. As noted
in [1], obtaining the necessary stability conditions (by linear equations of per-
turbed motion) leads to presenting very bulky calculations. In contrast to the
passive stabilization and orientation systems, the possibilities of active control of
a gravitational stabilizer are investigated in [2], in particular, the optimization
of the system by degrees of stability and accuracy.
The application of computer algebra methods and SPCA capabilities to the
problems of celestial mechanics has rich history and till today attracts academic
attention (see, for example, [3,4]).
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 16–26, 2017.
DOI: 10.1007/978-3-319-66320-3 2

On the Asymptotic Stability of a Satellite with a Gravitational Stabilizer
17
2
Description and Construction of a Symbolical Model
The system’s mass center moves along the Kepler circular orbit with constant
angular velocity ω . For the description of a motion of the system, two right-
handed rectangular Cartesian coordinate systems are introduced (the orbital
coordinate system (OCS) and the coordinate system rigidly connected to a satel-
lite). To deﬁne relative positioning of the axes of these coordinate systems, the
directional cosines deﬁned by the angles ψ , θ , ϕ of Euler’s type, are used (see,
for example, [2]). The stabilizer is a rigid rod with point mass at its free end.
The rod is connected to the satellite with a 2-degree-of-freedom suspension. The
rotation axes of the rod coincide with the direction of the axes of pitch and roll.
The system is inﬂuenced by a gravitation moment. When moving undisturbed,
the system’s principal central axes of inertia coincide with the axes of orbital
coordinate system, and the rod is oriented along the radius of the orbit. This is
the equilibrium position of a satellite with the stabilizer in regard to OCS.
With the help of the developed software [5,6], the following results are
obtained in a symbolic form on PC for the system of bodies in question:
• kinetic energy and force function of the approximate Newtonian ﬁeld of grav-
itation;
• nonlinear equations of motion in Lagrange form of the 2nd kind;
• matrices of equations of perturbed motion in the ﬁrst approximation in the
vicinity of equilibrium position;
• coeﬃcients of the system’s characteristic equation.
Linearized in the vicinity of the equilibrium position, equations of motion for
a satellite with a stabilizer are decomposed into two subsystems. Respectively,
a “pitch” subsystem ( θ ) and a “yaw-and-roll” subsystem (ψ, ϕ) are:

M1 ¨q1 + K1 q1 = Q1
M2 ¨q2 + G ˙q2 + K2 q2 = Q2,
(1)
where all derivatives are calculated on dimensionless time τ = ωt ( ω=|ω| is the
module of orbital angular velocity); q1 =

θ
δ

, q2 =
⎛
⎝
ψ
ϕ
σ
⎞
⎠; δ, σ are rotation
angles of the rod with regard to the satellite’s body; Q1 =

0
Qδ

, Q2 =
⎛
⎝
0
0
Qσ
⎞
⎠
are control forces;
M1 =

c f
f d

;
K1 = 3

b −a f
f
f

;
M2 =
⎛
⎝
a 0 0
0 b f
0 f d
⎞
⎠;
K2 =
⎛
⎝
c −b
0
0
0
4(c −a)
4f
0
4f
3f + d
⎞
⎠;
G =
⎛
⎝
0
c −b −a 0
a + b −c
0
0
0
0
0
⎞
⎠.

18
A.V. Banshchikov
Here, we introduce the following notations:
a = Jy;
b = Jx + m r (l + r) + 1
3 m l2 + m0(l + r)2;
c = b + Jz −Jx;
d =
	m
3 + m0

l2;
f =
	m
2 + m0

rl + d;
c −b −a = Jz −Jx −Jy,
where m and m0 are masses of the rod and the point load at the end, respectively;
l>0 is the rod length; r≥0 is the distance from the system’s mass center to the
point of attachment of the rod; Jx, Jy, Jz; a, b, c are principal inertia moments
of the satellite and whole system, respectively.
Taking into account the mass distribution in the system and in the ellipsoid
of inertia of rigid body, the following inequalities are valid
b > a > 0,
c > a,
f > d > 0,
c > f,
b > f,
c + a −b ≡Jz + Jy −Jx > 0,
b + a −c ≡Jx + Jy −Jz > 0,
(2)
Equation (1) may be interpreted as equations of oscillations of a mechanical
system inﬂuenced by potential (with the matrices K1, K2) and gyroscopic (with
the matrix G) forces. These forces are determined by gravitation forces as well
as by orbital motion. The matrices M1 and M2 play the role of diagonal blocks
of a positive deﬁnite matrix of kinetic energy.
3
Formulation of the Problem
According to Kelvin–Chetaev’s theorems [7], examination of stability of trivial
solution begins with the analysis of the matrix of potential forces. Let us write
out the conditions of positive deﬁniteness of matrices K1, K2:
b > a + f,
c > b,
(c −a)(3f + d) −4f 2 > 0.
(3)
Let us assume that
(1) for the “pitch” subsystem, the values of the parameters satisfy the condition
a < b < a + f (i.e., the ﬁrst inequality in (3) is violated);
(2) for the “yaw-and-roll” subsystem, the last inequality in (3) or simultaneously
the second and third inequalities are changed to the opposite.
Taking into account the assumptions presented, the system is unstable when
initial potential forces are in action. The simultaneous stabilization of the two
subsystems by additional forces of diﬀerent nature is required. For this purpose,
control forces with the suspension of the rod are added into the right-hand sides
of the motion Eq. (1) as it is shown below
Qδ = ˜k∗
θ ˙θ −˜k∗
δ ˙δ + ˜kθθ −˜kδδ;
Qσ = ˜k∗
ϕ ˙ϕ −˜k∗
σ ˙σ + ˜kϕϕ −˜kσσ,
(4)
where ˜k∗
θ = k∗
θ
ω ;
˜k∗
δ = k∗
δ
ω ;
˜kθ = kθ
ω2 ;
˜kδ = kδ
ω2 ;
˜k∗
ϕ = k∗
ϕ
ω ;
˜k∗
σ = k∗
σ
ω ;
˜kϕ = kϕ
ω2 ;
˜kσ = kσ
ω2
are constant coeﬃcients.

On the Asymptotic Stability of a Satellite with a Gravitational Stabilizer
19
The objective of the paper is to investigate the eﬀect of the structure of
forces on the stability of the equilibrium position of system (1). In addition,
the problem of the possibility of ensuring the asymptotic stability of the two
subsystems by a “reduced” set of forces represented in (4) is formulated.
By splitting the matrices in terms of velocities and coordinates in Eq. (1)
into the symmetric and skew-symmetric parts, it is not diﬃcult to write out
the structure of the forces aﬀecting the system. For example, concerning the
“yaw-and-roll” subsystem, potential (with a matrix P2), non-conservative (N2),
dissipative (D2) and gyroscopic (G2) forces are added to the initial potential
(with a matrix K2) and gyroscopic (with a matrix G) forces, where
P2 =
⎛
⎜
⎝
0
0
0
0
0
−
˜kϕ
2
0 −
˜kϕ
2
˜kσ
⎞
⎟
⎠;
N2 =
⎛
⎜
⎝
0
0
0
0
0
˜kϕ
2
0 −
˜kϕ
2
0
⎞
⎟
⎠;
D2 =
⎛
⎜
⎝
0
0
0
0
0
−
˜k∗
ϕ
2
0 −
˜k∗
ϕ
2
˜k∗
σ
⎞
⎟
⎠;
G2 =
⎛
⎜
⎝
0
0
0
0
0
˜k∗
ϕ
2
0 −
˜k∗
ϕ
2
0
⎞
⎟
⎠.
4
Regions of System’s Instability
For the convenience of graphical representation of the regions with diﬀerent
degrees of instability and subsequent parametric analysis, we introduce four
dimensionless parameters:
α = c −b
a
= Jz −Jx
Jy
;
γ = b −a
c
;
p1 = d
f ;
p2 = f
c .
(5)
The physically obtainable values of the parameters, taking into account (2),
lie within the intervals: −1 < α < 1,
0 < γ < 1,
0 < p1 ≤1,
0 < p2 < 1. It
is not diﬃcult to show that conditions (2) imply γ + α > 0.
The diagonal blocks of the initial matrix of potential forces (when Qδ = 0,
Qσ = 0) in notation (5) have the form:
K1 = 3

γ p2
1 1

;
K2 =
⎛
⎝
α
0
0
0 4(γ + α) 4p2(α + 1)
0
4
3 + p1
⎞
⎠.
In the space of the outlined parameters, the relations
γ = p2, α = 0,
S ≡(γ + α)(3 + p1) −4p2(α + 1) = 0 deﬁne the surfaces which separate the
regions having diﬀerent degrees of instability. For example, Fig. 1 shows these
regions for the values of the parameters p1 = 4/5, p2 = 5/7.
It is known that if the equilibrium position is unstable at potential forces,
Kelvin–Chetaev’s theorem [7] of inﬂuence of gyroscopic forces tells us that gyro-
scopic stabilization is possible only for systems with an even degree of instability.

20
A.V. Banshchikov
Fig. 1. Regions with diﬀerent degrees of instability.
Here, respectively, instability regions for the entire system have: Z – zero
degree; E – an even degree (when γ > p2 ) and E (when γ < p2); O, O, O –
odd degree.
The evenness (or oddness) of the degree of instability according to Poincar´e
is determined by positivity (or negativity) of the determinant of the matrix of
potential forces. It is necessary to emphasize that for the values of the para-
meters from the regions E and E , the unstable equilibrium position has an
even degree of instability (i.e., det K = det K1 ∗det K2 > 0 ). Thus, under cer-
tain conditions, equilibrium can be stabilized due to the inﬂuence of gyroscopic
forces. Earlier in [8], the author has proved the stabilization of the equilibrium
in the needle-shaped part (subregion) of the region E for an uncontrolled satel-
lite. The matrices K1 and K2 are positive deﬁnitive in the region Z. On the
basis of another Kelvin–Chetaev’s theorem, the addition to the potential forces
of gyroscopic forces preserves the nature of stability of the investigated motion.
The mass distribution in the system in which the initial matrix of potential
forces of the system will be positive deﬁnitive is usually given for the applied
problems of spacecraft dynamics. Further, due to the addition of primarily dis-
sipative forces, the asymptotic stability of motion is ensured by Lyapunov’s
theorem. However, unstable systems may also be of interest and, besides, “non-
standard” situations on the orbit are possible.
Thus, taking into account the assumptions made in the formulation of the
problem in Sect. 3, we shall consider the possibility of stabilizing an unstable

On the Asymptotic Stability of a Satellite with a Gravitational Stabilizer
21
system in the region O (when γ < p2, α < 0, (γ +α)(3+p1)−4p2(α +1) < 0 )
or in the region E (when γ < p2, α > 0, (γ + α)(3 + p1) −4p2(α + 1) < 0 ) to
asymptotic stability by additional forces (4).
5
Parametric Analysis of Asymptotic Stability Conditions
It is obvious that the characteristic equation of system (1) is factorized: Λ(λ) ≡
Λ(1) ∗Λ(2) = 0. After performing elementary transformations with the char-
acteristic matrices (multiplying their rows by positive factors), we obtain the
characteristic determinants in notation (5), respectively, in the “pitch” subsys-
tem and in the “yaw-and-roll” subsystem:
Λ(1) =

λ2 + 3γ
p2(λ2 + 3)
λ2 −λ˜k∗
θ + (3 −˜kθ) λ2p1 + λ˜k∗
δ + (3 + ˜kδ)
 =
4

i=0
wi λi,
where
w4 ≡det M1 = p1 −p2, w3 = ˜k∗
δ + p2˜k∗
θ, w2 = 3 (p1γ −2p2 + 1) + ˜kδ + p2˜kθ,
w1 = 3
	
γ ˜k∗
δ + p2˜k∗
θ

, w0 = 3
	
3 (γ −p2) + γ ˜kδ + p2˜kθ

;
Λ(2) =

λ2+ α
λ(α −1)
0
λ(α −1)(γ −1) λ2(1+ γα)+ 4(α+ γ)
(λ2+ 4)p2(α + 1)
0
λ2 −λ˜k∗
ϕ + (4 −˜kϕ)
λ2p1+ λ˜k∗
σ + (3+ p1+ ˜kσ)

=
=
6

i=0
vi λi,
where
v6 ≡det M2 = (1 + γ α) p1 −(α + 1)p2,
v5 = (1 + γ α) ˜k∗
σ + (α + 1) p2 ˜k∗
ϕ,
v1 = 4 α
	
(α + γ) ˜k∗
σ + (α + 1) p2 ˜k∗
ϕ

,
v3 = (1 + 3γ + α(α + 2γ + 3)) ˜k∗
σ + (α + 1)(4 + α) p2 ˜k∗
ϕ,
v4 = (1+ γα)(3+ p1+ ˜kσ) + (1+ 3γ+ α(3+ α+ 2γ))p1 −(α+ 1)p2(8+ α−˜kϕ),
v2 = (1 + 3γ + α (α + 2γ + 3))
	
3 + p1 + ˜kσ

+ 4 α p1 (γ + α) +
+ (α + 1) p2
	
(4 + α) ˜kϕ −8(α + 2)

,
v0 = 4 α
	
(α + γ)
	
3 + p1 + ˜kσ

+ (α + 1) p2
	
˜kϕ −4


.
The principal diagonal minors of the Hurwitz matrix, respectively, for two
subsystems
Δ(1)
3 = w1w2w3 −w4w2
1 −w0w2
3;
Δ(2)
3 =

v5 v3 v1
v6 v4 v2
0 v5 v3

;
Δ(2)
5 =

v5 v3 v1 0 0
v6 v4 v2 v0 0
0 v5 v3 v1 0
0 v6 v4 v2 v0
0 0 v5 v3 v1


22
A.V. Banshchikov
are analytically obtained with SPCA “Mathematica” and were used in further
calculations, but due to bulkiness, their explicit form is not given here.
The fulﬁllment of the conditions on the existence of roots with negative real
parts for the polynomial Λ(λ)
wi > 0, (i = 0, 4);
Δ(1)
3
> 0,
(6)
vi > 0, (i = 0, 6);
Δ(2)
3
> 0;
Δ(2)
5
> 0
(7)
ensures the asymptotic stability of the system’s equilibrium position on the basis
of Lyapunov’s theorem on the ﬁrst approximation.
It is worth noting that the conditions w4 > 0 and v6 > 0 are satisﬁed by
virtue of the positive deﬁniteness of the kinetic energy matrix.
5.1
Stabilization in the “Pitch” Subsystem
With the help of “Mathematica” function Reduce designed to ﬁnd the symbolic
(analytical) solution of the inequalities systems, the conditions for the control
parameters ˜k∗
θ, ˜k∗
δ, ˜kθ, ˜kδ (when p1 > p2, γ < p2) ensuring the fulﬁllment of
the system of inequalities (6) are obtained. Due to the solution’s bulkiness, its
presentation is omitted here. It is worth noting that “extra” forces entail “costs”
of their technical implementation.
An analysis of the solution obtained allows us to conclude that it is possible
to achieve stabilization of the subsystem to asymptotic stability by a “reduced”
set of control forces in Case 1 Qδ = −˜k∗
δ ˙δ −˜kδ δ or Case 2 Qδ = ˜k∗
θ ˙θ + ˜kθ θ.
In Case 1, additional dissipative and potential forces make an impact on the
subsystem, and in Case 2, all forces (potential, non-conservative, dissipative and
gyroscopic) are present. As a result, the following proposition is formulated and
proved.
Proposition 1. When choosing control parameters that satisfy the conditions
˜k∗
δ > 0,
˜kδ > 3
p2
γ −1

in Case 1
or ˜k∗
θ > 0,
˜kθ > 3

1 −γ
p2

in
Case 2, all the roots of the polynomial Λ(1)(λ) have negative real parts.
5.2
Stabilization in the “Yaw-and-Roll” Subsystem
We note that the control parameters ˜k∗
ϕ and ˜k∗
σ enter only the odd coeﬃcients
v1 , v3 , v5 of the characteristic equation. With the above mentioned Reduce func-
tion, their positivity is analyzed separately for the regions O and E. For example,
for the region O, the function call and the solution have the following form:
Reduce[ { 0 < p2 < p1 ≤1, 0 < γ < p2, −γ < α < 0, S < 0,
v1 > 0, v3 > 0, v5 > 0 }, { ˜k∗
σ, ˜k∗
ϕ }, Reals]

On the Asymptotic Stability of a Satellite with a Gravitational Stabilizer
23
p2 < p1 ≤1 ∧γ < p2 ∧−γ < α < 0 ∧
∧˜k∗
σ > 0 ∧−(1 + 3γ + α(3 + α + 2γ))˜k∗
σ
(α + 1)(α + 4)p2
< ˜k∗
ϕ < −(α + γ)˜k∗
σ
(α + 1)p2
.
Looking at the analytical solution of this system of inequalities, we note the
positivity of ˜k∗
σ and the negativity of ˜k∗
ϕ . Therefore, forces (in the matrix D2) can
only be dissipative but not accelerating. As a result, the following proposition is
formulated and proved.
Proposition 2. It is impossible to ensure the coeﬃcients v1, v3 , v5 are simul-
taneously positive for the values of the parameters from the region O when
˜k∗
σ = 0 or ˜k∗
ϕ = 0, but in the region E , this can be done.
Thus, in order to stabilize the system in the region O, a complete set of
control forces with respect to velocities is required (in contrast to the region E,
where a “reduced” set of forces is suﬃcient).
It is not possible to obtain an analytical solution for the entire system of
inequalities (7) because of the large number of parameters and the complexity
of the expressions being analyzed. Therefore, to simplify the analysis, let us move
on to symbolic-numerical analysis for ﬁxed values of some parameters.
To start with, we consider the question of the possibility of asymptotic sta-
bility for the region O. Since in this region v0
˜kσ=0, ˜kϕ=0 ≡det K2 > 0 , it is
possible not to take into account the positional forces in Qσ from (4) (i.e., let
us add ˜kσ = 0 and ˜kϕ = 0). When solving the system of inequalities (7) using
Reduce function for the speciﬁc numerical values ˜k∗
ϕ < 0, ˜k∗
σ > 0 (for example,
˜k∗
σ = 1, ˜k∗
ϕ = −γ/p2, p1 = 4/5) we get the answer FALSE (i.e. the system is
incompatible). The same answer was received in the case ˜kσ ̸= 0 , ˜kϕ ̸= 0 (i.e.
under the action of the whole set of forces Qσ). As a result of the analysis, the
following proposition can be formulated.
Proposition 3. For the values of the parameters in the region O system (1)
cannot be stabilized up to the asymptotic stability due to the control forces’
eﬀect (4).
Now, let us consider the question of the possibility of asymptotic stability for
the region E. Taking into account the second part of Proposition 2, we assume
that Qσ = −˜k∗
σ ˙σ −˜kσ σ (that is, additionally only dissipative and potential
forces act). In this case, the principal diagonal minors of the third and ﬁfth
order Hurwitz matrix do not depend on the second control parameter ˜kσ and
have the form:
Δ(2)
3 = −p2(α −1)2(α + 1)(γ −1)( 9 (1 −γ) + α (6 (1 −γ) + α + 1) )(˜k∗
σ)2,
Δ(2)
5 = −144 p2
2 α (α −1)4(α + 1)2(γ −1)3(˜k∗
σ)3.
When solving the system of inequalities (7) (where, as in Fig. 1, p1 = 4/5,
p2 = 5/7 ) in relation to ˜k∗
σ, ˜kσ using function
Reduce[ { 0 < γ < 5/7, 0 < α < 1, S < 0, v6 > 0, v0 > 0, v2 > 0, v4 > 0,
v1 > 0, v3 > 0, v5 > 0, Δ(2)
3 > 0, Δ(2)
5 > 0 }, { ˜k∗
σ, ˜kσ }, Reals],

24
A.V. Banshchikov
we get the answer:
˜k∗
σ > 0 ∧˜kσ > 100 −33 α −133 γ
35 ( α + γ )
∧
∧
		
0 < α ≤3
25 ∧0 < γ < 5
7

∨
	 3
25 < α ≤5
33 ∧25α −3
28α
< γ < 5
7

∨
∨
	 5
33 < α < 19
44 ∧25 α −3
28 α
< γ < 100 −33 α
133


.
(8)
It is not diﬃcult to show that in the region E , the value 100−33α−133γ
35(α+γ)
> 0,
and, therefore, the parameter ˜kσ in (8) is positive. We note that any positive
value of the other parameter ˜k∗
σ satisﬁes solution (8). Thus, in the present case,
Qσ are the forces of friction and elasticity.
Let us construct the region of asymptotic stability (8) in the parameter plane
α, γ using “Mathematica” function RegionPlot, designed for a graphical repre-
sentation of the solution of the system of inequalities, with the next value of
the parameter ˜kσ = 10 . The result obtained is shown with a shaded region in
Fig. 2. It has been found that with an increasing (decreasing) value ˜kσ, this area
expands (narrows) within the limits of the borders found v6 = 0 , S = 0 , α = 0 ,
γ = 0 , γ = 5/7 (see Fig. 2) and disappears at a value ˜kσ = 0 .
Fig. 2. Region of asymptotic stability.

On the Asymptotic Stability of a Satellite with a Gravitational Stabilizer
25
A similar symbolic-numerical analysis has also been carried out for the control
forces Qσ = ˜k∗
ϕ ˙ϕ + ˜kϕ ϕ . As a result of the analysis, the following proposition
can be formulated.
Proposition 4. For the values of the parameters from the region E , system
(1) can be stabilized up to an asymptotic stability thanks to the eﬀect of control
forces Qσ = −˜k∗
σ ˙σ −˜kσ σ or Qσ = ˜k∗
ϕ ˙ϕ + ˜kϕ ϕ.
6
Conclusion
Based on the analogy with the parametric analysis presented above, the pos-
sibility of asymptotic stability was also investigated for other regions in Fig. 1.
The study has shown that replacing the initial parameters a, b, c, f, d with the
parameters α, γ, p1, p2 only slightly simpliﬁed the symbolic-numerical analysis.
But due to the limited values of α, γ, p1, p2, this replacement allowed us to see
a qualitative picture of the research. For a future research, the problem of the
inﬂuence of the structure of forces on system’s stability and its stabilization
requires a more detailed study.
It is necessary to emphasize the problems of reliability and precision of com-
putations, as well as the problems of explicitness and speeding-up of the process
of investigations can be partially solved when SPCA is chosen as a software
tool. Along with the application of the SPCA (as “a calculator”) for solving a
deﬁnite problem, the approach, which implies the elaboration of some software
for solving a deﬁnite class of problems on the basis of the internal program-
ming language of the SPCA (in our case – “Mathematica”), is quite important.
Practically, the whole above analysis has been conducted using this software.
The work has been partially supported by the Russian Foundation for Basic
Research (grant No. 16-07-00201). The research is partially supported by the
Council for Grants of the President of Russian Federation, state support of the
leading scientiﬁc schools, project No. NSh-8081.2016.9.
References
1. Sarychev, V.A.: Problems of orientation of satellites. Itogi Nauki i Tekhniki. Ser.
“Space Res.” 11, 5–224 (1978). VINITI Publication, Moscow (in Russian)
2. Potapenko, E.M.: Dynamics of a spacecraft with direct active control of the gravity
gradient stabilizer. Kosmicheskie Issledovaniya 26(5), 699–708 (1988). (in Russian)
3. Gutnik, S.A., Guerman, A., Sarychev, V.A.: Application of computer algebra
methods to investigation of inﬂuence of constant torque on stationary motions
of satellite. In: Gerdt, V.P., Koepf, W., Seiler, W.M., Vorozhtsov, E.V. (eds.)
CASC 2015. LNCS, vol. 9301, pp. 198–209. Springer, Cham (2015). doi:10.1007/
978-3-319-24021-3 15
4. Prokopenya, A.N., Minglibayev, M.Z., Mayemerova, G.M.: Symbolic calculations
in studying the problem of three bodies with variable masses. Program. Comput.
Softw. 40(2), 79–85 (2014)

26
A.V. Banshchikov
5. Banshchikov, A.V., Burlakova, L.A., Irtegov, V.D., Titorenko, T.N.: Symbolic com-
putation in modelling and qualitative analysis of dynamic systems. Comput. Tech-
nol. 19(6), 3–18 (2014). (in Russian)
6. Banshchikov, A.V., Irtegov, V.D., Titorenko, T.N.: Software package for modeling
in symbolic form of mechanical systems and electrical circuits. Certiﬁcate of State
Registration of Computer Software No. 2016618253. Federal service for intellectual
property. Issued 25 July 2016 (in Russian)
7. Chetaev, N.G.: Stability of Motion. Works on Analytical Mechanics. AS USSR,
Moscow (1962). (in Russian)
8. Banshchikov, A.V.: Parametric analysis of stability conditions for a satellite with a
gravitation stabilizer. In: Ganzha, V.G., et al. (ed.) CASC 2002, pp. 1–6. Technische
Universit¨at M¨unchen, Munich (2002)

Sparse Interpolation, the FFT Algorithm
and FIR Filters
Matteo Briani(B), Annie Cuyt, and Wen-shin Lee
Department of Mathematics and Computer Science (Wis-Inf),
Universiteit Antwerpen, Middelheimlaan 1, B-2020 Antwerpen, Belgium
{Matteo.Briani,annie.cuyt,wen-shin.lee}@uantwerpen.be
Abstract. In signal processing, the Fourier transform is a popular
method to analyze the frequency content of a signal, as it decomposes
the signal into a linear combination of complex exponentials with integer
frequencies. A fast algorithm to compute the Fourier transform is based
on a binary divide and conquer strategy.
In computer algebra, sparse interpolation is well-known and closely
related to Prony’s method of exponential ﬁtting, which dates back to
1795. In this paper we develop a divide and conquer algorithm for sparse
interpolation and show how it is a generalization of the FFT algorithm.
In addition, when considering an analog as opposed to a discrete ver-
sion of our divide and conquer algorithm, we can establish a connection
with digital ﬁlter theory.
1
Sparse Interpolation
Let the function φ(t) be given by
φ(t) =
n

i=1
αi exp(2πiμit)
and let us consider the general nonlinear interpolation problem of the samples
φ(tj), given by
φ(tj) =
n

i=1
αi exp(2πiμij/M),
j = 0, . . . , 2n −1, . . .
(1)
with
√
−1 = i,
distinct μi ∈C,
αi ∈C \ {0},
|Re(μi)| < M/2,
tj = j/M,
where, without loss of generality, M ∈IN. A solution of this interpolation prob-
lem was already presented in 1795 in [1] and can also be found in [2, pp. 378–
382]. Let us denote Ωi = exp(2πiμi/M), with Ωi ̸= Ωk when i ̸= k because
|Re(μi)| < M/2. It is apparent that the data φ(tj) are structured, namely
M. Briani—This research is supported by the Instituut voor Wetenschap en Tech-
nology - IWT.
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 27–39, 2017.
DOI: 10.1007/978-3-319-66320-3 3

28
M. Briani et al.
φ(tj) =
n

i=1
αiΩj
i ,
j = 0, . . . , 2n −1, . . .
(2)
We now want to obtain the values Ωi, i = 1, . . . , n and αi, i = 1, . . . , n from
the 2n samples φ(tj). From Ωi the value μi can easily be deduced because
2π|Re(μi)|/M < π and hence no periodicity problem arises. Temporarily we
assume that n is known. How n can be extracted from the samples is explained
in Sect. 2.
Consider the polynomial
n

i=1
(z −Ωi) = zn + bn−1zn−1 + · · · + b1z + b0
(3)
with so far unknown coeﬃcients bi, i = 1, . . . , n. Since the Ωi are its zeroes, we
ﬁnd for k ≥0,
0 =
n

i=1
αiΩk
i (Ωn
i + bn−1Ωn−1
i
+ · · · + b0)
=
n

i=1
αiΩn+k
i
+
n−1

j=0
bj
 n

i=1
αiΩj+k
i

= φ(tk+n) +
n−1

j=0
bjφ(tk+j).
In other words, we can conclude that the structured data φ(tj) are linearly
generated,
⎛
⎜
⎝
φ(t0)
. . . φ(tn−1)
...
...
...
φ(tn−1) . . . φ(t2n−2)
⎞
⎟
⎠
⎛
⎜
⎝
b0
...
bn−1
⎞
⎟
⎠= −
⎛
⎜
⎝
φ(tn)
...
φ(t2n−1)
⎞
⎟
⎠.
(4)
This linear system allows us to compute the coeﬃcients bi, i = 0, . . . , n −1 and
actually compose the polynomial (3) having Ωi, i = 1, . . . , n as its zeroes. Let us
now denote by H(r)
n
the Hankel matrix
H(r)
n
=
⎛
⎜
⎝
φ(tr)
. . . φ(tr+n−1)
...
...
...
φ(tr+n−1) . . . φ(tr+2n−2)
⎞
⎟
⎠
and by H(0)
n (z) the Hankel polynomial [3, p. 625]
H(0)
n (z) =

φ(t0)
. . . φ(tn−1)
φ(tn)
...
...
...
...
φ(tn−1) . . . φ(t2n−2) φ(t2n−1)
1
. . .
zn−1
zn

.

Sparse Interpolation, the FFT Algorithm and FIR Filters
29
Then
n

i=1
(z −Ωi) = H(0)
n (z)
|H(0)
n |
,
where |H(0)
n | denotes the determinant of H(0)
n . From the matrix factorisations
H(0)
n
= VnDαV T
n ,
H(1)
n
= VnDα
⎛
⎜
⎝
Ω1
...
Ωn
⎞
⎟
⎠V T
n ,
where Vn and Dα respectively denote the Vandermonde matrix
Vn =
⎛
⎜
⎜
⎜
⎝
1
1
. . .
1
Ω1
Ω2
. . .
Ωn
...
...
...
Ωn−1
1
Ωn−1
2
. . . Ωn−1
n
⎞
⎟
⎟
⎟
⎠
and the diagonal matrix
Dα =
⎛
⎜
⎝
α1
...
αn
⎞
⎟
⎠,
it is easy to see that the polynomial zeroes Ωi can also be obtained as generalized
eigenvalues [4,5]. So the Ωi also satisfy
det

H(1)
n
−ΩiH(0)
n

= 0,
i = 1, . . . , n.
(5)
The coeﬃcients αi in the model (1) can be obtained from any set of n interpo-
lation conditions taken from (2),
⎛
⎜
⎝
Ωj
1
. . .
Ωj
n
...
...
Ωj+n−1
1
. . . Ωj+n−1
n
⎞
⎟
⎠
⎛
⎜
⎝
α1
...
αn
⎞
⎟
⎠=
⎛
⎜
⎝
φ(tj)
...
φ(tj+n−1)
⎞
⎟
⎠,
0 ≤j ≤n.
(6)
With Ωi computed as above, the remaining equations are linearly dependent.
Whether solving (4) or (5), the Hankel matrices involved tend to become
quite ill-conditioned when n increases [6,7]. So in practice, one may be interested
in a divide and conquer approach where the full system is divided into several
smaller systems, thus keeping the condition number under control. In Sect. 2 we
present such an algorithm, which we connect to the traditional FFT in Sect. 3.
Our goal is not to incorporate sparsity considerations into the FFT algorithm as
in [8], but rather to add the divide and conquer approach of the FFT to sparse

30
M. Briani et al.
interpolation. Related work can be found in [9] where digital ﬁlters are used as
a splitting technique and Prony’s method is used to solve for the non-ﬁltered μi.
So here the classical FFT algorithm will appear as a special case, when
restricting the μi to integer values. In its most general form, with μi complex,
our formula is related to a comb ﬁlter. The former is the subject of the Sects. 2
and 3, while the latter is discussed in the Sects. 4 and 5.
2
Divide and Conquer Approach
In this section we assume for simplicity that Re(μi) ∈ZZ and we introduce
ω = exp(2πi/N) with the integer N > 0. In addition we require that N divides
M, thus guaranteeing that M/N ∈IN. From our samples φ(tj) we now deduce
N linear combinations φk(tj) by the construction [10, pp. 15–17]
φk(tj) := 1
N
N−1

ℓ=0
ωkℓφ(tj + ℓ/N),
k = 0, . . . , N −1.
(7)
These φk(tj) are linear combinations of already collected samples φ(tj+Mℓ/N)
since tj+ℓ/N can be expressed as (j+Mℓ/N)/M. Figure 1 graphically illustrates
formula (7). Each derived sample contains only some of the original components
of (1), as can be seen from the rearrangement
φk(tj) = 1
N
N−1

ℓ=0
ωkℓφ(tj+Mℓ/N)
= 1
N
N−1

ℓ=0
ωkℓ
n

i=1
αi exp (2πiμi(j/M + ℓ/N))
= 1
N
N−1

ℓ=0
ωkℓ
n

i=1
αi exp(2πiμitj)ωℓμi
= 1
N
n

i=1
αi exp(2πiμitj)
N−1

ℓ=0
ωℓ(k+μi)

.
(8)
We remark that
N−1

ℓ=0
ωℓ(k+μi) = N if mod(k + μi, N) = 0,
N−1

ℓ=0
ωℓ(k+μi) = 0 otherwise.
(9)
So actually, every component of the original exponential sum (1) is present in
one and only one linear combination φk. When Re(μi) ∈ZZ formula (7) allows
a perfect split of (1) over N smaller sized problems. Since each φk has the same

Sparse Interpolation, the FFT Algorithm and FIR Filters
31
Fig. 1. Formula (7) with M = 80 and N = 8.
exponential structure as (1), we can apply (4) or (5) to it and identify the
parameters αi and μi present in φk from the values φk(tj). And this for each
smaller exponential sum φk, k = 0, . . . , N −1.
But (7) also remains valid for general μi ∈C as it is merely a linear combi-
nation of the samples taken at equidistant points. In Sect. 4 we see that, what
changes when going from Re(μi) ∈ZZ to μi ∈C, is that the factor
N−1

ℓ=0
ωℓ(k+μi)
that accompanies each term in a particular φk(tj) is replaced by expression (13)
of which the behaviour is illustrated in Fig. 2.
Let us now discuss the number of terms in each of the φk and for this we ﬁrst
consider the detection of n in (1) which we didn’t touch in Sect. 1. In an exact
(noisefree) context, the value of n can simply be detected from the theorems
given in [3, p. 603] and [11, pp. 20–31]:
det H(r)
n
̸= 0,
det H(r)
ν
= 0,
ν > n,
It is analyzed in [12] that when ν < n, the value det H(r)
ν
is not guaranteed zero
as for ν > n, or guaranteed nonzero as for ν = n, but can vanish accidentally
when by the choice of M and r one hits a zero of this expression. From these
statements the number of components n can be obtained as the rank of H(r)
ν
for
ν > n. In order to inspect |H(r)
ν | for ν > n, additional samples up to tr+2ν−2
need to be provided, in other words at least the additional sample φ(t2n) in case
r = 0 and ν = n + 1.

32
M. Briani et al.
The smaller exponential interpolation problems built with the values φk(tj)
for each k separately, may contain less exponential terms and hence their Hankel
matrices
H(r)
n,k =
⎛
⎜
⎝
φk(tr)
. . . φk(tr+n−1)
...
...
...
φk(tr+n−1) . . . φk(tr+2n−2)
⎞
⎟
⎠
may have a rank smaller than n. For each k = 0, . . . , N −1, the rank of H(r)
ν,k is
less than or equal to n and the sum of these ranks equals exactly n.
We present a small example to illustrate the principle of (7). Let (1) be
deﬁned by the values for αi and μi given in Table 1.
Table 1. Ill-conditioned example of (1).
Re(μi)
5 6
7
8
9
45
−10
−33
Im(μi)
0 0
0
0
0
0
0
0
| αi |
1 1
1
1
1
1
1
1
arg(αi) 0 π/4 π/2 3π/4 π 5π/4 3π/2 7π/4
With M = 100 and n = 8 the Hankel matrix H(0)
n
has a condition number of
the magnitude 7.7 × 109! In [13] oversampling is used as a means to reduce the
condition number. Here we use (8) to split the exponential analysis problem and
bring the condition number down. We take N = 5. Each of the samples φk(tj)
for k = 0, . . . , 4 involves only a subset of the original components exp(2πiμitj),
as detailed in Table 2.
Table 2. Example from Table 1 split into N = 5 subsets.
k Re(μi)
Condition nr
0 5
45
−10 2.2 × 100
1 9
1.0 × 100
2 8
1.0 × 100
3 7
−33
1.4 × 100
4 6
1.0 × 100
The major improvement in the conditioning is not only due to the reduction
in size of the Hankel matrices involved, but also to a much better disposition in
the complex plane of the frequencies μi per subsum.

Sparse Interpolation, the FFT Algorithm and FIR Filters
33
3
The FFT Algorithm
An algorithm related to formula (7) is the FFT algorithm which retrieves the
coeﬃcients αi from a set of samples φ(tj), j = 0, . . . , M −1 given by
φ(tj) =
M

i=1
αi exp(2πiij/M).
(10)
The diﬀerence between (10) and (1) is that now all integer frequencies appear,
so μi = i, and that therefore the number of terms in the sum equals M, which
is also the number of samples. The coeﬃcients αi in (10) are called Fourier
coeﬃcients. In a way, (7) is a generalization of the FFT to sparse interpolation
or Prony’s algorithm as we now explain in some more detail.
Let M = N1 × · · · × Nm with all Nk ∈IN. Then the FFT algorithm breaks
down the set of samples (10) into new diﬀerent sets as follows. We detail the ﬁrst
divide of φ(tj) into N1 smaller exponential sums, starting from (8). For μi = i
and n = M, we ﬁnd from (8):
φk(tj) = 1
N1
M

i=1
αi exp(2πiij/M)
N1−1

ℓ=0
ωℓ(k+i)

,
k = 0, . . . , N1 −1
where
1
N1
N1−1

ℓ=0
ωℓ(k+i)
evaluates to either 0 or 1. Preserving only the terms that are not multiplied by
zero leads to
φk(tj) =
M/N1

i=1
α1+(i−1)N1+k exp(2πij(1 + (i −1)N1 + k)/M)
=
M/N1

i=1
α1+(i−1)N1+k exp(2πijiN1/M) exp(2πij(1 −N1 + k)/M)
=
M/N1

i=1
α1+(i−1)N1+k exp(2πiij/(M/N1)) exp(2πij(1 −N1 + k)/M)
k = 0, . . . , N1 −1
(11)
The subsequent step in which each smaller sum is divided into N2 new smaller
sums is obvious for k = N1−1, but the other φk ﬁrst need to be multiplied by the
so-called twiddle factor exp(−2πij(1−N1 +k)/M) in order to bring them in the
correct form (1). For the subdivision of each of the N1 sums into N2 yet smaller
sums, one substitutes in (11) and the expression for the twiddle factors, M by
M/N1 and N1 by N2. In this way one continues until the algorithm has created

34
M. Briani et al.
M sums each containing only one component of the form αi exp(2πiij/M). Thus
at the ﬁnal stage each single component immediately reveals the coeﬃcient αi.
The case where M = 2m is of particular interest because then (8) and (11)
simplify even further (ω = exp(πi) = −1) into
φk(tj) = 1
2
1

ℓ=0
(−1)ℓkφ(tj + ℓ/2),
k = 0, 1.
4
An Analog Version of the Splitting Technique
We now consider a generalization of (7) when it does not make sense to require
that the Re(μi) be integer, as we did in the discrete case. To this end we intro-
duce, in addition to ω = exp(2πi/N),
Ω = ωκ,
||κ|| = 1.
Fig. 2. The functions M4(1, μ)/N (left) and |M4(1, μ)/N| (right) for μ ∈[0, 5].
The samples φk(tj) derived from the samples φ(tj) are then deﬁned by the
following continuous analogon of (7):
φk(tj) = 1
N
N−1

ℓ=0
Ωkℓφ(tj + ℓ/N)
= 1
N
N−1

ℓ=0
Ωkℓ
n

i=1
αi exp (2πiμij/M + 2πiμiℓ/N)
= 1
N
N−1

ℓ=0
Ωkℓ
n

i=1
αi exp(2πiμij/M)ωℓμi
= 1
N
n

i=1
αi exp(2πiμij/M)
N−1

ℓ=0
ωℓ(k+μi) κℓk
= 1
N
n

i=1
αi exp(2πiμij/M)Mk(κ, μi),
k = 0, . . . , N −1,
(12)

Sparse Interpolation, the FFT Algorithm and FIR Filters
35
where Mk(κ, μ), for ﬁxed N, is deﬁned by
Mk(κ, μ) := 1 −

ωk+μκkN
1 −ωk+μκk
.
(13)
In case κ = 1 formula (12) coincides with (8). However, the value of (13) does
not reduce to 0 or N as in (9). By (12) all integer frequencies Re(μi) are either
zeroed or copied to φk, as in (9), while the non-integer frequencies inbetween are
ampliﬁed as in Fig. 2, where we illustrate (12) for κ = 1, N = 5 and Re(μi) ∈
[0, 5]. The function Mk(κ, μ) is periodic, and in Fig. 2 the period equals 5. The
eﬀect on the integer frequencies μ = i, i = 0, . . . , 5 is accentuated in the graph
at the bottom in Fig. 2.
The complex number κ = exp(2πiθ) on the unit circle acts as a continuous
shifter of Re(μi), as shown in Fig. 3. Increasing k to k + 1 in (7) can also be
achieved by choosing κ = exp(2πi/N) in (12).
Fig. 3. Inﬂuence of the parameter κ while N and ω are kept equal in both Mk graphs.
Table 3. Analog divide and conquer illustration.
Re(μi)
5 6
7.3
8
9.5
45
−10
−33
Im(μi)
0 0
−0.1 0
−0.001 0
0
0
| αi |
1 1
1
1
1
1
1
1
arg(αi) 0 π/4 π/2
3π/4 π
5π/4 3π/2 7π/4
We repeat the example of Sect. 2 where the data have now been altered so
that Re(μi) ̸∈ZZ and Im(μi) ̸= 0. The new data can be found in Table 3. We
take a look at φ4(tj) given by (7) and (12) but with the μi from Table 3 and
with κ = 1. The components in φ4(tj) are now multiplied by M4(1, μi)/N.
So none of the non-integer frequencies is annihilated. The μi with non-integer

36
M. Briani et al.
Table 4. Analysis of φ4(tj) for μi from Table 3.
Re(μi)
7.3
9.5
Im(μi)
−0.1
−0.001
| αiM4(1, μi)/N | 0.1361 0.6456
real parts are weakened in modulus as indicated in Table 4. By repeating the
multiplication with M4(1, μi)/N this eﬀect is strengthened. In order to retrieve
the correct αi, the coeﬃcient of exp(2πiμij/M) in φ4(tj) which can be obtained
using a standard exponential analysis needs to be multiplied by N/M4(κ, μi).
The eﬀect of M4(1, μ) is graphically illustrated in Fig. 4.
Fig. 4. Eﬀect of the function M4(1, μ)/N on the frequencies in Table 3.
5
Connection to FIR Filters
We want to illustrate how formula (12) can be interpreted as the result of a
digital ﬁlter. In general, a digital ﬁlter takes a set of samples as input, applies
a transform and delivers another set of samples as output. In a ﬁnite impulse
response or FIR ﬁlter the output samples are a linear combination of the present

Sparse Interpolation, the FFT Algorithm and FIR Filters
37
and previous input samples. If we denote the ﬁlter coeﬃcients by βℓand the
sampling distance is 1/M, then the ﬁltered signal ψ(tj) equals
ψ(tj) =
L−1

ℓ=0
βℓφ(tj −ℓ/M).
When the input signal is the unit impulse δ(·) where δ is the Kronecker delta
function, then the output signal is called the impulse response h(tj) given by
h(tj) =
L−1

ℓ=0
βℓδ(tj −ℓ/M) = βj,
tj = j/M.
The transfer function associated with the FIR ﬁlter ψ equals
H(z) =
L−1

ℓ=0
βℓz−ℓ.
In order to establish a link with formula (12), we deﬁne for k ﬁxed and ℓ=
0, . . . , L −1 = M −1, (remember that N divides M),
βℓk :=
⎧
⎨
⎩
1
N Ωk(N−(ℓ+1)/(M/N)),
(ℓ+ 1)/(M/N) ∈IN
0,
otherwise.
When putting the βℓk for ﬁxed k in a vector, they are structured in N blocks of
size M/N, each block containing M/N −1 zeroes and one power of Ωk:
1
N

0, . . . , 0, Ω(N−1)k, 0, . . . , 0, Ωk, 0, . . . , 0, Ω0
Since formula (12) is based on the current and future samples, we also need to
shift the signal in order to ﬁt the ﬁlter description:
φ(tj) := φ(tj + (1 −1/M)).
Then
ψ(tj) = φk(tj) =
M−1

ℓ=0
βℓkφ(tj −ℓ/M).
(14)
The impulse response of the ﬁlter (12), rewritten as (14), is given by
hk(tj) = βjk.
The ﬁlter (12) gets a crisper look, meaning that it is ﬂatter in the neighborhood
of the zeroes and exhibits a sharper peak where it attains one, when applied
iteratively. In Fig. 5 we show the result of (12) applied once (as in Fig. 2), twice
and ﬁve times, reminding us more and more of a comb ﬁlter [14, p. 474].

38
M. Briani et al.
Fig. 5. FIR ﬁlter (12) applied once, twice and ﬁve times.
6
Conclusion
Sparse interpolation, which is a special case of multi-exponential analysis, can be
combined with a divide and conquer technique which is a direct generalization of
the fast Fourier transform algorithm in case the frequencies belong to a discrete
set. This connection opens up new computational possibilities in the ﬁtting of
sparse models to data.
An analog version of our general divide and conquer method is related to
digital ﬁlter theory, more precisely FIR ﬁlter theory.
References
1. de Prony, R.: Essai exp´erimental et analytique sur les lois de la dilatabilit´e des
ﬂuides ´elastiques et sur celles de la force expansive de la vapeur de l’eau et de la
vapeur de l’alkool, `a diﬀ´erentes temp´eratures. J. Ec. Poly. 1, 24–76 (1795)
2. Hildebrand, F.: Introduction to Numerical Analysis. McGraw Hill, New York
(1956)
3. Henrici, P.: Applied and computational complex analysis I. Wiley, New York (1974)
4. Hua, Y., Sarkar, T.K.: Matrix pencil method for estimating parameters of exponen-
tially damped/undamped sinusoids in noise. IEEE Trans. Acoust. Speech Signal
Process. 38(5), 814–824 (1990)
5. Golub, G., Milanfar, P., Varah, J.: A stable numerical method for inverting shape
from moments. SIAM J. Sci. Comput. 21, 1222–1243 (1999)
6. Beckermann, B.: The condition number of real Vandermonde, Krylov and positive
deﬁnite Hankel matrices. Numer. Math. 85, 553–577 (2000)
7. Beckermann, B., Golub, G., Labahn, G.: On the numerical condition of a general-
ized Hankel eigenvalue problem. Numer. Math. 106(1), 41–68 (2007)
8. Potts, D., Tasche, M., Volkmer, T.: Eﬃcient spectral estimation by MUSIC and
ESPRIT with application to sparse FFT. Front. Appl. Math. Stat. 2, 1–16 (2016).
Article 1

Sparse Interpolation, the FFT Algorithm and FIR Filters
39
9. Heider, S., Kunis, S., Potts, D., Veit, M.: A sparse prony FFT. In: Proceed-
ings of the 10th International Conference on Sampling Theory and Applications
(SAMPTA), pp. 572–575 (2013)
10. Cuyt, A., Lee, W.-s.: Smart sampling and sparse reconstruction. GB Priority
1114255.1, (ﬁled on 18.08.2011, published on 21.02.2013). WIPO Patentscope.
https://patentscope.wipo.int/search/docservicepdf pct/id00000020191665/PDOC
/WO2013024177.pdf
11. Baker Jr., G., Graves-Morris, P.: Pad´e Approximants. Encyclopedia of Mathemat-
ics and Its Applications, vol. 59, 2nd edn. Cambridge University Press, Cambridge
(1996)
12. Kaltofen, E., Lee, W.-s., Lobo, A.A.: Early termination in Ben-Or/Tiwari sparse
interpolation and a hybrid of Zippel’s algorithm. In: Proceedings of the 2000 Inter-
national Symposium on Symbolic and Algebraic Computation, pp. 192–201. ACM,
New York (2000)
13. Potts, D., Tasche, M.: Parameter estimation for nonincreasing exponential sums by
Prony-like methods. Linear Algebra Appl. 439(4), 1024–1039 (2013). 17th Confer-
ence of the International Linear Algebra Society, Braunschweig, Germany, August
2011
14. Schlichth¨arle, D.: Digital Filters. Springer, Heidelberg (2011). doi:10.1007/
978-3-642-14325-0

On New Integrals of
the Algaba-Gamero-Garcia System
Alexander D. Bruno1, Victor F. Edneral2,3(B), and Valery G. Romanovski4,5,6
1 Keldysh Institute of Applied Mathematics of RAS,
Miusskaya Sq. 4, Moscow 125047, Russia
abruno@keldysh.ru
2 Skobeltsyn Institute of Nuclear Physics, Lomonosov Moscow State University,
Leninskie Gory 1(2), Moscow 119991, Russia
edneral@theory.sinp.msu.ru
3 Peoples’ Friendship University of Russia (RUDN University),
6 Miklukho-Maklaya Street, Moscow 117198, Russian Federation
edneral vf@rudn.university
4 Faculty of Electrical Engineering and Computer Science, University of Maribor,
Smetanova 17, SI-2000 Maribor, Slovenia
5 CAMTP – Center for Applied Mathematics and Theoretical Physics,
University of Maribor, Krekova 2, SI-2000 Maribor, Slovenia
6 Faculty of Natural Science and Mathematics, University of Maribor,
Koroˇska cesta 160, SI-2000 Maribor, Slovenia
valery.romanovsky@uni-mb.si
Abstract. We study local integrability of a plane autonomous polyno-
mial system of ODEs depending on ﬁve parameters with a degenerate
singular point at the origin. The approach is based on making use of
the Power Geometry Method and the computation of normal forms. We
look for the complete set of necessary conditions on parameters of the
system under which the system is locally integrable near the degenerate
stationary point. We found earlier that the sets of parameters satisfy-
ing these conditions consist of four two-parameter subsets in the full
ﬁve-parameter co-space. Now we consider the special subcase of the case
b2 = 2/3 and separate subsubcases when additional ﬁrst integrals can
exist. Here we have found two such integrals.
Keywords: Ordinary diﬀerential equations · Integrability · Resonant
normal form · Power Geometry · Computer algebra
1
Introduction
We consider an autonomous system of ordinary diﬀerential equations of the form
dxi/dt
def
= ˙xi = ϕi(X),
i = 1, 2,
(1)
where X = (x1, x2) ∈C
2 and ϕi(X) are polynomials.
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 40–50, 2017.
DOI: 10.1007/978-3-319-66320-3 4

On New Integrals of the Algaba-Gamero-Garcia System
41
A method of the analysis of integrability of system (1) based on power trans-
formations [5] and computation of normal forms near stationary solutions of
transformed systems (see [3] and Chap. 2 in [4]) was proposed in [6–8].
In a neighborhood of the stationary point X = 0, system (1) can be written
in the form
˙X = A X + ˜Φ(X),
(2)
where ˜Φ(X) has no terms linear in X.
Let λ1 and λ2 be the eigenvalues of the matrix A. If at least one of them
is diﬀerent from zero, then the stationary point X = 0 is called an elementary
stationary point. In this case, system (2) has a normal form (see, e.g. Chap. 2
in [4]). If both eigenvalues vanish, then the stationary point X = 0 is called
a nonelementary stationary point. In this case, there is no normal form for
system (2). But using power transformations, we can split the nonelementary
stationary point X = 0 to a set of elementary stationary points [5]. For each of
these elementary stationary points, we can compute the normal form and write
the conditions of local integrability.
In the present paper, we demonstrate how this approach can be applied to
study the local and global integrability in the case of planar system near a
stationary point X0 = 0 of high degeneracy in the case of the system
˙x = α y3 + β x3 y + (a0 x5 + a1 x2y2),
˙y = γ x2 y2 + δ x5 + (b0 x4y + b1 x y3).
(3)
This system is a subfamily of the system introduced in [1].
Systems with a nilpotent matrix of the linear part were thoroughly studied
by Lyapunov and others. In system (3), there is no linear part, and the ﬁrst
approximation is not homogeneous. This is the simplest case of a planar system
without linear part and with Newton’s open polygon (see Chap. 2 in [4]) con-
sisting of a single edge. In generic case, such problems have not been studied.
However, the system with such support was considered in [1], where the authors
set −α = δ = 1 and 3 β + 2 γ = 0. Further, the authors of [1] studied the
Hamiltonian subcase of this system under the additional assumption that the
Hamiltonian function is expandable into the product of only square-free factors.
We study the following problem: what are the conditions on parameters
under which system (3) is locally or globally integrable when the system is
non-Hamiltonian.
We discuss the ﬁrst quasi-homogeneous approximation of system (3) and the
necessary conditions of local integrability. First, we calculate the sets of these
conditions. Then, we prove that these conditions are suﬃcient for the global
integrability by computing the corresponding ﬁrst integrals of motion. We do it
ﬁrst for the case when b2 ̸= 2/3 in system (6), and after that for a subcase of
the case b2 = 2/3. The search for the necessary conditions of local integrability
by this approach is impossible without computer algebra methods.

42
A.D. Bruno et al.
2
Problem Statement
We start from the study of the case when the ﬁrst quasi-homogeneous approxi-
mation of (3) considered in [7–9] has the form
˙˜x = α ˜y3 + β ˜x3 ˜y,
˙˜y = γ ˜x2 ˜y2 + δ ˜x5,
(4)
where α ̸= 0 and δ ̸= 0. Using the linear transformation x = σ˜x, y = τ ˜y we can
ﬁx two nonzero parameters in (4) and obtain the system
˙x = −y3 −b x3 y,
˙y = c x2 y2 + x5.
(5)
Each autonomous planar quasi-homogeneous system (4) has an integral, but
it is not necessarily analytic. We are interested to have the analytic integrability
of (4), so we look for conditions on parameters under which system (3) is locally
or globally analytically integrable.
The following result was proven in [7–9]:
Theorem 1. In the case D
def
= (3 b + 2 c)2 −24 ̸= 0, system (5) is locally inte-
grable if and only if the number N = (3 b −2 c)/
√
D is rational.
In this paper, we will study a simple particular case c = 1/b, then N = 1 and
D = (3b −2/b)2. In view of Theorem 1, the ﬁrst quasi-homogeneous approxima-
tion has an analytic integral if b2 ̸= 2/3 but it is not a Hamiltonian system.
We will study the integrability problem for the entire system (3) with the
ﬁrst quasi-homogeneous approximation (5) writing the system in the form
dx/dt = −y3 −b x3y + a0 x5 + a1 x2y2,
dy/dt = (1/b) x2y2 + x5 + b0 x4y + b1 x y3.
(6)
Thus, we consider the system with ﬁve arbitrary parameters ai, bi, (i = 0, 1) and
b ̸= 0.
3
Necessary Conditions of Local Integrability
The rationality of the ratio λ1/λ2 and the condition A (see [3,4,7,8]) are the
necessary and suﬃcient conditions for local analytical integrability of a planar
system near an elementary stationary point. The condition A is a strong algebraic
condition on coeﬃcients of the normal form. For local integrability of the original
system (1) near a degenerate (nonelementary) stationary point, it is necessary
to have local integrability near each of the elementary stationary points, which
are produced by the blowing up process described below.
The algorithm for calculation of the normal form and the normalizing
transformation together with the corresponding computer program are brieﬂy
described in [11].

On New Integrals of the Algaba-Gamero-Garcia System
43
At the ﬁrst step, we should rewrite (6) in a non-degenerate form. It can be
done using the power transformation (see Chap. 1, $1.8 in [4])
x = u v2,
y = u v3
(7)
and the time rescaling u2v7dt = dτ. As a result, we obtain system (6) in the
form
du/dτ = −3 u −[3 b + (2/b)]u2 −2 u3 + (3 a1 −2 b1)u2v+
(3 a0 −2 b0)u3v,
dv/dτ = v + [b + (1/b)]u v + u2v + (b1 −a1)u v2 + (b0 −a0)u2v2.
(8)
Under the power transformation (7), the point x = y = 0 blows up into two
straight invariant lines u = 0 and v = 0. Along the line u = 0, system (8) has a
single stationary point u = v = 0. Along the second line v = 0, this system has
four elementary stationary points
u = 0,
u = −1
b ,
u = −3b
2 ,
u = ∞.
(9)
The necessary condition of local integrability of system (6) near the point
x = y = 0 is local integrability near all stationary points of system (8).
Lemma 1. Near the points u = v = 0 and u = ∞, v = 0, system (8) is locally
integrable.
This lemma was proven in [7–9].
Thus, we must ﬁnd conditions of local integrability at two other stationary
points (9). Then we will have the conditions of local integrability of system (6)
near the original point.
Let us consider the stationary point u = −1/b, v = 0. First we restrict
ourselves to the case b2 ̸= 2/3 when the linear part of system (8), after the shift
u = w −1/b, has non-vanishing eigenvalues. At b2 = 2/3, the matrix of the
linear part of the shifted system in new variables w and v has the Jordan cell
with both zero eigenvalues (17). This case will be studied by means of one more
power transformation below. In papers [3,4], the condition A was formulated
and applied to the considered problem in [6]. For two-dimensional systems, it
is a suﬃcient condition of their integrability. It is an algebraic condition on
the coeﬃcients of the normal form. In our case, it can be written as a system
of algebraic equations. We have computed the condition A with the program
described in [11]. There are two solutions of the corresponding subset of equations
from the condition A [9] at b ̸= 0:
a0 = 0,
a1 = −b0 b,
b1 = 0,
b2 ̸= 2/3
(10)
and
a0 = a1 b,
b0 = b1 b,
b2 ̸= 2/3.
(11)

44
A.D. Bruno et al.
The consideration of the stationary point u = −3 b/2, v = 0 under condition
(11) gives three more two-parameter (depending on a1 and b) solutions
(1)
b1 = −2 a1,
a0 = a1b,
b0 = b1b,
b2 ̸= 2/3,
(2)
b1 = (3/2) a1, a0 = a1b,
b0 = b1b,
b2 ̸= 2/3,
(3)
b1 = (8/3) a1, a0 = a1b,
b0 = b1b,
b2 ̸= 2/3.
(12)
Thus, we have proved.
Theorem 2. Conditions (10) and (12) form the set of necessary conditions
of local integrability of system (8) near all its stationary points and the local
integrability of system (6) at the stationary point x = y = 0.
This theorem was proven in [7–9].
4
Suﬃcient Conditions of Integrability
The conditions presented in Theorem 2 as the necessary and suﬃcient conditions
for the local integrability of system (6) at the stationary point at the origin can
be considered as good candidates for suﬃcient conditions of global integrability.
However, it is necessary to prove the suﬃciency of these conditions by indepen-
dent methods. It is necessary to do it for each of four conditions (10) and (12)
at each of the stationary points u = −3b/2, v = 0 and u = −1/b, v = 0, for
b2 ̸= 2/3.
In [12], we found ﬁrst integrals of system (8) for all cases (10), (12) (mainly
by the Darboux method, see, e.g., [13]).
We found four families of solutions which exhausted all cases mentioned
above:
1. At
a0 = 0,
a1 = −b0 b,
b1 = 0:
I1uv = u2(3 b + 2 u)v6,
I1xy = 2 x3 + 3 b y2.
(13)
2. At
b1 = −2a1, a0 = a1b, b0 = b1b:
I2uv = u2 v6 (3 b + u (2 −6 a1 b v)),
I2xy = 2 x3 −6 a1 b x2 y + 3 b y2.
(14)
3. At
b1 = 3a1/2, a0 = a1b, b0 = b1b:
I3uv = [4 −4a1 u v + 35/6a1 ×2 F1 (2/3, 1/6; 5/3; −2u/(3b)) ×
u v (3 + 2u/b)1/6]/[u1/3v (3b + 2u)1/6],
I3xy = [a1x2(−4 + 35/6 2F1

2/3, 1/6; 5/3; −2 x3/(3 b y2)

×
(3 + 2x3/(b y2))1/6) + 4y]/[y4/3(3 b + 2 x3/y2)1/6],
(15)

On New Integrals of the Algaba-Gamero-Garcia System
45
4. At
b1 = 8a1/3, a0 = a1b, b0 = b1b:
I4u,v = [u (3 + 2 a2
1bu) + 6 a1 b v]/
[3 u [u3(6 + a2
1b u) + 6 a2
1b u2v + 9 b v2]1/6] −
(16)
8 a1
√
−b/35/3B6+a1
√−6 b u+3 v √
−6 b/u3(5/6, 5/6),
where Bt(a, b) is the incomplete beta function and 2F1(a, b; c; z) is the hyperge-
ometric function [2].
The ﬁrst integrals and solutions do not have any singularities for the values
b2 = 2/3, but the approach with the aid of which these solutions were found has
the limitation b2 ̸= 2/3, so there are possible additional integrals at these values.
Thus, we need to study the case b2 = 2/3 separately.
5
Case b2 = 2/3, Subcase 3a0 −2b0 = b(3a1 −2b1)
Let us consider the case b =

2/3. At these values b, both stationary points
u = −3b/2, v = 0 and u = −1/b, v = 0 are collapsing, and after the shift
u →w −1/b, we have instead of (8) the degenerate system
dw
dτ = v(−9
2

3
2 a0 + 9
2 a1 + 3

3
2 b0 −3 b1)+
wv( 27
2 a0 −3
√
6 a1 −9 b0 + 2
√
6 b1)+
√
6 w2 + w2v(−9

3
2 a0 + 3 a1 + 3
√
6 b0 −2 b1)−
2 w3 + w3v(3 a0 −2 b0),
dv
dτ = −
√
6
6 wv + v2(−3
2 a0 +

3
2 a1 + 3
2 b0 −

3
2 b1)+
w2v + wv2((
√
6 a0 −a1 −
√
6 b0 + b1)+
+w2v2(−a0 + b0).
(17)
This system has zero eigenvalues at the stationary point w = v = 0, so we should
apply a power transformation once again. In [9], we used the transformation
v →r2w,
˙v →2 ˙rrw + r2 ˙w,
(18)
and obtained the systems with resonances of 19th and 27th orders. We calculated
the corresponding normal form with 4 free parameters till 19th order, but for
ﬁnding new solutions we should have more equations, and we need to compute
the 27th order resonance. This resonance exists only if 3a0 −2b0 ̸= b(3a1 −2b1),
b2 = 2/3, and its calculation is very hard. We postpone the investigation of this
subcase.

46
A.D. Bruno et al.
But if b2 = 2/3, equation (17) can be rewritten as
dw
dτ = −3v/(2b)[(3a0 −2b0) −b(3a1 −2b1)]+
wv( 27
2 a0 −3
√
6 a1 −9 b0 + 2
√
6 b1)+
√
6 w2 + w2v(−9

3
2 a0 + 3 a1 + 3
√
6 b0 −2 b1)−
2 w3 + w3v(3 a0 −2 b0),
dv
dτ = −
√
6
6 wv + v2(−3
2 a0 +

3
2 a1 + 3
2 b0 −

3
2 b1)+
w2v + wv2((
√
6 a0 −a1 −
√
6 b0 + b1)+
+w2v2(−a0 + b0).
(19)
We see that in systems (17) and (19), the coeﬃcient of v in the linear part of the
ﬁrst equation is zero if 3a0 −2b0 = b(3a1 −2b1). So we have the special subcase
3a0 −2b0 = b(3a1 −2b1),
b2 = 2/3.
For this subcase, we use the transformation
u →w −1/b,
v →rw,
˙v →˙rw + r ˙w,
(20)
with the time scaling by division of the equations by w/
√
6, so ˜τ = wτ/
√
6.
Then, from (8) we have
dw
d˜τ = 6w + 3(3a1 −2b1)rw −2
√
6w2 −2
√
6(3a1 −2b1)rw2+
2(3a1 −2b1)rw3,
dr
d˜τ = −7r −(9a1 −

3
2b0 −5b1)r2 + 3
√
6rw+
(7
√
6a1 −2b0 −13

2
3b1)r2w −(8a1 −

3
2b0 −16
3 b1)r2w2.
(21)
This is a three-parameter system with the resonance of the 13th order at the
stationary point w = 0, r = 0 on the invariant line w = 0. Along this line, there
is also another stationary point. It is possible to prove the integrability of the
system there, and this point does not supply any additional restriction on the
parameters.
We have calculated the normal form for (21) till the 26th order and obtained
two equations for the condition A. They are a13 = 0 and a26 = 0, where a13
and a26 are given in [14]. Each of these equations is homogeneous in parameters
a1, b0, b1 of system (6) of sixth and twelfth orders, for example, a13 is
a13 =
77591416320*a1^6*s6+65110407552*a1^5*b0-343384549344*a1^5*b1*s6-
214574033664*a1^4*b0^2*s6-1084658542848*a1^4*b0*b1+
495240044652*a1^4*b1^2*s6-618953467392*a1^3*b0^3+
59995851552*a1^3*b0^2*b1*s6+1782026653968*a1^3*b0*b1^2-
325584668628*a1^3*b1^3*s6-8037029376*a1^2*b0^4*s6+
642627782784*a1^2*b0^3*b1+230489977896*a1^2*b0^2*b1^2*s6-
1080958485096*a1^2*b0*b1^3+105084809187*a1^2*b1^4*s6-

On New Integrals of the Algaba-Gamero-Garcia System
47
29504936448*a1*b0^5+95627128896*a1*b0^4*b1*s6-
130189857408*a1*b0^3*b1^2-155744503512*a1*b0^2*b1^3*s6+
270984738720*a1*b0*b1^4-15802409798*a1*b1^5*s6+
19669957632*b0^5*b1-20179406208*b0^4*b1^2*s6-
15425489664*b0^3*b1^3+25998124528*b0^2*b1^4*s6-
22559067296*b0*b1^5+882415736*b1^6*s6,
where s6 =
√
6. Both a13 and a26 are equal to zero at solutions (10) and (12).
Homogeneous algebraic equations in three variables can be rewritten as inho-
mogeneous equations in two variables. If we suppose that a1 = 0, we get only one
and zero dimensional solutions in the parametric cospace. Let us postpone the
consideration of these cases and suppose that a1 ̸= 0. In this case, we substitute
b0 = c0 a1, b1 = c1 a1 and obtain the system of two equations in two variables
a13(c0, c1) = 0,
a26(c0, c1) = 0. The resultant of two corresponding polynomi-
als in each of two variables is identically equal to zero. It is interesting that the
condition A of the 19th order from [9,15] is identically equal to a13 up to mul-
tiplication by a constant. So it is enough to solve only equation a13(c0, c1) = 0.
This equation can be factorized as the product of four factors including a6
1:
a13 = 48(c1 −3/2)×
(c0 −1/12
√
6c1 + 1/2
√
6)2×
[409790784c3
0 −104
√
6c2
0(−9152256 + 3385633c1)−
208c0(−10917702 + c1(−360720 + 3319927c1))+
√
6(−718439040 + c1(2461047528+
c1(−1944898681 + 441207868c1)))]×
a6
1.
(22)
From the ﬁrst two factors, we get two two-parametric solutions c1 = 3/2 and
c1 = 6 + 2
√
6c0 or
b1 = 3a1/2,
a0 = (2b0 + b(3a1 −2b1))/3,
b =

2/3,
b1 = 6a1 + 2
√
6b0, a0 = (2b0 + b(3a1 −2b1))/3,
b =

2/3.
(23)
For solutions (23), we calculate the normal form of (21) till the 36th order and
obtain that for each solution it is a diagonal linear system.
The use of general roots of the polynomial, which is a cubic factor in (22),
is too complicated. There are some partial one-parametric solutions which yield
vanishing a13 at (10), (12), for instance:
b1 = 5a1/3,
b0 = −5a1/(12
√
6), a0 = (2b0 + b(3a1 −2b1))/3,
b =

2/3,
b1 = 8a1/3,
b0 = 8
√
6a1/9,
a0 = (2b0 + b(3a1 −2b1))/3,
b =

2/3.
We note that the ﬁrst solution from given above is a new solution which does
not intersect with (10) and (12), but we do not see here one-parameter solutions.
For each set of parameters (23), one can ﬁnd the Darboux integrating factor
μ = f a
1 · f d
2 · f c
3 (see e.g. [12,13]). In both cases, system (21) has invariant lines
f1 = r, f2 = w, f3 = 1 −

2/3w.

48
A.D. Bruno et al.
In the ﬁrst case (when
b1 = 3/2a1)
μ1 = rawdf c
3,
where
a = −2,
d = −13
6 ,
c = −4
3.
In the second case (when
b1 = 6a1 + 2
√
6b0)
μ2 = rawdf c
3,
where
a = 3a1 + 2
√
6b0
3a1 +
√
6b0
,
d = 8a1 + 5
√
6b0
6a1 + 2
√
6b0
,
c =
−a1
3a1 +
√
6b0
.
The corresponding ﬁrst integrals of (21) are
I1rw = w−7/6(1 −

2
3w)−1/3[−9a1 + 3
√
6b0 −42
r −6(
√
6a1 + 5b0)w+
2(9a1 + 4
√
6b0)w2 −21/6(9
√
2a1 + 8
√
3b0)w5/3(−
√
6 + 2w)1/3×
2F1(−1/2, 1/3; 1/2;

2/3/w)],
I2rw = r
3
3a1
3a1+
√
6b0 · w
7/3+
7b0
3
√
6a1+6b0 · (1 −

2/3w)
−a1
3a1+
√
6b0 ×
{ −6+2
√
6w
6a1+3
√
6b0 + r[3 + 2w(−
√
6 + w)]}.
(24)
In the original variables x, y of Eq. (6), these integrals up to multiplication
by a number have the form:
I1xy = (y/x2)(
√
6 + 2x3/y2)−7/6(x3/y2)2/3 · {42
√
6+
1/(xy3)[−36a1x6 −16
√
6b0x6 + 84x4y24
√
6a1x3y2−
36b0x3y2 + 21/3(x3/y2)1/3y2(
√
6 + (x3/y2)2/3×
(2(9a1 + 4
√
6b0)x3 + 3(3
√
6a1 + 8b0)y2)×
2F1(−1/2, 1/3; 1/2;
3y2
3y2+
√
6x3 )]},
I2xy = y(

2/3 + x3/y2)
−1/2+
a1
−6a1−2
√
6b0 (x2/y)
−
a1
3a1+
√
6b0 ×
{3 + (x2/y2)[
√
6x + 3(2a1 +
√
6b0)y]}.
(25)
In the case b = −

2/3, we obtain a similar formula.
6
Analytical Properties of the Integrals
We should check analyticity the obtained ﬁrst integrals of (25) near the origin
x = y = 0.
We note that by Theorem 4.13 of [10], if a system has a Darboux integrating
factor of the form
μ = f β1
1 f β2
2 (1 + h.o.t)β
then it has an analytic ﬁrst integral except of the case when both β1 and β2 are
integer numbers greater than 1. In both cases, the above orders a and b of the
integrating factor μ1,2 are not integer simultaneously in the generic case.

On New Integrals of the Algaba-Gamero-Garcia System
49
7
Conclusions
For a ﬁve-parameter non-Hamiltonian planar system (6), we have found for the
case b2 ̸= 2/3 four sets of two-parametric necessary conditions on parameters
under which the system is locally integrable near the degenerate point x = y = 0.
These sets of conditions are also suﬃcient for local and global integrability of
system (6). For the subcase b2 = 2/3 and 3a0−2b0 = b(3a1−2b1), we have found
two more ﬁrst integrals. For our search of additional ﬁrst integrals, we need to
calculate the condition A at the point with the resonance of the 27th order for
the subcase b2 = 2/3, 3a0 −2b0 ̸= b(3a1 −2b1) [9].
We have used Standard Lisp for the normal forms calculations. The inte-
grating factors and integrals were calculated using the computer algebra system
Mathematica.
Acknowledgements. Victor F. Edneral was supported by the grant NSh-7989.2016.2
of the President of Russian Federation and by the Ministry of Education and Science
of the Russian Federation (Agreement number 02 A03.21.0008), Valery G. Romanovski
was supported by the Slovenian Research Agency (research core funding No. P1-0306).
References
1. Algaba, A., Gamero, E., Garcia, C.: The integrability problem for a class of planar
systems. Nonlinearity 22, 395–420 (2009)
2. Bateman, H., Erdˆelyi, A.: Higher Transcendental Functions, vol. 1. McGraw-Hill
Book Company, New York (1953)
3. Bruno, A.D.: Analytical form of diﬀerential equations (I, II). Trudy Moskov. Mat.
Obsc. 25, 119–262 (1971), 26, 199–239 (1972) (Russian). Trans. Moscow Math.
Soc. 25, 131–288 (1971), 26, 199–239 (1972) (English)
4. Bruno, A.D.: Local Methods in Nonlinear Diﬀerential Equations. Nauka, Moscow
(1979) (Russian). Springer, Berlin (1989) (English)
5. Bruno, A.D.: Power Geometry in Algebraic and Diﬀerential Equations. Fizmatlit,
Moscow (1998) (Russian). Elsevier Science, Amsterdam (2000) (English)
6. Bruno, A.D., Edneral, V.F.: Algorithmic analysis of local integrability. Dokl. Akad
Nauk 424(3), 299–303 (2009) (Russian). Doklady Mathem. 79(1), 48–52 (2009)
(English)
7. Bruno, A.D., Edneral, V.F.: On integrability of a planar ODE system near a
degenerate stationary point. In: Gerdt, V.P., Mayr, E.W., Vorozhtsov, E.V. (eds.)
CASC 2009. LNCS, vol. 5743, pp. 45–53. Springer, Heidelberg (2009). doi:10.1007/
978-3-642-04103-7 4
8. Bruno, A.D., Edneral, V.F.: On integrability of a planar system of ODEs near a
degenerate stationary point. J. Math. Sci. 166(3), 326–333 (2010)
9. Bruno, A.D., Edneral, V.F.: On possibility of additional solutions of the degenerate
system near double degeneration at the special value of the parameter. In: Gerdt,
V.P., Koepf, W., Mayr, E.W., Vorozhtsov, E.V. (eds.) CASC 2013. LNCS, vol.
8136, pp. 75–87. Springer, Cham (2013). doi:10.1007/978-3-319-02297-0 6
10. Christopher, C., Mardeˇsi´c, P., Rousseau, C.: Normalizable, integrable, and lineariz-
able saddle points for complex quadratic systems in C2. J. Dyn. Control Syst. 9,
311–363 (2003)

50
A.D. Bruno et al.
11. Edneral, V.F.: An algorithm for construction of normal forms. In: Ganzha, V.G.,
Mayr, E.W., Vorozhtsov, E.V. (eds.) CASC 2007. LNCS, vol. 4770, pp. 134–142.
Springer, Heidelberg (2007). doi:10.1007/978-3-540-75187-8 10
12. Edneral, V., Romanovski, V.G.: On suﬃcient conditions for integrability of a planar
system of ODEs near a degenerate stationary point. In: Gerdt, V.P., Koepf, W.,
Mayr, E.W., Vorozhtsov, E.V. (eds.) CASC 2010. LNCS, vol. 6244, pp. 97–105.
Springer, Heidelberg (2010). doi:10.1007/978-3-642-15274-0 9
13. Romanovski, V.G., Shafer, D.S.: The Center and Cyclicity Problems: A Compu-
tational Algebra Approach. Birkh¨auser, Boston (2009)
14. http://theory.sinp.msu.ru/∼edneral/CASC2017/a13-26.txt
15. http://theory.sinp.msu.ru/∼edneral/CASC2017/a19.txt

Full Rank Representation of Real Algebraic Sets
and Applications
Changbo Chen1,2, Wenyuan Wu1,2(B), and Yong Feng1,2
1 Chongqing Key Laboratory of Automated Reasoning and Cognition,
Chongqing Institute of Green and Intelligent Technology,
Chinese Academy of Sciences, Chongqing, China
changbo.chen@hotmail.com, {wuwenyuan,yongfeng}@cigit.ac.cn
2 University of Chinese Academy of Sciences, Beijing, China
Abstract. We introduce the notion of the full rank representation of
a real algebraic set, which represents it as the projection of a union of
real algebraic manifolds VR(Fi) of Rm, m ≥n, such that the rank of the
Jacobian matrix of each Fi at any point of VR(Fi) is the same as the
number of polynomials in Fi.
By introducing an auxiliary variable, we show that a squarefree regu-
lar chain T can be transformed to a new regular chain C having various
nice properties, such as the Jacobian matrix of C attains full rank at any
point of VR(C). Based on a symbolic triangular decomposition approach
and a numerical critical point technique, we present a hybrid algorithm
to compute a full rank representation.
As an application, we show that such a representation allows to better
visualize plane and space curves with singularities. Eﬀectiveness of this
approach is also demonstrated by computing witness points of polyno-
mial systems having rank-deﬁcient Jacobian matrices.
1
Introduction
Numerical algebraic geometry [12,33], which solves problems arising from alge-
braic geometry by numerical computation techniques, has made great progress
in the past decades with the advent of homotopy continuation methods [24,34].
For well-posed problems, such algorithms can often provide good approximated
answers in a shorter time than methods based on symbolic computation. On the
other hand, diﬀerent from symbolic approaches, the accuracy of computation
with a geometric object may depend greatly on the algebraic representations.
Rank deﬁciency of the Jacobian matrix is a typical factor aﬀecting the
performance of numerical root ﬁnding procedures like Newton iteration. The
iteration process may not converge or converge only linearly to points not meet-
ing the full rank condition. To handle this problem, approaches such as deﬂa-
tion [20,22,23,26,28] have been proposed. In the particular case of real varieties,
algebraic representations such as being implicitly sum of squares of polynomi-
als bring another level of diﬃculty for numerical computation since a slight
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 51–65, 2017.
DOI: 10.1007/978-3-319-66320-3 5

52
C. Chen et al.
deformation may change the dimension. Such ill-posed problems might be han-
dled to some extent by computing polynomial SOS [29] and real radicals [3,19,25]
based on semideﬁnite programming.
Diﬀerent from the above approaches, to handle the singularities of a real
algebraic set V caused by self-intersections, non-radicalness, sum of squares and
others, we lift V to a higher dimensional space such that the variety V becomes
the union of projections of real algebraic manifolds M1, . . . , Ms. Each Mi is rep-
resented by a set of polynomials attaining full rank at any point of Mi. See
Sect. 2 for illustrative examples. Such a representation allows some operations
like existential quantiﬁer elimination [13], witness points computing [11,30,35],
curve tracing and border curve generating [7] to be accomplished by ﬁrstly apply-
ing them to a smooth geometric object with full rank algebraic representation,
which allows eﬃcient algorithms with good complexity [32], and next making
use of the projection operator, which costs nothing if the object is represented
by points.
In this paper, we provide a proof-of-concept algorithm, see Sect. 3, for com-
puting such a full rank representation based on a symbolic triangular decom-
position method [6] and a numeric critical point technique [35]. The full rank
condition is achieved by introducing an auxiliary variable to a squarefree regular
chain. The resulting new object is still a squarefree regular chain but has extra
nice properties, such as the ideal generated by it being the same as its saturated
ideal (importance of such regular chains are discussed in [1]). Such a representa-
tion is partly motivated by the work [5] of decomposing a semi-algebraic system
into regular semi-algebraic systems and the work [31] for computing witness
points via triangular decompositions. Related theoretical work on lifting plane
curves as projections of non singular space curves can be found in [4].
We provide two applications of our methods. The ﬁrst is to plot singular plane
and space curves [8,14–18], detailed in Sect. 4. The second is to compute witness
points of systems for which classical Lagrange multiplier method fails due to
rank-deﬁciency of Jacobian matrices, presented in Sect. 5. Eﬀectiveness of this
method is justiﬁed by examples in the two applications. In Sect. 6, we summarize
the main results of the paper and discuss some possible future research directions.
2
Full Rank Representation of Real Algebraic Sets
In this section, we introduce the notion of full rank representation of a real
algebraic set and provide several examples to illustrate the notion.
Deﬁnition 1. Let F ⊆R[x1, . . . , xn] and VR(F) be the zero set of F in Rn.
Let m ≥n and π : Rm →Rn be the projection function which sends a point
(x1, . . . , xn, xn+1, . . . , xm) in Rm to the point (x1, . . . , xn) in Rn. A full rank
representation of VR(F) is a sequence of polynomial sets Fi ⊆R[x1, . . . , xm],
i = 1, . . . , s, such that
(i) We have VR(F) = π(∪s
i=1VR(Fi)), where VR(Fi) is the zero set of Fi in Rm.
(ii) For any i, we have VR(Fi) ̸= ∅.

Full Rank Representation of Real Algebraic Sets and Applications
53
(iii) For any i, the Jacobian matrix JFi with respect to the variables of Fi
attains full rank |Fi|, where |Fi| is the number of elements of Fi, at any
point of VR(Fi).
Geometrically, a full rank representation represents a real algebraic set VR(F)
as the projection of a union of possibly several smooth manifolds. To make it
precise, we recall the regular level set theorem, see pages 113–114 of [21].
Deﬁnition 2. Let M and N be smooth manifolds. If F : M →N is a smooth
map, a point p ∈M is said to be a regular point of F if the induced map between
tangent spaces F∗: TpM →TF (p)N is surjective; it is called a critical point
otherwise. A point c ∈N is said to be a regular value of F if every point of
the level set F −1(c) is a regular point, and a critical value otherwise. A level set
F −1(c) is called a regular level set if c is a regular value.
Theorem 1. Every regular level set of a smooth map is a closed embedded sub-
manifold whose codimension is equal to the dimension of the range.
Proposition 1. Let G ⊆R[x1, . . . , xm] and k be the number of elements in G.
Assume that VR(G) ̸= ∅and at any point of VR(G), the Jacobian matrix JG has
full rank k. Then VR(G) is a smooth submanifold of Rm with codimension k.
Proof. Consider the map G : Rm →Rk. Since at any point of VR(G), the Jaco-
bian matrix JG has full rank k, we know that any point of VR(G) is a regular
point of the map G by Deﬁnition 2. Thus VR(G) = G−1(0) is a smooth subman-
ifold of Rm with codimension k by Theorem 1.
Example 1. Consider the real algebraic set deﬁned by the polynomial
f := z4 + 2 z2y2 + 2 z2x2 + 2 y4 + 4 y2x2 + 2 x4 −4 z2 −6 y2 −6 x2 + 5.
Then G := {g1, g2}, where g1 := x2 + y2 −1, g2 := x2 + y2 + z2 −2, is a full rank
representation of VR(f). Indeed, we have f = g2
1 + g2
2. The Jacobian matrix Jf
has rank 0 at any point of VR(f) whereas JG has rank 2 at any point of VR(G).
An alternative full rank representation of f is as below:
⎧
⎨
⎩
y2 + x2 −1,
z + 1,
wy −1
⎫
⎬
⎭,
⎧
⎨
⎩
y2 + x2 −1,
z −1,
wy −1
⎫
⎬
⎭,
⎧
⎨
⎩
x −1,
y,
z + 1
⎫
⎬
⎭,
⎧
⎨
⎩
x + 1,
y,
z + 1
⎫
⎬
⎭,
⎧
⎨
⎩
x −1,
y,
z −1
⎫
⎬
⎭,
⎧
⎨
⎩
x + 1,
y,
z −1
⎫
⎬
⎭.
Example 2. Consider the Motzkin polynomial M := x4y2+x2y4−3 y2x2+1. It
is known that M can not be written as sum of squares of polynomials in R[x, y].
A full rank representation of it is: {x−1, y−1}, {x−1, y+1}, {x+1, y+1}, {x+
1, y −1}.
Example 3. Consider the lemniscate of Gerono deﬁned by the polynomial y2 +
x4 −x2. It has a singular point (0, 0). A full rank representation of it is:
x4 −x2 + y2,
zy −1

,
⎧
⎨
⎩
x −1,
y,
z −1
⎫
⎬
⎭,
⎧
⎨
⎩
x,
y,
z −1
⎫
⎬
⎭,
⎧
⎨
⎩
x + 1,
y,
z −1
⎫
⎬
⎭.

54
C. Chen et al.
The zero sets of the four polynomial systems are the union of four smooth curves
and three points in R3, as illustrated in Fig. 2 (one red point is hidden), whose
projection in (x, y) space is exactly the lemniscate in Fig. 1.
Fig. 1. Lemniscate of Gerono.
Fig. 2. A lifting of the lemniscate.
(Color ﬁgure online)
3
Compute Full Rank Representation
In this section, we show that after introducing an auxiliary variable, a squarefree
regular chain can be transformed to a new one having various nice properties,
based on which we present a hybrid algorithm to compute a full rank represen-
tation of a given real algebraic set.
Throughout this section, let k be a ﬁeld of characteristic 0 and K be the
algebraic closure of k. We refer to [2,6] for basic notions on regular chains.
Lemma 1 [2].
Let T be a regular chain in k[x1, . . . , xn]. Let init(T) be the
squarefree part of the product of the initials of polynomials in T. Let sat(T) :=
⟨T⟩: init(T)∞. Let W(T) be the quasi-component of T, that is W(T) = V (T) \
V (init(T)). Then V (sat(T)) = W(T) holds. Moreover, sat(T) is an unmixed
ideal with dimension n −|T|.
Deﬁnition 3 ([10], page 31).
Let V ⊆Kn be an equidimensional variety of
dimension d. Let F ⊆k[x1, . . . , xn] be a set of generators of its deﬁning ideal
I(V). V is nonsingular at a point p ∈V if the rank of the Jacobian matrix of F
at p is n −d. V is nonsingular if it is nonsingular at every point.
Lemma 2. Let F = {f1, . . . , fk} ⊆C[x1, . . . , xn]. Assume that at any point p of
VC(F), the Jacobian matrix JF has rank k. Then VC(F) is a complex submanifold
of Cn with codimension k.
Proof. It follows directly from the deﬁnition of complex submanifold (see Deﬁ-
nition 2.8 in [27]).

Full Rank Representation of Real Algebraic Sets and Applications
55
Theorem 2 (Jacobian Criterion [9], page 402). Let R := k[x1, . . . , xn] be
a polynomial ring. Let I := ⟨F⟩⊆k[x1, . . . , xn] be an ideal. Let p be a prime
ideal of R containing I. Let c be the codimension of Ip in Rp. Then:
– The Jacobian matrix JF modulo p has rank ≤c.
– (R/I)p is a regular local ring iﬀthe matrix J modulo p has rank = c.
Lemma 3. Let F = {f1, . . . , fk} ⊆C[x1, . . . , xn]. Assume that at any point p
of VC(F), the Jacobian matrix JF has rank k. Then the ideal generated by F is
an equidimensional radical ideal of codimension k.
Proof. By Lemma 2, the ideal ⟨F⟩is an equidimensional ideal of codimension k.
Next we show it is radical (the argument below is similar to those used in the
proof of Lemma 7 in [13]). Let R := C[x1, . . . , xn] and I := ⟨F⟩. Let p be an
associated prime ideal of I. Since ⟨F⟩is an equidimensional ideal of codimension
k, the codimension of Ip in Rp is also k no matter whether p is an isolated or
embedded prime. By Theorem 2, (R/I)p is a regular local ring, which implies
that Ip is prime. On the other hand, Ip = (∩Qi)p, where Qi are components of
a minimal primary decomposition of I such that √Qi = p. Thus ∩Qi is prime,
which implies that I is radical.
Theorem 3. Let T = {t1, . . . , tk} be a squarefree regular chain of Q[x1 < · · · <
xn]. Let d = n −k. Let init(T) be the squarefree part of the product of the
initials of polynomials in T and sep(T) be the squarefree part of the product
of the separants of polynomials in T. Set h to be the least common multiple of
init(T) and sep(T). We introduce a new variable w and let C := {T, hw −1}.
Then we have
(i) C is a squarefree regular chain of Q[x1 < · · · < xn < w].
(ii) At any point p of VC(C), the rank of the Jacobian matrix of C is k + 1.
(iii) The ideal ⟨C⟩is radical and C = sat(C) holds.
(iv) VC(C) is a nonsingular variety and a complex submanifold of Cn+1 with
dimension d.
(v) Assume VR(C) ̸= ∅holds. Then VR(C) is a smooth manifold of dimen-
sion d.
Proof. Note that sep(hw −1) = h. Since T is a squarefree regular chain and h
is regular modulo sat(T), we deduce (i).
To prove (ii), let f = hw −1, note that we have
JC :=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
∂t1
∂x1 · · · ∂t1
∂xd
∂t1
∂xd+1
∂t2
∂x1 · · · ∂t2
∂xd
∂t2
∂xd+1
∂t2
∂xd+2
· · ·
· · ·
∂tk
∂x1 · · · ∂tk
∂xd
∂tk
∂xd+1
∂tk
∂xd+2 · · · ∂tk
∂xn
∂f
∂x1 · · ·
∂f
∂xd
∂f
∂xd+1
∂f
∂xd+2 · · ·
∂f
∂xn
∂f
∂w
⎞
⎟
⎟
⎟
⎟
⎟
⎠
.
Consider the minor being the product of
∂t1
∂xd+1 · · · ∂tk
∂xn
∂f
∂w, which is the poly-
nomial sep(t1) · · · sep(tk)h. Thus the minor is nonzero at any point of VC(C).
Therefore the rank of the Jacobian matrix of C is k + 1 at any point of VC(C).

56
C. Chen et al.
Next we prove (iii). By Lemma 3, the ideal ⟨C⟩is radical. Since W(C) ⊆
V (C) ⊆W(C) holds, we have V (C) = W(C) = V (sat(C)) by Lemma 1. Since
both ⟨C⟩and sat(C) are radical, we have ⟨C⟩=

⟨C⟩=

sat(C) = sat(C).
Thus (iii) is proved.
Property (iv) follows directly from (ii), Deﬁnition 3 and Lemma 2 while
Property (v) follows directly from (ii) and Proposition 1.
Remark 1. In Theorem 3, if we replace h by the border polynomial of T, then
the conclusion still holds.
Next we present a hybrid algorithm to compute the full rank representation
of a real variety. The algorithm relies on the following subroutines:
– Triangularize(F) [6]: given a set of polynomials F, it returns a set of squarefree
regular chains T1, . . . , Ts such that V (F) = ∪s
i=1W(Ti).
– Intersect(p, T) [6]: given a polynomial p and a squarefree regular chain T, it
returns a set of squarefee regular chains T1, . . . , Ts such that V (p) ∩W(T) ⊆
∪s
i=1W(Ti) ⊆V (p) ∩W(T).
– WitnessPoints(F) [35]: given a set of polynomials F whose Jacobian matrix
has full rank at any regular point of VR(F), it numerically computes a ﬁnite
set W ⊆VR(F) such that W meets every connected component of VR(F).
Proposition 2. Algorithm 1 terminates and correctly computes a full rank rep-
resentation of VR(F).
Proof. In the ﬁrst for loop of Algorithm 1, each polynomial h ∈H is regu-
lar modulo sat(T), which implies that for any C ∈Intersect(h, T), we have
dim(C) < dim(T). So the algorithm terminates. Its correctness follows directly
from Theorem 3.
Remark 2. In Algorithm 1, if we replace H by the border polynomial set [5]
of T, then the witness points of G (same as that of [G, H]) can be computed
symbolically by combining open cylindrical algebraic decomposition and real root
isolation of zero-dimensional regular chains (see [5]). However, this approach
tends to produce polynomials of larger sizes due to computations of iterated resul-
tants, and thus makes the full rank representation less suitable as the input of
numerical algorithms.
Next we illustrate the main steps of Algorithms 1 by an example.
Example 4. Consider the following polynomial system
F :=

x22x1 −x32x1 + x12 + 2 x2 x1 + 2 x22 + 2 x3 x4 + x42 −x1 −1,
−x32x1 + x22x1 + x42 + 2 x3 x4 + 2 x32 + 2 x2 x1 + x12 −x1 + 1

.
1. The command Triangularize returns a single squarefree regular chain:
T :=
x42 + 2 x3 x4 + 2 x22 + 2 x2 x1 + x12 −1
x32 −x22 + 1

Full Rank Representation of Real Algebraic Sets and Applications
57
Algorithm 1. FullRankDecompose(F)
Input: A polynomial system F ⊆Q[x1 < · · · < xn].
Output: A full rank representation of VR(F) (with witness points information).
begin
introduce a new variable w; set S1 := Triangularize(F), S2 := ∅and S3 := ∅;
while S1 ̸= ∅do
choose and remove a regular chain T from S1;
let H1 (resp. H2) be the set of nonconstant irreducible factors of the
initials (resp. separants) of polynomials in T;
H := H1 ∪H2;
S2 := S2 ∪{[T, H]};
for h ∈H2 \ H1 do
S1 := S1 ∪Intersect(h, T);
for [T, H] ∈S2 do
let h be the product of polynomial in H; set g := hw −1; G := T ∪{g};
let S := WitnessPoints(G);
if S ̸= ∅then
S3 := S3 ∪{[G, S]}
return the sequence of elements in S3
2. By computing the initials and separants of polynomials in T, we have H1 = ∅
and H = H2 = {x4 + x3, x3}.
3. Calling Intersect(x4 + x3, T) returns T1 := {x2 + x1, x2
3 −x2
1 + 1, x4 + x3}
while calling Intersect(x3, T) returns T2 := {x2 −1, x3, x2
4 + x2
1 + 2x1 + 1} and
T3 := {x2 + 1, x3, x2
4 + x2
1 −2x1 + 1}.
4. In the next iterations of the while loop, we call successively Intersect(x3, T1),
Intersect(x4, T2) and Intersect(x4, T3) and obtain the following two regular
chains (by removing duplicated ones): T4 := {x1 + 1, x2 −1, x3, x4} and
T5 := {x1 −1, x2 + 1, x3, x4}.
5. We have S2 := {[T, H], [T1, {x3}], [T2, {x4}], [T3, {x4}], [T4, ∅], [T5, ∅]} after
the while loop terminates.
6. Consider the ﬁrst iteration of the second for loop. We have h = (x3 +x4)(x3),
g = hw −1 and G = T ∪{g}. Calling WitnessPoints(G) returns ∅.
7. When the for loop terminates, we have the ﬁnal full rank representation, built
on top of the regular chains T1, T4 and T5:
[[[x_2+x_1, x_3^2-x_1^2+1, x_4+x_3, _w*x_3-1],
[[x_1 = -1.356357868, x_2 = 1.356357868, x_3 = .9163550989,
x_4 = -.9163550989, _w = 1.091280008],
[x_1 = 1.356357868, x_2 = -1.356357868, x_3 = -.9163550989,
x_4 = .9163550989, _w = -1.091280008]]],
[[x_1-1, x_2+1, x_3, x_4, _w-1],
[[x_1 = 1.000000000, x_2 = -1.000000000, x_3 = 0., x_4 = 0., _w = 1.000000000]]],
[[x_1+1, x_2-1, x_3, x_4, _w-1],
[[x_1 = -1.000000000, x_2 = 1.000000000, x_3 = 0., x_4 = 0., _w = 1.000000000]]]]

58
C. Chen et al.
4
Applications on Plotting Singular Plane and Space
Curves
In this section, we present applications of the full rank representation on visual-
izing singular plane and space curves.
Algorithm PlotSingularCurve
Input: a plane or space curve deﬁned by F(x1, . . . , xr) = 0 and a box B of
Rr, where r = 2, 3.
Output: the plotting of the curve inside the box B.
Steps:
1. Let S := FullRankDecompose(F).
2. For each (G, W) of S, let W ′ be the union of points of W inside B and
the intersection points of VR(G) with the ﬁbers of the boundaries of B.
3. For each (G, W ′), trace the smooth curve VR(G) by a prediction- projec-
tion method using points in W ′.
4. Plot the projection of the traced points in B.
Paper [18] presents a list of challenges for real algebraic plane curve visual-
ization software. Among them, we choose two irreducible algebraic curves which
softwares like Maple have diﬃculties to correctly visualize near singular points.
The ﬁrst example is deﬁned in Challenge 15 of [18].
˜Y −−
r
:= −(x2 −y2)2 + axr.
Here we choose a = 7/6, r = 8. A full rank representation of it computed by
Algorithm 1 is as below, where some of the witness points are omitted for brevity.
To have better numerical stability when tracing the curve, we have rescaled the
coeﬃcients of the polynomials, which is a typical technique for dealing with
machine epsilon, and increased the degree of the auxiliary variable w, which
makes w converge faster to inﬁnity when its coeﬃcient approaches to zero.
[[[5.000000*10^6*y^4-1.0000000*10^7*x^2*y^2-5.833333333*10^6*x^8+5.000000*10^6*x^4,
(1000.*x^2*y-1000.*y^3)*_w^3-1],
[[x = -.8361036479, y = -.4137816431, _w = -.1660497798],
[x = .8361036479, y = .4137816431, _w = .1660497798],
[x = .9575968845, y = 0.9351001503e-1, _w = .2274988437],
[x = .4174607241, y = .3761233446, _w = .4327575059],
...
...
... ]],
[[10000.*x^4-8571.428571, 10.*y, 1.*_w-1],
[[x = -.9621954582, y = 0., _w = 1.000000000],
[x = .9621954582, y = 0., _w = 1.000000000]]],
[[10.*x, 10.*y, 1.*_w-1], [[x = 0., y = 0., _w = 1.000000000]]]]
Figure 3 shows the plotting of the curve by the command plots:- implicitplot
in Maple, where the curve is not completely plotted near the singular point (0, 0).
Figure 4 illustrates a correct visualization by PlotSingularCurve.
A second example is deﬁned in Challenge 17. The polynomial is:
SAk,ℓ:= (y −1 −xk)ℓ(y −xk)ℓ+ (y −1)kℓ+1ykℓ.

Full Rank Representation of Real Algebraic Sets and Applications
59
Fig. 3. By plots:- implicitplot in Maple.
Fig. 4. By PlotSingularCurve.
Here we choose k = ℓ= 2. A full rank representation of it computed by Algo-
rithm 1 is as below, where some of the witness points are omitted for brevity.
Again, we have rescaled the coeﬃcients of the polynomials and increased the
degree of the auxiliary variable w in the representation.
[[[1.000000*10^6*y^9+1.000000*10^6*x^8-5.000000*10^6*y^8-4.000000*10^6*y*x^6
+1.0000000*10^7*y^7+2.000000*10^6*x^6+6.000000*10^6*y^2*x^4-1.0000000*10^7*y^6
-6.000000*10^6*y*x^4-4.000000*10^6*y^3*x^2+5.000000*10^6*y^5+1.000000*10^6*x^4
+6.000000*10^6*y^2*x^2-2.000000*10^6*y*x^2-2.000000*10^6*y^3+1.000000*10^6*y^2,
(-1.285714286*10^6*y^8+5.714285714*10^6*y^7-1.0000000*10^7*y^6
+5.714285714*10^5*x^6+8.571428571*10^6*y^5-1.714285714*10^6*y*x^4
-3.571428571*10^6*y^4+1.714285714*10^6*y^2*x^2+8.571428571*10^5*x^4
-1.714285714*10^6*y*x^2+8.571428571*10^5*y^2+2.857142857*10^5*x^2
-2.857142857*10^5*y)*_w^3-1],
[[x = -.3683735588, y = .1510941675, _w = -0.6572659595e-1],
[x = .7826945944, y = -.8765869978, _w = -0.4645572677e-2],
... ... ... ]],
[[10.*x-10., 10.*y-10., 1.*_w-1],
[[x = 1.000000000, y = 1.000000000, _w = 1.000000000]]],
[[10.*x, 10.*y-10., 1.*_w-1], [[x = 0., y = 1.000000000, _w = 1.000000000]]],
[[10.*x, 10.*y, 1.*_w-1], [[x = 0., y = 0., _w = 1.000000000]]],
[[10.*x+10., 10.*y-10., 1.*_w-1],
[[x = -1.000000000, y = 1.000000000, _w = 1.000000000]]]]
Figure 5 shows the plotting of the curve by the command plots:-implicitplot
in Maple, where the curve is not completely plotted near the singular point (0, 0).
The solitary point (0, 1) is also missing. Figure 6 illustrates a correct visualization
by PlotSingularCurve.
Next we present an application on plotting a space curve. The example comes
from [8,16]. The curve is the zero set of the following polynomial system F.
F := {(x −y + z)2 + y2 −2(x −y + z), ((x −y + z)2 + y2 + z2)2 −4((x −y + z)2 + y2)}.
A visualization of VR(F) by PlotSingularCurve is shown in Fig. 7.

60
C. Chen et al.
Fig. 5. By plots:- implicitplot in Maple.
Fig. 6. By PlotSingularCurve.
Fig. 7. A visualization of VR(F) by PlotSingularCurve with green points being self inter-
section points. (Color ﬁgure online)
5
Experimentation
In this section, we report on the experimental results of a preliminary implemen-
tation of our algorithm in Maple. The test examples are created from random
polynomials with some transformation such that the Jacobian matrix is rank
deﬁcient and polynomial SOS methods do not directly apply. We compare with
the RealTriangularize command [5] in the RegularChains library of Maple,
which can also compute real witness points and real dimensions of polynomial
systems with rank-deﬁcient Jacobian matrices.

Full Rank Representation of Real Algebraic Sets and Applications
61
Example 5. Consider a set of polynomials G := {g1, g2, g3, g4, g5}, where g1 :=
−9 x4−4 x3+10 x1+1, g2 := −8+4 x3+10 x5, g3 := −7 x5 x2−5 x2−8 x5+1, g4 :=
7 x4 x3 −10 x22 +1, g5 := −6 x5 x2 −3 x42 +1. Let M be a 2×2 invertible matrix
M :=
 1
x1
x2 x1x2 −1
	
.
Set (f1, f2)T := M(g2
1 + g2
2, g3)T and F := {f1, f2, g4, g5}. That is
F =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
−7 x5 x2 x1 + 100 x5
2 + 80 x5 x3 −8 x5 x1 + 81 x4
2 + 72 x4 x3
−180 x4 x1 + 32 x3
2 −80 x3 x1 −5 x2 x1 + 100 x1
2
−160 x5 −18 x4 −72 x3 + 21 x1 + 65,
−7 x5 x2
2x1 + 100 x5
2x2 + 80 x5 x3 x2 −8 x5 x2 x1 + 81 x4
2x2
+ 72 x4 x3 x2 −180 x4 x2 x1 + 32 x3
2x2 −80 x3 x2 x1
−5 x2
2x1 + 100 x2 x1
2 −153 x5 x2 −18 x4 x2 −72 x3 x2
+ 21 x2 x1 + 8 x5 + 70 x2 −1,
7 x4 x3 −10 x2
2 + 1,
−6 x5 x2 −3 x4
2 + 1
⎫
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎭
.
A full rank representation of VR(F) (after normalizing the coeﬃcients) is
[[[0.4183612727e-1*x1^7-.2285796501*x1^6+.5030640417*x1^5-.3897793981*x1^4-.4559214459*x1^3
+1.000000000*x1^2-.6441493178*x1+.1609043497,
.1529373720*x2-0.5369788910e-1*x1^6+.2699715843*x1^5-.5451541805*x1^4+.2573550740*x1^3
+.7684914458*x1^2-1.000000000*x1+.3914206422,
0.6037479144e-1*x3-0.6628199824e-1*x1^6+.3082918410*x1^5-.5584479577*x1^4+.2069073607*x1^3
+.8725734034*x1^2-1.000000000*x1+.2543012124,
.1556812072*x4+0.7596151567e-1*x1^6-.3533133600*x1^5+.6400011227*x1^4-.2371231577*x1^3
-1.000000000*x1^2+.9730562705*x1-.3087361007,
.1509369786*x5+0.6628199820e-1*x1^6-.3082918408*x1^5+.5584479574*x1^4-.2069073606*x1^3
-.8725734030*x1^2+.9999999997*x1-.3750507950, 1.0*_w-1],
[[x1 = -1.234303648, x2 = -.5486535450, x3 = -.2498708024, x4 = -1.149283697,
x5 = .8999483232, _w = 1.000000000],
[x1 = 2.176655446, x2 = 1.618025934, x3 = 2.917166233, x4 = 1.233098803,
x5 = -.3668665021, _w = 1.000000000],
[x1 = .8850340374, x2 = -.4777062337, x3 = .1805787394, x4 = 1.014225043,
x5 = .7277685022, _w = 1.000000000]]]]
Let F ∗be the polynomial system in the output. It is a square system consist-
ing of 6 polynomials and deﬁnes a zero-dimensional real variety. The smallest
singular values of JF ∗at the three zeros of VR(F ∗) are respectively:
0.0454144731863706450, 0.0351114360453165381, 0.0176834495031580330.
Example 6. Consider the polynomial system F:
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
64 x3
4 −16 x1 x3
3 + 50 x3
2x1
2 −3 x3
2x5 −3 x4
3 + 42 x3
2x1 −126 x3 x2 x1
−2 x2
3 + 41 x3
2 −54 x3 x2 −18 x3 x1 + 81 x2
2 −6 x3 + 18 x2 + 6,
64 x3
4x1 −16 x3
3x1
2 + 50 x3
2x1
3 −3 x5 x3
2x1
−3 x4
3x1 + 64 x3
4 −16 x1 x3
3 + 92 x3
2x1
2 −126 x3 x2 x1
2
−2 x2
3x1 + 3 x3
2x5 + 3 x4
3 + 83 x3
2x1 −180 x3 x2 x1
−18 x3 x1
2 + 2 x2
3 + 81 x2
2x1 + 41 x3
2 −54 x3 x2 −24 x3 x1
+ 81 x2
2 + 18 x2 x1 −6 x3 + 18 x2 + 6 x1 + 4
⎫
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎭
.
Set the variable order to be x1 < x2 < x3 < x4 < x5. A full rank representation
of VR(F) computed by Algorithm 1 consists of 3 components, with dimensions

62
C. Chen et al.
being respectively 2, 1, 1, numbers of polynomials being respectively 4, 5, 5, and
total degrees of polynomials in them being respectively
(3, 2, 5, 5), (1, 1, 1, 3, 1), (1, 1, 1, 3, 1).
The number of computed witness points are respectively 9, 1, 3. The maximum
residuals after substituting the witness points into the three components are
respectively 5.34 × 10−7, 5. × 10−10, 6. × 10−10. The smallest singular values of
the Jacobian matrices evaluated at the witness points are respectively
0.217390601993606e −1, 0.217565704741585e −1, 3.32650846923473,
1.32160172547595, 0.184438528285523e −1, 0.184312217006799e −1,
.472264278357534, 1.13453515931668, 1.14395048107664, .125000000000000,
0.340696090687040e −1, 0.340696090687040e −1, .125000000000000.
Example 7. Consider the following polynomial system
F :=
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
3 x4 x1
3 + 100 x2
4 −119 x3
2x2
2 −6 x4
2x3 x2 + 36 x3
4 + 9 x4
4 + 3 x4 x1
2
−40 x2
2x1 −16 x3 x2 x1 + 24 x3
2x1 + 48 x4
2x1 + 71 x1
2
−7 x2 x1 −20 x2
2 + 2 x3 x2 + 12 x3
2 −6 x4
2 −8 x1 −7 x2 + 3,
9 x4
4 −6 x4
2x3 x2 + 3 x4 x1
3 + 36 x3
4 −119 x3
2x2
2 + 100 x2
4 + 48 x4
2x1
−3 x4 x1
2 + 24 x3
2x1 −16 x3 x2 x1 −40 x2
2x1 −6 x4
2 + 12 x3
2
+ 2 x3 x2 −20 x2
2 −7 x2 x1 + 71 x1
2 + 7 x2 −14 x1 + 1
⎫
⎪
⎪
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎪
⎪
⎭
.
Set the variable order as x1
<
x2
<
x3
<
x4. A full rank represen-
tation of VR(F) computed by Algorithm 1 consists of 3 components, with
dimensions being respectively 1, 0, 0, numbers of polynomials being respec-
tively 4, 5, 5, and total degrees of polynomials in them being respectively
(12, 19, 3, 35), (8, 9, 9, 7, 1), (20, 19, 19, 19, 1). The number of computed witness
points are respectively 4, 4, 4. The maximum residuals after substituting the wit-
ness points into the three components are respectively 0.1419818555e −4, 3.09 ×
10(−8), 0.2263585e −5. The smallest singular values of the Jacobian matrices
evaluated at the witness points are respectively
1.01136778729643, 1.54387370543690, 1.51495924713963, 1.52421428042282,
0.778955506429449e −1, .660455232353615, .317809084873550, .105611642184416,
0.000221843023788232e −3, 0.615594132791045e −3, 0.165300475257133e −3,
0.596754059706055e −3.
Example 8. Consider the following system F:
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
x1
3 −16 x3 x1
2 + 100 x2
2x1 + 160 x4 x2 x1 + 64 x3
2x1 + 64 x4
2x1 −14 x1
2
+ 20 x2 x1 + 112 x3 x1 + 16 x4 x1 −4 x3 x5 −3 x4 x5 + 50 x1 −8 x3 + 1,
x2 x1
3 −16 x3 x2 x1
2 + 100 x2
3x1 + 160 x4 x2
2x1 + 64 x3
2x2 x1
+ 64 x4
2x2 x1 −14 x2 x1
2 + 20 x2
2x1 + 112 x3 x2 x1 + 16 x4 x2 x1
−4 x2 x3 x5 −3 x2 x4 x5 −x1
2 + 50 x2 x1 + 16 x3 x1 −100 x2
2 −8 x3 x2
−160 x4 x2 −64 x3
2 −64 x4
2 + 14 x1 −19 x2 −112 x3 −16 x4 −50,
−6 x4 x1 + 2 x2 −7 x5 + 1,
−2 x6 x1 −4 x4 x3 + 3 x5 + 1
⎫
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎭
.

Full Rank Representation of Real Algebraic Sets and Applications
63
Set the variable order to be x1 < · · · < x6. A full rank representation of VR(F)
computed by Algorithm 1 consists of 6 components, with dimensions being respec-
tively 1, 1, 0, 0, 0, 0, numbers of polynomials being respectively 6, 6, 7, 7, 7, 7, and
total degrees of polynomials in them being respectively
(4, 2, 3, 2, 2, 9), (3, 13, 3, 2, 2, 19), (5, 4, 1, 4, 4, 4, 1),
(1, 1, 1, 1, 1, 1, 1), (5, 4, 1, 4, 4, 4, 1), (4, 3, 1, 3, 2, 3, 1).
The number of computed witness points are respectively 2, 3, 3, 1, 1, 2. The max-
imum residuals after substituting the witness points into the six components are
respectively 0.67701e −4, 3.8 × 10−8, 1.17 × 10−8, 0, 2.8 × 10−9, 4. × 10−10. The
smallest singular values of the Jacobian matrices of polynomial systems in the
output evaluated at the witness points are respectively
0.243258982897929e −5, 0.243028372806473e −5, .129125424091993,
.678574657147939, 0.295982421600441e −1, 0.735206574833336e −3,
0.712398160744306e −3, 0.467778814287120e −2, 0.571345331300000e −2,
0.457875824937666e −4, 0.301415284267564e −2, 0.306478932392468e −2.
In all four examples, the smallest singular values are far from machine epsilon,
which indicates that the Jacobian matrices have full (numerical) rank. Table 1
summarizes an experimental comparison with the RealTriangularize command.
The column in-deg and out-deg denote respectively the maximum total degrees
of polynomials appearing in the input and output of FullRankDecompose. The
experimentation was conducted on an Ubuntu Laptop (Intel i7-4700MQ CPU
@ 2.40GHz, 8.0GB memory). The memory usage was restricted to 60% of the
total memory. It is interesting to notice that FullRankDecompose outperforms
RealTriangularize for systems having larger degrees.
Table 1. Summary of experimentation results on four examples.
Sys
in-deg
out-deg
Complex-dim
Real-dim
RealTriangularize
FullRankDecompose
Ex 5
4
7
1
0
> 1800(s)
275(s)
Ex 6
3
5
3
2
1(s)
2(s)
Ex 7
4
35
2
1
> 1800(s)
92(s)
Ex 8
4
19
2
1
> 1800(s)
320(s)
6
Conclusion and Future Work
In this paper, we introduced the notion of the full rank representation of a real
algebraic set to remove various singularities, which often lead to ill-posedness
in numerical computation. A proof-of-concept hybrid algorithm was proposed
to compute it. Such a representation was successfully applied to visualize some
plane and space curves with singularities. Eﬀectiveness of the hybrid algorithm
was also illustrated by computing full rank representations of some nontrivial
examples with rank-deﬁcient Jacobian matrices.

64
C. Chen et al.
Nevertheless, we notice that the current algorithm computes more than nec-
essary (like in a triangular shape) for a full rank representation. How to reduce
the size (including degrees and coeﬃcients) of polynomials in the representation
remains a challenge, which is crucial for the stability of numerical procedures
taking such representation as input. In a future work, we will also investigate the
possibility of combining with other numerical methods, such as sum of squares
based on semideﬁnite programming.
Acknowledgements. The authors would like to thank Hoon Hong and anonymous
reviewers for their helpful comments. This work is partially supported by the projects
NSFC (11471307, 11671377, 61572024), cstc2015jcyjys40001, and the Key Research
Program of Frontier Sciences of CAS (QYZDB-SSW-SYS026).
References
1. Alvandi, P., Chen, C., Hashemi, A., Maza, M.M.: Regular chains under linear
changes of coordinates and applications. In: Gerdt, V.P., Koepf, W., Seiler, W.M.,
Vorozhtsov, E.V. (eds.) CASC 2015. LNCS, vol. 9301, pp. 30–44. Springer, Cham
(2015). doi:10.1007/978-3-319-24021-3 3
2. Aubry, P., Lazard, D., Moreno Maza, M.: On the theories of triangular sets. J.
Symb. Comput. 28(1–2), 105–124 (1999)
3. Brake, D., Hauenstein, J., Liddell, A.: Validating the completeness of the real
solution set of a system of polynomial equations. ISSAC 2016, 143–150 (2016)
4. Caire, L.: Plane curves as projections of non singular space curves. Manuscripta
Math. 67(1), 433–450 (1990)
5. Chen, C., Davenport, J., May, J., Moreno Maza, M., Xia, B., Xiao, R.: Triangular
decomposition of semi-algebraic systems. J. Symb. Comput. 49, 3–26 (2013)
6. Chen, C., Moreno Maza, M.: Algorithms for computing triangular decomposition
of polynomial systems. J. Symb. Comput. 47(6), 610–642 (2012)
7. Chen, C., Wu, W.: A numerical method for computing border curves of bi-
parametric real polynomial systems and applications. In: Gerdt, V.P., Koepf, W.,
Seiler, W.M., Vorozhtsov, E.V. (eds.) CASC 2016. LNCS, vol. 9890, pp. 156–171.
Springer, Cham (2016). doi:10.1007/978-3-319-45641-6 11
8. Daouda, D., Mourrain, B., Ruatta, O.: On the computation of the topology of a
non-reduced implicit space curve. ISSAC 2008, 47–54 (2008)
9. Eisenbud, D.: Commutative Algebra: With a View Toward Algebraic Geometry.
Graduate Texts in Mathematics, vol. 150. Springer, Heidelberg (2013). doi:10.1007/
978-1-4612-5350-1
10. Hartshorne, R.: Algebraic Geometry. Graduate Texts in Mathematics, vol. 52.
Springer, Heidelberg (1997). doi:10.1007/978-1-4757-3849-0
11. Hauenstein, J.D.: Numerically computing real points on algebraic sets. Acta Appl.
Math. 125(1), 105–119 (2012)
12. Hauenstein, J., Sommese, A.: What is numerical algebraic geometry. J. Symb.
Comp. 79, 499–507 (2017). Part 3
13. Hong, H., El Din, M.S.: Variant quantiﬁer elimination. J. Symb. Comp. 47(7),
883–901 (2012)
14. Hong, H.: An eﬃcient method for analyzing the topology of plane real algebraic
curves. Math. Comput. Simul. 42(4), 571–582 (1996)

Full Rank Representation of Real Algebraic Sets and Applications
65
15. Imbach, R., Moroz, G., Pouget, M.: Numeric and certiﬁed isolation of the singu-
larities of the projection of a smooth space curve. MACIS 2015, 78–92 (2016)
16. Jin, K., Cheng, J.: Isotopic epsilon-meshing of real algebraic space curves. SNC
2014, 118–127 (2014)
17. Jin, K., Cheng, J.-S., Gao, X.-S.: On the topology and visualization of plane alge-
braic curves. In: Gerdt, V.P., Koepf, W., Seiler, W.M., Vorozhtsov, E.V. (eds.)
CASC 2015. LNCS, vol. 9301, pp. 245–259. Springer, Cham (2015). doi:10.1007/
978-3-319-24021-3 19
18. Labs, O.: A list of challenges for real algebraic plane curve visualization software.
In: Emiris, I.Z., Sottile, F., Theobald, T. (eds.) Nonlinear Computational Geome-
try, pp. 137–164. Springer, New York (2010)
19. Lasserre, J., Laurent, M., Rostalski, P.: Semideﬁnite characterization and compu-
tation of zero-dimensional real radical ideals. Found. Comput. Math. 8(5), 607–647
(2008)
20. Lecerf, G.: Quadratic newton iteration for systems with multiplicity. Found. Com-
put. Math. 2(3), 247–293 (2002)
21. Lee, J.M.: Introduction to Smooth Manifolds. Graduate Texts in Mathematics, vol.
218. Springer, Heidelberg (2003). doi:10.1007/978-1-4419-9982-5
22. Leykin, A.: Numerical primary decomposition. ISSAC 2008, 165–172 (2008)
23. Leykin, A., Verschelde, J., Zhao, A.: Newton’s method with deﬂation for isolated
singularities of polynomial systems. TCS 359(1), 111–122 (2006)
24. Li, T.Y.: Numerical solution of multivariate polynomial systems by homotopy con-
tinuation methods. Acta Numerica 6, 399–436 (1997)
25. Ma, Y., Wang, C., Zhi, L.: A certiﬁcate for semideﬁnite relaxations in computing
positive-dimensional real radical ideals. J. Symb. Comput. 72, 1–20 (2016)
26. Mantzaﬂaris, A., Mourrain, B.: Deﬂation and certiﬁed isolation of singular zeros
of polynomial systems. ISSAC 2011, 249–256 (2011)
27. Morrow, J.A., Kodaira, K.: Complex Manifolds, vol. 355. American Mathematical
Society, Providence (1971)
28. Ojika, T., Watanabe, S., Mitsui, T.: Deﬂation algorithm for the multiple roots of
a system of nonlinear equations. J. Math. Anal. Appl. 96(2), 463–479 (1983)
29. Parrilo, P.: Semideﬁnite programming relaxations for semialgebraic problems.
Math. Program. 96(2), 293–320 (2003)
30. Rouillier, F., Roy, M.F., El Din, M.S.: Finding at least one point in each connected
component of a real algebraic set deﬁned by a single equation. J. Complex. 16(4),
716–750 (2000)
31. El Din, M.S., Schost, ´E.: Properness defects of projections and computation of
at least one point in each connected component of a real algebraic set. Discrete
Comput. Geom. 32(3), 417–430 (2004)
32. El Din, M.S., Spaenlehauer, P.: Critical point computations on smooth varieties:
degree and complexity bounds. In: ISSAC 2016, pp. 183–190 (2016)
33. Sommese, A., Verschelde, J., Wampler, C.: Numerical decomposition of the solution
sets of polynomial systems into irreducible components. SIAM J. Numer. Anal.
38(6), 2022–2046 (2001)
34. Sommese, A., Wampler, C.: The Numerical Solution of Systems of Polynomials
Arising in Engineering and Science. World Scientiﬁc Press, Singapore (2005)
35. Wu, W., Reid, G.: Finding points on real solution components and applications to
diﬀerential polynomial systems. ISSAC 2013, 339–346 (2013)

Certifying Simple Zeros of Over-Determined
Polynomial Systems
Jin-San Cheng(B) and Xiaojie Dou(B)
Key Lab of Mathematics Mechanization, Institute of Systems Science,
Academy of Mathematics and Systems Science, CAS, Beijing, China
{jcheng,xjdou}@amss.ac.cn
Abstract. We construct a real square system related to a given over-
determined real system. We prove that the simple real zeros of the over-
determined system are the simple real zeros of the related square system
and the real zeros of the two systems are one-to-one correspondence with
the constraint that the value of the sum of squares of the polynomials
in the over-determined system at the real zeros is identically zero. After
certifying the simple real zeros of the related square system with the
interval methods, we assert that the certiﬁed zero is a local minimum of
the sum of squares of the input polynomials. If the value of the sum of
the squares of the input polynomials at the certiﬁed zero is equal to zero,
then it is a zero of the input system. Notice that a complex system with
complex zeros can be transformed into a real system with real zeros.
Keywords: Over-determined polynomial system · Simple real zeros ·
Sum of squares · Minimum point · Interval methods
1
Introduction
Finding zeros of polynomial systems is a fundamental problem in scientiﬁc com-
puting. Newton’s method is widely used to solve this problem. For a ﬁxed approx-
imate solution of a system, we can use the α-theory [3,10,31], the interval meth-
ods or the optimization methods [11,16,20,23,28,32] to completely determine
whether it is related to a zero of the system. However, the α-theory or the inter-
val methods focus mainly on a simple zero of a square system, that is, a system
with n equations and n unknowns.
Some special certiﬁcations of a rational solution of rational polynomials with
certiﬁed sum of squares decompositions are considered [2,13,15,22,26,27,30].
How about singular zeros of a well-constrained polynomial system? Usually,
an over-determined system which contains the same zero as a simple one is
constructed by introducing new equations. The basic idea are the deﬂation tech-
niques [1,5,6,8,9,24,25,33]. In some references [4,12,17,18,21,29], new variables
are also included. Moreover, some authors verify that a perturbed system pos-
sesses an isolated singular solution within a narrow and computed error bound.
The multiplicity structures of singular zeros of a polynomial system are also
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 66–76, 2017.
DOI: 10.1007/978-3-319-66320-3 6

Certifying Simple Zeros of Over-Determined Polynomial Systems
67
studied [5,9,21]. Especially in [1,9], singular zeros of the input systems are trans-
formed into simple zeros of the new systems when the coeﬃcients are rational.
For the deﬂation methods mentioned above, on one hand, to be a zero of the
perturbed systems does not mean being a zero of the input system considering
the diﬀerence between the two systems; on the other hand, although the over-
determined systems without introducing new variables have the same zeros as
the input systems, the veriﬁcation methods, such as the α-theory or the interval
methods, could not be used directly on the over-determined systems in general.
In [7], the authors study computing simple zeros of over-determined polyno-
mial systems with Newton’s method in theory. They also extend the α-theory
from well-constrained systems to over-determined systems. A main result about
Newton’s method given in their paper is Theorem 4 [7], which says that under
the condition of 2α1(g, ζ) < 1, where g is an analytic function g : E →F, with E
and F two real or complex Hilbert spaces, ζ is an attractive ﬁxed point for New-
ton’s method and simultaneously, a strict local minimum for ∥g∥2. However, as
they stated, whether the attracting ﬁxed points for Newton’s method are always
local minima of ∥g∥2, or the zeros of the input system, is unknown.
In this paper, we consider the problem of certifying the simple real zeros
of an over-determined polynomial system. After transforming the input over-
determined system into a square one, we can use both the α-theory and the
interval methods to certify its simple zeros. In this paper we only consider using
the interval methods to certify the simple real zeros of the over-determined
system. We prove that the simple real zeros of the input system are local minima
of the sum of squares of the input polynomials. We also give the condition that
the local minimum is a simple zero of the input system.
Let R be the ﬁeld of real numbers. Denote R[x] = R[x1, . . . , xn] as the
polynomial ring. Let F = {f1, . . . , fm} ⊂R[x] be a polynomial system. Let
p = (p1, . . . , pn) ∈Rn.
The following theorem is our main result of this paper.
Theorem 1. Let Σ = {f1, . . . , fm} ⊂R[x] (m ≥n) and f =
m

i=1
f 2
i . If p ∈Rn
is a simple real zero of Σ, then, we have:
1. p is a local minimum of f;
2. p is a simple real zero of Σ if and only if (p, 0) is a simple real zero of the
square system Σr = {J1(f), . . ., Jn(f), f −r}, where Ji(f) = ∂f
∂xi .
In the above theorem, we get a necessary and suﬃcient condition to certify
the simple real zeros of the input system Σ by certifying the simple real zeros
of the square system Σr. Therefore, to certify that p is a simple real zero of Σ,
the key point is verifying that f(p) = 0.
However, it is diﬃcult to decide numerically if a point is a zero of a polyno-
mial. Thus we can not use the necessary and suﬃcient condition to certify the
simple real zeros of Σ by certifying the simple real zeros of Σr.
As an alterative, we reﬁne and certify the simple real zeros of Σ by reﬁning
and certifying a new square system Σ′ = {J1(f), . . ., Jn(f)} with the interval

68
J.-S. Cheng and X. Dou
methods and get a veriﬁed inclusion X, which contains a unique simple real
zero ˆx of Σ′. In fact, ˆx is a local minimum of f. On one hand, if f(ˆx) = 0, by
Theorem 1, (ˆx, 0) is a simple real zero of Σr, and then ˆx is a simple real zero of
Σ. Thus, we certiﬁed the input system Σ. On the other hand, if f(ˆx) ̸= 0, we
will have a very small positive value f(ˆx). At this point, We assert that Σr has
a unique zero in the veriﬁed inclusion X × [0, f(ˆx)], which means we certiﬁed
the system Σr.
The paper is organized as below. We will introduce some notations and pre-
liminaries in the next section. In Sect. 3, we will give a method to show how to
transform an over-determined system into a square one. The interval veriﬁcation
method on the obtained square system is considered in Sect. 4.
2
Preliminaries
Let C be the ﬁeld of complex numbers. Denote C[x] = C[x1, . . . , xn] as the
polynomial ring. Let F = {f1, . . . , fm} ⊂C[x] be a polynomial system. Let
p = (p1, . . . , pn) ∈Cn. F(p) = 0 denotes that p is a zero of F(x) = 0.
Let A be a matrix. Denote AT as the transpose of A and rank(A) as the
rank of A. Let Mat(ai,j) denote the matrix whose i-th row j-th column element
is ai,j.
Let Σ = {f1, . . . , fm} ⊂C[x] be a polynomial system. Denote J(Σ) as the
Jacobian matrix of Σ. That is,
J(Σ) =
⎛
⎜
⎝
∂f1
∂x1 . . . ∂f1
∂xn
...
...
...
∂fm
∂x1 . . . ∂fm
∂xn
⎞
⎟
⎠.
For a polynomial f ∈C[x], let J(f) denote ( ∂f
∂x1 , ∂f
∂x2 , . . . , ∂f
∂xn ), Ji(f) =
∂f
∂xi
and Ji,j(f) = Jj(Ji(f)) =
∂2f
∂xj∂xi . Denote Σr = {J1(f), . . ., Jn(f), f −r} with
f =
m

j=1
f 2
j .
We denote the value of a function matrix A at a point p ∈Cn as A(p). Let
J(F)(p) denote the value of a function matrix J(F) at a point p, similarly for
J(f)(p).
Deﬁnition 1. An isolated solution of F(x) = 0 is a point p ∈Cn which
satisﬁes:
∃ε > 0 : {y ∈Cn : ∥y −p∥< ε} ∩F−1(0) = {p}.
Deﬁnition 2. We call an isolated solution p ∈Cn of F(x) = 0 a singular
solution if and only if
rank(J(F)(p)) < n.
Else, we call p a simple solution.

Certifying Simple Zeros of Over-Determined Polynomial Systems
69
Deﬁnition 3. A stationary point of a polynomial function f(x) ∈C[x] is a
point p ∈Cn, which satisﬁes:
∂f
∂xi
(p) = 0, ∀i = 1, . . . , n.
We can ﬁnd the following lemma in many undergraduate text books about
linear algebra (see Example 7 on page 224 in [19]).
Lemma 1. Let A ∈Rm×n be a real matrix with m ≥n and B = AT A. Then
the ranks of A and B are the same, especially for the case that A is of full rank.
In the following, we will consider the real zeros of the systems with real coef-
ﬁcients. It is reasonable since for a system (m equations and n unknowns) with
complex coeﬃcients, we can rewrite the system into a new one with 2 m equa-
tions and 2 n unknowns by splitting the unknowns xi = xi,1+i xi,2 and equations
fj(x1, . . . , xn) = gj,1(x1,1, x1,2, . . . , xn,1, xn,2) + i gj,2(x1,1, x1,2, . . . , xn,1, xn,2),
where i2 = −1, fj ∈C[x], gj,1, gj,2 ∈R[x], j = 1, . . . , m, and ﬁnd out the
complex zeros of the original system by ﬁnding out the real zeros of the new
system.
3
Transforming Over-Determined Polynomial Systems
into Square Ones
In this section, we will show how to transform an over-determined polynomial
system into a square one with their zeros having a one-to-one correspondence,
especially for the simple zeros.
By Deﬁnition 3, we have the following lemma:
Lemma 2. Given a polynomial system Σ = {f1, . . . , fm} ⊂R[x] (m ≥n). Let
f =
m

i=1
f 2
i and Σ′ = {J1(f), J2(f), . . . , Jn(f)}. If p ∈Rn is a real zero of Σ′,
then p is a stationary point of f.
Lemma 3. Let a polynomial system Σ = {f1, . . . , fm} ⊂R[x] (m ≥n), f =
m

i=1
f 2
i and Σ′ = {J1(f), J2(f), . . . , Jn(f)}. If p ∈Rn is a real zero of Σ, then
we have:
1. p is a real zero of Σ′;
2. rank(J(Σ)(p)) = rank(J(Σ′)(p).
Proof. It is clear that p is a real zero of Σ′ providing that p is a real zero of Σ,
since Ji(f) = 2
m

k=1
fk Ji(fk).
To prove the second part of this lemma, we rewrite Ji(f) as follows.
Ji(f) = 2 ⟨f1, . . . , fm⟩⟨Ji(f1), . . . , Ji(fm)⟩T ,
(1)

70
J.-S. Cheng and X. Dou
where ⟨· ⟩T is the transpose of a vector or a matrix ⟨· ⟩. Then
Ji,j(f) = Jj(Ji(f)) = Jj(2
m
	
k=1
fk Ji(fk)) = 2
m
	
k=1
(Jj(fk)Ji(fk) + fk Ji,j(fk))
= 2 ⟨Jj(f1), . . . , Jj(fm)⟩⟨Ji(f1), . . . , Ji(fm)⟩T + 2
m
	
k=1
fk Ji,j(fk).
(2)
Then the Jacobian matrix of Σ′ is
J(Σ′) =
⎛
⎜
⎝
J1,1(f) . . . J1,n(f)
...
...
...
Jn,1(f) . . . Jn,n(f)
⎞
⎟
⎠= Mat(Ji,j(f)).
We rewrite
Mat(Ji,j(f)) = 2 AT A + 2 Mat(
m
	
k=1
fk Ji,j(fk)),
(3)
where
A =
⎛
⎜
⎝
J1(f1) . . . Jn(f1)
...
...
...
J1(fm) . . . Jn(fm)
⎞
⎟
⎠
is an m×n matrix which is exactly the Jacobian matrix of Σ, that is, J(Σ) = A.
Then we have
J(Σ′)(p) = A(p)T A(p).
(4)
By Lemma 1, the second part of the lemma is true. This ends the proof.
⊓⊔
Remark 1. In our construction of f and Σ′, the degree of the polynomials may
be doubled. However, it has no inﬂuence on our actual computation, if you notice
Eq. (4) in the above proof. In fact, to get J(Σ′)(p), we only need to compute
A(p), which does not increase our actual computing degree.
As a byproduct, thanks to the doubled degree of the polynomials, our ﬁnal
certifying accuracy is also improved in Lemma 4.
The following is the proof of Theorem 1
Proof. In fact, by ﬁxing the real zero p as a simple zero in Lemma 3, we have
p is a simple real zero of Σ′ = {J1(f), . . . , Jn(f)}. Since p is a simple zero
of Σ, A(p) is a column full rank matrix. Therefore, it’s easy to verify that
J(Σ′)(p) = A(p)T A(p) is a positive deﬁnite matrix. Thus, p is a local minimum
of f and the ﬁrst part of the theorem is true. Now we consider the second part.
First, it’s easy to verify that p is the real zero of Σ if and only if (p, 0) is the
real zero of Σr. With the same method as proving Lemma 3, we can get
rank(J(Σ)(p)) = rank(J(Σr)(p, 0)) −1,
(5)
which means that J(Σr)(p, 0) is of full rank if and only if J(Σ)(p) is of full rank.
Thus, p is the simple zero of Σ if and only if (p, 0) is the simple zero of Σr. The
second part is true. We have ﬁnished the proof.
⊓⊔

Certifying Simple Zeros of Over-Determined Polynomial Systems
71
From Theorem 1, we can know that the simple real zeros of Σ and Σr are
in one to one correspondence with the constraint that the value of the sum
of squares of the polynomials in Σ at the simple real zeros is identically zero.
Thus we can transform an over-determined polynomial system into a square
system Σr.
We will show a simple example to illustrate the theorem below.
Example 1. The simple zero p = (0, 0) of the over-determined system Σ =
{f1, f2, f3} corresponds to a simple zero of a square system Σr = {J1(f), J2(f),
f −r}, where f = f 2
1 + f 2
2 + f 2
3 with
f1 = x2 −2 y, f2 = y2 −x, f3 = x2 −2 x + y2 −2 y.
We can verify simply that (p, 0) is a simple zero of Σr.
Though the simple real zeros of Σ and Σr have a one to one correspondence,
it can not be used directly to do certiﬁcation of the simple zeros of Σ since
we can not certify r = 0 numerically. But we can certify the zeros of Σ′ =
{J1(f), J2(f), . . . , Jn(f)} as an alternative. By Theorem 1 and Lemma 2, when
the value of f is zero at the certiﬁed zero, the certiﬁed zero is the very zero of
the system Σ. We will discuss it in next section.
4
Certifying Simple Zeros of Over-Determined Systems
In this section, we consider to certify the over-determined system with the inter-
val methods. We will prove the same local minimum result as [7].
The classical interval veriﬁcation methods are based on the following
theorem:
Theorem 2 [16,20,28,29]. Let f : Rn →Rn with f = (f1, . . . , fn) ∈C1, ˜x ∈
Rn, real interval vector X ∈IRn with ˜x ∈X and real matrix R ∈Rn×n be given.
Let an interval matrix M ∈IRn×n be given whose i-th row Mi satisﬁes
{∇fi(ζ) : ζ ∈˜x + X} ⊆Mi.
Denote by I the n × n identity matrix and assume
−Rf(˜x) + (I −RM)X ⊆int(X),
where int(X) denotes the interior of X. Then, there is a unique ˆx ∈˜x + X with
f(ˆx) = 0. Moreover, every matrix
˜
M ∈M is nonsingular. In particular, the
Jacobian J(f)(ˆx) is nonsingular.
About interval matrices, there is an important property in the following
theorem.
Theorem 3 [14]. A symmetric interval matrix AI is positive deﬁnite if and only
if it is regular and contains at least one positive deﬁnite matrix.

72
J.-S. Cheng and X. Dou
Given an over-determined polynomial system Σ = {f1, . . . , fm} ⊂R[x] with
a simple real zero, we can compute a related square system
Σ′ = { ∂f
∂x1
, ∂f
∂x2
, . . . , ∂f
∂xn
} with f =
m
	
j=1
f 2
j .
Based on Lemma 3, a simple zero of Σ is a simple zero of Σ′. Thus, we
can compute the approximate simple zero of Σ by computing the approximate
simple zero of Σ′. Using Newton’s method, we can reﬁne these approximate
simple zeros with quadratic convergence to a relative higher accuracy. Then, we
can certify them with the interval method mentioned before and get a veriﬁed
inclusion X, which possesses a unique certiﬁed simple zero of the system Σ′ by
Theorem 2, denoted as ˆx ∈X.
However, even though we get a certiﬁed zero ˆx of the system Σ′, considering
Lemma 2, we cannot say ˆx is a zero of the input system Σ, because the certiﬁed
zero ˆx is just a stationary point of f. Considering Theorem 1 and the diﬀerence
between Σ′ and Σr, we have the following theorem.
Theorem 4. Let Σ, Σ′, Σr, f, ˆx and the interval X be given as above. Then,
we have:
1. ˆx is a local minimum of f;
2. there exists a veriﬁed inclusion X × [0, f(ˆx)], which possesses a unique sim-
ple zero of the system Σr. Especially, if f(ˆx) = 0, the veriﬁed inclusion X
possesses a unique simple zero of the input system Σ.
Proof. First, it’s easy to see that computing the value of the matrix J(Σ′) at
the interval X will give a symmetric interval matrix, denoted as J(Σ′)(X). By
Theorem 2, we know that for every matrix M ∈J(Σ′)(X), M is nonsingu-
lar. Therefore, the interval matrix J(Σ′)(X) is regular. Especially, the matrix
J(Σ′)(ˆx), which is the Hessian matrix of f, is of full rank and therefore is pos-
itive deﬁnite. Thus, ˆx is a local minimum of f. By Theorem 3, we know that
J(Σ′)(X) is positive deﬁnite. Thus, for every point q ∈X, J(Σ′)(q) is a positive
deﬁnite matrix. Considering Theorem 2, it’s trivial that for the veriﬁed inclusion
X × [0, f(ˆx)], there exists a unique simple zero of the system Σr. If f(ˆx) = 0,
by Theorem 1, the veriﬁed inclusion X of the system Σ′ is a veriﬁed inclusion
of the original system Σ.
⊓⊔
Remark 2. In the above proof, we know that for every point q ∈X, J(Σ′)(q) is
a positive deﬁnite matrix.
By Theorem 2, we know that there is a unique ˆx ∈X with Σ′(ˆx) = 0.
However, we could not know what the exact ˆx is. According to the usual practice,
in actual computation, we will take the midpoint ˆp of the inclusion X as ˆx and
verify whether f(ˆp) = 0 or not. Considering the uniqueness of ˆx in X, therefore,
if f(ˆp) = 0, we are sure that the veriﬁed inclusion X possesses a unique simple
zero of the input system Σ. If f(ˆp) ̸= 0, we can only claim that there is a local
minimum of f in the inclusion X and X × [0, f(ˆp)] is a veriﬁed inclusion for the
system Σr.

Certifying Simple Zeros of Over-Determined Polynomial Systems
73
Considering the expression of Σ and f and for the midpoint ˆp of X, we have
a trivial result below.
Lemma 4. Denote ϵ =
m
max
j=1 |fj(ˆp)|. Under the conditions of Theorem 4, we
have |f(ˆp)| ≤mϵ2.
Based on the above idea, we give an algorithm below. In the veriﬁcation
steps, we will apply the algorithm verifynlss in INTLAB [29], which is based
on Theorem 2, to compute a veriﬁed inclusion X for the related square system Σ′.
For simplicity, denote the interval X = [x1, x1], · · · , [xm, xm] and the midpoint
of X as ˆp = [(x1 + x1)/2, . . . , (xm + xm)/2].
Algorithm 1. VSPS: verifying a simple zero of a polynomial system
Input: an over-determined polynomial system Σ := {f1, · · · , fm} ⊂R[x] and an
approximate simple zero ˜p = (˜p1, · · · , ˜pn) ∈Rn.
Output: a veriﬁed inclusion X and a small non-negative number.
1: Compute f and Σ′;
2: Compute ˜p′ := Newton(Σ′, ˜p);
3: Compute X := verifynlss(Σ′, ˜p′) and f(ˆp);
4: if f(ˆp) = 0, then
5:
return (X, 0);
6: else
7:
return (X, f(ˆp)).
8: end if
The correctness and the termination of the algorithm is obvious by the above
analysis.
We give two examples to illustrate our algorithm.
Example 2. Continue Example 1. Given an approximate zero ˜p = (0.0003528,
0.0008131). Using Newton’s method, we will get a high accuracy approximate
zero
˜p′ = 10−11 · (−0.104224090958505, −0.005858368844383).
Compute f = f 2
1 +f 2
2 +f 2
3 and Σ′ = {J1(f), J2(f)}. After applying the algorithm
verifynlss on Σ′, we have a veriﬁed inclusion:
X =

[−0.11330049261083, 0.11330049261083]
[−0.08866995073891, 0.08866995073891]

· 10−321.
Based on Theorem 2, we know that there exists a unique ˆx∈X, s.t. Σ′(ˆx)=0.
Let Σr = {J1(f), J2(f), f −r}. By Theorem 1, we can certify the simple zero
of Σ by certifying the simple zero of Σr. Considering the diﬀerence between Σ′
and Σr, we check ﬁrst whether the value of f at some point in the interval X is
zero. According to the usual practice, we consider the midpoint ˆp of X, which
equals (0, 0) and further, f(ˆp) is zero. Therefore, we are sure that there exists

74
J.-S. Cheng and X. Dou
a unique ˆx = (ˆx, ˆy) ∈X, s.t. Σr((ˆx, 0)) = 0 and then, there exists a unique
simple zero (ˆx, ˆy), |ˆx|, |ˆy| ≤10−321, of the input system Σ in the interval X,
which means we certiﬁed the input system Σ.
Example 3. Let Σ = {f1 = x2
1 + 3 x1x2 + 3 x1x3 −3 x2
3 + 2 x2 + 2 x3, f2 =
−3 x1x2 + x1x3 −2 x2
2 + x2
3 + 3 x1 + x2, f3 = 2 x2x3 + 3 x1 −3 x3 + 2, f4 =
−6 x2
2x3 + 2 x2x2
3 + 6 x2
2 + 15 x2x3 −6 x2
3 −9 x2 −7 x3 + 6} be an over-determined
system. Given an approximate zero
˜p = (−1.29655, 0.47055, −0.91761).
Using Newton’s method, we will get a high accuracy zero
˜p′ = (−1.296687216045438, 0.470344502045004, −0.917812633399457).
Compute
f = f 2
1 + f 2
2 + f 2
3 + f 2
4 and Σ′ = {J1(f), J2(f), J3(f)}.
After applying the algorithm verifynlss on Σ′, we have a veriﬁed inclusion:
X =
⎛
⎝
[−1.29668721603974, −1.29668721603967]
[
0.47034450205107,
0.47034450205114]
[−0.91781263339256, −0.91781263339247]
⎞
⎠.
Similarly, based on Theorem 2, we know that there exists a unique ˆx ∈X, s.t.
Σ′(ˆx) = 0.
Proceeding as in the above example, we consider the midpoint ˆp of X and
compute f(ˆp) = 3.94·10−31 ̸= 0. Thus, by Theorem 4, we get a veriﬁed inclusion
X × [0, f(ˆp)], which contains a unique simple zero of the system Σr. It means
that X may contain a zero of Σ. Even if X does not contain a zero of Σ, it
contains a local minimum of f, which has a minimum value no larger than f(ˆp).
Acknowledgement. The work is partially supported by NSFC Grants 11471327.
References
1. Akogul, T.A., Hauenstein, J.D., Szanto, A.: Certifying solutions to overdetermined
and singular polynomial systems over Q, 12 August 2014. arXiv: 1408.2721v1
[cs.SC]
2. Allamigeon, X., Gaubert, S., Magron, V., Werner, B.: Formal proofs for nonlinear
optimization (2014). arXiv:1404.7282
3. Blum, L., Cucker, F., Shub, M., Smale, S.: Complexity and real computation.
Springer, New York (1998)
4. Dayton, B., Li, T., Zeng, Z.: Multiple zeros of nonlinear systems. Math. Comp. 80,
2143–2168 (2011)
5. Dayton, B., Zeng, Z.: Computing the multiplicity structure in solving polynomial
systems. In: Kauers, M. (ed.) Proceedings of ISSAC 2005, pp. 116–123. ACM, New
York (2005)

Certifying Simple Zeros of Over-Determined Polynomial Systems
75
6. Giusti, M., Lecerf, G., Salvy, B., Yakoubsohn, J.C.: On location and approximation
of clusters of zeros: case of embedding dimension one. Found. Comput. Math. 7,
1–58 (2007)
7. Dedieu, J.P., Shub, M.: Newton’s method for overdetermined systems of equations.
Math. Comput. 69(231), 1099–1115 (1999)
8. Hauenstein, J.D., Wampler, C.W.: Isosingular sets and deﬂation. Found. Comput.
Math. 13(3), 371–403 (2013)
9. Hauenstein, J.D., Mourrain, B., Szanto, A.: Certifying isolated singular points and
their multiplicity structure. In: Proceedings of ISSAC 2015, pp. 213–220 (2015)
10. Hauenstein, J.D., Sottile, F.: Algorithm 921: alphaCertiﬁed: certifying solutions to
polynomial systems. ACM Trans. Math. Softw. 38(4), 28 (2012)
11. Kanzawa, Y., Kashiwagi, M., Oishi, S.: An algorithm for ﬁnding all solutions
of parameter-dependent nonlinear equations with guaranteed accuracy. Electron.
Commun. Jpn. (Part III: Fundam. Electron. Sci.) 82(10), 33–39 (1999)
12. Kanzawa, Y., Oishi, S.: Approximate singular solutions of nonlinear equations and
a numerical method of proving their existence. S¯urikaisekikenky¯usho K¯oky¯uroku
990, 216–223 (1997). Theory and application of numerical calculation in science
and technology, II (Japanese) Kyoto (1996)
13. Kaltofen, E., Li, B., Yang, Z., Zhi, L.: Exact certiﬁcation of global optimality of
approximate factorizations via rationalizing sums-of-squares with ﬂoating point
scalars. In: Proceedings of ISSAC 2008, pp. 155–164, New York. ACM (2008)
14. Rohn, J.: Positive deﬁniteness and stability of interval matrices. SIAM J. Matrix
Anal. Appl. 15, 175–184 (1994)
15. Kaltofen, E.L., Li, B., Yang, Z., Zhi, L.: Exact certiﬁcation in global polynomial
optimization via sums-of-squares of rational functions with rational coeﬃcients. J.
Symb. Comput. 47(1), 1–15 (2012)
16. Krawczyk, R.: Newton-Algorithmen zur Bestimmung von Nullstellen mit Fehler-
schranken. Computing 4, 247–293 (1969)
17. Leykin, A., Verschelde, J., Zhao, A.: Newton’s method with deﬂation for isolated
singularities of polynomial systems. Theor. Comput. Sci. 359, 111–122 (2006)
18. Li, N., Zhi, L.: Veriﬁed error bounds for isolated singular solutions of polynomial
systems. SIAM J. Numer. Anal. 52(4), 1623–1640 (2014)
19. Li, S.: Linear Algebra. Higher Education Press (2006). ISBN 978-7-04-019870-6
20. Moore, R.E.: A test for existence of solutions to nonlinear systems. SIAM J. Numer.
Anal. 14, 611–615 (1977)
21. Mantzaﬂaris, A., Mourrain, B.: Deﬂation and certiﬁed isolation of singular zeros
of polynomial systems. In: Proceedings ISSAC 2011, pp. 249–256 (2011)
22. Monniaux, D., Corbineau, P.: On the generation of positivstellensatz witnesses
in degenerate cases. In: Eekelen, M., Geuvers, H., Schmaltz, J., Wiedijk, F. (eds.)
ITP 2011. LNCS, vol. 6898, pp. 249–264. Springer, Heidelberg (2011). doi:10.1007/
978-3-642-22863-6 19
23. Nakaya, Y., Oishi, S., Kashiwagi, M., Kanzawa, Y.: Numerical veriﬁcation of nonex-
istence of solutions for separable nonlinear equations and its application to all solu-
tions algorithm. Electron. Commun. Jpn. (Part III: Fundam. Electron. Sci.) 86(5),
45–53 (2003)
24. Ojika, T.: A numerical method for branch points of a system of nonlinear algebraic
equations. Appl. Numer. Math. 4, 419–430 (1988)
25. Ojika, T., Watanabe, S., Mitsui, T.: Deﬂation algorithm for the multiple roots of
a system of nonlinear equations. J. Math. Anal. Appl. 96, 463–479 (1983)

76
J.-S. Cheng and X. Dou
26. Peyrl, H., Parrilo, P.A.: A Macaulay2 package for computing sum of squares decom-
positions of polynomials with rational coeﬃcients. In: Proceeding of SNC 2007, pp.
207–208 (2007)
27. Peyrl, H., Parrilo, P.A.: Computing sum of squares decompositions with rational
coeﬃcients. Theor. Comput. Sci. 409(2), 269–281 (2008)
28. Rump, S.M.: Solving algebraic problems with high accuracy. In: Proceedings of the
Symposium on A New Approach to Scientiﬁc Computation, San Diego, CA, USA,
pp. 51–120. Academic Press Professional Inc. (1983)
29. Rump, S.M., Graillat, S.: Veriﬁed error bounds for multiple roots of systems of
nonlinear equations. Numer. Algorithms 54(3), 359–377 (2010)
30. El Din, M.S., Zhi, L.: Computing rational points in convex semialgebraic sets and
sum of squares decompositions. SIAM J. Optim. 20(6), 2876–2889 (2010)
31. Smale, S.: Newton’s method estimates from data at one point. In: Ewing, R.E.,
Gross, K.I., Martin, C.F. (eds.) The Merging of Disciplines : New Directions in
Pure, Applied and Computational Mathematics, pp. 185–196. Springer, Heidelberg
(1986). doi:10.1007/978-1-4612-4984-9 13
32. Yamamura, K., Kawata, H., Tokue, A.: Interval solution of nonlinear equations
using linear programming. BIT Numer. Math. 38(1), 186–199 (1998)
33. Zeng, Z.: Computing multiple roots of inexact polynomials. Math. Comput. 74,
869–903 (2005)

Decomposing Polynomial Sets Simultaneously
into Gr¨obner Bases and Normal Triangular Sets
Rina Dong and Chenqi Mou(B)
LMIB–SKLSDE–School of Mathematics and Systems Science,
Beihang University, Beijing 100191, China
{rina.dong,chenqi.mou}@buaa.edu.cn
Abstract. In this paper we focus on the algorithms and their imple-
mentations for decomposing an arbitrary polynomial set simultaneously
into pairs of lexicographic Gr¨obner bases and normal triangular sets with
inherent connections in between and associated zero relationship with the
polynomial set. In particular, a method by temporarily changing the vari-
able orderings to handle the failure of the variable ordering assumption
is proposed to ensure splitting needed for characteristic decomposition.
Experimental results of our implementations for (strong) characteristic
decomposition with comparisons with available implementations of tri-
angular decomposition are also reported.
Keywords: Normal triangular set · Gr¨obner basis · Characteristic
decomposition · Variable ordering
1
Introduction
Polynomial elimination theory, a classical branch of algebra, mainly studies
the variable elimination, ordering, and decomposition of polynomial systems
to reduce them into new ones with speciﬁc algebraic structures like being trian-
gularized [28]. There are three main elimination methods based respectively on
resultants, triangular sets [24,28,34], and Gr¨obner bases [6,9], the last two of
which can be considered as generalizations of the well-known method of Gaussian
elimination for linear systems.
Triangular decomposition is the process to decompose a polynomial set into
ﬁnitely many triangular sets, which are ordered triangularized polynomial sets
with respect to (w.r.t.) the variable ordering, such that the zero set of the poly-
nomial set is equal to the union of those of the triangular sets. With continuous
development on the theory, methods, and algorithms for triangular decompo-
sition (see, e.g., [2–4,8,14,18,26,27,34] and references therein), triangular sets
have become a computational tool for polynomial elimination and polynomial
This work was partially supported by the National Natural Science Foundation of
China (NSFC 11401018).
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 77–92, 2017.
DOI: 10.1007/978-3-319-66320-3 7

78
R. Dong and C. Mou
system solving. Currently there are eﬀective algorithms for decomposing poly-
nomial sets of moderate size into triangular sets of various kinds [7,8,16,21,28].
In this paper we are particularly interested in one kind of triangular sets,
namely the normal sets, which are also called normalized triangular sets [18] and
p-chain [14]. Normal sets, the initials of whose polynomials involve only the para-
meters, are convenient for dealing with parametric polynomial systems [7,14].
Algorithms have been proposed to normalize triangular sets or to decompose
arbitrary polynomial sets into normal sets [28,32].
The Gr¨obner basis is a set of special generators of the ideal generated by a
polynomial set w.r.t. a certain term ordering. Since its introduction by Buch-
berger in his Ph.D. thesis [6], the Gr¨obner basis has gained extensive study
on the theory, methods, and algorithms [12,13,15,25,33] and become a pow-
erful tool for computational commutative algebra and algebraic geometry with
diverse applications. The elimination method with Gr¨obner bases is mainly based
on the lexicographic (LEX) term ordering because of the good structures and
rich properties of LEX Gr¨obner bases, for example their elimination property for
elimination ideals. The structures of LEX Gr¨obner bases were studied ﬁrst for
bivariate ideals [17] and then extended to general zero-dimensional polynomial
ideals [10,22]. Furthermore, the relationships between triangular decomposition
of a polynomial set and the LEX Gr¨obner basis of the ideal generated by the
polynomial set have also been studied. For zero-dimensional polynomial ideals,
the relationship has been studied in [19] and algorithms for computing triangular
decomposition from LEX Gr¨obner bases have been proposed in [10,19].
The relationship between Ritt characteristic sets and LEX Gr¨obner bases is
investigated in [30] for polynomial ideals of arbitrary dimension via the so-called
W-characteristic sets which are the smallest triangular sets contained in the LEX
Gr¨obner bases. In particular, it is shown in [30] that when the W-characteristic
set is abnormal, some certain polynomial in it is pseudo-divisible by another
polynomial. By using such pseudo-divisibility, an algorithm is proposed in [31]
to decompose an arbitrary polynomial set simultaneously into pairs of LEX
Gr¨obner bases and normal sets (called characteristic decomposition) which have
rich interconnections and provide two kinds of representations for the zeros of the
polynomial set, and the structures and properties of characteristic decomposition
are also studied.
As the follow-up work of [31], this paper focuses on the algorithms for
(strong) characteristic decomposition and their implementations. In particular,
an assumption on the variable ordering for the pseudo-divisibility to occur, the
failure of which is not touched in [31] but happens indeed in our experiments (in
8 out of 35 positive-dimensional test examples, with more details in Table 1), is
further handled by temporarily changing the variable orderings. We also make an
enriched comparison on the performances of our implementation for character-
istic decomposition with some available implementations for triangular decom-
position via normal decomposition. As shown by the experimental results, our
implementation performs comparably well with other similar implementations,
but with richer output.

Decomposing Polynomial Sets
79
After a brief review of triangular sets, Gr¨obner bases, and (strong) charac-
teristic decomposition in Sect. 2, we present the method to handle the variable
ordering condition and recall the algorithms for (strong) characteristic decom-
position in detail, followed by an illustrative example, in Sect. 3. Then the exper-
imental results with our implementations for (strong) characteristic decomposi-
tion are reported in Sect. 4.
2
Preliminaries
In this section some basic notions and notations used in the sequel are recalled.
The reader is referred to [2,5,9,28] and references therein for more details on the
theories of Gr¨obner bases and triangular sets and to [30,31] for the deﬁnitions
and properties of characteristic decomposition.
2.1
Triangular Set and Triangular Decomposition
Let K be a ﬁeld and K[x1, . . . , xn] be the ring of polynomials in n ordered
variables x1 < · · · < xn with coeﬃcients in K. For the sake of simplicity, we
denote K[x1, . . . , xn] by K[x].
Let F be a polynomial in K[x]\K. With respect to the variable ordering, the
greatest variable eﬀectively appearing in F is called the leading variable of F and
denoted by lv(F). Let lv(F) = xi. Then F can be written as F = Ixk
i + R with
I ∈K[x1, . . . , xi−1], R ∈K[x1, . . . , xi], and deg(R, xi) < k = deg(F, xi). The
polynomial I is called the initial of F, denoted by ini(F). For any polynomial
set F ⊆K[x], we use ini(F) to denote the set {ini(F) | F ∈F}.
Deﬁnition 1. A ﬁnite, nonempty, ordered set T = [T1, . . . , Tr] of polynomials
in K[x] \ K is called a triangular set if lv(T1) < · · · < lv(Tr).
The saturated ideal of a triangular set T = [T1, . . . , Tr] is deﬁned as sat(T ) =
⟨T ⟩: J∞, where J = ini(T1) · · · ini(Tr). For a triangular set T ⊂K[x], the
variables in {x1, . . . , xn} \ {lv(T1), . . . , lv(Tr)} are called its parameters. T is
said to be zero-dimensional if it has no parameter, and positive-dimensional
otherwise.
Deﬁnition 2. A triangular set T = [T1, . . . , Tr] ⊆K[x] is said to be a normal
set if ini(T1), . . . , ini(Tr) only involve the parameters of T .
One of the most commonly used triangular sets are the so-called regular sets
or regular chains [16,27]. Any normal set is regular by deﬁnition. For any two
polynomial sets F, G ⊂K[x], we denote by Z(F/G) the set
Z(F/G) := {¯x ∈¯Kn : F(¯x) = 0, G(¯x) ̸= 0, ∀F ∈F, G ∈G},
where ¯K is the algebraic closure of K. In particular, Z(F) := Z(F/{}).
Deﬁnition 3. Let F ⊂K[x] be a polynomial set. A ﬁnite number of triangular
sets T1, T2, . . . , Tt ⊂K[x] are called a triangular decomposition of F if Z(F) =
t
i=1 Z(Ti/ ini(Ti)).

80
R. Dong and C. Mou
2.2
Gr¨obner Basis and W-Characteristic Set
A term ordering <t is a total and well ordering on all the terms in K[x]. With
a ﬁxed term ordering <t, the greatest term in a polynomial F ∈K[x] w.r.t. <t
is called the leading term of F and denoted by lt(F).
In this paper the LEX term ordering <LEX is of our main concern. For two
diﬀerent terms xα and xβ in K[x] with α = (α1, . . . , αn) and β = (β1, . . . , βn),
we say that xα <LEX xβ if there exists an integer i (1 ≤i ≤n) such that
αi < βi and for j = i + 1, . . . , n, αj = βj.
Deﬁnition 4. Let I ⊆K[x] be an ideal. A ﬁnite set {G1, . . . , Gs} of poly-
nomials in I is called a Gr¨obner basis of I w.r.t. the term ordering <t if
⟨lt(G1), . . . , lt(Gs)⟩= ⟨lt(I)⟩, where ⟨lt(I)⟩denotes the ideal generated by the
leading terms of all the polynomials in I.
A Gr¨obner basis G is said to be reduced if for each G ∈G, its coeﬃcient w.r.t.
lt(G) is 1 and any term in G is not divisible by lt(G′) for any G′ ∈G \ {G}.
The reduced Gr¨obner basis of any ideal w.r.t. a ﬁxed term ordering is unique.
In particular, from the reduced LEX Gr¨obner basis of an ideal, one can extract
the W-characteristic set of this ideal as deﬁned below.
Deﬁnition 5 [30, Deﬁnition 3.1]. Let G be the reduced LEX Gr¨obner basis of
the ideal ⟨P⟩⊆K[x]. Denote G(i) = {G ∈G | lv(G) = xi}. Then the ordered set
of all the smallest polynomials w.r.t. <LEX in every set G(i) for i = 1, . . . , n is
called the W-characteristic set of ⟨P⟩.
Obviously, a W-characteristic set is also a triangular set. Basic properties
of W-characteristic sets and the pseudo-divisibility relationship between poly-
nomials in W-characteristic sets are recalled respectively in Proposition 1 and
Theorem 1 below. We would like to mention here that it is the relationship in
Theorem 1 that enables us to adopt an eﬀective splitting strategy in algorithms
for characteristic decomposition of polynomial sets.
Proposition 1 [30, Proposition 3.1]. Let C be the W-characteristic set of ⟨P⟩⊆
K[x]. Then (a) For any P ∈⟨P⟩, prem(P, C) = 0; (b) ⟨C⟩⊆⟨P⟩⊆sat(C); (c)
Z(C/ ini(C)) ⊆Z(P) ⊆Z(C).
Theorem 1 [30, Theorem 3.9]. Let C = [C1, . . . , Cr] be the W-characteristic set
of ⟨P⟩⊆K[x]. If C is not normal, then there exists an integer k (1 ≤k ≤r)
such that [C1, . . . , Ck] is normal and [C1, . . . , Ck+1] is not regular.
Assume that the variables x1, . . . , xn are ordered such that the parameters of
C are all smaller than the other variables and let Ik+1 = ini(Ck+1) and l be the
integer such that lv(Ik+1) = lv(Cl).
(a) If Ik+1 is not R-reduced w.r.t. Cl, then
prem(Ik+1, [C1, . . . , Cl]) = 0,
prem(Ck+1, [C1, . . . , Ck]) = 0.

Decomposing Polynomial Sets
81
(b) If Ik+1 is R-reduced w.r.t. Cl, then prem(Cl, [C1, . . . , Cl−1, Ik+1]) = 0 and
either res(ini(Ik+1), [C1, . . . , Cl−1]) = 0 or prem(Ck+1, [C1, . . . , Cl−1, Ik+1,
Cl+1, . . . , Ck]) = 0.
For a triangular set T ⊂K[x] and a variable ordering <, we say that the
variable ordering condition is satisﬁed for T w.r.t. < if all the parameters of T
are ordered before the leading variables of polynomials in T in <. As a counter-
example [30, Example 3.1(b)] shows, when the variable ordering condition is
not satisﬁed for a W-characteristic set, Theorem 1 does not hold in general. We
will work on the case when the variable ordering condition is not satisﬁed in
Algorithm 1 in Sect. 3.
2.3
(Strong) Characteristic Decomposition and Characterizable
Gr¨obner Basis
Deﬁnition 6 [31, Deﬁnition 3.1]. A pair (G, C) with G, C ⊆K[x] is called a
characteristic pair if G is a reduced LEX Gr¨obner basis, C is the W-characteristic
set of ⟨G⟩, and C is normal.
For any polynomial set F ⊆K[x], we want to compute ﬁnitely many char-
acteristic pairs (G1, C1), . . . , (Gt, Ct) such that
Z(F) =
t
i=1
Z(Gi) =
t
i=1
Z(Ci/ ini(Ci)) =
t
i=1
Z(sat(Ci)).
(1)
A ﬁnite number of characteristic pairs (G1, C1), . . . , (Gt, Ct) are said to be a char-
acteristic decomposition of F if the zero relationship (1) holds.
Remark 1. As can be seen from the deﬁnition of characteristic decomposition
above, from a characteristic decomposition (G1, C1), . . . , (Gt, Ct) of a polynomial
set F one can easily extract a normal decomposition C1, . . . , Ct of F.
Theorem 2 [31, Theorem 4.1]. For any ﬁnite, nonempty polynomial set F ⊆
K[x], its characteristic decomposition can be computed within a ﬁnite number of
operations if the variable ordering condition is satisﬁed.
Deﬁnition 7 [31, Deﬁnitions 3.7 and 3.8]. A reduced LEX Gr¨obner basis G is
said to be characterizable if ⟨G⟩= sat(C), where C is the W-characteristic set of
G. A characteristic pair (G, C) is said to be strong if sat(C) = ⟨G⟩.
Clearly the reduced LEX Gr¨obner basis in a strong characteristic pair is
characterizable. Furthermore, it is proved that the W-characteristic set of a
characterizable Gr¨obner basis is also normal [31, Proposition 3.9], and thus a
characterizable Gr¨obner basis and its W-characteristic set form a strong charac-
teristic pair.
A characteristic decomposition is said to be strong if each characteristic pair
within is strong. For any characteristic decomposition Ψ = {(G1, C1), . . . , (Gt, Ct)}

82
R. Dong and C. Mou
of F ⊆K[x], by [31, Theorem 3.22] one can explicitly transform Ψ into a strong
characteristic decomposition ¯Ψ = {( ¯G1, ¯C1), . . . , ( ¯Gt, ¯Ct)} such that the following
zero relationships hold.
Z(F) =
t
i=1
Z( ¯Gi) =
t
i=1
Z( ¯Ci/ ini( ¯Ci)) =
t
i=1
Z(sat( ¯Ci)).
(2)
3
Algorithm for (Strong) Characteristic Decomposition
In this section we ﬁrst handle the variable ordering condition in Theorem 1 by
temporarily changing the variable orderings, then we incorporate this process
into the proposed algorithm (Algorithm 1 in [31]) for characteristic decom-
position and represent it as Algorithm 2 for self-containedness. The algo-
rithm to transform a characteristic decomposition into a strong one by using
[31, Theorem 3.22] is formulated as Algorithm 3. The algorithms for (strong)
characteristic decomposition are able to decompose an arbitrary polynomial set
into simultaneously (characterizable) Gr¨obner bases and normal triangular sets.
3.1
Algorithm to Handle the Variable Ordering Condition
For a polynomial set P ⊂K[x] and a variable ordering <, let G be the reduced
LEX Gr¨obner basis of ⟨P⟩w.r.t. < and C be the W-characteristic set extracted
from G. If C is abnormal but the variable ordering condition is not satisﬁed for C
w.r.t. <, then Theorem 1 does not hold in general, which means that the psuedo-
divisibility between the polynomials in C may not be found and thus the splitting
needed in algorithms for characteristic decomposition is not guaranteed. In order
to make the psuedo-divisibility happen, the following procedure is proposed.
(1) Take a new ordering <′ on the variables x1, . . . , xn.
(2) Compute the reduced LEX Gr¨obner basis G′ of ⟨P⟩w.r.t. <′ and extract
the new W-characteristic set C′ of ⟨P⟩from G′.
(3) Check whether the variable ordering condition is satisﬁed for C′ w.r.t. <′
and C′ is abnormal.
If the variable ordering condition is veriﬁed to be satisﬁed for C′ w.r.t. <′ and C′ is
abnormal, then we stop with G′, C′, and <′, otherwise the above steps (1)–(3) are
repeated until the variable ordering condition is satisﬁed or the repetition ends
after the ﬁnite choices of possible variable orderings. The procedure described
above is formulated into Algorithm 1 below.
When the Gr¨obner basis G = {1}, the W-characteristic C′ = [1]. In this case
the variable ordering condition is assumed to be satisﬁed for C′, and ({1}, [1], <)
is returned as in Algorithm 1.
Algorithm 1 may return a new variable ordering <′ which is diﬀerent from
the original one, but we make use of the fact that w.r.t. <′ the abnormal W-
characteristic set C′ of the considered ideal ⟨P⟩satisﬁes the variable ordering

Decomposing Polynomial Sets
83
Algorithm 1. ((G′, C′), <′) := VOC(P, <) (algorithm for ensuring the vari-
able ordering condition)
Input: P, a ﬁnite, nonempty set of nonzero polynomials in K[x];
<, a variable ordering.
Output: either (G′, C′), a pair of reduced LEX Gr¨obner basis of ⟨P⟩and its
W-characteristic set, and <′, a variable ordering such that C′ satisﬁes
the variable ordering condition w.r.t. <′.
or “ERROR”
Compute the reduced LEX Gr¨obner basis G of ⟨P⟩w.r.t <;
1
Extract the W-characteristic set C from G w.r.t <;
2
if C satisﬁes the variable ordering condition w.r.t. < then
3
return ((G, C), <)
4
else
5
Ψ :={all possible variable orderings on x1, . . . , xn}\{<};
6
for <′∈Ψ do
7
Compute the reduced LEX Gr¨obner basis G′ of ⟨P⟩w.r.t <′;
8
Extract the W-characteristic set C′ from G′ w.r.t <′;
9
if C′ is abnormal and satisﬁes variable ordering condition w.r.t. <′ then
10
return ((G′, C′), <′)
11
return “ERROR”
12
condition, and thus Theorem 1 ensures psuedo-divisibility between polynomials
in C′ and the splitting needed is also guaranteed. We would like to emphasize
that whether Theorem 1 holds or not depends on the variable ordering, but
the resultant polynomial sets after splitting do not. This means that after the
splitting we can continue the computation for characteristic decomposition w.r.t.
the original variable ordering (see the descriptions of Algorithm 2 below for more
details), and thus this change of variable orderings is only temporary.
In Line 7 of Algorithm 1, we use the heuristic to choose ﬁrst the new orderings
w.r.t. which C satisﬁes the variable ordering condition, namely the parameters
of C are ordered smaller than the other variables in such orderings, and then the
other orderings in Ψ. This strategy works well to succeed with the ﬁrst pick of
such orderings in most cases.
Even with the change of variable orderings in Algorithm 1 one may fail to
ﬁnd a proper abnormal W-characteristic set which satisﬁes the variable ordering
condition. Below is one simple example for such failure, which is also the only
one we have found out of over one hundred examples in our experiments. The
characterization of this phenomenon and the way to continue the process of
characteristic decomposition when such phenomenon occurs are our future work.
Let P = {x2, (x + y)z + x} ⊆K[x, y, z] with x < y < z. The reduced LEX
Gr¨obner basis of ⟨P⟩is G = {x2, xz + yz + x} and the W-characteristic set is
C = [x2, (x + y)z + x]. Clearly C does not satisfy the variable ordering condition
w.r.t. x < y < z. But for all the other possible variable orderings, the new
W-characteristic sets are either normal or do not satisfy the variable ordering
condition w.r.t. the new variable ordering.

84
R. Dong and C. Mou
3.2
Algorithms for Characteristic Decomposition
Following the overall splitting strategies sketched in [30], an algorithm for char-
acteristic decomposition is proposed in [31]. For the purpose of clarity, this
algorithm is recalled and represented here, but updated with the addition of
Algorithm 1 for handling the variable ordering condition.
Let F ⊆K[x] be the input polynomial set and < be a variable ordering. We
use a set Φ to store the polynomial sets which need further computation and a
set Ψ to store the characteristic pairs already computed.
(1) Pick up a polynomial set P ∈Φ and remove it from Φ. Then we use Algo-
rithm 1, if it succeeds, to ﬁnd a proper variable ordering <′ w.r.t. which the
W-characteristic set C of the ideal ⟨P⟩satisﬁes the variable ordering condi-
tion, where C is extracted from the reduced LEX Gr¨obner basis G of ⟨P⟩.
(2) If C is a normal set, then one knows that no change of variable orderings
occurs, namely in this case <′=<, and a characteristic pair (G, C) is found
and adjoined to Ψ. Splitting for this case follows the strategy proposed in
Algorithm 1 in [31].
Otherwise, the variable ordering condition is satisﬁed for C w.r.t. <′ (diﬀer-
ent from <) and C is abnormal, and Theorem 1 furnishes pseudo-divisibility
between polynomials in C. By using the same splitting strategies in [31], but
w.r.t. the new variable ordering <′, we are able to split P into G ∪{H1}, . . . ,
G ∪{Hs}, where H1, . . . , Hs are polynomials reduced w.r.t. G. Then the new
polynomial sets G ∪{H1}, . . . , G ∪{Hs} are adjoined to Φ.
(3) After the splitting another polynomial set P′ ∈Φ is picked up and steps
(1)–(2) are repeated for P′ w.r.t. the original variable ordering <.
The above steps are repeated until Φ becomes empty, when we will get ﬁnitely
many characteristic pairs (G1, C1), . . . , (Gt, Ct) which form a characteristic decom-
position of the input polynomial set F. Following the same proving strategies as
in [31], one can show that after the addition of Algorithm 1 for handling the vari-
able ordering condition, the algorithm for characteristic decomposition remains
correct and to terminate. The method of characteristic decomposition, whose
main steps are outlined above, is described formally as Algorithm 2.
The complexity of Algorithm 2 for characteristic decomposition is not
touched in this paper partially due to the same underlying diﬃculty as in the
complexity analyses of algorithms for triangular decomposition: the very com-
plicated behaviors of splittings.
3.3
Algorithm for Strong Characteristic Decomposition
In [31, Theorem 3.22] it is proved that for any characteristic pair (G, C), one can
explicitly construct a strong characteristic pair ( ¯G, ¯C), where ¯G is the reduced
LEX Gr¨obner basis of sat(C) and ¯C is the W-characteristic set of ¯G. Further-
more, a characteristic decomposition (G1, C1), . . . , (Gt, Ct) of a polynomial set F
can be transformed into a strong one ( ¯G1, ¯C1), . . . , ( ¯Gt, ¯Ct) of F, without further

Decomposing Polynomial Sets
85
Algorithm 2. Ψ := CharPair(F, <) (algorithm for characterstic decompo-
sition)
Input: a ﬁnite, nonempty set F of nonzero polynomials in K[x] and a variable
ordering <.
Output: either a characteristic decomposition Ψ of F such that
Z(F) = ∪(G,C)∈Ψ Z(G) = ∪(G,C)∈Ψ Z(C/ ini(C)),
or the empty set meaning that Z(F) = ∅,
or the message “The variable ordering condition is not satisﬁed.”
Ψ := ∅, Φ := {F};
1
while Φ ̸= ∅do
2
Choose P from Φ and set Φ := Φ \ {P};
3
if VOC(P, <) = “ERROR” then
4
return “ The variable ordering condition is not satisﬁed.”;
5
else
6
((G, C), <′) := VOC(P, <);
7
if G ̸= {1} then
8
if C is normal then
9
Ψ := Ψ ∪{(G, C)};
10
Φ := Φ ∪{G ∪{ini(C)} | ini(C) is the initial of C w.r.t <′,
11
ini(C) ̸∈K, C ∈C};
else
12
C := ﬁrst polynomial in C such that [T ∈C | lv(T) ≤′ lv(C)],
13
ordered as a triangular set, is abnormal;
I := ini(C); y := lv(I);
14
¯C := the polynomial in C whose leading variable is y;
15
if I is not reduced w.r.t. ¯C then
16
Φ := Φ ∪{G ∪{ini(T)} | lv(T) ≤′ y, T ∈C} ∪{G ∪{I}};
17
else
18
Q := pquo( ¯C, I);
19
if prem(ini(Q), [T ∈C | lv(T) <′ y]) = 0 then
20
Φ := Φ∪{G∪{ini(T)} | lv(T) <′ y, T ∈C}∪{G∪{ini(I)}};
21
else
22
Φ := Φ ∪{G ∪{ini(T) | lv(T) <′ y, T ∈C}}
23
∪{G ∪{prem(Q, [T ∈C | lv(T) <′ y])}, G ∪{I}};
return Ψ
24
splitting to induce additional branches. An algorithm for strong characteristic
decomposition based on Algorithm 2 and the above observations is formulated
as Algorithm 3.
Remark 2. Algorithm 2 decomposes any polynomial set not only into normal
triangular sets, but also into reduced LEX Gr¨obner bases at one stroke. These
two diﬀerent objects which have their own structures and properties (see [31,
Sect. 3.2]) are interconnected. This makes our algorithm distinct from other
existing ones for triangular decomposition like RegSer and Triangularize and so

86
R. Dong and C. Mou
Algorithm 3. Σ := SCharPair(F, <) (algorithm for strong characterstic
decomposition)
Σ := ∅;
1
Ψ := CharPair(F, <);
2
for (G, C) ∈Ψ do
3
Compute the reduced LEX Gr¨obner basis ¯G of sat(C);
4
if ⟨¯G⟩= ⟨G⟩then
5
Σ := Σ ∪{(G, C)};
6
else
7
Extract the W-characteristic set ¯C of from ¯G;
8
Σ := Σ ∪{( ¯G, ¯C)};
9
return Σ
10
on. Algorithm 3 which decomposes polynomial sets into characterizable Gr¨obner
bases and normal triangular sets has richer properties (see [31, Sect. 3.3]). In
particular, sat(T ) = ⟨T ⟩holds naturally for any zero-dimensional triangular set
T , and thus in this case a characteristic decomposition is also a strong one.
3.4
An Illustrative Example
Let P = {ax2y +3b2 +a, a(b−c)xy +abx+5c} ⊆K[c, b, a, y, x] with c < b < a <
y < x. Part of computation of strong characteristic decomposition of P with
Algorithm 3 is recorded below to illustrate how Algorithms 1 and 3 in Sect. 3
work.
In a certain step of computation of characteristic decomposition of P, the
polynomial set
P1 ={b(ax2 −3b2 −a), b(y + 1)(3b2 + a), ab(y + 1)(3b2 + a −5x),
a(3b2 + a)b(y + 1), ax2y + 3b2 + a, c}
is chosen, with the reduced LEX Gr¨obner basis of ⟨P1⟩computed as
G1 = {c, 3b3y + aby + 3b3 + ab, b3xy + b3x, abx2 −3b3 −ab, ax2y + 3b2 + a}
and the W-characteristic set as C1 = [c, 3b3y + aby + 3b3 + ab, b3xy + b3x]. One
can check that the W-characteristic set C1 is abnormal, and it does not satisfy
the variable ordering condition w.r.t. c < b < a < y < x.
Then a new variable ordering b < a < c < y < x is chosen, and the reduced
LEX Gr¨obner basis
G′
1 = {c, 3b3y + aby + 3b3 + ab, b3xy + b3x, abx2 −3b3 −ab, ax2y + 3b2 + a}
of P1 is computed and its W-characteristic set C′
1 = [c, 3b3y + aby + 3b3 +
ab, b3xy + b3x] is extracted. At this point one can ﬁnd that C′
1 satisﬁes the
variable ordering condition and is abnormal w.r.t. the new variable ordering.

Decomposing Polynomial Sets
87
With the set of initials of the polynomials C′
11, C′
12, C′
13 w.r.t. b < a < c <
y < x being {b3y + b3, 3b3 + ab}, the polynomial sets P2 = G′
1 ∪{3b3 + ab}
and P3 = G′
1 ∪{b3y + b3} are adjoined to Φ, and the computation continues
w.r.t. the original variable ordering c < b < a < y < x until the characteristic
decomposition of P is computed.
For one characteristic pair (G, C) = ({c, 3b2 + a, bx}, [c, 3b2 + a, bx]) in the
characteristic decomposition, the inequality sat(C) ̸= ⟨G⟩is conﬁrmed. Then the
reduced LEX Gr¨obner basis ¯G = {c, 3b2 +a, x} of sat(C) is computed and its W-
characteristic set ¯C′ = [c, 3b2 + a, x] is extracted, forming a strong characteristic
pair ( ¯G, ¯C) in the strong characteristic decomposition of P.
4
Implementation and Experimental Results
We have implemented Algorithms 1, 2, and 3 in Maple 17 and made exper-
iments with the implementation on an Intel(R) Core(TM) i5-4210U CPU at
2.39 GHz with 8.00 GB RAM under Windows 8. The implementation is based
on the functions for Gr¨obner basis computation available in the FGb package
[11] shipped with Maple 17 and Maple’s built-in packages.
In our implementation polynomial factorization is used heuristically to
enhance the performance of algorithms for triangular decomposition. In our
algorithm, the splitting of a polynomial set P into G ∪{H1}, . . . , G ∪{Hs}
is an essential step. Instead of H1, . . . , Hs, we can use any set of polynomi-
als L1, . . . , Lt which are irreducible factors w.r.t. G such that Z(H1 · · · Hs) =
Z(L1 · · · Lt). In order to further simplify the computation, we can also use
L1
′ = nform(L1, G), . . ., Lt
′ = nform(Lt, G) which are all reduced w.r.t. G
instead of L1, . . . , Lt. The polynomials L1
′, . . . , L′
t are usually smaller in size com-
pared with H1, . . . , Hs. Our experiments show that this simple strategy is rather
eﬀective.
In the zero-dimensional case where the W-characteristic sets do not involve
any parameters, the variable ordering condition, which is required in Theorem 1
to ensure the pseudo-divisibility relationship, holds naturally. However, the con-
dition does not necessarily hold in general in the positive-dimensional case.
Among 35 test examples in our experiments in which the ideals are positive-
dimensional, the variable ordering condition is not satisﬁed for 8 of them (marked
with † in Table 1). We would like to remark that Algorithm 1 succeeds for all
of these 8 test examples with only one time of changing the variable orderings
whenever the failure of satisfaction of the variable ordering condition happens.
The experimental results with our implementation of Algorithm 2 on 50
examples are reported in Table 1. Among these 50 examples, Ex 1–13 are from
the Epsilon package, Ex 14–16 from [28], Ex 17–18 from [1], Ex 19–33 from [23],
Ex 34–36 from [7], Ex 37–47 from the FGb library, and Ex 48–50 can be found
with this link1.
The computational performances of our implementation are compared with
existing implementations of methods for triangular decomposition by means of
1 http://www.liﬂ.fr/∼lemaire/BCLM09/BCLM09-systems.txt.

88
R. Dong and C. Mou
normal decomposition. A normal decomposition is extracted out of the computed
characteristic decomposition for our implementation and for other implementa-
tions it is computed by normalization of regular decomposition: the function
Triangularize (in the RegularChains package [20] shipped in Maple 17) and the
function RegSer (in the Epsilon package [29] for Maple) are used for regular
decomposition, and the function normat (in Epsilon) is used for normalization of
the computed regular sets.
Remark 3. In spite of the comparison made by computing normal decomposition
with diﬀerent implementations, we would like to emphasize that the normal
sets are only part of the computation output of Algorithm 2, for reduced LEX
Gr¨obner bases and normal W-characteristic sets are computed simultaneously
and they enjoy remarkable interconnecting properties.
In Table 1, “Label” indicates the label in the above-cited references and
“Var,” “Eqs,” and “Dim” denote the number of variables, the number of equa-
tions, and the dimension of the ideal in the examples, respectively. “Total” and
“GB” under CharPair record respectively the total time (followed by the num-
ber of pairs in parenthesis) for normal decomposition using Algorithm 2 and the
time for computing all the reduced LEX Gr¨obner bases; “Total” and “Regular”
under “RegSer” and “Triangularize” record the total time for normal decom-
position and the time for regular decomposition (followed by the numbers of
components in parenthesis) respectively. The marks “lost” and “>4000” in the
columns mean that Maple reports “lost kernel connections” (which persists
with several repeated attempts) and that the computation does not terminate
within 4000 seconds respectively.
The experimental results in Table 1 also show that for most of the examples in
which the ideals are zero-dimensional, the number of normal components in the
characteristic decomposition computed by Algorithm 2 is smaller than that in the
normal decomposition computed by RegSer or Triangularize with normalization.
This happens because the initials of polynomials in the normal W-characteristic
sets of zero-dimensional ideals do not involve any variables or parameters such
that no initials cause any splitting and no more polynomial set is adjoined into
Φ in Algorithm 2.
It is reasonable to claim that the built-in implementation in Maple for
algorithms to change the term orderings, especially that for the Gr¨obner walk in
the positive-dimensional case, is the bottleneck of our current implementation in
terms of eﬃciency. With our comparisons between implementations of algorithms
for the Gr¨obner walk in Mathematica and Maple, we predict that the total
time for normal decomposition with our implementation can be greatly reduced
for the positive-dimensional case if the step of Gr¨obner walk is performed with
the corresponding built-in command in Mathematica. For this reason, we are
working on an interface to call the Gr¨obner walk function in Mathematica from
Maple. According to our preliminary experiments, this may bring a speedup of
about 10 times for conversion of Gr¨obner bases in the positive-dimensional case
(the total time decreases from 27.2 s to 2.2 s for Ex 32, for example).

Decomposing Polynomial Sets
89
Table 1. Timings for characteristic decomposition (in second)
Algorithm 2
RegSer
Triangularize
Ex
Label
Var
Eqs
Dim
Total
GB
Total
Regular
Total
Regular
1
E1
10
10
1
0.844(5)
0.736
0.234(2)
0.187(2)
2.187(13)
2.109(13)
2
E5
15
17
4
2.187(7)
2.091
0.453(4)
0.328(4)
6.156(7)
6.078(7)
3
E14
4
3
1
0.656(10)
0.452
1.875(7)
0.125(1)
2.547(7)
0.156(1)
4
E20
4
4
1
0.094(2)
0.079
—
0.016(1)
—
0.031(1)
5
E22
3
3
0
0.046(1)
0.046
0.312(2)
0.250(2)
0.140(3)
0.109(3)
6
E23†
9
5
4
0.469(11)
0.249
—
0.094(8)
—
0.094(1)
7
E28
4
4
0
0.015(1)
0.015
0.109(1)
0.093(1)
0.141(3)
0.125(3)
8
E32†
8
6
2
0.234(6)
0.157
0.110(3)
0.110(3)
—
0.110(1)
9
E33†
13
11
2
20.313(8)
18.924
0.765(5)
0.656(5)
1.687(1)
1.594(1)
10
E35†
8
8
3
0.406(11)
0.234
0.626(5)
0.532(5)
0.781(7)
0.688(7)
11
E40
6
6
0
0.344(1)
0.344
>4000
>4000
>4000
>4000
12
E47
5
4
1
1.234(8)
1.076
47.766(18)
1.297(12)
1.953(8)
0.547(2)
13
E48
7
3
4
0.453(2)
0.407
—
0.031(2)
—
0.015(1)
14
N4
5
3
3
0.094(6)
0.094
—
0.016(4)
—
0.047(3)
15
N6
4
3
2
0.063(2)
0.047
0.016(3)
0.016(3)
—
0.031(3)
16
N20
4
3
2
0.078(3)
0.047
—
<0.01(3)
—
0.016(2)
17
F1
3
2
1
0.047(3)
0.047
—
0.016(3)
—
0.062(2)
18
F2
4
3
1
1.531(13)
1.157
—
0.015(1)
0.281(6)
0.188(5)
19
S1
4
3
2
0.047(3)
0.047
—
0.015(3)
—
0.046(3)
20
S2
4
9
0
0.032(1)
0.032
—
0.015(1)
—
0.031(1)
21
S5
8
4
4
0.187(8)
0.125
3.188(31)
0.344(19)
1.513(9)
0.124(1)
22
S6
4
3
2
0.062(4)
0.062
—
0.016(3)
—
0.046(3)
23
S7
4
3
1
0.281(8)
0.156
0.156(7)
0.047(4)
0.249(5)
0.109(1)
24
S8
4
3
2
0.031(2)
0.015
0.062(3)
0.062(3)
0.156(2)
0.141(2)
25
S9
6
4
2
0.375(12)
0.236
0.483(21)
0.14(8)
0.188(6)
0.094(1)
26
S10
7
4
3
0.594(16)
0.313
0.438(7)
0.172(5)
0.360(3)
0.235(1)
27
S12
8
4
4
0.140(1)
0.140
—
0.016(1)
—
0.047(1)
28
S13†
5
2
3
0.296(10)
0.187
0.171(8)
0.109(8)
0.125(2)
0.094(1)
29
S14
5
3
2
0.203(8)
0.156
0.14(6)
0.109(6)
0.157(8)
0.125(8)
30
S15
12
7
5
0.344(1)
0.344
—
0.031(1)
—
0.093(1)
31
S16
16
14
3
0.640(6)
0.344
0.703(7)
0.609(7)
—
4.609(8)
32
S17
8
3
6
lost
lost
lost
lost
lost
lost
33
S18
5
3
3
lost
lost
>4000
>4000
2058.235(56)
2.641(10)
34
maclane†
10
6
5
476.766(335)
433.867
>4000
>4000
10.937(21)
2.969(11)
35
nueral
4
3
1
1.844(13)
1.611
>4000
>4000
0.250(6)
0.125(5)
36
Leykin 1†
8
6
4
184.890(116)
169.983
>4000
>4000
5.625(15)
5.578(15)
37
Rose
3
3
0
0.859(1)
0.859
1.872(1)
1.607(1)
1.125(2)
0.875(2)
38
F663†
10
9
2
2.969(6)
2.328
1.935(16)
1.202(15)
1.607(6)
1.045(4)
39
Dessin2
10
10
0
26.718(1)
26.718
>4000
>4000
>4000
>4000
40
Liu
6
4
2
203.125(26)
192.531
>4000
>4000
18.907(20)
0.469(8)
41
Wang16
4
4
0
0.125(1)
0.109
14.555(1)
0.437(1)
16.047(1)
0.172(1)
42
Cyclic5
5
5
0
0.453(11)
0.281
lost
lost
1.906(15)
1.531(15)
43
lichtblau
3
2
1
lost
lost
>4000
>4000
>4000
0.109(1)
44
ﬁlter9
9
9
0
0.515(1)
0.515
>4000
>4000
lost
lost
45
fabrice24
9
9
0
436.7(1)
436.7
lost
lost
lost
lost
46
uteshev b
4
4
0
3.438(1)
3.438
lost
lost
>4000
>4000
47
cyclic6
6
6
0
1.922(25)
1.095
lost
lost
>4000
>4000
48
4-body-h
3
3
0
5.953(3)
5.531
655.406(2)
15.922(2)
439.547(5)
0.359(5)
49
5-body-h
3
3
0
12.297(3)
11.250
1990.656(2)
107.891(2)
1560.984(5)
0.437(5)
50
fabfaux
3
3
0
0.219(1)
0.219
14.016(1)
0.188(1)
54.000(1)
0.172(1)

90
R. Dong and C. Mou
Table 2. Timings for strong characteristic decomposition (in second)
Ex Label
Total
Transform Branches
4
E20
0.125
0.078
2
6
E23
0.953
0.547
10
9
E33
>4000 —
—
10
E35
0.828
0.422
10
12
E47
>4000 —
—
14
N4
0.281
0.141
6
15
N6
0.297
0.235
2
16
N20
0.171
0.062
3
17
F1
0.219
0.125
3
18
F2
2.985
1.469
13
19
S1
0.140
0.093
3
21
S5
0.578
0.422
8
22
S6
0.266
0.188
4
24
S8
0.078
0.047
2
25
S9
0.735
0.391
12
28
S13
24.937 24.672
10
29
S14
35.437 35.219
8
35
Neural 3.094
1.516
13
38
f663
>4000 —
—
The experimental results of our implementation of Algorithm 3 are presented
in Table 2 for 19 test examples selected from those in Table 1. In Table 2, the
columns “Total” and “Transform” record the total time for computing strong
characteristic decomposition using Algorithm 3 and the time for transforming
a characteristic decomposition into a strong one, and “Branches” denotes the
number of branches in the strong characteristic pairs computed.
As one ﬁnds from Algorithm 3, there is no more splitting in the transforma-
tion from a characteristic decomposition into a strong one, but in practice the
number of branches in the characteristic decomposition may be strictly greater
than that in the strong one, as the example E23 in Tables 1 and 2 shows. In
the computed characteristic decomposition for the example E23, there are two
diﬀerent characteristic pairs
({a, c −1, bd, −b2 + r2, −d2 + s2, −b2 −d2 + t2 −1, bx, dx −d},
[a, c −1, bd, −b2 + r2, −d2 + s2, −b2 −d2 + t2 −1, bx])
and
({a, bc −b, bd, −b2 + r2, −c2−d2+s2+2c−1, −b2−c2−d2 + t2, bx, −cy + dx
−d + y}, [a, bc−b, bd, −b2+r2, −c2−d2+s2+2c−1, −b2−c2−d2+t2, bx])

Decomposing Polynomial Sets
91
which lead to the same strong characteristic pair
({a, c−1, d, −b2 +r2, s2, −b2 +t2 −1, x}, [a, c−1, d, −b2 +r2, s2, −b2 +t2 −1, x])
after the transformation.
Acknowledgements. The authors would like to thank the reviewers for their detailed
comments which have led to eﬀective improvements on this paper.
References
1. Alvandi, P., Chen, C., Marcus, S., Moreno Maza, M., Schost, ´E., Vrbik, P.: Doing
algebraic geometry with the RegularChains library. In: Hong, H., Yap, C. (eds.)
ICMS 2014. LNCS, vol. 8592, pp. 472–479. Springer, Heidelberg (2014). doi:10.
1007/978-3-662-44199-2 71
2. Aubry, P., Lazard, D., Moreno Maza, M.: On the theories of triangular sets. J.
Symb. Comput. 28(1–2), 105–124 (1999)
3. Aubry, P., Moreno Maza, M.: Triangular sets for solving polynomial systems: a
comparative implementation of four methods. J. Symb. Comput. 28(1), 125–154
(1999)
4. B¨achler, T., Gerdt, V., Lange-Hegermann, M., Robertz, D.: Algorithmic Thomas
decomposition of algebraic and diﬀerential systems. J. Symb. Comput. 47(10),
1233–1266 (2012)
5. Becker, T., Weispfenning, V., Kredel, H.: Gr¨obner Bases: A Computational
Approach to Commutative Algebra. Graduate Texts in Mathematics. Springer,
New York (1993)
6. Buchberger, B.: Ein Algorithmus zum Auﬃnden der Basiselemente des Restk-
lassenrings nach einem nulldimensionalen Polynomideal. Ph.D. thesis, Universit¨at
Innsbruck, Austria (1965)
7. Chen, C., Golubitsky, O., Lemaire, F., Moreno Maza, M., Pan, W.: Comprehensive
triangular decomposition. In: Ganzha, V.G., Mayr, E.W., Vorozhtsov, E.V. (eds.)
CASC 2007. LNCS, vol. 4770, pp. 73–101. Springer, Heidelberg (2007). doi:10.
1007/978-3-540-75187-8 7
8. Chen, C., Moreno Maza, M.: Algorithms for computing triangular decompositions
of polynomial systems. J. Symb. Comput. 47(6), 610–642 (2012)
9. Cox, D., Little, J., O’Shea, D.: Ideals, Varieties, and Algorithms: An Introduction
to Computational Algebraic Geometry and Commutative Algebra. Undergraduate
Texts in Mathematics. Springer, New York (1997)
10. Dahan, X.: On lexicographic Gr¨obner bases of radical ideals in dimension zero:
interpolation and structure. Preprint at arXiv:1207.3887 (2012)
11. Faug`ere, J.-C.: FGb: a library for computing Gr¨obner bases. In: Fukuda, K.,
Hoeven, J., Joswig, M., Takayama, N. (eds.) ICMS 2010. LNCS, vol. 6327, pp.
84–87. Springer, Heidelberg (2010). doi:10.1007/978-3-642-15582-6 17
12. Faug`ere, J.-C., Gianni, P., Lazard, D., Mora, T.: Eﬃcient computation of zero-
dimensional Gr¨obner bases by change of ordering. J. Symb. Comput. 16(4), 329–
344 (1993)
13. Gao, S., Volny, F., Wang, M.: A new framework for computing Gr¨obner bases.
Math. Comput. 85(297), 449–465 (2016)

92
R. Dong and C. Mou
14. Gao, X.-S., Chou, S.-C.: Solving parametric algebraic systems. In: Proceedings of
ISSAC 1992, pp. 335–341. ACM Press (1992)
15. Gianni, P., Trager, B., Zacharias, G.: Gr¨obner bases and primary decomposition
of polynomial ideals. J. Symb. Comput. 6(2), 149–167 (1988)
16. Kalkbrenner, M.: A generalized Euclidean algorithm for computing triangular rep-
resentations of algebraic varieties. J. Symb. Comput. 15(2), 143–167 (1993)
17. Lazard, D.: Ideal bases and primary decomposition: case of two variables. J. Symb.
Comput. 1(3), 261–270 (1985)
18. Lazard, D.: A new method for solving algebraic systems of positive dimension.
Discrete Appl. Math. 33(1–3), 147–160 (1991)
19. Lazard, D.: Solving zero-dimensional algebraic systems. J. Symb. Comput. 13(2),
117–131 (1992)
20. Lemaire, F., Moreno Maza, M., Xie, Y.: The RegularChains library in Maple 10.
In: Kotsireas, I. (ed.) Maple Conference 2005, pp. 355–368. Maplesoft, Waterloo
(2005)
21. Li, B., Wang, D.: An algorithm for transforming regular chain into normal chain. In:
Kapur, D. (ed.) ASCM 2007. LNCS, vol. 5081, pp. 236–245. Springer, Heidelberg
(2008). doi:10.1007/978-3-540-87827-8 20
22. Marinari, M.G., Mora, T.: A remark on a remark by Macaulay or enhancing Lazard
structural theorem. Bull. Iran. Math. Soc. 29(1), 1–45 (2003)
23. Mou, C., Wang, D., Li, X.: Decomposing polynomial sets into simple sets over ﬁnite
ﬁelds: the positive-dimensional case. Theoret. Comput. Sci. 468, 102–113 (2013)
24. Ritt, J.F.: Diﬀerential Algebra. American Mathematical Society, New York (1950)
25. Shimoyama, T., Yokoyama, K.: Localization and primary decomposition of poly-
nomial ideals. J. Symb. Comput. 22(3), 247–277 (1996)
26. Wang, D.: Decomposing polynomial systems into simple systems. J. Symb. Com-
put. 25(3), 295–314 (1998)
27. Wang, D.: Computing triangular systems and regular systems. J. Symb. Comput.
30(2), 221–236 (2000)
28. Wang, D.: Elimination Methods. Springer, Wien (2001)
29. Wang, D.: Elimination Practice: Software Tools and Applications. Imperial College
Press, London (2004)
30. Wang, D.: On the connection between Ritt characteristic sets and Buchberger-
Gr¨obner bases. Math. Comput. Sci. 10, 479–492 (2016)
31. Wang, D., Dong, R., Mou, C.: Decomposition of polynomial sets into characteristic
pairs. arXiv:1702.08664 (2017)
32. Wang, D., Zhang, Y.: An algorithm for decomposing a polynomial system into
normal ascending sets. Sci. China Ser. A 50(10), 1441–1450 (2007)
33. Weispfenning, V.: Comprehensive Gr¨obner bases. J. Symb. Comput. 14(1), 1–29
(1992)
34. Wu, W.-T.: Basic principles of mechanical theorem proving in elementary geome-
tries. J. Autom. Reason. 2(3), 221–252 (1986)

Symbolic Versus Numerical Computation
and Visualization of Parameter Regions
for Multistationarity of Biological Networks
Matthew England1
, Hassan Errami2, Dima Grigoriev3, Ovidiu Radulescu4
,
Thomas Sturm5,6(B)
, and Andreas Weber2
1 Fac. Engineering, Environment & Computing, Coventry University, Coventry, UK
Matthew.England@coventry.ac.uk
2 Institut f¨ur Informatik II, Universit¨at Bonn, Bonn, Germany
{errami,weber}@cs.uni-bonn.de
3 CNRS, Math´ematiques, Universit´e de Lille, Villeneuve d’Ascq, France
Dmitry.Grigoryev@math.univ-lille1.fr
4 DIMNP UMR CNRS/UM 5235, University of Montpellier, Montpellier, France
ovidiu.radulescu@umontpellier.fr
5 University of Lorraine, CNRS, Inria, and LORIA, Nancy, France
thomas.sturm@loria.fr
6 MPI Informatics and Saarland University, Saarbr¨ucken, Germany
sturm@mpi-inf.mpg.de
Abstract. We investigate models of the mitogenactivated protein
kinases (MAPK) network, with the aim of determining where in para-
meter space there exist multiple positive steady states. We build on
recent progress which combines various symbolic computation methods
for mixed systems of equalities and inequalities. We demonstrate that
those techniques beneﬁt tremendously from a newly implemented graph
theoretical symbolic preprocessing method. We compare computation
times and quality of results of numerical continuation methods with our
symbolic approach before and after the application of our preprocessing.
1
Introduction
The mathematical modelling of intra-cellular biological processes has been using
nonlinear ordinary diﬀerential equations since the early ages of mathematical bio-
physics in the 1940s and 50s [28]. A standard modelling choice for cellular circuitry
is to use chemical reactions with mass action law kinetics, leading to polyno-
mial diﬀerential equations. Rational functions kinetics (for instance the Michaelis-
Menten kinetics) can generally be decomposed into several mass action steps. An
important property of biological systems is their multistationarity which means
having multiple stable steady states. Multistationarity is instrumental to cellular
memory and cell diﬀerentiation during development or regeneration of multicel-
lular organisms and is also used by micro-organisms in survival strategies. It is
thus important to determine the parameter values for which a biochemical model
is multistationary. With mass action reactions, testing for multiple steady states
boils down to counting real positive solutions of algebraic systems.
c
⃝The Author(s) 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 93–108, 2017.
DOI: 10.1007/978-3-319-66320-3 8

94
M. England et al.
The models benchmarked in this paper concern intracellular signaling path-
ways. These pathways transmit information about the cell environment by induc-
ing cascades of protein modiﬁcations (phosphorylation) all the way from the
plasma membrane via the cytosol to genes in the cell nucleus. Multistation-
arity of signaling usually occurs as a result of activation of upstream signal-
ing proteins by downstream components [2]. A diﬀerent mechanism for produc-
ing multistationarity in signaling pathways was proposed by Kholodenko [26].
In this mechanism the cause of multistationarity are multiple phosphoryla-
tion/dephosphorylation cycles that share enzymes. A simple, two steps phos-
phorylation/dephosphorylation cycle is capable of ultrasensitivity, a form of all
or nothing response with no multiple steady states (Goldbeter–Koshland mech-
anism). In multiple phosphorylation/dephosphorylation cycles, enzyme sharing
provides competitive interactions and positive feedback that ultimately leads to
multistationarity [23,26].
Our study is complementary to works applying numerical methods to ordi-
nary diﬀerential equations models used for biology applications. Gross et al. [18]
used polynomial homotopy continuation methods for global parameter estima-
tion of mass action models. Bifurcations and multistationarity of signaling cas-
cades was studied with numerical methods based on the Jacobian matrix [30].
Other symbolic approaches to multistationarity either propose necessary condi-
tions or work for particular networks [8,9,20,27].
Our work here follows [5], where it was demonstrated that determination
of multistationarity of an 11-dimensional model of a mitogen-activated pro-
tein kinases (MAPK) cascade can be achieved by currently available symbolic
methods when numeric values are known for all but potentially one parameter.
We show that the symbolic methods used in [5], viz. real triangularization and
cylindrical algebraic decomposition, and also polynomial homotopy continuation
methods, beneﬁt tremendously from a graph theoretical symbolic preprocessing
method. This method has been sketched by Grigoriev et al. [17] and has been
used for a “hand computation,” but had not been implemented before. For our
experiments we use the model already investigated in [5] and a higher dimen-
sional model of the MAPK cascade.
2
The Systems for the Case Studies
For our investigations we use models of the MAPK cascade that can be found in
the Biomodels database1 as numbers 26 and 28 [24]. We refer to those models
as Biomod-26 and Biomod-28, respectively.
2.1
Biomod-26
Biomod-26, which we have studied also in [5], is given by the following set of
diﬀerential equations. We have renamed the species names as x1, . . . , x11 and
the rate constants as k1, . . . , k16 to facilitate reading:
1 http://www.ebi.ac.uk/biomodels-main/

Symbolic Versus Numerical Computation for Biological Networks
95
˙x1 = k2x6 + k15x11 −k1x1x4 −k16x1x5
˙x2 = k3x6 + k5x7 + k10x9 + k13x10 −x2x5(k11 + k12) −k4x2x4
˙x3 = k6x7 + k8x8 −k7x3x5
˙x4 = x6(k2 + k3) + x7(k5 + k6) −k1x1x4 −k4x2x4
˙x5 = k8x8 + k10x9 + k13x10 + k15x11 −x2x5(k11 + k12) −k7x3x5 −k16x1x5
˙x6 = k1x1x4 −x6(k2 + k3)
˙x7 = k4x2x4 −x7(k5 + k6)
˙x8 = k7x3x5 −x8(k8 + k9)
˙x9 = k9x8 −k10x9 + k11x2x5
˙x10 = k12x2x5 −x10(k13 + k14)
˙x11 = k14x10 −k15x11 + k16x1x5
(1)
The Biomodels database also gives us meaningful values for the rate constants,
which we generally substitute into the corresponding systems for our purposes
here:
k1 = 0.02,
k2 = 1,
k3 = 0.01,
k4 = 0.032,
k5 = 1,
k6 = 15,
k7 = 0.045,
k8 = 1,
k9 = 0.092,
k10 = 1,
k11 = 0.01,
k12 = 0.01,
k13 = 1,
k14 = 0.5,
k15 = 0.086,
k16 = 0.0011.
(2)
Using the left-null space of the stoichiometric matrix under positive conditions
as a conservation constraint [14] we obtain three linear conservation laws:
x5 + x8 + x9 + x10 + x11 = k17,
x4 + x6 + x7 = k18,
x1 + x2 + x3 + x6 + x7 + x8 + x9 + x10 + x11 = k19,
(3)
where k17, k18, k19 are new constants computed from the initial data. Those
constants are the parameters that we are interested in here.
The steady state problem for the MAPK cascade can now be formulated
as a real algebraic problem as follows. We replace the left hand sides of all
equations in (1) with 0 and substitute the values from (2). This together with
(3) yields a system of parametric polynomial equations with polynomials in
Z[k17, k18, k19][x1, . . . , x11]. Since all entities in our model are strictly positive,
we add to our system positivity conditions k17 > 0, k18 > 0, k19 > 0 and x1 > 0,
. . . , x11 > 0. In terms of ﬁrst-order logic the conjunction over our equations and
inequalities yields a quantiﬁer-free Tarski formula.
2.2
Biomod-28
The system with number 28 in the Biomodels database is given by the following
set of diﬀerential equations. Again, we have renamed the species names into
x1, . . . , x16 and the rate constants into k1, . . . , k27 to facilitate reading:

96
M. England et al.
˙x1 = k2x9 + k8x10 + k21x15 + k26x16 −k1x1x5 −k7x1x5 −k22x1x6 −k27x1x6
˙x2 = k3x9 + k5x7 + k24x12 −k4x2x5 −k23x2x6
˙x3 = k9x10 + k11x8 + k16x13 + k19x14 −k10x3x5 −k17x3x6 −k18x3x6
˙x4 = k6x7 + k12x8 + k14x11 −k13x4x6
˙x5 = k2x9 + k3x9 + k5x7 + k6x7 + k8x10 + k9x10 + k11x8 + k12x8 −
k1x1x5 −k4x2x5 −k7x1x5 −k10x3x5
˙x6 = k14x11 + k16x13 + k19x14 + k21x15 + k24x12 + k26x16 −
k13x4x6 −k17x3x6 −k18x3x6 −k22x1x6 −k23x2x6 −k27x1x6
˙x7 = k4x2x5 −k6x7 −k5x7
˙x8 = k10x3x5 −k12x8 −k11x8
˙x9 = k1x1x5 −k3x9 −k2x9
˙x10 = k7x1x5 −k9x10 −k8x10
˙x11 = k13x4x6 −k15x11 −k14x11
˙x12 = k23x2x6 −k25x12 −k24x12
˙x13 = k15x11 −k16x13 + k17x3x6
˙x14 = k18x3x6 −k20x14 −k19x14
˙x15 = k20x14 −k21x15 + k22x1x6
˙x16 = k25x12 −k26x16 + k27x1x6
The estimates of the rate constants given in the Biomodels database are:
k1 = 0.005,
k2 = 1,
k3 = 1.08,
k4 = 0.025,
k5 = 1,
k6 = 0.007,
k7 = 0.05,
k8 = 1,
k9 = 0.008,
k10 = 0.005,
k11 = 1,
k12 = 0.45,
k13 = 0.045,
k14 = 1,
k15 = 0.092,
k16 = 1,
k17 = 0.01,
k18 = 0.01,
k19 = 1,
k20 = 0.5,
k21 = 0.086,
k22 = 0.0011,
k23 = 0.01,
k24 = 1,
k25 = 0.47,
k26 = 0.14,
k27 = 0.0018.
Again, using the left-null space of the stoichiometric matrix under positive con-
ditions as a conservation constraint [14] we obtain the following:
x6 + x11 + x12 + x13 + x14 + x15 + x16 = k28,
x5 + x7 + x8 + x9 + x10 = k29,
x1 + x2 + x3 + x4 + x7 + x8 + x9 + x10 + x11 +
x12 + x13 + x14 + x15 + x16 = k30,
where k28, k29, k30 are new constants computed from the initial data. We formu-
late the real algebraic problem as described at the end of Sect. 2.1. In particular,
note that we need positivity conditions for all variables and parameters.

Symbolic Versus Numerical Computation for Biological Networks
97
3
Graph-Theoretical Symbolic Preprocessing
The complexity, primarily in terms of dimension, of polynomial systems obtained
with steady-state approximations of biological models plus conservation laws is
comparatively high for the application of symbolic methods. It is therefore highly
relevant for the success of such methods to identify and exploit particular struc-
tural properties of the input. Our models have remarkably low total degrees
with many linear monomials after some substitutions for rate constants. This
suggests to preprocess with essentially Gaussian elimination in the sense of solv-
ing single suitable equations with respect to some variable and substituting the
corresponding solution into the system.
Generalizing this idea to situations where linear variables have parametric
coeﬃcients in the other variables requires, in general, a parametric variant of
Gaussian elimination, which replaces the input system with a ﬁnite case distinc-
tion with respect to the vanishing of certain coeﬃcients and one reduced system
for each case. With Biomod-26 and Biomod-28 considered here it turns out
that the positivity assumptions on the variables are strong enough to eﬀectively
guarantee the non-vanishing of all relevant coeﬃcients so that case distinctions
are never necessary. On the other hand, those positivity conditions establish
an apparent obstacle, because we are formally not dealing with a parametric
system of linear equations but with a parametric linear programming problem.
However, here the theory of real quantiﬁer elimination by virtual substitution
tells us that it is suﬃcient that the inequality constraints play a passive role.
Those constraints must be considered when substituting Gauss solutions from
the equations, but otherwise can be ignored [22,25].
Parametric Gaussian elimination can increase the degrees of variables in the
parametric coeﬃcient, in particular destroying their linearity and suitability to
be used for further reductions. As an example consider the steady-state approx-
imation, i.e., all left hand sides replaced with 0, of the system in (1), solving the
last equation for x5, and substituting into the ﬁrst equation. The natural ques-
tion for an optimal strategy to Gauss-eliminate a maximal number of variables
has been answered positively only recently [17]: draw a graph, where vertices
are variables and edges indicate multiplication between variables within some
monomial. Then one can Gauss-eliminate a maximum independent set, which
is the complement of a minimum vertex cover. Figure 1 shows that graph for
Biomod-26, where {x4, x5} is a minimal vertex cover, and all other variables can
be linearly eliminated. Similarly, for Biomod-28 we ﬁnd {x5, x6} as a minimum
vertex cover. Recall that minimum vertex cover is one of Karp’s 21 classical NP
complete problems [21]. However, our instances considered here and instances
to be expected from other biological models are so small that the use of existing
approximation algorithms [16] appears unnecessary. We have used real quantiﬁer
elimination, which did not consume measurable CPU time; alternatively one
could use integer linear programming or SAT-solving.
It is a most remarkable fact that a signiﬁcant number of biological mod-
els in the databases have that property of loosely connected variables. This
phenomenon resembles the well-known community structure of propositional

98
M. England et al.
Fig. 1. The graph for Biomod-26 is loosely connected. Its minimum vertex cover
{x4, x5} is small. All other variables form a maximum independent set, which can
be eliminated with linear methods.
satisﬁability problems, which has been identiﬁed as one of the key structural rea-
sons for the impressive success of state-of-the-art CDCL-based SAT solvers [15].
We conclude this section with the reduced systems as computed with our
implementation in Redlog [11]. For Biomod-26 we obtain x5 > 0, x4 > 0, k19 > 0,
k18 > 0, k17 > 0 and
1062444k18x2
4x5 + 23478000k18x2
4 + 1153450k18x4x2
5 + 2967000k18x4x5
+ 638825k18x3
5 + 49944500k18x2
5 −5934k19x2
4x5 −989000k19x4x2
5
−1062444x3
4x5 −23478000x3
4 −1153450x2
4x2
5 −2967000x2
4x5
−638825x4x3
5 −49944500x4x2
5 = 0,
1062444k17x2
4x5 + 23478000k17x2
4 + 1153450k17x4x2
5 + 2967000k17x4x5
+ 638825k17x3
5 + 49944500k17x2
5 −1056510k19x2
4x5 −164450k19x4x2
5
−638825k19x3
5 −1062444x2
4x2
5 −23478000x2
4x5 −1153450x4x3
5
−2967000x4x2
5 −638825x4
5 −49944500x3
5 = 0.
For Biomod-28 we obtain x6 > 0, x5 > 0, k30 > 0, k29 > 0, k28 > 0 and
3796549898085k29x3
5x6 + 71063292573000k29x3
5 + 106615407090630k29x2
5x2
6
+ 479383905861000k29x2
5x6 + 299076127852260k29x5x3
6
+ 3505609439955600k29x5x2
6 + 91244417457024k29x4
6
+ 3557586742819200k29x3
6 −598701732300k30x3
5x6
−83232870778950k30x2
5x2
6 −185019487578700k30x5x3
6
−3796549898085x4
5x6 −71063292573000x4
5 −106615407090630x3
5x2
6
−479383905861000x3
5x6 −299076127852260x2
5x3
6 −3505609439955600x2
5x2
6
−91244417457024x5x4
6 −3557586742819200x5x3
6 = 0,
3796549898085k28x3
5x6 + 71063292573000k28x3
5 + 106615407090630k28x2
5x2
6
+ 479383905861000k28x2
5x6 + 299076127852260k28x5x3
6
+ 3505609439955600k28x5x2
6 + 91244417457024k28x4
6
+ 3557586742819200k28x3
6 −3197848165785k30x3
5x6
−23382536311680k30x2
5x2
6 −114056640273560k30x5x3
6
−91244417457024k30x4
6 −3796549898085x3
5x2
6 −71063292573000x3
5x6
−106615407090630x2
5x3
6 −479383905861000x2
5x2
6 −299076127852260x5x4
6
−3505609439955600x5x3
6 −91244417457024x5
6 −3557586742819200x4
6 = 0.

Symbolic Versus Numerical Computation for Biological Networks
99
Notice that no complex positivity constraints come into existence with these
examples. All corresponding substitution results are entailed by the other con-
straints, which is implicitly discovered by using the standard simpliﬁer from [12]
during preprocessing.
4
Determination of Multiple Steady States
We aim to identify via grid sampling regions of parameter space where multi-
stationarity occurs. Our focus is on the identiﬁcation of regions with multiple
positive real solutions for the parameters introduced with the conservation laws.
We will encounter one or three such solutions and allow ourselves for biologi-
cal reasons to assume monostability or bistability, respectively. Furthermore, a
change in the number of solutions between one and three is indicative of a saddle-
node bifurcation between a monostable and a bistable case. A mathematically
rigorous treatment of stability would, possibly symbolically, analyze the eigen-
values of the Jacobian of the respective polynomial vector ﬁeld. We consider two
diﬀerent approaches: ﬁrst a polynomial homotopy continuation method imple-
mented in Bertini, and second a combination of symbolic computation methods
implemented in Maple. We compare the approaches with respect to performance
and quality of results for both the reduced and the unreduced systems.
4.1
Numerical Approach
We use the homotopy solver Bertini [1] in its standard conﬁguration to compute
complex roots. We parse the output of Bertini using Python, and determine
numerically, which of the complex roots are real and positive using a threshold
of 10−6 for positivity. Computations are done in Python with Bertini embedded.
For System Biomod-26 we produced the two plots in Fig. 2 using the original
system and the two in Fig. 3 using the reduced system. The sampling range for
k19 was from 200 to 1000 by 50. In the left plots the sampling range for k17
is from 80 to 200 by 10 with k18 ﬁxed at 50. In the right plots the sampling
range for k18 is 5 to 75 by 5 with k17 ﬁxed to 100. We see two regions forming
according to the number of ﬁxed points: yellow discs indicate one ﬁxed point
and blue boxes three. The diamonds indicate numerical errors where zero (red)
or two (green) ﬁxed states were identiﬁed. We analyse these further in Sect. 4.3.
For Biomod-28 we produced the two plots in Fig. 5 using the original system.
The sampling range for k30 was from 100 to 1600 by 100. In the left plots the
sampling range for k28 is from 40 to 160 by 10 with k29 ﬁxed at 180. In the right
plots the sampling range for k29 is from 120 to 240 by 10 with k28 ﬁxed to 100.
The colours and shapes indicate the number of ﬁxed points as before. For the
reduced system Bertini (wrongly) could not ﬁnd any roots (not even complex
ones) for any of the parameter settings. The situation did not change when
going from adaptive precision to a very high ﬁxed precision. However, we have
not attempted more sophisticated techniques like providing user homotopies. We
analyse these results further in Sect. 4.3.

100
M. England et al.
4.2
Symbolic Approach
Our next approach will still use grid sampling, but each sample point will
undergo a symbolic computation. The result will still be an approximate identi-
ﬁcation of the region (since the sampling will be ﬁnite) but the results at those
sample points will be guaranteed free of numerical errors. The computations
follow the strategy introduced in [5, Sect. 2.1.2]. This combined tools from the
Regular Chains Library2 available for use in Maple. Regular chains are the tri-
angular decompositions of systems of polynomial equations (triangular in terms
of the variables in each polynomial). Highly eﬃcient methods for working in
complex space have been developed based on these (see [29] for a survey).
We make use of recent work by Chen et al. [6] which adapts these tools to the
real analogue: semi-algebraic systems. They describe algorithms to decompose
any real polynomial system into ﬁnitely many regular semi-algebraic systems:
both directly and by computation of components by dimension. The latter (the
so called lazy variant) was key to solving the 1-parameter MAPK problem in
[5]. However, for the zero dimensional computations of this paper there is only
one solution component and so no savings from lazy computations.
For a given system and sample point we apply the real triangularization (RT)
on the quantiﬁer-free formula (as described at the end of Sect. 2.1: a quantiﬁer
free conjunction of equalities and inequalities) evaluated with the parameter
estimates and sample point values. This produces a simpliﬁed system in several
senses. First, as guaranteed by the algorithm, the output is triangular according
to a variable ordering. So there is a univariate component, then a bivariate
component introducing one more variable and so on. Secondly, for all the MAPK
models we have studied so far, all but the ﬁnal (univariate) of these equations
has been linear in its main variable. This thus allows for easy back substitution.
Thirdly, most of the positivity conditions are implied by the output rather than
being an explicit part of it, in which case a simpler sub-system can be solved
and back substitution performed instantly.
Biomod-26. For the original version of Biomod-26 the output of RT was a
component consisting of 11 equations and a single inequality. The equations
were in ascending main variable according to the provided ordering (same as the
labelling). All but the ﬁnal equation is linear in its main variable, with the ﬁnal
equation being univariate and degree 6 in x1. The output of the triangularization
requires that this variable be positive, x1 > 0, with the positivity of the other
variables implied by solutions to the system. So to proceed we must ﬁnd the
positive real roots of the degree 8 univariate polynomial in x1: counting these
will imply the number of real positive solutions of the parent system. We do
this using the root isolation tools in the Regular Chains Library. This whole
process was performed iteratively for the same sampling regime as Bertini used
to produce Fig. 4.
We repeated the process on the reduced version of the system. The trian-
gularization again reduced the problem to univariate real root isolation, this
2 http://www.regularchains.org/

Symbolic Versus Numerical Computation for Biological Networks
101
time with only one back substitution step needed. As to be expected from a
fully symbolic computation, the output is identical and so again represented
by Fig. 4. However, the computation was signiﬁcantly quicker with this reduced
system. More details are given in the comparison in Sect. 4.3.
Biomod-28. The same process was conducted on Biomod-28. As with Biomod-
26 the system was triangular with all but the ﬁnal equation linear in its main
variable; this time the ﬁnal equation is degree 8. However, unlike Biomod-26
two positivity conditions were returned in the output meaning we must solve a
bivariate problem before we can back substitute to the full system. Rather than
just perform univariate real root isolation we must build a Cylindrical Algebraic
Decomposition (CAD) (see, e.g., [4] and the references within) sign invariant for
the ﬁnal two equations and interrogate its cells to ﬁnd those where the equations
are satisﬁed and variable positive. Counting these we ﬁnd always 1 or 3 cells,
with the latter indicating bistability. This is similar to the approach used in [5],
although in that case the 2D CAD was for one variable and one parameter. We
used the implementation of CAD in the Regular Chains Library [3,7] with the
results producing the plots in Fig. 6.
For the reduced system we proceeded similarly. A 2D CAD still needed to be
produced after triangularization and so in this case there was no reduction in the
number of equations to study with CAD via back substitution. However, it was
still beneﬁcial to pre-process CAD with real triangularization: the average time
per sample point with pre-processing (and including time taken to pre-process)
was 0.485 s while without it was 3.577 s.
4.3
Comparison
Figures 2, 3, and 4 all refer to Biomod-26. The latter, produced using the sym-
bolic techniques in Maple, is guaranteed free of numerical error. We see that
Fig. 2. Bertini grid sampling on the original version of Biomod-26 (see Sect. 4.1). The
online version of this article contains colored ﬁgures

102
M. England et al.
Fig. 3. Bertini grid sampling on the reduced version of Biomod-26 (see Sect. 4.1)
Fig. 4. Maple grid sampling on Biomod-26 (see Sect. 4.2)
computing with the reduced system rather than the original system allowed
Bertini to avoid such errors: the rouge red and green diamonds in Fig. 2. However,
in the case of Biomod-28 the reduction led to catastrophic eﬀects for Bertini:
built-in heuristics quickly (and wrongly) concluded that there are no zero dimen-
sional solutions for the system, and when switching to a positive dimensional run
also no solutions could be found.
Bertini computations (v1.5.1) were carried out on a Linux 64 bit Desktop
PC with Intel i7. Maple computations (v2016 with April 2017 Regular Chains)
were carried out on a Windows 7 64 bit Desktop PC with Intel i5.
For Biomod-26 the pairs of plots together contain 476 sample points. Table 1
shows timing data. We see that both Bertini and Maple beneﬁted from the
reduced system: Bertini took a third of the original time while the speedup for
Maple was even greater: a tenth of the original. Also, perhaps surprisingly, the

Symbolic Versus Numerical Computation for Biological Networks
103
Fig. 5. Bertini grid sampling on the original version of Biomod-28 (see Sect. 4.1)
Fig. 6. Maple grid sampling on Biomod-28 (see Sect. 4.2)
Table 1. Timing data (in seconds) of the grid samplings described in Sect. 4. Numerical
computation is using Bertini; Symbolic computation is using Maple Regular Chains
Numerical Symbolic
Mean
Mean
Median StdDev Maximum
026 – Original
2.4
0.568
0.530
0.107
0.905
026 – Reduced 0.85
0.053
0.047
0.036
0.343
028 – Original
16.57
42.430 40.529
8.632
84.116
028 – Reduced ⊥
0.485
0.468
0.119
0.796

104
M. England et al.
symbolic methods were quicker than the numerical ones here. For Biomod-28 the
speed-up enjoyed by the symbolic methods was even greater (almost 100 fold).
However, for this system Bertini was signiﬁcantly faster. The symbolic methods
used are well known for their doubly exponential computational complexity (in
the number of variables) so it is not surprising that as the system size increases
there so should the results of the comparison. We see some other statistical data
for the timings in Maple: the standard deviation for the timings is fairly modest
but in each row we see there are outliers many multiples of the mean value and
so the median is always a little less than the mean average.
4.4
Going Further
Of course, we could increase the sampling density to get an improved idea of
the bistability region, as in Figs. 7 and 8. However, a greater understanding
comes with 3D sampling. We have performed this using the symbolic approach
described above, at a linear cost proportional to the increased number of sample
points. This was completed for Biomod-26: the region in question is bounded to
both sides in the k17 and k18 directions but extends inﬁnitely above in k19. With
the k19 range bound at 1000 the region is bounded by extending k17 to 800 and
k18 to 600. For obtaining exact bounds (in one parameter) see [5].
Sampling in 20 s for k17 and k18 and 50 s for k19 produced a Maple point plot
of 20400 in 18 min. Figure 9 shows 2D captures of the 3D bistable points and
Fig. 10 the convex hull of these, produced using the convex package3. We note
the lens shape seen in the orientation in the left plots is comparable with the
image in the original paper of Markevich et al. [26, Fig. S7].
Fig. 7. As Fig. 6 but with a higher sampling rate
3 http://www-home.math.uwo.ca/∼mfranz/convex/

Symbolic Versus Numerical Computation for Biological Networks
105
Fig. 8. As Fig. 4 but with a higher sampling rate
Fig. 9. 3D Maple Point Plot produced grid sampling on Biomod-26 (see Sect. 4.4)
Fig. 10. Convex Hull of the bistable points in Fig. 9

106
M. England et al.
5
Conclusion and Future Work
We described a new graph theoretical symbolic preprocessing method to reduce
problems from the MAPK network. We experimented with two systems and
found the reduction oﬀered computation savings to both numerical and symbolic
approaches for the determination of multistationarity regions of parameter space.
In addition, the reduction avoided instability from rounding errors in the numer-
ical approach to one system, but uncovered major problems in that approach for
the other. An interesting side result is that, at least for the smaller system, the
symbolic approach can compete with and even outperform the numerical one,
demonstrating how far such methods have progressed in recent years.
In future work we intend to combine the results of the present paper and
our recent publication [5] to generate symbolic descriptions of the bistability
region beyond the 1-parameter case. Other possible routes to achieve this is to
consider the eﬀect of the various degrees of freedom with the algorithms used. For
example, we have a free choice of variable ordering: Biomod-26 has 11 variables
corresponding to 39 916 800 possible orderings while Biomod-28 has 16 variables
corresponding to more than 1013 orderings. Heuristics exist to help with this
choice [10] and machine learning may be applicable [19]. Also, since MAPK
problems contain many equational constraints an approach as described in [13]
may be applicable when higher dimensional CADs are needed.
Acknowledgements. D. Grigoriev is grateful to the grant RSF 16-11-10075.
H. Errami, O. Radulescu, and A. Weber thank the French-German Procope-DAAD
program for partial support of this research. M. England and T. Sturm are grateful to
EU H2020-FETOPEN-2015-CSA 712689 SC2.
Research Data Statement: Data supporting the research in this paper is available
from doi:10.5281/zenodo.807678.
References
1. Bates, D.J., Hauenstein, J.D., Sommese, A.J., Wampler, C.W.: Bertini: software
for numerical algebraic geometry. doi:10.7274/R0H41PB5
2. Bhalla, U.S., Iyengar, R.: Emergent properties of networks of biological signaling
pathways. Science 283(5400), 381–387 (1999)
3. Bradford, R., Chen, C., Davenport, J.H., England, M., Moreno Maza, M., Wilson,
D.: Truth table invariant cylindrical algebraic decomposition by regular chains. In:
Gerdt, V.P., Koepf, W., Seiler, W.M., Vorozhtsov, E.V. (eds.) CASC 2014. LNCS,
vol. 8660, pp. 44–58. Springer, Cham (2014). doi:10.1007/978-3-319-10515-4 4
4. Bradford, R., Davenport, J., England, M., McCallum, S., Wilson, D.: Truth table
invariant cylindrical algebraic decomposition. J. Symb. Comput. 76, 1–35 (2016)
5. Bradford, R., Davenport, J., England, M., Errami, H., Gerdt, V., Grigoriev, D.,
Hoyt, C., Kosta, M., Radulescu, O., Sturm, T., Weber, A.: A case study on the
parametric occurrence of multiple steady states. In: Proceedings of the ISSAC
2017, pp. 45–52. ACM (2017)
6. Chen, C., Davenport, J., May, J., Moreno Maza, M., Xia, B., Xiao, R.: Triangular
decomposition of semi-algebraic systems. J. Symb. Comput. 49, 3–26 (2013)

Symbolic Versus Numerical Computation for Biological Networks
107
7. Chen, C., Moreno Maza, M., Xia, B., Yang, L.: Computing cylindrical algebraic
decomposition via triangular decomposition. In: Proceedings of the ISSAC 2009,
pp. 95–102. ACM (2009)
8. Conradi, C., Mincheva, M.: Catalytic constants enable the emergence of bistability
in dual phosphorylation. J. Roy. Soc. Interface 11(95) (2014)
9. Conradi, C., Flockerzi, D., Raisch, J.: Multistationarity in the activation of a
MAPK: parametrizing the relevant region in parameter space. Math. Biosci.
211(1), 105–31 (2008)
10. Dolzmann, A., Seidl, A., Sturm, T.: Eﬃcient projection orders for CAD. In: Pro-
ceedings of the ISSAC 2004, pp. 111–118. ACM (2004)
11. Dolzmann, A., Sturm, T.: Redlog: computer algebra meets computer logic. ACM
SIGSAM Bull. 31(2), 2–9 (1997)
12. Dolzmann, A., Sturm, T.: Simpliﬁcation of quantiﬁer-free formulae over ordered
ﬁelds. J. Symb. Comput. 24(2), 209–231 (1997)
13. England, M., Bradford, R., Davenport, J.: Improving the use of equational con-
straints in cylindrical algebraic decomposition. In: Proceedings ISSAC 2015, pp.
165–172. ACM (2015)
14. Famili, I., Palsson, B.Ø.: The convex basis of the left null space of the stoichiometric
matrix leads to the deﬁnition of metabolically meaningful pools. Biophys. J. 85(1),
16–26 (2003)
15. Girvan, M., Newman, M.E.J.: Community structure in social and biological net-
works. Proc. Natl. Acad. Sci. USA 99(12), 7821–7826 (2002)
16. Grandoni, F., K¨onemann, J., Panconesi, A.: Distributed weighted vertex cover via
maximal matchings. ACM Trans. Algorithms 5(1), 1–12 (2008)
17. Grigoriev, D., Samal, S.S., Vakulenko, S., Weber, A.: Algorithms to study large
metabolic network dynamics. Math. Model. Nat. Phenom. 10(5), 100–118 (2015)
18. Gross, E., Davis, B., Ho, K.L., Bates, D.J., Harrington, H.A.: Numerical algebraic
geometry for model selection and its application to the life sciences. J. Roy. Soc.
Interface 13(123) (2016)
19. Huang, Z., England, M., Wilson, D., Davenport, J.H., Paulson, L.C., Bridge, J.:
Applying machine learning to the problem of choosing a heuristic to select the vari-
able ordering for cylindrical algebraic decomposition. In: Watt, S.M., Davenport,
J.H., Sexton, A.P., Sojka, P., Urban, J. (eds.) CICM 2014. LNCS, vol. 8543, pp.
92–107. Springer, Cham (2014). doi:10.1007/978-3-319-08434-3 8
20. Joshi, B., Shiu, A.: A survey of methods for deciding whether a reaction network
is multistationary. Math. Model. Nat. Phenom. 10(5), 47–67 (2015)
21. Karp, R.M.: Reducibility among combinatorial problems. In: Complexity of Com-
puter Computations, pp. 85–103. Plenum Press, New York (1972)
22. Koˇsta, M.: New concepts for real quantiﬁer elimination by virtual substitution.
Doctoral dissertation, Saarland University, Germany, December 2016
23. Legewie, S., Schoeberl, B., Bl¨uthgen, N., Herzel, H.: Competing docking interac-
tions can bring about bistability in the MAPK cascade. Biophys. J. 93(7), 2279–
2288 (2007)
24. Li, C., Donizelli, M., Rodriguez, N., Dharuri, H., Endler, L., Chelliah, V., Li, L.,
He, E., Henry, A., Stefan, M.I., Snoep, J.L., Hucka, M., Le Nov`ere, N., Laibe, C.:
BioModels database: an enhanced, curated and annotated resource for published
quantitative kinetic models. BMC Syst. Biol. 4, 92 (2010)
25. Loos, R., Weispfenning, V.: Applying linear quantiﬁer elimination. Comput. J.
36(5), 450–462 (1993)

108
M. England et al.
26. Markevich, N.I., Hoek, J.B., Kholodenko, B.N.: Signaling switches and bistability
arising from multisite phosphorylation in protein kinase cascades. J. Cell Biol.
164(3), 353–359 (2004)
27. P´erez Mill´an, M., Turjanski, A.G.: MAPK’s networks and their capacity for mul-
tistationarity due to toric steady states. Math. Biosci. 262, 125–37 (2015)
28. Rashevsky, N.: Mathematical Biophysics: Physico-Mathematical Foundations of
Biology. Dover, New York (1960)
29. Wang, D.: Elimination Methods. Springer, Heidelberg (2000)
30. Zumsande, M., Gross, T.: Bifurcations and chaos in the MAPK signaling cascade.
J. Theoret. Biol. 265(3), 481–491 (2010)
Open Access This chapter is licensed under the terms of the Creative Commons
Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),
which permits use, sharing, adaptation, distribution and reproduction in any medium
or format, as long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons license and indicate if changes were
made.
The images or other third party material in this chapter are included in the chapter’s
Creative Commons license, unless indicated otherwise in a credit line to the material. If
material is not included in the chapter’s Creative Commons license and your intended
use is not permitted by statutory regulation or exceeds the permitted use, you will
need to obtain permission directly from the copyright holder.

The Polymake Interface in Singular
and Its Applications
Raul Epure1, Yue Ren2(B), and Hans Sch¨onemann1
1 Department of Mathematics, University of Kaiserslautern,
Kaiserslautern, Germany
{epure,hannes}@mathematik.uni-kl.de
2 Max Planck Institute for Mathematics in the Sciences, Leipzig, Germany
yueren@mis.mpg.de
Abstract. Singular and polymake are computer algebra systems for
research in algebraic geometry and polyhedral geometry respectively. We
illustrate the implementation and the functionality of the polymake-
interface in Singular and exhibit its application to the arithmetic of
polyhedral divisors and the reconstruction of hypersurface singularities
from the Milnor algebra.
Keywords: Computer algebra · Algebraic geometry · Convex geometry
1
Introduction
Singular [6] is a computer algebra system for polynomial computations with
particular emphasis on applications in algebraic geometry, commutative algebra
and singularity theory. polymake [9] is a software for research in polyhedral
geometry. It deals with polytopes, polyhedra and fans as well as simplicial com-
plexes, matroids, graphs and tropical hypersurfaces.
Since the initial release of Singular, algebro geometric topics in which poly-
hedral methods play a crucial role have gained in importance (prominent exam-
ples are toric and tropical geometry). In an eﬀort to do justice to these subjects,
which lie in the intersection of algebraic and convex geometry, Singular now
features an interface to polymake. This gives Singular users access to com-
plex algorithms in convex geometry, some of which in turn rely on other third
party software such as lattice point enumeration in normaliz [3]1.
In this article, we describe the interface to polymake, and we will brieﬂy
comment on its implementation and its features. More importantly, we will show
how it can be used to produce a framework for polyhedral divisors, and, on
1 normaliz is a tool for computations in aﬃne monoids, vector conﬁgurations, lattice
polytopes, and rational cones. Normaliz computes normalizations of aﬃne monoids,
integer hulls and triangulations of vector conﬁgurations, lattice points, Hilbert-
/Ehrhart series and polynomials of rational polytopes, duals and Hilbert bases of
rational cones.
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 109–117, 2017.
DOI: 10.1007/978-3-319-66320-3 9

110
R. Epure et al.
this example, how to create user-deﬁned data types in Singular in general.
Furthermore, we will discuss the reconstruction of hypersurface singularities from
their Milnor algebra, to give a glimpse on ongoing research projects which were
made viable through the interface.
The interface was created in the DFG priority programme SPP 1489 in a col-
laboration of several big open source computer algebra systems. Its development
continues in the DFG collaborative research centre SFB-TRR 195, which aims
at taking a leading role in driving the development of interdisciplinary, open
source infrastructure. The authors would like to thank Michael Joswig, Ewgenij
Gawrilow, Benjamin Lorenz and Lars Kastner from the polymake team for
their advice and continued technical support.
2
An Interface to Polymake
The polymake interface in Singular is made possible through:
1. The polymake callable library functionality [10], which allows people to use
polymake as a C++ callable library.
2. The Singular blackbox functionality [14,15], which streamlines the integra-
tion of third party libraries into Singular.
Thanks to the the aforementioned functionalities, its actual implementation
is very simple, see Fig. 1. Each function merely requires a wrapper which:
Lines 3–6: checks the data type of the Singular input and reads the data,
Lines 7–9: converts the Singular input to a polymake object and calls the
respective function in polymake,
Lines 10–12: converts the polymake output to a Singular type and passes
it on.
The wrapper returns TRUE if an error occurred and FALSE otherwise. This
tells Singular whether or not to abort the procedures in which the interface
function was called.
Finally, there needs to be a line (Line 22), which tells Singular:
1. the Singular interpreter library containing the documentation (here:
polymake.lib),
2. the name of the function in the Singular interpreter (here: latticePoints),
3. whether the function is static, i.e. only to be used in Singular interpreter
libraries but invisible to the Singular user (here: FALSE),
4. the name of the C++ wrapper function (here: PMlatticePoints).
For a more detailed exposition on the integration of third party C++ libraries
into Singular, please check the in-depth report on the Gfanlib2 Singular-
interface [14,15].
2 gfanlib is a C++-library for basic manipulations of convex polyhedral cones and
polyhedral fans. It also contains a fast algorithm for computing mixed volumes using
homotopy methods.

The Polymake Interface in Singular and Its Applications
111
1
BOOLEAN
PMlatticePoints (leftv res , leftv
args)
2
{
3
leftv u = args;
4
if ((u != NULL) && (u->Typ() ==
polytopeID) && (u->next == NULL ))
5
{
6
gfan :: ZCone* p0 = (gfan :: ZCone *) u->Data ();
7
polymake :: perl :: Object* p1 = ZPolytope2PmPolytope (p0);
8
polymake ::Matrix <polymake :: Integer > lp1 =
9
p-> CallPolymakeMethod (" LATTICE_POINTS ");
10
intvec* lp0 = PmMatrixInteger2Intvec (&lp1 ,ok);
11
res ->rtyp = INTMAT_CMD;
12
res ->data = (char *) lp0;
13
return
FALSE;
14
}
15
WerrorS(" latticePoints :␣unexpected␣parameters ");
16
return
TRUE;
17
}
18
19
extern "C" int
SI_MOD_INIT (polymake )( SModulFunctions * p)
20
{
21
[...]
22
p-> iiAddCproc ("polymake.lib"," latticePoints ",FALSE , PMlatticePoints );
23
[...]
24
}
Fig. 1. C++ code for blackbox wrapper for latticePoints
Note that some polymake functionality depends on third party software,
e.g. visualization through jreality3 [16], which is why some functions in the
polymake interface can only be called if the necessary third party software is
installed.
The polymake interface is publicly available as part of the oﬃcial Singular
release and a complete list of its functionality can be found in the documentation.
The three main functions and their respective third party software used in the
examples in this article are:
– computation of Minkowski sums,
– optimization of linear functionals via lrs4 [1] or cdd5 [8],
– enumeration of lattice points using LattE6 [4] or normaliz [3].
3 JReality is a Java based full-featured 3D scene graph package designed for 3D
visualization and specialized in mathematical visualization. It provides several back-
ends and can export graphics in several formats, allowing users for example to create
interactive 3D elements in pdf-ﬁles.
4 lrs is a C implementation of the reverse search algorithm for vertex enumeration and
convex hull problems. All computations are done exactly in either multiple precision
or ﬁxed integer arithmetic, and the output is not stored in memory, so even problems
with very large output sizes can sometimes be solved.
5 cddlib is a C implementation of the Double Description Method of Motzkin et al.
for generating all vertices of a general convex polyhedron in Rd given by a system
of linear inequalities and vice versa.
6 LattE is a computer software dedicated to the problems of counting lattice points
and integration inside convex polytopes. It contains the ﬁrst ever implementation of
Barnivok’s algorithm.

112
R. Epure et al.
3
User Deﬁned Types in Singular: Polyhedral Divisors
Singular oﬀers the possibility to create user deﬁned types from already known
types and overload operators, such as +, ˆ or &&, in cases in which the ﬁrst
operand is of a user deﬁned type. In this section, we describe how this is done on
the example of polyhedral divisors as in divisors.lib [2], relying on the inter-
face to polymake for the computation of Minkowski sums and optimal values.
Polyhedral divisors were introduced by Altmann and Hausen, and they rep-
resent aﬃne algebraic varieties with torus action in a way which encodes the
torus action in a purely combinatorial fashion. To be more precise, any aﬃne
variety of dimension n with an eﬀective action of an algebraic torus of dimen-
sion k corresponds to a polyhedral divisor living on a semiprojective variety of
dimension n −k. They generalize the concept of aﬃne toric varieties and are
the building blocks for so-called divisorial fans, which generalize the concept of
toric varieties. Of particular interest are the evaluation maps, which are used to
characterize properness of polyhedral divisors.
Deﬁnition 1. Let N be a lattice and σ ⊂NQ a pointed cone. Then Polσ(NQ)
is the set of all polyhedra in NQ with tail cone σ. It has a natural semigroup
structure under the Minkowski sum with neutral element σ.
For a normal algebraic variety Y the group of rational polyhedral (Weil)
divisors is deﬁned to be
DivQ(Y, σ) = Polσ(NQ) ⊗Z Div(Y ),
so that its elements are formal sums D = k
i=1 Δi ⊗Di, where Δi ⊂NQ are
polytopes with tail cone σ and Di are Weil divisors on Y .
For any u ∈σ∨there exists a natural evaluation map
DivQ(Y, σ) −→DivQ(Y ),
D =
k

i=1
Δi ⊗Di −→D(u) =
k

i=1
evalu(Δi)Di,
where evalu(Δi) = max{⟨u, v⟩| v ∈σ}.
New types can be created using the command newstruct. To call newstruct,
one must specify a name for the new type, as well as types and names for the
subobjects of which it should consist. In Fig. 2, lines 4–5 show the declaration
of the type pdivisor, which consists of a list summands representing the formal
sum which makes up a polyhedral divisor plus the ﬁxed tail cone tail. The list
summands will in turn consist of pairs of polytopes with tail cone tail and a
Weil divisor.
By default, copy and print are the only two operations deﬁned for the
new type, the ﬁrst copying, the second printing each subobject in order of
their declaration. However, it is possible to overload more operators, using
system(‘‘install’’,...), with a user deﬁned procedure with matching num-
ber of input parameters (of which the ﬁrst must be of the user deﬁned type).

The Polymake Interface in Singular and Its Applications
113
Lines 9–12 are overloading the + and the * operators with the procedures deﬁned
in Lines 15–49.
The procedure pdivmult simply iterates through all elements of summands,
which represent the summands in the formal sum, and scale each polytope by
a given factor. The procedure pdivplus merges the summands of its two inputs,
relying on polymake to compute the Minkowski sum of polytopes whose Weil
divisors appear in both summands.
Moreover, polymake was used for implementing the evaluation of polyhedral
divisors.
1
proc
mod_init ()
2
{
3
LIB "polymake.lib";
4
newstruct("pdivisor",
5
"list␣summands ,␣cone␣tail");
6
7
[...]
8
9
system("install","pdivisor",
10
"+",pdivplus ,2);
11
system("install","pdivisor",
12
"*",pdivmult ,2);
13
}
14
15
proc
pdivmult(pdivisor A, int l)
16
{
17
list LA = A.summands;
18
for (int i=1; i<= size(LA); i++)
19
{
20
LA[i ,1] = scale(LA[i,1],l);
21
}
22
pDivisor
Al;
23
Al.sum = LA;
24
Al.tail = A.tail;
25
return (Al);
26
}
27
proc
pdivplus(pdivisor A, pdivisor B)
28
{
29
list
LAB = A.summands;
30
list LB
= B.summands;
31
for (int i=1; i<= size(LB); i++)
32
{
33
p = findIndex(LAB ,LB[i][2]);
34
if (p >0)
35
{
36
LAB[p][1] =
37
minkowskiSum (LAB[p][1] ,
38
LB[i][1]);
39
}
40
else
41
{
42
LAB[size(LAB )+1] = LB[i];
43
}
44
}
45
pdivisor C;
46
C.summands = LAB;
47
C.tail = A.tail;
48
return(C);
49
}
Fig. 2. Singular code for polyhedral divisors using newstruct
4
Quasihomogeneous Isolated Hypersurface Singularities
In this section we present two applications regarding quasihomogeneous isolated
hypersurface singularities. Before we start we need some basic deﬁnitions. In
order to avoid unnecessary terminology, we give slightly restrictive deﬁnitions
compared to the standard literature.
Deﬁnition 2. Denote by C{x1, . . . , xn}7 the ring of convergent power series
over C and denote by m its maximal ideal. Let f ∈C{x1, . . . , xn}. We say f
deﬁnes an isolated hypersurface singularity, if there exists some k ∈N such
that mk ⊆J(f) := ⟨∂x1f, . . . , ∂xnf⟩. We say f is a quasihomogeneous isolated
7 Note that while we are working mathematically over the algebraically closed ﬁeld C,
our purposes allow us to restrict ourselves to rational numbers, provided our initial
data is rational.

114
R. Epure et al.
hypersurface singularity (qhis), if there exist weights w1, . . . , wn ∈Z>0 such that
gcd(w1, . . . , wn) = 1 and, for some ﬁxed d ∈N,
n
i=1
wimi = d, for all monomials
xm1
1
· . . . · xmn
n
in the support of f. We refer to d as the weighted degree of f.
We call Mf = C{x1, . . . , xn}/J(f) the Milnor algebra and μf = dimC Mf the
Milnor number of f.
The following lemma gives an intrinsic characterization of quasi-homogeneity
in terms of derivations, adapted to Deﬁnition 2.
Lemma 1 [17, Lemma 2.3]. Let f ∈C{x1, . . . , xn} deﬁne an isolated hypersur-
face singularity. Then f is a qhis with weights w1, . . . , wn ∈N if and only if
gcd(w1, . . . , wn) = 1 and, after a suitable coordinate change, w1x1∂x1f + . . . +
wnxn∂xnf = df for some d ∈N.
Although qhis are deﬁned as power series, the following lemma states that
they can be considered as polynomials after a suitable coordinate change.
Theorem 1 [5, Theorem 9.1.4]. Let f ∈C{x1, . . . , xn} deﬁne a qhis and denote
by fk its truncation up to degree k ∈N. If k ≥μf + 1, then there exists an
automorphism ϕ of C{x1, . . . , xn} such that ϕ(f) = fk.
Theorem 1 gives us a bound for the degree of the possible monomials of f.
The next theorem gives a bound on the weighted degree d of f and a formula
for μf in terms of the weights w1, . . . , wn of f and d.
Theorem 2 [11, Theorem 4.3]. Let f ∈C{x1, . . . , xn} deﬁne a qhis with wei-
ghts w1, . . . , wn ∈N and weighted degree d ∈N. Then the following hold:
1. d ≤Cμf for some explicitly given C ∈N.
2. μf =
n
j=1

d
wj −1

.
4.1
Finding Quasihomogeneous Isolated Hypersurface Singularities
A simple and nice application of the polymake interface in Singular is the
construction of examples of quasihomogeneous isolated hypersurface singular-
ities. The question is, which possible weights are possible for qhis, in case a
bound μf for the Milnor number is given. This has already been investigated by
Hertling and Kurbel in [11], though they are not explicit in their constructions.
We will explain how to tackle the question in Singular constructively on a
speciﬁc example. We will be using their results and lattice point enumeration,
which is done by Normaliz [3] through the polymake interface.
Example 1. Suppose f ∈C{x, y, z} with μf = 14. Theorem 2 (a) then provides
a bound for the weighted degree of f and thus also a bound for all weights, as we
have wi ≤d
2 +1 using [17, Satz 1.3]. In our case, we have d ≤42 so that wi ≤22
for i = 1, 2, 3. In Fig. 3 this is done in Lines 3 by the function wdeg bound, which

The Polymake Interface in Singular and Its Applications
115
reads of the number of variables from the active basering and has the bound on
the Milnor number as input.
In the Lines 4–15 we then construct the bounded polytope PB cut out by our
inequalities. It is the intersection of the positive orthant P and the polytope B
given by the weight bounds. Note that all coordinates are homogenized as in the
convention of polymake.
1
> ring R = 0,(x,y,z),ds;
2
> int mu = 14;
3
> int d = wdeg_bound(mu); // =42
4
> intmat p[5][4] = 0,1,0,0,
5
.
0,0,1,0,
6
.
0,0,0,1;
7
> polytope P =
8
.
polytopeViaInequalities (p);
9
> intmat b[3][4] = 22,-1,0,0,
10
.
22,0,-1,0,
11
.
22,0,0,-1;
12
> polytope B =
13
.
polytopeViaInequalities (b);
14
> polytope
PB =
15
.
convexIntersection (P,B);
16
> bigintmat
candidates =
17
.
latticePoints (PB);
18
> nrows( candidates );
19
12167
20
> matrix
checkedCandidates =
21
.
find_all_weights (candidates ,mu ,d);
22
> nrows( checkedCandidates );
23
156
Fig. 3. Computing a list of candidates
Using the polymake interface, we then see that PB contains 12167 lattice
points of which only 156 satisfy Theorem 2(b), checked by find all weights. It
is known that any generic linear combination of monomials of suitable weighted
degree result in a qhis, allowing us to ﬁnd explicit qhis with the given properties.
4.2
Reconstruction of QHIS from the Milnor Algebra
Let f, g ∈C{x1, . . . , xn} deﬁne qhis. The famous Mather–Yau Theorem [5, The-
orem 9.1.8] states that C{x1, . . . , xn}/⟨f⟩∼= C{x1, . . . , xn}/⟨g⟩if and only if
C{x1, . . . , xn}/J(f) ∼= C{x1, . . . , xn}/J(g). Hence it implies that it should be
possible to reconstruct f from Mf up to isomorphism, though its proof oﬀers no
way on how to do so. For homogeneous polynomials, this has been addressed in
[12]. We tackle the problem in the quasihomogeneous case through methods in
convex geometry. The following results are part of ongoing research.
Example 2. Consider f = x3 + y3 + z3y ∈C{x, y, z}, which is quasihomoge-
neous and deﬁnes an isolated hypersurface singularity (see Fig. 4, generated using
surfer [18]) with J(f) = ⟨x2, 3y2+z3, yz2⟩. We will now attempt to reconstruct
f from its Milnor algebra.
For the sake of simplicity, our example is chosen such that f and J(f) are
quasihomogeneous in the coordinates x, y, z. Hence, a reduced Gr¨obner basis
computation yields that the space of weights, under whom J(f) is quasihomo-
geneous, is spanned by the rows of
A =

1 0 0
0 3 2

.

116
R. Epure et al.
Fig. 4. The isolated hypersurface singularity of x3 + y3 + z3y
If this were not the case, we would have to use results from [7] regarding the
structure of the derivation module of Mf and an adapted version of the coor-
dinate changes as in the proof of [17, Lemma 2.4]. By [19, Theorem 1.2] f is
quasihomogeneous if and only if J(f) is weighted homogeneous with respect to
a positive weight. This is clearly the case in our example, conﬁrming that Mf
does come from a qhis.
To see under which weight f is quasihomogeneous, we use the fact that
J(f) is weighted homogeneous with respect to all the weights under whom f
is quasihomogeneous (though J(f) may admit many more weights than f). A
quick computation reveals the Milnor number μf = 14, for which we have already
determined a polytope PB of possible candidates in Example 1. Intersecting PB
with the subspace generated by the rows of A reduces the number of lattice
points from 12167 to 184.
In a separate computation we determine the socle of Mf to be the equivalence
class of xz4, pinpointing the weighted degree of f. Checking the equation in
Theorem 2(b) then reduces the number of lattice points from 184 to 2 (Fig. 5).
Using syzygies we can construct examples of qhis with these weights and the
corresponding degrees under certain polynomial constraints using the same idea
as in Example 1. In this case our algorithm returns g = 2x3 + 3(y3 + z3y) with
Mg ∼= Mf.
1
> // Computing
weight
cone
2
> cone C= positive_cone (A);
3
> // Bound
for
degree d
4
> int d= wdeg_bound (J);
5
> d;
6
42
7
> // List
of
weights
8
> bigintmat L=
9
. find_weights_by_bound (A,d);
8
> nrows(L);
9
184
10
> // Candidates
after
reduction
11
. matrix LL=
12
. find_weights_by_formula (L,14,s);
13
> print(LL);
14
3,3,2,9,
15
4,3,2,10
Fig. 5. Restricting the number of weight candidates using polymake

The Polymake Interface in Singular and Its Applications
117
References
1. Avis, D.: Living with lrs. In: Akiyama, J., Kano, M., Urabe, M. (eds.) JCDCG
1998. LNCS, vol. 1763, pp. 47–56. Springer, Heidelberg (2000). doi:10.1007/
978-3-540-46515-7 4
2. B¨ohm, J., Kastner, L., Lorenz, B., Sch¨onemann, H., Ren, Y.: A Singular 4-1-0
library for divisors and p-divisors (2016). www.singular.uni-kl.de
3. Bruns, W., Ichim, B., R¨omer, T., Sieg, R., S¨oger, C.: Normaliz 3.0. algorithms for
rational cones and aﬃne monoids (2016). https://www.normaliz.uni-osnabrueck.
de
4. De Loera, J.A., Hemmecke, R., Tauzer, J., Yoshida, R.: Eﬀective lattice point
counting in rational convex polytopes. J. Symb. Comput. 38(4), 1273–1302 (2005)
5. de Jong, T., Pﬁster, G.: Local Analytic Geometry. Basic Theory and Application.
Advanced Lectures in Mathematics. Friedrich Vieweg & Sohn, Braunschweig (2000)
6. Decker, W., Greuel, G.-M., Pﬁster, G., Sch¨onemann, H.: Singular 4-1-0 – a com-
puter algebra system for polynomial computations (2016). http://www.singular.
uni-kl.de
7. Epure, R.-P.: Homogeneity and derivations on analytic algebras. Master’s thesis.
University of Kaiserslautern, Germany (2015)
8. Fukuda, K.: Cddlib, a C implementation of the double description method of
Motzkin et al. (2016). https://www.inf.ethz.ch/personal/fukudak/cdd home/
9. Gawrilow, E., Joswig, M.: Polymake: a framework for analyzing convex polytopes.
In: Polytopes–Combinatorics and Computation (Oberwolfach, 1997), DMV Semi-
nar, vol. 29, pp. 43–73. Birkh¨auser, Basel (2000)
10. Polymake
team:
Polymake
callable
library.
https://polymake.org/doku.php/
reference/callable
11. Hertling, C., Kurbel, R.: On the classiﬁcation of quasihomogeneous singularities.
J. Singul. 4, 131–153 (2012)
12. Isaev, A.V., Kruzhilin, N.G.: Explicit reconstruction of homogeneous isolated
hypersurface singularities from their Milnor algebras. Proc. Amer. Math. Soc.
142(2), 581–590 (2014)
13. Jensen, A.N.: Gfan 0.5, a software system for Gr¨obner fans and tropical varieties
(2011). http://home.math.au.dk/jensen/software/gfan/gfan.html
14. Jensen, A., Ren, Y., Seelisch, F.: gfan.lib- A Singular 4-1-0 interface to gfanlib
(2017)
15. Jensen, A., Ren, Y., Sch¨onemann, H.: Blackbox types in Singular and Gfanlib
interface. to appear
16. Jreality: a Java library for real-time interactive 3D graphics and audio. http://
www3.math.tu-berlin.de/jreality/jrealityStatic/index.php
17. Saito, K.: Quasihomogene isolierte Singularit¨aten von Hyperﬂ¨achen. Invent. Math.
14, 123–142 (1971)
18. Surfer: a software for visualizing real algebraic geometry in real-time. https://
imaginary.org/program/surfer
19. Xu, Y.-J., Yau, S.-T.: Micro-local characterization of quasi-homogeneous singular-
ities. Amer. J. Math. 118(2), 389–399 (1996)

Computation of Some Integer Sequences
in Maple
W.L. Fan1(B), D.J. Jeﬀrey1, and Erik Postma2
1 Department of Applied Mathematics, The University of Western Ontario,
London, ON, Canada
{wfan54,djeffrey}@uwo.ca
2 Maplesoft, Waterloo, Canada
Abstract. We consider some integer sequences connected with combi-
natorial applications. Speciﬁcally, we consider Stirling partition and cycle
numbers, associated Stirling partition and cycle numbers, and Eulerian
numbers of the ﬁrst and second kinds. We consider their evaluation in
diﬀerent contexts. One context is the calculation of a single value based
on single input arguments. A more common context, however, is the cal-
culation of a sequence of values. We compare strategies for both. Where
possible, we compare with existing Maple implementations.
1
Introduction
For extended discussions of Stirling and Eulerian numbers, we refer to [1,2,7].
These and similar numbers arise frequently in combinatorial applications, and
have therefore been implemented in several computer algebra systems. To date,
the standard libraries of most systems have included Stirling numbers, but not
associated Stirling numbers [3], even though they have found several applications
in recent years. For example, they have appeared in series expansions for the
Lambert W function [4], and also appeared in one form of Stirling’s series for
the Gamma function [2]. (Stirling did not deﬁne the associated numbers.)
Another feature of many implementations is that the functions expect a sin-
gle argument, and return a single value. In practice, however, an application
will usually require a sequence of values, for example, to provide successive coef-
ﬁcients in a series. The requirement of returning multiple values has already
been recognized in some Maple functions, for example, in the implementation of
Bernoulli numbers: they accept a mode parameter. To quote from Maple help:
The mode parameter controls whether or not the bernoulli routine com-
putes additional Bernoulli numbers in parallel with the requested one.
For example, if your computer has 4 cores, then the command bernoulli
(1000, singleton = false) will compute and store bernoulli (1002), bernoulli
(1004), and bernoulli (1006). Since in practice nearly all computations
which use Bernoulli numbers require many of them, and require them in
sequence, this results in considerable eﬃciency gains.
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 118–133, 2017.
DOI: 10.1007/978-3-319-66320-3 10

Computation of Some Integer Sequences in Maple
119
This paper addresses both the computation of single values and of the integer
sequences associated with the combinatorial functions under consideration. As a
matter of terminology, we shall call a function that accepts a unique argument
and returns the corresponding unique result a singleton function, and the corre-
sponding operation a singleton computation. In contrast, a function accepting a
range (explicit or implicit) of arguments and returning the corresponding list of
values will be a sequence function, and the calculation a sequence calculation.
1.1
Deﬁnitions of Numbers
We collect here the deﬁnitions of all numbers considered.
Deﬁnition 1. The r-associated Stirling numbers of the ﬁrst kind, more brieﬂy
Stirling r-cycle numbers, are deﬁned by the generating function
⎛
⎝ln
1
1 −z −
r−1

j=1
zj
j
⎞
⎠
m
= m!

n≥0
n
m

≥r
zn
n! .
(1)
Remark 1. The number
	 n
m

≥r gives the number of permutations of n distinct
objects into m cycles, each cycle having a minimum cardinality r [2, p. 256].
Deﬁnition 2. The r-associated Stirling numbers of the second kind, called more
brieﬂy here Stirling r-partition numbers, are deﬁned, using Karamata–Knuth
notation, by the generating function
⎛
⎝ez −
r−1

j=0
zj
j!
⎞
⎠
m
= m!

n≥0
n
m

≥r
zn
n! .
(2)
Remark 2. The number
 n
m

≥r gives the number of partitions of a set of size n
into m subsets, each subset having a minimum cardinality of r [2,5,6].
Deﬁnition 3. The Eulerian numbers of the ﬁrst kind
n
k

are deﬁned as the
number of permutations π1π2 . . . πn of {1, 2, . . . n} that have k ascents, i.e. k
places where πj < πj+1.
Deﬁnition 4. The Eulerian numbers of the second kind
n
k

are deﬁned as the
number of permutations of the multiset {1, 1, 2, 2, . . . , n, n} for which all numbers
between the two occurrences of every m, with 1 ≤m ≤n, are greater than m,
for each permutation having k ascents, i.e. k places where πj < πj+1.
Remark 3. Note that m is not an argument. For example, given the multiset
{112233}, permutations such as 122133 or 123321 are permitted, but 211233 is
not. Amongst these permitted permutations, we count those with k ascents.
Nomenclature: In [1], the numbers
n
k

are called simply ‘Eulerian numbers’,
while the numbers
n
k

are called ‘second-order Eulerian numbers’.

120
W.L. Fan et al.
2
Stirling Partition Numbers
The Maple 2017 implementation is a singleton function, denoted stirling2 in
the combinat package. It uses the formula
n
m

= 1
m!
m

k=0
(−1)m−k
m
k

kn.
(3)
For the singleton computation, Table 1 shows that the times1 are much less using
(3). In this table, we compared the Maple function stirling2 with the method
given below using the recurrence relation (7). Timings for a sequence calculation,
however, given in Table 2, show the new method is more eﬃcient.
Table 1. Timings (sec) for generating a singleton Stirling partition number. The time
using (7) is compared with the Maple stirling2 function.
n
m
Recurrence stirling2
100 50
0.002
0.002
200 100 0.010
0.003
500 250 0.079
0.007
4000 200 2.700
0.009
5000 250 6.940
0.013
2.1
Sequence Calculation
Given n, m, we wish to compute all Stirling partition numbers
i
j

such that
i ≤n and j ≤m. We use the recurrence relation
i
j

= j
i −1
j

+
i −1
j −1

,
(4)
subject to the boundary conditions
j
j

= 1,
and
i
1

= 1.
(5)
Since
i
j

= 0 for j > i (see Fig. 1), we deﬁne a matrix P which will not store
these zeros.
Pij =
i + j −1
j

.
(6)
Then the recurrence relation becomes
P(i, j) = j P(i −1, j) + P(i, j −1).
(7)
The boundary conditions then become, respectively, P(1, j) =
j
j

= 1, and
P(i, 1) = 1.
1 Product placement: times found using an Intel i7 in a Lenovo Ultrabook.

Computation of Some Integer Sequences in Maple
121
Timings: Table 2 shows the timings for ﬁlling matrices of various sizes with
integer sequences of Stirling partition numbers. The recurrence relation (7) is
compared with creating each entry through a call to Maple’s stirling2. Filling
the square matrix P(n, n) actually calculates all partition numbers
i
j

with
i ≤2n and j ≤n. This is done for timing convenience, and the matrix can be
reshaped for other applications.
Table 2. Timings (sec) for generating sequences of Stirling partition numbers. The
time using (7) compared with Maple stirling2 function.
n
m
Recurrence stirling2
100 100 0.031
7.87
200 200 0.093
69.2
300 300 0.265
259
400 400 0.437
667
500 500 0.843
1450
3
Stirling Cycle Numbers
We consider the computation of
	 n
m

, implemented in Maple 2017 as stirling1
in the combinat package. The computational method used by stirling1 is
based on Stirling’s original deﬁnition of his numbers:
xn =

k
n
k

(−1)n−kxk.
(8)
For given n, stirling1 constructs the product on the left, which is then collected
in powers of x, so that by equating the coeﬃcients of xk, all numbers
	n
k

for
1 ≤k ≤n are determined and stored. Thus, a future call to
	 n
m

with 1 ≤m ≤n
will be returned by table lookup, but a future call with a diﬀerent n will initiate
a new computation. It is interesting that although the interface appears to oﬀer
the user only a singleton computation, in fact a particular integer sequence has
been computed silently.
3.1
Singleton Computation
A singleton computation returns the value of a function for a single pair of input
arguments. We implement the known recurrence relation
n
m

= (n −1)
n −1
m

+
n −1
m −1

,
(9)

122
W.L. Fan et al.
subject to boundary conditions
m
m

= 1, for m ≥1,
(10)
n
1

= (n −1)!.
(11)
We deﬁne the vector
u(i)
j
=
i + j −1
j

.
In Fig. 1, we see that for ﬁxed i, u(i)
j
describes numbers along the ith diagonal
line, counting from the left. The recurrence relation (9) can be written in terms
of u as
u(i)
j
= (i + j −2)u(i−1)
j
+ u(i)
j−1,
with u(i)
1
= (i −1)!. We note that once u(i−1)
j
is used, it does not need to be
stored further, so we can overwrite storage. Our iteration scheme is thus (Maple
notation for the ith element of a vector is u[i])
u[j] = (i + j −2)u[j] + u[j −1].
Therefore, we initialize u[1] = u(i)
1
=
	i
1

= (i −1)! and ﬁll in diagonal lines
successively.
m
n
Fig. 1. Scheme for calculating singleton Stirling cycle
 n
m

or partition numbers
 n
m

.
The computation proceeds from left to right and bottom to top. At each stage only
the numbers on one diagonal need to be stored in the vector u(i)
j
which is progressively
overwritten. The open circles show the base of each successive loop. The black ﬁlled
circles show the recurrence relation used. The larger circle is calculated from the two
smaller ones. The triangles line show the points computed by one call to stirling1.

Computation of Some Integer Sequences in Maple
123
Complexity. The aim of this subsection is to gain insight into the best ways to
test the implementations, by identifying the worst cases for the methods. A full
bit complexity is beyond the scope of this paper, and will require more work
on estimates for the sizes of Stirling numbers. As pointed out by Wilf [9], the
available estimates are for
	n
k

when k is ﬁxed and n →∞, whereas the present
algorithms require knowledge of the opposite case.
In order to calculate the number
	 n
m

, a vector of length m must be re-
computed (overwritten) n −m times. Each iteration requires one multiplication
and 3 additions. Therefore the complexity is m(n−m). We can therefore expect
that the worst case for the method will be m = n/2.
Since Maple’s approach and the present one calculate diﬀerent sets of num-
bers, a direct comparison is not very meaningful, and so we simply make a brief
comparison between one-time calculations. Notice that in Table 3, the times
taken by stirling1 are approximately independent of m as expected.
Table 3. Times for a single call to Maple’s stirling1 and the present singleton com-
putation. Timings (sec) based on 10 trials, with memory being cleared before each
call.
n
m
stirling1 Present scheme
300
150
0.023
0.035
400
200
0.063
0.052
400
20
0.062
0.011
1000
500
0.612
0.491
2000
500
4.92
3.00
3.2
A Finite Sum
For completeness, we mention that a singleton cycle number can be found from
a ﬁnite sum, as was done for a singleton partition number. We have
n
m

=
n−m

j=0
(−1)n−k+j
n −1 + j
n −k + j
 2n −k
n −k −j
n −k + j
j

.
(12)
Combining this with (3), we can express a cycle number as a double sum. This,
however, is too slow to warrant further consideration.
3.3
Sequence Calculation
The method used above for partition numbers can be readily adapted for cycle
numbers. Given n, m, we compute all Stirling cycle numbers
	i
j

such that i ≤n
and j ≤m. We use the recurrence relation
i
j

= (i −1)
i −1
j

+
i −1
j −1

,
(13)

124
W.L. Fan et al.
subject to the boundary conditions
j
j

= 1,
and
i
1

= (i −1)!.
(14)
Since
	i
j

= 0 for j > i (see Fig. 1), we deﬁne a matrix C which will not store
these zeros.
Cij =
i + j −1
j

.
(15)
Then the boundary conditions are C(1, j) =
	j
j

= 1, and C(i, 1) = (i −1)!. The
recurrence relation becomes
C(i, j) = (i + j −2) C(i −1, j) + C(i, j −1).
(16)
Timings. Table 4 shows the timing for ﬁlling matrices of various sizes with inte-
ger sequences of Stirling cycle numbers. The recurrence relation (16) is compared
with creating each entry through a call to Maple’s stirling1. Filling the square
matrix C(n, n) actually calculated all cycle numbers
	i
j

with i ≤2n. This is
done for timing purposes, and the matrix can be reshaped for other applications.
The comparison is to compute the same numbers using the sequence calculation
function stirling1. Larger values of (n, m) are not tabulated because a bug in
Maple 2016 (and earlier) caused larger arguments to fail. This will be corrected
in Maple 2017.
Table 4. Timings (sec) for generating sequences of Stirling cycle numbers. The time
using (16) compared with Maple’s stirling1 function.
n
m
Recurrence stirling1
40
40
0.000
0.842
60
60
0.000
4.446
80
80
0.015
14.414
100
100
0.015
35.037
120
120
0.015
193.004
4
Associated Stirling Numbers
There are no known analogues of (3) or (8) for the associated Stirling numbers
for r ≥2; hence we must use either the generating functions (1) and (2), or the
following recurrence relations.
n + 1
k

≥r
= k
n
k

≥r
+

n
r −1
n −r + 1
k −1

≥r
,
(17)

Computation of Some Integer Sequences in Maple
125
n + 1
k

≥r
= n
n
k

≥r
+ nr−1
n −r + 1
k −1

≥r
.
(18)
Note that n0 = 1. The boundary cases are
n
1

≥r
= 1,
n ≥r,
(19)
n
1

≥r
= (n −1)!,
n ≥r,
(20)
kr
k

≥r
=
(rk)!
(r!)k k!,
k ≥1,
(21)
kr
k

≥r
= (rk)!
rkk! ,
k ≥1.
(22)
4.1
Singleton Stirling 2-Partition and 2-Cycle
The two computations have the same structure, and can be described in parallel.
We choose to implement
n
m

≥2
= m
n −1
m

≥2
+ (n −1)
n −2
m −1

≥2
,
(23)
n
m

≥2
= (n −1)
n −1
m

≥2
+ (n −1)
n −2
m −1

≥2
.
(24)
We also have boundary conditions
2n
n

≥2
=
2n
n

≥2
= (2n)!
n!2n = (2n −1)!!,
2n + 1
n

≥2
= 2
(2n + 1)!
3(n −1)!2n = 2
2n + 1
n

≥2
.
We deﬁne the vector
u(i)
j
=
i + 2j −1
j

≥2
,

126
W.L. Fan et al.
and similarly for 2-partition numbers. In Fig. 2, we see that if we ﬁx i, then u(i)
j
describes numbers along the ith diagonal line. Now u(i)
1
= i! and
u(i)
j
= (i + 2j −2)u(i−1)
j
+ (i + 2j −2)u(i)
j−1.
We note that once u(i−1)
j
is used, it does not need to be stored further, so we
can overwrite storage. Our iteration scheme is thus
u[j] = (i + 2j −2)(u[j] + u[j −1]).
For initialization, we can use a special case of (24):
2j + 2
j + 1

≥2
= u(1)
j+1 = (2j + 1)
2j
j

≥2
= (2j + 1)u(1)
j .
Therefore, we initialize u to i = 1 using u(1)
j
= u[j] = 1 and ﬁll in one line at a
time by ﬁxing i and looping over j. Each j loop starts setting u(i)
1
= i! = iu(i−1)
1
.
We then loop over i.
m
n
Fig. 2. Calculating 2-partition and 2-cycle numbers. As with the r = 1 case, only
numbers on one sloping line need to be kept at any stage of the computation. The
same convention for illustrating the recurrence relation is used.
4.2
Sequence Calculation of 2-Partition and 2-Cycle Numbers
Given n, m, we compute all Stirling 2-partition numbers
i
j

≥2 or 2-cycle numbers
	i
j

≥2 such that i ≤n and j ≤m. We use the recurrence relations (23) or (24) as

Computation of Some Integer Sequences in Maple
127
appropriate. Since
i
j

≥2 =
	i
j

≥2 = 0 for 2j > i (see Fig. 2), we deﬁne a matrix
C which will not store these zeros.
Cij =
i + 2j −1
j

≥2
.
(25)
Then the recurrence relation for 2-partition becomes
C(i, j) = j C(i −1, j) + (i + 2j −2)C(i −1, j −1).
(26)
The recurrence relation for 2-cycle becomes
C(i, j) = (i + 2j −2) C(i −1, j) + (i + 2j −2)C(i −1, j −1).
(27)
4.3
Singleton Stirling r-Partition and r-Cycle Numbers
From the above discussion of 1-associated and 2-associated numbers, the gener-
alization is clear. We have to implement
n + 1
m

≥r
= n
n
m

≥r
+ n(n −1)(n −2) . . . (n −r + 2)
n −r + 1
m −1

≥r
.
(28)
We deﬁne the vector
u(i)
j
=
i + rj −1
j

≥r
.
The generalization of Fig. 2 to one containing lines of slope 1/r is not shown.
For ﬁxed i, u(i)
j
describes numbers along one of the lines, with u(i)
1
= (i + r −2)!
and
u(i)
j
= (i + rj −2)u(i−1)
j
+ (i + rj −2)(i + rj −3) . . . (i + rj −r)u(i)
j−1.
We note that once u(i−1)
j
is used, it does not need to be stored further, so we
can overwrite storage. Our iteration scheme is thus
u[j] = (i + rj −2)u[j] + (i + rj −2) . . . (i + rj −r)u[j −1].
For initialization, we can use a special case of (28):
rj + r
j + 1

≥r
= u(1)
j+1 =(rj + r −1)(rj + r −2) . . . (rj + 1)
rj
j

≥r
=(rj + r −1)(rj + r −2) . . . (rj + 1)u(1)
j .
We initialize u using u(1)
j
= u[j] = 1 and ﬁll in each line by ﬁxing i and looping
over j. We start each j loop by setting u(i)
1
= (i + r −2)! = (i + r −2)u(i−1)
1
. We
then loop over i.

128
W.L. Fan et al.
4.4
Sequence Calculation of r-Partition and r-Cycle Numbers
Given n, m, we compute all Stirling r-partition numbers
i
j

≥r or r-cycle numbers
	i
j

≥r such that i ≤n and j ≤m. The recurrence relation applied here can refer
to (17) and (18), which is subject to the boundary conditions
1
1

≥r
=
1
1

≥r
= 1.
(29)
Since
i
j

≥r =
	i
j

≥r = 0 for rj > i, we deﬁne a matrix C which will not store
these zeros.
Cij =
i + rj −1
j

≥r
.
(30)
Then the recurrence relation for r-partition becomes
C(i, j) = j C(i −1, j) +
i + rj −2
r −1

C(i −r + 1, j −1).
(31)
The recurrence relation for r-cycle becomes
C(i, j) = (i+rj−2) C(i−1, j)+(i+rj−2)(i+rj−3) . . . (i+rj−r) C(i−r+1, j−1).
(32)
4.5
Implementation in Maple
In our implementation of Stirling numbers, we provide procedures for users to
compute either a singleton Stirling number or a sequence of Stirling numbers.
The procedures are
1. StirlingRCycle: to calculate a singleton Stirling r-cycle number.
2. StirlingRCycleMatrix: to calculate a sequence of Stirling r-cycle numbers.
3. StirlingRPartition: to calculate a singleton Stirling r-partition number.
4. StirlingRPartitionMatrix: to calculate a sequence of Stirling r-partition
numbers.
Neither Maple nor Mathematica has an implementation with which to compare
our programs. Therefore we have programmed the recurrence relations, as well
as the generating functions in Maple. In Table 5 below, we compared our new
scheme for computing a singleton r-associated Stirling cycle number with using
the generating function. The generating function for Stirling r-cycle numbers is:
StirRCycleGen := proc(n,k,r) local t, z, p;
t:=series((ln(1/(1-z)) - add(z^p/p , p=1..r-1))^k, z=0,n+1);
n!*coeff(t, z, n)/k!;
end proc;

Computation of Some Integer Sequences in Maple
129
Table 5. Timings in seconds of computations of single Stirling r-cycle number. Column
headings give the functions used. The numbers tested were
1700
m

≥100.
m
Singleton scheme Generating function
2
0.062
2.979
3
0.093
14.461
4
0.109
26.707
5
0.140
41.184
6
0.156
38.797
7
0.171
51.121
8
0.171
45.240
9
0.171
53.055
10
0.171
53.289
Table 5 shows that the singleton scheme is much faster than the generating
function for the computation of single r-associated Stirling cycle number. For the
computation of a sequence of r-associated Stirling cycle numbers, we compared
three methods: (1) a loop calling the singleton function; (2) a loop calling the
generating function; (3) the sequence procedure. The results are collected in
Tables 6 and 7, and show that the sequence procedure is fastest.
Table 6. Timings in seconds of computations of a sequence of r-associated Stirling
cycle numbers. Column headings give the functions used. The input argument is n,
and the return is an n × n matrix.
n
Singleton scheme Generating function
10 0.011
2.402
20 0.024
8.746
30 0.037
18.952
40 0.063
36.477
50 0.771
72.817
Similar tests were performed for r-associated Stirling partition numbers. The
generating function for Stirling r-partition numbers is:
StirRPartGen := proc(n,k,r) local t, z, p;
t:=series((exp(z) - add(z^p/p! , p=0..r-1))^k, z=0, n+1);
n!*coeff(t, z, n)/k!;
end proc;
The test data are collected in Tables 8, 9 and 10. Since the pattern is similar to
that for cycle numbers, the discussion and tables are abbreviated.

130
W.L. Fan et al.
Table 7. Timings in seconds of computations of a sequence of r-associated Stirling
cycle numbers. Column headings give the functions used. The input argument is n,
and the return is an n × n matrix.
n
Singleton scheme Sequence scheme
100
7.145
0.015
150
36.411
0.031
200 117.734
0.062
250 295.102
0.124
300 638.474
0.202
Table 8. Timings in seconds of computations of single r-associated Stirling partition
number. Column headings give the functions being used. The numbers tested were
1000
m

≥18.
m
Singleton scheme Generating function
2 0.031
7.129
4 0.062
18.969
6 0.093
29.250
8 0.140
32.276
10 0.156
43.664
Table 9. Timings in seconds of computations of a sequence of r-associated Stirling
partition number. Column headings give the functions being used. The input argument
is n, and the return is an n × n matrix.
n
Singleton scheme Generating function
10 0.020
1.864
20 0.035
8.345
30 0.070
22.047
40 0.144
44.074
50 0.201
79.233
Table 10. Timings in seconds of computations of a sequence of r-associated Stirling
partition numbers. Column headings give the functions used. The input argument is
n, and the return is an n × n matrix.
n
Singleton scheme Sequence scheme
100
7.145
0.015
200 117.734
0.062
250 295.102
0.124
300 638.474
0.202

Computation of Some Integer Sequences in Maple
131
5
A Multiple Threads Approach to Sequence Calculations
The Maple help for Bernoulli numbers, quoted in the introduction, states that
additional values of Bernoulli numbers are calculated in parallel. This section
explored ways in which parallel computation could be applied to Stirling num-
bers. For this, we use the Threads package in Maple. When we generate the
numbers inside a matrix, instead of ﬁlling the matrix row by row and column
by column, we ﬁll each diagonal from left to right. Here is the main part in the
sequential code to ﬁll Stirling r-cycle numbers in the matrix by diagonal with
given input arguments (n, r) where n is the size of matrix.
for N from 3 to n do
for k from 2 to N-1 do
pd := mul(N-k+r-1-l, l = 1 .. r-1);
A(N-k+r, k) := pd*A(N-k, k-1)+(N-k+r-2)*A(N-k+r-1, k);
end do;
end do;
According to the recurrence relation, we know that we can divide such diag-
onal into a left half and right half. So we deﬁne two subroutines accordingly.
fileft := proc (N, r) local k, Nsplit, pd, l; global A;
Nsplit := floor((1/2)*N+1/2);
for k from 2 to Nsplit do
pd := mul(N-k+r-1-l, l = 1 .. r-1);
A(N-k+r, k) := pd*A(N-k, k-1)+(N-k+r-2)*A(N-k+r-1, k);
end do; end proc;
and
filrght := proc (N, r) local k, Nsplit, pd, l; global A;
Nsplit := floor((1/2)*N+1/2);
for k from Nsplit+1 to N-1 do
pd := mul(N-k+r-1-l, l = 1 .. r-1);
A(N-k+r, k) := pd*A(N-k, k-1)+(N-k+r-2)*A(N-k+r-1, k);
end do; end proc;
And for each half of the diagonal, we can establish an independent thread to
fulﬁll the task. We implemented this approach in Maple.
Threaded := proc (n, r) local N, k, Nsplit; global A;
A := Matrix(n, n, fill = 0);
A(1, 1) := 1;
for N from 3 to n do
Threads:-Task:-Start(null, Task = [fileft, N, r],
Task = [filrght, N, r])
end do;
end proc;

132
W.L. Fan et al.
Table 11 compares the threaded scheme with the sequential scheme in the
computation of an n × n matrix of Stirling cycle numbers. The table reﬂects
the limitation that there is an overhead cost to setting up new threads, and the
beneﬁt of the threaded approach is felt only when the amount of work achieved
within a thread outweighs the overhead. In this implementation, new threads
are created for each loop. We are exploring new methods of calculation which
will allow the threads to work more eﬃciently, with less overhead.
Table 11. Timings in seconds of comparison of threaded code with sequential code
in generating sequences of Stirling Cycle numbers. The tests were made on an AMD
8-core processor.
n
Threaded scheme Sequential scheme
500
0.856
0.329
1000
2.420
1.380
2000
9.240
9.610
2500 17.900
24.900
3000 29.880
39.870
4000 87.960
142.800
6
Implementation of Eulerian Numbers
The Eulerian numbers share many similarities with the Stirling numbers, and
all the methods described above can be applied to their case. The numbers obey
the following recurrence relations [1].
n
m

= (m + 1)
n −1
m

+ (n −m)
n −1
m −1

,
(33)
n
m

= (m + 1)
n −1
m

+ (2n −m −1)
n −1
m −1

.
(34)
The present Maple functions eulerian1 and eulerian2 are recursively pro-
grammed implementations of these equations. As a consequence, they are very
slow for large arguments. The new implementation of these numbers consists of
4 functions, which follow the patterns of the Stirling number implementations.
1. Eulerian1: calculates a singleton Eulerian number of the ﬁrst kind.
As with Stirling partition numbers, a ﬁnite sum is known which is distinctly
the fastest method for a singleton computation [8]:
n
k

=
k+1

j=0
(−1)j
n + 1
j

(k −j + 1)n.
(35)

Computation of Some Integer Sequences in Maple
133
2. Eulerian1Matrix: calculates a sequence of Eulerian numbers of the ﬁrst kind.
This follows the sequence calculation of Stirling numbers, using (33).
3. Eulerian2: calculates singleton Eulerian numbers of the second kind.
This follows the simpleton method used earlier for Stirling cycle numbers.
There is no counterpart of (8) for Eulerian numbers.
4. Eulerian2Matrix: calculates a sequence of Eulerian numbers of the second
kind.
This follows the sequence calculation of Stirling numbers.
6.1
Timings for Eulerian Number Calculations
In view of the similarities with Stirling numbers, we shall not labour the com-
parisons between methods, since they form the same procession of speeds seen
before. Table 12 compares the implementations, following the patterns set above.
Table 12. Timings in seconds of computations of Eulerian numbers. Column headings
give the functions used.
n
Eulerian1 Eulerian1Matrix Eulerian2 Eulerian2Matrix
60
2.776
0.015
3.135
0.000
80
9.656
0.015
10.795
0.015
100
23.743
0.015
27.924
0.015
120
52.884
0.015
60.684
0.015
140 101.634
0.046
115.159
0.046
160 180.430
0.062
204.049
0.062
180 300.036
0.062
342.952
0.062
References
1. Graham, R.L., Knuth, D.E., Patashnik, O.: Concrete Mathematics. Addison-Wesley
Publishing Co., Reading (1994)
2. Comtet, L.: Advanced Combinatorics. D. Reidel Publishing Co., Dordrecht (1974)
3. Howard, F.T.: Associated stirling numbers. Fibonacci Quart. 18(4), 303–315 (1980)
4. Corless, R.M., Jeﬀrey, D.J., Knuth, D.E.: A sequence of series for the Lambert W
Function. In: Kuechlin, W.W. (ed.) Proceedings of the ISSAC 1997. ACM Press
(1997)
5. Karamata, J.: Theoreme sur la sommabilite exponentielle et d’autres sommabilites
rattachant. Mathematica, Cluj 9, 164–178 (1935). Romania
6. Knuth, D.E.: Two notes on notation. Am. Math. Mon. 99, 403–422 (1992)
7. Stirling, J.: Methodus Diﬀerentialis, London (1730)
8. Lehmer, D.H.: Generalized Eulerian numbers. J. Combin. Theory Ser. A 32, 195–
215 (1982)
9. Wilf, H.S.: The asymptotic behavior of the stirling numbers of the ﬁrst kind. J.
Combin. Theory Ser. A 64, 344–349 (1993)

Symbolic-Numerical Algorithm for Generating
Interpolation Multivariate Hermite Polynomials
of High-Accuracy Finite Element Method
A.A. Gusev1(B), V.P. Gerdt1,2, O. Chuluunbaatar1,3, G. Chuluunbaatar1,
S.I. Vinitsky1,2, V.L. Derbov4, and A. G´o´zd´z5
1 Joint Institute for Nuclear Research, Dubna, Russia
gooseff@jinr.ru
2 RUDN University, 6 Miklukho-Maklaya St., Moscow 117198, Russia
3 Institute of Mathematics, National University of Mongolia, Ulaanbaatar, Mongolia
4 N.G. Chernyshevsky Saratov National Research State University, Saratov, Russia
5 Institute of Physics, University of M. Curie-Sklodowska, Lublin, Poland
Abstract. A symbolic-numerical algorithm implemented in Maple for
constructing Hermitian ﬁnite elements is presented. The basis functions
of ﬁnite elements are high-order polynomials, determined from a spe-
cially constructed set of values of the polynomials themselves, their par-
tial derivatives, and their derivatives along the directions of the normals
to the boundaries of ﬁnite elements. Such a choice of the polynomials
allows us to construct a piecewise polynomial basis continuous across
the boundaries of elements together with the derivatives up to a given
order, which is used to solve elliptic boundary value problems using the
high-accuracy ﬁnite element method. The eﬃciency and the accuracy
order of the ﬁnite element scheme, algorithm and program are demon-
strated by the example of the exactly solvable boundary-value problem
for a triangular membrane, depending on the number of ﬁnite elements
of the partition of the domain and the number of piecewise polynomial
basis functions.
Keywords: Hermite
interpolation
polynomials ·
Boundary-value
problem · High-accuracy ﬁnite element method
1
Introduction
In Refs. [9,10], the symbolic-numeric algorithms and programs for the solution
of boundary-value problems for a system of second-order ordinary diﬀerential
equations using the ﬁnite element method (FEM) of high accuracy order with
Hermite interpolation polynomials (HIP) were developed, aimed at the calcula-
tion of spectral and optical characteristics of quantum systems.
It is known that the approximating function of the boundary-value problem
solution in the entire domain can be expressed by means of its values and the
values of its derivatives at the node points of the domain via the basis functions,
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 134–150, 2017.
DOI: 10.1007/978-3-319-66320-3 11

Interpolation Multivariate Hermite Polynomials
135
referred to as Lagrange interpolation polynomials (LIP), which are nonzero only
on a few elements, adjacent to the corresponding nodes. Generally, the approxi-
mating function for the entire domain is represented in terms of linear combina-
tions of the basis functions. The coeﬃcients of these linear combinations are the
values of the approximating function and its directional derivatives on a given
mesh of nodes. The basis functions themselves or their directional derivatives
take a unit value at one of the nodes. In many cases, the schemes are restricted
to the set of node values of the basis functions themselves. However, there are
problems, in which the values of directional derivatives are also necessary. They
are of particular importance when high smoothness between the elements is
required, or when the gradient of solution is to be determined with increased
accuracy. The construction of such basis functions, referred to as Hermite inter-
polation polynomials, is not possible on an arbitrary mesh of nodes. It is one of
the most important and diﬃcult problems in the ﬁnite element method and its
applications in diﬀerent ﬁelds, solved to date in the explicit form only for certain
particular cases [1,2,4–8,11,13,14,17,19,21].
This motivation determines the aim of the present work, namely, the develop-
ment of a symbolic-numerical algorithm implemented in any CAS for computing
in analytical form the basis functions of Hermitian ﬁnite elements for a few vari-
ables and their application to constructing the FEM schemes with high order of
accuracy.
In the paper, we present the symbolic-numeric algorithm implemented in the
CAS Maple [15] for constructing the interpolation polynomials (basis functions)
of Hermitian ﬁnite elements of a few variables based on a specially constructed
set of values of the polynomials themselves, their partial derivatives, and deriva-
tives along the normals to the boundaries of ﬁnite elements. The corresponding
piecewise continuous basis of the high-order accuracy FEM provides the conti-
nuity not only of the approximate solution, but also of its derivatives to a given
order depending on the smoothness of the variable coeﬃcients of the equation
and the domain boundary. This basis is used to construct the FEM scheme
for high-accuracy solution of elliptic boundary-value problems in the bounded
domain of multidimensional Euclidean space, speciﬁed as a polyhedral domain.
We also used the symbolic algorithm to generate Fortran routines that allow the
solution of the generalized algebraic eigenvalue problem with high-dimension
matrices. The eﬃciency of the FEM scheme, the algorithm, and the program
is demonstrated by constructing typical bases of Hermitian ﬁnite elements and
their application to the benchmark exactly solvable boundary-value eigenvalue
problem for a triangle membrane.
The paper is organized as follows. In Sect. 2, the setting of the boundary-
value eigenvalue problem is given. In Sect. 3, we formulate the symbolic-numeric
algorithm for generating the bases of Hermitian ﬁnite elements with multiple
variables. In Sect. 4, we present the results of the calculations for the benchmark
boundary-value problem, demonstrating the eﬃciency of the FEM scheme. In the
Conclusion, we discuss the prospects of development of the proposed algorithm
of constructing the Hermitian ﬁnite elements and its applications to high-order
accuracy FEM schemes.

136
A.A. Gusev et al.
2
Setting of the Problem
Consider a self-adjoint boundary-value problem for the elliptic diﬀerential equa-
tion of the second order:
(D −E) Φ(z) ≡
⎛
⎝−
1
g0(z)
d

ij=1
∂
∂zi
gij(z) ∂
∂zj
+ V (z) −E
⎞
⎠Φ(z) = 0.
(1)
For the principal part coeﬃcients of Eq. (1), the condition of uniform ellipticity
holds in the bounded domain z = (z1, . . . , zd) ∈Ω of the Euclidean space Rd,
i.e., the constants μ > 0, ν > 0 exist such that μξ2 ≤d
ij=1 gij(z)ξiξj ≤
νξ2, ξ2 = d
i=1 ξ2
i ∀ξ ∈Rd. The left-hand side of this inequality expresses the
requirement of ellipticity, while the right-hand side expresses the boundedness of
the coeﬃcients gij(z). It is also assumed that g0(z) > 0, gji(z) = gij(z) and V (z)
are real-valued functions, continuous together with their generalized derivatives
to a given order in the domain z ∈¯Ω = Ω ∪∂Ω with the piecewise continuous
boundary S = ∂Ω, which provide the existence of nontrivial solutions obeying
the boundary conditions [6,12] of the ﬁrst kind
Φ(z)|S = 0,
(2)
or the second kind
∂Φ(z)
∂nD

S= 0,
∂Φ(z)
∂nD
=
d

ij=1
(ˆn, ˆei)gij(z)∂Φ(z)
∂zj
,
(3)
where ∂Φm(z)
∂nD
is the derivative along the conormal direction, ˆn is the outer normal
to the boundary of the domain S = ∂Ω, ˆei is the unit vector of z = d
i=1 ˆeizi,
and (ˆn, ˆei) is the scalar product in Rd.
For a discrete spectrum problem, the functions Φm(z) from the Sobolev space
Hs≥1
2
(Ω), Φm(z) ∈Hs≥1
2
(Ω), corresponding to the real eigenvalues E: E1 ≤
E2 ≤... ≤Em ≤... satisfy the conditions of normalization and orthogonality
⟨Φm(z)|Φm′(z)⟩=
	
Ω
dzg0(z)Φm(z)Φm′(z) = δmm′,
dz = dz1...dzd.
(4)
The FEM solution of the boundary-value problems (1)–(4) is reduced to the
determination of stationary points of the variational functional [3,6]
Ξ(Φm, Em, z) ≡
	
Ω
dzg0(z)Φm(z) (D −Em) Φ(z) = Π(Φm, Em, z),
(5)
where Π(Φm, Em, z) is the symmetric quadratic functional
Π(Φm, Em, z) =

Ω
dz

d

ij=1
gij(z)∂Φm(z)
∂zi
∂Φm(z)
∂zj
+ g0(z)Φm(z)(V (z) −Em)Φm(z)

.

Interpolation Multivariate Hermite Polynomials
137
Fig. 1. (a) Enumeration of nodes Ar, r = 1, . . . , (p + 1)(p + 2)/2 with sets of numbers
[n0, n1, n2] for the standard (canonical) triangle element Δ in the scheme with the ﬁfth-
order LIP p′ = p = 5 at d = 2. The lines (ﬁve crossing straight lines) are zeros of LIP
ϕ14(z′) from (12), equal to 1 at the point labeled with the number triple [n0, n1, n2] =
[2, 2, 1]. (b) LIP isolines of ϕ14(z′)
3
FEM Calculation Scheme
In FEM, the domain Ω = Ωh(z) = 
Q
q=1 Δq, speciﬁed as a polyhedral domain,
is covered with ﬁnite elements, in the present case, the simplexes Δq with d + 1
vertices ˆzi = (ˆzi1, ˆzi2, . . . , ˆzid) with i = 0, . . . , d. Each edge of the simplex Δq is
divided into p equal parts, and the families of parallel hyperplanes H(i, k) are
drawn, numbered with the integers k = 0, . . . , p, starting from the correspond-
ing face, e.g., as shown for d = 2 in Fig. 1 (see also [6]). The equation of the
hyperplane is H(i, k) : H(i; z) −k/p = 0, where H(i; z) is a linear function of z.
The node points of hyperplanes crossing Ar are enumerated with sets of
integers [n0, . . . , nd], ni ≥0, n0 + . . . + nd = p, where ni, i = 0, 1, . . . , d are
the numbers of hyperplanes, parallel to the simplex face, not containing the i-th
vertex ˆzi = (ˆzi1, . . . ˆzid). The coordinates ξr = (ξr1, . . . , ξrd) of the node point
Ar ∈Δq are calculated using the formula
(ξr1, . . . , ξrd) = (ˆz01, . . . , ˆz0d)n0/p + (ˆz11, . . . , ˆz1d)n1/p + . . . + (ˆzd1, . . . , ˆzdd)nd/p (6)
from the coordinates of the vertices ˆzj = (ˆzj1, . . . , ˆzjd). Then the LIP ϕr(z) equal
to one at the point Ar with the coordinates ξr = (ξr1, . . . , ξrd), characterized by
the numbers [n0, n1, . . . , nd], and equal to zero at the remaining points ξr′, i.e.,
ϕr(ξr′) = δrr′, has the form
ϕr(z) =
⎛
⎝
d

i=0
ni−1

n′
i=0
H(i; z) −n′
i/p
H(i; ξr) −n′
i/p
⎞
⎠.
(7)

138
A.A. Gusev et al.
Note that the construction of the HIP ϕκ
r(z), where κ ≡κ1, . . . , κd, with the
ﬁxed values of the functions {ϕκ
r(ξr′)} and the derivatives {∂•
•ϕκ
r(z)|z=ξr′} at
the nodes ξr′, already at d = 2 leads to cumbersome expressions, improper for
FEM using nonuniform mesh.
The economical implementation of FEM is the following:
1. The calculations are performed in the local (reference) coordinates z′, in which
the coordinates of the simplex vertices are the following: ˆz′
j = (ˆz′
j1, . . . , ˆz′
jd),
ˆz′
jk = δjk,
2. The HIP in the physical coordinates z in the mesh is sought in the form of
linear combinations of polynomials in the local coordinates z′, the transition
to the physical coordinates is executed only at the stage of numerical solution
of a particular boundary-value problem (1)–(5),
3. The calculation of FEM integrals is executed in the local coordinates.
Let us construct the HIP on an arbitrary d-dimensional simplex Δq with the
d + 1 vertices ˆzi = (ˆzi1, ˆzi2, . . . , ˆzid), i = 0, . . . , d. For this purpose, we introduce
the local coordinate system z′ = (z′
1, z′
2, . . . , z′
d) ∈Rd, in which the coordinates
of the simplex vertices are the following: ˆz′
i = (ˆz′
ik = δik, k = 1, . . . , d). The
relation between the coordinates is given by the formula:
zi = ˆz0i +
d

j=1
ˆJijz′
j,
i = 1, . . . , d,
ˆJij = ˆzji −ˆz0i.
(8)
The inverse transformation and the relation between the diﬀerentiation oper-
ators are given by the formulas
z′
i =
d

j=1
( ˆJ−1)ij(zj −ˆz0j),
(9)
∂
∂z′
i
=
d

j=1
ˆJji
∂
∂zj
,
∂
∂zi
=
d

j=1
( ˆJ−1)ji
∂
∂z′
j
.
(10)
Equation (10) is used to calculate the HIP ϕκ
r(z′) = { ˇϕκ
r(z′), Qs(z′)} from
(20) that satisfy the conditions (13), (17), and (18) of the next section, with the
ﬁxed derivatives to the given order at the nodes ξr′. In this case, the derivatives
along the normal to the element boundary in the physical coordinate system are,
generally, not those in the local coordinates z′. When constructing the HIP in the
local coordinates z′ one has to recalculate the ﬁxed derivatives at the nodes ξr′ of
the element Δq to the nodes ξ′
r′ of the element Δ, using the matrices ˆJ−1, given
by cumbersome expressions. Therefore, the required recalculation is executed
based on the relations (8)–(10) for each ﬁnite element Δq at the stage of the
formation of the HIP basis {ϕ¯κ′
r (z′)}P
r=1 on the ﬁnite element Δ, implemented
numerically using the analytical formulas, presented in the next section.

Interpolation Multivariate Hermite Polynomials
139
Fig. 2. Schematic diagram of the conditions on the element Δq (upper panel) and
Δ (lower panel) for constructing the basis of HIP [pκmaxκ′]: [131], [141], [231], [152].
The squares are the points ξ′
r, where the values of the functions and their derivatives
are ﬁxed according to the conditions (13), (16); the solid (dashed) arrows begin at the
points η′
s, where the values of the ﬁrst (second) derivative in the direction of the normal
in the physical coordinates are ﬁxed, according to the condition (17), respectively; the
circles are the points ζ′
s, where the values of the functions are ﬁxed according to the
condition (18)
The integrals that enter the variational functional (5) on the domain Ωh(z) =

Q
q=1 Δq, are expressed via the integrals, calculated on the element Δq, and
recalculated to the local coordinates z′ on the element Δ,

Δq
dzg0(z)ϕκ
r (z)ϕκ′′
r′ (z)U(z) = J

Δ
dz′g0(z(z′))ϕκ
r (z′)ϕκ′′
r′ (z′)U(z(z′)),
(11)

Δq
dzgs1s2(z)∂ϕκ
r (z)
∂zs1
∂ϕκ′′
r′ (z)
∂zs2
= J
d

t1,t2=1
ˆJ−1
s1s2;t1t2

Δ
dz′gs1s2(z(z′))∂ϕκ
r (z′)
∂z′
t1
∂ϕκ′′
r′ (z′)
∂z′
t2
,
where J
= det ˆJ
> 0 is the determinant of the matrix
ˆJ from Eq. (8),
ˆJ−1
s1s2;t1t2 = ( ˆJ−1)t1s1( ˆJ−1)t2s2, dz′ = dz′
1 . . . dz′
d, and ϕκ
r(z′) = { ˇϕκ
r(z′), Qs(z′)}
from Eq. (20).
3.1
Lagrange Interpolation Polynomials
In the local coordinates, the LIP ϕr(z′) is equal to one at the node point ξ′
r
characterized by the numbers [n0, n1, . . . , nd], and zero at the remaining node
points ξ′
r′, i.e., ϕr(ξ′
r′) = δrr′, are determined by Eq. (7) at H(0; z′) = 1 −z′
1 −
. . . −z′
d, H(i; z′) = z′
i, i = 1, . . . , d:

140
A.A. Gusev et al.
Table 1. Characteristics of the HIP bases (20) at d = 2
[pκmaxκ′]
[131]
[141]
[231]
[152]
[162]
[241]
[173]
p′
κmax(p + 1) −1
5
7
8
9
11
11
13
Nκmaxp′
(p + 1)(p + 2)κmax(κmax + 1)/4
18
30
36
45
63
60
84
N1p′
(p′ + 1)(p′ + 2)/2
21
36
45
55
78
78
105
K
p(p + 1)κmax(κmax −1)/4
3
6
9
10
15
9
21
T1(1)
3p
3
3
6
3
3
6
3
T1(2)
9p
9
9
18
9
9
18
9
N(AP1)
Nκmaxp′
18
30
36
45
63
60
84
N(AP2)
T1(κ′)
3
3
6
9
9
6
18
N(AP3)
K −T1(κ′)
0
3
3
1
6
12
3
Restriction of derivative order κ′ :
3pκ′(κ′ + 1)/2 ≤K
ϕr(z′) =
⎛
⎝
d

i=1
ni−1

n′
i=0
z′
i −n′
i/p
ni/p −n′
i/p
⎞
⎠
⎛
⎝
n0−1

n′
0=0
1 −z′
1 −. . . −z′
d −n′
0/p
n0/p −n′
0/p
⎞
⎠. (12)
Setting the numerators in Eq. (12) equal to zero yields the families of equa-
tions for the straight lines, directed “horizontally”, “vertically”, and “diago-
nally” in the local coordinate system of the element Δ, which is related by
the aﬃne transformation with the “oblique” family of straight lines of the ele-
ment Δq. In Fig. 1, an example is presented that illustrates the construction
of the LIP at d = 2, r, r′ = 1, . . . , (p + 1)(p + 2)/2, p = 5 on the element Δ
in the form of a rectangular triangle with the vertices ˆz′
0 = (ˆz′
01, ˆz′
02) = (0, 0),
ˆz′
1 = (ˆz′
11, ˆz′
12) = (1, 0), ˆz′
2 = (ˆz′
21, ˆz′
22) = (0, 1).
The piecewise polynomial functions P¯l(z) forming the ﬁnite-element basis
{P¯l(z)}P
¯l=1, which are constructed by joining the LIP ϕr(z) of Eq. (7), obtained
from Eq. (12) by means of the transformation (9), on the ﬁnite elements Δq:
Pl(z) = {ϕl(z), Al ∈Δq; 0, Al ̸∈Δq} ,
are continuous, but their derivatives are discontinuous at the boundaries of the
elements Δq.
3.2
Algorithm for Calculating the Basis of Hermite Interpolating
Polynomials
Let us construct the HIP of the order p′ by joining of which the piecewise poly-
nomial functions (27) with the continuous derivatives up to the given order κ′
can be obtained.
Step 1. Auxiliary Polynomials (AP1). To construct HIP in the local coor-
dinates z′, let us introduce the set of auxiliary polynomials (AP1)

Interpolation Multivariate Hermite Polynomials
141
ϕκ1...κd
r
(ξ′
r) = δrr′δκ10 . . . δκd0,
∂μ1...μdϕκ1...κd
r
(z′)
∂z′
1
μ1 . . . ∂z′
d
μd

z′=ξ′
r′
= δrr′δκ1μ1 . . . δκdμd, (13)
0 ≤κ1 + κ2 + . . . + κd ≤κmax −1,
0 ≤μ1 + μ2 + . . . + μd ≤κmax −1.
Here at the node points ξ′
r, deﬁned according to (6), in contrast to LIP, the
values of not only the functions themselves, but of their derivatives to the order
κmax −1 are speciﬁed. AP1 are given by the expressions
ϕκ1κ2...κd
r
(z′) = wr(z′)

μ∈Δκ
aκ1...κd,μ1...μd
r
(z′
1 −ξ′
r1)μ1 × . . . × (z′
d −ξ′
rd)μd, (14)
wr(z′) =
⎛
⎝
d
	
i=1
ni−1
	
n′
i=0
(z′
i −n′
i/p)κmax
(ni/p −n′
i/p)κmax
⎞
⎠
⎛
⎝
n0−1
	
n′
0=0
(1 −z′
1 −. . . −z′
d −n′
0/p)κmax
(n0/p −n′
0/p)κmax
⎞
⎠,
wr(ξ′
r) = 1,
where the coeﬃcients aκ1...κd,μ1...μd
r
are calculated from recurrence relations
obtained by substitution of Eq. (14) into conditions (13),
aκ1...κd,μ1...μd
r
=
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
0,
μ1 + . . . + μd ≤κ1 + . . . + κd, (μ1, . . . , μd) ̸= (κ1, . . . , κd),
d
i=1
1
μi!,
(μ1, . . . , μd) = (κ1, . . . , κd);
−
ν∈Δν
 d
i=1
1
(μi−νi)!

gμ1−ν1,...,μd−νd
r
(ξ′
r)aκ1...κd,ν1...νd
r
,
μ1 + . . . + μd > κ1 + . . . + κd;
(15)
gκ1κ2...κd(z′) =
1
wr(z′)
∂κ1κ2...κdwr(z′)
∂z′
1
κ1∂z′
2
κ2 . . . ∂z′
d
κd .
For d > 1 and κmax > 1, the number Nκmaxp′ of HIP of the order p′ and the
multiplicity of nodes κmax are smaller than the number N1p′ of the polynomials
that form the basis in the space of polynomials of the order p′ (e.g., the LIP
from (12)), i.e., the polynomials satisfying Eq. (13) are determined not uniquely.
Table 2. The HIP p = 1, κmax = 3, κ′ = 1, p′ = 5 (the Argyris element [5,6,14])
AP1 : ξ1 = (0, 1), ξ2 = (1, 0), ξ3 = (0, 0)
ϕ0,0
1
= z3
2(6z2
2 −15z2 + 10)
ϕ0,0
2
= z3
1(6z2
1 −15z1 + 10)
ϕ0,0
3
= z3
0(6z2
0 −15z0 + 10)
ϕ0,1
1
= −z3
2(z2 −1)(3z2 −4)
ϕ0,1
2
= −z3
1z2(3z1 −4)
ϕ0,1
3
= −z3
0z2(3z0 −4)
ϕ1,0
1
= −z1z3
2(3z2 −4)
ϕ1,0
2
= −z3
1(z1 −1)(3z1 −4)
ϕ1,0
3
= −z3
0z1(3z0 −4)
ϕ0,2
1
= z3
2(z2 −1)2/2
ϕ0,2
2
= z3
1z2
2/2
ϕ0,2
3
= z3
0z2
2/2
ϕ1,1
1
= z1z3
2(z2 −1)
ϕ1,1
2
= (z1 −1)z3
1z2
ϕ1,1
3
= z3
0z1z2
ϕ2,0
1
= z2
1z3
2/2
ϕ2,0
2
= z3
1(z1 −1)2/2
ϕ2,0
3
= z3
0z2
1/2
AP2 : η1 = (0, 1/2), η2 = (1/2, 0), η3 = (1/2, 1/2)
Q1 = 16z2
0z1z2
2/f11
Q2 = 16z2
0z2
1z2/f22
Q3 = −8z0z2
1z2
2/f01

142
A.A. Gusev et al.
Step 2. Auxiliary Polynomials (AP2 and AP3). For unique determination
of the polynomial basis let us introduce K = N1p′−Nκmaxp′ auxiliary polynomials
Qs(z) of two types: AP2 and AP3, linearly independent of AP1 from (14) and
satisfying the following conditions at the node points ξ′
r′ of AP1:
Qs(ξ′
r′) = 0,
∂κ′
1κ′
2...κ′
dQs(z′)
∂z′
1
μ1∂z′
2
μ2 . . . ∂z′
d
μd

z′=ξ′
r′
= 0,
s = 1, . . . , K, (16)
0 ≤κ1 + κ2 + . . . + κd ≤κmax −1,
0 ≤μ1 + μ2 + . . . + μd ≤κmax −1.
Note that to provide the continuity of derivatives the part of polynomials referred
to as AP2 must satisfy the condition
∂kQs(z′)
∂nk
i(s)

z′=η′
s′
= δss′,
s, s′ = 1, . . . , T1(κ′),
k = k(s′),
(17)
where η′
s′ = (η′
s′1, . . . , η′
s′d) are the chosen points lying on the faces of vari-
ous dimensionalities (from 1 to d −1) of the d-dimensional simplex Δ and not
coincident with the nodal points of HIP ξ′
r, where (13) is valid, ∂/∂ni(s) is the
directional derivative along the vector ni, normal to the corresponding ith face
of the d-dimensional simplex Δq at the point ηs′ in the physical coordinate sys-
tem, which is recalculated to the point η′
s′ of the face of the simplex Δ in the
local coordinate system using relations (8)–(10), e.g., for d = 2 see Eq. (25). Cal-
culating the number T1(κ) of independent parameters required to provide the
continuity of derivatives to the order κ, we determine its maximal value κ′ that
can be obtained for the schemes with given p and κmax and, correspondingly,
the additional conditions (17).
T2 = K −T1(κ′) parameters remain independent and, correspondingly, T2
additional conditions are added, necessary for the unique determination of the
polynomials referred to as AP3,
Qs(ζ′
s′) = δss′,
s, s′ = T1(κ′) + 1, . . . , K,
(18)
where ζ′
s′ = (ζ′
s′1, . . . , ζ′
s′d) ∈Δ are the chosen points belonging to the simplex
without the boundary, but not coincident with the node points of AP1 ξ′
r.
The auxiliary polynomials AP2 are given by the expression
Qs(z′) =
 d

t=0
z′
t
kt
 
j1,...,jd
bj1,...,jd;sz′
1
j1...z′
d
jd,
z′
0 = 1 −z′
1 −... −z′
d, (19)
where kt = 1, if the point ηs, in which the additional conditions (17) are speciﬁed,
lies on the corresponding face of the simplex Δ, i.e., H(t, ηs) = 0, and kt = κ′,
if H(t, ηs) ̸= 0. The auxiliary polynomials AP3 are given by the expression
(19) at kt = κ′. The coeﬃcients bj1,...,jd;s are determined from the uniquely
solvable system of linear equations, obtained as a result of the substitution of
the expression (19) into conditions (16)–(18).

Interpolation Multivariate Hermite Polynomials
143
Table 3. The HIP p = 1, κmax = 4, κ′ = 1, p′ = 7
AP1 : ξ1 = (0, 1), ξ2 = (1, 0), ξ3 = (0, 0)
ϕ0,0
1
= −z4
2P0(z3)
ϕ0,0
2
= −z4
1P0(z1)
ϕ0,0
3
= −z4
0P0(z0)
ϕ0,1
1
= z4
2(z2 −1)P1(z2)
ϕ0,1
2
= z4
1z2P1(z1)
ϕ0,1
3
= z4
0z2P1(z0)
ϕ1,0
1
= z1z4
2P1(z2)
ϕ1,0
2
= z4
1(z1 −1)P1(z1)
ϕ1,0
3
= z4
0z1P1(z0)
ϕ0,2
1
= −z4
2(z2 −1)2(4z2 −5)/2 ϕ0,2
2
= −(1/2)z4
1z2
2(4z1 −5)
ϕ0,2
3
= −z4
0z2
2(4z0 −5)/2
ϕ1,1
1
= −z1z4
2(z2 −1)(4z2 −5)
ϕ1,1
2
= −z4
1z2(z1 −1)(4z1 −5)
ϕ1,1
3
= −z4
0z1z2(4z0 −5)
ϕ2,0
1
= −z2
1z4
2(4z2 −5)/2
ϕ2,0
2
= −z4
1(z1 −1)2(4z1 −5)/2 ϕ2,0
3
= −z4
0z2
1(4z0 −5)/2
ϕ0,3
1
= z4
2(z2 −1)3/6
ϕ0,3
2
= z4
1z3
2/6
ϕ0,3
3
= z4
0z3
2/6
ϕ1,2
1
= z1z4
2(z2 −1)2/2
ϕ1,2
2
= z4
1z2
2(z1 −1)/2
ϕ1,2
3
= z4
0z1z2
2/2
ϕ2,1
1
= z2
1z4
2(z2 −1)/2
ϕ2,1
2
= z4
1z2(z1 −1)2/2
ϕ2,1
3
= z4
0z2
1z2/2
ϕ3,0
1
= z3
1z4
2/6
ϕ3,0
2
= z4
1(z1 −1)3/6
ϕ3,0
3
= z4
0z3
1/6
AP2 : η1 = (0, 1/2), η2 = (1/2, 0), η3 = (1/2, 1/2)
Q1 = 8z1z2
2z2
0(12z2
1 −7z1 −8z1z2 −8z2
2 + 8z2)/f11
Q2 = −8z2
1z2z2
0(8z2
1 + 8z1z2 −8z1 + 7z2 −12z2
2)/f22
Q3 = 4z2
1z2
2z0(12z2
2 −17z2 + 5 −17z1 + 32z1z2 + 12z2
1)/f01
AP3 : ζ4 = (1/4, 1/2), ζ5 = (1/2, 1/4), ζ6 = (1/4, 1/4)
Q4 = 1024z2
0z2
1z2
2(4z2 −1)
Q5 = 1024z2
0z2
1z2
2(4z1 −1)
Q6 = 1024z2
0z2
1z2
2(4z0 −1)
P0(zj) = (20z3
j −70z2
j + 84zj −35) ,
P1(zj) = (10z2
j −24zj + 15)
Step 3. As a result, we get the required set of basis HIP
ϕκ
r(z′) = { ˇϕκ
r(z′), Qs(z′)},
κ = κ1, . . . , κd,
(20)
composed of the polynomials Qs(z′) of the type AP2 and AP3, and the polyno-
mials ˇϕκ
r(z′) of the type AP1 that satisfy the conditions
ˇϕκ1...κd
r
(ξ′
r) = δrr′δκ10 . . . δκd0,
∂μ1...μd ˇϕκ1...κd
r
(z′)
∂z′
1
μ1 . . . ∂z′
d
μd

z′=ξ′
r′
= δrr′δκ1μ1 . . . δκdμd, (21)
0 ≤κ1 + κ2 + . . . + κd ≤κmax −1,
0 ≤μ1 + μ2 + . . . + μd ≤κmax −1;
∂k ˇϕκ1...κd
r
(z′)
∂nk
i(s)

z′=η′
s′
= 0,
s′ = 1, . . . , T1(κ′),
k = k(s′),
(22)
ˇϕκ1...κd
r
(ζ′
s′) = 0,
s′ = T1(κ′) + 1, . . . , K,
(23)
and are calculated using the formulas
ˇϕκ
r(z′) = ϕκ
r(z′) −
K

s=1
cκ;r;sQs(z′), cκ;r;s =
⎧
⎨
⎩
∂kϕκ
r (z′)
∂nk
i(s)

z′=η′
s
, Qs(z′)∈AP2,
ϕκ
r(ζs),
Qs(z′)∈AP3.
(24)
Step 4. The AP1 ˇϕκ
r(z′) from (20), where κ denotes the directional derivatives
along the local coordinate axes, are recalculated using formulas (10) into ˇϕκ
r(z′),
speciﬁed in the local coordinates, but now κ denotes already the directional
derivatives along the physical coordinate axes.

144
A.A. Gusev et al.
Table 4. The HIP p = 2, κmax = 3, κ′ = 1, p′ = 8
AP1 : ξ1 = (0, 1), ξ2 = (1/2, 1/2), ξ3 = (1, 0), ξ4 = (0, 1/2), ξ5 = (1/2, 0), ξ6 = (0, 0)
ϕ0,0
1
= z3
2(2z2 −1)3S0(z2)
ϕ0,0
3
= z3
1(2z1 −1)3S0(z1)
ϕ0,0
6
= z3
0(2z0 −1)3S0(z0)
ϕ0,1
1
= −z3
2(z2 −1)S1(z2)
ϕ0,1
3
= −z3
1z2S1(z1)
ϕ0,1
6
= −z3
0z2S1(z0)
ϕ1,0
1
= −z1z3
2S1(z2)
ϕ1,0
3
= −z3
1(z1 −1)S1(z1)
ϕ1,0
6
= −z3
0z1S1(z0)
ϕ0,2
1
= z3
2(z2 −1)2(2z2 −1)3/2
ϕ0,2
3
= z3
1(2z1 −1)3z2
2/2
ϕ0,2
6
= z3
0z2
2(2z0 −1)3/2
ϕ1,1
1
= z3
2(2z2 −1)3z1(z2 −1)
ϕ1,1
3
= z3
1z2(z1 −1)(2z1 −1)3
ϕ1,1
6
= z3
0z1z2(2z0 −1)3
ϕ2,0
1
= z3
2(2z2 −1)3z2
1/2
ϕ2,0
3
= z3
1(z1 −1)2(2z1 −1)3/2 ϕ2,0
6
= z3
0z2
1(2z0 −1)3/2
ϕ0,0
2
= 64z3
1z3
2S2(z0)
ϕ0,0
4
= 64z3
0z3
2S2(z1)
ϕ0,0
5
= 64z3
0z3
1S2(z2)
ϕ0,1
2
= 32z3
1z3
2S3(z2, z0)
ϕ0,1
4
= 32z3
0z3
2S3(z2, z1)
ϕ0,1
5
= 64z3
0z3
1z2(6z2 + 1)
ϕ1,0
2
= 32z3
1z3
2S3(z1, z0)
ϕ1,0
4
= 64z3
0z1z3
2(6z1 + 1)
ϕ1,0
5
= 32z3
0z3
1S3(z1, z2)
ϕ0,2
2
= 8z3
1z3
2(2z2 −1)2
ϕ0,2
4
= 8z3
0z3
2(2z2 −1)2
ϕ0,2
5
= 32z3
0z3
1z2
2
ϕ1,1
2
= 16z3
1z3
2(2z1 −1)(2z2 −1) ϕ1,1
4
= 32z3
0z1z3
2(2z2 −1)
ϕ1,1
5
= 32z3
0z3
1z2(2z1 −1)
ϕ2,0
2
= 8z3
1z3
2(2z1 −1)2
ϕ2,0
4
= 32z3
0z2
1z3
2
ϕ2,0
5
= 8z3
0z3
1(2z1 −1)2
AP2 : η1 = (0, 1/4), η2 = (0, 3/4), η3 = (1/4, 0), η4 = (3/4, 0), η5 = (1/4, 3/4), η6 = (3/4, 1/4)
Q1 =
(512/9)z2
0z1z2
2(2z0 −1)(2z2 −1)(4z0 −1)/f11
Q2 = −(512/9)z2
0z1z2
2(2z0 −1)(2z2 −1)(4z2 −1)/f11
Q3 = −(512/9)z2
0z2
1z2(2z0 −1)(2z1 −1)(4z0 −1)/f22
Q4 = −(512/9)z2
0z2
1z2(2z0 −1)(2z1 −1)(4z1 −1)/f22
Q5 =
(256/9)z0z2
1z2
2(2z1 −1)(2z2 −1)(4z2 −1)/f01
Q6 =
(256/9)z0z2
1z2
2(2z1 −1)(2z2 −1)(4z1 −1)/f01
AP3 : ζ7 = (1/4, 1/2), ζ8 = (1/2, 1/4), ζ9 = (1/4, 1/4)
Q7 = 4096z2
0z2
1z2
2(2z0 −1)(2z1 −1)
Q8 = 4096z2
0z2
1z2
2(2z0 −1)(2z2 −1)
Q9 = 4096z2
0z2
1z2
2(2z1 −1)(2z2 −1)
S0(zj) = (48z2
2 −105z2 + 58) , S1(zj) = (2zj −1)3(9zj −10),
S2(zj) = (24z2
j −12z0z1z2/zj + 4), S3(zi, zj) = (2zi −1)(6zj + 1)
Step 5. The ﬁnal transition to the physical coordinates is implemented by
means of transformation (9).
3.3
Example: HIP for d = 2
For d = 2, the order p′ of the polynomial with respect to the tangential variable
t at the boundary of the triangle
∂κ′+1
∂nκ′∂t,. . . ,
∂κmax
∂nκ′∂tκmax−κ′−1 . Thus, since the
triangle has three sides, the unique determination of the derivatives to the order
of κ′ at the boundary requires T1(κ′) = 3p+. . .+3κ′p = 3pκ′(κ′+1)/2 parameters
and, correspondingly, the additional conditions (17).
For example, if p = 1 and κmax = 4, then there are K = 6 additional
conditions for the determination of AP2 and AP3. The order p′ = 7 of the
polynomial in the tangential variable t at the boundary of the triangle coincides
with the order of the polynomial of two variables, and its unique determination
requires p′ + 1 = 8 parameters. The ﬁrst-order derivative κ′ = 1 in the variable

Interpolation Multivariate Hermite Polynomials
145
Table 5. The HIP p = 1, κmax = 5, κ′ = 2, p′ = 9
AP1 : ξ1 = (0, 1), ξ2 = (1, 0), ξ3 = (0, 0)
ϕ0,0
1
= z5
2T0(z2)
ϕ0,0
2
= z5
1T0(z1)
ϕ0,0
3
= z5
0T0(z0)
ϕ0,1
1
= −z5
2(z2 −1)T1(z2)
ϕ0,1
2
= −z5
1z2T1(z1)
ϕ0,1
3
= −z5
0z2T1(z0)
ϕ1,0
1
= −z1z5
2T1(z2)
ϕ1,0
2
= −z5
1(z1 −1)T1(z1)
ϕ1,0
3
= −z5
0z1T1(z0)
ϕ0,2
1
= z5
2(z2 −1)2T2(z2)/2
ϕ0,2
2
= z5
1z2
2T2(z1)/2
ϕ0,2
3
= z5
0z2
2T2(z0)/2
ϕ1,1
1
= z1z5
2(z2 −1)T2(z2)
ϕ1,1
2
= z5
1z2(z1 −1)T2(z1)
ϕ1,1
3
= z5
0z1z2T2(z0)
ϕ2,0
1
= z2
1z5
2T2(z2)/2
ϕ2,0
2
= z5
1(z1 −1)2T2(z1)/2
ϕ2,0
3
= z5
0z2
1T2(z0)/2
ϕ0,3
1
= −z5
2(z2 −1)3(5z2 −6)/6
ϕ0,3
2
= −z5
1z3
2(5z1 −6)/6
ϕ0,3
3
= −z5
0z3
2(5z0 −6)/6
ϕ1,2
1
= −z1z5
2(z2 −1)2(5z2 −6)/2 ϕ1,2
2
= −z5
1z2
2(z1 −1)(5z1 −6)/2
ϕ1,2
3
= −z5
0z1z2
2(5z0 −6)/2
ϕ2,1
1
= −z2
1z5
2(z2 −1)(5z2 −6)/2
ϕ2,1
2
= −z5
1z2(z1 −1)2(5z1 −6)/2 ϕ2,1
3
= −z5
0z2
1z2(5z0 −6)/2
ϕ3,0
1
= −z3
1z5
2(5z2 −6)/6
ϕ3,0
2
= −z5
1(z1 −1)3(5z1 −6)/6
ϕ3,0
3
= −z5
0z3
1(5z0 −6)/6
ϕ0,4
1
= z5
2(z2 −1)4/24
ϕ0,4
2
= z5
1z4
2/24
ϕ0,4
3
= z5
0z4
2/24
ϕ1,3
1
= z1z5
2(z2 −1)3/6
ϕ1,3
2
= z5
1z3
2(z1 −1)/6
ϕ1,3
3
= z5
0z1z3
2/6
ϕ2,2
1
= z2
1z5
2(z2 −1)2/4
ϕ2,2
2
= z5
1z2
2(z1 −1)2/4
ϕ2,2
3
= z5
0z2
1z2
2/4
ϕ3,1
1
= z3
1z5
2(z2 −1)/6
ϕ3,1
2
= z5
1z2(z1 −1)3/6
ϕ3,1
3
= z5
0z3
1z2/6
ϕ4,0
1
= z4
1z5
2/24
ϕ4,0
2
= z5
1(z1 −1)4/24
ϕ4,0
3
= z5
0z4
1/24
AP2 : η1 = (0, 1/2), η2 = (1/2, 0), η3 = (1/2, 1/2), η4 = (0, 1/3), η5 = (0, 2/3),
η6 = (1/3, 0), η7 = (2/3, 0), η8 = (1/3, 2/3), η9 = (2/3, 1/3)
Q1 = 256z3
0z1z3
2((3z1z2 −5z2
1 −z2
2 + z2)f11 −4z1(z2 −z0)f12)/f 2
11
Q2 = 256z3
0z3
1z2((3z1z2 −5z2
2 −z2
1 + z1)f22 + 4z2(z1 −z0)f21)/f 2
22
Q3 = 128z0z3
1z3
2((7z2
0 −2z0 −z1z2)f01 + 2z0(z1 −z2)f02)/f 2
01
Q4 = (729/16)z3
0z2
1z3
2(3z0 −1)/f 2
11
Q5 = (729/16)z3
0z2
1z3
2(3z2 −1)/f 2
11
Q6 = (729/16)z3
0z3
1z2
2(3z0 −1)/f 2
22
Q7 = (729/16)z3
0z3
1z2
2(3z1 −1)/f 2
22
Q8 = (729/64)z2
0z3
1z3
2(3z2 −1)/f 2
01
Q9 = (729/64)z2
0z3
1z3
2(3z1 −1)/f 2
01
AP3 : ζ10 = (1/3, 1/3)
Q10 = 19683z3
0z3
1z3
2
T0(zj) = (70z4
j −315z3
j + 540z2
j −420zj + 126)
T1(zj) = (35z3
2 −120z2
2 + 140z2 −56), T2(zj) = (15z2
j −35zj + 21)
normal to the boundary will be a polynomial of the order p′ −κ′ = 6, and
its unique determination will require p′ −κ′ + 1 = 7 parameters. However, it is
determined by only p′−κ′(p+1) = 6 parameters: the mixed derivatives
∂
∂n,
∂2
∂n∂t
and
∂3
∂n∂t2 , speciﬁed at two vertices. The missing parameter can be determined
by specifying the directional derivative along the direction, non-parallel to the
triangle boundary, at one of the points on its side (e.g., in the middle of the
side). Thus, for p = 1 and κmax = 4, one can construct HIP with the ﬁxed values
of the ﬁrst derivative on the boundary of the triangle, and 6 −3 = 3 parameters
remain free.
The second-order derivative κ′ = 2 in the variable normal to the boundary
is a polynomial of the order p′ −κ′ = 5, and its unique determination requires
p′ −κ′ + 1 = 6 parameters. However, it is determined by only p′ −κ′(p + 1) = 4
parameters: the mixed derivatives
∂2
∂n2 and
∂3
∂n2∂t speciﬁed at two vertices of the

146
A.A. Gusev et al.
triangle. Thus, the unique determination of the second derivative will require 6
parameters. This fact means that using this algorithm for p = 1 and κmax = 4,
it is impossible to construct the FEM scheme with continuous second derivative.
In this case, one should use the scheme with κmax > 4, e.g., denoted as [152]
in Table 1 and Fig. 2. Then the three remaining free parameters are used to
construct AP3. Note that it is possible to construct the schemes providing the
continuity of the second derivatives at some boundaries of the ﬁnite elements.
This case is not considered in the present paper.
For d = 2, the derivatives ∂/∂ni along the direction ni, perpendicular to the
appropriate face i = 0, 1, 2 in the physical coordinate system are given in terms
of the partial derivatives ∂/∂z′
j, j = 1, 2 in the local coordinate system Δ, using
(8)–(10), by the expressions
∂
∂ni = fi1 ∂
∂z′
1
+ fi2 ∂
∂z′
2
,
i = 1, 2,
∂
∂n0 = (f01 + f02) ∂
∂z′
1
+ (f01 −f02) ∂
∂z′
2
, (25)
where fij = fij(ˆz0, ˆz1, ˆz2) are the functions of the coordinates of vertices ˆz0, ˆz1, ˆz2
of the triangle Δq in the physical coordinate system
f11 = J−1R(ˆz2, ˆz0),
f12 = −(ˆz12 −ˆz02)(ˆz22 −ˆz02) + (ˆz21 −ˆz01)(ˆz11 −ˆz01)
JR(ˆz2, ˆz0)
,
f22 = J−1R(ˆz1, ˆz0),
f21 = −(ˆz12 −ˆz02)(ˆz22 −ˆz02) + (ˆz21 −ˆz01)(ˆz11 −ˆz01)
JR(ˆz1, ˆz0)
,
f01 = −(2J)−1R(ˆz2, ˆz1), f02 = (ˆz11 −ˆz01)2 + (ˆz12 −ˆz02)2 −(ˆz22 −ˆz02)2 −(ˆz21 −ˆz01)2
2JR(ˆz2, ˆz1)
,
J = (ˆz11 −ˆz01)(ˆz22 −ˆz02) −(ˆz12 −ˆz02)(ˆz21 −ˆz01),
(26)
R(ˆzj, ˆzj′) = ((ˆz1j −ˆz1j′)2 + (ˆz2j −ˆz2j′)2)1/2.
The implementation of conditions (13), (16), (17), and (18), using which the
basis HIP were constructed, is schematically shown for d = 2 in Fig. 2. The
characteristics of the polynomial basis of HIP on the element Δ at d = 2 are
presented in Table 1.
Tables 2, 3, 4 and 5 present the results of executing the Algorithm from
Sect. 3.2 for the HIP (p = 1, κmax = 3, κ′ = 1, p′ = 5), (p = 1, κmax = 4,
κ′ = 1, p′ = 7), (p = 2, κmax = 3, κ′ = 1, p′ = 8) and (p = 1, κmax = 5, κ′ = 2,
p′ = 9): AP1 ϕk
r(z′), AP2 and AP3 Qk
s(z′), and the corresponding coeﬃcients
cκ;r;s are calculated using Eq. (24). The notations are as follows: ξr, ηs, ζs are the
coordinates of the nodes, in which the right-hand side of Eqs. (21), (17) or (18)
equals one, z0 = 1 −z1 −z2, fij is found from formulas (26), the arguments of
functions and the primes at the notations of independent variables are omitted.
The explicit expressions for the HIPs (p = 1, κmax = 6, κ′ = 2, p′ = 11), (p = 2,
κmax = 4, κ′ = 1, p′ = 11), and (p = 1, κmax = 7, κ′ = 3, p′ = 13) were calculated
too, but are not presented here because of the paper size limitations (one can
receive it with request to authors or using program TRIAHP implemented in
Maple which will be published in the library JINRLIB). The calculations were
carried out using the computer Intel Pentium CPU 987, ×64, 4 GB RAM, the
Maple 16. The computing time for the considered examples did not exceed 6 s.

Interpolation Multivariate Hermite Polynomials
147
Remark 1. At κ′ = 1 on uniform grids, one can make use of the basis with
continuous ﬁrst derivative consisting of the reduced HIP ˇϕk
r(z′) and Qs(z′) for
f01 = f11 = f22 = 1. In this case, the derivatives of such polynomials along the
direction normal to the boundary generally do not satisfy conditions (17).
Fig. 3. (a) The mesh on the domain Ωh(z) = Q
q=1 Δq of the triangle membrane
composed of triangle elements Δq (b) the proﬁles of the fourth eigenfunction Φh
4(z)
with Eh
4 = 3 + 1.90 · 10−17 obtained using the LIP of the order p′ = p = 8
Fig. 4. The error ΔEh
4 of the eigenvalue Eh
4 versus the number of elements n and the
length of the vector N
3.4
Piecewise Polynomial Functions
The piecewise polynomial functions Pl(z) with continuous derivatives to the
order κ′ are constructed by joining the polynomials ϕκ
r(z) = { ˇϕκ
r(z), Qs(z)}
from (20), obtained using the Algorithm on the ﬁnite elements Δq ∈Ωh(z) =

Q
q=1 Δq:

148
A.A. Gusev et al.
Pl′(z) =

±ϕκ
l(l′)(z), Al(l′) ∈Δq; 0, Al(l′) ̸∈Δq

,
(27)
where the sign “−” can appear only for AP2, when it is necessary to join the
normal derivatives of the odd order.
The expansion of the sought solution Φm(z) in the basis of piecewise poly-
nomial functions Pl′(z), Φh
m(z) = N
l′=1 Pl′(z)Φh
l′m and its substitution into the
variational functional (5) leads to the generalized algebraic eigenvalue prob-
lem, (A −BEh
m)Φh
m = 0, solved using the standard method (see, e.g., [3]).
The elements of the symmetric matrices of stiﬀness A and mass B comprise
the integrals like Eq. (5), which are calculated on the elements in the domain
Δq ∈Ωh(z) = 
Q
q=1 Δq, recalculated into the local coordinates on the
element Δ.
The deviation of the approximate solution Eh
m, Φh
m(z) ∈Hκ′+1≥1
2
(Ωh) from
the exact one Em, Φm(z) ∈H2
2(Ω) is theoretically estimated as [6,20]
Em −Eh
m
 ≤c1h2p′,
Φm(z) −Φh
m(z)

0 ≤c2hp′+1,
(28)
where ∥Φi(z)∥2
0 =

Ω g0(z)dzΦi(z)Φi(z), h is the maximal size of the ﬁnite ele-
ment Δq, p′ is the order of the FEM scheme, m is the number of the eigenvalue,
c1 and c2 are coeﬃcients independent of h.
4
Results and Discussion
As an example, let us consider the solution of the discrete-spectrum problem (1)–
(4) at d = 2, g0(z) = gij(z) = 1, and V (z) = 0 in the domain Ωh(z) = ∪Q
q=1Δq
in the form of an equilateral triangle with the side 4π/3 under the boundary
conditions of the second kind (3) partitioned into Q = n2 equilateral triangles Δq
with the side h = 4π/3n. The eigenvalues of this problem having the degenerate
spectrum [16,18] are the integers Em = m2
1+m2
2+m1m2 = 0, 1, 1, 3, 4, 4, 7, 7, . . .,
m1, m2 = 0, 1, 2, . . .. Figure 3 presents the ﬁnite-element mesh with the LIP of
the eighth order and the proﬁle of the fourth eigenfunction Φh
4(z). Figure 4 shows
the errors ΔEm = Eh
m −Em of the eigenvalue Eh
4 (z) depending on the number n
(the number of elements being n2) and on the length N of the vector Φh
m of the
algebraic eigenvalue problem for the FEM schemes from the ﬁfth to the ninth
order of accuracy: using LIP with the labels [pκmaxκ′] = [510], . . . , [910], and
using HIP with the labels [131], [141], [231] and [152] from Table 1, conserving
the continuity of the ﬁrst and the second derivative of the approximate solution,
respectively.
As seen from Fig. 4, the errors of the eigenvalue ΔEh
4 (z) of the FEM schemes
of the same order are nearly similar and correspond to the theoretical estimates
(28), but in the FEM schemes conserving the continuity of the ﬁrst and the
second derivatives of the approximate solution, the matrices of smaller dimension
are used that correspond to the length of the vector N smaller by 1.5–2 times
than in the schemes with LIP that conserve only the continuity of the functions
themselves at the boundaries of the ﬁnite elements. The calculations were carried

Interpolation Multivariate Hermite Polynomials
149
out using the computer 2× Xeon 3.2 GHz, 4 GB RAM, the Intel Fortran 77 with
quadruple precision real*16, with 32 signiﬁcant digits. The computing time for
the considered examples did not exceed 3 min.
5
Conclusion
We presented a symbolic-numeric algorithm, implemented in the Maple system
for analytical calculation of the basis of Hermite interpolation polynomials of
several variables, which is used to construct a FEM computational scheme of
high-order accuracy. The scheme is intended for solving the eigenvalue problem
for the elliptic partial diﬀerential equation in a bounded domain of multidimen-
sional Euclidean space. The procedure provides the continuity not only of the
approximate solution itself, but also of its derivatives to a given order. By the
example of the exactly solvable problem for the triangle membrane it is shown
that the errors for the eigenvalue are nearly the same for the FEM schemes of
the same order and correspond to the theoretical estimates. To achieve the given
accuracy of the approximate solution the FEM schemes with HIP, providing the
continuity of the ﬁrst and the second derivatives of the approximate solutions
the required matrices have smaller dimension, corresponding to the length of the
vector N smaller by 1.5–2 times than for the schemes with LIP, providing only
the continuity of the approximate solution itself at the boundaries of the ﬁnite
elements.
The FEM computational schemes are oriented at the calculations of the spec-
tral and optical characteristics of quantum dots and other quantum mechanical
systems. The implementation of FEM with HIP in the space with d ≥2 and the
domains diﬀerent from a polyhedral domain will be presented elsewhere.
The
work
was
partially
supported
by
the
Russian
Foundation
for
Basic Research (grants Nos. 16-01-00080 and 17-51-44003 Mong a) and the
Bogoliubov-Infeld program. The reported study was partially funded within the
Agreement N 02.03.21.0008 dated 24.04.2016 between the MES RF and RUDN
University.
References
1. Ames, W.F.: Numerical Methods for Partial Diﬀerential Equations. Academic
Press, London (1992)
2. Argyris, J.H., Buck, K.E., Scharpf, D.W., Hilber, H.M., Mareczek, G.: Some new
elements for the matrix displacement method. In: Proceedings of the Conference
on Matrix Methods in Structural Mechanics (2nd), Wright-Patterson Air Force
Base, Ohio, 15–17 October 1968
3. Bathe, K.J.: Finite Element Procedures in Engineering Analysis. Prentice Hall,
Englewood Cliﬀs/New York (1982)
4. Bell, K.: A reﬁned triangular plate bending element. Int. J. Numer. Methods Eng.
1, 101–122 (1969)

150
A.A. Gusev et al.
5. Brenner, S.C., Scott, L.R.: The Mathematical Theory of Finite Element Methods.
Texts in Applied Mathematics, vol. 15, 3rd edn. Springer, New York (2008). doi:10.
1007/978-0-387-75934-0
6. Ciarlet, P.: The Finite Element Method for Elliptic Problems. North-Holland Pub-
lishing Company, Amsterdam (1978)
7. Dhatt, G., Touzot, G., Lefran¸cois, E.: Finite Element Method. Wiley, Hoboken
(2012)
8. Gasca, M., Sauer, T.: On the history of multivariate polynomial interpolation. J.
Comp. Appl. Math. 122, 23–35 (2000)
9. Gusev, A.A., Chuluunbaatar, O., Vinitsky, S.I., Derbov, V.L., G´o´zd´z, A., Le Hai,
L., Rostovtsev, V.A.: Symbolic-numerical solution of boundary-value problems
with self-adjoint second-order diﬀerential equation using the ﬁnite element method
with interpolation hermite polynomials. In: Gerdt, V.P., Koepf, W., Seiler, W.M.,
Vorozhtsov, E.V. (eds.) CASC 2014. LNCS, vol. 8660, pp. 138–154. Springer, Cham
(2014). doi:10.1007/978-3-319-10515-4 11
10. Gusev, A.A., Hai, L.L., Chuluunbaatar, O., Vinitsky, S.I.: KANTBP 4M: Program
for Solving Boundary Problems of the System of Ordinary Second Order Diﬀeren-
tial Equations. http://wwwinfo.jinr.ru/programs/jinrlib/indexe.html
11. Habib, A.W., Goldman, R.N., Lyche, T.: A recursive algorithm for Hermite inter-
polation over a triangular grid. J. Comput. Appl. Math. 73, 95–118 (1996)
12. Ladyzhenskaya, O.A.: The Boundary Value Problems of Mathematical Physics.
Applied Mathematical Sciences, vol. 49. Springer, New York (1985). doi:10.1007/
978-1-4757-4317-3
13. Lekien, F., Marsden, J.: Tricubic interpolation in three dimensions. Int. J. Numer.
Meth. Eng. 63, 455–471 (2005)
14. Logg, A., Mardal, K.-A., Wells, G.N. (eds.): Automated Solution of Diﬀeren-
tial Equations by the Finite Element Method (The FEniCS Book). Springer,
Heidelberg (2012). doi:10.1007/978-3-642-23099-8
15. www.maplesoft.com
16. McCartin, B.J.: Laplacian Eigenstructure of the Equilateral Triangle. Hikari Ltd.,
Ruse, Bulgaria (2011)
17. Mitchell, A.R., Wait, R.: The Finite Element Method in Partial Diﬀerential Equa-
tions. Wiley, Chichester (1977)
18. Pockels, F.: ¨Uber die Partielle Diﬀerential-Gleichung Δu + k2u = 0 und deren
Auftreten in der Mathematischen Physik. B.G. Teubner, Leipzig (1891)
19. Ramdas Ram-Mohan, L.: Finite Element and Boundary Element Aplications in
Quantum Mechanics. Oxford University Press, New York (2002)
20. Strang, G., Fix, G.J.: An Analysis of the Finite Element Method. Prentice-Hall,
Englewood Cliﬀs/New York (1973)
21. Zienkiewicz, O.C.: Finite elements. The background story. In: Whiteman, J.R.
(ed.) The Mathematics of Finite Elements and Applications, p. 1. Academic Press,
London (1973)

Symbolic-Numerical Algorithms for Solving
the Parametric Self-adjoint 2D Elliptic
Boundary-Value Problem Using High-Accuracy
Finite Element Method
A.A. Gusev1, V.P. Gerdt1,2, O. Chuluunbaatar1,3, G. Chuluunbaatar1,2,
S.I. Vinitsky1,2(B), V.L. Derbov4, and A. G´o´zd´z5
1 Joint Institute for Nuclear Research, Dubna, Russia
gooseff@jinr.ru, vinitsky@theor.jinr.ru
2 RUDN University, 6 Miklukho-Maklaya St., Moscow 117198, Russia
3 Institute of Mathematics, National University of Mongolia,
Ulaanbaatar, Mongolia
4 N.G. Chernyshevsky Saratov National Research State University,
Saratov, Russia
5 Institute of Physics, University of Maria Curie-Sklodowska, Lublin, Poland
Abstract. We propose new
symbolic-numerical
algorithms
imple-
mented in Maple-Fortran environment for solving the parametric self-
adjoint elliptic boundary-value problem (BVP) in a 2D ﬁnite domain,
using high-accuracy ﬁnite element method (FEM) with triangular ele-
ments and high-order fully symmetric Gaussian quadratures with posi-
tive weights, and no points are outside the triangle (PI type). The algo-
rithms and the programs calculate with the given accuracy the eigenval-
ues, the surface eigenfunctions and their ﬁrst derivatives with respect to
the parameter of the BVP for parametric self-adjoint elliptic diﬀerential
equation with the Dirichlet and/or Neumann type boundary conditions
on the 2D ﬁnite domain, and the potential matrix elements, expressed
as integrals of the products of surface eigenfunctions and/or their ﬁrst
derivatives with respect to the parameter. We demonstrated an eﬃciency
of algorithms and program by benchmark calculations of helium atom
ground state.
Keywords: Parametric
elliptic
boundary-value
problem ·
Finite
element method · High-order fully symmetric high-order Gaussian
quadratures · Kantorovich method · Systems of second-order ordinary
diﬀerential equations
1
Introduction
The adiabatic representation is widely applied for solving multichannel scattering
and bound-state problems for systems of several quantum particles in molecular,
atomic and nuclear physics [6,7,11,14].
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 151–166, 2017.
DOI: 10.1007/978-3-319-66320-3 12

152
A.A. Gusev et al.
Such problems are described by elliptic boundary value problems (BVPs)
in a multidimensional domain of the conﬁguration space, solved using the
Kantorovich method, i.e., the reduction to a system of self-adjoint ordinary
diﬀerential equations (SODEs) using the basis of surface functions of an auxil-
iary BVP depending on the independent variable of the SODEs parametrically
[10,16]. The elements of matrices of variable coeﬃcients of these SODEs includ-
ing the matrix of the ﬁrst derivatives are determined by the integrals of prod-
ucts of surface eigenfunctions and/or their ﬁrst derivatives with respect to the
parameter [4].
Thus, the key problem of such a method is to develop eﬀective algorithms and
programs for calculating with given accuracy the surface eigenfunctions and the
corresponding eigenvalues of the auxiliary BVP, together with their derivatives
with respect to the parameter, and the corresponding integrals that present the
matrix elements of the eﬀective potentials in the SODEs [9].
In this paper, we propose new calculation schemes and symbolic-numerical
algorithms implemented in Maple-Fortran environment for the solution of the
parametric 2D elliptic boundary-value problem using high-accuracy ﬁnite ele-
ment method (FEM) with triangular elements. For the integration, the new
high-order fully symmetric high-order Gaussian quadratures on a triangle are
performed. We used the symbolic algorithms to generate Fortran routines that
allow the solution of the algebraic eigenvalue problem with high-dimension matri-
ces. The algorithms were implemented in a package of programs that calculate
with the given accuracy eigenvalues, eigenfunctions, and their ﬁrst derivatives
with respect to the parameter of the parametric self-adjoint elliptic diﬀerential
equations with the boundary conditions of the Dirichlet and/or Neumann type in
the 2D ﬁnite domain and the integrals of products of the surface eigenfunctions
and their ﬁrst derivatives with respect to the parameters that express the matrix
elements of the eﬀective potentials in the SODEs. Eﬃciency of the FEM scheme
is demonstrated by benchmark calculations of Helium atom ground state.
The structure of the paper is the following. In Sect. 2, the 2D FEM schemes
and algorithms for solving the parametric 2D BVP are presented. In Sect. 3,
fully symmetric high-order Gaussian quadratures are constructed. In Sect. 4,
the algorithm for calculating the parametric derivatives of eigenfunctions and
eﬀective potentials is presented. In Sect. 5, the benchmark calculations of 2D
FEM algorithms and programs are analyzed. In the Conclusion we discuss the
results and perspectives.
2
FEM Algorithm for Solving the Parametric 2D BVP
Let us consider a BVP for the parametric self-adjoint 2D PDE in the domain
Ωx, x = (x1, x2) with the piecewise continuous boundary S = ∂Ωx,
(D(x; z)−εi(z)) Φi(x; z) = 0,
(1)
D≡D(x; z) = −
1
g0(x)
⎛
⎝
2

ij=1
∂
∂xi
gij(x) ∂
∂xj
⎞
⎠+ U(x; z),
(2)

Algorithms for Solving a 2D Elliptic Boundary-Value Problem
153
with the mixed Dirichlet/Neumann boundary conditions
(I) : Φ(x; z)|S = 0,
(3)
(II) : ∂Φ(x; z)
∂nD

S= 0,
∂Φ(x; z)
∂nD
=
2

ij=1
(ˆn, ˆei)gij(x)∂Φ(x; z)
∂xj
.
(4)
Here z ∈Ωz = [zmin, zmax] is a parameter, the functions g0(x) > 0, gij(x) > 0,
and ∂xkgij(x), U(x; z), ∂zU(x; z) and ∂zΦi(x; z) are continuous and bounded
for x ∈Ωx; g12(x) = g21(x), g11(x)g22(x) −g2
12(x) > 0. Also assume that the
BVP (1), (3) has only the discrete spectrum, so that ε(z) : ε1(z) < . . . <
εjmax(z) < . . . is the desired set of real eigenvalues. The eigenfunctions satisfy
the orthonormality conditions
⟨Φi|Φj⟩=

Ω
g0(x)Φi(x; z)Φj(x; z)dx=δij,
dx = dx1dx2.
(5)
The FEM solution of the boundary-value problems (1), (3) is reduced to the
determination of stationary points of the variational functional [1,2]
Ξ(Φm, εm(z)) ≡

Ω
dxg0(x)Φm(x; z) (D −εm(z)) Φ(x; z) = Π(Φm, εm(z)), (6)
where Π ≡Π(Φm, εm(z)), Φm ≡Φm(x; z) is the symmetric quadratic functional
Π=

Ω
dx
	
2

ij=1
gij(x)∂Φm
∂xi
∂Φm
∂xj
+g0(x)Φm(U(x; z)−εm(z))Φm

.
The domain Ω(x, y) = Q
q=1 Δq, speciﬁed as a polygon in the plane (x1, x2) ∈
R2, is covered with ﬁnite elements, the triangles Δq with the vertices (x11, x21),
(x12, x22), (x13, x23) (here xik ≡xik;q, i = 1, 2, k = 1, 3, q = 1, Q). On each
of the triangles Δq (the boundary is considered to belong to the triangle), the
shape functions ϕp
l (x1, x2) are introduced. For this purpose we divide the sides of
the triangle into p equal parts and draw three families of parallel straight lines
through the partition points. The straight lines of each family are numbered
from 0 to p, so that the line passing through the side of the triangle has the
number 0, and the line passing through the opposite vertex of the triangle has
the number p.
Three straight lines from diﬀerent families intersect at one point Al ∈Δq,
which will be numbered by the triplet (n1, n2, n3), ni ≥0, n1+n2+n3 = p, where
n1, n2, and n3 are the numbers of the straight lines passing parallel to the side of
the triangle that does not contain the vertex (x11, x21), (x12, x22), and (x13, x23),
respectively. The coordinates of this point xl = (x1l, x2l) are determined by the
expression (x1l, x2l) = (x11, x21)n1/p + (x12, x22)n2/p + (x13, x23)n3/p.
As shape functions we use the Lagrange triangular polynomials ϕp
l (x) of the
order p that satisfy the condition ϕp
l (x1l′, x2l′) = δll′, i.e., equal 1 at one of the
points Al and zero at the other points.

154
A.A. Gusev et al.
In this method, the piecewise polynomial functions N p
l (x) in the domain Ω
are constructed by joining the shape functions ϕp
l (x) in the triangle Δq:
N p
l (x) = {ϕp
l (x), Al ∈Δq; 0, Al ̸∈Δq}
and possess the following properties: the functions N p
l (x) are continuous in the
domain Ω; the functions N p
l (x) equal 1 at one of the points Al and zero at the
rest points; N p
l (x1l′, x2l′) = δll′ in the entire domain Ω. Here l takes the values
l = 1, N.
The functions N p
l (x) form a basis in the space of polynomials of the pth
order. Now, the function Φ(x; z) ∈Fh
z ∼H1(Ωx) is approximated by a ﬁnite
sum of piecewise basis functions N p
l (x)
Φh(x; z) =
N

l=1
Φh
l (z)N p
l (x).
(7)
The vector function Φh = {Φh
l (z)}N
l=1 has a generalized ﬁrst-order partial
derivative and belongs to the Sobolev space H1(Ωx) [13]. After substituting
expansion (7) into the variational functional and minimizing it [1,13], we obtain
the generalized eigenvalue problem
ApΦh = εhBpΦh.
(8)
Here Ap is the stiﬀness matrix; Bp is the positive deﬁnite mass matrix; Φh is
the vector approximating the solution on the ﬁnite-element grid; and εh ≡εh(z)
is the corresponding eigenvalue. The matrices Ap and Bp have the form:
Ap={ap
ll′}N
ll′=1, Bp={bp
ll′}N
ll′=1,
(9)
where the matrix elements ap
ll′ and bp
ll′ are calculated for triangular elements as
ap
ll′ =

Δq
g0(x)ϕp
l (x; z)ϕp
l′(x; z)U(x; z) dx+
2

i,j=1

Δq
gij
∂ϕp
l (x; z)
∂xi
∂ϕp
l′(x; z)
∂xj
dx,
bp
ll′ =

Δq
g0(x)ϕp
l (x; z)ϕp
l′(x; z)dx.
Let us construct the LIP on a triangle Δq with the vertices ˆxi = (xi1, xi2, x3d).
For this purpose we introduce the local coordinate system x′ = (x′
1, x′
2) ∈R2,
in which the coordinates of the simplex vertices are the following: ˆx′
i = (x′
ik =
δik, k = 1, 2). The relation between the coordinates and derivatives is given by
the formula:
xi = x0i +
2

j=1
ˆJijx′
j,
x′
i =
2

j=1
( ˆJ−1)ij(xj −x0j),
i = 1, 2,
(10)
∂
∂x′
i
=
2

j=1
ˆJji
∂
∂xj
,
∂
∂xi
=
2

j=1
( ˆJ−1)ji
∂
∂x′
j
,
(11)

Algorithms for Solving a 2D Elliptic Boundary-Value Problem
155
where ˆJij = ˆxji −ˆx0i. When constructing the LIP in the local coordinates x′
one has to recalculate the ﬁxed derivatives at the nodes Φr′ of the element Δq
to the nodes Φ′
r′ of the element Δ, using the matrices ˆJ−1, given by cumber-
some expressions. Therefore, the required recalculation is executed based on the
relations (10) and (11) for each ﬁnite element Δq at the stage of the formation
of the LIP basis {ϕp
r(x′)}N
r=1 on the ﬁnite element Δ, implemented numerically
using the analytical formulas

Δq
dxg0(x)ϕp
rϕp
r′U(x; z) = J

Δ
dx′g0(x′)ϕp
r(x′; z)ϕp
r′(x′; z)U(x′; z),

Δq
dxgs1s2(x) ∂ϕp
r
∂xs1
∂ϕp
r′
∂xs2
=J
2

t1,t2=1
ˆJ−1
s1s2;t1t2

Δ
dx′gs1s2(x′)∂ϕp
r(x′; z)
∂x′
t1
∂ϕp
r′(x′; z)
∂x′
t2
,
where ϕp
r ≡ϕr(x; z), J = det ˆJ > 0 is the determinant of the matrix ˆJ from
Eq. (10), ˆJ−1
s1s2;t1t2 = ( ˆJ−1)t1s1( ˆJ−1)t2s2, dx′ = dx′
1dx′
2. In this case, we have
explicit expression for shape functions ϕp
l (z′
1, z′
2):
ϕp
l (x′) =
n1−1

n′
1=0
1 −x′
1 −x′
2 −n′
1/p
n1/p −n′
1/p
n2−1

n′
2=0
x′
1 −n′
2/p
n2/p −n′
2/p
n3−1

n′
3=0
x′
2 −n′
3/p
n3/p −n′
3/p. (12)
The integrals (10) are evaluated using the 2p-order 2D Gaussian quadrature.
In order to solve the generalized eigenvalue problem (8), the subspace itera-
tion method [1,13] elaborated by Bathe [1] for the solution of large symmetric
banded-matrix eigenvalue problems has been chosen. This method uses the sky-
line storage mode which stores the components of the matrix column vectors
within the banded region of the matrix, and is ideally suited for banded ﬁnite-
element matrices. The procedure chooses a vector subspace of the full solution
space and iterates upon the successive solutions in the subspace (for details, see
[1]). The iterations continue until the desired set of solutions in the iteration sub-
space converges to within the speciﬁed tolerance on the Rayleigh quotients for
the eigenpairs. If the matrix Ap in Eq. (8) is not positively deﬁned, the problem
(8) is replaced with the following problem:
˜Ap Φh = ˜εh Bp Φh,
˜Ap = Ap −αBp.
(13)
The number α (the shift of the energy spectrum) is chosen such that the
matrix ˜Ap is positive deﬁned. The eigenvector of problem (13) is the same, and
εh = ˜εh + α.
3
Fully Symmetric High-Order Gaussian Quadratures
Let consider the two-dimensional integral on triangular domain △xy with vertices
(x1, y1), (x2, y2), (x3, y3):
I =
1
S△xy

△xy
f(x, y)dydx
(14)

156
A.A. Gusev et al.
Table 1. The quadrature rule for p = 15 with np = 52, [n0, n1, n2] = [1, 5, 6], N w
i
is
the number of diﬀerent permutations of the areal coordinates ( αi, βi, γi).
Nw
i
wi
αi, βi, γi
1
0.033266408301048
0.333333333333333
0.333333333333333
0.333333333333333
3
0.045542949984995
0.202687173029433
0.398656413485283
0.398656413485283
3
0.018936193317852
0.075705168935176
0.462147415532411
0.462147415532411
3
0.046595625404608
0.555517449279976
0.222241275360011
0.222241275360011
3
0.014390824709404
0.878972401688571
0.060513799155714
0.060513799155714
3
0.000733389561154
0.822518347845233
0.088740826077383
0.088740826077383
6
0.011157489727398
0.016416695030487
0.426971506367034
0.556611798602478
6
0.031443815585368
0.096704376730713
0.328778565825110
0.574517057444176
6
0.014551780499648
0.019017867773827
0.282103601487049
0.698878530739123
6
0.010312560870261
0.015907369998417
0.141176714757054
0.842915915244527
6
0.027717303713350
0.089942179570517
0.180738614626992
0.729319205802489
6
0.002839823398123
0.004434769410597
0.037262719444011
0.958302511145391
where S△xy is a square of triangular domain △xy. Using change of variables
x = x1γ + x2α + x3β,
y = y1γ + y2α + y3β,
γ = 1 −α −β,
(15)
we obtain
I =
|J|
S△xy

△αβ
f(α, β)dβdα = 2
 1
0
 1−α
0
f(α, β)dβdα,
(16)
where J is a Jacobian and |J| = 2S△xy, and domain △αβ is an isosceles right
triangle with vertices (0, 0), (0, 1), (1, 0). The pth ordered fully symmetrical
Gaussian quadrature rules for this integral may be written as
I ≈
np

i=1
wif(αi, βi).
(17)
We consider fully symmetric rules, where if a point with areal coordinates (α, β, γ)
is used in the quadrature, then all points resulting from the N w
i permutation of
the areal coordinates are also used, with the same weight wi. Integration points
in a fully symmetric rule can thus belong to one of three diﬀerent types of point
sets, or orbits, depending on the number of areal coordinates which are equal.
The number of points for such a rule is np = n0 + 3n1 + 6n2. Here n0 is the
number of points which three areal coordinates are equal, i.e., n0 = 0 or 1.
n1 is the number of points which two areal coordinates are equal, i.e., we get
three points which lie on the medians of the triangle. n2 is the number of points
which three areal coordinates are diﬀerent, i.e., we get six points.
In paper [5], the weights and coordinates of the fully symmetric rules were
presented up to order p = 20 with minimal number of points using the moment
equations. Calculation was performed with double precision accuracy. However,

Algorithms for Solving a 2D Elliptic Boundary-Value Problem
157
Table 2. The quadrature rule for p = 16, np = 58, type [n0, n1, n2] = [1, 7, 6]
Nw
i
wi
αi, βi, γi
1
0.0415207350648329
0.3333333333333333
0.3333333333333333
0.3333333333333333
3
0.0101046137864021
0.0121739816884923
0.4939130091557539
0.4939130091557539
3
0.0363778998629740
0.1778835483267153
0.4110582258366423
0.4110582258366423
3
0.0253955775082257
0.0671491113178838
0.4664254443410581
0.4664254443410581
3
0.0359208834794810
0.5001385533336064
0.2499307233331968
0.2499307233331968
3
0.0267742614985530
0.6719362487011838
0.1640318756494081
0.1640318756494081
3
0.0136749716214666
0.8476751119345034
0.0761624440327483
0.0761624440327483
3
0.0031626040488014
0.9688994524978406
0.0155502737510797
0.0155502737510797
6
0.0266514412829383
0.1235525166817187
0.3005378086834664
0.5759096746348149
6
0.0089313378511684
0.0119532031311031
0.3372065794794446
0.6508402173894523
6
0.0152078872638436
0.0523853085701298
0.3143393035872713
0.6332753878425989
6
0.0183760532268712
0.0658032190776827
0.1786829962718098
0.7555137846505075
6
0.0080645623746130
0.0117710730623248
0.1921850841541305
0.7960438427835448
6
0.0068098562534747
0.0149594704947242
0.0806342445495042
0.9044062849557716
Table 3. The quadrature rule for p = 18, np = 76, type [n0, n1, n2] = [1, 9, 8]
Nw
i
wi
αi, βi, γi
1
0.0223535614716711
0.3333333333333333
0.3333333333333333
0.3333333333333333
3
0.0059334988479546
0.0460021789844010
0.4769989105077995
0.4769989105077995
3
0.0165585324593954
0.0730729604309092
0.4634635197845454
0.4634635197845454
3
0.0195910892704527
0.1551748557050338
0.4224125721474831
0.4224125721474831
3
0.0074160344816382
0.1550933080132821
0.4224533459933590
0.4224533459933590
3
0.0174049699198115
0.2365578681901632
0.3817210659049184
0.3817210659049184
3
0.0296996298680842
0.4863851422108091
0.2568074288945954
0.2568074288945954
3
0.0222906281899201
0.6736478731957263
0.1631760634021368
0.1631760634021368
3
0.0134460768460945
0.8559888247875595
0.0720055876062202
0.0720055876062202
3
0.0005486878691143
0.9921639450656871
0.0039180274671564
0.0039180274671564
6
0.0098384904247447
0.0120605799230755
0.4119329978824294
0.5760064221944951
6
0.0262821659985039
0.1420581973687457
0.2846674905460437
0.5732743120852107
6
0.0161450882618767
0.0645759925263757
0.3322842391902052
0.6031397682834191
6
0.0078521623046175
0.0411153725698427
0.2629574865443483
0.6959271408858090
6
0.0066043565050862
0.0091463267009754
0.2594416877532075
0.7314119855458171
6
0.0174843686058097
0.0725930398678583
0.1734580428423163
0.7539489172898254
6
0.0080232785271782
0.0147258776438553
0.1349402463458236
0.8503338760103211
6
0.0042665885840052
0.0124575576578779
0.0477763926862289
0.9397660496558932
some rules have the points outside the triangle and/or negative weights. We
need to use Gaussian quadrature rules with positive weights, and no points are
outside the triangle (so-called PI type).

158
A.A. Gusev et al.
The above Gaussian quadrature rules are constructed with Algorithm:
Step 1. Transfer the isosceles right triangular domain △αβ to the equilateral tri-
angular domain with vertices (−1, 0), (1/2, −
√
3/2), (1/2,
√
3/2), which centroid
of triangle located at the origin of the coordinate system.
Step 2. Write the moment equations in polar coordinates [5].
Step 3. Minimize nonlinear moment equation for solving n0 + 2n1 + 3n2 unkno-
wns using the Levenberg–Marquardt algorithm.
Step 4. Transformation of the calculated areal coordinates to the isosceles right
triangular domain △αβ.
A new high ordered PI type rules that are not listed in the Encyclopedia
of Quadrature Formulas [3,12] are presented in Tables 1, 2, 3 and 4 calculated
by the above algorithm implemented in Maple. In the considered problems, the
maximal number of the nonlinear moment equations equals 44, and the number
of unknowns equals 47 at p = 20. The explicit expressions for Gauss quadratures
weights and areal coordinates with 32 signiﬁcant digits were calculated, but are
not presented here because of the paper size limitations. Note, the alternative
equilateral triangle quadrature formulas were calculated in [17].
Table 4. The quadrature rule for p = 20, np = 85, type [n0, n1, n2] = [1, 8, 10]
Nw
i
wi
αi, βi, γi
1
0.0284956488386955
0.3333333333333333
0.3333333333333333
0.3333333333333333
3
0.0142039534279209
0.0474234283023599
0.4762882858488200
0.4762882858488200
3
0.0194408133550425
0.1095872167894353
0.4452063916052824
0.4452063916052824
3
0.0273065929935536
0.4916898571477065
0.2541550714261467
0.2541550714261467
3
0.0190593173083705
0.6282404953903102
0.1858797523048449
0.1858797523048449
3
0.0153240833856847
0.7827490888114787
0.1086254555942607
0.1086254555942607
3
0.0003407707226317
0.8487205009418537
0.0756397495290731
0.0756397495290731
3
0.0046354964939763
0.9218908161548015
0.0390545919225992
0.0390545919225992
3
0.0016717238812827
0.9775115344410667
0.0112442327794667
0.0112442327794667
6
0.0146283618671282
0.2120524546203612
0.3758687560757836
0.4120787893038552
6
0.0172080000328995
0.0546435084561301
0.3335452223628692
0.6118112691810008
6
0.0073409966477119
0.0097859886040601
0.4202306323332298
0.5699833790627102
6
0.0232450825127741
0.1383472868057439
0.3152308903849581
0.5464218228092980
6
0.0070480826238744
0.0106040218922527
0.2811743979692607
0.7082215801384866
6
0.0153834272762777
0.1032538874333241
0.2130007906781420
0.6837453218885339
6
0.0041951209853354
0.0070915889018085
0.1595497908201870
0.8333586202780045
6
0.0114288995104660
0.0449113089652980
0.1997044919178251
0.7553841991168769
6
0.0081140164445318
0.0377602618140266
0.1028511090917952
0.8593886290941782
6
0.0023340281749869
0.0051697211528337
0.0641281242816143
0.9307021545655520

Algorithms for Solving a 2D Elliptic Boundary-Value Problem
159
4
The Algorithm for Calculating the Parametric
Derivatives of Eigenfunctions and Eﬀective Potentials
Taking a derivative of the boundary-value problem (1)–(5) with respect to the
parameter z, we ﬁnd that ∂zΦi(x; z) is a solution of the following boundary-value
problem with the mixed boundary conditions
(D(x; z)−εi(z)) ∂Φi(x; z)
∂z
=−
	 ∂
∂z (U(x; z)−εi(z))

Φi(x; z),
∂Φ(x; z)
∂z

S
= 0 or ∂2Φ(x; z)
∂nD∂z

S= 0.
(18)
The parametric BVP (18) has a unique solution, if and only if it satisﬁes the
conditions
∂εi(z)
∂z
=

Ω
dxg0(x) (Φi(x; z)) ∂U(x; z)
∂z
Φi(x; z),
(19)

Ω
dxg0(x)Φi(x; z)∂Φi(x; z)
∂z
= 0.
(20)
Below we present an eﬃcient numerical method that allows the calculation of
∂zΦi(x; z) with the same accuracy as achieved for the eigenfunctions of the BVP
(1)–(5) and the use of it for computing the matrices of the eﬀective potentials
deﬁned as
Hij(z)=Hji(z)=

Ω
dxg0(x)∂Φi(x; z)
∂z
∂Φj(x; z)
∂z
,
(21)
Qij(z)=−Qji(z)=−

Ω
dxg0(x)Φi(x; z)∂Φj(x; z)
∂z
.
(22)
The boundary-value problem (18)–(20) is reduced to the linear system of
inhomogeneous algebraic equations with respect to the unknown ∂Φh/∂z:
L∂Φh
∂z
≡(Ap −εhBp)∂Φh
∂z
= b,
b = −
∂Ap
∂z −∂εh
∂z Bp

Φh.
(23)
The normalization condition (5), the condition of orthogonality between the
function and its parametric derivative (20), and the additional conditions (19)
for the solution of (23) read as

ΦhT
BpΦh = 1,

∂Φh
∂z
T
BpΦh = 0,
∂εh
∂z =

ΦhT ∂Ap
∂z Φh.
(24)
Then the potential matrix elements Hh
ij(z) and Qh
ij(z) (21) can be calculated
using the formulas
Hh
ij(z) =

∂Φh
i
∂z
T
Bp ∂Φh
j
∂z ,
Qh
ij(z) = −

Φh
i
T
Bp ∂Φh
j
∂z .
(25)

160
A.A. Gusev et al.
Since εh is an eigenvalue of (8), the matrix L in Eq. (23) is degenerate. In this
case, the algorithm for solving Eq. (23) can be written in three steps as follows:
Step k1. Calculate the solutions v and w of the auxiliary inhomogeneous sys-
tems of algebraic equations
¯Lv = ¯b,
¯Lw = d,
(26)
with the non-degenerate matrix ¯L and the right-hand sides ¯b and d
¯Lss′ =
Lss′, (s −S)(s′ −S) ̸= 0,
δss′, (s −S)(s′ −S) = 0,
(27)
¯bs =

bs, s ̸= S,
0, s = S,
ds =

LsS, s ̸= S,
0,
s = S,
(28)
where S is the number of the element of the vector BpΦh having the greatest
absolute value.
Step k2. Evaluate the coeﬃcient γ
γ = −
γ1
(DS −γ2),
γ1 = vT BpΦh,
γ2 = wT BpΦh,
DS = (BpΦh)S. (29)
Step k3. Evaluate the vector ∂zΦh
∂Φh
s
∂z =
vs −γws, s ̸= S,
γ,
s = S.
(30)
From the above consideration, it is evident that the computed derivative has the
same accuracy as the calculated eigenfunction.
Let D(x; z) in Eq. (1) be a continuous and bounded positive deﬁnite operator
on the space H1 with the energy norm, εi(z), Φi(x, z) ∈H2 being the exact solu-
tions of Eqs. (1)–(5), and εh
i (z), Φh
i (x; z) ∈H1 being the corresponding numerical
solutions. Then the following estimates are valid [13]
εi(z) −εh
i (z)
 ≤c1h2p,
Φi(x; z) −Φh
i (x; z)

0 ≤c2hp+1,
(31)
∥Φi(x; z)∥2
0 =

Ωx
dxg0(x)Φi(x; z)Φi(x; z),
(32)
where h is the largest distance between any two points in Δq, p is the order
of the ﬁnite elements, i is the number of the corresponding solutions, and the
constants c1 and c2 are independent of the step h.
The following theorem can be formulated.
Theorem. Let D(x; z) in Eq. (1) be a continuous and bounded positive deﬁnite
operator on the space H1 with the energy norm. Also let ∂zU(x; z) be continuous
and bounded for each value of the parameter z. Then for the exact values of
the solutions ∂zεi(z), ∂zΦi(x; z) ∈H2, Hij(z), Qij(z) from (18)–(21) and the

Algorithms for Solving a 2D Elliptic Boundary-Value Problem
161
corresponding numerical values ∂zεh
i (z), ∂zΦh
i (x; z) ∈H1, Hh
ij(z), Qh
ij(z) from
(23)–(25), the following estimates are valid:

∂εi(z)
∂z
−∂εh
i (z)
∂z
 ≤c3h2p,

∂Φi(x; z)
∂z
−∂Φh
i (x; z)
∂z

0
≤c4hp+1,
Qij(z) −Qh
ij(z)
 ≤c5h2p,
Hij(z) −Hh
ij(z)
 ≤c6h2p,
(33)
where h is the largest distance between any two points of the ﬁnite element
Δq, p is the order of ﬁnite elements, i, j are the numbers of the corresponding
solutions, and the constants c3, c4, c5, and c6 are independent of the step h.
The proof is straightforward following the scheme in accordance with [13].
5
Benchmark Calculations of Helium Atom Ground State
In the hyperspheroidal coordinates 0 ≤R < ∞, 1 ≤ξ < ∞, −1 ≤η ≤1
r12 =
√
2R

ξ2 + η2 ,
r1 =
R(ξ + η)
√
2

ξ2 + η2 ,
r2 =
R(ξ −η)
√
2

ξ2 + η2
(34)
the equation for the wave functions Ψ(R, ξ, η) =

ξ2 + η2Φ(R, ξ, η) for S-states
of the Helium atom reads as [15]

−1
R5
∂
∂RR5 ∂
∂R −3
R2 −1
R2
(ξ2 + η2)2
ξ2 −η2
 ∂
∂ξ (ξ2 −1) ∂
∂ξ + ∂
∂η (1 −η2) ∂
∂η

+
√
2

ξ2 + η2
R

1 −
8ξ
ξ2 −η2

−2E

Φ(R, ξ, η) = 0.
(35)
The function Φ(R, ξ, η) satisﬁes the boundary conditions
lim
R→0 R5 ∂Φ(R, ξ, η)
∂R
= 0,
lim
R→∞R5Φ(R, ξ, η) = 0,
lim
ξ→1(ξ2 −1)∂Φ(R, ξ, η)
∂ξ
= 0,
lim
ξ→∞Φ(R, ξ, η) = 0,
lim
η→±1(1 −η2)∂Φ(R, ξ, η)
∂η
= 0,
(36)
and is normalized by the condition
8π2
 ∞
0
dRR5
 ∞
1
dξ
 1
−1
dη ξ2 −η2
(ξ2 + η2)2 Φ2(R, ξ, η) = 1.
(37)
The parametric function φi ≡φi(ξ, η; R) and the corresponding potential
curves εi(R) are eigensolutions of the 2D BVP having a purely discrete spectrum

−∂
∂ξ (ξ2−1) ∂
∂ξ −∂
∂η (1−η2) ∂
∂η +
√
2R

ξ2−η2−8ξ


ξ2+η23
−εi(R) ξ2−η2
(ξ2+η2)2

φi = 0(38)

162
A.A. Gusev et al.
subject to the following boundary conditions
lim
ξ→1(ξ2−1)∂φi(ξ, η; R)
∂ξ
= 0,
lim
ξ→∞φi(ξ, η; R) = 0,
lim
η→±1(1−η2)∂φi(ξ, η; R)
∂η
= 0,
and the normalization condition
 ∞
1
dξ
 1
−1
dη ξ2 −η2
(ξ2 + η2)2 φ2
i (ξ, η; R) = 1.
(39)
In terms of scaled variable and parametric surface functions
ξ = 1 + λ
1 −λ,
0 ≤λ < 1,
φi(ξ, η; R) = pi(ξ, η; R)
ξ + 1
≡pi(λ, η; R)
ξ + 1
,
(40)
we rewrite the 2D BVP (38)–(39) in the form

−∂
∂λλ(1−λ)2 ∂
∂λ−∂
∂η (1−η2) ∂
∂η +
√
2R(1−λ)(1+λ)2−(1−λ)2η2−8(1−λ2)

(1+λ)2+(1−λ)2η23
+1−λ−εi(R)(1−λ)2 (1+λ)2−(1−λ)2η2
((1+λ)2+(1−λ)2η2)2

pi(λ, η; R) = 0.
(41)
The surface functions pi(λ, η; R) satisfy the following boundary and normaliza-
tion conditions
lim
λ→0,1 λ(1−λ)∂pi(λ, η; R)
∂λ
= 0,
lim
η→±1(1−η2)∂pi(λ, η; R)
∂η
= 0,
(42)
1
2
 1
0
dλ
 1
−1
dη(1−λ)2 (1+λ)2−(1−λ)2η2
((1+λ)2+(1−λ)2η2)2 p2
i (λ, η; R) = 1.
(43)
The numerical experiments in the ﬁnite-element grids have shown a strict
correspondence with the theoretical estimations (31) and (33) for the eigenvalues,
eigenfunctions, and the matrix elements. In particular, we calculated the values
of the Runge coeﬃcients
βl = log2
(σh
l −σh/2
l
)/(σh/2
l
−σh/4
l
)
 ,
l = 1, 2,
(44)
with absolute errors on three twice condensed grids for their eigenvalues and
eigenfunctions, respectively
σh
1 = |E2h
m (z) −Eh
m(z)|,
σh
2 = ∥Φ2h
m (x; z) −Φh
m(x; z)∥0.
(45)
The Runge coeﬃcients for six eigenvalues presented in Table 5 equal 7.52 ÷
8.19 and for their parametric derivatives equal 7.46 ÷ 7.76 are nearly similar
and correspond to the theoretical estimates (31) and (33) for the fourth-order
scheme (2p ≈8).
The calculations were carried out using the server 2 × 4 kernels i7k (i7-3770K
4.5 GHz, 32 GB RAM, GPU GTX680), and the Intel Fortran compiler 17.0. The

Algorithms for Solving a 2D Elliptic Boundary-Value Problem
163
Table 5. Comparison of the transformed potential curves Ej(R) = (εj(R) −3)/4 and
their ﬁrst derivative with respect to parameter R with results [9] at jmax = 12. The
mesh points are λ = {0(L)1} and η = {0(L)1}, and R = 7.65 a.u.
j
Ej(R) (L = 40)
∂REj(R) (L = 40) Ej(R) [9]
∂REj(R) [9]
1 −63.499 153 248 −15.796 136 178
−63.499 153 256 −15.796 136 189
2 −21.451 891 391 −3.997 429 168
−21.451 886 907 −3.997 431 891
3 −19.082 406 592 −4.142 660 217
−19.082 325 834 −4.142 711 985
4 −13.371 481 961 −3.897 822 460
−13.371 480 623 −3.897 824 374
5 −11.876 679 683 −3.314 363 652
−11.876 677 566 −3.314 347 679
6 −8.898 981 042
−2.705 445 931
−8.897 839 854
−2.705 544 197
j
Ej(R) (L = 20)
∂REj(R) (L = 20) Ej(R) (L = 10)
∂REj(R) (L = 10)
1 −63.499 151 482 −15.796 133 881
−63.498 825 358 −15.795 727 590
2 −21.451 891 369 −3.997 429 139
−21.451 886 770 −3.997 423 220
3 −19.082 406 568 −4.142 660 186
−19.082 401 572 −4.142 653 692
4 −13.371 481 948 −3.897 822 446
−13.371 479 034 −3.897 819 472
5 −11.876 679 657 −3.314 363 641
−11.876 674 062 −3.314 361 245
6 −8.898 980 996
−2.705 445 914
−8.898 971 861
−2.705 442 515
Table 6. Matrix elements Hji(R), i, j = 1, ..., 6 at R = 7.65.
.1291804E-1
−.1264117E-1
.7293917E-2
.3763094E-2
−.1051774E-1
−.6007265E-2
−.1264117E-1
.3871021E-1
−.4493495E-2
−.1899806E-1
.2378084E-1
.5400750E-2
.7293917E-2
−.4493495E-2
.3270711E-1
.2565576E-1
.2270581E-1
−.1199926E-1
.3763094E-2
−.1899806E-1
.2565576E-1
.8136326E-1
.9664928E-2
−.2314799E-1
−.1051774E-1
.2378084E-1
.2270581E-1
.9664928E-2
.8335278E-1
.1949047E-1
−.6007265E-2
.5400750E-2
−.1199926E-1
−.2314799E-1
.1949047E-1
.2743837E-1
Table 7. Matrix elements Qji(R), i, j = 1, ..., 6 at R = 7.65.
.37E-15
−.5859058E-1 .2863643E-1
.4422091E-1
.3362249E-1
.1621148E-1
.5859058E-1
.43E-16
.2502732E-1
−.1657796E+0 −.6079201E-1
−.1728211E-1
−.2863643E-1 −.2502732E-1 .36E-15
−.4584596E-1
.1345970E+0
.8980072E-1
−.4422091E-1 .1657796E+0
.4584596E-1
−.12E-15
.2029277E+0
.1556143E-1
−.3362249E-1 .6079201E-1
−.1345970E+0 −.2029277E+0 .92E-16
.1142082E+0
−.1621148E-1 .1728211E-1
−.8980072E-1
−.1556143E-1
−.1142082E+0 .13E-15
computing time for the considered examples with 10−12 accuracy on the uniform
grids λ = {0(L)1}, η = {0(L)1} at L = 10, 20, 40 is 0.38, 5.08, and 41.21 s,
respectively. The matrix elements Qij(R) and Hij(R) are presented in Tables 6
and 7. As an example eigenfunctions and their parametric derivatives are shown
in Figs. 1 and 2.

164
A.A. Gusev et al.
Fig. 1. The eigenfunction p1(λ, η; R) and its parametric derivative ∂Rp1(λ, η; R) at
R = 7.65.
Fig. 2. The eigenfunction p4(λ, η; R) and its parametric derivative ∂Rp4(λ, η; R) at
R = 7.65.
We seek for the solution of the BVP (35)–(37) by Kantorovich expansion
Φ(R, ξ, η) =
jmax

j=1
φj(ξ, η; R)χj(R)
(46)

Algorithms for Solving a 2D Elliptic Boundary-Value Problem
165
over the eigenfunctions φj(ξ, η; R) of the parametric 2D BVP having a purely
discrete spectrum Ej(R) = (εj(R) −3)/R2, j = 1, 2, .... Substituting expansion
(46) into the 3D BVP Eqs. (35)–(37), we get the 1D BVP for a ﬁnite set of jmax
coupled SOODEs for χ(R) = {χ1(R), ..., χN(R)}T

−1
R5 I d
dRR5 d
dR+V(R)+Q(R) d
dR+ 1
R5
dR5Q(R)
dR
−2E I

χ(R) = 0,
with the boundary and normalization conditions
lim
R→0 R5 dχ(R)
dR
= 0,
lim
R→∞R5χ(R) = 0,
8π2
 ∞
0
dRR5(χ(R))T χ(R) = 1.
The solution of this BVP with the help of KANTBP program [8] on the non-
uniform grids R = {0(50), 5, (75), 20} using calculated Ej(R), Vij(R) = Hij(R),
Vjj(R) = Hjj(R) + Ej(R), Qij(R), i, j = 1, ..., 12 gives us the energy of Helium
atom ground state E1 = −2.90372430 a.u. with 8 signiﬁcant digits.
6
Conclusion
We have elaborated new calculation schemes, algorithms, and the program for
solving the parametric 2D elliptic BVP using the high-accuracy FEM with tri-
angular elements. The program calculates the potential matrix elements, the
integrals of the eigenfunctions multiplied by their ﬁrst derivatives with respect
to the parameter. The parametric eigenvalues (potential curves) and the matrix
elements computed by the program can be used for solving the bound-state
and multi-channel scattering problems for a system of the coupled second-order
ODES with using the Kantorovich method. We demonstrated the eﬃciency of
the proposed ﬁnite element schemes, algorithms, and codes by benchmark cal-
culations of BVPs of helium atom ground state.
The
work
was
partially
supported
by
the
Russian
Foundation
for
Basic Research (grants Nos. 16-01-00080 and 17-51-44003 Mong a) and the
Bogoliubov-Infeld program. The reported study was funded by the Agreement
N 02.03.21.0008 dated 24.04.2016 between the MES of the RF and RUDN
University.
References
1. Bathe, K.J.: Finite Element Procedures in Engineering Analysis. Prentice Hall,
Englewood Cliﬀs (1982)
2. Ciarlet, P.: The Finite Element Method for Elliptic Problems. North-Holland Publ.
Comp., Amsterdam (1978)
3. Cools, R.: An encyclopaedia of quadrature Formulas. J. Complex. 19, 445 (2003).
http://nines.cs.kuleuven.be/ecf/
4. Chuluunbaatar, O., Gusev, A.A., Abrashkevich, A.G., Amaya-Tapia, A., Kaschiev,
M.S., Larsen, S.Y., Vinitsky, S.I.: KANTBP: a program for computing energy lev-
els, reaction matrix and radial wave functions in the coupled-channel hyperspher-
ical adiabatic approach. Comput. Phys. Commun. 177, 649–675 (2007)

166
A.A. Gusev et al.
5. Dunavant, D.A.: High degree eﬃcient symmetrical Gaussian quadrature rules for
the triangle. Int. J. Numer. Methods Eng. 21, 1129–1148 (1985)
6. Esry, B.D., Lin, C.D., Greene, C.H.: Adiabatic hyperspherical study of the helium
trimer. Phys. Rev. A 54, 394–401 (1996)
7. Fano, U., Rau, A.R.P.: Atomic Collisions and Spectra. Academic Press, Florida
(1986)
8. Gusev, A.A., Chuluunbaatar, O., Vinitsky, S.I., Abrashkevich, A.G.: KANTBP
3.0: new version of a program for computing energy levels, reﬂection and transmis-
sion matrices, and corresponding wave functions in the coupled-channel adiabatic
approach. Comput. Phys. Commun. 185, 3341–3343 (2014)
9. Gusev, A.A., Chuluunbaatar, O., Vinitsky, S.I., Abrashkevich, A.G.: POTHEA: a
program for computing eigenvalues and eigenfunctions and their ﬁrst derivatives
with respect to the parameter of the parametric self-adjoined 2D elliptic partial
diﬀerential equation. Comput. Phys. Commun. 185, 2636–2654 (2014)
10. Kantorovich, L.V., Krylov, V.I.: Approximate Methods of Higher Analysis. Wiley,
New York (1964)
11. Kress, J.D., Parker, G.A., Pack, R.T., Archer, B.J., Cook, W.A.: Comparison
of Lanczos and subspace iterations for hyperspherical reaction path calculations.
Comput. Phys. Commun. 53, 91–108 (1989)
12. Papanicolopulos,
S.-A.:
Analytical
computation
of
moderate-degree
fully-
symmetric quadrature rules on the triangle. arXiv:1111.3827v1 [math.NA]
13. Strang, G., Fix, G.J.: An Analysis of the Finite Element Method. Prentice-Hall,
Englewood Cliﬀs (1973)
14. Vinitskii, S.I., Ponomarev, L.I.: Adiabatic representation in the three-body prob-
lem with Coulomb interaction. Sov. J. Part. Nucl. 13, 557–587 (1982)
15. Vinitsky, S.I., Gusev, A.A., Chuluunbaatar, O., Derbov, V.L., Zotkina, A.S.: On
calculations of two-electron atoms in spheroidal coordinates mapping on hyper-
sphere. In: Proceedings of SPIE, vol. 9917, p. 99172Z (2016)
16. Vlasova, Z.A.: On the method of reduction to ordinary diﬀerential equations. Trudy
Mat. Inst. Steklov. 53, 16–36 (1959)
17. Zhang, L., Cui, T., Liu, H.: A set of symmetric quadrature rules on triangles and
tetrahedra. J. Comput. Math. 27, 89–96 (2009)

A Symbolic Study of the Satellite Dynamics
Subject to Damping Torques
Sergey A. Gutnik1(B) and Vasily A. Sarychev2
1 Moscow State Institute of International Relations (University), 76, Prospekt
Vernadskogo, Moscow 119454, Russia
s.gutnik@inno.mgimo.ru
2 Keldysh Institute of Applied Mathematics (Russian Academy of Sciences), 4,
Miusskaya Square, Moscow 125047, Russia
vas31@rambler.ru
Abstract. The dynamics of the rotational motion of a satellite moving
in the central Newtonian force ﬁeld in a circular orbit under the inﬂuence
of gravitational and active damping torques is investigated with the help
of computer algebra methods. The properties of a nonlinear algebraic
system that determines equilibrium orientations of a satellite under the
action of gravitational and active damping torques were studied. An algo-
rithm for the construction of a Gr¨obner basis is proposed for determining
the equilibrium orientations of a satellite with given central moments of
inertia and given damping torques. The conditions of the equilibria’s
existence were obtained by the analysis of real roots of algebraic equa-
tions from the constructed Gr¨obner basis. The domains with an equal
number of equilibria were speciﬁed by using algebraic methods for the
construction of discriminant hypersurfaces. The conditions of asymptotic
stability of the satellite’s equilibria were determined as a result of the
analysis of linearized equations of motion using Routh–Hurwitz criterion.
1
Introduction
In this paper, a symbolic investigation of a satellite dynamics under the inﬂuence
of gravitational and active damping torques is presented. The gravity orienta-
tion systems are based on the fact that a satellite with diﬀerent moments of
inertia in the central Newtonian force ﬁeld in a circular orbit has 24 equilibrium
orientations and four of them are stable [1]. An important property of gravity
orientation systems is that these systems can operate for a long time without
spending energy. The problem to be analyzed in the present work is related to
the behavior of the satellite acted upon by the gravity gradient and active damp-
ing torques. We assume that active damping torques depend on the projections
of the angular velocity of the satellite. Such active damping torques can be pro-
vided by using the angular velocity sensor. The action of damping torques both
leads to new equilibrium orientations and can provide the asymptotic stability
of the well known equilibria of the gravity oriented satellites. Therefore, it is
necessary to study the joint action of gravitational and active damping torques
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 167–182, 2017.
DOI: 10.1007/978-3-319-66320-3 13

168
S.A. Gutnik and V.A. Sarychev
and, in particular, to analyze the necessary and suﬃcient conditions for asymp-
totic stability of the satellite’s equilibria in a circular orbit. Such solutions can
be used in practical space technology in the design of control orientation systems
of the satellites.
In the present work, the problem of determination of the classes of equilibrium
orientations and the conditions for asymptotic stability of deﬁned equilibria for
the general values of damping torques is considered. The equilibrium orientations
are determined by real roots of the system of algebraic equations. The investi-
gation of equilibria was performed by using the computer algebra Gr¨obner basis
methods. The evolution of domains with a ﬁxed number of equilibria is inves-
tigated by the analysis of the singular points of the discriminant hypersurface
depending on three dimensionless damping parameters.
The conditions of equilibria stability are determined as a result of an analysis
of the linearized equations of motion using the Routh–Hurwitz criterion. The
detailed investigation of the regions of the necessary and suﬃcient conditions
of stability is carried out by a numerical-analytical method in the plane of two
dimensionless inertia parameters at diﬀerent values of damping coeﬃcients. The
types of transition decay processes of spatial oscillations of a satellite at diﬀerent
damping parameters have been investigated numerically.
The computer algebra methods for determination of the equilibrium orien-
tation of a satellite had been successfully used earlier to analyze the equilib-
rium orientations of a satellite under the inﬂuence of gravitational and constant
torques [3]. The study of the equilibria of polynomial dynamical systems by
means of symbolic computation is a very popular application of computer alge-
bra. The detailed analysis of typical problems on parametric dynamical systems
and computer algebra algorithms for solving this problem was presented at the
CASC 2011 Workshop [4]. The symbolic methods for analyzing the stability of
the equilibria of polynomial dynamical systems were presented at the CASC
2002 [5] and CASC 2007 Workshops [6].
2
Equations of Motion
Consider the attitude motion of a satellite subjected to gravitational and active
damping torques in a circular orbit. We assume that the satellite is a triaxial
rigid body, and active damping torques depend on the projections of the angular
velocity of the satellite. To write the equations of motion we introduce two right-
handed Cartesian coordinate systems with origin at the satellite’s center of mass
O. The orbital coordinate system is OXY Z, where the OZ axis is directed along
the radius-vector connecting the centers of mass of the Earth and the satellite,
the OX axis is in the direction of a satellite orbital motion. Then, the OY axis
is directed along the normal to the orbital plane. The satellite body coordinate
system is Oxyz, where Ox, Oy, and Oz are the principal central axes of inertia
of the satellite. The orientation of the satellite body coordinate system Oxyz
with respect to the orbital coordinate system is determined by means of the
aircraft angles of pitch (α), yaw (β), and roll (γ), and the direction cosines in

Inﬂuence of Damping Torques on Satellite Dynamics
169
the transformation matrix between the orbital coordinate system OXY Z and
Oxyz are represented by the following expressions [2]:
a11 = cos(x, X) = cos α cos β,
a12 = cos(y, X) = sin α sin γ −cos α sin β cos γ,
a13 = cos(z, X) = sin α cos γ + cos α sin β sin γ,
a21 = cos(x, Y ) = sin β,
a22 = cos(y, Y ) = cos β cos γ,
a23 = cos(z, Y ) = −cos β sin γ,
a31 = cos(x, Z) = −sin α cos β,
a32 = cos(y, Z) = cos α sin γ + sin α sin β cos γ,
a33 = cos(z, Z) = cos α cos β −sin α sin β sin γ.
(1)
For small oscillations of the satellite, the angles of pitch, yaw, and roll correspond
to the rotations around the OY, OZ, and OX axes, respectively.
Let the satellite be acted upon by the moments of active damping, their
integral vector projections on the axes Ox, Oy, and Oz are equal to the following
values: Mx = ¯k1p1, My = ¯k2(q1−ω0), and Mz = ¯k3r1. Here ¯k1, ¯k2, and ¯k3 are the
damping coeﬃcients, p1, q1, and r1 are the projections of the satellite’s angular
velocity onto the axes Ox, Oy, and Oz; ω0 is the angular velocity of the orbital
motion of the satellite’s center of mass. The equations of satellite attitude motion
can then be written in the Euler form:
Ap′
1 + (C −B)q1r1 −3ω2
0(C −B)a32a33 + ¯k1p1 = 0,
Bq′
1 + (A −C)r1p1 −3ω2
0(A −C)a31a33 + ¯k2(q1 −ω0) = 0,
Cr′
1 + (B −A)p1q1 −3ω2
0(B −A)a31a32 + ¯k3r1 = 0,
(2)
p1 = (α′ + ω0)a21 + γ′,
q1 = (α′ + ω0)a22 + β′ sin γ,
r1 = (α′ + ω0)a23 + β′ cos γ.
(3)
Here A, B, and C are the principal central moments of inertia of the satellite.
The prime denotes the diﬀerentiation with respect to time t.
After the introduction of dimensionless parameters θA = A/B, θC = C/B,
p = p1/ω0, q = q1/ω0, r = r1/ω0, ˜k1 = ¯k1/Bω0, ˜k2 = ¯k2/Bω0, ˜k3 = ¯k3/Bω0, and
τ = ω0t one can rewrite system (2)–(3) in the form
θA ˙p + (θC −1)qr −3(θC −1)a32a33 + ˜k1p = 0,
˙q + (θA −θC)rp −3(θA −θC)a31a33 + ˜k2(q −1) = 0,
θC ˙r + (1 −θA)pq −3(1 −θA)a31a32 + ˜k3r = 0,
(4)
p = ( ˙α + 1)a21 + ˙γ,
q = ( ˙α + 1)a22 + ˙β sin γ,
r = ( ˙α + 1)a23 + ˙β cos γ.
(5)
The dot denotes the diﬀerentiation with respect to τ.

170
S.A. Gutnik and V.A. Sarychev
3
Equilibrium Orientations of Satellite
Setting in (2) and (3) α = α0 = const, β = β0 = const, γ = γ0 = const, we
obtain at A ̸= B ̸= C the equations
a22a23 −3a32a33 + k1a21 = 0,
a21a23 −3a31a33 + k2(a22 −1) = 0,
a21a22 −3a31a32 + k3a23 = 0,
(6)
which allow us to determine the satellite equilibria in the orbital coordinate
system. Here k1 = ˜k1/(C −B), k2 = ˜k2/(A −C), and k3 = ˜k3/(B −A). We
will consider the case when damping coeﬃcients k1, k2, and k3 are positive.
Substituting the expressions for the direction cosines from (1) in terms of the
aircraft angles into Eq. (6), we obtain three equations with three unknowns α, β,
and γ. Another way of closing Eq. (6) is to add the following three conditions
for the orthogonality of direction cosines:
a2
21 + a2
22 + a2
23 −1 = 0,
a2
31 + a2
32 + a2
33 −1 = 0,
a21a31 + a22a32 + a23a33 = 0.
(7)
Equations (6) and (7) form a closed system of equations with respect to the
six direction cosines identifying the satellite equilibrium orientations. For this
system of equations, we formulate the following problem: for given values of
k1, k2, and k3, it is required to determine all the nine directional cosines, i.e.,
all satellite equilibrium orientations in the orbital coordinate system. After
a21, a22, a23, a31, a32, and a33 are found, the direction cosines a11, a12, and a13
can be determined from the conditions of orthogonality.
It should be noted that to solve system (6), (7) it is suﬃcient to ﬁnd the
values of two unknowns a21 and a22. Indeed, for each value a21 and a22, one can
ﬁnd two values of a23 from the ﬁrst equation of system (7) and then uniquely
determine their corresponding values a31, a32, and a33 from system (6), (7).
To ﬁnd solutions of the algebraic system (6), (7) we used the algorithm for
constructing the Gr¨obner bases [7]. The method for constructing a Gr¨obner basis
is an algorithmic procedure that reduces the problem in the case of polynomials
of several variables to a problem with a polynomial of a single variable.
In our study, for Gr¨obner bases construction, we applied the command
Groebner[Basis] from the package Groebner implemented in the computer
algebra system Maple 15 [8]. We constructed the Gr¨obner basis of the system of
six second-order polynomials (6), (7) with six variables aij (i = 2, 3; j = 1, 2, 3),
with respect to the lexicographic ordering of variables by using option plex. In
the list of polynomials F:=[fi(i = 1, 2, . . . 6)], fi are the left–hand sides of the
algebraic equations (6), (7):
G:=map(factor,Groebner[Basis]([F, plex(a31, ... a22))).
Here, calculating the Gr¨obner basis over the ﬁeld of rational functions in
k1, k2, and k3, we compute the generic solutions of our problem only. In our task

Inﬂuence of Damping Torques on Satellite Dynamics
171
from the area of satellite dynamics, the main goal of the study is to estimate a
wide range of system parameters for which the satellite’s equilibria exist, and
the task is to determine the regions in the space of parameters for which these
equilibria are asymptotically stable.
Taking into account the errors of the angular velocity sensors and the errors of
the signals, which generate damping torques, the exact bifurcation values of the
coeﬃcients are very diﬃcult to obtain in practice. We are interested in estimating
the size of regions in the space of damping parameters where equilibria exist.
In the case of parametric dynamical system solving, when the parameters reach
non-generic solutions, the symbolic application based on comprehensive Gr¨obner
bases [9], discriminant varieties [10] and comprehensive triangular decomposition
[4] methods are used.
Here we write down the polynomial in the Gr¨obner basis that depends only
on one variable x = a22. This polynomial has the form
P(a22, k1, k2, k3) = (a2
22 −1)[(k1k2 +k2k3 +k1k3 −4)a22 −k2(k1 +k3)] = 0. (8)
To determine the equilibria it is required to consider separately the following
three cases: a22 = 1, a22 = −1 and (k1k2 +k2k3 +k1k3 −4)a22 −k2(k1 +k3) = 0.
In the ﬁrst case, when a22 = 1 (a21 = a23 = 0), we will get the following
eight equilibrium solutions from system (6) and (7):
a2
31 = 1, a32 = a33 = 0; a2
32 = 1, a31 = a33 = 0; a2
33 = 1, a32 = a33 = 0.
(9)
In the second case, when a22 = −1, system (6), (7) takes the form
a32a33 = 0, a31a32 = 0,
a31a33 + 2k2 = 0,
a2
31 + a2
33 = 1.
(10)
From (10) we obtain the following solutions:
a32 = 0, a31 = −2k2/3a33;
9a4
33 −9a2
33 + 4k2
2 = 0,
a2
33 = 3 −

9 −16k2
2
2
.
(11)
Solutions (11) exist in the case when the discriminant of the biquadratic equation
9a4
33 −9a2
33 +4k2
2 = 0 is non-negative and a2
33 ≤1. These conditions are satisﬁed
when k2
2 ≤1/2.
Now let us consider the third case, where the satellite equilibrium solutions
are determined by the linear equation (k1k2+k2k3+k1k3−4)a22−k2(k1+k3) = 0,
from which we can obtain:
a22 =
k2(k1 + k3)
k1k2 + k2k3 + k1k3 −4.
(12)
From the condition for the existence of a solution for the direction cosine a22 ≤1,
we obtain the inequality k1k3 ≥4. From the condition a22 ≥−1, we obtain the

172
S.A. Gutnik and V.A. Sarychev
inequality 2k1k2+2k2k3+k1k3 ≥4. Consequently, solution (12) is possible when
the inequality k1k3 ≥4 holds.
Thus, from Eq. (8), we obtain all possible values of the direction cosine a22
satisfying the initial system (6), (7).
To ﬁnd the a21 values, we have recalculated the Gr¨obner basis with respect
to the variable a21. The polynomial depending on only one variable a21 in the
Gr¨obner basis obtained is given by
P(a21) = p0a8
21 + p1a6
21 + p2a4
21 + p3a2
21 + p4 = 0,
(13)
where
p0 = p8
01,
p01 = k1k2 + k2k3 + k1k3 −4,
p1 = −2p6
01p11,
p11 = (k1k3 −4)2 + 2k2(k1 + k3)(k1k3 −4) + k2
2(k2
1 −k2
3),
p2 = p4
01p21,
p21 = (k1k3 −4)4 + 4k2(k1 + k3)(k1k3 −4)3
+ k2
2(6k2
1 + 8k1k3 + 17)(k1k3 −4)2
+ 2k3
2(k1 + k3)(2k2
1 −4k2
3 + 17)(k1k3 −4)
+ k4
2(k1 + k3)2((k1 −k3)2 + 25),
p3 = p2
01p31p32,
p31 = k2
2(k1k3 −4)(2k1k2 + 2k2k3 + k1k3 −4),
p32 = (2k2
3 −17)(k1k3 −4)2 + 2k2(2k2
3 −17)(k1 + k3)(k1k3 −4)
+ k2
2(k1 + k3)(2k2
3(k1 −k3) −17k1 −33k3),
p4 = (k2
3 + 4)2p2
31.
Equation (13) together with (12), (6), and (7) can be used to determine all the
equilibrium orientations of the satellite under the inﬂuence of gravitational and
active damping torques.
The number of real roots of the algebraic equation (13) is even and does not
exceed 8. Let us show that each real root a21 of Eq. (13) corresponds to two
equilibrium solutions of the original system (6), (7). Indeed, for each solution
a21 of Eq. (13) and a22 of Eq. (12), one can ﬁnd two values of a23 from the
ﬁrst equation of system (7) and then uniquely determine their corresponding
values a31, a32, and a33 from system (6), (7). Once the set of six values a21, a22,
a23, a31, a32, and a33 is found, the remaining three values a11, a12, and a13 can be
uniquely determined from the conditions of the orthogonality of the directional
cosines. Since the number of real roots of Eq. (13) does not exceed eight, the
number of the satellite equilibria in this case does not exceed sixteen.

Inﬂuence of Damping Torques on Satellite Dynamics
173
4
Conditions for the Existence of Equilibrium
Orientations of the Satellite
Equations (6)–(8) and (12), (13) make it possible to determine all the equilibrium
orientations of the satellite due to gravity and active damping torques for the
given values of dimensionless damping parameters k1, k2, and k3 of the problem.
In studying the satellite equilibrium orientations, we determine the domains
with an equal number of real roots of Eq. (13) in the space of parameters. To
identify these domains, we use the Meiman theorem [11], which yields that the
decomposition of the space of parameters into domains with an equal number of
real roots is determined by the discriminant hypersurface. It is also possible to
calculate the number of real roots of a polynomial by means of ith subdiscrimi-
nants using Jacobi theorem [12,13].
In our case, the discriminant hypersurface is given by the discriminant of
polynomial (13). This hypersurface contains a component of codimension 1,
which is the boundary of domains with an equal number of real roots. The set
of singular points of the discriminant hypersurface in the space of parameters
k1, k2, and k3 is given by the following system of algebraic equations:
P(x) = 0,
P ′(x) = 0.
(14)
Here x = a2
21, and the prime denotes the diﬀerentiation with respect to x.
We eliminate the variable x from system (14) by calculating the determinant
of the resultant matrix of Eq. (14) with the help of symbolic matrix functions in
Maple and obtain an algebraic equation of the discriminant hypersurface as
P1(k1, k2, k3)P2(k1, k2, k3) = 0.
(15)
Here P1(k1, k2, k3) and P2(k1, k2, k3) are 14th and 8th degree polynomials,
respectively, in terms of k2. The polynomial P1(k1, k2, k3) has the form
P1(k1, k2, k3) = 625k8
2p4
01(k1k3 −4)2(2k1k2 + 2k2k3 + k1k3 −4)2.
(16)
Here p01 = k1k2 + k2k3 + k1k3 −4. P2(k1, k2, k3) has the form
P2(k1, k2, k3) = p2,0k8
2 + p2,1k7
2 + p2,2k6
2 + p2,3k5
2 + p2,4k4
2
+ p2,5k3
2 + p2,6k2
2 + p2,7k2 + p2,8 = 0,
(17)
where
p2,0 = 4(k1 + k3)4[4(k1k3 −4)2 −9((k1 + k3)2][((k1 −k3)2 + 25]2,
p2,1 = 8(k1k3 −4)(k1 + k3)3[4(4k2
3 −9)k6
1 −2k3(16k2
3 + 39)k5
1
+ (32k4
3 + 220k2
3 −7)k4
1 −2k3(16k4
3 + 506k2
3 −1557)k3
1
+ (16k6
3 + 220k4
3 −14186k2
3 −15827)k2
1
−6k3(13k4
3 −519k2
3 + 9041)k1 −36k6
3 −7k4
3 −15827k2
3 + 28800],
p2,2 = 4(k1k3 −4)2(k1 + k3)2[28(4k2
3 −9)k6
1 −2k3(16k2
3 + 339)k5
1

174
S.A. Gutnik and V.A. Sarychev
+ 2(48k4
3 −1158k2
3 + 2325)k4
1 −2k3(16k4
3 + 3106k2
3 −15007)k3
1
+ (112k6
3 −2316k4
3 + 54848k2
3 −58559)k2
1 −2k3(339k4
3
−15007k2
3 + 65823)k1 −252k6
3 + 4650k4
3 −58559k2
3 + 49536],
p2,3 = 4(k1k3 −4)3(k1 + k3)[56(4k2
3 −9)k6
1 + 8k3(32k2
3 −197)k5
1
+ (320k4
3 −7176k2
3 + 11101)k4
1 + 4k3(64k4
3 −3276k2
3 + 10397)k3
1
+ (224k6
3 −7176k4
3 + 57150k2
3 −53748)k2
1 −4k3(394k4
3
−10397k2
3 + 24858)k1 −504k6
3 + 11101k4
3 −53748k2
3 + 20736],
p2,4 = 4(k1k3 −4)4[280(4k2
3 −9)k6
1 + 40k3(64k2
3 −219)k5
1
+ (3136k4
3 −34408k2
3 + 44617)k4
1
+ 4k3(640k4
3 −14308k2
3 + 30053)k3
1
+ (1120k6
3 −34408k4
3 + 147366k2
3 −108828)k2
1
−4k3(2190k4
3 −30053k2
3 + 52398)k1 −2520k6
3
+ 44617k4
3 −108828k2
3 + 20736],
p2,5 = 4(k1k3 −4)5(k1 + k3)[56(4k2
3 −9)k4
1
+ 4k3(64k2
3 −219)k3
1 + (224k4
3 −3240k2
3 + 5481)k2
1
−6k3(146k2
3 −441)k1 −504k4
3 + 5481k2
3 −8262],
p2,6 = 2(k1k3 −4)6[56(4k2
3 −9)k4
1 + 4k3(96k2
3 −241)k3
1
+ (224k4
3 −1752k2
3 + 2583)k2
1 −2k3(482k2
3 −1197)k1
−504k4
3 + 2583k2
3 −2754],
p2,7 = 8(4k2
1 −9)(4k2
3 −9)(k1 + k3)(k1k3 −4)7,
p2,8 = (4k2
1 −9)(4k2
3 −9)(k1k3 −4)8.
Now we should check the change in the number of equilibria when one of
the surfaces (15) is intersected. This can be done numerically by determin-
ing the number of equilibria at a point of each domain P1(k1, k2, k3) = 0 and
P2(k1, k2, k3) = 0 in the space of parameters k1, k2 and k3.
It should be noted that when the boundaries of the surface P1(k1, k2, k3) = 0
are intersected no change in the equilibria occurs due to the condition (12). From
(12) it follows that the factor k1k2+k2k3+k1k3−4 from (16) is not equal to zero.
When k1k3−4 = 0, then a22 = 1 and a21 = 0; when 2k1k2+2k2k3+k1k3−4 = 0,
then a22 = −1 and a21 = 0. Thus, in these cases, we have only zero solutions.
To study the evolution of the domains of the existence of a diﬀerent number
of equilibrium orientations depending on the magnitude of the damping torque
vector in the space of dimensionless parameters k1, k2, and k3, we perform a
detailed analysis of the surface P2(k1, k2, k3) = 0. The satellite equilibrium ori-
entations exist when Eqs. (12) and (13) have real solutions. Equation (12) has a
solution if the condition k1k3 ≥4 is satisﬁed.
Below we present the results of the numerical and analytical analysis of the
properties and form of the discriminant hypersurface P2(k1, k2, k3) = 0, which

Inﬂuence of Damping Torques on Satellite Dynamics
175
are two-dimensional cross sections of the surface in the plane (k1, k3) at a ﬁxed
value of parameter k2 (Figs. 1, 2 and 3).
Figures 1, 2 and 3 show the distributions of domains with an equal number
of real roots of Eq. (13) for the cases of signiﬁcantly changed characteristics. The
distributions are classiﬁed for the values of k2 in the range 0.1 ≤k2 ≤5. The
ﬁgures demonstrate the domains with a ﬁxed number of real solutions in the
plane (k1, k3) (here, k1 is the vertical axis, and k3 is the horizontal axis), and
the domain boundaries are cross sections of the surface P2(k1, k2, k3) = 0 with
the plane k2 = const.
Fig. 1. The regions with the ﬁxed number of equilibria for k2 = 0.1
The ﬁgures indicate the domains where eight and four real solutions exist
as well as the domains where no real solutions exist (marked by 0). It can be
seen from Fig. 1 that for small values of k2 (k2 < 0.5), there are eight real
roots of Eq. (13) in the region near the origin of the coordinate system. In these
cases, there is only one region located above the positive branch of the hyperbola
k1k3 = 4, where eight equilibria of the satellite exist (four real roots of Eq. (13)).
Grey shaded regions correspond to the existence of equilibria.
For k2 = 0.5, the regions with the number of real roots of Eq. (13) equal to
8 disappear in the positive quadrant k1 ≥0, k3 ≥0 (Fig. 2) and, with further
increase of parameter k2, there are regions with the number of real roots equal to
4, and the regions with no real roots (Fig. 3). There is only one region (marked by

176
S.A. Gutnik and V.A. Sarychev
Fig. 2. The regions with the ﬁxed number of equilibria for k2 = 0.5
Fig. 3. The regions with the ﬁxed number of equilibria for k2 = 5.0

Inﬂuence of Damping Torques on Satellite Dynamics
177
grey color), which is located above the positive branch of the hyperbola k1k3 = 4,
with 8 equilibrium orientations (four real roots of Eq. (13)).
The results of the analysis of the equilibria total number in the third case can
be summarized as follows. The curves P2(k1, k2, k3) = 0 and k1k3 = 4 decompose
the plane (k1, k3) into three domains where no equilibria (8 or 4 real roots exist),
8 equilibria (4 real roots exist), and no equilibria (no real roots) exist.
The ﬁnal decomposition of the plane (k1, k3) for k2 = 0.1, k2 = 0.5, and
k2 = 5.0 is presented in Figs. 1, 2 and 3.
5
Necessary and Suﬃcient Conditions of Asymptotic
Stability of the Equilibrium Orientations of Satellite
In order to study the necessary and suﬃcient conditions of asymptotic stability of
the above-determined equilibrium orientations of system (6)–(7) let us linearize
the system of Eqs. (4) and (5) in the vicinity of the equilibrium solution α = α0,
β = β0, γ = γ0. We represent α, β, and γ in the form α = α0 + ¯α, β = β0 + ¯β,
γ = γ0+¯γ, where ¯α, ¯β and ¯γ are small deviations from the equilibrium orientation
of the satellite α = α0, β = β0, γ = γ0.
After rather exhausting symbolic transformations, the linearized system of
equations of motion takes the following form:
θA¨¯α sin β0 + [2(θC −1)a22a23 + k1a21] ˙¯α + 3(θC −1)(a12a33 + a13a32)¯α
+ cos β0[(θA + θC −1) −2(θC −1) sin2 γ0] ˙¯β + cos β0[(θC −1)
[(1 + 3 sin2 α0) sin β0 sin 2γ0 −3
2 sin 2α0 cos 2γ0] + k1]¯β + θA¨¯γ + k1 ˙¯γ
+(θC −1)[(a2
23 −a2
22) −3((a2
33 −a2
32)]¯γ = 0,
¨¯αa22 + [2(θA −θC)a21a23 + k2a22] ˙¯α + 3(θA −θC)(a13a31 + a11a33)¯α
+¨¯β sin γ0 + [(θA + θC −1) sin β0 cos γ0 + k2 sin γ0] ˙¯β −[(θA −θC)
[(1 + 3 sin2 α0) cos 2β0 sin γ0 + 3
2 sin 2α0 sin β0 cos γ0] + k2 sin β0 cos γ0]¯β
+(θA + θC −1)a23 ˙¯γ + [(θC −θA)(a21a22 −3a31a32) + k2a23]¯γ = 0,
θC ¨¯αa23 + cos 2β0[2(1 −θA) sin β0 cos γ0 −k3 sin γ0] ˙¯α + θC ¨¯β cos γ0
+[(θC −θA + 1) sin β0 sin γ0 + k3 cos γ0] ˙¯β
+[(1 −θA)[(1 + 3 sin2 α0) cos 2β0 cos γ0 −3
2 sin 2α0 sin β0 sin γ0]
+k3 sin β0 sin γ0]¯β + 3(1 −θA)(a11a32 + a12a31)¯α
−(θA + θC −1)a22 ˙¯γ + [(1 −θA)(a21a23 −3a31a33) −k3a22]¯γ = 0. (18)
Now let us consider small oscillations of the satellite in the vicinity of the
speciﬁc equilibrium orientation, when the principal axes of inertia of the satellite
coincide with the orbital coordinate system:
α0 = β0 = γ0 = 0.
(19)

178
S.A. Gutnik and V.A. Sarychev
This is one of the equilibrium solutions from (9), when a22 = 1, a11 = 1, and
a33 = 1. Taking into account expressions (1) for solution (19), we get sin α0 = 0,
sin β0 = 0, sin γ0 = 0, and linearized equations (18) take the form
¨¯α + k2 ˙¯α + 3(θA −θC)¯α = 0,
θC ¨¯β + k3 ˙¯β −(θA + θC −1)˙¯γ + (1 −θA)¯β −k3¯γ = 0,
θA¨¯γ + k1 ˙¯γ + (θA + θC −1) ˙¯β + 4(1 −θC)¯γ + k1 ¯β = 0.
(20)
The characteristic equation of system (20)
[λ2 + k2λ + 3(θA −θC)](A0λ4 + A1λ3 + A2λ2 + A3λ + A4) = 0
(21)
decomposes into quadratic and 4th degree equations. Here the following nota-
tions are introduced:
A0 = θAθC,
A1 = k1θC + k3θA,
A2 = k1k3 + (θA + θC −1)2 + θA(1 −θA) + 4θC(1 −θC),
A3 = k1θC + k3(θA −3θC + 3),
A4 = k1k3 + 4(1 −θA)(1 −θC).
The necessary and suﬃcient conditions for asymptotic stability (Routh–
Hurwitz criterion) of the equilibrium solution (19) take the following form:
k2 > 0,
θA −θC > 0,
Δ1 = A1 = k1θC + k3θA > 0,
Δ2 = A1A2 −A0A3 = k2
1k3θC + k1k2
3θA
+ (1 −θC)[k1θC(3θC −θA + 1) + k3θA(1 −θA)] > 0,
Δ3 = A1A2A3 −A0A2
3 −A2
1A4 = 3(1 −θC)[k2
1k2
3θC + k1k3
3θA
+ k2
1θ2
C(θA + θC −1) + k1k3θC[(θA + θC −1)(2θA −1)
+ 3θC(1 −θC)] −k2
3θA(1 −θA)(θA + θC −1)] > 0,
Δ4 = Δ3A4 > 0,
A4 = k1k3 + 4(1 −θA)(1 −θC) > 0.
(22)
Let us consider the special case when k1 = k2 = k3 = k. In this case,
conditions (22) take a simpler form
k > 0,
θA −θC > 0,
Δ1 = k(θC + θA) > 0,
Δ2 = k[k2 + (1 −θC)2]θA
+ k[k2 + (1 −θC)(1 + 3θC)]θC −k(1 −θC)θ2
A > 0,
Δ3 = 3k2(1 −θC)[θ3
A + (3θC −2)θ2
A
+ [k2 + (1 −θC)(1 −3θC)]θA
+ θC[k2 + (1 −θC)(1 + 2θC)]] > 0,
Δ4 = Δ3A4 > 0,
A4 = k2 + 4(1 −θA)(1 −θC) > 0.
(23)

Inﬂuence of Damping Torques on Satellite Dynamics
179
Fig. 4. The region of fulﬁllment of the asymptotic stability conditions for k = 0.5
Fig. 5. The region of fulﬁllment of the asymptotic stability conditions for k = 1.0
The detailed analysis of the regions where necessary and suﬃcient condi-
tions of stability (23) hold is studied in the plane of two dimensionless iner-
tia parameters (θA, θC) at diﬀerent values of damping coeﬃcient k. It is evi-
dent that along with (23), the triangle inequalities should also be satisﬁed:

180
S.A. Gutnik and V.A. Sarychev
Fig. 6. The transitional process of damping oscillations for k = 0.5
θA + θC > 1, θC + 1 > θA, θA + 1 > θC. One may disregard the third trian-
gle inequality, since when θA > θC it holds automatically. Thus, the region is
limited by the straight lines
θC = 1 −θA,
θC = θA,
θC = θA −1.
(24)
An example of such a region and also all the lines on which one of inequalities
(23) converts into equality are shown in Figs. 4 and 5. The region where the
necessary and suﬃcient conditions of stability are satisﬁed is marked out by
gray color. In Fig. 4, the region of fulﬁllment of the necessary and suﬃcient
conditions of stability (23) for k = 0.5 is bounded by the straight lines (24) and
by hyperbola A4 = 0. In Fig. 5 for k = 1, the region where stability conditions
(23) hold is bounded only by the straight lines (24).
The numerical integration of system (4) and (5) has been done in the special
case when k1 = k2 = k3 = k. The diﬀerent types of transition decay processes
of spatial oscillations of the satellite at diﬀerent damping parameters have been
investigated numerically. Figure 6 shows an example of transition decay processes
of spatial oscillations for k = 0.5 and for inertia parameters θA = 1, θC =
0.5 where conditions of asymptotic stability (23) hold. The system in this case
reaches the equilibrium position (19) at all three angles at the τ value, equal
to 25.

Inﬂuence of Damping Torques on Satellite Dynamics
181
6
Conclusion
In this paper, we have analyzed the rotational motion of the satellite relative
to the center of mass in a circular orbit due to gravity and active damping
torques. The main focus is the study of satellite equilibrium orientations and
the conditions for their stability. A computer algebra method (based on the
construction of Gr¨obner bases) has been proposed to determine all the equilibria
of the satellite in the orbital coordinate system for the given values of the active
damping torque vector in the general case; the conditions of their existence have
been obtained.
The two-dimensional cross sections of domains with equal number of equilib-
rium orientations using algebraic methods for the construction of discriminant
hypersurfaces have been classiﬁed. We have made a detailed analysis of the evo-
lution of diﬀerent domains of existence of equilibrium orientations in the plane
of parameters k1 and k3 for the ﬁxed values of parameter k2.
Necessary and suﬃcient conditions for asymptotic stability of the equilib-
rium orientations were obtained with the help of the Routh–Hurwitz criterion.
The transition decay processes of spatial oscillations of the satellite have been
investigated numerically. The results of this study can be used for a preliminary
design of gravitational systems to control the satellite’s orientation and make it
possible to simulate the inﬂuence of the damping torque on its orientation.
Acknowledgements. The authors thank the reviewers for very useful remarks and
suggestions and Professor V. Gerdt for the advice on the eﬀectiveness of methods and
algorithms of Gr¨obner basis construction.
References
1. Beletsky, V.V.: Attitude Motion of Satellite in Gravitational Field. MGU Press,
Moscow (1975)
2. Sarychev, V.A.: Problems of orientation of satellites. Itogi Nauki i Tekhniki. Ser.
“Space Research”, 11 (1978). VINITI, Moscow
3. Gutnik, S.A., Guerman, A., Sarychev, V.A.: Application of computer algebra
methods to investigation of inﬂuence of constant torque on stationary motions
of satellite. In: Gerdt, V.P., Koepf, W., Seiler, W.M., Vorozhtsov, E.V. (eds.)
CASC 2015. LNCS, vol. 9301, pp. 198–209. Springer, Cham (2015). doi:10.1007/
978-3-319-24021-3 15
4. Chen, C., Maza, M.M.: Semi-algebraic description of the equilibria of dynamical
systems. In: Gerdt, V.P., Koepf, W., Mayr, E.W., Vorozhtsov, E.V. (eds.) CASC
2011. LNCS, vol. 6885, pp. 101–125. Springer, Heidelberg (2011). doi:10.1007/
978-3-642-23568-9 9
5. El Kahoui, M., Weber, A.: Symbolic equilibrium point analysis of parameterized
polynomial vector ﬁelds. In: Ganzha, V., Mayr, E.W., Vorozhtsov, E.V. (eds.)
Computer Algebra in Scientiﬁc Computing (CASC 2002), pp. 71–83. Institut f¨ur
Informatik. Technische Universit¨at M¨unchen, Garching (2002)

182
S.A. Gutnik and V.A. Sarychev
6. Chen, C., Golubitsky, O., Lemaire, F., Maza, M.M., Pan, W.: Comprehensive tri-
angular decomposition. In: Ganzha, V.G., Mayr, E.W., Vorozhtsov, E.V. (eds.)
CASC 2007. LNCS, vol. 4770, pp. 73–101. Springer, Heidelberg (2007). doi:10.
1007/978-3-540-75187-8 7
7. Buchberger, B.: A theoretical basis for the reduction of polynomials to canonical
forms. SIGSAM Bull. 10(3), 19–29 (1976)
8. Char, B.W., Geddes, K.O., Gonnet, G.H., Monagan, M.B., Watt, S.M.: Maple
Reference Manual. Watcom Publications Limited, Waterloo (1992)
9. Weispfenning, V.: Comprehensive Gr¨obner bases. J. Symb. Comput. 14(1), 1–30
(1992)
10. Lazard, D., Rouillier, F.: Solving parametric polynomial systems. J. Symb. Com-
put. 42(6), 636–667 (1992)
11. Meiman, N.N.: Some problems on the distribution of the zeros of polynomials.
Uspekhi Mat. Nauk 34, 154–188 (1949)
12. Gantmacher, F.R.: The Theory of Matrices. Chelsea Publishing Company,
New York (1959)
13. Batkhin, A.B.: Parameterization of the discriminant set of a polynomial. Program.
Comput. Softw. 42(2), 65–76 (2016)

Characteristic Set Method for Laurent
Diﬀerential Polynomial Systems
Youren Hu(B) and Xiao-Shan Gao
KLMM, UCAS, Academy of Mathematics and Systems Science,
Chinese Academy of Sciences, Beijing 100190, China
huyouren14@mails.ucas.ac.cn
Abstract. In this paper, a characteristic set method for Laurent (dif-
ferential) polynomial systems is given. In the Laurent polynomial case,
the concept of Laurent regular chain is introduced and a characteristic
set algorithm for Laurent polynomial system is given. In the Laurent
diﬀerential polynomial case, we give a partial method to decide whether
a Laurent diﬀerential chain A is Laurent regular.
Keywords: Characteristic set · Gr¨obner basis · Laurent diﬀerential
polynomial · Laurent regular
1
Introduction
The characteristic set method can be used to decompose the zero set of a
general polynomial set into the union of zero sets of polynomials in triangu-
lar form. This method has applications in automated reasoning, robotics, com-
puter vision, computer-aided design, and analysis of cryptosystems, etc [20]. The
characteristic set method was proposed by Ritt and was extensively studied in
the past thirty years for polynomial systems [7,18,19,21], semi-algebraic sets
[6], polynomial systems over ﬁnite ﬁelds [9,15], diﬀerential polynomial systems
[2,3,8,13,17,23], and diﬀerence polynomial systems [11].
In this paper, we consider the characteristic set method for Laurent polyno-
mial systems and Laurent diﬀerential polynomial systems. This is motivated by
the work on diﬀerence binomial ideals [10], where the characteristic set method
in the Laurent case plays a key role.
In the Laurent polynomial case, we introduce the concept of Laurent regular
chain (or triangular set) and prove that it has similar properties with regular
chains in the non-Laurent case. Then, a characteristic set algorithm is given to
decompose the zero set of a Laurent polynomial system into the union of zero
sets of Laurent regular chains. We also introduce the concept of Laurent Gr¨obner
basis for a Laurent polynomial ideal and use it to give a minimal triangular
set decomposition for the zero set of a Laurent polynomial system. Laurent
Partially supported by an NSFC grant No. 11688101.
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 183–195, 2017.
DOI: 10.1007/978-3-319-66320-3 14

184
Y. Hu and X.-S. Gao
Gr¨obner bases were deﬁned in [16,22]. The method given in [16] is direct but
quite complicated. The method given in [22] is similar to our deﬁnition, but our
treatment is simpler and more direct.
In the Laurent diﬀerential polynomial case, we introduce the concept of
Laurent regular diﬀerential chain. But, the problem of deciding whether a Lau-
rent diﬀerential chain is Laurent regular is quite diﬃcult and is still open. We
ﬁrst show that deciding whether a Laurent diﬀerential chain is Laurent regular
can be reduced to deciding whether a univariate diﬀerential polynomial f(z) is
Laurent regular, then give a partial method to decide whether f(z) is Laurent
regular, which is complete when f(z) is of the ﬁrst order. The method is based
on the work of Cano [4,5] and Grigoriev-Singer [12], where a special type of
Newton polygon is introduced to describe the minimal monomial of the series
solution to a diﬀerential equation.
It should be noticed that the extension of the characteristic set method [11]
to Laurent diﬀerence polynomial systems is straightforward. The reason is that
if a diﬀerence polynomial f is invertible w.r.t a proper irreducible regular and
coherent diﬀerence chain A , then σkf is also invertible w.r.t A for the diﬀerence
operator σ and any k ∈N [11], which is not true in the diﬀerential case.
The paper is organized as follows. In Sect. 2, we present the characteristic set
method for Laurent polynomial systems. In Sect. 3, we present the characteristic
set method for Laurent diﬀerential polynomial systems.
2
Laurent Polynomial Systems
2.1
Laurent Regular Chain
In this section, we will deﬁne and prove the basic properties of Laurent regu-
lar chains. For the basic concepts about the characteristic set, please refer to
[2,3,18,19].
Let k be a ﬁeld and Y = {y1, . . . , yn} a set of indeterminates. We denote
k[Y] = k[y1, . . . , yn] to be the polynomial ring in Y and k[Y±] the Laurent
polynomial ring in Y. For F ⊆k[Y] and G ⊆k[Y±], we denote by (F) or
(G)k[Y±] to be the ideals generated by F or G in k[Y] or k[Y±], respectively.
A polynomial f ∈k[Y] is called monomial-primitive if no yi divides f. The
normal form of a Laurent polynomial f = s
i=1 ai
mi
ni ∈k[Y±], where mi, ni are
monomials in k[Y] and gcd(mi, ni) = 1, is the monomial-primitive polynomial
flcm(n1,...,ns)
gcd(m1,...,ms), denoted by ˜f.
As in the characteristic set theory, we ﬁx a variable order y1 < · · · < yn. For
f ∈k[Y], denote lv(f) to be the largest variable occurring in f.
Deﬁnition 1. A Laurent ascending chain, or simply a Laurent chain A
=
A1, . . . , Ap in k[Y±] is a chain in k[Y] such that A1, . . . , Ap ∈k[Y] are monomial-
primitive. A characteristic set of a Laurent polynomial set P ⊆k[Y±] is deﬁned
to be the characteristic set of P.

Characteristic Set Method for Laurent Diﬀerential Polynomial Systems
185
A Laurent polynomial f is said to be reduced w.r.t a Laurent chain A if f
is reduced w.r.t A . We deﬁne the pseudo-remainder of a Laurent polynomial f
w.r.t a Laurent chain A as lprem(f, A ) = prem( ˜f, A ), where prem( ˜f, A ) is the
usual pseudo-remainder.
Assume A = A1, . . . , Ap in k[Y] is a chain and let xi be the leading variable
of Ai, X = {x1, . . . , xp} and U = Y −X. U and X are called the parameter
set and the leading variable set of A respectively. We denote k[Y] as k[U, X].
A polynomial f ∈k[U, X] is said to be invertible w.r.t A if either f ∈k[U]
or (f, A1, . . . , As)  k[U] ̸= {0} where lv(f) = lv(As). A is called regular if
the initial of Ai is invertible w.r.t A , for i = 1, . . . , n. Regular chains have the
following nice properties [1,3].
Lemma 1. The following statements are equivalent.
1. A is a regular chain.
2. A is a characteristic set of sat(A ), where sat(A ) is the saturation ideal of A .
3. If a polynomial N is invertible w.r.t A such that Nf ∈sat(A ), then f ∈
sat(A ).
Lemma 2. A polynomial f is not invertible w.r.t a regular chain A if and only
if there exists a nonzero polynomial N reduced w.r.t A , such that Nf ∈(A ).
Let A be a Laurent chain and IA the product of the initials of A . The
Laurent saturation ideal of A in k[Y±] is deﬁned as follows
lsat(A ) = {f ∈k[Y±] | ∃s ∈N, Is
A f ∈(A )k[Y±]}.
Deﬁnition 2. A Laurent chain A is called a Laurent regular chain if it is a
regular chain in k[Y] and yi is invertible w.r.t A for any i = 1, . . . , n. Let A
be a Laurent regular chain, H ∈k[Y±] is said to be invertible w.r.t A if H is
invertible w.r.t A in k[Y].
A Laurent regular chain has similar properties to that of a regular chain.
Lemma 3. If A is a Laurent regular chain, then f ∈lsat(A ) if and only if
lprem(f, A ) = 0.
Proof. It is obvious that if lprem(f, A ) = 0, then f ∈lsat(A ). We need only to
prove the converse implication. Let f ∈lsat(A ). Then there exists a monomial
M and m ∈N such that Im
A M ˜f ∈(A ), or equivalently M ˜f ∈sat(A ). Because
A is Laurent regular and M is invertible w.r.t A , by Lemma 1, ˜f ∈sat(A ). By
Lemma 1, prem( ˜f, A ) = 0, that is lprem(f, A ) = 0.
⊓⊔
Lemma 4. Assume A is a Laurent regular chain and let U be the parameter
set of A and D ∈k[U±], then PD ∈lsat(A ) implies P ∈lsat(A ).
Proof. Suppose H = lprem(P, A ), then there exists m ∈N such that Im
A P −H ∈
(A ). So DP ∈lsat(A ) implies DH ∈lsat(A ). Since A is Laurent regular and
DH is reduced w.r.t A , by Lemma 3, DH = 0. So H = 0, which means that
P ∈lsat(A ).
⊓⊔

186
Y. Hu and X.-S. Gao
Lemma 5. Let A be a Laurent regular chain. Then P ∈k[Y±] is not invertible
w.r.t A if and only if there exists a nonzero Laurent polynomial Q reduced w.r.t
A , such that QP ∈(A )k[Y±].
Proof. First, suppose that P ∈k[Y±] is not invertible w.r.t A . By Lemma 2,
there exists a nonzero polynomial Q ∈k[Y] reduced w.r.t A , such that ˜PQ ∈
(A ), so PQ ∈(A )k[Y±]. To prove the inverse implication, consider some P
invertible w.r.t A . Then there exist U ∈k[U] and A ∈k[Y] such that U −
A P ∈(A ). So for nonzero Laurent polynomial Q reduced w.r.t A , we have
UQ −A PQ ∈(A )k[Y±] ⊆lsat(A ). By Lemmas 3 and 4, UQ cannot be in
lsat(A ), which implies PQ cannot be in lsat(A ). So PQ cannot be in (A )k[Y±],
which is a contradiction.
⊓⊔
Theorem 1. A Laurent chain A is Laurent regular if and only if A is the
characteristic set of lsat(A ).
Proof. Assume that A is Laurent regular. By Lemma 3, for P ∈lsat(A ),
lprem(P, A ) = 0. Thus A is the characteristic set of lsat(A ). Now suppose
that A is the characteristic set of lsat(A ). Then A is the characteristic set
of sat(A ). By Lemma 1, A is regular. If some monomial M is not invertible
w.r.t A , by Lemma 5, there exists a nonzero Laurent polynomial Q reduced
w.r.t A such that MQ ∈(A )k[Y±] ⊆lsat(A ). Thus Q ∈lsat(A ), which is a
contradiction.
⊓⊔
Let A = {A1, . . . , Ap} be a regular chain in k[Y] = k[U, X], where U is the
parameter set of A . A polynomial P is called reducible modulo A if there exist
0 ̸= M ∈k[U], P1, P2 ∈k[Y] with the same leading variable as P and the initials
of Pi, i = 1, 2 are invertible w.r.t A such that MP = P1P2 mod(A ). If such
M, P1, P2 do not exist, P is called irreducible modulo A . A is called irreducible
if A1 is irreducible as a polynomial in k[U][x1] and Ai is irreducible modulo
Ai−1 = {A1, . . . , Ai−1}, i = 2, . . . , p.
Let K be an algebraically closed extension of k and K∗= K \ {0}. For a
Laurent polynomial set S ⊂k[Y±], we use LZero(S) to denote the elements
e ∈(K∗)n, which are zeros of the Laurent polynomials in S.
2.2
Characteristic Set Method
In this subsection, we present the characteristic set method for Laurent poly-
nomial systems, which is basically the same as that given in [7,19], and the
correctness can be similarly proved. We ﬁrst show how to check whether a poly-
nomial is invertible w.r.t a regular chain.
Algorithm 1. Invert(f, A )
Input: a polynomial f and a regular chain A .
Output: a pair (test,g) such that
test = true, if f is invertible w.r.t A , and g = 0.

Characteristic Set Method for Laurent Diﬀerential Polynomial Systems
187
test = false, if f is not invertible w.r.t A , and g is a nonzero polynomial
reduced w.r.t A such that fg ∈(A ).
Begin
w := a new indeterminate;
P(U, w) = Res(w −f, A ), and Res(g, A ): the resultant of g w.r.t A [21].
If P(U, 0) ̸= 0, then test := true and g = 0
else test := false and g = prem(P(f), A ), where P(U, w) = P(U, w)/w.
End.
The above algorithm is based on methods in [3,21], where the details could
be found. We now show how to check whether a chain is Laurent regular.
Algorithm 2. LRegular(A )
Input: A chain A = f1(U, x1), . . . , fp(U, x1, . . . , xp) ⊆k[U, x1, . . . , xp].
Output: If A is Laurent regular, output (∅, ∅). Otherwise let i be the largest
number such that Ai−1 = f1, . . . , fi−1 is Laurent regular. If Ii = init(fi) is not
invertible w.r.t Ai−1, output (Ii, g) such that Iig ∈(Ai−1) and g is reduced
w.r.t Ai−1. If xi is not invertible w.r.t Ai−1, then output (xi, g) such that xig ∈
(Ai−1), where g is reduced w.r.t Ai−1.
Begin
Let s be the largest integer such that xs
1|f1.
If s > 0, then return (x1, f1/xs
1).
i=2;
while i ≤p
(test1, g1) = Invert(Ii, Ai−1). If test1 = false return (I, g) = (Ii, g1)
(test2, g2) = Invert(xi, Ai−1) If test2 = false return (I, g) = (xi, g2)
i = i + 1
end while
if i = p + 1 then return (∅, ∅)
End.
We now give the main algorithm.
Algorithm 3. ZDec(P)
Input: a ﬁnite set P of Laurent polynomials in k[Y±].
Output: W = {T1, . . . , Tk} such that each Ti is a Laurent regular chain and
LZero(P) = k
i=1 LZero(lsat(Ti)).
Begin
P = Normalize(P): returns normal forms of the polynomials in P.
C = Charset(P): C is a Wu-characteristic set of P [19].
C = Normalize(C): Remove the monomial factors from elements in C.
(I, g) = LRegular(C).
If I = yi for some i then W = ZDec(P {g}  C).
If I ̸= ∅then W = ZDec(P {I}  C)  ZDec(P {g}  C).
If I = ∅then W = {C}  
I∈IC ZDec (P {I}  C),
where Ic is the set of initials of C.
End.

188
Y. Hu and X.-S. Gao
2.3
Laurent Gr¨obner Basis and Minimal Decomposition
To obtain a minimal zero decomposition, we need the concept of Laurent Gr¨obner
basis.
Deﬁnition 3. A ﬁnite set G of monomial-primitive polynomials is said to be a
Laurent Gr¨obner basis of the Laurent polynomial ideal I ⊆k[Y±] if G ⊆I and
for any f ∈I, grem( f, G) = 0, where grem( f, G) is the normal form of f w.r.t
G as deﬁned in the Gr¨obner basis theory in the polynomial ring.
Assuming that F = {f1, . . . , fs} ⊆k[Y±], we can compute the Laurent
Gr¨obner basis of (F)k[Y±] as follows.
Theorem 2. Let G0 be the reduced Gr¨obner basis of ( f1, . . . , fs, yizi −1, i =
1, . . . , n) ⊆k[y1, . . . , yn, z1, . . . , zn] w.r.t some monomial order satisfying yi ≺zj
for i, j = 1, . . . , n and G = G0
 k[y1, . . . , yn]. Then G is a Laurent Gr¨obner
basis of (F)k[Y±].
Proof. We claim G ⊆(F)k[Y±]. Since G ⊆( f1, . . . , fs, yizi −1, i = 1, . . . , n),
any g ∈G can be written as g = s
i=1 ai fi + n
i=1 bi(yizi −1), where
ai, bi ∈k[y1, . . . , yn, z1, . . . , zn]. Substituting 1/yi for zi, we get Mg = s
i=1 ai fi,
where M is a monomial in k[y1, . . . , yn] and ai ∈k[y1, . . . , yn]. So the claim
is proved. For any f ∈(F)k[Y±], there exists a monomial N in k[y1, . . . , yn]
such that Nf = s
i=0 ci fi, where ci ∈k[y1, . . . , yn]. By the deﬁnition of
f, we have H f = Nf, for some monomial H in k[y1, . . . , yn]. Assume that
H = yi0H1, for some i0. Then yi0H1 f = s
i=0 ci fi, we have zi0yi0H1 f =
s
i=0 cizi0 fi. Then H1 f = s
i=0 cizi0 fi −(yi0zi0 −1)H1 f ∈( f1, . . . , fs, yizi −
1, i = 1, . . . , n)  k[y1, . . . , yn]. Repeating the above process, we have f ∈
( f1, . . . , fs, yizi −1, i = 1, . . . , n)  k[y1, . . . , yn]. Since G is a Gr¨obner basis
of ( f1, . . . , fs, yizi −1, i = 1, . . . , n) under the order yi ≺zj, for i, j = 1, . . . , n,
grem( f, G0) = 0 which implies grem( f, G) = 0.
We prove that G0 is monomial-primitive. If g ∈G0 is not monomial-primitive,
then there exists an i ∈{1, . . . , n} and a g′ ∈k[Y] such that g = yig′.
So we have g′ = zig + (1 −yizi)g′ ∈( f1, . . . , fs, yizi −1, i = 1, . . . , n) ⊆
k[y1, . . . , yn, z1, . . . , zn], which implies that there exists a q ∈G0 such that
lt(q)|lt(g′)|lt(g). It is a contradiction to the fact that G0 is a reduced Gr¨obner
basis. Thus G0 is monomial-primitive.
⊓⊔
Example 1. Let F = {y1y3 −y2, y2y4 −y1}. Then F is already a Gr¨obner basis,
but not a Laurent Gr¨obner basis. With the method given in Theorem 2, we can
compute the Laurent Gr¨obner basis of (F)k[Y±]: {y3y4 −1, y1y3 −y2, y2y4 −y1}.
Remark: To obtain a minimal decomposition, we ﬁrst compute a decomposition
LZero(P) = k
i=1 LZero(lsat(Ti)), where Ti are irreducible [19]. Then, compute
the Laurent Gr¨obner basis Bi of lsat(Ti) with Theorem 2. We have LZero(P) =
 LZero(Bi) and a minimal decomposition can be obtained easily.

Characteristic Set Method for Laurent Diﬀerential Polynomial Systems
189
3
Diﬀerential Polynomial Systems
3.1
Laurent Regular Diﬀerential Chains
In this section, we will extend the characteristic set method to the Laurent
diﬀerential case. For details of diﬀerential characteristic set method, please refer
to [2,8,13,17].
Let F be a diﬀerential ﬁeld with the diﬀerential operator δ, Y = {y1, y2, . . . ,
yn} diﬀerential indeterminates, and F{Y} the diﬀerential polynomial ring in Y
over F. Let f ∈F{Y}, denote ld(f) to be the leader of f and ord(f, yi) the
order of f in yi.
Let A = f1, . . . , fp be a diﬀerential chain, ci = ord(fi, ld(fi)), and o ∈N.
Then denote
A (o) = f1, f (1)
1 , . . . , f (o−c1)
1
, . . . , fp, f (1)
p , . . . , f (o−cp)
p
where o = max{o, c1, . . . , cp}. Note that A (o) is a chain in the polynomial ring
F[Y, . . . , Y(o)]. Let f ∈F{Y}. Then f is called invertible w.r.t A if f is invertible
w.r.t A (ord(f)) in the polynomial ring F[Θ(Y)].
Deﬁnition 4. If the initials and separants of a diﬀerential chain A are invert-
ible w.r.t A , then A is called diﬀerential regular. Besides, if y(j)
i
is invertible
w.r.t A , for any i = 1, . . . , n, j ∈N, A is called Laurent regular. If A consists
of only one element, then this polynomial is called a Laurent regular polynomial.
Let A be a diﬀerential chain and HA be the product of the initials and
separants of A . We deﬁne the Laurent saturation ideal of A to be
ldsat(A ) = [A ] : H∞
A = {f ∈F{Y±} | ∃m ∈N, s.t. Hm
A f ∈[A ]F{Y±}}.
Theorem 1 can be easily extended to the following diﬀerential version.
Theorem 3. A is Laurent diﬀerential regular if and only if A is the charac-
teristic set of ldsat(A ).
If we can solve the following problem, then we can extend the characteristic
set method to the Laurent diﬀerential case.
Problem LR. For an irreducible and diﬀerential regular chain A , either decide
A is Laurent regular or ﬁnd some y(e)
c
which is not invertible w.r.t A .
The decision of whether a diﬀerential chain A is Laurent diﬀerential regular
is still open and we will give some partial answers to this problem in the rest of
this paper. We ﬁrst give an example to show why the problem is diﬃcult.
Example 2. Let f = xy′
1 −ky1, k ∈N. Then f (i) = xy(i+1)
1
+ (i −k)y(i) for
i = 1, . . . , k. Hence f (k) = xy(k+1)
1
. That is, y(k+1)
1
is not invertible w.r.t f and
f is not a Laurent regular diﬀerential polynomial.
In this section, a diﬀerential chain A is always irreducible and regular.

190
Y. Hu and X.-S. Gao
Lemma 6. A diﬀerential regular chain A is not Laurent diﬀerential regular if
and only if prem(y(m)
i
, A ) = 0 for some i ∈{1, . . . , n} and m ∈N.
Proof. If A is not Laurent diﬀerential regular, then y(m)
i
is not invertible w.r.t
A for some i ∈[1, n] and m ∈N. By Lemma 2, there exists an N reduced w.r.t
A and Ny(m)
i
∈[A ]. Since A is regular and irreducible, we have y(m)
i
∈dsat(A )
and thus prem(y(m)
i
, A ) = 0. On the other hand, assume prem(y(m)
i
, A ) = 0.
Then we have Hy(m)
i
= 0
mod
[A ], where H is a power of the product of
the initials and separants of A . Since A is regular, H is invertible w.r.t A . By
Lemma 1, y(m)
i
is not invertible w.r.t A and hence A is not Laurent regular. ⊓⊔
As a consequence, we have the following results.
Corollary 1. If y(m)
p
is not invertible w.r.t A , then dsat(A )  F{yp} ̸= {0}.
Corollary 2. If f ∈F{Y} is irreducible and there are more than one diﬀerential
indeterminate in f, then f is Laurent regular.
Theorem 4. Let A be a diﬀerential regular and irreducible diﬀerential chain.
Then deciding whether A is Laurent regular can be reduced to deciding whether
a univariate diﬀerential polynomial is Laurent regular.
Proof. Let U = {u1, . . . , uq} be the parameter set of A and X = Y \ U =
{x1, . . . , xp}(p + q = n). Then A can be written as A = f1(U, x1), f2(U, x1, x2),
. . . , fp(U, x1, . . . , xp) with ld(fi) = x(oi)
i
. By Lemma 6 and Corollary 1, u(j)
i , i =
1, . . . , q, j ∈N is invertible w.r.t A . We now consider whether x(e)
c , c =
1, . . . , p, e ∈N is invertible w.r.t A . For each c, we can use the change of order
algorithm given in [2] to compute a regular and irreducible diﬀerential chain Ac
under the variable order U < xc < x1 < · · · xc−1 < xc+1 < · · · < xp such that
dsat(A ) = dsat(Ac). Since A is irreducible, it is clear that
Ac = g1(U, xc), g2(U, xc, x1), . . . , fp(U, xc, x1, . . . , xc−1, xc+1, . . . , xp).
Since Ac is diﬀerential regular, we have dsat(A )∩F{xc} = dsat(Ac)∩F{xc} =
dsat(g1) ∩F{xc}. If g1 contains some ui, then dsat(Ac) ∩F{xc} = dsat(g1) ∩
F{xc} = {0}. By Corollary 1, x(e)
c
is invertible w.r.t A for any e ∈N. If g1 ∈
F{xc}, then dsat(Ac)∩F{xc} = dsat(g1)∩F{xc} = dsat(g1). By Lemma 6, x(e)
c
is invertible w.r.t A if and only if it is invertible w.r.t g1, that is, g1 is Laurent
regular. The theorem is proved.
⊓⊔
3.2
Decision of Univariate Laurent Regular Diﬀerential Polynomial
Let z be a diﬀerential indeterminate and F = Q(x) with δ =
d
dx, and f(z) ∈
F{z} a univariate irreducible diﬀerential polynomial. In this section, we will give
a partial solution to decide whether f(z) is Laurent regular.

Characteristic Set Method for Laurent Diﬀerential Polynomial Systems
191
Let n = ord(f) and zi = z(i) for i ∈N with z0 = z. Then, we can write f as
a polynomial in Q[x, z, z1, . . . , zn]:
f(z) =

(α,β)∈N×Nn+1
cα,βxαzβ0
0 . . . zβn
n
(1)
where β = (β0, . . . , βn) and cα,β ∈Q are not zero for a ﬁnite number of terms.
Note that zm ∈sat(f) if and only if f(z) = 0 has a generic polynomial
solution. Therefore, we need to give a method to ﬁnd polynomial solutions of
f(z) = 0. Following [4,5], we deﬁne a special Newton polygon.
Deﬁnition 5. For f(z) given in (1), we set
ε(f(z)) = {P(α, β) = (α −β1 −2β2 −. . . −nβn, β0 + . . . + βn) : c(α,β) ̸= 0}.
Deﬁne NP(f(z)) to be the convex hull of ε(f(z)) in R2. In the rest of this paper,
we use u and v to represent horizontal and vertical axes of R2, respectively.
Example 3. Assume f(z) = z′2−2z′−4z+4x. Then ε(f) = {(−2, 2), (−1, 1), (0,
1), (1, 0)} and NP(f) is given in Fig. 1.
Fig. 1. NP(f(z)) in Example 3
Fig. 2. NP(f(z)) in Example 4
Deﬁnition 6. Given f(z) as before and μ ∈R, a straight line L(f, μ) in R2 is
called feasible for f(z) if L(f, μ) is a line with slope −1/μ, which has the deﬁning
equation u + μv = w, for some w ∈R, and L(f, μ) left-supports NP(f(z)), that
is, NP(f(z))  L(f, μ) ̸= ∅and NP(f(z)) lies in the left side of L(f, μ).
Example 4. Assume that f is deﬁned in Example 3. The μ corresponding to the
feasible lines in Fig. 2 are 2 and 4/3.
Deﬁnition 7. Let f(z) be deﬁned as in (1). We denote the vertices of NP(f)
from the top-right to the right-most as P1, . . . , Pt clockwise, where Pi = (ui, vi).
Then the line PiPi+1, i = 1, . . . , t −1, is denoted li and its slope is vi+1−vi
ui+1−ui < 0.
We set si = −ui+1−ui
vi+1−vi > 0, for i = 1, . . . , t −1, s0 = +∞and st = 0, for t ≥2.
If t = 1, set s1 = 0.

192
Y. Hu and X.-S. Gao
It is easy to see that
Lemma 7. We have s0 > s1 > . . . > st−1 > st and the lines
L(f, δi) : u + δiv = ui + δivi, δi ∈[si, si−1), i = 1, . . . , t −1
are all the feasible lines whose slopes are negative.
Associated to Pi = (ui, vi), we deﬁne the polynomials
Φi(μ) =

P (α,β)=(ui,vi)
cα,β(μ)β1
1 . . . (μ)βn
n ∈Q[μ],
(2)
where i = 1, . . . , t and (μ)k = μ(μ −1) . . . (μ −k + 1).
Example 5. Let f be deﬁned in Example 3. Then P1 = (−2, 2), P2 = (0, 1), P3 =
(1, 0), s0 = +∞, s1 = 2, s2 = 1, s3 = 0 and Φ1 = μ2, Φ2 = −4, Φ3 = 4.
We ﬁrst consider a diﬀerential polynomial of ﬁrst order, for which there exists
a complete method to decide whether f(z) has a polynomial solution.
Theorem 5. Assuming f(z) = 
(α,β)∈N×N2 cα,βxαzβ0
0 zβ1
1
and P1, . . . , Pt be
deﬁned as in Deﬁnition 7, then associated to Pi, Φi becomes
Φi = cui,vi,0 + cui+1,vi−1,1μ + · · · + cui+vi,0,viμvi.
(3)
If there exists an n ∈(si, si−1)  N such that Φi(n) = 0, we denote ηi to be the
maximal number satisfying the condition. Otherwise, ηi = si. Let m be the ﬁrst
positive integer in the sequence η1, . . . , ηt. Then m is an upper bound for the
polynomial solutions of f(z) = 0.
Proof. If such an m exists, then ∃i0 ∈{1, . . . , t} such that m = ηi0 ∈[si0, si0−1).
Suppose that there is a number m < μ0 ∈N such that ˜z = aμ0xμ0 + · · · + a0 is a
polynomial solution of f(z) = 0. Then μ0 must be in [si, si−1) for some i ≤i0.
Substitute ˜z into f(z) and we get
f(˜z) =

cα,β0,β1xα(aμ0xμ0 + . . . + a0)β0(μ0aμ0xμ0−1 + . . . + a1)β1.
The leading term in the expansion of cα,β0,β1xαzβ0zβ1
1
in f(˜z) is
cα,β0,β1aβ0+β1
μ0
μβ1
0 xα−β1+μ0(β0+β1).
If we choose the maximal number in {α −β1 + μ0(β0 + β1) : cα,β0,β1 ̸= 0},
denoted by u0 + μ0v0, where u0 = α −β1, v0 = β0 + β1 for some cα,β0,β1 ̸= 0.
Then u + μ0v ≤u0 + μ0v0, for any (u, v) ∈ε(f). So we can deﬁne the line
L(f, μ0) : u+μ0v = u0 +μ0v0, which is feasible for f(z). Because μ0 ∈[si, si−1),
(u0, v0) = (ui, vi) or (ui+1, vi+1). Then we consider the coeﬃcient of xu0+μ0v0
in the expansion of f(˜z): H = 
α−β1+μ0(β0+β1)=u0+μ0v0 cα,β0,β1μβ1
0 aβ0+β1
μ0
=

(α−β1,β0+β1)∈L(f,μ0) cα,β0,β1μβ1
0 aβ0+β1
μ0
, and it must be zero. Now we consider
two cases.

Characteristic Set Method for Laurent Diﬀerential Polynomial Systems
193
1. If there exists only one vertex (u0, v0) ∈L(f, μ0)  NP(f), then H can be
written as H = v0
k=0 cu0+k,v0−k,kμk
0av0
μ0 = av0
μ0Φi(μ0). So μ0 must be a zero
of Φi. Thus if i < i0, we have m < μ0 ≤ηi ∈N, a contradiction to the choice
of m. Otherwise, i = i0 implies m ≥μ0, a contradiction to the choice of μ0.
2. If L(f, μ0)  NP(f) is an edge of NP(f), μ0 = si. If i = i0, μ0 = si0 ≤m <
μ0, a contradiction. If i < i0, m < μ0 ≤ηi ∈N, a contradiction to the choice
of m.
⊓⊔
Example 6. Let f be deﬁned in Example 3. We have η1 = 2, η2 = 1, η3 = 0.
By Theorem 5, an upper bound for the degrees of the polynomial solutions of
f(z) = 0 is 2.
We now consider the general case and a partial solution is given.
Theorem 6. For f(z) ∈F{z} of order n as deﬁned in (1),
Φ1(μ) =

P (α,β)=(u1,v1)
cα,β(μ)β1
1 . . . (μ)βn
n .
Use the notations in Deﬁnition 7. Then
1. If Φ1 is not the zero polynomial and t ≥2, let m be the maximal integer root
of Φ1(μ) = 0 in [s1, +∞). If such a number exists, then it is an upper bound
of the polynomial solutions of f(z) = 0. Otherwise, m = ⌊s1⌋is an upper
bound of the polynomial solutions of f(z) = 0.
2. If Φ1 is not the zero polynomial and t = 1, let m be the maximal integer root
of Φ1(μ) = 0 in (0, +∞). If such a number exists, then it is an upper bound
of the polynomial solutions of f(z) = 0. Otherwise, f = 0 has no polynomial
solutions.
3. If Φ1 ≡0 and t ≥2, let m = ⌊s1⌋. If prem(zm+1, f) = 0, then m is an upper
bound for the polynomial solutions of f(z) = 0.
Proof. 1. If M > s1 and ˜z = M
i=0 aixi is a polynomial solution of f(z) = 0
with aM ̸= 0. Then u1 + Mv1 > u + Mv for any (u1, v1) ̸= (u, v) ∈ε(f).
Indeed, from the line L(f, s1) : u + s1v = u1 + s1v1, we have u + s1v ≤
u1 + s1v1, for any (u1, v1) ̸= (u, v) ∈ε(f). Then u1 + Mv1 = u1 + s1v1 +
(M −s1)v1 ≥u + s1v + (M −s1)v1 > u + s1v + (M −s1)v = u + Mv, for any
(u1, v1) ̸= (u, v) ∈ε(f). So as in the proof of Theorem 5, the leading term
of f(˜z) is 
P (α,β)=(u1,v1) cα,β(M)β1
1 . . . (M)βn
n av1
Mxu1+Mv1. Because f(˜z) = 0
and aM ̸= 0, we have
Φ1(M) =

P (α,β)=(u1,v1)
cα,β(M)β1
1 . . . (M)βn
n = 0.
So if Φ1(μ) has no positive integer roots larger than s1, the degree of the
polynomial solutions of f(z) = 0 is not larger than s1. Otherwise, the number
m exists and we have M < m by the choice of m.

194
Y. Hu and X.-S. Gao
2. t = 1 implies that L(f, δ) : u + δv = u1 + δv1, 0 < δ < +∞are all the
feasible lines and L(f, δ)  NP(f) = {(u1, v1)}, which means that for any
(u′, v′) ∈NP(f), (u′, v′) ̸= (u1, v1) we have u′ + δv′ < u1 + δv1. Then if
z′ = N
i=0 aixi is a solution of f(z) = 0, where aN ̸= 0, substitute z′ into
f(z) and we have the leading term of f(z′(x)) is av1
N Φ1xu1+Nv1. So Φ1(N) = 0.
Because Φ1 is not the zero polynomial, we have N ≤m. Note that case 3 is
trivial.
Remark: An upper bound for the polynomial solutions is not given if Φ1 ≡0,
t ≥2 and prem(zm+1, f) ̸= 0 or Φ1 ≡0 and t = 1.
Example 7. For f(y) = x2y′4 −2xyy′3 −x2y2y′′2 + y2y′2, ε(f(y)) has one point
(−2, 4) and Φ1 ≡0 and t = 1. For g(y) = x2y′4 −2xyy′3 −x2y2y′′2 + y2y′2 + x6,
ε(g(y)) has two points P1 = (−2, 4) and P2 = (6, 0) and Φ1 ≡0. We have m
which is 2 and prem(y′′′, g) ̸= 0. Then, we can not bound the degrees of the
polynomial solutions of f = 0 and g = 0 in these two cases.
After an upper bound d for the degrees of the polynomial solutions of f(z) = 0
is given, we can check whether f is Laurent regular with the following theorem.
Theorem 7. Let f(z) be an irreducible diﬀerential polynomial in F{z}, n =
ord(f), d = deg(f, {z, z1, . . . , zn}), and m ∈N>0. Then f(z) = 0 has a generic
polynomial solution of degree no more than m if and only if prem(z(m+1), f(z)) =
0, which can be done with 22.4[2(d + 1)m+3]2.4(n+m+2) F-arithmetic operations.
Proof. If ˜z is a generic polynomial solution of f(z) = 0 with degree not larger
than m, then ˜z(m+1) = 0. Suppose g = prem(z(m+1), f). Then g =  aif (i) +
Hz(m+1), where ai ∈F{z} and H is a power of the product of the initial and
separant of f. So we have g(˜z) = 0. But g is reduced w.r.t f and f is irreducible,
thus g = 0. Assume prem(z(m+1), f(z)) = 0. Then we have  bif (i) = H′z(m+1),
for some bi ∈F{z} and H′ a power of the product of the initial and separant of
f. So if ˜z is a generic zero of f(z) = 0, then H′(˜z) ̸= 0, because f is irreducible.
Thus we have ˜z(m+1) = 0, which implies that ˜z is a polynomial of order less than
m + 1. The complexity follows from [14, Theorem 3.15].
⊓⊔
References
1. Aubry, P., Lazard, D., Maza, M.M.: On the theories of triangular sets. J. Symb.
Comput. 28, 105–124 (1999)
2. Boulier, F., Lemaire, F., Maza, M.M.: Computing diﬀerential characteristic sets
by change of ordering. J. Symb. Comput. 45, 124–149 (2010)
3. Bouziane, D., Kandri Rody, A., Maˆarouf, H.: Unmixed-dimensional decomposition
of a ﬁnitely generated perfect diﬀerential ideal. J. Symb. Comput. 31, 631–649
(2010)
4. Cano, J.: An extension of the Newton-Puiseux polygon construction to give solu-
tions of Pfaﬃan forms. Ann. Inst. Fourier 43, 125–142 (1993)
5. Cano, J.: On the series deﬁned by diﬀerential equations, with an extension of the
Puiseux polygon construction to these equations. Analysis 13, 103–120 (1993)

Characteristic Set Method for Laurent Diﬀerential Polynomial Systems
195
6. Chen, C., Davenport, J.H., May, J.P., Maza, M.M., Xia, B., Xiao, R.: Triangular
decomposition of semi-algebraic systems. J. Symb. Comput. 49, 3–26 (2013)
7. Chou, S.-C., Gao, X.-S.: Ritt-Wu’s decomposition algorithm and geometry theorem
proving. In: Stickel, M.E. (ed.) CADE 1990. LNCS, vol. 449, pp. 207–220. Springer,
Heidelberg (1990). doi:10.1007/3-540-52885-7 89
8. Chou, S.C., Gao, X.S.: Automated reasoning in diﬀerential geometry and mechan-
ics: part I. An improved version of Ritt-Wu’s decomposition algorithm. J. Autom.
Reason. 10, 161–172 (1993)
9. Gao, X.S., Huang, Z.: Characteristic set algorithms for equation solving in ﬁnite
ﬁelds. J. Symb. Comput. 47, 655–679 (2012)
10. Gao, X.S., Huang, Z., Yuan, C.M.: Binomial diﬀerence ideals. J. Symb. Comput.
80, 665–706 (2017)
11. Gao, X.S., Luo, Y., Yuan, C.: A characteristic set method for diﬀerence polynomial
systems. J. Symb. Comput. 44, 242–260 (2009)
12. Grigoriev, D.Y., Singer, M.: Solving ordinary diﬀerential equations in terms of
series with real exponents. Trans. AMS 327, 329–351 (1991)
13. Hubert, E.: Factorization-free decomposition algorithms in diﬀerential algebra. J.
Symb. Comput. 129, 641–662 (2000)
14. Li, W., Li, Y.H.: Computation of diﬀerential chow forms for ordinary prime diﬀer-
ential ideals. Adv. Appl. Math. 72, 77–112 (2016)
15. Li, X., Mou, C., Wang, D.: Decomposing polynomial sets into simple sets over
ﬁnite ﬁelds. Comput. Math. Appl. 60, 2983–2997 (2010)
16. Pauer, F., Unterkircher, A.: Gr¨obner bases for ideals in Laurent polynomial rings
and their application to systems of diﬀerence equations. AAECC 9, 271–291 (1999)
17. Sit, W.: The Ritt-Kolchin theory for diﬀferential polynomials. In: Diﬀerential Alge-
bra and Related Topics, pp. 1–70 (2002)
18. Wang, D.: Elimination Methods. Springer Science & Business Media, Heidelberg
(2012)
19. Wu, W.T.: Mathematics Mechanization. Science Press/Kluwer, Beijing (2001)
20. Wu, W.T., Gao, X.S.: Mathematics mechanization and applications after thirty
years. Front. Comput. Sci. 1, 1–8 (2007)
21. Yang, L., Zhang, J.: Searching dependency between algebraic equations. ICTP,
IC/91/6 (1991)
22. Zampieri, S.: A solution of the Cauchy problem for multidimensional discrete linear
shift-invariant systems. Linear Algebra Appl. 202, 143–162 (1994)
23. Zhu, W., Gao, X.S.: A triangular decomposition algorithm for diﬀerential poly-
nomial systems with elemenray complexity. J. Syst. Sci. Complex. 30, 464–483
(2017)

Sparse Polynomial Interpolation with Finitely
Many Values for the Coeﬃcients
Qiao-Long Huang(B) and Xiao-Shan Gao
KLMM, UCAS, Academy of Mathematics and Systems Science,
Chinese Academy of Sciences, Beijing 100190, China
huangqiaolong13@mails.ucas.ac.cn
Abstract. In this paper, we give new sparse interpolation algorithms
for black box polynomial f whose coeﬃcients are from a ﬁnite set. In the
univariate case, we recover f from one evaluation f(β) for a suﬃciently
large number β. In the multivariate case, we introduce the modiﬁed Kro-
necker substitution to reduce the interpolation of a multivariate polyno-
mial to that of the univariate case. Both algorithms have polynomial
bit-size complexity.
Keywords: Sparse polynomial interpolation · Modiﬁed Kronecker sub-
stitution · Polynomial time algorithms
1
Introduction
The interpolation for a sparse multivariate polynomial f(x1, . . . , xn) given as a
black box is a basic computational problem. Interpolation algorithms were given
when we know an upper bound for the terms of f [3] and upper bounds for the
terms and the degrees of f [12]. These algorithms were signiﬁcantly improved
and these works can be found in the references of [1].
In this paper, we consider the sparse interpolation for f whose coeﬃcients are
taken from a known ﬁnite set. For example, f could be in ZZ[x1, . . . , xn] with an
upper bound on the absolute values of coeﬃcients of f, or f is in Q[x1, . . . , xn]
with upper bounds both on the absolute values of coeﬃcients and their denom-
inators. This kind of interpolation is motivated by the following applications.
The interpolation of sparse rational functions leads to interpolation of sparse
polynomials whose coeﬃcients have bounded denominators [6, p. 6]. In [7], a
new method is introduced to reduce the interpolation of a multivariate polyno-
mial f to the interpolation of univariate polynomials, where we need to obtain
the terms of f from a larger set of terms and the method given in this paper is
needed to solve this problem.
In the univariate case, we show that if β is larger than a given bound depend-
ing on the coeﬃcients of f, then f can be recovered from f(β). Based on this idea,
we give a sparse interpolation algorithm for univariate polynomials with ratio-
nal numbers as coeﬃcients, whose bit complexity is O((td log H(log C + log H))
Partially supported by an NSFC grant No. 11688101.
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 196–209, 2017.
DOI: 10.1007/978-3-319-66320-3 15

Sparse Polynomial Interpolation
197
or 
O(td), where t is the number of terms of f, d is the degree of f, C and H
are upper bounds for the coeﬃcients and the denominators of the coeﬃcients
of f. It seems that the algorithm has the optimal bit complexity 
O(td) in all
known deterministic and exact interpolation algorithms for black box univariate
polynomials as discussed in Remark 2.
In the multivariate case, we show that by choosing a good prime, the inter-
polation of a multivariate polynomial can be reduced to that of the univariate
case in polynomial-time. As a consequence, a new sparse interpolation algorithm
for multivariate polynomials is given, which has polynomial bit-size complexity.
We also give its probabilistic version.
There exist many methods for reducing the interpolation of a multivariate
polynomial into that of univariate polynomials, like the classical Kronecker sub-
stitution, randomize Kronecker substitutions [2], Zipple’s algorithm [12], Klivans-
Spielman’s algorithm [9], Garg-Schost’s algorithm [4], and Giesbrecht-Roche’s
algorithm [5]. Using the original Kronecker substitution [10], interpolation for
multivariate polynomials can be easily reduced to the univariate case. The main
problem with this approach is that the highest degree of the univariate polyno-
mial and the height of the data in the algorithm are exponential. In this paper,
we give the following modiﬁed Kronecker substitution
xi = xmod((D+1)i−1,p), i = 1, 2, . . . , n
to reduce multivariate interpolations to univariate interpolations. Our approach
simpliﬁes and builds on previous work by Garg-Schost [4], Giesbrecht-Roche
[5], and Klivans-Spielman [9]. The ﬁrst two are for straight-line programs. Our
interpolation algorithm works for the more general setting of black box sampling.
The rest of this paper is organized as follows. In Sect. 2, we give interpola-
tion algorithms about univariate polynomials. In Sect. 3, we give interpolation
algorithms about multivariate polynomials. In Sect. 4, experimental results are
presented.
2
Univariate Polynomial Interpolation
2.1
Sparse Interpolation with Finitely Many Coeﬃcients
In this section, we always assume
f(x) = c1xd1 + c2xd2 + · · · + ctxdt
(1)
where d1, d2, . . . , dt ∈IN, d1 < d2 < · · · < dt, and c1, c2, · · · , ct ∈A, where
A ⊂C is a ﬁnite set. Introduce the following notations
C := max
a∈A(|a|),
ε := min(ε1, ε2)
(2)
where ε1 := mina,b∈A,a̸=b |a −b| and ε2 := mina∈A,a̸=0 |a|.
Theorem 1. If β ≥2C
ε + 1, then f(x) can be uniquely determined by f(β).

198
Q.-L. Huang and X.-S. Gao
Proof. Firstly, for k = 1, 2, · · · , we have β ≥2C
ε + 1 =⇒β −1 ≥2C
ε =⇒β −1 >
2C
ε
βk−1
βk
=⇒εβk > 2C βk−1
β−1 =⇒εβk > 2C(βk−1 + βk−2 + · · · + β + 1).
From (1), we have f(β) = c1βd1 + c2βd2 + · · · + ctβdt. Assume that there is
another form f(β) = a1βk1 + a2βk2 + · · · + asβks, where a1, a2, . . . , as ∈A and
k1 < k2 < · · · < ks. It suﬃces to show that ctβdt = asβks. The rest can be proved
by induction. First assume that dt ̸= ks. Without loss of generality, let dt > ks.
Then we have 0 = |(c1βd1 +c2βd2 +· · ·+ctβdt)−(a1βk1 +a2βk2 +· · ·+asβks)| ≥
|ct|βdt −C(βdt−1 + · · · + β + 1) −C(βks + · · · + β + 1) ≥|ct|βdt −2C(βdt−1 +
· · · + β + 1) > |ct|βdt −εβdt ≥0. It is a contradiction, so dt = ks.
Assume ct ̸= as, then 0 = |(c1βd1 + c2βd2 + · · · + ctβdt) −(a1βk1 + a2βk2 +
· · · + asβks) ≥|ct −as|βdt −2C(βdt−1 + · · · + β + 1) > |ct −as|βdt −εβdt ≥0.
It is a contradiction, so ct = as. The theorem has been proved.
⊓⊔
2.2
The Sparse Interpolation Algorithm
The idea of the algorithm is ﬁrst to obtain the maximum term m of f, then
subtract m(β) from f(β) and repeat the procedure until f(β) becomes 0.
We ﬁrst show how to compute the leading degree dt.
Lemma 1. Assume β ≥2C
ε + 1. If k ≤dt, then | f(β)
βk | > ε
2; if k > dt, then
| f(β)
βk | < ε
2.
Proof. From |f(β)| = |c1βd1 + c2βd2 + · · · + ctβdt| ≤C(βdt + · · · + β + 1) =
C( βdt+1−1
β−1
) and |f(β)| = |c1βd1 +c2βd2 +· · ·+ctβdt| ≥|ct|βdt −C(βdt−1 +· · ·+
β + 1) = |ct|βdt −C βdt−1
β−1 , we have |ct|βdt −C βdt−1
β−1 ≤|f(β)| ≤C( βdt+1−1
β−1
).
When k ≤dt, | f(β)
βk | ≥|ct|βdt−k −
C
β−1(βdt−k −
1
βk ) ≥εβdt−k −ε
2(βdt−k −
1
βk ) ≥
ε
2βdt−k + ε
2
1
βk >
ε
2. When k > dt, | f(β)
βk | ≤
C
β−1(βdt+1−k −
1
βk ) ≤
ε
2(βdt+1−k −
1
βk ) ≤ε
2βdt+1−k −ε
2
1
βk < ε
2.
⊓⊔
If we can use logarithm, we can change the above lemma into the following
form.
Lemma 2. If β ≥2C
ε + 1, then dt = ⌊logβ
2|f(β)|
ε
⌋.
Proof. By Lemma 1, we know
|f(β)|
βdt
>
ε
2 and
|f(β)|
βdt+1
<
ε
2. Then we have
logβ
|f(β)|
βdt
> logβ
ε
2 and logβ
|f(β)|
βdt+1 < logβ
ε
2, this can be reduced to logβ
2|f(β)|
ε
−
1 < dt < logβ
2|f(β)|
ε
. As dt is an integer, then we have dt = ⌊logβ
2|f(β)|
ε
⌋.
⊓⊔
Based on Lemma 2, we have the following algorithm which will be used in
several places.
Algorithm 2 (UDeg)
Input: f(β), ε, where β ≥2C
ε + 1.
Output: the degree of f(x).

Sparse Polynomial Interpolation
199
Step 1: return ⌊logβ( 2|f(β)|
ε
)⌋.
Remark 1. If we cannot use logarithm operation, then it is easy to show that we
need O(log2 D) arithmetic operations to obtain the degree based on Lemma 1.
In the following section, we will regard logarithm as a basic step.
Now we will show how to compute the leading coeﬃcient ct.
Lemma 3. If β ≥2C
ε +1, then ct is the only element in A that satisﬁes | f(β)
βdt −
ct| < ε
2.
Proof. First we show that ct satisﬁes | f(β)
βdt −ct| < ε
2. We rewrite f(β) as f(β) =
ctβdt+g(β), where g(x) := ct−1xdt−1+ct−2xdt−2+· · ·+c1xd1. So f(β)
βdt = ct+ g(β)
βdt .
As deg(g) < dt, by Lemma 1, we have | g(β)
βdt | < ε
2. So | f(β)
βdt −ct| < ε
2. Assume there
is another c ∈A also have | f(β)
βdt −c| < ε
2, then |ct−c| ≤| f(β)
βdt −c|+| f(β)
βdt −ct| < ε.
This can only happen when ct = c, so we have proved the uniqueness.
⊓⊔
Based on Lemma 3, we give the algorithm to obtain the leading coeﬃcient.
Algorithm 3 (ULCoef)
Input: f(β), β, ε, dt
Output: the leading coeﬃcient of f(x)
Step 1: Find the element c in A such that | f(β)
βdt −c| < ε
2.
Step 2: Return c.
Now we can give the complete algorithm.
Algorithm 4 (UPolySI)
Input: A black box univariate polynomial f(x), whose coeﬃcients are in A.
Output: The exact form of f(x).
Step 1: Find the bounds C and ε of A, as deﬁned in (2).
Step 2: Let β := 2C
ε + 1.
Step 3: Let g := 0, u := f(β).
Step 4: while u ̸= 0 do
d :=UDeg(u, ε, β); c :=ULCoef(u, β, ε, d); u := u −cβd; g := g + cxd;
end do.
Step 5: Return g.
Note that the complexity of Algorithm 3 depends on A, which is denoted by
OA. Note that OA ≤|A|. We have the following theorem.
Theorem 5. The arithmetic complexity of the Algorithm 4 is O(tOA)
≤
O(t|A|), where t is the number of terms in f.
Proof. Since ﬁnding the maximum degree needs one operation and ﬁnding the
coeﬃcient of the maximum term needs OA operations, and ﬁnding the maximum
term needs O(OA) operations, we have proved the theorem.
⊓⊔

200
Q.-L. Huang and X.-S. Gao
2.3
The Rational Number Coeﬃcients Case
In this section, we assume that the coeﬃcients of f(x) are rational numbers in
A = { b
a | 0 < a ≤H, | b
a| ≤C, a, b ∈ZZ}
(3)
and we have ε =
1
H(H−1). Notice that in Algorithm 4, only Algorithm 3
(ULCoef) needs reﬁnement. We ﬁrst consider the following general problem
about rational numbers.
Lemma 4. Let 0 < r1 < r2 be rational numbers. Then we can ﬁnd the small-
est d > 0 such that a rational number with denominator d is in (r1, r2) with
computational complexity O(log(r2 −r1)).
Proof. We consider three cases.
1. If one of the r1 and r2 is an integer and the other one is not, then the smallest
positive integer d such that (r2 −r1)d > 1 is the smallest denominator, and
d = ⌈
1
r2−r1 ⌉.
2. Both of r1, r2 are integers. If r2 −r1 > 1, then 1 is the smallest denominator.
If r2 −r1 = 1, then 2 is the smallest denominator.
3. Both of r1, r2 are not integers. This is the most complicated case.
First, we check if there exists an integer in (r1, r2). If ⌈r1⌉< r2, then ⌈r1⌉is
in the interval which has the smallest denominator 1.
Now we consider the case that (r1, r2) does not contain an integer. Assume
r1 < d1
d < r2, where d > 1 is the smallest denominator. Denote w :=trunc(r1),
ϵ1 := r1 −w,ϵ2 := r2 −w. Then ϵ1 < ϵ2 < 1 and d is smallest positive integer
such that (dr1, dr2) contains an integer. Since dr1 = d(w + ϵ1), dr2 = d(w + ϵ2),
d is the smallest positive integer such that interval (dϵ1, dϵ2) contains an integer.
We still denote it d1. Then dϵ1 < d1 < dϵ2, so d1
ϵ2 < d < d1
ϵ1 , and we can see that
d1 is the the smallest integer such that ( d1
ϵ2 , d1
ϵ1 ) contains an integer. Suppose we
know how to compute the number d1. Then d = ⌈d1
ϵ2 ⌉when d1
ϵ2 is not an integer,
and d = d1
ϵ2 + 1 when d1
ϵ2 is an integer.
Note that d1 is the smallest denominator such that some rational number
d
d1
is in ( 1
ϵ2 , 1
ϵ1 ). To ﬁnd d1, we need to repeat the above procedure to ( 1
ϵ2 , 1
ϵ1 ) and
obtain a sequence of intervals (r1, r2) →( 1
ϵ2 , 1
ϵ1 ) →· · · . The denominators of end
points of the intervals becomes smaller after each repetition. So the algorithm
will terminate.
Now we prove that the number of operations of the procedure is O(log(r2 −
r1)). First, we know the length of the interval (r1, r2) is r2−r1. Now we prove that
every time we run one or two recursive steps, the length of the new interval will
be 2 times bigger. Let ( b1
a1 , b2
a2 ) be the ﬁrst interval. If it contains an integer, then
we ﬁnish the algorithm. We assume that case does not happen, so we can assume
| b1
a1 | ≤1, | b2
a2 | ≤1. Then the second interval is ( a2
b2 , a1
b1 ). Now the new interval
length is a1
b1 −a2
b2 . If b1
a1 ≤1
2, then we have
a1
b1 −a2
b2
b2
a2 −b1
a1
=
a1b2−a2b1
b1b2
a1b2−a2b1
a1a2
= a1a2
b1b2 ≥2.

Sparse Polynomial Interpolation
201
If
b1
a1 > 1
2, then we let a1 = b1 + c1, a2 = b2 + c2 and the third interval is
( b1
c1 , b2
c2 ).
Then we have
b2
c2 −b1
c1
b2
a2 −b1
a1
=
c1b2−c2b1
c1c2
a1b2−a2b1
a1a2
= a1a2
c1c2 > 2. In this case, if we have an
interval whose length is bigger than 1, then the recursion will terminate. So if
(r2 −r1)2k ≥1, then 2k is the upper bound of the number of recursions. So the
complexity is O(log(r2 −r1)). We proved the lemma.
⊓⊔
Based on Lemma 4, we present a recursive algorithm to compute the rational
number in an interval (r1, r2) with the smallest denominator.
Algorithm 6 (MiniDenom)
Input: r1, r2 are positive rational numbers.
Output: the minimum denominator of rational numbers in (r1, r2)
Step 1: if one of r1, r2 is an integer and the other one is not an integer then
return ⌈
1
r2−r1 ⌉.
Step 2: if both of r1 and r2 are integers and r2 −r1 > 1 then return 1.
if both of r1 and r2 are integers and r2 −r1 = 1 then return 2.
Step 3: if ⌈r1⌉< r2, then return 1.
Step 4: let w :=trunc(r1), ϵ1 := r1 −w,ϵ2 := r2 −w;
d1 := MiniDenom( 1
ϵ2 , 1
ϵ1 );
if
d1
ϵ2 is a integer then return d1
ϵ2 + 1 else return ⌈d1
ϵ2 ⌉.
We now show how to compute the leading coeﬃcient of f(x).
Lemma 5. Suppose ct =
b
a, where gcd(a, b) = 1, a > 0, and Ii = ( f(β)
βdt i −
ε
2i, f(β)
βdt i + ε
2i), i = 1, 2, . . . , H. Then Ia ∩ZZ = {b} and if Ia0 ∩ZZ = {b0} then
b
a = b0
a0 .
Proof. By Lemma 3, we have f(β)
βdt −ε
2 <
b
a < f(β)
βdt + ε
2, so f(β)
βdt a −ε
2a < b <
f(β)
βdt a + ε
2a, and the existence is proved. The length of ( f(β)
βdt a −ε
2a, f(β)
βdt a + ε
2a)
is < 2 ε
2a ≤εH ≤
1
H−1 ≤1, so b is the unique integer in the interval.
Assume that there is another a0 ∈{1, 2, . . . , H}, such that ( f(β)
βdt a0 −
ε
2a0, f(β)
βdt a0+ ε
2a0) contains the integer b0. Then f(β)
βdt a0−ε
2a0 < b0 < f(β)
βdt a0+ ε
2a0,
so f(β)
βdt −ε
2 < b0
a0 < f(β)
βdt + ε
2. If a
b ̸= a0
b0 , then | a
b −a0
b0 | = | ab0−a0b
bb0
| ≥
1
H(H−1) = ε,
which contradicts that the length of the interval is less than ε.
⊓⊔
Let r1 := f(β)
βdt −ε
2, r2 := f(β)
βdt + ε
2. By Lemma 5, if a0 is the smallest positive
integer such that (a0r1, a0r2) contains the unique integer b0, then we have ct =
b0
a0 . Note that a0 is the smallest integer such that (a0r1, a0r2) contains the unique
integer b0 if and only if a0 is the smallest integer such that b0/a0 is in (r1, r2),
and such an a0 can be found with Algorithm 6. This observation leads to the
following algorithm to ﬁnd the leading coeﬃcient of f(x).

202
Q.-L. Huang and X.-S. Gao
Algorithm 7 (ULCoefRat)
Input: f(β)
βdt , ε, dt
Output: the leading coeﬃcient of f(x).
Step 1: if f(β)
βdt > 0, then r1 := f(β)
βdt −ε
2, r2 := f(β)
βdt + ε
2; else r1 := −f(β)
βdt −ε
2,
r2 := −f(β)
βdt + ε
2;
Step 2: Let a := MiniDenom (r1, r2);
Step 3: Return
⌈a( f(β)
βdt −ε
2 )⌉
a
Replacing Algorithm ULCoef with Algorithm ULCoefRat in Algorithm
UPolySI, we obtain the following interpolation algorithm for sparse polynomials
with rational coeﬃcients.
Algorithm 8 (UPolySIRat)
Input: A black box polynomial f(x) ∈Q[x] whose coeﬃcients are in A given in
(3).
Output: The exact form of f(x).
Theorem 9. The arithmetic operations of Algorithm 8 are O(t log H) and the
bit complexity is O(td log H(log C + log H)), where d is the degree of f(x).
Proof. In order to obtain the degree, we need one log arithmetic operation in
ﬁeld Q, while in order to obtain the coeﬃcient, we need O(log H) arithmetic
operations, so the total complexity is O(t log H). Assume f(β) = a1
h1 βd1+ a2
h2 βd2+
· · · + at
ht βdt and let Hi := h1 · · · hi−1hi+1 · · · ht. Then we have
f(β) = a1H1βd1 + a2H2βd2 + · · · + atHtβdt
h1h2 · · · ht
Then |a1H1βd1 + a2H2βd2 + · · · + atHtβdt| ≤Ht−1C(βdt + · · · + β + 1) =
Ht−1
C
β−1(βdt+1 −1), so its bit length is O(t log H + d log C + d log H). It is easy
to see that the bit length of h1h2 · · · ht is O(t log H). So the total bit complex-
ity is O((t log H)(t log H + d log C + d log H)). As t ≤d, the bit complexity is
O(td log H(log C + log H)).
⊓⊔
Corollary 1. If the coeﬃcients of f(x) are integers in [−C, C], then Algo-
rithm 8 computes f(x) with arithmetic complexity O(t) and with bit complexity
O(td log C).
Remark 2. The bit complexity of Algorithm 8 is 
O(td), which seems to be the
optimal bit complexity for deterministic and exact interpolation algorithms for
a black box polynomial f(x) ∈Q[x]. For a t-sparse polynomial, t terms are
needed and the arithmetic complexity is at least O(t). For β ∈C, we have
|f(β)| ≤C βd+1−1
β−1 , where C is deﬁned in (2). If |β| ̸= 1, then the height of f(β)
is d| log β| + log C or 
O(d). For a deterministic and exact algorithm, β satisfying
|β| = 1 seems not usable. So the bit complexity is at least 
O(td). For instance,
the height of the data in Ben-or and Tiwari’s algorithm is already 
O(td) [3,8].

Sparse Polynomial Interpolation
203
3
Multivariate Polynomial Sparse Interpolation with
Modiﬁed Kronecker Substitution
In this section, we give a deterministic and a probabilistic polynomial-time reduc-
tion of multivariate polynomial interpolation to univariate polynomial interpo-
lation.
3.1
Find a Good Prime
We will show how to ﬁnd a prime number which can be used in the reduction.
We assume f(x1, x2, . . . , xn) is a multivariate polynomial in Q[x1, x2, . . . , xn]
with a degree bound D, a term bound T, and p is a prime. We use the substitution
xi = xmod((D+1)i−1,p), i = 1, 2, . . . , n.
(4)
For convenience of description, we denote
fx,p := f(x, xmod((D+1),p), . . . , xmod((D+1)n−1,p)).
(5)
Then the degree of fx,p is no more than D(p −1) and the number of terms of
fx,p is no more than T.
If the number of terms of fx,p is the same as that of f(x1, x2, . . . , xn), there
is no collision in diﬀerent monomials and we call such prime as a good prime for
f(x1, x2, . . . , xn).
If p is a good prime, then we can consider a new substitution:
xi = qixmod((D+1)i−1,p), i = 1, 2, . . . , n,
(6)
where qi, i = 1, 2, . . . , n is the i-th prime. In this case, each coeﬃcient will change
according to monomials of f. Note that in [4], the substitution is
f(x, x(D+1), . . . , x(D+1)n−1)mod(xp −1). With this substitution, the substi-
tution (6) cannot be used.
We show how to ﬁnd a good prime p. We ﬁrst give a lemma.
Lemma 6. Suppose p is a prime. If mod(a1+a2(D+1)+· · ·+an(D+1)n−1, p)
̸= 0, then a1 + a2mod(D + 1, p) + · · · + anmod((D + 1)n−1, p) ̸= 0.
Proof. If a1+a2mod(D+1, p)+· · ·+anmod((D+1)n−1, p) = 0, then mod(a1+
a2(D + 1) + · · · + an(D + 1)n−1, p) = 0, which contradicts to the assumption. ⊓⊔
Now, we have the following theorem to ﬁnd the good prime.
Theorem 10. Let f(x1, . . . , xn) be polynomial with degree at most D and t ≤T
terms. If
N > T(T −1)
2
log2[(D + 1)n −1] −1
4T 2 + 1
2T
then there at least one of N distinct odd primes p1, p2, . . . , pN is a good prime
for f.

204
Q.-L. Huang and X.-S. Gao
Proof. Assume m1, m2, . . . , mt are all the monomials in f, and mi = xei,1
1
xei,2
2
· · · xei,n
n
. In order for p to be a good prime, we need ei,1 + ei,2(mod(D +
1, p)) + · · · + ei,n(mod((D + 1)n−1, p)) ̸= ej,1 + ej,2(mod(D + 1, p)) + · · · +
ej,n(mod((D + 1)n−1, p)), for all i ̸= j. This can be changed into (ei,1 −ej,1) +
(ei,2 −ej,2)(mod(D + 1, p)) + · · · + (ei,n −ej,n)(mod((D + 1)n−1, p)) ̸= 0. By
Lemma 6, it is enough to show
mod((ei,1 −ej,1) + (ei,2 −ej,2)(D + 1) + · · · + (ei,n −ej,n)(D + 1)n−1, p) ̸= 0, i ̸= j
Firstly, |(ei,1 −ej,1) + (ei,2 −ej,2)(D + 1) + · · · + (ei,n −ej,n)(D + 1)n−1| ≤
D(1 + (D + 1) + · · · + (D + 1)n−1) = (D + 1)n −1.
We assume that f(x) = a1xk1 + a2xk2 + · · · + atxkt is the polynomial after
the Kronecker substitution, where ki = ei,1 + ei,2(D + 1) + · · · + ei,n(D + 1)n−1.
If t = 2, it is trivial. So now we assume t > 2 and we analyse how many kinds
of primes the number 
i>j(ki −kj) has. Without loss of generality, assume
k1, k2 . . . , kw are even, kw+1, kw+2 . . . , kt are odd, denote v := t −w. It is easy
to see that ki −kj has factor 2 if 1 ≤i ̸= j ≤w or w + 1 ≤i ̸= j ≤t. If one
of w and v is zero, then 
i>j(ki −kj) has a factor 2
t(t−1)
2
. If both w, v are not
zero, then 
i>j(ki −kj) has a factor 2
w(w−1)
2
+ v(v−1)
2
. We give a lower bound of
w(w−1)
2
+ v(v−1)
2
.
As w(w−1)
2
+ v(v−1)
2
= w2+v2−t
2
≥1/2(w+v)2−t
2
= 1
4t2 −1
2t, 
i>j(ki −kj) at
least has a factor 2
1
4 t2−1
2 t. Since |ki−kj| ≤(D+1)n−1, we have 
i>j(ki−kj) ≤
[(D + 1)n −1]
t(t−1)
2
. If p1, p2, . . . , pN are distinct primes satisfying p1p2 . . . pN >
[(D+1)n−1]
t(t−1)
2
2
1
4 t2−1
2 t
, then at least one of the primes is a good prime. Since pi ≥2,
N > t(t−1)
2
log2[(D + 1)n −1] −1
4t2 + 1
2t. As we just know the upper bound T
of t, we can choose T −t diﬀerent positive integer kt+1, kt+2, . . . , kT which are
diﬀerent from k1, k2, . . . , kt. So we still can use T as the number of the terms.
We have proved the lemma.
⊓⊔
3.2
A Deterministic Algorithm
Lemma 7. Assume f =
c1
H1 xd1 + c2
H2 xd1 + · · · + ct
Ht xdt, where c1, c2, . . . , ct ∈
ZZ, H1, H2, . . . , Ht ∈ZZ+, d1, d2, . . . , dt ∈IN, d1 < d2 < · · · < dt, | ci
Hi | ≤C,
H1, H2, . . . , Ht, d1, d2, . . . , dt are known. Let Hmax := max{H1, H2, . . . , Ht}. If
β ≥2CHmax + 1, then we can recover c1, c2, . . . , ct from f(β).
Proof. It suﬃces to show that ct can be recovered from f(β). As β −1 ≥
2CHmax ≥2CHt, then 1
2 ≥CHt
β−1 . So |f(β)Ht −ctβdt| = | c1Ht
H1 βd1 + c2Ht
H2 βd2 +
· · · + ct−1Ht
Ht−1 βdt−1| ≤CHt( βdt−1
β−1 ) ≤1
2(βdt −1). So | f(β)Ht
βdt
−ct| < 1
2. That is
f(β)Ht
βdt
−1
2 < ct < f(β)Ht
βdt
+ 1
2. Since ct is an integer, ct = ⌈f(β)Ht
βdt
−1
2⌉. The rest
can be proved by induction.
⊓⊔

Sparse Polynomial Interpolation
205
Algorithm 11 (MPolySIMK)
Input: A black box polynomial f(x1, x2, . . . , xn) ∈A[x1, x2, . . . , xn], whose coef-
ﬁcients are in A given in (3), an upper bound D for the degree, an upper bound
T of the number of terms, a list of n diﬀerent primes q1, q2, . . . , qn(q1 < · · · < qn).
Output: The exact form of f(x1, x2, . . . , xn).
Step 1: Randomly choose N diﬀerent odd primes p1, p2, . . . , pN, where N =
⌊T (T −1)
2
log2[(D + 1)n −1] −1
4T 2 + 1
2T⌋+ 1.
Step 2: for i = 1, 2, . . . , N let fi := UPolySIRat(fx,pi, A, T) via Algorithm 8,
where fx,pi is deﬁned in (5).
Step 3: Let S := {};
for i = 1, 2, . . . , N do if fi ̸= failure, then S := S {fi}. end do;
Step 4: Repeat:
Choose one integer i such that fi has the most number of the terms in S.
if fi(j) = fx,pi(j) for j = 1, 2, . . . , D(pi −1) + 1 then break Repeat;
S := S\{fi}
end Repeat
Let i0 be the integer found and fi0 = c1
H1 xd1 + c2
H2 xd2 +· · ·+ ct
Ht xdt, d1 < d2 <
· · · < dt
Step 5: Let β := 2CqD
n max{H1, H2, . . . , Ht} + 1. [Lemma 7]
Denote g = f(q1x, q2xmod(D+1,pi0), . . . , qnxmod((D+1)n−1,pi0)) and let u :=
g(β);
Step 6: Let h := 0;
for i = t, t −1, . . . , 1 do
Let b := ⌈u
βdi Hi −1
2⌉. Factor
b
ci into qe1
1 qe2
2 · · · qen
n .
h := h + ci
Hi xe1
1 xe2
2 · · · xen
n . u := u −
b
Hi βdi.
Step 7: return h.
Remark 3. If pi is not a good prime for f, then the substitution fx,pi of f has
collisions. fx,pi may have some coeﬃcients not in A. So we need to modify step 4
of Algorithm 8 as follows, with T as an extra input. For c = a
b , if |c| > C, |b| > H,
or the number of the terms of fi are more than T, then we let fi = failure.
Theorem 12. Algorithm 11 is correct and its bit complexity is 
O(n2T 5D log H
log C + n2T 5D log2 H + n3T 6D2).
Proof. First, we show the correctness. If pi is a good prime for f, then all the coef-
ﬁcients of fx,pi are in A. So in step 2, Algorithm 8 can be used to ﬁnd fi = fx,pi.
It is suﬃcient to show that the prime pi0 corresponding to i0 obtained in step 4 is
a good prime. In step 4, if there exists a j0 such that fi(j0) ̸= fx,pi(j0), then fi ̸=
fx,pi. This only happens when some of the coeﬃcients of fx,pi are not in A. That
is, pi is not a good prime for f. So we throw it away. If fi0(j) = fx,pi0 (j) for j =
1, 2, . . . , D(pi0−1)+1 for some i0. Since deg fi0 ≤D(pi0−1), we have fi0 = fx,pi0 .
Assume by contradiction that pi0 is not a good prime for f, then the num-
ber of terms of fi0 is less than that of f. Since S includes at least one fi1 such
that pi1 is a good prime for f, the number of terms in fi1 is more than fi0.

206
Q.-L. Huang and X.-S. Gao
It contradicts that fi0 has the most number of the terms in S. So pi0 is a good
prime for f.
As fi0 =
c1
H1 xd1 + c2
H2 xd2 + · · · + ct
Ht xdt, d1 < d2 < · · · < dt, we can assume
f = c1
H1 m1+ c2
H2 m2+· · ·+ ct
Ht mt, where mi = xei,1
1
xei,2
2
· · · xei,n
n
. We can write g as
g = f(q1x, q2xmod(D+1,pi0), . . . , qnxmod((D+1)n−1,pi0)) = c1q
e1,1
1
q
e1,2
2
···q
e1,n
n
H1
xd1 +
c2q
e2,1
1
q
e2,2
2
···q
e2,n
n
H2
xd2+· · ·+ ctq
et,1
1
q
et,2
2
···q
et,n
n
Ht
xdt. Since | ciq
ei,1
1
q
ei,2
2
···q
ei,n
n
Hi
| ≤CqD
n , by
Lemma 7, in step 6, b = ciqei,1
1
qei,2
2
· · · qei,n
n
. By Factoring
b
ci = qei,1
1
qei,2
2
· · · qei,n
n
,
we obtain the degrees of mi. We have proved the correctness.
We now analyse the complexity. In step 2, we call Algorithm UPolySIRat
O(nT 2 log D) times. The degree of fx,pi is bounded by D(pi −1). Since the
i-th prime is O(i log i) and we use at most O(nT 2 log D) primes, the degree
bound is

O(nT 2D). So by Theorem 9, the bit complexity of getting all fi
is 
O((nT 3D log H)(log C + log H)(nT 2 log D)), that is 
O(n2T 5D log H log C +
n2T 5D log2 H).
In step 4, since deg fi is 
O(nT 2D), by fast multipoint evaluation [11, p.
299], it needs 
O(nT 2D) operations. The number of the fi that we need to
check is at most 
O(nT 2 log D), so the total arithmetic operations for evalu-
ations is 
O(n2T 4D). As the coeﬃcients of fi are in A and the number of
terms is less than T, the data is 
O(TC(nT 2D)nT 2DHT ). So the height of
the data is 
O(nT 2D + log C + T log H). The total bit complexity of step 4 is

O(n3T 6D2 + n2T 4D log C + n2T 5D log H).
In step 6, we need to obtain t terms of g. We analyse the bit complex-
ity of one step of the cycle. To obtain b, we need O(1) arithmetic opera-
tions. The height of the data is 
O(nT 2D(log C + D log n + log H)), so the bit
complexity is 
O(nT 2D log C + nT 2D2 + nT 2D log H). To factor
b
ci , we need
n log2 D operations. The data of b and ci is 
O(CqD
n H), so the bit complexity is

O(n log2 D log C + nD + n log2 D log H). So the total bit complexity of step 6 is

O(nT 3D log C + nT 3D2 + nT 3D log H).
Therefore, the bit complexity is 
O(n2T 5D log H log C +n2T 5D log2 H +n3T 6
D2).
⊓⊔
Remark 4. If A = {a|C ≥|a|, a ∈ZZ}, we can modify Algorithm 11. Assume
AT = {a|TC ≥|a|, a ∈ZZ}. In step 2, we let fi := UPolySIRat(fx,pi, AT ).
Note that fx,pi is an integer polynomial with coeﬃcients bounded by TC, fi =
fx,pi. So in step 4, we just ﬁnd the smallest integer i0 that fi0 has the most
number of the terms in S. In this case, pi0 is a good prime for f. The bit
complexity of the algorithm will be 
O(n2T 5D log C + nT 3D2).
3.3
Probabilistic Algorithm
Giesbrecht and Roche [5, Lemma 2.1] proved that if λ = max{21, 5
3nT(T −
1) ln D}, then a prime p chosen at random in [λ, 2λ] is a good prime for f(x1, . . . ,
xn) with probability at least
1
2. Based on this result, we give a probabilistic
algorithm.

Sparse Polynomial Interpolation
207
Algorithm 13 (ProMPolySIMK)
Input: A black box polynomial f(x1, . . . , xn) ∈A[x1, . . . , xn], whose coeﬃcients
are in A given in (3), an upper bound D for the degree, an upper bound T of
the number of terms, a list of n diﬀerent primes q1, q2, . . . , qn(q1 < · · · < qn).
Output: The exact form of f(x1, . . . , xn) with probability ≥1
2.
Step 1: Let λ := max{21, 5
3nT(T −1) ln D}, randomly choose a prime p in
[λ, 2λ].
Step 2: Let fp := UPolySIRat(fx,p, A, T) via Algorithm 8.
if fp = failure then return failure;
Assume fp = c1
H1 xd1 + c2
H2 xd2 + · · · + ct
Ht xdt, d1 < d2 < · · · < dt
Step 3: Let β := 2CqD
n max{H1, H2, . . . , Ht} + 1. [Lemma 7]
Denote g(x) = f(q1x, q2xmod(D+1,p), . . . , qnxmod((D+1)n−1,p)). Let u:= g(β);
Step 4: Let s := 0;
for i = t, t −1, . . . , 1 do
Let b := ⌈u
βdi Hi −1
2⌉
Factor
b
ci = kqe1
1 qe2
2 · · · qen
n , where qi ∤k, i = 1, 2, . . . , n
if k ̸= 1 or e1 + e2 + · · · + en > D then return failure;
s := s + ci
Hi xe1
1 xe2
2 · · · xen
n .
u := u −
b
Hi xdi.
end do;
if u = 0 then return s else return failure;
Theorem 14. The bit complexity of Algorithm 13 is

O(nT 3D log H log C +
nT 3D log2 H + nT 3D2).
Proof. In step 2, the degree of fx,p is bounded by D(p−1). As p is O(nT 2 log D),
the degree bound is 
O(nT 2D). By Theorem 9, the complexity is 
O((nT 3D log H)
(log C + log H)), or 
O(nT 3D log H log C + nT 3D log2 H).
In step 4, we need to obtain t terms of g. We analyse the bit complexity
of one step of the cycle. To obtain b, we need O(1) arithmetic operations. The
height of the data is 
O(nT 2D(log C + D log n + log H)), so the bit complexity is

O(nT 2D log C+nT 2D2+nT 2D log H). To factor b
ci , we need n log2 D operations.
The height of b and ci is 
O(CqD
n H), so the bit complexity is 
O(n log2 D log C +
nD + n log2 D log H). So the total bit complexity of step 4 is 
O(nT 3D log C +
nT 3D2 + nT 3D log H).
Therefore, the total bit complexity of the algorithm is 
O(nT 3D log H log C +
nT 3D2 + nT 3D log2 H).
⊓⊔
Remark 5. In Algorithm 13, we also modify step 4 of Algorithm 8 as in Remark 4.
4
Experimental Results
In this section, practical performances of the algorithms will be presented. The
data are collected on a desktop with Windows system, 3.60 GHz Core i7 – 4790

208
Q.-L. Huang and X.-S. Gao
CPU, and 8 GB RAM memory. The implementations in Maple can be found in
http://www.mmrc.iss.ac.cn/∼xgao/software/sicoeﬀ.zip
We randomly construct ﬁve polynomials, then regard them as black box
polynomials and reconstruct them with the algorithms. The average times are
collected. The results for univariate interpolation are shown in Figs. 1, 2, 3 and
4. In each ﬁgure, three of the parameters C, H, D, T are ﬁxed and one of them is
variable. From these ﬁgures, we can see that Algorithm UPolySIRat is linear
in T, approximately linear in D, logarithmic in C and H. The results in the
multivariate case are shown in Figs. 5 and 6. We just test the probabilistic algo-
rithm. From these ﬁgures, we can see that the Algorithm ProMPolySIMK is
polynomial in T and D.
Fig. 1. UPolySIRat: average running
times with varying T
Fig. 2. UPolySIRat: average run-
ning times with varying D
Fig. 3. UPolySIRat: average running
times with varying C
Fig. 4. UPolySIRat: average run-
ning times with varying H
Fig. 5.
ProMPolySIMK:
average
running times with varying T
Fig. 6. ProMPolySIMK: average
running times with varying D

Sparse Polynomial Interpolation
209
5
Conclusion
In this paper, a new type of sparse interpolation is considered, that is, the coef-
ﬁcients of the black box polynomial f are from a ﬁnite set. Speciﬁcally, we
assume that the coeﬃcients are rational numbers such that the upper bounds of
the absolute values of these numbers and their denominators are given, respec-
tively. We ﬁrst give an interpolation algorithm for a univariate polynomial f,
where f is obtained from one evaluation f(β) for a suﬃciently large number β.
Then, we introduce the modiﬁed Kronecker substitution to reduce the interpola-
tion of a multivariate polynomial into the univariate case. Both algorithms have
polynomial bit-size complexity and the algorithms can be used to recover quite
large polynomials.
References
1. Arnold, A.: Sparse polynomial interpolation and testing. Ph.D. thesis, Waterloo
Unversity, Canada (2016)
2. Arnold, A., Roche, D.S.: Multivariate sparse interpolation using randomized Kro-
necker substitutions. In: ISSAC 2014, 23–25 July, Kobe, Japan (2014)
3. Ben-Or, M., Tiwari, P.: A deterministic algorithm for sparse multivariate polyno-
mial interpolation. In: 20th Annual ACM Symposium on Theory of Computing,
pp. 301–309 (1988)
4. Garg, S., Schost, ´E.: Interpolation of polynomials given by straight-line programs.
Theoret. Comput. Sci. 410(27–29), 2659–2662 (2009)
5. Giesbrecht, M., Roche, D.S.: Diversiﬁcation improves interpolation. In: Proceed-
ings of the ISSAC 2011, pp. 123–130. ACM Press (2011)
6. Huang, Q.L., Gao, X.S.: Sparse rational function interpolation with ﬁnitely many
values for the coeﬃcients arXiv:1706.00914 (2017)
7. Huang, Q.L., Gao, X.S.: New algorithms for sparse interpolation and identity test-
ing of multivariate polynomials. Preprint (2017)
8. Kaltofen, E., Yagati, L.: Improved sparse multivariate polynomial interpolation
algorithms. In: Gianni, P. (ed.) ISSAC 1988. LNCS, vol. 358, pp. 467–474. Springer,
Heidelberg (1989). doi:10.1007/3-540-51084-2 44
9. Klivans, A.R., Spielman, D.: Randomness eﬃcient identity testing of multivariate
polynomials. In: Proceedings of the STOC 2001, pp. 216–223. ACM Press (2001)
10. Kronecker, L.: Grundz¨uge einer arithmetischen Theorie der algebraischen Gr¨ossen.
J. Reine Angew. Math. 92, 1–122 (1882)
11. von zur Gathen, J., Gerhard, J.: Modern Computer Algebra. Cambridge University
Press, Cambridge (1999)
12. Zippel, R.: Interpolating polynomials from their values. J. Symbolic Comput. 9(3),
375–403 (1990)

On Stationary Motions of the Generalized
Kowalewski Gyrostat and Their Stability
Valentin Irtegov and Tatyana Titorenko(B)
Institute for System Dynamics and Control Theory SB RAS,
134, Lermontov str., Irkutsk 664033, Russia
{irteg,titor}@icc.ru
Abstract. The stationary motions of the Kowalewski gyrostat in two
constant force ﬁelds are studied. It is revealed that the equations of
motion of the gyrostat have the families of permanent rotations when
the force ﬁelds are parallel, and the families of equilibria when these
ﬁelds have special directions. It is shown that all the found solutions
belong to an intersection of two invariant manifolds of codimension 2.
The analysis of stability in the Lyapunov sense for these solutions is
conducted.
1
Introduction
The problem of the rotational motion of a gyrostat (a rigid body with a sym-
metrical rotor placed inside it) in two constant force ﬁelds is considered. Mass
distribution in the body is subject to the Kowalewski conditions [1]. Similar prob-
lems arise, e.g., in space dynamics [2], quantum mechanics [3]. The equations of
motion of the gyrostat represent a completely Liouville integrable system: an
additional ﬁrst integral of the problem has been found in [4]. It should be noted
that so far this system is poorly studied because of computational diﬃculties
arising in the process of its investigation. There exists a series of works devoted
to the topological analysis of the system (see, e.g., [5,6]). We conduct the quali-
tative analysis for the equations of motion of the gyrostat. Our approach to the
study of similar problems is based on solving an extremum problem for the ele-
ments of the algebra of the problem’s ﬁrst integrals that enables us to reduce the
problem of the qualitative analysis of diﬀerential equations to an algebraic one
and to apply computer algebra tools in our study. In the present work, within
the framework of the qualitative analysis of the system under consideration, we
study the stationary motions of the gyrostat and their stability. By stationary
motions, we mean solutions of the equations of motion on which the ﬁrst inte-
grals (or their combinations) in the problem under study take stationary values.
The latter allows one to use these integrals for obtaining a Lyapunov function
to investigate the stability of such solutions. All computations represented in
this paper have been performed with “Mathematica” computer algebra system
(CAS) as well as the software package [7].
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 210–224, 2017.
DOI: 10.1007/978-3-319-66320-3 16

On Stationary Motions of the Generalized Kowalewski Gyrostat
211
2
Formulation of the Problem
For describing the motion of the gyrostat, an inertial coordinate system with its
origin at a ﬁxed point O of the body is introduced. The Oxyz frame is rigidly
attached to the body. The axes of the frame are directed along the principal
axes of inertia of the body. The rotor axis coincides with the Oz axis. The
inertia moments of the gyrostat are related as follows: A = B = 2C.
The equations of motion of the gyrostat in the Oxyz frame can be written as:
2 ˙p = q (r −λ) + b δ3,
˙γ1 = γ2r −γ3q,
˙δ1 = δ2r −δ3q,
2 ˙q = x0γ3 −p (r −λ),
˙γ2 = γ3p −γ1r,
˙δ2 = δ3p −δ1r,
˙r = −bδ1 −x0γ2,
˙γ3 = γ1q −γ2p,
˙δ3 = δ1q −δ2p.
(1)
Here p, q, r are the projections of the angular velocity vector onto the axes of
the Oxyz frame; γi (i = 1, 2, 3) are the components of the direction vector of the
1st force ﬁeld; δi (i = 1, 2, 3) are the components of the direction vector of the
2nd force ﬁeld; (x0, 0, 0), (0, b, 0) are the radius vectors of the 1st and 2nd force
centers, respectively; λ = const is the gyrostatic parameter.
Equation (1) admit the following ﬁrst integrals:
2H = 2(p2 + q2) + r2 + 2(x0γ1 −b δ2) = 2h,
V1 = (p2 −q2 −x0γ1 −b δ2)2 + (2p q −x0γ2 + b δ1)2
+2λ[(p2 + q2)(r −λ) + 2(b qδ3 −p x0γ3)] = c1,
V2 = γ2
1 + γ2
2 + γ2
3 = 1, V3 = δ2
1 + δ2
2 + δ2
3 = 1,
V4 = γ1δ1 + γ2δ2 + γ3δ3 = c2,
V5 = x2
0 [pγ1 + qγ2 + 1
2(r + λ)γ3]2 + b2 [pδ1 + qδ2 + 1
2(r + λ)δ3]2
−x0b (r −λ)[(γ2δ3 −γ3δ2)p + (γ3δ1 −γ1δ3)q + 1
2(r + λ)(γ1δ2 −γ2δ1)]
+x0b2γ1(δ2
1 + δ2
2 + δ2
3) −x2
0b δ2(γ2
1 + γ2
2 + γ2
3)
−bx0(bδ1 −γ2x0)(δ1γ1 + δ2γ2 + δ3γ3) = c3,
(2)
where V5 is the additional ﬁrst integral found in [4], h, c1, c2, c3 are some con-
stants.
Note, as c2 is an arbitrary constant, Eq. (1) together with the integral V4 = c2
describe the motion of the gyrostat in the constant force ﬁelds having special
directions (according to the value of the parameter c2). Thus, system (1), (2)
can be considered as a family of the systems parameterized by c2.
When b = 0, system (1), (2) corresponds to an integrable case in the problem
of motion of the Kowalewski gyrostat in a gravitational force ﬁeld [8].
The purpose of this work is to ﬁnd the stationary solutions of Eq. (1) and to
investigate their qualitative properties.
3
Finding the Stationary Solutions
Traditionally, stationary solutions can be obtained from the conditions of station-
arity for the ﬁrst integrals of a problem (see, e.g., [9]). In the case under consid-
eration, following this technique, we should solve a system of 9 nonhomogeneous
cubic equations with respect to the phase variables p, q, r, γi, δi (i = 1, 2, 3). In
the given work, we apply another technique represented below.

212
V. Irtegov and T. Titorenko
3.1
Permanent Rotations
For ﬁnding the desired solutions, we equate the right-hand sides of diﬀerential
Eq. (1) to zero, and add relations V2 = 1, V3 = 1 (2) to them. For the polyno-
mials of a resulting system (the system of quadratic equations), we construct a
lexicographical Gr¨obner basis with respect to some part of the phase variables,
e.g., δ1, δ2, δ3, γ1, γ2, γ3, p, q with the “Mathematica” procedure GroebnerBasis.
A result will be a system which is decomposed into two subsystems:
(I)
b4r2 −q2(λ −r)2 [b2(q2 + r2) + x2
0q2] = 0, bp −x0q = 0,
b γ3 + q (λ −r) = 0, b rγ2 + q2 (λ −r) = 0, b2 rγ1 + x0q2 (λ −r) = 0,
b δ3 −q (λ −r) = 0, b rδ2 −q2 (λ −r) = 0, −b2 rδ1 + x0q2 (λ −r) = 0.
(3)
(II)
b4r2 −q2(λ −r)2 [b2(q2 + r2) + x2
0q2] = 0, bp + x0q = 0,
b γ3 −q (λ −r) = 0, b rγ2 −q2 (λ −r) = 0, b2 rγ1 −x0q2 (λ −r) = 0,
b δ3 −q (λ −r) = 0, b rδ2 −q2 (λ −r) = 0, −b2 rδ1 −x0q2 (λ −r) = 0.
(4)
As can easily be veriﬁed by invariant manifold (IM) deﬁnition, Eqs. (3), (4) deﬁne
2 one-dimensional IMs of Eq. (1).
The vector ﬁeld on each IM is given by the equation ˙r = 0. It has the following
family of solutions:
r = r0 = const.
(5)
Hence, geometrically, IMs (3), (4) in space R9 correspond to 2 curves, over each
point of which the family of solutions (5) is deﬁned.
Equations (3) with (5) represent 4 families of solutions for the equations of
motion (1):
p = ∓x0 α1 r1/2
0
(λ −r0)−1/2, q = ∓b α1 r1/2
0
(λ −r0)−1/2, r = r0,
γ1 = −x0 α2
1, γ2 = −b α2
1, γ3 = ±[r0 (λ −r0)]1/2α1, δ1 = x0 α2
1,
δ2 = b α2
1, δ3 = ∓[r0 (λ −r0)]1/2α1;
(6)
p = ±x0 α2 r1/2
0
(r0 −λ)−1/2, q = ±b α2 r1/2
0
(r0 −λ)−1/2, r = r0,
γ1 = x0 α2
2, γ2 = b α2
2, γ3 = ±[r0 (r0 −λ)]1/2α2, δ1 = −x0 α2
2,
δ2 = −b α2
2, δ3 = ∓[r0 (r0 −λ)]1/2α2.
(7)
Here r0 is the parameter of the families, α1 = ρ1 β, α2 = ρ2 β, β = [2 (b2 +
x2
0)]−1/2, ρ1 = [r0 (r0 −λ) + (4(b2 + x2
0) + (λ −r0)2r2
0)1/2]1/2,
ρ2 = [r0 (λ −r0) + (4(b2 + x2
0) + (λ −r0)2r2
0)1/2]1/2.
Equations (4), (5) determine the following 4 families of solutions for
system (1):
p = ±x0 α1r1/2
0
(λ −r0)−1/2, q = ∓b α1r1/2
0
(λ −r0)−1/2, r = r0,
γ1 = −x0 α2
1, γ2 = b α2
1, γ3 = ∓[r0 (λ −r0)]1/2α1, δ1 = −x0 α2
1,
δ2 = b α2
1, δ3 = ∓[r0 (λ −r0)]1/2α1;
(8)

On Stationary Motions of the Generalized Kowalewski Gyrostat
213
p = ∓x0 α2 r1/2
0
(r0 −λ)−1/2, q = ±b α2 r1/2
0
(r0 −λ)−1/2, r = r0,
γ1 = x0 α2
2, γ2 = −b α2
2, γ3 = ∓[r0 (r0 −λ)]1/2α2,
δ1 = x0 α2
2, δ2 = −b α2
2, δ3 = ∓[r0 (r0 −λ)]1/2α2.
(9)
On substituting solutions (6), (7) into integral V4 (2), the latter is identically
equal to −1. On solutions (8), (9), this integral becomes identically 1. So, with
mechanical viewpoint, the elements of the families of solutions (6)–(9) corre-
spond to the permanent rotations of the gyrostat in the parallel (or opposite in
direction) force ﬁelds. The gyrostat rotates around the coinciding (or opposite)
directions of the force ﬁelds with the angular velocity ω2 = r0 (ρ2
1/(2(λ−r0))+r0)
(ω2 = r0 (ρ2
2/(2(r0−λ))+r0)). The axis position of rotation in the body depends
on the parameter r0 and does not coincide with the principal axes of inertia of
the body.
Now, we show that all the solutions found above are stationary. First, we
consider the families of solutions (6).
Let
2K = 2λ0H −λ1V1 −λ2V2 −λ3V3 −2λ4V4 −4λ5V5
(10)
be the family of the problem’s ﬁrst integrals, where λi (i = 0, . . . , 5) are the
parameters of the family.
We write down the necessary conditions for the integral K to have an
extremum with respect to the phase variables
∂K
∂p = 0, ∂K
∂q = 0, ∂K
∂r = 0, ∂K
∂γi
= 0, ∂K
∂δi
= 0, (i = 1, 2, 3)
(11)
and ﬁnd the values of λ2, λ3, λ5
λ2 = x2
0 χ + λ4, λ3 = b2 χ + λ4, λ5 =
1
(λ2 −r2
0 + ρ2
1)
2λ0
ρ2
1
−
λλ1
(λ −r0)

,
under which solutions (6) satisfy equation (11). Here χ = [2(r2
0 −λ2)λ0−λρ2
1(λ−
3r0)λ1] [ρ1 (λ −r0)]−2.
Having substituted the above expressions into (10), we have the following
family of integrals:
2K1 = 2 ˜K1 −(V2 + V3 + 2V4) λ4,
where 2 ˜K1 = 2

H + (λ + r0)(x2
0V2 + b2V3)
ρ2
1(λ −r0)
−
4V5
ρ2
1(λ2 −r2
0 + ρ2
1)

λ0
+
λ (λ −3r0)(x2
0V2 + b2V3)
(λ −r0)2
−V1 +
4λV5
(λ −r0)(λ2 −r2
0 + ρ2
1)

λ1.
(12)
It is split up into 3 subfamilies of the integrals which correspond to the coeﬃ-
cients of λ0, λ1, λ4, respectively. The elements of both the family of the integrals

214
V. Irtegov and T. Titorenko
K1 and its subfamilies take stationary values on the elements of families (6). The
latter is veriﬁed by direct computation. So, the solutions under consideration are
stationary. The family K1 and its subfamilies – both individually and in combi-
nation – can be used for obtaining a Lyapunov function to analyze stability of
solutions (6).
In a similar manner, we prove the stationarity of solutions (7)–(9). Note that
IM (3) and IM (4), which the families of solutions under consideration belong
to, are stationary. The integrals V2 + V3 + 2V4 and V2 + V3 −2V4 take stationary
values on these IMs, respectively.
3.2
Equilibria
Using the technique chosen, we have found another group of the stationary
solutions of diﬀerential Eq. (1) in the case when p = q = r = 0.
The equations
p = 0, q = 0, r = 0, γ3 = 0, δ3 = 0,
x0γ2 + bδ1 = 0, b2δ2
1 + x2
0 (γ2
1 −1) = 0, δ2
1 + δ2
2 = 1
(13)
deﬁne one-dimensional IM of Eq. (1). Likewise as above, it can be veriﬁed by IM
deﬁnition. The vector ﬁeld on this IM is described by the equation ˙δ1 = 0 which
has the following family of solutions:
δ1 = δ0
1 = const.
(14)
Equations (13) with (14) determine 4 families of solutions for the equations
of motion (1):
p = 0, q = 0, r = 0, γ1 = ∓(x2
0 −b2δ02
1 )1/2x0
−1, γ2 = −b δ0
1x−1
0 , γ3 = 0,
δ1 = δ0
1, δ2 = (1 −δ02
1 )1/2, δ3 = 0;
(15)
p = 0, q = 0, r = 0, γ1 = ∓(x2
0 −b2δ02
1 )1/2x−1
0 , γ2 = −b δ0
1x−1
0 , γ3 = 0,
δ1 = δ0
1, δ2 = −(1 −δ02
1 )1/2, δ3 = 0.
(16)
Here δ0
1 is the family parameter, |δ0
1| ≤1 and |δ0
1| ≤|x0b−1| are the conditions
for the solutions to be real.
With mechanical viewpoint, the elements of the families of solutions (15),
(16) correspond to the equilibria of the gyrostat.
The integral V4 takes the values ±δ0
1x−1
0
b (1 −δ02
1 )1/2 ± (x2
0 −b2δ02
1 )1/2 on
the corresponding elements of the families of solutions (15), (16). So, each equi-
librium corresponds to a deﬁnite angle between the directions of the force ﬁelds.
In [5], four equilibria of the above families for the case of the orthogonal force
ﬁelds are presented.
With the aid of the technique described above, we have found the families of
the integrals the elements of which take stationary values on the corresponding
elements of families (15), (16). Below, one of these families is represented.

On Stationary Motions of the Generalized Kowalewski Gyrostat
215
2K2 =
2 [4V5 −((λ2 + 4z)z −2(b2 + x2
0)) H]
λ2 + 2z
−V1

λ1
+
4V5 −2[(λ2 + 3z)z −(b2 + x2
0)] H
x2
0 (λ2 + 2z)
−V2 −b2V3
x2
0

λ2 +
 ¯zV3
δ0
1x0
+2

1
δ0
1x0
−2 [δ02
1 z −(x2
0 −b2δ02
1 )1/2]
δ0
1x0(λ2 + 2z)

H −2V4 −4(1 −δ02
1 )1/2V5
b δ0
1x0(λ2 + 2z)

λ4.
(17)
Here z = b (1 −δ02
1 )1/2 + (x2
0 −b2δ02
1 )1/2, ¯z = b (1 −δ02
1 )1/2 −(x2
0 −b2δ02
1 )1/2.
The elements of the family K2 and its subfamilies (the coeﬃcients of
λ1, λ2, λ4) assume stationary values on the elements of the 1st family of solutions
(15). The corresponding families of the integrals for other solutions (15), (16)
are similar to the above family.
4
On Invariant Manifolds of Codimension 2
Let us show that all the solutions found above belong to two IMs of codimension
2 of the equations of motion (1). We shall obtain these IMs from the station-
ary conditions for integral K (10) by resolving them with respect to part of the
phase variables and the parameters λi. The given technique, based on Gr¨obner
basis method, was already applied by the authors repeatedly (see, e.g., [10]).
In the problem under study, its direct application reduces to cumbersome com-
putations. In order to avoid these diﬃculties, we replace the initial problem’s
variables with the following ones:
x1 = −(x0γ1 + bδ2) −i(x0γ2 −bδ1), x2 = −(x0γ1 + bδ2) + i(x0γ2 −bδ1),
y1 = −(x0γ1 −bδ2) −i(x0γ2 + bδ1), y2 = −(x0γ1 −bδ2) + i(x0γ2 + bδ1),
z1 = −x0γ3 + ib δ3, z2 = −x0γ3 −ib δ3, w1 = p + iq, w2 = p −iq, w3 = r.
(18)
These are similar to (3.8) [5].
In the above variables, the equations of motion and the problem’s ﬁrst inte-
grals can be written as
2 ˙w1 = i(w1(λ −w3) −z1), ˙x1 = i(w1z1 −w3x1),
˙y1 = i(w1z2 −w3y1),
2 ˙w2 = i(w2(w3 −λ) + z2), ˙x2 = i(w3x2 −w2z2),
˙y2 = i(w3y2 −w2z1),
2 ˙w3 = i(y2 −y1), 2 ˙z1 = i(w2x1 −w1y2), 2 ˙z2 = i(w2y1 −w1x2)
(19)
and
2 ˜H = 2w1w2 + w2
3 −y1 −y2 = 3˜h,
˜V1 = (w2
1 + x1)(w2
2 + x2) + 2λ [w2z1 + w1(w2w3 + z2)] −2λ2w1w2 = ˜c1,
˜V2 = (x1 + y1)(x2 + y2) + (z1 + z2)2 = 1,
˜V3 = (x1 −y1)(x2 −y2) −(z1 −z2)2 = 1,
˜V4 = x1y2 −x2y1 + z2
1 −z2
2 = ˜c2,
˜V5 = x1y1(2w2
2 + x2) + x2y2(2w2
1 + x1) −y1y2(y1 + y2)
+ 2w1(w2x1x2 + w2y1y2 + 2w3x2z1) + 4w2w3x1z2 −2(y1 + y2)z1z2
(20)

216
V. Irtegov and T. Titorenko
+ 2(x2z2
1 + x1z2
2) + (λ2 −w2
3)(x1x2 −y1y2) + 2(λ2 + w2
3)z1z2
+ 4λ(w2y1z1 + w1y2z2 + w3z1z2) = ˜c3,
respectively.
Let ˆK be integral K (10) in variables (18). We write down the stationary
conditions for the integral ˆK
∂ˆK
∂w1
= 0, ∂ˆK
∂w2
= 0, ∂ˆK
∂w3
= 0, ∂ˆK
∂xi
= 0, ∂ˆK
∂yi
= 0, ∂ˆK
∂zi
= 0 (i = 1, 2)
which represent a system of nonhomogeneous cubic equations with respect to the
variables w1, w2, w3, x1, x2, y1, y2, z1, z2 with the parameters λ, λi (i = 0, . . . , 5),
and then, for the polynomials of this system, construct a lexicographical Gr¨obner
basis with respect to λ0, λ1, λ2, λ3, λ4, y1, y2. A result will be a system which is
split up into two subsystems.
The 1st subsystem:
fi(w1, w2, w3, x1, x2, z1, z2, λ0, λ1, λ2, λ3, λ4, λ) = 0(i = 1, . . . , 5)
(21)
w2 (λ −w3)(w2x1 + w1y2) + (λ + w3)(λw2 −w2w3 −z2)z1 −w1x2z1
−w2x1z2 = 0,
w1 (λ −w3)(w1x2 + w2y1) + (λ + w3)(λw1 −w1w3 −z1)z2 −w1x2z1
−w2x1z2 = 0.
(22)
The 2nd subsystem:
gi(w1, w2, w3, x1, x2, z1, z2, λ0, λ1, λ2, λ3, λ4, λ) = 0(i = 1, . . . , 5)
(23)
λy2(w1w2 + λw3)(w1x2 + λz2) −w1(w2
2 + x2) [(w2x1 + λz1)z2
+ (w1x2 + λz2)z1 −w3x1x2] −λx1(w2w3 + z2)(w2z2 −w3x2)
+ λ2z1(w2
3x2 −2w2w3z2 −z2
2) = 0,
λy1(w1w2 + λw3)(w2x1 + λz1) −w2(w2
1 + x1) [(w2x1 + λz1)z2
+ z1(w1x2 + λz2) −w3x1x2] −λx2(w1w3 + z1)(w1z1 −w3x1)
+ λ2z2(w2
3x1 −2w1w3z1 −z2
1) = 0.
(24)
Here fi = 0, gi = 0 (i = 1, . . . , 5) are the linear equations with respect to
λ0, λ1, λ2, λ3, λ4.
Equations (22), (24) deﬁne two IMs of codimension 2 of diﬀerential Eq. (19).
Equations (21), (23) enable us to obtain the ﬁrst integrals of vector ﬁelds on
these IMs. For this purpose, it is necessary to resolve the equations with respect
to λ0, λ1, λ2, λ3, λ4.
In the initial variables, Eq. (22) can be written as:
F + i G = 0, F −i G = 0, where
F = −b (b δ3 + qr)σ2 + x0[2b (p (γ2δ3 −γ3δ2) + q (γ3δ1 −γ1δ3)) + p rσ1]
−x2
0γ3σ1 + λ2(bqδ3 −x0pγ3) + λ [b (2q (pδ1 + qδ2) −bδ2
3)
−x0 (x0γ2
3 + 2p (pγ1 + qγ2))],
G = −r (bp σ2 + x0q σ1) + 2λ [x0q(pγ1 + qγ2) + bp (pδ1 + qδ2)]
+λ2 (bpδ3 + x0qγ3).
Here σ1 = 2pγ1 + 2qγ2 + rγ3,σ2 = 2pδ1 + 2qδ2 + rδ3.

On Stationary Motions of the Generalized Kowalewski Gyrostat
217
It is not diﬃcult to verify that the equations F = 0, G = 0 deﬁne IM of
codimension 2 of the equations of motion (1). In order to represent the IM
equations in more compact form, we have constructed a lexicographical basis for
the polynomials of left-hand sides of these equations, e.g., with respect to the
variables γ2, δ1. As a result, we have:
2b p (γ2δ3 −γ3δ2) −(p (λ −r) + x0γ3) [2 (pγ1 + qγ2) + (λ + r)γ3] = 0,
2x0 q (γ1δ3 −γ3δ1) + (q (r −λ) + bδ3) [2 (p δ1 + qδ2) + δ3 (λ + r)] = 0.
(25)
After analogous transformations of Eq. (24), we obtain:
(p (p2 + q2) + b (δ1q −δ2p) −x0(γ1p + γ2q)) σ + λ (ϱ0λ2
+ϱ1λ + ϱ2) = 0,
(q (p2 + q2) + b (δ1p + δ2q) −x0 (pγ2 −qγ1)) σ −λ (¯ϱ0λ2
+¯ϱ1λ + ¯ϱ2) = 0.
(26)
Here σ, ϱj, ¯ϱj are the expressions of p, q, r, γi, δi. Their full form is given in the
Appendix.
Equation (26) deﬁne IM of codimension 2 of the equations of motion (1) that
is veriﬁed by IM deﬁnition.
Integral V4 (2) does not turn into any constant on IM (25). Hence, Eq. (25)
together with this integral can be considered as a family of IMs. Each element
of the given family corresponds to some element of the family of systems (1), (2)
(according to the value of the parameter c2). The above statement is also true
for IM (26).
Now, we resolve Eqs. (3) and (4) with respect to the variables p, q, γi, δi (i =
1, 2, 3), and then substitute the obtained expressions into (25) and (26). The
latter equations turn into identities. Hence, IMs (3), (4) are submanifolds of both
IM (25) and IM (26), i.e., they belong to their intersection. As the integral V4 is
identically equal to 1 (or −1) on IMs (3), (4), then IMs (25), (26) have to also
satisfy this condition. In other words, the result under discussion corresponds to
the cases c2 = ±1 of the initial problem.
Analogously, one can show that IM (13) belongs to an intersection of IM (25)
and IM (26). As the integral V4 takes the form ±[b (1 −δ2
1)1/2±(x2
0 −b2δ2
1)1/2] δ1
x−1
0
= c2 on IM (13), then IMs (25), (26) have to also satisfy the latter condition.
5
On Stability of the Stationary Solutions
In this section, we investigate the stability of the above found stationary solu-
tions by the 2nd Lyapunov method [11] and on the base of the Lyapunov stability
theorem for linear approximation [12]. The 2nd Lyapunov method requires con-
structing a function (Lyapunov’s function) possessing special properties. There is
no method for obtaining such function, besides some approaches. We construct
a Lyapunov’s function from the ﬁrst integrals of the problem. Here, e.g., the
following problem arises. To ﬁnd a combination of the integrals which provides
“softest” suﬃcient stability conditions, i.e. these conditions are close to neces-
sary ones. For this purpose, it is necessary to make a series of computational
experiments. In this case, computer algebra tools provide essential help.

218
V. Irtegov and T. Titorenko
5.1
On Stability of the Permanent Rotations
Let us investigate the stability for the elements of the 1st family of permanent
rotations (6)
p = −x0 α1 r1/2
0
(λ −r0)−1/2, q = −b α1 r1/2
0
(λ −r0)−1/2, r = r0,
γ1 = −x0 α2
1, γ2 = −b α2
1, γ3 = [r0 (λ −r0)]1/2α1, δ1 = x0 α2
1,
δ2 = b α2
1, δ3 = −[r0 (λ −r0)]1/2α1
(27)
by the 2nd Lyapunov method. From now and further, the denotations of the
Subsect. 3.1 are used.
For obtaining a Lyapunov function, we shall use integral K1 (12) at the
following constraints on the parameters: λ0 = (λρ2
1λ1)/(2(λ −r0)), λ4 = 0.
Under these conditions, the integral takes the form:
2F1 = λ (ρ2
1H + 2(x2
0V2 + b2V3)
λ −r0
−V1.
Introduce
y1 = δ1 −x0 α2
1, y2 = δ2 −b α2
1, y3 = δ3 + [r0 (λ −r0)]1/2α1,
y4 = γ1 + x0 α2
1, y5 = γ2 + b α2
1, y6 = γ3 −[r0 (λ −r0)]1/2α1,
y7 = p + x0 α1 r1/2
0
(λ −r0)−1/2, y8 = q + b α1 r1/2
0
(λ −r0)−1/2,
the deviations from the elements of family (27).
In the above deviations, the 2nd variation of the integral F1 in the neigh-
bourhood of the elements of the family under study on the linear manifold
δH = (λ −r0)1/2 (x0y4 −by2) −2 r1/2
0
α1 (x0y7 + by8) = 0,
δV3 = 21/2ρ1 (b2 + x2
0) [α1 (x0y1 + by2) −[r0 (λ −r0)]1/2y3] = 0
can be written as:
δ2F1 = a11y2
1 + a12y1y2 + a22y2
2 + (1 −λ r−1
0 ) x2
0y1y4 + a24y2y4 + a44y2
4
+bx0y1y5 + x2
0y2y5 −x3
0 b−1y4y5 + (λ + r0) x2
0 [2(λ −r0)]−1y2
5
+λx2
0(λ −r0)−1y2
6 + a17y1y7 + a27y2y7 + a47y4y7 + a57y5y7
+2λx0y6y7 + a77y2
7.
(28)
Here the coeﬃcients aij depend on the parameters b, x0, r0, λ (see Appendix).
The conditions for the quadratic form δ2F1 to be positive deﬁnite are suﬃ-
cient for the stability of the elements of family (27). In the form of the Sylvester
inequalities, they are:
λD > 0, λ (λ + r0)D2 > 0, λD2
2 b2

λ2 x2
0 (λ2 −r2
0) + 2σ1

> 0,
λ2D3
b2 r0

(λ −r0) [b2λ (λ −r0)2 −2r0 x2
0 (λ −2r0)(λ + r0)] + 4α1r0σ2

> 0,

On Stationary Motions of the Generalized Kowalewski Gyrostat
219
λ3D5
r0 x2
0

(λ −2 r0) (λ −r0) [α2
1 (b2 (λ −2 r0) −r0 x2
0) −r2
0 (λ −r0)] + α4
1σ2

> 0,
−b2 λ4D6
r0 x4
0
(λ −2r0) [2 r0 (λ −r0) + ρ2
1]2 > 0,
(29)
where D = x2
0 (λ −r0)−1, σ1 = λ (b2 + x2
0)2 + 4b2r0x2
0,
σ2 = λ (b2 −x2
0)2 −2x4
0 (λ −r0) + 4b2r0x2
0.
The solutions under study are real when λ < 0 and λ < r0 ≤0 or λ > 0 and
0 ≤r0 < λ. With these conditions we ﬁnd that inequalities (29) are compatible
at the following constraints on the parameters b, x0, r0, λ:
b ̸= 0, x0 ̸= 0 and ((λ < 0, 2λ < r0 < λ) or (λ > 0, λ < r0 < 2λ)).
These isolate some subfamily from the family of solutions (27), the elements of
which are stable.
The above conditions are also suﬃcient for the stability of the elements of
the 2nd family of solutions (6) when the same integral (the integral F1) is used
to construct a Lyapunov function. In a similar manner, we have analyzed the
stability for the elements of the families of permanent rotations (7)–(9).
Using the integrals V2 + V3 ± 2V4 for obtaining Lyapunov functions, it is
possible to investigate the stability of IMs (3), (4) which the families of solutions
(6)–(9) belong to.
For the equations of perturbed motion, the variation of the integral F =
V2 + V3 + 2V4 in the neighbourhood of IM (3) is:
2ΔF = (y1 + y4)2 + (y2 + y5)2 + (y3 + y6)2,
where y1 = δ1 −x0 ¯α2
1, y2 = δ2 −b ¯α2
1, y3 = δ3 + [r (λ −r)]1/2 ¯α1, y4 = γ1 +
x0 ¯α2
1, y5 = γ2 + b ¯α2
1, y6 = γ3 −[r (λ −r)]1/2 ¯α1, y7 = p + x0 ¯α1 r1/2 (λ −
r)−1/2, y8 = q +b ¯α1 r1/2 (λ−r)−1/2 are the deviations from the IM under study,
¯α1 = ¯ρ1 β,
¯ρ1 = [r (r −λ) + (4(b2 + x2
0) + (λ −r)2r2)1/2]1/2.
Since the quadratic form ΔF is sign-deﬁnite for the variables appearing in
it, then IM (3) is stable with respect to the variables δ1 + γ1, δ2 + γ2, δ3 + γ3.
The latter means that the IM keeps stability when the directions of the force
ﬁelds are perturbed. Analogously, the stability of IM (4) with respect to part of
the variables is proved.
5.2
On Stability of the Equilibria
Now, we analyze the stability for the elements of the 1st family of equilibria (15)
p = 0, q = 0, r = 0, γ1 = −(x2
0 −b2δ02
1 )1/2x0
−1, γ2 = −b δ0
1x−1
0 ,
γ3 = 0, δ1 = δ0
1, δ2 = (1 −δ02
1 )1/2, δ3 = 0
(30)
by the 2nd Lyapunov method.

220
V. Irtegov and T. Titorenko
For constructing a Lyapunov function, we use integral K2 (17) at the follow-
ing constraints: λ2 = −2λ1x2
0, λ4 = 0. Under these conditions, the integral is:
2F2 = 2 [zH + x2
0V2 + b2V3] −V1.
From now and further, the denotations of the Subsect. 3.2 are used.
For the equations of perturbed motion, the 2nd variation of the integral F2
in the neighbourhood of the elements of the family under study on the linear
manifold
δH = x0y4 −by2 = 0, δV1 = 2 [2b δ0
1 (by1 −x0y5) + (by2 + x0y4) ¯z] = 0,
δV2 = −2 [(x2
0 −b2δ0
1
2)1/2 y4 + b δ0
1y5] = 0, δV3 = 2 [δ0
1y1 + (1 −δ0
1
2)1/2y2] = 0
can be written as: δ2F2 = Q1 + Q2, where
Q1 = b2y2
3 + x2
0y2
6 + 2λ(x0y6y7 −by3y8) + [λ2 + 2b(1 −δ02
1 )1/2] y2
7
−4b δ0
1y7y8 + [λ2 + 2(x2
0 −b2δ02
1 )1/2] y2
8, 2Q2 = z2y2
2
δ0
1
2 + zy2
9.
Here yi (i = 1, . . . , 9) are the deviations from the elements of family (30).
The conditions for the quadratic form δ2F2 to be positive deﬁnite
Δ1 = b2 > 0, Δ2 = b3x2
0 > 0, Δ3 = b3x2
0 (1 −δ0
1
2)1/2 > 0,
Δ4 = b3x2
0 [(1 −δ0
1
2)1/2(x2
0 −b2δ0
1
2)1/2 −b δ0
1
2] > 0, z > 0 and δ0
1 ̸= 0 (31)
are suﬃcient for the stability of the elements of the family under consideration.
Taking into account the conditions for solution (30) to be real, we ﬁnd that
inequalities (31) are compatible when b > 0, x0 ̸= 0 and ((−σ < δ0
1 < 0 or
0 < δ0
1 < σ)). Here σ = |x0| (b2 + x2
0)−1/2.
As mentioned before, the integral V4 takes the form −[b (1 −δ2
1)1/2 + (x2
0−
b2δ2
1)1/2] δ1 x−1
0
= c2 on solutions (30). This relation together with the above
constraints sets the boundaries of varying the angles between the directions of
the force ﬁelds, within of which the elements of family (30) are stable. Similar
stability conditions have been obtained for the elements of the 1st family of
solutions (16).
For the rest of the families of the equilibria, we have derived the conditions
of their instability on the base of the Lyapunov stability theorem for linear
approximation.
Consider the 2nd family of solutions (15). We introduce the deviations
y1 = δ1 −δ0
1, y2 = δ2 −(1 −δ0
1
2)1/2, y3 = δ3, y4 = γ1 −(x2
0 −b2δ0
1
2)1/2x−1
0 ,
y5 = γ2 + bδ0
1x−1
0 , y6 = γ3, y7 = p, y8 = q, y9 = r

On Stationary Motions of the Generalized Kowalewski Gyrostat
221
from the elements of the family under study, and write down the equations of
1st approximation in the neighbourhood of the elements of the family:
˙y1 = (1 −δ0
1
2)1/2 y9, ˙y2 = −δ0
1y9, ˙y3 = δ0
1y8 −(1 −δ0
1
2)1/2 y7,
˙y4 = −b δ0
1 x−1
0 y9, ˙y5 = −(x2
0 −b2δ0
1
2)1/2 x−1
0 y9,
˙y6 = [b δ0
1y7 + (x2
0 −b2δ0
1
2)1/2 y9] x−1
0 , 2 ˙y7 = b y3 −λy8,
2 ˙y8 = x0y6 + λy7, ˙y9 = −(by1 + x0y5).
(32)
The characteristic equation of system (32) has the form:
x3 [4x6 + x4(6¯z + λ2) + 1
2x2 [(5¯z + 2λ2) ¯z −(b2 + x2
0)] −(2 b2 δ02
1 −x2
0) ¯z
−(b2 −x2
0) (x2
0 −b2δ02
1 )1/2] = 0.
(33)
It is split up into two equations:
x3 = 0,
4ζ6 + ζ4(6¯z + λ2) + 1
2ζ2 [(5¯z + 2λ2) ¯z −(b2 + x2
0)] −(2 b2 δ02
1 −x2
0) ¯z
−(b2 −x2
0) (x2
0 −b2δ02
1 )1/2 = 0,
(34)
where ζ = x2.
Consider the free term of the latter equation:
R = −(2 b2 δ02
1 −x2
0) ¯z −(b2 −x2
0) (x2
0 −b2δ02
1 )1/2.
Taking into consideration the conditions for the solutions under study to be
real, we ﬁnd that R > 0 when 0 < b < |x0|and |δ0
1| ≤1. The latter means
that Eq. (34) (and also (33)) has no less than one positive root. So, the elements
of the family under consideration are unstable over the above range of varying
the parameter δ0
1. Similar conditions of instability have been obtained for the
elements of the 2nd family of solutions (16).
The above algorithms for the stability analysis of stationary solutions by
the Lyapunov methods have been encoded in “Mathematica” as the software
package mentioned before. This package is intended for the qualitative analy-
sis of phase spaces of dynamical systems having ﬁrst integrals. The package
contains programs for the stability analysis of stationary solutions on the base
of Lyapunov’s linear stability theorems and the 2nd Lyapunov method. In the
considered stability problems, using a solution under study and a combination
of the ﬁrst integrals as input data, the package returns the conditions for the
sign-deﬁniteness of the quadratic form (Sylvester’s inequalities). The resulting
inequalities are analyzed with the aid of the corresponding computer algebra
tools. In a similar manner, the package is employed for the stability analysis
of stationary solutions on the base of Lyapunov’s stability theorems for linear
approximation.

222
V. Irtegov and T. Titorenko
6
Conclusion
With the aid of computer algebra methods and the software package developed
on the base of “Mathematica” CAS, the analysis of the stationary motions of
the Kowalewski gyrostat in two force ﬁelds has been performed. Such motions
have been found directly from the equations of motion by Gr¨obner basis method.
These represent the families of permanent rotations and equilibria. It has been
revealed that the elements of these families correspond to both the parallel force
ﬁelds and to the ﬁelds of special directions. It has been also shown that all the
found solutions belong to an intersection of two IMs of codimension 2 of the
equations of motion.
The stability of the stationary solutions has been analyzed on the base of
Lyapunov’s stability theorems. For the elements of the families of permanent
rotations, the suﬃcient conditions of their stability have been obtained. For
the elements of the families of equilibria, both the suﬃcient conditions of their
stability, and the conditions of their instability have been derived.
The obtained results as well as the approaches and methods which were
applied in this paper, can be used in the analysis of similar problems, in partic-
ular, at the stage of preliminary design of satellite systems.
Acknowledgements. This work was supported by the RFBR (Project 16-07-00201a)
and the Program for the Leading Scientiﬁc Schools of the Russian Federation (NSh-
8081.2016.9).
7
Appendix
The coeﬃcients of Eq. (26):
σ = b2 [(δ2
1 + δ2
2) r −2δ3 (p δ1 + qδ2)] + 2b x0 [p (δ3γ2 −δ2γ3) + q (δ1γ3 −δ3γ1)
+ r (δ2γ1 −δ1γ2)] + x2
0 [(γ2
1 + γ2
2) r −2γ3 (pγ1 + qγ2)],
ϱ0 = r (b2 δ1δ3 + b x0 (δ3γ2 −δ2γ3) + x2
0γ1γ3),
ϱ1 = b2 [p r (δ2
1 −δ2
2 −δ2
3) + δ1(δ3 (p2 + q2 + r2) + 2q r δ2) −δ2
3(p r −x0γ3)]
+ b x0 (p2 + q2 −r2)(δ3γ2 −δ2γ3) + x2
0 [p r (γ2
1 −γ2
2 −γ2
3) + γ1 (γ3 (p2 + q2
+ r2) + 2q rγ2) −γ2
3(p r −x0γ3)],
ϱ2 = b3δ2
3 (δ2p −δ1q) + b2 [(p2 + q2) [p (δ2
1 −δ2
2) + 2 (qδ1δ2 −p δ2
3)]
+ δ1δ3r (q2 −p2) + p r (r (δ2
1 + δ2
2) −2δ2δ3q)] + b2x0 [ δ3 (2γ3(δ1p + δ2q)
+ δ3 (pγ1 + qγ2)) −rγ3(δ2
1 + δ2
2)] + b x0 r [(p2 −q2) (δ3γ2 −δ2 γ3)
+ 2p [γ1 (r δ2 −q δ3) + δ1 (q γ3 −r γ2)] ] + b x2
0 γ3 [ 3γ3 (pδ2 −qδ1)
−2[δ3 (p γ2 −q γ1) + r (γ1 δ2 −γ2 δ1)] ]
+ x2
0 [(p2 + q2) [p (γ2
1 −γ2
2 −γ2
3) + 2q γ1γ2] + r γ3 (γ1 (q2 −p2) −2p qγ2)
+ p r2 ((γ2
1 + γ2
2) −γ2
3(p2 + q2))] + x3
0 r γ3 [ 3γ3(pγ1 + qγ2) −(γ2
1 + γ2
2)],
¯ϱ0 = −r(b2 δ2δ3 + b x0 (δ1γ3 −δ3γ1) + x2
0γ2γ3),
¯ϱ1 = b2 [q r (δ2
1 −δ2
2 + δ2
3) −δ2 (δ3 (p2 + q2 + r2) + 2p r δ1) + δ2
3 (q r + b δ3)]
+ b x0 (p2 + q2 −r2)(δ3γ1 −δ1γ3) + x2
0 [q r (γ2
1 −γ2
2 + γ2
3) −γ2 (γ3 (p2 + q2
+ r2) + 2p r γ1) + γ2
3 (q r + b δ3)],
¯ϱ2 = b3 δ3 [ 3 δ3 (p δ1 + q δ2) −r (δ2
1 + δ2
2)] + b2 [(p2 + q2) [ q (δ2
1 −δ2
2 + δ2
3)

On Stationary Motions of the Generalized Kowalewski Gyrostat
223
−2p δ1δ2] + r δ3 [δ2 (q2 −p2) + 2p q δ1] −q [r2 (δ2
1 + δ2
2) −δ2
3 (p2 + q2)] ]
+ b2x0 δ3 [2γ3 (p δ2 −q δ1) + 3 δ3 (2r (δ1γ2 −δ2γ1) −(p γ2 −q γ1))]
+ b x0 r [(p2 −q2) (δ1γ3 −δ3γ1) + 2q (γ2 (r δ1 −p δ3) + δ2 (p γ3 −r γ1))]
+ b x2
0 [ γ3 [γ3 (p δ1 + q δ2) + 2δ3 (p γ1 + q γ2)] −r δ3 (γ2
1 + γ2
2)]
+ x2
0 [ (p2 + q2) [q (γ2
1 −γ2
2 + γ2
3) −2p γ1γ2] + r γ3 [γ2 (q2 −p2) + 2p q γ1]
−q [r2 (γ2
1 + γ2
2) −γ2
3 (p2 + q2)] ] + x3
0γ2
3(qγ1 −pγ2).
The coeﬃcients of quadratic form (28):
a11 =
b2
2(λ −r0)

λ + r0 +
2λx2
0α2
1
r0 (λ −r0)

, a22 =
b2
r0 (λ −r0)

(λ −r0)2
+r2
0 + b2α2
1 λ
λ −r0

+ (λ −r0)
2r0

x2
0 + 2−1λ (λ −r0) α−2
1

,
a44 = x2
0

λ
λ −r0
+ λ −r0
2 b2r0

x2
0 + 2−1λ (λ −r0) α−2
1

,
a77 = b2 + x2
0
b2
[λ (λ −r0) + ρ2
1], a12 = λ b x0
r0
 2−1b2 α2
1
(λ −r0)2 + r0
λ

−1

,
a17 = 21/2α1 [b2r0 + x2
0(λ −r0)] [r0 (λ −r0)]−1/2,
a24 = −2x0
b r0 α2
1
[λ (λ −r0)2 + 2α2
1 (b2λ + x2
0 (λ −r0))],
a27 = x0 [λ (λ −r0)2 + 2α2
1 (b2 (2λ −3r0) + x2
0 (λ −r0))] (b α1)−1
×[r0 (λ −r0)]−1/2,
a47 = −x2
0 [λ (λ −r0)2 + 2α2
1 (b2 (λ + r0) + x2
0 (λ −r0))] (b2α1)−1
×[r0 (λ −r0)]−1/2, a57 = 2 r1/2
0
x0 α1 (x2
0 −b2) b−1(λ −r0)−1/2.
References
1. Kowalewski, S.: Sur le probleme de la rotation d’un corps solide autour d’un point
ﬁxe. Acta Math. 12, 177–232 (1889)
2. Sarychev, V.A., Gutnik, S.A.: Dynamics of a satellite subject to gravitational and
aerodynamic torques. Investigation of equilibrium positions. Cosm. Res. 53(6),
449–457 (2015)
3. Adler, V.E., Marikhin, V.G., Shabat, A.B.: Quantum tops as examples of com-
muting diﬀerential operators. Theoret. Math. Phys. 3(172), 1187–1205 (2012)
4. Bobenko, A.I., Reyman, A.G., Semenov-Tian-Shansky, M.A.: The Kowalewski top
99 years later: a lax pair, generalizations and explicit solutions. Commun. Math.
Phys. 122, 321–354 (1989)
5. Kharlamov, M.P.: Critical subsystems of the Kowalevski gyrostat in two constant
ﬁelds. J. Nonlin. Dyn. 3(3), 331–348 (2007)
6. Kharlamov, M.P., Ryabov, P.E., Savushkin, A.Y., Smirnov, G.E.: Types of critical
points of the Kowalevski gyrostat in double ﬁeld. NAS of Ukraine. Mech. Solids
41, 26–37 (2011)
7. Banshchikov, A.V., Burlakova, L.A., Irtegov, V.D., Titorenko, T.N.: Software pack-
age for ﬁnding and stability analysis of stationary sets. Certiﬁcate of State Regis-
tration of Software Programs. FGU-FIPS. No. 2011615235 (2011)

224
V. Irtegov and T. Titorenko
8. Komarov, I.V.: A generalization of the Kovalevskaya top. Phys. Lett. A 1(123),
14–15 (1997)
9. Irtegov, V.D., Titorenko, T.N.: On one approach to investigation of mechanical
systems. The institute of mathematics of NAS of Ukraine. Electron. J. Symmetry
Integr. Geom.: Methods Appl. 2, 049 (2006)
10. Irtegov, V., Titorenko, T.: Qualitative analysis of the Reyman – Semenov–Tian–
Shansky integrable case of the generalized Kowalewski top. In: Gerdt, V.P., Koepf,
W., Seiler, W.M., Vorozhtsov, E.V. (eds.) CASC 2016. LNCS, vol. 9890, pp. 289–
304. Springer, Cham (2016). doi:10.1007/978-3-319-45641-6 19
11. Lyapunov, A.M.: On permanent helical motions of a rigid body in ﬂuid. Collected
works. USSR Acad. Sci. Moscow-Leningrad 1, 276–319 (1954)
12. Lyapunov, A.M.: The general problem of motion stability. Collected works. USSR
Acad. Sci. Moscow-Leningrad 2, 7–263 (1956)

Computing the Integer Points
of a Polyhedron, I: Algorithm
Rui-Juan Jing1,2(B) and Marc Moreno Maza2
1 KLMM, UCAS, Academy of Mathematics and Systems Science,
Chinese Academy of Sciences, Beijing, China
2 University of Western Ontario, London, Canada
rjing8@uwo.ca, moreno@csd.uwo.ca
Abstract. Let K be a polyhedron in Rd, given by a system of m linear
inequalities, with rational number coeﬃcients bounded over in absolute
value by L. In this series of two papers, we propose an algorithm for
computing an irredundant representation of the integer points of K, in
terms of “simpler” polyhedra, each of them having at least one integer
point. Using the terminology of W. Pugh: for any such polyhedron P,
no integer point of its grey shadow extends to an integer point of P. We
show that, under mild assumptions, our algorithm runs in exponential
time w.r.t. d and in polynomial w.r.t m and L. We report on a software
experimentation. In this series of two papers, the ﬁrst one presents our
algorithm and the second one discusses our complexity estimates.
1
Introduction
The integer points of polyhedral sets are of interest in many areas of mathe-
matical sciences, see for instance the landmark textbooks of Schrijver [19] and
Barvinok [3], as well as the compilation of articles [4]. One of these areas is the
analysis and transformation of computer programs. For instance, integer pro-
gramming [7] is used by Feautrier in the scheduling of for-loop nests [8] and
Barvinok’s algorithm [2] for counting integer points in polyhedra is adapted by
K¨oppe and Verdoolaege in [16] to answer questions like how many memory loca-
tions are touched by a for-loop nest. In [17], Pugh proposes an algorithm, called
the Omega Test, for testing whether a polyhedron has integer points. In the
same paper, Pugh shows how to use the Omega Test for performing dependence
analysis [17] in for-loop nests. Then, in [18], he uses the Omega Test for deciding
Presburger arithmetic formulas.
In [18], Pugh also suggests, without stating a formal algorithm, that the
Omega Test could be used for quantiﬁer elimination on Presburger formulas.
This observation is a ﬁrst motivation for the work presented in this series of
two papers: we adapt the Omega Test so as to describe the integer points of a
polyhedron via a projection scheme, thus performing elimination of existential
quantiﬁers on Presburger formulas. Projections of polyhedra and parametric
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 225–241, 2017.
DOI: 10.1007/978-3-319-66320-3 17

226
R.-J. Jing and M. Moreno Maza
programming are tightly related problems, see [13]. Since the latter is essential
to the parallelization of for-loop nests [7], which is of interest to the authors [5],
we had here a second motivation for developing the proposed algorithm.
In [9], Fischer and Rabin show that any algorithm for deciding Presburger
arithmetic formulas has a worst case running time which is doubly exponential
in the length of the input formula. However, this worst case scenario is based
on a formula alternating existential and universal quantiﬁers. Meanwhile, in
practice, the original Omega Test (for testing whether a polyhedron has integer
points) can solve “diﬃcult problems” as shown by Pugh in [18] and others, e.g.
Wonnacott in [22]. This observation brings our third motivation: determining
realistic assumptions under which our algorithm, based on the Omega Test,
could run in a single exponential time.
Our algorithm takes as input a system of linear inequalities Ax ≤b where
A is a matrix over Z with m rows and d columns, x is the unknown vector
and b is a vector of m coeﬃcients in Z. The points x ∈Rd satisfying Ax ≤b
form a polyhedron K and our algorithm decomposes its integer points (that is,
K ∩Zd) into a disjoint union (K1 ∩Zd1) ⊍⋯⊍(Ke ∩Zde), where K1,...,Ke are
“simpler” polyhedra such that Ki ∩Zd ≠∅holds and di is the dimentions of Ki,
for 1 ≤i ≤e. To use the terminology introduced by W. Pugh for the Omega test,
no integer point of the grey shadow of any polyhedron Ki extends to an integer
point of Ki. As a consequence, applying our algorithm to Ki would return Ki
itself, for 1 ≤i ≤e. Let us present the key principles and features of our algorithm
through an example. Consider the polyhedron K of R4 given below:
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
2x + 3y −4z + 3w ≤1
−2x −3y + 4z −3w ≤−1
−13x −18y + 24z −20w ≤−1
−26x −40y + 54z −39w ≤0
−24x −38y + 49z −31w ≤5
54x + 81y −109z + 81w ≤2
.
A ﬁrst procedure, called IntegerNormalize, detects implicit equations and
solves them using techniques based on Hermite normal form, see Sects. 3
and 4.1. In our example 2x + 3y −4z + 3w = 1 is an implicit equation and
IntegerNormalize(Ax ≤b) returns a triple (t,x = Pt + q, Mt ≤v) where t is
a new unknown vector, the linear system x = Pt+q gives the general form of an
integer solution of the implicit equation(s) and Mt ≤v is obtained by substitut-
ing x = Pt + q into Ax ≤b. In our example, the systems x = Pt + q and Mt ≤v
are given by:
⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩
x = −3t1 + 2t2 −3t3 + 2
y = 2t1 + t3 −1
z = t2
w = t3
and
⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩
3t1 −2t2 + t3 ≤7
−2t1 + 2t2 −t3 ≤12
−4t1 + t2 + 3t3 ≤15
−t2 ≤−25
.

Computing the Integer Points of a Polyhedron, I: Algorithm
227
A second procedure, called DarkShadow, takes Mt ≤v as input and returns a
couple (t′,Θ) where t′ stands for all t-variables except t1, and Θ is a linear
system in the t′-variables such that any integer point solving Θ extends to an
integer point solving Mt ≤v. In our example, t′ = {t2,t3} and Θ is given by:
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
2t2 −t3 ≤48
−5t2 + 13t3 ≤67
−t2 ≤−25
.
The polyhedron D of R2 deﬁned by Θ, and the inequalities of Mt ≤v
not involving t1, is called the dark shadow of the polyhedron deﬁned by Mt ≤v.
Fig. 1. The real, the dark and the grey shadows of a polyhedron.
On the left-hand side of Fig. 1, one can see the polyhedron deﬁned in R3 by
Mt ≤v together with its dark shadow D (shown in dark grey) as well as its
projection on the (t2,t3)-plane, denoted by R and called real shadow by W. Pugh.
The right-hand side of Fig. 1 gives a planar view of D and R. As we will see in
Sect. 4.4, if M′t′ ≤v′ is the linear system generated by applying Fourier-Motzkin
elimination (without removing redundant inequalities) to Mt ≤v (in order to
eliminate t1) then Θ is given by a linear system of the form M′t′ ≤w′. This
explains why, on the right-hand side of Fig. 1, each facet of the dark shadow D is
parallel to a facet of the real shadow R. While this property is observed on almost
all practical problems, in particular in the area of analysis and transformation
of computer programs, it is possible to build examples where this property does
not hold. We have examples in Sect. 5 of the second paper.
On the right-hand side of Fig. 1, one observes that the region R ∖D, called
grey shadow, contains integer points. Some of them, like (t2,t3) = (29,9), do not
extend to an integer solution of Mt ≤v. Indeed, plugging (t2,t3) = (29,9) into
Mt ≤v yields 37
2 ≤t1 ≤56
3 , which has no integer solutions. However, other integer
points of R∖D may extend to integer solutions of Mt ≤v. In order to determine
them, a third procedure, called GreyShadow, considers in turn the negation of
each inequality θ of Θ. However, for each θ of Θ, instead of simply making a
recursive call to the entire algorithm applied to Mt ≤v ∪{θ}, simpliﬁcations
(involving θ and the inequalities from which θ is derived) permit to replace this

228
R.-J. Jing and M. Moreno Maza
recursive call by several ones in lower dimension, thus guaranteeing termination
of the whole algorithm. Details are given in Sects. 4.5 and 4.6.
Returning to our example, the negation of the inequality 2t2 −t3 ≤48 from
Θ, combined with the system Mt ≤v, yields the following
⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩
−2t1 + 2t2 −t3 = 12
3t1 −2t2 + t3 ≤7
−4t1 + t2 + 3t3 ≤15
−t2 ≤−25
,
which, by means of IntegerNormalize, rewrites to:
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
t1 = t4
t2 = t5 + 1
t3 = −2t4 + 2t5 + 1
,
and
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
t4 ≤8
−10t4 + 7t5 ≤11
−t5 ≤−24
,
where t4,t5 are new variables. Continuing in this manner with the GreyShadow
procedure, a decomposition of the integer points of Mt ≤v is given by:
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
3t1 −2t2 + t3 ≤7
−2t1 + 2t2 −t3 ≤12
−4t1 + t2 + 3t3 ≤15
2t2 −t3 ≤48
−5t2 + 13t3 ≤67
−t2 ≤−25
2 ≤t3 ≤17
,
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
t1 = 15
t2 = 27
t3 = 16
,
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
t1 = 18
t2 = 33
t3 = 18
,
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
t1 = 14
t2 = 25
t3 = 15
,
⎧⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎩
t1 = 19
t2 = 50 + t6
t3 = 50 + 2t6
−25 ≤t6 ≤−16.
.
Denoting these 5 systems respectively by S1,...,S5, the integer points of K are
ﬁnally given by the union of the integer points of the systems x = Pt+q ∪Si, for
1 ≤i ≤5. The systems S2,...,S5 look simple enough to be considered as solution
sets. What about S1? The system S1, as well as S2,...,S5, satisﬁes a “back-
substitution” property which is similar to that of a regular chain in the theory
of polynomial system solving [1]. This property (formally stated in Sect. 4.2),
when applied to S1, says that for all 2 ≤i ≤3, every integer point of R4−i solving
all the inequalities of S1 involving ti,...,t3 only, extends to an integer point of
R5−i solving all the inequalities of S1 involving ti−1,...,t3.
With respect to the original Omega Test [17], our contributions are as follows.
1. We turn the decision procedure of the Omega Test into an algorithm decom-
posing all the integer points of a polyhedron.
2. Our decomposition is disjoint whereas the recursive calls in the original
Omega Test may search for integer points in intersecting polyhedral regions.
3. The original Omega Test uses an ad-hoc routine for computing the integer
solutions of linear equation systems, while we rely on Hermite normal form
for this task. Consequently, we deduce complexity estimates for that task.

Computing the Integer Points of a Polyhedron, I: Algorithm
229
4. We also provide complexity estimates for the procedures GreyShadow and
DarkShadow under realistic assumptions. From there, we derive complexity
estimates for the entire algorithm, whereas no complexity estimates were
known for the original Omega Test.
We report our work in a series of two papers. The present one describes and
proves our algorithm. The second one establishes our complexity estimates.
2
Polyhedral Sets
This section is a review of the theory of polyhedral sets. It is based on the books
of Gr¨unbaum [10] and Schrijver [19], where proofs of the statements below can
be found.
Given a positive integer d, we consider the d-dimensional Euclidean space Rd
equipped with the Euclidean topology. Let K be a subset of Rd. The dimension
dim(K) of K is a −1 where a is the maximum number of aﬃnely independent
points in K. Let a ∈Rd, let b ∈R and denote by H the hyperplane deﬁned by
H = {x ∈Rd ∣aT x = b}. We say that the hyperplane H supports K if either
sup{aT x ∣x ∈K} = b or inf{aT x ∣x ∈K} = b holds, but not both.
From now on, let us assume that K is convex. A set F ⊆K is a face if either
F = ∅or F = K, or if there exists a hyperplane H supporting K such that we
have F = K ∩H. The set of all faces of K is denoted by F(K). We say that
F ∈F(K) is proper if we have F ≠∅or F ≠K. We note that the intersection of
any family of faces of K is itself a face of K.
We say that K is a polyhedral set or a polyhedron if it is the intersection of
ﬁnitely many closed half-spaces of Rd. We say that K is full-dimensional, if we
have dim(K) = d, that is, if the interior of K is not empty. The proper faces of
K that are ⊆-maximal are called facets and those of dimension zero are called
vertices. We observe that every face of K is also a polyhedral set.
Let H1,...,Hm be closed half-spaces such that the intersection ∩i=m
i=1 Hi is
irredundant, that is, ∩i=m
i=1 Hi ≠∩i=m
i=1,j≠i Hi for all 1 ≤j ≤m. We observe that this
intersection is closed and convex. For each i = 1⋯m, let ai ∈Rd and bi ∈R such
that Hi is deﬁned by aT
i x ≤bi. We denote by A the m×d matrix (aT
i ,1 ≤i ≤m)
and by b the vector (b1,...,bd)T .
From now on, we assume that K = ∩i=m
i=1 Hi holds. Such an irredundant
decomposition of a polyhedral set can be computed from an arbitrary inter-
section of ﬁnitely many closed half-spaces, in time polynomial in both d and
m, using linear programming, see Khachian in [15]. The following property is
essential. For every face F of K, there exists a subset I of {1,...,m} such that
F corresponds to the set of solutions to the system of equations and inequalities
aT
i x = bi
for i ∈I,
and
aT
i x ≤bi
for i /∈I.
This latter property has several important consequences. For each i = 1⋯m, the
set Fi = K ∩{aT
i x = bi} is a facet of K and the border of K equals ∪i=m
i=1 Fi. In
particular, each proper face of K is contained in a facet of K. Each facet of a

230
R.-J. Jing and M. Moreno Maza
facet of K is the intersection of two facets of K. Moreover, if the (m×d)-matrix
A has full column rank, then the ⊆-minimal faces are the vertices. The set F(K)
is ﬁnite and has at most 2m elements.
For a ∈Rd and b ∈R, we say that aT x ≤b is an implicit equation in Ax ≤b
if for all x ∈Rd we have
Ax ≤b
⇒
ax = b.
(1)
Following [19], we denote by A= (resp. A+) and b= (resp. b+) the rows of A
and b corresponding to the implicit (resp. non-implicit) equations. The follow-
ing properties are easy to prove. If K is not empty, then there exists x ∈K
satisfying both
A=x = b=
and A+x < b+.
The facets of K are in 1-to-1 correspondence with the inequalities of A+x ≤
b+. In addition, if K is full-dimensional, then A+ = A and b+ = b both hold;
moreover the system of inequalities Ax ≤b is a unique representation of K, up to
multiplication of inequalities by positive scalars.
From now on and in the sequel of this paper, we assume that variables are
ordered as x1 > ⋯> xd. We call initial coeﬃcient, or simply initial, of an inequal-
ity aT
i x ≤bi, for 1 ≤i ≤m, the coeﬃcient of aT
i x in its largest variable. Following
the terminology of Pugh in [17], if v is the largest variable of the inequality
aT
i x ≤bi, we say that this inequality is an upper (resp. lower) bound of v when-
ever the initial c of aT
i x ≤bi is positive (resp. negative); indeed, we have v ≤γ
c
(resp. v ≥γ
c ) where γ = bi −aT
i x −cv.
Canonical Representation. Recall that we assume that none of the inequal-
ities of Ax ≤b is redundant. If K is full-dimensional and if the initial of each
inequality in Ax ≤b is 1 or −1, then we call Ax ≤b the canonical representation of
K w.r.t. the variable ordering x1 > ⋯> xd and we denote it by can(K;x1,...,xd).
We observe that the notion of canonical representation can also be expressed
in a more geometrical and less algebraic way, that is, independently of any coor-
dinate system. Assume again that K is full-dimensional and that the inter-
section ∩i=n
i=1 Hi = K of closed half-spaces H1,...,Hn is irredundant. Since K
is full-dimensional, the supporting hyperplane of each facet of K must be the
frontier of one half-space among H1,...,Hn. Clearly, two (or more) half-spaces
among H1,...,Hn may not have the same frontier without contradicting one of
our hypotheses (K is full-dimensional, ∩i=n
i=1 Hi is irredundant). Therefore, the
half-spaces H1,...,Hn are in one-to-one correspondence with the facets of K.
This implies that there is a unique irredundant intersection of closed half-spaces
equaling K and we denote it by can(K).
Projected Representation. Let again Ax ≤b be the canonical representation
of the polyhedral set K w.r.t. the variable ordering x1 > ⋯> xd. We denote
by Ax1 (resp. A<x1) and bx1 (resp. b<x1) the rows of A and b corresponding
to the inequalities whose largest variable is x1 (resp. less than x1). For each
upper bound cx1 ≤γ of x1 and each lower bound −ax1 ≤−α of x1 (where c > 0,
a > 0, γ ∈R[x2,...,xd] and α ∈R[x2,...,xd] hold), we have a new inequality
cα−aγ ≤0. Augmenting A<x1 with all inequalities obtained in this way, we obtain

Computing the Integer Points of a Polyhedron, I: Algorithm
231
a new linear system which represents a polyhedral set which is the standard
projection of K on the d −1 least coordinates of Rd, namely (x2,...,xd); hence
we denote this latter polyhedral set by Πx2,...,xdK and we call it the real shadow
of K, following the terminology of [17]. The procedure by which Πx2,...,xdK
is computed from K is the well-known Fourier-Motzkin elimination procedure,
see [15]. We call projected representation of K w.r.t. the variable ordering x1 >
⋯> xd and denote by proj(K;x1,...,xd) the linear system given by Ax1x ≤bx1
if d = 1 and, by the conjunction of Ax1x ≤bx1 and proj(Πx2,...,xdK;x2,...,xd),
otherwise.
3
Integer Solutions of Linear Equation Systems
We review how Hermite normal forms [6,19] can be used to represent the integer
solutions of systems of linear equations. Let A = (ai,j) and H = (hi,j) be two
matrices over Z with m rows and d columns, and let b be a vector over Z with
d coeﬃcients. We denote by r the rank of A and by h the maximum bit size of
coeﬃcients in the matrix [A b]. Deﬁnition 1 is taken from [14], see also [12].
Deﬁnition 1. The matrix H is called a column Hermite normal form (abbr.
column HNF) if there exists a strictly increasing map f from [d −r + 1,d] ∩Z to
[1,m] ∩Z satisfying the following properties for all j ∈[d −r + 1,d] ∩Z:
1. for all integer i such that 1 ≤i ≤m and i > f(j) both hold, we have hi,j = 0,
2. for all integer k such that j < k ≤d holds, we have hf(j),j > hf(j),k ≥0,
3. the ﬁrst d −r columns of H are equal to zero.
We say that H is the column Hermite normal form of A if H is a column Hermite
normal form and there exists a uni-modular d × d-matrix U over Z such that we
have H = AU. When those properties hold, we call {f(d −r + 1),...,f(d)} the
pivot row set of A.
Remark 1. The matrix A admits a unique column Hermite normal form. Let H
be this column Hermite normal form and let U be the uni-modular (d×d)-matrix
given in Deﬁnition 1. Let us decompose U as U = [UL,UR] where UL (resp. UR)
consist of the ﬁrst d −r (resp. last r) columns of U. Then we deﬁne HL ∶= AUL
and HR ∶= AUR. We have HL = 0m,d−r, where 0m,d−r is the zero-matrix with
m rows and d −r columns. We observe that UR is a full column-rank matrix.
Moreover, if A is full row-rank, that is, if r = m holds, then HR is non-singular.
Lemma 2 shows how to compute the integer solutions of the system of linear
equations Ax = b when A is full row-rank. In the general case, one can use
Lemma 1 to reduce to the hypothesis of Lemma 2. While the construction of
this latter lemma relies on the HNF, alternative approaches are available. For
instance, one can use the equation elimination procedure of the Omega Test [17],
However, no running-time estimates are known for that procedure.

232
R.-J. Jing and M. Moreno Maza
Notation 1. For I ⊆{1,...,m}, we denote by AI (resp. bI) the sub-matrix
(resp. vector) of A (resp. b) consisting of the rows of A (coeﬃcients of b) with
indices in I.
Lemma 1. Let I be the pivot row set of A, as given in Deﬁnition 1. Assume
that Ax = b admits at least one solution in Rd. Then, for any x ∈Rd, we have
Ax = b
⇐⇒
AIx = bI.
Proof. We clearly have {x ∣Ax = b} ⊆{x ∣AIx = bI}. We prove the reversed
inclusion. Since I is the pivot row set of A, one can check that rank(A) =
rank(AI) holds. Since Ax = b admits solutions, we have rank(A) = rank([A b]).
Similarly, we have rank(AI) = rank([AI bI]). Therefore, we have rank([A b]) =
rank([AI bI]). Hence, any equation aT x = b in Ax = b is a linear combination
of the equations of AIx = bI, thus {x ∣AIx = bI} ⊆{x ∣Ax = b} holds.
Lemma 2. We use the same notations as in Deﬁnition 1 and Remark 1. We
assume that HR is non-singular. Then, the system Ax = b has an integer solution
if and only if H−1
R b is integral. In this case, all integral solutions to Ax = b are
given by x = Pt + q where
1. the columns of P consist of a Z-basis of the linear space {x ∶Ax = 0},
2. q is a particular solution of Ax = b, and
3. t = (t1,...,td−r) is a vector of d −r unknowns.
The maximum absolute value of any coeﬃcient in P (resp. q) can be bounded
over by rr+1L2r (resp. rr+1L2r), where L is the maximum absolute value of any
coeﬃcient in A (resp. in either A or b). Moreover, P and q can be computed
within O(mdr2(log r + log L)2 + r4(log r + log L)3) bit operations.
Proof. Except for the coeﬃcient bound and running time estimates, we refer
to [11] for a proof of this lemma. The running time estimate follows from The-
orem 19 of [20] whereas the coeﬃcient bound estimates are taken from [21].
⊓⊔
Example 1. Let A, H and U be as follows:
A =
⎛
⎜⎜⎜⎜⎜
⎝
3 4 −4 −1
2 −2 8
4
5 2
4
3
3 5 −5 −2
2 −3 9
5
⎞
⎟⎟⎟⎟⎟
⎠
,
H =
⎛
⎜⎜⎜⎜⎜
⎝
0 −18 −1 −15
0 18 2 16
0
0
1
1
0 0
1
0
0 0
0
1
⎞
⎟⎟⎟⎟⎟
⎠
,
U =
⎛
⎜⎜⎜
⎝
−1 30 −3 −25
1 −37 4
31
0 −19 2
16
1
0
0
0
⎞
⎟⎟⎟
⎠
.
The matrix H is the column HNF of A, with unimodular matrix U and pivot
row set [2,4,5]. We denote by HR the sub-matrix of H whose coeﬃcients are
in bold fonts. Applying Lemma 1, we deduce that for any vector b such that
Ax = b admits one rational solution, we have:
⎧⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎩
3x1 + 4x2 −4x3 −x4 = b1
2x1 −2x2 + 8x3 + 4x4 = b2
5x1 + 2x2 + 4x3 + 3x4 = b3
3x1 + 5x2 −5x3 −2x4 = b4
2x1 −3x2 + 9x3 + 5x4 = b5
⇔
⎧⎪⎪⎪⎪⎨⎪⎪⎪⎪⎩
2x1 −2x2 + 8x3 + 4x4 = b2
3x1 + 5x2 −5x3 −2x4 = b4
2x1 −3x2 + 9x3 + 5x4 = b5
.
(2)

Computing the Integer Points of a Polyhedron, I: Algorithm
233
We apply Lemma 2: if Ax = b is consistent over Q and if H−1
R [b2,b4,b5]T is inte-
gral, then all the integer solutions of the second equation system in Relation (2)
are given by x = Pt + q, where P = [−1,1,0,1]T , q = [ 5
3b2 −19
3 b4 −155
3 b5,−37
18b2 +
73
9 b4 + 575
9 b5,−19
18b2 + 37
9 b4 + 296
9 b5]T , t = (t1) and t1 is a new variable.
4
Integer Solutions of Linear Inequality Systems
In this section, we present an algorithm for computing the integer points of a
polyhedron K ⊆Rd, that is, the set K ∩Zd. To do so, we adapt the Omega
Test invented by Pugh [17] for deciding whether or not a polyhedral set has an
integer point. Our algorithm decomposes the set K ∩Zd into a disjoint union
(K1 ∩Zd) ∪⋯∪(Ks ∩Zd), where K1,...,Ks are polyhedral sets in Rd, for which
the integer points can be represented in a sense speciﬁed in Sect. 4.2. Section 4.3
states the speciﬁcations of the main procedure while Sects. 3, 4.1, 4.4, 4.5 and
4.6. describe its main subroutines and its proof. We use the same notations as in
Sect. 2. However, from now on, we assume that all matrix and vector coeﬃcients
are integer numbers, that is, elements of Z. To be precise, we have the following.
Notation 2. We consider a polyhedral set K ⊆Rd given by an irredundant
intersection K = ∩i=m
i=1 Hi of closed half-spaces H1,...,Hm such that, for each
i = 1,...,m, the half-space Hi is deﬁned by aT
i x ≤bi, with ai ∈Zd and bi ∈Z.
The conjunction of those inequalities forms a system of linear inequalities that we
denote by Ax ≤b, as well as Σ. We do not assume that K is full-dimensional.
4.1
Normalization of Linear Inequality Systems
The purpose of the procedure IntegerNormalize, presented below, is to solve the
system consisting of the equations of Ax ≤b and substitute its solutions into
the system consisting of the inequalities of Ax ≤b. This process is performed
by Steps (S2) to (S6) and relies on Lemmas 1 and 2; this yields Proposition 1,
which provides the output speciﬁcation of IntegerNormalize. Step (S1) is an opti-
mization: performing it is not needed, but improves performance in practice.
When applied to Ax ≤b, the IntegerNormalize procedure proceeds as follows.
(S1) It computes proj(K;x1,...,xd), obtaining a new system of linear inequali-
ties that we denote again by Σ; if this proves that K has no rational points,
then the procedure stops and returns (∅,∅,∅) implying that K ∩Zd is
empty,
(S2) for every inequality ax ≤b, let g be the absolute value of the GCD of
coeﬃcients in a: if g > 1, replace ax ≤b by a
g x ≤⌊b
g⌋.
(S3) Every pair of inequalities of the form (aT
i x ≤bi,−aT
i x ≤−bi) is replaced by
the equivalent equation, that is, aT
i x = bi; Every pair of inequalities of the
form (aT
i x ≤bi,aT
i x ≤bj) is replaced by aT
i x ≤min(bi,bj).
(S4) Equations and inequalities form, respectively, a system of linear equations
A=x = b= and a system of linear inequalities A≤x ≤b≤, as speciﬁed in
Notation 3, so that the conjunction of these two systems is equivalent to Σ.

234
R.-J. Jing and M. Moreno Maza
(S5) If A=x = b= is empty, that is, if Σ has no equations, then the procedure
stops returning (x,∅,A≤x ≤b≤).
(S6) Proposition 1 is applied to A=x = b=; if this proves that this latter sys-
tem has no integer solutions, then the procedure stops returning (∅,∅,∅),
otherwise the change of variables given by (3) is applied to A≤x ≤b≤;
as a result, the output of the IntegerNormalize procedure is the triple
(t,x = Pt + q,Mt ≤v), where t,P,q,M,v are deﬁned in Proposition 1.
Notation 3. From now we consider an equation system A=x = b= and an
inequality system A≤x ≤b≤. The matrices A=,A≤as well as the vectors b=,b≤
have integer coeﬃcients. The total number of rows in both A= and A≤is m,
each of A=,A≤has d columns, and A≤has e rows. We denote by L and h the
maximum absolute value and maximal bit size of any coeﬃcient in the matrix in
either [A= b=] or [A≤b≤] respectively. We deﬁne r ∶= rank(A=).
Proposition 1. One can decide whether or not A=x = b= has integer solutions.
If this system has integer solutions, then, for any ε > 0, one can compute
1. a matrix P ∈Zd×(d−r) within O(mdr2+ε h3) bit operations,
2. a vector q ∈Zd within O(mdr2+ε h3) bit operations,
3. a matrix M ∈Ze×(d−r), whose coeﬃcients can be bounded over by drr+1L2r+1,
within O(md2 r1+εh3) bit operations,
4. a vector v ∈Ze, whose coeﬃcients can be bounded over by 2drr+1L2r+1, within
O(md2 r1+εh3) bit operations,
such that an integer point (x1,...,xd) ∈Zd solves A=x = b= and A≤x ≤b≤if
and only if there exists an integer point (t1,...,td−r) ∈Zd−r such that we have
{
(x1,...,xd)T = P(t1,...,td−r)T + (q1,...,qd)T
M(t1,...,td−r)T ≤(v1,...,ve)T
.
(3)
That is, one can perform the IntegerNormalize procedure within O(md2 r1+ε h3)
bit operations.
Proof. We ﬁrst observe that one can decide whether or not A=x = b= has solu-
tions in Rd, using standard techniques, say Gaussian elimination. If A= is not full
row-rank, this observation allows us to apply Lemma 1 and thus to reduce to the
case where A= is full row-rank, via the computation of the column HNF of A=.
Hence, from now on, we assume that A= is full row-rank. We apply Lemma 2
which yields the matrix P and the vector q. Next, we compute M and v as
follows: M ∶= A≤P and v ∶= −A≤q+b. The coeﬃcient bounds and cost estimates
for M and v follow easily from Lemma 2 and the inequality r ≤d.
⊓⊔
4.2
Representing the Integer Points
Applying IntegerNormalize to Ax ≤b produces a triple (t,x = Pt + q,Mt ≤v),
with P,q,M,v as in Proposition 1. Assume t ≠∅. Since the system x = Pt + q
solves the x-variables as functions of the t-variables, we turn our attention to
Mt ≤v. Deﬁnition 2 states conditions on M under which we view (x = Pt + q,
Mt ≤v) as a “solved system”, that is, a system describing its integer solutions.

Computing the Integer Points of a Polyhedron, I: Algorithm
235
Deﬁnition 2. Let ̂
K be the polyhedron of Z2d−r deﬁned by the system of linear
equations and inequalities given by x = Pt + q and Mt ≤v, in Relation (3). We
say that this system is a representation of the integer points of the polyhedron
̂
K whenever M has the following form:
⎛
⎜⎜⎜⎜⎜
⎝
M11 M12 ⋯M1,ℓ−1
M1,ℓ
M22 ⋯M2,ℓ−1
M2,ℓ
⋱
⋮
⋮
Mℓ−1,ℓ−1 Mℓ−1,ℓ
Mℓ,ℓ
⎞
⎟⎟⎟⎟⎟
⎠
,
(4)
where for each i,j with 1 ≤i,j ≤ℓ, the block Mi,j has mi rows and kj columns
such that the following six assertions hold:
(i) k1,...,kℓ−1 ≥1, kℓ≥0 and k1 + ⋯+ kℓ= d −r;
(ii) m1,...,mℓ−1 ≥2 and mℓ≥0;
(iii) for 1 ≤i < ℓ, each column in Mi,i has both positive coeﬃcients and negative
coeﬃcients, but no null coeﬃcients;
(iv) if mℓ> 0 holds, then in each column of Mℓ,ℓ, all coeﬃcients are non-zero
and have the same sign;
(v) (Consistency) the system Mt ≤v admits at least one integer point in Zd−r;
(vi) (Extensibility) for all 1 < i < d −r, every integer point of Rd−r−i solving
all the inequalities of Mt ≤v involving ti+1,...,td−r only extends to an
integer point of Rd−r−i+1 solving all the inequalities of Mt ≤v involving
ti,...,td−r.
More generally, we say that x = Pt+q and Mt ≤v form a representation of the
integer points of ̂
K if M satisﬁes (i) to (vi) up to a permutation of its columns.
Remark 2. Assume that the above matrix M satisﬁes the properties (i) to (vi)
of Deﬁnition 2. Then, the values of the ﬁrst k1+⋯+kℓ−1 (resp. last kℓ) variables of
t are bounded (resp. unbounded) in the polyhedron given by Mt ≤v. For these
reasons, we call those variables bounded and unbounded in Mt ≤v, respectively.
Clearly, the original polyhedron Ax ≤b is bounded if and only if mℓ= kℓ= 0.
4.3
The IntegerSolve Procedure: Speciﬁcations
We are ready to specify the main algorithm presented in this paper. This pro-
cedure, called IntegerSolve will be formally stated in Sect. 4.6. When applied to
Ax ≤b, with the assumptions of Notation 2, IntegerSolve produces a decompo-
sition of the integer points of the polyhedron K in the sense of the following.
Deﬁnition 3. Let A,x,b,K be as in Notation 2. A sequence of pairs (y1,Σ1),
..., (ys,Σs) is called a decomposition of the integer points of the polyhedron K
whenever the following conditions hold:
(i) yi is a sequence of di ≥d independent variables x1,...,xd,xd+1,...,xdi
thus starting with x,

236
R.-J. Jing and M. Moreno Maza
(ii) Σi is a system of linear inequalities with yi as unknown,
(iii) Σi is a representation of the integer points of a polyhedral set Ki,
and we have VZ(Σ) = VZ(Σ1,x) ∪⋯∪VZ(Σs,x), where VZ(Σ) denotes the set
of the integer points of Σ and where VZ(Σi,x) is deﬁned as the set of the points
(x1,...,xd) ∈Zd such that there exists a point (xd+1,...,xdi) ∈Zdi−d such that
(x1,...,xd,xd+1,...,xdi) solves Σi.
In the sequel of Sect. 4, we shall propose and prove an algorithm satisfying
the above speciﬁcations. The construction is by induction on d ≥1. We observe
that the case d = 1 is trivial. Indeed, in this case, K is necessarily an interval
of the real line. Then, either K ∩Z is empty and IntegerSolve(Σ) returns the
empty set, or K ∩Z is not empty and the system Σ is clearly a representation of
the integer points of K in the sense of Deﬁnition 2. The case d > 1 will be treated
in Sect. 4.6, after presenting the main subroutines of the IntegerSolve procedure.
4.4
The DarkShadow Procedure
Let M,v be as in Proposition 1. Recall that we write t = (t1,...,td−r) and assume
0 ≤r < d. The system Mt ≤v represents a polyhedral set that we denote by
Kt. We order the variables as t1 > ⋯> td−r. We call DarkShadow the procedure
stated by Algorithm 1, for which Proposition 2 serves as output speciﬁcation. In
Algorithm 1, the polyhedral set represented by M<t1t ≤v<t1 (resp. Θ) is called
the dark shadow of Kt, denoted as Dt1 when case 1 (resp. case 2) holds.
Algorithm 1. DarkShadow(Mt ≤v)
1: case 1: for all 1 ≤i ≤d −r, the inequalities in ti are either all lower bounds of ti
or all upper bounds of ti
2:
return ((t2, . . . , td−r), M<t1t ≤v<t1).
3: case 2: otherwise
4:
re-order the variables, such that t1 has both lower bounds and upper bounds.
5:
initialize Δ to the empty set.
6:
for each upper bound c t1 ≤γ of t1, where c > 0, γ ∈Z[t2, . . . , td−r] do
7:
for each lower bound −a t1 ≤−α of t1, where a > 0, α ∈Z[t2, . . . , td−r] do
8:
let Δ ∶= Δ ∪{c α −a γ ≤−(c −1)(a −1)}.
9:
end for
10:
end for
11:
Let Θ0 ∶= Δ ∪M<t1t ≤v<t1
12:
Let Θ be the system obtained by removing from Θ0 all redundant inequalities.
13:
return ((t2, . . . , td−r), Θ).
For the inequalities in the set Δ in Algorithm 1, we have the following.
Lemma 3 Pugh [17]. Let ct1 ≤γ be an upper bound of t1 and −at1 ≤−α be a
lower bound of t1, where c > 0, a > 0, γ ∈Z[t2,...,td−r] and α ∈Z[t2,...,td−r]
hold. Then, every integer point (t2,...,td−r) satisfying cα −aγ ≤−(c −1)(a −1)
extends to an integer point (t1,t2,...,td−r) satisfying both ct1 ≤γ and −at1 ≤α.

Computing the Integer Points of a Polyhedron, I: Algorithm
237
Proposition 2. Let ((t2,...,td−r),Θ) be the output of the DarkShadow proce-
dure. Then, every integer point of VZ(Θ,(t2,...,td−r)) extends to an integer
point solving Ax ≤b.
Proof. If the DarkShadow procedure returns at Line 2 of Algorithm 1, the claim
holds easily. Lemma 3 shows that any integer point (t2,...,td−r) solving Δ can
be extended to an integer point solving Mt ≤v, thus with Proposition 1, to an
integer point solving Ax ≤b. Therefore, if the DarkShadow procedure returns at
Line 13, the claim also holds.
4.5
The GreyShadow Procedure
Let M, t, v, Kt,Dt1 be as in Sect. 4.4. We call grey shadow
of Kt, denoted
by Gt1, the set-theoretic diﬀerence (Πt2,...,td−rKt) ∖Dt1. Algorithm 2 states the
GreyShadow procedure, for which Lemma 4 serves as output speciﬁcation.
Lemma 4. Let G = {(u1,t = P1u1 +q1,M1u1 ≤v1),...,(us,t=Psus+qs,Msus≤
vs)} be the output of Algorithm 2. Then, the disjoint union
⊍
1≤i≤sVZ(t = Piui +
qi ∪Miui ≤vi, t) forms the set of the integer points of the grey shadow Gt1.
Proof. The correctness of case 1 follows from the fact that Gt1 is empty when
all t-variables are unbounded. From now on, we consider case 2. At Line 12,
all the t-variables are solved by IntegerNormalize as functions of new variables
ui. The fact that
⋃
1≤i≤sVZ(t = Piui + qi ∪Miui ≤vi, t) equals Gt1 follows
Algorithm 2. GreyShadow(Mt ≤v)
1: case 1: for all 1 ≤i ≤d −r, the inequalities in ti are either all lower bounds of ti
or all upper bounds of ti
2:
return (∅, ∅, ∅)
3: case 2: otherwise
4:
Re-order the variables, such that t1 has both lower bounds and upper bounds.
5:
Initialize both Υ and G to the empty set; the former set will be a set of linear
inequalities while the latter will form the result of the procedure.
6:
for each upper bound c t1 ≤γ of t1, where c > 0, γ ∈Z[t2, . . . , th] do
7:
for each lower bound −a t1 ≤−α of t1, where a > 0, α ∈Z[t2, . . . , th] do
8:
let Θ2 ∶= Υ ∪Mt ≤v ∪{cα −aγ > −(c −1)(a −1)},
9:
for each non-negative integer i ≤ca−c−a
c
do
10:
check whether at1 = α + i is consistent over Z using Lemma 2,
11:
case no: move to the next iteration,
12:
case yes: let G ∶= G ∪IntegerNormalize({at1 = α + i} ∪Θ2),
13:
end for
14:
let Υ ∶= Υ ∪{cα −aγ ≤−(c −1)(a −1)}.
15:
end for
16:
return G.
17:
end for

238
R.-J. Jing and M. Moreno Maza
from Sect. 2.3.1. of [17].
Now, at Line 8 of Algorithm 2, we add the constraint
cα −aγ > −(c −1)(a −1) to Θ2, while at Line 14, we use cα −aγ ≤−(c −1)(a −1)
to construct Υ in the next loop iteration. From that construction of Θ2 and Υ,
we easily deduce that the above union is disjoint.
4.6
The IntegerSolve Procedure: Algorithm
We are ready to state an algorithm satisfying the speciﬁcations of Integer-
Solve introduced in Sect. 4.3. The recursive nature of this algorithm leads us
to deﬁne an “inner procedure”, called IntegerSolve0, of which IntegerSolve is a
wrapper function. The procedure IntegerSolve0 takes as input the system to be
solved, namely Ax ≤b, together with another system of linear equations and
inequalities, denoted by E, see Notation 4. This second system E keeps track
of the relations between those variables that have already been solved and those
that remain to be solved. To be more precise, the procedure IntegerSolve0, see
Algorithm 3, relies on IntegerNormalize and thus introduces new variables when
solving systems of linear equations over Z. For this reason, variables appearing
in E may not be present in x and we need another vector of variables, namely
y = (y1,...,yd′), to denote the unknowns of E that are regarded as “solved”.
Notation 4. We denote by E a second system of linear equations and inequal-
ities, with coeﬃcients in Z and with y ⊕x as “unknown” vector, where y ⊕x
denotes the concatenated vector (y1,...,yd′,x1,...,xd). In fact, the variables of
y are regarded as solved by the equations and inequalities of E, meanwhile those
of x remain to be solved. Hence, we can view the conjunction of the systems
Ax ≤b and E as a system of linear equations and inequalities with y ⊕x as
unknown vector, deﬁning a polyhedron KE in Rd′+d.
Theorem 1 states, that Algorithm 3 returns a decomposition (in the sense of
Deﬁnition 3) of the integer points of the polyhedron KE, deﬁned in Notation 4.
From Algorithm 3, we easily implement the IntegerSolve procedure (as speciﬁed
in Sect. 4.3) with the call IntegerSolve0({ },{ },x,Ax ≤b).
Theorem 1. Algorithm 3 terminates and returns a decomposition of the integer
points of the polyhedron KE.
Proof. We ﬁrst prove termination. Lines 1 to 21 in Algorithm 3 handle the case
where Ax ≤b has a single unknown. This is simply done by case inspection.
Consider now the case where Ax ≤b has more than one variable. The calls to
the procedures DarkShadow and GreyShadow at Lines 29 and 32 generate the
input to the recursive calls. From Lines 2 and 13 of Algorithm 1, and Lines 2
and 12 of Algorithm 2, we deduce that the number of unknowns decreases at
least by one after each recursive call. Therefore, Algorithm 3 terminates.
Next we prove that Algorithm 3 is correct. Let (y1,Σ1), ..., (ys,Σs) be the
output of Algorithm 3 where each Σi is a system of linear inequalities with yi
as unknown. The fact that each Σi is a representation of the integer points of

Computing the Integer Points of a Polyhedron, I: Algorithm
239
Algorithm 3. IntegerSolve0(y, E, x, Ax ≤b)
1: Let d be the cardinality of x;
2: case d = 1
3:
let x = {x}, solve Ax ≤b over R,
4:
case only lower bounds of x exist in Ax ≤b
5:
the solution to Ax ≤b over R is {x ∶−x ≤q1} for some q1 ∈R,
6:
y ∶= y ⊕x and E ∶= E ∪{ −x ≤⌊q1⌋};
7:
return {(y, E)}
8:
case only upper bounds of x exist in Ax ≤b
9:
the solution to Ax ≤b over R is {x ∶x ≤q2} for some q2 ∈R,
10:
y ∶= y ⊕x and E ∶= E ∪{x ≤⌊q2⌋};
11:
return {(y, E)}
12:
case both lower bounds and upper bounds of x exist in Ax ≤b
13:
the solution to Ax ≤b over R is {x ∶x ≤q3 and −x ≤q4} for some q3, q4 ∈R,
14:
case ⌊q3⌋> −⌊q4⌋
15:
y ∶= y ⊕x and E ∶= E ∪{ x ≤⌊q3⌋, −x ≤⌊q4⌋};
16:
return {(y, E)}
17:
case ⌊q3⌋= −⌊q4⌋
18:
y ∶= y ⊕x, E ∶= eval(E, x = ⌊q3⌋) ∪{x = ⌊q3⌋},
19:
return {(y, E)}
20:
case ⌊q3⌋< −⌊q4⌋
21:
return {(∅, ∅)}
22: case d > 1
23:
(t, x = Pt + q, Mt ≤v) ∶= IntegerNormalize(Ax ≤b),
24:
case (t, x = Pt + q, Mt ≤v) = (∅, ∅, ∅)
25:
return {(∅, ∅)}
26:
case (t, x = Pt + q, Mt ≤v) ≠(∅, ∅, ∅)
27:
y ∶= y ⊕x, E ∶= eval(E, x = Pt + q) ∪x = Pt + q ∪Mt1t ≤vt1,
28:
G ∶= ∅,
29:
(t′, Θ) ∶= DarkShadow(Mt ≤v),
30:
y ∶= y ⊕{t1},
31:
G ∶= G ∪IntegerSolve0(y, E, t′, Θ);
32:
for (u, Eu, Muu ≤vu) ∈GreyShadow(Mt ≤v) do
33:
G ∶= G ∪IntegerSolve0(y ∪t, E ∪Eu, u, Muu ≤vu)
34:
end for
35:
return G
the polyhedron it deﬁnes, can be established by induction on the length of yi.
To give more details, the properties required by Deﬁnition 2 are easy to check in
the case d = 1. For the cases d > 1, these properties, in particular the consistency
and the extensibility, follow from the way the set E is incremented at Lines 27
and 33, as well as from Proposition 2. Finally, the fact that the integer points
of the input system of the initial call to Algorithm 3 are given by the integer
points of Σ1,...,Σs can be established by induction on the length of yi, thanks
to Lemma 4.

240
R.-J. Jing and M. Moreno Maza
Software
We have implemented the algorithm presented in the ﬁrst paper within the
Polyhedra library in Maple. This library is publicly available in source on the
download page of the RegularChains library at www.regularchains.org.
Acknowledgements. The authors would like to thank IBM Canada Ltd (CAS
project 880) and NSERC of Canada (CRD grant CRDPJ500717-16), as well as the
University of Chinese Academy of Sciences, UCAS Joint PhD Training Program, for
supporting their work.
References
1. Aubry, P., Lazard, D., Moreno Maza, M.: On the theories of triangular sets. J.
Symb. Comput. 28, 105–124 (1999)
2. Barvinok, A.I.: A polynomial time algorithm for counting integral points in poly-
hedra when the dimension is ﬁxed. Math. Oper. Res. 19(4), 769–779 (1994)
3. Barvinok, A.I.: Integer Points in Polyhedra. Contemporary Mathematics. Euro-
pean Mathematical Society (2008)
4. Beck, M.: Integer Points in Polyhedra-Geometry, Number Theory, Representa-
tion Theory, Algebra, Optimization, Statistics: AMS-IMS-SIAM Joint Summer
Research Conference, 11–15 June 2006, Snowbird. Utah. Contemporary mathe-
matics - Amer. Math, Soc. (2008)
5. Chen, C., Chen, X., Keita, A., Moreno Maza, M., Xie, N.: MetaFork: a compilation
framework for concurrency models targeting hardware accelerators and its applica-
tion to the generation of parametric CUDA kernels. In: Proceedings of CASCON
2015, pp. 70–79 (2015)
6. Cohen, H.: A Course in Computational Algebraic Number Theory, vol. 138.
Springer Science & Business Media, Heidelberg (2013)
7. Feautrier, P.: Parametric integer programming. RAIRO Recherche Op´erationnelle
22 (1988). http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.30.9957&re
p=rep.1&type=pdf
8. Feautrier, P.: Automatic parallelization in the polytope model. In: Perrin, G.-R.,
Darte, A. (eds.) The Data Parallel Programming Model. LNCS, vol. 1132, pp.
79–103. Springer, Heidelberg (1996). doi:10.1007/3-540-61736-1 44. http://dl.acm.
org/citation.cfm?id=647429.723579
9. Fischer, M.J., Fischer, M.J., Rabin, M.O.: Super-exponential complexity of pres-
burger arithmetic. Technical report, Cambridge, MA, USA (1974)
10. Gr¨unbaum, B.: Convex Polytops. Springer, New York (2003)
11. Hung, M.S., Rom, W.O.: An application of the hermite normal form in integer
programming. Linear Algebra Appl. 140, 163–179 (1990)
12. Jing, R.-J., Yuan, C.-M., Gao, X.-S.: A polynomial-time algorithm to compute
generalized hermite normal form of matrices over Z[x]. CoRR, abs/1601.01067
(2016)
13. Jones, C.N., Kerrigan, E.C., Maciejowski, J.M.: On polyhedral projection and para-
metric programming. J. Optim. Theory Appl. 138(2), 207–220 (2008)
14. Kannan, R., Bachem, A.: Polynomial algorithms for computing the smith and
hermite normal forms of an integer matrix. SIAM J. Comput. 8(4), 499–507 (1979)

Computing the Integer Points of a Polyhedron, I: Algorithm
241
15. Khachiyan, L.: Fourier-motzkin elimination method. In: Floudas, C.A., Parda-
los, P.M. (eds.) Encyclopedia of Optimization, 2nd edn, pp. 1074–1077. Springer,
Heidelberg (2009). doi:10.1007/978-0-387-74759-0 187
16. K¨oppe, M., Verdoolaege, S.: Computing parametric rational generating functions
with a primal Barvinok algorithm. Electr. J. Comb. 15(1), R16 (2008)
17. Pugh, W.: The omega test: a fast and practical integer programming algorithm
for dependence analysis. In: Martin, J.L. (ed.) Proceedings Supercomputing 1991,
Albuquerque, NM, USA, 18–22 November 1991, pp. 4–13. ACM (1991)
18. Pugh, W.: Counting solutions to presburger formulas: how and why. In: Sarkar,
V., Ryder, B.G., Soﬀa, M.L. (eds.) Proceedings of the ACM SIGPLAN 1994 Con-
ference on Programming Language Design and Implementation (PLDI), Orlando,
Florida, USA, 20–24 June 1994, pp. 121–134. ACM (1994)
19. Schrijver, A.: Theory of Linear and Integer Programming. Wiley, New York (1986)
20. Storjohann, A.: A fast practical deterministic algorithm for triangularizing integer
matrices. Citeseer (1996)
21. Storjohann, A.: Algorithms for matrix canonical forms. Ph.D. thesis, Swiss Federal
Institute of Technology Z¨urich (2000)
22. Wonnacott, D.: Omega test. In: Encyclopedia of Parallel Computing, pp. 1355–
1365 (2011)

Computing the Integer Points
of a Polyhedron, II: Complexity Estimates
Rui-Juan Jing1,2(B) and Marc Moreno Maza2
1 KLMM, UCAS, Academy of Mathematics and Systems Science,
Chinese Academy of Sciences, Beijing, China
2 University of Western Ontario, London, Canada
rjing8@uwo.ca, moreno@csd.uwo.ca
Abstract. Let K be a polyhedron in Rd, given by a system of m linear
inequalities, with rational number coeﬃcients bounded over in absolute
value by L. In this series of two papers, we propose an algorithm for
computing an irredundant representation of the integer points of K, in
terms of “simpler” polyhedra, each of them having at least one integer
point. Using the terminology of W. Pugh: for any such polyhedron P,
no integer point of its grey shadow extends to an integer point of P. We
show that, under mild assumptions, our algorithm runs in exponential
time w.r.t. d and in polynomial w.r.t m and L. We report on a software
experimentation. In this series of two papers, the ﬁrst one presents our
algorithm and the second one discusses our complexity estimates.
1
Introduction
In the ﬁrst paper of that series of two, we have presented an algorithm, called
IntegerSolve, for decomposing the set of integer points of a polyhedron. See Sect. 4
of the ﬁrst paper. This second paper is dedicated to complexity estimates con-
sidering both running time and output size. Our main result is Theorem 1, which
states an exponential time complexity1 for IntegerSolve, under Hypothesis 1, that
we call Pugh’s assumption. Before discussing this hypothesis and stating the the-
orem, we set up some notations.
Notation 1. Recall that we consider a polyhedral set K ⊆Rd given by an irre-
dundant intersection K = ∩i=m
i=1 Hi of closed half-spaces H1, . . . , Hm such that,
for each i = 1, . . . , m, the half-space Hi is deﬁned by aT
i x ≤bi, with ai ∈Zd and
bi ∈Z. The conjunction of those inequalities forms a system of linear inequalities
that we denote by Ax ≤b. Let L (resp. h) be the maximum absolute value (resp.
maximum bit size) of a coeﬃcient in either A or b. Thus h = ⌊log2(L)⌋+ 2.
Hypothesis 1. We assume that during the execution of the function call
IntegerSolve(K), for any polyhedral set K′, input of a recursive call, each facet
of the dark shadow2 of K′ is parallel to a facet of the real shadow of K′.
1 To be precise, in the EXP complexity class.
2 The notions of real shadow, dark shadow and grey shadow are presented in Sect. 3
of the ﬁrst paper.
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 242–256, 2017.
DOI: 10.1007/978-3-319-66320-3 18

Computing the Integer Points of a Polyhedron, II: Complexity Estimates
243
The ﬁgure in the introduction of the ﬁrst paper shows a polyhedron for which
each facet of its dark shadow is parallel to a facet of its real shadow. This property
is commonly observed in practice, see Sect. 5. In [9], W. Pugh observes that it
is possible to build polyhedra K that challenge the Omega Test by generating
many recursive calls when searching for integer points of K that extend integer
points of its grey shadow. But he notices that, in practice, this combinatorial
explosion is rare, due to the fact that the grey shadow of K is often empty (or at
least for most of the recursive calls of the Omega test, when searching for integer
points in K). This experimental observation leads us to Hypothesis 1 which is
less strong than the property observed by W. Pugh, while being suﬃcient to
guarantee that our algorithm runs in exponential time.
We believe that this running estimate could still hold with the following even
weaker hypothesis: during the execution of the function call IntegerSolve(K), for
any polyhedral set K′, input of a recursive call, the number of facets of the
dark shadow of K′ is in “big-O” of the number of facets of its real shadow.
Investigating this question is left for future work.
To state our main result, we need a notation for the running time of solving
a linear program. Indeed, linear programming is an essential tool for removing
redundant inequalities generated by Fourier-Motzkin elimination, see [7].
Notation 2. For an input linear program with total bit size H and with d vari-
ables, we denote by LP(d, H) an upper bound for the number of bit operations
required for solving this linear program. For instance, in the case of Karmarkar’s
algorithm [6], we have LP(d, H) ∈O(d3.5H2 · log H · log log H).
Theorem 1. Under Hypothesis 1, the call function IntegerSolve(K) runs within
O(m2d2d4d3L4d3LP(d, mdd4(log d + log L))) bit operations.
The running time estimate in Theorem 1 is exponential w.r.t. d but poly-
nomial w.r.t m and L. Since our algorithm transforms the Omega Test from a
decision procedure into a system solving algorithm, our result also holds for the
original Omega Test. To our knowledge, this is the ﬁrst complexity estimate for
the whole Omega Test procedure.
The proof follows from a series of results established in Sects. 2, 3 and 4. We
believe that some of them are interesting on their own.
Section 2 deals with the following problem. Let F be a k-dimensional face
of K, for 0 ≤k < d. What is the computational cost of projecting F onto a
k-dimensional linear subspace of Rd?
Section 3 gives complexity estimates for Fourier-Motzkin elimination (FME).
While it is known that FME can run in single exponential time [5,7], we are
not aware of running time estimates for FME in the literature. Thanks to
Hypothesis 1, our FME estimates applies to the DarkShadow sub-routine of Inte-
gerSolve.
Section 4 gathers results for completing the proof of Theorem 1. The recursive
nature of this algorithm leads us to give upper bounds for three quantities: the
number of nodes in the tree of the recursive calls, the number of facets of each
polyhedron input of a recursive call, the maximum absolute value of a coeﬃcient
in a linear system deﬁning such a polyhedron.

244
R.-J. Jing and M. Moreno Maza
2
Properties of the Projection of Faces of a Polyhedron
This section gathers preliminary results towards the complexity analysis of the
IntegerSolve algorithm. Some of these results are probably not new, but we could
not ﬁnd a reference for them in the literature.
Deﬁnition 1. Let I be a subset of {1, . . . , m} and denote by BI the aﬃne space
{x ∈Rd | aT
i x = bi for i ∈I}. If BI ∩K is not empty, then BI ∩K is a face
of K. We call such an index set I a deﬁning index set of the face BI ∩K.
Let F be a k-dimensional face of K for an integer 0 ≤k < d and let I be
a deﬁning index set of F with maximum cardinality. Consider the set OI given
by:
OI
=
BI ∩{x |aT
i x < bi, i ̸∈I}.
(1)
Proposition 1. The set OI is not empty.
Proof. The assumption on I implies that for all i ̸∈I the equality aT
i x = bi is
not an implicit equation of F. Indeed, if aT
i x = bi were an implicit equation of
F, then the set {i}∪I would be a deﬁning index set of F as well, a contradiction.
From Sect. 8.1 of [10] and since no equation aT
i x = bi for i ̸∈I is an implicit
equation of F deﬁned by I, we deduce that the set OI is not empty.
⊓⊔
Using Gaussian elimination, we can compute a parametric representation of
OI where dim(BI) variables are treated as parameters; we denote by x′ those
parameters. The other d −dim(BI) variables are referred as main variables or
leading variables, following the terminology of the theory of regular chains [2].
Once we substitute the main variables by their linear forms in the parameters
(solved from BI) into the system {x |aT
i x < bi, i ̸∈I}, we obtain a consistent
strict inequality system in the parameters, whose solution set, that we call Oo,
is of dimension dim(BI) in the parameter space.
Proposition 2. We have dim(BI) = dim(OI) = dim(Oo) = dim(F) = k.
Proof. Note that the set Oo is the image of OI in the standard projection onto
the parameter space and that Oo is open in that space (equipped with the
Euclidean topology). Hence, we have dim(BI) = dim(OI) = dim(Oo). In fact,
this elimination-and-substitution process shows that OI is the solution set of a
so-called regular semi-algebraic system [4] where the regular chain part is given by
a regular chain of height d−dim(BI). Meanwhile, we have dim(BI) ≥dim(F) ≥
dim(OI), since BI ⊇F ⊇OI holds by deﬁnition. Moreover, we have dim(OI) ≥
dim(Oo) = dim(BI) since Oo is the image of OI. Finally, since dim(F) = k holds
by assumption, we deduce dim(BI) = dim(F) = k.
⊓⊔
The following lemma was found by the authors independently of the work of
Imbert [5] but it is likely that our result could be derived from that paper.
Lemma 1. Let F be a k-dimensional face of K for some integer 0 ≤k < d.
Then, the face F admits a deﬁning index set with cardinality d −k.

Computing the Integer Points of a Polyhedron, II: Complexity Estimates
245
Proof. First, we shall prove that there exists a deﬁning index set with cardinal-
ity at least d −k. Assume that I is a deﬁning index set of F with maximum
cardinality. From Proposition 2, we have dim(BI) = k, hence I has at least d−k
elements. Assume I = {i1, i2, . . . , it}, with t ≥d −k. Since dim(BI) = k holds,
one can easily deduce that the rank of the matrix
(aT
i1, aT
i2, . . . , aT
it)
and the rank of the matrix
(aT
i1, aT
i2, . . . , aT
it, (bi1, bi2, . . . , bit)T )
are both d −k. Thus, we can further assume w.l.o.g. that
(aT
i1, aT
i2, . . . , aT
id−k)
has rank d −k. Then clearly, the set I∗= {i1, i2, . . . , id−k} is also a deﬁning
index set of F. That is, the k-dimensional face F admits a deﬁning index set
with cardinality d −k.
⊓⊔
Corollary 1 follows immediately from Lemma 1.
Corollary 1. Let 0 ≤k < d be an integer. Let fd,m,k be the number of k-
dimensional faces of K. Then, we have
fd,m,k ≤
 m
d −k

.
Therefore, we have
fd,m,0 + fd,m,1 + · · · + fd,m,d−1 ≤md.
Note that, from now on, when we say a deﬁning index set of a k-dimensional
face of K, we shall always refer to one with cardinality d −k. Let FP be the
closure of Oo in the Euclidean topology. Then, FP is the projection of F on the
coordinates x′, where x′ stand for the parameters introduced above. Thus, FP is
a polyhedron and Corollary 2 gives upper-bound estimates on a representation
of FP with a system of linear inequalities.
Corollary 2. One can compute a matrix C over Z and a vector d over Z such
that the integer points of FP are given by Cx′ ≤d and the maximum absolute
value of a coeﬃcient in either C or d is no more than (d −k + 1)
d−k+1
2
Ld−k+1,
where L is the maximum absolute value of a coeﬃcient in either A or b.
Proof. Without loss of generality, assume I = {1, . . . , d−k}. From Proposition 2,
we have dim(FP ) = k. From the proof of Lemma 1, the rank of the matrix
(aT
1 , aT
2 , . . . , aT
d−k) and the rank of matrix (aT
1 , aT
2 , . . . , aT
d−k, (b1, b2, . . . , bd−k)T )
are both equal to d −k. Without loss of generality, assume that the ﬁrst d −k
rows of each of the above two matrices are linearly independent. Therefore, we
have x′ = [xd−k+1, . . . , xd]T . It follows that FP can be deﬁned by the inequality

246
R.-J. Jing and M. Moreno Maza
system Cx′ ≤d obtained by Fraction-Free Gaussian Elimination, where C and
d are given by:
C = (ci,j)d−k<i,j≤d, where ci,j is the determinant of
⎛
⎜
⎜
⎜
⎝
a11
· · ·
a1,d−k
a1,j
...
...
...
...
ad−k,1 · · · ad−k,d−k ad−k,j
ai,1
· · ·
ai,d−k
ai,j
⎞
⎟
⎟
⎟
⎠,
d = [dd−k+1, . . . , dd]T , where di is the determinant of
⎛
⎜
⎜
⎜
⎝
a11
· · ·
a1,d−k
b1
...
...
...
...
ad−k,1 · · · ad−k,d−k bd−k
ai,1
· · ·
ai,d−k
bi
⎞
⎟
⎟
⎟
⎠.
Using Hadamard’s inequality, the absolute value of any ci,j and dj can be
bounded by (d −k + 1)
d−k+1
2
Ld−k+1.
⊓⊔
3
Complexity Estimates for Fourier-Motzkin Elimination
Proposition 4 states a running time estimate for computing the linear inequality
system proj(K; x1, . . . , xd) deﬁned in Sect. 2 of the ﬁrst paper. Note that the arti-
cle [7] states that Fourier-Motzkin elimination can be run in single exponential
time but without giving a running time estimate. Let k < d be a positive integer.
Following the notations of Sect. 2 of the ﬁrst paper, we denote by Πxk+1,...,xd the
standard projection from Rd to Rd−k mapping (x1, . . . , xd) to (xk+1, . . . , xd).
Proposition 3. Assume that K is full-dimensional. Then, we have:
(i) The projected polyhedron Πxk+1,...,xdK admits at most

m
d−k−1

facets.
(ii) Any facet of Πxk+1,...,xdK can be given by a system consisting of one linear
equation and m−k−1 linear inequalities, all in Rd−k, such that the absolute
value of any coeﬃcient in those constraints is at most (k + 1)
k+1
2 Lk+1.
Proof. Let G be a facet of Πxk+1,...,xdK. There exists a face F of K such that G
is the projection of F. Since K is full-dimensional, it is clear that Πxk+1,...,xdK
is full-dimensional as well. Hence, we have dim(G) = d −k −1 ≤dim(F).
Clearly, choosing F with minimum dimension implies d −k −1 = dim(F). With
Corollary 1, we deduce (i). Now we prove (ii). It follows from Lemma 1 that one
can choose a deﬁning index set I of F with cardinality d −(d −k −1) = k + 1.
Thus, we have BI ∩K = F, with BI given in Deﬁnition 1. Consider, then, the
set OI given by (1). We know from Proposition 1 that OI is not empty and
from Proposition 2 that dim(BI) = d −k −1. Consider now the system of linear
equations given by:
GI
=
BI ∩{x |aT
i x = bi, i ̸∈I}.
(2)
Using Fraction-Free Gaussian Elimination on GI and since dim(BI) = d −k −1
holds, one can use the k+1 equations deﬁning BI to eliminate x1, . . . , xk from the

Computing the Integer Points of a Polyhedron, II: Complexity Estimates
247
inequalities {x |aT
i x < bi, i ̸∈I} and, in addition, obtain one equation involving
the variables xk+1, . . . , xd only. Clearly, the resulting inequalities and equation
exactly deﬁne G. Using Hadamard’s inequality as in the proof of Corollary 2, we
deduce (ii).
⊓⊔
Deﬁnition 2. Let θ be an inequality in the irredundant representation of the
projected polyhedron Πxk+1,...,xdK. Let G be the facet of Πxk+1,...,xdK associated
with θ. There exists a (k+1)-dimensional face G′ of K such that Πxk+1,...,xdG′ =
G holds. We call deﬁning index set of θ any deﬁning index set of G′.
Lemma 2. Let v ∈Rd and s ∈R such that h is also the maximum bit size
of any coeﬃcient in v and s. Hence, the total bit size of the linear program
sup{−(vx −s) | Ax ≤b} is H ∈O(h m d). Moreover, deciding whether the
inequality vx ≤s is implied by Ax ≤b or not can be done within O(LP(d, H))
bit operations.
Proof. The estimate H ∈O(h m d) clearly holds. On the other hand, the inequal-
ity vx ≤s is implied by Ax ≤b if only if sup{−(vx −s) | Ax ≤b} is zero.
⊓⊔
Proposition 4. Within O(d2 m2d LP(d, 2dhd2md)) bit operations, the projected
representation proj(K; x1, . . . , xd) of K can be computed.
Proof. Following the notations of Sect. 2 of the ﬁrst paper, the process of elimi-
nating x1 in Ax ≤b generates at most m2
4 new inequalities. Augmenting A<x1
with all these new inequalities and, making this augmented system irredundant,
we obtain a total number of inequalities that we denote by c2. We deﬁne c1 := m,
m1 := c1 and m2 := c1 + c2. We observe that:
1. generating all the new inequalities (irredundant or not) amounts to at most
O( m2
1
4 d h2
1) bit operations, and
2. removing the redundant ones amounts to at most to O( m2
1
4
LP(d, h1 d m1))
bit operations, thanks to Lemma 2.
Similarly, during the process of eliminating x2, we observe that:
1. generating all the new inequalities (irredundant or not) amounts to at most
O( c2
2
4 d h2
2) bit operations, and
2. removing the redundant ones amounts to at most to O( c2
2
4 LP(d, h2 d m2)) bit
operations and yields a total number of c3 inequalities in x3.
Continuing in this manner, we deduce that for successively eliminating x1, . . . ,
xd−1,
1. generating all the new inequalities (irredundant or not) amounts to at most
O(c2
1
4 d h2
1 + · · · + c2
d−1
4
d h2
d−1),
(3)

248
R.-J. Jing and M. Moreno Maza
2. removing the redundant ones amounts to at most to
O(c2
1
4 LP(d, h1 d m1) + · · · + c2
d−1
4
LP(d, hd−1 d md−1)),
(4)
where mi := c1 + · · · + ci, for 1 ≤i < d, as well as h0 := h and hi+1 ≤2 hi + 1,
for 0 ≤i < d. We observe that ci is bounded over by the number of facets
of Πxi,...,xdK, for 1 ≤i < d. Observe also that, for 1 < i < d, each facet
of Πxi,...,xdK is the projection of a face of Πxi−1,...,xdK. Using Lemma 1, we
deduce that, for all 1 ≤i < d, we have: ci ≤md. Therefore, the running
time estimates of (3) and (4) can be bounded over by O(d2 m2d d (2dh)2) and
O(d2 m2d LP(d, (2dh)d(dmd))). The latter dominates the former; the conclusion
follows.
⊓⊔
4
Proof of Theorem 1
We use Fig. 1 and Notation 3 to provide further explanation on Algorithm
IntegerSolve0, presented in the ﬁrst paper.
Fig. 1. Diagram
Notation 3. Fig. 1 illustrates the tree of recursive calls for the IntegerSolve0
procedure. The root of the tree is labelled with S, which stands for the input
system. The left (resp. right) child of a node, other than a leaf, is labelled by
D (resp. G) which stands for the output of the DarkShadow procedure (resp.
the GreyShadow procedure). Since the DarkShadow procedure generates one input
system for IntegerSolve, we use a simple →arrow as an edge to a D-node. How-
ever, the GreyShadow procedure may generate several linear inequality systems,
leading to several recursive calls to IntegerSolve0. Thus, we use a ⇒arrow as an
edge to a G-node. The numbers on the right-hand side of Fig. 1 stand for the
levels in the tree.
Let Ax ≤b, m, d be as in Notation 1. Let L and h denote the maximum
absolute value and height of any coeﬃcient in either A or b.

Computing the Integer Points of a Polyhedron, II: Complexity Estimates
249
Notation 4. Recall that Fig. 1 depicts the tree of recursive calls in Algorithm
IntegerSolve0. Let N denote any node in that tree, whether it is labelled S, D or
G. If N is labelled with S or D, it is associated with a single linear system denoted
by MNtN ≤vN. If N is labelled with G, it is associated with a sequence of linear
systems produced by the GreyShadow procedure and we denote by MNtN ≤vN
any of those systems. For any linear system MNtN ≤vN (whether N is labelled
S, D or G), we denote by mN and dN the number of rows and columns of MN.
We denote by LN (resp. ℓN) be the maximum absolute value of any coeﬃcient in
MN (resp. in either MN or vN). We denote by hN = ⌊log2 ℓN⌋+1 the maximum
bit size of a coeﬃcient in either MN or vN. The system MNtN ≤vN encodes
a polyhedron KN in RdN and we denote by FN an arbitrary facet of KN. Every
path from the root to a leaf Nr in the tree depicted in Fig. 1 can be labelled
S →N1 →· · · →Nr for some r ≤d −1. Note that a leaf (that is, a node with
no children) may have level less than d −1. For simplicity, for the node Nr, we
write dr, Lr, ℓr, hr, tr, Mr, vr, Kr, Fr instead of dNr, LNr, ℓNr, hNr, tNr,
MNr, vNr, KNr, FNr respectively, when there is no ambiguity.
In particular, let d0, L0, ℓ0, h0, t0, M0, v0, K0, F0 denote the corresponding
values of node S.
Without loss of generality, we assume the polyhedron K is full-dimensional,
that is, dim(K) = d and, thus, that the input system S has no implicit equa-
tions. Then, each call to the DarkShadow or GreyShadow procedures at level 1
reduces the dimension of the ambient space by one. Similarly, at every level, we
assume that the input system of inequalities of IntegerSolve0 (that is, the fourth
argument of this procedure) is full-dimensional. Hence at Line 23 of Algorithm
IntegerSolve0, the output of IntegerNormalize(Ax ≤b) is (∅, ∅, Ax ≤b).
This full-dimensionality assumption has two consequences. First, along any
path S →N1 →· · · →Nr we have dk+1 = dk −1, for 1 ≤k < r, and thus, we
have dk = d −k. Second, at node Nk, the input system is Mk−1tk−1 ≤vk−1
(while the output is Mktk ≤vk).
It is easy to see that this full-dimensionality assumption is a worst case
scenario as far as running time is concerned. Indeed, when this assumption does
not hold, for at least one path S →N1 →· · · →Nr, implicit equations will be
discovered at Line 23 of Algorithm IntegerSolve0 in the ﬁrst paper, and dimension
will drop by more than one at one node of that path.
To prove Theorem 1 we shall establish a series of intermediate results.
Lemmas 5, 7, 8 provide upper bounds for the absolute values of any coeﬃcient
in the systems MNtN ≤vN while Lemmas 3, 4, 9, 10 deal with running time
estimates. We start with Lemmas 3 and 4, which give running time estimates for
the DarkShadow and GreyShadow procedures at level k. The proof of Lemma 3
follows that of Proposition 4.
Lemma 3. For any non-negative integer k < d −1, the DarkShadow procedure
at level k + 1 runs within O( m2
k
4 LP(dk, dkhkmk)) bit operations.
Proof. The input system of Dk+1 is Mktk ≤vk, which has mk inequalities and
hk as maximum coeﬃcient size in either Mk or vk. The process of

250
R.-J. Jing and M. Moreno Maza
(E) eliminating the ﬁrst variable of tk in Mktk ≤vk,
(A) adding the at most m2
k
4
resulting inequalities to those of Mktk ≤vk where
the ﬁrst variable of tk does not appear, and
(R) removing all redundant inequalities,
yields Mktk ≤vk, see Algorithm DarkShadow. Observe that Steps (E) and (R)
amount to at most O( m2
k
4 dk h2
k) and O( m2
k
4
LP(dk, hk dk mk)) bit operations,
respectively. The latter dominates the former. The conclusion follows.
⊓⊔
Lemma 4. For any non-negative integer k < d −1, the GreyShadow procedure
at level k + 1 runs within O(m2
kd3+ε
k
h3
k) bit operations.
Proof. For the GreyShadow procedure at level k + 1, we need to call at most mk
times the IntegerNormalize procedure. Then, the lemma follows from Proposi-
tion 1 in the ﬁrst paper.
⊓⊔
Lemma 5. Consider a path of the form
S →D1 →· · · →Dr,
(5)
where all nodes, except the ﬁrst one, are labelled by D. Then, for all 1 ≤k ≤r,
we have mk ≤mk+1 and the maximum absolute value Lk of any coeﬃcient in
Mk is no more than (k + 1)
k+1
2 Lk+1.
Proof. Let 1 ≤k ≤r. Under Hypothesis 1, each facet of Kk is parallel to a
facet of the real shadow of Kk−1. Inductively, each facet of Kk is parallel to a
facet of the projection Πxk+1,...,xdK. By Proposition 3, we have mk ≤mk+1 and
Lk ≤(k + 1)
k+1
2 Lk+1.
⊓⊔
Next, we will consider an arbitrary path:
S →D1 →· · · →Dj1−1 →Gj1 →· · · →Gjs →Djs+1 →· · · →Dr.
(6)
In the path (6), only the subscripts j1, j2, . . . , js correspond to the GreyShadow
procedures.
To make things simpler, instead of setting Θ2 := Υ ∪Mt ≤v ∪{cα −aγ >
−(c −1)(a −1)} in Line 8 of Algorithm GreyShadow in the ﬁrst paper, we let
Θ2 := Mt ≤v. This simpliﬁcation cannot guarantee that VZ(t = Pkuk + qk ∪
Mkuk ≤vk, t) for k = 1, . . . , s form a disjoint union. However, it will endow
Mk with good structural properties, as we will see later. Actually, since all the
inequalities in Υ and the negation of cα −aγ > −(c −1)(a −1) can be obtained
by the DarkShadow procedure and since we are doing the worst case complexity
analysis, all the coming conclusions apply to our algorithm as it was originally
stated in the ﬁrst paper.
First, we consider the sub-path of (6): S →D1 →· · · →Dj1−1 →Gj1. We
assume the variable order is x1 > x2 > · · · > xd. Thus, we can denote the variable
set tj1−1 for the input system of node Dj1−1 as: tj1−1 = [xj1, xj1+1, . . . , xd]T
since tj1−1 ⊂x. For the node Gj1, we need to add one equation based on the

Computing the Integer Points of a Polyhedron, II: Complexity Estimates
251
output system Mj1−1tj1−1 ≤vj1−1 of node Dj1−1. Without loss of generality,
we assume the new equation is mtj1−1 = v + i for some non-negative integer
i ≤Lj1−1, where mtj1−1 ≤v is the ﬁrst inequality in the system Mj1−1tj1−1 ≤
vj1−1. Let I be the deﬁning index set of mtj1−1 ≤v, which has cardinality j1.
Recall that M0t0 ≤v0 is the input system of node D1. Let M(1)
0
and M(2)
0
be the sub-matrices of M0 consisting of the ﬁrst j1 −1 columns and the last
d −j1 + 1 columns, respectively. Denote by (v0)I (resp. (M0)I) the sub-vector
(resp. sub-matrix) of v0 (resp. M0) with index (resp. row index) I. Let Qj1−1
be a matrix whose columns consist of a Z-basis of the space {x : (M0)I x = 0 }.
We have assumed that the input polyhedron K is full-dimensional, which implies
that the rank of M0 is d. By the deﬁnition of deﬁning index set I, we can easily
deduce that the rank of (M0)I is j1, that is, Qj1−1 is an integer matrix with
d rows and d −j1 columns. Let Q′
j1−1 be the sub-matrix consisting of the last
d−j1+1 rows of Qj1−1. Let V1 := [e1, . . . , ej1−1,

0
Q′
j1−1

] ∈Zd×(d−1). Let S1 be
a node associated with the system M(1)
0 [x1, . . . , xj1−1] + M(2)
0 Q′
j1−1tj1+1 ≤v′
0,
i.e. M0V1[x1, . . . , xj1−1, tj1+1]T ≤v′
0. For j1 ≤k < j2, let M′
kt′
k ≤v′
k be the
output system of the node Dk−1 in the path: S1 →D1 →· · · →Dk−1.
Lemma 6. With the above notations, we have Mj1 = M′
j1. Consequently,
Mk = M′
k for j1 ≤k < j2.
Proof. The second statement will follow once the ﬁrst lemma is valid.
Following the algorithm DarkShadow in the ﬁrst paper, there exists a matrix
U
∈Zmj1−1×m0, such that UM(1)
0
= 0 and Mj1−1 = UdUM(2)
0 , where
Ud = DiagonalMatrix(
1
gcd1 , . . . ,
1
gcdmj1−1 ) and gcdi is the gcd of all the coef-
ﬁcients in the i-th row of UM(2)
0
for 1 ≤i ≤mj1−1. Let u ∈Zm0 be the
ﬁrst row of U. Then, m =
1
gcd1 uM(2)
0
since mtj1−1 ≤v is the ﬁrst inequal-
ity of Mj1−1tj1−1 ≤vj1. Then, m =
1
gcd1 uI(M(2)
0 )I. Solving the equation
mtj1−1 = v+i by Lemma 2 in the ﬁrst paper, we have tj1−1 = Pj1−1 tj1 + qj1−1,
where Pj1−1 ∈Z(d−j1+1)×(d−j1) whose columns consist of a Z-basis for {y :
my = 0} = {y : uI(M(2)
0 )Iy = 0}. Therefore, Mj1tj1 ≤vj1 comes from
Mj1−1Pj1−1tj1
≤vj1−1 −Mj1−1qj1−1, i.e. UdUM(2)
0 Pj1−1tj1
≤vj1−1 −
Mj1−1qj1−1.
Next, we will show that Pj1−1 can be replaced by Q′
j1−1 introduced above.
Since uI(M(1)
0 )I = 0, we have any y ∈Zd satisfying uI(M0)Iy = 0 is equivalent
to uI(M(2)
0 )Iy(2) = 0, where y(2) is the last d −j1 + 1 elements of y. Thus,
[e1, . . . , ej1−1,

0
Pj1−1

] is a Z-basis for the space {y : u(M0)Iy = 0}. For any row
vector y ∈Zd such that uI(M0)Iy = 0, either 0 ̸= (M0)Iy ∈{ z : uIz = 0 }
or (M0)Iy = 0. For the ﬁrst case, e1, . . . , ej1−1 is a Z-basis for the solutions
of y, where ek ∈Zd is the k-th standard basis for 1 ≤k ≤j1 −1. For the
second case, columns of Qj1−1 consisting of a Z-basis for the solutions of y.
Thus, [e1, . . . , ej1−1, Qj1−1] is a Z-basis for the space {y : uI(M0)Iy = 0}.
Consequently, Pj1−1 is equivalent to Q′
j1−1, which is the last d −j1 + 1 rows of

252
R.-J. Jing and M. Moreno Maza
Qj1−1. That is, the integer solutions to m tj1−1 = v + i can be represented by
tj1−1 = Q′
j1−1 tj1 + qj1−1, where |Q′
j1−1| ≤jj1+1
1
L2j1. Therefore, we can make
Mj1 = UdUM(2)
0 Q′
j1−1.
Remember that S1 is associated with the system M0V1[x1, . . . , xj1−1, tj1+1]T
≤v0, where V1 := [e1, . . . , ej1−1,

0
Q′
j1−1

], and M′
kt′
k ≤v′
k is the output system
of the node Dk−1 in the path: S1 →D1 →· · · →Dk−1. We have M′
j1 =
UdUM(2)
0 Q′
j1. Consequently, M′
k = Mk for any integer k : j1 ≤k < j2.
⊓⊔
Then, we have the following lemma:
Lemma 7. For j1 ≤k < j2, the maximum absolute value of any coeﬃcient in
Mk can be bounded over by dkk2k2L3k2. Moreover, we have mk ≤mk+1.
Proof. By Lemma 5, the maximum absolute value of any coeﬃcient in M′
k = Mk
can be bounded over by |Mk| ≤k
k
2 (d −j1 + 1)kjkj1+k
1
L2kj1+k ≤dkk2k2L3k2.
Moreover, mk ≤mk+1 follows from the equivalent path S1 →D1 →· · · →
Dk−1 for integer k : j1 ≤k < j2.
⊓⊔
For any 1 ≤t ≤s, we assume that the new equation is mttjt−1 = vt + it
for some non-negative integer it ≤Ljt−1, where mttjt−1 ≤vt comes from the
input system Mjt−1tjt−1 ≤vjt−1 of the node Gjt. Let It be the deﬁning index
set of the inequality mttjt−1 ≤vt, with cardinality jt. Let Qt ∈Zd×(d−jt)
consist of the columns of a Z-basis of space {y : (M0)Ity = 0}. For any 1 ≤
t ≤s, we deﬁne Vt = [e1, . . . , ej1−1, Q(1)
1 , . . . , Q(t−1)
t−1 , Q(t)
t ] ∈Zd×(d−t) and
tt := [x1, . . . , xj1, t(1)
j1−1, . . . , t(t)
jt−1]T as follows:
1. When k < t, we let Q′
k be the sub-matrix consisting of the last d −jk + 1
rows and (jk+1 −jk −1) columns of Qk. Let Q(k)
k
∈Zd×(jk+1−jk−1) be the
matrix
 0
Q′
k

, where 0 is a zero matrix which has jk −1 rows and jk+1 −jk −1
columns.
2. When k = t, we let Q′
t be the sub-matrix consisting of the last d −jt + 1
rows of Qt. Let Q(t)
t
∈Zd×(d−jt) be the matrix
 0
Q′
t

, where 0 is a zero matrix
which has jt −1 rows and d −jt columns.
3. Denote by t(k)
jk−1 (resp. tjt−1) the set of jk+1 −jk −1 (resp. d −jt) variables.
and the variables in tt are independent variables.
Let St be the system represented by M0Vttt ≤v0 for 1 ≤t ≤s.
Lemma 8. The maximum absolute value of any coeﬃcient in |Mk| (resp. |vk|)
can be bounded by dkk2k2L3k2 (resp. d3k2k4k3L6k3) for 1 ≤k ≤r. Moreover, we
have mk ≤mk+1.
Proof. Similar to the notations deﬁned before Lemma 6, for 1 ≤t ≤s and
jt ≤k < jt+1, let M′
kt′
k ≤v′
k be the output system of the path: St →D1 →
· · · →Dk−t, where js+1 is deﬁned as r + 1.

Computing the Integer Points of a Polyhedron, II: Complexity Estimates
253
We claim Mjt = M′
jt for any 1 ≤t ≤s, where Mjttjt ≤vjt is the output
system of the path: S →D1 →· · · →Dj1−1 →Gj1 →· · · →Gjt. This claim
is valid if t = 1 by Lemma 6. We suppose it is valid for t = 1, . . . , s −1. Then,
we have Mjs−1 = M′
js−1, where M′
js−1t′
js−1 ≤v′
js−1 is the output system of
the node Djs−s in the path: Ss−1 →D1 →· · · →Djs−s. Let M′′
jst′′
js ≤v′′
js be
the output system of node Gjs−s+1 in the path: Ss−1 →D1 →· · · →Djs−s →
Gjs−s+1. Note that Ss−1 is associated with M0Vs−1ts−1 ≤v0 and the input
system of node Gjs−s+1 is M′
js−1t′
js−1 ≤v′
js−1, i.e. Mjs−1t′
js−1 ≤v′
js−1. We
have M′′
js = Mjs immediately, since both of them come from the output system
of node Gjs−s+1 of the path: Ss−1 →D1 →· · · →Djs−s →Gjs−s+1. By the
proof of Lemma 6, M′′
js can be obtained from the output system of the path: S′
s →
D1 →· · · →Djs−s, where S′
s is associated with M0[Vs−1[e1, . . . , ejs−s], Qs]ts ≤
v0, i.e. M0Vsts ≤v0, which associates to the label Ss. Then, we have M′′
js =
M′
js. The claim is valid. That is, Mjs can be obtained from the output system
of the path: Ss →D1 →· · · →Djs−s.
By Proposition 1 of the ﬁrst paper, we know that the maximum absolute value
of any coeﬃcient in M0Vt can be bounded by djjt+1
t
L2jt+1. Thus, by Lemma 5,
for any 1 ≤t ≤s and jt ≤k < jt+1, we have the maximum absolute value of any
coeﬃcient in Mk can be bounded by (k −t)
k−t
2 (djjt+1
t
L2jt+1)k−t ≤dkk2k2L3k2.
The ﬁrst statement is valid.
Let 1 ≤k ≤r. For the node Dk, we have |vk| ≤L2
k−1 + 2Lk−1|vk−1|. For
the node Gk, we have |vk| ≤2dkL2
k−1|vk−1| since we only need to solve one
equation. That is, for any node Nk, we will have |vk| ≤2dkL2
k−1|vk−1|. Thus,
|vk| ≤2kdkL2
k−1 · · · L2
1|v0|2 ≤d3k2k4k3L6k3 for any 1 ≤k ≤r.
⊓⊔
Until now, we can safely say that any coeﬃcient in Mr (resp.vr) produced
by each path in Fig. 1 can be bounded over by Lr ≤drr2r2L3r2 (resp. ℓr ≤
d3r2r4r3L6r3). That is, the coeﬃcient size associated with the node Nr can be
bounded over by hr ≤6r3(log d + log L). Moreover, we can have at most mr
inequalities in Mrtr ≤vr. The following lemma shows the complexity estimates
for implementing each path of the tree in Fig. 1:
Lemma 9. The path (6) can be implemented within O(m2r+2d3+εr10(log d +
log L)3) + O(rm2r+2LP(d, dmrr3(log d + log L))) bit operations.
Proof. By Lemma 3 (resp. Lemma 4), each node Dk (resp. Gk) can be imple-
mented with O( m2
k
4 LP(dk, dkhkmk)) (resp. O(m2
kd3+ε
k
h3
k)) bit operations. Thus,
the path (6) can be implemented within
r · O(m2
rd3+ε
r
h3
r) + r · O(m2
r
4 LP(d, dhrmr))
≤O(m2r+2d3+εr10(log d + log L)3) + O(rm2r+2LP(d, dm2r+2r3(log d + log L)))
bit operations.
⊓⊔
Let Tr be the total number of nodes in the r-th level. In particular, we have
T0 = 1, T1 ≤mL. We have the following lemma:

254
R.-J. Jing and M. Moreno Maza
Lemma 10. We have: Tr+1 ≤mr+1drr2r2L3r2Tr for r = 0, . . . , d−2. Thus, we
have Td−1 ≤md2d3d3L3d3.
Proof. By Lemma 8, each node can have at most mr+1 inequalities as the
input and each inequality has coeﬃcient bound Lr. Following the Algorithm
IntegerSolve0 and Fig. 1, each node can give out at most mr+1Lr branches.
Considering we have Tr nodes in the r-th level, we can easily deduce that
Tr+1 ≤mr+1LrTr ≤mr+1drr2r2L3r2Tr. The second statement follows easily.
⊓⊔
Now we give the proof for Theorem 1:
Proof. Under Hypothesis 1, by Lemmas 9 and 10, the complexity estimates for
IntegerSolve(K) can be bounded over
Td−1O(m2r+2d3+εr10(log d + log L)3)+
Td−1O(m2r+2rLP(d, dmr+1r3(log d + log L)))
≤O(m2d2d4d3L4d3LP(d, mdd4(log d + log L))) bit operations, since r < d.
The theorem is valid.
⊓⊔
5
Experimentation
We have implemented the algorithm presented in the ﬁrst paper within the
Polyhedra library in Maple. This library is publicly available in source on the
download page of the RegularChains library at www.regularchains.org.
We have used test-cases coming from various application areas: regular poly-
topes (ﬁrst 5 examples in Table 1), examples from Presburger arithmetic (next
5 examples in Table 1), random polytopes (next 5 examples in Table 1), random
unbounded polyhedra (next 5 examples in Table 1), examples from text-books
(next 3 examples in Table 1) and examples from research articles on automatic
parallelization of for-loop nests (last 4 examples in Table 1).
For each example, Table 1 gives the number of deﬁning inequalities (Column
m), the number of variables (Column d), the maximum absolute value of an
input coeﬃcient (Column L), the number of polyhedra returned by IntegerSolve
(Column mo), the maximum absolute value of an output coeﬃcient (Column
Lo) and whether Hypothesis 1 holds or not (Column ?Hyp).
Recall from Sect. 4.1 of the ﬁrst paper that Step (S4) of the IntegerNormalize
procedure can use either the HNF method introduced in Lemma 2 of the ﬁrst
paper, or the method introduced by Pugh in [9]. We implemented both of them.
It is important to observe that Pugh’s method does not solve systems of linear
equations according to our prescribed variable order, in contrast to the HNF
method. In fact, Pugh’s method determines a variable order dynamically, based
on coeﬃcient size considerations. In Table 1, the columns tH and tP correspond
to the timings for the HNF and Pugh’s method, respectively.

Computing the Integer Points of a Polyhedron, II: Complexity Estimates
255
Table 1. Implementation
Example
m
d L
mo
Lo
?Hyp tH
tP
Tetrahedron
4
3
1
1
1
Yes
0.695
0.697
Cuboctahedron
14
3
2
1
2
Yes
1.855
1.846
Octahedron
8
3
1
1
1
Yes
1.357
1.357
TruncatedOctahedr.
14
3
3
1
1
Yes
1.995
1.977
TruncatedTetrahedr.
8
3
1
1
1
Yes
1.461
1.468
Presburger 1
3
2
2
1
1
Yes
0.083
0.082
Presburger 2
3
2
20
1
20
Yes
0.184
0.182
Presburger 3
3
2
18
3
4
Yes
0.287
0.260
Presburger 4
3
4
5
2
12
Yes
0.706
0.871
Presburger 6
4
5
89
6
35
Yes
0.893
0.746
Bounded 5
6
3
19
4
224
Yes
16.433
15.091
Bounded 7
8
3
19
3
190
No
138.448 239.637
Bounded 8
4
3
25
5
67
Yes
6.462
3.821
Bounded 9
6
3
18
6
74
No
23.574
16.763
Bounded 10
4
3
15
1
176
Yes
0.559
0.558
Unbounded 2
3
4
10
61
2255
No
0.547
0.600
Unbounded 3
4
4
20
1
20
No
0.981
0.987
Unbounded 4
6
5
2
1
2
No
0.722
0.510
Unbounded 5
5
4
8
1
8
No
1.321
1.319
Unbounded 6
10
4
8
1
8
No
1.494
1.479
P91
12
3
96
5
96
No
19.318
15.458
Sys1
6
3
15
2
67
Yes
2.413
1.915
Sys3
8
3
1
1
1
Yes
1.481
1.479
Automatic
8
2 999
1
999
Yes
0.552
0.549
Automatic2
6
4
1
1
2
Yes
1.115
1.113
Automatic3
3
4
1
1
1
Yes
0.130
0.135
Automatic4
3
5
1
1
1
Yes
0.227
0.232
From Table 1, we make a few observations:
1. Hypothesis 1 holds for most examples while it usually does not hold for ran-
dom ones.
2. For 16 out of 27 examples, IntegerSolve produces a single component, which
means that each such input polyhedron has no integer points in its grey
shadow; this is, in particular, the case for regular polytopes and for examples
from automatic parallelization.
3. When a decomposition consists of more than one component, most of those
components are points; for example, the decomposition of Unbounded 2 has
61 components and 46 of them are points.

256
R.-J. Jing and M. Moreno Maza
4. Coeﬃcients of the output polyhedra are usually not much larger than the
coeﬃcients of the corresponding input polyhedron.
5. Among the challenging problems, some of them are solved faster when Inte-
gerNormalize is based on HNF (e.g. Bounded 7) while others are solved faster
when IntegerNormalize is based on Pugh’s method (e.g. Bounded 9) which
suggests that having both approaches at hand is useful.
To the best of our knowledge, there are two other published software libraries
which are capable of describing the integer points of a polyhedron: one is 4ti2 [1]
and the other is Normaliz [3]. Both softwares rely on Motzkin’s theorem [8] which
expresses any rational polyhedron as the Minkowski sum of a rational polytope
and a rational cone. Hence, they do not decompose a polyhedron in the sense of
our algorithm IntegerSolve.
Acknowledgements. The authors would like to thank IBM Canada Ltd (CAS
project 880) and NSERC of Canada (CRD grant CRDPJ500717-16), as well as the
University of Chinese Academy of Sciences, UCAS Joint PhD Training Program, for
supporting their work.
References
1. 4ti2 team. 4ti2–a software package for algebraic, geometric and combinatorial prob-
lems on linear spaces. www.4ti2.de
2. Aubry, P., Lazard, D., Moreno Maza, M.: On the theories of triangular sets. J.
Symb. Comput. 28, 105–124 (1999)
3. Bruns, W., Ichim, B., R¨omer, T., Sieg, R., S¨oger, C.: Normaliz. Algorithms for
rational cones and aﬃne monoids. https://www.normaliz.uni-osnabrueck.de
4. Chen, C., Davenport, J.H., May, J.P., Moreno Maza, M., Xia, B., Xiao, R.: Trian-
gular decomposition of semi-algebraic systems. J. Symb. Comput. 49, 3–26 (2013)
5. Imbert, J.-L.: Fourier’s elimination: which to choose? pp. 117–129 (1993)
6. Karmarkar, N.: A new polynomial-time algorithm for linear programming. In: Pro-
ceedings of the Sixteenth Annual ACM Aymposium on Theory of Computing.
STOC 1984, pp. 302–311. ACM, New York, NY, USA (1984)
7. Khachiyan, L.: Fourier-motzkin elimination method. In: Floudas, C.A., Pardalos,
P.M. (eds.) Encyclopedia of Optimization, pp. 1074–1077. Springer, Heidelberg
(2009). doi:10.1007/978-0-387-74759-0 187
8. Motzkin, T.S.: Beitr¨age zur Theorie der linearen Ungleichungen. Azriel Press,
Jerusalem (1936)
9. Pugh, W.: The omega test: a fast and practical integer programming algorithm
for dependence analysis. In: Martin, J.L. (ed.), Proceedings Supercomputing 1991,
Albuquerque, NM, USA, 18–22 November 1991, pp. 4–13. ACM (1991)
10. Schrijver, A.: Theory of Linear and Integer Programming. Wiley, New York (1986)

Non-linearity and Non-convexity in Optimal
Knots Selection for Sparse Reduced Data
Ryszard Kozera1,2(B) and Lyle Noakes3
1 Faculty of Applied Informatics and Mathematics,
Warsaw University of Life Sciences-SGGW,
Nowoursynowska Str. 159, 02-776 Warsaw, Poland
ryszard.kozera@gmail.com
2 School of Computer Science and Software Engineering,
The University of Western Australia, 35 Stirling Highway, Crawley,
Perth, WA 6009, Australia
3 School of Mathematics and Statistics, The University of Western Australia,
35 Stirling Highway, Crawley, Perth, WA 6009, Australia
lyle.noakes@uwa.edu.au
Abstract. The problem of ﬁtting sparse reduced data in arbitrary
Euclidean space is discussed in this work. In our setting, the unknown
interpolation knots are determined upon solving the corresponding opti-
mization task. This paper outlines the non-linearity and non-convexity of
the resulting optimization problem and illustrates the latter in examples.
Symbolic computation within Mathematica software is used to generate
the relevant optimization scheme for estimating the missing interpolation
knots. Experiments conﬁrm the theoretical input of this work and enable
numerical comparisons (again with the aid of Mathematica) between var-
ious schemes used in the optimization step. Modelling and/or ﬁtting
reduced sparse data constitutes a common problem in natural sciences
(e.g. biology) and engineering (e.g. computer graphics).
Keywords: Reduced sparse data · Optimization · Interpolation · Knots
selection · Symbolic computation
1
Problem Formulation
A sequence of interpolation points M = {x0, x1, x2, . . . , xn} (here n ≥2) in
Euclidean space Em is called reduced data if the corresponding interpolation
knots {ti}n
i=0 are not given (see e.g. [6,10,12,13,20,23,25,29,30]). Let the class
of admissible curves γ (denoted by IT ) form the set of piece-wise C2 curves
γ : [0, T] →Em interpolating M with the ordered free unknown admissible
knots {ti}n
i=0 satisfying γ(ti) = xi. Here ti < ti+1 are free with, upon re-scaling
t0 = 0 and tn = T set to an arbitrary constant T > 0. More precisely, for each
choice of ordered knots, the curve γ is assumed to be C2 except of being only at
least C1 over {ti}n
i=0. The analysis to follow is not restricted to a thinner class
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 257–271, 2017.
DOI: 10.1007/978-3-319-66320-3 19

258
R. Kozera and L. Noakes
of γ ∈C2([t0, tn]) due to the ultimate choice of computational scheme (called
herein Leap-Frog - see [18,24,27,28]) which eﬀectively deals with the optimiza-
tion problem (1). However, the computed optimum by Leap-Frog belongs to the
tighter class of functions coinciding with C2([t0, tn]) as addressed in [17,18].
Assume now, we search for an optimal γopt ∈IT to minimize:
JT (γ) =
 T
t0
∥¨γ(t)∥2dt =
n−1

i=0
 ti+1
ti
∥¨γ(t)∥2dt.
(1)
The latter deﬁnes an inﬁnite dimensional optimization task over IT . The
unknown interpolation knots {ti}n
i=0 (t0 = 0 and 0 < tn = T can be ﬁxed)
belong to:
ΩT
t0 = {(t1, t2, . . . , tn−1) ∈Rn−1 : t0 = 0 < t1 < t2 < . . . < tn−1 < tn = T < ∞}. (2)
For any aﬃne reparameterization φ : [0, T] →[0, ˜T] deﬁned as φ(t) = t ˜T/T
(with t = φ−1(s) = sT/ ˜T) φ−1′ ≡T/ ˜T and φ−1′′ ≡0, formula (1), for ˜γ(s) =
(γ ◦φ−1)(s) reads:
J ˜T (˜γ) =
n−1

i=0
 si+1
si
∥¨˜γ(s)∥2ds = T 3
˜T 3
n−1

i=0
 ˜ti+1
˜ti
φ−1′(s)∥(¨γ ◦φ−1)(s)∥2ds
= T 3
˜T 3 JT (γ).
(3)
Thus, a curve γopt ∈IT is optimal to JT if and only if a corresponding ˜γopt ∈
I ˜T is optimal for J ˜T . Hence tn = T can be taken as arbitrary, and with the
additional aﬃne mapping φ(t) = t −t0, one can also set t0 = 0.
Recall now a cubic spline interpolant γCi
T = γC
T |[ti,ti+1] (see e.g. [3]), which for
given temporarily ﬁxed admissible interpolation knots T = (t0, t1, . . . , tn−1, tn)
reads as:
γCi
T (t) = c1,i + c2,i(t −ti) + c3,i(t −ti)2 + c4,i(t −ti)3,
(4)
and fulﬁlls (for i = 0, 1, 2, . . . , n −1; cj,i ∈Rm, where j = 1, 2, 3, 4)
γCi
T (ti+k) = xi+k,
˙γCi
T (ti+k) = vi+k,
k = 0, 1
with the assumed unknown velocities v0, v1, v2, . . . , vn−1, vn ∈Rm. The coeﬃ-
cients cj,i (with Δti = ti+1 −ti) are deﬁned as follows:
c1,i = xi,
c2,i = vi,
c4,i =
vi + vi+1 −2 xi+1−xi
Δti
(Δti)2
,
c3,i =
(xi+1−xi)
Δti
−vi
Δti
−c4,iΔti.
(5)
Adding n −1 conditions ¨γCi−1
T
(ti) = ¨γCi
T (ti) over x1, x2, . . . , xn−1 yields m tridi-
agonal linear systems (see [3]) of n −1 equations in n + 1 vector unknowns

Non-linearity and Non-convexity for Sparse Reduced Data
259
v0, v1, . . . , vn ∈Rm:
vi−1Δti + 2vi(Δti−1 + Δti) + vi+1Δti−1 = bi,
bi = 3

Δti
xi −xi−1
Δti−1
+ Δti−1
xi+1 −xi
Δti

.
(6)
In case of the so-called natural cubic spline interpolant (denoted as γC
T = γNS
T ),
two extra constraints involving v0 and vn stipulate that ¨γC
T (0) = ¨γC
T (T) = 0
which leads to:
2v0 + v1 = 3x1 −x0
Δt0
,
vn−1 + 2vn = 3xn −xn−1
Δtn−1
.
(7)
The resulting m linear systems (i.e. (6) and (7)), each of size (n + 1) × (n + 1),
determine unique vectors v0, v1, v2, . . . , vn (see [3, Chap. 4]), which when fed into
(5) and then passed to (4) determine explicitly a natural cubic spline γNS
T
(with
ﬁxed T ). Visibly all computed velocities {vi}m
i=0 (and, thus, γNS
T ) with the aid
of the above procedure depend in fact on the interpolation knots {ti}m
i=0 and
ﬁxed data M . It is well known (see e.g. [3]) that if the respective knots {ti}m
i=0
are frozen the optimization task (1) is minimized by a unique natural spline γNS
T
deﬁned by {ti}n
i=0 and M . Therefore, upon relaxing all internal knots {ti}n−1
i=1
in (1) (for arbitrarily ﬁxed terminal knots to e.g. t0 = 0 and tn = T) one arrives
at the following (see [3,17–19]):
Theorem 1. For a given M with points in Euclidean space Em, the subclass of
natural splines I NS ⊂IT satisﬁes
min
γ∈IT JT (γ) =
min
γNS∈I NS JT (γNS),
(8)
which reduces to the ﬁnite dimensional optimization in
ˆ
J = (t1, t2, . . . , tn−1)
over non-compact ΩT
t0 introduced in (2):
JT (γNS
opt ) = min
ˆ
T ∈ΩT
t0
J F
T (t1, t2, . . . , tn−1)
=
min
ˆ
T ∈ΩTc
t0
4
n−1

i=0

−1
(Δti)3 (−3∥xi+1 −xi∥2 + 3⟨vi + vi+1|xi+1 −xi⟩Δti
−(∥vi∥2 + ∥vi+1∥2 + ⟨vi|vi+1⟩)(Δti)2
,
(9)
for which at least one global minimum
ˆ
Jopt = (topt
1 , topt
2 , . . . , topt
n−1) ∈ΩT
t0 exists.
We take here the computed optimal values of
ˆ
Jopt, as estimates {ˆti}m
i=0 ≈
{ti}m
i=0. In this paper, we demonstrate strong non-linearity and non-convexity
eﬀects built-in the optimization scheme (9). The relevant examples and the-
oretical insight is supplemented to justify the latter. Suﬃcient conditions for
convexity (or unimodality) of (9) are proved at least for n = 2. The complexity

260
R. Kozera and L. Noakes
of the optimization scheme (9) not only impedes its theoretical analysis but also
impacts on the choice of feasible numerical scheme handling computationally
(9). Finally, this work is supplemented with illustrative examples and numerical
tests used to ﬁt input sparse reduced data M for various n and m = 2, 3.
Related work on ﬁtting reduced data M (sparse or dense) can also be found in
[8,9,15,16,21,22,26,33,34]. Some applications in computer vision and graphics,
image processing, engineering, physics, and astronomy are discussed e.g. in [1,2,
5,7,11,21,31,32].
2
Non-Linearity of J F
T and Numerical Diﬃculties
First we demonstrate a high non-linearity featuring the optimization task (9).
This is accomplished by generating an explicit formula for (9) whose complexity
is substantial even for n small and gets complicated for n incremented. The latter
is illustrated by the next two examples followed by pertinent computational tests.
Example 1. Consider four data points (i.e. here n = 3) M = {x0, x1, x2, x3} in
Em. Formula for J F
T (see (9)) reads here as J F,3
Tc ( ˆ
T ) = J 3
0 +J 3
1 +J 3
2 (with
ˆ
T = (t0, t1, t2, t3) and t0 = 0 and e.g. t3 = T = Tc - see (12)), where
J 3
0 =
1
(t0 −t1)3 (−3∥x0∥2 −3∥x1∥2 + (t0 −t1)(3⟨v0|x0⟩−3⟨v0|x1⟩+ 3⟨v1|x0⟩
−3⟨v1|x1⟩+ (∥v0∥2 + ∥v1∥2 + ⟨v0|v1⟩)(t1 −t0)) + 6⟨x0|x1⟩),
J 3
1 =
1
(t1 −t2)3 (−3∥x1∥2 −3∥x2∥2 + (t1 −t2)(3⟨v1|x1⟩−3⟨v1|x2⟩+ 3⟨v2|x1⟩
−3⟨v2|x2⟩+ (∥v1∥2 + ∥v2∥2 + ⟨v1|v2⟩)(t2 −t1)) + 6⟨x1|x2⟩),
J 3
2 =
1
(t2 −t3)3 (−3∥x2∥2 −3∥x3∥2 + (t2 −t3)(3⟨v2|x2⟩−3⟨v2|x3⟩+ 3⟨v3|x2⟩
−3⟨v3|x3⟩+ (∥v2∥2 + ∥v3∥2 + ⟨v2|v3⟩)(t3 −t2)) + 6⟨x2|x3⟩).
(10)
The missing velocities {v0, v1, v2, v3} for natural spline γNS
T
(see (4)) are deter-
mined here by the following four matrix equations, with i = 1, . . . , m (see (6)
and (7))
⎛
⎜
⎜
⎝
2
1
0
0
t2 −t1 2(t2 −t0)
t1 −t0
0
0
t3 −t2
2(t3 −t1) t2 −t1
0
0
1
2
⎞
⎟
⎟
⎠
⎛
⎜
⎜
⎝
vi
0
vi
1
vi
2
vi
3
⎞
⎟
⎟
⎠=
⎛
⎜
⎜
⎜
⎜
⎜
⎝
3
xi
1−xi
0
t1−t0
3(
(t2−t1)(xi
1−xi
0)
t1−t0
+
(t1−t0)(xi
2−xi
1)
t2−t1
)
3(
(t3−t2)(xi
2−xi
1)
t2−t1
+
(t2−t1)(xi
3−xi
2)
t3−t2
)
3
xi
3−xi
2
t3−t2
⎞
⎟
⎟
⎟
⎟
⎟
⎠
yielding (with the aid of symbolic computation in Mathematica - see [35]) a
unique solution. For the sake of this example, we consider exclusively the case
of m = 1. This can be easily extended to m > 1, since both square of norms and
dot products (appearing in non-reduced form of J F
Tc( ˆ
T )) are additive by each
vector component. Upon substituting computed velocities from the last matrix

Non-linearity and Non-convexity for Sparse Reduced Data
261
equations into J F,3
Tc
(as previously we set t0 = 0 and t3 = Tc - see (12)) and
taking into account that m = 1, Mathematica FullSimplify (see [35]) function
yields an explicit formula for
J F,3
Tc (t1, t2) = N3(t1, t2)/(t2
1(t1 −t2)2(t2 −Tc)2((t1 + t2)2 −4Tct2)),
where
N3(t1, t2) = (3(−T 3
c t2
2(x0 −x1)2 + 2T 2
c t3
2(x0 −x1)2 + Tct4
2(x0 −x1)(x1 −x0)
+t3
1(−Tc(x0 + x1 −2x2) + t2(x0 + x1 −2x3))(Tc(x2 −x0) + t2(x0 −x3))
−t2
1(T 3
c (x0 −x2)2 −3Tct2
2(x0 −x2)(x0 −x3)
+t3
2(x0 −x3)(2x0 −x2 −x3)) + t1(2T 3
c t2(x0 −x1)(x0 −x2)
−3T 2
c t2
2(x0 −x1)(x0 −x2) + t4
2(x0 −x1)(x0 −x3)
−Tct3
2(x0 −x1)(x2 −x3)) −t4
1(Tc(x2 −x0) + t2(x0 −x3))(x2 −x3))).
Note that N3(t1, t2) is a 5th order polynomial in t1 and t2.
□
Example 2. Let ﬁve data points (i.e. here n = 4) M = {x0, x1, x2, x3, x4} be
given in Em. Formula (9) reads here J F,4
Tc ( ˆ
T ) = J 4
0 + J 4
1 + J 4
2 + J 4
3 (for
ˆ
T = (t0, t1, t2, t3, t4) with t0 = 0 and t4 = Tc - see (12)), where J 4
k = J 3
k , for
k = 0, 1, 2 (see (10)) and
J 4
3 =
1
(t3 −t4)3 (−3∥x3∥2 −3∥x4∥2 + (t3 −t4)(3⟨v4|x3⟩−3⟨v3|x4⟩+ 3⟨v4|x3⟩
−3⟨v4|x4⟩+ (∥v3∥2 + ∥v4∥2 + ⟨v3|v4⟩)(t4 −t3)) + 6⟨x3|x4⟩).
Again the missing velocities {v0, v1, v2, v3, v4} for the natural spline γNS
T
deﬁned
by (4) are determined here by ﬁve matrix equations, with i = 1, . . . , m (see (6)
and (7)):
⎛
⎜
⎜
⎜
⎜
⎝
2
1
0
0
0
t2 −t1 2(t2 −t0)
t1 −t0
0
0
0
t3 −t2
2(t3 −t1)
t2 −t1
0
0
0
t4 −t3
2(t4 −t2) t3 −t2
0
0
0
1
2
⎞
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎜
⎝
vi
0
vi
1
vi
2
vi
3
vi
4
⎞
⎟
⎟
⎟
⎟
⎠
= Bi,
(11)
where
Bi =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
3 xi
1−xi
0
t1−t0
3( (t2−t1)(xi
1−xi
0)
t1−t0
+ (t1−t0)(xi
2−xi
1)
t2−t1
)
3( (t3−t2)(xi
2−xi
1)
t2−t1
+ (t2−t1)(xi
3−xi
2)
t3−t2
)
3( (t4−t3)(xi
3−xi
2)
t3−t2
+ (t3−t2)(xi
4−xi
3)
t4−t3
)
3 xi
4−xi
3
t4−t3
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Again the system (11) renders a unique solution {v0, v1, v2, v3, v4} (found e.g.
upon using Mathematica software - see [35]). As previously only the case of m = 1

262
R. Kozera and L. Noakes
is here considered. Upon substituting computed velocities from (11) into J F,4
Tc
and setting t0 = 0 and t4 = Tc (see (12)) Mathematica FullSimplify function
yields an explicit formula for J F,4
Tc (t1, t2, t3) =
N4(t1, t2, t3)
4t2
1(t1 −t2)2(t2 −t3)2(t3 −Tc)2(t2(t3 −t1)(t1 + 2t2 + t3) + Tc((t1 + t2)2 −4t2t3)).
It can be checked that N4(t1, t2, t3) is an 8th order polynomial in t1, t2 and
t3. We omit here to present a full explicit formula for N4(t1, t2, t3) since it takes
more than one A4 format page size.
□
Examples 1 and 2 indicate the growing complexity of the non-linearity in (9)
while n increases. Thus, in a search for global minimum of (9), any numerical
optimization scheme relying on derivative computation (irrespectively of an ini-
tial guess) faces the computational diﬃculties for n getting bigger. The latter
is demonstrated in the next Example 3 for n = 7, where Mathematica Find-
Minimum applied with Newton Method fails (see [35]). Similar eﬀects appear
when Mathematica Minimize[f,constraints,variables] is invoked (see [35]) which
works eﬃciently for both minimized function and imposed constraints (such as
inequality or equations) expressed as polynomials. The latter happens with the
numerator of the derivative of (9). To alleviate this problem and to eﬃciently
optimize (9) we invoke ﬁrst a multidimensional version of the Secant Method (not
relying on derivative computation) given in Mathematica software e.g. for two
free variables as FindMinimum[f, {{var1, 2num1}, {var, 2num2}}] - see [35].
Its super-linear convergence order (e.g. for m = 1 equal to ((1+
√
5)/2) ≈1.618)
though slower than Newton quadratic rate, makes it still both faster and com-
putationally feasible as opposed to most standard optimization techniques based
on derivative calculation. In the last section of this paper, we compare the Secant
Method with a Leap-Frog Algorithm (see [18,27,28])). One of the advantages of
Leap-Frog over the Secant Method is a faster execution time (see also [17,18]).
In order to set up a computationally feasible numerical optimization scheme
a good initial guess is needed. In particular, for the Secant Method for each free
variable, two numbers are needed to be selected. A possible choice is the so-called
cumulative chord Tc = {tc
i}n
i=0 (see e.g. [13,14,20,25]):
tc
0 = 0,
tc
i+1 = tc
i + ∥xi+1 −xi∥,
i = 0, . . . , n −1,
(12)
with T c = n−1
i=0 ∥xi+1−xi∥. Cumulative chord parameterization in a normalized
form Tcc reads tcc
i
= tc
i/T c (for i = 0, 1, . . . , n). Here an additional assumption
about reduced data M i.e. xi ̸= xi+1 is also drawn. For the Secant Method and
each free knot ti appearing in (9) (here i = 1, 2, . . . , n−1) we choose two starting
numbers as tc
i −ε and tc
i + ε, with some prescribed small value for ε.
The next example illustrates expected computational diﬃculties in optimiz-
ing (9).
Example 3. (a) Consider four 2D reduced data points (i.e. here n = 3):
M3 = {(−4, 0), (−0.5, −4), (0.5, −3), (−0.5, 4)}.

Non-linearity and Non-convexity for Sparse Reduced Data
263
The cumulative chord knots (see (12)) based on M3 coincide with Tc =
{0, 5.31507, 6.72929, 13.8004}. In fact, here we have only two free variables
{t1, t2} corresponding to the unknown knots at points xi (i = 1, 2). Find-
Minimum (Newton Method) applied to (9) with the initial guess as cumula-
tive chords Tc yields the following optimal knots (with optimal energy value
J F
Tc( ˆ
Topt) = 0.741614):
Topt = {0, 5.3834, 8.2118, 13.8004},
(13)
where
ˆ
Topt = {5.3834, 8.2118}. The execution time T N
M3 = 3.012858 s. Find-
Minimum for the Secant Method (with two numbers associated to each free
variables taken as ε = ±0.5 variation of (12)) gives exactly the same optimal
knots (13) (with the same J F
Tc( ˆ
Topt) = 0.741614) but shorter execution time
T S
M3 = 0.647756 s. Finally, Minimize with constraints 0 < t1 < t2 < 13.8004
gives optimal knots (13) with J F
Tc( ˆ
Topt) = 0.741614. The execution time
amounts here T M
M3 = 9.229526 s > T N
M3 > T S
M3.
(b) Consider now six 2D reduced data points (i.e. here n = 5):
M5 = {(0, 0), (−0.5, −4), (0.5, −4), (−0.5, 4), (0.5, 4), (−1, 3.8)}.
The resulting cumulative chord knots (based on (12) and M5) are equal here to
Tc = {0, 4.03113, 5.03113, 13.0934, 14.0934, 15.6067} with internal knots
ˆ
Tc =
{4.03113, 5.03113, 13.0934, 14.0934}. In this case, there are four free variables
{t1, t2, t3, t4} corresponding to the unknown knots at points xi (i = 1, . . . , 4).
FindMinimum (Newton Method) applied to (9) with initial guess as Tc yields
the following optimal knots (with optimal energy value J F
Tc( ˆ
Topt) = 4.65476):
Topt = {0, 2.9185, 5.12397, 11.1964, 13.507},
(14)
where ˆ
Topt = {2.9185, 5.12397, 11.1964}. The execution time T N
M5 = 29.946006 s.
FindMinimum (Secant Method) (here again ε = ±0.5 is added to cumula-
tive chord initial guess along each free knot ti) yields exactly the same opti-
mal knots (14) (with J F
Tc( ˆ
Topt) = 4.65476) and again shorter execution time
T S
M5 = 6.385922 s. As previously, Minimize with constraints 0 < t1 < t2 < t3 <
t4 < 13.507 yields optimal knots (14) and J F
Tc( ˆ
Topt) = 4.65476. The execution
time here reads T M
M5 = 358.390915 s >> T N
M5 > T S
M5.
(c) Finally, consider now eight 2D reduced data points (i.e. here n = 7):
M6 = {(0, 0), (−0.5, −4), (0.5, −4), (−0.5, 4),
(0.5, 4), (−1, 3.8), (0.3, 0.3), (0.5, 0.5)}.
By (12) Tc = {0, 4.03113, 5.03113, 13.0934, 14.0934, 15.6067, 19.3403, 19.6231}.
As previously
ˆ
Tc = {4.03113, 5.03113, 13.0934, 14.0934, 15.6067, 19.3403}. For
both optimization schemes FindMinimum (Newton Method) and Minimize no
result was reported within 60 min. FindMinimum (Secant Method) (with as pre-
viously ε = ±0.5 variations of cumulative chords for each free variable) yields

264
R. Kozera and L. Noakes
optimal knots (with energy J F
Tc( ˆ
Topt) = 8.27118):
Topt = {0, 2.67713, 4.69731, 10.3221, 12.3943, 14.8132, 19.0316, 19.6231},
where Topt = {2.67713, 4.69731, 10.3221, 12.3943, 14.8132}. The execution time
is T S
M7 = 35.708519 s.
□
The above experiments illustrate that for n ≥7, FindMinimum (Secant
Method) oﬀers a feasible computational scheme to optimize (9). In Sect. 4 of
this paper, we compare the performance of already discussed Secant Method
with Leap-Frog Algorithm (see [17,18,27,28]).
3
Non-Convexity of J F
T
We demonstrate in this section that J F
Tc introduced in (9) may not be convex.
In doing so, a simple degenerate case of (9) with n = 2 is examined. Three points
M = {x0, x1, x2} are admitted with one relaxed internal knot t1 ∈[t0 = 0, t2 =
Tc] (see (12)).
Example 4. For n = 2 and arbitrary natural m ≥1 (using Mathematica pack-
age), by (9) the energy J F
Tc(t1) = (Tc −t0)−3( ˜Edeg ◦φ−1)(t1), where ˜Edeg(s1) =
3∥x0−x1
s1
+ x2−x1
1−s1 ∥2 - here t1 ∈(t0, t2 = Tc), and s1 = φ(t1) = (t−t0)(Tc−t0)−1 ∈
(0, 1). Obviously since φ−1′ ≡Tc −t0 > 0 and φ−1′′ ≡0 the convexity (non-
convexity) of ˜Edeg is inherited by J F
Tc. Take now for m = 1 the following points
x0 = −1, x1 = 0 and x2 = 20 (here (x0 −x1)(x2 −x1) = −20 < 0). The graph of
the energy ˜Edeg(s1) = 3( −1
s1 +
20
1−s1 )2 over the interval (0, 1) is plotted in Fig. 1(a).
The non-convexity is better visible in Fig. 1(b) with the graph of the energy ˜Edeg
localized over the sub-interval (0.05, 0.35). Finally the change of sign in the sec-
ond derivative ˜E ′′
deg is also illustrated in Fig. 1(c). In fact, ˜E ′′
deg(0.21) = 1338.7
and ˜E ′′
deg(0.20) = −1640.63. Thus, ˜Edeg is not convex which also implies non-
convexity of J F
Tc in the general case.
□
Fig. 1. The graph of the non-convex energy ˜Edeg for x0 = −1, x1 = 0, and x2 = 20 (a)
over the interval (0, 1), (b) in the proximity of non-convexity sub-interval (0.05, 0.35)
(c) and the graph of the corresponding changing signs second derivative ˜E ′′
deg in the
proximity of (0.05, 0.35)

Non-linearity and Non-convexity for Sparse Reduced Data
265
Fig. 2. The graph of the convex energy ˜Edeg for x0 = 2, x1 = 0, and x2 = 5 (a) over
the interval (0, 1), (b) and the graph of the corresponding second derivative ˜E ′′
deg ≥0
over (0, 1)
The next example formulates suﬃcient conditions to enforce convexity of
J F
Tc, but only for m = 1 and n = 2. The latter can be extended to the general
case of m ≥1 and n = 2. Such general case is here omitted due to the paper
length limitation.
Example 5. (i) For m = 1 it is easy to show that ˜Edeg is convex (and so thus J F
Tc)
if (x0−x1)(x2−x1) ≥0 - under this constraint, we have exactly one critical point
for ˜Edeg. Indeed, recalling that ˜Edeg(s1) = 3f 2(s1) with f(s1) = x0−x1
s1
+ x2−x1
1−s1
it suﬃces to show that f is either convex and f ≥0 or it is concave and f ≤0,
for s1 ∈(0, 1). Indeed the latter combined with ˜E ′′
deg = 3(f 2)′′ = 6(f ′)2 + 6ff ′′
yields the convexity of J F
Tc given non-negativity of ff ′′ over s1 ∈(0, 1) which
follows from (x0 −x1)(x2 −x1) ≥0 applied both to f ′′(s1) = x0−x1
s3
1
+
x2−x1
(1−s1)3
and f. Figure 2(a) shows that convexity of ˜Edeg(s1) = 3( 2
s1 +
5
1−s1 )2 indeed
follows for x0 = 2, x1 = 0 and x2 = 5 with (x0 −x1)(x2 −x1) ≥0 clearly
fulﬁlled. As expected ˜E ′′
deg ≥0 over (0, 1) - see Fig. 2(b). The corresponding
suﬃcient conditions guaranteeing the convexity of ˜Edeg for m ≥1 and n = 2 can
also be formulated (though omitted here) - see [19]. As it turns out, the vector
generalization ⟨x0 −x1|x2 −x1⟩≥0 of scalar inequality (x0 −x1)(x2 −x1) ≥0
assures the convexity of ˜Edeg and thus of J F
Tc.
(ii) In case of scalar data (i.e. when m = 1) if (x0 −x1)(x2 −x1) < 0 holds
then the existence of exactly one critical point (and thus one global minimum -
see Theorem 1) of ˜Edeg (and so of J F
Tc) follows, which yields the unimodality of
˜Edeg = 3f 2 (see [4]). Indeed, assume that (x0 −x1)(x2 −x1) < 0. Since now x0 ̸=
x1 and x1 ̸= x2 we have x0 −x1 ̸= 0 and x2 −x1 ̸= 0. To show unimodality of f 2
we need to prove the existence of exactly one critical point s1 ∈(0, 1) satisfying
(f 2)′(s1) = 2f(s1)f ′(s1) = 0. Taking into account (x0 −x1)(x2 −x1) < 0 we
have that f ′(s1) = −x0−x1
s2
1
+
x2−x1
(1−s1)2 is either always positive or negative over
(0, 1). Hence for unimodality of f 2 it suﬃces to show that f(s1) = 0 has one
root s0
1 ∈(0, 1) deﬁning a unique global minimum of f 2 = 0 (and of ˜Edeg). In
this case as ˜Edeg(s1) = 3f 2(s1) the energy ˜Edeg also vanishes at s0
1. The latter
may not be the case for the convexity case, since another factor f ′(s1) = 0 may
contribute to (f 2)′(s1) = 0. A simple inspection shows that

266
R. Kozera and L. Noakes
s0
1 =
(x0 −x1)
(x0 −x1) −(x2 −x1).
(15)
Note that the denominator x0 −x2 in (15) does not vanish due to (x0 −x1)(x2 −
x1) < 0. Of course, s0
1 > 0 since (x0 −x1)(x2 −x1) < 0. To justify s0
1 < 1
two cases are here considered, namely either x0 −x1 > 0 and x2 −x1 < 0 or
x0 −x1 < 0 and x2 −x1 > 0. In the ﬁrst (second) case s0
1 < 1 in (15) leads to a
true inequality x2 −x1 < 0 (x2 −x1 > 0). Figure 1 conﬁrms the unimodality of
Edeg for (x0 −x1)(x2 −x1) = −20 < 0. As proved the global minimum of (9) is
attained at s0
1 = 1/21 ≈0.047619 nullifying J F
Tc.
Note that in case of convexity (enforced by (x0 −x1)(x2 −x1) ≥0), the
unique global minimum s0
1 can also be found in analytic form. Indeed as x0 ̸= x1
and x1 ̸= x2 it suﬃces to assume a stronger inequality (x0 −x1)(x2 −x1) > 0.
The latter results in either f > 0 or f < 0. Consequently for 6f ′f to vanish we
need to solve f ′(s1) = 0 over (0, 1) which leads to s0
1 =
√
|x0−x1|
√
|x2−x1|+√
|x0−x1| ∈
(0, 1). Thus, f 2 is unimodal (and so ˜Edeg) since (f 2)′ = 2ff ′ vanishes at exactly
one point s0
1 ∈(0, 1). The unimodality of ˜Edeg can also be proved in case of
⟨x0 −x1|x2 −x1⟩< 0 for arbitrary data with m ≥1 and n = 2.
□
4
Numerical Experiments for Fitting Sparse Reduced
Data
All experiments are conducted in Mathematica - see [35]. The numerical tests
compare the Leap-Frog algorithm (see [17,18]) with the Secant Method both used
to optimize (9). Only sparse reduced data points M in E2,3 are admitted here,
though the entire setting is applicable for arbitrary m, i.e. for reduced data M
in arbitrary Euclidean space.
The ﬁrst example admits reduced data M in E2 (i.e. for m = 2).
Fig. 3. Natural splines interpolating data points M2D3 (a) γNS
Tuni with uniform knots
Tuni, (b) γNS
Tc with cumulative chords Tc, (c) γNS
T LF
opt with optimal knots T LF
opt = T SM
opt
(thus γNS
T LF
opt = γNS
T SM
opt ) (d) γNS
T LF
opt and γNS
Tc plotted together

Non-linearity and Non-convexity for Sparse Reduced Data
267
Example 6. Assume for n = 6, the following 2D points (see dotted points in
Fig. 3):
M2D3 = {(−3, −3), (−3.1, −2.6), (2.5, −2.6), (2.4, −2.8), (−3, 2.8), (−3, 2.6)}.
The uniform interpolation knots, {ˆti = i
6Tc}6
i=0 (rescaled to Tc - see (12)) taken
as a blind guess of {ti}6
i=0, read as:
Tuni = {0, 2.84308, 5.68615, 8.52923, 11.3723, 14.2154}
and the initial guess based on cumulative chord Tc (see (12)) coincides with:
Tc = {0, 0.412311, 6.01231, 6.23592, 14.0154, 14.2154}.
Here ˆ
Tuni (and ˆ
Tc) is deﬁned as Tuni (and Tc) stripped from its terminal values.
The natural splines γNS
Tuni (based on Tuni) and γNS
Tc (based on Tc) yield the fol-
lowing energies J F
Tc( ˆ
Tuni) = 15.4253 > J F
Tc( ˆ
Tc) = 8.51108. Both interpolants
γNS
Tuni and γNS
Tc are shown in Fig. 3(a) and (b), respectively.
One expects that the Secant Method with two initial numbers tc
i ± 0.5 may
produce a bad solution as |tc
0 −tc
1| < 0.5, |t2 −t3| < 0.5 and |t4 −t5| < 0.5.
Indeed the Secant Method returns topt
1
= −8.2211 < t0 = 0, which is disallowed.
Upon adjusting tc
i ± 0.05 the Secant Method yields (for (9)) the optimal knots
ˆ
T SM
opt
augmented by terminal times t0 = 0 and t5 = Tc as:
T SM
opt
= {0, 0.737027, 6.07314, 7.14642, 13.5208, 14.2154}
with the optimal energy J F
Tc( ˆ
T SM
opt ) = 5.04331. The execution time amounts to
T SM = 9.204045 s. The resulting curve γS
T SM
opt is plotted in Fig. 3(c). In fact, for
general data it is safer for each free variable optimized by the Secant Method to
choose a pair of numbers tc
i ± 0.5 min0≤i≤n−1{|tc
i+1 −tc
i|}.
Leap-Frog decreases the initial energy to J F
Tc( ˆ
T LF
opt ) = J F
Tc( ˆ
T SM
opt ) (as for
the Secant Method) with the iteration stopping conditions
ˆ
T LF
opt = ˆ
T SM
opt
(up to
6th decimal point) upon 38 iterations. The respective execution time amounts
to T LF = 3.247230 < T SM. The 0th (i.e. J F
Tc( ˆ
Tc)), 1st, 2nd, 10th, 18th, and
38th iterations of Leap-Frog decrease the energy to:
{8.51108, 5.91959, 5.23031, 5.04455, 5.04331, 5.04331}
with again only the ﬁrst three iterations contributing to the major correction of
the initial guess knots Tc. The resulting natural spline γNS
T LF
opt (clearly the same
as γNS
T SM
opt yielded by the Secant Method) based on T LF
opt is shown in Fig. 3(c) and
also visually compared with γNS
Tc in Fig. 3(d).
Again if Leap-Frog iteration bound condition is changed e.g. to make current
Leap-Frog energy equal to J F
Tc( ˆ
T SM
c
) (say up to 5th decimal place) then only 18
iterations are needed here with shorter execution time T LF
E
= 1.785042 < T SM
and with optimal times
T LFE
opt
= {0, 0.736394, 6.0697, 7.14349, 13.5205, 14.2154}.

268
R. Kozera and L. Noakes
We miss out here a bit on a precise estimation of the optimal knots but we
accelerate the Leap-Frog execution time by obtaining almost the same interpo-
lating curve as the optimal one (as ˆ
T LFE
opt
≈ˆ
T SM
opt ). For other iteration stopping
criteria accelerating the execution of Leap-Frog at almost no cost in diﬀerence
between computed and optimal curve see [19].
□
We pass now to an example of reduced data in E3 (i.e. with m = 3).
Fig. 4. Natural splines interpolating data points M3D3 (a) γNS
Tuni with uniform knots
Tuni, (b) γNS
Tc with cumulative chords Tc, (c) γNS
T LF
opt with optimal knots T LF
opt = T SM
opt
(thus γNS
T LF
opt = γNS
T SM
opt ) (d) γNS
T LF
opt and γNS
T SM
c
plotted together
Example 7. Consider for n = 7 the following 3D points (see dotted points in
Fig. 4):
M3D3 = {(0, 0, 1), (0, 0, −1), (0, 0, −0.8), (1, 0, 0), (1, 0.2, 0), (1, 0.4, 0), (1, 0.8, 0.2),
(1, 1, 0)}.
The uniform interpolation knots {ˆti = i
7Tc}7
i=0 ≈{ti}7
i=0 (rescaled to t0 = 0 and
to Tc – see (12)) read as:
Tuni = {0, 0.658669, 1.31734, 1.97601, 2.63467, 3.29334, 3.95201, 4.61068}
and the initial guess based on cumulative chords Tc is equal to:
Tc = {0, 2, 2.2, 3.48062, 3.68062, 3.88062, 4.32784, 4.61068}.
Here
ˆ
Tuni = {0.658669, 1.31734, 1.97601, 2.63467, 3.29334, 3.95201}, while
the other one
ˆ
Tc = {0, 2, 2.2, 3.48062, 3.68062, 3.88062, 4.32784, 4.61068}. The
natural splines γNS
Tuni (based on Tuni) and γNS
Tc
(based on Tc) yields the fol-
lowing energies J F
Tc( ˆ
Tuni) = 46.7919 > J F
Tc( ˆ
Tc) = 22.3564. Both interpolants
γNS
Tuni and γNS
Tc are shown in Fig. 4(a) and(b), respectively. Noticeably the energy
based on blind guess of knots (i.e. for uniform knots) is far from the optimal one.
The Secant Method yields for (9) the optimal knots
ˆ
T SM
opt
(augmented by
terminal knots t0 = 0 and t7 = Tc - see (12))
T SM
opt
= {0, 1.34728, 1.82093, 3.12718, 3.39487, 3.62307, 4.19613, 4.61068}

Non-linearity and Non-convexity for Sparse Reduced Data
269
with the optimal energy J F
Tc( ˆ
T SM
opt ) = 15.407. The execution time amounts to
T SM = 128.804084 s. The resulting curve γNS
T SM
opt is plotted in Fig. 4(c). Note that
for each free variable, the Secant Method uses here two initial numbers tc
i ± 0.1.
Leap-Frog decreases the initial energy to J F
Tc( ˆ
T LF
opt ) = J F
Tc( ˆ
T SM
opt ) (as for
the Secant Method) with the iteration stopping conditions
ˆ
T LF
opt = ˆ
T SM
opt
(up to
5th decimal point) upon 620 iterations. The execution time amounts to T LF =
73.749111 s < T SM. The 0th (i.e. J F
Tc( ˆ
Tc)), 1st, 2nd, 10th, 50th, 40th and
100th, 200th, 281th and 620th iterations of Leap-Frog decrease the energy to:
{22.3564, 18.5598, 18.274, 17.4628, 15.8596.15.5049, 15.409115.407, 15.407}
with again only the ﬁrst three iterations contributing to major correction of the
initial guess knots Tc. The resulting natural spline γNS
T LF
opt (clearly the same as
γNS
T SM
opt
yielded by the Secant Method) based on T LF
opt2 is shown in Fig. 4(c) and
also visually compared with γNS
Tc in Fig. 4(d). The optimal curve does not vary
too much from the initial guess curve based on cumulative chord knots.
Again if Leap-Frog iteration bound condition is changed e.g. to make current
Leap-Frog energy equal to J F
Tc( ˆ
T SM
c
) (say up to 4th decimal place) then only
281 iterations are needed here with shorter execution time T LF
E
= 33.931990 s <
T SM and with optimal knots:
T LFE
opt
= {0, 1.348043, 1.82195, 3.12892, 3.39651, 3.62453, 4.19679, 4.61068}.
As previously, we lose here slightly on a precise estimation of the optimal knots
but we accelerate the Leap-Frog execution time by obtaining almost the same
interpolating curve as the optimal one (as T LFE
opt
≈T SM
opt ). For other iteration
stopping criteria accelerating the execution of Leap-Frog at almost no cost in
diﬀerence between computed curve and optimal curve see [19].
□
5
Conclusions
In this paper, we discuss the method of estimating the unknown interpolation
knots {ti}n
i=0 by {ˆti}n
i=0 to ﬁt reduced sparse data M = {qi}n
i=0 with the nat-
ural cubic spline in arbitrary Euclidean space Em. As indicated here, the above
task is transformed into the corresponding ﬁnite-dimensional constrained opti-
mization task (9) in (t1, t2, . . . , tn−1)-variables, subject to the satisfaction of the
inequalities t0 < t1 < t2, < . . . < tn−1 < tn. We ﬁrst demonstrate a high non-
linearity and possible non-convexity of (9) - Sects. 1, 2, and 3. Consequently, the
latter hinders the use of standard optimization techniques like Newton Method to
deal with such optimization task. Finally, two computationally feasible schemes
are implemented, i.e. Leap-Frog and the Secant Method to examine the quality
of the reconstructed interpolants. The derivation of the explicit formula in (9)
including its particular forms examined in Examples 1 and 2 relies on Mathemat-
ica symbolic computation - see [35]. All numerical computations performed in

270
R. Kozera and L. Noakes
Examples 3, 6 and 7 resort to the numerical functions supplied by Mathematica
software (see [35]). In addition, suﬃcient conditions to enforce the convexity (or
unimodality) of (9) are generated for the special case of n = 2 - see Example 5.
Future work involves the analysis of a more general case i.e. when n is arbitrary.
Alternatively one may also consider to derive a similar to (9) optimization task
set for a complete spline interpolant (see [3]), with the initial velocities v0 and
vn either a priori given or approximated according to [15].
References
1. B´ezier, P.E.: Numerical Control: Mathematics and Applications. Wiley, New York
(1972)
2. Boehm, E., Farin, G., Kahmann, J.: A survey of curve and surface methods in
CAGD. Comput. Aided Geom. Des. 1(1), 1–60 (1988)
3. de Boor, C.: A Practical Guide to Spline. Springer, New York (1985)
4. Boyd, S., Vandenberghe, L.: Convex Optimization. Cambridge University Press,
Cambridge (2004)
5. Budzko, D.A., Prokopenya, A.N.: On the stability of equilibrium positions in
the circular restricted four-body problem. In: Gerdt, V.P., Koepf, W., Mayr,
E.W., Vorozhtsov, E.V. (eds.) CASC 2011. LNCS, vol. 6885, pp. 88–100. Springer,
Heidelberg (2011). doi:10.1007/978-3-642-23568-9 8
6. Epstein, M.P.: On the inﬂuence of parameterization in parametric interpolation.
SIAM J. Numer. Anal. 13, 261–268 (1976)
7. Farin, G.: Curves and Surfaces for Computer Aided Geometric Design. Academic
Press, San Diego (1993)
8. Farouki, R.T.: Optimal parameterizations. Comput. Aided Geom. Des. 14(2), 153–
168 (1997)
9. Floater, M.S.: Chordal cubic spline interpolation is fourth order accurate. IMA J.
Numer. Anal. 26, 25–33 (2006)
10. Hoschek, J.: Intrinsic parametrization for approximation. Comput. Aided Geom.
Des. 5(1), 27–31 (1988)
11. Janik, M., Kozera, R., Koziol, P.: Reduced data for curve modeling - applications
in graphics, computer vision and physics. Adv. Sci. Tech. 7(18), 28–35 (2013)
12. Koci´c, L.M., Simoncinelli, A.C., Della, V.B.: Blending parameterization of poly-
nomial and spline interpolants. Facta Universitatis (NIˇS), Ser. Math. Inform. 5,
95–107 (1990)
13. Kozera, R.: Curve modelling via interpolation based on multidimensional reduced
data. Stud. Informatica 25, 1–140 (2004). (4B(61))
14. Kozera, R., Noakes, L.: Piecewise-quadratics and exponential parameterizations
for reduced data. Appl. Maths Comput. 221, 620–638 (2013)
15. Kozera, R., Noakes, L.: C1 Interpolation with cumulative chord cubics. Funda-
menta Informaticae 61(3–4), 285–301 (2004)
16. Noakes, L., Kozera, R.: Interpolating sporadic data. In: Heyden, A., Sparr, G.,
Nielsen, M., Johansen, P. (eds.) ECCV 2002. LNCS, vol. 2351, pp. 613–625.
Springer, Heidelberg (2002). doi:10.1007/3-540-47967-8 41
17. Kozera, R., Noakes, L.: Optimal knots selection for sparse reduced data. In: Huang,
F., Sugimoto, A. (eds.) PSIVT 2015. LNCS, vol. 9555, pp. 3–14. Springer, Cham
(2016). doi:10.1007/978-3-319-30285-0 1

Non-linearity and Non-convexity for Sparse Reduced Data
271
18. Kozera, R., Noakes, L.: Modeling reduced sparse data. In: Romaniuk, R.S. (ed.)
Photonics, Applications in Astronomy, Communications, Industry, and High-
Energy Physics Experiments 2016. SPIE 2016, vol. 10031. Society of Photo-Optical
Instrumentation Engineers, Bellingham (2016)
19. Kozera, R., Noakes, L.: Fitting Data via Optimal Interpolation Knots. (Submitted)
20. Kvasov, B.I.: Methods of Shape-Preserving Spline Approximation. World Scientiﬁc,
Singapore (2000)
21. Kuznetsov, E.B., Yakimovich, A.Y.: The best parameterization for parametric
interpolation. J. Comp. Appl. Maths 191, 239–245 (2006)
22. Marin, S.P.: An approach to data parameterization in parametric cubic spline
interpolation problems. J. Approx. Theory 41, 64–86 (1984)
23. Mørken, K., Scherer, K.: A general framework for high-accuracy parametric inter-
polation. Math. Comput. 66(217), 237–260 (1997)
24. Noakes, L.: A global algorithm for geodesics. J. Math. Austral. Soc. Ser. A 64,
37–50 (1999)
25. Noakes, L., Kozera, R.: Cumulative chords piecewise-quadratics and piecewise-
cubics. In: Klette, R., Kozera, R., Noakes, L., Weickert, J. (eds.) Geometric Proper-
ties from Incomplete Data. Computational Imaging and Vision, vol. 31, pp. 59–75.
Springer, The Netherlands (2006)
26. Noakes, L., Kozera, R.: More-or-less uniform sampling and lengths of curves. Quar.
Appl. Maths 61(3), 475–484 (2003)
27. Noakes, L., Kozera, R.: Nonlinearities and noise reduction in 3-source photometric
stereo. J. Math. Imag. Vis. 18(3), 119–127 (2003)
28. Noakes, L., Kozera, R.: 2D leap-frog algorithm for optimal surface reconstruction.
In: Latecki, M.J. (ed.) SPIE 1999. Vision Geometry VIII, vol. 3811, pp. 317–328.
Society of Industrial and Applied Mathematics, Bellingham (1999)
29. Lee, E.T.Y.: Corners, cusps, and parameterization: variations on a theorem of
Epstein. SIAM J. Numer. Anal. 29, 553–565 (1992)
30. Lee, E.T.Y.: Choosing nodes in parametric curve interpolation. Comput. Aided
Geom. Des. 21, 363–370 (1989)
31. Piegl, L., Tiller, W.: The NURBS Book. Springer, Berlin (1997)
32. Prokopenya, A.N.: Hamiltonian normalization in the restricted many-body prob-
lem by computer algebra methods. Program. Comput. Softw. 38(3), 156–166
(2012)
33. Rababah, A.: High order approximation methods for curves. Comput. Aided Geom.
Des. 12, 89–102 (1995)
34. Schaback, R.: Optimal geometric Hermite interpolation of curves. In: Dæhlen, M.,
Lyche, T., Schumaker, L. (eds.) Mathematical Methods for Curves and Surfaces
II, pp. 1–12. Vanderbilt University Press, Nashville (1998)
35. Wolfram, S.: The Mathematica Book, 5th edn. Wolfram Media, Champaign (2003)

The Convergence Conditions of Interval
Newton’s Method Based on Point Estimates
Zhe Li1,2(B), Baocheng Wan2,3, and Shugong Zhang2
1 School of Science, Changchun University of Science and Technology,
Changchun 130022, China
lizhe@amss.ac.cn
2 School of Mathematics, Key Laboratory of Symbolic Computation
and Knowledge Engineering, Jilin University, Changchun 130012, China
wanbaocheng@163.com, sgzh@jlu.edu.cn
3 Jilin Agricultural University, Changchun 130118, China
Abstract. Both Smale’s alpha theory and Rump’s interval theorem pro-
vide the conditions which guarantee the existence of a simple solution
of a square nonlinear system. In this paper, we generalize the conclusion
provided by Rall to reveal the relationship between Smale’s alpha the-
ory and Rump’s interval theorem. By point estimates, we propose the
conditions under which the condition of Rump’s interval theorem holds.
Furthermore, using only the information of the given system at the ini-
tial approximate point, we give the convergence conditions of interval
Newton’s algorithm proposed by Rump.
Keywords: Newton iteration · Alpha theory · Interval algorithm · Point
estimate · Veriﬁcation
1
Introduction
Solving a nonlinear system in the form f(x) = 0 with f = (f1, f2, . . . , fn)T
and x = (x1, . . . , xn)T is one of the most fundamental problems in scientiﬁc
computing. In this paper, we assume that f : Rn →Rn and f1, f2, . . . , fn have
all order continuous partial derivatives. Denote the Jacobian matrix of f at x
by f ′(x).
Newton’s method and its modiﬁcations have long played an important role
in solving nonlinear systems. Under certain conditions, Newton’s method con-
structs a sequence of iteration points that will converge to a solution of the
given nonlinear system. In 1948, the author of [2] established the Kantorovich
theorem based on the assumption that the Jacobian matrix of the nonlinear
Z. Li—This research was supported by Chinese National Natural Science Foundation
under Grant Nos. 11601039, 11671169, 11501051, by the open fund Key Lab. of Sym-
bolic Computation and Knowledge Engineering under Grant No. 93K172015K06,
and by the Education Department of Jilin Province, “13th Five-Year” science and
technology project under Grant No. JJKH20170618KJ.
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 272–284, 2017.
DOI: 10.1007/978-3-319-66320-3 20

The Convergence Conditions of Interval Newton’s Method
273
system is Lipschitz continuous on some domain. The Kantorovich theorem ﬁrst
gives the condition to ensure that a simple solution exists close to the initial
point and the Newton iteration sequence quadratically converges to this simple
solution. Using the technique of point estimates, Smale et al. [15–17] developed
the alpha theory to locate and approximate simple solutions. The alpha theory
requires only information concerning the nonlinear system at the initial point of
the Newton iteration. By introducing the dominating sequence technique, Wang
and Han [18] improved both the condition and conclusion of the alpha theory.
With the aid of Schr¨oder operator, Giusti et al. [3] provided a criterion for locat-
ing clusters of solutions of univariate nonlinear functions. Later on, Giusti et al.
[4] generalized their results to locate breadth-one singular solutions of multivari-
ate nonlinear systems. For the performance of the alpha theory, Hauenstein and
Sottile [6] described the program alphaCertiﬁed to certify solutions of polyno-
mial systems. Recently, Hauenstein and Levandovskyy [5] extended the program
alphaCertiﬁed to verify solutions to polynomial-exponential systems.
Interval arithmetic is another important tool of veriﬁcation methods. In
1960s, Krawczyk [9] ﬁrst introduced an interval version of Newton’s method for
verifying the existence of simple solutions. Moore [10] proposed computationally
veriﬁable suﬃcient condition for interval Newton’s method given by Krawczyk.
Rump [12] made interval Newton’s method perform better in practice, which
is called Rump’s interval theorem and included in verifynlss function in INT-
LAB toolbox [13] in Matlab. Based on the deﬂation technique using smoothing
parameters, Rump and Graillat [14] described a veriﬁcation algorithm to ver-
ify multiple solutions of univariate nonlinear functions and double solutions of
multivariate nonlinear systems. Further, Li and Zhi [8] provided an algorithm
to verify breadth-one singular solutions of polynomial systems, which had been
generalized to deal with the veriﬁcation of isolated singular solutions in [7].
By combining interval algorithms with some other methods, Yang et al. [19]
investigated the veriﬁcation for real solutions of positive-dimensional polynomial
systems.
In 1980, Rall [11] exhibited the relationship between the Kantorovich theorem
and Moore’s interval theorem. By the quantities of the Kantorovich theorem,
Rall provided the conditions under which Moore’s veriﬁable suﬃcient condition
holds. For an initial approximate x(0) ∈Rn and a radius ρ > 0, let Xρ denote
the set {x : ∥x −x(0)∥∞< ρ}, and let η, B, κ be the constants such that
∥f ′(x(0))
−1f(x(0))∥∞≤η,
∥f ′(x(0))
−1∥∞≤B,
∥f ′(u) −f ′(v)∥∞≤κ∥u −v∥,
u, v ∈Ω,
where Ω is a suﬃciently large region containing x(0). Rall’s conclusion is that if
h = Bκη < 1
4,
then
x(0) −f ′(x(0))
−1f(x(0)) + (I −f ′(x(0))
−1f ′(Xρ))(Xρ −x(0)) ⊂Xρ

274
Z. Li et al.
holds for any ρ satisfying the inequality
1 −
√
1 −4h
2h
η ≤ρ ≤1 +
√
1 −4h
2h
η.
Since the alpha theory and Rump’s interval theorem are respectively the
generalization of the Kantorovich theorem and Moore’s interval theorem, we
generalize Rall’s conclusion to discuss the relationship between the alpha theory
and Rump’s interval theorem in this paper. By only the information of the given
system at the initial approximate point, we provide the conditions to guarantee
that we can obtain an approximate point after ﬁnite Newton’s iterations, where
this approximate iteration point corresponds to an interval solution satisfying the
condition of Rump’s interval theorem. The next section will give some notation
and background results.
2
Notation and Preliminaries
First of all, we emphasize that the norm ∥·∥of the vector and the matrix in this
paper are both the inﬁnite norm ∥· ∥∞since the metric for the interval vector
is closely related to the inﬁnite norm.
Henceforward, we use boldface letters to express tuples and denote their
entries by the same letter with subscripts, for example α = (α1, . . . , αn)T .
Denote the usual product order on Rn by ≤, that is, for arbitrary α, β ∈Rn,
α ≤β if and only if αi ≤βi for 1 ≤i ≤n.
For x ∈Rn, if f ′(x) is nonsingular, then deﬁne
α(f, x) = β(f, x)γ(f, x),
β(f, x) = ∥f ′(x)−1f(x)∥,
γ(f, x) = sup
k≥2
∥f ′(x)−1 f (k)(x)
k!
∥
1/(k−1)
.
Given the initial approximate x(0) with the associated simple root x∗of f, we
let α, β and γ to stand for α(f, x(0)), β(f, x(0)) and γ(f, x(0)), respectively.
Applying Newton’s method for f can get the Newton iteration sequence {x(k)},
that is,
x(k+1) = x(k) −f ′(x(k))
−1f(x(k)),
k ∈N.
The alpha theory provides the convergence conditions which ensure the sequence
{x(k)} converges to x∗only with the values α, β and γ. The dominating sequence
technique is a powerful tool for improving the alpha theory. The dominating
sequence {t(k)} is produced by the Newton iteration with the initial approximate
t(0) = 0 for the univariate function
h(t) = β −t +
γt2
1 −γt,

The Convergence Conditions of Interval Newton’s Method
275
where the equation h(t) = 0 has the following two solutions
t∗=
2β
1 + α +
√
1 −6α + α2 ,
t∗∗= 1 + α +
√
1 −6α + α2
4γ
.
(1)
The following theorem is a version of the alpha theory given by Wang and Han,
where the condition on the quantity α is best possible.
Theorem 1. [4,18] If 0 < α < 3 −2
√
2, then for any t∗≤ρ < t∗∗, the system
f(x) = 0 has exactly one simple root x∗in B(x(0), ρ). In addition, the New-
ton iteration sequence {x(k)} converges quadratically to x∗, and the dominating
sequence {t(k)} increases and converges quadratically to t∗. Furthermore, for all
k ∈N,
∥x(k+1) −x(k)∥≤t(k+1) −t(k),
∥x(k+1) −x(k)∥≤q(α)2k−1β
with
q(α) =
4α
(1 −α +
√
1 −6α + α2)
2 .
(2)
Denote the set of intervals by IR. An interval vector X = [x, x] ∈IRn with
x, x ∈Rn and x ≤x is deﬁned by
X = [x, x] = {x ∈Rn : x ≤x ≤x}.
For x ∈Rn, X = [x, x] ∈IRn, x + X = [x + x, x + x]. Let Yρ = {y ∈Rn :
∥y∥≤ρ}, then B(x, ρ) = x + Yρ.
The norm of the interval vector X = [x, x] is deﬁned by
∥X∥= ∥[x, x]∥= max{∥x∥: x ∈X}.
Besides int(X) designates the interior of the interval vector X. Given a set Z ⊂
Rn, the interval hull of Z is the narrowest interval vector containing Z, namely,
hull(Z) =

{X ∈IRn : X ⊇Z}.
Given a continuous mapping g : Rn →Rm and an interval vector X, the interval
vector g(X) ∈IRm is deﬁned as
g(X) = hull{g(x) : x ∈X}.
Given an interval matrix A ∈IRm×n and an interval vector X ∈IRn, the
interval vector AX is deﬁned by
AX = hull{Ax : A ∈A, x ∈X}.
Specially, the norm of the interval matrix A is deﬁned as
∥A∥= max{∥A∥: A ∈A}.
The following theorem is a version of the interval Newton’s method given by
Rump.

276
Z. Li et al.
Theorem 2. [12] Given x(0) ∈Rn, Y ∈IRn with 0 ∈Y, R ∈Rn×n, if
S(Y, x(0)) := −Rf(x(0)) + (I −Rf ′(x(0) + Y))Y ⊆int(Y),
then there exists a unique x∗∈x(0) + Y such that f(x∗) = 0.
3
Main Results
To give the main results of this paper, we need the following functions,
ψ(u) = 2u2 −4u + 1,
g1(α) = α2 −4α + 10,
g2(α) = α3 −6α2 + 21α + 28,
g3(α) = 4α3 −25α2 + 88α −8,
θ(α) = 1
3 arccos(
g2(α)

g1(α)3 ),
ω1(α) =

g1(α) cos(θ(α) + 2π
3 ) + 2 + α
2 ,
ω2(α) =

g1(α) cos(θ(α) + 4π
3 ) + 2 + α
2 ,
ω3(α) =

g1(α) cos(θ(α)) + 2 + α
2 .
Here we give some lemmas and one proposition from which the main theorem
will easily follow.
Lemma 1. [15] Given x ∈Rn, if γ∥x −x(0)∥< 1 −
√
2/2, then
γ(f, x) ≤
γ
ψ(γ∥x −x(0)∥)(1 −γ∥x −x(0)∥).
Lemma 2. If 0 < α < 3 −2
√
2, then for all k ≥1,
γ∥x(k) −x(0)∥< 1 −
√
2
2 ,
(3)
γ(f, x(k)) ≤
γ
ψ(γ∥x(k) −x(0)∥)(1 −γ∥x(k) −x(0)∥).
(4)
Proof. If 0 < α < 3 −2
√
2, then by Theorem 1,
∥x(k) −x(0)∥≤
k

j=1
∥x(j) −x(j−1)∥≤
k

j=1
(t(j) −t(j−1))
≤t(k) ≤t∗.

The Convergence Conditions of Interval Newton’s Method
277
Therefore
γ∥x(k) −x(0)∥≤
2α
1 + α +
√
1 −6α + α2 .
Since the right hand side of the above inequality monotonously increases from 0
to 1 −
√
2/2 as α goes from 0 to 3 −2
√
2, it follows that (3) holds. By means of
Lemma 1, (4) follows.
⊓⊔
Lemma 3. If
0 < ρ < 1 −
√
2/2
γ
,
(5)
β + ρ(
1
(1 −γρ)2 −1) < ρ,
(6)
then
S(Yρ, x(0)) ⊆int(Yρ).
(7)
Proof.
Given an arbitrary real vector y ∈Yρ, we expand the Jacobian matrix
f ′(x(0) + y) into power series and get
f ′(x(0))
−1f ′(x(0) + y) = f ′(x(0))
−1(f ′(x(0)) +
∞

k=2
f (k)(x(0)) yk−1
(k −1)!)
= I +
∞

k=2
kf ′(x(0))
−1f (k)(x(0))yk−1
k! .
If (5) holds, then
∥I −f ′(x(0))
−1f ′(x(0) + y)∥≤
∞

k=2
k∥f ′(x(0))
−1 f (k)(x(0))
k!
∥∥y∥k−1
≤
∞

k=2
k(γρ)k−1
=
1
(1 −γρ)2 −1.
Suppose that (5) (6) hold, then for an arbitrary real vector y ∈Xρ, we can infer
that
∥−f ′(x(0))
−1f ′(x(0)) + (I −f ′(x(0))
−1f ′(x(0) + y))y∥< ρ,
which implies that (7) holds.
⊓⊔
Lemma 4. Let
α∗= −1
12(22247 + 1320
√
330)
1/3 +
431
12(22247 + 1320
√
330)
1/3 + 25
12
(8)
≈0.093347623,

278
Z. Li et al.
then for an arbitrary 0 < α < α∗, we have
g1(α∗) < g1(α) < g1(0),
g2(0) < g2(α) < g2(α∗),
g3(0) < g3(α) < g3(α∗) = 0,
θ(α∗) < θ(α) < θ(0),
ω1(0) < ω1(α) < ω1(α∗),
ω2(α∗) < ω2(α) < ω2(0) < 3 −3
√
2
2 ,
ω3(α) > 3 −3
√
2
2 .
Proof. According to Cartan’s root-ﬁnding formula, the equation g3(α) = 0 has
only a positive real root α∗as in (8). Obviously, g′
1(α) < 0 and g′
2(α) > 0 for all
0 < α < α∗. Thus both θ(α) and ω2(α) monotonously decrease on the interval
(0, 3 −2
√
2). A routine computation gives rise to
ω′
1(α) = (4 −2α)

−3g3(α) sin(θ(α) + π
6 ) + (84 −36α) cos(θ(α) + π
6 )
2

−3g1(α)g3(α)
+ 1
2,
then for all 0 < α < α∗, ω′
1(α) > 0. This lemma follows immediately from what
we have proved.
⊓⊔
Proposition 1. Under the condition (5), the inequality (6) holds if and only if
0 <α < α∗,
(9)
ω1(α)
3γ
<ρ < ω2(α)
3γ
.
(10)
Proof. Deﬁne Δ = (q/2)2 + (p/3)3 with
p = −1
3(4 + βγ
−2γ )
2
+ 1 + 2βγ
2γ2
,
q = 2(4 + βγ
−6γ )
3
+ β
2γ2 −(4 + βγ)(1 + 2βγ)
−12γ3
,
then
p = −g1(α)
12γ2 ,
q = −g2(α)
108γ3 ,
Δ = g3(α)
1728γ6 .
It follows by Lemma 4 that for all α > 0, p < 0 and q < 0, then we have three
cases to consider.
Case I: α > α∗. In this case, Δ > 0 and the equation
ρ3 −4 + βγ
2γ
ρ2 + 1 + 2βγ
2γ2
ρ −β
2γ2 = 0
(11)

The Convergence Conditions of Interval Newton’s Method
279
has only one real solution
ρ∗=
3

−q
2 +
√
Δ +
3

−q
2 −
√
Δ.
An easy computation yields that for all α > α∗, ρ∗> (1 −
√
2/2)/γ. Therefore,
in this case, (6) holds if and only if ρ > ρ∗, which contradicts the condition (5).
Case II: α = α∗. In this case, Δ = 0 and (11) has only two unequal real
solutions
−
3

−q
2 ,
2
3

−q
2 .
Clearly,
−
3

−q
2 < 0,
2
3

−q
2 > 1 −
√
2/2
γ
.
Hence in this case, (6) can not hold under the condition (5).
Case III: 0 < α < α∗. In this case, Δ < 0 and (11) has three unequal real
solutions
ω1(α)
3γ
,
ω2(α)
3γ
,
ω3(α)
3γ
.
Recalling Lemma 4, we know that for all 0 < α < α∗,
ω3(α)
3γ
> 1 −
√
2/2
γ
,
ω2(α)
3γ
< 1 −
√
2/2
γ
,
0 < ω1(α)
3γ
< ω2(α)
3γ
.
Thus in this case, (6) holds if and only if (10) holds.
As a whole, under the condition (5), the inequality (6) holds if and only if
(9) and (10) hold.
⊓⊔
On the basis of α, β and γ, the following theorem provides the conditions
such that (7) holds, which is an immediate conclusion of Proposition 1.
Theorem 3. If 0 < α < α∗, then for any ρ satisfying the inequality
ω1(α)
3γ
< ρ < ω2(α)
3γ
,
the condition (7) holds.
The following corollary indicates that the alpha theory is of greater preci-
sion than Rump’s interval theorem, which can be directly deduced by an easy
computation.

280
Z. Li et al.
Corollary 1. If 0 < α < α∗, then
ω1(α)
3γ
> t∗.
For the Newton iteration sequence {x(k)}, deﬁne
ρ∗(f, x(k)) = ω1(α(f, x(k)))
3γ(α(f, x(k))),
ρ∗∗(f, x(k)) = ω2(α(f, x(k)))
3γ(α(f, x(k))).
(12)
In view of the quantity α of the alpha theory proposed by Wang and Han [18], we
give the following convergence condition of interval Newton’s algorithm proposed
by Rump.
Proposition 2. Let ⌈·⌉be the integer ceiling function, p(α) be deﬁned by
p(α) =
2α
1 + α +
√
1 −6α + α2 ,
and q(α) be deﬁned in (2). If 0 < α < 3 −2
√
2, then for any
k ≥⌈log2(ln α∗+ ln(1 −p(α)) + ln ψ(p(α)) −ln α
ln q(α)
+ 1)⌉,
(13)
the condition
S(Yρ(k), x(k)) ⊆int(Yρ(k))
(14)
holds for any ρ(k) satisfying the inequality
ρ∗(f, x(k)) < ρ(k) < ρ∗∗(f, x(k)).
(15)
Proof. By Theorem 1 and Lemma 2, we know that if 0 < α < 3 −2
√
2, then for
any k ∈N,
α(f, x(k)) ≤
αq(α)2k−1
ψ(γ∥x(k) −x(0)∥)(1 −γ∥x(k) −x(0)∥).
If 0 < α < 3 −2
√
2, then for all k ∈N,
γ∥x(k) −x(0)∥≤p(α) < 1 −
√
2
2 ,
which implies that for all k ∈N,
ψ(γ∥x(k) −x(0)∥)(1 −γ∥x(k) −x(0)∥) ≥ψ(p(α))(1 −p(α)).
Hence if 0 < α < 3 −2
√
2, then for all k ∈N,
α(f, x(k)) ≤
αq(α)2k−1
ψ(p(α))(1 −p(α)),
which implies that 0 < α(f, x(k)) < α∗holds if the iteration number k satisﬁes
the inequality (13). Our conclusion will follow from Theorem 3.
⊓⊔

The Convergence Conditions of Interval Newton’s Method
281
With the aid of the quantity α of the alpha theory given in [1], the conclusion
of the above proposition can be improved as follows.
Proposition 3. If 0 < α ≤(13 −3
√
17)/4, then for any k ≥3, the condition
(14) holds for any ρ(k) satisfying the inequality (15).
Proof. Since the function α/ψ(α)2 monotonously increases on the interval [0, 1−
√
2/2), it follows that α/ψ(α)2 < 1 for any 0 < α ≤(13 −3
√
17)/4. Recalling
Proposition 1 in [15], we can deduce that if 0 < α ≤(13 −3
√
17)/4, then for
any k ∈N,
α(f, x(k)) ≤(
α
ψ(α)2 )
2k−1
α.
As a result, if 0 < α ≤(13 −3
√
17)/4, then 0 < α(f, x(3)) < α∗.
⊓⊔
4
Example
In this section, we propose some examples to illustrate our conclusion, which are
done in Matlab R2012a with INTLAB V6 under Windows 7. In these examples,
the true interval of the display as −0.0059
is obtained by subtracting
and adding 1 to the last displayed digit, namely,
−0.0059
= [−0.00600000000000, −0.00580000000000].
Example 1. Let
f1 = x2
1 + x2 −2 = 0,
f2 = x1 −x2 = 0,
and x(0) = (1.8, 1.8)T . The program of the alpha theory computes
α∗< α = 0.1436672967 < 13 −3
√
17
4
,
then by Proposition 3, we can immediately deduce that 0 < α(f, x(3)) < α∗. The
values of α(f, x(k)), ρ∗(f, x(k)), ρ∗∗(f, x(k)), k = 1, 2, 3, are shown in Table 1,
and the values of x(k), ρ(k), S(Yρ(k), x(k)), k = 1, 2, 3, are shown in Table 2.
Example 2. [11] Let
fi(x) = xi −0.7xi
9

j=1
ai,jxj −1 = 0,
i = 1, 2 . . . , 9,
with
ai,j = 3
4
ti(1 −t2
j)
2wj
ti + tj
,

282
Z. Li et al.
Table 1. The values of α(f, x(k)), ρ∗(f, x(k)) and ρ∗∗(f, x(k)) about Example 1
k α(f, x(k))
ρ∗(f, x(k))
ρ∗∗(f, x(k))
1 0.04063913776
0.1474393618
0.8654682610
2 0.001956685388
0.005916476543
0.8785582303
3 0.000003858607108 0.00001157539288 0.8785582303
Table 2. The values of x(k), ρ(k) and S(Yρ(k), x(k)) about Example 1
k
x(k)
ρ(k)
S(Yρ(k) , x(k))
1

1.13913043490000
1.13913043490000

0.147440361800000

[−0.14648800759416, −0.11996338231131]
[−0.14648800759416, −0.11996338231131]

2

1.00590473980000
1.00590473980000

0.005917476543000

−0.0059
−0.0059

3

1.00001157619900
1.00001157619900

1.257539288000000e-05
1.0e-005 ∗

−0.11576
−0.11576

where ti, wi, i = 1, 2, . . . , 9, are respectively the nodes and weights of Gaussian
integration rule of order 17 on the interval [0, 1] (See Table 3).
For the initial approximate
x(0) = [1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36, 1.36]T ,
we have
α∗< α = 0.144327213206543 < 13 −3
√
17
4
.
It follows by Proposition 3 that 0 < α(f, x(3)) < α∗. To be precise,
α(f, x(3)) = 9.771724278220212e-10,
ρ∗(f, x(3)) = 2.106476714805001e-09,
ρ∗∗(f, x(3)) = 0.721018979786819.
Choose ρ(3) = 2.206476714805001e-09, then S(Yρ(3), x(3)) ⊆int(Yρ(3)). The
values of x(3) and S(Yρ(3), x(3)) are shown in Table 4, where i stands for the
coordinate index.
Example 3. Let
fi = x2
i + xi+1 −2 = 0,
i = 1, 2, . . . , 99,
f100 = x99 −x100 = 0.
Given x(0) with x(0)
i
= 1.28, i = 1, 2, . . . , 100, it follows that
13 −3
√
17
4
< α = 0.165370210314031 < 3 −2
√
2.

The Convergence Conditions of Interval Newton’s Method
283
Table 3. The values of ti, wi of Example 2
i
ti
wi
1 0.015919880000000 0.040637193262940
2 0.081984445000000 0.090324080584283
3 0.193314285000000 0.130305351576427
4 0.337873290000000 0.156173536255424
5 0.500000000000000 0.165119676661738
6 0.662126710000000 0.156173536254610
7 0.806685715000000 0.130305351576465
8 0.918015555000000 0.090324080583571
9 0.984080120000000 0.040637193262741
Table 4. The values of x(3) and S(Yρ(3), x(3)) about Example 2
i
x(3)
S(Yρ(3), x(3))
1 1.03266743259998 1.0e-008 ∗-0.2175469960
2 1.10583043469589 1.0e-008 ∗0.058000438
3 1.17693975483393 1.0e-008 ∗0.074880771
4 1.23474234890867 1.0e-008 ∗-0.046156608
5 1.27801354526920 1.0e-008 ∗0.033335094
6 1.30888887603008 1.0e-008 ∗-0.052842004
7 1.32995480221212 1.0e-008 ∗0.015004650
8 1.34328756776955 1.0e-008 ∗-0.029283159
9 1.35027189327776 1.0e-008 ∗0.057999652
Recalling Proposition 2, we know that there exists k ∈N such that α(f, x(k)) <
α∗. Indeed, for the ﬁrst-step iteration point x(1), we have
α(f, x(1)) = 0.0209408111113277 < α∗,
ρ∗(f, x(1)) = 0.0229019461130693,
ρ∗∗(f, x(1)) = 0.291551749088838.
Choose ρ(1) = 0.0229019461130694, then S(Yρ(1), x(1)) ⊆int(Yρ(1)).
References
1. Blum, L., Cucker, F., Shub, M., Smale, S.: Complexity and Real Computation.
Springer, New York (1998)
2. Kantorovich, L.V.: Functional analysis and applied mathematics. Uspehi. Mat.
Nauk. 3(6), 89–185 (1948)

284
Z. Li et al.
3. Giusti, M., Lecerf, G., Salvy, B., Yakoubsohn, J.C.: On location and approximation
of clusters of zeros of analytic functions. Found. Comput. Math. 5(3), 257–311
(2005)
4. Giusti, M., Lecerf, G., Salvy, B., Yakoubsohn, J.C.: On location and approximation
of clusters of zeros: case of embedding dimension one. Found. Comput. Math. 7(1),
1–58 (2007)
5. Hauenstein, J.D., Levandovskyy, V.: Certifying solutions to square systems of
polynomial-exponential equations. J. Symb. Comput. 79(3), 575–593 (2015)
6. Hauenstein, J.D., Sottile, F.: Algorithm 921: alphacertiﬁed: certifying solutions to
polynomial systems. ACM Trans. Math. Softw. 38(4), 1–20 (2011)
7. Li, N., Zhi, L.: Veriﬁed error bounds for isolated singular solutions of polynomial
systems: case of breadth one. Theor. Comput. Sci. 479, 163–173 (2013)
8. Li, N., Zhi, L.: Veriﬁed error bounds for isolated singular solutions of polynomial
systems. SIAM J. Numer. Anal. 52(4), 1623–1640 (2014)
9. Krawczyk, R.: Newton-algorithmen zur bestimmung von nullstellen mit fehler-
schranken. Computing 4, 187–201 (1969)
10. Moore, R.E.: A test for existence of solutions to nonlinear system. SIAM J. Numer.
Anal. 14(4), 611–615 (1977)
11. Rall, L.B.: A comparison of the existence theorems of Kantorvich and Moore. SIAM
J. Numer. Anal. 17(1), 148–161 (1980)
12. Rump, S.M.: Solving algebraic problems with high accuracy. In: Kulisch, W.L.,
Miranker, W.L. (eds.) A New Approach to Scientiﬁc Computation, pp. 51–120.
Academic Press, San Diego (1983)
13. Rump, S.M.: INTLAB-Interval Laboratory. Springer, Netherlands, Berlin (1999)
14. Rump, S.M., Graillat, S.: Veriﬁed error bounds for multiple roots of systems of
nonlinear equations. Numer. Algorithm 54, 359–377 (2009)
15. Smale, S.: Newton method estimates from data at one point. The Merging of
Disciplines: New Directions in Pure, Applied and Computational Mathematics,
pp. 185C–196C. Springer, Berlin (1986)
16. Shub, M., Smale, S.: Computational complexity: on the geometry of polynomials
and a theory of cost. I. Ann. Sci. `Ecole Norm. Sup. 18(1), 107–142 (1985)
17. Shub, M., Smale, S.: Computational complexity: on the geometry of polynomials
and a theory of cost. II. SIAM J. Comput. 15(1), 145–161 (1986)
18. Wang, X.H., Han, D.F.: On dominating sequence method in the point estimate
and Smale theorem. Sci. China Ser. A 33(2), 135–144 (1990)
19. Yang, Z., Zhi, L., Zhu, Y.: Verﬁed error bounds for real solutions of positive-
dimensional polynomial systems. In: The 2013 International Symposium on Sym-
bolic and Algebraic Computation, pp. 371–378. ACM press, San Jose (2013)

Normalization of Indexed Diﬀerentials Based
on Function Distance Invariants
Jiang Liu(B)
Department of Systems Science, University of Shanghai for Science and Technology,
Shanghai 200093, China
jliu113@126.com
Abstract. This paper puts forward the method of function distance
invariant, and develops an eﬃcient normalization algorithm for indexed
diﬀerentials. The algorithm allows us to determine the equivalence of
indexed diﬀerentials in R2[/∂], and is mainly based on two algorithms.
One is an index replacement algorithm. The other is a normalization
algorithm with respect to monoterm symmetries, whose complexity is
lower than known algorithms.
1
Introduction
Diﬀerential geometry often involves massive calculation of indexed diﬀerentials,
such as tensor veriﬁcation problem or the problem of ﬁnding transformation
rules of indexed functions under the transformation of local coordinates. The
following examples are typical in diﬀerential geometry.
Example 1. Let hi
j be a (1, 1)-typed tensor. Prove that
Hi
jk = −1
8(hp
j∂phi
k −hp
k∂phi
j) + 1
8(hi
p∂jhp
k −hi
p∂khp
j)
is a (1, 2)-typed tensor, i.e., it satisﬁes the following transformation rule of
(1, 2)-typed tensor:
Hi′
j′k′ −/∂i′
i /∂j
j′ /∂k
k′Hi
jk = 0.
(1)
Example 2. A Riemannian manifold M n admits an almost complex structure
Ji
j. Let Hi
jk be 1/8 times the Nijenhuis tensor of Ji
j. The “torsional derivative”
of a tensor ﬁeld on M n is deﬁned as follows:
T i1...ia
j1...jb||rs = Hp
rs
∂T i1...ia
j1...jb
∂xp
+
a

u=1
T
i1...iu−1piu+1...ia
j1...jb
hiu
prs −
b

v=1
T i1...ia
j1...jv−1pjv+1...jbhp
jvrs,
where hi
jrs has the following “rather strange” deﬁnition:
2hi
jrs = Ji
p(Jq
j
∂Hp
rs
∂xq −Hq
rs
∂Jp
j
∂xq + Hp
qs
∂Jq
j
∂xr −Hp
qr
∂Jq
j
∂xs ) −∂Hi
rs
∂xj .
What is the transformation rule of hi
jrs under coordinate transformation?
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 285–300, 2017.
DOI: 10.1007/978-3-319-66320-3 21

286
J. Liu
However, the simpliﬁcation of indexed diﬀerential expressions is tricky and
cumbersome to perform by manual calculations. For example, it is not easy to
prove that the following two monomials are equivalent:
∂2xf
∂xf∂xc′
∂2xc′
∂xa′∂xb
∂2xr
∂xq∂xp′
∂2xp′
∂xs′∂xr
∂xq
∂xq′
∂2xs′
∂xl′∂xs
∂xs
∂xk′
∂2xb
∂xj∂xd′
∂2xd′
∂xg′∂xh
∂2xj
∂xm∂xn′ ;
∂2xf′
∂xf′∂xc
∂2xc
∂xb′∂xa′
∂2xr
∂xs′∂xq′
∂2xs′
∂xr∂xs
∂2xs
∂xl′∂xk′
∂2xb′
∂xj′∂xd
∂2xd
∂xh∂xg′
∂2xj′
∂xn′∂xm .
Then a natural problem is that in computer algebra, how can we judge the
equivalence of indexed diﬀerential expressions?
Symbolic manipulation of indexed expressions, e.g. tensor expressions, is one
of the oldest research topics in computer algebra [1–13]. It remains to be a
challenging problem, for the reason as follows: In order to compute the canonical
form of an indexed polynomial, we need to ﬁnd a ﬁnite Gr¨obner basis for the
ideal generated by the basic syzygies. But unfortunately, the ideal cannot be
ﬁnitely generated, mainly due to the property of dummy index renaming. Eﬀorts
have been made to describe algorithms for simplifying tensor expressions. For
example, Refs. [5,6] presented algorithms to put tensor expressions into canonical
forms with respect to monoterm symmetries and cyclic symmetry. However,
those algorithms are not applicable for indexed diﬀerential expressions, because
index elimination is indispensable for simplifying them.
Liu [14] presented a normalization algorithm for indexed diﬀerential expres-
sions in R2[/∂], which consists of two parts. In the ﬁrst part, a polynomial is
rewritten modulo monoterm symmetries. In the second part, we compute the
canonical form of a polynomial with respect to monoterm symmetries. Then two
polynomials are equal if and only if they have the same canonical forms.
However, the algorithm with respect to monoterm symmetries in [14] has
a factorial complexity. More precisely, suppose that f is a monomial indexed
with i, j, k, where i is the number of functions, j is the number of pairs of
dummy indices, and k is the number of pairs of commutable lower indices. Since
we need to compare all the monomials equivalent to f, the complexity is at least
O(i! × j! × 2k). An other method for computing canonical forms with respect to
monoterm symmetries is due to [6]: First replacing the dummy indices by num-
bers according to the index positions, the tensor names and index classes, then
ﬁnding the smallest element in the equivalence class, i.e., ﬁnding the smallest
from i!n1! . . . nk! objects. So the complexity is at least O(i!n1! . . . nk!), where k
is the number of groups of pairwise interchangeable lower indices, and ni is the
number of indices in each group.
The reason for such factorial complexity in some of these algorithms is that
dummy indices within an indexed polynomial f are described either by original
letters or by integers according to the order of their appearance. Neither of the
descriptions is invariant. In other words, they vary when we rewrite f. Conse-
quently, to ﬁnd the canonical form, we need to list and compare all the elements
in the equivalence class of f.

Normalization of Diﬀerentials Using Function Distance Invariants
287
So a question arises: How do we describe an invariant (with respect to
monoterm symmetries), and provide an eﬃcient normalization algorithm based
on the invariants?
Besides, the normalization algorithm in [14] depends on a skillful and tricky
classiﬁcation of 2nd-order partial diﬀerential functions according to their con-
nections (e.g. circle, chain, maximum lower tree and so on). But the classiﬁcation
is no longer valid for higher order. It appears infeasible to classify partial diﬀer-
ential functions for each order.
Then another question arises: How can we provide a normalization algorithm
that is independent of function classiﬁcations?
To answer the questions, we ﬁrst deﬁne the distance from one indexed func-
tion to another, and deﬁne the type list. Then we prove that they are both invari-
ants. Next, we present an index replacement algorithm, and use it to develop
a normalization algorithm with respect to monoterm symmetries for polynomi-
als in R[/∂], whose complexity is less than the factorial complexity of existing
algorithms, and reduces to at most O(i2) or O(
k
j=1
C2
nj). Finally, by the method
of index replacement, a normalization algorithm is provided for polynomials in
R2[/∂], which is independent of function classiﬁcations.
2
Indexed Diﬀerential Polynomial Ring
In this section we brieﬂy review some notions in [14].
An indexed function is composed of four parts: a function name, a sequence
of upper indices, a sequence of lower indices, and variables. For example, the
Christoﬀel symbol Γ i
kh = Γ i
kh(x) has the function name Γ, upper index i, lower
indices k, h, and variable x. An indexed monomial is the product of indexed func-
tions and obeys Einstein summation convention. A sub-monomial refers to the
product of some indexed functions within an indexed monomial. In an indexed
monomial, a free index occurs only once, and a dummy index occurs twice, as an
upper index and a lower one respectively. If a free index occurs as a lower (or
upper) index, then it is called a covariant (or contravariant) free index.
Einstein summation convention has a basic property of renaming dummy
indices (Ren): If f(i) is an indexed monomial taking i as a dummy index, then
for any j which does not occur in f(i),
f(i) = f(j).
(2)
Let A be a non-associative ring generated by indexed functions, and I be
the two-sided ideal generated by (2). The quotient ring A/I is called Ein-
stein summation ring. The multiplication induced in A/I is called Einstein
multiplication.
Let two overlapping local coordinate neighborhoods on an n-dimensional dif-
ferentiable manifold be (x1, x2, . . . , xn) and (x1′, x2′, . . . , xn′). The former coor-
dinate component indices are denoted by small letters while the latter by small

288
J. Liu
letters with apostrophes. If there is no need to make a distinction between dif-
ferent coordinate systems, the indices are denoted by capital letters.
A partial diﬀerential function /∂B
A1...Ar is deﬁned by
/∂B
A1...Ar := ∂A1...ArxB.
The product of partial diﬀerential functions of order at most r is called an
rth order partial diﬀerential polynomial.
All partial diﬀerential polynomials form a commutative ring under Einstein
multiplication, which is denoted by R[/∂]. The subring composed of partial dif-
ferential polynomials of order at most r is denoted by Rr[/∂]. In particular, M[/∂]
and Mr[/∂] denote the monoids of monomials in R[/∂] and Rr[/∂], respectively.
As an algebraic ring, R[/∂] can be formally deﬁned by the following syzygies.
(i) Evaluations (Eval): If Ar and B are indices of the same coordinate system,
then /∂B
A1...Ar equals 0 if r > 1, and equals δB
A1 if r = 1.
(ii) Unifying symmetry (US): If Ai1, Ai1+1, . . . , Ai1+s are indices in the same
coordinate system, 1 ≤i1 ≤r −s, and Aj0, Aj1, . . . , Ajs is a permutation
of Ai1, Ai1+1, . . . , Ai1+s, then
/∂B
A1...Ar = /∂B
A1...Aj0Aj1...Ajs...Ar.
(3)
(iii) Kronecker rule (Kron): For any indexed monomial M B1...Br
A1...As , if 1 ≤i ≤r
and 1 ≤j ≤s, then
δAi
BjM B1...Br
A1...As = M B1...Br
A1...Ai−1BjAi+1...As = M B1...Bj−1AiBj+1...Br
A1...As
.
(iv) Jacobi rule:
/∂j′
i /∂B
j′A1...As = /∂B
iA1...As.
(v) Leibniz rule: For any sequence of indices I = C1C2 . . . Ct,
t

i=0

(i,t−i)⊢I
/∂j′
I(1)i
/∂B
I(2)j′A1...As = /∂B
IiA1...As.
On the other hand, R[/∂] obviously is also a diﬀerential ring, with the partial
diﬀerential operator formally deﬁned by the following properties.
(i’) Evaluations (Eval): If f is a constant, e.g. f = δj
i , then ∂Af = 0.
(ii’) Unifying symmetry (US): If for some 1 ≤i1 < i2 ≤r, Ai1, Ai1+1, . . . , Ai2
are indices of the same coordinate system, then ∂A1...Ar is symmetric in
Ai1, Ai1+1,. . . , Ai2.
(iii’) Jacobi rule:
/∂j′
i ∂j′ = ∂i.

Normalization of Diﬀerentials Using Function Distance Invariants
289
(iv’) Leibniz rule: For any diﬀerentiable functions f, g, and any sequence of
indices I = C1C2 . . . Ct,
∂I(fg) =
t

i=0

(i,t−i)⊢I
(∂I(1)f)(∂I(2)g).
The following is a generated syzygy.
(v’) Bottom antisymmetry (BS):
/∂j′
k′i∂j′ = −/∂j
ik′∂j.
(4)
By commutativity of multiplication (Com), we mean
/∂B
A1A2...Am /∂D
C1C2...Cn = /∂D
C1C2...Cn /∂B
A1A2...Am,
(5)
where /∂B
A1A2...Am, /∂D
C1C2...Cn are two partial diﬀerential functions in an indexed
monomial.
In what follows, we refer to Ren, US and Com as monoterm symmetries.
3
Distances Between Indexed Functions
In this section, we prove that the type lists and the distances are both invariants
with respect to monoterm symmetries.
Deﬁnition 1. Let f = /∂B
A1A2...Am be a partial diﬀerential indexed function.
Suppose f has i lower dummy indices and j upper dummy indices. Deﬁne the
type list of f as
(m, i, j, LFdn, LF ′
dn, LFup, LF ′
up),
and denote it by TL(f), where LFdn (or LF ′
dn) denotes the sequence of all the
covariant free indices without (or with) apostrophes in alphabetical order, and
LFup (or LF ′
up) denotes the sequence with respect to contravariant free indices.
Deﬁnition 2. Suppose f ∈M[/∂], and the indexed functions from left to right of
f are f 1, f 2, . . . , f n. If f i has m lower dummy indices occurring as upper ones
in f j, then deﬁne the distance from f i to f j as m, and denote it by d⟨f i, f j⟩= m.
Suppose {g1, g2, . . . , gt} is an indexed function sequence. We deﬁne the adja-
cency matrix of the sequence by letting the (i, j)th element of the matrix be
d⟨gi, gj⟩(i, j = 1, 2, . . . , t).
For any two n × n adjacency matrices A and B, if there exist i0, j0 ⩽n, s.t.
ai0j0 < bi0j0; for i = i0, j < j0 and for i < i0, aij = bij, then deﬁne A < B.
Deﬁnition 3. Let f i and f j be two indexed functions. (d⟨f i, f j⟩, d⟨f j, f i⟩) is
called the distance pair between f i and f j, and denoted by D(f i, f j).

290
J. Liu
Example 3. Consider the monomial /∂b′
b′c/∂c
eas′ /∂a
d′ /∂x
tpqr′ /∂w′
xl /∂r′
w′zg. The indexed
functions from left to right are denoted by f 1, f 2,. . .,f 6 respectively. Accord-
ing to Deﬁnition 2, we get the following nonzero distances
d⟨f 1, f 1⟩= 1, d⟨f 1, f 2⟩= 1, d⟨f 2, f 3⟩= 1,
d⟨f 4, f 6⟩= 1, d⟨f 5, f 4⟩= 1, d⟨f 6, f 5⟩= 1.
Notation.
If f1, f2 ∈M[/∂] are equivalent with respect to monoterm symme-
tries, i.e., f1 −f2 ∈¯0 = 0 + Smon, where Smon is the ideal generated by (2), (3)
and (5), then denote it by f1
mon
∼f2. Similarly, we write f1
mon,BS
∼
f2 if they are
equivalent with respect to BS and monoterm symmetries.
Lemma 1. If f1, f2 ∈M[/∂] and f1
mon,BS
∼
f2, then f1 can be rewritten as f2 in
a ﬁnite number of steps by Eqs. (2), (3), (4) and (5).
Proof. We adapt the proof for M2[/∂] in [15] to this case. In what follows, two
indexed monomials are called like terms if they are identical except for constant
coeﬃcients.
Since f1, f2 are equivalent,
f1 −f2 = r1( ¯A1 −¯B1) + r2( ¯A2 −¯B2) + . . . + rn( ¯An −¯Bn),
where r1, r2, . . . , rn are nonzero monomials in M[/∂], ¯Ai and ¯Bi are the two
sides of one of the Eqs. (2), (3), (4) and (5). Obviously, ri ¯Ai and ri ¯Bi are not
like terms. Denote the set {ri ¯Ai, −ri ¯Bi|i = 1, . . . , n} by E, and call ri ¯Ai the
matching monomial of −ri ¯Bi. For any subset X of E, the matching monomials
of all elements in X form a subset of E\X, and we denote it by X.
If both f1 and f2 can be rewritten as 0, the conclusion holds obviously.
Otherwise, without loss of generality, assume that f1 cannot be rewritten as
0. In E, let E1 be {kf1 | k ∈C}  E, and let E2 be 
E1. If none of the elements in
E2 is a product of a constant and f2, then we can construct E3 and E4 as follows.
First in E\
2
i=1
Ei, we can ﬁnd the subset E3, such that the sum of all elements
in E3 is the opposite of the sum of those in E2. Then, let E4 be 
E3
(E\E3).
Obviously E4 ⊆E\
3
i=1
Ei, since 
E3 ⊆E\
2
i=1
Ei.
We claim that E4 is not empty, for the reason as follows.
We ﬁnd all the groups of like terms in E2, and denote them by 
1, 
2, . . . ,

p. Let xi be the term obtained by omitting the coeﬃcient of any element in

i (i = 1, . . . , p). Since f1 cannot be rewritten as 0, for any element kxi ∈E2
(k ̸= 0), there is a unique nonzero ki such that the matching monomial of kxi
is
k
ki f1. Let the sum of all elements in E2 be a1k1x1 + a2k2x2 + . . . + apkpxp,
then by the deﬁnitions of E1 and E2, we get a1 + a2 + . . . + ap = −1. On the
other hand, assume that E4 is empty. Then any element in E3 has its matching
monomial lying in E3. Besides, the matching monomial of kixi can only be kjxj
(i ̸= j), therefore a1 + a2 + . . . + ap = 0, contradicting a1 + a2 + . . . + ap = −1.
Hence E4 cannot be empty.

Normalization of Diﬀerentials Using Function Distance Invariants
291
From the proof of the claim, we also get the following: Let a′
1k1x1 + . . . +
a′
pkpxp be the result of collecting the like terms of 
E4, then a′
1 + . . . + a′
p = 1,
since (−a1 −a′
1) + . . . + (−ap −a′
p) = 0.
Similar to the process of constructing E3 and E4 from E2, if none of the
elements in E4 is a product of a constant and f2, we construct E5 and E6 from
E4 (E6 ⊆E\
5
i=1
Ei). E6 must be non-empty, as otherwise any element in E5
has its matching monomial lying in E5, and it follows that a′
1 + . . . + a′
p = 0,
contradicting a′
1 + . . . + a′
p = 1.
Generally, if none of the elements in E2n (n ∈N) is a product of a constant
and f2, we can obtain a non-empty set E2n+2 (E2n+2 ⊆E\
2n+1

i=1
Ei). Since E is
ﬁnite, such a process cannot be endless, i.e., there exists m such that at least one
element in E2m is a product of a constant and f2. Therefore by the deﬁnitions
of Ei, f1 can be rewritten as cf2 (c ̸= 0). c must equal to 1, which can be proved
by contradiction. Let us assume that c is unequal to 1. Then f1 is equivalent to
0 since f1 and f2 are equivalent. Consequently, f1 can be rewritten as a product
of a constant and 0, contradicting that f1 cannot be rewritten as 0.
Proposition 1. Suppose f1
mon
∼f2.
(i) There is a bijection Ψ from the set of indexed functions of f1 to that of f2.
For any indexed function f i of f1, TL(f i) = TL(Ψ(f i)).
(ii) For any two indexed functions f i and f j, we have d⟨f i, f j⟩= d⟨Ψ(f i), Ψ(f j)⟩
(i may equal j).
Proof. According to Lemma 1, f1 can be rewritten as f2 in several steps. If
the ith step is generated from US (see Eq. (3)), then deﬁne Ψi(/∂B
A1...Ar) =
/∂B
A1...Aj0Aj1...Ajs...Ar, and for any other indexed function f, deﬁne Ψ i(f) = f.
Such Ψi is a bijection, and keeps invariant of type lists.
Since US does not involve interchange of indices among indexed functions,
by the deﬁnition of distance, we have d⟨f i, f j⟩= d⟨Ψ(f i), Ψ(f j)⟩.
A similar argument is applied to Ren and Com.
Then the composition of all the mappings Ψn ◦. . . ◦Ψ2 ◦Ψ1 forms a bijection
denoted by Ψ, and d⟨f i, f j⟩= d⟨Ψ1(f i), Ψ1(f j)⟩= d⟨Ψ2 ◦Ψ1(f i), Ψ2 ◦Ψ1(f j)⟩=
. . . = d⟨Ψ(f i), Ψ(f j)⟩.
4
Normalization with Respect to Monoterm Symmetries
As mentioned in [14], to ﬁnd the canonical form of a polynomial in R2[/∂], it
is suﬃcient to consider the problem in M2[/∂]. Hence in this section, we only
consider M[/∂] and M2[/∂].
According to Proposition 1, the type list and the distance are both invari-
ants with respect to monoterm symmetries. In what follows, ﬁrstly we will sort
indexed functions of a monomial based on the invariants, and then replace the

292
J. Liu
indices with numbers according to the order on indexed functions. Finally an
algorithm with respect to monoterm symmetries is presented, whose complexity
is showed to be smaller than existing algorithms.
Notation. The result of applying Step 1 of the following Algorithm I to f is
denoted by f (tri).
Algorithm I. Indexed function rearrangement algorithm.
Input: All the indexed functions of f.
Output: A sequence of indexed functions.
Step 1. Let Ω = ∅. Rewrite all the sub-monomials /∂A
A and /∂A1
A /∂A2
A1 . . . /∂A
Ar
(except for names of indices) (r ≥1) as 0 and n respectively, and denote the
new monomial by f (tri).
Step 2. Suppose f i, f j are two indexed functions. Then f i < f j if and only if
one of the following conditions holds:
(a) TL(f i) < TL(f j).
(b) TL(f i) = TL(f j), and d⟨f i, f i⟩< d⟨f j, f j⟩.
Step 3. All the indexed functions of f (tri) are classiﬁed as Γ1, Γ2, . . . , Γm,
such that the order on each class has not been deﬁned, and if i < j, then all
elements in Γi are smaller than those in Γj.
Step 4. For each element f
(i1)
j
of Γi1, sort the elements of the set {D(f
(i1)
j
,
f
(i2)
k
)|∀f
(i2)
k
∈Γi2} in ascending order, and get a sequence denoted by
L(D(i1,i2)
j
). Deﬁne f
(i1)
j
< f
(i1)
l
, if and only if
L(D(i1,1)
j
) < L(D(i1,1)
l
),
or
L(D(i1,1)
j
) = L(D(i1,1)
l
), . . . , L(D(i1,h−1)
j
) = L(D(i1,h−1)
l
), L(D(i1,h)
j
) < L(D(i1,h)
l
).
Step 5. If for each i, there is only one element in Γi, then put the sequence
into Ω; If for each i, the order on Γi has not been deﬁned, then go to Step 6.
Otherwise, return to Step 3.
Step 6. Suppose the classes whose elements contain covariant free indices are
Γ (Fdn)
1
, Γ (Fdn)
2
, . . . , Γ (Fdn)
s
. Consider all possible orders on each class, i.e.,
m1! × m2! × . . . × ms! possible orders, where mi is the number of elements of
Γ (Fdn)
i
. Under each possibility, return to Step 3.
Step 7. Find the smallest among the adjacency matrices of the elements of Ω,
and output the corresponding sequences, denoted by [f](min)
1
, [f](min)
2
, . . .,
[f](min)
p
.
Example 4. Sort the indexed functions of
f = /∂x′
ca/∂y′
ab /∂c
y′ghae′ /∂b
m′pqhx′.
(6)

Normalization of Diﬀerentials Using Function Distance Invariants
293
Step 1. Denote the indexed functions of f from left to right by f 1, f 2, f 3, f 4.
By comparing type lists, we get f 1, f 2 < f 4 < f 3.
Step 2. The indexed functions of f are classiﬁed as Γ1={f 1, f 2}, Γ2={f 4},
Γ3 = {f 3}.
Step 3. L(D(1,2)
1
) = {(0, 1)}, L(D(1,2)
2
) = {(1, 0)}. Hence, L(D(1,2)
1
) < L(D(1,2)
1
),
f 1 < f 2.
The following proposition prepares for the complexity analysis of the
algorithms.
Proposition 2. The set Ω in Step 7 of Algorithm I must be non-empty. In
another word, if the order on Γ (Fdn)
j
(j = 1, 2, . . . , s) of Step 6 is deﬁned, then
by Algorithm I, the order on all indexed functions is deﬁned.
Proof. (a) Firstly, if f has a lower dummy index occurring in g, and the order
on the class containing g is deﬁned, then by Algorithm I, the order on the class
containing f can be deﬁned. The reason is as follows. Suppose f
′ and f are in
the same class, then d⟨f, g⟩= 1 ̸= 0 = d⟨f
′, g⟩, consequently D(f, g) ̸= D(f
′, g).
Hence by Algorithm I, f and f
′ are ordered.
(b) If each element of a class Γi of Step 3 does not contain covariant free
indices, then for any element f 0 ∈Γi, there must exist an sequence {f 1, f 2,
. . . ,f r}, satisfying: f i has a lower dummy index occurring in f i+1, 0 ≤i ≤r −1,
and f r contains covariant free indices. Assume that such sequence does not
exist, then it can be derived that f (tri) must contain the submonomial /∂A
A or
/∂A1
A /∂A2
A1 . . . /∂A
Ar (except for index names), contradicting Step 1.
The class containing f r belongs to {Γ (Fdn)
j
|j = 1, 2, . . . , s}, and the order on
it is deﬁned. Therefore, according to (a), the order on the class containing f r−1
can be deﬁned by Algorithm I, and then it holds for f r−2, and so on, ﬁnally the
order on the class Γi containing f 0 is deﬁned. This completes the proof.
Corollary 1. If the covariant free indices in a monomial f have diﬀerent
names, then Step 6 and Step 7 can be skipped.
Proof. Any two indexed functions have no covariant free indices in common, so
their type lists must be diﬀerent. Hence, the order on Γ (Fdn)
i
of Step 6 is deﬁned
by Step 2, and there is only one possibility in Step 6, which implies that Steps
6 and 7 can be skipped.
Algorithm II. Index replacement algorithm.
Input: f ∈M[/∂], with an order on its indexed functions.
output: f.
Step 1. Replacement of the free indices of f.
1. Remove apostrophes from the free indices.
2. Replace the contravariant free indices alphabetically by 1, . . . , N (Fup)
(among which a positive integer may occur more than once, since some
contravariant free indices may have identical names), N (Fup) ≥0.

294
J. Liu
3. Replace the covariant free indices alphabetically by 1+N (Fup), 2+N (Fup),
. . . , N (Fdn)+N (Fup), where N (Fdn) is the number of covariant free indices.
4. Put apostrophes back.
Step 2. Replacement of the dummy indices of f.
1. Sort the indexed functions of f in ascending order.
2. Determine the order of replacement as follows. For any dummy index D,
occurring as an upper index of the i(D)th indexed function, as a lower
one of the j(D)th function, it corresponds to a pair (i(D), j(D)). Then for
any two dummy indices D1, D2, the replacement of D2 is behind D1 if
and only if i(D1) < i(D2), or i(D1) = i(D2) and j(D1) < j(D2).
3. Remove apostrophes from the dummy indices.
4. Replace the upper dummy indices in the order of replacement (determined
in 2 of Step 2) by N (Fdn) + N (Fup) + 1, N (Fdn) + N (Fup) + 2, . . . , N (Fdn) +
N (Fup) + N (Dup), where N (Dup) is the number of upper dummy indices.
5. Replace the lower dummy indices in the order of replacement by N (Fdn)+
N (Fup) + N (Dup) + 1, . . . , N (Fdn) + N (Fup) + 2N (Dup).
6. Put apostrophes back, and to avoid ambiguity, commas are added between
any two neighbor lower indices within an indexed function.
Notation. In Algorithm II, each index is replaced by a positive integer n with
or without an apostrophe. Denote the set {i, i′|i ∈N} by N′.
Deﬁnition 4. A total order ≺on the set N′ is deﬁned as follows. For any two
positive integers a and b, deﬁne a ≺b′ if and only if b −a > 0.
Deﬁnition 5. Suppose f ∈M[/∂], and its indices are elements in N′. The
numerical list of f, denoted by Lf, is identical to C, if Rep(f) is identical to a
constant C. Otherwise list all the indices of f according to the order of appear-
ance from left to right, and bottom to top.
Deﬁnition 6. A total order ≺on the set of numerical lists is deﬁned as follows.
Suppose f1, f2 ∈M[/∂], and ni is the number of elements of Lfi (i = 1, 2).
Lf1 ≺Lf2 if and only if
n1 < n2
or
n1 = n2, Lf1[1] < Lf2[1]
or
n1 = n2, Lf1[1] = Lf2[1], . . . , Lf1[j] = Lf2[j]; Lf1[j + 1] < Lf2[j + 1],
where j ≥1, Lfi[k] denotes the kth element in the list Lfi.

Normalization of Diﬀerentials Using Function Distance Invariants
295
Algorithm III. Normalization with respect to monoterm symmetries.
Input: f ∈M[/∂].
Output: f.
Step 1. Apply Algorithm I to f to get indexed function sequences [f](min)
1
,
[f](min)
2
, . . ., [f](min)
p
.
Step 2. Apply Algorithm II to [f](min)
1
, [f](min)
2
, . . . , [f](min)
p
, and get f (Rep)
1
,
f (Rep)
2
, . . ., f (Rep)
p
respectively.
Step 3. For each f (Rep)
i
(1 ≤i ≤p), carry out the following steps to get f (US)
i
.
1. In each partial diﬀerential function /∂N0
N1...Ns, where Ni ∈N′, ﬁnd all the
indices Ni1, Ni1+1,. . ., Ni1+r s.t. Ni1+k (0 ≤k ≤r, i1 ≥1) are in the
same coordinate system, while Ni1−1 and Ni1+r+1 are in another one.
2. Sort Ni1, Ni1+1,. . ., Ni1+r in ascending order.
Step 4. Let Lf (US)
q
be the smallest among Lf (US)
i
(1 ≤i ≤p). Output f (US)
q
.
Example 5. Normalize
the
monomial
in
(6)
with
respect
to
monoterm
symmetries.
Step 1. By Example 4, f 1 < f 2 < f 4 < f 3.
Step 2. Replace the covariant free indices a, e′, g, h, m′, p, q alphabetically by
1, 2′, 3, 4, 5′, 6, 7.
Step 3. The dummy indices c, x′, b, y′ correspond to (4, 1), (3, 1), (3, 2), (4, 2)
respectively. Hence x′ < b < c < y′. Replace the upper indices x′, b, c, y′ by
8′, 9, 10, 11′ respectively, the lower ones by 12′, 13, 14, 15′.
Step 4. Rewrite /∂9
5′,6,7,4,12′, /∂10
15′,3,4,1,2′ as /∂9
5′,4,6,7,12′, /∂10
15′,1,3,4,2′ respectively.
Step 5. Output
/∂8′
1,14 /∂11′
1,13 /∂9
5′,4,6,7,12′ /∂10
15′,1,3,4,2′.
Now we analyze the complexity of Algorithm III.
Let i be the number of indexed functions in a monomial, m the number of
indexed functions that contain covariant free indices, k the number of groups
of pairwise interchangeable lower indices, and ni the number of indices in each
group.
As mentioned in Sect. 1, the complexity of the algorithm in [6] is O(i!
k
j=1
nj!)
(which does not include the complexity of index replacement step in [6]). Hence,
to compare with [6], we only need to consider Algorithm I and Step 3 of
Algorithm III.
The indexed diﬀerential monomials that we have met in diﬀerential geometry
all satisfy that the covariant free indices have diﬀerent names, therefore we have
the following consideration.
(i) When the covariant free indices have diﬀerent names, by Corollary 1, only
Steps 1–5 of Algorithm I are needed. Step 2 compares indexed functions in
type list and distances, so the complexity is at most C2
i . The order on the
indexed functions that contain covariant free indices is deﬁned in Step 2.

296
J. Liu
Step 4 compares indexed functions of the same class in distance pair
sequence. It is carried out for at most (i−m)
2
times, since according to the
proof of Proposition 2, once it is carried out, the order on at least one class
(whose elements do not contain covariant free indices) is deﬁned. Hence the
complexity of Algorithm I is at most C2
i + (i−m)
2
C2
i−m, or O(i3), much smaller
than O(i!
k
j=1
nj!).
Step 3 of Algorithm III compares interchangeable indices inside partial dif-
ferential functions, so the complexity is
k
j=1
C2
nj, also much smaller than
O(i!
k
j=1
nj!).
(ii) When some covariant free indices have identical names, Step 6 of Algorithm I
considers
s
j=1
mj! possible orders. Step 7 chooses the smallest from
s
j=1
mj!
objects, whose complexity is
s
j=1
mj! −1. Therefore the complexity of Algo-
rithm I is at most C2
i +
s
j=1
mj! (i−m)
2
C2
i−m +
s
j=1
mj! −1). It must be less
than O(i!), since (i−m)
2
C2
i−m < (i −m)! < (i −m1 −m2 −. . . −ms)!.
The complexity of Step 3 in Algorithm III is p
k
j=1
C2
nj, also smaller than
O(i!
k
j=1
nj!), since p <
s
j=1
mj! < i!.
Notation.
The result of applying Algorithm III to f is denoted by f (mon).
Proposition 3. Suppose f1, f2 ∈M[/∂]. If f1
mon
∼f2, then f (tri)
1
mon
∼f (tri)
2
.
Proof. Since the sub-monomials /∂A
A or /∂A1
A /∂A2
A1 . . . /∂A
Ar have no common indices
with other sub-monomials, fi (i = 1, 2) can be divided into two independent
parts. If f1
mon
∼
f2, then the part composed of /∂A
A or /∂A1
A /∂A2
A1 . . . /∂A
Ar in f1 is
equivalent to that in f2 with respect to monoterm symmetries. So is the other
part. This implies f (tri)
1
mon
∼f (tri)
2
.
Theorem 1. Suppose f1, f2 ∈R[/∂]. If f1
mon
∼f2, then f1, f2 are rewritten as
identical forms by Algorithm III. Conversely, if two polynomials are rewritten as
identical forms, they must be equivalent.
Proof. By Proposition 3, f (tri)
1
mon
∼f (tri)
2
. Therefore by Proposition 1, there is a
bijection Ψ from the set of indexed functions of f (tri)
1
to that of f (tri)
2
, and Ψ
keeps invariant of type lists and distances.
In Algorithm I, the order on indexed functions is determined only by type lists
and distances. Therefore, if the sequence {f11, . . . , f1k} is among the output of

Normalization of Diﬀerentials Using Function Distance Invariants
297
applying Algorithm I to f (tri)
1
, i.e., it belongs to {[f1](min)
1
, [f1](min)
2
, . . . , [f1](min)
p
},
then
{Ψ(f11), . . . , Ψ(f1k)} ∈{[f2](min)
1
, [f2](min)
2
, . . . , [f2](min)
p
}.
Let f ′
1 be the product of f11, . . ., f1k, and f ′
2 be the product of Ψ(f11), . . .,
Ψ(f1k). Because the replacement of dummy indices in Algorithm II is uniquely
determined by the order on functions, f ′
1, f ′
2 are rewritten as equivalent monomi-
als with respect to US, and ﬁnally as identical forms by Step 3 of Algorithm III.
5
Normalization
In this section, we present a normalization algorithm for monomials in M2[/∂]
by using the method of index replacement.
According to Proposition 4.1 in [14], suppose f1 and f2 are equivalent, if
we rewrite fi as f ′
i (i = 1, 2) successively by elimination, mixed rewriting
of unmixed interior circle, and coordinate system uniﬁcation of self-restrained
dummy indices, then f
′
1
mon,BS
∼
f
′
2. Hence in order to ﬁnd the canonical form of
any monomial, we only need to develop a normalization algorithm with respect
to BS and monoterm symmetries.
Suppose g
mon,BS
∼
f. Since g(mon) and f (mon) have identical set of indices, the
set
	
g(mon)|g
mon,BS
∼
f

is ﬁnite. Therefore we can choose the element associated
with the smallest numerical list from the set as the canonical form of f, and have
the following normalization algorithm.
Algorithm IV. Normalization with respect to BS and monoterm symmetries.
Input: f ∈M2[/∂], assuming all the covariant free indices have diﬀerent names.
Output: The canonical form of f with respect to BS and monoterm symmetries.
Step 1. Apply Algorithm III to f to get f (Rep) and f (mon).
Step 2. Let S1 = {Lf (mon)}, S2 = ∅, R1 = {(f (Rep), 0)}, R2 = ∅, Θ = 1, and
N (Dup) be the number of upper dummy indices of f (Rep).
Step 3. Let R2 = R1, R1 = ∅, Θ = (−1)Θ.
Step 4. For the ﬁrst component g of each element (g, n(g)) in R2, carry out the
following steps.
1. In g, ﬁnd the pairs of functions in one of the four forms
(a) /∂n
′
3
n′
1n2 and /∂N6
n′
4N5,
(b) /∂n
′
3
n′
1n2 and /∂N6
n′
5n′
4,
(c) /∂n3
n1n′
2 and /∂N6
n4N5,
(d) /∂n3
n1n′
2 and /∂N6
n5n4,
such that ni (i = 1, . . . , 5) is a positive integer, n3 ̸= n(g), n4 = n3 +
N (Dup), and N5, N6 ∈N′.

298
J. Liu
2. Rewrite the couple functions by
/∂n
′
3
n′
1n2 /∂N6
n′
4N5 −→−/∂n3
n2n′
1 /∂N6
n4N5,
/∂n
′
3
n′
1n2 /∂N6
n′
5n′
4 −→−/∂n3
n2n′
1 /∂N6
n4n′
5,
/∂n3
n1n′
2 /∂N6
n4N5 −→−/∂n
′
3
n′
2n1 /∂N6
n′
4N5,
/∂n3
n1n′
2 /∂N6
n5n4 −→−/∂n
′
3
n′
2n1 /∂N6
n′
4n5
respectively, and denote the new monomial by g′.
3. Apply Algorithm III to g′, and get g′(mon).
4. If Θ = −1 and Lg′(mon) /∈S2, then add Lg′(mon) and the ordered pair
(g′, n3) to S2 and R1 respectively.
5. If Θ = 1 and Lg′(mon) /∈S1, then add Lg′(mon) and the ordered pair (g′, n3)
to S1 and R1 respectively.
Step 5. If R1 = ∅, let Lh be the smallest among S1
 S2, and output h. Other-
wise, return to Step 3.
Remark 1
(i) In the above algorithm, Θ is the sign symbol of a monomial. Two monomi-
als with diﬀerent sign symbols are impossibly identical, hence are put into
diﬀerent sets S1 and S2.
(ii) Suppose f can be rewritten as g. When we rewrite g, to prevent the result
from being f, we use the positive integer n(g) as a criterion.
Due to Algorithm IV, we directly have the following normalization algorithm.
Algorithm V. Normalization algorithm.
Input: f ∈M2[/∂].
Output: The canonical form of f.
Step 1. Apply the simpliﬁcation algorithm in [14] to f.
Step 2. Carry out the mixed rewriting of unmixed interior circle and coordinate
system uniﬁcation of self-restrained dummy indices [14]:
/∂a
′
2
b1a1 /∂a3
b′
2a′
2 /∂a
′
4
b3a3 . . . /∂a
′
2k
b2k−1a2k−1 /∂a1
b′
2ka′
2k −→
/∂a2
b1a′
1 /∂a
′
3
b′
2a2 /∂a4
b3a′
3 . . . /∂a2k
b2k−1a′
2k−1 /∂a
′
1
b′
2ka2k,
and
/∂d
dc′ /∂c′
b′a −→/∂d′
d′c/∂c
ab′.
Step 3. Divide the covariant free indices into groups by name. The order of
replacement of indices in diﬀerent groups are alphabetical, and the order
within each group is arbitrary.
Step 4. Under all the possible orderings, apply Algorithm IV, and denote all
the outputs by f1, . . . , fp.

Normalization of Diﬀerentials Using Function Distance Invariants
299
Step 5. Let fs (1 ≤s ≤p) be the monomial which has the smallest numerical
list among Lfi (i = 1, . . . , p). Output fs.
Example 6. Put f = /∂r′
a′
1a2 /∂t′
a′
3a4 /∂d
dt′ /∂c
r′s′ /∂p
ce′ /∂l
k′ into canonical form.
Step 1. By the ﬁrst two steps of Algorithm V, /∂t′
a′
3a4/∂d
dt′ is rewritten as /∂t
a4a′
3/∂d′
d′t.
Step 2. According to Algorithm IV, rewrite f by BS. And among all the
results, we ﬁnd that h(mon) = /∂1
8′ /∂10
4,3′ /∂11
6,5′ /∂2
7′,16′ /∂12′
9′,14 /∂13′
17′,15 has the small-
est numerical list. Since the numbers from 1 to 9 denote the free indices
l, p, k′, a′
1, a2, a′
3, a4, e′, s′, output /∂l
k′ /∂10
a2a′
1 /∂11
a4a′
3 /∂p
e′,16′ /∂12′
s′,14 /∂13′
17′,15.
Example 7. Prove that Hi
jk in Example 1 is a (1, 2)-typed tensor.
It suﬃces to verify Eq. (1).
Since h is a (1, 1)-typed tensor, hi′
j′ = /∂j
j′ /∂i′
i hi
j. Substituting the expressions
of Hi
jk, Hi′
j′k′ (given in Example 1) and hi′
j′ into the left side of (1), we get a
polynomial with 16 terms, denoted by f. Each term of f is in M2[/∂, h], which
is a monoid composed of the partial derivatives of h and M2[/∂] (see also [14]).
Note that Algorithm III is independent of the diﬀerential function name /∂(or
we can take h as /∂). Besides, the simpliﬁcation algorithm has been extended
to M2[/∂, h], as presented by [14]. Hence, Algorithm V can put f into canonical
form. For each term of f, we ﬁnd that there is another term such that the sum
of their canonical forms is 0. For instance, the two terms /∂i′
i /∂s
p′hi
s/∂k
k′ /∂p′
j′php
k and
/∂m′
t
/∂k
k′ht
k /∂j
m′j′ /∂i′
i hi
j have the canonical forms ∓/∂4
k′ /∂i′
9 h5
11h6
8 /∂7
10,j′. Therefore, the
canonical form of f is 0.
Acknowledgements. The author is grateful to the reviewers for helpful comments.
This work was supported by Natural Science Foundation of Shanghai (15ZR1401600).
References
1. Fulling, S.A., King, R.C., Wybourne, B.G., Cummins, C.J.: Normal forms for
tensor polynomials: I. The Riemann tensor. Class. Quantum Grav. 9, 1151–1197
(1992)
2. Christensen, S., Parker, L.: MathTensor, A System for Performing Tensor Analysis
by Computer. Addison-Wesley, Boston (1994)
3. Ilyin, V.A., Kryukov, A.P.: ATENSOR-REDUCE program for tensor simpliﬁca-
tion. Comput. Phys. Commun. 96, 36–52 (1996)
4. Ja´en, X., Balfag´on, A.: TTC: symbolic tensor calculus with indices. Comput. Phys.
12, 286–289 (1998)
5. Portugal, R.: An algorithm to simplify tensor expressions. Comput. Phys. Com-
mun. 115, 215–230 (1998)
6. Portugal, R.: Algorithmic simpliﬁcation of tensor expressions. J. Phys. A: Math.
Gen. 32, 7779–7789 (1999)
7. Portugal, R.: The Riegeom package: abstract tensor calculation. Comput. Phys.
Commun. 126, 261–268 (2000)

300
J. Liu
8. Balfag´on, A., Ja´en, X.: Review of some classical gravitational superenergy tensors
using computational techniques. Class. Quantum Grav. 17, 2491–2497 (2000)
9. Manssur, L.R.U., Portugal, R., Svaiter, B.F.: Group-theoretic approach for sym-
bolic tensor manipulation. Int. J. Mod. Phys. C. 13, 859–880 (2002)
10. Manssur, L.R.U., Portugal, R.: The Canon package: a fast kernel for tensor manip-
ulators. Comput. Phys. Commun. 157, 173–180 (2004)
11. Mart´ın-Garc´ıa, J.M., Portugal, R., Manssur, L.R.U.: The Invar tensor package.
Comput. Phys. Commun. 177, 640–648 (2007)
12. Mart´ın-Garc´ıa, J.M., Yllanes, D., Portugal, R.: The Invar tensor package: diﬀer-
ential invariants of Riemann. Comput. Phys. Commun. 179, 586–590 (2008)
13. Liu, J., Li, H.B., Zhang, L.X.: A complete classiﬁcation of canonical forms of a class
of Riemann tensor indexed expressions and its applications in diﬀerential geometry
(in Chinese). Sci. Sin. Math. 43, 399–408 (2013)
14. Liu, J., Li, H.B., Cao, Y.H.: Simpliﬁcation and normalization of indexed diﬀeren-
tials involving coordinate transformation. Sci. China Ser. A. 52, 2266–2286 (2009)
15. Liu, J.: Simpliﬁcation and normalization of indexed polynomials. Ph.D. Thesis,
Chinese Academy of Sciences, Beijing (2009)

Symbolic-Numeric Integration of the Dynamical
Cosserat Equations
Dmitry A. Lyakhov1(B), Vladimir P. Gerdt3,4, Andreas G. Weber5,
and Dominik L. Michels1,2
1 Visual Computing Center, King Abdullah University of Science and Technology,
Al Khawarizmi Building, Thuwal 23955-6900, Kingdom of Saudi Arabia
{dmitry.lyakhov,dominik.michels}@kaust.edu.sa
2 Department of Computer Science, Stanford University, 353 Serra Mall, Stanford,
CA 94305, USA
michels@cs.stanford.edu
3 Laboratory of Information Technologies, Joint Institute for Nuclear Research,
6 Joliot–Curie St., Dubna 141980, Russian Federation
4 Peoples’ Friendship University of Russia, 6 Miklukho–Maklaya St.,
Moscow 117198, Russian Federation
gerdt@jinr.ru
5 Institute of Computer Science II, University of Bonn, Friedrich-Ebert-Allee 144,
53113 Bonn, Germany
weber@cs.uni-bonn.de
Abstract. We devise a symbolic-numeric approach to the integration of
the dynamical part of the Cosserat equations, a system of nonlinear par-
tial diﬀerential equations describing the mechanical behavior of slender
structures, like ﬁbers and rods. This is based on our previous results on
the construction of a closed form general solution to the kinematic part
of the Cosserat system. Our approach combines methods of numerical
exponential integration and symbolic integration of the intermediate sys-
tem of nonlinear ordinary diﬀerential equations describing the dynamics
of one of the arbitrary vector-functions in the general solution of the
kinematic part in terms of the module of the twist vector-function. We
present an experimental comparison with the well-established general-
ized α-method illustrating the computational eﬃciency of our approach
for problems in structural mechanics.
Keywords: Analytical solution · Cosserat rods · Dynamic equations ·
Exponential integration · Generalized α-method · Kinematic equations ·
Symbolic computation
1
Introduction
Deformable-body dynamics can be considered as a subarea of continuous
mechanics that studies motion of deformable solids subject to the action of
internal and external forces (cf. [11]). The equations describing the dynamics of
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 301–312, 2017.
DOI: 10.1007/978-3-319-66320-3 22

302
D.A. Lyakhov et al.
such solids are nonlinear partial diﬀerential equations (PDEs) whose independent
variables are three spatial coordinates and time. Given a particular deformable
mechanical structure, to describe its dynamics, it is necessary to satisfy these
equations at each point of the structure together with appropriate boundary
conditions. For a mechanical structure having special geometric properties, it is
worthwhile to exploit these properties to develop a simpliﬁed but geometrically
exact mechanical model of the structure. Classical examples of such models are
Cosserat theories of shells and rods; see e.g. [18] and references therein.
Rods are nearly one-dimensional structures whose dynamics can be described
by the Cosserat theory of (elastic) rods (cf. [1], Chap. 8; [18], Chap. 5; and the
original work [5]). This is a general and geometrically exact dynamical model
that takes bending, extension, shear, and torsion into account, as well as rod
deformations under external forces and torques. In this context, the dynamics of
a rod is described by a governing system of twelve ﬁrst-order nonlinear partial
diﬀerential equations (PDEs) with a pair of independent variables (s, t), where s
is the arc-length and t the time parameter. In this PDE system, the two kinematic
vector equations ((9a)–(9b) in [1], Chap. 8) are parameter free and represent the
compatibility conditions for four vector functions κ, ω, ν, v in (s, t). Whereas the
ﬁrst vector equation only contains two vector functions κ and ω, the second one
contains all four vector functions κ, ω, ν, v. The remaining two vector equations
in the governing system are dynamical equations of motion and include two
more dependent vector variables ˆm(s, t) and ˆn(s, t). Moreover, these dynamical
equations contain parameters (or parametric functions of s) to characterize the
rod and to include the external forces and torques. Studying the dynamics of
Cosserat rods has various scientiﬁc and industrial applications, for example, in
civil and mechanical engineering (cf. [2]), microelectronics and robotics (cf. [3]),
biophysics (cf. [7] and references therein), and visual computing (cf. [15]).
Because of its inherent stiﬀness caused by diﬀerent deformation modes, the
treatment of the underlying equations usually requires the application of speciﬁc
solvers; see e.g. [16]. In order to reduce the computational overhead caused by
the stiﬀness, we employed Lie symmetry based integration methods (cf. [9,17])
and the theory of completion to involution (cf. [19]) to the two kinematic vector
equations (cf. [1], Chap. 8, Eq. (9a)–(9b)) and constructed their general and
analytic solution in [13,14], which depends on two arbitrary vector functions in
(s, t).
In this contribution, we exploit the general analytic solution to the kine-
matic part of the governing Cosserat system constructed in [13,14] and develop
a symbolic-numeric approach to the integration of the dynamical part of the sys-
tem. Our approach combines the ideas of numerical exponential integration (see
e.g. [8,12] and references therein) and symbolic integration of the intermediate
system of nonlinear ordinary diﬀerential equations describing the dynamics of the
arbitrary vector function in the general solution to the kinematic part in terms
of the module of the twist vector function. The symbolic part of the integration
is performed by means of Maple. We present an experimental comparison of the

Symbolic-Numeric Integration of the Dynamical Cosserat Equations
303
computational eﬃciency of our approach with that of the generalized α-method
for the numerical integration of problems in structural mechanics (see e.g. [20]).
This paper is organized as follows. In Sect. 2, we present the governing PDE sys-
tem in the special Cosserat theory of rods and analytic solution to its kinematic
part constructed in [13,14]. In Sect. 3, we show ﬁrst that the (naive) straight-
forward numerical integration of the dynamical part of Cosserat system has
a severe obstacle caused by a singularity in the system. Then we describe a
symbolic-numeric method to integrate the dynamical equations based on the
ideas of exponential integration and the construction of a closed form analytic
solution to the underlying nonlinear dynamical system. In doing so, we show
that this symbolic-numeric method is free of the singularity problem. In Sect. 4,
we present an experimental comparison of our method with the generalized α-
method. Some concluding remarks are given in Sect. 5.
2
Governing Cosserat Equations and the General
Solution of Their Kinematic Part
The governing PDE system in the special Cosserat theory of rods (cf. [1,3,5,13,14])
can be written in the following form:
κt = ωs −ω × κ,
(1a)
νt = vs + κ × v −ω × ν,
(1b)
ρJ · ωt = ˆms + κ × ˆm + ν × ˆn −ω × (ρJ · ω) + L,
(1c)
ρAvt = ˆns + κ × ˆn −ω × (ρAv) + F .
(1d)
Here, the independent variable t denotes the time and another independent
variable s the arc-length parameter identifying a material cross section of the rod,
which consists of all material points whose reference positions are on the plane
perpendicular to the rod at s. The Darboux vector-function κ = 3
k=1 κkdk
and the twist vector-function ω = 3
k=1 ωkdk are determined by the kinematic
relations
∂sdk = κ × dk,
∂tdk = ω × dk,
where the vectors d1, d2, and d3 := d1 × d2 form a right-handed orthonor-
mal moving frame. These vectors are called directors. The use of the triple
(d1, d2, d3) is natural for the intrinsic description of the rod deformation. More-
over, r describes the motion of the rod relative to the ﬁxed frame (e1, e2, e3).
This is illustrated in Fig. 1.
In doing so, the motion of a rod is deﬁned by the mapping
[a, b] × R ∋(s, t) →(r(s, t), d1(s, t), d2(s, t), d3(s, t)) ∈E3.
Furthermore, the governing system (1a)–(1d) includes additional vector-
valued dependent variables: linear strain ν of the rod and the velocity v of
the material cross-section:
ν := ∂sr =
3

k=1
νkdk,
v := ∂tr =
3

k=1
vkdk.

304
D.A. Lyakhov et al.
r(s, t)
r(s, t)
d1
d1
d2
d2
d3
d3
e1
e1
e2
e2
e3
e3
s = a
s = a
s = b
s = b
Fig. 1. The vector set {d1, d2, d3} forms a right-handed orthonormal basis. The direc-
tors d1 and d2 span the local material cross-section, whereas d3 is perpendicular to
the cross-section. Note that in the presence of shear deformations d3 is unequal to the
tangent ∂sr of the centerline of the rod.
The components of the strain variables κ and ν describe the deformation
of the rod: the ﬂexure with respect to the two major axes of the cross section
(κ1, κ2), torsion (κ3), shear (ν1, ν2), and extension (ν3).
The kinematic part of the governing Cosserat system consists of equa-
tions (1a)–(1b) ((9a)–(9b) in [1], Chap. 8). The remaining equations (1c)–(1d)
((9c)–(9d) in [1], Chap. 8) make up the dynamical part of the governing equa-
tions. For a rod density ρ(s) and cross section A(s), these equations follow from
Newton’s laws of motion:
ρ(s)A(s)∂tv = ∂sn(s, t) + F (s, t),
∂th(s, t) = ∂sm(s, t) + ν(s, t) × n(s, t) + L(s, t),
(2)
where m(s, t) = 3
k=1 mk(s, t) dk(s, t) are the contact torques, n(s, t) =
3
k=1 nk(s, t) dk(s, t) are the contact forces, h(s, t) = 3
k=1 hk(s, t) dk(s, t) are
the angular momenta, and F (s, t) and L(s, t) are the external forces and torque
densities.
The contact torques m(s, t) and contact forces n(s, t) corresponding to the
internal stresses, are related to the extension and shear strains ν(s, t) as well as
to the ﬂexure and torsion strains κ(s, t) by the constitutive relations
m(s, t) = ˆm (κ(s, t), ν(s, t), s) ,
n(s, t) = ˆn (κ(s, t), ν(s, t), s) .
Under certain reasonable assumptions (cf. [1,3,13]) on the structure of the right-
hand sides in (2), they take the form (1c)–(1d) in which J is the inertia tensor
of the cross section per unit length. Unlike the kinematic part, the dynamical
part contains parameters characterizing the rod under consideration: ρ, A and
J together with the external force F and torque L, whereas the kinematic part
is parameter free.
In our previous papers [13,14], by treating the kinematic Cosserat equa-
tions (1a)–(1b) with computer algebra aided methods of the modern Lie sym-
metry analysis (cf. [9,17]) and the theory of completion of partial diﬀerential

Symbolic-Numeric Integration of the Dynamical Cosserat Equations
305
systems to involution (cf. [19]), we constructed the following closed form of an
analytical solution to the kinematic part and proved its generality:
ω = pt + p −sin(p)
p3

p (p · pt) −p2 pt

−1 −cos(p)
p2
p × pt,
(3a)
κ = ps + p −sin(p)
p3

p (p · ps) −p2 ps

−1 −cos(p)
p2
p × ps,
(3b)
ν = q × κ −qs,
(3c)
v = q × ω −qt,
where p(s, t) and q(s, t) are arbitrary analytic vector functions, and
p =

p2
1 + p2
2 + p2
3.
For the eﬃcient numerical solving of the dynamical Cosserat equations (1c)–
(1d), we use the following fact: the vector equation (3a) uniquely deﬁnes pt in
terms of p and ω. We formulate this fact as the following statement.
Proposition 1. The temporal derivative pt of the vector function p, as a solu-
tion of (3a), reads
pt = p · ω
p2
p + 1
2 p × ω −p
2 cot
p
2

· p × (p × ω)
p2
.
(4)
Proof. The vector function pt occurs linearly in (3a). In the component form, it
is a linear system of three equations in three unknowns (p1)t, (p2)t, (p3)t whose
matrix has a non-singular determinant (cf. formula (11) in [14])
2 cos(p) −1
p2
.
(5)
The vector form of pt as a solution to equality (3a) is given by (4). This can
be veriﬁed either by hand computation or by using the routines of the Maple
package VectorCalculus after the substitution of (4) into the right-hand side
of (3a) and simpliﬁcation of the obtained expression to ω.
⊓⊔
Instead of system (1a)–(1d) for unknowns (ω, κ, ν, v), we are going to solve
the equivalent system
pt = p · ω
p2
p + 1
2 p × ω −p
2 cot
p
2

· p × (p × ω)
p2
,
(6a)
qt = q × ω −v,
(6b)
ρJ · ωt = ˆms + κ × ˆm + ν × ˆn −ω × (ρJ · ω) + L,
(6c)
ρAvt = ˆns + κ × ˆn −ω × (ρAv) + F
(6d)
for unknown vector functions (p, q, ω, v), where κ and ν are given by (3b)–(3c).

306
D.A. Lyakhov et al.
3
Symbolic-Numeric Integration Method
3.1
Naive Approach: Explicit Numerical Solving
Suppose we know the values of the vector-functions ω and p on a time layer t.
Then pt in (6a) can be approximated by the forward Euler diﬀerence
pt →p(s, t + △t) −p(s, t)
△t
.
However, since Eq. (6a) has a singularity at p = 2π related with vanishing (5),
there is a restriction to the time step Δt caused by the condition
p(s, t + △t) ∈(0, 2π)
(7)
to be held for all values of s. This is a severe problem for the numerical solving of
the governing Cosserat system, because in the course of solving, one must control
the time step at every value of s to keep the values of p(s, t) within the interval
indicated in (7). This problem is resolved in the symbolic-numeric integration
method described in the next subsection.
3.2
Advanced Approach Based on Exponential Integration
To avoid the problem of controlling the condition (7) we use the diﬀerential
equation (6a) for p(t) and rewrite it in terms of p and the unit vector e where
p = pe. It leads to the following diﬀerential system:
pt = e · ω,
2 et = e × ω −cot
p
2

e × (e × ω).
Now assume that the vector ω is independent of t on the time interval Δt and
choose the Cartesian coordinate system e1, e2, e3 such that e3||ω:
e = A1 e1 + A2 e2 + A3 e3,
ω = ω e3,
ω :=

ω2
1 + ω2
2 + ω2
3.
Then, we obtain the following system of four ﬁrst-order diﬀerential equations:
2 (A1)t = A2 ω −cot
p
2

A1A3 ω,
(9a)
2 (A2)t = −A1 ω −cot
p
2

A2A3 ω,
(9b)
2 (A3)t = −cot
p
2

(A2
3 −1) ω,
(9c)
pt = A3 ω.
(9d)
From the Eqs. (9c)–(9d) it follows
2 A3(A3)t
A2
3 −1
= −cot
p
2

pt,

Symbolic-Numeric Integration of the Dynamical Cosserat Equations
307
and hence,
(1 −A2
3) sin2 p
2

= C,
Ct = 0.
(10)
Equation (10) immediately implies the following statement providing fulﬁllment
of (7).
Proposition 2. If C ̸= 0, then p(t) ∈(0, 2π) for all t ≥t0 if p(t0) ∈(0, 2π).
If one substitutes A3 = pt/ω from (9c) and replaces p with q := cos
 p
2

, then
(10) takes the form
4 q2
t = ω2 
1 −q2 −C

.
This equation is easily solvable by the Maple routine dsolve which outputs
four solutions. These solutions can be uniﬁed into the general solution
q =
√
1 −C sin
	1
2ω(C1 −t)

,
(C1)t = 0.
Then, the whole system (9a)–(9d) admits the following general analytic solution
A1(s, t) = −
√
C · sin( 1
2ω(C2 −t))

ω2 cos2( 1
2ω(C1 −t)) + C sin2( 1
2ω(C1 −t))
,
(11a)
A2(s, t) =
√
C · cos( 1
2ω(C2 −t))

ω2 cos2( 1
2ω(C1 −t)) + C sin2( 1
2ω(C1 −t))
,
(11b)
A3(s, t) =
√
ω2 −C · cos( 1
2ω(C1 −t))

ω2 cos2( 1
2ω(C1 −t)) + C sin2( 1
2ω(C1 −t))
,
(11c)
p(s, t) = 2 arccos
√
ω2 −C sin( 1
2ω(C1 −t))
ω

,
(11d)
where C, C1, C2 are functions of s. These functions are determined by the
following initial data:
C(s) := ω2 
1 −A2
3(s, t0)

sin2
	p(s, t0)
2

,
C1(s) := t0 + A3(s, t0)| sin(p(s, t0))|

ω2 −C(s)
,
C2(s) := t0 + 2
ω arctan
	A1(s, t0)
A2(s, t0)

.
Proposition 3. C(s) ≡0 if and only if A3(s, t) = ±1 which corresponds to a
degenerated solution
A1(s, t) = 0,
A2(s, t) = 0,
p(s, t) = p(s, t0) ± ωt.

308
D.A. Lyakhov et al.
Computationally, this solution is not of interest, since it is unstable: a small (e.g.
numerical) deviation of A3(s, t0) from ±1 converts the solution into a generic
one.
Proof. The solution
p(s, t) = p(s, t0) ± ωt, A3(s, t) = ±1, A2(s, t) = A1(s, t) = 0
is singular. Unlike the generic solution, the value of |p(t)| in this solution may
increase indeﬁnitely. However, it is unstable, since any small perturbation ϵ > 0
to the initial value A3(s, t0) = ± (1 −ϵ(s)) leads to the generic case when p(t)
remains bounded.
□
Our symbolic-numeric approach to the derivation of equations (9a)–(9d) and
the construction of their explicit analytic solution (11a)–(11d) is in accord with
the general principles of exponential integration (see e.g. [8]). The basic idea
behind exponential integration is the identiﬁcation of a prototypical diﬀerential
system which has the stiﬀness properties similar to those in the original equation
and which admits explicit solving.
In our case, the stiﬀness properties of the diﬀerential system (9a)–(9d) are
similar to those in system (6a)–(6d). In doing so, the last system belongs to the
second class of stiﬀproblems (cf. [8], p. 210) whose stiﬀness is caused by the
highly oscillatory behavior of their solutions. For such problems both explicit
and implicit Euler schemes fail to provide the required stability unless the step
size is strongly reduced to provide the resolution of all the oscillations in the
solution. Thereby, the standard numerical treatment of the equations (1a)–(1d)
and hence equations (6a)–(6d) is computationally ineﬃcient. Just by this reason,
special numerical solvers have been designed (cf. [10,20]) for the Eqs. (1a)–(1d).
Figure 2 illustrates the stiﬀness of the diﬀerential system (6a)–(6d). The
behavior is shown, at ω = 1, of the functions p(s0, t) and A3(s0, t) as solu-
tions of (9c)–(9d) for the initial conditions p(s0, 0) = 1 and A3(s0, 0) = 0.99. As
illustrated, the solution oscillates and changes drastically over time. A numeri-
cal reconstruction of such a behavior is possible only for very small step sizes of
diﬀerence approximations.
4
Numerical Comparison with the Generalized α-Method
In 1993, Chung and Hulbert (cf. [4]) presented the generalized α-method as a
new integration algorithm for problems from structural mechanics. It is char-
acterized primarily by a controllable numerical dissipation of high-frequency
components in the numerical solution. These occur, for example, in the con-
text of ﬁnite element-based simulations, when the high-frequency states are too
roughly resolved. Such methods usually improve the convergence behavior of
iterative solving strategies for nonlinear problems.
The generalized α-method is well-established in the ﬁeld of structural
mechanics and has the major advantage of unconditional stability as well as

Symbolic-Numeric Integration of the Dynamical Cosserat Equations
309
Fig. 2. Illustration of the temporal evolution of p(s0, t) (left) and A3(s0, t) (right).
user controllable numerical damping. The idea of the introduction of a con-
trollable numerical damping in the integration process is not new and found,
among other things, realization in α-HHT (cf. [6]) or in the WBC-α-method
(cf. [21]). The aim of the development of such methods is to maximize the
attenuation of high-frequency components while preserving the important low-
frequency components. Using the generalized α-method, this ratio is optimal,
i.e., for a given attenuation of the high-frequency components, the attenuation of
the low-frequency components is minimized. A brief speciﬁcation of this method
for general problems in structural mechanics is given in Appendix A.
Following [20], using a state vector
x(s, t) = (v(s, t), ω(s, t), κ(s, t), n(s, t))T
describing the rod, we can rewrite its equations of motion (1a)–(1d) in terms of
a system
ˆM∂tx(s, t) + ˆK∂sx(s, t) + Λ(s, t) = 0.
(12)
Here
ˆM = diag(ρA, ρA, ρA, ρI1, ρI2, ρI3, 1, 1, 1, 0, 0, 0)
is the mass matrix and ˆK = −adiag(1, 1, K, 1) is the stiﬀness matrix with K =
diag(EI1, EI2, Gμ), in which the bending stiﬀness in the direction of the principal
components of the cross section A is denoted by EI1,2, and the torsional stiﬀness
by Gμ. As above, the rod’s density is given by ρ, the Young’s modulus by E, and
the shear modulus by G. The nonlinear terms are included in the nonlinearity Λ.
We do not explicitly write out the resulting equations here for brevity and refer
to [20] for the explicit form of (12). In the generalized α-method, the update

310
D.A. Lyakhov et al.
schemes of positions and velocities at point i in time correspond to those of
the classical Newmark integrator. Its accurate and eﬃcient application in the
context of the simulation of elastic rods was demonstrated in [20].
According to [20], we consider four diﬀerent test cases: (i) a sine-like shaped
rod which is released under gravity from a horizontal position (no damping);
(ii) a highly damped helical rod subject to a time-varying end point load (the
damping is obtained by setting the integration parameters (see Appendix A) of
the generalized α-method to α := αm = αf = 0.4, β = 0, and γ = 1.0); (iii) a
straight rod (45 cm) subject to a time-varying torque; (iv) a helical rod with low
damping that is excited by a force parallel to the axis of the helix and released
after 0.1 s showing the typical oscillating behavior of a steel-like coil spring.
We simulate these scenarios using the generalized α-method and the
symbolic-numeric integration scheme presented in this contribution. All ﬁbers are
discretized using 100 individual segments. Using the symbolic-numeric method,
damping is incorporated using the linear Rayleigh damping as described in [15].
We enforce a maximally tolerated relative L2-error of 1% in the position and
velocity space in order to ensure suﬃcient accuracy and measure the required
computation time on a machine with an Intel(R) Xeon E5 with 3.5 GHz and 32
GB DDR-RAM without parallelization. For all test cases, we obtain signiﬁcant
speedups of the presented symbolic-numeric method (“snm”) compared to the
generalized α-method (“α”), in particular:
(i) speedup of a factor more than 20 (α: 4.1 s; snm: 0.2 s);
(ii) speedup of over 21× (α: 4.3 s; snm: 0.2 s);
(iii) speedup of approx. 19× (α: 3.8 s; snm: 0.2 s);
(iv) speedup of approx. 34× (α: 6.8 s; snm: 0.2 s).
Please note that the computation time of 0.2 s of the presented symbolic-numeric
method is constant for all test cases (with identical duration of 8 s).
5
Conclusison
Based on the closed form solution to the kinematic part (1a)–(1b) of the gov-
erning Cosserat system (1a)–(1d) and with assistance of Maple, we have devel-
oped a new symbolic-numeric method for their integration. Our computational
experiments demonstrate the superiority of the new method over the general-
ized α-method for the accurate and eﬃcient integration of Cosserat rods. Its
application prevents from numerical instabilities and allows for highly accurate
and eﬃcient simulations. This clearly shows the usefulness of the constructed
analytic solution to the kinematic equations and should enable more complex
and realistic Cosserat rod-based scenarios to be explored in scientiﬁc computing
without compromising eﬃciency.
Acknowledgements. The authors appreciate the insightful comments of the anony-
mous referees. This work has been partially supported by the King Abdullah Univer-
sity of Science and Technology (KAUST baseline funding), the Max Planck Center

Symbolic-Numeric Integration of the Dynamical Cosserat Equations
311
for Visual Computing and Communication (MPC-VCC) funded by Stanford Univer-
sity and the Federal Ministry of Education and Research of the Federal Republic of
Germany (BMBF grants FKZ-01IMC01 and FKZ-01IM10001), the Russian Founda-
tion for Basic Research (grant 16-01-00080) and the Ministry of Education and Science
of the Russian Federation (agreement 02.a03.21.0008).
A
Generalized α-Method
In this appendix, we brieﬂy explain the application of the generalized α-method
for the common case of a system described by the standard equation from struc-
tural mechanics,
M ¨x + D ˙x + Kx + Λ(t) = 0,
(13)
in which M, D, and K denote the mass, damping, and stiﬀness matrices. The
time-dependent displacement vector is given by x(t), and its ﬁrst- and second-
order temporal derivatives describe velocity and acceleration. The vector Λ(t)
describes external forces acting on the system at time t. We are searching for
functions x(t), υ(t) = ˙x(t), and a(t) = ¨x(t) satisfying (13) for all t with initial
conditions x(t0) = x0 and υ(t0) = υ0.
For the employment of the generalized α-method, we can write the integration
scheme with respect to (13) as follows:
Ma1−αm + Dx1−αf + Kx1−αf + Λ(t1−αf ) = 0,
with the substitution rule (·)1−α := (1−α)(·)i +α(·)i−1 and the approximations
xi = xi−1 + Δtvi−1 + Δt2
		1
2 −α

ai−1 + βai

,
υi = υi−1 + Δt ((1 −γ) ai−1 + γai) .
The parameters αm, αf, γ, and β are integration coeﬃcients.
References
1. Antman, S.S.: Nonlinear Problems of Elasticity. Applied Mathematical Sciences,
vol. 107. Springer, Heidelberg (1995). doi:10.1007/0-387-27649-1
2. Boyer, F., De Nayer, G., Leroyer, A., Visonneau, M.: Geometrically exact Kirchhoﬀ
beam theory: application to cable dynamics. J. Comput. Nonlinear Dyn. 6(4),
041004 (2011)
3. Cao, D.Q., Tucker, R.W.: Nonlinear dynamics of elastic rods using the Cosserat
theory: modelling and simulation. Int. J. Solids Struct. 45, 460–477 (2008)
4. Chung, J., Hulbert, G.M.: A time integration algorithm for structural dynamics
with improved numerical dissipation: the generalized-α method. J. Appl. Mech.
60(2), 371–375 (1993)
5. Cosserat, E., Cosserat, F.: Th´eorie des corps d´eformables. Hermann, Paris (1909)
6. Hilber, H.M., Hughes, T.J.R., Taylor, R.L.: Improved numerical dissipation for
time integration algorithms in structural dynamics. Earthq. Eng. Struct. Dyn. 5,
283–292 (1977)

312
D.A. Lyakhov et al.
7. Hilﬁnger, A.: Dynamics of cilia and ﬂagella. Ph.D. thesis, Technische Universit¨at
Dresden (2006)
8. Hochbruck, M., Ostermann, A.: Exponential integrators. Acta Numerica 19, 209–
286 (2010)
9. Ibragimov, N.H.: A Practical Course in Diﬀerential Equations and Mathematical
Modelling. Classical and New Methods. Nonlinear Mathematical Models. Symme-
try and Invariance Principles. Higher Education Press/World Scientiﬁc, Beijing
(2009)
10. Lang, H., Linn, J., Arnold, M.: Multibody Dynamics Simulation of Geometri-
cally Exact Cosserat Rods. Berichte des Fraunhofer ITWM, vol. 209. Fraunhofer,
Munich (2011)
11. Luo, A.C.J.: Nonlinear Deformable-Body Dynamics. Springer, Heidelberg (2010).
doi:10.1007/978-3-642-12136-4
12. Michels, D.L., Desbrun, M.: A semi-analytical approach to molecular dynamics. J.
Comput. Phys. 303, 336–354 (2015)
13. Michels, D.L., Lyakhov, D.A., Gerdt, V.P., Sobottka, G.A., Weber, A.G.: Lie
symmetry analysis for cosserat rods. In: Gerdt, V.P., Koepf, W., Seiler, W.M.,
Vorozhtsov, E.V. (eds.) CASC 2014. LNCS, vol. 8660, pp. 324–334. Springer, Cham
(2014). doi:10.1007/978-3-319-10515-4 23
14. Michels, D.L., Lyakhov, D.A., Gerdt, V.P., Hossain, Z., Riedel-Kruse, I.H., Weber,
A.G.: On the general analytical solution of the Kinematic Cosserat equations. In:
Gerdt, V.P., Koepf, W., Seiler, W.M., Vorozhtsov, E.V. (eds.) CASC 2016. LNCS,
vol. 9890, pp. 367–380. Springer, Cham (2016). doi:10.1007/978-3-319-45641-6 24
15. Michels, D.L., Mueller, J.P.T., Sobottka, G.: A physically based approach to the
accurate simulation of stiﬀﬁbers and stiﬀﬁber meshes. Comput. Graph. 53B,
136–146 (2015)
16. Michels, D.L., Sobottka, G.A., Weber, A.G.: Exponential integrators for stiﬀelas-
todynamic problems. ACM Trans. Graph. 33, 7:1–7:20 (2014)
17. Olver, P.J.: Applications of Lie Groups to Diﬀerential Equations. Graduate Texts
in Mathematics, vol. 107, 2nd edn. Springer, Heidelberg (1993). doi:10.1007/
978-1-4684-0274-2
18. Rubin, M.B.: Cosserat Theories: Shells, Rods and Points. Kluwer Academic Pub-
lishers, Dordrecht (2000)
19. Seiler, W.M.: Involution: The Formal Theory of Diﬀerential Equations and its
Applications in Computer Algebra. Algorithms and Computation in Mathematics,
vol. 24. Springer, Heidelberg (2010). doi:10.1007/978-3-642-01287-7
20. Sobottka, G.A., Lay, T., Weber, A.G.: Stable integration of the dynamic Cosserat
equations with application to hair modeling. J. WSCG 16, 73–80 (2008)
21. Wood, W.L., Bossak, M., Zienkiewicz, O.C.: An alpha modiﬁcation of Newmarks
method. Int. J. Numer. Methods Eng. 15, 1562–1566 (1981)

Algorithms for Zero-Dimensional Ideals Using
Linear Recurrent Sequences
Vincent Neiger1(B), Hamid Rahkooy2, and ´Eric Schost2
1 Department of Applied Mathematics and Computer Science,
Technical University of Denmark, Lyngby, Denmark
vinn@dtu.dk
2 Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada
Abstract. Inspired by Faug`ere and Mou’s sparse FGLM algorithm, we
show how using linear recurrent multi-dimensional sequences can allow
one to perform operations such as the primary decomposition of an ideal,
by computing of the annihilator of one or several such sequences.
1
Introduction
In what follows, K is a perfect ﬁeld. We consider the set S = KNn of n-
dimensional sequences u = (um)m∈Nn, and the polynomial ring K[X1, . . . , Xn],
and we are interested in the following question. Let I ⊂K[X1, . . . , Xn] be a zero-
dimensional ideal. Given a monomial basis of Q = K[X1, . . . , Xn]/I, together
with the corresponding multiplication matrices M1, . . . , Mn, we want to compute
the Gr¨obner bases, for a target order >, of pairwise coprime ideals J1, . . . , JK
such that I = ∩1≤k≤KJk.
Faug`ere et al.’s paper [11] shows how to solve this question with K = 1 (so
J1 is simply I) in time O(nD3), where D = deg(I); here, the degree deg(I) is the
K-vector space dimension of Q. More recently, algorithms have been given with
the cost bound O˜(nDω) [9,10,20], where the notation O˜ hides polylogarithmic
factors, still with K = 1. The algorithms in this paper allow splittings (so K > 1
in general) and assume that > is a lexicographic order.
To motivate our approach, assume that the algebraic set V (I) is in shape
position, that is, the coordinate Xn separates the points of V (I). Then, the Shape
Lemma [14] implies that the Gr¨obner basis of the radical
√
I for the lexicographic
order X1 > · · · > Xn has the form ⟨X1−G1(Xn), . . . , Xn−1−Gn−1(Xn), P(Xn)⟩,
for some squarefree polynomial P, and some G1, . . . , Gn−1 of degrees less than
deg(P). The polynomials P and G1, . . . , Gn−1 can be deduced from the values
(ℓ(Xi
n))0≤i≤2D and (ℓ(XjXi
n))1≤j<n,0≤i<D, for a randomly chosen linear form
ℓ: Q →K, in time O˜(D) [4]. The algorithms in the latter reference use baby
steps/giant steps techniques for the calculation of the values of ℓ.
Similar ideas were developed in [12]; the algorithms in this reference make no
assumption on I but may fail in some cases, then falling back on the FGLM algo-
rithm. For instance, if I itself (rather than
√
I) is known to have a lexicographic
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 313–328, 2017.
DOI: 10.1007/978-3-319-66320-3 23

314
V. Neiger et al.
Gr¨obner basis of the form ⟨X1 −H1(Xn), . . . , Xn−1 −Hn−1(Xn), Q(Xn)⟩, the
algorithms in [12] recover this basis, also by considering values of linear forms
ℓi : Q →K. A key remark made in that reference is that the values of the linear
forms ℓi that we need can be computed eﬃciently by exploiting the sparsity of
the multiplication matrices M1, . . . , Mn; this sparsity is then analyzed, assum-
ing the validity of a conjecture due to Moreno-Soc´ıas [18]. These techniques are
related as well to Rouillier’s Rational Univariate Representation algorithm [21],
which uses values of a speciﬁc linear form Q →K called the trace. However, com-
puting the trace (that is, its values on the monomial basis of Q) is non-trivial,
and using random choices instead makes it possible to avoid this issue.
In this paper, we work in the continuation of [4]. Assuming V (I) is in shape
position, the results in that reference allow us to compute the Gr¨obner basis of
√
I, and our goal here is to recover Gr¨obner bases corresponding to a decom-
position of I as stated above. Following [1,12], we discuss the relation of this
question to instances of the following problem: given sequences u1, . . . , us in S ,
ﬁnd the Gr¨obner basis of their annihilator ann(u1, . . . , us) ⊂K[X1, . . . , Xn], for
a target order >. The annihilator, discussed in the next section, is a polynomial
ideal corresponding to the linear relations which annihilate all sequences.
A direct approach to solve the FGLM problem using such techniques would
be to pick initial conditions at random; knowing multiplication matrices modulo
I allows us to compute the values of a sequence u, for which I is contained in
ann(u). If I = ann(u) holds, computing suﬃciently many values of u and feeding
them into an algorithm such as Sakata’s [22] would solve our problem. This is
often, but not always, possible: there exists a sequence u for which I = ann(u)
if and only if Q = K[X1, . . . , Xn]/I is a Gorenstein ring, a notion going back
to [15,16] (see e.g. [5, Proposition 5.3] for a proof of the above assertion). This
is for instance the case if I is a complete intersection, or if I is radical over a
perfect ﬁeld [8]; however, an ideal such as I = ⟨X2
1, X1X2, X2
2⟩⊂K[X1, X2] is
not Gorenstein.
To remedy this, we may have to use more than one sequence, so as to be able
to recover I as I = ann(u1, . . . , us). However, proceeding directly in this manner,
we do not expect the algorithm to be signiﬁcantly better than applying directly
the FGLM algorithm (the techniques we will use for computing annihilators
follow essentially the same lines as the FGLM algorithm itself). We will see that
starting from the Gr¨obner basis of
√
I, we will be able to decompose I into
e.g. primary components (assuming we allow the use of factorization algorithms
over K), and that our approach is expected to be competitive in those cases
where the multiple components of I have low degrees.
2
Generalities on Sequences and Their Annihilators
Deﬁne the shift operators s1, . . . , sn on S in the obvious manner, by setting
si(u) = (um+ei)m∈Nn, where e1, . . . , en are the unit vectors. This makes S a
K[X1, . . . , Xn]-module, by setting f · u = f(s1, . . . , sn)(u). For f = 
m fmXm,
the entries of f·u are thus (⟨u | Xmf⟩)m∈Nn, where we write Xm = Xm1
1
· · ·Xmn
n

Algorithms for Zero-Dimensional Ideals Using Linear Recurrent Sequences
315
and ⟨u|f⟩= 
m′ fm′um′. To a sequence u = (um)m∈Nn in S , we can then
associate its annihilator ann(u), deﬁned as the ideal of all polynomials f in
K[X1, . . . , Xn] such that f · u = 0. If we consider several sequences u1, . . . , us
in S , we then deﬁne ann(u1, . . . , us) = ann(u1) ∩· · · ∩ann(us).
We will also occasionally discuss kernels of sequences. For u ∈S , the kernel
ker(u) is the K-vector space formed by all polynomials f in K[X1, . . . , Xn] such
that ⟨u|f⟩= 0; this is not an ideal in general. If we consider several sequences
u1, . . . , us, we will write ker(u1, . . . , us) = ker(u1) ∩· · · ∩ker(us).
Let I be a zero-dimensional ideal in K[X1, . . . , Xn]. Deﬁne the residue class
ring Q = K[X1, . . . , Xn]/I and let D = deg(I) = dimK(Q). Consider also the
dual Q∗= homK(Q, K). To a linear form ℓin Q∗, we associate the sequence uℓ
deﬁned by uℓ= (ℓ(Xm mod I))m∈Nn.
For any linear form ℓon Q, and any g in Q, deﬁne the linear form g·ℓ∈Q∗by
(g·ℓ)(h) = ℓ(gh). This induces a Q-module structure on Q∗, and we remark that
we have the equality g·uℓ= u(g mod I)·ℓfor any g in K[X1, . . . , Xn]. Following [23]
(where it is described with n = 1), we call this operation transposed product.
For ℓin Q∗, we can then deﬁne annQ(ℓ) as the set of all g in Q such that
g ·ℓ= 0; this is an ideal of Q. The following lemma clariﬁes the relation between
ann(uℓ) ⊂K[X1, . . . , Xn] and annQ(ℓ) ⊂Q; it implies that ann(uℓ) is generated
by I and any element of annQ(ℓ) lifted to K[X1, . . . , Xn].
Lemma 1. With notation as above, for f in K[X1, . . . , Xn], f is in ann(uℓ) if
and only if f mod I is in annQ(ℓ).
Proof. Take f in K[X1, . . . , Xn]. Then f is in ann(uℓ) if and only if f · uℓ= 0,
that is, if and only if u(f mod I)·ℓ= 0, if and only if (f mod I) · ℓitself is zero. ⊓⊔
When Q∗is a free Q-module of rank one, we say that Q is a Gorenstein ring,
and that I is Gorenstein. In this case, there exists a linear form λ such that
Q∗= Q · λ; by the previous lemma, ann(uλ) = I. Conversely, if ann(uλ) = I,
annQ(λ) = {0}, so that Q∗= Q · λ (and Q∗is free of rank one). For instance, it
is known that if I is radical, or I a complete intersection, then I is Gorenstein.
On the other hand, if I = ⟨X2
1, X1X2, X2
2⟩, the inclusion I ⊂ann(uℓ) is strict
for any linear form ℓ. Using several sequences, we can however always recover I.
Lemma 2. Let ℓ1, . . . , ℓD be linearly independent in Q∗, and let u1, . . . , uD be
the corresponding sequences. Then ann(u1, . . . , uD) = ker(u1, . . . , uD) = I.
Proof. Note ﬁrst that the inclusion I ⊂ann(u1, . . . , uD) = ann(u1) ∩· · · ∩
ann(uD) is a direct consequence of Lemma 1, and that ann(u1, . . . , uD) is con-
tained in ker(u1, . . . , uD). For the converse, let ω1, . . . , ωD be the basis of Q dual
to ℓ1, . . . , ℓD. Suppose that f is in ker(u1, . . . , uD), and assume without loss of
generality that f has been reduced by I, so that f is a linear combination of the
form f1ω1 + · · · + fDωD. Fix i in 1, . . . , D and apply ℓi to f; we obtain fi. On
the other hand, because f is in ker(ui), ℓi(f) must vanish. So we are done.
⊓⊔
We may however need less than D linear forms, as explained in the following
discussion, which generalizes the comments we made in the Gorenstein case.

316
V. Neiger et al.
Let B = (b1, . . . , bD) be a monomial basis of Q. Given a linear form ℓin
Q∗, we deﬁne Kℓas the D × D matrix whose (i, j)th entry is ℓ(bibj); this is the
matrix of the mapping f ∈Q →f · ℓ∈Q∗, so that its nullspace is annQ(ℓ).
More generally, given a positive integer s and linear forms ℓ1, . . . , ℓs, we deﬁne
Kℓ1,...,ℓs as the D × sD matrix obtained as the concatenation of Kℓ1, . . . , Kℓs;
this is the matrix of the mapping (f1, . . . , fs) ∈Qs →f1 · ℓ1 + · · · + fs · ℓs ∈Q∗.
Lemma 3. For any linear forms (ℓ1, . . . , ℓs), with all ℓi in Q∗, ann(uℓ1, . . . , uℓs)
= I if and only if (ℓ1, . . . , ℓs) are Q-module generators of Q∗.
Proof. (ℓ1, . . . , ℓs) are Q-module generators of Q∗if and only if Kℓ1,...,ℓs has rank
D, if and only if K⊥
ℓ1,...,ℓs has a trivial nullspace. The nullspace of this matrix
is the intersection of those of the matrices K⊥
ℓ1, . . . , K⊥
ℓs. All these matrices are
symmetric, and we saw that for all i, the nullspace of K⊥
ℓi = Kℓi is annQ(ℓi); thus,
the condition above is equivalent to annQ(ℓ1) ∩· · · ∩annQ(ℓs) = {0}. Lemma 1
shows that this is the case if and only if ann(uℓ1) ∩· · · ∩ann(uℓs) = I.
⊓⊔
Proposition 1. There exists a unique integer τ ≤D such that for a generic
choice of linear forms (ℓ1, . . . , ℓτ), with all ℓi in Q∗, the sequence of ideals
(ann(uℓ1, . . . , uℓt))1≤t≤τ is strictly decreasing, with ann(uℓ1, . . . , uℓτ ) = I.
Proof. Remark ﬁrst that if τ exists with the properties above, it is necessar-
ily unique. Let (L1,1, . . . , L1,D), . . . , (LD,1, . . . , LD,D) be new indeterminates, let
L = K(L1,1, . . . , LD,D) and deﬁne the matrices KL1, . . . , KLD as follows. Let
QL = Q ⊗K L; this allows us to deﬁne the linear forms L1, . . . , LD in Q∗
L by
Lt(bj) = Lt,j, for 1 ≤t ≤D; then KLt is the matrix with entries Lt(bibj). The
entries of KLt are linear forms in Lt,1, . . . , Lt,D.
Deﬁne KL1,...,Lt as we did for Kℓ1,...,ℓt. Then, for any linear forms ℓ1, . . . , ℓt
in Q∗, the matrix Kℓ1,...,ℓt is obtained by evaluating KL1,...,Lt at Lt,j = ℓt(bj),
for all t, j. The rank of Kℓ1,...,ℓt (over K) is at most that of KL1,...,Lt (over L).
We can then let τ be the smallest integer such that the matrix KL1,...,Lτ has
full rank D. Such an index exists, and is at most D, since by Lemma 2 (and by
the remarks of the above paragraph) KL1,...,LD has rank D.
Let ℓ1, . . . , ℓτ be such that Kℓ1,...,ℓτ has rank D (this is our genericity con-
dition); in this case, by the previous lemma, ann(uℓ1, . . . , uℓτ ) = I. To con-
clude, it suﬃces to prove that the sequence of ideals (ann(uℓ1, · · · , uℓt))1≤t≤τ
is strictly decreasing. Suppose it is not the case, so that ann(uℓ1, . . . , uℓt) =
ann(uℓ1, . . . , uℓt+1) for some t < τ. Then, ann(uℓ1, . . . , uℓt, uℓt+2, . . . , uℓτ ) = I.
Let us deﬁne ℓ′
1 = ℓ1, . . . , ℓ′
t = ℓt, ℓ′
t+1 = ℓt+2, . . . , ℓ′
τ−1 = ℓτ. Then, we have
ann(uℓ′
1, . . . , uℓ′
τ−1) = I, so that Kℓ′
1,...,ℓ′
τ−1 has rank D. This in turn implies (by
the discussion above) that KL1,...,Lτ−1 has rank D, a contradiction.
⊓⊔
If Q is a local algebra with maximal ideal m, we can deﬁne the socle of Q as
the K-vector space of all elements f in Q such that mf = 0. For instance, if Q
is local, the integer τ in the previous lemma is the dimension of the socle of Q.
(we omit the proof, since we will not use this result in the rest of the paper).

Algorithms for Zero-Dimensional Ideals Using Linear Recurrent Sequences
317
3
Computing Annihilators of Sequences
Consider sequences (u1, . . . , ut) with ui ∈S for all i, let J be the annihilator
ann(u1, . . . , ut) ⊂K[X1, . . . , Xn], and suppose that it has dimension zero; our
goal is to compute a Gr¨obner basis of it. We ﬁrst review an algorithm due to
Marinari et al. [17], then introduce a modiﬁcation of it that relaxes some of
its assumptions. As a result, the algorithms in this section work under slightly
diﬀerent assumptions, and feature slightly diﬀerent runtimes.
An algorithm with cost (nt deg(J))O(1) would be highly desirable, but we are
not aware of any such result. Most approaches (ours as well) involve reading a
number of values of u1, . . . , ut and looking for dependencies between the columns
of what is often called a generalized Hankel matrix, built using these values; the
delicate question is how to control the size of the matrix.
Consider for instance the case t = 1, ⟨u1 | Xm1
1
· · · Xmn
n
⟩= 1 for m1 + · · · +
mn < δ and ⟨u1 | Xm1
1
· · · Xmn
n
⟩= 0 otherwise. The annihilator J = ann(u1)
admits the lexicographic Gr¨obner basis ⟨X1 −Xn, . . . , Xn−1 −Xn, Xδ
n⟩, so we
have deg(J) = δ; on the other hand, this sequence takes
deg(J)+n−1
n

non-zero
values, so taking them all into account leads us to an exponential time algorithm.
In the case t = 1, Mourrain in [19] associates a Hankel operator to a sequence
such that the kernel of the Hankel operator corresponds to the annihilator of the
sequence. Algorithm 2 in that paper computes a border basis for the kernel of
such a Hankel operator, taking as input its values over a ﬁnite set of monomials.
As in the FGLM algorithm, this algorithm looks for linear dependencies between
the monomials in the border of already computed linearly independent mono-
mials. However, for examples as in the previous paragraph, we are not aware of
how to avoid taking into account up to
deg(J)+n−1
n

values.
Several algorithms were also proposed in [1] for computing an annihilator
ann(u1), and partly extended to arbitrary t in [2]. A ﬁrst algorithm relies on
the Berlekamp-Massey Algorithm, by means of a change of coordinates, which
may require an exponential number of value of u1. The other algorithms extend
the idea of FGLM, considering maximal rank sub-matrices of a truncated multi-
Hankel matrix to compute a basis for the quotient algebra and a Gr¨obner basis.
An algorithm with certiﬁed outcome (Scalar-FGLM) is presented; it considers the
values of u1 at all monomials up to a given degree ≃deg(J), so the issue pointed
out above remains. An “adaptive” version uses fewer values of the sequence,
but may fail in some cases (the conditions that ensure success of this algorithm
seem to be close to the genericity assumptions we introduce in Subsect. 3.2).
A comparison of Scalar-FGLM and Sakata’s algorithm is presented in [3].
3.1
A First Algorithm
The ﬁrst solution we discuss requires a strong assumption (written H1 below):
for any i and for any monomial b in X1, . . . , Xn, b · ui is in the K-span of
(u1, . . . , ut); as a result, the annihilator J of (u1, . . . , ut) equals the nullspace
ker(u1, . . . , ut). For this situation, Marinari et al. gave in [17] an algorithm that
computes a Gr¨obner basis of J, for any order (for deﬁniteness, we refer here

318
V. Neiger et al.
to their second algorithm); it is an extension of both the Buchberger-M¨oller
interpolation algorithm and the FGLM change of order algorithm.
Assumption H1 above implies that deg(J) ≤t, and the runtime of the algo-
rithm, expressed in terms of n and t, is O(nt3) operations in K, together with
the computation of all values ⟨ui | b⟩, 1 ≤i ≤t, for O(nt) monomials b. These
evaluations are done in incremental order, in the sense that for any monomial b
for which we need all ⟨ui | b⟩, there exists j ∈{1, . . . , n} such that b = Xjb′ and
all ⟨ui | b′⟩are known.
We will need the following property of this algorithm. Suppose that
(u1, . . . , ut) is a subsequence of a larger family of sequences (u1, . . . , ut′) that
satisﬁes H1, but that (u1, . . . , ut) itself may or may not, and that (u1, . . . , ut)
and (u1, . . . , ut′) have diﬀerent K-spans. Then, on input (u1, . . . , ut), the algo-
rithm will still run its course, and at least one of the elements in the output will
be a polynomial g that does not belong to ann(u1, . . . , ut′).
3.2
An Algorithm Under Genericity Assumptions
We now give a second algorithm for computing J = ann(u1, . . . , ut), whose
runtime is polynomial in n, t, D = deg(J) and an integer B ≤deg(J) deﬁned
below. We do not assume that H1 holds, but we will require other assumptions;
if they hold, the output is the lexicographic Gr¨obner basis G of J for the order
X1 > · · · > Xn. Our ﬁrst assumption is:
H2. We are given an integer B such that the minimal polynomial of Xj in
K[X1, . . . , Xn]/J has degree at most B for all j.
For j in 1, . . . , n, we will denote by Jj the ideal ann(πj(u1), . . . , πj(ut)) ⊂
K[Xj, . . . , Xn], where for all i, πj(ui) is the sequence Nn−j+1 →K deﬁned
by ⟨πj(ui) | (mj, . . . , mn)⟩= ⟨ui | (0, . . . , 0, mj, . . . , mn)⟩for all (mj, . . . , mn)
in Nn−j+1; in particular, J1 = J. We write deg(Jj) = Dj ≤D, we let Gj be the
lexicographic Gr¨obner basis of Jj, and we let Bj be the corresponding monomial
basis of K[Xj, . . . , Xn]/Jj.
We can then introduce our genericity property; by contrast with H2, we will
not necessarily assume that it holds, and discuss the outcome of the algorithm
when it does not. We denote this property by H3(j), for j = 1, . . . , n −1.
H3(j). We have the equality Jj ∩K[Xj+1, . . . , Xn] = Jj+1.
Remark that the inclusion Jj ∩K[Xj+1, . . . , Xn] ⊂Jj+1 always holds.
Suppose that for some j in 1, . . . , n, we have computed a sequence of mono-
mials B′
j+1 in K[Xj+1, . . . , Xn] (if j = n, we let B′
j+1 = (1)). Since we will
use them repeatedly, we deﬁne properties P and P′ as follows, the latter being
stronger than the former.
P(j + 1). The cardinality D′
j+1 of B′
j+1 is at most Dj+1.
P′(j + 1). The equality B′
j+1 = Bj+1 holds.

Algorithms for Zero-Dimensional Ideals Using Linear Recurrent Sequences
319
We describe in the following paragraphs a procedure that computes a new family
of monomials B′
j, and we give conditions under which they satisfy P(j) and P′(j).
We call a family of monomials B in K[Xj, . . . , Xn] independent if their images
are K-linearly independent modulo Jj (we call it dependent otherwise). We
denote by MB the matrix with entries ⟨ui|bb′⟩, with rows indexed by i = 1, . . . , t
and b′ in Cj+1 = B′
j+1 ×(1, Xj, . . . , XB−1
j
), and columns indexed by b in B (for
any monomial b in K[Xj, . . . , Xn], Mb is the column vector deﬁned similarly).
Lemma 4. If B is dependent, the right nullspace of MB is non-trivial. If both
P′(j + 1) and H3(j) hold, the converse is true.
Proof. Any K-linear relation between the elements of B induces the same rela-
tion between the columns of MB, and the ﬁrst point follows.
By deﬁnition, a polynomial f in K[Xj, . . . , Xn] belongs to Jj if and only if
it annihilates πj(u1), . . . , πj(ut), that is, if ⟨πj(ui) | Xmj
j
. . . Xmn
n
f⟩= 0 for all
(mj, . . . , mn) in Nn−j+1 and all i = 1, . . . , t. Now, assumptions P′(j +1), H2 and
H3(j) imply that Cj+1 generates K[Xj, . . . , Xn]/Jj, so that f is in Jj if and only
if ⟨ui | bf⟩= 0, for all b in Cj+1 and all i = 1, . . . , t.
⊓⊔
The following lemma, that essentially follows the argument used in the proof
of the FGLM algorithm [11], will be useful to justify our algorithm as well.
Lemma 5. Suppose that b1 < · · · < bu < bu+1 are the ﬁrst u + 1 standard
monomials of K[Xj, . . . , Xn]/Jj, for the lexicographic order induced by Xj >
· · · > Xn, with b1 = 1. Then for any monomial b such that bu < b < bu+1,
{b1, . . . , bu, b} is a dependent family.
Proof. We prove the result by induction on u ≥0, the case u = 0 being vacuously
true. Assuming the claim is true for some index u ≥0, we prove it for u + 1.
We proceed by contradiction, and we let b be the smallest monomial such that
bu < b < bu+1 and {b1, . . . , bu, b} is an independent family (b exists by the
well-ordering property of monomial orders).
We will use the fact that any monomial c less than b can be rewritten as a
linear combination of b1, . . . , bi, with bi < c, for some i ≤u: if c < bu, this is by
the induction assumption; if c = bu, this is obvious; if bu < c < b, this is by the
deﬁnition of b.
Now, either b is the leading term of an element in the Gr¨obner basis of Jj, or
it must be of the form b = Xeb′, for some monomial b′ not in {b1, . . . , bu}. We
prove that in both cases, b can be rewritten as a linear combination of b1, . . . , bu,
which is a contradiction. In the ﬁrst case, b rewrites as a linear combination
of smaller monomials, say c1, . . . , cv, and by the previous remark, all of them
can be rewritten as linear combinations of b1, . . . , bu. Altogether, b itself can be
rewritten as a linear combination of b1, . . . , bu, a contradiction.
In the second case, b = Xeb′, for some monomial b′ not in {b1, . . . , bu}. As
above, b′ can be rewritten modulo Jj as a linear combination of monomials
b1, . . . , bi, for some i ≤u, with bi < b′. Then, b = Xeb′ is a linear combination
of Xeb1, . . . , Xebi. Since bi < b′, we get Xeb1 < · · · < Xebi < Xeb′ = b, so all of

320
V. Neiger et al.
Xeb1, . . . , Xebi can be rewritten as linear combinations of b1, . . . , bu. As a result,
this is also the case for b itself, so we get a contradiction again.
⊓⊔
Suppose that P(j+1) holds. Then, the algorithm at step j proceeds as follows.
We compute the reduced row echelon form of MCj+1. Using assumption P(j +1),
this matrix has at most tBDj+1 rows and at most BDj+1 columns, and it has
rank at most Dj (by the ﬁrst item of Lemma 4). This computation can be done
in time O(tB2D2
j+1Dj) ∈O(tB2D3). The column indices of the pivots allow us
to deﬁne the monomials B′
j = (b′
1 < · · · < b′
D′
j), for some D′
j ≤Dj.
Lemma 6. Property P(j) holds, and if P′(j + 1) and H3(j) hold, then P′(j)
holds.
Proof. The ﬁrst item is a restatement of the inequality D′
j ≤Dj. To prove the
second item, assuming that P′(j + 1) and H3(j) hold, we deduce from Lemma 4
that the columns indexed by the genuine Bj form a column basis of MCj+1, and
we claim that it is actually the lexicographically smallest column basis (this will
prove that Bj = B′
j). Indeed, write Bj = (b1, . . . , bDj), and let (f1, . . . , fDj)
be another subsequence of Cj+1 whose corresponding columns form a column
basis of MCj+1. Let m be the smallest index such that bm ̸= fm. Then, applying
Lemma 5 to (b1, . . . , bm−1) and fm, we deduce that bm < fm (otherwise, since
they are diﬀerent, we must have bm−1 < fm < bm, which implies that fm is a
linear combination of (b1, . . . , bm−1) = (f1, . . . , fm−1), a contradiction).
⊓⊔
Thus, running this procedure for j = n, . . . , 1, we maintain P(j); this implies
that the running time is O(ntB2D3), computing the values ⟨ui|b⟩, for 1 ≤i ≤t,
for O(nB2D2) monomials b (with the same monotonic property as in the previous
subsection). If H3(j) holds for all j, the second item in the last lemma proves
that B′
1 = B1, the monomial basis of K[X1, . . . , Xn]/J.
Once B′
1 is known, we compute and return a family of polynomials G′ deﬁned
as follows. We determine the sequence Δ of elements in X1B′
1∪· · ·∪XnB′
1−B′
1,
all of whose factors are in B′
1 (ﬁnding them does not require any operation
in K; this can be done by using e.g. a balanced binary search tree with the
elements of B′
1, using a number of comparisons that is quasi-linear time in nD).
Then, we rewrite each column Mb, for b in Δ, as a linear combination of the
form 
1≤i≤D′
1 ciMb′
i and we put b −
1≤i≤D′
1 cib′
i in G′. If the reduction is not
possible, the algorithm halts and returns fail. Using the reduced row echelon
form of MC2, each reduction takes time O(D2
1) ∈O(D2) operations in K, for a
total of O(nD3).
If H3(j) holds for all j, since B1 = B′
1, the fact that G′ = G follows from
Lemma 4. Assume now that G′ diﬀers from G; we prove that there exists an
element in G not in J (we will use this in our main algorithm to detect failure
cases). Indeed, in this case, B′
1 must be diﬀerent from B1, and since B′
1 has
cardinality at most equal to that of B1, there exists a monomial b in B1 not
in B′
1. This in turn implies that there exists an element g in G′ that divides b,
and thus with leading term in B1. Reducing g modulo G, we must then obtain
a non-zero remainder, so that g does not belong to J.

Algorithms for Zero-Dimensional Ideals Using Linear Recurrent Sequences
321
4
Main Algorithm
4.1
Representing Primary Zero-Dimensional Ideals
Let I be a zero-dimensional ideal in K[X1, . . . , Xn]; we assume that I is m-
primary, for some maximal ideal m, and we write D = deg(I). In this paragraph,
we brieﬂy mention some possible representations for I (our main algorithm will
compute either one of these representations).
The ﬁrst, and main, option we will consider is simply the Gr¨obner basis G
of I, for the lexicographic order induced by X1 > · · · > Xn. As an alterna-
tive, consider the following construction. Our assumption on I implies that the
minimal polynomial R of Xn in K[X1, . . . , Xn]/I takes the form R = P e, for
some irreducible polynomial P in K[Z], of degree say f (remark that R(Xn) is
also the last polynomial in G). Let L = K[Z]/⟨P⟩; this is a ﬁeld extension of
degree f of K, and the residue class ζ of Z in L is a root of P. We then let I′
be the ideal I + ⟨(Xn −ζ)e⟩in L[X1, . . . , Xn], and let D′ be its degree. Then,
a second option is to compute the lexicographic Gr¨obner basis G′ of I′, for the
order X1 > · · · > Xn. The following lemma relates D and D′.
Lemma 7. The ideal I′ has degree D′ = D/f.
Proof. Let M be the splitting ﬁeld of P and let ζ1, . . . , ζf be the roots of P in
M. The ideals Ji = I +⟨(Xn −ζi)e⟩⊂[X1, . . . , Xn] are such that deg(J1)+· · ·+
deg(Jf) = deg(I). On the other hand, there exist f embeddings σ1, . . . , σf of L
into M, with σi given by ζ →ζi; as a result, deg(I′) = deg(Ji) holds for all i,
and the claim follows.
⊓⊔
The point behind this construction is to lower the degree of the ideal we
consider, at the cost of working in a ﬁeld extension of K. This may be beneﬁcial,
as the cost of the main algorithm (which essentially relies on the one in the
previous section) will be a polynomial of rather large degree with respect to the
degree of the ideal, whereas computation in a ﬁeld extension such as K →L is
a well-understood task of cost ranging from quasi-linear to quadratic.
Our last option aims at producing a “simpler” Gr¨obner basis, by means of a
change of coordinates. For this, we will assume that Xn separates the points of
V (m) (over an algebraic closure of K). As a result, the ideal m being maximal,
it admits a lexicographic Gr¨obner basis of the form ⟨X1 −G1(Xn), . . . , Xn−1 −
Gn−1(Xn), P(Xn)⟩. Deﬁne ξ1 = G1(ζ), . . . , ξn−1 = Gn−1(ζ), ξn = ζ, for ζ ∈L as
above; then, (ξ1, . . . , ξn) is the unique zero of I′ (in fact, I′ is m′-primary, with
m′ = ⟨X1 −ξ1, . . . , Xn −ξn⟩). We can then apply the change of coordinates that
replaces Xi by Xi +ξi in I′, for all i, and call I′′ the ideal thus obtained (so that
I′′ is generated by the polynomials f(X1 +ξ1, . . . , Xn +ξn), for f in I, and Xe
n).
Now, I′′ is m′′-primary, with m′′ = ⟨X1, . . . , Xn⟩; one of our options will be to
compute the Gr¨obner basis G′′ of I′′.

322
V. Neiger et al.
Example 1. Consider the polynomials in Q[X1, X2]
X2
1 −2X1X2 −2X1 + X2
2 + 2X2 + 1,
X1X2
2 + X1X2 + 2X1 −X3
2 −2X2
2 −3X2 −2,
X4
2 + 2X3
2 + 5X2
2 + 4X2 + 4,
the last of them being P(X2)2 = (X2
2 + X2 + 2)2, and let I be the ideal they
deﬁne. The polynomials above are the lexicographic Gr¨obner basis G of I for the
order X1 > X2. Let L = Q[Z]/⟨Z2 + Z + 2⟩, and let ζ be the image of Z in L;
then, the ideal I′ = I + ⟨(X2 −ζ)2⟩in L[X1, X2] admits the Gr¨obner basis G′
X2
1 −2X1ζ −2X1 + ζ −1,
X1X2 −X1ζ −X2ζ −X2 −2,
X2
2 −2X2ζ −ζ −2.
Here, we have e = 2, f = 2, D = 6 and D′ = 3. The ideal I is m-primary,
where m admits the Gr¨obner basis ⟨X1 −X2 −1, X2
2 + X2 + 2⟩, so that we have
(ξ1, ξ2) = (ζ +1, ζ), and I′ is m′-primary, with m′ = ⟨X1 −ξ1, X2 −ξ2⟩. Applying
the change of coordinates (X1, X2) ←(X1 + ξ1, X2 + ξ2), the resulting ideal
I′′ admits the Gr¨obner basis G′′ = ⟨X2
1, X1X2, X2
2⟩, from which we can readily
conﬁrm that it is ⟨X1, X2⟩-primary.
4.2
The Algorithm
We consider a zero-dimensional ideal I in K[X1, . . . , Xn]. We assume that we
know a monomial basis B = (b1, . . . , bD) of Q = K[X1, . . . , Xn]/I, so that
we let D = dimK(Q), together with the corresponding multiplication matri-
ces M1, . . . , Mn of respectively X1, . . . , Xn. We assume that the last variable Xn
has been chosen generically; in particular, Xn separates the points of V = V (I).
The algorithm in this section computes a decomposition of I into primary
components J1, . . . , JK. Each such component Jk will be given by means of one
of the representations described in the previous subsection; we will emphasize the
ﬁrst of them, the lexicographic Gr¨obner basis of Jk, and mention how to modify
the algorithm in order to obtain the other representations. In order to ﬁnd the
primary components of I, we cannot avoid the use of factorization algorithms
over K; if desired, one may avoid this by relying on dynamic evaluation tech-
niques [7], replacing for instance the factorization into irreducibles used below by
a squarefree factorization (thus producing a decomposition of I into ideals that
are not necessarily primary). In that case, if one wishes to compute descriptions
such as the second or third ones introduced above, involving algebraic numbers
as coeﬃcients, one should take into account the possibility of splittings of the
deﬁning polynomials, as is usual with this kind of approach (a complete descrip-
tion of the resulting algorithm, along the lines of [6], is beyond the scope of this
paper).
The Ideal I and Its Primary Decomposition. Let Pmin ∈K[Xn] be the
minimal polynomial of Xn in Q, let P be its squarefree part, and let polynomials

Algorithms for Zero-Dimensional Ideals Using Linear Recurrent Sequences
323
G1, . . . , Gn−1 in K[Xn], with deg(Gi) < deg(P) for all i, be such that
√
I admits
the lexicographic Gr¨obner basis ⟨X1 −G1(Xn), . . . , Xn−1 −Gn−1(Xn), P(Xn)⟩.
We write Pmin = P e1
1 · · · P eK
K , with the Pk’s pairwise distinct irreducible poly-
nomials in K[Xn] and ek ≥1 for all k. In particular, the factorization of P is
P1 · · · PK; we write fk = deg(Pk) for all k.
Correspondingly, let V1, . . . , VK be the K-irreducible components of V and
for k = 1, . . . , K, let mk be the maximal ideal deﬁning Vk; hence, the reduced
lexicographic Gr¨obner basis of mk is ⟨X1 −(G1 mod Pk), . . . , Xn−1 −(Gn−1 mod
Pk), Pk⟩. We can then write I = J1 ∩· · ·∩JK, with Jk mk-primary for all k; note
that the ideal Jk is deﬁned by Jk = I + ⟨P ek
k ⟩. In what follows, we explain how
to compute a Gr¨obner basis of this ideal by means of the results of the previous
section. Without loss of generality, assume that L is such that ek = 1 for k > L
and ek ≥2 for k = 1, . . . , L. The fact that Xn is a generic coordinate implies
that for k > L, Jk = mk, so there is nothing left to do for such indices; hence,
we are left with showing how to use the algorithms of the previous section to
compute Gr¨obner bases of J1, . . . , JL.
Data Representation. An element f of Q is represented by the column vector
vf of its coordinates on the basis B, whereas a linear form ℓ: Q →K is repre-
sented by the row vector wℓ= [ℓ(b1), . . . , ℓ(bD)]. Computing ℓ(f) is then done
by means of the dot product wℓ·vf. Multiplying f by Xi amounts to computing
Mivf, and the linear form Xi · ℓ: g →ℓ(Xig) is obtained by computing the
vector wXi·ℓ= wℓMi.
In terms of complexity, we assume that multiplying any matrix Mi by a vector
(either on the left or on the right) can be done in m operations in K. The naive
bound on m is O(D2), but the sparsity properties of these matrices often result
in much better estimates; see [12] for an in-depth discussion of this question. On
the other hand, we assume D ≤m.
Computing. Pmin and G1, . . . , Gn−1. First, we compute generators of
√
I.
We choose a random linear form ℓ1 : Q →K, and we compute the values
(ℓ1(Xi
n))0≤i<2D and ℓ1(X1Xi
n), . . . , ℓ1(Xn−1Xi
n), for 0 ≤i < D. This is done by
computing 1, Xn, . . . , X2D−1
n
by repeated applications of Mn, which amounts
to O(Dm) operations, and doing the corresponding dot products with ℓ, X1 ·
ℓ, . . . , Xn−1 · ℓ. For the latter, we have to compute the linear forms Xi · ℓin
O(nm) operations, then do a D × D by D × (n + 1) matrix product, which costs
O(nD2) operations (without using fast linear algebra).
Using the algorithm given in [4], given these values, we can compute the
minimal polynomial Pmin, as well as the polynomials G1, . . . , Gn−1 describing
V (I) in O˜(D) operations in K. Then, as per the discussion in the preamble,
we assume that we have an algorithm for factoring polynomials over K, so that
(P1, e1), . . . , (PK, eK) and P can be deduced from Pmin.
Constructing the Orthogonal of Jk. For k = 1, . . . , K, we will write Qk =
K[X1, . . . , Xn]/Jk. Any linear form ℓ: Q →K induces a linear form ϕk(ℓ) :
Qk →K, deﬁned as follows.

324
V. Neiger et al.
Let Tk be the polynomial Pmin /P ek
k . For f in Qk, let ˆf be any lift of f to
K[X1, . . . , Xn], and deﬁne ϕk(ℓ)(f) = ℓ(Tk ˆf mod I). Notice that this expression
is well-deﬁned: indeed, any two lifts of f diﬀer by an element δ of Jk = I +⟨P ek
k ⟩,
so that Tkδ is in I, since TkP ek
k
= Pmin is.
Lemma 1. The mapping ϕk : Q∗→Q∗
k is K-linear and onto.
Proof. Linearity is clear by construction; we now prove that ϕk is onto. Let
indeed Ak, Bk in K[Xn] be such that AkTk +BkP ek
k
= 1 (they exist by deﬁnition
of Tk). Consider λ in Q∗
k, and deﬁne ℓin Q∗by ℓ(f) = λ(Akf mod Jk). Since
P ek
k
vanishes modulo Jk, we have AkTk = 1 mod Jk, so ℓ(f) = λ(f mod Jk)
holds for all f in Q; this in turn readily implies that ϕk(ℓ) = λ.
⊓⊔
We saw in Subsect. 2 how to associate to an element ℓ∈Q∗a sequence uℓ∈
S , by letting ⟨uℓ| m⟩= ℓ(m mod I). The following tautological observation will
then be useful below: for ℓin Q∗, the sequences uTk·ℓand uϕk(ℓ) coincide, where
uϕk(ℓ) is deﬁned starting from the linear form ϕk(ℓ) ∈Q∗
k. Indeed, take any
monomial m in X1, . . . , Xn; then, ϕk(ℓ)(m mod Jk) is deﬁned as ℓ(Tkm mod I),
which is equal to (Tk · ℓ)(m mod I). We will use this remark to compute values
of ϕk(ℓ), through the computation of values of Tk · ℓinstead.
In algorithmic terms, computing a single transposed product by a polynomial
T(Xn), that is, T ·ℓ, can be done using Horner’s rule, using d right-multiplications
by Mn, with d = deg(T); this takes O(dm) operations in K. If several transposed
products are needed, such as for instance computing T1 · ℓ, . . . , TL · ℓas below,
the cost becomes O(LDm), using D as an upper bound on deg(T1), . . . , deg(TL).
One can actually do better, by computing inductively and storing the products
Xi
n · ℓ, for i = 0, . . . , D −1. Then, the coeﬃcients of T1 · ℓ, . . . , TL · ℓcan be
computed as the product of the D × d′ matrix of coeﬃcients of (Xi
n · ℓ)0≤i<D by
the matrix of coeﬃcients of T1, . . . , TL; the cost is O(Dm + LD2).
One can improve this idea further using subproduct tree techniques, since
the polynomials T1, . . . , TL have a very speciﬁc structure. Recall that we
deﬁned Tk
= Pmin /P ek
k . Hence, all of T1, . . . , TL share a common factor
R = P eL+1
L+1 · · · P eK
K . We can then treat the common factor R separately, by writ-
ing Tk = RUk for all these indices k, and computing U1 · ℓ′, . . . , UL · ℓ′ instead,
with ℓ′ = R · ℓ. The cost to compute ℓ′ is O(Dm).
The polynomials U1, . . . , UL have no common factor anymore, but they are
all of the form P e1
1 · · · P ek−1
k−1 P ek+1
k+1 P eL
L . We can then deﬁne a subproduct tree as
in [13, Chap. 10], that is, a binary tree T having the polynomials (P ek
k )1≤k≤L
at its leaves, and where each node is labeled by the product of the polynomials
at its two children. We proceed in a top-down manner: we associate ℓ′ to the
root of the tree, and recursively, if a linear form λ has been assigned to an inner
node of T , we associate to each of its children the transposed product of λ by the
polynomial labelling the other child. At the leaves, this gives us UL·ℓ′, . . . , UK ·ℓ′,
as claimed. The total cost at each level is O(Dm), for a total of O(D log(L)m).
The Main Procedure, Using the Algorithm of Subsect. 3.1. The ﬁrst
version of the main procedure determines the Gr¨obner bases of JL, . . . , JK by
applying the algorithm of Subsect. 3.1 to successive families of linear forms.

Algorithms for Zero-Dimensional Ideals Using Linear Recurrent Sequences
325
We maintain a list of “active” indices S, initially set to S = (1, . . . , L);
these are the indices for which we are not done yet. The algorithm proceeds
iteratively; at step i ≥1, we pick a random linear form ℓi ∈Q∗, and compute
all ℓk,i = Tk · ℓi, for k in S. We then apply the algorithm of Subsect. 3.1 to
(uℓk,1, . . . , uℓk,i), for all k independently, and obtain families of polynomials
Gk,i as output. For veriﬁcation purposes, we also choose a random ℓ0 ∈Q∗, and
compute the corresponding ℓk,0.
Write Dk = deg(Jk), for k ≤K. Combining Lemma 2 and the equality
uℓk,i = u(ϕk(ℓi)) seen above, we deduce that for a generic choice of ℓ1, . . . , ℓDk,
(ℓk,1, . . . , ℓk,Dk) satisﬁes assumption H1 needed for our algorithm, and that Gk,Dk
is a Gr¨obner basis of Jk. In view of the discussion in Subsect. 3.1, for any i < Dk,
Gk,i contains a polynomial g not in Jk. Since ℓ0 was chosen at random, ℓk,0 will
in general not vanish at g; hence, at every step i, we evaluate ℓk,0 at all elements
of Gk,i, and continue the algorithm for this index k if we obtain a non-zero value;
else, we remove k from our list S, and append Gk,i to the output.
In terms of complexity, we will have to apply the process in the previ-
ous paragraph to μ linear forms ℓD1, . . . , ℓμ, with μ = maxk≤L(Dk), for a
cost O(μDm log(L)). Then, we will exploit a feature of Marinari-M¨oller-Mora’s
second algorithm: it is incremental in the number of linear forms given as
input, so that the overall runtime of our Dk successive invocations is the
same as if we called it once with ℓ1, . . . , ℓDk. For a given k, it adds up to
O(nD2
km + nD3
k) = O(nD2
km), where the ﬁrst term describes the cost of the
evaluations of the linear forms we need (since each new value requires the prod-
uct by one of the Mi). Overall, the runtime is O(μD log(L)m + n 
k≤L D2
km).
This supports the comment made in the introduction: if the degrees of the mul-
tiple components are small, say Dk = O(1) for all k, this is O(nD log(D)m).
Using the Algorithm of Subsect. 3.2. We can adapt our main procedure in
order to use the algorithm of Subsect. 3.2 instead; the main diﬀerence is that we
expect to use fewer linear forms.
For k ≤K, let indeed tk ≤Dk be the maximum of τ(Qk,≥1), . . . , τ(Qk,≥n),
with Qk,≥j = K[Xj, . . . , Xn]/Jk∩K[Xj, . . . , Xn], and with τ deﬁned as in Propo-
sition 1 (for instance, if I is a complete intersection ideal, tk = 1 for all k). The
main algorithm proceeds as in the previous variant: we choose random linear
forms ℓ1, . . . and deduce ℓk,i = Tk · ℓi; we will compute the Gr¨obner basis Gk of
Jk as ann(uℓk,1, uℓk,2, . . . ). We claim that we only need tk linear forms ℓ1, . . . , ℓtk
in order to recover Gk.
To conﬁrm this, we consider again assumptions H2 and H3 made in Sub-
sect. 3.2. The appendix of [4] implies that the minimal polynomial of any variable
Xi in Qk has degree at most ek, except for Xn. We already know the minimal
polynomial P ek
k
of Xn in Qk, so we skip the ﬁrst pass in the loop of the algorithm
of Subsect. 3.2, and use the value B = ek.
Regarding H3, we prove that if ℓ1, . . . , ℓtk are chosen generically, assumption
H3(j) holds for j = 1, . . . , n. For i ≥1 and j = 1, . . . , n, deﬁne ℓk,i,j as the linear
form in Q∗
k,≥j induced by restriction of ϕk(ℓi) ∈Q∗
k. Applying Proposition 1
to Qk,≥j shows that there exists a Zariski open Ωk,j ⊂Q∗
k,≥j
tk such that if

326
V. Neiger et al.
ℓk,1,j, . . . , ℓk,tk,j are in Ωk,j, they generate Q∗
k,≥j as a Qk,≥j-module, and thus
(Lemma 3) Jk ∩K[Xj, . . . , Xn] = ann(uℓk,1,j, . . . , uℓk,tk,j). If this is true for some
index k and all j, H3(j) follows as well for these indices. Now, the mapping
Δk,j : (ℓ1, . . . , ℓtk) →(ℓk,1,j, . . . , ℓk,tk,j) is K-linear and onto (we proved above
that (ℓ1, . . . , ℓtk) →(ϕk(ℓ1), . . . , ϕk(ℓtk)) is onto, and the surjectivity of the
projection is straightforward), so that the preimage Δ−1
k,j(Ωk,j) is Zariski open
in Q∗tk for all k, j. In other words, for generic ℓ1, . . . , ℓtk, H3(j) holds for all j
and all k, so the algorithm of Subsect. 3.2 computes Gk for all k.
We still need to discuss what happens when applying this algorithm to
ℓk,1, . . . , ℓk,i for some i < tk. In this case, as per the discussion in Sub-
sect. 3.2, either we get generators of ann(uℓk,1, . . . , uℓk,i), which is a strict super-
set of Jk, or at least one of the polynomials in the output does not belong to
ann(uℓk,1, . . . , uℓk,i). In any case, the output contains at least one polynomial g
not in Jk, so we can use the same stopping criterion as in the previous paragraph,
using a linear form ℓ0 to test termination.
To control the complexity, at the ith step, we now use linear forms ℓ1, . . . , ℓ2i;
as a result, we need to go up to i = t, with t = maxk(tk), and the overall runtime
is proportional to that at i = t. The cost of preparing the linear forms ℓk,i is
O(tDm log(L)), and the cost of computing annihilators is O(nt 
k≤L e2
kD2
km).
The ﬁrst term is better than the equivalent term for our ﬁrst algorithm, but the
second one is obviously worse. On the other hand, the analysis in Subsect. 3.2
can be reﬁned signiﬁcantly, and possibly lead to improved estimates.
Using a Scalar Extension. To conclude, we discuss (without giving proofs)
how to put to practice the idea introduced in Subsect. 4.1 of computing Gr¨obner
bases of ideals of smaller degree over larger base ﬁelds, in the context (for deﬁ-
niteness) of the algorithm of the previous paragraph.
Let ℓk,1, . . . , ℓk,tk be deﬁned as before, let uℓk,1, . . . , uℓk,tk be the corre-
sponding sequences, and assume that these linear forms are such that the
annihilator of uℓk,1, . . . , uℓk,tk is Jk. Let further Lk be the ﬁeld extension
K[Z]/Pn(Z), and let ζk be the residue class of Z in Lk Then, the annihila-
tor of J′
k = Jk + ⟨Xn −ζk⟩ek in L[X1, . . . , Xn] has degree Dk/fk by Lemma 7,
so we might want to compute it instead of Jk. To accomplish this, we need
sequences whose annihilator would be J′
k; we do this following the same strat-
egy as above. Deﬁne Sk = Pk/(Xn −ζk) ∈Lk[Xn], as well as the linear form
ℓ′
k,i = Sek
k
· ℓk,i : L[X1, . . . , Xn]/I →L, for i ≥1. Then, one veriﬁes that
ann(uℓ′
k,1, . . . , uℓ′
k,tk ) is indeed J′
k.
Our last comment discusses the translation mentioned in Subsect. 4.1. The
ideal J′
k is m′-primary, with m′ = ⟨X1 −ξ1, . . . , Xn −ξn⟩, as in Subsect. 4.1. To
replace J′
k by a ⟨X1, . . . , Xn⟩-primary ideal, we need to modify the sequences
uℓ′
k,1, . . . , uℓ′
k,tk . For i ≥1, let Uk,i ∈L[[X1, . . . , Xn]] be the generating series
of uℓ′
k,i, and let ˜Uk,i =
1
(1+ξ1X1)···(1+ξnXn)Uk,i(
X1
1+ξ1X1 , . . . ,
Xn
1+ξnXn ). Letting ˜uk,i
be the sequence whose generating series is ˜Uk,i, ann(˜uk,1, . . . , ˜uk,tk) is indeed
the ⟨X1, . . . , Xn⟩-primary ideal J′′
k obtained by translation by (ξ1, . . . , ξn) in J′
k.

Algorithms for Zero-Dimensional Ideals Using Linear Recurrent Sequences
327
Acknowledgements. We thank the reviewers for their remarks and suggestions. The
third author is supported by an NSERC Discovery Grant.
References
1. Berthomieu, J., Boyer, B., Faug`ere, J.-C.: Linear algebra for computing Gr¨obner
bases of linear recursive multidimensional sequences. J. Symb. Comput. 83, 36–67
(2016)
2. Berthomieu, J., Faug`ere, J.-C.: Guessing linear recurrence relations of sequence
tuples and P-recursive sequences with linear algebra. In: ISSAC 2016, pp. 95–102.
ACM (2016)
3. Berthomieu, J., Faug`ere, J.-C.: In-depth comparison of the Berlekamp-Massey-
Sakata and the Scalar-FGLM algorithms: the non adaptive variants. hal-01516708,
May 2017
4. Bostan, A., Salvy, B., Schost, ´E.: Fast algorithms for zero-dimensional polynomial
systems using duality. AAECC 14, 239–272 (2003)
5. Brachat, J., Comon, P., Mourrain, B., Tsigaridas, E.: Symmetric tensor decompo-
sition. Linear Algebra Appl. 433(11), 1851–1872 (2010)
6. Dahan, X., Moreno Maza, M., Schost, ´E., Xie, Y.: On the complexity of the D5
principle. In: Transgressive Computing, pp. 149-168 (2006)
7. Della Dora, J., Dicrescenzo, C., Duval, D.: About a new method for computing in
algebraic number ﬁelds. In: Caviness, B.F. (ed.) EUROCAL 1985. LNCS, vol. 204,
pp. 289–290. Springer, Heidelberg (1985). doi:10.1007/3-540-15984-3 279
8. Eisenbud, D.: Commutative Algebra: With a View Toward Algebraic Geome-
try, vol. 150. Springer Science & Business Media, New York (2013). doi:10.1007/
978-1-4612-5350-1
9. Faug`ere, J.-C., Gaudry, P., Huot, L., Renault, G.: Polynomial Systems Solving by
Fast Linear Algebra (2013). https://hal.archives-ouvertes.fr/hal-00816724
10. Faug`ere, J.-C., Gaudry, P., Huot, L., Renault, G.: Sub-cubic change of ordering for
Gr¨obner basis: a probabilistic approach. In: ISSAC 2014, pp. 170-177. ACM (2014)
11. Faug`ere, J.-C., Gianni, P., Lazard, D., Mora, T.: Eﬃcient computation of zero-
dimensional Gr¨obner bases by change of ordering. J. Symb. Comput. 16(4), 329–
344 (1993)
12. Faug`ere, J.-C., Mou, C.: Sparse FGLM algorithms. J. Symb. Comput. 80(3), 538–
569 (2017)
13. von zur Gathen, J., Gerhard, J.: Modern Computer Algebra, 3rd edn. Cambridge
University Press, Cambridge (2013)
14. Gianni, P., Mora, T.: Algebrric solution of systems of polynomirl equations using
Groebher bases. In: Huguet, L., Poli, A. (eds.) AAECC 1987. LNCS, vol. 356, pp.
247–257. Springer, Heidelberg (1989). doi:10.1007/3-540-51082-6 83
15. Gr¨obner, W.: ¨Uber irreduzible Ideale in kommutativen Ringen. Math. Ann. 110(1),
197–222 (1935)
16. Macaulay, F.S.: Modern algebra and polynomial ideals. Math. Proc. Camb. Philos.
Soc. 30(1), 27–46 (1934)
17. Marinari, M.G., Mora, T., M¨oller, H.M.: Gr¨obner bases of ideals deﬁned by func-
tionals with an application to ideals of projective points. AAECC 4, 103–145 (1993)
18. Moreno-Soc´ıas, G.: Autour de la fonction de Hilbert-Samuel (escaliers d’ideaux
polynomiaux). Ph.D. thesis, ´Ecole polytechnique (1991)
19. Mourrain, B.: Fast algorithm for border bases of Artinian Gorenstein algebras.
ArXiv e-prints, May 2017

328
V. Neiger et al.
20. Neiger, V.: Bases of relations in one or several variables: fast algorithms and appli-
cations. Ph.D. thesis, ´Ecole Normale Sup´erieure de Lyon, November 2016
21. Rouillier, F.: Solving zero-dimensional systems through the rational univariate rep-
resentation. AAECC 9(5), 433–461 (1999)
22. Sakata, S.: Extension of the Berlekamp-Massey algorithm to N dimensions. Inform.
Comput. 84(2), 207–239 (1990)
23. Shoup, V.: A new polynomial factorization algorithm and its implementation. J.
Symb. Comput. 20(4), 363–397 (1995)

Symbolic-Numerical Analysis of the Relative
Equilibria Stability in the Planar Circular
Restricted Four-Body Problem
Alexander N. Prokopenya1,2(B)
1 Department of Applied Informatics, Warsaw University of Life Sciences – SGGW,
Nowoursynowska Str. 159, 02-776 Warsaw, Poland
alexander prokopenya@sggw.pl
2 Collegium Mazovia Innovative Higher School,
Sokolowska Str. 161, 08-110 Siedlce, Poland
Abstract. We study the stability of relative equilibrium positions in
the planar circular restricted four-body problem formulated on the basis
of the Euler collinear solution of the three-body problem. The stabil-
ity problem is solved in a strict nonlinear formulation in the framework
of the KAM theory. We obtained algebraic equations determining the
equilibrium positions and showed that there are 18 diﬀerent equilibrium
conﬁgurations of the system for any values of the two system parame-
ters µ1, µ2. Canonical transformation of Birkhoﬀ’s type reducing the
Hamiltonian of the system to the normal form is constructed in a gen-
eral symbolic form. Combining symbolic and numerical calculations, we
showed that only 6 equilibrium positions are stable in Lyapunov’s sense if
parameters µ1 and µ2 are suﬃciently small, and the corresponding points
in the plane Oµ1µ2 belong to the domain bounded by the second order
resonant curve. It was shown also that the third order resonance results
in instability of the equilibrium positions while in case of the fourth order
resonance, either stability or instability can take place depending on the
values of parameters µ1 and µ2. All relevant symbolic and numerical cal-
culations are done with the aid of the computer algebra system Wolfram
Mathematica.
1
Introduction
The circular restricted three-body problem is a well-known model of celestial
mechanics (see, for example, [21]). Recall that we are interested in motion of the
particle P3 of negligible mass in the gravitational ﬁeld of two massive particles
P0 and P1 having masses m0, m1, respectively, and moving uniformly on cir-
cular Keplerian orbits around their common center of mass. A general solution
of the problem cannot be written in symbolic form but there exist ﬁve exact
particular solutions known as the homographic ones [22]. In the rotating frame
of reference, where the particles P0 and P1 have rest, these solutions determine
the equilibrium positions of the particle P3 (or relative equilibrium positions)
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 329–345, 2017.
DOI: 10.1007/978-3-319-66320-3 24

330
A.N. Prokopenya
which are called the points of libration Lj (j = 1, 2, . . . , 5). The libration points
are of great interest for applications and so their stability was a subject of many
papers during the past two hundred years. As a result, it was proven that three
points L1, L2, L3 situated at the line P0P1 (collinear equilibrium positions) are
unstable while the libration points L4, L5 (triangular equilibrium positions) may
be stable if the mass ratio μ1 = m1/m0 is suﬃciently small (see [14,21]).
The systems of particular interest in Celestial Mechanics and Cosmic Dynam-
ics usually contain more than three bodies. So it makes sense to add the third par-
ticle P2 of mass m2 to the system P0P1P3 and to analyze its inﬂuence on stability
of equilibrium positions of the particle P3. Note that the collinear and triangular
equilibrium conﬁgurations exist also in a general case of the three-body problem.
Moreover, both of them are realized in the Solar System (see [17]). So the particle
P2 may be situated in any of the ﬁve equilibrium positions of the corresponding
three-body problem which coincide with the libration points L1, . . . , L5 in case
of m2 = 0. In this way, we obtain the restricted four-body problem that has
been a subject of many papers (see, for example, [1,5,10,18,19]). It should be
emphasized that in the framework of this model, motion of the massive parti-
cles P0, P1, P2 is given and is determined by the corresponding solutions of the
three-body problem.
The case when the particle P2 is situated in the vertex L4 of the equilateral
triangle P0P1L4 was studied in detail in [2,4,7,18]. It was shown that four new
equilibrium positions of particle P3 arise from the point of libration L4 if the
second mass parameter μ2 = m2/m0 becomes greater than zero. The rest four
libration points L1, L2, L3, and L5 only change their positions depending on the
values of parameters μ1 and μ2. Besides, one or two new equilibrium positions
may arise inside the triangle P0P1P2. Stability of all these equilibrium positions
was completely investigated in [5,6] on the basis of the KAM theory [3,15] that
is widely used for solving similar problems, starting from the famous works [8,
13]. In particular, it was shown that all equilibrium positions situated near the
collinear libration points L1, L2, L3 remain unstable for any values of parameters
μ1, μ2. A special case of the stability problem when μ1 = μ2 was investigated
in [1,19]. Note that only two points (μ1, μ2) in the plane Oμ1μ2 remain for which
theorems of Arnold [3] and Markeev [14] cannot be applied, and the stability
problem for the corresponding equilibrium positions has not been solved yet
(see [5]).
To complete investigation of the inﬂuence of the particle P2 on the motion
of particle P3 one needs to consider the case when particle P2 is situated in one
of the collinear libration points L1, L2, L3, and this is the main research task of
the present paper. Although such positions of particle P2 are unstable, its quasi-
periodic orbits near the collinear libration points may exist. So it is a matter of
interest to investigate an inﬂuence of the particle P2 mass on the equilibrium
positions of particle P3 and their stability. In addition, the problem is interesting
from the theoretical point of view because it diﬀers essentially from the case of
triangular conﬁguration of the particles P0, P1, P2. Actually, in the latter case,
the particles P0, P1, P2 are ﬁxed in the vertices of the equilateral triangle for

Symbolic-Numerical Analysis of Stability in a Planar Four-Body Problem
331
any values of parameters μ1, μ2, while in the collinear case, mutual distances
between particles depend on these parameters. Therefore, given the values of
μ1, μ2 we have to look for both equilibrium conﬁguration of the massive particles
P0, P1, P2 and equilibrium positions of the particle P3 as solutions of the corre-
sponding algebraic equations. Only afterwards we can analyze the Hamiltonian
function in the neighborhood of the equilibrium conﬁguration and conclude on
stability or instability of equilibrium positions applying theorems of the KAM
theory [3,15]. Realization of such an approach involves very advanced symbolic
and numerical calculations (see [5,12,16]) which can be reasonably performed
only with computers and modern software such as the computer algebra system
Wolfram Mathematica [23], for example.
Note that if two particles P1 and P2 have the same mass and are situated
symmetrically with respect to the particle P0, the problem is simpliﬁed consid-
erably because the system has only one parameter μ = μ1 = μ2 and positions
of the massive particles are ﬁxed. Just such a case of the stability problem was
considered earlier in linear approximation [11] and in a strict nonlinear formula-
tion (see [9,20]). In case of μ1 = μ2, our results agree completely with the results
of [20] and correct some inaccuracies in the computations performed in [9].
The paper is organized as follows. In Sect. 2, we obtain algebraic equations
determining equilibrium conﬁguration of the system and analyze their solutions
for diﬀerent values of parameters μ1 and μ2. Then in Sect. 3, we analyze the
system stability in linear approximation and determine the domains of stability
in the plane Oμ1μ2 for diﬀerent positions of the particle P2. Section 4 is devoted
to calculation of the third order term in the Hamiltonian expansion and analysis
of the equilibrium positions’ stability under the third order resonance. In Sect. 4,
we consider the fourth order term of the Hamiltonian expansion and conclude on
stability of the equilibrium positions applying theorems of Arnold and Markeev.
At last, we conclude in Sect. 5.
2
Equilibrium Solutions
In the rotating frame of reference, where the particles P0, P1, P2 have rest in
the Oxy plane at the points (0, 0), (1, 0), (a, 0), respectively, the Hamiltonian
function of the system can be written in the form
H = 1
2

p2
x + p2
y

−xpy + ypx −1
κ

1

x2 + y2
+ μ1

1

(x −1)2 + y2 −x

+ μ2

1

(x −a)2 + y2 −ax
|a|3

,
(1)
where x, px and y, py are two pairs of canonically conjugate dimensionless coor-
dinate and momentum, parameter κ is given by
κ = 1 + μ1 + μ2
 a
|a|3 +
1 −a
|1 −a|3

,
(2)

332
A.N. Prokopenya
and dimensionless parameter a determining position of the particle P2 at the Ox
axis, is a real root of the equation
1 + μ1 + μ2
 a
|a|3 +
1 −a
|1 −a|3

= 1 + μ2
|a|3
+ μ1
a

1 +
a −1
|a −1|3

.
(3)
We assume here that both parameters μ1 and μ2 belong to the interval 0 <
μ1,2 ≤1, and this enables us to consider all physically diﬀerent conﬁgurations
of the system.
Equation (3) arises in the three-body problem (see [14,21]) and has three
diﬀerent real roots for any values of parameters μ1, μ2. Each of the three intervals
0 < a < 1, a > 1 and a < 0 contains only one root, and in case of μ2 = 0, these
roots determine the collinear points of libration L1, L2, L3, respectively. For
μ2 > 0, equilibrium positions of the particle P2 are shifted from the points
L1, L2, L3 but the geometrical conﬁguration of particles P0, P1, P2 doesn’t
change. Note that symbolic solution of nonlinear equation (3) cannot be found
but each of the three roots may easily be calculated numerically with necessary
precision using the built-in function FindRoot (see [23]).
Using the Hamiltonian (1), one can easily write the equations of motion of
particle P3 and show that its equilibrium coordinates (x, y) are determined by
the following system of two algebraic equations
x
(x2 + y2)3/2 −κx + μ1

1 +
x −1
((x −1)2 + y2)3/2

+ μ2
 a
|a|3 +
x −a
((x −a)2 + y2)3/2

= 0,
y

1
(x2 + y2)3/2 −κ +
μ1
((x −1)2 + y2)3/2 +
μ2
((x −a)2 + y2)3/2

= 0.
(4)
In case of μ2 = 0, system (4) reduces to the equations determining the
libration points in the restricted three-body problem (see [14,21]). If particle P2
is situated at the collinear point of libration Lj, (j = 1, 2, 3) then for μ2 > 0, two
new collinear equilibrium positions arise from the point Lj, and the other two
collinear libation points change their positions at the Ox axis. The x-coordinates
of the four collinear equilibrium positions as functions of parameter μ2 are shown
in Fig. 1 for a ﬁxed value of parameter μ1 and the particle P2 being located
between P0 and P1 (0 < a < 1). Note that position of P2 also changes when
parameter μ2 grows (dashed curve in Fig. 1). Similar pictures are obtained for
other positions of the particle P2 and diﬀerent values of μ1.
If μ2 = 0 and particle P2 is situated in one of the libration points L1, L2,
L3 the particle P3 may also stay in equilibrium in each of the triangular points
of libration L4, L5. Numerical analysis of the system (4) shows that increasing
of parameter μ2 results in shifting of the corresponding libration points in the
Oxy plane (see Fig. 2). Three bold arrows starting at the points L4, L5 show
equilibrium positions of the particle P3 in the cases when particle P2 is situated in
the neighbourhood of one of the libration points L1, L2, L3. The corresponding

Symbolic-Numerical Analysis of Stability in a Planar Four-Body Problem
333
Fig. 1. Four collinear equilibrium positions of the particle P3 arising from the libration
points L1, L2, L3 if particle P2 is situated at the point L1, µ1 = 0.2, 0 ≤µ2 ≤1.
Fig. 2. Equilibrium positions of particle P3 arising from the libration points L4, L5 if
particle P2 is situated at one of the libration points L1, L2, L3, µ1 = 0.2, 0 ≤µ2 ≤1.
shifts of particle P2 on the Ox axis are also shown by arrows starting at the
points L1, L2, L3.
Note that three cases of the particle P2 localization in the neighbourhood
of the libration points L1, L2, L3 describe all physically diﬀerent collinear geo-
metrical conﬁgurations of the massive particles P0, P1, P2. For each of these
conﬁgurations, system (4) determines four collinear and two non-collinear equi-
librium positions of particle P3, geometrically they are represented as points of
intersections of the solid and dashed curves (see Fig. 3) determined by the equa-
tions of system (4). Therefore, there are 18 diﬀerent equilibrium solutions in
the restricted four-body problem, where positions of the three massive particles

334
A.N. Prokopenya
Fig. 3. Four collinear (S1, S2, S3, S4) and two non-collinear (S5, S6) equilibrium posi-
tions of the particle P3 if particle P2 is situated in the neighbourhood of the libration
point L3 (a < 0), µ1 = 0.4, µ2 = 0.2.
are determined by the corresponding Euler solutions of the three-body problem.
Similar results were obtained earlier in [18].
3
Stability Analysis in Linear Approximation
Let us denote an equilibrium position of particle P3 in the xOy plane by (x0, y0).
The corresponding stationary values of the momenta can easily be found from
the equations of motion determined by the Hamiltonian (1) and are equal to
px0 = −y0, py0 = x0. To analyze stability of this equilibrium solution we per-
form in the Hamiltonian (1) a substitution written in terms of the Wolfram
language [23] as the following list of rules
rul1 = {x →x0 + δx, y →y0 + δy, px →−y0 + δpx, py →x0 + δpy};
(5)
The variables x, y, px, py in the right-hand side of each rule in (5) are small
deviations from to the equilibrium solution. To simplify calculation and to be
able to extract in the Hamiltonian expansion in power series in terms of the
perturbations the kth order term Hk (k = 0, 1, . . .), we add to each variable
x, y, px, py a multiplier δ. Then it is suﬃcient to expand the Hamiltonian in
the power series in terms of δ in the neighborhood of the point δ = 0. The
corresponding command written in the Wolfram Language is given by
H = Series[H/. rul1, {δ, 0, 4}] // Normal;

Symbolic-Numerical Analysis of Stability in a Planar Four-Body Problem
335
Here we obtain the Hamiltonian expansion up to the fourth order, and the kth
order term is extracted by application of the built-in function Coefficient
Hk = Coefficient[H, δ, k];
As a result, we calculate the Hamiltonian (1) in the form of power series in the
neighborhood of equilibrium point, and it is represented in the form
H = H2 + H3 + H4 + . . . ,
(6)
where Hk is the kth order homogeneous polynomial with respect to canonical
variables x, y, px, py. Note that zero-order term H0 in (6) has been omitted as a
constant, which does not inﬂuence the equations of motion, and the ﬁrst-order
term H1 is equal to zero owing to Eq. (4) determining equilibrium positions.
Therefore, the ﬁrst non-zero term in expansion (6) is a quadratic one that has
the form
H2 = 1
2

p2
x + p2
y

−pyx + pxy + h20x2 + h11xy + h02y2,
(7)
where coeﬃcients h20, h11, and h02 are given by
h20 = −
2x2
0 −y2
0
2κ(x2
0 + y2
0)5/2 −μ1
2κ
2(x0 −1)2 −y2
0
((x0 −1)2 + y2
0)5/2 −μ2
2κ
2(x0 −a)2 −y2
0
((x0 −a)2 + y2
0)5/2 ,
h11 = −3y0
κ

x0
(x2
0 + y2
0)5/2 +
μ1(x0 −1)
((x0 −1)2 + y2
0)5/2 +
μ2(x0 −a)
((x0 −a)2 + y2
0)5/2

, (8)
h02 =
x2
0 −2y2
0
2κ(x2
0 + y2
0)5/2 + μ1
2κ
(x0 −1)2 −2y2
0
((x0 −1)2 + y2
0)5/2 + μ2
2κ
(x0 −a)2 −2y2
0
((x0 −a)2 + y2
0)5/2 .
One can readily check that the linearized equations of motion determined
by the quadratic part H2 of the Hamiltonian (6) form the fourth-order linear
system of diﬀerential equations with constant coeﬃcients. Characteristic expo-
nents λ1, . . . , λ4 for such a system can easily be found (see [5,6]) and may be
represented in the form
λ1,2 = ±iσ1,
λ3,4 = ±iσ2,
(9)
where the frequencies σ1 and σ2 are given by
σ1,2 =

1 + h20 + h02 ±
	
h2
20 + h2
02 + h2
11 −2h20h02 + 4h20 + 4h02
1/2
. (10)
Recall that equilibrium position (x0, y0) may be stable for some values of
parameters μ1 and μ2 only if the corresponding characteristic exponents (9) are
diﬀerent purely imaginary numbers or the frequencies σ1 and σ2 are diﬀerent real
numbers. As coeﬃcients h20, h11, and h02 depend on both the mass parameters

336
A.N. Prokopenya
μ1 and μ2 and geometrical parameters a, x0, y0 which are also functions of μ1 and
μ2 (see (2), (3), (4)), it is very diﬃcult to check the conditions of the equilibrium
positions’ stability in analytic form. To estimate the values of parameters μ1 and
μ2 for which the equilibrium positions may be stable we can choose a grid with
small step in the plane Oμ1μ2 and calculate numerically the frequencies σ1 and
σ2 in the grid nodes.
Fig. 4. Stability boundary and resonance curves for equilibrium positions S5, S6 if
particle P2 is situated in the neighbourhood of the libration points L1 (0 < a < 1) or
L2 (a > 1). The curve f = 0 corresponds to the points for which condition (30) is not
fulﬁlled.
Numerical analysis of the frequencies (10) in the domain 0 < μ1,2 ≤1 has
shown that for the collinear equilibrium points S1, S2, S3, S4, at least one of the
frequencies σ1 and σ2 has an imaginary part for any values of parameters μ1 and
μ2 and any of the three possible equilibrium positions of particle P2. Therefore,
all twelve collinear equilibrium positions are unstable in Lyapunov’s sense. Due
to the symmetry of the system with respect to the Ox axis, the equilibrium points
S5 and S6 have the same properties, and they are stable in linear approximation
if parameters μ1 and μ2 are smaller than their values on the stability boundaries
which are determined from the condition σ1 = σ2. The corresponding curves for
the equilibrium points S5 and S6 have been found numerically and are shown as
dashed curves in the Oμ1μ2 plane in Figs. 4 and 5. It should be noted that the
stability domains are the same if particle P2 is situated in the neighbourhood of
the libration points L1 and L2. It is quite natural because one case is obtained
from another one by means of mutual replacement μ1 →μ2, μ2 →μ1 and the

Symbolic-Numerical Analysis of Stability in a Planar Four-Body Problem
337
scale transformation a →1/a. Besides, both domains of stability shown in Figs. 4
and 5 are symmetrical with respect to the line μ2 = μ1.
Fig. 5. Stability boundary and resonance curves for equilibrium positions S5, S6 if
particle P2 is situated in the neighbourhood of the libration point L3 (a < 0).
4
Normalization of the Hamiltonian
The most-used method for studying the Hamiltonian systems of nonlinear dif-
ferential equations is the Poincar´e method of normal forms (see [14]). It includes
constructing a real-valued canonical transformation, reducing the Hamiltonian of
the system to the Birkhoﬀnormal form, and applying theorems of the KAM the-
ory [3,15]. As in the neighbourhood of the equilibrium position the Hamiltonian
is represented in the form of expansion (6), we have to normalize successively
the terms H2, H3, H4, . . .
Canonical transformation normalizing the quadratic part H2 of the Hamil-
tonian may be constructed in symbolic form, the corresponding algorithm has
been described in detail in [14,16]. Doing necessary symbolic calculations, we
obtain the transformation in the form
x = 2c1p1 + 2c2p2,
y = −2σ1u1q1 + 2σ2u2q2 + h11u1p1 + h11u2p2,
px = −v1q1 + v2q2 −h11u1p1 −h11u2p2,
py = −h11σ1u1q1 + h11σ2u2q2 + g1p1 + g2p2,
(11)

338
A.N. Prokopenya
where p1, q1 and p2, q2 are two pairs of new canonically conjugated variables,
uk = 2ck(1 −h20 + σ2
k)
h2
11 + 4σ2
k
, vk = 2ckσk(−2 + 4h20 + h2
11 + 2σ2
k)
h2
11 + 4σ2
k
,
gk = 2ck(h2
11 + (2 + 4h20)σ2
k −2σ4
k)
h2
11 + 4σ2
k
,
ck =

(−1)k(h2
11 + 4σ2
k)
4σk(−3 + h2
11 + 4h20 + 4h2
20 + 2(1 −2h20)σ2
k + σ4
k)
1/2
, k = 1, 2.
Applying this transformation to (7), we reduce the quadratic part of the
Hamiltonian H2 to the normal form
H2 = 1
2

σ1(p2
1 + q2
1) −σ2(p2
2 + q2
2)

.
(12)
It should be emphasized that the quadratic form (12) is neither a positive
nor negative deﬁnite function and, hence, one cannot conclude on stability or
instability of equilibrium solutions, using the principle of linearized stability.
Therefore, we have to solve the problem in a strict nonlinear formulation.
To normalize the third-order term H3 in the Hamiltonian (6) we use the
method of constructing the real-valued canonical transformation of Birkhoﬀ’s
type described in [5,6,9]. We start from the term H3 in (6) given by
H3 =
 2x3
0 −3y2
0
(x2
0 + y2
0)7/2 + μ1(x0 −1)(2(x0 −1)2 −3y2
0)
((x0 −1)2 + y2
0)7/2
+ μ2(x0 −a)(2(x0 −a)2 −3y2
0)
((x0 −a)2 + y2
0)7/2
 x3
2κ
+

4x3
0 −y2
0
(x2
0 + y2
0)7/2 + μ1(4(x0 −1)2 −y2
0)
((x0 −1)2 + y2
0)7/2 + μ2(4(x0 −a)2 −y2
0)
((x0 −a)2 + y2
0)7/2
 3y0x2y
2κ
−
 x3
0 −4x0y2
0
(x2
0 + y2
0)7/2 + μ1(x0 −1)((x0 −1)2 −4y2
0)
((x0 −1)2 + y2
0)7/2
+
+ μ2(x0 −a)((x0 −a)2 −4y2
0)
((x0 −a)2 + y2
0)7/2
 3xy2
2κ
−
 3x3
0 −2y2
0
(x2
0 + y2
0)7/2 + μ1(3(x0 −1)2 −2y2
0)
((x0 −1)2 + y2
0)7/2 + μ2(3(x0 −a)2 −2y2
0)
((x0 −a)2 + y2
0)7/2
 y0y3
2κ .
(13)
On substituting the coordinates x, y from (11) into (13) and simplifying the
expression obtained, we reduce the third-order term H3 to the form
H3 =

i+j+k+l=3
h(3)
ijklqi
1qj
2pk
1pl
2.
(14)
As transformation (11) is linear, the coeﬃcients h(3)
ijkl in (14) are determined
only by the corresponding coeﬃcients from (13) and coeﬃcients of q1, q2, p1, p2

Symbolic-Numerical Analysis of Stability in a Planar Four-Body Problem
339
in (11). The expressions obtained for h(3)
ijkl are quite bulky, so we do not write
them here. Similar, though more cumbersome calculations are performed with
the fourth-order term H4 and the transformation (11) reduces it to the form
H4 =

i+j+k+l=4
h(4)
ijklqi
1qj
2pk
1pl
2.
(15)
Note that canonical transformation (11) does not mix the terms of diﬀerent
orders H2, H3, H4 in the Hamiltonian (6).
The next step is to construct a canonical transformation that eliminates the
third-order term H3 in the Hamiltonian expansion. In addition to linear terms,
such a transformation must contain the second-degree terms in new canonical
variables and may be obtained with the aid of the following generating function
S(˜p1, ˜p2, q1, q2) = q1˜p1 + q2˜p2 +

i+j+k+l=3
s(3)
ijklqi
1qj
2˜pk
1 ˜pl
2,
(16)
where coeﬃcients s(3)
ijkl are to be found. The function (16) determines new
momenta ˜p1, ˜p2 and coordinates ˜q1, ˜q2 according to the relationships
˜q1 = ∂S
∂˜p1
, ˜q2 = ∂S
∂˜p2
, p1 = ∂S
∂q1
, p2 = ∂S
∂q2
,
(17)
which form a system of algebraic equations with respect to the old canonical
variables q1, q2, p1, p2. Solution of this system can be sought in the form of
polynomials in terms of the new canonical variables ˜q1, ˜q2, ˜p1, ˜p2. To ﬁnd an
expression for the third-order term H3 in new variables it is suﬃcient to consider
the polynomials of second degree but the third-degree terms also should be
taken into account because they inﬂuence the fourth-order term H4 that will be
normalized at the next step. In general, to calculate the term Hk in new variables
one needs to consider solutions of system (17) in the form of polynomials of the
(k −1)th degree.
Applying the method of iterations and doing quite standard symbolic calcula-
tion, we solve system (17) and obtain the old canonical variables q1, q2, p1, p2 as
functions of the new ones ˜q1, ˜q2, ˜p1, ˜p2 which in linear approximation determine
an identical canonical transformation. Then we substitute these variables into
the Hamiltonian expansion H = H2 +H3 +H4, where H2, H3, and H4 are given
by (12), (14), and (15), respectively, and expand the expression obtained into
the power series in terms of new variables ˜q1, ˜q2, ˜p1, ˜p2 up to the fourth order.
One can easily check that the second-order term ˜H2 in new variables retains the
normal form (12) while coeﬃcients ˜h(3)
ijkl in the term ˜H3 represented in the form
(14) become linear functions of h(3)
ijkl and unknown coeﬃcients s(3)
ijkl determining
the generating function (16). Equating the coeﬃcients ˜h(3)
ijkl to zero, we obtain a
system of linear equations with respect to the coeﬃcients s(3)
ijkl. The calculations
show that if the frequencies σ1, σ2 satisfy the conditions
σ1 ± 2σ2 ̸= 0, 2σ1 ± σ2 ̸= 0, σ1 ± σ2 ̸= 0,
(18)

340
A.N. Prokopenya
and σ1 ̸= 0, σ2 ̸= 0 then this system has a unique solution. It means that real-
ization of canonical transformation (17) with the found coeﬃcients s(3)
ijkl elimi-
nates the third-order term H3 in the Hamiltonian expansion. The corresponding
expressions for coeﬃcients s(3)
ijkl are given in [5,6,16].
Analyzing frequencies (10), we obtain that there are such points (μ1, μ2) in
the Oμ1μ2 plane in the domain of linear stability of the equilibrium positions S5,
S6 for which the condition of third-order resonance σ1 −2σ2 = 0 is fulﬁlled (see
Figs. 4 and 5). For such values of parameters μ1 and μ2, some coeﬃcients ˜h(3)
ijkl
cannot be equal to zero and, therefore, it is not possible to eliminate the term ˜H3
completely (see [5,16]). There are six such coeﬃcients and they should be chosen
in such a way that the Hamiltonian H takes the form admitting application of
Markeev’s theorem on instability of equilibrium positions under the third-order
resonance [14]. Requiring the following conditions to be fulﬁlled
˜h(3)
0012 = B1
2
√
2,
˜h(3)
0210 = −B1
2
√
2,
˜h(3)
1101 = −B1
√
2,
˜h(3)
0111 = B2
√
2,
˜h(3)
1002 = B2
2
√
2,
˜h(3)
1200 = −B2
2
√
2,
(19)
where B1, B2 are some constants, and solving the system of equations (19), we
obtain the corresponding coeﬃcients s(3)
ijkl of the generating function (16) and
ﬁnd the constants B1, B2 as
B1 =
1
√
2(h(3)
0012 −h(3)
0210 −h(3)
1101),
B2 =
1
√
2(h(3)
0111 + h(3)
1002 −h(3)
1200).
(20)
Then the Hamiltonian (6) takes a form
˜H = 1
2σ1

˜q2
1 + ˜p2
1

−1
2σ2

˜q2
2 + ˜p2
2

+ B1
2
√
2

˜p1˜p2
2 −˜p1˜q2
2 −2˜q1˜q2˜p2

+ B2
2
√
2

˜q1˜p2
2 −˜q1˜q2
2 + 2˜q2˜p1˜p2

+ ˜H4 + . . . .
(21)
Numerical analysis of parameter B =

B2
1 + B2
2 for the equilibrium points
S5 and S6 has shown that it is not equal to zero for all points (μ1, μ2) belonging
to the resonance curves σ1 −2σ2 = 0 for any of the three possible collinear
conﬁgurations of the particles P0, P1, P2. According to Markeev’s theorem [14],
we can conclude that equilibrium points S5 and S6 in the circular restricted
four-body problem formulated on the basis of Euler’s collinear conﬁgurations
are unstable under third-order resonance of the form σ1 −2σ2 = 0.
Let us consider the points (μ1, μ2) in the domain of linear stability of equi-
librium positions S5, S6 for which the condition σ1 ̸= 2σ2 is fulﬁlled, and there
is no resonance in the system up to the third order inclusively. Then after nor-
malization of the second and third order terms, we obtain the Hamiltonian (6)
in the form
˜H = ˜H2 + ˜H4 + . . . ,
(22)

Symbolic-Numerical Analysis of Stability in a Planar Four-Body Problem
341
where the second-order term
˜H2 = 1
2

σ1(˜p2
1 + ˜q2
1) −σ2(˜p2
2 + ˜q2
2)

(23)
has the normal form, the third-order term ˜H3 is absent, and the fourth-order
term ˜H4 may be written as
˜H4 =

i+j+k+l=4
˜h(4)
ijkl˜qi
1˜qj
2˜pk
1 ˜pl
2.
(24)
The sum (24) contains 35 terms but coeﬃcients ˜h(4)
ijkl are very cumbersome, and
we do not write them here. Now we look for the fourth-degree polynomial
S(p∗
1, p∗
2, ˜q1, ˜q2) = ˜q1p∗
1 + ˜q2p∗
2 +

i+j+k+l=4
s(4)
ijkl˜qi
1˜qj
2p∗k
1 p∗l
2 ,
(25)
generating the canonical transformation reducing the fourth order term ˜H4 to
the simplest form. New momenta p∗
1, p∗
2 and coordinates q∗
1, q∗
2 are determined
by the relationships
q∗
1 = ∂S
∂p∗
1
, q∗
2 = ∂S
∂p∗
2
, ˜p1 = ∂S
∂˜q1
, ˜p2 = ∂S
∂˜q2
.
(26)
Resolving (26) with respect to the old canonical variables ˜q1, ˜q2, ˜p1, ˜p2 in the
neighborhood of the point q∗
1 = q∗
2 = p∗
1 = p∗
2 = 0 and substituting the solution
into (22), we expand the Hamiltonian ˜H in the Taylor series in terms of q∗
1, q∗
2,
p∗
1, p∗
2. Obviously, the second order term H∗
2 in this expansion retains the form
(23), the third order term H∗
3 is absent, and the fourth order term H∗
4 is a sum
of 35 terms of the form
h∗(4)
ijklq∗i
1 q∗j
2 p∗k
1 p∗l
2
(i + j + k + l = 4),
where new coeﬃcients h∗(4)
ijkl are linear functions of ˜h(4)
ijkl and unknown coeﬃcients
s(4)
ijkl, determining the generating function (25).
Analysis of the coeﬃcients h∗(4)
ijkl shows that they are divided into several
independent groups, and each group forms a system of equations determining
some coeﬃcients s(4)
ijkl. If the following conditions
σ1 ̸= 0,
σ2 ̸= 0,
σ1 ± σ2 ̸= 0,
σ1 ± 3σ2 ̸= 0,
3σ1 ± σ2 ̸= 0,
(27)
are fulﬁlled we can solve the equations h∗(4)
ijkl = 0 and ﬁnd coeﬃcients s(4)
ijkl of the
canonical transformation (26) eliminating the corresponding terms in (24). Nev-
ertheless, there are ten terms in the expansion (24) which cannot be eliminated.
They can be only simpliﬁed in such a way that the fourth order term ˜H4 takes
the form (see [5,16])
H∗
4 = 1
4

c20(p∗2
1 + q∗2
1 )2 + c11(p∗2
1 + q∗2
1 )(p∗2
2 + q∗2
2 ) + c02(p∗2
2 + q∗2
2 )2
.

342
A.N. Prokopenya
Then, using the standard canonical transformation
q∗
1 =
√
2τ1 sin ϕ1, p∗
1 =
√
2τ1 cos ϕ1,
q∗
2 =
√
2τ2 sin ϕ2, p∗
2 =
√
2τ2 cos ϕ2,
(28)
we rewrite the Hamiltonian (22) as
H∗= σ1τ1 −σ2τ2 + c20τ 2
1 + c11τ1τ2 + c02τ 2
2 + H∗
5(ϕ1, ϕ2, τ1, τ2) + . . . .
(29)
Recall that Arnold’s theorem [3] states that in the case of absence of res-
onances up to the fourth order inclusively (conditions (18),(27) are fulﬁlled),
equilibrium positions are stable if
f = c20σ2
2 + c11σ1σ2 + c02σ2
1 ̸= 0.
(30)
Numerical analysis of parameter f shows that for equilibrium points S5 and S6,
there exist such values of parameters μ1 and μ2, for which f = 0 (see Figs. 4
and 5). For such μ1, μ2 the ﬁfth and higher order terms in the Hamiltonian
expansion (6) need to be analyzed to conclude on stability or instability of equi-
librium solution. The corresponding calculations are very cumbersome, and this
case will be analyzed in our next paper.
Besides, there are curves in the Oμ1μ2 plane (see Figs. 4 and 5), where the
condition of fourth-order resonance of the form σ1 = 3σ2 is fulﬁlled. In this case,
eight additional terms appear in the expression for H∗
4 because the following
coeﬃcients h∗(4)
ijkl do not vanish and are expressed via two parameters A1, A2
h∗(4)
0013 = −1
3h∗(4)
0211 = −1
3h∗(4)
1102 = h∗(4)
1300 = A1
4 ,
h∗(4)
1003 = 1
3h∗(4)
0112 = −1
3h∗(4)
1201 = −h∗(4)
0310 = A2
4 .
(31)
Solving system (31), we ﬁnd the corresponding coeﬃcients s(4)
ijkl, and parameters
A1 and A2 are obtained in the form
A1 = 1
2(˜h(4)
0013 −˜h(4)
0211 −˜h(4)
1102 + ˜h(4)
1300),
A2 = 1
2(˜h(4)
0112 −˜h(4)
0310 + ˜h(4)
1003 −˜h(4)
1201).
Finally, the Hamiltonian (22) is reduced to the form
H∗= 3σ2
2

p∗2
1 + q∗2
1

−σ2
2

p∗2
2 + q∗2
2

+ 1
4

c20(p∗2
1 + q∗2
1 )2 + c11(p∗2
1 + q∗2
1 )(p∗2
2 + q∗2
2 ) + c02(p∗2
2 + q∗2
2 )2
+ A1
4

p∗
1p∗3
2 −3q∗2
2 p∗
1p∗
2 −3q∗
1q∗
2p∗2
2 + q∗
1q∗3
2

+ A2
4

q∗
1p∗3
2 −3q∗
1q∗2
2 p∗
2 + 3q∗
2p∗
1p∗2
2 −p∗
1q∗3
2

.
(32)

Symbolic-Numerical Analysis of Stability in a Planar Four-Body Problem
343
According to the theorem of Markeev [14], stability of the equilibrium
solutions under the fourth-order resonance depends on the values of c20 +3c11 +
9c02 and 3

3(A2
1 + A2
2). Our calculations show that if the particle P2 is situated
in the neighborhood of the points of libration L1 and L2, an inequality
c20 + 3c11 + 9c02 < 3
	
3(A2
1 + A2
2)
takes place for all points (μ1, μ2) belonging to the resonant curve σ1 = 3σ2 (see
Fig. 4). It means that the fourth-order resonance results in instability of equilib-
rium positions S5, S6 for such collinear conﬁgurations of the massive particle.
The same result is obtained in the case when the particle P2 is situated in the
neighborhood of the point L3, and the point (μ1, μ2) is located at the resonant
curve σ1 = 3σ2 to the left of the points C1 and C2 (see Fig. 5). However, for the
points (μ1, μ2) belonging to the arc C1C2, we obtain
c20 + 3c11 + 9c02 > 3
	
3(A2
1 + A2
2),
and equilibrium positions S5 and S6 are stable in Liapunov’s sense.
5
Conclusion
In the present paper, we have studied stability of the equilibrium positions in the
planar circular restricted four-body problem formulated on the basis of the Euler
collinear solutions of the three-body problem. We have proved that all collinear
equilibrium positions of particle P3 are unstable for any values of the system
parameters μ1 and μ2. Similar results were obtained earlier in the restricted
three-body problem (see [14,21]). The equilibrium positions S5 and S6 (see
Fig. 3) are stable in Liapunov’s sense if parameters μ1 and μ2 are suﬃciently
small and belong to the domains bounded by the curves σ1 = σ2 (Figs. 4 and 5).
However, in these domains, there are such values of parameters μ1 and μ2 for
which the conditions of the third- or fourth-order resonances are fulﬁlled. The
third-order resonance results in instability of equilibrium points S5 and S6 for
any collinear position of the particle P2 while in case of the fourth-order reso-
nance, stability of these points may take place if the particle P2 is situated in
the neighborhood of the point of libration L3, and the point (μ1, μ2) is located
at the resonant curve between the points C1 and C2.
There are also such values of parameters μ1 and μ2 for which the conditions
of Arnold’s theorem are not fulﬁlled (curves f = 0 in Figs. 4 and 5), and analysis
of the ﬁfth and higher order terms in the Hamiltonian expansion is required for
the entire solution of the stability problem. Such analysis will be done in our
next paper.
Note that all relevant calculations and visualization of the obtained results
are performed with the computer algebra system Wolfram Mathematica.

344
A.N. Prokopenya
References
1. Alvares-Ramirez, M., Skea, J.E.F., Stuchi, T.J.: Nonlinear stability analysis in
a equilateral restricted four-body problem. Astrophys. Space Sci. (2015). doi:10.
1007/s10509-015-2333-4
2. Arenstrof, R.E.: Central conﬁgurations of four bodies with one inferior mass.
Celest. Mech. 29, 9–15 (1982)
3. Arnold, V.I.: Small denominators and problems of stability of motion in classical
and celestial mechanics. Russ. Math. Surv. 18(6), 85–191 (1963)
4. Budzko, D.A., Prokopenya, A.N.: Symbolic-numerical analysis of equilibrium solu-
tions in a restricted four-body problem. Program. Comput. Softw. 36(2), 68–74
(2010)
5. Budzko, D.A., Prokopenya, A.N.: On the stability of equilibrium positions in the
circular restricted four-body problem. In: Gerdt, V.P., Koepf, W., Mayr, E.W.,
Vorozhtsov, E.V. (eds.) CASC 2011. LNCS, vol. 6885, pp. 88–100. Springer, Hei-
delberg (2011). doi:10.1007/978-3-642-23568-9 8
6. Budzko, D.A., Prokopenya, A.N.: Stability of equilibrium positions in the spatial
circular restricted four-body problem. In: Gerdt, V.P., Koepf, W., Mayr, E.W.,
Vorozhtsov, E.V. (eds.) CASC 2012. LNCS, vol. 7442, pp. 72–83. Springer, Heidel-
berg (2012). doi:10.1007/978-3-642-32973-9 7
7. Budzko, D.A., Prokopenya, A.N.: Symbolic-numerical methods for searching equi-
librium states in a restricted four-body problem. Program. Comput. Softw. 39(2),
74–80 (2013)
8. Deprit, A., Deprit-Bartholom´e, A.: Stability of the triangular Lagrangian points.
Astron. J. 72(2), 173–179 (1967)
9. Gadomski, L., Grebenikov, E.A., Prokopenya, A.N.: Studying the stability of equi-
librium solutions in the planar circular restricted four-body problem. Nonlinear
Oscil. 10(1), 66–82 (2007)
10. Grebenikov, E.A., Ikhsanov, E.V., Prokopenya, A.N.: Numeric-symbolic computa-
tions in the study of central conﬁgurations in the planar newtonian four-body prob-
lem. In: Ganzha, V.G., Mayr, E.W., Vorozhtsov, E.V. (eds.) CASC 2006. LNCS,
vol. 4194, pp. 192–204. Springer, Heidelberg (2006). doi:10.1007/11870814 16
11. Kozak, D., Oniszk, E.: Equilibrium points in the restricted four-body problem.
Suﬃcient conditions for linear stability. Rom. Astron. J. 8(1), 27–31 (1998)
12. Kozera, R., Noakes, L., Klette, R.: External versus internal parameterizations for
lengths of curves with nonuniform samplings. In: Asano, T., Klette, R., Ronse, C.
(eds.) Geometry, Morphology, and Computational Imaging. LNCS, vol. 2616, pp.
403–418. Springer, Heidelberg (2003). doi:10.1007/3-540-36586-9 26
13. Leontovich, A.M.: On the stability of Lagrangian periodic solutions of the restricted
three-body problem. Soviet Math. Dokl. 3, 425–429 (1962)
14. Markeev, A.P.: Libration points in Celestial Mechanics and Cosmodynamics.
Nauka, Moscow (1978). (in Russian)
15. Moser, J.: Lectures on the Hamiltonian systems. Mir, Moscow (1973). (in Russian)
16. Prokopenya, A.N.: Hamiltonian normalization in the restricted many-body prob-
lem by computer algebra methods. Program. Comput. Softw. 38(3), 156–166
(2012)
17. Roy, A.E.: Orbital Motion, 4th edn. Institute of Physics Publishing, Bristol/
Philadephia (2005)
18. Simo, C.: Relative equilibrium solutions in the four body problem. Celest. Mech.
18, 165–184 (1978)

Symbolic-Numerical Analysis of Stability in a Planar Four-Body Problem
345
19. Singh, J., Vincent, A.E.: Eﬀect of perturbations in the Coriolis and centrifugal
forces on the stability of equilubrium points in the restricted four-body problem.
Few-Body Syst. 56, 713–723 (2015)
20. Schmidt, D., Vidal, C.: Stability of the planar equilibrium solutions of a restricted
1 + N body problem. Regul. Chaotic Dyn. 19(5), 533–547 (2014)
21. Szebehely, V.: Theory of Orbits. The Restricted Problem of Three Bodies. Acad-
emic Press, New York/London (1967)
22. Wintner, A.: The Analytical Foundations of Celestial Mechanics. Princeton Math-
ematical Series, vol. 5. Princeton University Press, Princeton (1941)
23. Wolfram, S.: An Elementary Introduction to the Wolfram Language, 2nd edn.
Wolfram Media, Champaign (2017)

The Method of Collocations and Least Residuals
Combining the Integral Form of Collocation
Equations and the Matching Diﬀerential
Relations at the Solution of PDEs
Vasily P. Shapeev1,2 and Evgenii V. Vorozhtsov1(B)
1 Khristianovich Institute of Theoretical and Applied Mechanics,
Russian Academy of Sciences, Novosibirsk 630090, Russia
{shapeev,vorozh}@itam.nsc.ru
2 Novosibirsk National Research University, Novosibirsk 630090, Russia
Abstract. To increase the accuracy of computations by the method
of collocations and least residuals (CLR) it is proposed to increase the
number of degrees of freedom with the aid of the following two tech-
niques: an increase in the number of basis vectors and the integration of
the linearized partial diﬀerential equations (PDEs) over the subcells of
each cell of a spatial computational grid. The implementation of these
modiﬁcations, however, leads to the necessity of increasing the amount
of symbolic computations needed for obtaining the work formulas of the
new versions of the CLR method. The computer algebra system (CAS)
Mathematica has proved to be successful at the execution of all these
computations. It is shown that the proposed new symbolic-numeric ver-
sions of the CLR method possess a higher accuracy than the previous
versions of this method. Furthermore, the version of the CLR method,
which employs the integral form of collocation equations, needs a much
lesser number of iterations for its convergence than the “diﬀerential”
CLR method.
Keywords: Computer algebra system · Symbolic-numerical algorithm ·
Collocation of integral relations · Preconditioner · Krylov subspaces ·
Multigrid
1
Introduction
At present, the numerical simulation of various processes in technologies and
industry with the aid of the numerical solution of the initial- and boundary-value
problems for the systems of nonlinear partial diﬀerential equations (PDEs) has
gained widespread acceptance. In particular, some applied tasks involving the
solution of the Navier–Stokes equations are very computationally intensive and
require a CPU time from several weeks to one year [2,14]. In this connection,
the development of more eﬃcient methods for the numerical solution of the PDE
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 346–361, 2017.
DOI: 10.1007/978-3-319-66320-3 25

The Method of Collocations and Least Residuals
347
systems, which would enable a signiﬁcant reduction of the needed CPU times,
is urgent as before.
At the derivation of the work formulas of complex high-accuracy numerical
methods, the errors are practically unavoidable in the cases when the above
formulas are derived by a mathematician “by hand” with the aid of pen and
paper. There are already by now fairly many works the authors of which have
shown a substantial beneﬁt from using computer algebra systems (CASs) in the
process of deriving the formulas of new numerical algorithms, their realization
and veriﬁcation of the corresponding computer codes [1,7,8,25].
The CLR method, which was proposed in [15] and developed further in the
subsequent works of other authors, is one of the methods which enable the eﬃ-
cient solution of PDEs [10,16,17,19–24]. The works [10,20–23] have shown the
usefulness of the application of a CAS to the derivation of formulas of the diﬀer-
ent versions of the CLR method. The versions of the method were constructed,
which have enabled the obtaining of the solutions of the 2D and 3D benchmark
problems, which are among the most accurate ones at present [3,18].
During the last three decades, a class of the numerical techniques named
LSFEM (Least-Squares Finite Element Method) [11,12] has gained a fairly wide
acceptance. In this class of methods, the FEM (Finite Element Method) is com-
bined with the method of least squares. In FEM, the PDEs to be solved are at
ﬁrst integrated over each ﬁnite element, which represents a subregion of the spa-
tial computational region. This approach has stimulated the present authors to
consider the following versions of the CLR method: (i) a version in which all the
collocation equations derived from the PDE system are replaced with their inte-
gral counterparts, which are obtained at the integration of the PDEs over several
subcells, into which each cell of the spatial grid is partitioned; (ii) a version in
which both the collocation equations obtained from the PDEs and the equations
obtained by integrating over the subcells are employed. All the analytic compu-
tations needed for obtaining the work formulas of the above modiﬁcations of the
CLR method have been carried out with the aid of corresponding Mathematica
codes to avoid any errors and to speed up all the needed jobs.
We describe below in the present work both the original “diﬀerential” CLR
method and the CLR methods in which the “diﬀerential” version is combined
with the integral form of collocation equations and the diﬀerential forms of
matching conditions. The computational examples are presented, which show
that the new modiﬁcations of the CLR method enable the obtaining of more
accurate results than in the case of the “diﬀerential” versions of the CLR method.
2
The “Diﬀerential” CLR Method
2.1
Description of the Method
Consider a boundary-value problem for the system of Navier–Stokes equations
(V · ∇)V + ∇p = 1
ReΔV −f,
div V = 0,
(x1, x2) ∈Ω,
(1)
V

∂Ω = g
(2)

348
V.P. Shapeev and E.V. Vorozhtsov
in the region Ω with the boundary ∂Ω. In Eq. (1), x1, x2 are the Cartesian spatial
coordinates, V = (v1(x1, x2), v2(x1, x2)) is the velocity vector; p = p(x1, x2)
is the pressure, f = (f1, f2) is the given vector function, Re is the Reynolds
number, Δ =
∂2
∂x2
1 +
∂2
∂x2
2 , (V · ∇) = v1
∂
∂x1 + v2
∂
∂x2 . System (1) is solved under
the Dirichlet boundary conditions (2), where g = g(x1, x2) = (g1, g2) is a given
vector function. The pressure is determined from (1) and (2) with the accuracy
up to a constant. We will choose this constant in the following in such a way
that the following condition is satisﬁed:

Ω p dx1dx2 = 0.
(3)
The square
Ω = {(x1, x2), 0 ≤xi ≤L, i = 1, 2},
(4)
is taken as the problem solution region, where L > 0 is the given length of the
square side. The quantity L was used in speciﬁc computations as the reference
length at the non-dimensionalization of variables, and it enters the deﬁnition of
the Reynolds number Re in (1) in a natural way. We will term in the following
the boundary-value problem for the PDE the diﬀerential problem.
In the given problem (1)–(4), region (4) is discretized by a grid with square
cells Ωij, i, j = 1, . . . , I, I ≥1. It is convenient to introduce the local coordinates
y1 and y2 in each cell Ωij. The dependence of local coordinates on global spatial
variables x1 and x2 is speciﬁed by relations ym = (xm −xm,i,j)/h, m = 1, 2,
where xm,i,j is the value of the coordinate xm at the center of cell Ωij, and h
is the halved length of the square cell side. Let u(y1, y2) = (u1, u2) = V(hy1 +
x1,i,j, hy2 + x2,i,j), q(y1, y2) = p(hy1 + x1,i,j, hy2 + x2,i,j). The Navier–Stokes
equations then take the following form:
Δum −Reh

u1
∂um
∂y1
+ u2
∂um
∂y2
+ ∂q
∂ym

= Re · h2fm,
m = 1, 2;
(5)
1
h
∂u1
∂y1
+ ∂u2
∂y2

= 0,
(6)
where Δ =
∂2
∂y2
1 + ∂2
∂y2
2 . The Newton linearization of Eq. (5) gives the equation
ξ[Δus+1
m
−(Re · h)(us
1us+1
m,y1 + us+1
1
us
m,y1 + us
2us+1
m,y2
+ us+1
2
us
m,y2 + qs+1
ym )] = ξFm,
(7)
where m = 1, 2, and s is the number of the iteration over the nonlinearity,
s = 0, 1, 2, . . ., us
1, us
2, qs is the known approximation to the solution at the
sth iteration starting from the chosen initial guess with index s = 0, Fm =
Re

h2fm−h

us
1us
m,y1 + us
2us
m,y2
	
, um,yl = ∂um/∂yl, qym = ∂q/∂ym, l, m =
1, 2. The user-speciﬁed parameter ξ has been introduced here as in [24] for the
purpose of controlling the magnitude of the condition number of a system of
linear algebraic equations (SLAE), which must be solved in each cell Ωij.

The Method of Collocations and Least Residuals
349
The approximate solution in each cell Ωi,j is sought in the form of a linear
combination of the basis vector functions ϕl:
(us
1, us
2, qs)T = 
mb
l=1 bs
i,j,lϕl,
(8)
where the superscript T denotes the transposition operation, and mb is the user-
speciﬁed number of the basis vector functions. In the given version of the method,
the ϕl are the polynomials. Thus, the approximate solution is a piecewise poly-
nomial. In the work [24], the second-degree polynomials in variables y1, y2 were
employed for the approximation of velocity components, and the ﬁrst-degree
polynomial was used for the pressure approximation so that the total number of
the basis vector functions in (8) amounted to mb = 12.
It was shown previously in [10] that it is possible to increase the accuracy of
the numerical solution obtained by the CLR method by using the polynomials of
higher degrees. In this connection, we use in the present paper the second-degree
polynomial also for the pressure approximation. In this case, there are eighteen
basis functions in total. Since the coeﬃcients are constant in the continuity
equation, which has a simple form, it is easy to satisfy it at the expense of the
choice of basis polynomials ϕl. It is not diﬃcult to ﬁnd that it is required to
this end that they satisfy three linear relations. There will ﬁnally remain only
ﬁfteen independent basis polynomials from the original eighteen ones. They are
presented in Table 1. One can term their set a solenoidal basis because div ϕl = 0.
The set of basis functions, which was employed in [24], is obtained from the set
presented in Table 1 if one sets mb = 12 in (8), that is if one retains in Table 1
only the ﬁrst 12 basis vector functions.
Table 1. The form of basis functions ϕl
l
1 2
3
4
5
6
7 8
9
10 11 12 13 14
15
ϕl 1 y1
y2 y2
1
−2y1y2 y2
2
0 0
0
0
0
0
0
0
0
0 −y2 0
−2y1y2 y2
2
0
1 y1 y2
1
0
0
0
0
0
0
0 0
0
0
0
0
0 0
0
1
y1
y2
y2
1
y1y2 y2
2
The number of collocation points and their location inside the cell may vary
in diﬀerent versions of the method. In the given work, three versions of the
speciﬁcation of the collocation point coordinates have been implemented. Denote
by Nc the number of collocation points inside each cell. In the case when Nc =
2, the coordinates of collocation points are as follows: (ω, ω), (−ω, ω), where
0 < ω < 1. At Nc = 4, the local coordinates of collocation points have the
form (±ω, ±ω). In the case of Nc = 8, the coordinates of collocation points were
speciﬁed in the following way: the locations of the ﬁrst four points were the
same as at Nc = 4, and the coordinates of the next four points were speciﬁed by
formulas (±ω, 0), (0, ±ω).

350
V.P. Shapeev and E.V. Vorozhtsov
Substituting (8) as well as the numerical values of the coordinates of each
collocation point in (7) we obtain 2Nc linear algebraic equations:

mb
m=1 a(1)
ν,m · bs+1
m
= f s
ν,
ν = 1, . . . , 2Nc.
(9)
By analogy with [24] let us augment the system of equations of the approx-
imate problem in the Ωij cell by the conditions of matching with the solutions
of the discrete problem, which are taken in all cells adhering to the given cell.
We will write these conditions at separate points (called the matching points)
on the sides of the Ωij cell, which are common with its neighboring cells. The
matching conditions are taken here in the form
h ∂(u+)n
∂n
+ η(u+)n = h ∂(u−)n
∂n
+ η(u−)n,
(10)
h ∂(u+)τ
∂n
+ (u+)τ = h ∂(u−)τ
∂n
+ (u−)τ,
(11)
q+ = q−.
(12)
Here h ∂
∂n = h

n1
∂
∂x1 + n2
∂
∂x2

= n1
∂
∂y1 + n2
∂
∂y2 , n = (n1, n2) is the external
normal to the side of the Ωij cell, (·)n, (·)τ are the normal and tangential com-
ponents of the velocity vector with respect to the cell side, u+, u−are the limits
of the function u as its arguments tend to the matching point from inside and
outside the Ωij cell. The user-speciﬁed parameter η has been introduced here as
in [24] for the purpose of controlling the magnitude of the condition number of
a SLAE, which must be solved in each cell Ωij.
For the uniqueness of the pressure determination in the solution, we either
specify its value at a single point of the region or approximate condition (3) by
the formula
1
h

Ωi,j q dy1dy2

= 1
h

−I∗+

Ωi,j q∗dy1dy2

.
(13)
Here I∗is the integral over the entire region, which is computed as a sum of
the integrals over each cell at the foregoing iteration, q∗is the pressure in a cell
from the foregoing iteration.
Denote by Nm the number of matching points for the velocity vector com-
ponents on the sides of each cell. At Nm = 4, the coordinates of these matching
points are speciﬁed by the formulas (±1, 0), (0, ±1). At Nm = 8, the coordinates
of matching points are as follows: (±1, −ζ), (±1, ζ), (−ζ, ±1), (ζ, ±1), where
0 < ζ < 1. In the computational examples presented below, the value ζ = 1/2
was used. The matching conditions for pressure (12) are set at four points with
coordinates (±1, 0), (0, ±1).
Using Eq. (8), we substitute the coordinates of these points in each of three
matching conditions (10)–(12). We obtain from the ﬁrst two conditions 2Nm
linear algebraic equations for velocity components. The substitution of represen-
tation (8) in (12) also yields four linear algebraic (matching) equations.
In the present work, the pressure was speciﬁed at the vertex of the Ω1,1 cell
or condition (13) was used. If the cell side coincides with the boundary of region

The Method of Collocations and Least Residuals
351
Ω, then the boundary conditions are written at the corresponding points instead
of the matching conditions for the discrete problem solution: um = gm, m = 1, 2.
Uniting the equations of collocations, matching, and the equations obtained
form the boundary conditions, if the cell Ωij is the boundary cell, we obtain in
each cell a SLAE of the form
Ai,j · Xs+1
i,j
= f s,s+1
i,j
,
(14)
where Xs+1
i,j
= (bs+1
i,j,1, . . . , bs+1
i,j,mb)T . In the versions studied in the present work,
system (14) is overdetermined. The symbolic expressions for the coeﬃcients of
all equations of SLAE (14) were derived on computer in Fortran form by using
symbolic computations with Mathematica. At the obtaining of the ﬁnal form
of the formulas for the coeﬃcients of the equations, it is useful to perform the
simpliﬁcations of the arithmetic expressions of polynomial form to reduce the
number of the arithmetic operations needed for their numerical computation. To
this end, we employed standard functions of the Mathematica system, such as
Simplify[...] and FullSimplify[...] for the simpliﬁcation of complex symbolic
expressions arising at the symbolic stages of the construction of the formulae of
the method. Their application enabled a two-three-fold reduction of the length
of polynomial expressions.
For the numerical solution of the SLAE of the discrete problem a process was
applied which may be called conventionally the Gauss–Seidel iteration scheme.
One global (s + 1)th iteration meant that all the cells were considered sequen-
tially in the computational region Ω. In each cell, SLAE (14) was solved by
the orthogonal method (of Givens or Householder), and the values known at the
solution construction at the (s+1)th iteration were taken in the right-hand sides
of Eqs. (10)–(12) as the u−and q−in a given cell.
2.2
Preconditioners for the CLR Method
It is necessary to solve in each cell Ωij the SLAE of the form (14). Let us omit
in (14) the superscripts and subscripts for the sake of brevity:
AX = f.
(15)
The condition number of a rectangular matrix A is calculated by the formula
κ(A) =

∥A1 ∥· ∥A−1
1
∥,
(16)
where it is assumed that matrix A1 = AT A is non-singular. In our case, we
have a preconditioner involving the parameters ξ and η. A simple algorithm was
described in [24] for ﬁnding the optimal values ξopt and ηopt in any cell from the
requirement of minimizing the condition number κ(ξ, η). The value κ(ξopt, ηopt)
typically satisﬁed the inequalities 3 < κ(ξopt, ηopt) < 10. It has turned out that
the optimal values ξopt and ηopt depended weakly on the location of a speciﬁc
cell in the spatial grid, at least in the cases of those test and benchmark problems
which were considered in [24]. Some properties of the preconditioner were then
investigated in [24]. In particular, it was shown that a reduction of Nc aﬀects
more signiﬁcantly the value ξopt than the value ηopt.

352
V.P. Shapeev and E.V. Vorozhtsov
2.3
Convergence Acceleration Algorithm Based on Krylov’s
Subspaces
To accelerate the convergence of the iterations used for the approximate solution
construction we have used in all new versions of the CLR method, which are
discussed in the present paper, a new variant of the well-known method [13] based
on Krylov’s subspaces, which was previously presented in detail in [22,26]. We
present in the following a very brief description of the corresponding algorithm.
Let the SLAE have the form X = TX + f, where the vector X is the sought
solution, T is a square matrix, and f is a column vector. Let the matrix T have
a full rank, and let the following iteration process converge: Xn+1 = TXn + f,
n = 0, 1, . . ., in which Xn is the approximation for the solution at the nth
iteration. By the deﬁnition, r n = TX n +f −X n = X n+1 −X n is the residual
of equations X = TX + f, and it is not diﬃcult to obtain the following relation
from the above formulas: r n+1 = T r n. Let us assume that k + 1 iterations
have been made starting from some initial guess X0, that is the quantities X1,
X2, . . ., Xk+1 and r 0, r 1, . . . , r k have been computed. The value Xk+1 is
then reﬁned by the formula X∗k+1 = Xk+1 +Y k+1. One employs the correction
of the form
Y k+1 =
k
i = 1
αi r i
(17)
with indeﬁnite coeﬃcients α1, . . ., αk that are found from the condition of the
minimization of the residual functional Φ(α1, . . . , αk) =∥X ∗k+1 −TX ∗k+1 −
f ∥2
2, which arises at the substitution of X ∗k+1 into the system X = TX + f.
Here ||u||2 is the Euclidean norm of the vector u of dimension N. The reﬁned
vector of the k + 1th approximation X ∗k+1 is used as the initial approximation
for further continuation of the sequence of iterations.
2.4
Convergence Acceleration by Using the Multigrid Algorithm
The main idea of multigrid is the selective damping of the error harmonics
[5,27]. In the CLR method, as in other methods, the number of iterations nec-
essary for reaching the given accuracy of the approximation to the solution
depends on the initial guess. As a technique for obtaining a good initial guess
for the iterations on the ﬁnest grid among the grids used in a multigrid complex
we have applied the prolongation operations along the ascending branch of the
V-cycle — the computations on a sequence of reﬁning grids. The passage from
a coarser grid to a ﬁner grid is made with the aid of the prolongation operators.
Let us illustrate the algorithm of the prolongation operation by the example of
the velocity component u1(y1, y2, b1, . . ., b15). Let h1 = h, where h is the half-
step of the coarse grid, and let h2 = h1/2 be the half-step of the ﬁne grid on
which one must ﬁnd the expansion of function u1 over the basis.
Step 1. Let X1 and X2 be the global coordinates of the coarse grid cell center.
We make the following substitutions into the polynomial expression for u1:
yl = (xl −Xl)/h1,
l = 1, 2.
(18)

The Method of Collocations and Least Residuals
353
As a result, we obtain the polynomial
U1(x1, x2, b1, . . . , b15) = u1

x1−X1
h1
, x2−X2
h1
, b1, . . . , b15

.
(19)
Step 2. Let ( ˜X1, ˜X2) be the global coordinates of the center of any of the four
cells of the ﬁne grid, which lie in the coarse grid cell. We make the substitution in
(19) xl = ˜Xl+˜yl·h2, l = 1, 2. As a result, we obtain the second-degree polynomial
˜U1 = P(˜y1, ˜y2,˜b1, . . . ,˜b15) in variables ˜y1, ˜y2 with coeﬃcients ˜b1, . . . ,˜b15. After
the collection of terms of similar structure it turns out that the coordinates
X1, X2 and ˜X1, ˜X2 enter ˜bl (l = 1, . . . , 15) only in the form of combinations
δxl = (Xl −˜Xl)/h1. According to (18), the quantity −δxl = ( ˜Xl −Xl)/h1 is the
local coordinate of the ﬁne grid cell center in the coarse grid cell.
Let us present the expressions for coeﬃcients ˜bj
(j = 1, . . . , 15) of the
solution representation in a ﬁne grid cell with the half-step h2 in terms of the
coeﬃcients b1, . . . , b15 of the solution representation in a cell with the half-step
h1 = 2h2:
˜b1 = b1 −δx1(b2 −b4δx1) −δx2(b3 + 2b5δx1 −b6δx2), ˜b2 = σ1(T1 + b5δx2),
˜b3 = σ1[b3 + 2(b5δx1 −b6δx2)], ˜b4 = σ2b4,
˜b5 = σ2b5, ˜b6 = σ2b6,
˜b7 = b7 −δx1(b8 −b9δx1) + δx2T1, ˜b8 = σ1(b8 −2b9δx1 + 2b4δx2),
˜b9 = σ2b9, ˜b10 = b10 −δx1T2 −δx2(b12 −b15δx2), ˜b11 = σ1(T2 −b13δx1),
˜b12 = σ1(b12 −b14δx1 −2b15δx2), ˜b13 = σ2b13, ˜b14 = σ2b14, ˜b15 = σ2b15,
where σ1 = h2/h1, σ2 = σ2
1, T1 = b2 −2b4δx1 +b5δx2, T2 = b11 −b13δx1 −b14δx2.
The analytic expressions for coeﬃcients ˜b1, . . . ,˜b15 were found eﬃciently with the
aid of the Mathematica functions Expand[...], Coefficient[...], Simplify[...].
To reduce the length of obtained coeﬃcients we have applied a number of trans-
formation rules as well as the Mathematica function FullSimplify[...]. As a
result, the length of the ﬁnal expressions for ˜b1, . . . ,˜b15 proved to be three times
shorter than the length of the original expressions. Note that the above expres-
sions for ˜b1, . . . ,˜b9 coincide with the expressions which were presented in [24,26]
for the case of mb = 12 in (8).
3
The Use of the Integral Form of Collocation Equations
The “diﬀerential” version of the CLR method, in which the collocation equations
(7) were obtained from the diﬀerential equations (5), was described in Sect. 2. By
analogy with the LSFEM [11,12], one can use instead of collocation equations
(7) their integral counterparts, which are obtained by integrating equations (5)
over several subregions, and the consideration of collocation equations at several
user-deﬁned collocation points is replaced with the consideration of collocation
relations, which account for the inﬂuence of the entire area of each cell Ωij of
the spatial computational grid.
Furthermore, it was shown in [26] that the inclusion of the approximation
(13) of the integral condition (3) in the overdetermined SLAE (14) instead of

354
V.P. Shapeev and E.V. Vorozhtsov
specifying the pressure at a single point speeds up considerably the iteration
process convergence.
Let us at ﬁrst introduce a uniform computational grid in each cell, which
subdivides each cell face into Nsub intervals, where Nsub > 1 is the user-speciﬁed
number of cells along each local coordinate yk, k = 1, 2. The lines of this grid
subdivide the Ωij cell into Nc = N 2
sub subcells Ω(l)
ij , l = 1, . . . , Nc. The Nsub
value must be speciﬁed in such a way that the quantity N 2
sub be comparable
with the number of unknown coeﬃcients mb in (8) or be higher than mb. After
that, the integration of Eq. (7) is carried out in each cell Ω(l)
ij :
ξ
 
Ω(l)
ij

Δus+1
m
−(Re · h)(us
1us+1
m,y1 + us+1
1
us
m,y1 + us
2us+1
m,y2 + us+1
2
us
m,y2
+ qs+1
ym )
	
dy1dy2 = ξ
 
Ω(l)
ij
Fmdy1dy2, m = 1, 2; l = 1, . . . , Nc.
(20)
It is to be noted that the integration of the left-hand side of (20) can be per-
formed in the analytic form because the integrand involves only the polyno-
mial expressions according to Table 1, and the use of the Mathematica function
Integrate[...] proves to be eﬃcient here.
It turns out that there are in the obtained 2 · N 2
sub collocation equations
many common subexpressions. To reduce the CPU time needed for the numerical
computation of the entries of the matrix Aij in SLAE (14) it is reasonable to
perform the common subexpression elimination (CSE) in (20). The basic idea
is here to evaluate common subexpressions only once and put the results in
temporary variables [6]. We have implemented an interactive CSE technique. As
a result, 15 temporary variables have been introduced, and most of them are the
functions of other temporary variables. This has resulted in a considerable (by
factors from 2 to 5) reduction of the lengths of the expressions for the entries of
the matrix Aij.
The built-in Mathematica function ReplaceRepeated(//.) repeatedly per-
forms replacements until the expression no longer changes. But our practice
shows that this function is not reliable, it performs not all replacements of com-
mon subexpressions with temporary variables. For illustration, let us consider
the following very simple example: let us take the expression expr = 6*y1L -
6*y1R. The transformation rule y1L - y1R -> dy1 enables the replacement of
the subexpression y1L - y1R with the temporary variable dy1. However, the
application of the command expr = expr//. {y1L - y1R -> dy1} leaves the
original expression unchanged. This situation can be rectiﬁed owing to the avail-
ability in CAS Mathematica of many other built-in functions performing elemen-
tary transformations, and their combination enables one, as a rule, to obtain the
needed result. In the speciﬁc example considered above, the application of the
built-in function Factor[...] has enabled us to obtain the needed result: expr
= Factor[expr]/.y1L - y1R -> dy1 yields the desired result expr = 6*dy1.
At the symbolic implementation of the stages of deriving the work formu-
las of the versions of the CLR method, which are described in Sects. 2 and 3,

The Method of Collocations and Least Residuals
355
we have used the following built-in Mathematica functions for symbolic com-
putation and manipulation: AppendTo[...], Coefficient[...], D[...], Det[...],
Expand[...],
Factor[...],
FortranForm[...],
FullSimplify[...],
Input
Form[...], Integrate[...], Inverse[...], Length[...], Norm[...], Replace
Repeated(//.),
Sum[...],
Table[...],
ToExpression[...],
ToString[...],
Transpose[...].
It is to be noted that the analogs of the above functions are available also in
such well-known CASs as Maple, REDUCE, and in a number of other general-
purpose CASs. Thus, the researcher wishing to implement the above-described
symbolic-numeric methods for solving the Navier–Stokes equations has a pos-
sibility to choose a speciﬁc CAS; this choice, in turn, depends on his personal
experience in the matter of using one or other CAS. Although each of the CASs
mentioned here has its individual advantages depending on a problem to be
solved, the capabilities which Mathematica possesses were suﬃcient for doing
with its aid the job presented in the given paper.
4
Results of Numerical Experiments
Consider the following exact solution of the Navier–Stokes equations (1) [4]:
u1 =
−2(1 + x1)
(1 + x1)2 + (1 + x2)2 , u2 =
2(1 + x1)
(1 + x1)2 + (1 + x2)2 ,
p = −
2
(1 + x1)2 + (1 + x2)2 ,
0 ≤x1, x2 ≤1.
(21)
Note that the functions u1(x1, x2) and u2(x1, x2) describe the divergence-free
velocity ﬁeld. Furthermore,
 1
0
 1
0
p dx1dx2 = 4G −π ln 2 −2i

Li2

−i
2

−Li2
 i
2

≈−0.46261314677281549872,
where i = √−1, G is the Catalan’s constant [28], G ≈0.91596559417721901505,
Li2(z) is the polylogarithmic function. To ensure the satisfaction of Eq. (3) with
an error not exceeding the error of machine computations, the pressure p in (3)
was replaced with the quantity ¯p = p + 0.4626131467728155.
The root-mean-square solution errors were calculated as
Err(u(h)) =
 1
2M 2
M

i=1
M

j=1
2

ν=1
(uν,i,j−uex
ν,i,j)2 1
2 ,
Err(p(h)) =
 1
M 2
M

i=1
M

j=1
(pi,j−pex
i,j)2 1
2,
where M is the number of cells along each coordinate direction, uex
i,j and pex
i,j
are the velocity vector and the pressure according to the exact solution (21).

356
V.P. Shapeev and E.V. Vorozhtsov
The quantities ui,j and pi,j denote the numerical solution obtained by the CLR
method described above. The convergence orders νu and νp are computed by the
well-known formulas [21,23]. Let bs
i,j,l, s = 0, 1, . . . be the values of the coeﬃcients
bi,j,l in (8) at the sth iteration. The following condition was used for termination
of the iterations: δbs+1 < ε, where δbs+1 = maxi,j(max1≤l≤mb |bs+1
i,j,l −bs
i,j,l|), and
ε < h2 is a small positive quantity. We will call the quantity δbs+1 the pseudo-
error of the approximate solution.
Along with the criterion δbs+1 < ε, we have also applied the following crite-
rion for the termination of iterations:
δun+1 =∥un+1 −un ∥< ε2,
where ∥· ∥is the Euclidean norm of the vector, ε2 is a user-speciﬁed small
positive quantity.
In the work [26], we have introduced the deﬁnition of the overdetermination or
underdetermination ratio of system (15) as the quantity χ(A) = mr/mc, where
mr and mc are, respectively, the number of rows and the number of columns of
the matrix A. We have investigated the inﬂuence of the quantity χ(A) on the
condition number (16) when the “diﬀerential” CLR method is employed, and
mb = 12 in (8). This study was carried out by using the Dirichlet boundary
conditions corresponding to the analytic solution (21).
For the sake of brevity, we omit the obtained tabular data and only enumerate
the conclusions summarizing the above study.
1◦. The convergence rate of the iteration process in the “diﬀerential” CLR
method depends signiﬁcantly on the condition numbers of the SLAEs to be
solved in each cell.
2◦. In cases where the matrix A includes only the rows corresponding to the
collocation equations and one row corresponding to approximation (13) of the
pressure integral, a very large condition number of the order of 105 is obtained
depending on the number of collocation points, that is the matrix A is ill-
conditioned. The inclusion in the matrix A of the rows corresponding to matching
conditions results in a reduction of the condition number by three – ﬁve decimal
orders depending on the number of grid cells and the number of collocation and
matching points. It is this considerable reduction of the condition number which
ensures the performance of the CLR method at the solution of boundary-value
problems for linear and nonlinear partial diﬀerential equations.
3◦. The condition number in boundary cells is always higher than in internal
cells.
4◦. At Nc = 4 and Nmat = 1, the convergence of the CLR method slows down
signiﬁcantly in comparison with the case of Nc = 8 and Nmat = 2.
5◦. The data obtained present a practical proof of the fact that at the use of an
overdetermined system in the approximate problem (χ(A) > 1), the correspond-
ing SLAE of the approximate problem proves to be better conditioned than in
the case when the SLAE is not overdetermined. This is an important advantage
of the method of collocations and least squares (and the CLR method) over

The Method of Collocations and Least Residuals
357
the collocation method, which does not use the overdetermined SLAE in the
approximate problem.
To study the convergence and accuracy properties of the above-presented
versions of the CLR method numerous numerical experiments were performed
with the use of the analytic solution (21). The following names were used for
diﬀerent versions of the CLR method: CLRD12 is the “diﬀerential” CLR method
of [24,26] employing twelve basis vectors, that is mb = 12 in (8); CLRD15 is the
“diﬀerential” CLR method with mb = 15 in (8); and, ﬁnally, CLRI15 is the
“integral” CLR method with mb = 15 in (8).
Tables 2, 3, and 4 present the results of numerical experiments, in which
only two of the above-described techniques for convergence acceleration were
used: the two-parameter preconditioner and the Krylov subspace method. In
these computations, it was assumed that the Reynolds number Re = 1000 and
L = 1 in (4). These computations were done with Nc = 8 in the case of methods
CLRD12 and CLRD15 and with Nc = N 2
sub = 16 in the case of method CLRI15.
The satisfaction of the inequality δbn < 10−9 was the criterion for termination of
the computations by the versions of the CLR method. Comparing Table 2 with
Tables 3 and 4 one can see that the 25% increase in the number of basis vector
functions (from 12 to 15) reduces the error in the pressure and velocity in the
numerical solution by the factors ranging from two to three orders of magnitude.
Figure 1 shows the solution obtained by the method CLRD15 by symbols △
(v1), ◦(v2), and ∇(p); the curves of the exact solution are depicted by the
Table
2. The errors Err(un),
Err(pn)
and
their
convergence
orders νu and νp on a sequence of
grids, Re = 1000, L = 1, Nc = 8.
Method CLRD12
M Err(un)
Err(pn)
νu
νp
10 1.309e−02 9.754e−03
20 5.484e−03 3.664e−03 1.26 1.41
40 1.869e−03 1.135e−03 1.55 1.69
80 4.938e−04 2.870e−04 1.92 1.98
Table
3.
The
errors
Err(un),
Err(pn) and their convergence orders
νu and νp on a sequence of grids,
Re = 1000, L = 1, Nc = 8. Method
CLRD15
M Err(un)
Err(pn)
νu
νp
10 2.947e−05 4.731e−05
20 6.971e−06 2.168e−05 2.08 1.13
40 2.289e−06 1.046e−05 1.61 1.05
80 9.816e−07 4.950e−06 1.21 1.09
Table 4. The errors Err(un), Err(pn) and their convergence orders νu and νp on a
sequence of grids, Re = 1000, L = 1, Nc = 8. Method CLRI15
M
Err(un)
Err(pn)
νu
νp
10 2.910e−05 5.437e−05
20 6.807e−06 2.249e−05 2.10 1.27
40 2.325e−06 1.076e−05 1.55 1.06
80 9.877e−07 4.893e−06 1.24 1.14

358
V.P. Shapeev and E.V. Vorozhtsov
x1
0.2
0.4
0.6
0.8
1.0
0.5
0.0
0.5
Fig. 1. Comparison of the approxi-
mate and exact solution proﬁles at
x2 = L/2. The 40 × 40 grid
x2
v1
0
0.5
1.
0.2
0.4
0.6
0.8
1.0
Fig. 2. Solution of the benchmark problem
by the method CLRI15 at Re = 100 on
the 40 × 40 grid: the proﬁle of the velocity
component v1 along the centerline x1 = 0.5
(solid line); (◦◦◦) Ghia et al. [9]
solid, dashed, and dash-dot lines for v1, v2, and p, respectively. One can see here
a good agreement between the numerical results and the analytic solution.
In the 2D driven cavity problem, the computational region is the cavity,
which is a square (4) with side L = 1, the coordinate origin lies in its left lower
corner. The upper lid of the cavity moves with unit velocity in dimensionless
variables in the positive direction of the Ox1 axis. The other sides of cavity (4)
are at rest. The no-slip conditions are speciﬁed on all sides: v1 = 1, v2 = 0 at
x2 = L and vm = 0, m = 1, 2 on the remaining sides.
The lid-driven cavity ﬂow has the singularities in the region upper corners.
Their inﬂuence on the numerical solution accuracy enhances with increasing
Reynolds number. Therefore, at high Reynolds numbers, it is necessary to apply
adaptive grids for obtaining a more accurate solution: the grids with ﬁner cells
in the neighborhood of singularities. Only the uniform grids were applied here.
Table 5. The error δu1 obtained at the use of diﬀerent versions of the CLR method,
Re = 100
Method
ξ
η
κ(ξ, η) Kmgr k Nit
CPU time, s δu1
CLRD12 2.0
3.5
8.536
4
8 2336
53.59
1.726e−02
CLRD15 2.0
3.5
10.590
4
8 2540
99.59
1.150e−02
CLRI15
0.25 1.75
6.774
4
9 1700 110.38
8.521e−03
We have compared the accuracy of diﬀerent versions of the CLR method,
which were presented in the foregoing sections, in the case when the Reynolds
number Re = 100. For our comparisons, we have used the numerical results from
[9]. Let us introduce the error δu1 = maxj |u1,Ghia(0.5, x2j) −u1,CLR(0.5, x2j)|,
where the coordinates x2j were taken from [9]. In all computations whose results

The Method of Collocations and Least Residuals
359
are presented in Table 5, we have used the combination of all three acceleration
techniques, which were presented brieﬂy above in Subsects. 2.2, 2.3, and 2.4. The
grids, which were used in the multigrid complex, were as follows: 5 × 5, 10 ×
10, 20 × 20, and 40 × 40 grids; Kmgr is the number of sequentially used grids in
the multigrid complex. Nit is the number of iterations, which are necessary to
satisfy the inequality δbn < 10−9. The value k is the number of residuals used in
the Krylov’s method (see Eq. (17)). The optimal values of the parameters ξ and
η entering the two-parameter preconditioner were found using the algorithm
of [24]. The condition number κ(ξ, η) was computed for all three methods by
formula (16) in the cell with indices (20, 20) on the 40×40 grid after the execution
of 200 iterations on this grid. One can see from Table 5 that the method CLRI15
ensures the best accuracy among three considered versions of the CLR method
(see also Fig. 2). In addition, it ensures the least condition number κ. This is the
important property of the CLRI15 method, which extends the capabilities of the
CLR method at the solution of ill-conditioned problems.
5
Conclusions
New versions of the CLR method have been presented. A large amount of sym-
bolic computations, which arose at the derivation of the basic formulae of the
new versions of the method, was done eﬃciently with Mathematica. It is very
important that the application of CAS has facilitated greatly this work, reduced
at all its stages the probability of errors usually introduced by the mathemati-
cian at the development of a new algorithm and also reduced the time needed
for the development of new Fortran programs implementing the numerical stages
of the CLR method. It is shown by examples of numerous computations that
the new proposed versions of the CLR method produce more accurate numerical
solutions than the previous “diﬀerential” version CLRD12 of the CLR method.
References
1. Amodio, P., Blinkov, Y., Gerdt, V., La Scala, R.: On consistency of ﬁnite diﬀerence
approximations to the Navier-Stokes equations. In: Gerdt, V.P., Koepf, W., Mayr,
E.W., Vorozhtsov, E.V. (eds.) CASC 2013. LNCS, vol. 8136, pp. 46–60. Springer,
Cham (2013). doi:10.1007/978-3-319-02297-0 4
2. Bailly, O., Buchou, C., Floch, A., Sainsaulieu, L.: Simulation of the intake and
compression strokes of a motored 4-valve SI engine with a ﬁnite element code. Oil
Gas Sci. Technol. 54, 161–168 (1999)
3. Botella, O., Peyret, R.: Benchmark spectral results on the lid-driven cavity ﬂow.
Comput. Fluids 27, 421–433 (1998)
4. Chiu, P.H., Sheu, T.W.H., Lin, R.K.: An eﬀective explicit pressure gradient scheme
implemented in the two-level non-staggered grids for incompressible Navier-Stokes
equations. J. Comput. Phys. 227, 4018–4037 (2008)
5. Fedorenko, R.P.: The speed of convergence of one iterative process. USSR Comput.
Math. Math. Phys. 4(3), 227–235 (1964)

360
V.P. Shapeev and E.V. Vorozhtsov
6. Fritzson, P., Engelson, V., Sheshadri, K.: MathCode: a system for C++ or Fortran
code generation from Mathematica. Math. J. 10, 740–777 (2008)
7. Ganzha, V.G., Mazurik, S.I., Shapeev, V.P.: Symbolic manipulations on a com-
puter and their application to generation and investigation of diﬀerence schemes.
In: Caviness, B.F. (ed.) EUROCAL 1985. LNCS, vol. 204, pp. 335–347. Springer,
Heidelberg (1985). doi:10.1007/3-540-15984-3 290
8. Gerdt, V.P., Blinkov, Y.A.: Involution and diﬀerence schemes for the Navier–
Stokes Equations. In: Gerdt, V.P., Mayr, E.W., Vorozhtsov, E.V. (eds.) CASC
2009. LNCS, vol. 5743, pp. 94–105. Springer, Heidelberg (2009). doi:10.1007/
978-3-642-04103-7 10
9. Ghia, U., Ghia, K.N., Shin, C.T.: High-Re solutions for incompressible ﬂow using
the Navier-Stokes equations and a multigrid method. J. Comput. Phys. 48, 387–
411 (1982)
10. Isaev, V.I., Shapeev, V.P.: High-accuracy versions of the collocations and least
squares method for the numerical solution of the Navier-Stokes equations. Comput.
Math. Math. Phys. 50, 1670–1681 (2010)
11. Jiang, B., Lin, T.L., Povinelli, L.A.: Large-scale computation of incompressible
viscous ﬂow by least-squares ﬁnite element method. Comput. Meth. Appl. Mech.
Eng. 114(3–4), 213–231 (1994)
12. Jiang, B.N.: The Least-Squares Finite Element Method: Theory and Applications
in Computational Fluid Dynamics and Electromagnetics. Springer, Berlin (1998).
doi:10.1007/978-3-662-03740-9
13. Krylov, A.N.: On the numerical solution of the equation, which determines in
technological questions the frequencies of small oscillations of material systems.
Izv. AN SSSR, Otd. matem. i estestv. nauk 4, 491–539 (1931). (in Russian)
14. Li, K., Li, Q.: Three-dimensional gravity-jitter induced melt ﬂow and solidiﬁcation
in magnetic ﬁelds. J. Thermophys. Heat Transf. 17(4), 498–508 (2003)
15. Plyasunova, A.V., Sleptsov, A.G.: Collocation-grid method of solving the nonlinear
parabolic equations on moving grids. Modelirovanie v mekhanike 18(4), 116–137
(1987)
16. Semin, L., Shapeev, V.: Constructing the numerical method for Navier —
Stokes equations using computer algebra system. In: Ganzha, V.G., Mayr, E.W.,
Vorozhtsov, E.V. (eds.) CASC 2005. LNCS, vol. 3718, pp. 367–378. Springer, Hei-
delberg (2005). doi:10.1007/11555964 31
17. Semin, L.G., Sleptsov, A.G., Shapeev, V.P.: Collocation and least -squares method
for Stokes equations. Comput. Technol. 1(2), 90–98 (1996). (in Russian)
18. Shapeev, A.V., Lin, P.: An asymptotic ﬁtting ﬁnite element method with exponen-
tial mesh reﬁnement for accurate computation of corner eddies in viscous ﬂows.
SIAM J. Sci. Comput. 31, 1874–1900 (2009)
19. Shapeev, V.: Collocation and least residuals method and its applications. EPJ Web
Conf. 108, 01009 (2016). doi:10.1051/epjconf/201610801009
20. Shapeev, V.P., Isaev, V.I., Idimeshev, S.V.: The collocations and least squares
method: application to numerical solution of the Navier-Stokes equations. In: Eber-
hardsteiner, J., B¨ohm, H.J., Rammerstorfer, F.G. (eds.) CD-ROM Proceedings of
the 6th ECCOMAS, September 2012. Vienna University of Technology (2012).
ISBN: 978-3-9502481-9-7
21. Shapeev, V.P., Vorozhtsov, E.V.: CAS application to the construction of the
collocations and least residuals method for the solution of 3D Navier–Stokes
equations. In: Gerdt, V.P., Koepf, W., Mayr, E.W., Vorozhtsov, E.V. (eds.)
CASC 2013. LNCS, vol. 8136, pp. 381–392. Springer, Cham (2013). doi:10.1007/
978-3-319-02297-0 31

The Method of Collocations and Least Residuals
361
22. Shapeev, V.P., Vorozhtsov, E.V., Isaev, V.I., Idimeshev, S.V.: The method of collo-
cations and least residuals for three-dimensional Navier-Stokes equations. Vychis-
lit. metody i programmirovanie 14, 306–322 (2013). (in Russian)
23. Shapeev, V.P., Vorozhtsov, E.V.: Symbolic-numeric implementation of the method
of collocations and least squares for 3D Navier–Stokes equations. In: Gerdt, V.P.,
Koepf, W., Mayr, E.W., Vorozhtsov, E.V. (eds.) CASC 2012. LNCS, vol. 7442, pp.
321–333. Springer, Heidelberg (2012). doi:10.1007/978-3-642-32973-9 27
24. Shapeev, V.P., Vorozhtsov, E.V.: Symbolic-numerical optimization and realiza-
tion of the method of collocations and least residuals for solving the Navier–
Stokes equations. In: Gerdt, V.P., Koepf, W., Seiler, W.M., Vorozhtsov, E.V. (eds.)
CASC 2016. LNCS, vol. 9890, pp. 473–488. Springer, Cham (2016). doi:10.1007/
978-3-319-45641-6 30
25. Valiullin, A.N., Ganzha, V.G., Meleshko, S.V., Murzin, F.A., Shapeev, V.P.,
Yanenko, N.N.: Application of Symbolic Manipulations on a Computer for Genera-
tion and Analysis of Diﬀerence Schemes. Prepr. Inst. Theor. Appl. Mech. Siberian
Branch of the USSR Acad. Sci., Novosibirsk No. 7 (1981). (in Russian)
26. Vorozhtsov, E.V., Shapeev, V.P.: On combining the techniques for convergence
acceleration of iteration processes during the numerical solution of Navier-Stokes
equations. Vychislit. metody i programmirovanie 18, 80–102 (2017). (in Russian)
27. Wesseling, P.: An Introduction to Multigrid Methods. Wiley, Chichester (1992)
28. Wolfram, S.: The Mathematica Book, 5th edn. Wolfram Media Inc., Champaign
(2003)

A Special Homotopy Continuation Method
for a Class of Polynomial Systems
Yu Wang1, Wenyuan Wu2(B), and Bican Xia1
1 LMAM & School of Mathematical Sciences, Peking University, Beijing, China
yuxiaowang@pku.edu.cn, xbc@math.pku.edu.cn
2 Chongqing Institute of Green and Intelligent Technology Chinese Academy of
Sciences, Chongqing, China
wuwenyuan@cigit.ac.cn
Abstract. A special homotopy continuation method, as a combination
of the polyhedral homotopy and the linear product homotopy, is proposed
for computing all the isolated solutions to a special class of polynomial
systems. The root number bound of this method is between the total
degree bound and the mixed volume bound and can be easily computed.
The new algorithm has been implemented as a program called LPH using
C++. Our experiments show its eﬃciency compared to the polyhedral
or other homotopies on such systems. As an application, the algorithm
can be used to ﬁnd witness points on each connected component of a
real variety.
1
Introduction
In many applications in science, engineering, and economics, solving systems
of polynomial equations has been a subject of great importance. The homotopy
continuation method was developed in 1970s [1,2] and has been greatly expanded
and developed by many researchers (see for example [3–7]). Nowadays, homotopy
continuation method has become one of the most reliable and eﬃcient classes
of numerical methods for ﬁnding the isolated solutions to a polynomial system
and the so-called numerical algebraic geometry based on homotopy continuation
method has been a blossoming area. There are many famous software packages
implementing diﬀerent homotopy methods, including Bertini et al. [8], Hom4PS-
2.0 [9], HOMPACK [10], PHCpack [11], etc.
Classical homotopy methods compute solutions in complex spaces, while in
applications, it is quite common that only real solutions have physical mean-
ing. Computing real roots of an algebraic system is a diﬃcult and fundamental
problem in real algebraic geometry. In the ﬁeld of symbolic computation, there
are some famous algorithms dealing with this problem. The cylindrical algebraic
decomposition algorithm [12] is the ﬁrst complete algorithm which has been
implemented and used successfully to solve many real problems. However, in
The work is partly supported by the projects NSFC Grants 11471307, 11290141,
11271034, 61532019 and CAS Grant QYZDB-SSW-SYS026.
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 362–376, 2017.
DOI: 10.1007/978-3-319-66320-3 26

A Special Homotopy Continuation Method
363
the worst case, its complexity is doubly exponential in the number of variables.
Based on the ideas of Seidenberg [13] and others, some algorithms for computing
at least one point on each connected component of an real algebraic set were
proposed through developing the formulation of critical points and the notion
of polar varieties, see [14–17] and references therein. The idea behind is study-
ing an objective function (or map) that reaches at least one local extremum on
each connected component of a real algebraic set. For example, the function of
square of the Euclidean distance to a randomly chosen point was used in [18,19].
On the other hand, some homotopy based algorithms for real solving have been
proposed in [20–25]. For example, in [24], a numerical homotopy method to ﬁnd
the extremum of Euclidean distance to a point as the objective function was
presented. More recently, the Euclidean distance to a plane was proposed as a
linear objective function in [26].
Such critical point/plane approaches introduced above lead to a special class
of polynomial systems. The main contribution of this paper is to give a special
homotopy method for solving the system of that type eﬃciently by combining
the polyhedral homotopy and the linear product homotopy. The root number
bound of this method is not only easy to compute but also much smaller than
the total degree bound and close to the BKK bound [27] when the polynomials
deﬁning the algebraic set are not very sparse. This key observation enables us
to design an eﬃcient homotopy procedure to obtain critical points numerically.
The ideas and algorithms we proposed in this article avoid a great number of
divergent paths to track compared with the total degree homotopy and save the
great time cost for mixed volume computation compared with the polyhedral
homotopy. The new algorithm has been implemented as a program called LPH
using C++. Our experiments show its eﬃciency compared to the polyhedral or
other homotopies on such systems.
The rest of this paper is organized as follows. Section 2 describes some pre-
liminary concepts and results. Section 3 introduces a special type of polynomial
systems we are considering. The new homotopy for these polynomial systems is
also presented. It naturally leads to an algorithm which is described in Sect. 4.
Based on this algorithm, in Sect. 5, we present a method to ﬁnd real witness
points of positive dimensional varieties, together with an illustrative example.
The experimental performance of the software package LPH, which is an imple-
mentation of the method in C++, is given in Sect. 6.
2
Preliminary
2.1
Algebraic Sets and Genericity
For a polynomial system f : Cn →Ck, let V (f) = {x ∈Cn|f(x) = 0} and
VIR(f) = V (f) ∩IRn = {x ∈IRn|f(x) = 0} be the set of complex solutions and
the set of real solutions of f(x) = 0, respectively. A set X ⊂Cn is called an
algebraic set if X = V (g), for some polynomial system g.
An algebraic set X is irreducible if there does not exist a decomposition
X1 ∪X2 = X with X1, X2 ̸= X of X as a union of two strict algebraic subsets.

364
Y. Wang et al.
An algebraic set is reducible, if there exist such a decomposition. For example,
the algebraic set V (xy) ⊂C2 is consisting of the two coordinate axes, and is
obviously the union of V (x) and V (y), hence reducible.
For an irreducible algebraic set X, the subset of smooth (or manifold) points
Xreg is dense, open and path connected (up to the Zariski topology) in X. The
dimension of an irreducible algebraic set X is the dimension of Xreg as a complex
manifold.
Let Jf(x) denote the n × k Jacobian matrix of f evaluated at x. By the
Implicit Function Theorem, for an irreducible algebraic set X deﬁned by a
reduced system f, x ∈Xreg ⇔rank(Jf(x)) = n −dim X. When n = k, the
system f is said to be a square system. In this case, a point x ∈V (f) is nonsin-
gular if det(Jf(x)) ̸= 0, and singular otherwise.
On irreducible algebraic sets, we can deﬁne the notion of genericity, adapted
from [6].
Deﬁnition 1. Let X be an irreducible algebraic set. Property P holds generically
on X, if the set of points in X that do not satisfy property P are contained in
a proper algebraic subset Y of X. The points in Y are called nongeneric points,
and their complements X\Y are called generic points.
Remark 1. From the deﬁnition, one sees that the notion of generic is only mean-
ingful in the context of property P in question.
Every algebraic set X has a (uniquely up to reordering) expression X =
X1 ∪. . . ∪Xr with Xi irreducible and Xi ̸⊂Xj for i ̸= j. And Xi are the
irreducible components of X. The dimension of an algebraic set is deﬁned to be
the maximum dimension of its irreducible components. An algebraic set is said
to be pure-dimensional if each of its components has the same dimension.
2.2
Trackable Paths
In homotopy continuation methods, the notion of path tracking is fundamental,
the following deﬁnition of trackable solution path is adapted from [28].
Deﬁnition 2. Let H(x, t) : Cn × C →Cn be polynomial in x and complex
analytic in t, and let x∗be nonsingular isolated solution of H(x, 0) = 0, we
say x∗is trackable for t ∈[0, 1) from 0 to 1 using H(x, t) if there is a smooth
map ξx∗: [0, 1) →Cn such that ξx∗(0) = x∗, and for t ∈[0, 1), ξx∗(t) is a
nonsingular isolated solution of H(x, t) = 0. The solution path started at x∗is
said to be convergent if lim
t→1 ξx∗(t) ∈Cn, and the limit is called the endpoint of
the path.
2.3
Witness Set and Degree of an Algebraic Set
Let X ⊂Cn be a pure i-dimensional algebraic set, given a generic co-dimension
i aﬃne linear subspace L ⊂Cn, then W = L ∩X consists of a well-deﬁned

A Special Homotopy Continuation Method
365
number d of points lying in Xreg. The number d is called the degree of X and
denoted by deg(X). We refer to W as a set of witness points of X, and call L
the associated (n −i)-slicing plane, or slicing plane for short [6].
It will be convenient to use the notations adapted from ([6], Chap. 8), when
we prove the theorems in Sect. 3.
1. Let ⟨e1, . . . , en⟩be the n dimensional vector space having basis elements
e1, . . . , en with complex coeﬃcients. That is, a point in this space may be
written as
n
i=1
ciei, with ci ∈C for i = 1, . . . , n. Note that we have not
speciﬁed anything about the basis elements, it could be individual variables,
monomials, or polynomials.
2. Let {p1, . . . , pn} ⊗{q1, . . . , qm} be the product of two sets, that is, the set
{pi · qj|i = 1, . . . , n; j = 1, . . . , m}. In Sect. 3 we take this product as the image
inside the ring of polynomials; that is, x ⊗y = xy is just the product of two
polynomials.
3. Deﬁne P ×Q = {pq|p ∈P, q ∈Q}. Accordingly, we have ⟨P⟩×⟨Q⟩⊂⟨P ⊗Q⟩.
4. For repeated products, we use the shorthand notations P (2) = P ⊗P,⟨P⟩(2) =
⟨P⟩× ⟨P⟩, and similar for three or more products.
5. For a square polynomial system P, we denote by MV (P) the mixed volume
of the system P.
2.4
Critical Points
Let X ⊂Cn be an algebraic set deﬁned by a reduced polynomial system f =
{f1, . . . , fk}, and objective function Φ is polynomial function restricted to X.
Deﬁnition 3. A point x ∈X is a critical point of Φ if and only if x ∈Xreg
and rank[∇Φ(x)T , Jf(x)] = rank[∇Φ(x)T , ∇f T
1 , . . . , ∇f T
k ] ⩽k, where ∇Φ(x) is
the gradient vector of Φ evaluated at x.
Let Y denote the zero dimensional critical sets of Φ. One way to compute the
critical points is to introduce auxiliary unknowns and consider a zero dimensional
variety ˆY and then project ˆY onto Y . We use Lagrange Multipliers to deﬁne a
squared system as follows
F(x, λ) :=

f
λ0∇Φ(x)T + λ1∇f T
1 + . . . + λk∇f T
k

(1)
Note that if x∗∈X is a critical point of Φ, then there exist λ∗∈IPk, such that
F(x∗, λ∗) = 0 by the Fritz John condition [29]. In the aﬃne patch where λ0 = 1,
the system F becomes a square system, and its solution (x∗, λ∗) projects to the
critical point x∗. We will use system (1) in Sect. 5 with an objective function Φ
deﬁned by a linear function, and consider the aﬃne patch where λ0 = 1, to ﬁnd
at least one point on each component of VIR(f).

366
Y. Wang et al.
3
Main Idea
In this section, we give a description of our idea. First we introduce a family of
polynomial equations that we will be considering. In fact, such type of polynomial
systems appears naturally when using the Method of Lagrange Multipliers in
mathematical optimization, if the constraints are algebraic equations.
Thus, we consider the following class of polynomial systems:
F(x, λ) =

f
J · λ −β
(2)
where
1. f = {f1, . . . , fk} are polynomials in C [x1, . . . , xn], and V (f1, . . . , fk) is a pure
n −k dimension algebraic set in Cn.
2. J =
⎛
⎜
⎝
g11 · · · g1k
...
...
...
gn1 · · · gnk
⎞
⎟
⎠and gij(1 ⩽i ⩽n, 1 ⩽j ⩽k) are polynomials in
C [x1, . . . , xn] with max
i,j deg(gij) = d.
3. β = (β1, . . . , βn)T is a nonzero constant vector in Cn, λ = (λ1, . . . , λk)T are
unknowns, and n > k ⩾1.
Remark 2. Note that, for any invertible n×n matrix A, F(x, λ) = {f, J · λ −β}
and F ′(x, λ) = {f, A · (J · λ −β)} have the same solutions. It’s easy to know
that there exists an invertible matrix A such that A · β = (0, . . . , 0, 1)T. So
without loss of generality, we may assume that β = (0, . . . , 0, 1)T. Then, J · λ −β
has n −1 equations in

{x1, . . . , xn, 1}d ⊗{λ1, . . . , λk}

and one equation in

{x1, . . . , xn, 1}d ⊗{λ1, . . . , λk, 1}

.
Theorem 1. Let F(x, λ) = {f, J · λ −β} be given as in (2), β = (0, . . . , 0, 1)T,
and G = {f, g} where g = {g1, . . . , gn}. For each i = 1, . . . , n −1, gi =
li1 · · · lidhi ∈⟨x1, . . . , xn, 1⟩d × ⟨λ1, . . . , λk⟩, where li1, ..., lid are linear func-
tions in C[x1, . . . , xn] with randomly chosen coeﬃcients, and hi is a homoge-
neous linear function in C[λ1, . . . , λk] with randomly chosen coeﬃcients. And
gn =
k
i=1
λigni −1, where gni is the (n, i)th entry of J.
Let H : Cn × Ck × C →Cn+k be the homotopy deﬁned by H(x, λ, t) =
G · (1 −t) + F · γ · t where γ is a randomly chosen complex number for Gamma
Trick (see [6], Chap. 7 for details). Then, generically the following items hold,
1. The set S ⊆Cn+k of roots of H(x, λ, 0) = G(x, λ) is ﬁnite and each is a
nonsingular solution of H(x, λ, 0).
2. The number of points in S is equal to the maximum number of isolated solu-
tions of H(x, λ, 0) as coeﬃcients of lij, hi, (i = 1, . . . , n −1, j = 1, . . . , d)
and γ vary over C.

A Special Homotopy Continuation Method
367
3. The solution paths deﬁned by H starting, with t = 0, at the points in S are
trackable.
Proof. As for item 1, since f has k equations only in x, and V (f1, . . . , fk) is a
pure n −k dimension algebraic set in Cn. To solve system G, it needs only n −k
linear functions L in g from diﬀerent gi with i ∈{1, . . . , n −1} to determine x.
{f, L} is a n × n square system, V (f, L) is a ﬁnite witness set for the algebraic
set V (f1, . . . , fk), and each of the points is a nonsingular solution of V (f, L)
(see [6], Chap. 13 for details). And, we ﬁnally determine λ by solving a square
system of linear equations. As for item 2, and item 3, it’s a trivial deduction of
Coeﬃcient-Parameter Continuation [30].
⊓⊔
Remark 3. From the proof of Theorem 1, the number of points of the ﬁnite set
V (f, L) is the degree of V (f), and is independent of the choice of L. Thus, based
on the number of diﬀerent choices of L, and item 2, we can give a root count
bound of system F(x, λ) as in the following theorem, which is similar to the
bound in [31].
Theorem 2. For a system F(x, λ) = {f, J · λ −β} as in (2), the number of
complex roots is bounded by

n −1
n −k

dn−kD
(3)
where D is the degree of V (f).
Due to Theorem 1, its proof and the remarks, we can design an eﬃcient proce-
dure to numerically ﬁnd the isolated solutions of system F(x, λ) = {f, J · λ −β}
in the form of (2). First, we solve a square system {f, L}, where L are n −k
randomly generated linear functions. Then for each group of n −k linear func-
tions L′ chosen in g from diﬀerent gi with i ∈{1, . . . , n −1}, we construct a
linear homotopy from {f, L} to {f, L′}, starting from points of V (f, L), and
solve the square linear equation of λ respectively. Let S be the set that con-
sists of all the pairs of x and λ, i.e. (x, λ). Finally construct a linear homotopy
H(x, λ, t) = G · (1 −t) + F · γ · t starting from points in S, thus the endpoints
of the convergent paths of homotopy H(x, λ, t) are isolated solutions of system
F(x, λ) = {f, J · λ −β}. We put a speciﬁc description of this procedure in the
next section.
4
Algorithm
From Theorem 1, its proof and Remarks 2 and 3, we propose an approach
for computing isolated solutions of system F(x, λ) as described in the end of
last section. For consideration of the sparsity, we use the polyhedral homotopy
method for solutions of the square system {f, L}. Actually, we use polyhedral
homotopy method only once. Now we describe our algorithms.

368
Y. Wang et al.
Algorithm 1. LPH (Linear Product Homotopy)
input : (n + k) × (n + k) square polynomial system
F(x, λ) = {f, J · λ −β} as in (2);
output: ﬁnite subset V (F) of Cn+k
1 Let L = {l1, . . . , ln−k} where li are linear equations with randomly chosen
coeﬃcients in C;
2 Solve system {f, l} by polyhedral homotopy method and denote the
solution set as M;
3 Let F ′(x, λ) = {f, A · (J · λ −β)}, G = {f, g}, A ∈GLn(C) such that
A · β = (0, . . . , 0, 1), g = {g1, . . . , gn}.
gi = li1 · · · lid · hi ∈⟨x1, . . . , xn, 1⟩d × ⟨λ1, . . . , λk⟩for i = 1, . . . , n −1 with
coeﬃcients randomly chosen in C, and gn is the last equation of
A · (J · λ −β);
4 Let C =

I
I = (α1, . . . , αn−1) ∈{0, 1}n−1,
n−1

i=1
αi = n −k

, and Ω = ∅;
5 repeat
6
Pick one vector I = (α1, . . . , αn−1) from C, and C = C\I;
7
Let L′ = ∅;
8
for i from 1 to n −1 do
9
if αi = 1 then
10
pick one linear equation li
′ from {li1, . . . , lid} and
L′ = L′ ∪{li
′}.
11
end
12
end
13
Construct linear homotopy H1(x, t) = {f, L} · (1 −t) + {f, L′} · γ1 · t
starting at points in M. γ1 is a randomly chosen complex number for
gamma trick. Let the set of endpoints of the tracked paths be M ′;
14
Take every point x∗= (x∗
1, . . . , x∗
n) in M ′ into the system G = {f, g}
and resolve λ∗= (λ∗
1, . . . , λ∗
k). Ω = Ω ∪{(x∗, λ∗)}. ;
15 until C = ∅;
16 Construct linear homotopy H2(x, λ, t) = G · (1 −t) + F · γ2 · t starting at
points in Ω, γ2 is a randomly chosen complex number for gamma trick.
Let the set of convergent endpoints of the tracked paths be V (F);
17 return V (F);
Remark 4. #C =

n −1
n −k

, and in Step 5, I = (α1, . . . , αn−1) has exactly n−k
entries αi = 1. When αi = 1, we choose linear equation in gi, and there are
d candidates {li1, . . . , lid} to choose. It adds up to be

n −1
n −k

dn−k diﬀerent
{f, L′}. Each {f, L′} has the same number D = deg(V (f)) of isolated roots as
{f, L}, so the homotopy in Step 13 will have no path divergent. Thus we have

n −1
n −k

dn−kD points in Ω, which is the root bound we mention in Theorem

A Special Homotopy Continuation Method
369
2. Should it so happen that some of the homotopy paths are divergent in Step
16, the method of end games for homotopy should be used [32–35].
5
Real Critical Set
In this section, we will combine the LPH algorithm in Sect. 4 and methods in [26]
to compute a real witness set which has at least one point on each irreducible
component of a real algebraic set, and give an illustrative example.
5.1
Critical Points on a Real Algebraic Set
We make the following assumptions (adapted from [26]). Let f : Cn →Ck be a
polynomial system, and f = (f1, . . . , fk) in IR[x1, . . . , xn] satisfying the so-called
Full Rank Assumption:
1. VIR(f1, . . . , fi) has dimension n −i for i = 1, . . . , k;
2. the ideal I(f1, . . . , fi) is radical for i = 1, . . . , k.
Under these assumptions, (∇f T
1 , . . . , ∇f T
i ) has rank i for a generic point
p ∈V (f1, . . . , fi) for i = 1, . . . , k.
The main problem we consider is ﬁnding at least one real witness point on
each connected component of VIR(f). For this purpose, we choose Φ in Deﬁnition
3 to be a linear function with Φ = x · β + c, where β is a random vector in IRn,
and c is a random real number. Then system (1) becomes
F =

f,
k

i=1
λi∇fi −β

= 0.
(4)
It may happen that there is no critical points of Φ in some connected com-
ponent of VIR(f1, . . . , fk). In that case, we add Φ to f and construct a system
with k + 1 equations
f (1) = {f, x · β + c} .
(5)
Then, recursively, we choose another linear function Φ1, compute the critical
points of Φ1 with respect to V (f (1)); and so on.
We give a concrete deﬁnition of the set of real witness points WIR(f) we are
going to compute (see [26]).
Deﬁnition 4. Let f : Cn →Ck be a polynomial system, k ⩽n, and f =
(f1, . . . , fk) in IR[x1, . . . , xn] satisfying Full Rank Assumption. F and f (1) deﬁned
as in (4) and (5). We deﬁne WIR(f) as follows:
1. WIR(f) = VIR(f) if n = k;
2. WIR(f) = VIR(F) ∪WIR(f (1)) if k < n.

370
Y. Wang et al.
It is obvious from the deﬁnition that we can recursively solve the square
system (4), and apply plane distance critical points formulation of f (1) to ﬁnally
get the set of witness points WIR(f) which contains ﬁnitely many real points on
VIR(f), and there is at least one point on each connected component of VIR(f).
Since the formulation introduces auxiliary unknowns, it increases the size of
the system and leads to computational diﬃculties. For example, when n = 15
and k = 10, the size of system (4) becomes 25, which is challenging for general
homotopy software. Combining the LPH algorithm, Theorems 1 and 2, we have
the following algorithm and an upper bound of number of points in WIR(f), as
in [31].
Algorithm 2. RWS (Real Witness Set)
input : a polynomial system f = (f1, . . . , fk), k ⩽n, which satisﬁes the
full rank assumption;
output: a ﬁnite subset WIR(f) of IRn, which contains at least one point
on each connected component of the real algebraic set VIR(f)
1 Let WIR(f) = ∅;
2 while k ⩽n do
3
VIR ←LPH(f, Jf(x) · λ −β);
4
WIR(f) ←WIR(f) ∪VIR;
5
f ←{f, x · β + c} where n is a random vector in IRn, and c is a
random real number;
6
k ←k + 1;
7 end
8 return WIR(f)
Remark 5. Algorithm 2 is essentially a recursive calling of Algorithm 1.
Theorem 3 ( [31] Theorem 2.1). For a system f = (f1, . . . , fk) with n variables
and degrees di = deg(fi) for i = 1, . . . , k, the number of complex roots of system
(4) is bounded by

n −1
n −k

(d −1)n−kD
(6)
where d = max{d1, . . . , dk} > 1 and n > k > 0, D is the degree of the pure n−k
dimensional component of V = V (f).
Moreover, the total number of points in WIR(f) is bounded by
n−k

j=0

n −1 −j
n −k −j

(d −1)n−k−jD.
(7)
Obviously we have the following inequalities:
MV (F) ⩽

n −1
k −1

(d −1)n−kD ⩽

n −1
k −1

(d −1)n−k
k

i=1
di ⩽dn
k

i=1
di.

A Special Homotopy Continuation Method
371
If f is dense, the equalities hold. And if f is sparse, they vary considerably most of
the time. For example, let f ={−62xy+97y−4xyz−4, 80x−44xy+71y2−17y3+2}
with d = 3, n = 3, k = 2. We have MV (F) = 11,
n −1
k −1

(d −1)n−kD = 28,

n −1
k −1

(d −1)n−k
k
i=1
di = 36, and dn
k
i=1
di = 243.
5.2
Illustrative Example
In this subsection, we present an illustrative example for Algorithm 2.
Example 1. Consider the hypersurface deﬁned by f = (y2 −x3 −ax −b) ·
((x −y + e)3 + x + y), e = 6, a = −4, b = −1. Clearly, VIR(f) is the combination
of a cubic ellipse (y2 −x3 −ax −b), and a cubic curve (x −y + e)3 + x + y, as
plotted in Fig. 1. We show how to compute WIR(f) by Algorithm 2.
– For computing VIR = LPH(f), we randomly choose a line l in C2 and solve
L = {f, l} by polyhedral homotopy, which follows D = 6 paths. Then to com-
pute Ω by linear homotopy, we follow
2 −1
2 −1

(6 −1)2−16 = 30 convergent
paths, and for VIR by linear homotopy, we follow 30 paths, of which 6 are
convergent and 19 divergent. Then
VIR =
(−1.44299, −1.32941), (−0.781143, 1.28371)
.
– For computing WIR(f), we solve f (1) = {f, x · β + c} by polyhedral homotopy,
with x · β + c = 0.874645x + 1.0351y −3.9825 and
WIR(f (1)) =

(2.4052801, 1.815026), (−1.992641, 5.531208)

.
So WIR(f) = WIR(f (1)) ∪VIR, which has at least one point in each connected
component of VIR(f) as in Fig. 1.
Fig. 1. n = 10, k = 4, deg = 2

372
Y. Wang et al.
6
Experiment Performance
As shown in Sect. 5, to compute the set WIR(f), the key and most time con-
suming steps are solving the system F =

f,
k
i=1
λi∇fi −β

in Algorithm
1. In this section, given f = {f1, . . . , fk}, we solve the square system F =

f,
k
i=1
λi∇fi −β

. We compare our program LPH which implements Algorithm
1 to Hom4PS-2.0 (available at http://users.math.msu.edu/users/li/). All the
examples were computed on a PC with Intel Core i5 processor (2.5GHz CPU, 4
Cores and 6 GB RAM) in the Windows environment. We mention that LPH is
a program written in C++, available at http://arcnl.org/PDF/LHP.zip, and an
interface of Maple is provided on this site.
6.1
Dense Examples
In Table 1, we provide the timings of LPH and Hom4ps-2.0 for solving systems
F =

f,
k
i=1
λi∇fi −β

, where f = (f1, . . . , fk) consists of dense polynomials
of degree 2, n = 2, . . . , 14 and 1 ⩽k ⩽n −1. T1 ,T2 are the the timings for
LPH and Hom4ps-2.0, respectively, and RAT is the ratio of T1 to T2. ‘overﬂow’
means running out of memory. When T2=overﬂow, we set RAT=ε.
It may be observed that LPH is much faster than Hom4ps-2.0 when k > 1.
Note also that LPH is a little bit slower than Hom4ps-2.0 when k = 1. The main
reason is obvious. That is, the root number bound of LPH, i.e.

n −1
n −k

(d −1)n−kD,
is close to the mixed volume MV (F) when F is dense but the computation of
MV (F) is very time-consuming.
6.2
Sparse examples
In Table 2, we provide the timings of LPH and Hom4ps-2.0 on sparse examples:
Czapor Geddes2, Morgenstern AS(3or), Gerdt2, Hairer1, and Hawes2 which are
available at: http://www-sop.inria.fr/saga/POL/. #1 and #2 is the number of
curves followed by LPH and Hom4ps-2.0, respectively. # is the number of roots
of the Jacobian systems constructed from the examples. ‘d’ means the minimal
and maximal degree of the example. “term” means the minimal and maximal
number of terms of the example. T1 and T2 are the timings of LPH and Hom4ps-
2.0, respectively. RAT means the ratio of T1 to T2.
Note that LPH is much slower than Hom4ps on these sparse examples. The
main reason is that LPH pays the overhead cost for the Ω and homotopy
H2(x, λ, t) = G · (1 −t) + F · γ2 · t.

A Special Homotopy Continuation Method
373
Table 1. Dense Examples (n: number of variables; k: number of polynomials; T1: time
for LHP; T2: time for Hom4PS-2.0; RAT: ratio of T1 to T2.)

374
Y. Wang et al.
Table 2. Sparse examples
Ex
n k d
term #1
#2
#
T1
T2
RAT
C2
5
4 4–5 7–32 2*1767
692 383 16.6s
19.4 s 0.85
M3 9
5 2
2–7
2*2240
368 32
69 s
6 s
11
G2
5
2 4
8–9
2*1080
17
15
8.4 s
0.2 s
31.8
H1
8
6 1–3 2–4
2*400
15
15
9.8 s
0.18 s 52
H2
8
5 2–4 3–5
2*12320 148 80
5m49 s 1.2s
267
Moreover, LPH executes 2∗
n −1
k −1

(d −1)n−kD times of curve following, while
Hom4ps does only MV (F) times of curve following.

n −1
k −1

(d −1)n−kD is
not tight for these sparse examples and much greater than MV (F).
6.3
RAT/Density
In Fig. 2, we present the changes of ratio of T1 to T2 as terms increase. We
randomly generate f = (f1, . . . , fk) with diﬀerent n, k and degrees, and increase
the number of terms from 2 to dense.
Fig. 2. LPH vs Hom4ps-2.0 with the growth of density
It can be observed that, when the polynomials are not very sparse, e.g. the
number of terms are more than 10% of

n + d
d

, LPH is faster than Hom4ps-2.0.

A Special Homotopy Continuation Method
375
Actually, when the polynomials are not very sparse, the root number bound
n −1
k −1

(d −1)n−kD is close to MV (F).
Acknowledgement. We gratefully acknowledge the very helpful suggestions of Hoon
Hong on this paper with emphasis on Sect. 6. We also thank Changbo Chen for his
helpful comments. And the authors would like to thank the anonymous reviewers for
their constructive comments that greatly helped improving the paper.
References
1. Garcia, C.B., Zangwill, W.I.: Finding all solutions to polynomial systems and other
systems of equations. Math. Program. 16(1), 159–176 (1979)
2. Drexler, F.J.: Eine Methode zur Berechnung s¨amtlicher L¨osungen von Polynom-
gleichungssystemen. Numer. Math. 29(1), 45–58 (1977)
3. Sommese, A.J., Verschelde, J., Wampler, C.W.: Numerical algebraic geometry. In:
The Mathematical of Numerical Analysis. Lectures in Applied Mathematics, vol.
32, pp. 749–763. AMS (1996)
4. Allgower, E.L., Georg, K.: Introduction to numerical continuation methods.
Reprint of the 1979 original. Society for Industrial and Applied Mathematics (2003)
5. Li, T.: Numerical solution of polynomial systems by homotopy continuation meth-
ods. In: Handbook of Numerical Analysis, vol. 11, pp. 209–304 Elsevier (2003)
6. Sommese, A.J., Wampler, C.W.: The Numerical Solution of Systems of Polynomials
Arising in Engineering and Science. World Scientiﬁc, Singapore (2005)
7. Morgan, A.: Solving Polynominal Systems Using Continuation for Engineering and
Scientiﬁc Problems. Society for Industrial and Applied Mathematics, Philadelphia
(2009)
8. Bates, D.J., Haunstein, J.D., Sommese, A.J., Wampler, C.W.: Numerically Solving
Polynomial Systems with Bertini. Society for Industrial and Applied Mathematics,
Philadelphia (2013)
9. Lee, T.L., Li, T.Y., Tsai, C.H.: Hom4ps-2.0: a software package for solving polyno-
mial systems by the polyhedral homotopy continuation method. Computing 83(2),
109 (2008)
10. Morgan, A.P., Sommese, A.J., Watson, L.T.: Finding all isolated solutions to poly-
nomial systems using hompack. ACM Trans. Math. Softw. 15(2), 93–122 (1989)
11. Verschelde, J.: Algorithm 795: Phcpack: a general-purpose solver for polynomial
systems by homotopy continuation. ACM Trans. Math. Softw. 25(2), 251–276
(1999)
12. Collins, G.E.: Quantiﬁer elimination for real closed ﬁelds by cylindrical algebraic
decompostion. In: Brakhage, H. (ed.) GI-Fachtagung 1975. LNCS, vol. 33, pp.
134–183. Springer, Heidelberg (1975). doi:10.1007/3-540-07407-4 17
13. Seidenberg, A.: A new decision method for elementary algebra. Ann. Math. 60(2),
365–374 (1954)
14. El Din, M.S., Schost, ´E.: Polar varieties and computation of one point in each
connected component of a smooth real algebraic set. In: Proceedings of ISSAC
2003, pp. 224–231. ACM, New York (2003)
15. El Din, M.S., Spaenlehauer, P.J.: Critical point computations on smooth varieties:
degree and complexity bounds. In: Proceedings of ISSAC 2016, pp. 183–190. ACM,
New York (2016)

376
Y. Wang et al.
16. Bank, B., Giusti, M., Heintz, J., Pardo, L.M.: Generalized polar varieties and an
eﬃcient real elimination. Kybernetika 40(5), 519–550 (2004)
17. Bank, B., Giusti, M., Heintz, J., Pardo, L.: Generalized polar varieties: geometry
and algorithms. J. Complex. 21(4), 377–412 (2005)
18. Rouillier, F., Roy, M.F., El Din, M.S.: Finding at least one point in each connected
component of a real algebraic set deﬁned by a single equation. J. Complex. 16(4),
716–750 (2000)
19. El Din, M.S., Schost, ´E.: Properness defects of projections and computation of
at least one point in each connected component of a real algebraic set. Discrete
Comput. Geom. 32(3), 417–430 (2004)
20. Li, T.Y., Wang, X.: Solving real polynomial systems with real homotopies. Math.
Comp. 60(202), 669–680 (1993)
21. Lu, Y., Bates, D.J., Sommese, A.J., Wampler, C.W.: Finding all real points of a
complex curve. Technical report. In: Algebra, Geometry and Their Interactions
(2006)
22. Bates, D.J., Sottile, F.: Khovanskii-rolle continuation for real solutions. Found.
Comput. Math. 11(5), 563–587 (2011)
23. Besana, G.M., Rocco, S., Hauenstein, J.D., Sommese, A.J., Wampler, C.W.: Cell
decomposition of almost smooth real algebraic surfaces. Numer. Algorithms 63(4),
645–678 (2013)
24. Hauenstein, J.D.: Numerically computing real points on algebraic sets. Acta Appl.
Math. 125(1), 105–119 (2013)
25. Shen, F., Wu, W., Xia, B.: Real root isolation of polynomial equations based on
hybrid computation. In: Feng, R., Lee, W., Sato, Y. (eds.) Computer Mathematics,
pp. 375–396. Springer, Heidelberg (2014). doi:10.1007/978-3-662-43799-5 26
26. Wu, W., Reid, G.: Finding points on real solution components and applications
to diﬀerential polynomial systems. In: Proceedings of ISSAC 2013, pp. 339–346.
ACM, New York (2013)
27. Bernshtein, D.N.: The number of roots of a system of equations. Funct. Anal. Appl.
9(3), 183–185 (1975)
28. Hauenstein, J.D., Sommese, A.J., Wampler, C.W.: Regeneration homotopies for
solving systems of polynomials. Math. Comp. 80(273), 345–377 (2011)
29. John, F.: Extremum problems with inequalities as subsidiary conditions. In: Giorgi,
G., Kjeldsen, T.H. (eds.) Traces and Emergence of Nonlinear Programming, pp.
197–215. Springer, Basel (2014). doi:10.1007/978-3-0348-0439-4 9
30. Morgan, A.P., Sommese, A.J.: Coeﬃcient-parameter polynomial continuation.
Appl. Math. Comput. 29(2), 123–160 (1989)
31. Wu, W., Reid, G., Feng, Y.: Computing real witness points of positive dimen-
sional polynomial systems. Theoretical Computer Science (2017). http://doi.org/
10.1016/j.tcs.2017.03.035. Accessed 31 Mar 2017
32. Morgan, A.P., Sommese, A.J., Wampler, C.W.: A power series method for comput-
ing singular solutions to nonlinear analytic systems. Numer. Math. 63(1), 391–409
(1992)
33. Morgan, A.P.: A transformation to avoid solutions at inﬁnity for polynomial sys-
tems. Appl. Math. Comput. 18(1), 77–86 (1986)
34. Huber, B., Verschelde, J.: Polyhedral end games for polynomial continuation.
Numer. Algorithms 18(1), 91–108 (1998)
35. Bates, D.J., Hauenstein, J.D., Sommese, A.J.: A parallel endgame. Contemp. Math.
556, 25–35 (2011). AMS, Providence, RI

Penalty Function Based Critical Point Approach
to Compute Real Witness Solution Points
of Polynomial Systems
Wenyuan Wu1,2, Changbo Chen1,2(B), and Greg Reid3
1 Chongqing Key Laboratory of Automated Reasoning and Cognition,
Chongqing Institute of Green and Intelligent Technology,
Chinese Academy of Sciences, Chongqing, China
{wuwenyuan,chenchangbo}@cigit.ac.cn
2 University of Chinese Academy of Sciences, Beijing, China
3 Applied Mathematics Department, Western University, London, Canada
reid@uwo.ca
Abstract. We present a critical point method based on a penalty func-
tion for ﬁnding certain solution (witness) points on real solutions compo-
nents of general real polynomial systems. Unlike other existing numerical
methods, the new method does not require the input polynomial system
to have pure dimension or satisfy certain regularity conditions.
This method has two stages. In the ﬁrst stage it ﬁnds approximate
solution points of the input system such that there is at least one real
point on each connected solution component. In the second stage it
reﬁnes the points by a homotopy continuation or traditional Newton iter-
ation. The singularities of the original system are removed by embedding
it in a higher dimensional space.
In this paper we also analyze the convergence rate and give an error
analysis of the method. Experimental results are also given and shown
to be in close agreement with the theory.
1
Introduction
Computational real algebraic geometry is the study of the global structure of
real solution sets of polynomial systems, including positive dimensional solution
components (see [2] for a background text on algorithms for exact real alge-
braic geometry). This paper is a contribution to the development of numerical
algorithms for computational real algebraic geometry directed at numerically
describing such global structure. In contrast, conventional numerical methods
seek local solutions which are points, and generally do not give information on
positive dimensional solution components.
Numerical algebraic geometry [16,29] was pioneered by Sommese, Wampler,
Verschelde and others (see the texts [9,27] for references and background). They
ﬁrst considered the easier characterization of complex solution components of
each possible dimension, by slicing the solution set with appropriate random
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 377–391, 2017.
DOI: 10.1007/978-3-319-66320-3 27

378
W. Wu et al.
planes, that intersected the solution components in complex points called witness
points. The complex points are computed by homotopy continuation solvers. For
example, a one dimensional circle, x2+y2−1 = 0 in C2 is intersected by a random
line in two such witness points, but this method obviously fails for (x, y) ∈R2
since a real line can miss the circle.
Instead the method in [31,32] yields real witness points as critical points of
the distance from a random hyperplane to the real variety. The reader can easily
see this yields two real witness points for the circle example. An alternative
numerical approach where the witness points are critical points of the distance
from a random point to the real variety has been developed in [15]. The works
[15,31,32] use Lagrange multipliers to set up the critical point problem.
A contribution of our current paper is to remove the assumptions in [15,
31,32] by developing a penalty function based critical point method where the
singularities are removed by embedding systems in a higher dimensional space.
The method has two stages. In the ﬁrst stage it ﬁnds approximate solution points
of the input system such that there is at least one real point on each connected
solution component. In the second stage it reﬁnes the points by a homotopy
continuation or traditional Newton iteration. We also analyze the convergence
rate and give an error analysis of our method. Experimental results are given
and shown to closely agree with the theory.
Critical point methods in Lagrange form appeared previously in important
symbolic works [24–26]. In those works, the systems are analyzed using Gr¨obner
Bases. Ultimately numerical methods have to be used to approximate points on
components, but only after application of symbolic algorithms to the systems,
instead of the fully numerical methods used here and in [15,31,32]. Also see the
early related symbolic works [1,6] and the recent work [5,11].
More distantly related symbolic approaches for computational real alge-
braic geometry include cylindrical algebraic decomposition (CAD) introduced
by Collins [13] and improved by many others. Recent improvement of CAD by
using triangular decompositions are given in [12] for solving semi-algebraic sys-
tems. But the double exponential cost of the CAD algorithm [14] is the main
barrier to its application.
Numerical methods based on moment matrices and semi-deﬁnite program-
ming techniques have been developed to approximate real radical ideals of zero
dimensional systems, e.g. [20,21]. For a positive dimensional system, an app-
roach is given in [7] which combines numerical algebraic geometry and sums of
squares programming to test whether the input is real radical or not. Also see
[23,33], based on moment matrices, and [22].
As a development of critical point approaches [15,24–26,31], this article will
propose an approximation method to compute real witness points of polynomial
systems without any regularity assumption [31,32] or pure dimension assumption
[15]. In Sect. 2, we will describe how the polynomial systems are embedded in a
higher dimensional space. In Sect. 3, we will describe error control with a rank
assumption. In Sect. 4, this rank assumption is removed and error control is

A Critical Point Approach to Compute Real Witness Solution Points
379
provided for general systems. In Sect. 5, our method is illustrated with examples
and concluding remarks are given in Sect. 6.
2
Augmented System
In this section we introduce our augmented system, that involves adding a vari-
able to each equation, so the original system is obtained when these slack vari-
ables are set to zero. The resulting augmented system has solution set that is a
smooth real manifold. We alert the reader that this is diﬀerent to the embedding
systems of (complex) numerical algebraic geometry. To avoid confusion with the
well known embedding systems of the complex case we have used a diﬀerent
name for our systems, Augmented System.
Let x = (x1, . . . , xn). Let f = {f1, . . . , fk} be a set of polynomials in the ring
R[x]. We construct the following augmented system g for f with slack variables
z = (z1, . . . , zk):
g = {f1 + z1, f2 + z2, . . . , fk + zk}.
(1)
Note that g ⊂R[x, z] holds.
Lemma 1. The Jacobian matrix of g w.r.t. the variables (x1, . . . , xn, z1, . . . , zk)
has rank k at any point of VR(g) and VR(g) is a smooth submanifold of Rn+k
with dimension n.
Proof. Firstly, VR(g) ̸= ∅since {x1 = 0, . . . , xn = 0, z1 = −f1(0), . . . , zk =
−fk(0)} is a real solution. Secondly, it is easy to see that the Jacobian matrix
∂g
∂(x,z) has full rank k at any solution (x∗, z∗) ∈VR(g), which implies that VR(g) is
a smooth submanifold of Rn+k with dimension n by the regular level set theorem
(see pp. 113–114 of [19]).
⊓⊔
By Lemma 1, the augmented system g satisﬁes the regularity assumptions A1
and A2 of [31]. Moreover, any point on VR(g) being smooth is a crucial property
for numerical stability of numerical methods applied to VR(g).
Using the critical point technique [24], we choose a random point a =
(a1, . . . , an), where a ̸∈VR(f), in x-space and consider the minimal distance
from VR(f) to this point. As the norm of the slack variables z approaches zero,
the corresponding point of VR(g) approaches VR(f). To force the slack variables
z to be very small, we introduce a penalty function β · (z2
1 + · · · + z2
k)/2 with
penalty factor β ≫0 and formulate the following optimization problem
min μ = (β · (z2
1 + · · · + z2
k) +
n
i=1(xi −ai)2)/2
(2)
s.t.
g = 0.
To solve the optimization problem above, we can use Lagrange multiplier
techniques:

380
W. Wu et al.
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
x1 −a1
...
xn −an
β z1
...
β zk
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
∂f1/∂x1 · · · ∂fk/∂x1
...
...
...
∂f1/∂xn · · · ∂fk/∂xn
1
...
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(n+k)×k
·
⎛
⎜
⎝
λ1
...
λk
⎞
⎟
⎠.
(3)
Then (3) implies that λi = βzi = −βfi. Substituting this solution back into the
Eq. (3) above yields a square system with n variables
⎛
⎜
⎝
x1
...
xn
⎞
⎟
⎠+ β · J t ·
⎛
⎜
⎝
f1
...
fk
⎞
⎟
⎠=
⎛
⎜
⎝
a1
...
an
⎞
⎟
⎠.
(4)
where the n × k matrix J t is the transpose of the Jacobian of f.
The optimization problem (2) is equivalent to the following unconstrained
optimization problem (will be used in the next two sections):
min μ = (β · (f 2
1 + · · · + f 2
k) + 	n
i=1(xi −ai)2)/2.
(5)
Setting the gradient of μ to be zero, we also obtain Eq. (4).
Note that the left hand side of Eq. (4) deﬁnes a smooth mapping M : Rn →Rn.
Lemma 2. For a random point a = (a1, . . . , an) /∈VR(f), Problem (2) has
solutions and M −1(a) ̸= ∅. Moreover, every point of the real variety M −1(a) is
a regular point of M with probability 1.
Proof. Let zi = wi/√β, i = 1, . . . , k, and substitute them into (2). Let h =
{√βf1 + w1, . . . , √βfk + wk}. We obtain another equivalent form of Problem
(2), where the objective function is now formulated as a distance function:
min (w2
1 + · · · + w2
k +
n
i=1(xi −ai)2)/2
(6)
s.t.
h = 0.
By Lemma 1, VR(g) is a smooth submanifold of Rn+k with dimension n. So
is VR(h). For any a /∈VR(f), the point (x = a, w = 0) does not belong to VR(h).
Thus, (6) always has minimum distance from (a, 0) to VR(h) by completeness
of the real numbers, which implies that the minimal value of (2) can always be
attained. Since the Jacobian matrix of g has full rank at any point of VR(g),
a solution {x∗, z∗} of Problem (2) must be a solution of Eq. (3), which implies
that x∗is a solution of Eq. (4).
Thus M −1(a) ̸= ∅. By Sard’s Theorem [30], for almost all a, every point of
the real variety M −1(a) is regular point of M.
⊓⊔

A Critical Point Approach to Compute Real Witness Solution Points
381
Lemma 2 implies that all the solutions of Eq. (4) can be obtained by applying
homotopy continuation methods.
Among these solutions, we look for solutions with small residuals i.e. ∥z∥≪1.
It is possible that such points do not exist, which then provides strong evidence
that VR(f) is empty. Intuitively, this is because if VR(f) is not empty, increasing
the penalty factor β will force ∥z∥to be close to zero.
A theoretical study on the relationship between the magnitude of the residual
∥z∥and the emptiness of VR(f) is out of the scope of this paper and will be
treated in a future work. In the rest of this paper, we always assume
that VR(f) ̸= ∅. A natural question is how to estimate the distances of the local
minima of Problem (5) to VR(f). We divide this problem into two cases w.r.t.
the rank of the Jacobian and will address them in the next two sections.
Note that, throughout this paper, the norm ∥· ∥always means the 2-norm.
3
Error Control with Rank Assumption
Since Problem (5) with penalty function is diﬀerent from the goal of ﬁnding
real witness points of the original system f = 0, it is of great importance to
study the diﬀerence between their solutions. In this section, we will give an error
estimate of the approximate answer given by solving Problem (5) under a rank
assumption. In the next section, we will remove this assumption and give an
error estimate for general systems.
For a smooth point x on VR(f), let the local dimension of VR(f) at point x
be ℓ. The Jacobian matrix at x is a k×n matrix denoted by Jx. Suppose its rank
is m, where m ≤min{k, n}. Then we say that x satisﬁes the rank condition,
if m = n −ℓ, i.e.
rank Jx = n −dim VR(f)x
(7)
If any smooth point on VR(f) satisﬁes the rank condition, then we say the
system f satisﬁes the rank condition. For example this occurs if f = {x−y, x2 −
y2}. This means that f can be an over-determined system and even generate a
non-radical ideal (e.g. consider f = {(x2 + 1)2(x −y)}). Note that f = (x −y)2
does not satisfy the rank condition, although its graph is a smooth line. Such
systems will be discussed in the next section.
For a random point a ∈Rn, there is at least one point on each connected
component of VR(f) with minimal distance to a satisfying the following problem:
min
n

i=1
(xi −ai)2
(8)
s.t.
x ∈VR(f).
Let us consider such a point p of (8) with local minimal distance to a. For
this point, there exists a constant c and we have ∥fi(p + Δx)∥< c∥Δx∥for each
polynomial fi when Δx is suﬃciently small. The value of the target function μ
at p of (5) is D2/2 where D = ∥p−a∥. If we move p towards a with a suﬃciently
small distance Δx to p′ then

382
W. Wu et al.
μ(p′) = β∥f(p′)∥2/2 + (p′ −a)2/2 < βc2 Δx2/2 + (D −Δx)2/2 < D2/2.
(9)
It means that p of Problem (8) is not a local minimum of Problem (5). Let
p′ be a local minimum of (5) for a given β. Consequently, p′ /∈VR(f). But we
have the following result.
Corollary 1. Let p be a local minimum of (8). There exists a local minimum p′
of (5) for suﬃciently large β, such that ∥p −p′∥can be arbitrarily small.
Proof. For any small δ > 0, consider the sphere S of a ball centered at p with
radius δ. Let D = ∥p −a∥, where a /∈VR(f) is the given point for both problems
(5) and (8). The sphere S can be divided into two sets: S1 = {x ∈S : ∥x −a∥≤
D} and S2 = {x ∈S : ∥x −a∥> D}. Since p obtains the local minimum
distance from VR(f) to a, we have S1 ∩VR(f) = ∅for a small enough δ. Let
s = minx∈S1(	
j fj(x)2). So s > 0. When βs + (D −δ)2 > D2, i.e. β > 2δD−δ2
s
,
we have μ(x) > D2/2 = μ(p) for any point x on the sphere S. Since the ball is
a compact set, the local minimal value of μ must be attained at p′ inside this
ball.
⊓⊔
We now consider how to estimate the error ∥p−p′∥for a given β. First let us
consider a simple case when a local minimum p of (8) satisﬁes the rank condition
(7). Then, we have the following result.
Theorem 2. Suppose p is a local minimum of (8) satisfying the rank condition
(7). Then there is at least one real solution p′ of Eq. (4) such that ∥p −p′∥<
D
βσ2
m+1, where D = ∥p −a∥and σm is the smallest nonzero singular value of Jp.
Proof. Without loss of generality, we assume that p is the origin o. Because of
the rank condition, the local dimension at p is equal to n −rank Jp = n −m.
Moreover, the null-space of Jp is the tangent space T at p. Let N be the orthog-
onal complement of T in Rn. Since p ∈T has the minimum distance to a, the
vector a belongs to N.
Let U T JpV
= Σk×n = diag(σ1, . . . , σm, 0, . . . , 0) be the singular value
decomposition of the Jacobian matrix at p, where U = ([u1| · · · |uk]) ∈Rk×k
and V = ([v1| · · · |vn]) ∈Rn×n. Then, the space N is spanned by {v1, . . . , vm}.
By Corollary 1 for suﬃciently large β, there exists a local minimum p′ of (5)
such that ∥p′ −p∥= δ ≪1 and f(p′) = f(p) + Jp · p′ + O(δ2) since p is the
origin.
Let p′ = t + b, where t ∈T, b ∈N. Recall that a ∈N and N⊥T. Then
we have Jp · p′ = Jp · b and ∥p′ −a∥2 = ∥t∥2 + ∥a −b∥2. Since a, b ∈N,
we choose {v1, . . . , vm} as the coordinates of N and suppose a = (a1, . . . , am),
b = (b1, . . . , bm). Thus, ignoring high order errors we have
∥f(p′)∥2 = ∥Jp · b∥2 = bT V Σ2V T b =
m

i=1
σ2
i b2
i .

A Critical Point Approach to Compute Real Witness Solution Points
383
Hence, p′ is a point near p satisfying the following problem
min
t,b
μ =

β(
m

i=1
σ2
i b2
i ) +
m

i=1
(ai −bi)2 + ∥t∥2
/2.
It is straightforward to show that when bi =
ai
βσ2
i +1 and t = 0, the function μ
attains the minimum 	
i
βσ2
i
βσ2
i +1a2
i /2, which is less than μ(p) = 	
i a2
i /2 = D2/2.
Therefore,
∥p′ −p∥2 =

i
b2
i =

i
(
ai
βσ2
i + 1)2 ≤

i
(
ai
βσ2m + 1)2 = (
D
βσ2m + 1)2.
Moreover, p′ can be found by solving Eq. (4).
⊓⊔
Example 1. Consider the system f = {x2 + y2 −2x, 2x2 + 2y2 −4x} and
a = (−0.8, 0.6). In this case m < k holds. The real variety is a circle centered at
(1, 0) with radius 1. The point p = (1−3
√
10
10 ,
√
10
10 ) has the minimal distance to a.
Consequently, we have D = 1, σm = 4.472 and r = ∥p −p′∥≤e =
1
20β+1. The
behaviors of both the actual error r and the estimated error e with increasing of
β are given in Fig. 1, where the diﬀerences between the log of estimated errors
and the log of actual errors are greater than 0.047 and less than 0.048. That is
0.895e < r < 0.897e. Thus, the theoretical estimation is quite sharp.
Here increasing the value of β and producing more and more accurate roots
aim to verify Theorem 2. Since the local minimum satisﬁes the rank condition,
we can simply apply Gauss-Newton iteration [4] to improve the accuracy.
Remark 1. Since we only have a local minimum p′ which is an approximation of
p, a good estimate of σm can be obtained by Jp′ for suﬃciently small ∥p −p′∥
because of Weyl’s theorem [28].
This theorem only works for σm > 0. However, if σm is close to zero because
of singularity of p or non-radicalness of the system f, the convergence will be
very slow as β →∞. But Corollary 1 still applies.
Example 2. In an example of [31], f = {x2
2 + x2
3 −(2x1 −x2
1)3} and a =
(−0.5, −1, 0.1). The point p = (0, 0, 0) with the minimal distance to a is singular
in VR(f). To see the asymptotic behavior of the error given in Corollary 1, we
plot the magnitude of the actual error against the magnitude of the penalty fac-
tor β in Fig. 2. Applying the CurveFitting[LeastSquares] command in Maple
yields log(r) .= −0.538 −0.202 log(β).
4
Error Control for General Systems
Previously, we know that if a local minimum of (8) satisﬁes the rank condition
m = n −ℓ, the estimated error is of order O(1/β) as can be observed in Fig. 1.

384
W. Wu et al.
Fig. 1. For Example 1, the log of the estimated error, log(e) (resp. actual error, log(r))
is decreasing linearly with the increase of the magnitude of penalty factor β.
Fig. 2. The actual error log(r) for Example 2 is decreasing slowly with slope −0.202.
For this case, although increasing the value of β can improve the accuracy, the
Gauss-Newton iteration will improve convergence.
However, whether the input system satisﬁes the regularity assumptions or
not, a local minimum of Problem (5) could be close to singularities (as in Exam-
ple 2) which means m < n −ℓ. Since such singularities are unavoidable, we will
address the convergence for general systems with no assumptions in this section.
4.1
Degree Index
In Problem (5), the residual of f is ampliﬁed by a penalty factor β when x does
not belong to VR(f). In this section we will give a lower bound of the residual
ﬁrst. Then it leads to an error control of our method for general systems.

A Critical Point Approach to Compute Real Witness Solution Points
385
Let f ∈R[x1, . . . , xn]. Suppose f(0) = 0. If we write f = fm + fm+1 + . . .
with fα homogeneous of degree α and fm ̸= 0, then m is the multiplicity of f at
the origin. For example f = x2 + y3 has a cusp at the origin with multiplicity 2.
Here we will consider the value of f near the origin. Let a direction be
v = (a, b) with a2 + b2 = 1. Then the bivariate polynomial f = x2 + y3 becomes
a univariate polynomial a2t2+b3t3 = t2(a2+b3t) by substituting (x = at, y = bt).
For a generic direction v, the magnitude of f will be O(t2) as t →0. Here the
degree 2 coincides with the multiplicity. But a lower bound is obtained for the
direction v = (0, 1) where the value of f = t3 is even smaller of order 3.
In general we deﬁne degree index to study multiplicity discussed above.
Deﬁnition 3. Let fv = f(vt) which is a polynomial in R[t] by substituting
x = vt into f with v ̸= 0 ∈Rn. The lowest degree of nonzero terms of fv is
denoted by degmin(fv). We deﬁne the degree index of f to be
degind(f) = max
v
degmin(fv)
(10)
Furthermore, for any polynomial f and a point p ∈Rn, if f(p) = 0 then we
deﬁne the degree index of f at p to be degind(f(x + p)).
For instance, degind(x2 −y2) = 2, degind(x2 + y2) = 2, and degind(x2 + y3) = 3,
etc. But it is diﬃcult to compute the degree index of an arbitrary multivariate
polynomial f. It can be reduced to ﬁnding a nonzero common real root of the
sequence {fm = 0, fm+1 = 0, . . .}. However, by deﬁnition, if deg(f) = d > 0,
then we have 1 ≤degind(f) ≤d, which gives a simple bound.
Suppose fv(t) = a0tα0 + a1tα1 + · · · + aktαk is not a zero polynomial and
degmin(fv) = α0 < α1 < · · · < αk. The lowest degree term is a0tα0 which is the
dominant term when t ≪1. Thus, we have the following result.
Proposition 4. Let f ∈R[x1, . . . , xn] and f(0) = 0. For any direction v ̸= 0 ∈
Rn, if fv(t) = f(vt) is not a zero polynomial, then there is a constant c > 0 such
that |fv(t)| > c tdegind(f) for suﬃciently small t > 0.
As in Sect. 3, suppose the point p ∈VR(f) minimizing distance to a random
point a is singular. Let p′ be the corresponding local minimum close to p of
Problem (5). We have the following estimation for ∥p −p′∥.
Theorem 5. For a random point a ∈Rn and a suﬃciently large β, suppose
p ∈VR(f) attains the local minimal distance to a. Then there is a solution p′ of
Eq. (4) such that ∥p′ −p∥≤O( 2I−1
1/β ), where I = max{degind(fi(x + p)), i =
1, . . . , k}.
Proof. By Eq. (5), μ(p′) = (β 	
i fi(p′)2 + ∥p′ −a∥2)/2. Let D = ∥p −a∥. The
relationship between points p and p′ is shown in Fig. 3, where r = ∥p′ −p∥. Since
D is the local minimal distance from VR(f) to a and μ(p′) < μ(p) = D2/2, we
have ∥p′ −a∥< D and p′ /∈VR(f) and the angle θ between pp′ and pa is less

386
W. Wu et al.
Fig. 3. The minimum value μ is attained at p′ which is not a zero of f.
than π/2. Then 2μ(p′) = (D −r cos θ)2 + r2 sin2 θ + β 	
i fi(p′)2 < D2 = 2μ(p),
which is equivalent to β 	
i fi(p′)2 + r2 < 2r cos θD.
Thus β 	
i fi(p′)2 < 2rD holds. Since f(p′) ̸= 0, there is at least one nonzero
fi(p′). Recall that r = ∥p′ −p∥, thus for the direction v = (p′ −p)/∥p −p′∥,
fi(vr + p) = fi(p′) ̸= 0, which implies that fi(vt + p) is a nonzero polynomial
in t. By Proposition 4, when r is small enough, we have
fi(p′)2 = fi(vr + p)2 > c2 r2 degind(fi(x+p)) ≥c2 r2I.
Thus, we get βc2 r2I < 2rD ⇒r2I−1 < O(1/β).
⊓⊔
Remark 2. If the input system f satisﬁes the rank condition, then we have I = 1
and ∥p′ −p∥≤O(1/β ) in Theorem 5, which is in consistent with Theorem 2.
But Theorem 2 provides a more precise estimation in this case.
Recall Example 2 in Sect. 3, I = degind(f) = 3. By Theorem 5, r =
C 2I−1
1/β for some constant C. Then log(r) = log(C) −
1
2I−1 log(β) = log(C) −
0.2 log(β) which is in close agreement with the experimental results.
4.2
Improve Accuracy
In contrast to Sect. 3, the input polynomial system may not satisfy the rank
assumption. Consequently it is diﬃcult to apply local methods such as Newton
iteration to improve accuracy. For example if f = x2 + y2 and an approximate
root of f = 0 close to 0 is given, it is still diﬃcult to determine how to update
the root because there is only one equation in f.
By Corollary 1, theoretically we can use Eq. (4) to update the approximate
root x′ by increasing β. But introducing a very large β will lead to numerical
instability. To ease this diﬃculty, we substitute β = 1/t into Eq. (4). Multiplying
by t gives
t
⎛
⎜
⎝
x1 −a1
...
xn −an
⎞
⎟
⎠+ J t ·
⎛
⎜
⎝
f1
...
fk
⎞
⎟
⎠= 0.
(11)

A Critical Point Approach to Compute Real Witness Solution Points
387
which can be considered as a homotopy with initial points in the form of (t0, x0),
where t0 = 1/β with β ≫0 and x0 is a real solution of Eq. (4). When t →0,
the homotopy path x(t) will approach VR(f). The invertibility of the Jacobian
along the homotopy path is guaranteed by Lemma 2.
Let us reduce the value of t by a half at each step i.e. t =
1
2sβ after s steps.
Combining with the result of Theorem 5, we have the following result.
Corollary 6. Let τ = 2−1/(2d−1) < 1. After s steps of path tracking, the error
of root is reduced to O(τ s r), where r is the initial error ∥p′ −p∥.
Example 3. Next we consider a sum of squares f = x2 + y2 with a = (−1, 0.5).
When β = 1000, the solution of (4) is the real point (x = −0.0719, y = 0.0359)
with a small residual f(x, y) = 0.00646. By tracking the path of the homotopy
(11), it yields a sequence of points shown in Fig. 4. After 30 steps, we obtain the
point (x = 0.0000517, y = 0.000103) with residual f(x, y) = 1.34 × 10−8.
The CurveFitting[LeastSquares] command in Maple gives the formula
log(r) .= −0.101 −0.331 log(β), where the coeﬃcient −0.331 is very consistent
with the formula −
1
2I−1 = −0.333 in Theorem 5, where I = degind(x2 +y2) = 2.
Fig. 4. The error for f = x2 + y2 in Example 3.
5
Examples
In this section, we demonstrate the generalized critical point method for ﬁnding
real witness points of a general system on several examples. The numerical tool
for solving the zero dimensional system (4) can be found in [18].
5.1
ISSAC 2016 System
Let us consider an interesting example f = {xyz, z(x2 + y2 + z2 + y), y(y + z)}
in [7]. The real variety consists of a line {y = 0, z = 0} and an isolated point
(0, −1/2, 1/2). We verify this result by RealTriangularize [12] in Maple 18.

388
W. Wu et al.
To obtain the initial approximation, we set β = 10000, a = (1, 0.5, 2) and
solve the corresponding system (4) by Hom4ps2 [18] with the output {[x =
0.00159, y = −0.499, z = 0.500], [x = 0.999, y = 0.0286, z = 0.000169]}. The
ﬁrst is an isolated solution with full rank Jacobian, so it can be reﬁned simply.
But the Jacobian at the second point is close to singular with singular val-
ues {1.03, 0.057, 0.0000045}. Using the homotopy (11), we can reﬁne this point
to (1.0, 0.000044, 0.0) after 30 steps of path tracking. At the exact solution
(1, 0, 0) the degree index is 2. Hence, by Corollary 6 we have τ = 0.794 and
τ 30 × 0.0286 = 0.000028 which has the same magnitude with 0.000044.
5.2
Seiler System
The system is f = {x2
3 +x2x3 −x2
1, x1x3 +x1x2 −x3, x2x3 +x2
2 +x2
1 −x1} whose
real variety is a curve. We use RealTriangularize to obtain a triangular set
{(x2 + x3) x1 −x3, x3
2 + 3 x3x2
2 + 3 x2
3x2 + x3
3 −x3}.
Our method gives three initial points when β = 10000 and a = (1, 1, 1), namely
p1 =(0.233, 0.37, 0.113), p2 =(0.0546, −0.22, −0.013) and p3 =(1.12, 0.13, −1.19)
with residuals ∥f(p1)∥= 0.000096, ∥f(p2)∥= 0.00013, ∥f(p3)∥= 0.000014. The
rank of the Jacobian at p1 is 2 with singular values {3.47, 2.06, 4.16 × 10−6}
which means f satisﬁes the rank condition. By Theorem 2, the accuracy can
be improved quickly as β approaches inﬁnity by the homotopy (11). In our
experiment, the new residual becomes ∥f(p′
1)∥= 1.35 × 10−11 after 20 steps of
path tracking. A similar situation happens for p2 and p3.
5.3
Larger Examples
Let f1, f2 and f3 be random linear polynomials in variables {x1, . . . , xn} (where
n ≥4) and f = {(x2
1 −x2)2 + f 2
1 + f 2
2 , f 2
2 −f 2
3 }. Since the ﬁrst polynomial is a
sum of squares, it implies that x2
1 −x2 = 0, f1 = 0, f2 = 0, f3 = 0 which deﬁnes
an n −4 dimensional real variety of degree two. Let f1 = 97x1 −67x2 + 58x3 +
29x4 +37, f2 = 5x1 −36x2 −57x3 +85x4 +80, f3 = 90x1 +74x2 +27x3 +9x4 −91.
Applying RealTriangularize to f yields a triangular set T in 3.6 s:
{876997x1+665882x4+70645 = 0, 876997x2−321399x4−932414 = 0, 876997x3−
1046403x4 −635783 = 0, 443398837924x2
4 −187783491023x4 −812733564733 =
0}. Thus, there are two isolated solutions:
(0.799123750840176, 0.638598769156873, −0.657417515791706, −1.15857484076095),
(−1.28179034942671, 1.64298649988345, 2.61264348189493, 1.58208404954058).
Let β = 10000 and a = (0, 1, 0.21, −0.053) and solve the correspond-
ing square system (4) numerically by Hom4ps2 in 3.09 s to obtain 4 real
solutions, which are (0.775, 0.642, −0.613, −1.12), (0.775, 0.642, −0.613, −1.12),
(0.781, 0.650, −0.614, −1.12) and (−1.24, 1.60, 2.53, 1.49).

A Critical Point Approach to Compute Real Witness Solution Points
389
We cannot reﬁne these roots directly since f consists of only two equations.
To improve the accuracy we apply the techniques introduced in Sect. 4.2 and
after 30 steps of path tracking the reﬁned roots are
(0.799096390527587, 0.638616146725934, −0.657352004837882, −1.15851537630920),
(0.799085595521195, 0.638625438788475, −0.657323985904906, −1.15848904058360),
(0.799096848516397, 0.638616471399574, −0.657351954804033, −1.15851596503370),
(−1.28174450424170, 1.64292754794384, 2.61255064161436, 1.58197011638993).
Apparently, the ﬁrst three are multiple roots.
When n = 5, numerical solving for approximate points costs 17.6 s and reﬁne-
ment costs 0.26 s. It gives two real witness points. On the other hand, if we still
use RealTriangularize to compute the triangular set of f, after 2902 s Maple
18 displayed an Error message and indicated that “Maple was unable to allocate
enough memory to complete this computation”.
Moreover, numerical solving stage costs 129 and 559 s for n = 6 and n = 7
respectively. Since it is diﬃcult to compute the exact solutions, we verify the
output by substituting the reﬁned witness points back to f and the residuals are
less than 10−7.
6
Conclusions
This paper is part of a series in which we develop algorithms for numerical
algebraic geometry. In current paper, we present a new formulation with penalty
function, which is a development of critical point techniques. Comparing with
existing numerical critical point methods, this method does not require the input
system to have pure dimension or satisfy regularity assumptions. It leads to a
kind of penalty function approximation method. The convergence rate of the
method is given and it is in close agreement with our experimental results.
We plan to apply our method to larger systems, and those with approximate
coeﬃcients, which are beyond limitations of current (e.g. symbolic computa-
tion) based approaches. Since a non-radical polynomial system does not satisfy
the rank condition, it is still diﬃcult to move the obtained approximate real
witness points on positive dimensional components to detect more geometric
information. Extracting such information will be a focus of future work. In the
second stage of our method, we assume local convergence of Newton iteration.
An interesting research problem is to compare our approach with the certiﬁed
path tracking approach [10].
Acknowledgements. The authors would like to thank the anonymous reviewers
for their constructive comments that greatly helped improving the paper. This
work is partially supported by the projects NSFC (11471307, 11671377, 61572024),
cstc2015jcyjys40001, and the Key Research Program of Frontier Sciences of CAS
(QYZDB-SSW-SYS026).

390
W. Wu et al.
References
1. Aubry, P., Rouillier, F., El Din, M.S.: Real solving for positive dimensional systems.
J. Symb. Comput. 34(6), 543–560 (2002)
2. Basu, S., Pollack, R., Roy, M.-F.: Algorithms in Real Algebraic Geometry. Algo-
rithms and Computation in Mathematics, vol. 10, 2nd edn. Springer, Heidelberg
(2006). doi:10.1007/3-540-33099-2
3. Besana, G.M., DiRocco, S., Hauenstein, J.D., Sommese, A.J., Wampler, C.W.: Cell
decomposition of almost smooth real algebraic surfaces. Numer. Algorithms 63(4),
645–678 (2013)
4. Bjorck, A.: Numerical Methods for Least Squares Problems. SIAM, Philadelphia
(1996)
5. Bank, B., Giusti, M., Heintz, J.: Point searching in real singular complete intersec-
tion varieties - algorithms of intrinsic complexity. Math. Comput. 83(286), 873–897
(2014)
6. Bank, B., Giusti, M., Heintz, J., Mbakop, G.-M.: Polar varieties, real equation
solving, and data structures: the hypersurface case. J. Complex. 13, 5–27 (1997)
7. Brake, D.A., Hauenstein, J.D., Liddell, A.C.: Numerically validating the complete-
ness of the real solution set of a system of polynomial equations. ISSAC 2016,
143–150 (2016)
8. Bates, D.J., Hauenstein, J.D., Sommese, A.J., Wampler, C.W.: Adaptive multi-
precision path tracking. SIAM J. Numer. Anal. 46(2), 722–746 (2008)
9. Bates, D.J., Hauenstein, J.D., Sommese, A.J., Wampler, C.W.: Numerically Solv-
ing Polynomial Systems with the Software Package Bertini. SIAM, Philadelphia
(2013)
10. Beltr´an, C., Leykin, A.: Robust Certiﬁed Numerical Homotopy Tracking. Found.
Comput. Math. 13(2), 253–295 (2013)
11. Basu, S., Roy, M.-F., El Din, M.S., Schost, ´E.: A baby step-giant step roadmap algo-
rithm for general algebraic sets. Found. Comput. Math. 14(6), 1117–1172 (2014)
12. Chen, C., Davenport, J.H., May, J.P., Moreno Maza, M., Xia, B., Xiao, R.: Trian-
gular decomposition of semi-algebraic systems. J. Symb. Comput. 49, 3–26 (2013)
13. Collins, G.E.: Quantiﬁer elimination for real closed ﬁelds by cylindrical algebraic
decompostion. In: Brakhage, H. (ed.) GI-Fachtagung 1975. LNCS, vol. 33, pp.
134–183. Springer, Heidelberg (1975). doi:10.1007/3-540-07407-4 17
14. Davenport, J.H., Heintz, J.: Real quantiﬁer elimination is doubly exponential. J.
Symb. Comp. 5, 29–35 (1988)
15. Hauenstein, J.: Numerically computing real points on algebraic sets. Acta Appl.
Math. 125(1), 105–119 (2013)
16. Hauenstein, J., Sommese, A.: What is numerical algebraic geometry? J. Symb.
Comp. 79, 499–507 (2017). Part 3
17. Hong, H.: Improvement in CAD-Based Quantiﬁer Elimination. Ph.D. thesis. Ohio
State University, Columbus, Ohio (1990)
18. Li, T.Y., Lee, T.L.: Homotopy method for solving Polynomial Systems software.
http://www.math.msu.edu/∼li/Software.htm
19. Lee, J.M.: Introduction to Smooth Manifolds, vol. 218. Springer, Heidelberg (2003).
doi:10.1007/978-0-387-21752-9
20. Lasserre, J.B., Laurent, M., Rostalski, P.: Semideﬁnite characterization and compu-
tation of zero-dimensional real radical ideals. Found. Comput. Math. 8(5), 607–647
(2008)

A Critical Point Approach to Compute Real Witness Solution Points
391
21. Lasserre, J.B., Laurent, M., Rostalski, P.: A prolongation-projection algorithm for
computing the ﬁnite real variety of an ideal. Theoret. Comput. Sci. 410(27–29),
2685–2700 (2009)
22. Lu, Y.: Finding all real solutions of polynomial systems. Ph.D thesis. University of
Notre Dame (2006). Results of this thesis appear. In: (with Bates, D.J., Sommese,
A.J., Wampler, C.W.), Finding all real points of a complex curve, Contemp. Math.
vol. 448, pp. 183–205 (2006)
23. Ma, Y., Zhi, L.: Computing Real Solutions of Polynomial Systems via Low-rank
Moment Matrix Completion. In: ISSAC, pp. 249–256 (2012)
24. Rouillier, F., Roy, M.-F., El Din, M.S.: Finding at least one point in each connected
component of a real algebraic set deﬁned by a single equation. J. Complex. 16(4),
716–750 (2000)
25. El Din, M.S., Schost, ´E.: Polar varieties and computation of one point in each
connected component of a smooth real algebraic set. In: ISSAC 2013, pp. 224–231
(2003)
26. El Din, M.S., Schost, ´E.: Properness defects of projection functions and computa-
tion of at least one point in each connected component of a real algebraic set. J.
Discrete Comput. Geom. 32(3), 417–430 (2004)
27. Sommese, A.J., Wampler, C.W.: The Numerical Solution of Systems of Polynomials
Arising in Engineering and Science. World Scientiﬁc Press (2005)
28. Stewart, G.W.: Perturbation theory for the singular value decomposition. In: SVD
and Signal processing, II: Algorithms, Analysis and Applications, pp. 99–109. Else-
vier (1990)
29. Sommese, A.J., Verschelde, J., Wampler, C.W.: Introduction to numerical algebraic
geometry. In: Bronstein, M., et al. (eds.) Solving Polynomial Equations. AACIM,
vol. 14, pp. 339–392. Springer, Heidelberg (2005). doi:10.1007/3-540-27357-3 8
30. Sternberg, S.: Lectures on Diﬀerential Geometry. Prentice-Hall, Englewood Cliﬀs
(1964)
31. Wu, W., Reid, G.: Finding points on real solution components and applications to
diﬀerential polynomial systems. In: ISSAC, pp. 339–346 (2013)
32. Wu, W., Reid, G., Feng, Y.: Computing real witness points of positive dimensional
polynomial systems. Accepted by Theoretical Computer Sciences (2017). http://
doi.org/10.1016/j.tcs.2017.03.035
33. Yang, Z., Zhi, L., Zhu, Y.: Veriﬁed error bounds for real solutions of positive-
dimensional polynomial systems. In: ISSAC, pp. 371–378 (2013)

Computing Multiple Zeros of Polynomial
Systems: Case of Breadth One
(Invited Talk)
Lihong Zhi1,2(B)
1 Key Laboratory of Mathematics Mechanization,
Academy of Mathematics and System Sciences, Beijing, China
2 School of Mathematical Sciences, University of Chinese Academy of Sciences,
Beijing 100190, China
lzhi@mmrc.iss.ac.cn
Abstract. Given a polynomial system f with a multiple zero x whose
Jacobian matrix at x has corank one, we show how to compute the
multiplicity structure of x and the lower bound on the minimal distance
between the multiple zero x and other zeros of f. If x is only given with
limited accuracy, we give a numerical criterion to guarantee that f has
μ zeros (counting multiplicities) in a small ball around x. Moreover, we
also show how to compute veriﬁed and narrow error bounds such that a
slightly perturbed system is guaranteed to possess an isolated breadth-
one singular solution within computed error bounds. Finally, we present
modiﬁed Newton iterations and show that they converge quadratically
if x is close to an isolated exact singular solution of f. This is joint work
with Zhiwei Hao, Wenrong Jiang, Nan Li.
1
Introduction
Let If be an ideal generated by polynomials f = {f1, . . . , fn}, where fi ∈
C[X1, . . . , Xn]. An isolated zero of multiplicity μ for f is a point x ∈Cn such that
1. f(x) = 0,
2. there exists a ball B(x, r) of radius r > 0 such that B(x, r) ∩f −1(0) = {x},
3. μ = dim(C[X]/Qf,x),
where
B(x, r) := {y ∈Cn : ∥y −x∥< r},
and Qf,x is a primary component of the ideal If whose associate prime is
mx = (X1 −x1, . . . , Xn −xn).
This research was supported in part by the National Key Research Project of China
2016YFB0200504 (Zhi) and the National Natural Science Foundation of China under
Grants 11571350 (Zhi).
c
⃝Springer International Publishing AG 2017
V.P. Gerdt et al. (Eds.): CASC 2017, LNCS 10490, pp. 392–405, 2017.
DOI: 10.1007/978-3-319-66320-3 28

Computing Multiple Zeros of Polynomial Systems: Case of Breadth One
393
Let dα
x : C[X] →C denote the diﬀerential functional deﬁned by
dα
x (g) =
1
α1! · · · αn! ·
∂|α|g
∂xα1
1 · · · ∂xαn
n
(x),
∀g ∈C[X],
(1)
where x ∈Cn and α = [α1, . . . , αn] ∈Nn. We have
dα
x

(X −x)β
=

1, if α = β,
0, otherwise.
(2)
The local dual space of If at a given isolated singular solution x is a subspace
Df,x of Dx = spanC{dα
x} such that
Df,x = {Λ ∈Dx | Λ(g) = 0, ∀g ∈If}.
(3)
When the evaluation point x is clear from the context, we write dα1
1 · · · dαn
n
instead of dα
x for simplicity.
Let D(k)
f,x be the subspace of Df,x with diﬀerential functionals of orders
bounded by k, we deﬁne
1. breadth κ = dim

D(1)
f,x \ D(0)
f,x

,
2. depth ρ = min

k | dim

D(k+1)
f,x
\ D(k)
f,x

= 0

,
3. multiplicity μ = dim

D(ρ)
f,x

.
If x is an isolated singular solution of f, then 1 ≤κ ≤n and ρ < μ < ∞.
We recall α-theory below according to [1] and refer to [16,37–41,43] for more
details.
Let Df(x) denote the Jacobian matrix of f at x. Suppose Df(x) is invertible,
x is called a simple (regular) zero of f. The Newton’s iteration is deﬁned by
Nf(x) = x −Df(x)−1f(x).
(4)
Shub and Smale [37] deﬁned
γ(f, x) = sup
k≥2
				Df(x)−1 · Dkf(x)
k!
				
1
k−1
,
(5)
where Dkf denotes the k-th derivative of f which is a symmetric tensor whose
components are the partial derivatives of f of order k, ∥· ∥denotes the classical
operator norm.
According to [1, Theorem 1], if
∥z −x∥≤3 −
√
7
2γ(f, x),
(6)
then Newton’s iterations starting at z will converge quadratically to the simple
zero x.
If y is another zero of f, according to [1, Corollary 1], we have
∥y −x∥≥5 −
√
17
4γ(f, x) ,
(7)
which separates the simple zero x from other zeros of f.

394
L. Zhi
Furthermore, according to [1, Theorem 2], if only a system f and a point x
are given such that
α(f, x) ≤13 −3
√
17
4
≈0.157671,
(8)
where α(f, x) = β(f, x)γ(f, x) and
β(f, x) = ∥x −Nf(x)∥= ∥Df(x)−1f(x)∥,
then Newton’s iterations starting at x will converge quadratically to a simple
zero ξ of f and
∥x −ξ∥≤2β(f, x).
It is a challenge to extend α-theory for polynomial systems with singular solu-
tions. When Df(x) is not invertible, many modiﬁcations of Newton’s iteration
to restore the quadratic convergence for singular solutions have been proposed
in [2,6–8,12–14,29–33,36,46]. Recently, some symbolic-numeric methods based
on deﬂated systems have also been proposed for reﬁning approximate isolated
singular solutions to high accuracy [3–5,10,11,18–20,25]. For example, as shown
in [19], let r = rank(Df(x)), with probability one, there exists a unique vec-
tor λ = (λ1, λ2 . . . , λr+1)T such that (x, λ) is an isolated solution of a deﬂated
polynomial system, i.e.,
⎧
⎨
⎩
f(x) = 0,
Df(x)Bλ = 0,
hT λ = 1,
(9)
where B ∈Cn×(r+1) is a random matrix, h ∈Cr+1 is a random vector. If (x, λ)
is still a singular solution of (9), the deﬂation is repeated. Furthermore, they
proved that the number of deﬂations needed to derive a regular solution of an
augmented system is strictly less than the multiplicity of x. Dayton and Zeng
showed that the depth of Df,x is a tighter bound for the number of deﬂations [5].
In [44,45], we present a method based on the reduction to geometric invo-
lutive form to compute the primary component and a basis of the local dual
space of a polynomial system at an isolated singular solution. We also present
an algorithm based on correctly computed multiplicity structure such as index
and multiplicity at an approximate singular solution to restore the quadratic
convergence of Newton’s iterations.
In this paper, we introduce some recent contributions related to extend-
ing α-theory for polynomial systems with singular zeros satisfying f(x) = 0,
dim ker Df(x) = 1. It is also called breadth-one singular zero in [5] as
dim(D(k)
f,x \ D(k−1)
f,x
) = 1, k = 1 . . . , ρ, ρ = μ −1.
(10)
Therefore, the local dual space of If at x is
Df,x = spanC{Λ0, Λ1, . . . , Λμ−1},
where deg(Λk) = k and Λ0 = 1.

Computing Multiple Zeros of Polynomial Systems: Case of Breadth One
395
As pointed out in [11], the breath one case is the least degenerate one and
therefore most likely to be of practical signiﬁcance. Moreover, it is also the worst
case for the deﬂation method [5,19,29,30] since the deﬂation always terminates
at step μ −1. Hence the size of the matrices grows extremely fast with the
multiplicity.
2
Local Dual Space
Let us introduce a morphism Φσ : Dx →Dx which is an anti-diﬀerentiation
operator deﬁned by
Φσ(dα1
1 · · · dαn
n ) =

dα1
1 · · · dασ−1
σ
· · · dαn
n , if ασ > 0,
0,
otherwise.
Computing a closed basis of the local dual space is done essentially by matrix-
kernel computations based on the stability property of Df,x [26,28,42]:
∀Λ ∈D(k)
f,x, Φσ(Λ) ∈D(k−1)
f,x
,
σ = 1, . . . , n.
(11)
Let D(k)
f,x be the subspace of Df,x with diﬀerential functionals of orders
bounded by k. Let Ψσ : Dx →Dx be a diﬀerential operator deﬁned by
Ψσ(dα1
1 · · · dαn
n ) =
dασ+1
σ
· · · dαn
n , if α1 = · · · = ασ−1 = 0,
0,
otherwise.
We deal with multiple zeros satisfying f(x) = 0, dim ker Df(x) = 1. The
local dual space of If at a given isolated singular solution x is
Df,x = spanC{Λ0, Λ1, . . . , Λμ−1},
where deg(Λk) = k and Λ0 = 1.
As shown in [23, Theorem 3.4], suppose Λ1 = a1,1d1 + · · · + a1,ndn, without
loss of generality, we assume a1,1 = 1, ak,1 = 0, k = 2, . . . , n. Then for k =
2, . . . , μ −1, we have
Λk = Δk + ak,2d2 + · · · + ak,ndn,
(12)
where
Δk =
n

σ=1
Ψσ(a1,σΛk−1 + · · · + ak−1,σΛ1),
(13)
and ak,2, . . . , ak,n are determined by solving the linear system obtained from
setting Λk(fi) = 0, i = 1, . . . , n:
⎛
⎜
⎝
d2(f1) · · · dn(f1)
...
...
...
d2(fn) · · · dn(fn)
⎞
⎟
⎠
⎛
⎜
⎝
ak,2
...
ak,n
⎞
⎟
⎠= −
⎛
⎜
⎝
Δk(f1)
...
Δk(fn)
⎞
⎟
⎠.
(14)

396
L. Zhi
Deﬁnition 1 [15]. For a polynomial function f : Cn →Cn, suppose f(x) =
0, dim ker Df(x) = 1. Then Df(x) has a normalized form if
Df(x) =

0 D ˆf(x)
0
0

,
(15)
D ˆf(x) is the nonsingular Jacobian matrix of polynomials ˆf = {f1, . . . , fn−1}
with respect to variables X2, . . . , Xn.
If x is a multiple zero of multiplicity μ for f and Df(x) has the normalized
form (15), which is always possible to obtain by performing unitary transforma-
tions when dim ker Df(x) = 1, see [15, Sect. 2.3], then we have Δk(fn) = 0, for
k = 2, . . . , μ −1, Δμ(fn) ̸= 0, and the linear system (14) for getting the values
of ak,2, . . . , ak,n can be simpliﬁed to:
⎛
⎜
⎝
d2(f1)
· · ·
dn(f1)
...
...
...
d2(fn−1) · · · dn(fn−1)
⎞
⎟
⎠
⎛
⎜
⎝
ak,2
...
ak,n
⎞
⎟
⎠= −
⎛
⎜
⎝
Δk(f1)
...
Δk(fn−1)
⎞
⎟
⎠.
(16)
3
Local Separation Bound and Cluster Location
In [9], Dedieu and Shub gave quantitative results for simple double zeros satis-
fying f(x) = 0 and
(A) dim ker Df(x) = 1,
(B) D2f(x)(v, v) /∈imDf(x),
where ker Df(x) is spanned by a unit vector v ∈Cn. They generalized the
deﬁnition of γ in (5) to
γ2(f, x) = max

1, sup
k≥2
				A(f, x, v)−1 · Dkf(x)
k!
				
1
k−1 
,
(17)
where
A(f, x, v) = Df(x). + 1
2D2f(x)(v, Πv),
(18)
is a linear operator which is invertible at the simple double zero x, and Πv
denotes the Hermitian projection onto the subspace [v] ⊂Cn.
In [9, Theorem 1], Dedieu and Shub also presented a lower bound for sepa-
rating simple double zeros x from the other zeros y of f,
∥y −x∥≥
d
2γ2(f, x)2 ,
(19)
where d ≈0.2976 is a positive real root of

1 −d2 −2d

1 −d2 −d2 −d = 0.
(20)

Computing Multiple Zeros of Polynomial Systems: Case of Breadth One
397
In [9, Theorem 4], Dedieu and Shub showed that if the following criterion is
satisﬁed at a given point x and a given vector v
∥f(x)∥+ ∥Df(x)v∥
d
4γ2(f, x, v)2 <
d3
32γ4
2∥B(f, x, v)−1∥,
(21)
then f has two zeros in the ball of radius
d
4γ2(f, x)2 ,
(22)
around x. Let us set
B(f, x, v) = A(f, x, v) −L,
where L(v) = Df(x)v, L(w) = 0 for w ∈v⊥, and
γ2(f, x) = max

1, sup
k≥2
				B(f, x, v)−1 · Dkf(x)
k!
				
1
k−1 
.
(23)
Based on the multiplicity structure of the singular zero x of f computed in
the last section, we generalize Dedieu and Shub’s results to multiple zeros with
arbitrary large multiplicity.
Let f : Cn →Cn, and x be a singular zero of f of multiplicity μ, where
Df(x) has the normalized form Df(x) =

0 D ˆf(x)
0
0

, D ˆf(x) is invertible and
Δk(fn) = 0, for k = 2, . . . , μ −1,
Δμ(fn) ̸= 0.
(24)
Let y be another vector in Cn and y ̸= x. Recall that ϕ = dP (v, y −x),
v = (1, 0, . . . , 0)T and w = x −y = (ζ, η2, . . . , ηn)T , η = (η2, . . . , ηn)T , then we
have |ζ| = ∥w∥sin ϕ, ∥η∥= ∥w∥cos ϕ. Let
A =
√
2D ˆf(x)
0
0
1
√
2Δμ(fn)

,
and γμ = max(ˆγμ, γμ,n), where
ˆγμ = ˆγμ(f, x) = max
⎛
⎝1, sup
k≥2
					D ˆf(x)−1 Dk ˆf(x)
k!
					
1
k−1 ⎞
⎠,
(25)
where Dk ˆf(x) for k ≥2 denote the partial derivatives of ˆf of order k with
respect to X1, X2, . . . , Xn evaluated at x, and
γμ,n = γμ,n(f, x) =

1, sup
k≥2
				
1
Δμ(fn) · Dkfn(x)
k!
				
1
k−1 
,
(26)

398
L. Zhi
Deﬁnition 2 [15, Deﬁntion 3]. We deﬁne d = min(d1, d2, d3), where
d1 =

1
c2
μ−1,1 + 1, d2 =

1
μ −1,
and d3 is the smallest positive real root of the polynomial
p(d) = (1 −d2)
μ
2 −

i+j=μ,j>0
ci,jd(1 −d2)
i
2 dj−1
(27)
−d
⎛
⎝

1≤i≤μ−2
ti,0 +

1≤i+j≤μ−2,j>0
ti,j(1 −d2)
i
2 dj + 1
⎞
⎠,
where ci,j and ti,j can be obtained by the method given in [15, Case 2].
Theorem 1 [15, Theorem 5]. Let x be a multiple zero of f of multiplicity μ,
dim ker Df(x) = 1, and y be another zero of f, then
∥y −x∥≥
d
2γμ
μ
.
Remark 1. For μ = 2, we have [15, Sect. 3.3]
p(d) = 1 −2d2 −2d

1 −d2 −d.
(28)
The smallest positive real root of p(d) is
d ≈0.2865.
For μ = 3, we have [15, Lemma 3]
p(d) = (1 −2d −8d2)

1 −d2 −9d −d2 + 6d3.
(29)
The smallest positive root of p(d) is
d ≈0.08507.
Theorem 2 [15, Theorem 8]. Given f : Cn →Cn, x ∈Cn, such that D ˆf(x) is
invertible, and Δμ(fn) ̸= 0. Let
H1 =

∂ˆ
f(x)
∂X1
0
∂fn(x)
∂X1
∂fn(x)
∂ˆ
X

,
Hk =
⎛
⎜
⎝

0
0
Δk(fn) 0

0n × · · · × n



k
×(n −1)
⎞
⎟
⎠, 2 ≤k ≤μ −1,

Computing Multiple Zeros of Polynomial Systems: Case of Breadth One
399
and polynomials
g(X) = f(X) −f(x) −

1≤k≤μ−1
Hk(X −x)k.
Let γμ = γμ(g, x), if
∥f(x)∥+

1≤k≤μ−1
∥Hk∥
 d
4γμ
μ
k
<
dμ+1
2 (4γμ
μ)μ ∥A−1∥,
(30)
then f has μ zeros (counting multiplicities) in the ball of radius
d
4γμ
μ around x.
4
Veriﬁed Error Bound
Let IR be the set of real intervals, and let IRn and IRn×n be the set of real
interval vectors and real interval matrices, respectively. Standard veriﬁcation
methods for nonlinear systems are based on the following theorem.
Theorem 1 [17,27,34]. Let f : Rn →Rn be a system of nonlinear equations.
Suppose x ∈Rn, X ∈IRn with 0 ∈X and R ∈Rn×n are given. Let M ∈IRn×n
be given such that
{Dfi(y) : y ∈x + X} ⊆Mi,:, i = 1, . . . , n.
(31)
Denote by In the n × n identity matrix and assume
−Rf(x) + (In −RM)X ⊆int(X).
(32)
Then there is a unique ˜x ∈x + X satisfying f(˜x) = 0. Moreover, every matrix
˜
M ∈M is nonsingular. In particular, the Jacobian matrix Df(˜x) is nonsingular.
Theorem 1 is restricted to verifying the existence of a simple solution of a
square and regular system. Notice that Theorem 1 is valid over complex numbers
with the necessary modiﬁcations. In [35], by introducing a smoothing parameter,
Rump and Graillat developed a veriﬁcation method for computing veriﬁed and
narrow error bounds, such that a slightly perturbed system is proved to possess
a double root within computed error bounds.
In [23], by adding a univariate polynomial in one selected variable with some
smoothing parameters to one selected equation of the original system, we gen-
eralized the algorithm in [35] to compute guaranteed error bounds such that
a slightly perturbed system is proved to have a breadth-one isolated singular
solution within computed error bounds.
For a polynomial function f : Cn →Cn, where fi ∈C[X1, . . . , Xn], and
suppose x is a zero of f of multiplicity μ and satisfying dim ker Df(x) = 1.
Suppose the i-th column of Df(x) can be written as a linear combination of the
other n −1 columns, then we choose xi as the variable. Similarly, suppose the
j-th row of Df(x) can be written as a linear combination of the other n −1

400
L. Zhi
linearly independent rows, then we add the perturbed univariate polynomial in
xi to fj. Finally, we permute
x1 ↔xi and f1 ↔fj
to construct a deﬂated system below.
We introduce μ −1 smoothing parameters b0, b1, . . . , bμ−2 and construct a
deﬂated system G(X, b, a) with μn variables and μn equations:
G(X, b, a) =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
F1(X, b) = f(X) −
μ−2
ν=0
bνxν
1
ν!

e1
F2(X, b, a1)
F3(X, b, a1, a2)
...
Fμ(X, b, a1, . . . , aμ−1)
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
(33)
where e1 = (1, 0, . . . , 0)T , b = (b0, b1, . . . , bμ−2), a = (a1, a2, . . . , aμ−1), a1 =
(1, a1,2, . . . , a1,n)T , ak = (0, ak,2, . . . , ak,n)T for 1 < k ≤μ, and
Fk(X, b, a1, . . . , ak−1) = Lk−1(F1),
(34)
where Lk are diﬀerentiation operators corresponding to Λk deﬁned by (12).
Theorem 2 [23, Theorem 4.3]. Suppose G(x,˜b, ˜a) = 0. If the Jacobian matrix
of the deﬂated polynomial system G(X, b, a) at (x,˜b, ˜a) is nonsingular, then x
is an isolated root of the perturbed polynomial system F(X) = F1(X,˜b) with
multiplicity μ and the corank of DF(x) is one.
Theorem 3 [23, Theorem 4.5]. Suppose Theorem 1 is applicable to G(X, b, a)
in (33) and yields inclusions for x, ˜b and ˜a such that G(x,˜b, ˜a) = 0. Then x is
an isolated breadth-one root of F(X) = F1(X,˜b) with multiplicity μ.
5
Modiﬁed Newton Iterations
In [22], we presented a symbolic-numeric method to reﬁne an approximate iso-
lated singular solution ˜x = (x1, . . . , xn) of a polynomial system f = {f1, . . . , fn}
when the Jacobian matrix of f evaluated at ˜x has corank one approximately.
Our approach is based on the regularized Newton iteration and the computa-
tion of diﬀerential conditions satisﬁed at the approximate singular solution. The
size of matrices involved in our algorithm is bounded by n × n. The algorithm
converges quadratically if ˜x is close to the isolated exact singular solution of f.
Theorem 4 [22, Theorem 3.16]. If the Jacobian matrix of f evaluated at x has
corank one and the approximate singular solution ˜x of f satisfying
∥˜x −x∥= ε ≪1,
where the positive number ϵ is small enough such that there are no other solutions
of f nearby, then the reﬁned singular solution ˜x returned by Algorithm 1 satisﬁes
∥Nf(˜x) −x∥= O(ε2).

Computing Multiple Zeros of Polynomial Systems: Case of Breadth One
401
Algorithm 1. Modiﬁed Newton’s Iterations for Breadth-one Multiple Zero
Input:
f: a polynomial system;
˜x: an approximate singular zero of f;
μ: the multiplicity
Output:
Nf(˜x): a reﬁned solution after one iteration;
1: solve the regularized least squares problem
(Df(˜x)∗Df(˜x) + σnIn)˜y = Df(˜x)∗b,
where b = −f(˜x), In is the n × n identity matrix and σn is the smallest singular
value of Df(˜x);
2: compute the singular value decomposition of Df(˜x + ˜y) = U · Σ · V ∗, let
g(X) = f(W · X),
W = (vn, v1, . . . , vn−1),
and set ˜z ←W ∗(˜x + ˜y);
3: construct Δµ and a closed approximate basis of the local dual space
Dg,˜z = Span(Λ0, Λ1, . . . , Λµ−1),
by Algorithm MultiplicityStructureBreadthOneNumeric in [21];
4: solve the linear system

Δµ(g), ∂g(˜z)
∂z2 , . . . , ∂g(˜z)
∂zn

δ = −Λµ−1(g)
5: update the zero of g
˜z1 ←˜z1 + δ1
μ ,
˜zi ←˜zi, 2 ≤i ≤n
and
Nf(˜x) ←W · ˜z.
The proof of Theorem 4 in [22] is based on studying zeros of deﬂated sys-
tems. It is diﬃcult to quantify the quadratical convergence of Algorithm 1. In
[15], we present a new algorithm for reﬁning an approximate singular zero whose
Jacobian matrix has corank one. The main idea is to perform the unitary trans-
formations to both variables and equations deﬁned at the approximate singular
solutions, then deﬁne the modiﬁed Newton’s iteration which are very similar to
Step 4 in Algorithm 1.
Theorem 3. Given an approximate zero z of a polynomial system f associated
to a multiple zero ξ of multiplicity μ and satisfying f(ξ) = 0, dim ker Df(ξ) = 1.
Suppose
ˆγμ(f, z)∥z −ξ∥< 1
2,

402
L. Zhi
Algorithm 2. Modiﬁed Newton’s Iteration for Breadth-one Multiple Zeros
Input:
f: a polynomial system;
z: an approximate singular zero of f;
μ: the multiplicity;
Output:
Nf(z): a reﬁned solution after one iteration;
1: compute the singular value decomposition
Df(z) = U ·
 Σn−1 0
0
σn

· V ∗, W† = (vn, v1, . . . , vn−1);
2: perform the unitary transformations to equations and variables
f(X) ←U ∗· f(W† · X),
z ←W ∗
† z;
3: update the last n −1 elements of the approximate zero
N1( ˆf, ˆz) ←ˆz −D ˆf(z)−1 ˆf(z),
y = (y1, ˆy) ←(z1, N1( ˆf, ˆz));
4: compute the singular value decomposition
Df(y) = U ·
 Σn−1 0
0
σn

· V ∗, W‡ = (vn, v1, . . . , vn−1);
5: perform the unitary transformations to equations and variables:
g(X) ←U ∗· f(W‡ · X),
w = (w1, ˆw) ←W ∗
‡ y;
6: update the ﬁrst element of the approximate zero
N2(gn, w) ←w1 −1
μΔµ(gn)−1Δµ−1(gn),
x = (x1, ˆx) ←(N2(gn, w), ˆw);
7: update the zero of f
Nf(z) ←W† · W‡ · x.
where ˆγμ(f, z) is deﬁned by (25), then the reﬁned singular solution Nf(z)
returned by Algorithm 2 satisﬁes
∥Nf(z) −ξ∥= O(∥z −ξ∥2).
(35)
In [15, Theorem 12], we give a quantiﬁed quadratic convergence proof of the
Algorithm 2 for simple triple zeros. There is no signiﬁcant obstacle to extend
the proof to multiple zeros of higher multiplicities. However, the computation
will become more complicated.
Theorem 4 [15, Theorem 12]. Given an approximate zero z of a system f
associated to a simple triple zero ξ of multiplicity 3 and satisfying f(ξ) = 0,
dim ker Df(ξ) = 1. Let u = max{γ3(f, ξ)3∥ξ −z∥, Lγ3(f, ξ)2∥ξ −z∥}, where L
is the Lipschitz constant of the function Df(X).

Computing Multiple Zeros of Polynomial Systems: Case of Breadth One
403
(1) If u < u3 ≈0.0137,
then the output of Algorithm 2 satisﬁes:
∥Nf(z) −ξ∥< ∥z −ξ∥.
(2) If u < u′
3 ≈0.0098 then after k times of iteration we have
		N k
f (z) −ξ
		 <
1
2
2k−1
∥z −ξ∥.
6
Conclusion
The Maple code of algorithms mentioned in the paper and test results are avail-
able http://www.mmrc.iss.ac.cn/∼lzhi/Research/hybrid.
Although the algorithms and proofs of quadratic convergence given in the
paper are for polynomial systems with exact multiple zeros, examples are given
to demonstrate that our algorithms are also applicable to analytic systems and
polynomial systems with a cluster of simple roots.
References
1. Blum, L., Cucker, F., Shub, M., Smale, S.: Complexity and Real Computation.
Springer, New York (1998)
2. Chen, X., Nashed, Z., Qi, L.: Convergence of Newton’s method for singular smooth
and nonsmooth equations using adaptive outer inverses. SIAM J. Optim. 7(2),
445–462 (1997)
3. Corless, R.M., Gianni, P.M., Trager, B.M.: A reordered Schur factorization method
for zero-dimensional polynomial systems with multiple roots. In: K¨uchlin, W.W.
(ed) Proceedings of ISSAC 1997, pp. 133–140. ACM, New York (1997)
4. Dayton, B., Li, T., Zeng, Z.: Multiple zeros of nonlinear systems. Math. Comput.
80, 2143–2168 (2011)
5. Dayton, B., Zeng, Z.: Computing the multiplicity structure in solving polynomial
systems. In: Kauers, M. (ed) Proceedings of ISSAC 2005, pp. 116–123. ACM, New
York (2005)
6. Decker, D.W., Kelley, C.T.: Newton’s method at singular points I. SIAM J. Numer.
Anal. 17, 66–70 (1980)
7. Decker, D.W., Kelley, C.T.: Newton’s method at singular points II. SIAM J.
Numer. Anal. 17, 465–471 (1980)
8. Decker, D.W., Kelley, C.T.: Convergence acceleration for Newton’s method at sin-
gular points. SIAM J. Numer. Anal. 19, 219–229 (1982)
9. Dedieu, J.P., Shub, M.: On simple double zeros and badly conditioned zeros of
analytic functions of n variables. Math. Comput. 70(233), 319–327 (2001)
10. Giusti, M., Lecerf, G., Salvy, B., Yakoubsohn, J.C.: On location and approximation
of clusters of zeros of analytic functions. Found. Comput. Math. 5(3), 257–311
(2005)
11. Giusti, M., Lecerf, G., Salvy, B., Yakoubsohn, J.C.: On location and approximation
of clusters of zeros: case of embedding dimension one. Found. Comput. Math. 7(1),
1–58 (2007)

404
L. Zhi
12. Griewank, A.: On solving nonlinear equations with simple singularities or nearly
singular solutions. SIAM Rev. 27(4), 537–563 (1985)
13. Griewank, A.: Analysis and modiﬁcation of Newton’s method at singularities.
Australian National University, thesis (1980)
14. Griewank, A., Osborne, M.R.: Newton’s method for singular problems when the
dimension of the null space is >1. SIAM J. Numer. Anal. 18, 145–149 (1981)
15. Hao, Z., Jiang, W., Li, N., Zhi, L.: Computing simple multiple zeros of polynomial
systems (2017). https://www.arxiv.org/pdf/1703.03981.pdf
16. Hauenstein, J.D., Sottile, F.: Algorithm 921: AlphaCertiﬁed: certifying solutions
to polynomial systems. ACM Trans. Math. Softw. 38(4), 28:1–28:20 (2012)
17. Krawczyk, R.: Newton-Algorithmen zur Bestimmung von Nullstellen mit Fehler-
schranken. Computing 4(3), 187–201 (1969)
18. Lecerf, G.: Quadratic Newton iteration for systems with multiplicity. Found. Com-
put. Math. 2(3), 247–293 (2002)
19. Leykin, A., Verschelde, J., Zhao, A.: Newton’s method with deﬂation for isolated
singularities of polynomial systems. Theoret. Comput. Sci. 359(1), 111–122 (2006)
20. Leykin, A., Verschelde, J., Zhao, A.: Higher-order deﬂation for polynomial systems
with isolated singular solutions. In: Dickenstein, A., Schreyer, F.O., Sommese, A.J.
(eds.) Algorithms in Algebraic Geometry. IMA, vol. 146, pp. 79–97. Springer, New
York (2008)
21. Li, N., Zhi, L.: Compute the multiplicity structure of an isolated singular solution:
case of breadth one. J. Symb. Comput. 47, 700–710 (2012)
22. Li, N., Zhi, L.: Computing isolated singular solutions of polynomial systems: case
of breadth one. SIAM J. Numer. Anal. 50(1), 354–372 (2012)
23. Li, N., Zhi, L.: Veriﬁed error bounds for isolated singular solutions of polynomial
systems: case of breadth one. Theoret. Comput. Sci. 479, 163–173 (2013)
24. Li, N., Zhi, L.: Veriﬁed error bounds for isolated singular solutions of polynomial
systems. SIAM J. Numer. Anal. 52(4), 1623–1640 (2014)
25. Mantzaﬂaris, A., Mourrain, B.: Deﬂation and certiﬁed isolation of singular zeros of
polynomial systems. In: Leykin, A. (ed.) Proceedings of ISSAC 2011, pp. 249–256.
ACM, New York (2011)
26. Marinari, M.G., Mora, T., M¨oller, H.M.: Gr¨obner duality and multiplicities in
polynomial system solving. In: Proceedings of ISSAC 1995, pp. 167–179. ACM,
New York (1995)
27. Moore, R.E.: A test for existence of solutions to nonlinear systems. SIAM J. Numer.
Anal. 14(4), 611–615 (1977)
28. Mourrain, B.: Isolated points, duality and residues. J. Pure Appl. Algebra 117,
469–493 (1996). 117
29. Ojika, T.: Modiﬁed deﬂation algorithm for the solution of singular problems. i.
a system of nonlinear algebraic equations. J. Math. Anal. Appl. 123(1), 199–221
(1987)
30. Ojika, T., Watanabe, S., Mitsui, T.: Deﬂation algorithm for the multiple roots of
a system of nonlinear equations. J. Math. Anal. Appl. 96(2), 463–479 (1983)
31. Rall, L.B.: Convergence of the Newton process to multiple solutions. Numer. Math.
9(1), 23–37 (1966)
32. Reddien, G.W.: On Newton’s method for singular problems. SIAM J. Numer. Anal.
15(5), 993–996 (1978)
33. Reddien, G.W.: Newton’s method and high order singularities. Comput. Math.
Appl. 5(2), 79–86 (1979)

Computing Multiple Zeros of Polynomial Systems: Case of Breadth One
405
34. Rump, S.M.: Solving algebraic problems with high accuracy. In: Proceedings of the
Symposium on A New Approach to Scientiﬁc Computation, pp. 51–120. Academic
Press Professional Inc., San Diego (1983)
35. Rump, S.M., Graillat, S.: Veriﬁed error bounds for multiple roots of systems of
nonlinear equations. Numer. Algorithms 54(3), 359–377 (2010)
36. Shen, Y.Q., Ypma, T.J.: Newton’s method for singular nonlinear equations using
approximate left and right nullspaces of the Jacobian. Appl. Numer. Math. 54(2),
256–265 (2005)
37. Shub, M., Smale, S.: Complexity of bezout’s theorem IV: probability of success;
extensions. SIAM J. Numer. Anal. 33(1), 128–148 (1996)
38. Shub, M., Smale, S.: Computational complexity: on the geometry of polynomials
and a theory of cost: I. Ann. Sci. ´Ec. Norm. Sup´er. 18(1), 107–142 (1985)
39. Shub, M., Smale, S.: Computational complexity: on the geometry of polynomials
and a theory of cost: II. SIAM J. Comput. 15(1), 145–161 (1986)
40. Smale, S.: The fundamental theorem of algebra and complexity theory. Bull. Amer.
Math. Soc. 4(1), 1–36 (1981)
41. Smale, S.: Newton’s method estimates from data at one point. In: Ewing, R.E.,
Gross, K.I., Martin, C.F. (eds.) The Merging of Disciplines: New Directions in
Pure, Applied, and Computational Mathematics. Springer, New York (1986)
42. Stetter, H.: Numerical Polynomial Algebra. SIAM, Philadelphia (2004)
43. Wang, X., Han, D.: On dominating sequence method in the point estimate and
smale theorem. Sci. China Ser. A 33(2), 135–144 (1990)
44. Wu, X., Zhi, L.: Computing the multiplicity structure from geometric involutive
form. In: Jeﬀrey, D. (ed) Proceedings of ISSAC 2008, pp. 325–332. ACM, New
York (2008)
45. Wu, X., Zhi, L.: Determining singular solutions of polynomial systems via symbolic-
numeric reduction to geometric involutive forms. J. Symb. Comput. 47(3), 227–238
(2012)
46. Yamamoto, N.: Regularization of solutions of nonlinear equations with singular
Jacobian matrices. J. Inf. Process. 7(1), 16–21 (1984)

Author Index
Abramov, S.A.
1
Banshchikov, Andrei V.
16
Briani, Matteo
27
Bruno, Alexander D.
40
Chen, Changbo
51, 377
Cheng, Jin-San
66
Chuluunbaatar, G.
134, 151
Chuluunbaatar, O.
134, 151
Cuyt, Annie
27
Derbov, V.L.
134, 151
Dong, Rina
77
Dou, Xiaojie
66
Edneral, Victor F.
40
England, Matthew
93
Epure, Raul
109
Errami, Hassan
93
Fan, W.L.
118
Feng, Yong
51
Gao, Xiao-Shan
183, 196
Gerdt, Vladimir P.
134, 151, 301
Góźdź, A.
134, 151
Grigoriev, Dima
93
Gusev, A.A.
134, 151
Gutnik, Sergey A.
167
Hu, Youren
183
Huang, Qiao-Long
196
Irtegov, Valentin
210
Jeffrey, D.J.
118
Jing, Rui-Juan
225, 242
Kozera, Ryszard
257
Lee, Wen-shin
27
Li, Zhe
272
Liu, Jiang
285
Lyakhov, Dmitry A.
301
Michels, Dominik L.
301
Moreno Maza, Marc
225, 242
Mou, Chenqi
77
Neiger, Vincent
313
Noakes, Lyle
257
Postma, Erik
118
Prokopenya, Alexander N.
329
Radulescu, Ovidiu
93
Rahkooy, Hamid
313
Reid, Greg
377
Ren, Yue
109
Romanovski, Valery G.
40
Sarychev, Vasily A.
167
Schönemann, Hans
109
Schost, Éric
313
Shapeev, Vasily P.
346
Sturm, Thomas
93
Titorenko, Tatyana
210
Vinitsky, S.I.
134, 151
Vorozhtsov, Evgenii V.
346
Wan, Baocheng
272
Wang, Yu
362
Weber, Andreas G.
93, 301
Wu, Wenyuan
51, 362, 377
Xia, Bican
362
Zhang, Shugong
272
Zhi, Lihong
392


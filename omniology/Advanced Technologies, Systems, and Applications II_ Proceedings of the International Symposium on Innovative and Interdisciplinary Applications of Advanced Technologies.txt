Advanced 
Technologies, 
Systems,
and Applications II
Mirsad Hadžikadić
Samir Avdaković Editors
Proceedings of the International
Symposium on Innovative and
Interdisciplinary Applications of
Advanced Technologies (IAT)
Lecture Notes in Networks and Systems 28

Lecture Notes in Networks and Systems
Volume 28
Series editor
Janusz Kacprzyk, Polish Academy of Sciences, Warsaw, Poland
e-mail: kacprzyk@ibspan.waw.pl

The series “Lecture Notes in Networks and Systems” publishes the latest
developments in Networks and Systems—quickly, informally and with high quality.
Original research reported in proceedings and post-proceedings represents the core
of LNNS.
Volumes published in LNNS embrace all aspects and subﬁelds of, as well as
new challenges in, Networks and Systems.
The series contains proceedings and edited volumes in systems and networks,
spanning the areas of Cyber-Physical Systems, Autonomous Systems, Sensor
Networks, Control Systems, Energy Systems, Automotive Systems, Biological
Systems, Vehicular Networking and Connected Vehicles, Aerospace Systems,
Automation, Manufacturing, Smart Grids, Nonlinear Systems, Power Systems,
Robotics, Social Systems, Economic Systems and other. Of particular value to both
the contributors and the readership are the short publication timeframe and the
world-wide distribution and exposure which enable both a wide and rapid
dissemination of research output.
The series covers the theory, applications, and perspectives on the state of the art
and future developments relevant to systems and networks, decision making, control,
complex processes and related areas, as embedded in the ﬁelds of interdisciplinary
and applied sciences, engineering, computer science, physics, economics, social, and
life sciences, as well as the paradigms and methodologies behind them.
Advisory Board
Fernando Gomide, Department of Computer Engineering and Automation—DCA, School of
Electrical and Computer Engineering—FEEC, University of Campinas—UNICAMP,
São Paulo, Brazil
e-mail: gomide@dca.fee.unicamp.br
Okyay Kaynak, Department of Electrical and Electronic Engineering, Bogazici University,
Istanbul, Turkey
e-mail: okyay.kaynak@boun.edu.tr
Derong Liu, Department of Electrical and Computer Engineering, University of Illinois
at Chicago, Chicago, USA and Institute of Automation, Chinese Academy of Sciences,
Beijing, China
e-mail: derong@uic.edu
Witold Pedrycz, Department of Electrical and Computer Engineering, University of Alberta,
Alberta, Canada and Systems Research Institute, Polish Academy of Sciences, Warsaw,
Poland
e-mail: wpedrycz@ualberta.ca
Marios M. Polycarpou, KIOS Research Center for Intelligent Systems and Networks,
Department of Electrical and Computer Engineering, University of Cyprus, Nicosia, Cyprus
e-mail: mpolycar@ucy.ac.cy
Imre J. Rudas, Óbuda University, Budapest Hungary
e-mail: rudas@uni-obuda.hu
Jun Wang, Department of Computer Science, City University of Hong Kong
Kowloon, Hong Kong
e-mail: jwang.cs@cityu.edu.hk
More information about this series at http://www.springer.com/series/15179

Mirsad Hadžikadić
• Samir Avdaković
Editors
Advanced Technologies,
Systems, and Applications II
Proceedings of the International Symposium
on Innovative and Interdisciplinary
Applications of Advanced Technologies (IAT)
123

Editors
Mirsad Hadžikadić
Data Science Initiative
UNC Charlotte
Charlotte
USA
Samir Avdaković
Faculty of Electrical Engineering
University of Sarajevo
Sarajevo
Bosnia and Herzegovina
ISSN 2367-3370
ISSN 2367-3389
(electronic)
Lecture Notes in Networks and Systems
ISBN 978-3-319-71320-5
ISBN 978-3-319-71321-2
(eBook)
https://doi.org/10.1007/978-3-319-71321-2
Library of Congress Control Number: 2017958735
© Springer International Publishing AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Contents
Part I
Advanced Electrical Power Systems
(Planning, Operation and Control)
Electric Energy Losses Estimation in Power Distribution
System—Tuzla Canton Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Šeila Gruhonjić Ferhatbegović, Izet Džananović, and Samir Avdaković
Application of Teager Energy Operator for the Power
System Fault Identiﬁcation and Localisation . . . . . . . . . . . . . . . . . . . .
18
Nejra Čišija-Kobilica and Samir Avdaković
Analysis and Control of DG Inﬂuence on Voltage Proﬁle
in Distribution Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
Mirza Šarić, Jasna Hivziefendić, and Lejla Bandić
Fuzzy Logic Based Approach for Faults Identiﬁcation
and Classiﬁcation in Medium Voltage Isolated Distribution
Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
Mirza Šarić, Tarik Hubana, and Elma Begić
Inﬂuence of Solar PVDG on Electrical Energy Losses in Low
Voltage Distribution Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
Edin Šemić, Mirza Šarić, and Tarik Hubana
Connecting a Group of Small Hydropower Plants on the Side
of Neretvica River to a Medium Voltage Distribution Grid . . . . . . . . .
67
Dino Macić and Mirza Šarić
Analysis of the Impact of Distributed Generation on Voltage
Proﬁles (Case Study Long Feeder 10 kV Grebak) . . . . . . . . . . . . . . . .
78
Aiša Ramović, Lejla Terzić, Adnan Bosović, and Mustafa Musić
Power System Fault Detection, Classiﬁcation and Location
using Artiﬁcial Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
Almin Karić, Tatjana Konjić, and Admir Jahić
v

Single Phase Fault Location in Distribution Network Based
on Wavelet Transform of Current Traveling Waves . . . . . . . . . . . . . .
102
Šeila Gruhonjić Ferhatbegović
The Analysis and Beneﬁts of the New TS 110/x KV Interpolation
on the Distribution Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
Jasmina Čučuković, Faruk Hidić, and Ismet Kulović
Determination of Static Voltage Load Characteristics
in BiH Electricity System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
Husnija Ferizović, Vojislav Pantić, Senad Hadžić,
and Semir Hadžimuratović
Application of Modern Solutions on Grounded Neutral Point
in Distribution Grid . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
Alija Jusić and Zijad Bajramović
Comparison of CFD and Linear Model When Calculating Maps
of Wind Potential at the Location with Complex Topography . . . . . . .
155
Dino Trešnjo, Alma Ademović Tahirović, Muris Torlak, Elma Redžić,
and Mustafa Musić
Multicriteria Decision Making Model for HPP
Alternative Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
170
Zedina Lavić and Sabina Dacić-Lepara
Power System Planning: Part I—Basic Principles . . . . . . . . . . . . . . . .
178
Armin Demir and Nasiha Hadžijahić
Power System Planning: Part II—Practical Applications . . . . . . . . . . .
189
Armin Demir and Nasiha Hadžijahić
Identiﬁcations of Power System Dominant Low-Frequency
Eletromechanical Oscillations Using Hilbert Marginal Spectrum . . . . .
203
Maja Muftić Dedović and Samir Avdaković
Part II
Computer Science
Implementation of ICT in Education . . . . . . . . . . . . . . . . . . . . . . . . . .
215
Amina Delić-Zimić and Naida Gadžo
Modern Teaching Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
223
Amina Delić-Zimić
Ferris Wheel Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
234
Dalila Isanovic
Inﬂuence of Schemaless Approach on Database Authorization . . . . . .
243
Dejan Radic
vi
Contents

Buffered Count-Min Sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
249
Ehsan Eydi, Dzejla Medjedovic, Emina Mekic, and Elmedin Selmanovic
The Relationship Between the Performance of Islamic
and Conventional Banks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
256
Emina Causevic
Stylometric Study of Ivo Andrić’s Short Stories . . . . . . . . . . . . . . . . . .
265
Edin Konjhodžić
Inﬂuence of Information Technology to Human Resources
Management: Key Trends in 21st Century. . . . . . . . . . . . . . . . . . . . . .
271
Ljubisa Micic and Veselin Radosavac
Science Battle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
282
Medina Krnic and Belma Ramic-Brkic
Constraint Satisfaction Problem: Professor Weekly Schedule . . . . . . .
290
Mirna Udovićić
Posture Activity Prediction Using Microsoft Azure . . . . . . . . . . . . . . .
299
Mirza Čurić and Jasmin Kevrić
Hybrid Comparative Predictive Modeling . . . . . . . . . . . . . . . . . . . . . .
307
Mohammad Asif Nawaz and Mirsad Hadzikadic
SSST-Cloud: Developing a Cloud System for a University . . . . . . . . . .
321
Tarik Catic and Belma Ramic-Brkic
DIY Smart Mirror . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
329
Sadeta Kulovic and Belma Ramic-Brkic
Farm: Serious Game for Addressing Child Obesity . . . . . . . . . . . . . . .
337
Sena Bajraktarević and Belma Ramić-Brkić
Part III
Power Quality
Application of EMD and STFT Methods in Analysis
of Energization of an Unloaded Overhead Line Under
Different Operating Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
349
Snežana Vujošević and Saša Mujović
Voltage Flickers as Voltage Quality Problem in Industrial Sector . . . .
363
Ivan Ramljak, Jurica Perko and Matej Znidarec
Power Quality Field Measurements on Photovoltaic System . . . . . . . .
375
Mia Lešić and Tatjana Konjić
Different Approaches for Analysis of Harmonics Impact
on the Transformer Losses and Life Expectancy . . . . . . . . . . . . . . . . .
392
Izudin Kapetanović, Jasna Hivziefendić, and Majda Tešanović
Contents
vii

Voltage Sag Propagation Caused by Faults in Medium
Voltage Distribution Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
409
Tarik Hubana, Elma Begić, and Mirza Šarić
Architecture of the Modern Power Quality Monitoring
Systems with Installation and Experiences Examples
from BiH, Croatia and Slovenia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
420
Ivan Vlahović and Ivo Novaković
Impact of Distributed Generation on Power Quality
in Medium Voltage Distribution Networks . . . . . . . . . . . . . . . . . . . . . .
433
Alija Jusić, Edin Jareb, and Zijad Bajramović
Part IV
Civil Engineering
3D Numerical Study of Sidewall Friction Inﬂuence
on Small Scale Reinforced Earth Wall Behavior . . . . . . . . . . . . . . . . .
449
Adis Skejić, Mladen Kapor, Senad Medić, and Đenari Čerimagić
3D Modeling and Nonlinear Analysis Stability of CWR Tracks . . . . . .
458
Sanjin Albinovic, Samir Dolarevic, and Dusan Marusic
A Study of Speed on Two-Lane Roadways. . . . . . . . . . . . . . . . . . . . . .
470
Mehmed Bublin
Adriatic-Ionian Road: Economic and Social Potential
of the Adriatic-Ionian Region . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
479
Predrag Sarkinovic and Naser Morina
Browninfo Methodology and Software for Development
of Interactive Brownﬁeld Databases . . . . . . . . . . . . . . . . . . . . . . . . . . .
484
Tijana Vujičić, Dijana Simonović, Aleksandra Đukić, and Maksim Šestić
Rehabilitation of the Urban Road and Pristina A.B Piles. . . . . . . . . . .
503
Bujar Emra, Naser Morin, Mirsad Tarić, and Mirza Hadžimujović
Analysis of Economic Feasibility and Usefulness
of Asphalt Mixtures of Recycled Asphalt in Relation
to the New Ones . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
513
Muamer Dubravac, Edis Softić, and Zlatan Talić
Seismic Vulnerability, Damage and Strengthening Evaluation
of Historical Buildings in Bosnia and Herzegovina. . . . . . . . . . . . . . . .
524
Mustafa Hrasnica
Analysis of Relations Between Freeway Geometry and Trafﬁc
Characteristics on Trafﬁc Accidents . . . . . . . . . . . . . . . . . . . . . . . . . . .
539
Marina Milenković and Drazenko Glavić
viii
Contents

Artiﬁcial Neural Networks Application in the Backcalculation
Process of Flexible Pavement Layers Elasticity Modulus . . . . . . . . . . .
549
Ammar Saric and Mirza Pozder
BIM Project Execution Planning Suited for Road Infrastructure
Pilot Project in Bosnia and Herzegovina . . . . . . . . . . . . . . . . . . . . . . .
560
Saša Džumhur, Žanesa Ljevo, and Jasmina Marić
Demographic Analysis Using Modern GIS Software
Tools—Case Study of the Republic of Srpska (Bosnia
and Herzegovina) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
571
Nikolina Mijic and Jovo Ateljevic
Estimation of Peak Flood Discharge for an Ungauged
River and Application of 1D Hec-Ras Model in Design
of Water Levels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
592
Emina Hadžić, Ajla Mulaomorević-Šeta,
Hata Milišić, and Nerma Lazović
Experimental Study on Behavior of Reinforced Concrete
Beam Subjected to Cyclic Loading . . . . . . . . . . . . . . . . . . . . . . . . . . . .
605
Edhem Živalj, Asad Kadić, Senad Medić, and Muhamed Zlatar
Landﬁll Leachate Management—Control and Treatment . . . . . . . . . .
618
Amra Serdarevic
Modeling Strategies for Masonry Structures . . . . . . . . . . . . . . . . . . . .
633
Senad Medić and Mustafa Hrasnica
Modern Geodetic Technologies As a Basis of the Design
and Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
645
Marina Davidović and Tatjana Kuzmić
Nonlinear Analysis of Hyperelastic Membrane . . . . . . . . . . . . . . . . . . .
661
Rasim Šehagić and Senad Medić
Quality Factors of Process and Products in Construction Projects . . . .
671
Žanesa Ljevo, Saša Džumhur, and Selena Grizić
Static and Dynamic Indicators for Composite Bridges . . . . . . . . . . . . .
681
Naida Ademovic
The Criteria for the Control of Condition of Railway
Lines in FB&H . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
691
Mirna Hebib-Albinovic and Sanjin Albinovic
The New Topographic Information System and Establishing
the Basic Topographic Database of the Federation
of Bosnia and Herzegovina . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
701
Slobodanka Kljucanin
Contents
ix

The Regression Model for Assignment of Diverted Trafﬁc
to Planned Bypass Road . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
711
Suada Dzebo
Transient Vibrations of Railway Track Elements
and the Inﬂuence of Support Conditions . . . . . . . . . . . . . . . . . . . . . . .
724
Emina Balic and Ciaran McNally
Part V
Information and Communication Technologies
Smart Parking System Based on Arduino SD Card
Ajax Web Server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
741
Edin Mujčić, Una Drakulić, and Merisa Škrgić
Implementation of Audiological Measurements at Persons
with Hearing Impairment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
751
Ferid Soﬁtć and Jasna Čošabić
Analysis of Techno-Economic Proﬁtability on the Example
of Construction of an Optical Suburban Access Network
in Srebrenica . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
759
Mujo Hohzic, Anis Maslo, and Evin Skaljo
Synchronization Between Arduino Based Appliance
and MATLAB Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
768
Adnan Felić and Edin Mujčić
Transmission of Two Optical Signals Through the Fibber
in Opposite Directions Using PLC Splitters—Practical
Measurements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
776
Mujo Hodzic, Edvin Skaljo, Nermin Suljanovic, and Aljo Mujcic
Soft to Hard Data Transformation Using Uncertainty
Balance Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
785
Migdat Hodzic
Programming and Experimental Analysis of MELFA
RV-2SDB Robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
810
Edin Mujčić, Sabina Lonić, and Mersa Muminović
A Customizable Embedded WebRTC Communication System . . . . . .
819
Edin Pjanić and Sanjin Lišić
Part VI
Mechatronics, Robotics and Embedded Systems
BCIs for Electric Wheelchair . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
833
Dalibor Đumić and Jasmin Kevrić
Brainiac’s Arm—Robotic Arm Controlled by Human Brain . . . . . . . .
848
Dalibor Đumić, Mehmed Đug, and Jasmin Kevrić
x
Contents

Identiﬁcation of Parameters for Robot PUMA 560 . . . . . . . . . . . . . . .
860
Dejan Jokić and Slobodan Lubura
System for Distributed Measurement of Ambient
Conditions in Homes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
868
Kenan Husić and Tarik Uzunović
Automatic SAR Target Recognition and Pose Estimation.
Part 1. Geometric Methods for Pose Estimation. . . . . . . . . . . . . . . . . .
876
Tarik Namas and Migdat Hodžić
Automatic SAR Target Recognition and Pose Estimation.
Part 2. Statistical Methods for Target Recognition . . . . . . . . . . . . . . . .
901
Migdat Hodžić and Tarik Namas
Part VII
BEM/MRM Formulation for Engineering Applications
Numerical Analysis of Screw Compressor Rotor and Casing
Deformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
933
Ermin Husak, Ahmed Kovacevic, and Sham Rane
Electric Field Calculation on Surface of High-Voltage
Transmission Line Conductors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
941
A. Carsimamovic, A. Mujezinovic, S. Carsimamovic,
Z. Bajramovic, and M. Kosarac
Calculation and Measurement Analysis of Transformer
Station Low-Frequency Electromagnetic Fields
in the Framework of Legislation on Protection from
Non-ionizing Radiation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
952
Hidajet Salkić, Adnan Muharemović, and Nerdina Mehinović
On the Use of Boundary Element Method for Cathodic
Protection System Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
967
Adnan Mujezinović, Sanja Martinez, and Slobodan Milojković
Part VIII
Robotics and Biomedical Engineering
Research and Development of New Generation Service
Robots for Medial Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
979
Isak Karabegovic
Rheological Models and Numerical Method for Simulation
of Blood Flow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
992
Ejub Džaferović, Muris Torlak, Almin Halač, and Amra Hasečić
Heuristic Optimization Methods in Industrial Robotics . . . . . . . . . . . .
1000
Ermin Husak and Isak Karabegović
Contents
xi

Impedance Control in the Rehabilitation Robotics . . . . . . . . . . . . . . . .
1007
Zlata Jelačić
Comparison of Numerical and Experimental Results
of Measuring Vehicle Movement Kinematic Parameters
Integrated into Advanced Mechatronic Systems . . . . . . . . . . . . . . . . . .
1026
Semir Mehremić and Isak Karabegović
Application of Mechatronic System in the Automation
Hydroforming Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1037
Edin Šemić, Safet Isić, and Edina Karabegović
Modeling of Mechatronic Systems of Robot Fingers . . . . . . . . . . . . . .
1044
Sanela Hrnjica
Application of Service Robots for Disinfection
in Medical Institutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1056
Aladin Begić
Detection of Parkinson’s Disease by Voice Signal . . . . . . . . . . . . . . . . .
1066
Fatima Mašić, Mehmed Đug, Jasna Nuhić, and Jasmin Kevrić
Modeling of Sensors for a Mechatronic Systems
of an Robotic Arm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1074
Silmija Ferizović
Part IX
Mechanical Engineering
Modelling the Micro Coaxial Helicopter. . . . . . . . . . . . . . . . . . . . . . . .
1085
Želimir Husnić
Effects of Primary Measures in Combustion Chamber
on Co-ﬁring of Coal with Woody Biomass . . . . . . . . . . . . . . . . . . . . . .
1102
N. Hodžić, S. Metović, and A. Kazagić
A Heat Exchanger with Finned Tube and Phase-Change
Material for Thermal-Energy Storage: Effect of Gravity
and Orientation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1112
Muris Torlak and Nijaz Delalić
Using of Remote Sensing Methods in Wind Analysis
in Conditions of Local Wind Bora and Complexity of Terrain . . . . . .
1118
Elvir Zlomušica, Suad Zalihić, Miralem Čampara, and Ramiz Zaimović
Computational Study of Pulverized Coal Combustion
in Boiler—Unit 7 of TPP Kakanj . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1126
Adnan Ðugum and Kemal Hanjalić
xii
Contents

The Application of Advanced Technologies
in Industrial Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1137
Edin Ibrahimovic
The Causes and Consequences of Deﬁcit in Nominal
Temperature of Reheated Steam After Implementation
of Primary Measures for NOx Reduction on the Boiler OB-650 . . . . . .
1147
Nedim Ganibegović and Amel Mešić
Transition of a Conventional Power Utility to Achieve
2050 RES and Carbon Cut Targets—EPBIH Case Study . . . . . . . . . .
1156
A. Kazagić, M. Musić, E. Redžić, and A. Merzić
The Effects of the Measuring and Consumption
of Thermal Energy Based Billing System in District
Heating Systems of Bosnia and Herzegovina . . . . . . . . . . . . . . . . . . . .
1170
N. Harbaš
CFD as an Enginer’s Tool for Investigation
of Large-Scale-Flow-Phenomena “at Land, Sea and Air” . . . . . . . . . .
1179
M. Muhasilović, B. Širok, K. Ciahotny, and M. O. Deville
Smart LEG Control System Optimization . . . . . . . . . . . . . . . . . . . . . .
1189
Haris Dindo, Zelimir Husnic, Remzo Dedic, and Adisa Vucina
Author Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1201
Contents
xiii

Part I
Advanced Electrical Power Systems
(Planning, Operation and Control)

Electric Energy Losses Estimation in Power
Distribution System—Tuzla Canton Case
Study
Šeila Gruhonjić Ferhatbegović1(&), Izet Džananović1,
and Samir Avdaković2
1 PE Elektroprivreda B&H, Sarajevo, Bosnia and Herzegovina
s.gruhonjic@epbih.ba
2 Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and
Herzegovina
Abstract. The method for estimation of electrical energy losses in distribution
system from the place of taking electrical energy from network of transmission
company or other suppliers to the place of delivery to the end customers and
determination the structure of losses by voltage levels is presented. For part of
the distribution network for which the losses are calculated, constant losses, load
dependent losses and losses in components of electrodistribution network are
taken into account. Distribution losses by voltage levels is made by balancing
the total electrical energy losses and losses estimated by method of stationary
load ﬂow calculations on medium-voltage (MV) of certain electrodistribution
network. The presented method is very practical and by its application the parts
of electrodistribution system with increased losses are located with high relia-
bility. Results of analysis of electrical energy losses, which is done by presented
method, can be taken as a reliable basis for taking action in the aim of reducing
total losses in electrodistribution network. The results of practical analysis are
presented for the area of Tuzla Canton—Bosnia and Herzegovina.
1
Introduction
The problem of electrical power and energy losses in electrodistribution systems is
continuing and relevant topic for companies engaged in distribution of electrical
energy. The electrical power losses respectively electrical energy losses are presented
as one of the key indicator of business efﬁciency and quality of the business processes
of electrical energy distribution. The total losses of electrical energy in distribution
system are monitored on a monthly and annual basis and calculated as the difference
between total taken and delivered electrical energy. Technical (physical) and
non-technical (i.e. commercial) losses are included in the total losses. It is necessary to
determine the technical and commercial losses by analysis in the aim of making
business decisions which are intended to reduce the total amount of electrical energy
losses in the observed network. Technical losses in the network are calculated by using
appropriate computational method, but it should be noted that the estimation of losses
in the distribution network especially demanding because of the speciﬁc topology and a
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_1

large number of customers. In electrodistribution companies there are commonly used
computer programs for calculation of electrical power losses and methods for calcu-
lation of electrical losses has been developed too. Electric power distribution system,
for which the losses are calculated should be fully modeled by applying the methods
for modeling all elements of the system. Due to unavailability of time load diagram and
large number of customer connections, certain approximations often are applied in the
estimations of losses in the aim of obtaining a more reliable results from the available
data. Generally, in the literature a large number of paper dealing with domain of losses
in electrodistribution systems can be found [1–6].
The current method of monitoring losses in the actual electrodistribution (in the part
relating to the Tuzla Canton) are described in the paper and the procedure of estimation
technical losses of electrical power and electrical energy in 10 and 20 kV network,
transformer 10/0,4 and 20/0,4 kV are shown. For the estimated technical losses of
electrical power and electrical energy on the observed network part, it is necessary to
determine structure of losses to the network elements and make separation of losses to
constant losses and load dependent losses. The presented methodology of losses esti-
mation is based on the existing resources in terms of available measurement of elec-
trical energy, calculations and network model. The purpose of the analysis of energy
losses is to locate places in the network where the losses are greater than the average.
After the analysis the corrective investment activities are undertaken, such as reha-
bilitation or reconstrucions of existing networks or building new parts of network, or
procedures and activities to optimize load distribution, balancing network and control
of connection places at the end customers are implemented in the existing network. In
the case of losses analysis, it operates with a large number of data, but the main
objective of analysis is to identify the parts of the network to which is necessary to
implement corrective actions and by priority of largest effect (economic and technical).
The estimation and monitoring methodology of losses are coordinated with busi-
ness policy and strategic business decision in the ﬁeld of electrical energy losses in
distribution activities of JP Elektroprivreda BiH d.d. Sarajevo (JP EP BiH). Distribu-
tion activity of electrical energy in JP EP BiH is realised in ﬁve Branches (Bihać,
Mostar, Sarajevo, Tuzla i Zenica) hereinafter described as the distribution parts or
abbreviated ED. Branches are divided into several business units of distribution (PJD),
which usually include the area of a municipality. A concrete example of analysis of
losses for the Branch Tuzla (ED Tuzla) and twelve associated PJD, is presented in the
paper, in 2015.
Generaly observed, the total losses of electrical energy in electrodistribution net-
work of JP EP BiH, are in the level of losses in electrodistribution networks in
neighboring countries or even lower. According to available reports, technical losses in
the distribution, according to the experience of the developed electric power company,
should not exceed 4,5% of taken electrical energy. The method of losses separation by
voltage levels can be carried out for distribution areas which belong to certain PJD,
supply transformer station (TS) or 10(20) kV feeder. The advantage of this method is
application to smaller distribution areas (PJD, TS or 10 and 20 kV outputs) to get a
more realistic presentation about causes of losses and easier determination of methods
to reduce them.
4
S. G. Ferhatbegović et al.

2
Methodology of Estimation and Monitoring Losses in ED
Units of JP EP BiH
2.1
Tuzla Canton and Related Power Distribution Grid
Tuzla Canton is one of the ten cantons of the Federation of Bosnia and Herzegovina,
Bosnia and Herzegovina. Canton is located in the northeastern part of Bosnia and
Herzegovina headquartered in Tuzla. The area of canton is 2.649 km2, accounting for
10.14% of the Federation of Bosnia and Herzegovina. Canton consists of 13 munici-
palities, namely: Banovići, Čelić, Doboj Istok, Gračanica, Gradačac, Kalesija, Kladanj,
Lukavac, Sapna, Srebrenik, Teočak, Tuzla i Živinice. The position of the canton in
B&H is shown in Fig. 1.
The area of Tuzla Canton has a population of about 500.000 inhabitants. The
annual consumption of electrical energy is around 1.100 GWh. The average peak load
consumption of ED Tuzla is about 220 MW. From the structure of delivered electrical
energy in ED Tuzla can be seen that the most electrical energy delivered to households.
For example, in 2015, industrial consumption at 35 kV voltage was 16,32%, and
consumption on 10(20) kV voltage was 15,97% of total consumption of electrical
energy. Electrical energy supplied on 0,4 kV voltage level amounted to 67,71% of the
total electrical energy supplied in ED Tuzla. Electrical energy distribution at voltage
level of 20 kV is represented only in the distribution areas of Sapna and Tečak. In the
area of ED Tuzla is still signiﬁcantly represented 35 kV network and transformations
35/10(20) kV. However, in the future it is planned to move to a one level of medium
voltage (20 kV) and direct transformation. In the present circumstances, the jurisdiction
Fig. 1. Tuzla Canton
Electric Energy Losses Estimation
5

of power distribution companies starts from medium voltage, and TS 110/x kV are
under the jurisdiction of the transmission company. The total length of overhead power
networks of ED Tuzla, which operates at voltage level 10 and 20 kV, is about
1.313 km, and cable network is about 366 km. The recorded overhead low-voltage
network has length of 6.489 km and cable network is about 321 km. The models of
MV network for estimation have satisfactory reliability and comply with the situation
in the ﬁeld, while the models of LV networks should be further developed.
2.2
Estimation of Total Losses
Electrical energy losses are estimated on a monthly and annual basis for all distribution
parts of JP EP BiH. In composition of certain Branch, losses are calculated separately
by respective business units of distribution (PJD). For example, in the case of ED
Tuzla, the total losses and losses for each of the twelve PJD are calculated. Losses that
are planned in the energy balance are the total losses incurred as the difference of taken
electrical energy and supplied electrical energy to customers for a particular accounting
period:
wloss %
½
 ¼ Wtak kWh
½
  Wsup kWh
½

Wtak kWh
½

 100 %
½

ð1Þ
where is:
• Wtak—taken electrical energy from supplier
• Wsup—supplied electrical energy to customers, for certain distribution area, i.e. for
certain PJD
The losses of electrical energy estimated in this manner consist technical and all
other losses, also called commercial. The locations where taken electrical energy Wpr are
measured are installed on the locations of taking electrical energy from suppliers on 10
(20) kV or 35 kV voltage in tie or supplied transformer station (TS), and supplied
electrical energy Wisp is measured by measuring devices installed at the locations of end
customers on 35 kV, 10(20) kV i 0,4 kV voltage levels. Managing of electrical energy
losses means systematic monitoring and analysis of the losses on a monthly and annual
Fig. 2. Electrical energy losses (%) in JP Elektroprivreda BiH (2011–2015)
6
S. G. Ferhatbegović et al.

basis and take procedures to reduce them in order to achieve the planned objectives. The
losses of electrical energy in JP EP BiH are shown in the Fig. 2 (2011–2015.).
2.3
Technical Losses in Distribution MV Network
Technical losses in the distribution system components are divided into losses
dependent on the voltage and losses dependent on the current. Losses dependent on
voltage are called idle running losses. Idle running losses are constant losses and due to
maintaining power network in station of continuing operation ability for supplying
customers with electrical energy. Idle running losses are consisted from losses in the
transformer cores and dielectric losses in cables and capacitors. Losses which depend
on the current are subsequent of current passing through the components of the power
system and they are dependent on the current squared or on the amount of transmitted
electrical energy. Losses in transmission networks are smaller than the losses in dis-
tribution networks and technical losses in transmission networks can be precisely
determined in relation to the losses in distribution networks. Technical losses depend
on the length and cross section in the network. Calculation of technical losses of
electrical power and electrical energy can be made only if a model of the network and
certain values of distributed loads are available. The results of calculation of power
ﬂows, which are presented in this paper have been calculated in the software package
PowerCad. Stationary power ﬂow calculation was made according to the method of
peak load [6–10]. Calculations are performed separately for each PJD and calculated
losses are technical losses of electrical power in: 10(20) kV network, transformers 10
(20)/0,4 kV, transformers in TS 35/10(20) kV and in 35 kV feeders. The values of
distributed loads are set at 0.4 kV side of TS 10(20)/0,4 kV. For a reliable calculation
of the power losses, in addition to the so-called passive network model (drawn all lines
and transformers) it is very important to specify the correct input on the average peak
load. Based on the calculation results of electric power losses, the approximate cal-
culation of electrical energy losses has been done on the annual level. For operating
conditions, corresponding to the measurements of the load diagram on 10(20) kV
feeders and transformers in TS, the constant losses (P0) are separated, which create
annual losses of electrical energy W0:
W0 ¼ P0  8760
ð2Þ
For the calculation of user time and annual losses of energy, it can be used the next
relation:
Tloss ¼ Wvar
Pvar
¼
a  Tup þ 1  a
ð
Þ 
T2
up
8760
"
#
ð3Þ
where is:
• Tloss—time of losses duration or estimated user time
• Wvar—annual losses of electrical energy dependent on loads
• Pvar = Ploss −Po—losses of electrical power dependent on loads
Electric Energy Losses Estimation
7

• Tup ¼ Wukg
Pmax—user time
• Pmax—peak power in the reporting period
• Wukg—total annual electrical energy supply
• A—constant that is in distribution networks, depending on the shape of load
duration curve, usually ranges from 0.15 to 0.2. This calculation assumed the
amount of constant a = 0.17.
Based on the calculated user time for all 10(20) kV feeders of certain distribution
area it is possible to make the annual estimation of electrical energy losses. During
estimation, idle running losses and variable losses dependent on the loads are calcu-
lated for all 10(20) kV feeders grouped according to the corresponding TS and PJD
[11, 12]. In general, the results obtained from the steady state load ﬂow, most are used
in the planning of network development (building of new substations and parts of the
network, the transition to the voltage level of 20 kV) and checking of operating con-
ditions. Due to lack of measurements on TS 10(20)/0,4 kV, the data obtained accordind
to the method of peak loads are also used when checking the possibilities of connection
new customers to the existing TS 10(20)/0,4 kV. The percentage of technical losses of
electrical energy can be considered as an indicator of network development and rep-
resentation of the cable network in relation to the network with Al–Fe conductors. To
achieve as small as possible losses it is necessary to plan operation conditions with
optimal power ﬂow and minimal ﬂow of reactive power. Additionally, it is needed to
plan appropriate cross sections of conductors and transformer schedules in accordance
with the actual needs of consumers.
2.3.1
Structure of Losses in 10(20) kV Network
The amount of calculated technical losses in 10(20) kV network is very different for
different distribution areas. In order to control and planning to reduce losses the
structure of technical losses in terms of how many losses relate to network elements is
considered. The value of losses by network elements is observed and the basic sepa-
ration is transformer losses and conductor losses. It can also be viewed as a separation
in idle running losses and losses dependent of load (variable losses). For example, to
give an idea about speciﬁc distribution area in connect to technical losses, the following
indicators can be calculated [11].
Efekt: TR 10ð20Þ=0; 4kV ¼ Losses in TR 10ð20Þ=0; 4kV
kWh
½

Number TR 10ð20Þ=0; 4kV
ð4Þ
Build: network ¼ Losses in network 10ð20ÞkV
kWh
½

Length network 10ð20ÞkV
m
½ 
ð5Þ
The above indicators are used to assess 10(20) kV network construction and proper
scheduling capacity of transformers 10(20)/0,4 kV (avoid substantial underloaded
transformers and eliminate overloaded). In principle certain distribution areas has a
favorable status with regard to energy losses if the value of these indicators are less. In
the case of considered the one electrodistribution unit (e.g. Branch ED Tuzla), the
average value of the indicators is calculated. For certain distribution areas within the
8
S. G. Ferhatbegović et al.

observed ED, whose indicators calculated according to (4) and (5) deviate from the
average values, speciﬁed corrections in the network should be planned in order to
reduce losses. The most common corrections are: optimizing of operating conditions,
changes in cross section, cabling of overhead feeders, replacing of overloaded and
signiﬁcantly underloaded transformer, transition to 20 kV voltage level or planning a
new supply point of 110/10(20) kV. Separated electrical energy losses by network
elements are calculated as a percentage level in relation to the total taken electrical
energy, in order to be comparable with the total realized losses on a monthly basis.
2.4
Estimation of Losses in 0.4 kV Network
For the low voltage networks, which include the main lines and house connections,
determination of energy losses dependent of loads is very difﬁcult. A set of variable
data which are used in calculations represents the main problem, in making analysis of
electrical power and energy losses in 0.4 kV network. Namely, the losses are depen-
dent on: the length of lines, resistance of lines, density of the connected customers, the
intensity of demand for power of individual customers as well as the simultaneously of
loads of individual customers, coefﬁcient of symmetry in the network and other factors.
Changing of certain parameters causes a change in electrical power losses in speciﬁed
time unit, i.e. electrical energy losses for speciﬁed time period. In the literature we can
ﬁnd the proposed methods for determination of active power and energy losses in three
phase LV networks, which are asymmetrically loaded. However, from the aspect of
exploitation of 0.4 kV network, it is possible to conclude that in practice rarely, almost
never, are found lines with concentrated load at the end of lines, or loads that are
usually distributed along the feeder (distributed loads). In addition, the simultaneous
measurement of currents in power lines with distributed loads, in several places along
the feeder, if very difﬁcult and sometimes impossible [6]. For example, in JP EP BiH,
as well as in most other electric power company there is a problem about modeling
complete 0.4 kV network. Calculations in 0.4 kV network are obtained for the purpose
of verifying the interpolation of the new TS or checking of time response of fuses.
However, these are individual cases when analyzing only a part of the transformer area
or certain 0.4 kV feeder at which it is necessary to check or conclude something. For
the analysis of losses in the LV network for speciﬁc distribution areas it is necessary to
have model which includes a complete 0.4 kV network, displacement of all customers
connected to 0.4 kV voltage and appropriate load measurements. This can not be
expected any time soon and it is unlikely that there are distribution organisation with
models of complete 0.4 kV network, displacement and loads of each terminal.
Therefore, for estimation of the total value of losses in 0.4 kV it is practically to make
combination of measured and calculated data as described below. Realized losses
according to PJD are total loses, including losses in 0.4 kV network also. Technical
losses, which are calculated, including losses in the 10(20) kV network and trans-
formers 10(20)/0,4 kV of certain PJD. Thus, the difference between total electrical
energy losses and calculated technical losses of electrical energy refers to the losses in
the 0.4 kV network. To illustrate, the method of assessing the losses in the 0.4 kV
network is given in Fig. 3. To make it possible to perform the calculation of the
realized losses of PJD, taken electrical energy on 10(20) kV buses of supply
Electric Energy Losses Estimation
9

transformer station is estimated (‘Wtaken on 10(20) kV’). In this way, all the PJD are
observed in the same manner and from losses estimation are removed customers at
35 kV voltage level. The data ‘Wsupplied on 10(20) kV’ is obtained as the sum of supplied
electrical energy to customers on 10(20) kV i 0,4 kV voltage levels. With respect to
relation (1) electrical energy supplied at 35 kV voltage level is removed.
wloss %
½
 ¼ Wtak10ð20Þ kV kWh
½
  Wsup 10ð20Þ kV kWh
½

Wtak10ð20Þ kV kWh
½

 100 %
½

ð6Þ
If we take into account the assumption that the situation with measurement on 10
(20) kV are a lot ‘clean’ in terms of illegal consumption or errors in measurements, then
it can be concluded that the calculated technical losses is equal to the realized (or total)
losses in 10(20) kV network. This means that, when estimation losses in 0.4 kV
network of concerned distribution area is observed in presented way, it should take into
account both technical and commercial aspects. Respectively, the estimated losses in
the 0.4 kV network are the total realized losses in the observed part of the distribution
system.
If the preceding considerations are translated into relations, it follows:
Wloss ¼ Wtak10ð20Þ kV  Wsup TS10ð20Þ=0;4 kV þ Wsup 0;4 kV


ð7Þ
Wloss ¼ WlossPowerCad þ Wloss0;4 kV
ð8Þ
where is:
• Wloss [kWh]—Total electrical energy losses from place of taking on 10(20) kV
voltage to place of delivering on 10(20) kV and 0.4 kV voltage
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
Wsup0.4kV
WsupTS10(20)/0.4kV
Wtak10(20)kV
X XX
XX
X X
Calculation in PowerCad
(losses estimation in 10(20) kV network and TS10(20)/0.4 kV
Fig. 3. Estimation of annual electrical energy losses according to voltage levels
10
S. G. Ferhatbegović et al.

• Wtak10kV [kWh]—Taken electrical energy on 10(20) kV side of transformers in
supply TS (measurement—accounting measurement place)
• WsupTS10/0,4 kV [kWh]—Supplied electrical energy to customers that have mea-
surements on TS 10(20)/0,4 kV (on 10(20) kV or 0,4 kV side)
• Wsup0,4 kV [kWh]—Supplied electrical energy to customers on 0.4 kV (households
and other consumtion with measurements that are not directly on the TS)
• WlossPowerCad [kWh]—Estimation of electrical energy losses for the part of the
network that was created in PowerCad and included load ﬂow (10(20) kV network
and transformations 10(20)/0,4 kV)
• Wloss0,4 kV [kWh]—Estimation of losses in 0,4 kV network
It follows that the assessment of losses in 0.4 kV network can be made on the basis
of the relation:
Wloss0;4 kV ¼ Wtak10ð20Þ kV  Wsup TS10ð20Þ=0;4kV þ Wsup 0;4 kV


 WlossPowerCad
ð9Þ
The sum Wsup TS10ð20Þ=0;4 kV þ Wsup 0;4 kV is delivered electrical energy to customers
on 10(20) kV and 0,4 kV. By applying of the above procedure, the total losses of
Branches, certain PJD or TS are devided to the losses in: 10(20) kV network, trans-
formers 10(20)/0,4 kV and losses in 0,4 kV network. From the variables in the rela-
tions (7), (8) i (9), the most difﬁcult is to obtain information WlossPowerCad.
This information includes: model of complete 10(20) kV network, view all
transformers 10(20)/0,4 kV, afﬁliated average peak loads and load ﬂow.
In the present circumstances, for the purpose of analysis and reduction of losses in
the 0.4 kV network, it can be treated in two ways. It is necessary to record the areas
where the voltage drops on 0.4 kV higher than required at the time of peak load and
make voltage measurement at the end points of LV feeders. For further analysis the
area with measured voltage values at the places of delivering that are lower than
required are taken. By bringing the voltage at the end customers within the prescribed
limits it will be signiﬁcantly reduced technical losses in the 0.4 kV network too.
Another way, in terms of location of losses, is control of measurement and possibly
illegal consumption of electrical energy.
Considering the existing concept of electrical energy measurement in electrodis-
tribution, it can be seen that the separation of losses in the 0.4 kV network can be made
according to 10(20) feeders too. Compared with complete supplied TS, in this way the
area where it is needed to conduct a detail control of consumption and connection
places is precisely deﬁned. It should be in mind that improperly downloading and use
of electrical energy, except losses, can cause a greater risk to humans and objects. It
should be noted that the process of separation losses according to 10(20) kV feeders
can be applied to radial feeders where there was no change in the operation state during
the period under consideration. If the operation states changed, then should have
accurate indication about it.
Electric Energy Losses Estimation
11

3
Results and Discussion (Tuzla Canton Case Study)
In this part of the paper, the analysis and structure of losses in the Branch ED Tuzla is
in short term presented. The distribution network of ED Tuzla is observed separated by
individual PJD (twelve PJD). 35 kV network and losses in transformers in supply TS
110/35/10(20) kV and 35/10(20) kV was excepted from the discussion. The volume of
consumption of industrial customers at 35 kV is considerably reduced compared to the
amount in the period before 1991. Operating condition of 35 kV network is usually
optimal and at this voltage level is not possible to reduce losses. Also in JP EP BiH
systematically transition to direct transformation and one level of medium voltage
(20 kV) is planned, and therefore the work on the construction or expansion of 35 kV
network is not considered. The Fig. 4 shows the structure of losses in distribution
network ED Tuzla for the year of 2015 [11]. Distribution network considered for
analysis starts from taking place of energy at 10 and 20 kV voltage. It can be seen that
the largest losses occurring in the 0.4 kV network. A small delivering electrical energy
at 10(20) kV voltage affects to this relation in losses, which in 2015 accounted for only
15.97% of the total supplied electrical energy. Losses in 10(20) kV network and
transformers 10(20)/0,4 kV have amount of 28,14%, while losses in 0,4 kV network
are 71,86% of the total losses.
Table 1 shows the structure of losses in network part for which calculation of load
ﬂow was done [11]. It can be noted that the constant losses are signiﬁcant and in some
PJD are greater than the losses dependent on the load. This is partly result of a large
number of undervoltage transformers, which produce signiﬁcant idle running losses.
Reduction of losses in the MV network can be achieved by transition to the voltage
level of 20 kV.
Fig. 4. Structure of losses in ED Tuzla in 2015
12
S. G. Ferhatbegović et al.

In the case of ED Tuzla, losses in 0.4 kV network are about 2,55 times larger than
the losses in 10(20) kV network and transformers 10(20)/0,4 kV. However, for each
PJD, composed of ED Tuzla, it can be noticed big differences. Table 2 shows the
estimated losses in the 0.4 kV network separated according to PJD [11]. In percentage
terms, these losses are counted in relation to the taken electrical energy at 10(20) and
0,4 kV of certain PJD. Compared to delivered electrical energy at 0.4 kV voltage, these
losses ranges from 3.91% for the area of PJD Teočak to 12.22% for the area of PJD
Tuzla. In relation to the electrical energy taken at 10(20) kV voltage (electrical energy
which ‘enters’ in the system at 10(20) kV voltage and consume at 10(20) and 0.4 kV
voltage) losses range from 3,52% for the PJD Teočak to 9,46% for the PJD Tuzla. For
the total estimated losses in the 0.4 kV network of ED Tuzla, it was made separation
according to PJD. The most part of losses relates to the PJD Tuzla (37,19%) and the
lowest to the PJD Teočak (0,38%). The diagram in Fig. 5 shows the participation of
individual PJD in the total losses at 0.4 kV network of ED Tuzla.
In the Fig. 6 the structure of losses of certain distribution areas in ED Tuzla was
shown. The results show that it is necessary to work at ﬁrst on reducing losses in LV
network of distribution areas Tuzla and Živinice, and then in LV networks of distri-
bution areas Lukavac, Kladanj, Srebrenik. In order to reduce technical losses in the low
voltage network there are proposed activities to: increase cross section of lines,
reconstruction of low voltage network, optimization of supplying customers at 0.4 kV,
reducing unbalanced loads, compensation of reactive power along the LV feeders.
Table 1. Losses in 10(20) kV network, in 2015, ED Tuzla
PJD
Wtaken
[kWh]
wo
[%]
wvar
[%]
wloss
[%]
Length of
10(20)
network
[km]
Number
of TR
[kom]
Build. of
network
[kWh/m]
Efekt. TR
[kWh/TR]
Banovići
81,414,926 58.79 41.21 1.27
82.67
108
5.97
4,991
Čelić
14,253,297 45.12 54.88 3.34
55.42
58
4.91
3,511
Gračanica
145,403,035 19.66 80.34 3.87
216.34
312
20.60
3,756
Gradačac
116,084,903 35.70 64.30 2.32
174.29
213
9.54
4,814
Kalesija
62,004,623 42.66 57.34 2.29
105.64
128
7.25
5,116
Kladanj
24,460,140 50.81 49.19 2.69
99.73
83
3.85
3,310
Lukavac
186,009,050 51.69 48.31 1.37
219.76
264
5.93
4,737
Sapna
10,431,039 59.50 40.50 2.01
35.68
35
1.47
4,477
Srebrenik
75,788,083 35.95 64.05 3.40
149.40
194
11.15
4,712
Teočak
7,547,605 49.68 50.32 3.63
44.25
38
3.17
3,518
Tuzla
305,441,434 65.47 34.53 1.64
261.04
448
4.94
8,311
Živinice
178,227,497 38.49 61.51 2.84
199.67
283
16.87
5,957
Total
PJD:
1,207,065,633 41.92 58.08 2.29
1,643.89
2,164
9.65
5,422
The symbols in the table are: Wpr—total of taken energy including customers at 35 kV, wo—idle
running losses, wvar—losses dependent on loads, wgub—estimated (technical) losses in 10(20)
kV network and TR 10(20)/0,4 kV
Electric Energy Losses Estimation
13

Table 2. Estimation of losses in LV network, in 2015, ED Tuzla
PJD
Wtak on 10
(20) kV
[kWh]
Realized
losses
without
customers
at 35 kV
[%]
Estimated
losses in 10
(20) kV
network and
TR 10/0,4 kV
[kWh]
Losses in
0.4 kV
related to
Wtak on 10
(20) kV
[%]
Losses in
0.4 kV
related to
Wtak on
0,4 kV
[%]
Part. of
PJD in total
losses on
0.4 kV ED
Tuzla [%]
Banovići
49,326,903
6.12
1,032,916
4.03
5.80
2.82
Čelić
14,253,297
8.02
475,803
4.69
5.12
0.95
Gračanica 145,403,035
8.33
5,628,557
4.46
6.71
9.21
Gradačac
116,084,903
7.69
2,688,587
5.38
8.98
8.86
Kalesija
62,004,623
6.84
1,421,093
4.55
6.41
4.00
Kladanj
24,460,140
8.70
658,736
6.00
7.67
2.08
Lukavac
91,295,311
10.48
2,553,790
7.68
9.19
9.96
Sapna
10,431,039
7.95
209,196
5.95
6.46
0.88
Srebrenik
75,788,083
9.58
2,580,183
6.18
7.32
6.65
Teočak
7,547,605
7.15
274,174
3.52
3.91
0.38
Tuzla
277,003,597 11.27
5,013,406
9.46
12.22
37.19
Živinice
151,942,691 11.21
5,054,566
7.89
11.68
17.01
Total
ED:
1,025,541,227
9.56
27,591,007
Fig. 5. Participation of individual PJD in total estimated losses in 0.4 kV network, ED Tuzla
14
S. G. Ferhatbegović et al.

When replacing conductors in MV and LV feeders it should be noted that due to
losses dependent on the load, the best results achieved by increasing the cross sections
in the initial lines. In order to reduce technical losses in the LV network it is proposed
to shorten the LV feeders and optimum supplying from the nearest TS 10(20)/0,4 kV.
It should be noted that the LV feeders can be burdened with asymmetric power ﬂows
due to the large number of single phase customers, causing additional losses in dis-
tribution of electrical energy. It is necessary to work on balancing the load on the entire
length of the LV feeders based on the results of measurements and calculations.
In order to reduce losses in the LV network it should be planned procedures to
reduce non-technical or commercial losses too. Due to the large number of customers
and connection places it is proposed to separate priority areas for controlling con-
nections and accounting metering points (protected measuring devices and equipment
by appropriate seal, wiring correctness, correctness of accounting constants, failures of
measuring and auxiliary equipment, etc.). It is recommended to install in TS
10(20)/0,4 kV measuring devices to measure the total consumption of transformer area,
which is compared with the amount of delivered electrical energy to all customers
connected to the concerned TS plus the estimated value of technical losses.
4
Conclusion
The total losses in the distribution system of JP EP BiH amounts to 9%. The negative
impact on the total amount of losses has a relatively low consumption of industrial
customers. Almost two-thirds of the total electrical energy supply is carried out on
Fig. 6. Structure of energy losses according to certain PJD
Electric Energy Losses Estimation
15

0.4 kV voltage level. To implement effective procedures for the management of the
losses it is necessary to reliable determine the structure of electrical energy losses by
voltage level and by type. By applying the methods of estimation of electrical energy
losses by voltage levels it can be concluded that the highest losses of electrical energy
are in 0.4 kV network. These ﬁndings have been known before, but the estimates were
ﬂat and never worked assessment of losses by voltage levels for smaller distribution
areas (PJD, TS or 10(20) kV feeders) in a way that takes into account the assessment of
technical losses in 10(20) kV network and transformers 10(20)/0,4 kV based on cal-
culation. In addition to insights into the structure of the total losses of ED Tuzla, a
special beneﬁt from the application of this method, is the knowledge about structure
losses for parts of distribution areas. Data for Branch ED Tuzla, losses in the 0.4 kV
network, for the period under consideration is 2.55 times higher than the losses in 10
(20) kV network and transformers 10(20)/0,4 kV, clearly indicate in which area should
be to act in order to achieve the greatest effect in reducing losses in the network.
Separated losses at 0.4 kV voltage level according to the presented method are the total
losses (technical and commercial together). By precise analysis of the situation in the
area with identiﬁed signiﬁcant losses of electrical energy it can be concluded that:
• parts of network with the largest estimated losses have the worst network in terms
of structure, dimensioning and length of LV feeders.
• parts of the network with losses above the average may have the highest concen-
tration of faulty measurements and illegal consumption of electrical energy.
Finally, the application of that method to the business decision makers clearly
shows what to do and on which aspect need to pay attention in order to reduce the total
realised losses on certain distribution area i.e. in particular distribution network.
References
1. Refou, O., Alsafasfeh, Q., Alsoud, M.: Evaluation of electric energy losses in southern
governorates of Jordan distribution electric system. Int. J. Ener. Eng. 5(2), 25–33 (2015)
2. Nourai, A., Kogan, V.I., Schafer, C.M.: Load leveling reduces T&D line losses. IEEE Trans.
Power Deliv (2008)
3. Donadel, C., Anicio, J., Fredes, M., Varejão, F., Comarela, G., Perim, G.: A methodology to
reﬁne the technical losses calculation from estimates of non-technical losses. In: 20th
International Conference on Electricity Distribution (2009)
4. Oliveira, C.C.B., Kagan, N., Meffe, A., Jonathan, S., Caparroz, S., Cavaretti, J.L.: A new
method for the computation of technical losses in electrical power distribution systems. In:
16th International Conference and Exhibition on Electricity Distribution, 2001. Part 1:
Contributions. CIRED. (IEE Conf. Publ No. 482)
5. Leonardo Queiroz, M.O., Marcio Roselli, A., Cavellucci, C., Lyra, C.: Energy losses
estimation in power distribution systems. IEEE Trans. Power Syst. 27, 4 (2012)
6. Goić, R., Jakus, D., Penović, I.: Distribucija električne energije, FESB Split (2008)
7. Goić, R., Mudnić, E.: Struktura gubitaka snage i energije u srednjenaponskoj distributivnoj
mreži, HO Cigre (2002)
8. Pavić, A., Trupinić, K.: Gubici električne energije u distribucijskoj mreži, časopis Energija,
vol. 56, no 2, pp. 182–215 (2007)
16
S. G. Ferhatbegović et al.

9. Žutobradić, S., Wagmann, L., Rajić, Ž., Miličić, H.: Analiza problematike gubitaka
električne energije u distribucijskim mrežama članica EU, HO međunarodne elektrodis-
tribucijske konferencije, 2. savjetovnje, Umag 16–19. svibnja (2010)
10. Jakus, D., Kovačić, Z., Mučić, D., Ćurković, M.: Tehnički i komercijalni gubici
distribucijske mreže, HO međunarodne elektrodistribucijske konferencije, 4. savjetovnje,
Trogir/Seget Donji, 11–14. svibnja (2014)
11. Džananović, I., Gruhonjić Ferhatbegović, Š., Divković I., Kurtalić N.: Metoda za
određivanje gubitaka po naponskim nivoima u distributivnoj mreži Podružnice Elektrodis-
tribucije Tuzla, 10. Savjetovanje BHK Cigre, Sarajevo, 25–29. 09. (2011)
12. Avdaković S. et. al.: Analiza postojećeg stanja i mjere za unapređenje postupaka
identiﬁkacije i lokalizacije gubitaka električne energije u elektrodistributivnim mrežama,
Studija, Elektroprivreda BiH
Electric Energy Losses Estimation
17

Application of Teager Energy Operator
for the Power System Fault Identiﬁcation
and Localisation
Nejra Čišija-Kobilica1(&) and Samir Avdaković2
1 International Burch University, Ilidža, Bosnia and Herzegovina
nejra.cisija@ibu.edu.ba
2 Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and
Herzegovina
Abstract. Power system is constantly exposed to the disturbances. Some dis-
turbances can cause cascading propagation and outages of other generators or
transmission lines in the system. Finally that can lead to the partial or total
system blackout. Wide Area Monitoring Protection and Control (WAMPAC)
systems based on phasor measurement units (PMU) are strongly improving in
order to eventually prevent these dangerous occurences. This platform presents
base for the development of the smart grids. One of the requirements for the
smart power systems is presence of the algorithms for fast and correct fault
identiﬁcation and localisation. In this paper Teager Energy Operator (TEO) is
proposed for the processing and analysis of the synchronized measurement
signals in order to perform fault identiﬁcation and localisation. Several fault
types are simulated in the NE 39 bus test system using DigSILENT Power
Factory software. Results of the fault identiﬁcation and localisation are com-
pared with Discrete Wavelet Transform (DWT).
1
Introduction
The function of the fault identiﬁcation and localisation in the power system is extre-
mely important since it is constantly exposed to the disturbances that can lead it to the
blackout. Application of the different algorithms for correct fault detection should
provide useful information to the system operators since security and reliability of the
system depends on their timely reaction as well. For that purpose Wide Area Moni-
toring Protection and Control (WAMPAC) are establishing and developing. These
systems should provide data for the mentioned algorithms. Data includes measurements
at the special points in the system marked as PMU. Real time monitoring of the system
will be achieved with real-tima data obtained by these devices and sent through a
communication system [1].
Fast and correct identiﬁcation and localisation of the disturbances is important
segment in smart WAMS. Different signal processing techniques for implementation of
these algorithms have been proposed until now.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_2

In [2] it is highlighted that for estimation of power system’s electromechanical
modal properties time-synchronized measurements (data) provide rich information.
One of the challenging tasks in data processing is spectral analysis of power system
oscillatory signal data. As technique that overcome disadvantage of ﬁnite time window
Hilbert-Huang technique is proposed [2].
Authors in [3] proposed Haar wavelets for frequency signals analysis in dynamic
events detection and identiﬁcation. Analysis is performed on real data collected from
the Portable Power System Monitors (a variant of PMU) in WAMS of Entergy net-
work. Event occurrence time is identiﬁed with estimation of the damping after an event
It is highlighted that performance of proposed detection method depends on the system
condition and choice of several user-speciﬁed parameters (number of decomposition
levels, the width and time step of the decomposition window, detection thresholds) [3].
Authors performed simulation of the disturbance propagation through WSCC 127
bus test system. It represents large interconnected power systems characteristic by
evident time delay in disturbance propagation. Frequency signals from the system are
analyzed with Wavelet Transform, Db4 wavelet function with ﬁve decomposition
levels. The ﬁrst level of the signal decomposition is used for the identiﬁcation of the
event. Proposed approach enables identiﬁcation of inter-area oscillations and coherent
group of generators [4].
In [5] authors proposed DWT of active power signal in analyzing and locating of
the low frequency oscillations. Proposed algorithm is tested on New England (NE) 39
bus test system Different wavelet families are tested (Daubechies (Db), symlet
(sym) and biorthogonal (bior)). They compared mentioned technique with Prony and
Eigenvalue analyses. It is concluded that Wavelet Transform analysis identify oscil-
lation modes hidden in the signal. It is highlighted that this approach allow identiﬁ-
cation of the network nodes in the close area of the original disturbance [5].
Wavelet transform is used as signal processing technique in [6] as well. It is applied
on the data measured by PMU in Japan 60 Hz power system. This data represents
single-phase instantaneous voltage. PMUs are located in each of six power systems that
constitute Western Japan power system.
Japan western 60 Hz system is analyzed in [7] as well. A long term oscillations are
detected from phase difference by application of the Wavelet transform (d6). It is
highlighted that power system oscillations are possible to detect at the domestic outlets.
For the purpose of the work in [8] it is assumed that a limited number of PMU
voltage measurements are available, and the positive-sequence parameters of the net-
work are known. Fault location is based on matching degree d which is deﬁned as a
function of fault distance x at the exact fault point value of matching degree is equal to
zero. Process of fault location is divided into two stages. In ﬁrst stage suspicious fault
region is found by computing matching degree at each bus in the network and selecting
buses with very small value of the matching degree. In stage 2 exact position of the
fault is found by minimizing all calculated matching degrees.
In [9] wavelet transform is applied in the protection of series-compensated lines.
Mother wavelet db4 is used for fault-zone identiﬁcation and Haar wavelet is used for
Application of Teager Energy Operator for the Power System
19

fault classiﬁcation. DWT is performed for the modal signal formed of the three-phase
currents. The coefﬁcients of the detail 1 (d1) and detail 6 (d6) are used in the fault
identiﬁcation algorithm. Fault classiﬁcation task is performed calculating the average of
the absolute value of the 200 post-fault coefﬁcients of d6. The phase with the largest
average value is marked as the faulty one. Different fault types, conditions and location
have been simulated in 500kV–50 Hz power system. Applied algorithm indicatated
very high accuracy.
Algorithm that combines DWT and support vector machine (SVM) techniques is
proposed in [10] for fault zone identiﬁcation in a series compensated line with series
capacitor placed at the middle of the line. DWT technique is applied to the three line
currents to extract features at the ﬁrst decomposition level using db2 as mother wavelet.
Extracted detailed coefﬁcients are used as input for SVM that should provide fault zone
as its output. Obtained accuracy is value of 93.917% tested on 25 200 test cases with
wide variation in system operating conditions.
Authors provide fault detection algorithm based on Teager energy operator
(TEO) in [11]. TEO is applied to the negative sequence current that appears in the
system during the power swing. Its value is approximately zero in pre-fault condition
while the value increases during fault conditions. Authors deﬁned index for fault
detection g if crossing its threshold indicates on the fault presence. It is tested for
symmetrical and unsymmetrical faults giving satisfactory performance for fault
detection task during power swing.
Comparison of different mother wavelet for fault detection and classiﬁcation of
series compensated transmission line (Haar, Daubechies, Symlets, Coiﬂet, BiorSplines,
Reverse BioSplines and Meyer families) is made in [12]. Wavelet transform analysis is
performed on voltage and current signals in case of LLG (line-line-ground) fault. Since
effectiveness of this analysis is inﬂuenced by the choice of the mother wavelet authors
highlighted those with best achieved results in their study. Db10 mother wavelet was
the most suitable for fault detection from Daubechies Wavalet family with remark that
db4 is faster for wavelet coefﬁcients computation in real-time application. For fault
classiﬁcation the Haar mother wavelet is highlighted as the most appropriate.
In [13] authors proposed method for the detection of high-impedance fault (HIF).
DWT is used for features extraction of the distorted voltage and current waveforms
caused by HIFs. Output coefﬁcients of the DWT analysis are then converted to RMS
values in various frequency rande. These values are used in Near Neighbor Rule
(NNR) performing fault classiﬁcation. Authors marked this method as possible deci-
sion support software package installed in an alarm system.
Real-time detection of fault-induced transients in transmission line based on
wavelet analysis is performed in [14]. Novelty is in taking into account the border
effects of the sliding windows. This task enables analysis which performance is not
affected by the choice of the mother wavelet.
Traveling-wave based method for fault classiﬁcation and localization is proposed in
[15]. The three-phase and ground-mode voltages at two terminals in three-terminal
transmission system are analyzed with DWT. Normalized wavelet energies are
obtained and used as the input for SVM. Various fault scenarios are applied to test
20
N. Čišija-Kobilica and S. Avdaković

performance of the proposed method. Authors marked achievement of the satisfactory
results.
This paper propose application of TEO for the fault identiﬁcation and localisation
in the power system. Analysis is done using NE 39 bus test system for simulation of
several fault types. Results are compared with DWT approach that performs locali-
sation on the base of local oscillations in the range of 1–2 Hz. Proposed approach
considers that signals from the synchronized measurement units (PMUs) are available
for the analysis. Further, paper is organized as follows: Used methodology and test
system are presented in the second part. Third part includes presentation of the obtained
results, while conclusion and future work are highlighted in the part 4.
2
Materials and Methods
2.1
TEO and DWT Signal Processing Techniques
The Teager energy operator is used for local (time) analysis of signals. This operator is
useful for expressing energy properties from the signal. It can be expressed in discrete
and continuos form [16].
The form of teager energy operator in continuous domain is deﬁned below:
W x tð Þ
½
 ¼
dx tð Þ
dt

2
x tð Þ d2x tð Þ
dt2
ð1Þ
(This form represents dependence on the signal x(t) and its ﬁrst two derivatives.)
The form of teager energy operator in discrete domain is deﬁned below:
W x n
ð Þ
½
 ¼ x2 n
ð Þ  x n þ 1
ð
Þ  x n  1
ð
Þ
ð2Þ
(It represents dependence on three successive samples of the signal: x(n-1), x(n)
and x(n + 1)) [17].
Wavelet transform allows signal processing by different window size. Low frequent
components of the signal can be detected with larger window size and high frequent
components with the shorter one. That feature enables multiresolution analysis in
frequency and time domain. Therefore nonstationary signals that are frequent in the
power systems can be efﬁciently analyzed with this technique.
If function x tð Þ is discrete function, its transform made with Discrete Wavelet
Transform is deﬁned as [18, 19]:
DWT x; m; n
ð
Þ ¼
1ﬃﬃﬃﬃﬃﬃ
am
0
p
Z1
1
x tð Þ w t  nb0am
0
am
0


dt
ð3Þ
where m and n are integer numbers, a0 [ 1 is ﬁxed scale parameter.
Usually, a0 ¼ 2 and b0 ¼ 1 (translation parameter).
Application of Teager Energy Operator for the Power System
21

Signal ﬁltering process with lowpass and highpass ﬁlters is accomplished with
DWT. Scale function deﬁnes lowpass ﬁlter and wavelet function deﬁnes highpass ﬁlter.
Original discrete signal will be split into two separate signals (approximation and
detail)—ﬁrst level. After that signal ﬁltration is performed again in a way that
approximation signal of the ﬁrst level is ﬁltered with lowpas and highpass ﬁlters. This
will gave approximation and detail signal of the second level. Previously mentioned
operation will be repeated until last decomposition level is reached. That way
approximation and detail signals at the different levels are obtained. With each new
level different frequency range is speciﬁed. Therefore DWT enables identiﬁcation of
the components that belongs to the speciﬁc frequency ranges.
There are different wavelet families established until now, and in each family
different wavelets implementations. The most known family is Daubechies ﬁlters. In
this paper Daubechies 4 mother wavelet function was used. DWT with 5 levels was
applied to the frequency signals. Detail 1 was extracted for the identiﬁcation part of the
analysis since it will detect any change in the signal (nature of the fault). The range of
the frequencies is 2–4 Hz. Amplitudes of the detail 2 are used for localisation of the
fault. Frequency of LFEO belongs to the range of the detail 2 frequency (1–2 Hz).
Simulations are made with ﬁxed time step of 0.125 s making sampling frequency
of 8 Hz. According to the Nyquist sampling theory frequency that can be contained and
analyzed in the signals is in the range 0–4 Hz, what is sufﬁcient for analyzing LFEO.
Analysis is based on the preliminary results obtained.
Techniques comparison is based on MSE (Mean Square Error) and RMSE (Root
Mean Square Error) calculated with real fault time and results obtained by speciﬁc
technique.
MSEðx; yÞ ¼ 1
N
X
N
i¼1
ðxi  yiÞ2
ð4Þ
(x—real fault time, y—obtained fault time, N—number of faults) [20].
RMSE has better physical interpretation since the unit is the same as for the interest
parameter (fault time). It presents average difference between real and predicted faul
time.
2.2
Test System
NE 39 bus test system includes 10 generators that have voltage and turbine regulators.
Loads are connected to the 21 buses (Fig. 1). Placement of the PMUs in the system is
deﬁned as proposed in [21] as optimal one. Measurement points are buses 2, 6, 9, 10,
11, 14, 17, 19, 20, 22, 23, 25 and 29. Number of transmission lines is 34 with 12
transformers included in the system. Voltage level is 345 kV.
22
N. Čišija-Kobilica and S. Avdaković

3
Result and Discussion
3.1
Overview of the Performed Analysis After the Short Circuit Followed
by Transmission Line Outage Fault Type
In this part one of the simulated faults will be explained in the performed analysis. Given
example is short circuit at bus 24 followed by outage of the transmission line between
buses 16 and 24 at 5 s and 5.125 s, respectively. Frequency signals after the fault are
presented in the Fig. 2. They are obtained and analyzed with TEO and DWT algorithms.
G1
G2
G8
G9
G6
G7
G4
G5
G10
G3
30
37
25
2
1
39
3
18
4
5
8
9
7
6
10
12
11
13
14
15
17
27
26
28
29
38
35
36
22
23
16
24
21
19
20
33
31
32
34
Fig. 1. NE 39 bus test system [22]
0
5
10
15
20
59.8
60
60.2
60.4
60.6
time (s)
Frequency (Hz)
Fig. 2. Frequency signals
Application of Teager Energy Operator for the Power System
23

In DWT analysis frequency signals obtained from the model are decomposed to the
second level and detail 2 signals extracted for all of them. Maximum value of all detail
2 signals are found. It is speciﬁed from the algorithm that signal from the bus 25 has
maximum value of extracted D2 (Fig. 3).
That indicates bus 25 as fault location. Time when Detail 1 signal reaches maxi-
mum value (5.125 s) is marked as the fault time.
In regards of the TEO analysis same algorithm logic is applied. Instead of extracted
detail 2 signals from the original ones Teager Energy Operator is calculated for the
obtained frequency signals (Fig. 4). Maximum value of TEO for all signals is found. It
is speciﬁed from the algorithm that signal from the bus 23 has maximum value of TEO.
That indicates bus 23 as fault location. Maximum TEO value is reached in 5.125 s
what is marked as the fault time.
0
5
10
15
20
-0.1
-0.05
0
0.05
0.1
Detail 2
time (s)
Fig. 3. Local oscillations (Detail 2) of the frequency signals
0
5
10
15
20
-40
-20
0
20
40
60
time (s)
Teager Energy
Fig. 4. Teager energy of the frequency signals
24
N. Čišija-Kobilica and S. Avdaković

In this simulated fault TEO results showed better performance when compared with
results given by DWT analysis. Bus 23 is the nearest measurement point and exact fault
time is 5 s.
3.2
Results of the Analysis
LFEO can appear as the consequence of the different fault types. That can include
generator outage, transmission line outage, short circuit or short circuit followed by
transmission line outage. Faults of ﬁrst two mentioned fault type are simulated in NE39
buses power system DigSILENTPower Factory model (Fig. 1)—44 faults totally.
Frequency signals are obtained from 13 determined possible PMUs locations (buses 2,
6, 9, 10, 11, 14, 17, 19, 20, 22, 23, 25 and 29). The signals are then analyzed with
application of Discrete Wavelet Transform and Teager Energy Operator.
Fault identiﬁcation and fault localisation as parts of the fault detection algorithm are
obtained through this work. Work is divided into two parts: identiﬁcation and locali-
sation part. In the identiﬁcation part value of fault time will be given as the result, and
in the localisation part bus closest to the simulated fault will be speciﬁed. The men-
tioned signal processing techniques are compared in their ability to precisely identify
and locate simulated fault.
Input signals for used signal processing techniques are frequency signals. Expla-
nation of the fault types and preliminary results of different techniques applied are
given below (Table 1, 2).
The paper includes 10 simulated generator outages. Results for TEO application in
terms the fault time are the same as those with DWT. Algorithm didn’t identify exact
time of the fault at 0.5 s in all cases but it was approximate—0.625 s.
The same results in the fault location with DWT and TEO are the same in the
following 7 cases. The closest measurement bus for the generator at the bus 39 is bus 9
Table 1. Generator outage at 0.5 s
Number
Simulation
DWT Results
TEO Results
Fault time
(s)
Fault
location
Fault
time
Fault
location
Fault
time
Fault
location
1
0.5
Bus 39
0.625
9
0.625
9
2
0.5
Bus 31
0.625
6
0.625
6
3
0.5
Bus 32
0.625
10
0.625
10
4
0.5
Bus 33
0.625
20
0.625
19
5
0.5
Bus 34
0.625
20
0.625
20
6
0.5
Bus 35
0.625
20
0.625
22
7
0.5
Bus 36
0.625
20
0.625
23
8
0.5
Bus 37
0.625
25
0.625
25
9
0.5
Bus 38
0.625
29
0.625
29
10
0.5
Bus 30
0.625
2
0.625
2
Application of Teager Energy Operator for the Power System
25

as speciﬁed with the applied algorithms. The same case is with the generator at the bus
31 where the outage is located at the bus 6. Bus 10 was located as the fault location for
the generator outage at the bus 32 (again the closest measurement point). Bus 20 is the
closest measurement bus to the generator at the bus 34 and it is speciﬁed as the fault
location for its outage. Buses 37, 38 and 30 are connected to the generators which
outage is located at the closest measurement buses 25, 29 and 2, respectively. In the
cases of the generators outages at the buses 33, 35 and 36 algorithm with DWT
speciﬁed bus 20 as fault location for all three outages. Bus 20 in these three situations
was not the closest point, but correct for generator outage at bus 33. Unlike DWT
Table 2. Transmission line outage at 0.5 s
Number Simulation
DWT Results
TEO Results
Fault time Fault location Fault time Fault location Fault time Fault location
1
0.5
1–2
0.625
2
0.5
2
2
0.5
1–39
0.625
2
0.625
2
3
0.5
9–39
0.625
9
0.625
9
4
0.5
8–9
0.75
6
0.625
9
5
0.5
5–8
0.625
29
0.625
9
6
0.5
7–8
0.625
29
0.625
6
7
0.5
6–7
0.625
29
0.625
6
8
0.5
5–6
0.625
29
0.625
6
9
0.5
4–5
0.625
29
0.625
6
10
0.5
3–4
0.625
14
0.625
6
11
0.5
4–14
0.625
14
0.625
14
12
0.5
2–3
0.625
29
0.625
2
13
0.5
2–25
0.5
25
0.625
25
14
0.5
13–14
0.625
10
0.625
10
15
0.5
10–13
0.625
14
0.625
10
16
0.5
10–11
0.625
11
0.625
11
17
0.5
6–11
0.625
6
0.625
11
18
0.5
14–15
0.5
20
0.5
10
19
0.5
15–16
0.625
20
0.625
14
20
0.5
16–17
0.625
29
0.625
17
21
0.5
16–19
0.5
19
0.5
19
22
0.5
16–21
0.625
22
0.5
22
23
0.5
16–24
0.625
23
0.625
23
24
0.5
17–18
0.625
20
0.625
17
25
0.5
17–27
0.625
29
0.625
29
26
0.5
3–18
0.625
20
0.625
17
27
0.5
21–22
0.625
29
0.5
22
28
0.5
22–23
0.625
23
0.625
23
29
0.5
23–24
0.625
23
0.625
23
(continued)
26
N. Čišija-Kobilica and S. Avdaković

application results achieved with TEO algorithm speciﬁed the closest measurement
buses, 19, 22 and 23, respectively.
In the part of transmission line outage DWT application speciﬁed vaule for fault
time was correct or very close to 0.5 s (0.625 s). The same case is achieved with TEO
application. Value of 0.5 s is identiﬁed in 5 cases with DWT algorithm and in 8 cases
with TEO algorithm.
The same results in the fault location with DWT and TEO are in the transmission
outages numbers 1–3, 11, 13, 14, 16, 21-23, 25, 28–34. All these mentioned 18 faults
are correctly located. In the fault numbers 14, 25 and 30 the closest measurement points
are buses 14, 17 and 25, respectively, but both algorithms speciﬁed buses 10, 29, 29,
respectively, deﬁned as the second nearest measurement points. Other faults where
located with the nearest bus. In the rest 16 cases techniques gave different results that
will be brieﬂy explained. For the fault numbers 15 and 17 DWT and TEO gave
different but correct results. Transmission line 10–13 outage (fault number 15) is
speciﬁed with buses 14 and 10, and transmission line outage 6–11 with buses 6 and 11,
respectively to the technique type, DWT and TEO. In one simulated case with TEO
technique result isn’t marked as correct because there were better candidates that could
determine fault location. That is in the transmission outage number 10 (transmission
line 3–4). Bus 6 was speciﬁed as the fault location but better candidates (in terms of the
bus order) were buses 2 and 14. DWT in this case speciﬁed bus 14. For the fault
number 4 and 5 (lines 8–9 and 5–8) TEO correctly speciﬁed bus 9. DWT speciﬁcation
of bus 6 for the line 8–9 outage is correct as the second nearest point but not in 5–8 line
case where speciﬁed 29 bus is not correct for the fault location. In the fault numbers
from 6 to 9 TEO correctly speciﬁed bus 6 unlike DWT which speciﬁed bus 29 that is
not near measurement point to the mentioned fault location. Bus 29 is uncoreectly
speciﬁed in the fault numbers 12 (line 2–3), 20 (line 16–17) and 27 (line 21–22) as
well, where TEO gave correct result with buses 2, 17 and 22, respectively. TEO gave
correct results for the fault numbers 18, 19, 24 and 26 in terms of the speciﬁed fault
location with buses 10, 14, 17 and 17 respectively, where DWT incorrectly speciﬁed
bus 20 in all four cases.
Values of RMSE in terms of identiﬁed fault time are presented in the next table
(Table 3).
Table 2. (continued)
Number Simulation
DWT Results
TEO Results
Fault time Fault location Fault time Fault location Fault time Fault location
30
0.5
25–26
0.625
29
0.625
29
31
0.5
26–27
0.625
29
0.625
29
32
0.5
26–29
0.5
29
0.5
29
33
0.5
26–28
0.5
29
0.5
29
34
0.5
28–29
0.625
29
0.5
29
Application of Teager Energy Operator for the Power System
27

Accuracy of the applied techniques for the different fault type is shown in the
Table 4. The used criteria for the fault localization states that speciﬁed fault location is
marked as correct if it is the nearest or second nearest measuring bus to the actual one
(line parameters are not taken into consideration).
Correct fault localisation with DWT algorithm is made in 30 cases of total 44
(generator outage 8, transmission line outage 22). With TEO algorithm almost all
simulated faults are correctly located (43 of them).
4
Conclusion
Timely identify and locate fault that occurs in the power system is extremely important
for the safety and the stability of the system in total. In this paper two signal processing
techniques, Discrete Wavelet Transform and Teager Energy Operator are applied for
the purpose of the fault identiﬁcation and localization. Several faults of different type
were simulated and frequency signals obtained. Results obtained by analysis of the
signals with the mentioned techniques are compared. Short comparison is made in
order to highlight which technique gave more accurate results. From the presented
information it is noticeable that more correct results in the identiﬁcation and locali-
sation of the faults in general are obtained with application of the TEO.
In this work up to 44 faults of two fault types are simulated. To achieve more
precise comparison all possible faults of mentioned types should be simulated in the
NE 39 bus test system. Same algorithm procedure should be applied on the obtained
signals, including Hilbert Huang Transform as third signal processing technique. Third
part of the fault detection algorithm is fault classiﬁcation that can be obtained with
Artiﬁcial Neural Networks (ANN). Output data from the optimal used algorithms for
the fault identiﬁcation and localisation is supposed to be input data for ANN. The fault
detection algorithm should be completed with this operation of the fault classiﬁcation.
Table 3. RMSE values for the fault identiﬁcation
DWT results TEO results
Fault type
Fault time
Fault time
Generator outage (s)
0.125
0.125
Transmission line outage (s) 0.121
0.109
Table 4. Signal processing techniques accuracy (%)
DWT results
TEO results
Fault type
Fault location Fault location
Generator outage
80
100
Transmission line outage 64,71
97,06
28
N. Čišija-Kobilica and S. Avdaković

References
1. La Scala, M., De Benedicts, M., Bruno, S., Grobovoy, A., Bondareva, N., Borodina, N., Den
isova, D.: Development of Applications in WAMS and WACS: An International
Cooperation Experience IEEE (2006)
2. Messina, A.R.: Inter-Area Oscillations in Power Systems—A Nonlinear and Nonstationary
Perspective Perspective. Springer (2009)
3. Mei, K., Rovnyak, S.M., Ong C.-M.: Dynamic event detection using wavelet analysis. In:
Proceedings of IEEE PES General Meeting (2006)
4. Avdakovic, S., Nuhanovic, A., Kusljugic, M., Becirovic, E.: Wavelet analysis of dynamic
behaviors of the large interconnected power system. Inter. J. Sci. Eng. Res. (2012)
5. Avdakovic, S., Nuhanovic, A., Kusljugic, M., Music, M.: Wavelet transform applications in
power system dynamics. Electr. Power Syst. Res. (2012)
6. Hojo, M., Ohnishi, T., Mitani, Y., Saeki, O., Ukai, H.: Observation of frequency oscillation
in western Japan 60 Hz power system based on multiple synchronized phasormeasurements.
In: Proceedings of IEEE-Power Technology (2003)
7. Hashiguchi, T., Mitani, Y., Saeki, O., Tsuji, K., Hojo, M., Ukai, H.: Monitoring power
system dynamics based on phasor measurements from demand side outlets developed in
Japan Western 60 Hz system. In: Proceedings of IEEE PES—Power Systems Conference
Expo (2004)
8. Jiang, Q., Li, X., Wang, B., Wang, H.: PMU-Based fault location using voltage
measurements in large transmission networks. IEEE Trans. Power Delivery (2012)
9. Megahed, A.I., Moussa, A.M., Bayoumy, A.E.: Usage of wavelet transform in the protection
of series-compensated transmission lines. IEEE Trans. Power Delivery (2006)
10. Parikh, U.B., Das, B., Maheshwari, R.P.: Combined wavelet-SVM technique for fault zone
detection in a series compensated transmission line. IEEE Trans. Power Delivery (2008)
11. Kumar, J., Jena, P.: Fault Detection during Power Swing using Teager-Kaiser Energy
Operator. IEEE (2016)
12. Gawali, N.U., Hasabe, R.P., Vaidya, A.P.: A comparison of different mother wavelet for
fault detection & classiﬁcation of series compensated transmission line. Int. J. Innov. Res.
Sci. Technol. (2015)
13. Lai, T.M., Snider, L.A., Sutanto, D.: High-Impedance fault detection using discrete wavelet
transform and frequency range and RMS conversion. IEEE Trans. Power Delivery (2005)
14. Costa, F.B.: Fault-Induced transient detection based on real-time analysis of the wavelet
coefﬁcient energy. IEEE Trans. Power Delivery (2014)
15. Livani, H., Evrenosoğlu, C.Y.: A fault classiﬁcation and localization method for
three-terminal circuits using machine learning. IEEE Trans. Power Delivery (2013)
16. Kvedalen, E.: Signal Processing Using the Teager Energy Operator and Other Nonlinear
Operators. University of Oslo (2003)
17. Kaiser, J.F.: Some Useful Properties of Teager’s Energy Operator. IEEE (1993)
18. Daubechies, I.: Ten Lectures on Wavelets. Society for Industrial and Applied Mathematics
(1992)
19. Mallat, S.: A Wavelet Tour of Signal Processing. Academic Press (1999)
20. Wang, Z., Bovik, A.C.: Mean squared error: love it or leave it. IEEE Signal Process. Mag.
(2009)
21. Ivatloo, B.M.: Optimal placement of PMUs for power system observability using toplogy
based formulated algorithms. J. Appl. Sci. (2009)
22. Avdakovic, S., Becirovic, E., Nuhanovic, A., Kusljugic, M.: Generator coherency using the
wavelet phase difference approach. IEEE Trans. Power Syst. (2014)
Application of Teager Energy Operator for the Power System
29

Analysis and Control of DG Inﬂuence
on Voltage Proﬁle in Distribution Network
Mirza Šarić1(&), Jasna Hivziefendić2, and Lejla Bandić2
1 Public Enterprise Elektroprivreda of Bosnia and Herzegovina,
Mostar, Bosnia and Herzegovina
m.saric@epbih.ba
2 International Burch University, Sarajevo, Bosnia and Herzegovina
{jasna.hivziefendic,lejla.bandic}@ibu.edu.ba
Abstract. Power generated by the Distributed Generator (DG) must satisfy
high quality standards and ensure compatibility with network operation and
customer supply requirements. The power system needs to maintain stable
operating conditions and continue to meet customer load demand for the entire
range of generations. This paper presents results of investigations related to
reactive power and voltage control requirements for distribution network with
DG. The load ﬂow analysis is performed for minimum and maximum load
levels for no generation case and range of generations up to the DG capacity for
all lines in service and following single line outages. Voltage developments in
the network are recorded and discussed in terms of DG reactive power
requirements. Results indicate that the variable power factor operation of larger
DG units must be employed in order to ensure static voltage stability and limit
voltage ﬂuctuations. This paper makes a contribution to the electrical engi-
neering practice in terms of providing additional evidence of DG inﬂuence on
static voltage proﬁle developments and control in the practical electrical dis-
tribution system. This paper also presents basic analysis for reactive yard design
required for voltage control by larger DG units.
1
Introduction
Voltage stability is one of the most fundamental phenomena that must be achieved in
order to ensure safe operation of the power system. It is one of the three important
categories of the power system stability. The illustration of power stability classiﬁca-
tion is shown in Fig. 1. Voltage is related to active and reactive power ﬂows in the
system. Voltage instability is caused by reactive power imbalance and can be classiﬁed
as long term instability (load increase, ←load recovery after faults, reactive losses in
line ⊣\C⟵loss of reactive supply) and short term (caused by the disturbance) [1].
Voltage stability analysis is discussed in [1], using the relation between active power
and voltage magnitude in the load node, (U-P or nose curve). Very important char-
acteristic of the voltage stability issue is that phenomena involved are nonlinear [2]. In
order to simplify the voltage stability analysis, a number of indicators based on small
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_3

signal properties of the system, derived from linearized models are used, such as
Voltage Sensitivity Factor, deﬁned as the change in voltage at node as a function of
reactive power injection at that node, DU/DE indicator where U is the bus voltage and
E is the controllable voltage value and ﬁnally and Qg/Ql criterion which relates the load
reactive power to the reactive power generated by the synchronous machines [1].
Voltage proﬁle analysis is one of the most important and frequently considered
quantities in the DG allocation problem domain. It can be considered as both objective
function and as inequality constraint. DG allocation for voltage stability enhancement
is a well documented topic and it continues to be a vibrant area of research [3–5]. The
same applies to optimal voltage control strategies considering DG [6]. Reference [7]
discusses a simpliﬁed voltage stability method in distribution system, considering DG.
Further, [4] presents a study on combined capacitor and DG allocation for voltage
stability improvement while [5] considers voltage improvements, losses and load
variability. One of the most fundamental voltage magnitude condition, by which the
resulting voltage Vi at all points in the network must be within prescribed limits, is
deﬁned by the European norm EN50160. According to this norm, the voltage varia-
tions must be within 10% of nominal voltage, in other words:
Vmin  Vi  Vmax
ð1Þ
The existing National Electricity Rules in Bosnia and Herzegovina (BH) do not
contain code which strictly speciﬁes power factor (pf) requirements. This means that pf
has to be determined based on the network parameters and operational requirements.
The auxiliary services are not currently exercises in distribution networks in BH which
means that DG cannot bill the reactive energy delivered to the network. However, DGs
are regularly billed for the excess reactive power absorbed, which means that design,
construction and appropriate operation of shunt reactive power sources might be an
interesting option for DG owners.
Fig. 1. Schematic representation of fuzzy inference system with three input and one output
variable, based on [1]
Analysis and Control of DG Inﬂuence on Voltage Proﬁle
31

2
Voltage and Reactive Power Relationship
Voltage proﬁle developments in the power system greatly depend on the amounts of
reactive power. This also means that DG can be used to control voltage development in
the grid by absorbing or delivering reactive power. It can be expected that during heavy
loads voltage will drop and during light load periods voltage will rise. DG can con-
tribute to voltage control by implementing variable pf operation scheme. Figure 2.
show a simple illustration of phasor diagram for voltage, current and impedance angle
relations and how reactive power might inﬂuence it in the case of lagging (a) and
leading (b) pf. The most suitable power factor is one that minimizes reactive power
requirements while maintaining stable voltage levels throughout the network. One way
of improving power factor is by adding shunt capacitors which deliver some reactive
power to generators thus making more DG active power available. When capacitors are
installed in parallel with the load they improve power factor and reduce load current
and voltage drop. This is essential during high loads when costumer demand for
reactive power increases. To achieve this, DG should be operated with lagging pf, i.e.,
DG delivers reactive power. The shunt capacitors improve the stability on the grid
during high loads by preventing voltage drops. This is the reason why larger amount of
reactive power is switched in during heavy loads when switchable shunt devices are
available. The sending end voltage (Vs) which represent the network connection point,
can also be reduced by shifting the current phase in front of the receiving end voltage
(Vr). In the case of light load when the voltage increase is high, the static voltage
increase on the grid is reduced and DG must be operated with leading pf. For this
reason, a large portion of shunt devices is switched out of service during light loads, if
possible. In conclusion, by manipulating phasors presented in Fig. 2. It is possible to
control the voltage at the DG connection point and prevent it from rising too high or
dropping too low. In this way, the ratio of active and reactive power is thus determined
and assigned to a DG as an operational requirement.
3
Experimental Setup
The single line diagram of the test system is shown in Fig. 3. In total, there are three
customer zone substations marked as SUB A, B and C, which are fed by a total of six
aluminium conductor lines. L5 and L6 have the highest thermal and current rating and are
used to (SUB A, B, C) to the source station. L2 is altered to accommodate DG connection
and is divided in L2a and L2b. In SLD, this BB 10 is deﬁned as swing bus and is used as
Fig. 2. Phasor diagram of V, I and Z for: a lagging pf b leading pf
32
M. Šarić et al.

reference point. Transformers are modelled with resistance and reactance of 0.005 and 0.1
p.u. respectively and all transformer MVA ratings are per winding base. At times of low
demand, pf generally improves and gets closer to unity. This means that demand for
reactive power decreases and some or all capacitors need to be switched off. This is
important since reactive power has great inﬂuence on voltage development in power
system. It therefore needs for to be determined when capacitors will be switched in and out
of service. SUB a capacitor banks are VAr controlled and switch on and off automatically
based on the amount of reactive load requirements. In fact there is one 6.8 MVAr
capacitor bank with step switch resulting in two effective capacitor banks each of
3.4 MVAr. Controller settings result in stack 1 switching on at a feeder load of 2.0 MVAr
and switching off at a load of −2.0 MVAr. Stack 2 switches on at a feeder load of
4.0 MVAr and off at a load of 0.0 MVAr. SUB B has two time clock controlled capacitor
banks. Capacitor bank no.1 size is 2.5 MVAr and is daily switched on week days.
Capacitor bank no.2 has is always on. SUB C does not have a zone substation capacitor
bank. However line capacitor banks on the distribution feeders help to keep the power
within an acceptable range and they can generally use a range of control modes from ﬁxed
(always on) to time clock, temperature controlled or even be VAr controlled.
The ﬁrst case to be analysed is the low load scenario for the unity pf which provides
useful insight into system behaviour. At this point Vr and Vs are in phase and by plotting
voltage developments against DG active power output, it is possible determine whether
voltage is rising or falling and to provide indication in which direction current curve needs
to be rotated. Generator’s reactive power is limited to zero and in case of additional
absorption capacitors banks are added to maintain unity power factor. This arrangement
Fig. 3. Single line diagram of the test system
Analysis and Control of DG Inﬂuence on Voltage Proﬁle
33

prevents reactive power interchange between grid and generators. The load ﬂow simu-
lations are performed for the various DG power output scenarios ranging from no load
scenarios up to the DG rated output power. If the system voltage is determined to be out of
the prescribed limits, load ﬂow is continued for the range of non-unity pf scenarios, in
order to determine the acceptable pf for each DG output level. Further, the identical
analysis is repeated for the worst case single line outage contingency scenario. Finally, the
same analysis is repeated for the high system load scenario. The data collected during the
analysis using maximum and minimum load situations include the voltage assessment at
each of the 13 bus bars of the test power system. Based on Eq. 1. it is necessary to ensure
that voltages will be between 0.9 and 1.1 p.u. at all times. It would be desirable to reduce
voltage oscillations further during simulations in order to allow for transient develop-
ments and voltage rise occurring within DG reticulation system. An extra 0.3 p.u. will be
allowed for this and reactive power design at this stage is aimed at limiting maximum
voltage at 1.07 p.u. (1.1 – 0.3) and similarly, minimum voltage to 0.93 p.u. The results are
presented and discussed in following section.
4
Results and Discussion
This section of the paper presents the results and discussion of the obtained results from
the simulations, which were carried out as described in previous sections.
4.1
Low Load Scenario
Figure 4 shows static voltage developments on all 13 buses during low load with all
lines in service. It is immediately clear that this is a highly unstable operating in the
Fig. 4. Voltage Developments for unity pf-low load scenario, all lines in service
34
M. Šarić et al.

case of increased DG output since even relatively small generation output values cause
large voltage ﬂuctuations. The voltage needs to be decreased, which means that DG
should be operated at a leading power factor.
Further, Fig. 4 also provides another important insight into system behaviour. It
could have been assumed that BB11, which represents the point of common coupling
would be the point with largest voltage value. However, it can be seen that costumer
load buses are more critical. Namely, buses 1, 5 and 7 experience high voltage rises and
cause instability. This would cause large disruptions of supply and needs to be avoided.
For example, low voltage side at SUB A reaches over 11% of increase in full capacity
and this can be even higher in reality. Bus11 is also exceeds 1.07 p.u. at maximum
generation capacity. With reference to Fig. 4, voltage can be decreased if generators
absorb reactive power and force pf to become leading. This is modelled in load ﬂow
software by inserting negative values in the generator Q ﬁeld. Initial estimate is pre-
sented which is then being adjusted until acceptable voltage values are reached for all
buses. The main objective of this simulation was to minimize the amount of Q absorbed,
thus keeping pf as close to unity as possible while ensuring that minimum margin of
voltage stability was reached. Figure 5 presents values of proposed pf (leading) for
different values of generation for all lines in service during the low load scenario. This
scenario counts for approximately 99% probability to occur during low load.
This is therefore the recommended mode of operation for normal condition at low
load. Figure 5 also shows values of B shunt used in simulation to hold required levels
of Q. The pf operational scheme presented in Fig. 5 ensures voltage static stability with
minimum margin, thus minimizing the amounts of reactive power. The beneﬁts of this
Fig. 5. Pf (Leading) and B shunt values yielding stability for low load all lines in service
scenario
Analysis and Control of DG Inﬂuence on Voltage Proﬁle
35

pf analysis are presented in Fig. 6 which demonstrates that the voltage levels are within
required limits. Load buses still have highest values, but do not exceed speciﬁed limits.
It can therefore be concluded that in the case of low load with all lines in service, the
system is statically stable when operated with leading pf schemes presented in Fig. 5.
The contingency conditions, such as single line outage represent an important
operational scenario of the power system. It can be expected that for a number of
reasons, any line could be out of service for approximately 1% of the times, which
represents around 3.5 days a year. During this unwelcome operational scenario,
additional steps need to be undertaken in order to avoid load supply interruptions and
to protect various components of the power system. In this experiment, simulations are
performed once more staring from the unity pf studying every single line outage. The
worst case scenario was determined to be following L5 being placed out of service.
During this time the highest voltage variations are noted. Figure 7 represents results of
this analysis. The voltage at bus 1 for example, rises as high as 1.14 of its nominal p.u.
value and this is not acceptable. It is also interesting to observe that voltage exceeds
limits on total of 10 nodes throughout the network. This is regarded as the case of worst
case disturbance. In order to use DG for voltage control purposes, the approach
identical to that used in the previous section is used. The results are presented in Fig. 8.
Once again, DG is to be operated with leading pf which means that it needs to absorb
reactive power in order to decrease voltage. Values of B shunt are presented on the
same graph with pf. Figure 9 shows stabilised voltage developments on all 13 buses for
this outage case. Pf varies with level of DG output but voltage remains stable. In
particular, pf improves with the increase in DG output. The other lines outage cases
display less dramatic voltage increase when compared with L5 outage and DG could be
operated with a range of pf deﬁned for the case of L5 outage. However, this is not
Fig. 6. Voltage developments versus Power for determined pf Value
36
M. Šarić et al.

entirely true in the case of L2a outage. When the unity pf case was presented, it was
observed that voltage kept rising with the increase in generation. It was found that at P
levels beyond network capacity, voltage slowly starts to drop. In the case of L2a
outage, this change of direction occurs before maximum rating has occurred. This is a
realistic possibility and needs to be accounted for in the analysis even if it has a low
probability to occur. This situation is presented in Fig. 10. At full capacity, 3 points in
the network have voltage lower than allowable value. These are buses 5, 12 and 13.
Fig. 7. Voltage versus Power for L5 out of service scenario, low load
Fig. 8. pf and B Shunt versus DG power output values
Analysis and Control of DG Inﬂuence on Voltage Proﬁle
37

This instability issue can be resolved by reversing the direction of phasor rotation
and making the pf lagging when the voltage starts to drop. In other words, generator
stars delivering reactive power which now has the same sign as active power from
generator point of view. A summary of these results is presented in Fig. 11. together
with values of B shunt used during simulations to maintain DG operation with desired
power factor. If DG is operated with the range of power factors presented in Fig. 11,
the network voltage will remain stable in the case of L2a outage for entire a range of
Fig. 9. Voltage developments versus Power for determined pf value
Fig. 10. Voltage versus DG output power for the Case of L2a outage during low load
38
M. Šarić et al.

generation. At this point it can be concluded that system can be operated in a stable and
safe way in the case of low load under conditions of voltage control with appropriate
values of pf by DG. Finally, it was determined that variable pf is more efﬁcient from
both stability and possibly a ﬁnancial point of view. Finally, the range of variable DF pf
was determined, ranging from 0.92 leading to 0.99 lagging. Each pf value is assigned
to particular power system operational scenario.
4.2
High Load Scenario
In the case of high load scenario, the analysis demonstrated that in the case of normal
operation, system can be operated with unity pf without compromising voltage values.
This is desired result since it might provide ﬁnancial beneﬁt to the operator. The results
of the load ﬂow analysis have conﬁrmed that voltage on all buses remains within
stability limits and these results are presented in Fig. 12. Similar to the low load case,
the contingency condition following single line outage for the high load scenario was
also considered. It was determined that the worst case outage for high load occurs when
L6 is placed out of service. In this case, the highest voltage drop is recorded in the
network. Figure 13 shows results of the analysis with L6 out of service and it can be
observed that there is a sharply and deep voltages drop which amounts to approxi-
mately 19%. This is not an acceptable operating condition and requires design of
appropriate operational strategy in order to bring the voltage back to allowable limits.
This contingency condition causes voltage on all load buses fall below permitted
values. The worst voltage drops are recorded on buses 13 and 12, both having the value
of 0.813 p.u. In order to ensure system stability, the voltage throughout the network
now need to be increased and this means that DG must be operated at lagging pf. The
analysis demonstrated that all other outage cases are proven to be less dramatic, as they
all display smaller voltage drops when compared to L6 outage. This means that pf
Fig. 11. Pf and B shunt versus P for the Case of L2a out, low load
Analysis and Control of DG Inﬂuence on Voltage Proﬁle
39

designed in the case of L6 outage will stabilise the voltage in the case of any other
single line outage. Figure 14 shows the range of variable pf recommended for running
the DG as a function of generation output. B shunt values, used to maintain desired pf
in software simulations are also presented. In the case of no generation, system is still
unstable and 20 MVAR are needed to keep the voltage up in the prescribed range. Pf at
10 MW generation is 0.67 and it was more practical to assign this value to the case of
Fig. 12. V versus P for high load normal mode of operation, all line in service
Fig. 13. Voltage developments versus DG power output for L5 out of service, high load
40
M. Šarić et al.

no generation, for the purposes of better illustration. This should not be confused to the
actual zero pf DG displays in the case of no generation scenario. Figure 15 shows the
ﬁnal static voltage developments in the case of L6 outage during the high load when
generators are operated according to pf deﬁned in Fig. 14.
Fig. 14. Pf and required B Shunt values versus DG power output, L2a out of service, low load
Fig. 15. Schematic representation of fuzzy inference system with three input and one output
variable
Analysis and Control of DG Inﬂuence on Voltage Proﬁle
41

5
Limitations and Future Work
The investigations presented in this paper present some of the most important aspect so
power system analysis during the DG connection pre approval phases. The required
parameters that need to be examined at this stage are by no means exhausted and need
to be included in comprehensive pre connection approval report. Further, one of the
most important future research direction proposed by the authors is practical conﬁr-
mation of obtained results especially in terms of pf design for various operating con-
ditions. Finally, design and experimental conﬁrmation of dedicated voltage controllers
should prove to be an interesting practical implication of research results presented in
this paper.
6
Conclusion
This paper presented results of investigations related to voltage variations and reactive
power requirement in power distribution system, considering DG. The results were
obtained from a series of load ﬂow simulations performed for the various DG power
output scenarios ranging from no load scenarios up to the DG rated output power for all
lines in service and for the worst case contingency scenario caused by a single line
outage. Results indicated that DG connection has signiﬁcant inﬂuence on voltage
developments on the network which can be controlled by adoption of the appropriate
voltage control strategy by DG operator. However, in deregulated markets, the interest
of DG operator might be different from interest of system operator. Results further
suggest that for larger DG units, construction of shunt reactive power sources might be
necessary. Contingency scenarios such as single line outages require special opera-
tional strategies. Finally, important future research directions arising from the results of
this paper were suggested.
References
1. Andersson, G.: Modelling and analysis of electric power systems. In: Lecture 227-0526-00,
ITET ETH Zurich EEH—Power Systems Laboratory (Sept 2008)
2. Van Cutsem, Vournas, : Voltage Stability of Electric Power Systems, 1st edn. Springer, USA
(1998)
3. Moradi, M.H., Abedini, M.: A combination of genetic algorithm and particle swarm
optimization for optimal DG location and sizing in distribution systems. Int. J. Electr. Power
Energy Syst. 34(1), 66–74 (2012)
4. Pradeepa, H., Ananthapadmanabha, T., Sandhya Rani, D.N., Bandhavya, C.: Optimal
allocation of combined DG and capacitor units for voltage stability enhancement. Procedia
Technol. 21, 216–223 (2015)
5. Poornazaryan, B., Karimyan, P., Gharehpetian, G.B., Abedi, Mehrdad: Optimal allocation
and sizing of DG units considering voltage stability, losses and load variations. Int. J. Electr.
Power Energy Syst. 7, 42–52 (2016)
42
M. Šarić et al.

6. Castro, J.R., Saad, M., Lefebvre, S., Asber, D., Lenoir, L.: Optimal voltage control in
distribution network in the presence of DGs. Int. J. Electr. Power Energy Syst. 78, 239–247
(2016)
7. Liu, K.Y., Sheng, W., Hu, L., Liu, Y., Meng, X., Jia, D.: Simpliﬁed probabilistic voltage
stability evaluation considering variable renewable distributed generation in distribution
systems. IET Gener. Transm. Distrib. 9(12), 1464–1473 (2015)
Analysis and Control of DG Inﬂuence on Voltage Proﬁle
43

Fuzzy Logic Based Approach for Faults
Identiﬁcation and Classiﬁcation in Medium
Voltage Isolated Distribution Network
Mirza Šarić(&), Tarik Hubana, and Elma Begić
Public Enterprise Elektroprivreda of Bosnia and Herzegovina, Mostar, Bosnia
and Herzegovina
{m.saric,t.hubana,elma.begic}@epbih.ba
Abstract. Power system faults are unwelcome events which pose a safety,
technical and social hazard. Power system operators are under increasing
pressure from customers and regulators to maintain high levels of power system
reliability. Appropriate fault identiﬁcation and classiﬁcations is a crucial part of
power system operation and management. This paper proposes a method for
identiﬁcation and classiﬁcation of faults in 10 kV isolated distribution network.
The proposed method uses three line voltages, available at the substation as
input variable and returns the fault code as the output variable for all ten shunt
fault types, as the output variable. Classiﬁcation is based on the fuzzy system
with three input variables and one output variable. Results show that proposed
system can detect low resistance faults with 100% accuracy for all then shunt
fault types that can occur in electric power distribution network. The input data
(voltage waveforms) are generated in simulations performed on a model of
realistic distribution system in Bosnia and Herzegovina.
1
Introduction
Electrical power systems are more frequently operated close to their technical limits
due to the increase of renewable energy systems and distributed generation. Therefore
they become more prone to fault occurrences. Especially the medium voltage grid is
affected since it experiences the most signiﬁcant changes. Therefore fault detection
identiﬁcation and localization are of great concern for the distribution grid operators.
Fault identiﬁcation and localization mainly includes the following aspects: type,
direction, and distance. It is generally independent from protective relaying. The
purpose is not to protect assets through a short-term generation of a trip signal to de
energize a faulted section as fast as possible. The objective is to supply the grid
operator with information about the fault prior to an on-site investigation. Transmission
lines are most prone to occurrence of fault. Fault detection, direction estimation and
faulty phase selection play a critical role in the protection for a transmission line.
Accurate and fast fault detection and classiﬁcation under a variety of fault conditions
are important requirements of any protective relaying scheme [1].
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_4

This paper aims to provide a contribution toward more effective identiﬁcation and
classiﬁcation of earth faults in 10 kV isolated network. The investigated network
modelled in accordance with realistic distribution network data in Bosnia and Herze-
govina. The main characteristic of Bosnian 10 kV distribution network is lightly loa-
ded, long overhead distribution lines which run through the harsh mountain terrain and
supply electricity to remote communities. These feeders are often run from the same
substation, which is used to supply electricity to urban areas and industrial customers
via shorter, but more heavily loaded underground cables. Most of 10 kV distribution
system in Bosnia and Herzegovina still operates with an isolated neutral point. This
mode of operation has numerous advantages (as well as disadvantages) over the other
methods of neutral point treatment, as shown in Table 1. The main advantage of an
isolated system is the ability of uninterrupted operation under the fault conditions
which is good for systems in the early stages of their development (such as the Bosnian
system in 1960s and 1970s). However, as the system develops and as the new sections
and feeders are added, the earth fault currents increase and overvoltage during fault
becomes a serious issue, causing an ongoing debate on the beneﬁts and challenges of
the existing conﬁguration.
2
Overview of Earth Fault Fundamentals
In the past 20 years, there has been a rapid development in various ﬁelds concerning
the detection, classiﬁcation and location of faults in power systems. The advances in
signal processing techniques, artiﬁcial intelligence and machine learning, global
positioning system (GPS) and communications have enabled more and more
researchers to carry out studies with high breadth and depth in that the limits of
traditional fault protection techniques can be stretched [2]. Earth fault is a type of
power system fault which occurs when contact is established between conducting phase
and the earth in the system with isolated neutral point of a transformer. Since in the
isolated system, there is no direct connection with the earth, the only connection that
Table 1. Overview of advantages and disadvantages of isolated system
Neutral point treatment: isolated system
Advantages
Disadvantages
Can continue to operate during the earth
fault and increase the system reliability
Overvoltage to the ground and rise of a voltage
of “healthy” phases
It has low fault current
Potential hazard
It is can be identiﬁed by observing the
voltage waveform
Difﬁcult to locate the fault
Very suitable for systems in the early
stages of development
Might not be cleared by the protection and cause
additional stress to the equipment
It is simple and relatively inexpensive
The self-healing system as arc can be
extinguished by the system
Not suitable for developed system since fault
current increases and arc extinction is less likely
to occur
Fuzzy Logic Based Approach for Faults Identiﬁcation
45

can be established is due to the ground level capacities of transmission lines. When the
earth fault is developed in an isolated network, currents ﬂow through earth capacities of
healthy phases and the earth fault current is dependent on the fault resistance and
network capacitance [3]. Figure 1 represents an illustration of basic distribution net-
work parameters during the earth fault. With reference to Fig. 2 and deﬁning the V0 as
the neutral point voltage, the general representation of the earth fault current can be
derived as follows:
Fig. 1. Illustration of basic power distribution network parameters during the earth fault
Fig. 2. Phasor diagram during earth fault in an isolated system
46
M. Šarić et al.

V1 ¼
ﬃﬃﬃ
3
p
Vnej150

ð1Þ
V2 ¼
ﬃﬃﬃ
3
p
Vnej210

ð2Þ
I1 ¼ V1jx C0 ¼
ﬃﬃﬃ
3
p
Vnx C0ej150

ej90

¼
ﬃﬃﬃ
3
p
Vnx C0ej240

ð3Þ
I2 ¼ V2jx C0 ¼
ﬃﬃﬃ
3
p
Vnx C0ej210

ej90

¼
ﬃﬃﬃ
3
p
Vnx C0ej300

ð4Þ
If ¼ I1 þ I2 ¼
ﬃﬃﬃ
3
p
Vnx C0ej240

þ
ﬃﬃﬃ
3
p
Vnx C0ej300

¼
ﬃﬃﬃ
3
p
Vnx C0ðej240

þ ej300

Þ
¼
ﬃﬃﬃ
3
p
Vnx C0ðcos 240
 þ j sin 240
 þ cos 300
 þ j sin 300
Þ
¼
ﬃﬃﬃ
3
p
Vnx C0ð1
2  j
ﬃﬃﬃ
3
p
2  1
2  j
ﬃﬃﬃ
3
p
2 Þ ¼ jVn3 x C0
ð5Þ
Earth fault modulus can be represented as:
If
j j ¼ Vn3xCtot
ð6Þ
3
Method and Data Collection
The input data (voltage waveforms) are generated in simulations performed on the
model of a realistic electric power distribution system in Bosnia and Herzegovina. The
single line diagram of power distribution network used in this study is shown in Fig. 3.
Figures 4 and 5 show the relevant electrical data of the underground cable and over-
head line conductors used to create the power distribution network model, respectively.
This is a typical 35/10 kV zone substation with eight, 10 kV feeders. The ﬁrst four are
underground cables, while feeders 5–8 are modelled as overhead lines. Normal mode
(steady state) operations and fault conditions, for each of the ten shunt fault types are
simulated and voltage waveforms for three line voltages at 10 kV substation bus bar are
reported. The types of faults are shown in Table 2. The value of an earth fault current in
an isolated network is independent from location. As shown in Eq. 5. earth fault
depends only on voltage and total line capacitance (which, obviously is related to line
lengths). Notwithstanding this, simulations are performed for faults in different parts of
the network in order to provide same diversity of input data. It was observed that some
difference in voltage at the faulted line measure at 10 kV substations is present as a
function of fault locations. For example, voltage in phase 1 drops to zero at the fault
locations, and increases to some 10% of nominal value as we move away from the
network.
Fuzzy Logic Based Approach for Faults Identiﬁcation
47

Fig. 3. Single line diagram of power distribution system used in this study
Fig. 4. Relevant electrical data for underground cables used in the model
Fig. 5. Relevant electrical data for the overhead line conductor used in the model
48
M. Šarić et al.

3.1
Development of Fault Classiﬁcation and Identiﬁcation System
Figure 6 represents schematic representation of the proposed system for fault identi-
ﬁcation and classiﬁcation. It is conveniently divided into four logical parts which might
be considered as stages of the proposed system. The ﬁrst stage is concerned with signal
generation and the obtained txt ﬁles contain signal time series are stored in the data-
base. The following stage is named signal processing and is used to store values of each
line phase as a separate vector. The next step in the signal processing stage is the
calculation of root mean square value for each vector. The obtained values are fuzziﬁed
and used as inputs into a fuzzy inference engine. The output of a fuzzy inference
procedure is a fuzzy variable which needs to be defuzzifﬁed in order to give a ﬁnal,
single valued quantity which represents the fault code. The ﬁnal values of the system
are represented by two vectors a and b, where a contains the information on fault code
and b on the number of repetitions for each fault code [4]. The proposed system
procedure is concluded by the output formatting and reporting stage which is used to
graphically represent the output data. Following subsections provide some more
details.
3.1.1
Data Collection
The generated waveforms are sampled at a rate of 1000 Hz and saved in database as txt
ﬁles. The total of 900 simulations was run, which produced 900 text ﬁles. Table 2
provides a summary of simulations performed for each type of fault and normal
operation conditions.
3.1.2
Fuzziﬁcation
If deﬁne l~A x
ð Þ 2 0; 1
½
 as membership function and X as a Universe or universe of
discourse, a fuzzy set ~A in X is expressed as a set of ordered pairs [5]:
~A ¼
x; l~A x
ð Þ


x 2 X
j


ð7Þ
Fuzzy logic models are particularly useful in applications which require the
description of imprecise and complex processes. Such descriptions are performed with
the use of fuzzy sets used in the process of fuzzy logic control [6]. In the proposed
Fig. 6. Schematic representation of the system for fault identiﬁcation and classiﬁcation
Fuzzy Logic Based Approach for Faults Identiﬁcation
49

fuzzy system, input and output linguistic variables are described by an expert
knowledge and represented by sets ~A, ~B, ~C and ~O which contain single terms Ai, Bj, Cl
i Ok [6]:
~A ¼ A1; A2; . . .; Ai; Ai þ 1; . . .; An
f
g
~B ¼
B1; B2; . . .; Bj; Bj þ 1; . . .; Bm


~C ¼ C1; C2; . . .; Cl; Cl þ 1; . . .; Cz
f
g
~O ¼ O1; O2; . . .; Ok; Ok þ 1; . . .; Of
f
g
ð8Þ
Terms Ai, Bj, Cl and Ok are fuzzy sets deﬁned as [6]:
~Ai ¼
x; l~Ai x
ð Þ

			x 2 ~Ai  U1
o
; i ¼ 1; . . .; n
n
~Bj ¼
y; l~Bj y
ð Þ

			y 2 ~Bj  U2
o
; j ¼ 1; . . .; m
n
~Cl ¼
z; l~Cl z
ð Þ

			z 2 ~Cl  U3
o
; l ¼ 1; . . .; z
n
~Ok ¼
w; l~Ok z
ð Þ

			w 2 ~Ok  U4
o
; k ¼ 1; . . .; f
n
ð9Þ
In particular, in the proposed model input variables are values the root mean square
of three line voltage, represented by Eqs. 10–16. Graphical representation of input
variables is presented in Fig. 7 while the output variable is presented in Fig. 8. Some of
the values are sub normalized in order to improve the classiﬁcation accuracy.
~V1 ¼ A1; A2; A3; A4
f
g
ð10Þ
~V1 ¼ verylow; low; normal; high
f
g
ð11Þ
Table 2. Number of generated samples, and binary and decimal binary code
Type of fault Number of samples Binary code Decimal code
none
100
0000
0
a–g
100
1001
8
b–g
100
0101
5
c–g
100
0011
3
abc
100
1110
14
ab–g
100
1101
13
ac–g
100
1011
11
bc–g
100
0111
7
ab
50
1100
12
ac
30
1010
10
bc
20
0110
5
50
M. Šarić et al.

~V2 ¼ B1; B2; B3; B4
f
g
ð12Þ
~V2 ¼ verylow; low; normal; high
f
g
ð13Þ
~V3 ¼ C1; C2; C3; C4
f
g
ð14Þ
~V3 ¼ verylow; low; normal; high
f
g
ð15Þ
~F ¼ O1; O2; O3; O4; O5; O6; O7; O8; O9; O10; O11
f
g
ð16Þ
~F ¼ none; ag; bg; cg; abc; abg; acg; bcg; ab; ac; bc
f
g
ð17Þ
Fig. 7. Graphical representation of fuzzy to represent input variables
Fig. 8. Graphical representation of fuzzy set used to represent output variable
Fuzzy Logic Based Approach for Faults Identiﬁcation
51

3.1.3
Inference
The inference model used in this paper is Mamdani type inference with three inputs
(root mean square of three line voltage) and one output (fault code) variable [7].
Schematic representation of the proposed inference model is shown in Fig. 9.
3.1.4
Rule Base
A total of eleven IF…AND…THEN rules are created on the bases of expert knowl-
edge. They form a rule base and can be represented as shown in Table 3.
3.1.5
Defuzziﬁcation
The last component of fuzzy system is defuzziﬁcation stage, sometimes also called
decoding. The purpose of this stage is to produce a non-fuzzy control output which
adequately represents the membership function laggðzÞ[6]. There are numerous
Fig. 9. Schematic representation of fuzzy inference system with three inputs and one output
variable
Table 3 Rule base
Rule 1
if (v1 is normal) and (v2 is normal) and (v3 is normal) then: no fault
Rule 2
if (v1 is very low) and (v2 is high) and (v3 is high) then: ag fault
Rule 3
if (v1 is high) and (v2 is very low) and (v3 is high) then: bg fault
Rule 4
if (v1 is high) and (v2 is high) and (v3 is very low) then: cg fault
Rule 5
if (v1 is very low) and (v2 is very low) and (v3 is very low) then: abc sym fault
Rule 6
if (v1 is very low) and (v2 is very low) and (v3 is high) then: abg fault
Rule 7
if (v1 is very low) and (v2 is high) and (v3 is very low) then: acg fault
Rule 8
if (v1 is high) and (v2 is very low) and (v3 is very low) then: bcg fault
Rule 9
if (v1 is low) and (v2 is low) and (v3 is normal) then: ab fault
Rule 10 if (v1 is low) and (v2 is normal) and (v3 is low) then: ac fault
Rule 11 if (v1 is normal) and (v2 is low) and (v3 is low) then: bc fault
52
M. Šarić et al.

defuzziﬁcation methods and in this paper, center of gravity method is chosen as
defuzziﬁcation method [8]. It is important to note that one can refrain from comput-
erized defuzziﬁcation, which means that one characterizes the faulty situation by a
gradual representation rather than by a yes-no statement [9].
4
Results and Discussion
The output values of the proposed identiﬁcation and classiﬁcations systems are two
vectors a and b which contains data on fault type and occurrence (in the form of
decimal fault code). Figure 10 show a graphical representation of these vectors. Results
shown in Fig. 10 are the output result of the proposed system, which for each fault type
gives occurrence rate. It can be seen, by comparing results in Fig. 10 with results
presented in the second column of the Table 2 that the proposed system is capable to
identify and classify the events with 100% accuracy.
5
Limitations and Future Work
This paper proposed a promising system for identiﬁcation and classiﬁcation of power
distribution network faults. The study focused signal processing of voltage waveforms,
used as input variables in the fuzzy system for identiﬁcation and classiﬁcation of power
distribution network faults. Proposed system proved to be very efﬁcient for identiﬁ-
cation and classiﬁcation of low impedance faults. Further investigation is required to
take into consideration fault resistance. The proposed system could be extended to
include identiﬁcation and classiﬁcation algorithms that are capable to account for
different network conﬁgurations (neutral point treatment). Particularly interesting
would be investigation of hardware implementation possibilities of the proposed
system.
Fig. 10. Output results of the proposed system—fault types and occurrence
Fuzzy Logic Based Approach for Faults Identiﬁcation
53

6
Conclusion
This paper proposed a fuzzy logic based approach to classiﬁcation and identiﬁcation of
isolated power distribution network. It was demonstrated that the proposed system is
able to identify the phase(s) involved in all ten types of shunt faults that may occur in
an electric power distribution system and to use this information to perform fault
classiﬁcation. The proposed method uses three line voltages, available at the substation
as input variable and returns the fault code as the output variable. The input data
(voltage waveforms) are generated in simulations performed on a model of realistic
distribution system in Bosnia and Herzegovina.
References
1. Mor, V., Vaghamshi, A.: Review on fault detection, identiﬁcation and localization in
elektrical networks using fuzzy-logic. Int. J. Appl. Res. Sci. Eng. (2016)
2. Chen, K., Huang, C., He, J.: Fault detection, classiﬁcation and location for transmission lines
and distribution systems: a review on the methods. High Volt. 1(1), 25–33 (1997)
3. Ćućić, R., Komen, V., Živić-Đurović, M.: Neutral point concept in distribution network. Eng.
Rev. 28(2), 77–89 (2008). Available at www.hrcak.srce.hr/ﬁle/48475. Accessed 15 Jan 2017
4. Das, B.: Fuzzy logic-based fault-type identiﬁcation in unbalanced radial power distribution
system. IEEE Trans. Power Deliv. 21(1), 278–285 (2006)
5. Ross, T.J.: Fuzzy logic with engineering applications, 2nd edn, p. 2004. Wiley, England
(2004)
6. Bojadziev, G., Bojadziev, M.: Fuzzy logic for business, ﬁnance and management. In:
Advances in Fuzzy Systems: Applications and Theory, vol. 23 (2007)
7. Tomosovic, K., Chow, M.Y.: Tutorial on Fuzzy Logic Applications in Power Systems.
IEEE-PES Winter Meeting, Singapore (2000)
8. Adhikari, S., Sinha, N., Dorendrajit, T.: Fuzzy logic based on-line fault detection and
classiﬁcation in transmission line (2016). Available at: http://link.springer.com/article/10.
1186/s40064-016-2669-4 Accessed 15 Feb 2017
9. Frank P.M., Seliger B.K.: Fuzzy logic and neural network applications to fault diagnosis. Int.
J. Approximate Reasoning (1997)
54
M. Šarić et al.

Inﬂuence of Solar PVDG on Electrical Energy
Losses in Low Voltage Distribution Network
Edin Šemić1(&), Mirza Šarić2, and Tarik Hubana2
1 Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and
Herzegovina
esemic1@etf.unsa.ba
2 Public Enterprise Elektroprivreda of Bosnia and Herzegovina, Mostar, Bosnia
and Herzegovina
{m.saric,t.hubana}@elektroprivreda.ba
Abstract. Renewable energy based distributed generation (DG) integration
provides potential beneﬁts to conventional distribution systems. However, DG
has inﬂuence on the power ﬂow in power system and hence presents one of the
major concerns in modern distribution systems. It gives signiﬁcant impact since
distributed generators can affect the distribution system, both positively and
negatively. In this paper, the inﬂuence of photovoltaic DG on electrical energy
losses in low voltage network is investigated. Experimental results and data
measured from Bosnia and Herzegovina low voltage distribution network are
used for model development. A novel approach using Distribution Loss Factor
(DLF) concept combined with power generation-demand probability is used for
loss calculation. This paper does not introduce the DLF concept, but takes it
from literature for the purpose of calculating losses in the considered network.
This paper makes a contribution towards investigation of the exact photovoltaic
distributed generation contribution to the overall cost of the network operation
and managements. The obtained results are useful for distribution system
operator in terms of better loss evaluation, and proposed analysis method could
be accepted as an important part of the distributed generation planning process.
1
Introduction
The traditional, vertically integrated utility incorporating generation, transmission,
distribution, and customer energy services is in the beginning stages of what could
prove to be quite revolutionary changes. The era of ever-larger central power stations
seems to have ended. The opening of the transmission and distribution grid to inde-
pendent power producers who offer cheaper, more efﬁcient, smaller-scale plants is well
underway. This change inﬂuences the vertically integrated systems and thus leads to
unbundling of generation, transmission and distribution segments into various auton-
omous business units. The objective of this change is to terminate the monopoly of
service providers and to provide reliable services to generators and consumers to
comply with the power agreement between them at speciﬁed tariff. The tariff includes
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_5

the charges related to services made available to generators and consumers, and a
signiﬁcant share/portion of these charges is associated with power loss. Therefore,
network loss plays a decisive role in determining the tariff of generators and consumers,
and therefore it should be allocated correctly and justiﬁably [1].
Under such deregulated environment, the generators would be in competition with
each other to serve more than one utility distributor. Generation is assigned by auction
or trade-contract instead of economic dispatching orders. Unlike the generation,
transmission and distribution segments, however, are immune to competition and are
generally considered as a natural monopoly. Like transmission network, the distribu-
tion network is also a service provider which charges the consumers for its services
with clearly deﬁned economic and technical logic. A large share of service charges is
occupied by the power loss. Also, the changed interaction between suppliers and
consumers has led to increase attention towards the economic aspects of the electricity
supply management, [2].
2
Photovoltaic-Based Distributed Generation
The use of solar energy as one of the renewable energy sources contribute to the
preservation and conservation of environment due to its cleanliness, emission reduction
and free pollutions. Photovoltaic-based distributed generation (PVDG) decrease gen-
eration of electrical energy by fossil fuel based power plant, so PVDG contribute to
decrease emission of CO2. PVDG is one of the major concerns in power system. PVDG
is a grid connected generation placed closer to load demand regardless of its power
capacity.
To put it simply, PVDG is a small-scale solar based generating plant directly
connected to distribution or transmission networks, or on the consumer site of the
meter. The capacities of DG units are setting out from several kilowatts to hundreds of
Megawatts. The utilization of PVDG into a distribution system will donate positive and
negative effects based from the characteristics itself and the operating management of
the distribution scheme. PVDG impacts depends on its utility system interfaced, the
size of DG units, the amount of capacity of the PVDG relative to the system, the size of
generation based on the load at the interconnection point, and the feeder voltage
regulation practice [3].
This paper evaluates the effect of PVDG penetration on power losses in the low
voltage distribution network. An expression of power losses in the network is very
important due to their impact on the management of companies and price of electrical
energy. Because of that it is necessary evaluate losses in order to provide optimal
electricity distribution cost, as a public service, and enable the achievement of the
optimum distribution network usage fee for the buyers.
56
E. Šemić et al.

3
Distribution Generation Impact on Distribution Network
In the past, the loss allocation problem was addressed only to transmission networks,
but with the increased penetration of distributed generation (DG) and introduction of
competition among suppliers, this problem became very interesting for distribution
networks [2].
There are various reasons for the increasing integration of DGs in the distribution
network. It is about the continuing environmental pollution, intensive climate change,
rising fossil fuel prices and forecasts of their disappearance in the future, etc. which
leads people to rationalize the use of energy through the implementation of energy
efﬁciency and renewable energy. Because of this, there is a continuously increasing
exploitation of renewable energy sources. Electric power systems have been originally
designed based on the unidirectional power ﬂow, but the concept of distributed gen-
eration (DG) has led to new considerations concerning the distribution networks. The
presence of distributed generations has signiﬁcant technical and economic impact on
the radial distribution network (RDN) as it alters the power ﬂows in the network from
unidirectional to bidirectional and thus affects network losses. In addition to this, DG
also changes the total network losses depending on its location and power rating.
Therefore, distributed generations (DG) should be rewarded/penalized according to its
impact on network losses. Further, the loss in a branch is a quadratic function of
current/power ﬂowing through it because of DGs and loads. Owing to this non-linearity
and interdependency between the network participants, losses cannot be assigned in a
straight forward manner among them. The losses allocated by any approach always
contain some degree of capriciousness [1].
The methodology of loss allocation should [2]:
• be accurate, consistent, economic, efﬁcient, simple/easy to understand and
implement;
• send correct signal of real and reactive power of DG and load;
• utilize only on-site practical value of the network data; and
• provide appropriate reward/penalize to DGs and consumers.
The penetration of DG may impact the operation of a distribution network in both
beneﬁcial and detrimental ways. Some of the positive impacts of DG are: voltage
support, power loss reduction, support of ancillary services and improved reliability,
whereas negative ones are protection coordination, dynamic stability and islanding. In
order to maximize beneﬁts and minimize problems, technical constraints concerning
the interconnection of DG units and their penetration levels are being adopted
worldwide. Furthermore, the presence of DG in the deregulated market has raised new
regulatory issues, concerning ﬁnancial incentives, cost allocation methods, generation
management techniques, etc. [4]. Recently, solar has become a popular distributed
generation option. Although the energy output is clean, it is also intermittent, making it
an incomplete strategy for businesses that need power around the clock, including
when the sun is not shining.
Inﬂuence of Solar PVDG on Electrical Energy Losses in Low
57

4
Distribution Loss Factor
Abandoning of the traditional vertically integrated power system demands more
parameters for system quantiﬁcation. Market liberalization is introducing new pricing
mechanisms and it is becoming increasingly important to transparently allocate energy
losses to each market participant [5]. The classical market clearing procedure does not
explicitly account for network losses and nonlinear electrical laws do not allow the
determining power ﬂow for a given generator and producer [6]. But this can be a
crucial factor when it comes to the investment decision. DLF is the important parameter
used for a number of purposes. It is used to estimate the average losses for energy
conveyed in transmission and distribution network connection point [7]. In Bosnia and
Herzegovina, distribution system losses are not eliminatory for new connections of
DGs on the network. This means that all DGs will be treated equally, regardless of the
losses that may occur with their integration. Further, it is used for regulatory purposes
to determine the amount of losses recognized in the annual retribution scheme of DSO.
This approach has stirred some controversy since it leads to the situation in which DSO
with good losses performance will be more appreciated than DSO with poor losses
performance [8]. Further, the consideration of site speciﬁc DLF is an important aspect
of electricity market design in terms of how much reward should be allocated to a
particular DG for its output [9]. Finally, DLF consideration appears to be a very
promising methodology for distribution system planning purposes, especially the dis-
tributed resources planning and placement. In this paper, it is proposed to use the DLF
to allocate a portion of the energy losses to individual DG units. In particular, deﬁnition
of DLF is based on [10]:
DLF ¼ 1 þ Annual energy losses without generator  Annual energy losses with generator
Annual generation volume
ð1Þ
5
Simulation Model
Previously calculated data will be used for the simulation model developed in Pow-
erCAD [11]. The analyzed low voltage distribution network is located in Bosnia and
Herzegovina, in the City of Mostar. Consumption area is fed over 10/0.4 kV 400 kVA
distribution transformer, and consumers are mainly households without large electricity
consumers. This area, together with the distributed generator, is shown in Fig. 1.
58
E. Šemić et al.

A simulation model is developed in PowerCAD, on the existing simulation model
of the entire distribution network of the City of Mostar, using advanced metering
infrastructure (AMI) devices reports from this area. The developed simulation model is
shown in Fig. 2.
Since the load and generation curves are divided into 7 and 5 discretized values,
respectively, the simulation will be performed 35 times, in order to simulate all
Fig. 1. Analyzed low voltage network with PVDG connected
Fig. 2. Simulation model of the analyzed low voltage network with DG connected
Inﬂuence of Solar PVDG on Electrical Energy Losses in Low
59

different load and generation scenarios. Power losses are calculated for the entire area,
including all the low voltage cables and a transformer. Table 1 shows the simulation
results (power losses) from all simulated scenarios.
Fig. 3. Daily power demand on low voltage substation feeder
Table 1. Load duration of each interval of year
Interval
1
2
3
4
5
6
7
Load (kW)
131.8
127.94 112.33 97.43 89.51 72.6
49.94
Load (%)
97.6
94.74
83.18 72.15 66.28 53.76 36.98
Duration (%)
12.08
16.21
10.15 16.35 13.54 15.04 16.63
Fig. 4. Normalized and discretized yearly load duration curve at low voltage substation feeder
60
E. Šemić et al.

Using the results provided in Table 1, total yearly consumption can be calculated as
follows:
Etot ¼ Th *
X
7
n¼1
Ln * DLn ¼ 8760 * 95:3356 ¼ 835140 kWh:
ð2Þ
Normalized and discretized yearly load duration curve at the low voltage substation
feeder are shown in Fig. 4. The discretized yearly duration curve is obtained by
dividing normalized curve into segments in order to determine levels of consumption
(discretized values of consumption) which are used to calculate energy losses. Nor-
malized curve is divided into seven segments in order to obtain a smaller deviation of
two curves and accurate results. For each segment from Fig. 4, the area under the curve
is found by integration, and discretized values (height of the rectangle) are obtained
using found areas on appropriate time intervals. Obtained values are shown in Table 1.
Normalized yearly load duration curve from Fig. 4. is the average annual curve of load
duration based on daily power demand, which is shown in Fig. 3. The same procedure
was carried out in order to determine discretize values on the discretized curve of
generation, which is shown in Fig. 6. Normalized curve from Fig. 6. is the average
annual curve of generation of PVDG based on daily power generation of PVDG, which
is shown in Fig. 5. The normalized curve of generation is divided into 5 segments,
while ﬁfth part of the normalized curve (ﬁrst column in Table 2) refers to the time in
which the plant does not produce electricity during the year. Determined levels of
generations (discretized values of the generation) are given in Table 2.
Fig. 5. Daily power generation of the PVDG
Inﬂuence of Solar PVDG on Electrical Energy Losses in Low
61

Using the results provided in Table 2, total yearly generation from 40 kW PVDG
can be calculated as follows:
Egen ¼ Th *
X
5
n¼1
Gn  DGn ¼ 8760  12:069 ¼ 105730 kWh
ð3Þ
6
Results and Discussion
Absolute power losses shown in Table 3 are based on power ﬂows with PVDG and
without it. Absolute power losses in considered low voltage distribution network
without PVDG are shown in the ﬁrst column of Table 3. In other columns are shown
absolute losses in case low voltage distribution network connected PVDG. It is
observed that energy losses in this case are decreased due to inﬂuence of PVDG.
PVDG is located in center of consumption, therefore consumers are partially supplied
by PVDG instead of being supplied by generators which are remote with respected to
the consumption area. In this regard, PVDG alters power ﬂow in low voltage distri-
bution network under consideration, hence it has a positive inﬂuence on energy losses.
Fig. 6. Normalized and discretized yearly generation duration curve for the PVDG
Table 2. Generation duration of each interval of year
Interval
1
2
3
4
5
Generation (kW) 0
2.7
13.5
28.1
37.838
Generation (%)
0
6.75 33.75 70.25 94.57
Duration (%)
39.58 15
16.21 16.19 13.02
62
E. Šemić et al.

The simulation was conducted for various scenarios of consumption and generation,
and losses are calculated for 35 combinations of PVDG generation and consumptions.
The scenario in which consumption is at minimal value (36.98%) while PVDG gen-
eration is close to the maximum value (94.57%) is particularly interesting This is the
worst-case scenario because it leads to an increase of losses due to inability of on-site
consumption of power generated by PVDG, hence generated power must be conveyed
in area which are remote with respected to the PVDG. In this regard can be concluded
that the optimum penetration level is based on the system’s power losses. The pene-
tration level must not exceed the maximum boundary in term of total losses for the
systems to maintain its efﬁciency and reliability. In this paper, increased losses in
considered network in the worst case is on acceptable level, and still lower than losses
in case without PVDG. From data in Table 3 energy losses in case without PVDG can
be calculated as follows:
Elossðno generatorÞ ¼ Th *
X
7
n¼1
DLn * Lossn
¼ 8760 * 6:3081 ¼ 55259:38 kWh
ð4Þ
In order to determine energy losses in case of low voltage network connected
PVDG it is necessary to calculate weighting factors for each combination of con-
sumption and PVDG generation. Weighting factors are calculated by multiplying
durations of obtained generation levels and durations of obtained consumption levels
(from Table 3) in considered low voltage network. Weighting factors are shown in
Table 4. Weighting factors are multiplied by appropriate amounts of absolute losses
from Table 3 in order to calculate normalized energy losses in the low voltage network
under consideration.
Table 3. Absolute power losses in low voltage distribution network under consideration
Generation
Power (%)
0
6.75 33.75 70.25 94.57
Duration (%)
39.58 15
16.21 16.19 13.02
System load (%) 97.6
12.08 11
10.4 8.34
6.4
5.56
94.74 16.21 10.24 9.67 7.73
5.92 5.16
83.18 10.15 7.98
7.49 5.88
4.47 3.96
72.15 16.35 5.65
5.42 4.19
3.13 2.89
66.28 13.54 5.06
4.91 3.75
2.89 2.72
53.76 15.04 3.5
3.23 2.45
2.03 2.1
36.98 16.63 2.25
2.07 1.61
1.58 1.91
Inﬂuence of Solar PVDG on Electrical Energy Losses in Low
63

Normalized energy losses from Table 5 are used to calculate energy losses in
network connected PVDG. From the data presented in Table 5 energy losses in the
network connected PVDG can be calculated as follows:
Eloss generator
ð
Þ ¼ Th *
X
7
m¼1
X
5
n¼1
Lossn
¼ 8760 * 5:183974 ¼ 45411:61 kWh
ð5Þ
The results of the simulation and obtained values of energy losses in a real low
voltage distribution network with and without a PVDG show that PVDG has a positive
inﬂuence on the network in terms of reducing losses in the low voltage distribution
network connected PVDG. In the case under consideration, network connected 40 kW
PVDG decreases energy losses in an amount of 9847,764 kWh per year.
Using the calculated losses in the network with and without, Distribution Loss
Factor can be calculated according to the Eq. (1):
Table 4. Estimated weighting factor PVDG
Generation
Power (%)
0
6.75
33.75
70.25
94.57
Duration (%)
39.58
15
16.21
16.19
13.02
System load (%) 97.6
12.08 0.047813 0.01812
0.019581 0.019558 0.015728
94.74 16.21 0.064159 0.024315 0.026276 0.026244 0.021105
83.18 10.15 0.040174 0.015225 0.016453 0.016433 0.013215
72.15 16.35 0.064713 0.024525 0.026503 0.026471 0.021288
66.28 13.54 0.053591 0.02031
0.021948 0.021921 0.017629
53.76 15.04 0.059528 0.02256
0.02438
0.02435
0.019582
36.98 16.63 0.065822 0.024945 0.026957 0.026924 0.021652
Table 5. Normalized demand loss for PVDG
Generation
Power (%)
0
6.75
33.75
70.25
94.57
Duration (%)
39.58
15
16.21
16.19
13.02
System load (%) 97.6
12.08 0.525943 0.188448 0.163306 0.125171 0.087448
94.74 16.21 0.656988 0.235126 0.203113 0.155364 0.108902
83.18 10.15 0.320589 0.114035 0.096744 0.073456 0.052331
72.15 16.35 0.365628 0.132926 0.111048 0.082854 0.061522
66.28 13.54 0.27117
0.099722 0.082305 0.063352 0.047951
53.76 15.04 0.208348 0.072869 0.059731 0.049431 0.041122
36.98 16.63 0.1481
0.051636 0.043401 0.04254
0.041355
64
E. Šemić et al.

DLF ¼ 1 þ 55259:38  45411:61
105730
¼ 1:093141
ð6Þ
Since DLF is directly proportional to the difference of losses in the network with
PVDG and without PVDG, and inversely proportional to the power of PVDG, obtained
value of losses in the considered low voltage distribution network conﬁrms the positive
effect PVDG on the losses and it can be concluded that the increase of the PVDG
power to the limit where the nominator of Eq. 6 approaches to zero, i.e. where losses
with and without the DG are the same, would lead to ideal case (DLF = 1).
7
Conclusion
The aim of this paper is to investigate the extent to which PVDG contribute to energy
losses in the low voltage distribution network. In order to have a positive effect on
energy losses in low voltage network PVDG size and location must be planned
appropriately. In this paper 40 kW PVDG decreases energy losses in an amount of
9847,764 kWh per year. Based on all cases in this paper, it is identiﬁed that the system
losses are signiﬁcantly reduced with the connection of the low voltage distribution
network 40 kW PVDG. The power generated by PVDG increases the voltage around
the place it is allocated, and the consumers are partially supplied by PVDG instead of
being supplied by generators which are remote with respected to the consumption area.
In this regard, PVDG changes the power ﬂow in the low voltage distribution network
under consideration, thus reducing the energy losses. It can be concluded that the
PVDG has the best impact on energy losses in low voltage distribution network when
the power demand and PVDG power output are roughly equal. If the PVDG power
output is larger than the power demand around PVDG location, it might result in
unwanted consequences. Proposed losses calculation method and DLF have demon-
strated that this particular DG has a good contribution to the overall cost of the
network, and the obtained results are useful for distribution system operator in terms of
better losses evaluation.
References
1. Jagtap, K.M., Khatod, D.K.: Loss allocation in distribution network with distributed
generations. IET Gener. Transm Distrib. 1628–1640 (2014)
2. Jagtap, K.M., Khatod, D.K.: Novel approach for loss allocation of distribution networks with
DGs. Electr. Power Res. 143, 303–311 (2017)
3. Daud, S., Kadiri, A.F., Gan, C.K.:The impacts of distributed photovoltaic generation on
power distribution network losses. In: IEEE Student Conference on Research and
Development (SCOReD), Kuala Lumpur (2015)
4. Naik, S.G., Khatod, D.K., Sharma, M.P.: Optimal allocation of distributed generation in
distribution system for loss reduction. In: IPCSIT IACSIT Press, vol. 28, pp. 42–46.
Singapore (2012)
Inﬂuence of Solar PVDG on Electrical Energy Losses in Low
65

5. De Olivera—De Jeusus, P.M., de Leao, M.P.: Comparative analysis of different cost loss
allocation methodologies in distribution networks with distributed generation. IEEE Lat.
Am. Trans. (Revista IEEE America Latina), 3(3), 290–295 (2005)
6. Conejo, A.J., Arroyo, J.M., Alguacil N., Guijarro, A.L.: Transmission loss allocation: a
comparison of different practical algorithms. IEEE Trans. Power Syst. 17(3), 571–576
(2002)
7. ETSA: Utilities, www.aer.gov.au, March (2010). [Online]. Available: https://www.aer.gov.
au/system/ﬁles/ETSA%20Utilities%20-%20Distribution%20Loss%20Factors%20-%
20Methodology%202010–11.pdf. [Accessed 22 02 2017]
8. Saenz, J.R., Eguia, P., Berasategui, J.L., Marin, J., Arcceluz, J.: Allocating distribution
losses to customers using distribution loss factor. In: Power Tech Proceedings, 2001 IEEE
Porto, vol. 1, pp. 134–211 (2001)
9. CitiPower and Powercor: www.citipower.com.au, November (2010). [Online]. Available:
https://www.citipower.com.au/media/1278/dlf-calculation-methodology-large-embedded-
generation.pdf. [Accessed 22 02 2017]
10. Šarić, M., Čolaković, A., Rahimić, N., Boloban, A.: Inﬂuence of distributed generation on
electrical energy losses in distribution network. In: CIRED-X Conference on Electricity
Distribution of Serbia, Vrnjačka Banja, Serbia, Sep (2016)
11. Fractal Split d.o.o.: PowerCAD 4.11, Split: Fractal (2009)
66
E. Šemić et al.

Connecting a Group of Small Hydropower
Plants on the Side of Neretvica River
to a Medium Voltage Distribution Grid
Dino Macić1(&) and Mirza Šarić2
1 Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and
Herzegovina
dmacic2@etf.unsa.ba
2 Public Enterprise Elektroprivreda of Bosnia and Herzegovina, Mostar, Bosnia
and Herzegovina
m.saric@elektroprivreda.ba
Abstract. Since Bosnia and Herzegovina is a country famous for its river ﬂows
there has been an expansion of constructing small hydropower plants
(SHP) where the whole process includes projecting, gaining concessions and
building. ‘JP Elektroprivreda’ has a plan for constructing 15 small hydropower
plants on the side of Neretvica. The construction is planned throughout three
phases. This paper discusses the issues of connecting small hydropower plants
of the planned ﬁrst phase to a medium voltage distribution grid, and effects it has
on the distributive grid. The already mentioned hydropower plants are to be
connected to an existing 35 kV voltage grid, although the strategical commit-
ment of JP Elektroprivreda was to leave this voltage level back. The main
focuses of this paper are the models of calculating the power ﬂow, the losses of
the voltage, the short circuit calculations, and the evaluation of the effect of
connecting a SHP group on the existing 35 kV distributive grid. All the cal-
culations are made using the professional software PowerCAD. Results of
analysis are showing meaningful impact of SHP group connecting to an existing
distributive grid, which are mostly seen voltage changes and increases in grid
loses. The parameters that are important for the work of this project stayed in the
allowed domain after the simulation of the SHP group has been done.
1
Introduction
The early 90s were characterised by the excessive grow of the usage of renewable
energy sources. The main condition for this growth was the technological development,
strictly speaking, in the ﬁeld of the energetical electronics. The mentioned growth
presented challenges for using that knowledge in the distributive as well as in the
general electro energetical system. The medium and the low voltage grids gained a new
philosophy of planning, projecting and exploiting. The distributive electro energetic
system is made of a distributive electro energetic grid, consumers of the electrical
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_6

energy and the distributive sources (generators) that are connected to this grid. By
connecting the distributive industry the network becomes active, which means the
power ﬂow in the distributive network isn’t going in one way anymore which could
lead to serious negative effects, such as huge losses, the ﬂuctuation of the voltage and
frequency outside the proposed domain, etc. Because of that it is very important to
make an effective control of the distributive industry, especially with many distributive
generators on board. Distributive generators are low power generators, connected
directly to the distributive grid, and using as a main source of energy the renewable
sources [1]. By saying renewable energy it is mean the energy subtracted from the
natural renewable processes, such as: geothermal energy, biomass energy, solar energy,
the energy of small water ﬂows and the wind energy. While the world has gone mad for
the producement of energy using windmills and solar panels, Bosnia and Herzegovina
is known for the development of small hydropower plants, because of the unused water
potentials it has. The small hydropower plant is a plant that transforms the energy of the
small water ﬂows into electrical. The output power of such a plant varies between
10 kW and 10 MW. This type of plants has no signiﬁcant environmental effects and
has great advantages at the same time (reducing the emission of greenhouse gases,
small area needed, small costs, has positive social effects-employment, etc.). The usage
of SHP is gaining popularity of the scientiﬁc and the technical society in the last couple
of years. Thus, the paper [2] is focusing on the identiﬁcation of the behaviour of a
group of synchronous generators connected to a medium voltage grid while applying
some noise, using the real system of six synchronous SHP generators. This work [3]
discusses the issues of connecting three SHP on a distributive grid in the location of
Vakuf municipality and the various voltage effects of such a connection. Beside
connecting SHP on a medium voltage grid, they can also be connected to a low voltage
distributive grid. The issues of making such a connection are discussed in the paper [4].
The most important aspect of connecting the SHP is the economical one. Thus, paper
[5] contains a techno-economical analysis of connecting small hydropower plants on a
distributive grid. Connecting distributive sources lead to reorganising the power ﬂows
in the distributive network, thus leading to changes in the amount of losses of the active
energy. In the paper [6] is the methodology of calculations of the annual losses of
active energy in the distributive grid in the occasions of a connected distributive source,
where it starts from the known slopes of load and producement.
2
Small Hydropower Plants on the Side of Neretvica River
In the municipality of Konjic, on the side of Neretvica river, there is a plan to build 15
small hydropower plants throughout three phases. All of the 15 small hydropower
plants are derivatebly ﬂowable-pressurable, and the overall installed power is to be
24.5 MW, with annual producment of 99.9 GWh. The whole project is valued at 103
million BAM, and its ﬁnanced by JP EPBiH and EBRD credit foundation. The planned
realisation of the project is given in the table: (Table 1)
68
D. Macić and M. Šarić

3
The Regulative Demands and Models
The technical recommendation for the connection and the plant of distributive gener-
ators was approven by JP Elektroprivreda BiH on 11.8.2016 (registered as TP-17) [7].
The technical recommendation TP-17 regulates the conditions for the connection and
the plant of distributive producing units on the electro distributive system of the JP
Elektroprivreda BiH d.d. Sarajevo (JP EP BiH). The maximum allowed voltage
deviations from the steady states nominal values in the distribution system are:
• D um =  10% for medium voltage network (35, 20, 10 kV).
• D um = +5 and −10% for the low voltage (0.4 kV) network.
The generators used for the small hydropower plants can be synchronous or
asynchronous. They can also be sources with electronic transformators with output
voltage of 50 Hz. The most used are the synchronous generators, their main charac-
teristic is their behaviour in short circuits. The synchronous generator will in the case of
short circuting provide the breakpoint with energy until the protection is activated
because of an independent motive and thus will deepen the current of the break. One of
the main elements that effect the power of the short circuit is the characteristic value of
the reactance of the short circuit. This reactance depends on the conditions in which the
distributive generator is working and it can be subtransient, transient or permanent.
Those three values differ in the synchronous generators. The connection of the dis-
tributive generator can’t allow the power of the three phase fault to grow above the
Table 1. Planned realisation of the project of SHP on the side of Neretvica river
Phase Small hydropower plants
Overall installed power (MW) Time framework
IA
SHP Srijanski most
6.4
Jan–Dec 2017
SHP Gorovnik ušće
IB
SHP Crna Rijeka
3
Jun 2017–Jun 2018
SHP Gorovnik
II
SHP Podhum 1
7,9
Jan–Dec 2018
SHP Podhum 2
SHP Donji Obalj
SHP Poželavka
SHP Mala Neretvica–ušće
III
SHP Obaščica
7,2
Jan–Dec 2019
SHP Duboki potok 2
SHP Ruste
SHP Plavuzi
SHP Prolaz
SHP Duboki potok 1
Connecting a Group of Small Hydropower Plants on the Side
69

value which the equipment is designed for in the EDS. Based on [7] every node in the
analysed network has to please the given condition:
Sk3 ¼
ﬃﬃﬃ
3
p
UnIk3 ¼
ﬃﬃﬃ
3
p
Un
cUn
ﬃﬃﬃ
3
p
Zd
 Sk3Max
ð1Þ
where:
Zd (X)
is the impedance of the direct ﬂow of the network from the source
(TS and DG) to the break point.
Ik3 (A)
the current of the three phase short circuit.
Sk3Max (MVA)
the maximum allowed power of the three phase short circuit in the
distributive circuit.
C
the voltage factor (c = 1.1 according to IEC 60909).
In the distributive networks the following values of the maximum allowed powers
of the three phase short circuits are given:
• 0.4 kV grid: 18 MVA in a cabel grid and 11 MVA in an overground grid.
• 10 kV grid: 250 MVA.
• 20 kV grid: 500 MVA.
• 35 kV grid: 750 MVA.
The energetic analysis is made using the professional PowerCAD software tool.
The PowerCAD is a package made for the calculation of power ﬂows and short circuits
with a graphical representation and a database. Its main characteristic is the fast
numerical analysis of the transformable, distributive and industrial grids and plants. It
has four modules:
• Graphical module
• Calculating voltages and power ﬂows module
• Calculation short and underground grids module
• Communication with the database module.
There are no signiﬁcant network size limits since the applied technique is with a
rarely ﬁlled matrices, thus it can be analyzed grids with several thousand nodes even on
computers with relatively small memory size. The whole software is C++ program
language based and the network is made completely using graphical interface. Every
element of the network can be deﬁned as active or inactive, which helps the user in a
fast analysis of various on/off conditions.
4
Simulation
4.1
Model Description
This paper analyses the Jablanica and Konjic grids and the connection of small
hydropower plants on the side of Neretvica river according to the ﬁrst phase. SHP are
connected to a 35 kV voltage level on TS 35/10 kV Buturović polje (2.5 MVA), and
the ﬁrst phase is consisted of connecting the powerplants in the following order:
70
D. Macić and M. Šarić

1. SHP Gorovnik ušće
2. SHP Gorovnik
3. SHP Sirjanski most
4. SHP Crna rijeka.
The statical analysis of the connection of the mentioned SHP is done in Power-
CAD, and the grid par that is being discussed is given in the picture (Fig. 1).
As seen above, SHP Podhum 2 is not in the plant since its connection is not planned
in the ﬁrst phase, which is simulated with disconnecting the transformators connected
with that SHP (the orange is for deactivated transformators). The ﬁrst analysis is made
when no SHP is connected to the grid, after that they were connected gradually in the
order given above. The current load for connection line Buturović Polje-Podhum 2 is
tracked as well as the voltage changes on the 35 kV voltage level source, the overall
losses on the 35 kV voltage level source and the change in power of the three phase
faults on the 35 kV buses which (using transformators) are connected to the mentioned
SHP and on which the analysis of the mentioned parameters is being made. To obtain
the values of the mentioned parameters two types of calculation were made in Pow-
erCAD: power ﬂow calculation and short circuits calculation. The power ﬂow calcu-
lations is done for two cases: minimum (according to [8] its about 30% of the
maximum load) and the maximum load. This calculation gave the values of the volt-
ages and the losses of active power, while the informations about powers of the three
phase faults are obtained by calculating the short circuit. For the power ﬂow
Fig. 1. Part of considered model scheme
Connecting a Group of Small Hydropower Plants on the Side
71

calculations only the values of the overall installed power of the SHP is needed, which
are given in the table. (Table 2)
As mentioned above, very important parameter in the short circuit case is the
reactance of the generator. Having that in mind, in the calculation of the short circuit
(which is deﬁning the power change of the three phase fault) with activating certain
SHP, beside the overall installed power of SHP, values of the subtransient (xd’’),
inverse (xi) and zero (x0) reactance are also needed, as well as the values of the
ohmical resistance of the statoric winding (R). Those informations are given in the table
(Table 3).
It is important to mention that all SHP work with a power factor of one (cos u = 1)
since this is the most demanding regime that the distributive generators work on. This
means that there is no reactive energy transfer with the grid.
4.2
The Effect of SHP on Load Current and Voltage Proﬁles
The change of the current load in connecting line Buturović Polje-mHe Pohum 2 which
was used for the connecting of the discussed SHP with grid and in the dependence of
the number of the connected SHP for for minimum and maximum grid load is shown in
the picture (Fig. 2).
The change of the overall network load (min or max) does not affect the change of
the current load of the analysed connecting line because discussed SHP are located
Table 2. Overall installed power od the SHP from ﬁrst phase
SHP
Overall installed power (MVA)
Gorovnik ušće 3.427
Gorovnik
0.859
Sirjanski most
3.03
Crna rijeka
2.113
Table 3. Values of reactanses and ohmicah resistances of SHP from ﬁrst phase [9]
SHP
Subtransient
reactanse xd’’
(%)
Inverse
reactanse xi
(%)
Zero
reactanse
x0 (%)
Ohmic resistance of
statoric winding R (%)
Number of
generators
Gorovnik
ušće
10
12
6
5
2
Gorovnik
12
10
5
2
1
Sirjanski
most
12
10
5
1.2
2
Crna rijeka
8
10
5
4
1
72
D. Macić and M. Šarić

where there is no electrical energy consumers. The current load given in the previous
picture is given in the percentages depending on the maximum allowed current value
which can ﬂow through the connecting line, which is 450A (obtained using Power-
CAD). Analyzing the picture above, it can be deduced that the current load won’t be
above 40% of the maximum allowed, which means that the current overﬂow protection
won’t be triggered to deactivate the discussed connecting line. The voltage changes to
35 kV on the buses that are connected to the small hydropower plants and depending
on the number of the connected SHP and in the minimum and maximum grid load, are
shown on the pictures (Figs. 3 and 4).
Analyzing the obtained results it can be deduced that the voltage on the discussed
buses has the least values when there is no SHP connected, and it rises with the gradual
connecting of the SHP. The reason behind that is that the SHP deliver an amount of
active power to the network where one part of it goes to the consumers and the other
one which does not go to the consumers effects on the rise of voltage on the buses. As
long as the number of SHP increases the active power delivered is higher and so the
Fig. 2. Current load of connecting line Buturović Polje—Podhum 2 in dependance of number of
connected SHP
Fig. 3. Voltage changes to 35 kV on the buses that are connected to the SHP in dependance of
the numer of connected SHP, in the minimum grid load (max. consumation)
Connecting a Group of Small Hydropower Plants on the Side
73

voltage on the buses is also higher which is visible in the previous pictures. In case of a
overﬂow value of the voltage, some of the generators needs to be transfered into the
underexcited regime to reduce the values of the voltage. In EES BiH the generators
rarely work in underexcited conditions since that could lead to the overheating of the
stators, and that problem also needs to be discussed. It is very important for the
criteriums set in the TP-17 to be satisﬁed, which means that the voltage deviation has to
be inside the 10% for medium voltage (SN) network. Since the mentioned buses are
at 35 kV that means that the changes may occur in between 31.5 and 38.5 kV. The
pictures shown above clearly shows that the voltages don’t violate the conditions given
and are all inside the bandwidth that is already mentioned, which means that neither
one of the SHP is in the underexcited state. It can also be deduced that the change of the
load does not affect majorly the voltage change on the buses, because the SHP are
located on an unique location where there is no electrical energy consuming.
4.3
The Effect of SHP on the Active Power Losses
The analysis of the losses of the electrical power is very important when planning and
exploiting the distributive network. The overall active power losses on a 35 kV voltage
level, with minimum and maximum grid load, are given in the picture (Fig. 5).
All SHP have power factor of 1, which means that there is no reactive power
transfer with the grid. It can be deduced from the pictures that overall active power
losses on 35 kV voltage level rises with connecting of SHP. The reason behind that is
that the overall producing exceeds the needs of the consumers and thus the part of
electrical energy which is not used for the consumers increases overall active power
losses. Effect of network loads on the overall active power losses also can be seen.
With minimum network load, in other words, with maximum electrical energy con-
sumation, the losses are less than in the case of maximum load, or in other words said,
the minimum consumation of the electrical energy. The reason behind that is that with
maximum consumation of the electrical energy, part of unused electrical energy that
produces the losses is smaller than in the case of minimum electrical energy con-
sumation. As it can be seen on picture below, connecting of considered small hydro-
power plants causes signiﬁcant increases of active power losses on 35 kV voltage level,
Fig. 4. Voltage changes to 35 kV on the buses that are connected to the SHP in dependance of
the numer of connected SHP, in the maximum grid load (min. consumation)
74
D. Macić and M. Šarić

which indicates bad inﬂuences on the whole EES. In order to decrease mentioned
losses, SHP in considered area are being connected in various ways, and analysis for
each of them have been made. Analysis results showed that, no matter which way of
connecting SHPs is chosen, active power losses can’t be affected so much. So it can be
concluded that problem is electrical energy which has been produced in considered
SHPs, and is being transferred to distant consumers, having in mind that the are where
these SHPs are located, doesn’t have an electrical energy consumers. According to that,
in order to reduce active power losses, connection of speciﬁed numbers of consumers at
a place where SHPs are located, is needed. In that way, biggest part of electrical energy
produced in considered SHPs would be consumed locally, while the part being
transferred to distant consumers would decrease.
4.4
The Effect of SHP on the Power of a Three Phase Fault
The connection of distributive generator of the small hydropower plants can’t lead to
rise in the power of a three phase fault above the values of the technical design of the
equipment in the electro distributive system. The change of the power of a three phase
fault on a 35 kV bus Buturović Polje which connects all small hydropower plants on
the side of Neretvica river, as well as the change on 35 kV buses that are connected to
the each SHP, depending on their connection, is shown in the picture (Fig. 6).
As it can be seen in the previous picture, the rise of the power of a three phase fault
is obvious with the connection of the SHP. With that, the power of a three phase fault
has greater value as the SHP approaches the 35 kV bus Buturović Polje. Also it can be
seen that there is no moment in which the power of a three phase fault, no matter how
many small hydropower plants are connected, won’t exceed the value of cca 175 MVA.
Since the technical recommendation for the connection and the plant of distributive
generators (TP-17) allows maximally the power of a three phase fault value of 750
MVA at a 35 kV voltage level, than it can be deduced that the conditions set for the
power of a three phase fault are met. That means that the SHPs in the case of a break
won’t produce break currents that have huge values, in other words said, the inﬂuence
of SHPs in the case of break is negligible [8].
Fig. 5. The overall active power losses on a 35 kV voltage level (cosu = 1 of all SHP)
Connecting a Group of Small Hydropower Plants on the Side
75

5
Conclusion
The paper discussed connecting small hydropower plants on the side of Neretvica rive,
which are planned in the ﬁrst phase of the construction, and those are: Gorovnik ušće,
Gorovnik, Sirjanski most and Crna Rijeka. Various effects of changes in the dis-
tributive grid are discussed when the small hydropower plants are connected to the
35 kV voltage level. Beside the many positive effects, the connection of SHP on
the Neretvica river effects the quality of the electrical energy, rising the voltage on the
buses, rising the losses of active energy, the power of a three phase fault, etc. However,
throughout the analysis it has been deduced that the considered changes which are
caused by connecting the SHP are in the bandwidth deﬁned within the technical
recommendation TP-17. Only problem in considered connecting is signiﬁcant increase
of active power losses. In order to lower mentioned losses, and to do connecting of
considered SHPs on 35 kV voltage level effectively, connection of speciﬁed number of
consumers in considered area is needed.
References
1. Adefarati, T., Bansal, R.C.: Integration of Renewable Distributed Generators into the
Distribution System: A Review. IET Renewable Power Generation, Pretoria, South Africa
(2016)
2. Avdaković, S., Jusić, A.: Dynamic response of a group of synchronous generators following
disturbances in distribution grid, Eng. Rev. 36(2), 181–186
3. Jamak, E., Avdaković, S.: Uticaj malih hidroelektrana na rad distributivnog sistema, Okrugli
sto u Tuzli, mart (2007)
4. Katić, A.: Priključenje malih hidroelektrana na elektroenergetsku razdjelnu mrežu, Šibenik,
18–21 maja 2008
5. Ivanović, M., Minić, S.: Uticaj malih hidroelektrana na elektrodistributivnu mrežu ED Pirot,
Elektrotehnički instititut ‘‘Nikola Tesla’’, Beograd (2010)
6. Goić, R., Jakus, D., Mudnić, E.: Proračun godišnjih gubitaka radne energije u distribucijskoj
mreži s priključenom vjetroelektranom. Fakultet elektrotehnike, strojarstva i brodogradnje,
Sveučilište u Splitu (2008)
7. BiH, J.P.E.: Tehnička preporuka za priključenje i pogon distribuiranih generatora (2016)
Fig. 6. Changes of the power of a three phase fault in dependance of number of connected SHP
76
D. Macić and M. Šarić

8. Goić, R.: Utjecaj vjetrogeneratora na varijacije napona i gubitke snage u razdjelnoj mreži,
Fakultet elektrotehnike, strojarstva i brodogradnje—Split
9. Electric, S.: Medium Voltage technical guide: Basics for MV cubicle design
Connecting a Group of Small Hydropower Plants on the Side
77

Analysis of the Impact of Distributed
Generation on Voltage Proﬁles (Case Study
Long Feeder 10 kV Grebak)
Aiša Ramović1, Lejla Terzić1, Adnan Bosović2(&),
and Mustafa Musić2
1 Faculty of Engineering and IT, Department of Electrical and Electronics
Engineering, International Burch University, Sarajevo, Bosnia and Herzegovina
2 Department of Strategic Development, Public Electric Utility Elektroprivreda
of Bosnia and Herzegovina, Sarajevo, Bosnia and Herzegovina
a.bosovic@epbih.ba
Abstract. In order to reduce carbon-dioxide emissions, Bosnia and Herze-
govina will be required to achieve the renewable energy share of 20% in total
energy consumption by 2020. Therefore, in the previous period an intensive
construction of power plants based on renewable energy sources has been
noticed. Aim of this paper was to analyze how different distributed generators
(DG) impact the voltage proﬁles of the system. In this paper medium voltage
(MV) distribution network of feeder 10 kV Grebak, and all existing and planned
DGs were modeled with real parameters. The analysis was done in PSAT
toolbox (MATLAB toolbox), which was used to perform load ﬂow calculations.
Furthermore, the impact of planned DGs on voltage proﬁles is shown for dif-
ferent scenarios. The obtained results show difference between the impact of
existing small hydro power plants and solar power plants, as well as the effect of
their capacitive and inductive regimes of reactive power production. Also, the
results showed that on this feeder it is not possible to connect planned additional
DG’s while maintaining voltage proﬁles within limits. The precise location and
size of the DGs have vital inﬂuence on voltage proﬁles.
1
Introduction
The term “distributed generation” (DG) refers to the production of electricity near the
consumption place. Resources of such generation are often renewable energies.
Renewable energy is energy from natural resources such as rivers, tides, waves, sun-
light, wind, biomass and geothermal heat.
One of the main advantages of DG is that it is close to the consumption, so the
energy can be distributed close to the place of production. DGs have important role in
improving the reliability of the system, reducing losses, providing better voltage
support and improving power quality. The distributed generation based on renewable
energy sources also reduces greenhouse gas emission which is one of the main reasons
for their increased integration in the network [1].
Focus of the analysis presented in this paper is feeder 10 kV Grebak. This feeder is
a part of medium voltage (MV) distribution network, which is placed on the hilly,
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_7

sparsely populated area near Goražde and is maintained by the Business unit of dis-
tribution Goražde. This is the longest MV feeder in the distribution network of Public
Electric Utility Elektroprivreda of Bosnia and Herzegovina (JP EP BiH) and it is a
subject of thorough analysis due to the distributed generation that is planned to be
connected to this network. One of the biggest problems arising is that there will be
much more power generation than consumption, during the minimum load conditions,
so the voltage will be out of the tolerance range (±10%). Planned generators include
small hydro power plants and solar power plants, and their impact will be shown and
explained later. Results presented in this paper refer to the minimum load, the worst
case scenario that can happen in this part of the network (measured during late night
hours) [2].
Bosovic et al. [2] studied methods of increasing the capacity of the same feeder
10 kV Grebak but with different software tool Power CAD in order for new DGs to be
connected. One of the methods proposed was transition to 20 kV voltage level for the
entire part of the network. In [3], Taxt et al. have researched integration of small hydro
power plants on the distribution network and have suggested three main solutions.
Those are connection using the existing MV network, connection using the existing
MV network with modiﬁcations, and redesign of the network, building new lines or
upgrading to higher voltages. Appen et al. [4] have dealt with PV storage systems in
distribution network and presented how that kind of control provides beneﬁt both to the
grid operators and to the owners of photovoltaic (PV) systems. The advantages and
disadvantages of distributed generation reactive power regulation in low voltage
(LV) networks were presented by Matvoz and Maksić [5]. Balamurugan et al. [6] have
used DigSILENT for modelling of IEEE 34 Node Distribution test feeder, where they
analyzed solar photovoltaic generators and their impact on the voltage proﬁle, power
losses, etc.
In this paper, distribution feeder 10 kV Grebak is modelled in PSAT toolbox with
real parameters in order to analyze the impact of DGs (Small Hydropower Plants and
PV) on voltage proﬁles. Few different scenarios with different DGs connected to the
network and their different regimes of operation will be presented. Aim of this paper is
to check if new planned generators can be connected to the feeder Grebak. Also, we
will suggest possible solutions for the case if planned generators could not be
connected.
After this introduction, in Sect. 2 the impact of distribution generators on voltage
proﬁles will be shortly described, with focus on small hydropower plants and photo-
voltaic systems. In Sect. 3 the materials and methods used in this paper are presented.
Section 4 presents several scenarios based on different load and generation conditions
in the network. At the end, there is a shot summary and conclusion in the Sect. 5.
2
Impact of Distribution Generators on Voltage Proﬁles
Due to the increase of the load demand and the exhaustion of conventional energy
sources, the power industry is drawing attention to the usage of alternative energy
resources lately. That is reason why the distributed generation become the preferred
Analysis of the Impact of Distributed Generation on Voltage Proﬁles
79

option to assist the energy supply for the customers of a distribution system. Such a
connection of the generator near the customer can have both advantages and disad-
vantages. The beneﬁts include losses reduction, load curve smoothing, and system
security enhancement. Also, due to its small size, the DG construction is faster and
cheaper than a conventional power plant (e.g. thermal, hydro). Additionally, envi-
ronmental concerns and policies to reduce CO2 emissions have been directing the
energy generation to using renewable sources. All these beneﬁts lead to the increase of
small hydro, wind and solar power in many countries, and, in this part, impact which
those generators have on the voltage proﬁle of the system will be thoroughly analyzed
[7].
Besides all beneﬁts that are brought to the system, there are many technical effects
of distributed generators on the distribution system that are considered as disadvantages
and issues. Such effects are thermal rating of equipment, system fault levels, stability,
reverse power ﬂow capabilities of tap-changers, line drop compensation, voltage rise,
power losses, power quality (e.g. ﬂickers and harmonics) and protection [8].
DG increases voltage along the feeder. The impact of DG on voltage proﬁles
depends on its delivered power and feeder loading. If DG delivers only active power,
the impact on voltages is smaller than if DG delivers both active and reactive power. In
the second case, the effect of DG is similar to that of capacitors; voltages can signif-
icantly increase. If voltages exceed the upper limit, when voltage regulators are used,
they will make corrections within their range. When PV systems are taken into the
consideration, the voltage variations due to PV system disconnection tend to be severe
for high PV power and maximum system loading. Thus, this can become a serious
power quality concern as the number of DGs installed in the system increases [7].
In [9] it is explained how the system behaves if there are small hydropower con-
nected to the power system and how that inﬂuences the voltage level. It is analyzed and
shown that voltage node level is improved and transmission power losses are reduced
when the small hydropower plants of interest are connected to the network.
Finally, we will mention the analysis of DG impacts on the feeder Grebak as well,
based on a study performed by JP EP BiH. The highest voltage drop is noticed when
SHP Osanica 4 is disconnected (about 8% on the integration point), about 4% when
SHP Osanica is disconnected, while the change of voltage is about 3% on the inte-
gration point of PVP Bujaci. Disconnection of PVP Bujaci did not make a meaningful
voltage change due to the small installed capacity. Also, the voltage change depends on
the type of disconnection: instantaneous in the case of connection line outage, or
gradual (couple minutes) if the disconnection is planned [10].
3
Materials and Methods
The feeder 10 kV Grebak is the longest MV feeder in the JP EP BiH distribution
network, with 128 km of distribution lines. It consists of 195 cables and overhead lines,
which are supplying 102 transformer substations 10(20)/0,4 kV that supply households
80
A. Ramović et al.

and other types of customers. There are four distributed generators currently connected
to this feeder and ﬁve are supposed to be connected to the grid, whose impact on
voltage proﬁles will be analyzed. In the tables below, parameters of the generators
analyzed through the paper are shown (Tables 1 and 2).
Analyzed part of the network (feeder 10 kV Grebak) was modelled in open source
Matlab toolbox for power system analysis—PSAT. A total of 102 transformers
10/0,4 kV were modelled with one 110/35/10 kV and one 35/20 kV transformer from
which also other feeders were supplied. The parameters of the network components,
obtained from JP EP BiH were recalculated accordingly in a way to be used properly in
PSAT. Distributed generators that are planned to be connected to the existing network
were also modelled. The four DGs that are already connected to the network have total
power of 2070 kW, and ﬁve planned DGs have power of 2153 kW. The maximum
load is 1986 kW, and the minimum load is 642 kW [2]. Figure 1 shows the georef-
erenced scheme of the feeder Grebak, with DG locations marked (mSE Dragovići,
mHE Osanica, mSE Bujaci, mHE Osanica 4). This paper analyses the same network
and DGs as paper [2], but with different methodology and in different software pro-
gram, and therefore these papers can be used for benchmark.
Table 1. Basic information about installed DG’s on feeder 10 kV Grebak
Name of DG
Type
Installed power
(kW)
Voltage level
(kV)
Type of
generator
mHE Osanica
SHP
1260 (2  630)
10
Synchronous
mHE Osanica
4
SHP
630
10
Synchronous
mSE Bujaci
Solar Power
Plant
150
0, 4
PV
mSE
Dragovići
Solar Power
Plant
30
0, 4
PV
Table 2. Basic information about planned DG’s on feeder 10 kV grebak
Name of DG
Type
Installed power (kW) Type of generator
mSE Rešetnica
Solar Power Plant 800
PV
mHE Kosova
SHP
420
Synchronous
mHE Babina voda SHP
133
Synchronous
mHE Kolina 4
SHP
400
Synchronous
mHE Kolina 4
SHP
400
Synchronous
Analysis of the Impact of Distributed Generation on Voltage Proﬁles
81

4
Results and Discussion
Five different scenarios are analysed in this paper:
1. Worst case scenario for existing situation: (i) maximum production of existing DGs
with minimum load and (ii) no production of existing DGs with maximum load.
2. Inductive and capacitive regimes of operation of all existing and planned DGs.
3. Minimum, maximum and average power production of existing DGs.
4. Minimum, maximum and average power production of all existing and planned
DGs.
5. Comparison of minimum and maximum load.
4.1
Worst Case Scenario for Existing Situation
Figure 2 shows how the voltage changes across the grid, from the three-winding
110/35/10 kV transformer (beginning of the feeder) to the transformer station
(TS) 10/0,4 kV Grebak (end of the feeder). The numbers on x-axis on this and all the
following ﬁgures represent the sequential number of busses from the beginning to the
end of the feeder. There are two cases observed in this scenario: (i) maximum pro-
duction of existing DG’s with minimum load and (ii) no production of existing DG’s
with maximum load. The most noticeable point on this graph is the voltage increase at
about 14th bus. The reason for that is that there are three PV generators connected to
the main feeder line which causes this sudden change in voltage.
Fig. 1. Georeferenced scheme of the feeder 10 kV Grebak
82
A. Ramović et al.

4.2
Inductive and Capacitive Regimes of Operation of All Existing
and Planned DGs
Figure 3 shows how voltages behave with inductive reactive power and with capacitive
reactive power regimes, combined with minimum load. The known fact is that the
synchronous generator can generate active and reactive power, and it can consume
reactive power as well. The regime in which the synchronous generator produces
reactive power is called inductive regime, the current lags (with respect to voltage) and
the generator is over excited. The regime in which the synchronous generator consumes
reactive power is called capacitive regime, the current leads and the generator is under
excited. As it can be noticed, capacitive case decreases voltages when compared with
inductive case.
9
9.5
10
10.5
11
11.5
12
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
Voltage [kV]
No. of bus
Existing_MinLoad
MaxLoad_NoDGs
Fig. 2. Worst case scenarios for existing DGs—(i) Minimum load and maximum production
and (ii) Maximum load and minimum production
0
2
4
6
8
10
12
14
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
Voltage [kV]
No. of bus
MinLoad_AllDGs
CapacitiveMode_MinLoad
Fig. 3. Inductive and capacitive regimes of all existing and planned DGs
Analysis of the Impact of Distributed Generation on Voltage Proﬁles
83

4.3
Minimum, Maximum and Average Power Production of Existing
DGs
In Fig. 4 there are three cases shown for the scenario when there are just current DGs
connected to the network, combined with minimum load in the network. Max case is
the case when the generators are working with maximum power achieved in real
operation, as it is given in real parameters provided by JP EP BiH. Min case is the case
when the power on all four generators is set to be zero, and the load remains the same.
Average case is when both active and reactive power on generators is divided by two.
4.4
Minimum, Maximum and Average Power Production of All Existing
and Planned DGs
On the Fig. 5, a scenario when all the current and planned generators are connected to
the network is shown, combined with minimum load in the network. When we com-
pare this scenario, with that at Fig. 3, we can see that the voltages are signiﬁcantly
increased. This is a proof that some change must be considered when connecting those
planned DGs on the grid.
9
9.5
10
10.5
11
11.5
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
Voltage [kV]
No. of bus
Existing_MinLoad
MinLoad_NoDGs
AvgExisting_MinLoad
Fig. 4. Minimum, maximum and average power production of existing DGs
84
A. Ramović et al.

4.5
Comparison of Minimum and Maximum Load
As it is expected to be, the voltages are lower when the load is higher. That is best
shown in Fig. 6, when all the existing DGs are connected to the network with different
load conditions. It can be concluded from the ﬁgure that there is no problem on this
feeder if the loads are higher, because there is enough customer load to consume the
produced power. In other hand, if the loads are lower, voltage starts to increase, and
threatens to go above the tolerance range.
9
9.5
10
10.5
11
11.5
12
12.5
13
13.5
14
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
Voltage [kV]
No. of bus
MinLoad_NoDGs
MinLoad_AllDGs
AvgAll_MinLoad
Fig. 5. Minimum, maximum and average power production of all existing and planned DGs
9
9.5
10
10.5
11
11.5
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
Voltage [kV]
No. of bus
Existing_MinLoad
Existing_MaxLoad
Fig. 6. Minimum and maximum load on existing DGs
Analysis of the Impact of Distributed Generation on Voltage Proﬁles
85

On Fig. 7, the scenario where all DGs, existing and planned, are connected to the
network with different loads is represented. There is a slightly better situation in this
scenario if the load is at its maximum values. Comparing to other scenarios, the voltage
boundaries are moved even more above the tolerance range of ±10%, proving that it is
impossible to connect the planned distributed generators on the network that exists
now.
Figure 8 shows what happens if there is no power generation from the distributed
generators at all. The maximum load drops the voltage below the 10% from the
nominal voltage, while with the minimum load voltage is bit higher, moving between
the tolerance ranges. The reason why the voltage increases going to the end of the
feeder is the signiﬁcant presence of MV cable lines on this feeder, which give rise to
the voltage due to reactive power that they are generating.
9
9.5
10
10.5
11
11.5
12
12.5
13
13.5
14
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
Voltage [kV]
No. of bus
MaxLoad_AllDGs
MinLoad_AllDGs
Fig. 7. Minimum and maximum load with maximum production (existing and planned DGs)
9
9.2
9.4
9.6
9.8
10
10.2
10.4
1
3
5
7
9
11
13
15
17
19
21
23
25
27
29
31
33
35
Voltage [kV]
No. of bus
MaxLoad_NoDGs
MinLoad_NoDGs
Fig. 8. Minimum and maximum load with minimum production
86
A. Ramović et al.

4.6
Discussion of the Results
As the title states, the aim of this paper is to analyze how different DGs impact voltage
proﬁles in the MV distribution grid. This feeder is speciﬁc due to its high generation
capacities, while at the same time the minimum load demand is several times lower.
This misbalance of production and load causes increase of the voltage, especially
during the night hours. For these reasons, in practical operation of this feeder all DGs
on this feeder cannot work at the same time with full power, in some periods of time.
However, this happens only during the night hours in the period 03–05 h, when the
load is minimal, and only during the two months in a year when there is enough water
for both generators in mHE Osanica to work. This paper did not analyse the effects of
different positions of transformer tap-changers, which is a voltage regulation measure
implemented on this feeder.
The problems of high voltages will be even bigger if planned DGs are connected to
the network. Considering installed power of every DG, it is possible to say that small
hydro power plants will contribute more to the voltage increase than the solar power
plants because they have higher installed power. Total installed power of small hydro
and solar power plants is 3243 kW and 980 kW respectively.
Analyzed feeder is interesting because it has much more production than demand of
power during minimum load conditions. There are four generators already connected
on the network, and ﬁve more that are supposed to be connected. If all nine generators
are connected to the network, voltage will be out of tolerable range of ±10%. There are
three possible ways to deal with this problem:
– Transition of 10 kV network to 20 kV voltage level
– Using capacitive power factor of DGs
– Increasing the consumption (e.g. building factories), which is out of inﬂuence of
Distribution system operator.
Capacitive power factor could help deal with problem of high voltages, that usually
appear late at night (when the demand is minimum). It is important to note that DGs
should not always work in this mode, but rather just couple of hours during the night
and that can be controlled with automatic voltage regulation on DG. In this way
negative effects that can appear if the generator works long-term in capacitive and
inductive mode can be mitigated. Synchronous generators technically can have
long-term work in the capacitive mode, but with respect to the limitations from the
manufacturer.
5
Conclusion
In this paper, different impacts of existing and planned DGs in real MV distribution
network of 10 kV feeder Grebak on voltage proﬁles are analyzed. Several scenarios
with minimum and maximum load in the network were analyzed to represent different
generating power of DGs and different operating regimes.
In each of analyzed scenarios voltages higher than 10 kV can be observed. This
highlights the fact that there are considerable problems with voltages on this feeder
Analysis of the Impact of Distributed Generation on Voltage Proﬁles
87

even with only existing DGs, when combined with minimum load in the network. On
this model transition to 20 kV voltage level and operation of DGs with capacitive
power factor are possible solutions, in terms of improving voltage conditions. Due to
the fact that on existing 10 kV network of feeder Grebak new planned DGs cannot be
connected, the transition to 20 kV voltage level is imposed as a logical solution. In
addition, operations of DGs with synchronous generators in the capacitive regime is
beneﬁcial, especially on feeders like Grebak, because of the occurrence of high voltage
in the distribution network.
References
1. Dulau, L.I., Abrudean, M., Bica, D.: Effects of distributed generation on electric power
systems. In: The 7th International Conference Interdisciplinarity in Engineering (2014)
2. Bosović, A., Musić, M., Hasanspahić, N., Mehović, M.: Povećanje Kapaciteta Elektrodis-
tributivne Mreže za Integraciju Distribuiranih Generatora, X jubilarno savjetovanje o
elektrodistributivnim mrežama Srbije sa regionalnim učešćem CIRED Srbija 2016, Vrnjačka
Banja, Srbija, 26–30.09 (2016)
3. Taxt, H., Catrinu, M.D., Nordgard, D.E.: Overall challenges and recommendations
concerning the integration of small scale hydro in MV distribution networks. In: 22nd
International Conference on Electricity Distribution, Stockholm (2013)
4. Appen, J.V., Braun, M., Kneiske, T.: Voltage control using PV storage systems in
distribution systems. In: 22nd International Conference on Electricity Distribution,
Stockholm (2013)
5. Matvoz, D., Maksić, M.: Advantages and drawbacks of distributed generators reactive power
regulation in the low voltage network. In: 22nd International Conference on Electricity
Distribution, Stockholm (2013)
6. Balamurugan, K., Srinivasan, D., Reindl, T.: Impact of distributed generation on power
distribution systems. PV Asia Paciﬁc Conference (2011)
7. Aramizu, J., Vieira, C.M.: Analysis of PV generation impacts on voltage imbalance and on
voltage regulation in distribution networks In: Power and Energy Society General Meeting
(PES), IEEE (2013)
8. Hayatdavudi, M., Rajabi, A.R., Raouf, M.H., Saeedimoghadam, M., Habibi, A.: Effects of
distributed generation on voltage proﬁle for reconﬁguration of distribution networks. Int.
J. Electr. Comput. Energ. Electron. Commun. Eng. (2014)
9. Bardhi, A., Braneshi, M., Pjetri, A.: Impact of small hydropower plant on improvement of
voltage level and energy efﬁciencies. Int. J. Adv. Res. Electr. Electron. Instrum. Eng.
10. Hasanspahić, N., Bosović, A., Gruhonjić-Ferhatbegović, Š., Hidić, F., Delić, S., Mehović, M.,
Šarić, M., Tuhčić, A., Redžić, N., Jusić, A., Cernica, E., Ladanović, E.: Analiza uticaja i
deﬁniranje smjernica kod priključenja distribuiranih izvora na distributivnu mrežu. Study,
Public Electric Utility Elektroprivreda of Bosnia and Herzegovina d.d. Sarajevo (2016)
88
A. Ramović et al.

Power System Fault Detection, Classiﬁcation
and Location using Artiﬁcial Neural Networks
Almin Karić(&), Tatjana Konjić, and Admir Jahić
Faculty of Electrical Engineering, University of Tuzla, Tuzla
Bosnia and Herzegovina
{almin.karic,admir.jahic}@fet.ba, tatjana.konjic@untz.ba
Abstract. This article focuses on detecting, classifying and locating faults in
power system using Artiﬁcial Neural Networks (ANNs). Feed-forward neural
networks have been employed and trained with back-propagation algorithm.
The model of WSCC 9 bus test system has been modelled in Matlab/Simulink,
and used to validate the proposed fault detection system. First, normal state of
the model was observed. After determination of normal state, different types of
faults have been simulated on all nine buses and on all lines of the model.
Voltage and current magnitudes, obtained by the fault simulation, are used as
inputs of the ANN. Output of the ANN should provide information about the
fault type and location in case the fault occurs. Number of hidden layers and
neurons per hidden layer is determined by testing performance and training time
of each developed neural network.
1
Introduction
Power systems all over the world are experiencing huge and rapid expansion. End users
who are very sensitive to power outages are demanding reliable and uninterrupted
supply of electric power [1]. On the other side, the appearances of large generations and
highly interconnected systems are making early fault detection and rapid equipment
isolation the most important functions to maintain system stability. One of the factors
that hinder the continuous supply of electricity and power is a fault in the power system
[2]. Power lines encounter various faults due to tree contact, animal contact or other
natural causes such as thunderstorms. These faults cannot be avoided, since they occur
because of natural reasons which humans cannot control. It is very important to have
protection system that can detect any abnormal ﬂow of current in the power system,
identiﬁes the type of fault and accurately locates the position of the fault in the power
system [6]. The automatic location of the fault can greatly enhance the systems reli-
ability because faster the power is restored, more money and time is saved.
Conventional algorithms based on deterministic computations and well-deﬁned
model of power lines, are resulting in the late detection and inaccurate results [1–5].
Conventional distance relays consider power swing as a fault. Such malfunctioning
could lead to serious disturbance of power system stability.
Artiﬁcial intelligence based methods are being used in the process of fault detection
and location to accelerate fault detection and to improve performance of protection
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_8

system. There are three major artiﬁcial intelligence based techniques that are widely
used in the power and automation industry:
• Expert System Techniques [7–9],
• Artiﬁcial Neural Networks [6, 10, 11, 15], and
• Fuzzy Logic Systems [12–14].
ANN based methods do not require a knowledge base for the detection of faults.
The purpose of fault detection is to detect, locate and classify the fault in the power
system as fast and accurate as possible [15]. Special features of ANN for fault tolerant
systems:
• ANN consists of layers of neurons which are connected with each other. In this
formation it is possible to carry out parallel processing of the information. This
helps processing huge amount of data in real time.
• ANN can solve any degree of nonlinearity and the fault detection problems are
nonlinear.
• ANN approach is non-algorithmic and there is no need for knowing functions that
are relating the problem variables. Also, there is no need for approximations unlike
the most of the mathematical models.
• ANN is capable of handling situations of incomplete information, corrupt data and
thus is highly fault tolerant.
In this work ANN based approach is used to detect, locate and classify faults that
occur in the network. Developed Artiﬁcial Neural Networks have three layers: input,
hidden and output. The input layer is receiving data from the power system, and then
hidden layers are processing that data so we can get the result on the output layer.
Neural networks are capable of working in real time environment and responding to the
changes immediately, which makes the system for the fault detection fast and ﬂexible.
2
Artiﬁcial Neural Networks
Artiﬁcial neural network theory has served to better identify how the neurons in the
brain function and to provide the basis for efforts to create artiﬁcial intelligence [15].
Neural network can be deﬁned as a composition of elements for receiving, processing
and transferring information. Those elements are called neurons. The neurons use
activation function that converts neuron’s weighted input to its output activation. The
activation function can be divided into linear and nonlinear functions [16].
There are various activation functions which are chosen depending on the type of
problem to be solved by the network. The most common activation function includes
linear function, tangent hyperbolic function, threshold function and sigmoidal function
[16]. Figure 1 shows a simple neuron.
Values w1j, w2j, …, wnj are weights to determine the strength of input vector
X = [x1, x2, ….., xn]T of neuron j. Each input is multiplied with its associated weight of
the neuron. To generate the ﬁnal output yj, the sum zj is passed on to a nonlinear ﬁlter f
(z) called activation function or transfer function, which releases the output.
90
A. Karić et al.

In ANN applications, the challenge is to ﬁnd the right values for the weights, based
on the known inputs and outputs. Various numbers of hidden layers and numbers of
neurons in each layer, transfer functions and training algorithms can be employed
depending on the complexity of the problem to be solved [15].
3
Faults in Power System
There are two types of faults in the power system: short circuit and open circuit. A short
circuit is a fault in which a current travels along an unintended path with no or very low
electrical impedance. Opposite of a short circuit is an open circuit fault which is an
inﬁnite resistance between two nodes. Information gained from fault studies are used
for proper relay setting and coordination [17, 18]. Fault studies are also used to obtain
the rating of the protective switchgears. The magnitude of the fault currents depends on
the internal impedance of the generator plus the impedance of the intervening circuit
[18].
Short circuit faults are divided into three-phase balanced faults and unbalanced
faults. Different types of unbalanced faults are single line-to-ground fault, line-to-line
fault and double line-to-ground fault.
The most
often
faults,
around
70–80%,
are single line-to-ground
faults.
Three-phase faults are very rare, around 5–10% [17].
In this paper, the short circuit faults will be simulated on WSCC 9 bus test system.
3.1
WSCC 9 Bus Test System
WSCC bus systems are used by researchers to implement new ideas and concepts. The
system is shown in Figure 2 [19].
This test system consists of 9 buses, 3 generators, 3 two-winding power trans-
formers, 6 lines and 3 loads. The bus 1 is a slack bus, which is used to regulate any
changes in the power system. The buses 5, 6 and 8 are representing a PQ buses, or
loads. All values are given in per units and are reduced to base power of 100 MVA.
Fig. 1. A single neuron model
Power System Fault Detection, Classiﬁcation and Location
91

3.2
Simulation of WSCC 9 Bus Test System in Simulink
Simulink model of WSCC 9 bus test system is shown in Figure 3.
Diagrams of the simulation can be viewed from the Data Acquisition subsystem,
shown in Figure 4. Each SCOPE element has two inputs, data of three-phase voltages
and data of three-phase currents.
3.2.1
Simulating the Test System in Normal State
Starting simulation it’s possible to get voltages and currents in the system in normal
state.
Voltage and current diagrams of bus 1 with no fault, as well as of bus 8 are shown
in Figure 5.
Simulating the test system with faults
Three-Phase Fault element from Simulink library is used for simulating any type of
fault. From its settings it is possible to select any phase (A, B, C) and ground (G) that
will be “connected”. By selecting the phase A and G, the single line-to-ground fault
will be simulated, and by selecting phases A and B the two-phase fault will be sim-
ulated. Voltage and current diagrams for both faults obtained by the simulation are
shown in Figure 6.
Fig. 2. WSCC 9 bus test system
92
A. Karić et al.

4
Artiﬁcial neural network for fault detection, classiﬁcation
and location
Developed artiﬁcial neural network consists of three layers: input, hidden and output.
Input layer depends on the system and data types. For WSCC 9 bus test system input
data will be three values of each phase voltage magnitude and three values of each
phase current magnitude, for all buses in the system, which means there will be a total
of 54 inputs in a network. Input values are obtained by simulating the system during the
short circuit, and then placed in a table that could be used to train the neural network.
The output layer depends on the function of a neural network. Neural network must
be able to locate and classify fault. Location and classiﬁcation of fault can be realized
with 5 outputs from the neural network, four of which are used for classiﬁcation (three
phases and one ground), and the last output provides information about the location of
the fault on the transmission line. Developed model of the neural network is presented
in Figure 7.
Activation function at the input is log-sigmoid because input data are in per units,
while the transfer function at the output is saturating liner because the distance where
the fault occurs can receive a value between 0 and 1. The neural network should be able
to ﬁnd a location of the fault even in the locations it is not trained for. This will be done
in a way to ﬁrst simulate all the faults at the beginning of the line (single-phase,
two-phase and three-phase), the resulting voltage and current magnitudes will be placed
Fig. 3. WSCC 9 bus test system in Simulink
Power System Fault Detection, Classiﬁcation and Location
93

Fig. 4. Data Acquisition subsystem
Fig. 5. Diagrams of voltages and currents: a on bus 1 with no fault, b on bus 8 with no fault
94
A. Karić et al.

in the input data table. For the output layer ﬁrst four values depending on the type of
failure are selected, and the last output is set to 0 (which represents the start of the line).
In the same way it is necessary to simulate all types of faults at the end of the line.
In this case, the output which represents the distance is set to 1. For additional precision
the line is separated in two and all kinds of faults are simulated in the middle of the line.
Output of the neural network representing the distance is set to 0.5 and other outputs in
accordance with the type of fault.
Fig. 6. Diagrams of voltages and currents: a Single line-to-ground fault on bus 1. b Two-phase
fault on bus 1
Fig. 7. Model of the neural network
Power System Fault Detection, Classiﬁcation and Location
95

Table 1. Input and output data for single-phase short circuit
Output Data
A
0
1
0
0
1
0
0
1
0
0
B
0
0
1
0
0
1
0
0
1
0
C
0
0
0
1
0
0
1
0
0
1
G
0
1
1
1
1
1
1
1
1
1
Distance
0
0
0
0
0.5
0.5
0.5
1
1
1
Input Data
Nofault
AG–0
BG–0
CG–0
AG–50 m
BG–50 m
CG–50 m
AG–1
BG–1
CG–1
VB1 a
0.8555
0.5022
0.8280
0.8438
0.5042
0.8280
0.8485
0.4854
0.8266
0.8266
VB1 b
0.8558
0.8477
0.5066
0.8312
0.8474
0.5012
0.8354
0.8510
0.4848
0.4848
VB1 c
0.8557
0.8279
0.8466
0.4941
0.8262
0.8473
0.5153
0.8281
0.8493
0.8493
IB1 a
1.1330
1.9994
1.2914
1.0342
1.9846
1.2745
1.0389
2.0729
1.2721
1.2721
IB1 b
1.1326
1.0997
1.7153
1.2215
1.0978
1.6847
1.2088
1.1006
1.7625
1.7625
IB1 c
1.1322
1.2887
1.1074
1.3733
1.2734
1.1045
1.3711
1.2726
1.1049
1.1049
VB2 a
0.8655
0.3157
0.8805
0.9057
0.3616
0.8530
0.8982
0.4109
0.8424
0.8424
VB2 b
0.8656
0.8928
0.3383
0.9347
0.8906
0.3641
0.8779
0.8693
0.4093
0.4093
VB2_c
0.8654
0.8955
0.9024
0.4721
0.8716
0.8976
0.3824
0.8435
0.8701
0.8701
IB2 a
0.8916
2.3079
0.8996
0.9044
2.0421
0.9019
0.8711
1.8907
0.9617
0.9617
IB2 b
0.8913
0.9318
2.2401
0.8776
0.8815
1.8962
0.8903
0.8869
1.7138
1.7138
IB2 c
0.8914
0.9000
0.9456
1.4243
0.8968
0.8822
1.2652
0.9555
0.8858
0.8858
VB3 a
0.8730
0.2944
0.8679
0.9270
0.2491
0.8707
0.9079
0.0702
0.8768
0.8768
VB3_b
0.8727
0.9015
0.2877
0.9040
0.9092
0.2533
0.8850
0.9332
0.0702
0.0702
VB3_c
0.8730
0.8908
0.9149
0.3233
0.8733
0.9080
0.3171
0.8768
0.9345
0.9345
IB3 a
0.6242
1.5739
0.6373
0.6152
1.7669
0.6384
0.6425
2.0900
0.6483
0.6483
IB3 b
0.6236
0.6355
1.5201
0.6207
0.7018
1.7338
0.5802
0.7852
2.0867
2.0867
(continued)
96
A. Karić et al.

Table 1. (continued)
Output Data
IB3 c
0.6238
0.6364
0.6330
0.9586
0.6388
0.7046
1.0481
0.6488
0.7866
0.7866
VB4 a
0.8535
0.4347
0.8261
0.8348
0.4361
0.8243
0.8447
0.4193
0.8223
0.8223
VB4 b
0.8536
0.8385
0.4424
0.8300
0.8406
0.4396
0.8375
0.8422
0.4174
0.4174
VB4 c
0.8533
0.8251
0.8352
0.4331
0.8248
0.8399
0.4653
0.8228
0.8463
0.8463
IB4 a
0.6563
1.1387
0.7223
0.6289
0.7592
0.6967
0.6316
0.4518
0.6800
0.6800
IB4 b
0.6560
0.6543
0.9795
0.7052
0.6428
0.6798
0.6800
0.6380
0.4501
0.4501
IB4 c
0.6557
0.7220
0.6629
0.7982
0.6982
0.6469
0.5974
0.6781
0.6379
0.6379
VB5 a
0.8353
0.3635
0.8244
0.8163
0.3977
0.8127
0.8264
0.3944
0.8093
0.8093
VB5 b
0.8354
0.8245
0.3564
0.8274
0.8169
0.3950
0.8248
0.8172
0.3944
0.3944
VB5 c
0.8350
0.8193
0.8158
0.4071
0.8232
0.8212
0.4075
0.8110
0.8176
0.8176
IB5 a
0.4470
1.0735
0.4602
0.4431
0.6677
0.4556
0.4407
0.4105
0.4492
0.4492
IB5 b
0.4467
0.4843
0.9703
0.4299
0.4814
0.6511
0.4262
0.4756
0.4052
0.4052
IB5 c
0.4469
0.4584
0.4910
0.6029
0.4547
0.4777
0.3596
0.4483
0.4760
0.4760
VB6 a
0.8483
0.3732
0.8309
0.8388
0.3462
0.8362
0.8426
0.2820
0.8331
0.8331
VB6 b
0.8485
0.8287
0.3811
0.8513
0.8446
0.3524
0.8333
0.8488
0.2822
0.2822
VB6 c
0.8484
0.8439
0.8328
0.3757
0.8313
0.8412
0.3718
0.8331
0.8477
0.8477
IB6 a
0.3683
0.8410
0.3576
0.3684
1.2138
0.3568
0.3813
1.6536
0.3538
0.3538
IB6 b
0.3683
0.4110
0.7677
0.3296
0.4137
1.0656
0.3121
0.4271
1.3959
1.3959
IB6 c
0.3683
0.3579
0.4087
0.4565
0.3531
0.4199
0.7568
0.3528
0.4317
0.4317
VB7 a
0.8650
0.2346
0.8811
0.9221
0.2918
0.8521
0.9009
0.3424
0.8414
0.8414
VB7 b
0.8652
0.8922
0.2591
0.9484
0.8990
0.2934
0.8666
0.8688
0.3425
0.3425
VB7 c
0.8649
0.9123
0.9200
0.4154
0.8724
0.9015
0.3150
0.8409
0.8707
0.8707
IB7 a
0.5119
3.4711
0.5460
0.5261
2.7337
0.5831
0.5153
2.2502
0.6283
0.6283
IB7 b
0.5117
0.5464
3.1509
0.5302
0.5426
2.4617
0.5493
0.5470
2.0081
2.0081
(continued)
Power System Fault Detection, Classiﬁcation and Location
97

Table 1. (continued)
Output Data
IB7 c
0.5118
0.5404
0.5461
2.0100
0.5816
0.5427
1.6478
0.6287
0.5466
0.5466
VB8 a
0.8575
0.0001
0.8783
0.9088
0.1345
0.8601
0.8872
0.2150
0.8475
0.8475
VB8 b
0.8577
0.9052
0.0001
0.9222
0.8858
0.1343
0.8641
0.8656
0.2196
0.2196
VB8 c
0.8578
0.8850
0.9013
0.0001
0.8609
0.8847
0.1456
0.8498
0.8695
0.8695
IB8 a
0.5224
3.7556
0.5394
0.5826
2.8028
0.5794
0.5531
2.3027
0.6301
0.6301
IB8 b
0.5221
0.5702
3.4475
0.5868
0.5690
2.5055
0.5628
0.5770
2.0565
2.0565
IB8 c
0.5222
0.5375
0.5727
2.5846
0.5824
0.5732
1.7059
0.6301
0.5789
0.5789
VB9 a
0.8739
0.2429
0.8698
0.9309
0.1929
0.8680
0.9106
0.0001
0.8737
0.8737
VB9 b
0.8739
0.9076
0.2347
0.8989
0.9134
0.1982
0.8829
0.9409
0.0001
0.0001
VB9 c
0.8736
0.8928
0.9186
0.2742
0.8718
0.9143
0.2944
0.8755
0.9421
0.9421
IB9 a
0.6224
1.5742
0.6357
0.6134
1.7678
0.6368
0.6410
2.0913
0.6465
0.6465
IB9 b
0.6219
0.6339
1.5203
0.6189
0.7004
1.7345
0.5785
0.7837
2.0879
2.0879
IB9 c
0.6221
0.6346
0.6314
0.9581
0.6370
0.7030
1.0478
0.6471
0.7852
0.7852
98
A. Karić et al.

This way a table of input and output data is obtained. Table 1 provides information
needed to train ANN to locate and classify single-phase short circuit.
Next problem is to determine the number of neurons in the hidden layer. This can
only be determined by testing, so the characteristics of the network are tested for 5, 10,
20, 30, 35 and 40 neurons in the hidden layer.
Figure 8 shows the results of the neural network with different number of neurons
in the hidden layer. The network is capable of locating the faults for places it is trained
for (at the begging, in the middle and at the end of the line) without a problem.
Negligible deviation can be seen when locating the fault in the middle of the line.
Figure 9 shows the characteristics of neural networks with different number of
neurons in the hidden layer for the fault at the location of 25 km from the begging of
the line. Neural network was not trained for this location so there are certain deviations
Fig. 8. Characteristics of the ANN for single-line fault
Fig. 9. Locations of different faults obtained by ANN with different number of neurons
Power System Fault Detection, Classiﬁcation and Location
99

in the output for the fault location, between the real location of the fault and the
location obtained with the neural network.
Network with 5 neurons in the hidden layer can be used only for single-phase
faults, while for other types of faults obtained results are not acceptable. Two-phase
faults with or without the ground can be located if the network has 10 or more neurons
in the hidden layer. However, for the three-phase faults, neural network has to have at
least 30 neurons in the hidden layer to be able to relatively well locate the fault. From
the Figure 9, it is obvious that the network with 35 neurons in the hidden layer is the
most accurate for all types of faults.
5
Conclusion
Obtained results clearly indicate that ANNs can be used for detection, classiﬁcation and
location of faults. Detection, location and classiﬁcation of faults for the points where the
network was trained, is an easy task for ANN. Also, results are showing that ANN can
relatively accurately detect, locate and classify the fault even for the places it is not
trained for. Depending on the type of fault, networks with different number of neurons
in hidden layer can be used. Single line-to-ground fault can be detected and located
with the smallest number of neurons in hidden layer, only 5. Network for the detection,
location and classiﬁcation of two-phase and two-phase-to-ground faults needs to have
minimum of 10 neurons in the hidden layer. The three-phase fault requires neural
network with 30 to 35 neurons in hidden layer while further increase of neurons in the
hidden layer does not lead to improvement of results.
References
1. Das, R., Novosel, D.: Review of fault location techniques for transmission and
sub-transmission lines. In: Proceedings of 54th Annual Georgia Tech Protective Relaying
Conference (2000)
2. IEEE Guide For Determining Fault Location on AC Transmission and Distribution Lines.
IEEE Power Engineering Society Publ., New York, IEEE Std C37.114 (2005)
3. Saha, M.M., Izykowski, J., Rosolowski, E.: Fault Location on Power Networks. Springer
Publications (2010)
4. Magnago, F.H., Abur, A.: Advanced techniques for transmission and distribution system
fault location. In: Proceedings of CIGRE—Study Committee 34 Colloquium and Meeting,
Florence, paper 215 (1999)
5. Tang, Y., Wang, H.F., Aggarwal, R.K.: Fault indicators in transmission and distribution
systems. In: Proceedings of International Conference on Electric Utility Deregulation and
Restructuring and Power Technologies—DRPT, pp. 238–243 (2000)
6. Ayyagari, S.B.: Artiﬁcial neural network based fault location for transmission lines.
University of Kentucky Master’s Thesis (2011)
7. Hsu, Y.Y., Lu, F.C., Chien, Y., Liu, J.P., Lin, J.T., Yu, H.S., Kou, R.T.: An expert system
for locating distribution system faults. In: IEEE Transaction on Power Delivery, vol. 6,
No. 1, Jan 1991
100
A. Karić et al.

8. Yongli, Z., Yang, Y.H., Hogg, B.W., Zhang, W.Q., Gao, S.: An expert system for power
systems fault analysis. IEEE Trans. Power Syst. 9(1), 503–509, Feb 1994
9. Chang, C.S., Chen, J.M., Liew, A.C.: An expert system approach for fault diagnosis
considering uncertainties. In: Proceedings of International Conference on Intelligent
Manufactureing’95 (ICIM’95), Wuhan, China, June 1995
10. Butler, K.L., Momoh, J.A.: Detection and classiﬁcation of line faults on power distribution
systems using neural networks. In: Midwest Symposium on Circuit and Systems, pp. 368–
371 (1993)
11. Eisa B.M.T.: Faults detection in power systems using artiﬁcial neural network. Am. J. Eng.
Res. (AJER) 02(06), 69–75 (2013)
12. Frank, P.M., Seliger, B.K.: Fuzzy logic and neural network applications to fault diagnosis.
Int. J. Approx. Reason (1997)
13. Mor, V., Vaghamshi, A.: Review on fault detection, identiﬁcation and localization in
electrical networks using fuzzy-logic. Int. J. Appl. Res. Sci. Eng. Nov 2016
14. Adhikari, S., Sinha, N., Dorendrajit, T.: Fuzzy logic based on-line fault detection and
classiﬁcation in transmission line. https://www.link.springer.com/article/10.1186/s40064-
016-2669-4
15. Seema, S., Mamatha, K.R., Thejaswini, S.: Intelligent fault identiﬁcation system for
transmission lines using artiﬁcial neural network. IOSR J. Comput. Eng. (IOSR-JCE) (2014)
16. Tatjana, K., Goran, Š.: Odlučivanje i Optimizacija. Repro Karić d.o.o, Tuzla (2010)
17. Nikola, R., Dragan, T., Gojko, S.: Distributivne i industrijske mreže. ETF Akademska
misao, Beograd (2004)
18. Haadi, S.: Power System Analysis. R.R. Donnelley & Sons, Boston (1999)
19. Manitoba HVDC Research Centre a division of Manitoba Hydro International Ltd. IEEE 09
Bus System (2014)
Power System Fault Detection, Classiﬁcation and Location
101

Single Phase Fault Location in Distribution
Network Based on Wavelet Transform
of Current Traveling Waves
Šeila Gruhonjić Ferhatbegović(&)
PE Elektroprivreda B&H, Sarajevo, Bosnia and Herzegovina
s.gruhonjic@epbih.ba
Abstract. Determination of single-phase fault in medium voltage (MV) distri-
bution network is important because this is by far the most common form of a
fault. During single-phase faults, higher values of fault resistance are possible
which adversely affect to the accuracy and reliability of distance determination
from supplied substation (TS) to the fault place. The presented method, based on
wavelet transformation, shows signiﬁcant beneﬁts during computer fault simu-
lations. By application of this method, it is possible to locate single-phase faults
in isolated and compensated distribution network as well as faults with signif-
icant values of a transient fault resistance.
1
Introduction
Electric power networks represent the most widely used part of the power system.
Statistics show that, the largest number of faults are occurring in the networks com-
pared to the total number of faults in the power system (over 70%). Faults in the
network include all fault types due to the number of affected phases and earth. How-
ever, exploitation experiences show that the most common are single-phase faults. In
addition, faults in power networks can be transient and permanent. Transient faults are
successfully eliminated by the application of single-phase and three-phase technique of
automatic reclosing (AR) do not lead to interruption of electrical energy transmission
and consumption of human work. Permanent faults are often caused by mechanical
failure. Transient faults can also be caused by mechanical failure but with a signiﬁcant
less impact. Transient faults also need to be removed because there is a possibility of
fault extending and transition from temporary to permanent fault. Permanent fault on
the network cause feeder switching off and interruption of electrical energy supplying.
The important components of fault management are fault point determination, coming
to the fault location and eliminate the cause of a fault. The time required for fault
determination on feeder signiﬁcantly affect to the overall time required for fault repairs
and restore parts of the network in normal operation condition. Using of numerical
protections and possibility of storing a large number of voltage and current samples,
from the moment of fault occurrence to the turning off circuit breaker for faulty feeder,
allows the application of various numerical methods in order to determine the distance
to the fault [1]. The most previous research and development in this area was focused
on the fault location in the transmission networks due to the greater impact of these
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_9

faults on the power system. In addition, the lines of the transmission system have grater
length and more time required for the round of the line route and fault detection.
Distribution network is a collection of interconnected power objects of medium and
low voltage, which provides continuous link between the consumer and the part of a
system that ensures production and transmission of electrical energy. In the current
conditions, the medium voltage levels of 10, 20 and 35 kV are in operation. In per-
spective it is trying to move to direct transformation 110/20 kV and the one level of
medium voltage. In general, the power distribution network comes to every consumer,
and because of its spatial dispersion, it represents the part that is most exposed to
environmental and atmosferic phenomenon. By now, automated determination of the
distance to the fault place usually is not used for distribution network in practice.
However, the numerous methods of fault location determination were developed for
distribution 10 and 20 kV network based on samples of current and voltage. In general,
there are methods for determining distance to the fault place from impedance or
reactance calculation [2–6] and methods based on transient appearances and traveling
waves [7–15]. In this paper, a method for determining the distance to the fault place
based on traveling waves that are extracted from current signals by applying wavelet
transformation is presented. The method referenced in [12] also uses wavelet transform
but it is applied on the voltage signals. The method shown in [14] is very similar and
takes into consideration non-uniform feeders but it uses voltage signals also. In this
method, the current signals were used due to the assumption of easier implementation
in the ﬁeld. Also, models of feeders in this methods include distributed generators.
During testing of this method, the different treatment of neutral point of distribution 10
(20) kV network and different values of fault resistance are taken into account. The
proposed approach is considered by performing computer simulations on the distri-
bution network models. Method shown in [15] is based on low-frequency transient and
is dependent from a fault resistance.
2
Importance of Automated or “On-line” Fault Location
The term of automated fault place determination relates to determination of the fault
distance on the basis of stored voltage and current signals. The signals are taken from
the time of fault occurrence to the moment of switching off a feeder circuit breaker. In
the literature it can be found the term “on-line” determination of fault place. Different
algorithms that take samples of current and voltage at the feeder beginning in supplied
TS are used for determining the fault place. In recent years, the possibility of deter-
mining the distance to the fault place on the MV feeders (10 and 20 kV) are
increasingly considered. Here are mainly involved overhead distribution feeders which
length of medium voltage network exceeds 10 or 15 km. In the Fig. 1 the network part
of partially urban area with routes of 220, 110 and 10(20) kV lines is shown. It may be
noted that the network of 10(20) kV also very widespread and can be exposed to a
signiﬁcant number of faults.
In recent times, the development of the energy system and appearance of the
electrical energy market is increasing importance of fast fault location in the distri-
bution networks. It is necessary to reduce the time in which consumers are without
Single Phase Fault Location in Distribution Network
103

power and that is the cost for distributors, in terms of the market conditions. In
addition, it should be noted that, in distribution networks, the concept of automated
locating the fault does not apply to the determination of the fault by using the devices
installed “in depth” of the distribution network. This method for fault location, which
uses data from fault indicators and fault locators set to multipoint on distribution
feeders, can be used only for determining faulty section. In recent years, there is also
the possibility of determining the area of a fault (or faulty section) with
SCADA/DMS/OMS (Supervisory Control and Data Acquisition/Distribution Man-
agement System/Outage Management System) system.
Orientation to one level of medium voltage in distribution (20 kV), even more will
highlight the need for automated fault location, because the feeders will have greater
length in the aim of better throughput. The commercial modules of fault locators, for
distribution networks, appear after the introduction of the ﬁrst numerical distribution
protection. These are locators that make the fault distance estimation on the basis of
current and voltage fundamental frequency signals and calculation reactance or
impedance to the fault place. In this case, it is necessary to know the potential reach and
usability of these devices, depending on the fault parameters, treatment of neutral point,
and other inﬂuencing factors. In MV networks, location of single-phase fault in isolated
and compensated network is particularly important. On the area of Bosnia and
Herzegovina and the region, isolated networks are signiﬁcantly present and the most
common fault is just a single-phase fault. The currents of single-phase fault in overhead
networks have not large values and do not depend on the fault point. Furthermore, for
single-phase faults in distribution networks with any treatment of neutral point, it is
Fig. 1. Topological display of the electric power network with direct transformation:
220 kV
line,
110 kV line,
10(20) kV line (feeder),
cables 10(20) kV placed in the tubes
104
Š. G. Ferhatbegović

important to take into account the signiﬁcant value of transitional fault resistance.
Location of single-phase fault in distribution networks, regardless of the neutral point
treatment, it is theoretically possible to make in the area of traveling waves. It can be
used traveling waves of voltage or currents registered at the beginning of distribution
feeder. In general, transient fault location methods use currents and voltages in transient
conditions that occur in the process of discharging and charging of the network
capacity in the moment of fault occurrence. The one perspective method for calculation
distance to the fault place shown here. This method is based on the calculation time of
traveling waves propagations from the fault place to the supplied TS.
3
Transients During Single-Phase Faults
The distribution system consists of resistance, inductance and capacity that exist as
concentrated parameters, but they are also distributed along the feeder or transformer.
During each change of condition, as operation of switching off a short circuit or in the
case of a fault, this system will generate a wide range of transients. The duration of the
transition process in distribution systems of 10(20) kV voltage level is limited to only a
few periods after the moment of fault occurrence. Charging transients of network
capacity occur in the ﬁrst milliseconds after the moment of single- phase fault
occurrence, in distribution networks with isolated, compensated and low—resistance
grounding of neutral point.
3.1
Charge Transients
In the case of single-phase fault in power system, voltages of two healthy phases rapid
increase, which creates transient phenomenon. These transients pass through the
windings of transformers in TS x/10(20) kV. The process of charging causes transients
that are visible in the zero currents of all feeders in the network. Transients of all
healthy phases are closed across the earth through a fault place and in the opposite
direction pass through the faulty feeder. The process of charging capacity of healthy
feeder can be described by the equation:
v0 tð Þ ¼ v0 t0
ð Þ þ 1
Cz
Z t
t0
i0 s
ð Þds
ð1Þ
where is:
Cz—equivalent earth capacity of a feeder
i0—zero current of a feeder
Charge transients are low frequency and in the distribution networks have fre-
quencies in the range of 100–800 Hz. The amplitude of charge transients in the process
of charging of network capacity can be 10–20 times greater than the fundamental
harmonic of the total uncompensated fault current and it can be easily extracted from
the fundamental harmonic. Charge transients still present in the zero current, but are
Single Phase Fault Location in Distribution Network
105

dependent on substation transformer inductance and feeder’s data. Charge transients
are reduced signiﬁcantly in the case of the higher resistance values at the fault point
[15].
3.2
Discharge Transients
At the time of single-phase fault occurrence, the charge accumulated in earth capacity
of faulty phase moves and discharge transient is formed. The amount of charge that is
discharged depends on the moment of the earth fault occurrence. For example, if the
faulty phase voltage passes through zero, at the time of fault occurrence, it will not
occur discharging. Conversely, if the voltage in the moment of fault has a maximum
value, then amplitude of discharge transient will be maximum too. The frequency of
transients during discharging of capacity is determined by the speed of traveling waves
and the distance of the fault. The Fig. 2 shows lattice diagram for the case of
single-phase fault and frequencies of discharge transients are determined by the Eq. (5)
[14]. Traveling waves, caused by the fault, reﬂected between fault point and TS x/10
(20) kV. The amount and polarity of the reﬂected wave depends on the reﬂection
coefﬁcients. The reﬂection coefﬁcient is a number between −1 and 1, and for the
voltage is deﬁned as:
bv ¼ ZT  ZC
ZT þ ZC
ð2Þ
ZC—characteristic impedance of the feeder
ZT—terminal impedance of the feeder
The reﬂection coefﬁcient for the current is bi ¼ bv. The factor of passage or
attenuation is coefﬁcient deﬁned as:
Fault
x [km]
t1
t2
t3
Fig. 2. The lattice diagram
106
Š. G. Ferhatbegović

av ¼
2ZT
ZT þ ZC
ð3Þ
From the lattice diagram it can be seen that the period of traveling waves, between
fault place and buses of supplied TS x/10(20) kV (or measuring place in supplied TS),
is equal to:
T ¼ 4 x
v
ð4Þ
Angular frequency of discharge transients can be calculated as:
x ¼ 2p v
4x
ð5Þ
where is:
v—velocity of the traveling wave
x—fault distance.
The situation is idealized in the Fig. 2. The reﬂection coefﬁcient of the voltage is 1
in substation (assumed that impedance of system is inﬁnite compared to feeder
impedance ZC) and −1 at the fault place (assumed that the fault impedance is zero). In
the case of a fault on the feeder, traveling waves move from the fault point to the
busbars of the supplied substation.
4
Fault Location Based on Current Traveling Waves
Fault on a feeder will cause high frequency signals of voltage and current spread
through a distribution network. The basic idea of the method of traveling waves begins
on the reﬂection of the faulty generated waves at the fault point and at the points of
discontinuity in distribution feeder. The developed method of determining fault place,
which is described here, based on the identiﬁcation of traveling waves, and respond to
more requests when determining the place of single-phase faults in distribution system.
In general, the methods of traveling waves can be graphically displayed by using a
lattice diagram (Fig. 3). The fault is assumed at a distance x from the busbars of the
supplied TS x/10(20) kV. This approach for determining fault place is based on iter-
ative identiﬁcation of arrival, on the busbars where the locator is placed, backward and
forward traveling waves reﬂected from the remote points of discontinuity. In MV
distribution networks should be taken into account a greater number of points of
discontinuity in relation to transmission networks. To be discussed traveling waves, it
is necessary to have a sufﬁciently high sampling frequency and the corresponding
current and voltage transformers for transmission necessary signals without distortion.
In this work, the method of traveling waves has been tested by using current signals, or
waves of current that occurred immediately after the moment of fault inception.
Single Phase Fault Location in Distribution Network
107

4.1
Wavelet Transformation
Traveling wave as a high frequency signal, can be separated difﬁculty from the noise
effects. Fault location systems, based on the traveling waves, require a high sampling
rate to obtain accurate information about the fault. The methods that allow the
time-frequency representation of a faulty signal are more effective, in analyzing
non-stationary signals. Wavelet transform has the property of multiple resolution in
time and frequency and can locate transient components in time, while maintaining
information about the fundamental frequency component and lower harmonics [16].
The wavelet transform (WT) of the continuous signal f(t) is deﬁned as:
CWT scale,position
ð
Þ ¼
Z1
1
f tð Þu sk.,pos.,t
ð
Þdt
ð6Þ
By introducing the parameters of scaling and translation (“a” and “b”), the previous
equation can be written as:
CWT a,b
ð
Þ ¼ 1ﬃﬃﬃa
p
Z1
1
f tð Þu t  b
a


dt
ð7Þ
In the previous equation, u(t) is the basic wavelet which is bandpass frequency and
u*(t) is complex-conjugated form. The parameter “a” is inversely proportional to the
frequency. The basic wavelet should satisfy the condition:
Z1
1
u tð Þdt ¼ 0
ð8Þ
High scaling gives generalized information about the signal (so that it usually
includes complete signal) while low scaling gives detailed information about the
hidden pattern in the signal, which usually takes a relatively short time. The terms of
translation refers to the translation of the window location. Discrete wavelet transform
(DWT), applied to the sampled signal f(n), is deﬁned as:
x
l-x
S
R
F
FL
T1
T2
T1- arrival time of the first peak corresponding to
the backward travel wave from the fault point
T2 - arrival time of the second peak corresponding
to the forwarded travel wave reflected from the
remote point of discontinuity
Fig. 3. Lattice diagram. S is sending end of a line, R is receiving end of a line, x is distance do
the fault place, FL is fault locator and F is fault place
108
Š. G. Ferhatbegović

DWT m,k
ð
Þ ¼
1ﬃﬃﬃﬃﬃﬃ
am
0
p
X
n
f n
ð Þu k  nam
0
am
0


ð9Þ
ui(n)—fundamental wavelet
a, b—scaling and translation parameters that are functions of an integer parameter
“m” (a ¼ am
0 b ¼ nam
0 )
4.2
Method of Determining the Place of Single-Phase Fault
in Distribution System by Applying Wavelet Transformation
The developed method of determining the fault point, which is described here, is based
on the identiﬁcation of traveling waves, and satisfy more requests when determining
the place of single-phase faults in distribution system. This method can be used
independently of the neutral point treatment of distribution network. With application
of adjusting the length, i.e. with reduction to the same speed of propagation, the method
is applicable to non-uniform distribution feeders. For speciﬁc conﬁguration of the
distribution feeders, it is necessary to have the appropriate lattice diagram. The lattice
diagram drawn in the scale, in accordance with the length of sections, provides
information about the reference values of time between the individual peaks. This
facilitates the identiﬁcation of relevant peaks, in separated signals, by applying of
wavelet transformation. For application the methods developed in this paper, the fault
initiated high frequency component of the current signal is extracted by using wavelet
transformation. In radial distribution networks, fault signals are recorded only in the
supplied substation and the distance to the fault is based on these measurements. The
fault location procedure comprises from the following steps:
• adjustment of correction of section’s length for non-uniform feeders
• calculation of time Tc,
• modal transformation of measured phase currents,
• discrete wavelet transformation,
• identiﬁcation of faulty section (fault diagnosis),
• distance to the fault point calculation
In the case of distribution feeders with different types of conductors, it is necessary,
for selected speed of propagation and for calculated the distance to the fault point, to
observe the adjusted section’s lenth. The adjustment of section’s length can be
calculated as:
s2 ¼ s1v1
v2
ð10Þ
where is:
s1—actual length of a section
v1—actual speed of propagation
Single Phase Fault Location in Distribution Network
109

s2—correction (adjustment) of length corresponding to the adopted speed of
propagation
v2—adopted speed of propagation for estimating the distance to the fault
This is especially important when the distribution feeders consist from overhead
and cable sections, or when there is a big difference in the speeds of propagation. In the
process of so-called fault diagnose, faulty section is ﬁrst determined by comparing the
time difference, of initial peak arrival of wavelet transform signal (CDWTs) for
homopolar and diagonal components, with times calculated for faults in the middle
sections of the observed 10(20) kV feeder.
Tc ¼ xc
v0
 xc
v1
ð11Þ
where is:
xc—assumed fault location (middle of each feeder’s section)
v0, v1—speeds of propagation for homopolar and diagonal components
For calculation of time Tc, it is necessary to know exact conﬁguration of 10(20)
feeders (sections lengths, type of conductors). If recorded time difference is less than
the previously calculated value of Tc, then the fault is in the ﬁrst half of section and the
distance to the fault can be calculated as:
x ¼ v1DT
2
; DT ¼ T2  T1
ð12Þ
Where is:
v1—speed of propagation of diagonal components
T1—arrival time of the ﬁrst peak in the diagonal component signal corresponding to
the backward travel wave (from the fault point)
T2—arrival time of the second peak in the diagonal signal component corre-
sponding to backward travel wave reﬂected from the fault point (Fig. 4, fault F1)
If the resulting time difference ΔT is greater than Tc, the fault is in the second half
of the section, and for determining the time difference, the next equation can be used:
DT ¼ 2l
v1
 T2  T1
ð
Þ
ð13Þ
Where is:
T1—arrival time of the ﬁrst peak in the diagonal component signal,
T2—arrival time of the second peak in the diagonal signal component corre-
sponding to the forward travel wave reﬂected from the remote end and
l—total length of the observed feeder’s section.
110
Š. G. Ferhatbegović

Assumed faults in different points of distribution feeder are shown in the Fig. 4
In the case of more sections of distribution feeder, in the process of calculating the
distance to the fault, for the length is used P
i
li, i.e. the total length of the sections from
supplied substation. Distance to the fault point can be deﬁned as:
x ¼ v1
2
2Rl
v1
 Tj  T1




ð14Þ
where is:
T1—arrival time of the ﬁrst peak in the diagonal component signal corresponding to
the backward travel wave reﬂected from the fault point,
Tj —arrival time of the j-th peak in the diagonal signal component corresponding to
the forwarded travel wave reﬂected from the remote end.
The arrival time Tj of the j-th peak in the observed signal corresponding to the
forwarded wave increases as the fault location moves closer to the point of measure-
ment (in the observed sections). So that, if there are multiple branches (points of
discontinuity) between the supplied substation and fault point, then peak with time T2
will not arrive as second in comparison with the time T1. For reasons of possible large
number of branches, for conﬁguration of particular 10(20) kV feeders, it is necessary to
consider the appropriate lattice diagram (corresponding to a speciﬁc distribution feeder
with all sections and branches). The previous formula takes into account the wide-
spread of distribution feeders, which is one of the main differences of the distribution
network in relation to transmission. Distance between the individual peaks, measured
in microseconds, on the lattice diagram with the assumed fault (as a result of fault
diagnosis), should correspond to the disposition of wavelet transform coefﬁcients.
F2
travel waves caused
by fault F1
travel waves
caused by F2
travel waves caused
by fault F3
T2
T1
T2
T2
T1
S
R
Fault
locator
F3
F1
T1
Fig. 4. Lattice diagram for single-phase faults on the feeder’s sections
Single Phase Fault Location in Distribution Network
111

Certainly, and in the case of faults in branches, ﬁrst of all the fault diagnosis is done. It
may be noted that, depending on the length of the branches, the arrival time of the
reﬂected wave increases as the fault location moves closer to the branching point. In
principle, the procedure is the same as for the main sections and the distance to the fault
can be calculated from the equation:
x ¼ DTbv1
2
; DTb ¼ 2Lb
v1
 DT
ð15Þ
DT ¼ T1  Tj,Lb ¼
X
i
Li þ
X
i
Bi
Where is:
T1—arrival time of the ﬁrst peak of diagonal component corresponding to the
backward travel wave (from the fault point)
Tj —arrival time of the j-th peak of diagonal component corresponding to the
forward travel wave reﬂected from the remote end
Li —length of the i-th section that belong to the main sections of feeder
Bi —length of the i-th section that belong to branches.
5
Testing by Computer Fault Simulations
For the distribution feeders presented in the Figs. 5 and 9, testing in mathlab has been
carried out. It was used a discrete wavelet transform in mathlab, db4 wavelet [17]. The
sampling time was set at 1 µs. The feeder model in the Fig. 5 is simply and uniform.
The feeder model in the Fig. 9 is more realistic for MV distribution network, includes
more sections and brances and feeder’s structure is non-uniform. In this case, both
sections, overhead and cable, are included. For presented results in this paper, fault
inception was set at the time of t = 0, 01 s.
For example, the calculated time differences (by Eq. 11) for the ﬁrst model are
listed in the Table 1. The assumed fault was at the distance of 14 km from the supplied
v(t)
Fault
locator
B1
B2
L3
L2
L1
Main sections:
L1=9 km,  L2=14 km,  L3=8 km
Branches:
B1=11 km,  B2=7 km
Supplied TS x/10(20) kV
Fig. 5. Single-phase diagram of modelled uniform MV feeder
112
Š. G. Ferhatbegović

substation. The corresponding lattice diagram is in the Fig. 6. One can see that for-
warded travel wave reﬂected from discontinuity point should arrive (on the busses in
the supply substation) as the third peak.
The discrete wavelet transform of current signals in the case of the fault distance of
14 km, for the uniform feeder, has been shown in the Fig. 7. The ﬁrst peak of the signal
dwt(i0) was recorded in the time of T0 = 0,010096 s. The ﬁrst peak in the signal dwi
(i1) was recorded in the time of T1 = 0,010046 s. Therefore, the delay of homopolar
components for this fault is DT ¼ T1  T0
ð
Þ ¼ 50 ls. This time difference, according
to Table 1, shows that the fault is in the section of L2 or B1. By checking samples of
dwt for the presumed faults on the L2 and B1, it can be concluded that the fault is on
the L2.
The third peak in the diagonal signal component is recorded in the time of 0,010112
s, which correspond to the delay of DT ¼ 1; 0112  1; 0046
ð
Þ102 ¼ 66 ls. The
calculated distance to the fault place (according to the Eq. 15) is:
x ¼ 275897
2
223
275897  66


¼ 13; 895 km
The speed of propagation of diagonal components for overhead lines is
v1 ¼ 275897 km/s,
The
speed
of
propagation
of
homopolar
components
is
v0 ¼ 140397 km/s. The pattern of dwt for the assumed fault on B1 is shown in the
Fig. 8. It was used the model of compensated distribution network. Fault resistance was
TS x/10(20) kV
l [km]
t [ s]
T1
T2
F
Fig. 6. Lattice diagram for the assumed fault at the distance of x = 14 km
Table 1. Time differences for the indicated fault place
xc [km] Calculated Tc [ls] Sections and branches
4,5
15,74
L1
14,5
50,72
L2, B1
16
55,97
L2, B1
26,5
92,70
B2, L3
27
94,45
L3, B2
Single Phase Fault Location in Distribution Network
113

Fig. 7. Wavelet transform coefﬁcients (homopolar and diagonal component) for the case of
single-phase fault at the distance of x = 14 km (on the section L2) from the supplied substation.
a discrete wavele transform of signal i0—dwt(i0). b discrete wavelet transform of signal
i1—dwt(i1)
114
Š. G. Ferhatbegović

set at Rf = 300 Ω. The modelled feeder is uniform. The current signals recorded at the
beginning of the feeder were used.
Testing was carried out on the model of distribution feeder shown in the Fig. 9,
also. This is example of non-uniform feeder with more branches and discontinuity
points. For example, the case of the fault at the distance of x = 6 km is presented.
The wavelet transform of the signals i0 and i1 for this fault are shown in the
Fig. 11.
The
signal
of
homopolar
component
is
recorded
at
the
time
of
T0 = 0,010036 s on the buses of the supplied TS. It is delayed for DT = 20 µs
compared to the diagonal signal component (diagonal signal component recorded at the
time of T1 = 0,010016 s). Based on the calculated time Tc for all sections of the feeder
it can be concluded that the fault is on the sections B6, L3 or B5 (it is necessary to
Fig. 8. Wavelet transform coefﬁcients (homopolar and diagonal component) for the presumed
single-phase fault on the branch B1
Single Phase Fault Location in Distribution Network
115

make tabular overview as in the previous case of uniform feeder). Additional fault
diagnosis indicates that the fault is on the section L3. Lattice diagram drawn in scale for
the modeled feeder shows that forwarded travel wave arrives at the buses of supplied
substation as third peak (Fig. 10).
The time difference of arrival of the ﬁrst and third peak in diagonal signal com-
ponent is DT = 7 µs DT ¼ T2  T1 ¼ 0,010023  0,010016
ð
Þ. Estimated distance to
the fault place is 5,08 km.
D1
D2
D3
D4
D5
D6
D7
TS 110/10(20) kV
1.5 km
L1
L2
L3
L4
L5
B1
B2
B3
B4
B5
B6
B7
B8
B9
B10
2 km
3 km
TS 1
TS 2
TS 3
TS4
TS 5
TS 6
TS 7
TS 8
1.6 km
0.5 km
0.9 km
1.5 km
0.5 km
2.5 km
0.9 km
2 km
0.5 km
0.8 km
1.2 km
0.9 km
630
160
1000
160
160
160
160
250
cable XHE 49-A 150 mm2-1 ž
cable XHE 49-A 150 mm2-1 ž
cable XHE 49-A 150 mm2-1 ž
line Al-Fe 35 mm2
line Al-Fe 35 mm2
line Al-Fe 35 mm2
line Al-Fe 35 mm2
line Al-Fe 35 mm2
MHE 1
MHE 2
400 V, 50 Hz, 85 kVA
400 V, 50 Hz, 910 kVA
Un=10 kV
F
Fig. 9. Single line diagram of non-uniform distribution 10 kV feeder with connected
distribution generators
F
T2
l [km]
t [ s]
T1
D1
D3
D5
Fig. 10. Lattice diagram for the assumed fault at the distance of x = 6 km
116
Š. G. Ferhatbegović

Fig. 11. Wavelet transform (homopolar and diagonal component) in the case of fault at the
distance of x = 6 km from supplied TS a discrete wavelet transform of signal i0—dwt(i0)
b discrete wavelet transform of signal i1—dwt(i1)
Single Phase Fault Location in Distribution Network
117

x ¼ 275897
2
km
s
2  6; 05
275897 ls  7ls


¼ 5; 08 km
The
speed
of
propagation
of
diagonal
components
for
cable
lines
is
v1 ¼ 104646 km/s. The adjusted length of the ﬁrst cable section is 0,55 km. The actual
length of the ﬁrst cable section is 1,5 km (Fig. 9). If we take into account the correction
for the ﬁrst cable section (according to Eq. 10), then the calculated distance to the fault
place is x = 6,03 km. The modelled distribution network was with compensated
neutral point. In this case, single-phase fault was simulated with fault resistance of
Rf = 500 Ω.
6
Conclusion
The most important beneﬁt of the method based on traveling wave is independence of
the fault resistance values and the neutral point treatment of MV distribution network.
Single-phase faults in distribution networks, typically include resistances that can have
great values (high-resistance ground faults and breakdowns of insulation to earth and
earthed parts). The current signals were used due to the assumption of simpler and
reasonably accurate conversion of primary signals. The method of determining the fault
place based on identiﬁcation of current travel waves is independent in terms of
asymmetry and dynamics of distributed loads. Also, the method is independent of the
moment of fault inception, i.e. of the initial phase angle of current for faulty phase. The
real problem of this method is identiﬁcation of relevant peaks in signals of diagonal and
homopolar components. This practically means identifying the arrival of reﬂected
waves from the fault location and discontinuity points, on the buses of supplied sub-
station 110/x kV. The advantage of the presented method is the possibility of fault
location in the cases that there are distribution generators in the network. The sampling
frequency for the methods of this type has minimum of 1 MHz.
References
1. Phadke, A.G., Thorp, J.S.: Computer Relaying for Power Systems. Wiley (1988)
2. Das, R., Sachdev, M.S., Sidhu, T.S.: A fault locator for radial subtransmission and
distribution lines. In: IEEE Power Engineering Society Summer Meeting, Seattle,
Washington, USA, July 2000
3. Zhu, J., Lubkeman, D.L., Girgis, A.A.: Automated fault location and diagnosis on electric
power distribution feeders. IEEE Trans. Power Deliv. (1997)
4. Jardini, J.A., et. al.: Daily load proﬁles for residential, commercial and industrial low voltage
consumers. IEEE Trans. Power Deliv. (2000)
5. Hanninen, S.: Single phase earth faults in high impedance grounded networks, Character-
istics, indication and location, Ph.D. dissertation, Helsinki University of Technology, Espoo
Finland
6. Mokhlis, H., Li, H.Y., Khalid, A.R.: The application of voltage sags pattern to locate a
faulted section in distribution network. Int. Rev. Electr. Eng. (IREE) 5(7), 173–179 (2010)
118
Š. G. Ferhatbegović

7. Mokhlis, H., Bakar, A.H.A., Talib, D.N.A., Mohamad, H.: The improvement of voltage sags
pattern approach to locate a fault in distribution network. Int. Rev. Electr. Eng. (IREE) 5(3),
1159–1164 (2010)
8. El-Hami, M., Lai, L.L., Daruvala, D.J., Johns, A.T.: A new traveling wave based scheme for
fault detection on overhead power distribution feeders. IEEE Trans. Power Deliv. 7(4)
(1992)
9. Bo, Z.Q., Weller, G., Redfern, M.A.: Accurate fault location technique for distribution
system using fault generated high—frequency transient voltage signals. In: IEE Proceeding
Generation Transmission and Distribution, vol. 146, No. 1, Jan 1999
10. Thomas, D.W.P., Carvalho, R.J.O., Pereira, E.T.: Fault location in distribution systems
based on Travelling waves. In: IEEE PowerTech Conference, June 2003. Bologna, Italy
(2003)
11. Nouri, H., Wang, C., Davies, T.: An accurate fault location technique for distribution lines
with tapped loads using wavelet transform. In: IEEE Porto PowerTech Conference, Porto,
Portugal (2001)
12. Cansin, Y., Evrenosoglu, A.A.: Fault location in distribution systems with distributed
generation. In: 15th PSCC, Liege, Aug 2005
13. Fernando, H., Magnago, A.A.: Fault location using wavelets. IEEE Trans. Power Deliv. 13
(4) (1998)
14. Ferhatbegović, Š.G., Marušić, A., Pavić, I.: Single phase fault distance estimation in medium
voltage distribution network based on traveling waves. In: International Review of Electrical
Engineering (I.R.E.E.), Praise Worthy Prize S.r.l., vol. 7, No. 1, Feb 2012. ISSN:1827-6660
15. Ferhatbegović, Š.G.: Single phase fault location in distribution network based on charge
transients. In: International Review of Electrical Engineering (I.R.E.E.), Praise Worthy
Prize S.r.1., vol. 10, No. 1, Feb 2015. ISSN:1827-6660
16. Robertson, D.C., Camps, O.L., at al.: Wavelets and electromagnetic power system transients.
IEEE Trans. Power Deliv. (1996)
17. Matlab—SimPowerSystems User’s Guide, Hydro—Quebec, Mathworks, Sept 2010
Single Phase Fault Location in Distribution Network
119

The Analysis and Beneﬁts of the New TS
110/x KV Interpolation on the Distribution
Network
Jasmina Čučuković1(&), Faruk Hidić1, and Ismet Kulović2
1 JP EP BiH d.d. Sarajevo – Podružnica ED Zenica, Put mira 4, Vareš,
Bosnia and Herzegovina
{j.cucukovic,f.hidic}@elektroprivreda.ba
2 JP EP BiH d.d. Sarajevo – Podružnica ED Zenica, Zukići 28, Zenica,
Bosnia and Herzegovina
i.kulovic@elektroprivreda.ba
Abstract. Intensive economic development in recent years, as well as growing
amounts of electricity consumption in the Municipality of Tešanj, indicate the
need for a systematic approach in deﬁning the ways of meeting growing elec-
tricity demands. Taking into account the expected increase in load (through the
year 2025), and using the appropriate mathematical models, this paper presents
the results of an impact assessment of the interpolation of the new 110/x kV Jelah
substation into the medium voltage Tešanj distribution network—speciﬁcally
regarding the Jelah area. The results of the analysis of power ﬂows, voltage
conditions and losses, with respect to the expected increases in energy con-
sumption in 2020 and 2025 in the analyzed area, indicate that the problem of
overload to the existing 35/10 kV substations requires special attention when
planning the development of distribution networks, and that inadequate devel-
opment and untimely investments into the electricity grid could lead to signiﬁcant
changes in voltage and active power losses in the network, thus compromising
standardized limits.
Keywords: Electric
power
distribution
system 
110/x kV 
Power
consumption  Voltage  Active power losses
1
Introduction
Distribution networks today are facing many challenges. Overall, population growth
and rising standard of living are causing an increase in electricity consumption, which
is why networks must ensure the delivery of larger quantities of certain quality
electricity.
In order to meet this growing demand for electricity, it is necessary to properly plan
the distribution system—both from a technical and an economic viewpoint. The
planning of distribution network is one of the central issues in their development. The
objective is to satisfy consumer electricity consumption needs at a required quantity,
quality, and continuity, while keeping costs at minimum. The task of planning the
development of distribution systems is reduced to satisfying consumer needs by
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_10

building additional distribution capacity. By selecting elements, performing installa-
tions, and conceptualizing the distribution system in general, the end-goal is to achieve
such a reliability in consumption satisfaction that is economically justiﬁed. Therefore,
reliability budgets are calculated based on statistical data, and comparisons of different
solutions are performed based on economic indicators.
The steps in planning the development of distribution networks:
1. Analysis of the long term development of the load and the basic concepts of
planning distribution networks.
2. Establishing baseline conﬁgurations and studying the parameters of the network.
3. Selection of equipment and integration locations.
4. Detailed analysis of the dynamics of development in the medium term.
From a number of different factors that affect the planning of distribution networks,
those of particular importance are: consumption forecast, integration of new substa-
tions, optimal locations of substations. Others include demographics, economics, etc.
[1].
The basis of any method of spatial load forecasting for planning the development of
distribution networks are data representing measured past loads in substations 110/x,
35/x and 10(20)/0.4 kV. The greater the availability of historical data, the more
credible the results of forecasting—and thus the planning itself will have better quality
[2].
According to the data on the consumption at 110/x kV substations from 2016
through 2025 submitted by Elektroprivreda BiH to the NOS BiH (the Independent
System Operator of Bosnia and Herzegovina), an average growth of 3% is deﬁned for
the base scenario, while the optimistic (higher) scenario and the pessimistic scenario
see an estimated growth of 4% and 2%, respectively.
The table below shows a forecast of peak load of distribution network. The forecast
was made on the basis of the electricity balance for 2015, as well as the average
percentages of increase for the base, higher, and lower scenario (Table 1).
The analysis of the achieved and forecasted loads at existing 110/x kV substations
within the planned period deﬁnes the need for an increase transformation power in an
existing substation either by installing a more powerful transformer, or constructing a
new 110/x kV node. This analysis is carried out before the formation of models for
load ﬂow and voltage calculation.
The decision to build a new 110/x kV facility is to be made based on the following
criteria:
Table 1. Planned load peak of distribution network—EP BiH (in MW) [3]
Consumer
2016
2017
2018
2019
2020
2021
2022
2023
2024
2025
EP BiH (base)
523,9 539,7 555,8 572,5 589,7 607,4 625,6 644,4 663,7 683,6
EP BiH (higher) 529,0 550,2 572,2 595,1 618,9 643,7 669,4 696,2 724,0 753,0
EP BiH (lower)
518,8 529,2 539,8 550,6 561,6 572,9 584,3 596,0 607,9 620,1
The Analysis and Beneﬁts of the New TS 110/x KV
121

• According to the proposal by the responsible public utilities ﬁrm, a planned load at
the new 110/x kV substation in the year of its commissioning exceeds 8 MVA
(matching the highest power transformer unit in the distribution network) for areas
where there are no 110/x kV substations;
• The measured peak load in the existing 35/x kV substation exceeds 8 MVA;
• Forecasted load of an existing 110/x kV substation exceeds 60% of the installed
capacity of the transformer;
• Distance to the emerging consumer subgroup, for whose purposes the new
110/x kV substation is being built, exceeds 10, 20, or 35 km, depending on the
value of the nominal voltage in the MV channel supplying the subgroup from the
existing 110/x kV;
• Unsatisfactory voltage conditions in the MV network powered by the existing
110/x kV substation (quality of power in accordance with the terms and conditions
for delivery and supply of electricity).
When deciding on the need to build, commitment is determined by considering
several criteria at the same time [4].
All the principal criteria for the development of the distribution network converge
into reliable supply of quality electricity within the future that is observed during the
planning period [5].
2
Procedure
During the construction or renovation of facilities of the distribution network it is
necessary to employ solutions that will enable ﬂexibility in the development of the
network in the future:
• Timely planning of new facilities—the construction of which can often take several
years—assuming an adequate construction of the 10(20) kV connection network,
• Rational solutions for the construction of new 35 kV lines, or laying cables, while
keeping in mind that after transitioning to direct transformation they will operate at
20 kV,
• Construction of 110/10(20) kV substations in major urban areas, with maximum
usage of the existing 35/10(20) kV transformation based on the appropriate con-
struction of the 10(20) kV connection network, respecting the (N-1) criterion in the
case of unavailability of a line or transformer,
• As a rule, at 35/10(20) kV substations in rural areas there is sufﬁcient reserve in the
power of transformation, but the problem is the voltage drop along the 10 kV lines.
The solution is usually the reconstruction of 35/10 kV into 35/20 kV, or, in the
more distant future, into 110/20 kV,
• In rural areas, in the vicinity of an already built 110 kV line, it is possible to use a
simpliﬁed, single-transformer, 110/10(20) kV substation with a small power
transformer (8 or 10 MVA). This may imply either a new transformer station, or the
reconstruction of an existing 35/10(20) kV into a 110/10(20) kV in order to avoid
the costs of network reconstruction and 35 kV installation.
122
J. Čučuković et al.

Table 2. Annual Procurement Plan for 2015 [6]
Project or facility
Funds
carried
over from
2014
Power line
overhaul or
reconstruction
Power line
construction
Substation
overhaul or
reconstruction
Substation
construction
Total funds for
2015
Funds carried over
from 2014 + Total
funds for 2015
1
2
3
4
5
6
7
(3 + 4 + 5 + 6)
8 (2 + 7)
110/x kV
substation with
connecting power
line
400,0
0
0
0
900,0
900,0
1,300,0
The Analysis and Beneﬁts of the New TS 110/x KV
123

Proper placing of simple 110/20/10 kV facilities can result in good overall cov-
erage of the consumption area, as well as high reliability of the power supply in an MV
network.
In designing and technically preparing the building and equipping of 110/35/10 kV
substations according to the principles of a typical 110/x kV facility, the possibilities of
expansion and upgrade of plants and subsystems, as well as of installation of trans-
formers (transformation reinforcement), are considered. The analysis is carried out with
the aim of optimizing the costs of building and tackling various needs of the system.
Admission of investments into annual and three-year plans is preceded by the
development and review of the project documentation, and general preparation of the
project (temporal and ﬁnancial realization plan, a division into technical units, and a
choice of an optimal approach to contracting). During preparation (approx. 1–5 years,
depending on the complexity of the capital investment), analyses on the importance of
power facilities in the local network, as well as on the general, technical, and opera-
tional projections of development of the local MV network, are conducted (comparing
situations from the scenario in the development study). Final preparations include the
creation and revision of the project documentation.
Construction and reconstruction of power facilities are generally complex
multi-year investments that are realized through several multi-year contracts. Project
implementation is typically preceded by a multi-year project and technical preparation.
For these procedures (particularly pertaining to the preparation of the construction
of a new 110/20/10 kV facility), when planning the beginning and preparation of
construction, it is taken into account that project and technical preparation—right up
until readiness for publication of ﬁrst purchase—can last 3–5 years, and usually
involves the administrative process of obtaining a location and/or building permit [2].
When it comes to the economic justiﬁcation of investment in the distribution
network, the basic elements of economic analyses are the costs of construction and/or
renovation of facilities in the distribution network, as well as the costs of undelivered
energy and power. Economic analyses enable comparisons of various possible solu-
tions to the development of the distribution network by updating all investment and
cost values. This is based on data such as the discount rate, and the cost of undelivered
electricity.
All economic analyses should be based on actual prices of electric power equip-
ment and installation work. If there is no accurate data on the cost of a particular object,
or if there is signiﬁcant deviation from the average value, standard prices—which are
given below—should be used (Table 2).
3
The Studied Part of the Distribution Network
The area of Jelah is powered from 110/35/10 kV Tešanj over 35 kV transmission line
“Tešanj-Jelah” and 35/10 kV Jelah (2  8 MVA), which is one of the oldest substa-
tions in this region (The electric power system of Bosnia and Herzegovina and the
geographical location of the analyzed area are presented in Fig. 1).
124
J. Čučuković et al.

35/10 kV Jelah feeds the overall consumption in the Usora River Valley with
twelve 10 kV outputs, or 137  10/0.4 kV totaling in 16 MVA of installed capacity.
The maximum recorded load at 35/10 kV Jelah in 2016 was *12.5 MVA. Con-
sumption is in constant expansion since each year has recently been averaging 7–10
new 10(20)/0.4 kV facilities built. In addition, more intensive economy development
of the Municipality of Usora is expected. There are three established industrial zones
with intensive construction within the area of Jelah [4].
Given the strong growth in industrial consumption from small and medium
enterprises, as well as from urban areas in the municipalities of Jelah and Usora, and
with the constant growth of peak load and electricity consumption, it is necessary—
according to the development plans of JP Elektroprivreda BiH d.d. Sarajevo—to take
urgent measures of starting the construction of 110/x kV Jelah. The discouraging
situation with electricity in the areas of Tešanj, Jelah, and Doboj South (Matuzići) has
already caused limitations in delivery of electricity several times to customers in the
area of Tešanj lasting for up to several hours—often at times of peak loads, and
especially during the holidays [7].
Fig. 1. a The electric power system of Bosnia and Herzegovina [Elektroprenos BiH].
b Geographical location of the Jelah area
The Analysis and Beneﬁts of the New TS 110/x KV
125

The construction of 110/x kV Jelah would secure power supplies for the growing
consumption in Jelah, Usora, and Matuzići, thus relieving 110/35/10 kV Tešanj. Thus
far, there has been a realized peak load at the Tešanj facility of about 27 MW. The sum
total load at the new 110/x kV Jelah—at the time of its entry into operation, which is
scheduled for 2020—will be circa 16 MW, and will thus relieve the substation in
Tešanj. The new 110/x kV Jelah shall be incorporated into the country’s power system
by input/output into the 110 kV Doboj 1—Teslić (*0.7 km) power line, and by the
construction of the 110 kV Jelah—Tešanj (*5 km) power line, providing a two-sided
feed into 110/35/10 kV Tešanj, which is, at this moment, radially powered over
110 kV transmission line “Maglaj-Tešanj” [4, 8].
When planning new facilities, all elements that could affect the increase in elec-
tricity consumption should take into account. In that case, it can be presumed that
population growth and an increase in industrial consumption at the rate of 5% at the
end of the planned year could result in an increase of power consumption by 5–7%. As
Fig. 1. (continued)
126
J. Čučuković et al.

presented above, the adopted goal of an increase in consumption at Tešanj of within the
planned timeframe presents a realistic expectation in relation to the planned increase in
power consumption at the level of the Federation of Bosnia and Herzegovina—also of
5%. It is rational to expect this high a consumption, if one takes into account the rapid
industrial development in Jelah, where there is rising development of larger and smaller
companies [9].
Delays in construction and commissioning are sure to have a more serious impact
on economic growth in the region (which is among the most intensive in FBiH), but
also on the situation in terms of delivery to existing customers [7].
3.1
The Model of an MV Electricity Distribution Network
For the purposes of this analysis, a part of the electric power system of Bosnia and
Herzegovina that is the real MV distribution network of Jelah has been modeled. The
Jelah area is supplied with power from the primary 35/10 kV Jelah 2  8 MVA
substation. The MV Jelah network is made up of 10(20) kV lines and 35 kV lines by
which TS 35/10 kV Jelah is connected with TS 110/x kV Tešanj i TS 110/x kV Doboj
1. The total length of the MV lines is *133 km. The total number of 10(20)/0.4 kV
lines with which 8 849 measurement points are supplied is 137 [10]. Lines, trans-
formers, loads, equivalent nodes (on the 110 kV side of 110/x kV) are modeled on the
basis of [11], while all the technical data on the analyzed network are taken from the
technical database by JP Elektroprivreda BiH d.d. Sarajevo [10]. Seeing as all the
applicable measurements were not available to the authors of this study (peak powers at
industrial consumers, measurement at 10(20)/0.4 kV, etc.), peak powers at 10(20)/
0.4 kV substations were estimated on the basis of the installed powers of transformers
and the total peak load in the observed consumer areas, as well as on historical data on
maximum current loads along 10 kV outputs to 35/10 kV Jelah [12]. The network was
modeled using the PowerCAD software package [11]. Based on available real data, all
the required values for the system were calculated and entered.
The value for the load (or consumption) of active power taken for this network model
was 12,488 MW (peak load), while the factor of power was 0.95 (for consumer nodes).
Based on energy data for the peak value of power at the 35/10 kV substation Jelah from
2013 through 2016, the growth rate in the coming years was set at 6.65% per year [13].
Given this assumption, the expected peak power values in the analyzed years (2020 and
2025) were calculated. In this regard, ﬁve different scenarios were considered for the
needs of this paper: (1). the current state (2016); (2a). The state in 2020 with forecasted
consumption without taking into account development plans; (2b). The state in 2020
with forecasted demand and interpolation of the new 110/x kV substation Jelah; (3a).
The state in 2025 with forecasted consumption without taking into account development
plans; (3b). The state in 2025 with forecasted demand and interpolation of the new
110/x kV substation Jelah; (3c). The state in 2025 with forecasted demand, interpolation
of the new 110/x kV substation Jelah and transition to 20 kV voltage level. The pre-
dicted peak load values during the period of 2016—2025 are presented in Fig. 2.
The Analysis and Beneﬁts of the New TS 110/x KV
127

4
Results and Discussion
4.1
Analysis of the Current Situation and Different Scenarios
For the purposes of analyzing the impact of interpolating the new 110/x kV substation
Jelah into the MV distribution network, voltage values on buses and active power
losses were observed. Figures 3, 4 and 5 show the voltage values (per unit), based on
the above-described model and deﬁned conditions. The results of calculations of power
ﬂows, voltage conditions, and losses for the base model (2016) show that the voltage
values at all buses are within acceptable limits (± 10% for the MV, and +5% −10% for
the LV network). It should be noted that the model was based on setting the trans-
former, i.e. regulating the settings of the transformer’s switches in a voltage-free state
within the range of ± 2  2.5%. Also, the analysis was made under the assumption
that the 110/x kV substation maintains its nominal voltage value. In other scenarios not
taking into account development plans by JP Elektroprivreda BiH d.d. Sarajevo (2020
Fig. 2. Forecasted peak loads for 2016 through 2025
Fig. 3. Voltage values on buses for 2016
128
J. Čučuković et al.

and 2025), signiﬁcant deteriorations in voltage in the network were identiﬁed. In the
model for 2020, limitations on voltage values were compromised on 5 different buses
(Fig. 4). For the 2025 peak load projection, the total number of buses with compro-
mised voltage limitations was 66 (Fig. 5).
4.2
Analysis with Respect to Development Plans
To avoid the above-mentioned (expected) violations of voltage values on the buses, it is
necessary to build a primary 110/x kV substation. At the end of 2016, Elektroprenos a.
d. Banja Luka started the process of procurement of the construction of 110/x kV Jelah
with a connecting transmission line. Furthermore, within the considered time period, a
large number of (low-power) solar power plants is expected to join the power distri-
bution network, which can have a signiﬁcant effect on peak values at the 110/x kV
during the day. However, because the peak active power is measured in the evening,
and because exactly then there is a minimum output from the mentioned power plants,
the positive effects of these production facilities today are rather difﬁcult to assess. The
voltage values on the buses for scenarios with respect to development plans by JP
Elektroprivreda BiH d.d. Sarajevo are shown in Figs. 6 and 7.
Fig. 4. Voltage values on buses for 2020
Fig. 5. Voltage values on buses for 2025
The Analysis and Beneﬁts of the New TS 110/x KV
129

Judging from the results shown in Figs. 6 and 7, it can be concluded that by
constructing a new 110/x kV would iron out the problems with regard to maintaining
voltage values within deﬁned limits in the observed network. However, it should be
noted that the voltage value on a number of buses (17, to be exact) in this model for
2025 is very close to the limit of 0.9 per unit suggesting that further development of the
electricity distribution network in the near future will require special attention.
Long-term development plans by the relevant power distribution company, after the
construction of a new 110/x kV substation Jelah (today’s 35/10 kV Jelah) in the
observed area, specify a transition to a 20 kV voltage network, which will doubtless
eliminate the problem of low voltage values on already mentioned 17 buses in model
for 2025. The voltage values on the buses for this scenario are shown in the following
(Fig. 8).
4.3
The Analysis of Power Losses
On the other hand, even though in operating conditions losses do not present a priority,
they certainly do draw the system operator’s attention—particularly in terms of eco-
nomics. As one might expect, increases in load by some scenarios can lead to growing
Fig. 6. Voltage values on buses for 2020 with interpolated TS 110/x kV Jelah
Fig. 7. Voltage values on buses for 2025 with interpolated TS 110/x kV Jelah
130
J. Čučuković et al.

active power losses, and the values of these for all the above-analyzed cases are shown
in Fig. 9.
It can be concluded that an increase in consumption causes a signiﬁcant increase in
the amount of power losses in the MV network. For the forecasted load in 2020,
without interpolating the new TS 110/x kV substation Jelah, there was a surge in the
amount of power losses in the MV network compared to 2016. For the expected
increase in consumption in 2025, there was an increase in the amount of power losses
of up to almost 3 times that of 2016. However, as one might expected, the construction
of a new 110/x kV and transition to 20 kV voltage level would introduce a signiﬁcant
reduction of power losses in the network, which is clearly shown in Fig. 9.
5
Conclusion
As part of the real power system of Bosnia and Herzegovina, the analyzed test example
was used to showcase how the planning of distribution networks must be done with
special care, as well as demonstrate how the integration of new substations of 110/x kV
Fig. 8. Voltage values on buses for 2025 with interpolated TS 110/x kV Jelah and the transition
to 20 kV voltage level
Fig. 9. Active power loss values in the MV network for all the analyzed scenarios
The Analysis and Beneﬁts of the New TS 110/x KV
131

leads to improved voltage conditions and the reduction of power losses in the network.
Also, preparations for the implementation of long-term development plans deﬁned by
relevant companies should be intensiﬁed, and the practical realization done timely.
Increasing the capacity of the network is planned in order to match growing load
and consumption. When investing in expanding capacity, it is necessary to respect the
criteria for planning the development of the network, as well as the technical, eco-
nomic, and regulatory requirements.
When planning the development of the network, it is necessary to maintain and
secure the consumers’ supply during regular operating status within the planned
timeframe, where the security of supply pertains to the permissible load of network
elements and voltage deviation.
One of the principal tasks of planning distribution networks is ﬁnding optimal
interim solutions, which enable a gradual transition to the new model, utilizing the
existing network in the process. It is a long and uneven process, which for the rural
surface network starts by switching the 10 kV equipment to 20 kV equipment, and in
the urban and densely populated areas by introducing a direct 110/x kV transformation.
References
1. Smaka, S.: Industrijski i distributivni sistemi: Predavanje I, Elektrotehnički fakultet Sarajevo
(2014–2015)
2. HEP—Operator distribucijskog sustava d.o.o., Desetogodišnji (2016–2025) plan razvoja
distribucijske mreže HEP ODS-a, Zagreb (2016)
3. NOS BiH, Indikativni plan razvoja proizvodnje 2016–2025, Sarajevo (2015)
4. Elektroprenos Bosne i Hercegovine a.d. Banja Luka, Plan investicija za 2014. godinu, Banja
Luka (2014)
5. ESSBIH, Studija energetskog sektora u BiH, Modul 5—distribucija električne energije
(2008)
6. Elektroprenos Bosne i Hercegovine a.d. Banja Luka, Godišnji plan nabavki za 2015. godinu,
Banja Luka (2015)
7. JP EP BiH d.d. Sarajevo, Analiza razvoja srednjenaponskih distributivnih mreža u JP
Elektroprivreda BiH d.d Sarajevo sa aspekta prelaska na 20 kV naponski nivo, Sarajevo
(2013)
8. Elektroprenos Bosne i Hercegovine a.d. Banja Luka, Dugoročni plan razvoja prenosne
mreže 2014–2023, Banja Luka (2014)
9. Urbanistički zavod Republike Srpske, a.d. Banja Luka, Prostorni plan Općine Tešanj (2007–
2020), Banja Luka (2009)
10. JP EP BiH d.d. Sarajevo, DEEO application. https://www.deeo/rest/data/tp_sn/986715.html
11. Fractal d.o.o. Split, Obuka za korištenje programskog paketa PowerCAD, Zenica (2009)
12. JP EP BiH d.d. Sarajevo, HISTORIAN application. https://doi.org/10.32.100.103/vtrin
13. Memić, A.: Analiza potrošnje, broja kupaca i gubitaka električne energije u elektrodistribu-
tivnoj mreži podružnice „Elektrodistribucija“ Zenica u periodu 2014–2016.godina, Travnik
(2017)
132
J. Čučuković et al.

Determination of Static Voltage Load
Characteristics in BiH Electricity System
Husnija Ferizović(&), Vojislav Pantić, Senad Hadžić,
and Semir Hadžimuratović
Independent System Operator, Sarajevo, Bosnia and Herzegovina
h.ferizovic@nosbih.ba
Abstract. From 2015 to 2016, the Independent System Operator in Bosnia and
Herzegovina (ISO BiH) performed periodic measurements in 26 substations
110/x kV, separately in winter and summer regimes. Power transformer tap
changer positions were experimentally shifted in order to determine static load
characteristics of complex consumer areas, ie. to determine self-regulating
consumption coefﬁcients that demonstrate the load change (in percentage) when
the voltage changes by 1%. Experiments with changing different transmission
ratios were realized in real exploitation conditions in BiH power system,
whereby in the process of reducing the voltage, special care was taken about the
security of supply of the most remote consumers. Reducing the voltage can be
implemented as a system service, when there is a shortage of active power
throughout the power system, or locally, as a result of problems in one part of
the system, in order to provide the required level of safety and to avoid voltage
collapse of the power system. The complete study and its accompanying mea-
surements and results can be found in [1].
1
Introduction
Load characteristics have a signiﬁcant effect on the static and dynamic performance of
power
systems.
Therefore,
an
accurate
analysis
requires
correct
models
of
consumption/load, with adequate production, transmission and distribution models.
However, load modeling is not an easy task, because there are a number of factors to be
taken into consideration, such as the diversity in the types of load characteristics, the
lack of information on the load structure and difﬁculties in assessing and conﬁrming
load models. Furthermore, the spatial and temporal variability in loads should also be
taken into account to assess the behavior of the system at different times of the year and
at different times of day, as well as in different geographical regions. In addition,
aggregated load models on medium and high voltage nodes used in the analysis of
power system include distribution transformers, devices for reactive power compen-
sation and distribution feeders, often without adequate assessment of the dynamics of
tap-changer regulators and other voltage regulator equipment which can be present at
lower voltage levels.
The Independent system Operator in Bosnia and Herzegovina (ISO BiH) is
authorized and responsible for the safe and stable operation of the power system of
BiH. ISO BiH will request load reductions in cases when the stability of the system is
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_11

jeopardized by a lack of active power, due to insufﬁcient production, failures or major
system disorders. One method of load management is the load reduction by decreasing
the voltage, when ISO BiH together with Elektroprenos BiH and relevant distribution
system operators can request voltage reduction of end users (up to 5%), and therefore
reduce the load.
2
Load Modeling Methodology
Load models are traditionally classiﬁed into two broad categories: static models and
dynamic models, and their application depends on the purpose of load modeling and on
the consumption structure in the network node where the modeling was performed.
3
Static Load Models
Static load models are used for calculations in stationary regimes, but also when a small
number of electric drives is present in the network. They are mainly used for the
presentation of thermic devices, lighting, general categories of residential consumption
and other similar aggregated loads that does not include the participation of large
induction motors and electric drives in the overall consumption structure. For this type
of load, it is estimated to have instant response when a voltage/frequency change
occurs within the node it is connected to. It can be said that the new steady state is
achieved instantly, ie. without any time delay. It is the most commonly used method for
power ﬂows calculations and voltage stability analyses.
The functions that describes the dependence of active and reactive power of the
voltage and frequency P(U, f) and Q(U, f) are called static load characteristics. The
dependence of the frequency is neglected, since the supply voltage is usually subject to
much more changes than of frequency, therefore, the functions P(U) and Q(U) are
called static voltage load characteristics (at constant frequency) and the functions P(f)
and Q(f) are called static frequency load characteristics (at constant voltage).
Static voltage characteristics (Fig. 1.) are used when analyzing various stationary
regimes in the system, as well as when testing static stability, determining the reserve
of active power in the system and for the compensation of reactive power, as well as
other reasons. In static regimes, voltage changes are relatively small (moderate). In
practice, acceptable voltage changes are: ±5%, −10%.
For moderate voltage changes, static voltage characteristics can very well be
simulated by a simple and very convenient mathematical function [2–4].
PP ¼ CPUkP and QP ¼ CQUkQ
ð1Þ
where:
PP—active power that is consumed by a complex consumer area, via busbars to
which it’s connected,
QP—reactive power that is consumed by a complex consumer area, via busbars to
which it’s connected,
134
H. Ferizović et al.

CPiCQ—ratio coefﬁcients, that do not depend on the voltage,
kPikQ—ratio exponents, that do not depend on the voltage, but on the consumption
structure,
U—busbars voltage.
With the rated voltage Un, the powers are:
PPn ¼ CPUkp
n and QPn ¼ CQUkp
n
ð2Þ
where is:
Cp ¼ PPn
Ukp
n
and CQ ¼ QPn
UkQ
n
ð3Þ
Therefore, the static characteristics can be presented in the following form:
PP ¼ PPn
U
Un

kP
QP ¼ QPn
U
Un

kQ
ð4Þ
Coefﬁcients kP and kQ are called self-regulating load coefﬁcients and are essential
in solving the voltage problem within the power system. They provide information on
the magnitude of the active and reactive power, respectively, to the voltage variation.
These are the sensitivity coefﬁcients of the active and reactive power by voltage. The
values of the self-regulating load coefﬁcients depends on the type of consumers and
common load types of consumers, as well as their participation in the total load. By
differentiating dPP=dU and dQP=dU, the result is:
kP ﬃ
DPP
PP
DU
U
kQ ﬃ
DQP
QP
DU
U
ð5Þ
and
Fig. 1. Static voltage load characteristics
Determination of Static Voltage Load Characteristics
135

kP ¼ DPP%
DU% kQ ¼ DQP%
DU%
ð6Þ
Coefﬁcient kP gives the change (percentage) of the active power, and the coefﬁcient
change kQ gives the reactive power change when changing the voltage by 1%.
Coefﬁcients kP and kQ depend on the types of consumers. If there is a aggregated load
formed by different consumers (which is almost always the case when considering
complex consumer areas of power system and distribution areas), then it depends on
the participation of individual consumers in the total load.
The coefﬁcients of self-regulating static voltage characteristics of a complex con-
sumer area, can be expressed as follows [3]:
kP ¼
X
n
i¼1
kPi
Pi
P kQ ¼
X
n
i¼1
kQi
Qi
Q
ð7Þ
In steady state analysis of the power system, the dependence of the load by the
voltage is deﬁned for each speciﬁc node, usually using an approximate model of
consumption, [5–7]:
• model constant power (P, Q),
• model of the constant current (I)
• model of constant impedance (Z).
To obtain a more general model, which expresses the dependence of the load power by
the voltage, the use of polynomial models combine the best features of each considered
models. Polynomial model is often used, most common form is the 2nd order poly-
nomial [2], since it is considered that approximation by polynomials of a higher order
doesn’t contribute to more accurate load modeling. This is because, when determining
the static voltage characteristics, in practice or by calculations, a certain error is made.
There are several variants of this model. A variant in which the dependence of the
frequency is not taken into account is:
P ¼ Pn p1
U
Un

2
þ p2
U
Un


þ p3
"
#
ð8Þ
Q ¼ Qn q1
U
Un

2
þ q2
U
Un


þ q3
"
#
ð9Þ
Polynomial model is also called a ZIP model, since it is composed of components
of the load type constant impedance (Z), constant current (I) and constant power (P). Pn
i Qn represent the active and reactive power at rated voltage Un, while the parameters p1
to p3 and q1 to q3 are deﬁning the relative share of each component in the total load.
Traditionally, the dependence of the load power by the voltage is usually repre-
sented as an exponential model:
136
H. Ferizović et al.

P ¼ Pn
U
Un

kP
Q ¼ Qn
U
Un

kQ
ð10Þ
where Pn i Qn are the active and reactive power (respectively) at the rated voltage Un .
kP and kQ are coefﬁcients of self-regulating static voltage characteristics.
The linear load model
P ¼ Pn a0 þ a1
U
Un


;
X
3
i¼1
pi ¼ 1 Q ¼ Qn b0 þ b1
U
Un


;
X
3
i¼1
qi ¼ 1
ð11Þ
has two parameters a0 and a1 for active and b0 i b1 for reactive power. It can be used in
the analyses of small voltage changes in the vicinity of the nominal value, as in the case
of the analyses of the stability of small disturbances, but is not recommended in case of
large voltage changes because it can produce inaccuracies in the calculations.
3.1
Dynamic Load Models
Dynamic models are used for load modeling when the voltage and frequency of the
consumer node change very quickly in time and they are also used for load modeling
which in large part consists of asynchronous and synchronous motors, electric drives,
air conditioners, as well as in the studies of the transient processes with lamps,
transformers, voltage controlled capacitors, protective relays, thermostat controlled
consumer devices, such as refrigerators and boilers.
Dynamic load models are usually described in the form of differential equations
that link the active and reactive power with voltage and frequency at the terminal bus,
taking into account the time dependence variables, and can be expressed as follows:
P ¼ Pn U; dU
dt ; f ; df
dt ; t


Q ¼ Qn U; dU
dt ; f ; df
dt ; t


ð12Þ
These models are used in the analyses of intersystem oscillations and all types of
stability analysis that includes a transient response, taking into account the load of the
system, usually followed by various types of disorders.
4
Determination of Load Model Parameters
In analyses related to the exploitation, management and development of the power
system, it is often pointed out that load modeling is one of the main reasons for
imprecise simulations. Inadequate load models can lead to inaccurate and inconsistent
results which can reduce the conﬁdence in simulations and their results. To model the
load adequately, it’s necessary to select proper load models and determine their
parameters correctly.
To solve the above mentioned problems and determine the parameters of the load
model there are generally two approaches [2, 5, 6, 8]:
Determination of Static Voltage Load Characteristics
137

• approach based on knowledge of the load components parameters (bottom-up),
• approach based on measurement (top-down).
4.1
Approach Based on Knowledge of the Load Components Parameters
In this approach, the methodology for load modeling is performed bottom-up, ie. from
the lower to higher voltage levels in which the aggregated load model is derived from:
1. The knowledge of the consumption category connected to the substation.
2. The structure and the composition of the category of consumption and / or com-
ponents of each load.
3. Typical characteristics of each category and/or the load components.
This aggregated model includes components of major individual consumers and usu-
ally is represented as a 2nd order polynomial or as a complex load. Categorizing the
load is far more challenging because of the distributed load nature and potentially
disputable input data.
4.2
Approach Based on Measurement
During the past decades, measuring instruments for the registration of events and
disturbances are widely installed in many power system and the quality of measure-
ments and data collection has signiﬁcantly improved. In this approach the modeling
methodology consumption is top-to-bottom, ie. from the higher to lower voltage levels.
That way, the device for registration is used to determine the characteristics of the
connected consumer load. Sometimes, this approach is referred to as “approach based
on consumption behavior”, as the device registers the static and dynamic response of
the load and then it is then used to obtain the required load model.
Conceptually, the approach based on the measurement is shown in Fig. 2. The
associated equipment for registration will register the response of the load power for all
events that occur on the high voltage side. Whenever there is a disturbance in the power
system, the response of the load is registered, but also the load power can also be
monitored and registered during normal operation, without any disturbances.
Fig. 2. Example of a digital device for registration setup
138
H. Ferizović et al.

Load model parameters are estimated, “ﬁtted” based on measured data in order to
assess the structure of the model, using techniques to identify parameters and curve
ﬁtting. To obtain the parameters for more complex load models, complex estimation
techniques can be used. This depends on the type and number of disturbances recorded
during the logging period. The identiﬁcation process involves identifying an appro-
priate mathematical model, ie. the performance function (objective function), and the
corresponding parameters of load model, so that it can accurately display the transient
load during and after the disturbance.
5
Measurement Results and Estimation of Load Model
Parameters
The level and composition of load depends on a number of factors such as economic,
social, climatic and the load power on the distribution network varies during the day,
days of the week, seasons. Taking into account the spatial and temporal changes in
workload and behavior of the power system at different times of the year the static
voltage characteristics and consumption in winter and summer regimes were
determined.
The goal of this research was to determine the dependence active and reactive
power change by voltage variation (ie. self-regulating consumption coefﬁcients that
show the load change when the voltage is changed by 1%). This was performed by
strategically planning the measurements in typical 110/x kV substations in BiH, sep-
arately for winter and summer regimes.
The selection of typical 110/x kV substations in the power system of Bosnia and
Herzegovina, was made taking by into consideration the following criteria:
• assessment of the share of each category in the total energy consumption of the
110/x kV substation,
• total load power of the 110/x kV substation,
• spatial or geographic location of the 110/x kV substation (within BiH),
• climatic conditions in different areas (within BiH).
Periodic measurements were performed at several locations in BiH power system, in
order to determine the static voltage load characteristics, both locally and for the whole
system, ie. to provide a reliable estimation of the whole BiH power system. These
measurements were organized in full cooperation with Elektroprenos BiH and with the
consent of distribution companies in the recorded area. Voltage and current measure-
ment were conducted from the substation’s measurement cabinets. All 110/x kV power
transformers in EES BiH, can be regulated under load, with a variable transmission
ratio of ±15%, which can be changed manually or automatically. Tap changers located
at the primary side of the transformer operate within the range of ±10  1.5%, with a
total of 21 positions. Voltage ﬂuctuations on the low voltage side of the transformer
110/x kV (secondary, tertiary) were within the permissible variation voltage (±10%) in
medium-voltage networks, in accordance with standard EN 50160 and the General
Conditions for Electricity Supply dictated by relevant regulatory commissions in BiH.
Determination of Static Voltage Load Characteristics
139

Measurements in all 110/x kV substations were performed using the MavoWatt 70
(PX5) measuring device. This device is used for measuring the quality of electricity,
measuring different set of values, both in the normal operation state, as well as for
system disorders that took place on the high voltage side of the transformer. The values
of active and reactive power unit are calculated every second. For larger intervals, such
as one minute, the following values are stored:
• the average value of one-second intertvals in one minute;
• the maximum and minimum one-second value during a one-minute interval.
The measurements were realized in real exploitation conditions of BiH power system
(in “real-time”), during which special account was taken considering the security of
supply of the most remote customers the electricity. Therefore, the voltage was not
reduced below 0.95 Un. Since the load in the distribution network varies during the
day, a few days before the planned measurements for each 110/x kV substation,
ﬁfteen-minute load data were taken from the database. That way, it was possible to
determine the most suitable time interval for measurement, when the natural changes in
consumption are the lowest, and that way natural changes were excluded from the
estimation process. An example of determining the time interval for measuring in the
110/x kV substation Sarajevo 7 is shown in Fig. 3, showing the daily load curve, for
18.01.2016. Based on this diagram, the measurement was scheduled for 25.01.2016
from 11:00 to 12:00 h.
Measurements in the 110/10 kV substation Sarajevo 7, were made on 25.01.2016.
(workday), on the lower voltage (10 kV) side of transformer T2, in the period 11:30 to
12:00 h. During the measurement, both transformers were in operation, whereby the
transformer T2 was selected because of a slightly higher load, which was about
12.2 MW. At the time of the measurement, the temperature was 4 °C, and the weather
was dry and without precipitation. This transformer powers part of the city, the factory
for the production of food “Klas” with mills, as well as an indoor swimming pool, so
that the consumption was assessed in the following way:
0
5
10
15
20
25
30
00:15:00
01:30:00
02:45:00
04:00:00
05:15:00
06:30:00
07:45:00
09:00:00
10:15:00
11:30:00
12:45:00
14:00:00
15:15:00
16:30:00
17:45:00
19:00:00
20:15:00
21:30:00
22:45:00
00:00:00
MW
Mvar
Fig. 3. Daily load diagram of 110/x kV substation Sarajevo 7, for 18.01.2016.
140
H. Ferizović et al.

• 20% industrial and
• 80% residential and commercial administrative.
Changing the position of the transformer tap changer was made from the initial value of
the secondary voltage U0 = 10.3 kV, corresponding to the tap-position 5. The voltage
was increased to the maximum voltage U = 10.9 kV (tap-position 10), then the voltage
was reduced to the value U = 9.8 kV (tap-position 2). The measured voltage change
and the power (active and reactive) are shown on the diagrams in Fig. 4. Table 1 gives
the measured maximum and minimum values of U, P and Q, as well as their differ-
ences. The ﬁtted curves are depicted in Figs 5 and 6.
The ﬁtted 2nd order polynomial for P and Q are the following:
P(U) = 0.7762 • U2 – 14.7925 • U + 81.997
Q(U) = 1.0997 • U2 – 21.2062 • U + 104.0355
Fitted charachteristics P(U) and Q(U) that describe the behavior of the load during
the voltage drop have high correlation coefﬁcients RP = 0.99 and RQ = 0.997.
Self-regulating consumption coefﬁcients kpu = 1.1 and kqu = 4.86 are calculated in
accordance with Eqs. (5) and (6). Table 2 gives the measured maximum and minimum
values of U, P and Q and their differences.
Fig. 4. Measured values U, P i Q
Determination of Static Voltage Load Characteristics
141

Table 1. Measured maximum and minimum values of U, P and Q and their differences (D)
Value
U [kV] P [MW] Q [MVAr]
Maximum
10.928
13.2
3.725
Minimum
9.82
11.484
1.779
D (Max-Min)
1.108
1.716
1.946
11.2
11.4
11.6
11.8
12
12.2
12.4
12.6
12.8
13
13.2
13.4
9.5
10
10.5
11
P (MW)
Voltage U (kV)
Fig. 5. P(U) diagram
1
1.5
2
2.5
3
3.5
4
9.5
10
10.5
11
Q (MVar)
Voltage U (kV)
Fig. 6. Q(U) diagram
Table 2. Fitted maximum and minimum values of U, P and Q and their differences (D)
Value
U [kV] P [MW] Q [MVAr]
Maximum
10.928
13.039
3.6238
Minimum
9.82
11.585
1.83929
D (Max-Min)
1.108
1.4538 1.78452
142
H. Ferizović et al.

6
Analysis of Measurement Results
In the winter regime, from December 2015 to February 2016, a total of 31 measure-
ments were performed in the following 26 substations (110/x kV):
Sarajevo 11 (10 kV), Sarajevo 5 (10 kV), Sarajevo 7 (10 kV), Goražde 1 (35,
10 kV), Zenica 4 (20, 35 kV), Sarajevo 20 (10 kV), Pale (10 kV), Gračanica (10 kV),
Tuzla Centar (35, 10 kV), Đurđevik (35 kV), Brčko 2 (35 kV), Bijeljina 1 (T3 10 kV,
T2 and T3), Tešanj (T1 35 kV, T2, 10 kV), Doboj 1 (T2, 10 kV), Laktaši (20 kV),
Banja Luka 2 (10 kV), Prijedor 1 (20 kV), Cazin 1 (10 kV), Bihać 2 (10 kV), Mostar 2
(10 kV), Mostar 6 (10 kV), Široki Brijeg (10 kV), Ljubuški (10 kV), Čapljina (10 kV),
Trebinje 1 (10 kV) and TS Bileća (10 kV).
In the summer regime, these periodic measurements were performed in the period
from 13th to 28th of June 2016 in the same 110/x kV substations.
The measurement results for each of these 110/x kV substations, for both winter
and summer regime are presented in [1]. It provides an overview of kPU, kQU, RP, RQ, as
well as voltage changes, active and reactive power from the largest to the smallest
change, the ﬁtted values. Based on analogies for complex consumer areas, using
equation (7), self-regulating consumption coefﬁcients kPU were calculated for all
110/x kV substations in which the measurements were made.
In the winter regime, the load of all the measured substation is calculated to 305.43
MW, and the self-regulating active load coefﬁcient kPU is 1.24741%. By reducing the
voltage 1% on the low voltage side of the 110/x kV transformer (secondary and
tertiary), the load is reduced by 3:8099 MW ¼ 305:43=100
ð
Þ  1:24741: By analogy, if
whole power system was taken into consideration with a load of 2000 MW, the
estimated
reduction
of
load
by
reducing
the
voltage
1%
would
equal
ð2000=100Þ  1:24741 ¼ 24:9483 MW. A 5% voltage decrease would result in a
reduction of 124.741 MW, or 6.237%.
In the summer regime, the load of all the measured substation is calculated to
223.49 MW, and the self-regulating active load coefﬁcient kPU is 1.02848 %. By
reducing the voltage 1% on the low voltage side of the 110/x kV transformer (sec-
ondary and tertiary), the load is reduced by 2:2985 MW ¼ 223:45=100
ð
Þ  1:02848: By
analogy, if whole power system was taken into consideration with a load of 1700 MW,
the
estimated
reduction
of
load
by
reducing
the
voltage
1%
would
equal
ð1700=100Þ  1:02848 ¼ 17:4842 MW. A 5% voltage decrease would result in a
reduction of 87.4709 MW, or 5.1424 %.
7
Conclusions
The goal of this research was to determine the dependence of change active and
reactive power with voltage variation (ie. self-regulating consupmtion coefﬁcients that
demonstrate the load change when the voltage changes by 1%). This was performed
with planned measurements in typical of 110/x kV substations in BiH, separately in the
winter and summer regimes.
Experiments with tap-changers transmission ratio were performed in real
exploitation conditions in BiH power system, whereby in the process of reducing the
Determination of Static Voltage Load Characteristics
143

voltage, special care was taken about the security of supply of the most remote
consumers.
Determining the parameters of the load model was based on measurement, wherein
the change of active and reactive power in relation to voltage variation. This is modeled
using a 2nd order polynomial, forming curves P(U) and Q(U), based on which the
parameters of the load model. High linear correlation coefﬁcients RP and RQ show that
the values obtained by measurement and the value obtained by modeling, ﬁtted P(U)
and Q(U) curves, are strongly correlated, conﬁrming the correctness of the selected ZIP
model.
In the winter regime, if the whole BiH power system is observed with a load of
2000 MW, load reduction estimated by reducing the voltage 1%, would amount to
24.95 MW. By reducing the voltage 5%, the estimated load reduction would amount to
124.74 MW, or 24.6%.
In the summer regime, if the whole BiH power system is observed with a load of
1700 MW (summer maximum), load reduction estimated by reducing the voltage 1%,
would amount to 17.48 MW. By reducing the voltage 5%, the estimated load reduction
would amount to 87.42 MW, or 5.14%.
References
1. Studija Određivanje statičke naponske karakteristike potrošnje u EES BiH. Nezavisno
operator sistema u BiH, Juni 2016
2. Korunović, L.M.: Parametri modela potrošnje distributivne mreže. Biblioteka DISSERTATIO
(2010)
3. Radović, J.: Statičke naponske karakteristike potrošnje. ETF Podgorica
4. Košarac, M., Korunović, L., Stojanović, D.: Parametri ekvivalentne potrošnje na 10 kV nivou
distributivnih mreža. Konferencija za ETRAN, Čačak (2004)
5. Kundur, P.: Power System Stability and Control. Mc Graw-Hill, New York (1994)
6. Machovski, J., Bialek, J.W., Bumby, J.R.: Power System Dynamics: Stability and Control,
2nd Edn. Wiley (2008)
7. Grigsby, LL.: The Electric Power Engineering Handbook. CRC Press LLC (2001)
8. Modellenig and Aggregation of Loads in Flexibile Power Networks, WG C4.605, CIGRE,
Feb 2014
144
H. Ferizović et al.

Application of Modern Solutions on Grounded
Neutral Point in Distribution Grid
Alija Jusić1(&) and Zijad Bajramović2
1 Public Enterprise Elektroprivreda of Bosnia and Herzegovina, Travnik, Bosnia
and Herzegovina
al.jusic@elektroprivreda.ba; al.jusic@epbih.ba
2 Public Enterprise Elektroprivreda of Bosnia and Herzegovina, Sarajevo,
Bosnia and Herzegovina
z.bajramovic@elektroprivreda.ba
Abstract. In practice are used several neutral point grounding modes for
medium voltage grids. Each mode has certain advantages, but also disadvan-
tages. Therefore, for the ﬁnal decision on the grounding method a special
analysis is performed for each transformer substation, given the speciﬁcity of the
distribution grid connected to it. Neutral point treatment also affects the relia-
bility of electricity supply to consumers, which in conditions of an open elec-
tricity market is gaining more and more importance. Along to the technical of
great importance is the economic aspect as some solutions differ signiﬁcantly.
This paper gives a speciﬁc review of basic technical characteristics for solutions
with compensated (resonant) grounding of the neutral point, i.e. application of
Arc Suppression Coil (ASC)—Peterson coil technology in distribution grids,
which implies neutral point grounding of the power transformer through an
inductive reactance. Also in the paper are given basic principles of applied
protections as well as modern approaches for failure detection in distribution
grids with compensated neutral points.
1
Introduction
Neutral point grounding of power transformers is of great signiﬁcance for the network
operation. Due to different types of grounding, during a single-phase fault in the grid
can occur different values and forms of overvoltages and fault currents. Statistics show
that most often these failures occur in mixed distribution grids.
The main task of the electric power companies which carry out activities of pro-
duction, distribution and supply of electricity, trade, representation and mediation in
domestic and foreign markets of electricity, as well as other activities deﬁned by the
relevant documents, is to ensure the reliability of distribution systems and to meet the
quality of supplied electricity in accordance with the regulations. With the opening of
the electricity market, Distribution System Operator (hereinafter referred to as DSO) is
being imposed to a requirement for “higher quality” electricity supply, which implies
higher delivery reliability, and shorter de-energization intervals. In recent years there
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_12

has been a rapid development of residential and commercial zones, tourist centers, etc.,
with a further trend of increase, and the general social development should be
accompanied by the appropriate electricity infrastructure, which has resulted in the
construction of new power lines (mostly cable), which signiﬁcantly increases the
overall value of capacitive current.
Increase of single-phase fault current makes the conditions for execution of 10
(20)/0.4 kV grounding and bringing the resistance propagation within the permissible
limiting values deﬁned in the relevant laws and regulations more difﬁcult, which is
directly related to hazardous voltage. All stated above indicates the need for a special
focus on the analysis of neutral point treatment, that is, neutral point grounding in the
TS 110/x kV transformer stations (and TS 35/x kV until their ﬁnal abandonment).
Capacitive ground fault current in 10 kV isolated grid should not exceed 20 A, and in
any case must not exceed 40 A with the shortening of shutdown period. Neutral point
grounding should be made in case of capacitive currents higher than 40 A. Capacitive
ground fault current in 20 kV isolated grid should not exceed 15 A, and in any case
must not exceed 30 A with the shortening of shutdown period.
Neutral point grounding should be made in case of capacitive currents higher than
30 A [1].
The 35 kV voltage distribution grids of JP Elektroprivreda BiH d.d. Sarajevo are
typically grounded by a low ohmic impedance with the limitation of ground fault
current at 300 A. Exceptionally, 35 kV branched cable grid is grounded so that the
ground fault current is limited to a higher value (not higher 1000 A). In 20 kV and
10 kV grids the conditions are much more complicated, since the savings in lowering
of isolation levels are not high, and there is also a danger of bringing grounding
potential to the low voltage grid. The following is applied in these grids:
• Isolation of the neutral point from the ground,
• Grounding through a low ohmic impedance or
• Resonant neutral point grounding.
The right answer to the method of neutral point grounding can be given by overall
assessment of all the disadvantages and advantages of each solution. Having in mind a
number of speciﬁc features that affect the choice of grounding method, it is difﬁcult to
recommend a solution which would satisfy all the requirements in terms of exploitation
of the grids themselves for a longer period of time. Essentially, the quest to identify the
most appropriate method of the neutral point treatment is a ﬁne compromise between
several opposing factors such as transient overvoltages, steady state voltage, fault
current level and permitted touch voltage and step voltage.
Some of the features typical for different methods of neutral point treatment are
illustrated in the Table 1.
In this paper, we will present the basic characteristics of the automatic compen-
sation coil, role and importance in distribution grids as one of the methods for neutral
point grounding [2] and [3].
146
A. Jusić and Z. Bajramović

2
Arc Suppression Coil (ASC)—Peterson Coil Technology
Waldemar Petersen discovered in the 1920s, that inserting an inductor in the earth fault
path could compensate zero sequence capacitive current.
A grid with compensated (resonant) grounding is a network in which the neutral
points of power transformers are connected to the ground by the inductive reactance
(the practice known as Petersen coil). When the capacitive single-phase fault current
exceeds by regulations deﬁned grid limiting values with the isolated neutral point, it
loses its major advantage, namely the possibility of ground fault self extinction.
In order to reduce capacitive ground fault current exceeding the limiting values, it is
possible to connect the inductive reactance between the transformer neutral point and
Table 1. Features of various neutral point treatments, [10]
Method of neutral point
grounding/feature
Isolation
Low ohmic
impedance
Resonant
(ASC)
Power quality
Low
Low
Very good
Isc
Depending on the length of
the network
Signiﬁcant
Small/moderate
Transient voltage
High
Moderate
Moderate
Self-extinction
Moderate
–
Very high
Relaying
Very high
Easy
Moderate
Fault detection
Very high
Easy
Moderate
Fig. 1. Illustration of a single-phase ground fault in the case of neutral point grounding by
Petersen coil with the vector diagram for the duration of ground fault, [11]
Application of Modern Solutions on Grounded Neutral Point
147

ground. In this way, the inductive currents compensate for capacitive ground fault
current. The inductive reactance is so adjusted to obtain resonance between it and the a
serial line capacity per grid phase, so it is called a resonant grounding grid. Illustration
of a single-phase ground fault in the case of neutral point grounding by Petersen coil
with the vector diagram for the duration of ground fault is shown in Fig. 1.
The equivalent impedance viewed from the point of failure is deﬁned as [4]:
Z0 ¼
j3xLPR
1
jxC0
j 3xLPR 
1
xC0


ð1Þ
It is desirable that current IPR which is approximately equal to the ground fault
current in the grid Iz ﬂows through reactance coil XPR:
IPR ¼ Iz  Ib  Ic
ð2Þ
The condition of the resonant neutral point grounding formally comes down to:
3x2LPRC0 ¼ 1: Overall, total compensation is avoided in practice in order to prevent
resonant surges that may occur in the interruption of phase conductors, and the coil in
the neutral point is practically executed with the shunts or extensions with the projected
(usually) nominal currents: 5; 10; 15; 20; 25; 30 A.
In case of failure, capacitive currents and currents of active conductivity of sound
phases ﬂow towards neutral point of the transformer. This also applies to all other
sound transmission lines of the grid, that is, from the grid, the sum of current con-
ductivity of all lines comes to the neutral point of the transformer. Through the faulty
phase, in case of total compensation, only the active part of these currents and the
active part of the coil current ﬂow, that make up the ground fault current. Capacitive
part of the ground fault current is compensated by the inductive coil current. The
neutral point of the transformer, in case of ground fault, comes to the phase voltage of
the grid, while the voltage of the sound phases come to phase to phase voltage, as is the
case with the grids with the isolated neutral point, [11].
Here the currents have the same ﬂows as in an isolated grid, except that in the phase
with the failure much lower ground fault current ﬂows to the point of ground fault. This
current is often called the residual ground fault current, and, in case of total com-
pensation, consists only of active component.
In case of total compensation two instances may occur. The ﬁrst instance is when
the capacitive component of the ground fault current is higher than the inductive
component of the coil current and when LP [
1
3x2 C0 and the second instance is when
LP\
1
3x2 C0, that is, when the capacitive component of the ground fault current is lower
than inductive. In the ﬁrst case, we talk about “under compensated” grid, in the second
case of “over compensated” grid. In this case, the reactive component of current also
exists in the residual ground fault current.
Conditions for single-phase ground fault self-extinction, as one of the main
advantages of these grids, primarily depend on the value of the ground fault current. In
this case, it refers to the residual ground fault current. Conditions for ground fault
148
A. Jusić and Z. Bajramović

extinction, after the causes of failure pass, also signiﬁcantly depend on the speed of
establishment of voltage on the phase that was out of order, so-called “reverse voltage”,
which aims to re-establish the ground fault. In incompletely compensated grids, reverse
voltage has a pulsatile—oscillatory ﬂow. With larger inaccuracies in compensation,
reverse voltage may have an overvoltage up to 1.7. However, in this case the amplitude
of the reverse voltage rises slowly and so reducing the possibility of ground fault
re-establishment.
In the resonant grounded grids it is necessary to take into account the order of
operations in manipulations of the grid. Compensation coil, at the moment of manip-
ulation in the grid, can have a considerable magnetic energy, which must be emptied
through small capacity power lines and other elements of the grid, which can cause
signiﬁcant overvoltage. Particularly large overvoltages can occur when two-phase
ground faults are turned off, because the current in defective lines has the phase shift of
900 compared to the current in the coil, so a considerable magnetic energy can remain
in the coil.
Early Arc Suppression Coil (ASC)-Peterson coil were crude, either a single ﬁxed
tap setting, or an off-load adjustable stepped tap design. These setting limitations
demand an almost perfect match of the Arc Suppression Coil (ASC)-Peterson coil to
the conditions speciﬁc for its particular network. Any subsequent change in the net-
work conﬁguration demands human intervention, [5].
For example, addition of new feeder circuits, especially cable or hybrid circuits, or
circuit switching, results in alteration of the capacitive current. Usually, ﬁxed or
stepped coils are incapable of dealing with such service conditions and thus in some
cases are rendered ineffective. Essentially, such Arc Suppression Coil (ASC)-Peterson
coil have added new problems to utility’s operational staff and in some cases led to the
protective relays malfunction, [5].
Recent introduction of automated systems to isolate the faulted section and to
restore the supply to the customers unaffected by the fault creates additional challenges
to the ﬁxed or stepped coil designs. It is unlikely that those designs are appropriate to
cope with, fault driven line and network dynamic reconﬁguration.
On the other hand, today’s ASC in conjunction with modern Arc Suppression Coil
(ASC)-Peterson coil controllers are coping with an easy with line and network dynamic
changes and thus are offering the potential to reach the best performing distribution
systems, [5].
Such designs offer ratings that are continuously adjustable, on-load, and depending
upon the coil size and the time duty of the service operation, setting ratios ranging from
5:1 to 14:1 are available. The adjustment should preferably be automatic, using an
integral motor-drive unit. For the back-up or emergency condition, manual hand-driven
crank is provided. To perform its function Arc Suppression Coil (ASC)-Peterson coil
may be equipped with purpose made windings: main, and Power Auxiliary Winding
(PAW), as illustrated in (Fig. 2b) with each winding having its speciﬁc function, [5]
and [6].
Older Arc Suppression Coil (ASC)-Peterson coil designs include also measuring
winding. The role of so designed winding diminished and nowadays either built in
Voltage Transformer (VT) is used or Neutral Point Voltage measurement is taken from
open delta busbar VT or by a voltage sensors mounted inside of an ASC, [5] and [6].
Application of Modern Solutions on Grounded Neutral Point
149

Main winding provides the inductive compensation current, [5] and [6]:
• Measurement winding provides a connection for instrumentation to measure the
zero sequence voltage (V0)
• Power auxiliary winding is for connection of system auxiliaries, for example
“damping resistors” to reduce the level of V0 for undesirable values of imbalance
voltage.
3
Application of Fault Detection and Protection in Grids
with Compensated Neutral Point
In this case, protection of the source transformer stations consist of protection of the
outgoing feeder and protection of the transformer. In grids with isolated or compen-
sated neutral point, ground fault is not accompanied by appearance of large short circuit
currents, so that the overcurrent or distance protection are unable to determine the
failure, [10].
Active part: 
1…….cover 
2…….core – movable part of the 
magnetic circuit 
3……yokes – fixed part of the 
magnetic circuit 
4……main winding 
5……main screw shaft 
6……frame
D1, D2 main winding / rated voltage / 
M1, N1 measuring winding / 100 V +/- 10 %, 3A/ 
M2, N2 auxiliary power winding /500 V +/- 10 % / 
k, l current transformer 
(a)
(b)
Fig. 2. a Active part of Arc suppression coil, b Power auxiliary winding (PAW), [5] and [6]
150
A. Jusić and Z. Bajramović

When the grid neutral point is grounded through a compensation coil, the capac-
itive ground fault current is superimposed with the inductive current which is deter-
mined by voltage of the neutral point and coil inductance. If the coil inductance is
adjusted to the grid capacity, then the current at the fault location is approximately zero.
Since the coil has a certain active resistance and because of grid admittance, angles
current to voltage are smaller than 90° and as result appears a small residual current,
active character. Ground fault current ﬂow, when the grid is grounded through a coil, is
shown in Fig. 3.
This residual active component of the current is used in grids with compensated
neutral point, for selectively determining the outgoing feeder with ground fault. In
general, performance of protection in isolated and compensated grids is more difﬁcult
because values of fault currents are small (particularly in grids with compensated
neutral point) and for selective protection activity it is necessary to know current
direction.
Fig. 3. Flow of currents—ground fault in compensated grid
Application of Modern Solutions on Grounded Neutral Point
151

Directional earth fault relay, connected to zero sequence components voltage and
current, should in that case be maximally sensitive to the current active component
(relay type cosu). The rest active components of current most often have small values
so that the directional earth fault relay needs to be extremely sensitive. Although there
are very sensitive relays, this current may be insufﬁcient to secure protection response.
In such cases after ground fault appearance, operating current can be temporarily
increased by including a resistor in parallel with the coil. The resistor should be
included with certain time delay so that the additional current cannot interfere with arc
suppression of temporarily earth faults.
Determining direction is possible by using the appearance of a quite high current
impulse, at the moment of arc ignition, i.e. by shorting charged capacity of the dam-
aged phase. Inrush currents are quite large, but last very brieﬂy, only during the
outbreak period and capacity redistribution. By means of special compounds and
extremely fast relays can be included inrush currents which last less than 1 ms. After
registration of such impact the relay remains blocked in that position and shows the
direction of the fault location. Advantage of this approach is that also transient ground
faults are registered, with review weak spots in the grid can be timely eliminated.
Disadvantage is the relay complexity [7, 10].
For calculation of frequency transients charging grid capacities, a replacement
scheme with concentrated parameters can be considered. In the ﬁrst approach the active
resistance of the conductor and transformer in the source TS x/10(20) kV are ignored.
In the case of a grid with compensated neutral point, frequency of charging transients is
roughly the same as in the case of grids with isolated neutral point. Resistance which
the compensation coil provides to the passage of charging transients has large values.
For example, assuming frequency of charging transients is 200 Hz and the inductance
of compensation coil is 5 H, then the resistance of the coil to passage of charging
transients is 1000 Ω. Therefore, the path through which the charging transients are
closing is approximately the same as in case of an isolated grid.
Besides the wattmetric and transient methods, in grids with compensated neutral
point, are also used harmonic (ground fault detection is carried out with detection up to
the 5. harmonic) and pulse method (inclusion of capacity parallel to the coil for
grounding) for detection of the outgoing feeder in failure.
Wattmetric relays (UIcos/ relays) are used for detection of permanent ground
faults. For detection of transient failures are used harmonic, pulse and transient
methods [8].
Features of each of the methods, [9]:
Harmonics
• The trigger threshold for the zero sequence voltage Uen is parameterizable
• High sensitivity through the comparative evaluation of the harmonic current in the
faulty busbar section
• Compensation of the daytime harmonic current ﬂuctuations through the compara-
tive evaluation
• Also suitable as individual relay by evaluating the angle information
• Suitable for isolated grids as sin/ relay for the fundamental frequency (50 Hz).
152
A. Jusić and Z. Bajramović

Pulse detection
• Dynamic adaptation of the trigger threshold for total current 3Io
• The pulse pattern to be detected is freely programmable
• Indicator is reset by an external signal or automatically at the end of a time period
• Clocking can be controlled by the EOR-D
• Depth detection’ up to the faulty section is possible.
Transient
• The trigger threshold for the zero sequence voltage Uen is parameterizable
• The trigger threshold for total current 3Io is parameterizable
• Suppression of transient message based on a selectable minimum duration of the
zero sequence voltage (transition to continuous earth fault)
• Suppression of earth fault messages in direction of the busbar (optional)
• Indicator is reset by an external signal or automatically at the end of a time period
• Recording of the transient process (error log for transients).
Wattmetric without residual current increase
• The trigger threshold for the zero sequence voltage Uen is parameterizable
• The trigger threshold for total current 3Io is parameterizable for each outgoing
circuit
• A ﬁxed angle correction can be set for the current transformer
• Suppression of earth fault messages in direction of the busbar (optional).
Wattmetric with residual current increase
• The trigger threshold for the zero sequence voltage Uen is parameterizable
• The trigger threshold for total current 3Io is parameterizable
• A ﬁxed angle correction can be set for the current transformer
• Suppression of earth fault messages in direction of the busbar (optional).
4
Conclusion
Analyzing trends in the concept of transformer neutral point grounding in MV grids in
the region and some European countries, it is indicated that lately most attention is paid
to neutral point grounding by compensation coils with automatic regulation.
ASC installation improves network performance by signiﬁcant improvement of the
security and availability of supply. The best results are achieved in compensated net-
works designed for prolonged operation with one earth fault anywhere in the system.
There are many data sources showing in clear terms the improved performances in
the quality of the power supply in the systems utilizing ASC. The most noticeable
improvements following the ASC installation are in the reduction of Customer Inter-
ruptions (CI) and Customer Minutes Lost (CML).
Also, nowadays with use of modern techniques for detection of fault location and
ASC, operating performances of the grid in terms of interruption in electricity supply
can be further improved.
Application of Modern Solutions on Grounded Neutral Point
153

References
1. BiH Electrical Utility Company: TP-18_Technical Recommendations for Design and
Construction of Grounding and Grounding Devices in Power Distribution Grids and
Installations, Sarajevo (2011)
2. BAS EN 50522:2011 Earthing of Power Installations Exceeding 1 kV AC
3. BAS HD 637 S1: 2010 Power Installations Exceeding 1 kV AC
4. Nahman, J.: Neutral Point Grounding in Distribution Grids. Naučna knjiga, Beograd (1980)
5. Watson, I.: The Role of Modern Arc Suppression Coils in Today’s Power Systems (2016)
6. EGE, spol. s r.o. České Budějovice, Arc Suppression Coils (2016)
7. Božuta, F.: Automatic Protection Devices of Power Electric System. Svjetlost, Sarajevo
(1989)
8. Watson I.: Neutral Point Earthing in MV Distribution Networks (2015)
9. Eberle, A.: GmbH & Co. KG, Operating Instructions-Earth Fault Detection Relay (2016)
10. BiH Electrical Utility Company: Study Neutral Point Treatment In Distribution Grids.
Sarajevo (2016)
11. Alija, J., Jasmina, A., Zijad, B., Irfan, T.: Automatic compensation coil-Petersen coil in
distribution grid. In: The Bosnian-Herzegovinian American Academy of Arts and Sciences,
8th BHAAAS International Technology Symposium (2016)
154
A. Jusić and Z. Bajramović

Comparison of CFD and Linear Model When
Calculating Maps of Wind Potential
at the Location with Complex Topography
Dino Trešnjo1(&), Alma Ademović Tahirović1, Muris Torlak2,
Elma Redžić1, and Mustafa Musić1
1 JP Elektroprivreda BiH d.d.-Sarajevo, Sarajevo, Bosnia and Herzegovina
d.tresnjo@elektroprivreda.ba
2 Faculty of Mechanical Engineering, University of Sarajevo, Sarajevo,
Bosnia and Herzegovina
Abstract. For the assessment of wind ﬂow model on locations in Bosnia and
Herzegovina so far, linear model WASP is mainly used, while the suitability and
reliability of the CFD models have rarely been studied. Due to the speciﬁc
conditions which include the dominant wind from one or two directions and
relatively complex topography of the potential sites of wind farms, this work
aims to develop a method of assessing wind potential in locations with complex
topography and the prevailing wind from one or two directions. The method is
applied using empirical data and the mathematical/computer models are
employed to determine and describe the impact of such a topography on the
wind ﬂow, including assessment of expected production of electricity from wind
power. Such research would be an original scientiﬁc contribution to the
assessment of wind potential and wind energy alone generally.
1
Introduction
The ﬁrst and most important criteria of each wind power project is selecting a location
with adequate wind potential. Wind speed and direction are fundamental values for the
design layout of wind turbines within the wind farm. Wind potential is typically
determined by measuring the wind characteristics at the site by using the mast on which
are installed measuring devices. For the accurate determination of wind, it’s recom-
mended installing measuring instruments (anemometers and wind vane) at least two
different heights in accordance with IEC standard [1] and MEASNET recommenda-
tions [2]. Besides anemometers and windvanes, temperature, pressure and humidity
sensors are also installed, in order to determine the air density. After a sufﬁciently long
measurement period, data on wind characteristics are processed and extrapolated on the
microlocation by using specialized softwares. The results of such an extrapolation
depend on location—relief (orography), land cover, and possible obstacles for airﬂow.
Currently the most widely used and best known specialized software to assess the wind
potential is the Wind Atlas Analysis and Application Program (WASP) developed in
the Danish RISO Institute. WASP is a linear computer model, and the principle of its
operation is described in [3]. The results obtained in WASP are deemed quite reliable
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_13

on uncomplexed sites with dominantly ﬂat and slightly wavy relief (typically, with the
slopes up to 17°). Lately, in order to model a wind ﬂown sites with complex orography,
asses the impact of the airﬂow obstacles and topography on wind data extrapolation,
models based on computational ﬂuid dynamics (Computational Fluid Dynamics—
CFD) are increasingly used [4]. This paper will give an overview of the fundamental
aims of the aforementioned softwares, and a discussion of the reliability of the wind
potential analysis using both models on the basis of available data.
2
The Project Area—Borisavac
Project area of the planned wind farm—Borisavac, municipality of Konjic, is located
on the mountain ridge, with an altitude between 900 and 1260 m [5]. The area is a quite
complex terrain, with a large number of sinkholes and hilly elevations, and on such all
the effects of terrain on the characteristics of the wind (wind shear, ﬂow separation,
increased levels of turbulence, etc.) will be observed. Total area of the future wind farm
is mostly barren land-rocky ground, without high vegetation due to the strong inﬂuence
of wind, Fig. 1.
Within the area there are not any facilities built. The process of wind potential
measurements according to [1] and [2] started on 17 July 2010 and ended on 15 April
2014. Evidence of measured data (wind speed, direction, temperature, pressure,
humidity) from the meteorological mast, was carried out through the so called data
Fig. 1. Location—Borisavac
156
D. Trešnjo et al.

loggers in the form of a series every 10 min. Wind speed measurements were carried
out at 30 and 60 m in order to determine the increase of wind speed with height [6].
3
Creating a 3D Terrain Model of the Observed Location
To start CFD modeling and analysing of wind potential at a given location, it is
necessary to build a 3D terrain model. While making topographic maps, usually
applied methods are geometrical methods presenting relief with contourlines and their
elevations. Terrain maps should ensure the creation of real spatial sense of prominence
forms, the maps should allow to determine the absolute and relative heights of all
points of relief, direction and steepness of the slope at any point [7]. Figure 2 shows the
project area on topographical map. For creating the 3D terrain model—Borisavac (size
5  4 km), software Rhinoceros was used, Fig. 3.
4
Numerical Calculation of Wind Potential Based on CFD
Approach
In addition to the analytical (theory) and experimental approaches, in recent years the
numerical simulations established themselves as a widespread approach in engineering
and
scientiﬁc
analysis.
The
advantages
of
numerical
solving
compared
to
experimental-ﬂoor analyzes are:
• Numerical solution is often achieved faster when an overall analysis process that
includes all preparatory and ﬁnal actions is considerd, and the costs of solving are
often smaller;
• Variation of the parameters in numerical solving is usually more easily attainable;
• In some cases carrying out the experiment is not possible due to the inability to
achieve conditions of interest, and analytical solution is often impossible due to the
complexity of the equations that describe the observed phenomena.
Fig. 2. Terrain view on a topographic map
Comparison of CFD and Linear Model When Calculating Maps
157

• Numerical solution generally gives more detailed information that applies to all
domains observed due to calculation methods which have to take into account the
interaction of all parts of the domain [8].
However, it is important to mention that for solving this kind of problems, which
treats this paper, the best method—indeed the only fully compelling method—of
establishing causation is to conduct a carefully designed experiment in which the
effects of possible lurking variables are controlled.
Numerical methods in engineering are usually made up of three stages:
1. Pre-processing: Pre-processing consists of deﬁning the input data that fully describe
the observed problem and the subsequent transformation of the input data in a form
that is suitable for use by a computer programs for the calculation (deﬁning the
geometry of the areas of interest—Domain solving, generating numerical mesh,
selection of physical and chemical phenomena, deﬁning the characteristics of the
ﬂuid, setting up the necessary boundary and initial conditions).
2. Calculation: Calculation represents solving a set of algebraic equations obtained by
discretization of the equations of physical-chemical model for the given set of
control volume deﬁned by numerical mesh. There are several methods for the
numerical solution of physical-chemical models.
3. Post-processing: This phase consists from presenting and visualization of the cal-
culation results, transforming them into a form suitable for further analysis and
processing, as well as their interpretation. Post-processing is certainly a key phase
of numerical calculation.
For simulation of wind ﬂow over complex terrain in this paper, software
STAR-CCM+ was used [9]. The software STAR-CCM+ is intended for numerical
modeling of ﬂuid ﬂow problems and is working on the basis of ﬁnite volume. It has an
integrated platform for: geometric CAD modeling, tools for surface preparation,
automatic generation of numerical meshes, modeling of physical processes (including
turbulence) and, calculation execution, visualization of results, and, if necessary,
interconnection with other programs for engineering calculations.
Fig. 3. Creating a 3D terrain model using the software Rhinoceros: starting digitized model
(above) and triangulated surface (bottom)
158
D. Trešnjo et al.

5
Numerical (Computational) Mesh
In this paper and software STAR-CCM+ available options for automatic generation of
numerical mesh were used. Hybrid type of numerical mesh was selected, comprising a
cube-shaped cells inside of the domain, while the tops of the cells in the vicinity of the
domain boundaries were trimmed in order to adapt the form of boundaries. Along solid
domain boundaries (walls), prism layers were automatically generated, which enable a
ﬁner resolution (a smaller thickness of cells, and smaller distance between calculation
points) in the direction perpendicular to the wall, in order to better represent the
boundary layer of the ﬂuid ﬂow. The program automatically optimizes the mesh
depending on the curvature surface of the domain and in the area speciﬁed by the user.
Figure 4 shows the section through the numerical mesh terrain that was used in the
calculations. A typical cell size on the ground is 12.5 m. The number of cells, and
therefore calculation points, for the obtained numerical mesh is approximately 1.5
million. Prism thickness of the layer is 60 m, the number of cells across the prism layer
is 20, and the thickness of the cell closest to the ground is 1 m.
6
Assesment of Wind Potential Using Linear Models
in Softwares WASP and WindPro
WASP was developed in the 1980s for the purpose of the European wind atlas.
Measured data on speed and wind direction statistics are compressed to Weibull dis-
tribution, which proved that very well approximates the actual characteristics of the
wind. Given the fact the wind at altitudes between 10 and 200 m are strongly inﬂu-
enced by topography, it is necessary to model the effect of the terrain on the wind
Fig. 4. Numerical mesh model over the project area
Comparison of CFD and Linear Model When Calculating Maps
159

characteristics on the observed location. Topography is thereby divided into three
categories: orography, roughness and obstacles. In order to obtain a generalized
regional wind climatology, it is necessary to design to exclude the effects of topog-
raphy, which are deducted from the measured data, and their approximations in the
form of Weibull’s distribution [3] Fig. 5.
Calculations based on WASP deemed reliable on ﬂat and moderately undulating
terrain (to 17°), because of the impossibility to model separation ﬂuid (wind ﬂow)
direction. WASP has also the option of correcting the results on more complex terrains
with using the so-called rugedness index (RIX and RIX), which leads to more accurate
results [7].
Fig. 5. WAsP model [10]
160
D. Trešnjo et al.

7
Results
7.1
Results of CFD Calculation
After the creation of 3D model, analysis and processing of available data, the condi-
tions were created to start with CFD modeling and analysis of the impact of terrain on
wind ﬂow. In ﬁrst modeling phase, as inputs, two dominant upstream directions are
taken: ENE (50°) and W (250°), and three different input speeds: 5, 10, 15 m/s. Flow
simulation is played from the limits of the model. The ﬁnal result will be presented as
spatial distribution of wind speed, power density, incident angle and intensity of tur-
bulence, Figs. 6 and 7. A very important characteristic of airﬂow on the observed
Fig. 6. Spatial view of wind speed and power density for the case when the wind ﬂow strikes at
an angle of 50° with respect to North
Comparison of CFD and Linear Model When Calculating Maps
161

complex area is the wind incidence angle, which represents the angle of the wind
“invase” on the motion vector, measured from the course line to the direction from
where the wind blows, and according to the standard for construction and installing of
wind turbines it must not exceed the limit of ± 8°. TI turbulence intensity is the mean r.
m.s. (root mean square) of velocity divided by the reference mean ﬂow velocity vref. It
is related to the turbulent kinetic energy and according to the latest standard it must not
exceed the limit of 0.16. All calculations were performed for a height of 80, 90 and
100 m.
After perceiving the results and analysis of the area matching with given boundary
conditions: speed value  3 m/s, turbulence intensity  0.16, angle of arrival −8 
a  °+ 8, it is possible to choose a region that is allocated as the best, i.e. one that has
Fig. 7. Spatial view of angle of incidence and intensity of turbulence for the case when the wind
ﬂow strikes at an angle of 50° with respect to North
162
D. Trešnjo et al.

fulﬁlled the set criterias. On Fig. 8 the white color is indicated the area which met all
the requirements for both analysed cases.
After detailed analysis of both scenarios, an area is isolated, Fig. 9, which will be
discussed in the next stages of results.
Fig. 8. Representation of area which meets the set criterias (white): a 50°, b 250° (Hill
Ratkamen above the measurement mast is marked with red dumped dash for easier orientation)
Fig. 9. View of area that meets the set criteria (Google Earth)
Comparison of CFD and Linear Model When Calculating Maps
163

The characteristics of the wind with the corresponding production of electricity
represent key factors in assessing the quality of potential locations. It is assumed that
the diameter of the rotor is approx. 100 m and Rule of thumb was used, Fig. 10, while
choosing the layout of wind turbines on the project area. Using this rule, on the
available surface could be optimally installed 11 wind turbines, which will be dis-
tributed manually, Fig. 11.
Fig. 10. Rule of thumb
Fig. 11. Possible spatial distribution of WTGs at the site Borisavac obtained with numerical
calculation in STAR CCM+
164
D. Trešnjo et al.

7.2
The Results of Linear Models—WASP/WindPRO
The calculation results will be divided into three parts. First, spatial distribution of wind
speed, power density, incident angle and turbulence of intesity will be presented,
Figs. 12 and 13. After that, the results obtained by optimizing arrangement of wind
turbines in WindPro will be shown. and at the end the estimates of total annual
production of wind farm for the case obtained by numerical modeling, and for the case
obtained by linear model will be presented.
Fig. 12. Layout view of wind speed obtained in WindPRO
Fig. 13. Layout view of power density obtained in WindPRO
Comparison of CFD and Linear Model When Calculating Maps
165

For the purpose of the feasibility study for this location it is necessary, for various
reasons, to make a wind turbine layout and calculate potential electricity production.
Therefore, application of measured data on a wider region requires methods for the
transformation of the measured values in the region. This means that it si needed to set
up a comprehensive model for vertical and horizontal interpolation and extrapolation of
the measured data and calculation of wind.
These models are based on physical principles of ﬂow in the atmospheric boundary
layer and they take into account the different effects of the terrain inﬂuences caused by
structures and other obstacles, as well as the question of wind speed changing caused
by different height characteristics around the measuring station. It should be noted that
in determining the spatial distribution of mentioned wind characteristics, following
parameters are taken into account:
• Wind direction and energy distribution
• Wake effects
• Land availability
The proposed layout, Fig. 14, promotes optimal energy production and reduces
wake losses.
Fig. 14. Possible spatial distribution of WTGs at the site Borisavac obtained with linear model
—WindPRO
166
D. Trešnjo et al.

8
Calculation of the Annual Production for the Obtained
Windfarm Designs
The basic question in the decision of building a wind farm is “How much is the
expected annual production and what is the payback period of investment?”. Sitting
requires more data and more detailed data from regional estimates. Both items require
and use the general concept of topographic analysis and regional meteorological
assessment of wind. Of course for every location it is necessary to conduct extensive
researches, as far as measuring of wind speed and direction, and making possible more
detailed topographic maps and maps of the terrain roughness.
In order to obtain more accurate results by WindPro [11], ideally would be if at
each future site of wind turbine, at least one metrological mast is installed with a height
of future wind turbine nacelle measuring wind speed and direction. We had only one
measuring station of which in both cases of obtained layouts, only one wind turbine in
located near the meteorological mast.
On Figs. 5.20 and 5.24, optimal wind farm layout solutions have been shown, and
in Tables 1 to 2 the calculation of annual production obtained in WindPro will be
presented. In this example, for all wind turbine units, uniform type of WTG was used:
Vestas V100–1800 kW with tower height of 80 m.
Table 1. Calculation of annual energy production of a wind farm obtained in WindPro
Installed power [MW]
14,4
Number of wind turbines
8
Annual energy production [MWh/year]
39861,5
Gross annual production (without wake) [MWh/year]
42626
Efﬁciency of a wind farm (wake effect) [%]
93,5
Capacitive factor [%]
31,6
The average annual production per wind turbine [MWh/year] 4982,7
Period of full load [h/year]
2768
Mean wind speed at a height of 80 m [m/s]
6,1
Table 2. Calculation of annual energy production of a wind farm obtained in STAR-CCM+
Installed power [MW]
19,8
Number of wind turbines
11
Annual energy production [MWh/year]
53061,2
Gross annual production (without wake) [MWh/year]
57504,9
Efﬁciency of a wind farm (wake effect) [%]
92,3
Capacitive factor [%]
30,6
The average annual production per wind turbine [MWh/year] 4823,7
Period of full load [h/year]
2680
Mean wind speed at a height of 80 m [m/s]
6,0
Comparison of CFD and Linear Model When Calculating Maps
167

9
Conclusions
In this study one year of measured wind data in accordance with [1] and [2], were used
from the 60 m meteorological mast and for the purpose of numerical simulations in
Star-CCM+, and a topographic map with generated contour lines spacing of 20 m was
used. In order to get more accurate results it is necessary to generate a map with a
smaller distance between the contour lines, approximately 5 m and less, as in this case
it was not possible because of the size of the observed area, and due to lack of
topographic maps. Also it is necessary to inspect each location visually, in order to
deﬁne possible obstacles in the form of various objects may be entered into the model.
An advantage of the CFD-models compared to WAsP/WindPRO, is the explicit
calculation of the turbulence ﬁeld. The comparison of the measured and modeled
turbulence intensities shows that WaSP/WindPRO overestimates the turbulence while
STAR-CCM+ tends to give too low values. The directional classiﬁcation uses 12
sectors of 30°. The model runs showed that large variation in the turbulence level may
occur within a 30° sector when the terrain is complex. Thus smaller sectors should be
employed, although this increases the computational efforts of the CFD models.
Through the analysis of the modeled turbulence ﬁelds a better understanding of the
measurements at Borisavac is obtained. For example, the Star-CCM+ clearly indicate
the inﬂuence of the hill Ratkamen located on south-west part of site on the turbulence
level (and consequently also the average wind speed level) at the location. After this
analysis, the question is whether the measuring station could be set up to better, at a
more appropriate location. Also, in terms of deﬁning the exact impact of the topog-
raphy to the airﬂow on such a complex terrain, it is advisable to install another met
mast and compare measurements. It would highly contribute accuracy of the modeling.
The results of this study emphasize the importance of high quality measurements in
complex terrain for a reliable wind resource mapping. Still, both micro- and mesoscale
models are important for the interpretation of the measurements and for an investi-
gation of the turbulence and speed-up effects in complex terrain. The data material of
this report is limited, and therefore further validations of the micro- and mesoscale
models in complex terrain are recommended. This is anticipated to give increased
insight into the potential and limitations of the models for wind farm development.
References
1. International Standard IEC 61400-12-1
2. Measnet Recommendation https://www.measnet.com/wp-content/uploads/2016/05/Measnet_
SiteAssessment_V2.0.pdf
3. Pep, M., Arne, R.G., Manel, R.: Wind Flow Over Complex Terrain: Application of Linear
and CFD Models, Barcelona (2003)
4. Probst, O., Cárdenas, D.: State of the art and trends in wind resource assessment. Energies. 3,
1087–1141 (2010)
5. Lukac, A., Jamak, M., Music, M., Turkovic, E.: Experiences in the measurement campaign
of wind potential on the territory of bosnia and herzegovina. Bosanskohercegovaka
elektrotehnika J, Edition January/December (2011)
168
D. Trešnjo et al.

6. Lukac, A., Music, M., Avdakovic, S., Rascic, M.: Flexible generating portfolio as basis for
high wind power plants penetration—Bosnia and Herzegovina case study, IEEE/PES. In:
10th International Conference on Environment and Electrical Engineering, Italy (2011)
7. Gravdahl, R.A., Rorgemoen, S., Thogersen, M.: Power Prediction and Siting—when the
Terrain gets Rough, The World Wind Energy Conference and Exhibition, Berlin (2002)
8. Undheim, O.: The non-linear microscale ﬂow solver 3DWind. Developments and
Validation. Ph.d. Thesis at the Norwegian Technical University, Trondheim (2005)
9. CD-Adapco: STAR-CCM+ User Guide (2006)
10. Lutgens, F., Tarbuck, E.: The atmosphere: an introduction to meteorology, Prentice Hall
(1994)
11. WindPRO 2.7 User Guide. 3 Ed. Aalborg: EMD International A/S
Comparison of CFD and Linear Model When Calculating Maps
169

Multicriteria Decision Making Model for HPP
Alternative Selection
Zedina Lavić(&) and Sabina Dacić-Lepara
Department of Capital Investments, EPC Elektroprivreda B&H D.D., Sarajevo,
Bosnia and Herzegovina
z.lavic@epbih.ba
Abstract. The development and implementation of new hydropower plant
projects as a way of increasing the share of electricity generation from renew-
able energy sources is an imperative for the energy sector. Electricity that will be
produced in new hydro power plants should be the basis for sustainable
development, which is conditioned by the balance between economic, technical,
social and environmental demands. Therefore, in addressing the problem of
making choice on technical solution variants for hydropower plant, these
requirements should be considered. Since the choice of optimal variants is
inﬂuenced by various factors (criteria), mostly conﬂicting with each other, this
problem is a matter of multicriteria decision making (MCDM). This paper
presents a model for multictiteria decision making on technical solution for
hydro power plant from the Pareto set of alternatives. An analytic hierarchical
process was applied, taking into account quantitative and qualitative criteria.
The model was tested on a speciﬁc example.
Keywords: Sustainable development  Hydropower plant  Multicriteria
decision making  Analytic hierarchy process  Economic  Technical
Social and environmental criteria
1
Introduction
The terms such as sustainability, energy and multicriteria decision making are terms
which are very often used together in papers on energy projects and sustainability
[1–8]. The main objective of this paper is to develop a model for the evaluation and
selection of solutions from the Pareto set of solutions (alternatives for HPP schemes)
using the Analytic Hierarchy Process method [9]. The model should also include the
economic, technical, social and environmental criteria (quantitative and qualitative)
which are immanent to the real problems of decision making on the HPP scheme, when
a decision maker is unable to accurately determine the value of individual criteria or
when they are hardly measurable.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_14

The development of the model is implemented through a series of steps:
1. Selection of decision making criteria,
2. Selection of methods for multicriteria decision aid,
3. Structuring of the decision making hierarchy,
4. Application of the Analytic Hierarchy Process method,
5. Testing of the model.
2
Decision Criteria
The criteria are selected in accordance with the tree dimensional view of sustainability
(economic, environmental and social) and they are immanent to real problems of
decision making on the HPP technical solution alternatives. It is possible to evaluate
the extent to which a certain alternative contributes to achieving the objective of
sustainable development referred to each other and to compare alternatives on the
criteria basis (attributes of alternatives). Analogous to the tree dimensional view of
sustainability, the criteria are also divided into 3 groups of criteria:
• economic and technical criteria (5 criteria given in Table 1).
• environmental criteria (6 criteria given in Table 2).
• social criteria (7 criteria given in Table 3).
Table 1. Economic and technical criteria
Criteria
Designation Unit of measure Max/Min
Installed capacity
EC1
[MW]
Max
Energy generation
EC2
[GWh/a]
Max
Investment costs
EC3
[Mio EUR]
Min
Speciﬁc costs
EC4
[EUR/kWh]
Min
Flexibility in generation EC5
qualitative
Max
Table 2. Environmental criteria
Criteria
Designation Unit of measure Max/Min
Aquatic life
ENC1
Linguistic value Min
Fauna/Flora
ENC2
Linguistic value Min
Water quality/sediments
ENC3
Linguistic value Min
Air quality and micro climate ENC4
Linguistic value Min
Noise and vibration
ENC5
Linguistic value Min
Climate change
ENC6
Linguistic value Min
Multicriteria Decision Making Model for HPP Alternative Selection
171

3
Selection of Multicriteria Decision Making Method
Due to the complexity of the decision making problem and large interdependence of
the selected criteria, as well as due to number and nature of the criteria imposed by the
need to rely on the assessment, Analytic Hierarchy Process is selected and applied as a
multicriteria decision making method, which, in its essence is the most appropriate for
the problem and the context of decision making. The basic concept of applied Analytic
Hierarchy Process method is hierarchical structure of decision making, pairwise
comparison and the bottom up synthesis of priorities through the hierarchy. The key
advantage compared to the other MCDM methods is that with the Analytic Hierarchy
Process, decision makers are not required to accurately determine the value of indi-
vidual criteria (when they are hardly measurable or when there is no available data).
4
Hierarchy of Decision Making and Model Parameters
The problem of decision making is structured in a hierarchy that has 4 hierarchical
levels:
1. Objective: selection of the best alternative from the Pareto set of HPP alternatives
2. Criteria groups
3. Criteria (listed in the criteria groups)
4. Alternatives (Pareto set of alternatives).
The number of alternatives is a variable in the model and the speciﬁc parameters of
the model are:
1. Number of levels in a hierarchical structure (3),
2. Number of criteria groups (3),
3. Total number of criteria (18),
4. Number of economic and technical criteria (5),
5. Number of environmental aspect criteria (6),
6. Number of social aspect criteria (7).
Table 3. Social criteria
Criteria
Designation Unit of measure Max/Min
Resettlement
SC1
Linguistic value Min
Loss of land
SC2
Linguistic value Min
Cultural heritage
SC3
Linguistic value Min
Trafﬁc and infrastructure SC4
Linguistic value Min
Labour and employment SC7
Linguistic value Max
Landscape
SC8
Linguistic value Min
Community acceptance
SC9
Linguistic value Max
172
Z. Lavić and S. Dacić-Lepara

5
Input/Output
Inputs for the model are expert assessments of criteria importance and priorities of
alternatives that are entered into pairwise comparison matrices:
1. Pairwise comparison matrix of criteria groups with respect to the objective
2. Pairwise comparison matrix of economic and technical criteria with respect to the
corresponding group
3. Pairwise comparison matrix of environmental criteria with respect to the corre-
sponding group
4. Pairwise comparison matrix of social criteria with respect to the corresponding
group
5. Pairwise comparison matrices of alternatives with respect to each of the criterion.
The output of the model are priority values for alternatives sorted from largest to
smallest. The best alternative has the highest value of priority.
6
Consistency Check
Pairwise comparison matrices are positive, squared and reciprocal. Elements of
matrices are numbers from Saaty’s scale and consistency check is made according [9].
The scale is shown in Table 4.
7
Algorithm Model
The model can be realized through the algorithm whose ﬂowchart is presented in
Fig. 1.
Table 4. Saaty’s scale
Deﬁnition
Importance intensity Reciprocals
Equal importance
1
1
Moderate importance
3
1/3
Strong importance
5
1/5
Very strong importance 7
1/7
Extreme importance
9
1/9
Intermediate values
2, 4, 6, and 8
1/2, 1/4, 1/6, and 1/8
Multicriteria Decision Making Model for HPP Alternative Selection
173

Fig. 1. Flowchart of algorithm model
174
Z. Lavić and S. Dacić-Lepara

8
Testing Model on a Concrete Example
The developed model is tested on a concrete example of the selection problem for
the best alternative of 5 HPP scheme alternatives, using a Java application made on
the basis of the algorithm of the developed model. The alternatives are given in the
Table 5. The evaluation scale for qualitative criteria (attributes) is given in the Table 6.
Table 5. The alternatives
Economic and technical criteria
Alternatives
A1
A2
A3
A4
A5
1. Installed capacity
Quantitative MW
5.12
6.45
6.35
10.45 10.18
2. Energy generation
Quantitative GWh/a
26.04 33.94 32.30 56.27 53.81
3. Investment costs
Quantitative Mio EUR
35.5
41.9
41.2
72.7
66.4
4. Speciﬁc costs
Quantitative EUR/kWh
1.365 1.234 1.277 1.291 1.233
5. Flexibility in operation
Qualitative
Ling. value 1
2
2
3
3
Environmental criteria
1. Aquatic life
Qualitative
Ling. value –2
–3
–3
–3
–3
2. Fauna/Flora
Qualitative
Ling. value –2
–2
–2
–3
–3
3. Water quality
Qualitative
Ling. value –1
–1
–1
–2
–2
4. Air quality
Qualitative
Ling. value –1
–2
–2
–2
–2
5. Noise and vibration
Qualitative
Ling. value –1
–2
–2
–2
–2
6. Climate Change
Qualitative
Ling. value 2
2
2
2
2
Social criteria
1. Resettlement
Qualitative
Ling. value –1
–3
–3
–4
–4
2. Loss of land
Qualitative
Ling. value –1
–3
–3
–4
–4
3. Cultural heritage
Qualitative
Ling. value 0
0
0
0
0
4. Trafﬁc and infrastructure Qualitative
Ling. value –1
–3
–3
–3
–3
5. Labour and employment Qualitative
Ling. value 1
–2
–2
–2
–2
6. Landscape
Qualitative
Ling. value –2
–3
–3
–4
–4
7. Community acceptance
Qualitative
Ling. value –2
–4
–4
–4
–4
Table 6. The evaluation scale for qualitative criteria
Deﬁnition of impact (linguistic values)
3
Strongly positive: highly beneﬁcial effect, affecting a wide area and/or an important
parameter
2
Positive: beneﬁcial effect
1
Small positive: beneﬁcial effect of lesser importance
0
None: no or negligible impact
–1
Small negative: negative impact of limited duration
–2
Negative: undesirable or harmful effect of limited concern
–3
Strongly negative: mitigation possible
–4
Strongly negative: mitigation not possible
Multicriteria Decision Making Model for HPP Alternative Selection
175

9
Results
Along with the previous explanations of the hierarchical structure of decision making
and the use of the Java application, the decision maker made the pairwise comparisons
to give assessment of importance of criteria groups with respect to the objective (matrix
1 in Table 7), assessment of criteria importance with respect to the corresponding
criteria group (matrices 2–4 in Table 7) and the assessment of the priorities of alter-
natives with respect to each of the criteria (matrices 5–22 in Table 7). The results
(priorities of alternatives) are given in Table 8.
Table 7. Pairwise comparisons matrices
Matrix
No.
CR
(%)
[1 1 1 | 1 1 1 | 1 1 1 | ]
1
0.0
[1 2 3 3 2 | 1/2 1 1 1 1 | 1/3 1 1 1 1 | 1/3 1 1 1 1 | 1/2 1 1 1 1 | ]
2
0.71
[1 1 1 1 1 1 | 1 1 1 1 1 1 | 1 1 1 1 1 1 | 1 1 1 1 1 1 | 1 1 1 1 1 1 | 1 1 1 1 1 1 | ]
3
0.0
[1 2 3 4 4 4 4 | 1/2 1 2 3 3 3 3| 1/3 1/2 1 2 1 1 1 | 1/4 1/3 1/2 1 1 1 1 | 1/4 1/3 1
1 1 1 1 | 1/4 1/3 1 1 1 1 1 | 1/4 1/3 1 1 1 1 1 | ]
4
0.93
[1 1/4 1/3 1/9 1/8 | 4 1 1/2 1/7 1/6 | 3 2 1 1/6 1/6 | 9 7 6 1 1 | 8 6 6 1 1 | ]
5
7.28
[1 1/4 1/3 1/9 1/8 | 4 1 1/2 1/7 1/6 | 3 2 1 1/6 1/6 | 9 7 6 1 1 | 8 6 6 1 1 | ]
6
7.28
[1 1/4 1/3 1/9 1/8 | 4 1 1/2 1/7 1/6 | 3 2 1 1/6 1/6 | 9 7 6 1 1 | 8 6 6 1 1 | ]
7
7.28
[1 1/5 1/4 1/3 1/6 | 5 1 4 4 ½ | 4 1/4 1 2 ¼ | 3 1/4 1/3 1 1/5 | 6 2 4 5 1 | ]
8
9.68
[1 1/2 1/2 1/3 1/3 2 1 1 1/2 1/2 | 2 1 1 1/2 1/2 | 3 2 2 1 1 | 3 2 2 1 1 | ]
9
0.34
[1 2 2 2 2 | 1/2 1 1 1 1 | 1/2 1 1 1 1 | 1/2 1 1 1 1 | 1/2 1 1 1 1 | ]
10
0.0
[1 1 1 2 2 | 1 1 1 2 2 | 1 1 1 2 2 | 1/2 1/2 1/2 1 1 | 1/2 1/2 1/2 1 1 | ]
11
0.0
[1 1 1 2 2 | 1 1 1 2 2 | 1 1 1 2 2 | 1/2 1/2 1/2 1 1 | 1/2 1/2 1/2 1 1 | ]
12
0.0
[1 2 2 2 2 | 1/2 1 1 1 1 | 1/2 1 1 1 1 | 1/2 1 1 1 1 | 1/2 1 1 1 1 | ]
13
0.0
[1 2 2 2 2 | 1/2 1 1 1 1 | 1/2 1 1 1 1 | 1/2 1 1 1 1 | 1/2 1 1 1 1 | ]
14
0.0
[1 1 1 1 1 | 1 1 1 1 1 | 1 1 1 1 1 | 1 1 1 1 1 | 1 1 1 1 1 | ]
15
0.0
[1 3 3 4 4 | 1/3 1 1 2 2 | 1/3 1 1 2 2 | 1/4 1/2 1/2 1 1 | 1/4 1/2 1/2 1 1 | ]
16
0.84
[1 3 3 4 4 | 1/3 1 1 2 2 | 1/3 1 1 2 2 | 1/4 1/2 1/2 1 1 | 1/4 1/2 1/2 1 1 | ]
17
0.84
[1 1 1 1 1 | 1 1 1 1 1 | 1 1 1 1 1 | 1 1 1 1 1 | 1 1 1 1 1 | ]
18
0.0
[1 3 3 3 3 | 1/3 1 1 1 1 | 1/3 1 1 1 1 | 1/3 1 1 1 1| 1/3 1 1 1 1 | ]
19
0.0
[1 4 4 4 4 | 1/4 1 1 1 1 | 1/4 1 1 1 1 | 1/4 1 1 1 1 | 1/4 1 1 1 1| ]
20
0.0
[1 2 2 3 3 | 1/2 1 1 33 | (1/2 1 1 2 2 | 1/3 1/3 1/2 1 1 | 1/3 1/3 1/2 1 1 | ]
21
1.59
[1 3 3 3 3 | 1/3 1 1 1 1 | 1/3 1 1 1 1 | 1/3 1 1 1 1| 1/3 1 1 1 1 |]
22
0.0
Table 8. Priorities of alternatives
Alternative Priority
A1
0.2491
A5
0.2182
A3
0.2061
A2
0.1659
A3
0.1603
176
Z. Lavić and S. Dacić-Lepara

10
Conclusion
In this paper HPP scheme selection problem formulated as a problem of multicriteria
(multi attributive) decision making is solved and a model for evaluation and selection
of solution from Pareto set of solutions (HPP scheme alternatives) is developed. The
proposed model for multicriteria decision making on the HPP scheme, developed on
the principals of sustainable development that includes a relatively large number of
criteria (quantitative and qualitative) is applicable in situations where it is difﬁcult to
measure criteria or in situations where it is difﬁcult to quantify and measure them.
Compared to models based on the other MCDM methods, the advantage of the pro-
posed model based on the Analytic Hierarchy Process method is reﬂected in the fact
that the expert assessments are expressed verbally which is a more realistic presentation
of relative importance assessment and the priority of alternatives given by experts.
Future research can be carried out to the direction of application of fuzzy numbers and
fuzzy logic on the proposed model for multicriteria decision making.
References
1. Morimoto, R.: Incorporating socio-environmental considerations into project assessment
models using multi-criteria analysis: a case study of Sri Lankan hydropower projects. Energy
Policy 59(C), 643–653 (2013)
2. Klimpt, J.E., Rivero, C., Puranen, H., Koch, F.: Recommendations for sustainable
hydroelectric development. Energy Policy 30(14), 1305–1312 (2002)
3. Maxim, A.: Sustainability assessment of electricity generation technologies using weighted
multi-criteria decision analysis. Energy Policy 65(C), 284–297 (2014)
4. Vučijak, B., Kupusović, T., Midžić-Kurtagić, S., Ćerić, A.: Applicability of multicriteria
decision aid to sustainable hydropower. Appl. Energy 101(C), 261–267 (2013)
5. Nautiyal, H., Singal, S.K., Varun, S., Sharma, A.: Small hydropower for sustainable energy
development in India. Renew. Sustain. Energy Rev. 15(4), 2021–2027 (2011)
6. Vera, I., Langlois, L.: Energy indicators for sustainable development. Energy 32(6), 875–882
(2007)
7. Dombi, M., Kuti, I., Balogh, P.: Sustainability assessment of renewable power and heat
generation technologies. Energy Policy 67(C), 264–271 (2014)
8. Sarkar, A.U., Karagöz, S.: Sustainable development of hydroelectric power. Energy 20(10),
977–981 (1995)
9. Saaty, T.L.: Analytic Hierarchy Process. McGraw-Hill, New York (1980)
Multicriteria Decision Making Model for HPP Alternative Selection
177

Power System Planning: Part I—Basic
Principles
Armin Demir(&) and Nasiha Hadžijahić
Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and
Herzegovina
ademir1@etf.unsa.ba
Abstract. Power system planning is an activity related to the development of
plans for designing and construction of the system and its elements, which will
satisfy assumed future needs, starting from the given state. First paper presents
basic principles of power system development planning with its concepts.
Electrical energy losses as well as forecasting of energy consumption are taken
in consideration. Basic principles of development planning for each subsystem
(generation, transmission and distribution) are presented. In the second paper,
practical application of techno-economic analysis of the transition from the
voltage level of 10–20 kV in Gračanica is presented with two different invest-
ment costs using four different approaches.
1
Introduction
Power engineering represents the energy sector responsible for the generation, trans-
mission and distribution of electrical energy to consumers. Power system is a complex
dynamic system, consisting of a set of power plants, transmission lines, transformers
and consumers that are interconnected so that they act as a unique unit. Power system
can be divided into four functionally independent units interconnected in one system:
• generation subsystem;
• transmission subsystem;
• distribution subsystem;
• consumption subsystem.
The basic task of these subsystems is to ensure reliable, high quality and rational
power supplying of different consumer types.
Power system planning is an activity related to the development of plans for
designing and construction of the system and its elements, which will satisfy assumed
future needs, starting from the given state. The main goal of power system exploitation
is to use currently built objects and systems in the best possible way. Time interval of
power system planning and exploitation is given in Fig. 1.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_15

2
Principles of Power System Development Planning
Power system development planning is a process that is related to the planning of the
total energy ﬂows development and it can not be observed separately from other energy
sectors. General strategy of energy development in certain country is the ﬁrst step in
planning system development.
The country’s energy development strategy is based on the development of three
global plans:
• socio-economic development plan of the country;
• plan for development of total energy consumption and primary energy resources
and power ﬂows;
• plan for the development of power engineering under the total energy development.
In terms of time decomposition of the problem, there are three following categories
of power system planning development:
• Long-term development planning, with a planning horizon of 10–30 years in the
future. Such procedures deﬁne a long-term development strategy.
• Mid-term development planning, with a planning horizon of 5–10 years in the
future. This is also the most important stage of planning. At this stage decisions on
construction of power system facilities are developed.
• Short-term development planning, with a planning horizon of 1–5 years in the
future. At this stage, decisions that were made are being adjusted with a new
situation.
Spatial decomposition corresponds to the division of power system into functional
units of generation, transmission and distribution of energy as well as consumption.
• Consumption development planning represents the prognosis of different parame-
ters that describe consumption of some consumers in the future. The most com-
monly forecasts are total electricity consumption, peak and minimum power
consumption, load factors, etc.
• Generation capacities development planning is a procedure in which a decisions on
the construction of new power plants and generating units are made. Within this
procedure, transmission and distribution network are neglected so the whole power
system is considered to be one node.
Fig. 1. Time interval of power system planning and exploitation
Power System Planning: Part I—Basic Principles
179

• Transmission and distribution development planning is an activity in which a
decisions to build new elements (transmission lines, transformers, distribution
systems) of transmission and distribution networks are made [1].
3
Power System Planning Concepts
Power system planning is based on four basic concepts:
• security and stability;
• reliability;
• power quality;
• economy.
3.1
Security and Stability Concept
Planning of power systems is based on n −1 concept of security, where systems can
withstand all possible single disturbances without disturbing steady state condition. In
this case, n is the total number of elements (generators, transformers, lines) that can by
affected by the fault.
The n −1 concept of security is deﬁned as follows: The failure on one of the total
n elements of the system (generators, lines, transformers, consumers and others) must
not have inﬂuence on the correct operation of the power system with the residual n −1
components [2].
Power system stability is the ability of an electric power system to regain a state of
operating balance after being subjected to physical disturbance, with most system
variables bounded so that practically the entire system remains intact [3].
3.2
Reliability Concept
Reliability represents the probability of proper functioning of the system and providing
electrical energy supply to consumers. This probability is related to certain reliability
indicators that are related to the size of the power disbalance, the reduction in delivery,
the amount of undelivered energy, the frequency of occurrence and the duration of the
failure.
These indicators are:
• LOLP (“Loss of Load Probability”)—the number of hours in a year when pro-
duction can not satisfy daily peak consumption;
• LOEP (“Loss of Energy Probability”)—production of insufﬁcient energy;
• EDNS (“Expected Value of Demand Not Served”)—inability to satisfy consumer
needs;
• EENS (“Expected Value of Energy Not Served”)—expected value of the energy not
delivered;
• F&D (“Frequency and Duration”)—frequency and duration of failures that cause
loss of load.
180
A. Demir and N. Hadžijahić

3.3
Power Quality Concept
The basic indicator of power quality is the continuity of the supply to consumers.
Voltage and frequency are two most important variables that characterize proper
functioning of each power system. Standards of power quality is related to these
mentioned variables.
Deﬁnition of the concept of ‘power quality’ according to IEC 61000-4-30: The
electrical energy characteristics at a particular point of the power system observed in
comparison to the reference technical parameters.
3.4
Economics Concept
This concept is based on an analysis of the revenues and costs associated with each
project, and the most sensible plan is made on the basis of determining the minimum or
maximum of one or more selected criteria. Attention should be paid to investment costs
as well as to the costs of exploitation and management, while avoiding any of the above
concepts for minimizing the investment during the lifetime of the system.
From various solutions available for a problem, a planner should select the best, in
terms of both technical and economic considerations.
Three methods may be used for economic appraisal of a project:
• Present worth method—All input and output cash ﬂows of a project are converted
to the present values. In this method, if the economic lives of the plans are different,
the study period may be chosen to cover both plans in a fair basis. For instance, if
the economic lives of two plans are 3 and 4 years, respectively, the study period
may be chosen to be 12 years.
• Annual cost method—All input and output cash ﬂows of a project are converted to a
series of uniform annual input and output cash ﬂows. A project with a uniform
annual output less than its respective input is considered to be attractive. This
method is especially attractive if the plans economic lives are different.
• Rate of return method—There are some input and output cash ﬂows during the
economic life of a project. If we consider an interest rate at which these cash ﬂows
are equal (i.e., the net is zero), the resulting rate is named as Rate Of Return (ROR).
ROR should be compared with the Minimum Attractive Rate Of Return (MAROR).
Provided ROR is greater than MAROR, the plan is attractive. From those attractive,
the one with the highest ROR is the most favorable [4].
The basic criteria for development planning of the Bosnia and Herzegovina power
system transmission network is to minimize total (investment and exploitation) costs,
while meeting the requirements of the power system security. Therefore, in planning of
transmission network development it is necessary to apply both technical and economic
criteria, in order to achieve the technical and economic optimum [5].
Power System Planning: Part I—Basic Principles
181

4
Electrical Energy Losses
The loss of electrical energy/power is usually deﬁned as the difference between the
power supplied to the transmission system and the registered power consumed at the
ﬁnal customer’s measuring point.
Calculation of power losses in the power grids at any voltage level should normally
be performed under the calculations of voltage and power ﬂows. These are sufﬁcient
information to determine the electrical power losses on each system element.
By summing up the losses on individual elements of the system, the total losses of
electric power are obtained.
4.1
Classiﬁcation of Electrical Energy and Power Losses
Electrical energy losses can be divided into two groups:
• technical and
• commercial (non-technical).
Technical losses are the result of exploitation and they represent the biggest part of
the overall losses in the network. They are caused by the fact that the elements of the
system are under voltage and that the current ﬂows through them.
They can be divided into:
• voltage-dependent losses,
• current dependent losses [6, 7].
Voltage-dependent losses are independent of the load and they occur in the ele-
ments, regardless of whether the current ﬂows through them or not. They are present as
long as the elements are under voltage. Voltage changes are relatively small, so
voltage-dependent losses are relatively constant. These include: losses in transformers
under no-load conditions, dielectric losses in cables and capacitor batteries, losses of
arrester currents of air lines.
Current dependent losses depend on the load (variable losses) and they occur in
network elements (lines and transformers) as a result of the current ﬂow through these
elements, which depends on the power of the consumer. The amount of these losses is
directly proportional to the square value of current.
Non-technical losses are the result of electrical energy that is not measured for some
reason. They arise as a consequence of their own consumption (known as “hidden”
consumption), illegal use of electrical energy (theft), unmeasured delivered electrical
energy (e.g. part of public lighting) as well as errors in the process of measuring and
processing data. Non-technical
losses are often referred to as “commercial”
losses [8–10].
Technical losses can be reduced with and without additional investments. Methods
that require additional investments are:
• increasing the cross-section of the conductor;
• adding new transformer stations in the network;
• reactive power compensation;
182
A. Demir and N. Hadžijahić

• using the three-phase lines;
• switching to a higher voltage level;
• direct transformation of 110/x kV usage;
• replacement of old equipment with new ones with fewer losses in exploitation.
Another set of methods that do not require additional investments are:
• improving the power supply;
• control of the load diagram and uniformly use of power capacities;
• managing the load schedule between the individual phases (especially the 0.4 kV
network);
• ﬁnding optimal transformer power and location.
According to the cause and location of power loss occurrence it is possible to divide
losses into three groups:
• losses due to the heating of the conductor through which the current ﬂows—
thermogenic losses,
• losses due to the magnetization of the conductor environment through which the
current ﬂows—the losses of magnetization,
• losses due to the imperfection of the conductor.
The largest part of total losses happened due to the heating of the conductor through
which current ﬂows, and depends on the current square and the electrical resistance of
the conductor through which the current ﬂows:
I tð Þ ¼
P tð Þ
U tð Þ cos u tð Þ
ð1Þ
R ¼ l
qk
ð2Þ
DP tð Þ ¼ I2 tð ÞR
ð3Þ
DW ¼ R
ZT
0
I2 tð Þdt
ð4Þ
where
P tð Þ
the active power transmitted through the line at the time t,
U tð Þ
the line voltage at the beginning of the line at the time t,
I tð Þ
the current ﬂowing through the line at the time t,
cosu tð Þ
power factor at the time t,
R
line resistance,
DP tð Þ
power losses at the time t [11].
Power System Planning: Part I—Basic Principles
183

5
Forecasting the Consumption of Electrical Energy
and Power
Forecasting the consumption of electrical energy and power is one of the most
important and signiﬁcant activities in the power system.
Forecast of consumption is important from the aspect of power generation units
engagement, elaboration of plans for repair and economic dispatching.
The forecast of electricity consumption and power can be observed through the
time and space framework (local consumption, region, state).
Time and space decomposition:
• Short-term forecast with horizon from one day to one week and hourly time dis-
cretization. This prognosis is characterized by periodicity, because daily and weekly
load changes have regular repetition. In this type of forecasting it is needed to
differentiate the effects of working days, festivals and holidays comparing to the
“normal loads”.
• The mid-term forecast includes a time period of one month to ﬁve years, with a
weekly or monthly time discretization. This forecast represents the link between
exploitation and power system development planning. Models used for mid-term
forecasting should be able to take into account weather and climate inﬂuencing
factors as there is a signiﬁcant inﬂuence on temperature and weather conditions on
load.
• Long-term forecast with a horizon of 5–30 years and annual time discretization.
This forecast is necessary for solving global long-term investment plans, especially
when designing a new construction plan or extending existing production and
transmission capacities.
Spatial decomposition coincides with the division of power system into the func-
tional units of production, transmission, distribution and consumption of electrical
energy.
In order to carry out system expansion planning in a cost-effective manner, com-
panies must be able to predict the need for electrical energy delivery and companies
must be able to answer three important questions:
• How much power is needed for delivery?
• Where will that power be needed?
• When will that power be needed? [1, 12, 13].
There are many different types of load forecasting techniques and methodologies.
Most forecasting techniques can be classiﬁed as either qualitative or quantitative as
shown in Fig. 2. Qualitative forecasting techniques are subjective, based on the opinion
and judgment of informed parties. Qualitative forecasting is appropriate when past data
are not available. Quantitative methods forecast future data as a function of past data.
They can be used when past numerical data is available and when it is reasonable to
assume that some of the patterns in the data are expected to continue into the future
[14].
184
A. Demir and N. Hadžijahić

6
Sources Development Planning
Planning the development of electrical energy sources is one of the most important and
most sensitive steps in the process of planning modern power systems.
Decisions made at this stage of planning have a signiﬁcant impact in planning the
transmission network and have dominant impact on the economic and ﬁnancial indi-
cators. This is the result of a very large investments in electrical energy sources, which
reaches around 60–65% of total investment in power system.
In this planning stage decisions on construction of new power plants and generating
units are made. Within this procedure, the transmission and distribution networks are
neglected and the whole power system is observed as one node in which the total
production and consumption of electrical energy is concentrated.
Planning the development of electrical energy sources must satisfy requirements for
electrical energy and power that are being increased by the time, with a deﬁned level of
reliability, security and power quality with acceptable price.
Development planning of the sources is a continuous process in which the fol-
lowing problems are solved successively:
• determining the size and timing of entering new required generating units in system
and
• the choice of types of new generation units that will be integrated into the system,
according to the needs expressed through solving the ﬁrst problem.
The time period for development of sources must be long enough to enable a series
of activities that need to be performed before releasing a new generating unit in work.
These are the following activities:
1. Designing (investment program with feasibility study, design and main project).
2. Providing ﬁnancial resources.
3. Contracting of equipment and actions.
4. Construction of facilities and installation of equipment.
Fig. 2. Quantitative and qualitative forecasting
Power System Planning: Part I—Basic Principles
185

5. Testing of the object.
6. Trial work [1, 12, 13].
7
Transmission System Development Planning
Transmission system development planning is determined by the long-term prognosis
of consumer loads and the generation or production of the system. The main question
that has to be answered in this stage is when should the network be supplemented with
additional components, such as transformers or lines. These components can have
different rated voltages. The answer to this question is based on the concept of static
system security. The traditional procedure is usually referred to as the horizon-year
study and begins by sketching a preliminary solution to the system load levels that are
several times greater than the existing load level. The aim is to identify long-term
network system needs. The solution is checked by a full power ﬂow calculation.
The stages in transmission system development planning are:
• development of network study for horizon-year conditions,
• creating network and network building to its ﬁnal conﬁguration in the horizon year.
It’s assumed:
• that the existing network represents the initial solution;
• that the information about the location and size of future loads of consumers exists;
• that voltage levels are already indicated.
The basic criteria for network transmission planning is to minimize constructive
and exploitative costs while satisfying the requirement to deliver electrical power to
consumers safely.
There are two types of transmission lines: overhead lines and underground cables.
Technically, the lines are characterized by the voltage level, the number of lines per
phase, phase cross-section, the number of parallel lines on the same towers and the type
of insulation. Cost-effectiveness is assessed by investment and operating costs.
The basic problems that need to be solved in the planning of transmission lines are:
• selection of the proper voltage level;
• selection of the route;
• selection of material, conductor cross-section, type and characteristics of lines;
• selection of type and design of overhead lines.
Development planning for construction of transmission power plants has similar
issues as the planning and construction of transmission lines. It consists of following
steps:
• selection of power plant type (for exterior or interior mounting);
• voltage level selection;
• site/location selection (considering energy and urban requirements);
• integration to transmission network;
• compensation of the system buses and single-pole scheme of the plant.
186
A. Demir and N. Hadžijahić

Individual optimization of the basic parameters of transmission networks (lines and
transformers) in power transfer is not a good way to obtain a global optimum for the
whole system because such approach would result in large number of voltage levels
and cross sections of transmission lines, different power of transformers and other parts
of the network. To avoid these situations process of standardization needs to be per-
formed. Standardization means the reduction of investment equipment that is achieved
by production in large series with a relatively small number of standard elements.
Standards are introduced through national and international professional organizations
(BAS, ASA, NEMA, IEEE, IEC, CIGRE and others).
For example, the standard cross-sections of the conductors are:
• 150/25 mm2 Al/Fe and 240/25 mm2 for 110 kV lines;
• 360/57 mm2 Al/Fe and 490/65 mm2 for 220 kV lines;
• 2  490/65 mm2 Al/Fe for 400 kV lines [1, 12, 13].
8
Distribution System Development Planning
Planning of the distribution network is a process that is implemented within the pro-
cedure of development planning of whole power system, whereby this process must
satisfy different requirements of users and owners or network operators. Planning of the
power distribution network includes geographic, technical and economic analysis of
various solutions to provide reliable and economically acceptable services to network
users. Modern methods of planning the development of power grids include several
interrelated analyzes. The purpose of planning the development of the power distri-
bution network is to ensure reliable operation and appropriate level of power quality as
well as the cooperation with transmission network and connected power plants.
Development planning of distribution network may be divided into:
• short-term development plan (1–5 years)
• mid-term development plan (5–10 years)
• long-term development plan (10 years and over).
The long-term development plan for the power distribution network is usually done
for a period of following 10–20 years. The purpose of creating development plans is:
• determining the direction of development of the distribution network by stages and
at the end of the planning period;
• appropriate dimensioning for reliable operation and maintenance of power quality
according to standards;
• cooperated performance of distribution network with the transmission network and
connected distribution network users;
• selecting the location of new objects.
The long-term and mid-term plans for the development of the power distribution
network are the basis for the preparation of three-year/annual investment plans.
Through the long-term development plan, the strategy for the development of
distribution network for a 20-year plan by stages and consumption areas is being
Power System Planning: Part I—Basic Principles
187

determined. The long-term development plan should be ﬂexible in terms of
re-examination in a certain period of time, foe example every 5 years.
In the process of distribution network development, it is necessary to analyze all
possible solutions that satisfy the technical and economic criteria of planning [1, 12,
13].
9
Conclusion
The importance of power system planning is presented through this paper. In order to
perform power system planning in the best possible way, concepts of security and
stability, reliability, power quality and economy have to be satisﬁed. Each subsystem of
overall power system has to be planned separately because of different requests that has
to be satisﬁed. Losses of electrical energy represent very important factor for planning
process and they have to be taken into consideration.
References
1. Ćalović, M.S., Sarić, A.T.: Planiranje elektreoenergetskih sistema I dio. Beopres, Beograd
(2000)
2. Union for the co-ordination of transmission of electricity—UCTE, Operation handbook,
Brisel (2004)
3. CIGRÉ and IEEE: Deﬁnition and classiﬁcation of power system stability. IEEE Trans.
Power Syst. 19(3), 1387–1399 (2004)
4. Seiﬁ, H., Sepasian, M.S.: Electric Power System Planning: Issues, Algorithms and Solutions.
Springer, Berlin, Heidelberg (2011)
5. Bilten Elektroprijenosa FBiH. Elektroprijenos, Sarajevo
6. Rajaković, N., Tasić, D.: Distributivne i industrijske mreže, Beograd (2008)
7. Rajaković, N.: Analiza elektroenergetskih sistema I. Elektrotehnički fakultet Beograd,
Beograd (2002)
8. Hajro, M., Bišanović, S.: Industrijski i distributivni elektroenergetski sistemi, Sarajevo
(2012)
9. Treatment of Losses by Network Operators: ERGEG Position Paper for Public Consultation,
15 July 2008
10. Treatment of Electricity Losses by Network Operators, ERGEG Position Paper, Conclusions
Paper (2009)
11. Žutobradić, S., Rajić, Ž., Wagmann, L., Miličić, H.: Analiza problematike gubitaka
električne energije u distribucijskim mrežama članica EU, Umag (2010)
12. Avdaković, S.: Predavanja iz predmeta Planiranje elektroenergetskih sistema. Elek-
trotehnički fakultet u Sarajevu, Sarajevo (2016)
13. Škokljev, I.: Planiranje elektroenergetskih sistema. Taurus Publik, Beograd (2000)
14. Ancell, G. (NZ), Avdakovic, S. (BA), Breedt, J. (ZA), Bugten, T. (NO), Carrasco, A.R. (ES),
Carruthers, G. (AE), Meng, Z. (CN), Pilenieks, D. (RU), van den Waeyenberg, S. (NL):
Establishing Best Practice Approaches for Developing Credible Electricity Demand and
Energy Forecasts for Network Planning, CIGRE (2012)
188
A. Demir and N. Hadžijahić

Power System Planning: Part II—Practical
Applications
Armin Demir(&) and Nasiha Hadžijahić
Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and
Herzegovina
ademir1@etf.unsa.ba
Abstract. Power system planning is an activity related to the development of
plans for designing and construction of the system and its elements, which will
satisfy assumed future needs, starting from the given state. First paper presents
basic principles of power system development planning with its concepts.
Electrical energy losses as well as forecasting of energy consumption are taken
in consideration. Basic principles of development planning for each subsystem
(generation, transmission and distribution) are presented. In the second paper,
practical application of techno-economic analysis of the transition from the
voltage level of 10–20 kV in Gračanica is presented with two different invest-
ment costs using four different approaches.
1
Introduction
Power system planning is an area where technology and economics must be inter-
connected. The construction of new electric power facilities and their exploitation are
closely related to the costs and revenues of the owners (companies). In power system
development planning process, the engineering economy methods are being used for
systematic analyzes and estimation of costs and revenues, resulting from various
system development projects in the future.
This paper presents a techno-economic analysis with cost effectiveness of the
transition from the voltage level of 10–20 kV in Gračanica . Two cases have been
observed:
• investment costs of 2 500 000,00 km (from now on Case 1)
• investment costs of 3 600 000,00 km (from now on Case 2).
It is important to note that for the Case 1 investment costs are reffered to replacement of
the power system elements that are not capable to work on 20 kV voltage level only.
Thus, certain number of elements will not be replaced.
On the other hand, in Case 2 all elements will be replaced with the new one that can
operate on voltage level 20 kV.
In this paper four methods are used for economic appraisal of a project, namely as:
• Rate of Return Method;
• Net Present Value;
• Internal Rate of Return Method;
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_16

• Proﬁtability Index Method [1].
The aim of the paper was to check the cost-effectiveness of transition from the voltage
level of 10 kV to voltage level of 20 kV.
2
Current State of Distribution Network Gračanica
Table 1 shows technical parameters of distribution network Gračanica for voltage level
10 kV as well as the voltage level 20 kV.
Difference in losses (DW) while using 10 kV voltage level comparing with voltage
level 20 kVs equals 2.552, 37 MWh/year.
Since the price of 1 kWh amounts 0.15 km total annual proﬁt can be calculated as:
2552370 KWh
year


 0,15
KM
KWh


¼ 382855,5 KM/year
½

ð1Þ
3
Rate of Return Method
The problem of selecting a project realization can be seen from the aspect of the time
the funds will be repaid.
This time period of return is deﬁned as the number of years for which the savings
on the exploitation costs are equal to the investment costs. The project is more prof-
itable as the time of investments return is less [3, 4].
The total investment costs of transition from 10 to 20 kV is 2 500 000,00 km in the
ﬁrst case and 3 600 000,00 km in the second one.
Expected revenues for the ﬁrst 15 years for Case 1 are given in Table 2.
Expected revenues for the ﬁrst 15 years for Case 2 are given in Table 3.
The time (or deadline) of return on invested costs represents the time when the total
amount of money invested in the realization of the project is returned from the pure
income of the economic ﬂow. The criterion of project rating according to this indicator
is actually the length of the return period. Shorter time means that the project is more
acceptable.
The return period is calculated as the ratio of the initial investment costs and the
annual cash ﬂow revenue of the project [5].
From the previous analysis cost effectiveness of the project would be 7 years in
Case 1 and 10 years in Case 2 resulting from the project conditions as shown in Figs. 1
and 2.
4
Net Present Value
Net present value represents the fundamental criteria of ﬁnancial decision-making. All
cash ﬂows are reduced to their present value.
In general case:
190
A. Demir and N. Hadžijahić

Table 1. Technical parameters of the distribution network Gračanica TS 10(20) kV/0.4 kV [2]
Losses
Pmax
[kW]
10 kV
20 kV
Ukupno
T [h]
DW
[MWh/year]
Ploss
[kW]
Ploss
[%]
P0
[kW]
Ploss
[kW]
Ploss
[%]
P0
[kW]
DPloss
[kW]
DPloss
[kW]
311,50
0,03
0,01
0,00
0,01
0,00
0,00
0,02
0,01
2.400
0,02
2.327,00
256,70
11,03
11,09
84,51
3,63
10,16
172,19
7,40
4.083
397,53
518,10
6,44
1,24
14,73
3,81
0,74
1,57
2,63
0,51
5.557
74,54
1.870,00
92,41
4,94
10,67
39,65
2,12
10,35
52,76
2,82
4.693
154,07
21,98
0,91
4,13
23,54
0,87
3,94
0,85
0,04
0,19
4.376
140,80
1.695,00
75,78
4,47
13,94
33,15
1,96
12,81
42,63
2,52
4.929
140,20
2.332,00
69,45
2,98
18,46
33,02
1,42
17,70
36,43
1,56
5.883
159,30
1.603,00
268,50
16,75
12,93
69,90
4,36
10,86
198,60
12,39
4.651
576,33
1.701,00
154,90
9,11
9,91
57,95
3,41
9,64
96,95
5,70
5.090
323,37
1.643,00
30,38
1,85
11,11
20,84
1,27
11,09
9,54
0,58
5.436
35,63
3.229,00
69,88
2,16
9,96
29,93
0,93
9,87
39,95
1,24
4.639
113,49
1.698,00
215,50
12,69
14,25
63,22
3,72
12,36
152,28
8,97
4.609
437,09
18.949,58
1.240,87
6,55
150,59
436,85
2,31
107,26
804,02
4,24
2.552,37
Power System Planning: Part II
191

Table 2. Expected revenues for the next 15 years (Case 1)
Project year Year Total investments Net income (annual amount) Cumulative net income
0
2018 2500000
−2500000
−2500000
1
2019
382855,5
−2117144,5
2
2020
382855,5
−1734289
3
2021
382855,5
−1351433,5
4
2022
382855,5
−968578
5
2023
382855,5
−585722,5
6
2024
382855,5
−202867
7
2025
382855,5
179988,5
8
2026
382855,5
562844
9
2027
382855,5
945699,5
10
2028
382855,5
1328555
11
2029
382855,5
1711410,5
12
2030
382855,5
2094266
13
2031
382855,5
2477121,5
14
2032
382855,5
2859977
Table 3. Expected revenues for the next 15 years (Case 2)
Project year Year Total investments Net income (annual amount) Cumulative net income
0
2018 3600000
−3600000
−3600000
1
2019
382855,5
−3217144,5
2
2020
382855,5
−2834289
3
2021
382855,5
−2451433,5
4
2022
382855,5
−2068578
5
2023
382855,5
−1685722,5
6
2024
382855,5
−1302867
7
2025
382855,5
−920011,5
8
2026
382855,5
−537156
9
2027
382855,5
−154300,5
10
2028
382855,5
228555
11
2029
382855,5
611410,5
12
2030
382855,5
994266
13
2031
382855,5
1377121,5
14
2032
382855,5
1759977
192
A. Demir and N. Hadžijahić

S0 ¼
X
T
t¼1
Vt
1 þ k
ð
Þt  I0
ð2Þ
S0 ¼
X
T
t¼1
Pt  Zt
ð
Þ
1 þ k
ð
Þt  I0
ð3Þ
where
I0
investment costs;
Pt
annual revenue;
Zt
annual expenses [5].
Fig. 1. Proﬁt per years (Case 1)
Fig. 2. Proﬁt per years (Case 2)
Power System Planning: Part II
193

Table 4. Discount factor, discount rate and annual net income for Case 1
Discount factor
Project
year
Net income (annual
amount)
4
6
8
10
12
14
16
Discount
rate
0
−2500000
1
382855,5
0,962 0,943 0,926 0,909 0,893 0,877 0,862
2
382855,5
0,925 0,890 0,857 0,826 0,797 0,769 0,743
3
382855,5
0,889 0,840 0,794 0,751 0,712 0,675 0,641
4
382855,5
0,855 0,792 0,735 0,683 0,636 0,592 0,552
5
382855,5
0,822 0,747 0,681 0,621 0,567 0,519 0,476
6
382855,5
0,790 0,705 0,630 0,564 0,507 0,456 0,410
7
382855,5
0,760 0,665 0,583 0,513 0,452 0,400 0,354
8
382855,5
0,731 0,627 0,540 0,467 0,404 0,351 0,305
9
382855,5
0,703 0,592 0,500 0,424 0,361 0,308 0,263
10
382855,5
0,676 0,558 0,463 0,386 0,322 0,270 0,227
11
382855,5
0,650 0,527 0,429 0,350 0,287 0,237 0,195
12
382855,5
0,625 0,497 0,397 0,319 0,257 0,208 0,168
13
382855,5
0,601 0,469 0,368 0,290 0,229 0,182 0,145
14
382855,5
0,577 0,442 0,340 0,263 0,205 0,160 0,125
Table 5. Discount factor, discount rate and annual net income for Case 2
Discount factor
Project
year
Net income (annual
amount)
4
6
8
10
12
14
16
Discount
rate
0
−3600000
1
382855,5
0,962 0,943 0,926 0,909 0,893 0,877 0,862
2
382855,5
0,925 0,890 0,857 0,826 0,797 0,769 0,743
3
382855,5
0,889 0,840 0,794 0,751 0,712 0,675 0,641
4
382855,5
0,855 0,792 0,735 0,683 0,636 0,592 0,552
5
382855,5
0,822 0,747 0,681 0,621 0,567 0,519 0,476
6
382855,5
0,790 0,705 0,630 0,564 0,507 0,456 0,410
7
382855,5
0,760 0,665 0,583 0,513 0,452 0,400 0,354
8
382855,5
0,731 0,627 0,540 0,467 0,404 0,351 0,305
9
382855,5
0,703 0,592 0,500 0,424 0,361 0,308 0,263
10
382855,5
0,676 0,558 0,463 0,386 0,322 0,270 0,227
11
382855,5
0,650 0,527 0,429 0,350 0,287 0,237 0,195
12
382855,5
0,625 0,497 0,397 0,319 0,257 0,208 0,168
13
382855,5
0,601 0,469 0,368 0,290 0,229 0,182 0,145
14
382855,5
0,577 0,442 0,340 0,263 0,205 0,160 0,125
194
A. Demir and N. Hadžijahić

Table 6. Net present value method (Case 1)
Discount cash ﬂow
Project year
4
6
8
10
12
14
16
Discount rate
0
−2500000
−2500000
−2500000
−2500000
−2500000
−2500000
−2500000
1
368130,3
361184,4
354495,8
348050,5
341835,3
335838,2
330047,8
2
353971,4
340740
328236,9
316409,5
305210,1
294594,9
284524
3
340357,1
321452,9
303923
287645
272509
258416,6
245279,3
4
327266,5
303257,4
281410,2
261495,5
243311,6
226681,2
211447,7
5
314679,3
286091,9
260565
237723,1
217242,5
198843,1
182282,5
6
302576,3
269898
241263,9
216111,9
193966,5
174423,8
157140,1
7
290938,7
254620,8
223392,5
196465,4
173184,4
153003,3
135465,6
8
279748,8
240208,3
206844,9
178604,9
154628,9
134213,5
116780,7
9
268989,2
226611,6
191523,1
162368,1
138061,5
117731,1
100673
10
258643,5
213784,5
177336,2
147607,4
123269,2
103272,9
86787,06
11
248695,6
201683,5
164200,2
134188,5
110061,8
90590,26
74816,43
12
239130,4
190267,5
152037,2
121989,6
98269,47
79465,14
64496,93
13
229933,1
179497,6
140775,2
110899,6
87740,6
69706,27
55600,8
14
221089,5
169337,4
130347,4
100817,8
78339,82
61145,85
47931,72
S0
1544150
1058636
656351,5
320376,8
37630,66
-202074
-406726
Power System Planning: Part II
195

Table 7. Net present value method (Case 2)
Discount cash ﬂow
Project year
4
6
8
10
12
14
16
Discount rate
0
−3600000
−3600000
−3600000
−3600000
−3600000
−3600000
−3600000
1
368130,3
361184,4
354495,8
348050,5
341835,3
335838,2
330047,8
2
353971,4
340740
328236,9
316409,5
305210,1
294594,9
284524
3
340357,1
321452,9
303923
287645
272509
258416,6
245279,3
4
327266,5
303257,4
281410,2
261495,5
243311,6
226681,2
211447,7
5
314679,3
286091,9
260565
237723,1
217242,5
198843,1
182282,5
6
302576,3
269898
241263,9
216111,9
193966,5
174423,8
157140,1
7
290938,7
254620,8
223392,5
196465,4
173184,4
153003,3
135465,6
8
279748,8
240208,3
206844,9
178604,9
154628,9
134213,5
116780,7
9
268989,2
226611,6
191523,1
162368,1
138061,5
117731,1
100673
10
258643,5
213784,5
177336,2
147607,4
123269,2
103272,9
86787,06
11
248695,6
201683,5
164200,2
134188,5
110061,8
90590,26
74816,43
12
239130,4
190267,5
152037,2
121989,6
98269,47
79465,14
64496,93
13
229933,1
179497,6
140775,2
110899,6
87740,6
69706,27
55600,8
14
221089,5
169337,4
130347,4
100817,8
78339,82
61145,85
47931,72
S0
444149,7
−41364,3
−443649
−779623
−1062369
−1302074
−1506726
196
A. Demir and N. Hadžijahić

Discount factor, discount rate and annual net income for both cases are given in
Tables 4 and 5.
After performing calculations, the results of the discount cash ﬂows are shown
below in Tables 6 and 7. As a condition for the acceptability of the project is S0  0, it
is clear from the results that this project is proﬁtable for discount rates up to 12% in ﬁrst
case and 4% in the second case.
Net Present Value for different discount rates is presented in Fig. 3 for Case 1 and
Fig. 4 for Case 2.
5
Internal Rate of Return Method
Internal rate of return method is also called a method of proﬁt index. The internal rate
of return sometime known as yield on project is the rate at which an investment project
promises to generate a return during its useful life. It is the discount rate at which the
present value of a project’s net cash inﬂows becomes equal to the present value of its
Fig. 3. Time of return of invested costs for different discount rates (Case 1)
Fig. 4. Time of return of invested costs for different discount rates (Case 2)
Power System Planning: Part II
197

Table 8. Internal rate of return method (Case 1)
Discount cash ﬂow
Project year
4
6
8
10
12
14
16
Discount rate
0
−2500000
−2500000
−2500000
−2500000
−2500000
−2500000
−2500000
1
368076,9
361132,1
354444,4
348000
341785,7
335789,5
330000
2
353920,1
340690,6
328189,3
316363,6
305165,8
294552,2
284482,8
3
340307,8
321406,3
303879
287603,3
272469,5
258379,1
245243,8
4
327219
303213,5
281369,4
261457,6
243276,3
226648,3
211417
5
314633,7
286050,4
260527,2
237688,7
217211
198814,3
182256,1
6
302532,4
269858,9
241228,9
216080,6
193938,4
174398,5
157117,3
7
290896,5
254583,9
223360,1
196436,9
173159,3
152981,2
135445,9
8
279708,2
240173,5
206814,9
178579
154606,5
134194
116763,7
9
268950,2
226578,7
191495,3
162344,6
138041,5
117714
100658,4
10
258606
213753,5
177310,5
147586
123251,4
103257,9
86774,48
11
248659,6
201654,3
164176,4
134169,1
110045,9
90577,13
74805,59
12
239095,8
190239,9
152015,1
121971,9
98255,23
79453,62
64487,58
13
229899,8
179471,6
140754,8
110883,5
87727,88
69696,16
55592,74
14
221057,5
169312,8
130328,5
100803,2
78328,46
61136,98
47924,77
IRR
7,97%
5,93%
3,97%
2,08%
0,26%
-1,50%
-3,20%
198
A. Demir and N. Hadžijahić

Table 9. Internal rate of return method (Case 2)
Discount cash ﬂow
Project year
4
6
8
10
12
14
16
Discout rate
0
−3600000
−3600000
−3600000
−3600000
−3600000
−3600000
−3600000
1
368076,9
361132,1
354444,4
348000
341785,7
335789,5
330000
2
353920,1
340690,6
328189,3
316363,6
305165,8
294552,2
284482,8
3
340307,8
321406,3
303879
287603,3
272469,5
258379,1
245243,8
4
327219
303213,5
281369,4
261457,6
243276,3
226648,3
211417
5
314633,7
286050,4
260527,2
237688,7
217211
198814,3
182256,1
6
302532,4
269858,9
241228,9
216080,6
193938,4
174398,5
157117,3
7
290896,5
254583,9
223360,1
196436,9
173159,3
152981,2
135445,9
8
279708,2
240173,5
206814,9
178579
154606,5
134194
116763,7
9
268950,2
226578,7
191495,3
162344,6
138041,5
117714
100658,4
10
258606
213753,5
177310,5
147586
123251,4
103257,9
86774,48
11
248659,6
201654,3
164176,4
134169,1
110045,9
90577,13
74805,59
12
239095,8
190239,9
152015,1
121971,9
98255,23
79453,62
64487,58
13
229899,8
179471,6
140754,8
110883,5
87727,88
69696,16
55592,74
14
221057,5
169312,8
130328,5
100803,2
78328,46
61136,98
47924,77
IRR
1,74%
-0,18%
−2,03%
−3,81%
−5,53%
−7,18%
−8,78%
Power System Planning: Part II
199

net cash outﬂows. In other words, internal rate of return is the discount rate at which a
project’s net present value becomes equal to zero [6].
Formula of internal rate of return factor:
IRR ¼ net initial investment
annual cash inflow
ð4Þ
Results of the IRR calculation for both cases are presented in Tables 8 and 9.
6
Proﬁtability Index Method
The Proﬁtability Index Method is an extension of the Net Present Value Method. It
provides comparative proﬁtability among different investments by dividing the present
value of future cash ﬂows by a project’s initial investment [6].
It represents an additional criterion in decision making and takes into account the
time value of money:
PI ¼
PT
t¼1
Vt
1 þ k
ð
Þt
I0
ð5Þ
Selection criterion: PI [ 1,max PI
Results of the PI calculations are given in Tables 10 and 11 for two different
investments costs [5].
Table 10. Proﬁtability index method (Case 1)
Discount rate 4
6
8
10
12
14
16
368076,9 361132,1 354444,4
348000 341785,7 335789,5
330000
353920,1 340690,6 328189,3 316363,6 305165,8 294552,2 284482,8
340307,8 321406,3
303879 287603,3 272469,5 258379,1 245243,8
327219 303213,5 281369,4 261457,6 243276,3 226648,3
211417
314633,7 286050,4 260527,2 237688,7
217211 198814,3 182256,1
302532,4 269858,9 241228,9 216080,6 193938,4 174398,5 157117,3
290896,5 254583,9 223360,1 196436,9 173159,3 152981,2 135445,9
279708,2 240173,5 206814,9
178579 154606,5
134194 116763,7
268950,2 226578,7 191495,3 162344,6 138041,5
117714 100658,4
258606 213753,5 177310,5
147586 123251,4 103257,9 86774,48
248659,6 201654,3 164176,4 134169,1 110045,9 90577,13 74805,59
239095,8 190239,9 152015,1 121971,9 98255,23 79453,62 64487,58
229899,8 179471,6 140754,8 110883,5 87727,88 69696,16 55592,74
221057,5 169312,8 130328,5 100803,2 78328,46 61136,98 47924,77
sum
4043563
3558120
3155894
2819968
2537263
2297593
2092970
PI
1,617425 1,423248 1,262358 1,127987 1,014905 0,919037 0,837188
200
A. Demir and N. Hadžijahić

7
Conclusion
In this paper it is shown that transition from 10 to 20 kV voltage level for investment
costs (2 500 000,00 and 3 600 000,00 km) is cost-effective.
In Case 1 where investment costs are 2 500 000,00 km the time of return of
invested costs is 7 years. Discount rates up to 12% are allowed. For these discount rates
IRR is positive and PI is greater than 1.
For Case 2 investment costs are 3 600 000,00 km and the time of return of invested
costs is 10 years. Discount rates up to 4% are allowed. For these discount rates IRR is
positive and PI is greater than 1.
Comparing these two cases it can be concluded that Case 1 is more acceptable from
the aspect of cost-effectiveness but it may require additional investments for the
upcoming period of time unlike the Case 2 where elements of the system will last
longer since they are replaced completely. For the end, investment costs of 3 600
000,00 km are likely to be the only costs for the next 30–40 years.
References
1. Seiﬁ, H., Sepasian, M.S.: Electric Power System Planning: Issues, Algorithms and Solutions.
Springer-Verlag, Heidelberg (2011)
2. JP EP BiH, Analiza razvoja srednjenaponskih distributivnih mreža u JP Elektroprivreda BiH
d.d Sarajevo sa aspekta prelaska na 20 kV naponski nivo (2013)
Table 11. Proﬁtability index method (Case 2)
Discount rate 4
6
8
10
12
14
16
368076,9 361132,1 354444,4
348000 341785,7 335789,5
330000
353920,1 340690,6 328189,3 316363,6 305165,8 294552,2 284482,8
340307,8 321406,3
303879 287603,3 272469,5 258379,1 245243,8
327219 303213,5 281369,4 261457,6 243276,3 226648,3
211417
314633,7 286050,4 260527,2 237688,7
217211 198814,3 182256,1
302532,4 269858,9 241228,9 216080,6 193938,4 174398,5 157117,3
290896,5 254583,9 223360,1 196436,9 173159,3 152981,2 135445,9
279708,2 240173,5 206814,9
178579 154606,5
134194 116763,7
268950,2 226578,7 191495,3 162344,6 138041,5
117714 100658,4
258606 213753,5 177310,5
147586 123251,4 103257,9 86774,48
248659,6 201654,3 164176,4 134169,1 110045,9 90577,13 74805,59
239095,8 190239,9 152015,1 121971,9 98255,23 79453,62 64487,58
229899,8 179471,6 140754,8 110883,5 87727,88 69696,16 55592,74
221057,5 169312,8 130328,5 100803,2 78328,46 61136,98 47924,77
sum
4043563
3558120
3155894
2819968
2537263
2297593
2092970
PI
1,123212 0,988367 0,876637 0,783324 0,704795
0,63822 0,581381
Power System Planning: Part II
201

3. Ćalović, M.S., Sarić, A.T.: Planiranje elektreoenergetskih sistema I dio. Beopres, Beograd
(2000)
4. Škokljev, I.: Planiranje elektroenergetskih sistema. Taurus Publik, Beograd (2000)
5. Avdaković, S.: Predavanja iz predmeta Planiranje elektroenergetskih sistema. Elektrotehnički
fakultet u Sarajevu, Sarajevo (2016). http://www.accountingformanagement.org/internal-rate-
of-return-method
6. Bacon, C.J.: The use of decision criteria in selecting information systems/technology
investments. MIS Quart. 16(3), 335–353 (1993)
202
A. Demir and N. Hadžijahić

Identiﬁcations of Power System Dominant
Low-Frequency Eletromechanical Oscillations
Using Hilbert Marginal Spectrum
Maja Muftić Dedović(&) and Samir Avdaković
Faculty of Electrical Engineering, University of Sarajevo, Sarajevo,
Bosnia and Herzegovina
maja.muftic-dedovic@etf.unsa.ba
Abstract. Identiﬁcation
of
low-frequency
electromechanical
oscillations
(LFEOs) in a power systems is an important aspect of modern systems for
monitoring, control and protection. Generally, available signals from power
systems are nonlinear and non-stationary, and their treatment requires adequate
mathematical techniques. One of the most popular technique for time-frequency
signal analysis is the Hilbert-Huang transform, with a very successful applica-
tion in different ﬁelds of science. In this paper, the Hilbert marginal spectrum
(HMS) obtained by the Empirical Mode Decomposition (EMD), which sepa-
rates the signal into several Intrinsic Mode Functions (IMFs), is applied for the
identiﬁcation of the dominant LFEOs in a power system. Results of the HMS
approach are tested in two examples and compared with the results obtained
using the global wavelet spectrum (GWS) approach.
Keywords: Power system  Low-Frequency electromechanical oscillations
Empirical mode decomposition  Hilbert marginal spectrum
1
Introduction
Power system low-frequency electromechanical oscillations (LFEOs) are the result of
various events [1]. The LFEOs are mostly damped in character, but in certain situations
these oscillations can be undamped in the power system and can lead to the system
collapse. Therefore, special attention is paid to the identiﬁcation of these oscillations,
with a special focus on the inter-area oscillations. In literature, the LFEOs are mostly
divided into local and inter-area oscillations, with frequencies going up to 5 Hz.
Because of the power systems’ characteristics available signals are generally nonlinear
and non-stationary, and their processing requires appropriate mathematical techniques.
A special focus is placed on the time-frequency signal processing techniques, whose
results can yield useful conclusions, like the commencement of the disturbance, the
dominant frequency, the amplitude, etc. [1, 2]. There are several time-frequency
techniques such as the Short-Time Fourier transform (STFT), the Continuous and the
Discrete Wavelet Transform (CWT and DWT), the Hilbert-Huang Transform (HHT),
which are often used in order to analyze the available signal. A practical application of
various techniques of identifying and analyzing the LFEOs, readers can ﬁnd in [3]. One
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_17

of the most popular techniques for the analysis of non-stationary and nonlinear signals
and time series in the last ﬁfteen years is the HHT, and in this paper the HHT is used
for practical analyses [3–12]. In Refs. [4, 5] two different analytical approaches are
presented, with the ﬁrst being the empirical mode decomposition (EMD) of measured
power system oscillations and the second is based on the wavelet analyses. When the
same features appear in both techniques at the same time, they have similar, correct
results and it is recommended to use these two approaches to analyze rapid variations
in non-stationary systems with transitory incorrect occurrence. Also, the comparison
between those nonlinear and non-stationary techniques and conventional approaches is
given. In [6], the CWT and the HHT are applied to identify aspects of the system’s
dynamic behavior, even in cases where a power system dynamic characteristics change
several times due to load shedding and generation tripping operations [6].
Browne et al. [7] applied comparative assessment of the two techniques for solving
the dynamic power system modal identiﬁcation problem. The ﬁrst is Prony analysis and
assumes stationary signals, and the HHT, which is able to identify non-stationary
system behavior. They yield similar results for the stationary signal, but it is recom-
mended that the two techniques complement each other [7]. Laila et al. use reﬁned
HHT to characterize time varying, multicomponent inter-area oscillations. This char-
acterization of temporal behavior can be applied to a wide-variety of signals found in
large time-variant systems [8].
Applications of the HHT approach for denoising and detrending of measured
oscillatory signal is presented in [9]. The measured signal is decomposed into a set of
intrinsic mode functions (IMFs) by the EMD. Next, the IMFs are divided into three
parts according to the energy relations between IMFs and appropriate distribution in the
frequency domain, which considers the features of signal both in time and frequency
domain [9]. On the other hand, Nilanjan [10] uses instantaneous phase differences in
inter-area oscillations to track generator coherency using the HHT. The EMD calcu-
lates the instantaneous phase differences between dominant inter-area modes. This
technique has reﬂection in the application of the reduction of large dynamical systems
using empirical method with limited understanding of the system. Also, movements of
low frequency electromechanical oscillations (LFEOs) are presented in [11] for the
identiﬁcation of power system areas with coherent generator groups, applying the CWT
and the HHT approaches [11].
In this paper, the Hilbert marginal spectrum (HMS) is used for the identiﬁcation of
dominant LFEOs, which represents the distribution of the total amplitude (energy)
depending on the frequency. With respect to [12, 13], it can be concluded that this
approach has the higher resolution ratio and a clearer frequency presentation of the
analyzed signal with respect to the Fourier spectrum. The results obtained in practical
analysis are compared with Global Wavelet Spectrum aspect [6]. The Matlab codes
based on Refs. [14, 15, 17] are used for calculations.
The paper is organized as follows: Sect. 2 brieﬂy presents the HHT method and
calculation of the HMS. The practical analysis of the test system and the signals from
the real European system is presented in Sect. 3, while the conclusions are presented in
Sect. 4.
204
S. Avdaković and M. M. Dedović

2
Background
In this section, a brief review of the approach is based on [12–14]. The HHT performs
signal decomposition or the time series on a ﬁnite number of components. After
application of the EMD, the Hilbert transform is applied on every component of a
signal, and the instantaneous frequency is obtained. The EMD decomposes signal to a
ﬁnite number of Intrinsic Mode Functions (IMFs). The IMFs must meet two
requirements:
(i) For a full set of data the number of extreme and the number of zero-crossings
must either be equal or different by one at most.
(ii) The mean value envelope deﬁned by the local maxima and the envelope, which is
determined by local minimum is zero at any point.
Process of extracting an IMF is given with the following algorithm:
STEP-1: For original signal x(t) identiﬁes the extremes (both maxima and minima).
Because of physicality of the LFEOs in the power system, signals x(t) selected for
practical analyses are oscillations of active power and frequency and best represent the
dynamic electromechanical processes in an electric power system.
STEP-2: Use cubic spline functions for connecting local maxima and local minima
and thus generate the upper and lower envelopes, u(t) and d(t), respectively.
STEP-3: Determine the local mean:
m1 tð Þ ¼ ðuðtÞ þ dðtÞÞ=2
ð1Þ
STEP-4: IMF should have zero local mean:
h1 tð Þ ¼ xðtÞ  m1
ð2Þ
STEP-5: If h1(t) do not meet conditions of IMFs, procedure is repeating and h1(t) is
concerned with a new signal. Determinate h11(t):
h11 tð Þ ¼ xðtÞ  m11
ð3Þ
where m11 is mean of the upper and lower envelope of the signal h1(t).
STEP-6: Procedure is repeated k times, or until component h1k(t) is obtained, which
meets the conditions for the IMF.
In that case h1k(t) is designated as the ﬁrst IMF component c1(t), derived function
contains the highest frequency occurring in the analyzed signal. After identifying the
ﬁrst IMF, it is subtracted from the original signal. The result of subtraction is residual
and proceeding EMD procedure is treated as the original signal, which is subject to the
same process of ﬁltering. The procedure is repeated until you ﬁnd all the components
(ﬁnal residue should be constant or monotonic function), or until a pre-deﬁned con-
dition is fulﬁlled. At the end of the decomposition, the original signal has the following
form:
Identiﬁcations of Power System Dominant Low-Frequency
205

xðtÞ ¼
X
n
i¼1
ciðtÞ þ rnðtÞ
ð4Þ
where,
ciðtÞ—ith IMF, n is the number of intrinsic modes;
rnðtÞ—ﬁnal residual.
Having obtained the IMF component of analyzed signal, perform the Hilbert
transform on each component. If H[ci(t)] denotes HHT of ith IMF component, then the
analytical form of the signal ciðtÞ is formed by:
zi tð Þ ¼ ci tð Þ þ jH ci tð Þ
½
 ¼ aiðtÞej/iðtÞ
ð5Þ
ai tð Þ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ciðtÞ2 þ H ciðtÞ
½
2
q
ð6Þ
/i tð Þ ¼ arctan H ciðtÞ
½

ciðtÞ
ð7Þ
The instantaneous frequency is as shown:
wi tð Þ ¼ d/iðtÞ
dt
ð8Þ
If ai(t), hi(t) and xi(t) denote amplitude, phase and instantaneous frequency of
signal zi(t) respectively, original signal is as shown:
x tð Þ ¼ H x; t
½
 ¼ Re aiðtÞ expðj 
Z
xiðtÞdtÞ


ð9Þ
This denotes to The Hilbert-Huang spectrum. For this The Hilbert-Huang spectrum
Hilbert marginal spectrum is deﬁned as:
h x
ð Þ ¼
ZT
0
H x; t
½
dt
ð10Þ
where, T is the duration of the signal.
The Hilbert-Huang spectrum produces time-frequency distribution of amplitude,
while the Hilbert marginal spectrum represents distribution of amplitude (energy)
depending on the frequency [12–14]. Having in mind the research presented in Refs.
[12, 13], we can conclude that this approach has the higher resolution ratio and a
clearer presentation of the analyzed signal frequency components compared to the
Fourier spectrum.
206
S. Avdaković and M. M. Dedović

In this paper, the HMS obtained by the EMD is applied for the identiﬁcation of the
dominant LFEOs in a power system and results of the HMS approach, tested in two
examples, are compared with the results obtained using the global wavelet spectrum
(GWS) approach. Detailed mathematical elaboration of the CWT and the GWS readers
can ﬁnd in [11, 17], so it will not be presented in this paper. Generally, the CWT of
discrete signal is deﬁned as the time series or a signal convolution with a scaled and
translated version of the complex conjugate of a wavelet function [11]. The Morlet
function is selected for practical analyses, because it provides a great balance between
time and frequency localization. As a results of the CWT analysis, wavelet power
spectrum (WPS) of a signal x is obtained and presented on the time-frequency maps,
providing a lot of useful information. After obtaining the WPS, it is very practical to
show that information as an average value of result on the range of a scale or time by
deﬁning the GWS [11, 17].
3
Results from the Test System and Signals from Real System
The Kundur two-area test system is selected as a ﬁrst example of practical analysis and
identiﬁcation of the LFEOs ([4], pp. 813). This system consists of four generating units,
where 400 MW transmits from one area to another through two parallel lines. In the
analysis generators are equipped with automatic voltage regulators (AVR). The results
of the LFEOs for different control device characteristics are presented in [1], which is
conﬁrmed by the modal analysis too. Results of modal analysis will not be presented in
this paper, but it should be noted that in this test system inter-area mode frequency is
around 0.5 Hz. After the simulation of outage of the transmission line between Bus 7
and Bus 8 (one of the parallel lines), the oscillations occurred through the system. The
active power oscillation in p.u. is presented in Fig. 1, where the base power of the test
system is 100 MVA. Results of the signal processing using two selected approaches are
presented in Fig. 2. Using the CWT with the Morlet wavelet function, the GWS result
as time-averaged wavelet spectrum is presented in Fig. 2a, and it clearly identiﬁes the
dominant oscillatory mode at about 0.5 Hz, which conﬁrms the expected results.
Further, the results of the HMS approach are shown in Fig. 2b, where it can also be
concluded that the HMS approach for signal from Fig. 1, clearly identiﬁes the domi-
nant oscillatory mode of 0.5 Hz.
The second example is the analysis of signal frequencies obtained after the dis-
turbance in the Turkey power system in 2010 (Fig. 3). The signals are measured in
different parts of the European power system and obtained via implemented European
wide area monitoring system. Over the same signals different analysis are presented in
[11, 16].
Identiﬁcations of Power System Dominant Low-Frequency
207

0
5
10
15
20
25
30
2
2.5
3
3.5
4
4.5
5
Time (s)
 
 
P (p.u.)
Fig. 1. An active power oscillation on transmission line 7–8 after tie-line tripping
0
0.2
0.4
0.6
0.8
2
1
0.5
0.25
Hz
0
0.002
0.004
0.006
0.008
0.01
0
0.5
1
1.5
2
Hz
(a)
(b)
Fig. 2. a GWS and b HMS results of a signal analysis from Fig. 1
0
10
20
30
40
50
60
49.88
49.9
49.92
49.94
49.96
49.98
50
Time (s)
Hz
29.09.2010.
Germany
Tunisia
Turkey
Fig. 3. Frequency oscillations during outage of the generation unit in the Eastern Turkey [11,
16]
208
S. Avdaković and M. M. Dedović

After disturbances in the system (outage of the generator) an imbalance of active
power has occurred and frequency oscillations in different points of interconnection can
be seen in Fig. 3. After a time interval of a few seconds, disturbance is propagated
through the system and affected other parts of the interconnection. In addition to the
frequency changes in Turkey, the frequency changes in Germany and Tunisia are also
presented (Fig. 3). The analysis is performed on the frequency signals (which are
synchronously measured through existing Wide Area Monitoring System), and the
results are presented in Fig. 4.
Analyzing the results obtained from the processing signal frequency oscillations in
Turkey, it can be concluded that the dominant oscillatory modes for both approaches
are identiﬁed in the frequency range of up to 0.25 Hz. (Figure 4a, b). Both approaches
identify the same frequencies at about 0.14 and 0.09 Hz. The amplitude of the 0.14 Hz
frequency for both approaches is higher compared to the amplitude of 0.09 Hz fre-
quency. Time scope of occurrence of these frequencies and length of their duration can
be identiﬁed at the time-frequency maps (time-frequency representation) for both
approaches, but in this paper due to the large number of ﬁgures, these results are not
presented. By analyzing the signal measured in Germany using the GWS approach, it
0
1
2
3
4
5
x 10
-3
1
0.5
0.25
0.125
0.0625
Hz
0
0.2
0.4
0.6
0.8
1
x 10
-3
0
0.5
1
0
0.5
1
1.5
2
x 10
-3
1
0.5
0.25
0.125
0.0625
Hz
0
0.5
1
1.5
x 10
-3
0
0.5
1
0
0.002
0.004
0.006
0.008
0.01
1
0.5
0.25
0.125
0.0625
Hz
0
1
2
3
4
x 10
-3
0
0.5
1
(a)
(b)
(c)
(d)
(e)
(f)
Fig. 4. Result comparison for GWS and HMS approach: a GWS Turkey, b HMS Turkey,
c GWS Germany, d HMS Germany, e GWS Tunisia and f HMS Tunisia
Identiﬁcations of Power System Dominant Low-Frequency
209

can be concluded that the dominant frequency is identiﬁed at about 0.17 Hz. A more
detailed analysis of the time-frequency map found that this frequency occurred
immediately after the disturbance and soon disappeared. The HMS approach identiﬁed
the frequencies around 0.09 Hz (Fig. 4d) and in this case results based on the GWS
approach are different compared to the HMS approach. The higher oscillations after
disturbance propagation through the observed power system are identiﬁed in Tunisia,
which is evident in Fig. 3. It is clear, for both approaches, that the dominant fre-
quencies are about 0.14 Hz (Fig. 4e, f). Generally from previous analyses, it can be
concluded that the HMS approach for analyzing signals from the real power system and
comparing them with the GWS approach, correctly identiﬁed the dominant oscillatory
modes.
4
Conclusion
In this paper, the HMS approach is applied to identify the dominant LFEOs. The results
obtained from the HMS approach are compared to the results of the GWS approach. By
practical signal analysis using the test system and the signal measured in real power
system, the HMS approach correctly identiﬁed the dominant oscillatory modes. In
general, both approaches (the CWT and the HHT) are excellent mathematical tools for
analyzing non-stationary and nonlinear signals and are very effective tools for the
localization of the frequency and amplitude of the observed signal in time. In the
context of identiﬁcation and analysis of the LFEOs, with special attention to inter-area
oscillations, both approaches in a time-frequency presentation of results, identify the
beginning of a certain disturbance, and from the CWT coefﬁcients or the IMFs, the
frequencies and their damping can be identiﬁed. The GWS and the HMS approaches
clearly distinguish dominant oscillation modes and can provide very useful information
for the operators of a power system
References
1. Kundur, P.: Power system stability and control. McGraw-Hill, New York (1994)
2. Bucciero, J., Terbrueggen, M.: Interconnected Power System Dynamics Tutorial. Third
Edition, EPRI (1998)
3. Messina, A.R.: Inter-area Oscillations in Power Systems -A Nonlinear and Nonstationary
Perspective. Springer-Verlag (2009)
4. Messina, A.R., Vittal, V., Heydt, G.T., Browne, T.J.: Nonstationary approaches to trend
identiﬁcation and denoising of measured power system oscillations. IEEE Trans. Power
Syst. 24, 1798–1807 (2009)
5. Messina, A.R., Vittal, V., Ruiz-Vega, D., Enriquez-Harper, G.: Interpretation and
visualization of wide-area PMU measurements using Hilbert analysis. IEEE Trans. Power
Syst. 21, 1763–1771 (2006)
6. Ruiz-Vega, D., Messina, A.R., Enriquez-Harper, G.: Analysis of inter-area oscillations via
non-linear time series analysis techniques. In: 15th PSCC Liege (2005)
210
S. Avdaković and M. M. Dedović

7. Browne, T.J., Vittal, V., Heydt, G.T., Messina, A.R.: A comparative assessment of two
techniques for modal identiﬁcation from power system measurements. IEEE Trans. Power
Syst. 23, 1408–1415 (2008)
8. Laila, D.S., Messina, A.R., Pal, B.C.: A reﬁned hilbert-huang transform with applications to
interarea oscillation monitoring. IEEE Trans. Power Syst. 24, 610–620 (2009)
9. Yang, D., Rehtanz, C., Li, Y., Liu, Q., Gorner, K.: Denoising and detrending of measured
oscillatory signal in power system. PRZEGLĄD ELEKTROTECHNICZNY (Electrical
Review) R. 88 NR 3b/2012: 135–139 (2012)
10. Senroy, N.: Generator coherency using the Hilbert-Huang transform. IEEE Trans. Power
Syst. 23, 1701–1708 (2008)
11. Avdakovic, S., Becirovic, E., Nuhanovic, A., Kusljugic, M.: Generator coherency using the
wavelet phase difference approach. IEEE Trans. Power Syst. 29, 271–278 (2014)
12. Huang, N., Shen, Z., Long, S., Wu, M., Shih, E., Zheng, Q., Tung, C., Liu, H.: The empirical
mode decomposition and the Hilbert spectrum for nonlinear and non-stationary time series
analyses. Proc. Royal Soc. London Ser. A-Math. Phys. Eng. Sci. A454, 903–995 (1998)
13. Kai, F., Jianfeng, Q., Chai, Y., Zou, T.: Hilbert marginal spectrum analysis for automatic
seizure detection in EEG signals. Biomed. Sig. Process. Control 18, 179–185 (2015)
14. Battista, B.M., Knapp, C., McGee, T., Goebel, V.: Application of the empirical mode
decomposition and Hilbert- Huang transform to seismic reﬂection data. Geophysics 72(2),
H29–H37 (2007)
15. Aguiar-Conraria, L., Azevedo, N., Soares, M.J.: Using wavelets to decompose the time–
frequency effects of monetary policy. Phys. A 387, 2863–2878 (2008)
16. Lehner, J.: Analysing inter-areas oscillations within the interconnected power system of
continental Europe using frequency domain simulations and signal analysis based on
wide-area measurement data. In: Proceedings IEEE International Energy Conference and
Exhibition (ENERGYCON), pp. 445–451 (2012)
17. Torrence, C., Compo, G.P.: A practical guide to wavelet analysis. Bull. Am. Meteorol. Soc.
79(1), 61–78 (1998)
Identiﬁcations of Power System Dominant Low-Frequency
211

Part II
Computer Science

Implementation of ICT in Education
Amina Deli´c-Zimi´c(B) and Naida Gadˇzo
Primary School “Aleksa ˇSanti´c”, Sarajevo, Bosnia and Herzegovina
minchysarajevo@hotmail.com, naidagadzo@yahoo.com
Abstract. In todays education system new techniques are being used,
as well as modern methods and means of work. ICT takes an impor-
tant a place in the educational process, as the application of modern
media in the lower and higher grades of primary school. The use of ICT
provides opportunities for every student to be more active and better
and to have greater motivation. Teaching process is more interesting and
clearer and is of high quality for any subject. A special place occupies
application of Microsoft tools which are important for many projects in
the teaching/entire teaching process. ICT provides a great advantage in
the preparation and organization of teaching classes, as well as teach-
ing the students, relatively using the modern technology in class, and
demands constant education of teachers by attending many seminars.
The use of ICT provides a greater performance, visual observation, bet-
ter perception and faster learning. Students are prepared with the help
of parents at home through creative work, they explore interesting top-
ics on the Internet. In the teaching process, teachers as assistants are
instructing students while they are doing their tasks. There is a possi-
bility of good interaction between teacher and student. Students have
the opportunity to prepare presentation or projects with the use of the
Internet, as well as to prepare school quizzes. They have the opportu-
nity to communicate with each other by e-mail, facebook, Sky Drive,
GeoGebra, Geometric Scatch Pade, Mindomo, interactive games. The
use of ICT has its disadvantages and advantages. Its disadvantages are
that it requires more preparation for teachers, more teaching materials
(crayons, school papers, etc.) and more time to prepare presentations on
CD. Today’s teaching process in the classroom is unimaginable without
the use of ICT in all primary schools.
Keywords: Distance learning · ICT (information comunication
technology) Primary school
1
Introduction
In this paper we will try to present the importance of quality application of
ICT in modern school. We will show quality and innovative examples of the
application of ICT in teaching, in which is very important and material and
c
⃝Springer International Publishing AG 2018
M. Hadˇzikadi´c and S. Avdakovi´c (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_18

216
A. Deli´c-Zimi´c and N. Gadˇzo
technical equipment of the school, or the existence of a modern and well equipped
classrooms in which students use actively tablet computers, projector and smart
board (smart panel). “In the whole history of teaching no media caused such
changes in terms of the eﬃciency of learning and learning methods as well as
in the terms of the position and role of students and teachers in the learning
process, nor contributed to the realization of permanent current aspirations of
a student to become the subject of the learning process, as computer did” [1].
We will show how modern information technology can contribute to quality-
teaching process, motivation of students, accessibility of knowledge, economy of
teaching, and realize other numerous beneﬁts of application of modern technol-
ogy [2].
The technology has a potential to remove barriers for students and educators
all over the world. Good software and the Internet change our approach to
knowledge. Innovative ways of teaching and learning in a new way deﬁne the
experience of learning and teaching. “And according to the students all these
are areas typical of the 21st century—and access to the learning tools that bring
closer these skills. Expectations: except the basic knowledge they need to know
how collaborate, communicate, and to use information” [3].
However, in highlighting the many advantages of the use of modern informa-
tion technology in teaching, we have to be careful and it should not allow the
technology in the classroom, its enchanting eﬀects, does not become an end in
itself, but to really be in function of teaching and contributes to the realization
of the set of educational goals.
“Education is required to be open and able to monitor and provide answers
to the scientiﬁc, social, technological and economic changes—it means that it
should create changes that will be in the service of a human, of their full devel-
opment and release” [4]. Some of these goals are to stimulate the interest of
young generations for new technologies and its application as a condition for the
development of computer literacy, which is certainly one of the most important
skills of the next century, allow equality and equal conditions for the acquisition
of knowledge and approach to information for each participants of the educa-
tional process, economize time and material costs of teaching, connect students
around the world, and in this way facilitate learning, encourage cooperation and
teamwork, create the conditions for setting the foundations of reforms of the
school system and the creation of “new” schools, schools for the child.
2
Pilot Project—Students of Grade IV
As a pilot project, the students of class IV are involved. The cabinet is equipped
with 5 tablets and wireless internet connection, electronic content and applica-
tions appropriate for the age of students. The teacher networked tablets (samba
client) with his computer, then made a unique e-mail address and in contacts
added students. Each time, when teacher shares a new document the students
arrive notice. The material is available to students at all times it is only necessary
that they apply to Hotmail [5].

Implementation of ICT in Education
217
They can put their works on SkyDrive at school and the works are available
everywhere in the world. In SkyDrive, students made one folder, which is pri-
vate and the other folder was public, shared documents with other pupils and
were able, in their native language, to control the security of their documents.
For a start, the documents are teaching papers and notes from class. Teachers,
students, classes and schools share teaching materials, collect them and, if neces-
sary, give permission to other users so that they can change the same document
or just read. Without tablet students and teachers would have to exchange doc-
uments and notes in writing, the teacher would have to deliver the document to
each student individually. Using the tablet would save funds that would be spent
copying or printing. The advantages of using tablet computers in education are
many and some of them are [6]:
• Listening and watching online lectures, speeches, plays, video recordings,
• At any time, over the Internet students can access electronic books and all
other useful information,
• Virtual visit to every museum, gallery or any other cultural and historical
attractions,
• While you are in Sarajevo, in a second you can ﬁnd any European or world
metropolis and get to know it,
• Teaching, watching movies or presentations online about all signiﬁcant works,
and with enjoyment and fun, learning about the lives and works of the most
prominent experts in various ﬁelds of art and science.
With the YouTube channel, each song or part of the novel will be closer and
more interesting [7].
3
The Case Study in Primary School “Aleksa ˇSanti´c”
Sarajevo Using Skype as a Medium of Communication
and as a Teaching Tool
M. M. is an excellent student who is for long time absent from school because
of a viral illness Varicella (varicella). Because of the health status of students
and inability to attend the classes there was a need to enable students to follow
the lessons from home. Within the project Examples of good practices realized
activity DISTANCE LEARNING (distance learning) with the help of students
from the class, which gave us full support and largely facilitated the realization
of demanding activities of this project.
“The dialogue and cooperation between teachers and students, directly or via
a computer educational program, represent the foundation of successful math
learning as well as a satisfactory math achievement” [1]. With Skype, which is
used for communication, student V. M. accomplished video call with student
M. M. After the call we greeted the student and began teaching. Teaching unit
that we studied was called “Communications Programs”. The teacher explained
the purpose of communication programs and wrote on the blackboard their divi-
sion [8].

218
A. Deli´c-Zimi´c and N. Gadˇzo
After that, the students got the task to investigate with the help of a teacher
on the Internet on communication programs. Communications Programs manage
the transfer of data between several connected computers, most users of comput-
ers are connected programs are via modem. Known communication programs as
MSN, Skype, ICQ, . . . .
Communication programs enable, in the ﬁrst place the ability to connect to
diﬀerent computer networks, not only the Internet, but also, for example, local
network computers in a company in which the user works. It is possible to per-
form various types of ﬁnancial transactions, such as orders of goods, paying bills
or making a reservation of airline tickets. Some communications programs can
be a function of answering machine. Using data communication applications it is
possible to transfer ﬁles from one computer to another by means of the telephone
network. Modern communications programs allow, also, receiving and sending
faxes. Multiple are the features and beneﬁts of implementation of activities of
this and similar projects that enrich the learning process and allow students who
cannot attend classes at school to learn and participate in the teaching process.
The project can and should be used in all schools that have equipped classrooms
with computer equipment.
This method of application of information and communication technology in
function of support the evaluation of the teaching process is useful and should
apply to all subjects. Participants in the project were teachers of informatics and
technical education. Inclusion of a large number of students in this project could
realize on the principle of conference call, which implies that classes could follow
at the same time increasing the number of students who are absent from school.
The lack of implementation of the teaching process is the lack of communication
skills, inadequate emotional reaction to the message and errors in perception.
Communication does not represent one-way ﬂow of information, but the phys-
ical absence of students would diﬃcult the inclusion of students in practical and
group work. To improve the process of learning and education of all students, it
is necessary to demonstrate to students the possibilities of the Internet through
establishing communication with teachers for help in solving tasks and other
work. The teacher as a leader of the teaching process should use the possibilities
that are oﬀered by this form of communication with students, and set tasks that
students need to make in order and in this way expand their knowledge and
apply adequate knowledge about the possibilities of search and retrieval and
relevant information on the Internet.
4
Information-Creative Workshop for Students of III
and IV Grade
Computer-creative workshop was organized within the project “Integration—
Support Roma education”, and the application of ICT and would like to achieve
the following objectives:

Implementation of ICT in Education
219
• awareness of the importance of respecting children’s rights;
• respect for the other and diﬀerent: children must not be discriminated because
of race, religion or ethnic origin;
• children should learn to respect diﬀerences as an acquired right, developing
awareness about tolerance;
• teach students to creative drawings in paint program to create mail address;
• meet student safety on the Internet.
The set and realized tasks are:
• Noticing with the help of a computer program Paint basic visual elements,
color lines, surfaces and volumes and their regulators: symmetry, rhythm
and composition as well as connecting with mathematics (geometric shapes,
surfaces and geometrical shapes)
• Developing artistic creativity and love for artistic expression, math through
computer program
• Find on the Internet
• Developing socio-emotional intelligence, respect for other
• Learning about respect of diﬀerences
• Build respect, tolerance and respect for other and diﬀerent
• Learn about the Children’s Rights
• Respect of children’s rights
• People should not be discriminated because of their skin color or ethnic origin
• Develop an awareness of the existence of diﬀerent cultures and races and that
people are diﬀerent because they are brought up in diﬀerent cultures
• Develop an awareness that it is normal that people diﬀer in their cultural
values
• To enable students to draw in Paint with the help of the characters that are
oﬀered in a drawing program, connect mathematical content with art with
the help of information technology,
• To enable students to create email addresses, use the Internet safely (Fig. 1).
Fig. 1. Examples of the implementation ICT in primary schools

220
A. Deli´c-Zimi´c and N. Gadˇzo
5
Using the Application Mindomo as Tools for Learning
and Systematization of Knowledge
Students of grade IV decided that speciﬁc themes from My environment subjects
are taught in a slightly diﬀerent, unusual way. They chose the online application
Mindomo, which allowed them to connect via mail in an interactive document,
mentioned applications, and that as part of the document creating a complex
and systematic mind map is a popular and eﬀective tool for learning. The appli-
cation allows to create a “skeleton” maps of choice and needs, the unlimited
space, and set diﬀerent types and forms of information: text ﬁles, video ﬁles,
photographs, drawings, videos, comments and etc. In this way, the students
explored the themes of Living creatures: plants and animals, and systematized
knowledge which is available in one document at any time (Fig. 2).
Fig. 2. Examples of the implementation the online application Mindomo
6
Conclusion
Our school system does not provide enough opportunities and constrains cre-
ativity of the students because the information are said in such a way that all
children can not adopt them. Those who fail are incapable of learning. They get
the message early that they will not succeed in life. Those who are identiﬁed
as above average, smart or talented, do not fare any better. On the other side

Implementation of ICT in Education
221
huge number of people through various types of games, social games and other
ways use the potential of computers and information technology more than their
teachers can imagine. The question is how to ﬁnd a way to get students’ com-
puter skills and other IT media to become an everyday certain skills which they
will use well in their school work. In this way, to ease your work improve the
skills and competencies of information literacy as a form of unavoidable liter-
acy for the 21st century. Students are accumulated more and more data with
little regard to their learning needs and at the expense of developing their nat-
ural talents and abilities. For young people who realize that it is impossible to
achieve this goal, the usual reaction is irresponsibility and even cynicism. Chil-
dren should be allowed to practice and experiment without fear of punishment
and mistakes. It also means that they have to have people who will support
them, guide and advocate for their learning process. The process of building
conﬁdence and moderation begins in childhood. When you respect the child’s
need of learning and when a parent and teacher dedicate time to help children to
learn about themselves and how they can learn best, they feel respected. When
we take into account the modalities and propensity for lessons and assignments
and when we encourage their talents and interests, children feel capable. When
students assess themselves, they set goals and track achievements, learn about
determination and positive thinking. When children have the people on their
side, they feel supported, capable and successful.
References
1. Peji´c, M.: Teorijske osnove primjene programiranog uˇcenja uz pomo´c kompjutora.
ˇSkola i razvoj, Petrinja (2004)
2. Adu, E.O.: The use and management of ICT in schools: strategies for school. Eur.
J. Comput. Sci. Inf. Technol. (EJCSIT) 1(2), 10–16 (2013)
3. Maly, D.: CEE Government Industry Manager—Congress. Microsoft (2012)
4. Mandi´c, D.: Imperativi obrazovanja informacione ere. TEPD (2011)
5. Dubai Cares Unicef: ˇSkole i zajednice po mjeri djeteta. Unicef, Sarajevo (2009)
6. Burgetti, M., et al.: 101 ideja za inovativne nastavnike Microsoft. Csaba Sarkadi
(2005)
7. Livazovi´c, G., Suˇcevi´c, M.: Online generacije i internetizacija obrazovanja–web
stranice ˇskola kao izazov internetizacije obrazovanja. ˇZivot i ˇskola 1(19), 79–86
(2008)
8. Previˇsi´c, V., ˇSoljan, N.N., Hrvati´c, N.: Pedagogija—prema cjeloˇzivotnom obrazo-
vanju i druˇstvu znanja, svezak 2. Hrvatsko pedagogijsko druˇstvo (2008)
Links
9. http://www.eajournals.org
10. http://www.webitcongress.com/en/eGov/2010/daniel maly.html
11. http://www.microsoft.com/scg/obrazovanje/pil/materijali/default.mspx
12. http://www.skolskidnevnik.net/?p=689
13. www.lugram.net/od igr do rac.html

222
A. Deli´c-Zimi´c and N. Gadˇzo
14. http://portal.skola.ba/
15. http://www.razredna-nastava.net/
16. http://www.artrea.com.hr/
17. http://math.arizona.edu/∼atpmena/conference/proceedings/Damodharan Inno
vative Methods.pdf
18. http://www.ascd.org/publications/educational
19. https://en.wikipedia.org/wiki/Computer network

Modern Teaching Approaches
Amina Deli´c-Zimi´c(B)
Primary School “Aleksa ˇSanti´c”, Sarajevo, Bosnia and Herzegovina
minchysarajevo@hotmail.com,aminadeliczimic@gmail.com
Abstract. Implementation of modern media and information technol-
ogy in primary schools has been increasing. Important conclusions are
economy of time and proper guidance of students in use of ICT, and
the constant training of teachers for lifelong learning. The use of ICT
provides opportunities for every student to be more active and better
and to have greater motivation. Teaching process is more interesting and
clearer and is of high quality for any subject. A special place occupies
application of Microsoft tools which are important for many projects in
the teaching/entire teaching process. ICT provides a great advantage in
the preparation and organization of teaching classes, as well as teach-
ing the students, relatively using the modern technology in class, and
demands constant education of teachers by attending many seminars.
They have the opportunity to communicate with each other by e-mail,
twitter, facebook, Sky Drive, Geometric Scatch Pade, GeoGebra etc.
Keywords: Modern media · ICT (information and communication
technologies) · Microsoft tools · GeoGebra · Motivation
1
Introduction
“In the whole history of teaching no media caused such changes in terms of the
eﬃciency of learning and learning methods as well as in the terms of the position
and role of students and teachers in the learning process, nor contributed to the
realization of permanent current aspirations of a student to become the subject
of the learning process, as computer did” [1].
The use of computers as a learning medium with precisely deﬁned steps,
information removal phase, asking questions and exercise testing, represents a
signiﬁcant advance in technology education. The use of computers in the educa-
tional process indicates a change in the way students learn. By using a computer
while learning motivation level gets higher and learning goes faster. The key ele-
ment of any multimedia application is embedded interactivity [2].
c
⃝Springer International Publishing AG 2018
M. Hadˇzikadi´c and S. Avdakovi´c (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_19

224
A. Deli´c-Zimi´c
It prevents users from being passive by including them in the process of learn-
ing and encouraging them to ﬁnd a growing number of information. This ensures
a high-quality learning process. A computer and its programs oﬀer a teacher the
following: rich multimedia presentations (illustrations, animations, simulations,
movies, 3D models. . . ), qualitative computational tasks to check what has been
learned as well as a high-quality feedback (auditory, text, image, graded and
ﬂexible feedback. . . ), “and ﬁnally it oﬀers a guide through the learning process
as well as it gives animated motivational examples” [3]. The use of ICT in the
teaching process has always been a challenge for teachers, as we strive to make
it easy for students to acquire certain knowledge as well as we strive to facilitate
the teaching process in classroom.
“Education is required to be open and able to monitor and provide answers to
the scientiﬁc, social, technological and economic changes—it means that it should
create changes that will be in the service of a human, of their full development
and release” [4].
This paper concentrates on two examples of the use of ICT in classroom
through the implementation of two projects, titled: “Water is the source of life”
and “GeoGebra”.
2
The Implementation of the Projects in Our Primary
School
2.1
Example 1
As for the project “Water is the source of life,” we decided to point out to our
students both the importance and the signiﬁcance of this constituent element in
life of humankind. The project “Water is the source of life” is realized through
the third grade primary school curriculum content program at Aleksa ˇSanti´c
Primary School in Sarajevo, as well as through the Ecology project. Worksheets
for group work were prepared in Microsoft Oﬃce Word. The illustrations with
the work assignments on are handled with the tools in Microsoft Oﬃce Word
and CorelDRAW.
The ﬁrst group: Find the diﬀerences between these two drawings. Write
how people can save water. As a reward for your answer—you can color the
drawing (Figs. 1 and 2).

Modern Teaching Approaches
225
Fig. 1. Group assignment-a girl with an umbrella
Fig. 2. A sample of students work
The second group: A line connecting the numbers given from 1, 2, 3 and
so on draw. In how many ways can people use water and for which purpose is it
used? Do not forget to colour the drawing (Figs. 3 and 4).

226
A. Deli´c-Zimi´c
Fig. 3. Group assignment-connect the lines
Fig. 4. A sample of students work
The third group: A line connecting the numbers given from 1, 2, 3 and
so on draw. You have recognized the numbers very well. How you would help
the ﬁsh to live in clean water. Write do you know any kinds of ﬁsh? Write their
names. Finally, the little ﬁsh will be happy when you colour it! (Figs. 5 and 6).

Modern Teaching Approaches
227
Fig. 5. Group assignment-dolphin, starﬁsh, jellyﬁsh
Fig. 6. A sample of students work
An interesting thing: How does water aﬀect our health? Use of the
Internet
We can be considered fortunate to have an access to clean drinking water
that is safe to use, which is not often the case in poor countries [5]. In Africa
and Asia, nearly three-quarters of the population does not have clean drinking
water. Approximately 5 million people die each year from diseases transmitted
by ﬁlthy water [6]. We have to keep our sources of drinking water clean—not
only because of the protection of our environment, but also because of our health
protection! There was a short video clip found on YouTube about planet Earth,
which the students watched by using (Fig. 7).

228
A. Deli´c-Zimi´c
Fig. 7. Students watching video clip about planet Earth
CyberLink Power DVD (they saw the position of the Earth in the universe
as well as the layer—the so-called “blue planet”, and plants and animals). Also
there was a multimedia presentation on marine animals (found on the Internet
and processed in Microsoft PowerPoint) and a puzzle (the image found on the
Internet and processed in Microsoft Oﬃce Picture Manager) (Fig. 8).
The students with the help of their parents learned to deal with some research
activities such as discovering any news and contents about water on the Internet
(artistic and literary works, posters, drawings, newspaper articles are put on the
school notice board).
Fig. 8. A mind map found on the internet and processed in Microsoft Oﬃce Word and
Microsoft Clip Organizer

Modern Teaching Approaches
229
2.2
Example 2
Project: Triangle in GeoGebra (the 4th and the 5th grade students) The project
idea was born during a math class, where students tried to draw, and measure
the structure of a triangle by using ruler and other devices such as a pair of
calipers (Figs. 9 and 10).
Fig. 9. Students work-ﬁfth grade
Fig. 10. Students work-fourth grade
During the classes when the main topic was triangle, we used available teach-
ing resources that could help students better appreciate and understand the fact,
that every triangle has three sides, three peaks and three angles, and that it
actually got its name according to these properties. In this project were used: a
multimedia presentation, a geometric body of a pyramid, an instructional plac-
ard about triangle and posters with tangram puzzles on them. The students put
the tans together in order to form various shapes [7].

230
A. Deli´c-Zimi´c
During the classes that followed, the students were instructed on how to
use software GeoGebra [8]. Using this program with the instructions that the
teachers gave them, the students worked practically during the whole class, and
they found out all the possible ways of the implementation of this interesting
software. In work groups, they drew many diﬀerent types of triangles as well as
they read algebraic expressions and had very precise drawings (Fig. 11).
In math teaching, computers have enabled an enhanced presentation of a con-
tent as well as they have enabled the implementation of completely new methods
of education, especially: introduction of audio and video clips, use of animations,
use of complex graphics (e.g. 3D representation of a model and structure), mul-
timedia display (the display of a content with a combination of diﬀerent media:
a combination of animation, sound and image), simulation models, interactive
approach, electronic interactive part of the contents of geometry and so on.
Fig. 11. Students working with GeoGebra software
The whole process of teaching using GeoGebra actually takes place in a form
of conversation between computers and students, and not in that kind of con-
versation in which the computers asks questions and the students only answers
them, but rather in a such one that resembles a real conversation between teach-
ers and students. The students’ in these math classes within the project, works
are considered very interesting in GeoGebra software, such as drawing a given
triangle with a description of its structure, dimensions, angles as well as with
the possibility of description inside and outside of the circle [9].
Each group of students (there were ﬁve groups) had a laptop with a previously
installed GeoGebra software, and after having been given precise instructions,
they started working in accordance with them—they drew a triangle (equilateral,
isosceles, scalene, rectangular and dull) noticing its peaks, its angles and its
dimensions.

Modern Teaching Approaches
231
3
Used GeoGebra Software
In math teaching at primary school, what is of great signiﬁcance is a dialogue
which must be clear and visual, because it allows students to experience their own
ways and means of solving math problems, spotting their mistakes in reasoning.
Students can be puzzled, if they are left to their own thoughts, because at a
certain point they do not know what to pay special attention to.
“The dialogue and cooperation between teachers and students, directly or via
a computer educational program, represent the foundation of successful math
learning as well as a satisfactory math achievement” [10].
The software of dynamic geometry is the “royal road” which leads into using
computers in teaching math [1]. All mathematical programs can be very useful
for teachers, eg. it can be used as an eﬀective tool in breaking the monotony of
exclusive use of chalk and boards in the process of learning and teaching math.
“The aim of the project Triangle in GeoGebra”: to raise the students
knowledge about triangle in general, so they can apply it practically, to draw,
move and calculate accurately the dimensions of the sides of any triangle using
the peaks and angles of it. Once the forth-and the ﬁfth-grade students are edu-
cated to use GeoGebra math software that joins geometry, algebra and analysis,
they will be able to use it for more eﬃcient studying, and will also be able to
pass their knowledge on to the next generation of students. Thus, this program
will become a reality in everyday teaching practice.
The following photos represent the activities that have been done
at our school (Figs. 12 and 13):
Fig. 12. Students using the GeoGebra software

232
A. Deli´c-Zimi´c
Fig. 13. Students work
4
Conclusion
Time in class is very important. The use of diﬀerent methods and techniques of
data, content and images processing with the help of this program, says enough
about the ease and the speed of preparation as well as of its eﬃciency and qual-
ity. We have combined diﬀerent types of methods that could not be used without
ICT. Preparing for this project without ICT would have certainly lasted much
longer, and would have been less “rich” in content, images and all of the above
mentioned components. Given that we have chosen the ﬁeld of “Water in our
country”, new technologies have enabled students to see and understand more
easily the concept of water. Colours and images that are enlarged and there-
fore, students are motivated to be actively involved in teaching. CD player, the
computer has replaced TV, DVD, an overhead projector, chalk, a blackboard,
pictures, educational posters, applications, didactic materials as well as it has
provided a better quality of both teaching and studying. During these projects,
our students were highly motivated for learning. They were active participants
and researchers. They connected experience with the use of ICT. Not only did
they develop the interest to expand the knowledge on water but they also learned
how to use ICT alone at home. Interesting a movie watching by using the pro-
jector, provides students with a more comfortable experience than watching the
same movie displayed on a TV screen, because the images are larger, the colour
is more vivid and so on. The eﬀect are of learning higher, but it also makes
students look forward to see or hear something new by using ICT. In the project
on GeoGebra, the cooperation between the students and their teachers, as well
as the cooperation among students themselves, were achieved at a higher level
due to their original approach to drawing a triangle, marking its sides, peaks

Modern Teaching Approaches
233
and angles. The knowledge acquired in this way of work is far more durable and
more applicable in practice. Students learned to link the knowledge acquired
from diﬀerent ﬁelds of science to what they have already known, so they are
encouraged to research more, and the possibility to use laptop is of a great help
to them. The implementation of ICT in subject teaching is considered absolutely
necessary when it comes to putting school education on a higher level of quality.
References
1. Peji´c, M.: Teorijske osnove primjene programiranog uˇcenja uz pomo´c kompjutora.
Hrvatsko pedagogijsko druˇctvo (2004)
2. Jaganjac, A.: ˇZivjeti u skladu sa okoliˇsem, priruˇcnik za edukaciju nastavnika (kako
razvijati svijest o okoliˇsu). Institut za hidrotehniku (2007)
3. Martinovi´c, D.: Prezentacija programa Geometr’s Sketchpad. GeoGebra (2011)
4. Duki´c, N.: Skripta HTML. Prirodno-matematiˇcki fakultet (2010)
5. Federalno ministarstvo turizma. Nauˇcno popularna revija o prirodi, ˇcovjeku i
ekologiji. Fondeko svijet (28) (2009)
6. JP za Vodno podruˇcje slivova rijeke Save. Voda i mi. ˇcasopis Javnog preduze´caza
Vodno podruˇcje slivova rijeke Save (95) (2017)
7. Glasnovi´c-Gracin, D.: Motivacija kao inspiracija. Miˇs, matematika i ˇskola 89, 146–
146 (2017)
8. Markus, H.: Pomo´c za program GeoGebra. Hrvatska verzija, GeoGebra (2015)
9. Livazovi´c, G., Suˇcevi´c, M.: Online generacije i internetizacija obrazovanja—web
stranice ˇckola kao izazov internetizacije obrazovanja. TMP (2007)
10. Peji´c, M.: Primjena kompjutora u cjeloˇzivotnom obrazovanju i druˇstvu znanja.
Hrvatsko pedagogijsko druˇstvo 2, 337–345 (2010)
Links
11. http://www.microsoft.com/scg/obrazovanje/pil/materijali/default.mspx
12. www.lugram.net/od igr do rac.html
13. http://mis.element.hr/
14. http://www.razredna-nastava.net/
15. http://www.artrea.com.hr/
16. http://www.activityvillage.co.uk/
17. www.youtube.com/SecretPlanet
18. https://www.geogebra.org/
19. http://www.irjes.com/Papers/vol4-issue2/

Ferris Wheel Tree
Dalila Isanovic(B)
Sarajevo School of Science and Technology, Sarajevo, Bosnia and Herzegovina
belma.ramic@ssst.edu.ba, dzejlam@gmail.com
Abstract. The goal of this paper is to describe the problem of data
structures sometimes having to keep track of the history of changes made
upon it, and a possible solution of this problem. Of course, numerous
solutions to this problem have already been implemented, however, this
paper oﬀers one that is uncomplicated, best suited for small scale appli-
cations. A new kind of binary tree is introduced—the Ferris wheel tree.
While being simple to implement, it achieves partial persistence, allow-
ing us to view particular versions of data. Each operation that can be
done on this tree is deﬁned and described. The runtime complexity of
those operations is analyzed and presented, and based on that, also the
variations and circumstances when this tree should best be used.
1
Introduction
Sometimes, when writing algorithms, a need arises for data structures that keep
track of the history of changes made upon it. There have been various ways in
which this problem was explored, and the goal of this paper is to introduce a
binary tree [5,9], the Ferris Wheel tree, that provides a simple solution, achieving
what is known as partial persistence.
There have been many solutions to this problem over the years, however,
we could not ﬁnd a very simple implementation, one that could be used in an
environment without huge amounts of data. Our goal here, then, would be to
design such a data structure.1
2
Related Work
The solution to the problem of keeping data structures persistent most similar
to the one described in this paper is the ’fat node’ method, as described in [6].
While both achieve partial persistence, there are some diﬀerences between these
1 Originally, the idea for this came up on the Data Structures course at Sarajevo
School of Science and Technology, when professor Dzejla Mededovic gave us an
assignment to think about and make a design for a new kind of binary search tree.
That assignment later expanded and with the professor’s assistance, turned into this
paper.
c
⃝Springer International Publishing AG 2018
M. Hadˇzikadi´c and S. Avdakovi´c (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_20

Ferris Wheel Tree
235
two approaches. The fat node method makes use of an array of pointers (as
timestamps) to keep track of data, while in this paper, that is done through one
pointer variable for the entire tree (this is discussed more in detail in the next
sections). As a consequence, the fat node approach has a worse access time of
versions than the Ferris wheel tree, but it oﬀers better modiﬁcation time.
We also made use of the well known balancing mechanism of the AVL tree,
as described in [7,10].
A similar solution could also be found in [3], which proposes a partially
persistent data structure with update and access time in O(1). While the Ferris
wheel tree does not have a constant running time, its implementation is much
simpler. The design of the Ferris wheel tree looks somewhat similar to that of the
B-tree [2], but they were made for diﬀerent purposes and have many diﬀerences.
The B-tree can have nodes with more than two children and has certain rules
regarding the number of children and keys, while the Ferris wheel tree is binary
and all nodes have the same number of array elements (this was decided for the
sake of simplicity).
Other papers, such as [8], also show implementations and usages of persistent
trees.
3
Ferris Wheel Tree Design
The tree has the structure of an ordinary binary search tree, except that each
node holds not only its current data, but also the past data it used to hold.
That is achieved by the node being an array structure, where the last element is
the newest value of the node and the rest of them represent the corresponding
versions of the node (let us assume that the array has a ﬁxed maximum size).
We also need an auxiliary variable that will hold information about the current
version of the tree and allow us to access any other version, too. This will be our
pointer variable.
Figure 1 illustrates how we can visualize the tree.
As the pointer of a node moves through it, the node can be seen as spinning
like a ferris wheel, hence the name.
Fig. 1. The Ferris wheel tree

236
D. Isanovic
We can also visualize the tree as nodes having diﬀerent levels, each of which
holds a value the node used to have. This means that we can look at the tree
one version at a time. If, for example, we are one the second level of the tree
(the second version of the nodes’ values, that is) the pointer of each node is set
to its second element (because there is only one pointer for the entire tree). This
is illustrated in Fig. 2.
Fig. 2. The Ferris wheel tree—level view
Figures 3 and 4 represent class deﬁnitions for the tree and its nodes.2
Fig. 3. The FerrisWheelTree class deﬁnition
2 Note: they contain only the basic attributes; for the actual implementation we would
have to add more operations common for binary search trees, that are irrelevant to
the topic of this paper.

Ferris Wheel Tree
237
Fig. 4. TreeNode class deﬁnition
3.1
The Binary Search Tree Property
As for the binary search tree property, it simply has to satisfy the rule that
the value of the left child must be smaller than the value of the root node, and
the value of the right child must be greater than the value of the root node.
This, however, would be done on the version level, that is, not every value of the
left child node will be smaller than every value of the root node, but only the
corresponding versions, or the pointer values. The binary search tree property
will hold true only for the latest version, dor reasons discussed later in the paper.
3.2
The Insert Operation
If a new node is to be initialized, it will be initialized with an array that has the
size of the current version of the tree (just like all the other nodes have), and all
the values will be set to null (assuming that they are automatically set to null).
Only the current version can be subject to change. The insert operation is similar
to the insert operation of an ordinary binary search tree and the pseudocode can
be seen below.
INSERT(TreeNode node, Key k)
if (node == null)
return new TreeNode(k);
if (k <= node.key)
node.left = INSERT(node.left, k);
else
node.right = INSERT(node.right, k);

238
D. Isanovic
3.3
The Lookup Operation
The lookup operation searches for a given value. Additional information, other
than the value we are searching for needs to be passed, which is the version of
the tree in which we are searching for the value. If no such information is passed,
it is to be assumed that we are searching for a value in the current version (the
version the pointer is set to, that is). The general pseudocode, where the function
takes the version number as an argument, can be seen below.
TREE_SEARCH(TreeNode node, k, version) {
if (node == NIL or k == node.value[version])
return node
if (k < node.value)
return TREE_SEARCH(node.left, k)
else
return TREE_SEARCH(node.right,k)
}
It is easy to modify the code to write a diﬀerent variants of this operation, in
particular searching for a value in the tree through a range of versions instead
of only one version.
3.4
The Update Operation
The update operation, commonly found among diﬀerent kinds of data structures
will not be available in the Ferris wheel tree in the traditional sense. A node’s
previous value should not be able to be modiﬁed, since we want to keep historical
data intact. But if we want to update the last version of a node, the pointer will
be incremented by one, and the new value will be written into the next element
of the node’s array. That way, we create a new version of the tree.
UPDATE(TreeNode node, new_value) {
node.value[++version] = new_value
}
Nonetheless, this approach has one setback, because after the operation is ﬁn-
ished on one node, the value of every other node will need to be copied into the
next version. Although it slows down the runtime of the operation, this solution
gives us beneﬁts concerning the version switch operation and the functions that
use it, and that can be seen in the following sections.
Additionally, a slight modiﬁcation in the code for this operation, where it
would allow for multiple nodes to be updated in one go, would prove to be useful
because it would reduce the number of nodes that need to copy their values onto

Ferris Wheel Tree
239
the next version. It would be preferred to use this tree in such situations, where
a number of objects is updated at the same time, instead of only one being
updated.
A ﬁnal variation of this code would be the lazy approach—after one update
operation, not copying the values of other nodes onto their new versions, but
leaving them null, until the point we use them, when we copy the values retroac-
tively. This is only to be used when some objects are expected not to be used as
much as others.
3.5
The Delete Operation
Since the tree needs to keep track of diﬀerent versions over time, it would not
make much sense to delete previous versions of individual nodes or of the entire
tree, especially since deleting one version of a node would result in not all nodes
having the same number of versions (which means they would not have an equal
array size), and that would complicate the performance of other operations that
can be done on this tree.
3.6
Switching Between the Versions
It will be possible to switch the tree to a diﬀerent version, so that each of its
nodes shows the value it held during the chosen version. This will simply be
done by changing the pointer value to the desired value. That way, all the other
operations that will be done after this one, will be done on the nodes’ version
which we selected. The version switch operation requires no extra space. The
pseudocode for this operation is as follows:
SWITCH(int new_version) {
version = new_version
}
Figure 5 shows a color-coded Ferris wheel tree with three nodes, each contain-
ing ﬁve values. Each node’s element is colored in its corresponding color. First,
the pointer was set to the ﬁrst array element, or the ﬁrst version. Now, if we
were to switch to the third version, the pointer would be set to the third element
of every node, as can be seen on the right part of the picture (this we visualized
as if the node were spinning around, just like on a ferris wheel, however, nothing
would really change place, the pointer would now only point to another value).
3.7
Rebalancing the Tree
While thinking of the best rebalancing mechanisms for this tree, a lot of already
existing ones have been explored, such as [1,4], but the AVL mechanism seemed
the most appropriate.

240
D. Isanovic
Fig. 5. Version switch
The rebalance operation is needed after inserting/updating a node in the
tree. It will be done the same way as it is done in an AVL tree, (doing certain
rotations based on the position of the node and the respective values of nodes
around it). However, rebalancing is done based only on the current version. This
is not a problem, since after rebalancing, all the nodes will still have all of their
previous versions, it is just the arrangement of the nodes that changes. This is
a trade-oﬀthat was chosen in favor of version management, the main feature of
this tree.
4
Analysis
Shown below are the worst-case runtimes of the operations described above.
The insert operation runs in O(logn) time.
The lookup operation also runs in O(logn) time.
The update operation itself takes O(logn) time, but after we perform it on
one node, we will have to copy the previous value of every other node into its
next version, which takes O(n) time.
The version switch operation takes O(1) time, since all we need to do is
change the value of the pointer.
Since the Ferris wheel is a binary search tree, the lookup operation is rela-
tively fast, in spite of each node having more versions, since accessing a certain
element in a node’s array is constant. The update operation is the bottleneck of
this structure, if we use it to update only one node. Still, if we update more than
one node at a time, the runtime will not be as bad, considering we are making
new versions on a number of nodes. As a trade-oﬀof that, the version-switch
operation, having constant time, is by far the fastest operation done on this data
structure.
When it comes to space this tree takes up, it needs O(nv) space, where n
is the number of nodes and v the number of versions stored, in addition to one
pointer variable for pointing to the current version that is being looked at, which
means that this data structure needs O(n) space.

Ferris Wheel Tree
241
5
Discussion
As a variation, it could have been possible to use a queue instead of an array
for each node of the tree, in case we wanted to save space and delete the oldest
versions once a maximum threshold in the number of versions has been reached.
Nevertheless, the array approach was chosen, not only because we are then able
to save all the versions of the tree (assuming we have enough memory), but also
because it is more suitable that it provides fast random access of elements. Next,
since the version switch operation is the fastest on this data structure, it should
be utilized as much as possible. For example, it would be fairly easy to print out
each value (or range of values) every node has ever had in O(nv) time, where
n is the number of nodes and v the number of versions (this would be done by
using nested for loops—one for traversing the nodes, the other for traversing the
array in each node).
6
Conclusion
In conclusion, the Ferris wheel tree is a relatively simple binary search tree to
implement, and contains basic operations that are performed on such trees, while
having some additional operations, which allow it to manage between particular
versions. The scenario where we would use this data structure would not be one
where it is expected to have a lot of insertions per only one node, but rather a
number of nodes, or even all the nodes.
The tree should best be used in an environment where read-only operations
are often performed (especially if we want to have a detailed overview of the
changes made upon the data), since it oﬀers better runtimes in such circum-
stances.
References
1. Baer, J.-L., Schwab, B.: A comparison of tree-balancing algorithms. Commun.
ACM 20(5), 322–330 (1977)
2. Bayer, R., McCreight, E.: Organization and maintenance of large ordered indices.
In: Proceedings of the 1970 ACM SIGFIDET (Now SIGMOD) Workshop on Data
Description, Access and Control, SIGFIDET70, pp. 107–141, New York, NY, USA.
ACM (1970)
3. Brodal, G.: Partially persistent data structures of bounded degree with constant
update time. BRICS Report Series 1(35) (1994)
4. Cannady, J.M.: Balancing methods for binary search trees. In: Proceedings of the
16th Annual Southeast Regional Conference, ACM-SE’16, pp. 181–186, New York,
NY, USA. ACM (1978)
5. Cormen, T.H., Stein, C., Rivest, R.L., Leiserson, C.E.: Introduction to Algorithms,
2nd edn. McGraw-Hill Higher Education (2001)
6. Driscoll, J.R., Sarnak, N., Sleator, D.D., Tarjan, R.E.: Making data structures
persistent. In: Proceedings of the Eighteenth Annual ACM Symposium on Theory
of Computing, STOC’86, pp. 109–121, New York, NY, USA. ACM (1986)

242
D. Isanovic
7. Foster, C.C.: A generalization of AVL trees. Commun. ACM 16(8), 513–517 (1973)
8. Sarnak, N., Tarjan, R.E.: Planar point location using persistent search trees. Com-
mun. ACM 29(7), 669–679 (1986)
9. Sedgewick, R., Wayne, K.: Algorithms, 4th edn. Addison-Wesley Professional
(2011)
10. Wright, W.E.: Dynamic binary search trees. In: Proceedings of the 1978 Annual
Conference, ACM’78, pp. 370–374, New York, NY, USA. ACM (1978)

Inﬂuence of Schemaless Approach
on Database Authorization
Dejan Radic(B)
Codaxy LLC, 78000 Banja Luka, Bosnia and Herzegovina
dejanradic@live.com
Abstract. NoSQL databases are oriented towards managing huge
amounts of data by distributing them over commodity servers. Diﬀer-
ent types of these databases exist, for diﬀerent use cases. Key/value
databases have the simplest interface. Document databases allow ﬁlter-
ing based on document contents. Column-family databases are oriented
towards column structuring. Their data models are simple and they do
not require schema speciﬁcation schemaless approach. Additionally, their
primary focus isn’t data security, which includes authorization. There are
diﬀerent levels on which authorization can be implemented. Only a few
NoSQL databases are meeting a common requirement of granular autho-
rization. Ways of mitigating authorization requirements on application-
level were listed and discussed.
Keywords: BigData · NoSQL · Schemaless databases · Security
Authorization
1
Introduction
BigData is increasingly popular term which describes problems of storing and
processing of huge data sets. Subset of those problems is eﬃciently solved
using NoSQL (Not Only SQL) databases. NoSQL systems are distributed, non-
relational databases designed for large-scale data storage for massively-parallel
data processing across a large number of commodity servers [1]. Moreover, as
opposed to relational databases, they trade consistency and security for per-
formance and scalability [2]. Unlike relational databases, NoSQL databases are
created with focus on data distribution and horizontal scalability. Thus, they usu-
ally don’t use SQL (Structured Query Language) as querying language. Three
most popular NoSQL databases (MongoDB, Cassandra and Redis) are in top
10 generally most popular database at the time of writing of this paper [3].
Amazon Dynamo and Google BigTable concepts brought to wider adoption of
NoSQL databases. Since the prime objective of NoSQL databases were eﬃcient
data storage and retrieval, core security features like data security techniques,
proper authentication mechanisms etc. were given least priority [4]. As a result,
c
⃝Springer International Publishing AG 2018
M. Hadˇzikadi´c and S. Avdakovi´c (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_21

244
D. Radic
many software architects implemented these features on application level. Over
time, security features have evolved and many NoSQL databases improved their
database level security mechanisms. Yet, many best practices from traditional
SQL databases are overlooked and the security of NoSQL deployments has not
matured enough [5]. This paper deals with authorization of these databases with
special focus their schemaless nature.
2
Types of NoSQL Databases
Three of the most widely adopted NoSQL database types are key/value data-
bases, document databases and column-family databases. Complex queries and
data analysis from diﬀerent aspects mostly belong to relational database domain.
Some applications need to get the data simple and eﬃciently, without the need
for data transformation. Key/value databases (such as Riak and Redis) store
key/value pairs, where value is accessed over immutable key (Fig. 1). That’s
equivalent with row access over primary key that is used by relational databases.
Key is usually a unique string and value is a BLOB (Binary Large Object) which
has semantic value to the client. Value contents can be anything and database
isn’t concerned with value structure. Actually, there are no limitations for value
contents except its size. This way of accessing data is available in programming
languages (maps, dictionaries or associative arrays). Key/value pairs are usu-
ally grouped into namespaces or buckets which oﬀer the possibility of logical
separation.
Fig. 1. Key/value database logic
Sometimes it is needed to ﬁlter the values according to speciﬁed criteria.
Document databases (such as MongoDB) pair each key with a complex data
structure known as a document [6]. Document contains ﬁelds and other doc-
uments (called sub-documents) (Fig. 2). Unlike key/value databases, document
databases allow ﬁltering of documents by ﬁelds (restriction), which implies that
documents are transparent. Documents have hierarchical structure, because they
contain other documents. Hierarchical structures can be represented with dif-
ferent data formats: XML (Extensible Markup Language), JSON (JavaScript
Object Notation), YAML (Yet Another Markup Language), BSON (Binary

Inﬂuence of Schemaless Approach on Database Authorization
245
Fig. 2. Document database high-level class diagram
JSON) etc. Those data formats can be used as a logical reference for users
of document databases.
Column-family databases (such as Cassandra or HBase) organize the data
into rows and columns, which was initially inspired by Google BigTable concept.
All rows don’t need to have the same columns (Fig. 3). If there are many columns
and rows have only some needed columns, than data is sparse. The real power of
this model is in denormalized approach to structuring sparse data [7]. Row is a
collection of columns which are connected together and identiﬁed by a row key,
which is similar like key/value databases have. Rows are logically grouped into
tables or keyspaces. Multiple similar columns are grouped into a column family.
Cell is a combination of row, column family and row. Document databases are
diﬀerent from key/value database, because they allow selection or restriction.
Column-family databases are diﬀerent from document databases, because they
allow eﬃcient projection operation.
Fig. 3. Column-family database organization
3
Schemaless Approach
Most NoSQL databases don’t require schema speciﬁcation before writing the
data. That means their query language doesn’t have DDL (Data Deﬁnition Lan-
guage) commands, like relational databases have, but only DML (Data Manip-
ulation Language) commands. Allegedly, schemaless approach allows ﬂexibility,
because knowing upfront what has to be stored is not a requirement. A key/value
database allows storing any data under a key [8]. A document database eﬀec-
tively does the same thing, since it makes no restrictions on the structure of the

246
D. Radic
documents. Two documents that belong to the same collection can have diﬀerent
ﬁelds or sub-documents. When using column-family databases, rows don’t need
to have the same columns. Schemaless approach helps when nonuniform data is
stored. For example, relational databases would have many null value columns
when storing that kind of data. Almost all applications access the data with
some kind of implicit schema. Applications make assumptions about ﬁeld names
and generally the structure of the data in a database. Problems can be caused if
multiple applications access the same schemaless database, because of possible
diﬀerent structure assumptions.
4
Authorization Granularity
Granularity can be deﬁned as a scale or level of detail a phenomenon has, which
is database authorization in this thematic. SQL (Structured Query Language)
has DCL (Data Control Language) commands like grant and revoke which can
give or take permissions on certain resources or database objects. It is possible
to regulate permission on database, table and even column-level (many popu-
lar relational databases including MySQL, PostgreSQL and SQL Server have
column-level permissions). For example, if permission regulation can be done on
table-level and not on lower levels, it is said that database has coarse-grained
authorization. On the other hand, the ability to set permission on column-level
means that database has ﬁne-grained authorization.
NoSQL databases have simpler data model. A common theme across all the
forms of NoSQL databases is that they are schemaless [8]. Key/value databases
like Riak usually allow setting permissions on a bucket or namespace level [9].
Permission cant be set on key/value pair level. MongoDB, as a document data-
base representative, has collection-level permissions [10], but no permissions on
a ﬁeld level. Column-family database representatives are Cassandra and HBase.
Starting CQL (Cassandra Query Language) version 3 it is required columns to be
declared before used, so Cassandra is not anymore schemaless [11]. Apart from
that column-level permissions are being implemented in Cassandra at the time
of writing of this paper [12]. On the other hand HBase is a schemaless database,
but it has experimental cell-level permissions [13]. That means implicit schema
is being assumed by the database itself, despite being schemaless.
More complex data models require more complex authorization mechanisms.
Granularity is measured relative to data model complexity. Fine-grained autho-
rization is important because enterprises require that authorization follows the
least privilege principle that states: Every program and every user of the sys-
tem should operate using the least set of privileges necessary to complete the
job [14]. The reason for aiming this principle is avoidance of privilege escalation
attacks. The downside of this is, if not implemented carefully, authorization can
negatively impact the performance of large queries [15].
Permission or authorization models should be diﬀerentiated. The most
famous permission model is UNIX permission model that proposes RWX (read,
write, execute) permissions. Relational databases have CRUD (create, read,

Inﬂuence of Schemaless Approach on Database Authorization
247
update, delete) permission model based on their DML commands (create, insert,
update, delete). For example, select permission on database can be granted to a
user. Riak uses HTTP (Hypertext Transfer Protocol) methods permission model
(get, put, delete). An alternative authorization model that associates permissions
with roles that has turned out to be scalable and predominant choice for large
organizations is called RBAC (Role Based Access Control) [15].
5
Mitigation
As it was stated, NoSQL databases mostly have permissions at bucket or
namespace, collection or table level. It is common that enterprises require ﬁner-
grained authorization than at mentioned levels. Consequently, it means that
authorization should be done at lower levels like ﬁelds, values and cells. Append-
ing key/value par, document or row with user or role identiﬁer, like presented
in [5] doesn’t solve lowest level authorization problem. Additionally, document
databases have hierarchical structure that adds extra complexity. If lower level
authorization rules are enforced at database level, schemaless databases are
assuming implicit schema.
One of mitigation options is to use the same way of solving these problems
like relational databases. Many NoSQL databases have views which can be used
to select subset of ﬁelds or columns. Aggravating circumstance is that some
databases lack authorization mechanisms for views. In that case, application
level authorization is the only option. One of the most diﬃcult questions for an
application architect is whether access can be controlled at the application level,
instead of the database level [15]. If database is accessed by a single applica-
tion, controlling the access on application level deﬁnitely gives more ﬂexibility.
Enforcing access control at database level is recommended when there are mul-
tiple applications (including administrative and reporting tools). The usual way
of overcoming these obstacles at application level is manual creation of users and
roles management. In addition, application can request database level projection,
or it can be done when executing deserialization or conversion. The last option
is to simulate views by creating separate collections, buckets or keyspaces for
diﬀerent roles. This way requires that application takes care of synchronization
or maintenance of those copies.
6
Conclusion
Managing huge data volumes is becoming a global challenge. NoSQL databases
are used for meeting those challenges. Security mechanisms of those databases
have improved over time, including authorization mechanisms. Key/value, doc-
ument and column-family databases have diﬀerent logical models, thus, they
require diﬀerent authorization options. The common ground for these databases
is that they are schemaless. Applications that work with schemaless databases
usually have some kind of implicit schema. Fine-grained authorization is a usual

248
D. Radic
request, but not all databases are eﬃciently responding to that request. Data-
bases with ﬁne-grained authorization, which are following schemaless approach
are making assumptions about the schema as applications do. Consequently,
they are internally assuming implicit schema. If database doesn’t have internal
ﬁne-grained authorization mechanisms, that problem can be mitigated on appli-
cation level. That adds extra complexity to the application, but, on the other
hand gives more ﬂexibility.
References
1. Moniruzzaman, A.B.M., Hossain, S.A.: NoSQL database: new era of databases for
big data analytics-classiﬁcation, characteristics and comparison. Int. J. Database
Theory Appl. 6(4) (2013)
2. Okman, L., Gal-Oz, N., Gonen, Y., Gudes, E., Abramo, J.: Security issues in
NoSQL databases. In: International Joint Conference of IEEE TrustCom-11/IEEE
ICESS-11/FCST-11 (2011)
3. Solid IT: DB-Engines. http://db-engines.com/en/. Accessed 22 Feb 2017
4. Sathyadevan, S., Muraleedharan, N., Rajan, S.P.: Enhancement of data level secu-
rity in MongoDB, intelligent distributed computing. In: International Symposium
on Intelligent Informatics (ISI-2014), Delhi, India (2014)
5. Ron, A., Shulman-Peleg, A., Bronshtein, E.: No SQL, no injection? Examining
NoSQL security. In: Proceedings of the 9th Workshop on Web 2.0 Security and
Privacy (W2SP) (2015)
6. MongoDB, Inc.: Types of NoSQL Databases. https://www.mongodb.com/scale/
types-of-nosql-databases. Accessed 22 Feb 2017
7. McMurtry, D., Oakley, A., Sharp, J., Subramanian, M., Zhang, H.: Data Access
for Highly-Scalable Solutions: Using SQL, NoSQL, and Polyglot Persistence.
Microsoft, USA (2013)
8. Sadalage, P.J., Fowler, M.: NoSQL Distilled. Addison-Wesley, USA (2012)
9. Basho
(2017)
Security
Basics.
http://docs.basho.com/riak/kv/2.2.0/using/
security/basics/. Accessed 22 Feb 2017
10. MongoDB, Inc.: Collection-Level Access Control. https://docs.mongodb.com/
manual/core/collection-level-access-control/. Accessed 22 Feb 2017
11. Dmytro
Sotnyk:
Notes
on
NoSQL:
Years
in
production
with
Apache
Cassandra.
http://rndblog.github.io/nosql/architecture/2015/09/13/
notes-on-nosql-cassandra.html. Accessed 22 Feb 2017
12. Apache Software Foundation: Column-level permissions. https://issues.apache.
org/jira/browse/CASSANDRA-12859. Accessed 22 Feb 2017
13. Cloudera,
Inc.:
Conﬁguring
HBase
Authorization.
https://www.cloudera.
com/documentation/enterprise/5-8-x/topics/cdh sg hbase authorization.html.
Accessed 22 Feb 2017
14. Saltzer, J., Schroeder, M.: The protection of information in computer systems. In:
Proceedings of the 4th ACM Symposium on Operating System Principles, Oct
1973
15. McCreary, D., Kelly, A.: Making Sense of NoSQL. Manning, USA (2014)

Buﬀered Count-Min Sketch
Ehsan Eydi1, Dzejla Medjedovic1(B), Emina Mekic1, and Elmedin Selmanovic2
1 Sarajevo School of Science and Technology, Hrasnicka cesta 3a,
71000 Sarajevo, Bosnia and Herzegovina
ehsan.eydi@ssst.edu.ba, dzejla.medjedovic@ssst.edu.ba,
emina.mekic@ssst.edu.ba
2 University of Sarajevo, Zmaja od Bosne 33, 71000
Sarajevo, Bosnia and Herzegovina
eselmanovic@pmf.unsa.ba
Abstract. In this paper, we present the preliminary work on the adap-
tation of Count-Min sketch (CMS) data structure to the external stor-
age. CMS is a probabilistic, hashing-based data structure that is used
to measure items’ frequencies using very compact space. CMS has broad
applications in the streaming context, i.e., in measuring popularity (e.g.,
top k elements, heavy hitters, quantiles, range queries, etc.) CMS has
two error parameters ϵ and δ: on the input size N, it guarantees that
the overestimate of the item frequency is within ϵN with probability at
least 1 −δ. The error parameters are tunable and determine the size
of the sketch. To maintain the same band of error, CMS grows linearly
with the size of the dataset. Simply placing the CMS on SSD or a hard-
drive results in very slow performance due to the random-write nature of
UPDATE operations. We suggest two adaptations of the Count-Min sketch
to alleviate this eﬀect: Elevator Count-Min Sketch (ECMS) and Buﬀered
Count-Min Sketch (BCMS). The two adaptations use buﬀering updates
in RAM and hash localization, two methods similar to those used in [5]
to alleviate similar issues with the Bloom ﬁlter. We show that opera-
tions on the ECMS and BCMS require asymptotically fewer I/Os than
the traditional CMS placed on disk.
1
Introduction
Many applications today produce large amounts of data at a rapid rate. Classical
examples of so-called massive data streams include sensor networks, ﬁnancial
markets, IP traﬃc, texts, etc. What diﬀerentiates the streaming context from
many other traditional models is that the data arrives in real-time, and the
quantity of data is so large that storing it in the available storage is highly
impractical. So one way to think about the streaming scenario is that every data
item passes by once, the system performs some computation on it, and lets it
go.
In such a context, it is important to maintain a compact representation of a
ﬂeeting set that can answer queries about the data, most commonly those that
c
⃝Springer International Publishing AG 2018
M. Hadˇzikadi´c and S. Avdakovi´c (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_22

250
E. Eydi et al.
measure popularity. Examples include producing the list of bestsellers, most
clicked-on websites, most frequent queries on a search engine, most frequently
occurring words in a large text, etc. In the technical literature, these problems
are known under the names of top k items, heavy hitters, quantiles and so
on [12,14,16]. Count-Min sketch [6] has been widely used to solve this type of
problems.
Count-Min Sketch (CMS) is a hashing-based, probabilistic and lossy repre-
sentation of a set. CMS is represented with a 2D array of counters of dimensions
b × ℓand ℓhash functions, h1, h2, . . . , hℓ. It is illustrative to think of CMS as a
cousin data structure to a well-known Bloom ﬁlter [4], where the Bloom ﬁlter
reports whether an element is present and with small probability gives a false
positive, and the Count-Min Sketch reports the item frequency and overshoots
the allowed range of overestimate with a small probability. Just like with Bloom
ﬁlter, the error in CMS is bounded: CMS guarantees that the overestimate is
up to ϵN with probability at least 1 −δ, where N is the total input size.
CMS is designed to stay in smaller levels of memory. But with the increas-
ing data sizes, CMS can also grow to become too large to ﬁt in main mem-
ory. Namely, to accomplish the error rates speciﬁed above, CMS size should be
O(ln(1/δ)/ϵ). That is, if we want to keep the error range ϵN ﬁxed or nearly ﬁxed,
then the number of columns in the CMS grows linearly with the input size.
Another context where we see CMS becoming large is in some text applica-
tions, where the number of elements inserted in the sketch is quadratic in the
original text size. For instance, [10] uses CMS to record distributional sensitivity
on the web, where each pair of words is inserted as a single item into the CMS,
and 90 GB of text requires a CMS of 8GB.
A simple solution is to place CMS on SSD/disk. However, due to the random
nature of inserts into the data structure, this would only allow us to do about
300–400 inserts per second or a little more, depending on how fast the underlying
random write of the disk is.
In this paper, we use buﬀering methods to alleviate the random-write eﬀect
of the CMS. We suggest two data structures, Elevator CMS and Buﬀered CMS
to speed up the UPDATE and ESTIMATE operations on CMS. Combining the ideas
of hash localization and using RAM for buﬀering, we obtain substantially faster
asymptotic performance of the CMS. Similar approach to ours has been already
used for Bloom ﬁlters [5,7], and we mimick their approach to speed up the CMS.
We show that the performance of UPDATE and ESTIMATE operations on our
adaptations to CMS are asymptotically better than on the plain CMS on disk.
2
Related Work
The streaming model [1,9,11,15] is traditionally used to represent various real-
life problems where the massive amount of data is generated on a constant
basis. Examples include monitoring web traﬃc [13], collecting sensor-network
and ﬁnancial sector data [11,12] or analyzing large text ﬁles [10].

Buﬀered Count-Min Sketch
251
For ad-hoc queries [1] and the popularity analysis such as top k elements,
heavy hitters and quantiles [12,15], space-eﬃcient and probabilistic data struc-
ture Count-Min sketch [6] has proved very useful [1,11].
A cousin data structure to Count-Min sketch is Bloom ﬁlter (and its vari-
ants) [3,4]. There has been extensive work on the subject of adapting Bloom
ﬁlters to the external storage using buﬀering and hash localization [5,7]. Our
paper employs similar methods to those in [5,7].
There has been work in designing cache-eﬃcient equivalents for Bloom ﬁlters
such as the Quotient ﬁlter and write-optimized on-disk Bloom ﬁlter equivalents
such as the Cascade ﬁlter [2,8], however we are unaware of similar work having
been done for the frequency-item-counting data structures.
3
Count-Min Sketch: Preliminaries
In this section, we review the basics of the streaming setup and the CMS.
In the streaming setup, the application receives pairs of items (ai, ci), where
ai is the item ID and ci is count/quantity.1
CMS records frequencies of elements and is most frequently used for the
approximate heavy hitter, top k items, and similar problems for analyzing pop-
ularity. Count-Min Sketch is represented via 2D table with b rows and ℓcolumns
and ℓhash functions. CMS has two operations: UPDATE(i) and ESTIMATE(i),
the respective equivalents of insert and lookup, and they are performed as
follows:
Fig. 1. UPDATE operation on Count-Min Sketch. Each row is incremented by ci. The
cell in the ith row is speciﬁed by the ith hash function
• UPDATE(i) inserts the item pair ai into the sketch by hashing ai with the
appropriate hash function in each row, and incrementing the appropriate slot
1 For example, ai can be a pair of IP addresses that exchange a packet, and ci is the
packet size, or ai can be a stock ID and ci is the amount by which its valuation
rose/fell since the last timeslot.

252
E. Eydi et al.
by ci (see Fig. 1). In other words, for each hash function hj, 1 ≤j ≤ℓ, we set
CMS[j][hj(ai)] = CMS[j][hj(ai)] + ci.
• ESTIMATE(i) returns the estimated frequency of ai by taking the minimum of
the values in the slots where UPDATE(i) for the item was performed. In other
words, we return min1≤j≤ℓ(CMS[j][hj(ai)]). Due to collisions, ESTIMATE(i)
may return an overestimate of the actual count, but for this to happen, there
would have to be a collision in every row.
CMS guarantees that the overestimate for any particular item is within the
range ϵN, with probability at least 1 −δ, if we set CMS dimensions b = e/ϵ and
ℓ= ln(1/δ).
4
Count-Min Sketch on SSD
In this section, we introduce two adaptations of CM sketch that are cache-
eﬃcient and disk-storage-friendly: Elevator Count-Min Sketch (ECMS) and
Buﬀered Count-Min Sketch (BCMS).
For the ease of reading, we ﬁrst recall the main parameters:
• N = # items
• M = memory size
• B = block size
• b = # columns in CM sketch
• ℓ= # rows in CM sketch (= # hash functions)
4.1
Elevator CM Sketch
Elevator CM sketch is a simple application of buﬀering to the CM sketch. We
maintain an in-RAM buﬀer that stores the information on the cell that should be
updated and by which quantity. We divide the memory into two halves: one half
for various internal computations, and the other half for storing updates of the
form (j, hj(ai), ci), where the update performed on CMS[j][hj(ai)] incremented
by ci. Once the buﬀer is full, we sweep over the entire CMS and apply the changes
(it does not make much diﬀerence here whether CMS is stored in a row-ﬁrst or
column-ﬁrst fashion).
Analysis.
• CMS size (bits): b ∗ℓ∗word size
• # updates that ﬁt into RAM: M/ℓ
• # I/Os needed for a ﬂush: b ∗ℓ∗word size/B
• #I/Os per update: b ∗ℓ2 ∗word size/BM

Buﬀered Count-Min Sketch
253
4.2
Buﬀered CM Sketch
In addition to simple buﬀering employed by the ECMS, buﬀered CM sketch also
uses hash localization as used in [5] to speed up the updates. The main idea
revolves around having an additional hash function h0 that maps an element
into a particular block of a CMS. All other hashes for that element must hash
inside a block speciﬁed by h0. CMS is divided into blocks of size B and every
block has a dedicated sub-buﬀer in RAM to store the updates destined to that
particular block. Once a particular sub-buﬀer ﬁlls up, it is emptied in 1 I/O onto
the corresponding block (Fig. 2).
Analysis.
• #blocks in CMS/sub-buﬀers in RAM: k
• Cost to ﬂush one sub-buﬀer: 1 I/O
• #updates one sub-buﬀer holds: M/kℓ
• #I/Os per update: kℓ/M = b ∗ℓ2 ∗word size/BM
Fig. 2. Buﬀered Count-Min Sketch. Updates are stored in RAM, and all updates are
destined for the same block on disk
5
Analysis
Both data structures oﬀer the same asymptotic performance of the number of
updates/estimates per I/O. More experiments are required to determine the
experimental performance between the two data structures and the behavior on

254
E. Eydi et al.
diﬀerent datasets. Also, one needs to ascertain what happens to the false positive
rate once hash functions change the way they do in BCMS. This paper is a report
on work in progress in developing the two adaptations to Count-Min sketch. Our
guess is that the experiments will expose the random vs. sequential-write nature
of writes thus showing that ECMS works faster in practice, even though BCMS
is more eﬃcient for estimate operations.
6
Conclusion
In this paper, we introduced two variants of Count-Min Sketch that scale to
the external storage such as SSD or hard-drive. Both data structures perform
asymptotically better on basic operations than the plain CMS placed on disk.
Experiments are required to determine the practical performance of the two data
structures. Also, as future work we hope to exploit more sophisticated write-
optimization methods to this data structure and develop even faster and more
scalable versions of the CM sketch that we will test on the real-world streaming
datasets.
References
1. Babcock, B., Babu, S., Datar, M., Motwani, R., Widom, J.: Models and issues in
data stream systems. In: Proceedings of the Twenty-ﬁrst ACM SIGMOD-SIGACT-
SIGART Symposium on Principles of Database Systems, PODS ’02, pp. 1–16, New
York, NY, USA. ACM (2002)
2. Bender, M.A., Farach-Colton, M., Johnson, R., Kraner, R., Kuszmaul, B.C., Med-
jedovic, D., Montes, P., Shetty, P., Spillane, R.P., Zadok, E.: Don’t thrash: how to
cache your hash on ﬂash. Proc. VLDB Endow. 5(11), 1627–1637 (2012)
3. Bonomi, F., Mitzenmacher, M., Panigrahy, R., Singh, S., Varghese, G.: An
improved construction for counting bloom ﬁlters. In: Proceedings of the 14th Con-
ference on Annual European Symposium—Volume 14, ESA’06, pp. 684–695, Lon-
don, UK. Springer (2006)
4. Broder, A., Mitzenmacher, M.: Network applications of bloom ﬁlters: a survey.
Internet Math. 636–646 (2002)
5. Canim, M., Mihaila, G.A., Bhattacharjee, B., Lang, C.A., Ross, K.A.: Buﬀered
bloom ﬁlters on solid state storage. In: Bordawekar, R., Lang, C.A. (eds.)
ADMS@VLDB, pp. 1–8 (2010)
6. Cormode, G., Muthukrishnan, S.: An improved data stream summary: the count-
min sketch and its applications. J. Algorithms 55(1), 58–75 (2005)
7. Debnath, B., Sengupta, S., Li, J., Lilja, D.J., Du, D.H.C.: Bloomﬂash: bloom ﬁlter
on ﬂash-based storage. In: Proceedings of the 2011 31st International Conference
on Distributed Computing Systems, ICDCS ’11, pp. 635–644, Washington, DC,
USA. IEEE Computer Society (2011)
8. Dutta, S., Narang, A., Bera, S.K.: Streaming quotient ﬁlter: a near optimal approx-
imate duplicate detection approach for data streams. Proc. VLDB Endow. 6(8),
589–600 (2013)
9. Gaber, M.M., Zaslavsky, A., Krishnaswamy, S.: Mining data streams: a review.
SIGMOD Rec. 34(2), 18–26 (2005)

Buﬀered Count-Min Sketch
255
10. Goyal, A., Jagarlamudi, J., Daum´e III, H., Venkatasubramanian, S.: Sketch tech-
niques for scaling distributional similarity to the web. In: Proceedings of the 2010
Workshop on GEometrical Models of Natural Language Semantics, GEMS ’10, pp.
51–56, Stroudsburg, PA, USA. Association for Computational Linguistics (2010)
11. Manku, G.S., Motwani, R.: Approximate frequency counts over data streams. In:
Proceedings of the 28th International Conference on Very Large Data Bases, VLDB
’02, pp. 346–357. VLDB Endowment (2002)
12. Nath, S., Gibbons, P.B., Seshan, S., Anderson, Z.R.: Synopsis diﬀusion for robust
aggregation in sensor networks. In: Proceedings of the 2nd International Conference
on Embedded Networked Sensor Systems, SenSys ’04, pp. 250–262, New York, NY,
USA. ACM (2004)
13. Sarl´os, T., Bencz´ur, A.A., Csalog´any, K., Fogaras, D., R´acz, B.: To randomize or
not to randomize: space optimal summaries for hyperlink analysis. In: Proceedings
of the 15th International Conference on World Wide Web, WWW ’06, pp. 297–306,
New York, NY, USA. ACM (2006)
14. Schechter, S., Herley, C., Mitzenmacher, M.: Popularity is everything: a new app-
roach to protecting passwords from statistical-guessing attacks. In: Proceedings
of the 5th USENIX Conference on Hot Topics in Security, HotSec’10, pp. 1–8,
Berkeley, CA, USA. USENIX Association (2010)
15. Woodruﬀ, D.P.: New algorithms for heavy hitters in data streams (2016).
arXiv:1603.01733
16. Zhao, Q.G., Ogihara, M., Wang, H., Xu, J.J.: Finding global icebergs over dis-
tributed data sets. In: Proceedings of the Twenty-ﬁfth ACM SIGMOD-SIGACT-
SIGART Symposium on Principles of Database Systems, PODS ’06, pp. 298–307,
New York, NY, USA. ACM (2006)

The Relationship Between the Performance
of Islamic and Conventional Banks
Emina Causevic(B)
Sarajevo School of Science and Technology, Ilidˇza, Bosnia and Herzegovina
dzejlam@gmail.com
Abstract. The purpose of this paper is to research existing models
of ﬁnancial institutions and determine the position of Islamic ﬁnance
in today’s modern world of conventional western ﬁnance. Theory suggests
that Islamic ﬁnancial principles are not inferior to conventional princi-
ples and perhaps may even supersede them in some aspects. This paper
will attempt to empirically display the relative strengths and weaknesses
of both bank types. Research was conducted in the Global Cooperation
Council (GCC) region, based on a sample size of 58 banks through the
years 2003–2015. . . .
Keywords: Islamic banking · GCC region · Conventional banking
1
Introduction
Drawing on the principles of conventional and Islamic ﬁnance, the purpose of
this paper is to research diﬀerences between these ﬁnancial institutions and their
theoretical principles of money lending. This paper will attempt to empirically
show the relationship between both bank types and display the relative strengths
and weaknesses of both.
2
Research Questions
H0: Conventional banks overall perform better than Islamic banks in
the GCC region. Based on the assumptions discussed in the Literature Review,
the research conducted will be focused on four main aspects of Islamic banking
that can be quantiﬁably expressed: size, eﬃciency, proﬁtability, growth. These
aspects will be surveyed through four sub-hypotheses, which are ordered from
the most basic and structural regarding size, to the most complex regarding
growth. Therefore, each hypothesis will serve to be a basis for understanding
the next one. All research was conducted in the GCC region, as explained in the
Methodology section.
c
⃝Springer International Publishing AG 2018
M. Hadˇzikadi´c and S. Avdakovi´c (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_23

The Relationship Between the Performance of Islamic
257
H1: Islamic banks are on average smaller than conventional banks.
H2: Islamic banks are not as cost eﬃcient as conventional banks due to their
size.
H3: Islamic banks are at least as proﬁtable as conventional banks.
H4: Islamic banks have a slower but more stable rate of growth than conven-
tional banks.
3
Literature Review
3.1
Overview of Islamic and Conventional Banking
Islamic banking has grown dramatically since the 1970s in Egypt when it was ﬁrst
introduced to the modern world of ﬁnance. This growth has been predominantly,
although not limited to, linked to countries with large Muslim populations. The
World Islamic Banking Competitiveness Report 2016 cites the global Islamic
ﬁnance sector as having around USD 1 trillion in 2015.
In the GCC region, which is the focal point of this paper, Islamic banks
have increased market penetration to the point that they have begun to outpace
conventional banks. Islamic ﬁnance assets account for more than 50% of system
ﬁnancing in Saudi Arabia. From this point onward, banks will be classiﬁed as
either conventional or Islamic banks to minimize confusion.
Principles of Islamic Finance
It is misleading to suggest that Islamic ﬁnance diﬀers in ﬁnancial principle from
the rules of conventional ﬁnance. The cores of both ﬁnancial systems function on
the same basic laws; the diﬀerence lies in the legal framework. Islamic banking,
contrary to the principles of conventional banking, functions entirely without
interest. The time value of money is considered excess compensation without
due consideration (Choudhury and Malike 1992). This principle forbids charging
more money in repayment of a loan than the amount of the loan itself, viewing
it as unwarranted income.
Islamic banking is not, however, without proﬁt. Earnings strictly cannot be
made by speculation, and very few ﬁnancial derivatives are recognized by Islamic
banking (Salam contracts as a variant of forward contracts, Istijrar contracts
similar to options, etc.). Rather, the system works based on risk sharing, proﬁt
and loss sharing, leasing, and joint ventures. This makes few conventional instru-
ments available to the Islamic ﬁnancial system. Proﬁt must be based either on
an underlying asset, or on the return of entrepreneurial investment.
a. Size and Structure of Islamic Banks
Many studies comparing Islamic and conventional banks begin with the assump-
tion that Islamic banks are in practice smaller than conventional banks. Rashid
and Jabeen (2016) found that conventional banks are signiﬁcantly larger than
Islamic banks in Pakistan, for example. However, Meero (2015) found in his
study of GCC banking that their size and structure are insigniﬁcantly diﬀerent.

258
E. Causevic
Therefore, there is a basis to state that on a global scale it may be true that
conventional banks are larger than Islamic banks, however in the GCC region
that this may not be statistically true. Several authors have found that there is
no statistical diﬀerence between these two bank types, possibly due to the fact
that it is the most saturated region for Islamic banking in the world.
b. Eﬃciency of Islamic Banks
Research done on eﬃciency in Islamic banks is disparate and even contradictory
(Johnes et al. 2014). However, Johnes did a relatively stable and comprehensive
study on 60 banks from the GCC over a 3-year period. Their results show with
signiﬁcance that Islamic banks are less cost-eﬀective and more proﬁt-eﬀective
than conventional banks.
The higher operating costs of Islamic banks have much to do with certain
speciﬁcities that are present in this type of banking. For example, Islamic banks
must maintain a Shariah compliance board that overlooks investment decisions,
which in itself creates the need for a larger number of high salaries. There are also
implied legal costs of maintaining the complexities of Islamic ﬁnancial products.
c. Proﬁtability of Islamic Banks
The research of Olson and Zoubi (2008), later followed by Johnes et al. (2014),
shows Islamic banks as having a signiﬁcantly higher return on assets ratio. Their
papers show that the proﬁtability of Islamic banks was higher than for conven-
tional banks in Malaysia between 1996 and 1999.
Various other authors (Abu Loghod, Hasan and Dridi, Johnes et al., etc.)
conclude that Islamic banks are more proﬁtable than conventional banks in the
GCC region. Dridi and Hasan (2010) compared the proﬁtability of these banking
systems in the period before the global crisis, during the crisis, and afterwards.
Their ﬁndings show that Islamic banks were more proﬁtable before and during
the crisis, however afterwards they suﬀered a larger decline in proﬁtability. This
may be due to the fall of the values of the most invested in areas of Islamic
ﬁnance: private equity and real estate.
d. Growth of Islamic Banks
The growth of Islamic banking has often been connected to the wealth Arab
Muslims amassed rapidly during the 1970s and the massive demand for oil.
Because interest is traditionally forbidden to Muslims, and because the newly
wealthy Muslims of that decade needed a halal place to deposit their fortunes,
the need arose for a modern solution to this dilemma.
More relevantly to this particular study, Islamic banking has grown in the
GCC region due to the overall economic growth in the region (Imam and Kpodar
2015). The GCC region is a Mecca, so to speak, for Islamic ﬁnance, accounting
for 34% of total global assets of Islamic banking (Johnes et al. 2014). It is a
speciﬁc region in that both ﬁnancial systems are developed to a high degree,
allowing for a more or less illustrative comparison.

The Relationship Between the Performance of Islamic
259
4
Methodology
The data used in this paper was extracted from several sources. The majority
of the ﬁnancial data was taken from the GulfBase database. Critical data about
individual banks that was missing or incomplete was retrieved from the online
public ﬁnancial statements of these banks.
The main reason for choosing the GCC region as the basis for research was
because of the relative coherence of the banking sectors of the countries in ques-
tion. It is the largest cooperative region of countries that has a legally recognized
parallel system of Islamic and conventional ﬁnance (Altaee et al. 2013). The
countries of the GCC are all exporters of oil, and all of them boast real estate
industries with large funds from regional banks (Table 1).
Table 1. Distribution of banks used in research
Country
All banks
Conventional Islamic
Bahrain
4
2
2
Kuwait
8
5
3
Oman
5
5
0
Qatar
8
5
3
Saudi
Arabia
12
7
5
Sudan
1
0
1
United Arab
Emirates
20
16
4
Total
58
40
18
The data for these 58 banks was gathered for the period between 1993 and
2015. However, the period of 1993–2002 was not taken into consideration for
any of the analyses done in this study because in those years, missing bank
information was more than 20% of total data.
Outliers in the data were tested with the Grubbs outlier test, and uncovered
no signiﬁcant outliers. In the case where outliers were found, the data in question
was not taken into consideration. This was done as a reﬂection of many authors
such as Isik and Hassan (2002), Soylu and Durmaz (2013), Alfarisi (2015), Farooq
et al. (2015), etc. Because outliers have a great eﬀect on data, especially in the
calculation of ﬁnancial ratios where diﬀerences were shown to be as high as
2000%, data deemed by the Grubbs test to be outliers were omitted.
The overall methodology used for this study was similar for almost every
hypothesis. First, all necessary input data was arranged by bank type. Average
values were arranged in a table, as well as in a graph to show yearly trends. Then
a two-sample Z-test was conducted for each year from 2003 to 2015. All Z-test

260
E. Causevic
results were arranged in a table in order to better recognize trends. A signiﬁcance
level of 5% was given to all Z-tests, meaning that the alternate hypothesis was
proven if the value of the Z-test was beneath 5% (Table 2).
Table 2. Data used for research
Hypothesis
Data needed
Ratios and formulas
Statistical test
Hypothesis
1—Size
Total Assets
Revenues
Total Employees
Z-Test
Graph
Histogram
Hypothesis
2—Eﬃciency
Expenses
Revenues
Bank Eﬃciency
Ratio =
Expenses/Revenues
Z-Test
Graph
Histogram
Hypothesis
3—
Proﬁtability
Net Income
Equity Capital
Total Assets
ROE = Net
Income/Shareholders
Equity
ROA = Net
Income/Total Assets
Z-Test
Graph
Histogram
Hypothesis
4—Growth
Total Assets
Revenues
Growth Rate =
(Total Assets (t) −
Total Assets (t −
1))/Total Assets (t
−1) * 100
Growth Rate =
(Total Revenues (t)
−Total Revenues (t
−1))/Total
Revenues (t −1) *
100
Annual Growth
St. deviation
5
Research Results and Discussion
a. Size
In order to test this hypothesis, three factors were tested: total revenues, total
assets, and number of employees. Z-tests were conducted on these three factors
using the following setup:
Alternate Hypothesis = Mean (conventional banks) −Mean (Islamic banks) > 0
The Z-tests conducted on Average Total Assets show a declining trend in
p-value over time, beginning with 57.22% in 2013 and ending with 25.31%.
Since the alternate hypothesis states that conventional banks are larger than

The Relationship Between the Performance of Islamic
261
Islamic banks, these results show that is statistically not the case. The alter-
nate hypothesis was originally phrased in an unfavorable light towards Islamic
banks. Eﬀectively, this means that conventional banks cannot be statistically
determined as larger than Islamic banks in terms of total asset value.
The Z-tests conducted on Average Revenues show a declining p-value over
time similar to the trend with total assets, but in this case the decline is steeper,
falling from 57.92 to 8.89%. Also similar to the previously described results, the
alternate hypothesis is statistically disproved, meaning that conventional banks
cannot be deﬁnitively determined as larger than Islamic banks. Unlike with total
assets, in 2015 the p-value was near the threshold of 5% which would make it
statistically signiﬁcant. However, the alternate hypothesis was not proved in any
of the years of the analysis.
By comparing the size of Conventional and Islamic banks by Total Assets
and Revenues, one cannot statistically state that Conventional banks are larger
than Islamic banks. This rejects the H1 hypothesis, but is positive in favor of
Islamic banks, proving that they are not at a disadvantage with regard to the
hypothesis in question.
b. Eﬃciency
In order to test this hypothesis, the cost eﬃciency ratio for conventional and
Islamic banks was tested. Z-tests were conducted using the following setup:
Alternate Hypothesis = Mean (conventional banks) −Mean (Islamic banks) < 0
Because the result of the cost-income ratio is by deﬁnition favorable if it
is lower, this setup of the alternate hypothesis in fact shows that the statistical
mean of cost eﬃciency in conventional banks is smaller than the statistical mean
of Islamic banks.
According to the results of the Z-tests, we cannot claim with statistic surety
that Conventional banks are more eﬃcient than Islamic banks. However, the
hypothesis was in fact accepted twice, in years 2003 and 2011. With 83% of the
results disproving the hypothesis, the overall conclusion of this paper is that
conventional banks are not statistically more cost-eﬃcient than Islamic banks.
c. Proﬁtability
In order to test this hypothesis, two ratios were tested: Return on Assets and
Return on Equity (ROA and ROE). Z-tests were conducted on these two factors
using the following setup:
Alternate Hypothesis = Mean (conventional banks) −Mean (Islamic banks) > 0
Both banks show a signiﬁcant drop in average net proﬁt in 2009 conventional
banks dropped at 3.79, while Islamic banks plunged at 10.19%. As mentioned
in the Literature Review, Dridi and Hasan (2010) compared the proﬁtability of
these banking systems in the period before the global crisis, during the crisis,
and afterwards. Similar to this study, their ﬁndings show that Islamic banks were
more proﬁtable before and during the crisis, however afterwards they suﬀered a
larger decline in proﬁtability.

262
E. Causevic
The Z-tests comparing conventional banks and Islamic banks in terms of
ROA are persistently larger than the p-value, disproving that conventional banks
are statistically more proﬁtable than Islamic banks. Otherwise, both Net Proﬁt
and Average Assets are relatively similar in value, moving at similar intervals
through the years.
Overall, Z-tests comparing conventional and Islamic banks in terms of ROA
and ROE are persistently large. Since the wording of H3 states that Islamic
banks are at least as proﬁtable as conventional banks, the alternate hypothesis
set to be tested was that conventional banks are more proﬁtable than Islamic
banks. Therefore, the persistent rejection of the alternate hypothesis shows that
H3 can be statistically claimed to be accurate.
d. Growth
In order to test this hypothesis, two factors were tested: revenues and total
assets. They were not tested statistically by z-test, but rather a growth rate was
calculated for average values of both factors. The average growth rate for years
2004–2015 according to the method described in the Methodology section was
calculated to compare total average growth between bank types.
Because the hypothesis takes into consideration two aspects of growth rate of
growth and stability of growth, it was necessary to compare standard deviations
of both bank types as well. In this way, it is possible to show if Islamic banks
truly grow slower but more steadily or not, etc.
Islamic banks show a larger average growth rate in both tests. The aver-
age growth from 2004 to 2015 for conventional banks is 17.25% compared to
30.54% for Islamic banks in revenue growth, and 18.00% compared to 22.24%
respectively in total asset growth.
For the purpose of showing the stability of growth, annual standard devia-
tions for conventional and Islamic banks were calculated. Standard deviation of
revenue growth is larger for Islamic banks on average, at 30.54% comparing to
22.12%, and 19.28%, compared to 17.54% respectively in terms of total asset
growth.
Both ﬁnancial indicators used to show the growth of conventional and Islamic
banks give the same result of a faster pace of growth in Islamic banks, disproving
the ﬁrst part of H4. However, average annual standard deviations of both bank
types show that Islamic banks do not statistically have a more stable growth
rate contrary to H4.
6
Conclusion
Through theoretical review and empirical research, the overall conclusion of this
paper is that it is not statistically justiﬁable to discern between conventional
and Islamic banks in terms of size, eﬃciency, proﬁtability or growth.
Analyses of accounting data and ratios were conducted in 8 countries of the
GCC region, in 58 banks over a 12 year period. For this sample and period,
Islamic banks were not shown to be signiﬁcantly inferior to their conventional

The Relationship Between the Performance of Islamic
263
counterparts by any of the aforementioned aspects. The limitations of the study
were limited ﬁnancial data prior to 2002, which decreased the initial sample size
almost by half. Moreover, there is no readily available uniﬁed database of Islamic
banks, which made it a necessity to create a database by hand. This was the
most time consuming and complex portion of the research.
These statements all lead to the singular conclusion of this paper, which is
that Islamic banking is a functioning and beneﬁcial system that shows great
potential for further growth and incorporation into the conventional ﬁnancial
world. This in itself disproves the primary hypothesis of this paper (H0), which
claims that conventional banks perform better than Islamic banks in the GCC
region (Table 3).
Table 3. Conclusions
Hypothesis
Conventional
banks
Islamic banks
Conclusion
H1—Size
Not statistically
larger than IB
Not statistically
smaller than CB
Contrary to
hypothesis IB not
smaller
H2—Eﬃciency
Not statistically
more eﬃcient than
IB
Not statistically
less eﬃcient than
CB
Contrary to
hypothesis IB not
less eﬃcient than
CB
H3—Proﬁtability
Not statistically
more proﬁtable
than IB
At least as
proﬁtable as
Conﬁrming
hypothesis IB at
least as CB
H4—Growth
Lower rate of
growth than IB,
smaller st. dev.
Higher rate of
growth than CB,
larger st. dev.
Contrary to
hypothesis IB have
larger and more
unstable rate of
growth
References
Abd. Majid, M., H. Kassim, S.: Assessing the contribution of Islamic ﬁnance to eco-
nomic growth. J. Islamic Account. Bus. Res. 6(2), 292–310 (2015). http://dx.doi.
org/10.1108/jiabr-07-2012-0050
Abu. Loghod, H.: Do Islamic banks perform better than conventional banks? Evidence
from Gulf Cooperation Council countries (2007)
Alfarisi, M.: Impact of ﬁnancial crisis on non-traditional income: Islamic banks vis a
vis conventional banks. JIF 4(1), 31–38 (2015)
Altaee, H., Talo, I., Adam, M.: Testing the ﬁnancial stability of banks in GCC countries:
pre and post ﬁnancial crisis. Int. J. Bus. Soc. Res. 3(4), 93–105 (2013)

264
E. Causevic
Choudhury, M.A., Malike, U.A.: The foundations of Islamic political economy. Macmil-
lan, London, St. Martin’s Press, NewYork and Islamic banking and ﬁnance (1992)
Dridi, J., Hasan, M.: The eﬀects of the global crisis on Islamic and conventional banks:
a comparative study. IMF Working Papers, vol. 10, no. 201, 1. Web (2010)
Farooq, M., van Wijnbergen, S., Zaheer, S.: Will Islamic banking make the world less
risky? An empirical analysis of capital structure, Risk shifting and ﬁnancial stability.
SSRN Electron. J. (2015). http://dx.doi.org/10.2139/ssrn.2605358
Ftiti, Z., Nafti, O., Sreiri, S.: Eﬃciency of Islamic banks during subprime crisis: Evi-
dence of GCC countries. JABR 29(1), 285 (2013). http://dx.doi.org/10.19030/jabr.
v29i1.7615
Hadriche, M.: Banks performance determinants: Comparative analysis between con-
ventional and Islamic banks from GCC countries. Inter. J. Econ. Fin. 7(9) (2015).
http://dx.doi.org/10.5539/ijef.v7n9p169
Imam, P., Kangni, K.: Is Islamic banking good for growth? IMF Working Papers, vol.
15, no. 18, 1. Web (2015)
Isik, I., Hassan, M.: Technical, scale and allocative eﬃciencies of Turkish bank-
ing industry. J. Banking Fin. 26(4), 719–766 (2002). http://dx.doi.org/10.1016/
s0378-4266(01)00167-4
Johnes, J., Izzeldin, M., Pappas, V.: A comparison of performance of Islamic and
conventional banks 2004–2009. J. Econ. Behav. Organ. 103, S93–S107 (2014)
Johnson, K.: The role of Islamic banking in economic growth. CMC Senior Theses.
Paper 642 (2013). http://scholarship.claremont.edu/cmc theses/642[1]
Kassim, S.: Islamic ﬁnance and economic growth: The Malaysian experience. Global
Fin. J. 30, 66–76 (2016). http://dx.doi.org/10.1016/j.gfj.2015.11.007
Meero, A.: The Relationship between capital structure and performance in gulf coun-
tries banks: a comparative study between Islamic banks and conventional banks. Int.
J. Econ. Finance 7(12), 140 (2015)
Mghaieth, A., Khanchel, I.: The determinants of cost/proﬁt eﬃciency of Islamic banks
before, during and after the subprime crisis using SFA approach. Ijafr 5(2), 74 (2015).
http://dx.doi.org/10.5296/ijafr.v5i2.7866
Mobarek, A., Kalonov, A.: Comparative performance analysis between conventional
and Islamic banks: Empirical evidence from OIC countries. Appl. Econ. 46(3), 253–
270 (2013). http://dx.doi.org/10.1080/00036846.2013.839863
Mohammad, S.: Liquidity risk management in Islamic banks: A survey. Afro Eurasian
Stud. 1(2), 215–230 (2013)
Olson, D., Zoubi, T.: Using accounting ratios to distinguish between Islamic and con-
ventional banks in the GCC region. Int. J. Account. 43(1), 45–65 (2008)
Rashid, A. Jabeen, S.: Analyzing performance determinants: Conventional versus
Islamic banks in Pakistan. Borsa Istanbul Rev. 16(2), 92–107 (2016). http://dx.
doi.org/10.1016/j.bir.2016.03.002
Rozzani, N. Rahman, R.: Determinants of bank eﬃciency: Conventional versus Islamic.
Inter. J. Bus. Manage. 8(14) (2013). http://dx.doi.org/10.5539/ijbm.v8n14p98
Soylu, A. Durmaz, N.: Proﬁtability of interest-free versus interest-based banks in
Turkey. Australian Econ. Rev. 46(2), 176–188 (2013). http://dx.doi.org/10.1111/
j.1467-8462.2013.12002.x[2]

Stylometric Study of Ivo Andri´c’s
Short Stories
Edin Konjhodˇzi´c(B)
University Sarajevo School of Science and Technology,
Sarajevo Hrasniˇcka cesta 3a, 71000 Sarajevo, Bosnia and Herzegovina
konjhodzic@gmail.com
Abstract. This paper presents a stylometric analysis of Ivo Andri´c’s
short stories written in the 54 year long-time period, and translations
into English by fourteen diﬀerent interpreters. The objective of this study
is to analyze stylometric similarities in two languages and correlations
with possible changes of author’s usage of words over time.
Keywords: Computational linguistic · Stylometry · Authorship
attribution
Ivo Andri´c was a Bosnian writer, a famous novelist, a passionate short story
writer and the 1961 Nobel Prize laureate. His celebrated novels The Bridge
on the Drina, Bosnian Chronicle and The Damned Yard were written during
World War II. In contrast to his novels Ivo Andri´c wrote and published short
stories over a period of ﬁfty years. That fact makes Andri´c’s short stories more
than interesting for stylometric studies, as they can oﬀer an insight into his
work without being aﬀected by political controversies currently surrounding his
literary work. This selection of short stories is providing a possibility to work with
the corpus of short stories written in the 54 year long-time period and translated
into English by fourteen diﬀerent interpreters with very diverse backgrounds. It
is a great opportunity for detecting translators’ stylistic ﬁngerprints and analysis
of measurable features of styles in two languages or for analysis of possible impact
of authors usage of words during the time. The choice of texts in corpus in itself
has crucial importance because of the fact that text length, topic and attribution
accuracy showing some correlation.
The prepared corpora include two sets of short stories. The ﬁrst set contains
16 short stories in the original language and second set consist of their Eng-
lish translation. This selection shows consideration for the possible eﬀects of the
story-topic on stylometric ﬁndings. It is mainly Andri´c’s corpus of short stories
about one topic. The question of the shortest acceptable length of text for reli-
able attribution was also taken into consideration. According to Burrows, “with
texts of 1,500 words or more, the Delta procedure is eﬀective enough to serve
as a direct guide to likely authorship” (Burrows 2002: 276). Some researchers
are also suggesting that the sample size should have at minimum 1000 words for
c
⃝Springer International Publishing AG 2018
M. Hadˇzikadi´c and S. Avdakovi´c (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_24

266
E. Konjhodzic
reliability of stylometric analysis (Holmes et al. 2001), but there are also suc-
cessful stylometric studies with sample size under 1000 words. A good example
is the “Reassessing of the Book of Mormon” (Jockers et al. 2008) with variation
of samples between 95 and 3752 words. The corpus that has been used in this
test for the stylometric comparison has size of 136988 words and average sample
size of 6949 words (Table 1).
Table 1. Corpus A of short stories
Original title
Translation
Year
ˇZena od slonove kosti An Ivory Women
1922
Ljubav u kasabi
Love in Kasaba
1923
Mara Milosnica
The Pasha’s Concubine
1926
ˇCudo u Olovu
Miracle in Olovo
1926
Anikina vremena
Anika’s Times
1931
ˇZed
Thirst
1934
Olujaci
Olujaci
1934
Bajron u Sintri
Bayron in Sintra
1935
Zlostavljanje
Maltreatment
1946
Porodiˇcna slika
A Family Portrait
1950
Nemirna godina
An Uneasy Year
1953
ˇZena na kamenu
Women on the Rock
1954
Jelena ˇzena koje nema Jelena, the Women Who is not 1962
Saraˇci
The Tanners
1966
Robinja
The Slave Girl
1976
Finding authorial stylistic ﬁngertips in stylometric studies is based on statis-
tical methods and involves the use of frequencies of the most used words. This
method appeared in the 1964 publication “Inference and Disputed Authorship”
when Frederick Mosteller and David L. Wallace analyzed the disputed author-
ship attribution of the Federalist papers (Mosteller and Wallace 1964).
With new research and progress in natural language processing and machine
learning, today it is possible to explore an increasing number of available elec-
tronic texts and evaluate the distances or diﬀerences between the frequency
data for each text with use of multivariate analysis like multidimensional scal-
ing, principal components analysis or cluster analysis. One of the methods with
very accurate results for comparing diﬀerent texts in computational stylistics is
Burrows’ Delta (Burrows 2002), an authorship attribution method, which is been
methodological accepted in recent years, especially in the quantitative computer
analysis of literary style because of combination of ’high accuracy of supervised

Stylometric Study of Ivo Andri´c’s Short Stories
267
methods of classiﬁcation with simplicity of multidimensional techniques using
distance measures of similarity’ (Eder 2015).
The two primary goals of this stylometric research are author identiﬁcation
and distinguishing of translators. It is used to produce lists of most frequent
words or n-grams and appears to be a very accurate instrument in authorship
attribution and in analysis of elemental aspects of a writer’s or translator’s style.
In the tests were the lists of most frequent words, so-called MFW—list, which had
the size of 500 words analysed and the procedure of Burrow’s Delta implementa-
tion (Burrows 2002) was completely followed. It is very important to emphasize
that the size of the most-frequent-word list in it itself is a matter of strong
disagreement with many variations.
One of two main questions is to analyze correlation of stylometric similarity
between translations and the original texts in proof of the thesis of transla-
tor’s invisibility (Rybicki 2012), meaning that the translators don’t have any
signiﬁcant inﬂuence with their style and choice of words on stylometric results.
Invisibility of translator in stylometric analysis is today one of the biggest obses-
sions of translation studies. The known premise is that the translator’s work is
the best when we don’t recognize any traces of his style at all.
To examine the validity of this assumption, cluster analysis of corpus was per-
formed in the original language and compared the ﬁndings with cluster analysis
of translated short stories (Fig. 1), both with 500 most frequent words with pro-
nouns deleted at culling values of 0. The programming language R was used for
conducting the analysis to check the relationship between texts on the basis of
the most frequent words used in the short stories.
Fig. 1. Cluster analysis—Corpus A
The analysis, which was conducted on the 500 most frequent words, shows
81 similarity and that we are observing some signiﬁcant diﬀerences only in the
three cases with three diﬀerent translators (Miracle at Olovo, The Slave Girl
and Thirst), while the three biggest clusters of short stories are showing obvious
similarities in both languages, especially ﬁrst (Byron in Sintra, An Ivory Women,

268
E. Konjhodzic
The Tanners and The Game) and third (Maltreatment, A Family Portrait, An
Uneasy Year, Women on the Rock and Jelena, the Women Who is Not) with
four of four and ﬁve of ﬁve same elements in the cluster.
While Cluster Analysis tree diagrams have tendency to depend on their con-
ﬁguration, in the second test the same corpora was evaluated with Bootstrap
Consensus Tree performed with 500 MFWs with culling at 10–50% (removed
words that don’t appear in all the texts of corpus). The aim of Bootstrap Con-
sensus Tree’s is to avoid possible outliers with several analyses in a row and
the resulting bootstrap tree is a consensus between possibly diﬀerent ﬁndings
(Fig. 2).
Fig. 2. Bootstrap consensus tree—Corpus A
The second test gave even better results, switching only two pairs of clus-
tered stories in the tree without any correlation with translator. The ﬁndings
statistically conﬁrmed that frequencies of word usage of translators don’t have
any signiﬁcant inﬂuences on stylometric results.
The questioning of the assumption that the publishing year reﬂecting possible
time period of writing should have more impact on stylometric results than
translator’s style was the main purpose of the third test. Four additional short
stories were added to the corpus—two of them from the same translator but
from the diﬀerent time period (ﬁrst from 1922 and second from 1948), and
two from two diﬀerent translators but with same publishing year. As a result
the corpus includes 20 texts, one translator with 5 translations, two translators
with 2 translations each, and eleven translators with one translations. On the
Bootstrap Consensus Tree are their initials paired with publishing year (Table 2).

Stylometric Study of Ivo Andri´c’s Short Stories
269
Table 2. Four additional short stories
Original title
Year
In the Camp
1922
The Snake
1948
The Surveyor and Julka
1976
Loves
1976
Robinja
1976
Fig. 3. Bootstrap consensus tree—Corpus B
Figure 3 is showing that the branches are formed with texts from the similar
time period, in most cases with time distinction before or after World War
II. This could be observed only on the smaller formation of the cluster without
conﬁrmation of clear diachronic line among patterns of occurrence and with three
exceptions already mentioned that could be the cause of individual translator’s
style or possible diﬀerence between publishing year and the time when the story
was in fact written.
The ﬁndings presented in this paper are addressing the assumption that
the stylistic ﬁngerprints of author are dominant over the translator’s style. The

270
E. Konjhodzic
stylistic development of one author during a longer period of time is carrying
more distinctive elements in the frequency of word usage than the individual
style of the translator. The statistical approaches conﬁrmed that the traceable
diﬀerences in the corpora with suﬃcient topic similarity have their source mostly
in style evolution of author and only in exceptional cases in speciﬁc style of
translator.
References
Burrows, J.F.: ‘Delta’: a measure of stylistic diﬀerence and a guide to likely authorship.
Literary Linguist. Comput. 17(3), 267–87 (2002)
Eder, M.: Does size matter? Authorship attribution, small samples, big problem. Digit.
Sch. Humanit. 30(2), 167–182 (2015)
Holmes, D., Gordon, L.J., Wilson, C.H.: A widow and her soldier: stylometry and the
American Civil War. Literary Linguist. Comput. 16(4), 403–20 (2001)
Jockers, M.L., Witten, D.M., Criddle, C.S.: Reassessing authorship of the ‘Book of
Mormon’ using delta and nearest shrunken centroid classiﬁcation. Literary Linguist.
Comput. 23(4), 465–91 (2008)
Mosteller, F., Wallace, D.L.: Inference and Disputed Authorship: The Federalist. Read-
ing, Massachusetts (1964)
Rybicki, J.: The great mystery of the (almost) invisible translator: stylometry in trans-
lation. In: Oakes, M., Ji, M. (eds.) Quantitative Methods in Corpus-Based Transla-
tion Studies. Benjamins, Amsterdam (2012)

Inﬂuence of Information Technology
to Human Resources Management: Key Trends
in 21st Century
Ljubisa Micic(B) and Veselin Radosavac
University of Banja Luka, 78000 Banja Luka, Bosnia and Herzegovina
Ljubisa.micic@ef.unibl.org
Abstract. Information technology (IT) has huge inﬂuence on all parts
of business process and it is impossible to imagine any of the business
aspects that were not, at least in some extends, applied appropriate IT
technology. Human Resources Management (HRM) is not an exception
especially having in mind beneﬁts that information technologies bring to
the quality of the HR processes from recruiting to talent management.
Paper analyzes concept of Human Resources Technology and how tech-
nology made impact to modern Human Resource Management practices.
It gives review of innovative and technological solutions to HR challenges
through company examples and studies presenting impact of IT to HR
management. There will be given analysis of inﬂuence of the cloud com-
puting, smart mobile technologies, online recruiting platforms, big data
and predictive analytics to processes of HR management.
Keywords: Information technology · Human resources · SAAS
1
Introduction
Success of modern companies is in great level dependent on quality level of the
people they hire. Modern HR is not just using technology as a tool to recruit new
employees but also as a tool to modify work environment, motivate employees,
make them more cooperative and collaborative and on the end more satisﬁed
and eﬀective.
Technologies such are cloud technologies, big data, gamiﬁcation of processes,
online tools for interviews and video support are used as a strategic part of
modern human resource technologies. HR managers are more using big data
and social media networks in order to increase quality of selected candidates, to
follow their progress and to measure their eﬀectiveness. It all leads to increase
of usage of auto calculative technologies in order to achieve the most desired
outcome. Big Data can also be used in order to increase level of understanding
of certain job description and activities in order to easier follow instructions and
eﬀectively increase quality of hiring processes. Additionally, it is important for
c
⃝Springer International Publishing AG 2018
M. Hadˇzikadi´c and S. Avdakovi´c (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_25

272
L. Micic and V. Radosavac
employers to accept new rules in modern HRM. One of the things they need to
accept is fact that, according to LinkedIn, 64% of the candidates are looking for
a job on their mobile phones. So HRM needs to become more digital and more
connected to modern achievements based on digital transformation.
Importance of the analytics, especially one based on signiﬁcant data sources,
is becoming crucial input for decision making process and therefore, HR depart-
ments are becoming more aware of importance of data in strategic decisions
especially those which are in great measurement depends on subjective human
perception. Example for this is deﬁnitely use of big data to analyze which are
possible candidates for certain current of future position based on their social
network activity, Internet activity and on-line behavior in general.
Based on Beth Hudson, human resource technology is in constant evolution
(Hudson 2017). As she says optimizations are made based on customer feedback
and one of the most used technology in future will be artiﬁcial intelligence in
order to deliver the most eﬀective results in HR.
Payne (2014) identiﬁed ﬁve main trends in development of HR technology:
1. Cloud Computing
2. Smart Phone and Tablets
3. Predictive Analytics
4. Workforce Dynamics Shift
5. Recruitment Marketplace.
Payne suggests that human resource will use above mentioned technologies in
order to do processes more eﬃciently and quicker. He mentions software as a
service (Saas) as technology that will enable us to lower the costs, enhance
team collaboration and give ability to access information on multiple devices.
Additionally, he predicts that future human resource technologies need to be
mobile friendly especially having in mind that new generation are more device
dependent and use devices even in job searches and job application processes.
He also adds that new generation is paying more attention to work live bal-
ance which is connected with fact that they live and work in more uncertain
time. Therefore, human resources technology needs to be adjusted to that fact.
Timot Geimer from Talentsquare is mentioning several diﬀerences in next 10
years in human resources (Geimer 2016). He is identifying several characteristics
of future HR:
• HR will be social
• HR will be mobile
• HR will be about Big Data
• HR will be integrated (Geimer 2016).
As he said HR technology will have a goal in ﬁrst order to increase collabora-
tion and communication in company and it will he HR responsibility in future
successful companies. He adds that these technologies would be used in order to
boost internal performance and to motivate employees to be brand ambassadors
and company promoters through that social media concept. He adds that social

Inﬂuence of Information Technology to Human Resources Management
273
component of HR will be increasingly important in future (Geimer 2016). Fur-
thermore, he emphasizes that HR tools need to be mobile in future in order to
achieve full eﬀectiveness of HR. HR person need to have application and tools
available at any time and at any place in order to communicate with his/her
team, give tasks, search for new employees etc. Additionally he is pointing that
Big Data will have one of the crucial roles in future HR and this technology will
enable HR managers to get some of the valuable answers such are those con-
nected with not just recruiting but also with other possibly quantitative data
such are length of hiring of a new employee, employees satisfaction and connec-
tion with eﬀectiveness, possible needed future skills etc. Still, as he says, Big data
is meaningless without interpretation and context and therefore HR managers
will have important role in order to interpret those data and make best possi-
ble decisions according to that. He concludes by deﬁning that human resources
department will be consisted not just from HR classic professionals but also tech
people, data scientists, recruiter experts, connectors and brand editors. Future
of HR, based on his opinion, will be integrative and will need diversity of skills
with diﬀerent background (Geimer 2016).
J. P Medved in his article HR Technology Landscape Infographic identiﬁes
547 diﬀerent HR software products across seventeen categories (Medved 2015).
He identiﬁes these main categories:
• Beneﬁts
• Training
• Workforce Management
• Recruiting
• Performance
• Talent Management
But also several subcategories such are:
• Beneﬁt administration
• Compensation Management
• Payroll
• Course Authoring
• LMS
• Onboarding
• Attendance Tracking
• Employee Engagement
• Employee Scheduling
• Time and Expense Tracking
• Workforce Management
• Applicant Tracking
• Job Board
• Talent Management
• 360 Degree Feedback
• Employee Recognition
• Performance Appraisal

274
L. Micic and V. Radosavac
2
Trends in HR Technology
Speaking about HR technology and service delivery, Information Services Group
(ISG) presented main trends in this area through Industry Trends in Human
Resources Technology and Service Delivery Survey (Card and Cadigan 2015),
such as:
a. Enterprises continue to migrate from on-premises Human Resource Manage-
ment Systems (HRMS) to Software-as-a-Service (SaaS)—based HR technolo-
gies; more than 70% of respondents have implemented or plan to implement
HR SaaS within the next two years.
Having in mind such a great percentage of companies which already imple-
mented or have a plan to implement HRMS as SaaS, proves the conclusion
of HRM about the importance of IT solutions, considering their ability to
make processes inside HRM more easy, precise, and relevant.
Fig. 1. Prevalence of HR technology deployment models (Card and Cadigan 2015)
Regarding to Fig. 1 it is obvious that companies are rapidly shifting to SaaS
solutions, but this approach is tight related to company size. Having that
in mind, companies with fewer than 10.000 employees are leading the trend
to SaaS, which proves 43% of organizations this size which use a primarily
SaaS model.
Also, large, global companies are slower in adopting process of full imple-
mentation of SaaS model, naming complications and complexities of cloud
technology, data security and the ability to support diﬀerent regional require-
ments, which prove 10% in 2014 and 17% in 2015 of organizations this size
which adopted SaaS model.
Considering the results of this survey it is clear that 48% of survey respon-
dents are replacing their current HRMS with SaaS model. At the same time,
only 12% of survey correspondents are upgrading their current HRMS. It
is easy to conclude there are more organizations which are ready to replace
their current model, than to upgrade it (Fig. 2).

Inﬂuence of Information Technology to Human Resources Management
275
Fig. 2. Most likely outcome for core HRMS technologies between 2015 and 2018 (Card
and Cadigan 2015)
b. User experience and usability factors are driving selection criteria and
expected beneﬁts for both HR technology and service delivery model deci-
sions.
It is crucial to reassess the implementation of HR activities so they can be
modiﬁed or improved. According to this, use of the beneﬁts, from various
experiences, inﬂuences HR technology and service delivery as well, because
they are important part of modern HRM.
c. HR organizations are shifting their focus from cost savings to strategic busi-
ness alignment, process improvement and employee engagement.
In the past, companies were, especially in HR area, focused on cost sav-
ings, but modern oriented companies realize the importance of non-material
aspects such as: strategic business alignment, brand, and for sure employee
motivation or engagement.
d. The market is seeing initial signs that enterprise-level HR decision making
is becoming increasingly data-driven (Card and Cadigan 2015).
Becoming more rational than emotional in decision making processes in com-
pany, so in HRM as well, is an imperative nowadays. Regarding to previous,
companies realized the importance of data collecting, its sorting, and its
proper use. Analysis based on previous collected data is daily made in com-
panies. Considering that, they are often used as a material for making con-
clusions and decisions.
Same study emphasizes importance of mobile as option not just for HR func-
tions but also in order to follow millennials as future employees and clients.
Previous HR companies provided limited or basic mobile set of functions.
Nowadays, majority of HR functions are integrated part of new HR technol-
ogy solutions even in design phase (Figs. 3 and 4).

276
L. Micic and V. Radosavac
Fig. 3. Perceived need for mobile access (Card and Cadigan 2015)
Fig. 4. How organizations use HR Big Data/Metrics/Predictive analytics to make deci-
sions (Card and Cadigan 2015)
Even the fact that the most of the HR solutions have just limited or basic
operational analytics and reporting, many new solutions provide big data and
advanced analytics as part of oﬀered product. Based on above ﬁgure only 6%
of organizations are analyzing and making predictions about their workforce via
dashboards that contain predictive analytics (Card and Cadigan 2015).
3
Big Data and Cloud Computing in Human Resource
Technology
Scientiﬁc community does not oﬀer unique deﬁnition of Big Data. However, there
is no doubt that certain characteristics are mentioned in most of the deﬁnitions.
Those characteristics are: Volume, Variety, Velocity and Value—co called 4 V of
Big Data (Ma and Jiang 2013). Some authors add complexity as characteristics
which is, especially having in mind size of Big data, always connected to it (Bar-
man and Ahmed 2015). There is no doubt that such big amount of data, applied
in human resource management, can be helpful tool and way to deal with HR

Inﬂuence of Information Technology to Human Resources Management
277
on better way and to do action that lead to more quality decisions. Zang and
Ye are mentioning several applications of Big Data in human resource manage-
ment. They argue that Big Data can be applied in Recruitment of Talents, Talent
Training, Talent Assessment and Career Management (Zang and Ye 2015). In
all three parts of HRM Big Data can contribute in better decision making which
can lead in better performance of employees as well as HR management.
Cloud computing is technology which is quite common considered to be good
addition to application of Big Data in Human resources management. Cloud
computing is and should be treated as cost saving and eﬃciency increase
technology and it is expected that this technology is going to have more appli-
cation in human resource management—the same way we use it in retail and
marketing in combination with Big Data it will ﬁnd its way to position itself as
every day needed technology in HR (Bara et al. 2015).
4
Study Cases: HR Technology Companies and Example
from Bosnia and Herzegovina
4.1
Lifeworks
LifeWorks is one of the most popular HR technology companies nowadays which
provides platform which is based on engagement of employees with integrated
various wellness programs. According to the oﬃcial website, LifeWorks at the
moment serves over 15 million users worldwide, but mostly in USA, Canada and
United Kingdom. Also this company at the moment has 485 employees and 49000
clients, such as Intel, IBM or Marriott etc. Aiming to position the employee at
the heart of their respective organizations, LifeWorks decided to include and
give special treatment to awards in wellness area. Therefore employee wellbeing
is the main goal of this platform. Speaking about success, it is necessary to
mention that this HR tech company is voted as the Next Great HR Technology
Company at HR Tech 2016 (Forbes link). Their unique employee engagement
platform provides:
• Employee Assistance Programs;
• Social Recognition;
• Exclusive Perks and Discounts;
• Private Social Network;
• Employee and Corporate Wellness;
• Employee Engagement Analytics (oﬃcial website).
Based on previous, LifeWorks presents three core values of their platform, which
are:
• Human centered (They strongly believe in keeping their employees, customers
and families at the heart, which is the unique and genuine perspective in
nowadays businesses and personal interactions at all);

278
L. Micic and V. Radosavac
• Inventive (This core value is based on the concept of whole—life learning,
according to it, LifeWorks strive to constantly improve their platform by
creating and developing new solutions);
• Positive (Successful personal interactions are closely related to positive
energy, so LifeWorks recognizes the importance of nurturing and maintaining
an energized, fun and spirited mindset that brings out the best in humans
every day).
Whole LifeWorks platform is based on integration of a holistic approach to well-
being and engagement. This integration contains three main components: My
Life, My Work and My Perks. This means that LifeWorks is unique by combining
personal and professional aspects of employees life in one place.
Therefore it would be important to explain how this platform works. After
downloading app on its smart phone, employee must create its own account.
Creating of own account means that employee is onboarded, and ready to use
this app. Firstly it is oﬀered few relevant areas of content, for example: work,
relationship, body, ﬁnances or mind, so employee can choose one or few of them.
Once employee chooses some content, he or she will be surrounded by topics
which are relevant for previously chosen content. All feeds which are related to
the topics are public and personal all in one place, so employee can choose which
of them should be visible for the rest of the employees of its organization. There
are various forms of feeds, such as audio or video material, newspaper articles
or survey. But all of them are characteristic by duration, never more than a few
minutes by day. The question is how this app can be so successful and how it can
produce such a good eﬀect on employee. The answer is award, at the beginning
of some task which is the main part of the feed; employee can see which award
will be given to him or her. This is the main reason which motivates employee
to be dedicated to use of this app.
Employer is the one who decides which awards will be available, but its
important that awards are related to the chosen topic. For example, if some
employee chooses body as a topic, that means employee can possibly get paid
monthly fee of ﬁtness, or get gift card at some shop of sport equipment.
Regarding to the importance of motivational aspect of employees in HRM,
LifeWorks recognizes awards like discounts, gift cards or paid fees, as an excellent
way to implement these activities in the company. Therefore, satisﬁed and happy
employee is for sure more motivated to give its best and to keep improving its
engagement in the company. Also, it is tricky to realize what the source of
employees motivation is, but platform as LifeWorks is a real example how one
company should try to inﬂuence employees engagement at work and that is
enriching of employees private life, through the newest IT solutions.
4.2
Clinch
Strategic way of design and implementation of the HR activities is an imper-
ative nowadays. Having that in mind, modern business philosophy discovers
a new concepts and models of HRM, like talent acquisition. This concept of

Inﬂuence of Information Technology to Human Resources Management
279
HRM proves the importance of, not only, managing the recruitment cycle and
processing applications, but also prediction and observing not-certiﬁcated skills
of employee, which could be related to the requirements of some position in the
company.
Regarding to this, Clinch is an HR Technology company which founders
realized the previous mentioned importance, so they decided to incorporate con-
cept of talent acquisition into the Clinchs software. This software brings a lot
of beneﬁts beside talent acquisition, it is built as a dynamic career page which
provides information about the company, its philosophy, the mission, the goals,
the employees and the positions.
Clinch was built on the simple idea that a natural ﬂow of information between
candidates and employers leads to a genuine relationship between the two. At
its core this creates employees that are engaged, and are greater suited to their
working environment. Clinch combines advanced content marketing with talent
acquisition technology to create a unique end-to-end experience. Our solution is
backed up by considerable research, development, testing and feedback, some of
the most extensive ever undertaken for a recruitment marketing platform (Clinch
2017). Information such as organization culture, team or clients experiences are
given before applying, and that mechanism is the one which makes this soft-
ware unique. Clinch through this software tried to save the valuable time of the
candidates, but also of the company by giving an insight to the candidates into
the company. This software is primarily intended for small and medium sized
companies, which dont have their own recruiter or HR department. Dynamic
career page such as Clinch provides continual updating, changing and posting
available positions by recruiters.
Beside the previous mentioned mechanism, unique features are one of the
main reasons why this platform is so popular nowadays, so it was nominated for
the Next Great HR Technology Company at HR Tech 2016. (Forbes link) Clinch
features are: Landing Pages, Career Sites, Social Promotions, Email Marketing,
Job Distribution, Analytics, CRM, Employer Branding, Candidate Pipelines,
Talent Network and Mobile Responsive Content (Clinch 2017). In further text
there will be given clariﬁcation of mentioned Clinchs feature.
Leading pages take your recruitment marketing campaigns from concept to
live in minutes with automated content creation (Clinch 2017). Every recruit-
ment campaign is successful if it is based on great content, but creating content
takes time, so Clinch provides recipes content and marketing automation feature.
It is just necessary to select the recipe which best suits to speciﬁc recruitment
marketing needs, and this platform will automatically build the associated land-
ing and jobs pages for user. Some of advantages of this feature are possibility to
measure campaign eﬀectiveness and more eﬀective recruitment. It is possible to
track and measure the performance of each component in some speciﬁc campaign
by Clinch Reports. Speaking about more eﬀective recruitment, it is necessary to
mention that Clinch gives the convenience of creating, managing, sharing and
measuring recruiting marketing content from one location (Clinch 2017).

280
L. Micic and V. Radosavac
Speaking about Career sites, Clinch emphasize that great candidate experi-
ence starts with great content. This feature is intended for candidates who are
seeking information on a company or job. Beneﬁts from this feature are that
Clinch makes it easy for you to get a great careers site live within hours. It
is possible to create, edit and publish quality recruitment content. Clinch Page
Editor doesnt require HTML knowledge, and it is mobile responsive (Clinch
2017). Analytics proves the importance of knowledge. Clinch Reports provides
a real data to shape and inform speciﬁc recruitment marketing strategy. An
optimal candidate experience is built on factual information and insight, rather
than assumption. Clinch Reporting kicks in the second your content goes live
on our platform, delivering real and accurate data on all important aspects of
your recruitment marketing campaign (Clinch 2017). This feature is built from
the ground up. Only data source is this platform, considering this all informa-
tion are accurate and unique to speciﬁc organization. Also, this feature provides
end-to-end reporting, because it considers data from the performance of content
that company pushes out to the number of applications company receives.
4.3
MANIA, Marketing Agency—Case Study from Bosnia and
Herzegovina
Marketing agency MANIA, based in Banja Luka is a small company which unites
at the same time creating of marketing strategies and campaigns, web, but also
graphic design and video production in its working process. Like the other small
companies at the beginning MANIA was dealing with problems in its employ-
ment process, especially in recruitment. They couldnt ﬁnd quality candidates
with proper knowledge and skills for requested job positions by traditional way.
Aﬀected by modern orientation of company and its management, they decided
to look up for some IT solutions, which could help them to ﬁnd in short time
candidates who perfectly match with the job descriptions. So they discovered
Clinch, which is previously explained, and then decided to implement this IT
solution in their employment process. At the beginning the biggest threat was
non-popularity of this IT solution in this region, but luckily there were open-
minded and well informed young people who were already active, and had their
accounts on Clinch, while they were searching for a new job. It will be described
shortly this process. After deﬁning job description, but also skills and knowledge
which are necessary for some position, MANIA use its oﬃcial account on Clinch
so they can search up for relevant candidates. After ﬁnding relevant candidates
they contact them, also by this platform. This is the way by which candidates
also can be informed about company employer. In case that candidate accept
MANIAs oﬀer, employment process continues in traditional way. MANIA already
ﬁnished second circle of employing using this IT solution. Management of this
company emphasizes only advantages of it like simplifying the process of recruit-
ment, and many quality candidates. It was really easy to choose good applicant
who matches with job description perfectly. MANIA constantly follows trends
in HR and IT areas, which is an imperative in nowadays working processes, and

Inﬂuence of Information Technology to Human Resources Management
281
successful implementation of Clinch is a consequence of it., says Danijel Tepˇs´c,
CEO of MANIA during interview we had.
5
Conclusion
Technology is without doubt changing how HR is functioning and what is
expected from this strategic function. From previous basic focus on recruiting
and employees satisfaction, HR using tools such are big data and similar tech-
nologies became an important strategic function in companies. HR due techno-
logical changes is becoming more and more oriented towards big data, predictive
analytics, strategic decisions made based on behavior analysis and data gather-
ing. Additionally, it uses cloud and mobile as a way to increase HR eﬀectiveness
but also to make it more adjusted to HR professionals in order not just to be
more eﬀective but also to get higher level of motivation and commitment of
HR professionals. Furthermore, trough technology HR is admits that new gen-
erations, millennials, are more oriented to have better balance between work
and personal live and that they are motivated to use technology if it will make
their work easier and help them to be more eﬀective and to have more free
time for personal live. Tech solutions that are trying to solve this kind of issues
have a signiﬁcant more chance to succeed in future. HR technology solutions
should embrace new developments in technology integration into HR but also
other parts of doing business and to work on solutions which would not just
bring beneﬁts to businesses but also to individual persons who are employees or
possible candidates for certain current or future positions.
References
Bara, A., Simnca, I., Belciu, A., Nedelcu, B.: Exploring data in human resource big
data. Database Syst. J. VI, 3–10 (2015)
Barman, A., Ahmed, H.: Big Data in Human Resource Management—Developing
Research (2015). https://www.researchgate.net. Accessed 2 Apr 2017
Card, D.M., Cadigan, S.: Industry Trends in Human Resources Technology and Service
Delivery Survey. Information Services Group (2015)
Clinch Homepage (2017). https://www.clinch.io. Accessed 21 Mar 2017
Geimer, T.: (2016). https://www.quora.com. Accessed 15 Feb 2016
Hudson, B.: What is the Future of Human Resources Technology? (2017). https://
www.quora.com. Accessed 11 Mar 2017
Medved, J.: Captera (2015). https://www.capterra.com. Accessed 25 Feb 2017
Ma, J.G., Jiang, W.: The Concept, characteristics and application of big data. Natl.
Def. Sci. Technol. 34, 10–16 (2013)
Payne, K.: Quora (2014). https://www.quora.com/. Accessed 12 Mar 2017
Zang, S.Y., Ye, M.L.: Human resource management in the era of big data. J. Hum.
Res. Sustain. Stud. 3, 41–45 (2015)

Science Battle
Medina Krnic and Belma Ramic-Brkic(B)
Sarajevo School of Science and Technology, Sarajevo, Bosnia and Herzegovina
belma.ramic@ssst.edu.ba
Abstract. Young minds are conditioned and trained in order to provide
the fundamental background knowledge that may one day lead to great-
ness. In this paper we present a 3D game “Science Battle” which is an
innovative attempt to develop an engaging educational tool for children
to learn science. The core concept of this game is to create a learning
environment where children are immersed to the extent of not realizing
that they are in essence learning. Science Battle aims to make the player
visualize scientiﬁc concepts represented by visually appealing 3D char-
acters: robots. The main focus is on teaching various scientiﬁc concepts
by displaying “battles” between the robots. In essence, Science Battle
quizzes the player without having the player realize that he/she is being
tested.
Keywords: 3D games · Education
1
Introduction
To educate means to empower, enable and engage [1]. Civilization needs educa-
tion in order to thrive, develop and get ahead. Young minds are conditioned and
trained in order to provide the background and fundamentals for them to take
that knowledge and turn it into something great. Children tend to ﬁnd school
and education quite tedious from the start. Taking part in the same exact routine
every day would drive anyone to accept it as something that they do not want
to do but are obliged to. Every generation is diﬀerent. This statement has been
proven to be true in many aspects, especially the educational one. Technology
takeovers have contributed a great deal to todays generations mentality [2]. For
this reason, the educational system has been restructured a great deal from the
mere existence of it.
Memorizing book deﬁnitions is considered as the most old-fashioned method
of educating. Trying to implement a more practical educational method for all
areas of education was next to follow in order to give students an actual reason
for which they learn all those matters. The most popular method of teaching
students at a young age is incorporating an entertaining aspect to education
c
⃝Springer International Publishing AG 2018
M. Hadˇzikadi´c and S. Avdakovi´c (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_26

Science Battle
283
given that their attention span and willingness to concentrate on dull matters
is known to be decreasing over time [3]. Visualizing the problem helps children
grasp the matter faster. Visualization incorporates techniques such as drawings,
diagrams and animations to communicate a message to the viewer [4].
In this paper, we present a game developed for children learning science that
could potentially be a useful teaching tool. The object is to stray away from
traditional means of teaching by creating an entertaining game environment
that examines the knowledge of the player and gives explicit feedback all while
providing entertainment and visualization. In Sect. 2, we give a brief overview
of concept of the game and chosen courses. Game environments is explained in
Sect. 3 while in Sect. 4, the development process is explained. Brief discussion is
provided in Sect. 5 while conclusions and future work suggestions are given in
Sect. 6.
2
The Concept of the Game
The aim of this game is to teach children science subjects, namely Biology and
Chemistry. Biology is a subject that is bound to be a part of the educational
experience at one point or another. A subdivision of Biology is the teaching of
human genetics in life sciences. The basics of genetics lecture on human genes;
dominant and recessive traits. Dominant and recessive traits are inherited in
certain patterns from parent to oﬀspring. Common methods of explaining this
from a teachers point of view is the creation of a Punnett square and mapping out
the diﬀerent possibilities between diﬀerent pairs of traits. The Punnett square
depicts the genotype and phenotype, the genetic information responsible for a
trait and the physical expression of the trait, respectively [5]. Although deemed a
great way of really getting across how diﬀerent pairs are possible, it is still tedious
for a student to listen to because it does not provide the proper visualization of
what is actually being taught.
Chemistry is also a compulsory subject in most educational facilities around
the world. The most basic teachings of Chemistry begin with the elements of the
periodic table. Students are usually required to learn either some or all of the
periodic elements names and symbols by heart. This leads to one of the most
common problems with teaching; the tendency to memorize without understand-
ing or learning. The most common method of learning the elements would be to
create some kind of acronym, poem or song out of the elements to help with mem-
orization. The goal of Science battle is to move away from the traditional way of
testing childrens knowledge while creating a visually appealing game that por-
trays scientiﬁc concepts as physical imaginary elements to associate them with.
The game is split into two categories: Biology and Chemistry.
When considering human genetic traits as one of those concepts, it is very
hard to explain it in a way that can be visualized due to the complexity of the
topic. The ﬂowchart of the Biology category is illustrated in Fig. 1.

284
M. Krnic and B. Ramic-Brkic
Students are usually required to memorize some or all of the periodic table
of elements. However, this leads to learning for a grade rather than understand-
ing what is taught. Additionally, usually little to no practice is provided. The
ﬂowchart of the Chemistry learning portion of the game is as shown in Fig. 2.
Fig. 1. Biology ﬂowchart
Fig. 2. Chemistry ﬂowchart

Science Battle
285
3
Game Environment
The game takes place in a 3D modelled environment—a school science lab. The
lab is modelled to resemble a standard science lab in schools around the world.
The tug of war scene takes place on a modelled boxing ring in the middle of the
science lab as shown in Fig. 3. The elements game scene takes place on one of
the tables of the science lab.
Fig. 3. The tug of war 3D scene
4
Development
The 3D modelling performed for this game is done in MAXON Cinema 4D [6].
The basic elements presented in the game include the robots, the environment
(the science lab), and the boxing ring. Science Battle is programmed in C#, in
the gaming engine Unity [7]. Modelling of the robot came with its fair share of
challenges. The goal was to create an AI being to represent the scientiﬁc concepts
without it appearing too aggressive or threatening to children as shown in Fig. 4.
The inspiration for the design of this robot was inspired by a list of the top 15
cutest robots [8,9]. The greatest challenges faced in creating the robot were
regarding the head, the most complicated component of the model. One the
biggest challenges was making the robot eyes seem as if they glow blue in the
dark. The main character also appears in the main menu in Fig. 5.

286
M. Krnic and B. Ramic-Brkic
Fig. 4. Robot
Fig. 5. Main menu
The boxing ring shown was chosen as the battle ﬁeld for this game because
it embodies the essence of the game Science Battle. Although it is a boxing ring
meant to have a boxing match held on it, an unconventional game of tug of war
is to be played on it because it is a less aggressive alternative. In addition, it is a
metaphor for the way traits overtake each other. In a genetic sense, it is indeed a
game of tug of war and while nature follows rules of predictability, there is never
any guarantee. For the sake of the game and explaining dominant and recessive
traits, the outcome of the battle features the dominant trait as the victor. For
each pair of traits, a word problem is given and the answer to the question is
one of the given options. For example, see Fig. 6.
As for the Chemistry elements game, it takes place in front of a modeled
chemistry table with various tools on it as shown in Fig. 7. The three elements
and symbols appear in front of the table and are supposed to be matched to
move on to the next level. For example, see Fig. 8.

Science Battle
287
Fig. 6. Biology game
Fig. 7. Chemistry table
Fig. 8. Chemistry game

288
M. Krnic and B. Ramic-Brkic
5
Discussion
In the initial phase of this project, we have consulted teachers of various subjects
to gather some information and see whether or not they thought it was an
idea worth pursuing. The science teachers believed that the game would be an
excellent practicing tool for younger students who are just being introduced to
the concept of Biology and Chemistry. They claim that approximately at that
stage is where they start to lose the students attention and Science becomes
merely another compulsory school subject rather than interesting facts learned
in school.
We have consulted Dr. Semira Galijasevic, the dean of Sarajevo Medical
School to gather some insight on the subject and suggestions on how to fur-
ther improve the game. “I am really glad that someone is ﬁnally speaking up
on this topic. Educational games seem to have taken over the educational sys-
tem in the United States. My son has in fact spent most of his early years of
education in the United States playing games that relate to everything from
biology and chemistry to geology and other sciences. It is a shame that such
interesting subjects that involve logical thinking and rationalizing are not get-
ting through to the kids. I remember, my son was about ﬁve years old when
they had Thursdays computer class. Imagine walking into a room and all these
preschoolers are on computers and have already mastered the use of it. Since
kids these days are mastering technology so early on, why not use that to our
advantage?” Children that have been learning through games early on become
individuals that are already molded in a certain way that they think logically and
learn through understanding and not memorization. “When it comes to math in
schools in Bosnia and Herzegovina, you either love it and youre an outcast or you
absolutely hate it. Unfortunately, sooner rather than later the outcome is the
latter. In the American school system, everything is highly visualized especially
for the lowest of grades. If you take a look at the textbooks, everything is in
color and there are pictures and graphs everywhere. When you manage to teach
some simple statistics to third graders through visualization alone, you know
that there must be something there that should be further explored and taken
advantage of.” She too believes that compelling students to memorize things
rather than to learn them does not lead to positive interpretation of education
but rather to learn, forget and repeat. “Since educational games seem to be the
future, it does not seem fair that students in certain countries have the advan-
tage of learning through games because the language seem to be adapted to
their respective countries while others do not have that luxury. I do believe that
through visualization in games, we have the power to reach students in a way
that has never been done before. It is important to reach kids while they are
in their formative years because that lays down the foundation and is a huge
inﬂuence on the rest of their lives.” It is safe to say that she believes that an
inclination for any ﬁeld in the early stages of education can inﬂuence interest
and curiosity to pursue a career in said ﬁeld.
There are several alternative solutions that strive to tackle the same area of
education as this game does. However, those solutions have been found to be

Science Battle
289
lacking in one aspect or another. Although those alternatives have been found
satisfactory in the past, educators are constantly making an eﬀort to revolution-
ize educational methods and strive to reach the children in a new and innovative
way. Educational games on the market approach the area of teaching children
through games by creating something that appeals to them. Solutions such as
building make believe monsters based on their made up genetic code and pre-
dispositions are creative and enjoyable. However, regardless of how entertaining
those games may be, they do not quite tackle the study and application of human
genetics.
6
Conclusion and Future Work
In this paper we presented a novel approach in delivering scientiﬁc concepts to
kids. Children are accelerated, in constant move in search for new adventures.
They are born with technology and we have to ﬁnd a way to utilize their energy
and available technology in the best possible manner. Developing serious games
is deﬁnitely one approach.
In the next stage of our project, we hope to incorporate more mini games
inside our game, grasping other scientiﬁc concepts in other branches of science
such as physics. As for the levels that we already have, we hope to add many more
word problems and questions. In addition to that, we also plan on expanding on
each level by adding an explanation or interesting fact after the completion of
each level. We hope to help incorporate this game into educational programs and
suggest that it be used as a teaching aid in educational institutions and perhaps
even test out the results of a group of students learning through conventional
methods versus the group that learned through this game.
References
1. New 3 E’s of Education: Enabled, Engaged, Empowered–How Today’s Students Are
Leveraging Emerging Technologies for Learning. Speak Up 2010 National Findings:
K-12 Students and Parents. https://eric.ed.gov/?id=ED536066
2. Prensky, M.: Digital Game-Based Learning, Games2Train, New York (2003)
3. Kafai, Y.B.: Playing and Making Games for Learning - Instructionist and Con-
structionist Perspectives for Game Studies, University of California, Los Angeles
(2006)
4. Munzner, T.: Visualization Analysis & Design. A K Peters/CRC Press, 1 Dec 2014
5. Understanding Genetics—Genetic Inheritance: Libraryindex.com. https://www.
libraryindex.com/pages/2223/Understanding-Genetics-GENETIC-INHERITANCE.
html (2017)
6. Overview: MAXON | 3D For the Real World. https://www.maxon.net/en/
products/cinema-4d/overview/ (2017)
7. Unity—Game Engine Unity. https://unity3d.com/ (2017)
8. Kumari, Y.: 15 Cutest Robots In The World, TechGYD. http://www.techgyd.com/
15-cutest-robots-world/19143/ (2017)
9. Cdn.techgyd.com: http://cdn.techgyd.com/2015/11/cute-robot.jpg (2017)

Constraint Satisfaction Problem: Professor
Weekly Schedule
Mirna Udovi´ci´c(B)
Sarajevo School of Science and Technology, Sarajevo, Bosnia and Herzegovina
mirna.udovicic@ssst.edu.ba
Abstract. A large number of problems in AI and other areas of com-
puter science can be viewed as special cases of the constraint satisfaction
problem. A number of diﬀerent approaches have been developed for solv-
ing these problems. Some of them use backtracking to directly search for
possible solutions. Intelligent backtracking is used in this paper, but the
algorithm is not standard. A speciﬁc problem of making a weekly sched-
ule for a professor is solved.
Keywords: Constraint · Backtracking · Intelligent backtracking
1
Introduction
Since the ﬁrst formal statements of backtracking algorithms over 40 years ago
[1,2], many techniques for improving the eﬃciency of a backtracking search algo-
rithm have been suggested and evaluated. A fundamental insight in improving
the performance of backtracking algorithms on CSPs is that local inconsistencies
can lead to much trashing or unproductive search [3,4]. A local inconsistency is
an instantiation of some of the variables that satisﬁes the relevant constraints
but cannot be extended to one or more additional variables and so cannot be
part of any solution. Mackworth [4,5] deﬁnes a level of local consistency called
arc consistency. Gaschnig [3] suggests maintaining arc consistency during back-
tracking search and gives the ﬁrst explicit algorithm containing this idea.
Stallman and Sussman [6] were the ﬁrst who informally proposed a nonchro-
nological backtracking algorithm, called dependency directed backtracking, that
discovered and maintained no goods in order to back jump. The ﬁrst explicit
back jumping algorithm was given by Gasching [7]. Gaschings back jumping
algorithm (BJ) is similar to backtracking algorithm, except that it back jumps
from dead ends. However, BJ only back jumps from a dead end node when all
the branches out of the node are leaves; otherwise it chronologically backtracks.
c
⃝Springer International Publishing AG 2018
M. Hadˇzikadi´c and S. Avdakovi´c (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_27

Constraint Satisfaction Problem: Professor Weekly Schedule
291
Prosser [8] proposes the conﬂict directed back jumping algorithm (CBJ), a
generalization of (BJ) to also back jump from internal dead ends.
In this paper, the original algorithm for solving one speciﬁc type of scheduling
problem is presented. The problem was how to make a weekly schedule for a
professor, such that given constraints connected to a schedule are satisﬁed. Since
we are given a condition that constraints are not binary in this example, a
formulation of a problem is completely diﬀerent and it is not possible to apply
any of the methods mentioned above. Also, we assume that all variables are
instantiated at the beginning of the algorithm. Precisely, we chose one faulty
plan, which means that it does not satisfy the conditions required in the task.
Our conclusion is that the approach to solving CSP in this paper is completely
diﬀerent from the previous ones.
2
Backtracking: A Method for Solving CSP
2.1
Preliminaries
A constraint satisfaction problem (CSP) is deﬁned by a set of variables
X1, X2,..., Xn, and a set of constraints, C1, C2, ..., Cm. Each variable Xi has a
nonempty domain Di of possible values. Each constraint Ci involves some subset
of the variables and speciﬁes the allowable combinations of values for that subset.
A state of the problem is deﬁned by an assignment of values to some or all of
the variables, {Xi = vi, Xj = vj, . . . } . An assignment that does not violate any
constraints is called a consistent or legal assignment. A complete assignment is
one in which every variable is mentioned, and a solution to a CSP is a complete
assignment that satisﬁes all the constraints.
In a literature, a discussion to CSPs is restricted to the discussion to problems
in which each constraint is either unary or binary. It is possible to convert CSP
with n-ary constraints to another equivalent binary CSP (Rossi, Petrie, and Dhar
1989). Binary CSP can be depicted by a constraint graph in which each node
represents a variable, and each arc represents a constraint between variables
represented by the end points of the arc.
2.2
Chronological Backtracking
CSP can be solved using generate and test paradigm. In this paradigm, each
possible combination of the variables is systematically generated and then tested
to see if it satisﬁes all the constraints. The ﬁrst combination that satisﬁes all the
constraints is the solution.
A more eﬃcient method uses the backtracking paradigm. In this method,
variables are instantiated sequentially. As soon as all the variables relevant to a
constraint are instantiated, the validity of the constraint is checked. If a partial

292
M. Udovi´ci´c
instantiation violates any of the constraints, backtracking is performed to the
most recently instantiated variable that still has alternatives available. Clearly,
whenever a partial instantiation violates a constraint, backtracking is able to
eliminate a subspace from the Cartesian product of all variable domains.
Although backtracking is strictly better than the generate and test method,
its run time complexity for most nontrivial problems is still exponential. The
main reason for this is that when using BT we suﬀer from trashing.
In the Fig. 1 a fragment of the backtrack tree generated by the chronological
BT algorithm for the 6-queens problem is shown. We see that, for example,
a node labeled 25 consists of the set of assignments {x1 = 2, x2 = 5} . White
dots denote nodes where all the constraints with no uninstantiated variables are
satisﬁed (no pair of queens attack each other).
2.3
Non Chronological Backtracking
A no good is a set of assignments and branching constraints that is not consistent
with any solution. Non chronological backtracking algorithms can be described
as a combination of a (1) strategy for a discovering and using no goods for back
jumping and (2) a strategy for deleting no goods from the no good database. Non
chronological backtracking algorithms are also called intelligent backtracking
algorithms.
A Back jumping algorithm (BJ) is similar to a chronological backtracking
algorithm, except that it back jumps from dead ends. However, BJ only back
jumps from a dead end node when all the branches out of that node are leaves:
otherwise, it chronologically backtracks.
CBJ checks backwards from the current variable to the past variables. If
atrial instantiation of Vi is inconsistent with respect to some past variable Vg
where g < i, then the index g is added to the conﬂict set CSi of variable Vi. On
reaching a dead end on Vi, CBJ jumps back to Vg where g is the largest value in
CSi. On jumping back to Vg the conﬂict set CSi is updated such that it becomes
the union of the conﬂict sets with index g removed. Conﬂict sets below Vg in a
search tree are then annulled. If on jumping back to Vg there are no values left
to be tried CBJ jumps back again to Vf, where f is the largest value in CSg.
In the example shown in the Fig. 1, the light shaded part of a tree contain
nodes that are skipped by Conﬂict Directed Back jumping (CBJ). A back jump
is represented by a dashed arrow. In contrast to CBJ, BJ only back jumps from
dead ends when all branches out of the dead end are leaves. The dark shaded
part of a tree contains two nodes that are skipped by Back-jumping (BJ). Again,
a back-jump is represented by a dashed arrow.

Constraint Satisfaction Problem: Professor Weekly Schedule
293
Fig. 1. A fragment of the backtrack tree for the 6-queens problem
In this paper, intelligent backtracking is used but approach to a problem
is completely diﬀerent. A main diﬀerence can be seen from a formulation of a
scheduling problem: given constraints are not binary so we can not use any of
the methods mentioned above.
3
One Example of Non Chronological Backtracking:
Weekly Schedule
3.1
Description of a Task
A result in this paper is a new algorithm for solving one speciﬁc scheduling
problem. Intelligent backtracking is used but approach to a problem is com-
pletely diﬀerent. A main diﬀerence can be seen directly from a formulation of a
scheduling problem: given constraints are not binary so we can not use any of
the methods mentioned above.
A detail description of the problem follows. It should make a weekly schedule
for a professor, such that some basic conditions for a professor week activities
are satisﬁed. Let us describe these basic conditions or constraints.
Each day of a week involves a set of choices for the following activities:
1. learning foreign languages
2. sport
3. teaching

294
M. Udovi´ci´c
• Tuesday, Wednesday and Thursday are the days reserved for teaching
• Monday and Friday are the days reserved for sports activities and also for
learning foreign languages
The previous is presented by the table below.
Monday
Tuesday
Wednesday Thursday Friday
Sport
Teaching Teaching
Teaching Sport
f. languages
f. languages
So, there are three kinds of professor activities. Each activity can be measured
in so called activity units. Also, a sum of money that a person needs to spend
every week is denoted by expenses and it is limited.
We are given the following set of choices for sport:
Sport
Sport units Expenses
Running
15
$0
Swimming 20
$0
Skating
30
$25
We are given the following set of choices for learning a foreign language:
Language Language units Expenses
German
4
$25
Spanish
2
$0
English
0
$0
We are given the following set of choices for teaching:
Teaching (h) Teaching units
Teaching 0
0
Teaching 4
4
Teaching 8
8
Teaching 12
12
The problem is how to make a weekly schedule, having at least k units of
teaching, m units of learning a foreign language and n units of sport. Expenses
must be limited to $t per week. Without loss of generality, let us assume that

Constraint Satisfaction Problem: Professor Weekly Schedule
295
Monday
Tuesday
Wednesday
Thursday
Friday
Running Teach 0 h Teach 0 h
Teach 0 h
Running
German
German
we are given the previous values: k = 12, m = 4, n = 40 and t = $40. We will
solve the problem using intelligent backtracking. The ﬁrst schedule could be any
schedule, we have chosen the following one.
This plan is faulty, because, for example, expenses are $50, but they mustn’t
be bigger than $40. We must ﬁx this plan by changing the choice which is
connected to the problem. In this plan, that choice could be:
• Learn a foreign language: learn German on Monday
• Learn a foreign language: learn German on Friday
We will change one of the language choices to learn Spanish. After this
change, the expenses are smaller, and it should check whether the new plan
is the solution.
The teaching days: Tuesday, Wednesday, and Thursday are independent from
the days for sport and foreign languages: Monday and Friday.
3.2
Solvable Algorithm
Now we will explain how this algorithm works. The ﬁrst part of a program
which gives a solution for Tuesday, Wednesday, and Thursday is trivial. A main
function in a Program is a function change which gives a solution for Monday
and Friday, if the solution exists. If it doesn’t exist, a function change returns 0.
Let us explain a function change in details. It is a recursive function. The
input values are variables mon and fri, which are initialized at the beginning
schedule for Monday and Friday. The beginning schedule could be faulty for
three reasons:
1. the expenses are not allowed
2. there isn’t enough sport units
3. there isn’t enough language units
So, a function change consists of three main parts. In the ﬁrst part it is
checked whether the expenses are allowed. If that’s not the case, we are changing
one professor activity to another which coasts less. After this change, we have
a new schedule, and it should check whether it is the solution (using a function
checking). If this schedule is the solution, function prints that and returns 1. In
the opposite case, function change is called recursively and is trying to improve
a current schedule, if possible. Every new schedule is better than a previous one.
Also, we have denoted a part of this function which changes one day activity to
another activity by ChangeMon (Change Fri).
The second and third part of a function change are similar to the ﬁrst one.
The only diﬀerence is the constraint that is checked, and we are changing a
diﬀerent activity connected to the constraint.

296
M. Udovi´ci´c
The algorithm is given below.
//function checking checks whether a current schedule for Monday and
//Friday is the solution
checking (mon,fri);
//mon is schedule for Monday
//fri is schedule for Friday
begin
if (all constraints are satisfied)
return 1;
// given schedule is the solution
else return 0;
// given schedule is not the solution
end
//Additional part of a function change
//ChangeMon(activity,choice,new choice)
//consists of the following set of lines
if (mon[activity] = choice)
begin
put next value mon[activity] = new choice
//the schedule is improved now
p = checking(mon,fri)
if
p = 1
begin
print(mon,fri)
//found a
solution
return 1
end if
else
begin
p = change(mon,fri)
if p = 1 return 1
end else
end if
// Part ChangeFri is the same as ChangeMon
// function which returns 1 if there is a solution, and prints
the result
// otherwise, it returns 0
change(mon,fri)
// mon is schedule for Monday
// fri is schedule for Friday
begin
if (expenses > 40)
begin
ChangeFri(sport,SKATING,SWIMMING)
ChangeFri(language,GERMAN,SPANISH)
ChangeMon(sport,SKATING,SWIMMING)
ChangeMon(language,GERMAN,SPANISH)
end if
//if expenses are allowed, we are checking other conditions
//we are checking if sport units ≥40
if (sport units < 40)
begin
ChangeFri(sport,RUNNING,SWIMMING)
ChangeMon(sport,RUNNING,SWIMMING)
ChangeFri(sport,SWIMMING,SKATING)
ChangeMon(sport,SWIMMING,SKATING)
end if
//if expenses are allowed, and there are enough sport units,

Constraint Satisfaction Problem: Professor Weekly Schedule
297
//we are checking if language units ≥4
if (language units < 4)
begin
ChangeFri(language,ENGLISH,SPANISH)
ChangeMon(language,ENGLISH,SPANISH)
ChangeFri(language,SPANISH,GERMAN)
ChangeMon(language,SPANISH,GERMAN)
end if
return 0;
// no solution
end
Program
begin
//initialize the beginning schedule
mon[sport] = RUNNING
mon[language] = GERMAN
fri[sport] = RUNNING
fri[language] = GERMAN
//a part of a program which solves teaching days is trivial
change(mon,fri);
end.
Now let us prove the correctness of the algorithm. Obviously, we should only
analyze a main function change. The ﬁrst condition in a function is related to
expenses and it is the most complex one. We see that a maximal number of
changes of a schedule in this ﬁrst part is four. If, for example, the beginning
schedule is one in which all four changes will be made, then a new schedule
(Monday: SWIMMING, SPANISH, Friday: SWIMMING, SPANISH) is a solu-
tion and a function prints a result. Analysis will be similar if a number of realized
changes in the ﬁrst part would be less than four.
In the opposite case (a solution is not found in the ﬁrst part), a condition
related to a number of sport units is checked. We know that expenses are allowed
now. The procedure is the same again. If an adequate change in a schedule is
made, then it is checked whether a new plan is the solution.
In case a solution is not found in the second part, a condition related to a
number of language units is checked.
If a solution does not exist, a function returns 0.
Note that expenses stay the same when applying any of the changes related
to a number of sport or language units. So, once expenses are allowed they will
not be checked again. Also, the same holds for the sport and language units,
since they are independent.
We can conclude from the previous that a function change is correct and
more precisely has a linear complexity O (n) .
3.3
Implementation
The algorithm is implemented in the programming language C++.
Input: the beginning schedule for Monday and Friday

298
M. Udovi´ci´c
Output: A new weekly schedule
Monday: sport = SWIMMING, language = SPANISH
Tuesday: teach 4 h
Wednesday: teach 4 h
Thursday: teach 4 h
Friday: sport = SWIMMING, language = GERMAN
References
1. Davis, M., Logemann, G., Loveland, D.: A machine program for theorem-proving.
Commun. ACM 394–397 (1962)
2. Golomb, S., Baumert, L.: Backtracking programming. J. ACM 12, 516–524 (1965)
3. Gaschnig, J.: A constraint satisfaction method for inference making. In: Twelfth
Annual Allerton Conference on Circuit and System Theory, pp. 866–874, Monticello,
Illinois (1974)
4. Mackworth, A.K.: Consistency in networks of relations. Artif. Intell. 8, 99–118
(1977)
5. Mackworth, A.K.: On reading sketch maps. In: Fifth International Joint Conference
on Artiﬁcial Intelligence, pp. 598-606, Cambridge, Mass (1977)
6. Stallman, R.M., Sussman, G.J.: Forward reasoning and dependency directed in a
system for computer aided circuit analysis. Artif. Intell. 9, 135–196 (1977)
7. Gaschnig, J.: Experimental case studies of backtrack vs. Waltz-type vs. new algo-
rithms for satisfying assignment problems. In: Proceedings of the Second Canadian
Conference on Artiﬁcial Intelligence, pp. 268-277, Toronto (1978)
8. Prosser, P.: Hybrid algorithms for the constraint satisfaction problems. Comput.
Intell. 9, 268–299 (1993)

Posture Activity Prediction Using Microsoft
Azure
Mirza ˇCuri´c(B) and Jasmin Kevri´c
International Burch University, Sarajevo, Bosnia and Herzegovina
mirza.curic@stu.ibu.edu.ba, jasmin.kevric@ibu.edu.ba
Abstract. Recently research on Human Activity Recognition (HAR)
has been reported on systems showing good overall recognition perfor-
mance. A machine learning based HAR classiﬁer was proposed in several
experimental setups. A public domain dataset comprising 165,633 sam-
ples was used for this purpose. Models of machine learning algorithms
are built up using Azure Machine Learning studio. Based on the men-
tioned dataset, and previous work we have done 5 experiments. First,
we have done experiments for classifying suitable algorithms for further
experiments. Other experiment is trained on male data, tested on female
data and vice versa. Than, we separated each subject from whole dataset.
Each of them was used as a test model while other 3 subjects were in
train model. In the last experiment each subject data is trained and
tested separately. It achieved the highest overall performance. Currently,
it is not possible to build subject-independent method for posture activ-
ity detection.
1
Introduction to Human Activity Recognition
1.1
HAR Design
There are seven fundamental issues related to human activity recognition:
1. Attributes and sensors selection can be separated into four groups as follows:
a. Environmental attributes give context information with description of the
individual’s environment, such as temperature, humidity, sound level, and
so forth...
b. Acceleration—The most comprehensively utilized sensors to perceive
ambulation exercises (for example walking, running, lying. . .), are called
triaxial accelerometers [1–4].
c. Locations are empowered by Global Positioning System (GPS).
d. Physiological signals are vital signs data such as hearth and breath rates,
ECG, skin temperature and conductivities.
2. Obtrusiveness is related to reducing the complexity of a system and energy
consumption as less amount of processed data and comfort that is achieved
with the limitation sensors number that are required for activity recognition.
c
⃝Springer International Publishing AG 2018
M. Hadˇzikadi´c and S. Avdakovi´c (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_28

300
M. ˇCuri´c and J. Kevri´c
3. Protocol of collecting data can be critical if is followed by individuals during
a procedure. Quantity of subjects and their physical characteristics are also
crucial factors.
4. Recognition performance relies on: an activity set, the quality of data col-
lected, a feature extraction method and learning algorithm.
5. Reduction of energy consumption in many situations is amplifying the battery
life. It is the most desired feature especially in military applications and
medical where critical information is exchanging.
6. Processing is a part where HAR systems running on a cell phone ought to
signiﬁcantly decrease energy consumptions, to prevent sending raw data to a
server for processing.
7. Flexibility is related to evaluation of activity recognition systems. It uses two
types of analysis:
a. Subject-dependent evaluation—for each individual a classiﬁer is trained
and tested with using his or her own data and the average accuracy is
computed for every subject.
b. Subject-independent evaluation is a single classiﬁer built for all individ-
uals with usage of leave-one-individual-out analysis (cross validation).
1.2
HAR Methods
To enable the recognition of human movements, raw data needs to pass through
FE process. Then, the recognition model is built from the feature instances set
with usage of machine learning techniques. Unseen instances (i.e., time win-
dows) In the recognition model might be evaluated when the model is trained,
and incoming a prediction on the performed movement. Feature extraction and
learning are the most noticeable approaches.
Feature extraction method (FE) acts like a ﬁlter for vital signs data. It is
also used to collect quantitative measures that permit comparison of signals. It
has two approaches and the nature of a given signal is the criterion to decide
between these two methods. Statistical methods, such as the Wavelet transform
or the Fourier transform, utilize quantitative qualities of the data in order to
extract features while structural approach considers the interrelationship among
data.
There are two approaches of learning method, supervised and unsupervised
learning, managing labeled and unlabeled data, separately. Most HAR systems
work in a supervised manner while human activity recognition systems should
have labels, such as, sitting, walking, running... In a totally unsupervised setting,
sometimes is very diﬃcult to separate exercises. Evaluation of HAR systems can
operate in online and oﬄine manner. Online activity recognition systems get
data from patients constantly after short period of time, for example consis-
tently observing patients with mental or physical pathologies. In oﬄine mode,
the subject doesn’t need to get prompt input, for example the number of calories
burned after an exercise.

Posture Activity Prediction Using Microsoft Azure
301
2
Materials and Methods
2.1
Decision Treee Ensemble Classiﬁers
A set of decisions or questions and their possible outcomes arranged in a progres-
sive parting is called a decision tree. Those are ensemble models and diﬀerences
are how the decisions are chosen or sampled. Decision forest or random for-
est and boosted decision tree or AdaBoost are two very powerful and popular
algorithms. These tree-based algorithms are mostly used for classiﬁcations or
regression problems.
2.2
Boosted Decision Tree
An iterative strategy that combines numerous feeable classiﬁers to get an approx-
imation of the Bayes classiﬁer C ∗(x) is called AdaBoost algorithm. Beginning
with the unweighted train sample, the AdaBoost creates a classiﬁer, for instance
a classiﬁcation tree that produces class labels [5]. If a train data point is mis-
classiﬁed, the weight of that train data point is increased (boosted). Another
classiﬁer is fabricated utilizing the new weights, which are do not equivalent any-
more. Again, misclassiﬁed train data have their weights boosted and the same
procedure is repeated over again. Commonly in this way, one is able to construct
500 or 1000 classiﬁers. The last classiﬁer is characterized as the straight com-
bination of the classiﬁers from each stage whereas a score is assigned to every
single classiﬁer. In particular, let assume that T(x) is a feeble multi-class clas-
siﬁer, assigned a class label to x. The AdaBoost algorithm procedure continues
as follows:
1. Initialization of the observation weights wi = 1
n, i = 1, 2, . . . , n.
2. For m = 1 to M:
(a) Utilize weights wi for ﬁtting a classiﬁer T (m)(x) to the train data.
(b) Compute
err(m) =
n

i=1
wi||(ci ̸= T (m)(xi))
wi
(1)
(c) Compute
α(m) = log 1 −err(m)
err(m)
(2)
(d) Set
wi ←wi · exp

α(m) · ||

ci ̸= T (m)(xi)

, i = 1, 2, . . . , n
(3)
(e) Re-normalize wi.
3. Output
C(x) = arg maxk
M

m=1
α(m) · ||(T (m)(x) = k)
(4)

302
M. ˇCuri´c and J. Kevri´c
2.3
Decision Jungle
After the famous decision forests, decision jungles are randomized ensembles of
rooted decision directed acyclic graphs (DAGs). The formulation of the learning
task each DAG in a jungle as the minimization energy problem. Expanding on
the data pick up measure normally utilized for train process of decision trees,
an objective is proposed that is characterized mutually across the features that
are consisted of the structure of the DAG and split nodes.
Every rooted decision DAG is trained in a jungle separately, however there
is extension for converging DAGs as future work. Method used for train DAGs
works is developing the DAG one level at any given moment [6]. The algorithm
is mutually trained for branching structure of the nodes and the features on
every level. This might be done with minimization an objective function which
is characterized over the expectations. It is made by the child nodes coming from
the nodes whose split features are learned [7].
2.4
Multiclass and Binary Classiﬁcations
Every training direct has a place toward one of N diverse classes. The objective
is to develop a function which will accurately foresee the class to which the new
point has a place, where a new data point is given.
There are numerous situations that own diﬀerent classes to which focuses
have a place, however a given indicate can have a place various classiﬁcations.
In its most fundamental form, issue breaks down trivially into an arrangement
of unlinked two class issues, that might be resolved naturally utilizing these
techniques for binary classiﬁcations.
Assume that, for each of the N classes the density is known pi(x). At that
point, prediction is done with utilizing
f(x) = arg
max
i∈1,2,...,N pi(x)
(5)
Multiclass classiﬁcation problem can be decayed into few paired arrangement
assignments, and then they might be resolved eﬀectively by using of binary
classiﬁers. The thought is like that of utilizing codewords for each class and
afterward utilizing a number paired classiﬁers in taking care of a few problems
of binary classiﬁcations, whose outcomes can decide the class mark for new data.
2.5
One-Versus-All (OVA) and All-Versus-All (AVA)
The most straightforward approach is to diminish the classifying problem
between K—classes into K—binary problems. Each problem separates a speci-
ﬁed class from the other classes [8]. With this process of approach, N = K binary
classiﬁers are required, in which the kth classiﬁer has been trained with positive
instances that appertain to a class k while negative instances appertain to the
other K −1 classes. A classiﬁer delivering the most extreme output is viewed

Posture Activity Prediction Using Microsoft Azure
303
as the champ when testing an unknown example, and this class name is allot-
ted to that case. In a work [8], stated this approach, albeit straightforward, the
binary classiﬁer is tuned well in a case when it gives execution that is practically
identical to other more confused methodologies.
In the AVA approach, each class is compared to each other class. A binary
classiﬁer is built to discriminate between each pair of classes, while discarding
the rest of the classes. This requires building K(K−1)
2
binary classiﬁers. When
testing a new example, a voting is performed among the classiﬁers and the class
with the maximum number of votes wins. Results [9,10] show that this approach
is in general better than the one-versus-all approach.
2.6
Microsoft Azure Machine Learning
A cloud service for predictive analytics that makes it conceivable to rapidly make
and convey prescient models as analytics solutions is called the Azure Machine
Learning. There are set of a ready-to-use library of algorithms that is possible
to work with, that we utilize them to build up models on a web associated PC,
and convey your prescient solution quickly.
Azure Machine Learning Studio is user—friendly visual workspace. One of
the greatest advantage provided is the ability of iterating on our model design,
editing the experiment, save a copy if desired, and start it again if we wish.
We can transform our training experiment into a predictive experiment. Then
publish it as a web service in order to make our model accessible to others who
wishes to work with our particular model. Programming is not required, there is
only connecting datasets or model blocks or modules in order to construct our
predictive model for analysis [11].
Azure Machine Learning Studio has accessible a huge number of machine
learning algorithms, along with modules that assistance with data input, prepa-
ration, output, and visualization. Usage of these components, development of a
predictive analytics experiment, iteration on it, and usage of it during a process
of train model [11].
3
Results
Wearable device, consisted of the 4 tri-axial accelerometers ADXL335 and a
microcontroller ATmega328V, was used in the data collection process. Lilypad
Arduino toolkit posse all modules used for an experiment [12].
Process of data collection last about 8 h of activities, 2 h with each of the
4 subjects: 2 women and 2 men, all healthy adults. Each subject was asked to
play out every activity (“sitting”, “sitting down”, “standing”, “standing up”,
“walking”) independently. Even the small number of subjects, but the amount
of collected data is sensible (2 h per each subject). In total, 165,633 samples
collected for the particular study. The dataset was stored in CSV (Comma Sep-
arated Value) format [12].

304
M. ˇCuri´c and J. Kevri´c
3.1
Experiments
According to the overall results we got from all experiments, we can conclude
that Binary classiﬁers obtained better results than Multiclass classiﬁers. One-
vs-all is a binary classiﬁer that decides between two states, one class versus all
others. Multiclass classiﬁers have to mark oﬀbetween more than two states. This
can be an answer why binary classiﬁers obtained better results than multiclass
classiﬁers. Decision tree classiﬁers that is especially suitable for large datasets
(Fig. 1).
The ﬁrst experiment included many algorithms for both binary and multiclass
classiﬁcations. The dataset containing 165,633 samples was randomly divided
into 80% training and 20% testing set. An idea of this experiment was to decide
which algorithms are going to be used for further experiments. According to
the Fig. 2 it is obvious that “Tree” algorithms gave the best result as it is also
conﬁrmed in the second experiment where we boost. Reasons for it can be that
this dataset ﬁts those algorithms better than others because of its huge number
of samples.
90
91
92
93
94
95
96
97
98
99
100
Multi
Class
Binary
Class
Multi
Class (3-
folds)
Binary
Class (3-
folds)
DECISION JUNGLE
DECISION FOREST
NEURAL NETWORK
PERCEPTRON
BAYES
BOOSTED DECISION
TREE
SVM
Fig. 1. Results of experiment 1 and experiment 2
In Experiment 3, dataset is separated into two parts, male dataset and female
dataset. In one case, male dataset is used as train dataset while female dataset
is used for test and vice versa. We couldn’t obtain such a good result in this
case. We have an interesting situation in case where female dataset is used as
test dataset. In particular, there was a noticeable diﬀerence in accuracy values
for classiﬁers used. The reason for this is having one older person who has less
data than other much younger patients in male dataset. Huge range of years
and smaller number of samples can be a generator of these variations during
experiment.
In Experiment 4, each person is treated as test dataset, while others all
together were observed as train dataset. Results obtained were not so satisfying.
As I said in previous sentence, and as we could see in experiment related to male

Posture Activity Prediction Using Microsoft Azure
305
and female datasets, it is not possible to train on one or many person dataset
and test on another person. Since we have four persons, I covered four cases.
Three of them gave almost the uniform results except one of them. It is a case
with an old person with less samples related to others. Again, I got a set of
results which shows that years of age and number of samples aﬀects results a
lot.
70,00
71,00
72,00
73,00
74,00
75,00
76,00
77,00
78,00
79,00
80,00
81,00
82,00
83,00
84,00
MultiClass
DECISION FOREST
BinaryClass
BOOSTED
DECISION TREE
BinaryClass
DECISION FOREST
Fig. 2. Results of experiment 3 and experiment 4
As a last experiment and as a conﬁrmation on a story, each person is take oﬀ
from original dataset in order to get dataset that contain only data related to
a single person. Each dataset is randomly divided into 80% train and 20% test
datasets. Results obtained are really good, where is obvious that mixing data of
two or more persons, negatively aﬀects results. Even transitional classes, “sitting
down” and “standing up”, are very well classiﬁed. Moreover, these transitional
classes achieved greater results than those ﬁxed (Fig. 3).
99,50
99,55
99,60
99,65
99,70
99,75
99,80
99,85
99,90
99,95
Subject 1
Subject 2
Subject 3
Subject 4
MultiClass DECISION
FOREST
Binary BOOSTED
DECISION TREE
Binary DECISION FOREST
Fig. 3. Results of experiment 5

306
M. ˇCuri´c and J. Kevri´c
4
Conclusion
In a process of developing algorithms, it is very important to include data directly
related to a test person. We can use existing dataset but data related to a person
needs to be included in the train process. The ideal case if we train and test
algorithms on the person dataset. As we could see through our experiments that
the best results comprised where we used a person dataset from train and test.
Even transitional classes gave highest results.
References
1. Bao, L., Intille, S.S.: Activity recognition from user-annotated acceleration data.
Pervasive (2004)
2. He, Z.-Y., Jin, L.-W.: Activity recognition from acceleration data using ar model
representation and svm. In: International Conference on Machine Learning and
Cybernetics (2008)
3. He, Z., Liu, Z., Jin, L., Zhen, L.-X., Huang, J.-C.: Weightlessness feature; a novel
feature for single tri-axial accelerometer based activity recognition. In: 19th Inter-
national Conference on Pattern Recognition (2008)
4. Chen, Y.-P., Yang, J.-Y., Liou, S.-N., Lee, G.-Y., Wang, J.-S.: Online classiﬁer
construction algorithm for human activity detection using a tri-axial accelerometer.
Appl. Math. Comput. (2008)
5. Breiman, L., Friedman, J., Olshen, R., Stone, C.: Classiﬁcation and Regression
Trees. Wadsworth, Belmont CA (1984)
6. Amit, Y., Geman, D.: Randomized inquiries about shape; an application to hand-
written digit recognition. Technical Report 401. University of Chicago, Dept. of
Statistics (1994)
7. Shotton, J., Sharp, T., Kohli, P., Nowozin, S., Winn, J., Criminisi, A.: Decision
jungles: compact and rich models for classiﬁcation, Microsoft Research
8. Rifkin, R., Klautau, A.: Parallel networks that learn to pronounce. J. Mach. Learn.
Res. 101–141 (2004)
9. Allwein, E., Shapire, R., Singer, Y.: Reducing multiclass to binary: A unifying
approach for margin classiﬁers. J. Mach. Learn. Res. 113–141 (2000)
10. Hsu, C.-W., Lin, C.-J.: A comparison of methods for multiclass support vector
machines. In IEEE Trans. Neural Netw. 415–425 (2002)
11. Barnes, J.: Microsoft Azure Essentials: Azure Machine Learning. Microsoft Press
(2015)
12. Ugulino, W., Cardador, D., Vega, K., Velloso, E., Milidiu, R., Fuks, H.: Wearable
computing: accelerometers’ data classiﬁcation of body postures and movements. In:
2012 Proceedings of 21st Brazilian Symposium on Artiﬁcial Intelligence. Advances
in Artiﬁcial Intelligence–SBIA 2012

Hybrid Comparative Predictive Modeling
Mohammad Asif Nawaz and Mirsad Hadzikadic(&)
UNCC, Woodward Hall 343-A, Charlotte, USA
mirsad@uncc.edu
Abstract. In this research, a hybrid predictive model was proposed for the
language assignment decision-making process where predictive modeling is
primarily based on personality type and learning style preference attributes.
Predictive models can be built through the use of machine learning using dif-
ferent classiﬁers/algorithms to predict results as well as provide recommenda-
tions to management for placement in appropriate language programs of study
and students for the adoption of appropriate study strategies and habits. A pre-
dictive model through machine learning was used in conjunction with proba-
bilistic classiﬁcation and clustering of speciﬁc segments within the data in order
to increase the rate of success for an improved decision-making process.
Variance in the actual and predicted results with respect to the difference in
success rates can assist the decision makers in student placement. An aggregate
of all the processes with the help of Cobb-Douglas utility function leads to a
Hybrid Predictive Model, which combined two different phases for better
placement,
an
increased
rate
of
success,
and
an
overall
improved
decision-making process. The introduction of Cobb-Douglas utility function can
further streamline the process to check any external factors that may have
inﬂuenced the predicted results.
Many teachers acknowledge the importance of learning styles preferences (LS) and
personality types (PT) in learning. However, devising a model that utilizes students’
personality type and learning style preference (PTLS) can both assist in predicting
student success and aide decision makers in placing a student in a program of study
where they are more likely to succeed. Further, by analyzing this success rate, trained
teachers can assist the students in adapting the attributes of the most successful PTLS
within a speciﬁc program. This analysis can be done by creating speciﬁc predictive
models for classiﬁcation, clustering, and association using Machine Learning tech-
niques such as Support Vector Machine (SVM) and Decision Tree (DT) algorithms and
classiﬁers. Machine learning results can further be combined with probabilistic clas-
siﬁcation to determine the success rates within each personality type segment. In this
research, the focus was only on speaking skills because the Oral Proﬁciency Interview
(OPI) is the predictor, dependent variable, and the benchmark to measure the success
outcome. In this study, the success outcome is considered Interagency Level Round-
table (ILR) level 2 or above.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_29

1
Motivation and Rationale for This Research
The motivation for this study was to explore the results with reference to PTLS
preferences and to ﬁnd any new or non-traditional ways to achieve a better success rate
in particular subjects with respect to those preferences. The motivation was also to
increase the success rate of the students in their ﬁeld of study (target language). Success
as determined by OPI score.
2
Purpose of the Study
The purpose of the study was to ﬁnd an improved method of predicting results and to
assist decision makers in recommending students for placement in different programs to
achieve the best results. Another objective was to share the ﬁndings and the recom-
mendations with the trainers in order to help the students improve their learning by
comparing and adapting their PTLS preferences to the PTLS that performed the best in
a speciﬁc program. Students can adopt behaviors similar to those exhibited by students
who have been successful. Additionally, a student who knows the strengths and
weaknesses of their particular learning preference can also modify their behavior in
order to learn more efﬁciently.
3
Statement of the Problem
How to maximize the learning potential of the students by placement based on their
personality types and learning styles preferences through predictive modeling?
4
Research Objective
Increase language program efﬁciency by developing a student placement model that
would help predict language results with respect to the ILR levels and further assist
decision makers in language placement recommendations as well as assist students in
selected languages adopt strategies and techniques based on their PTLS preferences.
5
Evaluation Goal
The goal of this research is a 70% success rate. If students are placed in speciﬁc
language programs based on results from the hybrid predictive model, then the prob-
ability of students to be successful is 70% i.e. seven out of ten students will be
successful (where the success factor is ILR level 2 or above on the OPI).
308
M. A. Nawaz and M. Hadzikadic

6
Methodology
In this research, a hybrid approach was proposed, where two different processes were
combined to devise a Hybrid Predictive Model for improved predictions and student
placement. This problem was taken as a classiﬁcation problem. In the Phase 1 (P1),
given data was normalized and converted for Machine Learning [12]. Then by using a
Support Vector Machine [5] and Decision Tree Algorithm Classiﬁers [10, 11], a
classiﬁer/model was developed to test the data for predictive results. Actual results
were then compared with the predicted results and the difference between actual and
predicted results was analyzed to make P1 recommendations. Different language results
were also compared and an additional run with various classiﬁers were performed to
see if the student with a particular PTLS would perform the same way in other lan-
guages. From the classiﬁers, a combined success probability of the results with ref-
erence to each PTLS was calculated. In Phase 2 (P2), PTLS preferences were
categorized through probabilistic classiﬁcation. The success rate of each personality
type and learning style was calculated, and updated whenever any new data was added.
The success rate of each PTLS was automatically updated whenever any new data was
added. This updating reveals any change in success rate and also shows the consistency
in the success rate based on each PTLS preference. Results from Phase 1 and Phase 2
were combined into a Hybrid Model by using the Cobb-Douglas function (Phase 3),
which gives a more accurate prediction of the success rate of each PTLS with respect to
the actual results. In addition to PTLS, this research explored another factor, teacher
impact that may have inﬂuenced the results by using the Cobb-Douglas utility function.
Hybrid Comparative Predictive Modeling
309

310
M. A. Nawaz and M. Hadzikadic

7
Summary of the Results
When the success rate of various personality types was calculated in each language and
when the success rates were updated with additional/new data it revealed that speciﬁc
personality types with the highest success rate were consistent with the success rate in
each language.
Here, (Phase 1) P1 snapshot of only the Arabic results is given for reference:
Table 2A-AD
ARABIC SCENARIOS
Model Accuracy
Iterations
DATA
SVM
DT
1
1-AD_ML PTLS-DLAB
228
(Trained Data)
81.1%
79.8%
1-AD_ML TestD
58
(Test Data)
1-Results-AD_ML
AD Prediction 
Accuracy
39-38
67%
66%
FR Assumption%
2PaFR-FaAD
RU Assumption%
35PaRU-FaAD
(Trained Data)
2-AD_ML TestD
28
(Test Data)
2-Results-AD_ML
2
2-AD_ML PTLS+DLAB
92
83.7%
81.5%
Hybrid Comparative Predictive Modeling
311

8
Summary and Conclusion from the Arabic Data
In the Arabic language, 228 data entries were available to formulate an SVM and DT
classiﬁer to check the predictions from 58 test data entries. SVM and DT classiﬁers had
81.1 and 79.8% model accuracy; prediction accuracy was 67% with the SVM and 66%
with the DT classiﬁer. From the test data, two of the students who did not succeed in
AD Prediction
Accuracy
19-17
68%
61%
FR Assumption%
6PaFR-FaAD
RU Assumption%
16PaRU-FaAD
3
3-ADef_ML-DLAB
228
(Trained Data)
81.1%
76.8%
3-ADef_ML TestD
58
(Test Data)
3-Results-ADef_ML
AD Prediction
Accuracy
39-40
67%
69%
FR Assumption%
2PaFR-FaAD
RU Assumption%
29PaRU-FaAD
4
4-ADef_ML+DLAB
92
(Trained Data)
83.7%
81.5%
4-ADef_ML TestD
28
(Test Data)
4-Results-ADef_ML
AD Prediction
Accuracy
19-19
68%
68%
FR Assumption%
2PaFR-FaAD
RU Assumption%
13PaRU-FaAD
5
5-ADef_ML-DLAB-R
228
(Trained Data)
81.1%
79.8%
5-ADef_ML TestD
58
(Test Data)
5-Results-ADef_ML
AD Prediction
Accuracy
39-40
67%
69%
FR Assumption%
2PaFR-FaAD
RU Assumption%
41PaRU-FaAD
6
6-ADef_ML+DLAB-R
92
(Trained Data)
83.7%
80.4%
6-ADef_ML TestD
28
(Test Data)
6-Results-ADef_ML
AD Prediction
Accuracy
19-18
68%
64%
FR Assumption%
2PaFR-FaAD
RU Assumption%
13PaRU-FaAD
312
M. A. Nawaz and M. Hadzikadic

Arabic would succeed in French and 35 of the students who would not succeed in
Arabic would succeed in Russian.
Phase 2 (P2)
Can data be looked at a different way where one can look at the success rate of each
PTLS to assist the students and to assist the decision makers in placing the students
accordingly so that the success rate of each PTLS can be improved with corrective
measures?
When the success rate of various personality types is calculated in each language
and when the success rates were updated with additional/new data. It showed that
speciﬁc personality types with the highest success rate are consistent with the success
rate in each language.
Hybrid Comparative Predictive Modeling
313

Whenever the new data is available, the new data is added to the base data to check
the consistency in the success rate of each PT within that speciﬁc environment.
Table-PTC1 reveals that in the Arabic language, with the updated data, PT with the
highest rate of success is still ISFP; ISFP was also the most successful PT in the base
data. In the updated data ISFP is still at the fourth place in Russian and thirteenth place
in French. In Russian it was ESFP and in French it was ENFP.
As more data is added and updated on a regular basis, success rate overview will
become clearer for each language and the big ﬂuctuations, which show up comparing
the previous and current update, will start normalizing (stabilizing).
An algorithm was developed with reference to the Probabilistic Classiﬁcation. The
algorithm can be coded in any program such as in C sharp in conjunction with the
PTLS success rates of each PTLS for the output.
Algorithm steps
For LN = 1 (Arabic Language)
1. if personality = ENFJ then the increased chance of success rate = ENFJ 0%
Table – PTC1
PT Preference
AD-2
AD-1
RU-2
RU-1
FR-2
FR-1
1
ISFP
ISFP
ESFP
ESFP
ENFP
ENFP
2
ENTJ
ENTJ
ENFJ
ENFJ
ESFP
ESFP
3
INTP
ENFP
INFP
INFP
ENFJ
ENFJ
4
ISFJ
INTP
ISFP
ISFP
INTJ
ESTP
5
ESFP
INFP
INTP
INTP
ESTP
ESTJ
6
ESTJ
ISFJ
INFJ
ENTJ
ENTJ
ISFJ
7
ISTP
INTJ
ISTP
ISTP
ESTJ
ESFJ
8
INTJ
ESTJ
ENTJ
ENFP
ESFJ
ISTJ
9
ESTP
ESFP
ENTP
ESTP
ISTJ
ISTP
10
ESFJ
ISTP
ESTP
ENTP
ISFJ
INTJ
11
ISTJ
ESFJ
ENFP
ESTJ
ISTP
INTP
12
ENFJ
ESTP
ISTJ
ISTJ
INTP
ENTP
13
ENFP
ISTJ
ESTJ
ISFJ
ISFP
ISFP
14
INFJ
ENFJ
ISFJ
ESFJ
INFP
INFP
15
INFP
INFJ
ESFJ
INTJ
ENTP
ENTJ
16
ENTP
ENTP
INTJ
INFJ
INFJ
INFJ
314
M. A. Nawaz and M. Hadzikadic

LS success rate = N/A
Note: Note: Enough data not available for any validation
Note: Student numbers ** (less than 15)
2. if personality is not ENFJ then check ENFP
3. if personality = ENFP then the increased chance of success rate = ENFP 0%
LS success rate = N/A
Note: Note: Enough data not available for any validation
Note: Student numbers ** (less than 15)
4. if personality is not ENFP then check ENTJ
5. if personality = ENTJ then the increased chance of success rate = ENTJ = 57%
LS success rate from high to low = G-I = 2/4; G-D = 1/4; P-D = 1/4
Note: Preferred – Other language background and overlapping PTLS
Note: Student numbers ** (less than 15)….
In this way, all the PTLS combinations are checked within each PTLS segment till
all the available 16 PT data is entered. 31 steps are deduced in each language program
to devise the program. The program is named as PTLS BRKEVEN Snapshot, and
various screen shots for analysis are provided here. Screen three provides the success
rate and any additional info of speciﬁc PT.
PTLS BRKEVEN Snapshot Program
There are seven screens in the PTLS BRKEVEN Snapshot program. In Screen 1 data
and text ﬁles are uploaded; data encompassed 24 months’ period.
Screen 4 provides the success rate comparison of the same PT in a different lan-
guage, and it can also provide the comparison of different PT in the same language.
Screen 6 combines the results from Phase 1 and Phase 2 through Cobb-Douglas
function.
Hybrid Model Output = (Phase 1 results).40 * (Phase 2 results).60.
Hybrid Comparative Predictive Modeling
315

316
M. A. Nawaz and M. Hadzikadic

Q: Do different personality types and learning style preferences impact the results
differently in various programs of study?
A: Results from the P1 and P2 indicate that different personality types and learning
style preferences impact the results differently in various programs of study. The
highest success rate of PT in Arabic is of ISFP, in Russian it is ESFP, and in French it
is ENFP.
Q: Can the decision makers improve their decision-making process by combining
different ways of predictive analysis to recommend students of different PTLS pref-
erences for placement with an end-result of a better rate of success?
A: Yes, the decision makers can improve their decision-making process by com-
bining different ways of predictive analysis to recommend students of different PTLS
preferences for placement by keeping in mind the objective of a better rate of success.
If management has to decide to place six different students of different personality types
in the Arabic language among the three languages (FR, RU, AD) and students are the
following PT:
Hybrid Comparative Predictive Modeling
317

ESTJ, ENFP, ISTJ, ESTJ, ESFP, ENFJ.
By looking at the data and the Table-PTC1, it is preferred to recommend the
students in the following order for the Arabic language:
ESFP, ESTJ, ESTJ, ISTJ, ENFJ, ENFP.
If management has to decide to place two of the students in French, two in Russian,
and two in Arabic; by looking at the Table-PTC1 student allocation priority wise will
be as follows:
French: ENFP, ISTJ
Russian: ESFP, ENFJ,
Arabic: ESTJ, ESTJ
Q: How can the predictive analysis become effective with limited data?
A: Predictive analysis can become more efﬁcient by regular updating of the current
data.
9
Limitations
Due to data limitations, it is advised to ignore ±5% change in success rate at this time.
Also, to note that where data entries of a speciﬁc PTLS are less than 15, even a small
ﬂuctuation will end up showing a big difference.
10
Future Recommendations
Teachers spend most of the time with the students during their language training. If
teachers are well trained and well qualiﬁed, then teachers can tailor their instruction to
student needs and students PTLS preferences. Teachers can also train the students in
facilitating their students to use their PTLS preferences to overcome students’ learning
weaknesses. Actual results can be checked with the predicted results. If the actual
results are better than the predicted results than it is called a boost. If the actual results
are worse than the predicted results than it is called a drag. This difference in predicted
to actual result i.e. the boost or drag effect is contributed to the external factor i.e. the
teacher effect. Cobb-Douglas utility function can be used to show the relationship
between the various inputs within the external factor i.e. teacher effect. Cobb-Douglas
is a concept from economics that shows the relationship between two or more inputs
and the amount of the output that can be produced by those inputs such as explained in
the article Human-Capital Investments and Productivity [1]. In the book Managing
Complexity: Practical Considerations in the Development and Application of ABMs to
Contemporary Policy Challenges on page 51, “The Utility Function” is mentioned;
“utility function can incorporate relevant theories and factors which show some kind of
a relationship between variables, and it can instantiate different theories by adjusting
parameters” [3]. Cobb-Douglas utility function was used to develop an Agent-Based
Model (ABM), Actionable Capability for Social and Economic Systems model
(ACSES model) because it is easily expandable to include additional preferences or
values or motivations if they are important for a theory (page 52) [3]. Speciﬁc version
of the Cobb-Douglas utility function is given in the form of Eq. (2) on page 55 in the
318
M. A. Nawaz and M. Hadzikadic

Managing Complexity: Practical Considerations in the Development and Applications
of ABMs to Contemporary Policy Challenges, U = (1 −L)WL (1 −C)WC(1 −
I)WI(1 −E)WE(1 −V)WV(1 −F)WF(1 −R)WR; where L is loyalty to leader, C is
coercion, I is ideology, E is economic welfare, and R is the repression and social
inﬂuence for defying repression and the weights Wx for motivation x, give the relative
importance of the different motivations to the agent and the relative effect they have on
U [3]. On the same pattern by using Argumentation Theory [2], and Attribution Theory
[4, 14], through Cobb-Douglas function teacher effect can be factored in by considering
teacher experience, qualiﬁcations, and effort. Y = (TE-Experience).25(TE-Qualiﬁca-
tions).35(TE-Effort).40
If the drag is high which impacted the results, then one can look into training the
teachers and at the difference in results after teacher training. This research can also
lead to any future research to devise models that may be utilized in speciﬁc instances
where the PTLS permutation for a particular student is not the best ﬁt for the speciﬁc
program. In such cases, that kind of a model may suggest different learning strategies
for achieving better results in training (just in case the student has no other option to
avoid that training program). This may also decrease the student turnover/failure rate,
enhance learning retention, and may lead to life-long/continuous learning. This is
because various personality types and learning styles prefer different learning strate-
gies. One may also calculate different scenarios where the cost savings, timelines of
courses, and learning efﬁciency of students can be connected.
The impact of technology on each segment within the external factor can also be
calculated. If new technology is introduced such as new smartboard technology, which
is helping the teacher in teaching and saving time, then an increment value can be
added to the assigned values for a set scale within the TE experience category and vice
versa. Also, the impact of professional development can be analyzed within the TE
qualiﬁcations on a similar pattern. Afterward, the feedback from the students about
teacher’s quality of instruction and experience with respect to the results can be veriﬁed
with the boost or drag result.
References
1. Black, S.E., Lynch, L.M.: Human-capital investments and productivity. Am. Econ. Rev. 86
(2), 263–267 (1996)
2. Grossi, D.: On the logic of argumentation theory. Paper presented at the Proceedings of the
9th International Conference on Autonomous Agents and Multiagent Systems: volume
1-Volume 1 (2010)
3. Hadzikadic, M., O’Brien, S., Khouja, M.: Managing Complexity: Practical Considerations in
the Development and Application of ABMs to Contemporary Policy Challenges, vol. 504.
Springer (2013)
4. Kelley, H.H., Michela, J.L.: Attribution theory and research. Annu. Rev. Psychol. 31(1),
457–501 (1980)
5. Kotsiantis, S.B.: Supervised machine learning: a review of classiﬁcation techniques.
Informatics 31, 249–268 (2007)
6. Kotsiantis, S.B., Zaharakis, I., Pintelas, P.: Supervised Machine Learning: A Review of
Classiﬁcation Techniques (2007)
Hybrid Comparative Predictive Modeling
319

7. Liskin-Gasparro, J.E.: The ACTFL proﬁciency guidelines and the oral proﬁciency interview:
a brief history and analysis of their survival. Foreign Lang. Ann. 36(4), 483–490 (2003)
8. Meyer, D., Wien, F.T.: Support vector machines. The Interface to libsvm in package e1071
(2015)
9. Mitchell, T.M.: The discipline of machine learning, vol. 9. Carnegie Mellon University,
School of Computer Science, Machine Learning Department (2006)
10. Neville, P.G.: Decision Trees for Predictive Modeling. SAS Institute Inc., 4 (1999)
11. Safavian, S.R., Landgrebe, D.: A Survey of Decision Tree Classiﬁer Methodology (1990)
12. Salzberg, S.L.: C4. 5: programs for machine learning by J. Ross Quinlan. Morgan Kaufmann
Publishers, Inc., 1993. Mach. Learn. 16(3), 235–240 (1994)
13. Taskar, B., Segal, E., Koller, D.: Probabilistic classiﬁcation and clustering in relational data.
Paper Presented at the International Joint Conference on Artiﬁcial Intelligence (2001)
14. Weiner, B.: Attribution theory, achievement motivation, and the educational process. Rev.
Educ. Res. 42(2), 203–215 (1972)
15. Wu, X., Kumar, V., Quinlan, J.R., Ghosh, J., Yang, Q., Motoda, H., Philip, S.Y.: Top 10
algorithms in data mining. Knowl. Inf. Syst. 14(1), 1–37 (2008)
320
M. A. Nawaz and M. Hadzikadic

SSST-Cloud: Developing a Cloud System
for a University
Tarik Catic and Belma Ramic-Brkic(B)
University Sarajevo School of Science and Technology,
Sarajevo, Bosnia and Herzegovina
belma.ramic@ssst.edu.ba
Abstract. Cloud systems are becoming increasingly popular for sharing
various types of ﬁles and accessing them from diﬀerent locations. The
idea behind it seems simple and ideal for diﬀerent types of users. The
reality, on the other hand, is quite diﬀerent. Companies for example,
might require certain functionalities that an oﬀ-the-shelf cloud system
cannot provide. Universities have certain procedures regarding sharing
of grades and other course related materials that must be met during the
development of the cloud system in order for it to be useful and eﬃcient.
In this paper, we talk about the functionalities that a cloud system has
to provide and the technical requirements it has to meet in order to
be successfully implemented and used by a University, its students and
professors. Thorough analysis was performed to analyze the drawbacks of
current solutions and consequently, oﬀer a novel approach in developing
cloud systems for one particular private University.
Keywords: University cloud · File sharing · Privacy
1
Introduction
Cloud computing refers to an online system which provides software and storage
as an internet service and is accessed through the internet [1]. In the last decade,
use of cloud systems across the board has signiﬁcantly inﬂuenced how we do
business today. One of the considered pioneers in the ﬁeld was Dropbox [2]. It is
one of the most used cross platform cloud systems and one of the leaders in the
world of cloud storage [3]. Even though services such as Dropbox oﬀer enterprise
solutions that allow companies to organize ﬁles among various departments, it
is still not an optimal solution for every type of consumer. The purpose and
functionalities diﬀer from company to company, but the major functionality
remains the same organizing data and ﬁles within the company.
Secondary beneﬁciaries of the cloud development are computer scientists and
people that spend signiﬁcant portion of time on their personal or companys com-
puter. That reﬂects perfectly on students and universities. For example, a student
c
⃝Springer International Publishing AG 2018
M. Hadˇzikadi´c and S. Avdakovi´c (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_30

322
T. Catic and B. Ramic-Brkic
might work on particular project in the lab using the universitys computer. If
the student isnt ﬁnished with the project, they can upload everything to the
cloud and continue working from a diﬀerent location. Another beneﬁt is that
the same student can easily share the work with their professors and request the
feedback.
From a technical aspect, implementing a cloud system can become a night-
mare if the system is designed poorly. There are various layers of authentication
that have to be implemented in order to keep the data secure and well organized.
Universities operate in quite unique way and no fully suited solution is pro-
vided yet. They urge developers to create solutions that could be acceptable for
demanding tasks such uploading of assignments and materials in both directions
between students and professors, securing the process and the communicational
link.
In this paper we present the challenge of developing a cloud storage system
that will ﬁt the needs of the University Sarajevo School of Science and Technol-
ogy. We will cover both technical and nontechnical aspects of the development
process and also state the beneﬁts of implementing such a system.
The paper is organized as follows: in Sect. 2 we give a brief overview of related
work and available solutions. In Sect. 3 we identify the beneﬁts and functional-
ities of a University private cloud system; Development and Implementation
processes are covered in Sects. 4 and 5 while in Sect. 6 conclusion and details of
planned future work are given.
2
Related Work
Cloud storage is not a new ﬁeld in the IT world and therefore numerous solutions
are available for individuals and companies. Besides previously mentioned Drop-
box, Google has Google Drive [4], Microsoft oﬀers OneDrive [5], Apple iCloud [6],
and so on. Even though they might be the right choice for individuals, for com-
panies they might not be the best option. The issues with oﬀ-the-shelf solutions
is that they are predeﬁned and allow only certain level of customization. Each
company is unique and has its own sets of rules and procedures and therefore
requires speciﬁc modules. For example, micro-credit organizations might need
speciﬁc modules for accounting; IT companies will not look for classical data
storage, but for version control (Git) and solutions for teams; Universities on
the other hand, cover various ﬁelds. It is important to distinguish between two
types of clouds. First of all, there are clouds such as those that we have men-
tioned above (Dropbox, iCloud) and clouds computing system such as Amazon
AWS [7] or Digital Ocean [8]. The second group of cloud systems oﬀers the most
customization, but also requires a lot of technical knowledge. They oﬀer cus-
tomers virtual servers which can be used as a cloud storage system, but they are
not meant to be used as cloud storage systems. Dropbox and similar providers
oﬀer a solution that does not require any technical knowledge and their solutions
are just for cloud storage. One diﬀerence that distinguishes the two groups is
the way how uploaded ﬁles are treated. If we want to upload a PDF ﬁle to a

SSST-Cloud: Developing a Cloud System for a University
323
server provided by Digital Ocean for example, we will be able to call that ﬁle
from the browser using the servers IP address and eventually adding the right
path if it is not part of the root directory. Systems like Dropbox do not allow
such actions. Firstly, we do not know the IP address of the server where our ﬁles
are stored. Second, each ﬁle is accessed through a generating URL with certain
layers of protection and authentication.
Universities can use a cloud system in three ways as a server for data storage,
PaaS (Platform as Service) or SaaS (Software as Service) [9]. The ﬁrst method is
a basic cloud storage system that is used as some kind of an online hard disk. It
is just there to store data and present it when it is requested. PaaS on the other
hand is a little bit more advanced. It allows the creation of services through the
platform. In this case, for example, that would be the ability of creating virtual
private servers on which projects would be hosted. SaaS is popular type of cloud
computing which allows the user to use some software directly from the browser.
The software runs on the server and is through the system served to the user. A
good example for that is Microsoft Oﬃce online which is the online equivalent
of Microsoft Oﬃce for desktops.
From SSSTs point of view all three implementations are equally important.
Implementing them would allow the overall system to grow, because every new
system would be added as a new service to the existing cloud system. The cloud
system itself would also have the aspects of both types of cloud systems, even
though the focus of this paper is on creating a suitable solution for sharing and
storing university related ﬁles.
3
Beneﬁts and Functionalities for the University
The work in this paper is directed towards enabling relatively small private Uni-
versity reach its full user satisfactory potential. The University Sarajevo School of
Science and Technology (SSST) has approximately 300 students and 80 members
of teaching staﬀ. It is composed of following Faculties: Computer Science, Infor-
mation Systems, Modern Languages, Political Science, Economics and Medicine.
Each Faculty oﬀers certain major and minor options to their students. The needs
of each faculty must be identiﬁed. For example, computer science students will
most probably use more storage than Political Science students, because they are
submitting projects with code and static ﬁles, while Political Science students
submit mostly Word documents. Being limited to a predeﬁned set of functionali-
ties will sooner or later become a problem for the university. The entire structure
of the University is visualized in Fig. 1.
Based on our related work and survey among students, we have here compiled
the list of main beneﬁts of a private cloud system:
1. Organized classes and courses. From the professors point of view this is
one of the main advantages. Talking to professors and students at SSST about
the process of sharing ﬁles and sending material has shown that there is no
standard for doing so. Professors send presentations, practice examples and other
ﬁles often. Those ﬁles can get lost, in the tons of emails that are sent and

324
T. Catic and B. Ramic-Brkic
Fig. 1. SSST internal structure
received. Having organized classes and courses with which ﬁles are shared, the
organizational problem will be solved and the overall process of sending/sharing
ﬁles will be faster. From the students point of view it is useful to have all ﬁles
organized in one place. Usually students are not that organized and often lose ﬁles
and have to ask for them again or look for them for hours. This way everything
is more organized for students.
2. Assignments and deadlines. This point is probably not the students favorite
beneﬁt, but for professors it is again useful. Professors will have the ability to
assign assignments or a project to a class and deﬁne a strict deadline. Students
on the other hand will submit their project right on the cloud. The advantage
is that the project can be uploaded earlier, so that the professor can leave a
feedback.
3. Overall organization. Each professor has its own way of organizing mate-
rial. Some professors tend to use emails, others tend to use Slack and similar
platforms or something entirely diﬀerent. There is no standardized approach.
Having a private cloud system changes that. Everything is cut down to two
communicational approaches: emails for communication and the cloud for ﬁle
sharing.
There are some other beneﬁts of having a private cloud system, but those
three are the most dominant once and they are still in the domain of a classic
cloud system. There are a lot of other features that can be implemented as part
of the cloud system, but when talking about University cloud systems these are
identiﬁed as the most important features.

SSST-Cloud: Developing a Cloud System for a University
325
4
Development Process Potential Bottlenecks and
Technical Challenges
For the development, we decided to use Pythons web framework Django (Django
1.10 and Python 3) in combination with a MySQL database [10–12]. Most devel-
opers in Bosnia and Herzegovina tend to use a classic LAMP (Linux, Apache,
MySQL, PHP) stack for web applications of any kind, but writing the back-
end in Python or any of Pythons frameworks makes the development process
shorter and the maintenance and further development easier. MySQL was cho-
sen because it is a robust, scalable, and open source. It is perfectly integrated
into Django using a module.
Through the development process it is important to identify the possible
bottlenecks of the system. They are listed below:
The ﬁrst possible bottleneck and the most important one to solve is data
organization. It is quite easy to upload every ﬁle to one and the same path, give
the ﬁle a random and unique name and that is it, but it would become a problem
soon. File have to be organized in a more organized and categorized way. If we
have a folder called uploads, we have to separate that folder into multiple folders.
That way it is a lot easier for the server to serve all documents that we need. If
we upload all ﬁles into one folder, it becomes a nightmare for the server because
it will have to process a huge amount of documents to serve the once we want.
One solution is to separate folders by the user they belong to. Then again, that
folder will be separated in some way. Another way is to separate the folder by
month and years. There are multiple approaches to separate folders, but there
is no optimal solution since it depends on the number of ﬁles that are uploaded
and on the number of users that use the system.
The second bottleneck is more a potential risk than a bottleneck. No matter
how good the ﬁles are organized they have to be protected even better. Cloud
storage can be protected, but it is more than just a technical issue to protect
it [13]. There is always the possibility that someone tries to harm the system
somehow. That means, that it is necessary to have regular backups, high security
standard, multiple layers of authentication, and deﬁned permissions for certain
ﬁle types. Potential risks are exe ﬁles or some scripts that are uploaded and
executed on the server. Another risk is allowing brute force attacks. Even though
that is one of the most straight forward types of attacks, a user should not be
allowed to somehow hack into the system using the brute force approach or some
other similar approach. Even though the system does not handle important data
such as bank information it is still important to protect the data and not allow
somebody to harm the university and the system in general in any possible way.
The third possible bottleneck is again not a real bottleneck more an orga-
nizational question. It is about the infrastructure. This might be a rare case,
because Universities will not handle millions of requests and users, but it is still
important to discuss it during the development process. How is the application
going to be organized in terms of servers? Will everything run on one server

326
T. Catic and B. Ramic-Brkic
or will there be multiple servers for the database, the ﬁles, and the application
itself? What kind of server is needed? Is it going to be hosted in-house? Those
questions might seem irrelevant in the beginning, but answering them in the
beginning makes the entire process easier. By knowing exactly how everything
will be organized, potential migrations and bugs that could come along with the
migrations could be avoided. Having a slow system because of the infrastructure
is a rare case, but a university with thousands of students will reach the number
of hundreds of thousands and more uploaded ﬁles quickly.
5
Implementation of a Private Cloud System at SSST
SSST, just like most other universities, has a system for tracking attendance
and grades. To avoid data redundancy and multiple disconnected systems, the
cloud system was integrated with the current system. The biggest advantage of
integrating the entire system and creating its backbone lies in the potential for
further development because the foundations were laid. The integration process
had its own challenges. The biggest challenge was to run two diﬀerent databases,
but have all user data in one database. That problem was solved using callbacks
and various APIs which allow the two systems to communicate and function like
one big system. The APIs have been designed in the way that they keep both
databases up-to-date and to send and receive data. Therefore, creating a mobile
app as a next step will not be a complex task since necessary APIs are already
written. Using callbacks and APIs can be a potential security risk, but in order
to secure the data custom hash functions and encryption have been added to
both sides of the system. The encryption is similar to Caesar Cipher [14] which
encrypts a string by shifting each letter for a speciﬁc number of places to the
right. To make the encryption more complex randomization has been added as
well. Therefore, each key can be represented in multiple diﬀerent ways. Since
some IDs can have just one number, oﬀset was added to every string. In the end,
each number can be encrypted and represented by a string with 64 characters.
In addition to that, accessing data and calling APIs is restricted to speciﬁc
IPs. IPs other than the system once ﬁltered out in the middleware and the data
cannot be accessed. Another challenge was the diﬀerent programming languages.
One system was developed in PHP and the other one in Python. Having two
diﬀerent languages means that the two systems are hosted individually and on
two diﬀerent sub domains, and are connected using APIs. The communication
is visualized in the following image (Fig. 2).
In general, the communication works in the way that a request is passed to
a view from where the cloud system makes a request to the old system. The old
system processes the request and returns data according to it. The view parses
the data, performs everything that has to be performed and presents it to the
user.

SSST-Cloud: Developing a Cloud System for a University
327
Fig. 2. The communication between the systems
6
Conclusion and Future Work
We have mentioned the beneﬁts of the cloud system in terms of data storage and
ﬁle sharing, but there are other kinds of systems than can be built and developed
with the cloud system in the background. A Git version control system is one of
the ﬁrst on the list, but there are also other functionalities that should be added
and are not directly related to the Computer Science department. The ﬁnance
oﬃce and the registry should become part of the system as well. Unpaid invoices
and remainders for them can be uploaded to the cloud. Students should be able
to request any document from the registry using the cloud. There are various
other ideas that can be implemented with a cloud storage in the background on
which everything is running. The cloud system itself can be developed further
as well. For example, the development of a mobile app or a desktop app. Having
all three ﬁelds covered, mobile, web, and desktop, will make the cloud system
even more eﬃcient.
Having a private cloud system might seem not useful or unnecessary for a
company or a university. Having it though, will improve the communication
between university or company members and employees in terms of ﬁle sharing
and data organization. A classic cloud system might not be the perfect solution
for everyone, but it can be used as the fundamental for further development and
growing the overall system that runs a university.

328
T. Catic and B. Ramic-Brkic
References
1. Stair, R.M., Reynolds, G., Chesney, T.: Fundamentals of Business Information
Systems, 2nd edn. Cengage Learning (2011)
2. DropBox, Inc. http://dropbox.com
3. Drake, N.: Top 10 best cloud storage services of 2017. http://www.techradar.com/
news/top-10-best-cloud-storage-services-of-2017. Accessed 23 Jan 2017
4. Google Drive. http://drive.google.com
5. Microsoft One Drive. http://onedrive.live.com
6. Apple iCloud. http://icloud.com
7. Amazon AWS. http://aws.amazon.com
8. Digital Ocean. http://digitalocean.com
9. Cenon
Gaytos.
How
Universities
Implement
Cloud
Computing.
https://
cloudtweaks.com/2012/02/how-universities-implement-cloud-computing/.
Accessed 17 Feb 2012
10. Django. http://djangoproject.com
11. Python. http://python.org
12. MySQL. http://mysql.com
13. Rimal, B.P., Choi, E., Lumb, I.: A Taxonomy and Survey of Cloud Computing
14. Macura, W.K.: Caesar’s Method. http://mathworld.wolfram.com/CaesarsMethod.
html

DIY Smart Mirror
Sadeta Kulovic and Belma Ramic-Brkic(B)
University Sarajevo School of Science and Technology, Sarajevo,
Bosnia and Herzegovina
sadeta.kulovic@stu.ssst.edu.ba, belma.ramic@ssst.edu.ba
Abstract. The Internet of Things concept has become increasingly pop-
ular when it comes to providing people with technology and tools that
would make their lives simpler and ease their day-to-day routines. One
such tool is a multipurpose mirror used both as a mirror and a device.
There have been several successful attempts of building such a device,
and some of them even turned into business opportunities. However, due
to its selling prices and shipping policies regarding Bosnia and Herzegov-
ina, it is almost impossible to acquire one. The main purpose of this paper
is to show how the process of building a Smart Mirror from scratch show-
ing basic information such as time, date, weather statistics, and recent
news headlines. The displayed information is accessible at any point in
time. The cost-eﬀective analysis indicates signiﬁcant cost savings for pro-
duced outcome compared to available oﬀ-the-shelf solutions.
Keywords: Smart mirror · Raspberry Pi
1
Introduction
In the past few years, technology has become important and inevitable part
of our daily routines. With technology progressing at rapid pace, people are
also expected to be more productive and eﬃcient in their daily activities. The
use of smart phones, tablets, laptops and other similar devices has provided
people with tools that help them stay productive and, more importantly, be
time-eﬃcient. However, as much as the use of such devices is time-eﬃcient, it is
also time-consuming as it has become yet another task on ones daily to-do list.
In addition to this, time demands (deadlines) are most often the main cause of
a person being under pressure. Therefore, good time management is the key to
getting things done. This does not only apply to ones professional life, but to
their private life as well.
One activity common to us all is getting ready to leave the house in the
morning for school/work duties. It also consumes a good amount of time in
the morning. Checking the phone constantly to see what time it is or what
the weather is like for that day does not seem much time-consuming, but it is.
Being focused on task at hand for a speciﬁc period of time is what good time
management is.
c
⃝Springer International Publishing AG 2018
M. Hadˇzikadi´c and S. Avdakovi´c (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_31

330
S. Kulovic and B. Ramic-Brkic
In this paper, we present a multipurpose mirror that is meant to serve as
both decoration and information source. With just one look at the mirror, one
will have the basic information on what to wear based on weather forecast for
that day or how much time they have left if they want to arrive on time at
planned destination.
The paper is organized in the following way: in Sect. 2 we give a brief overview
on what is currently available on the market as well as how it is being used in
the research. Section 3 deﬁnes the concept of the smart mirror. The design and
development are explained in more detail in Sects. 4 and 5. In Sect. 6 we provide
a cost-eﬀectiveness analysis of available smart mirrors compared to our solution,
while in Sect. 7 we give conclusion and suggestions for the future work.
2
Related Work
According to research, there are many diﬀerent and interesting approaches to
building a smart mirror. Some of the solutions are available on the market,
while others are just prototypes or Do-It-Yourself (DIY) projects developed by
enthusiasts for their own use.
The Cybertecture Mirror [1], developed by James Law, displays basic infor-
mation such as date, time, and weather. It also allows social network integration,
so the user has access to their social accounts even when in bathroom. The mir-
ror can be controlled either by the remote controller or the mobile application.
In addition to the mentioned features, the mirror also has embedded sensors
that monitor person’s health vitals such as weight, heart rate, or body fat. It
is possible to watch videos or listen to music, as well as workout while follow-
ing the exercise instructions displayed on the mirror. This device oﬀers a wide
variety of features which can be customized for every need. However, the price
of the mirror does not make it quite aﬀordable to everyone because the cost of
customization goes from 3,600 to 7,700 [1]. In 2012, at the Consumer Electronics
Show held in Las Vegas [2], Samsung presented Smart Window a window that
is also a device [3]. Although not a mirror, this device allows users to access
applications such as weather or social apps all the while serving as a window.
Serakus Smart Washbasin running on Android OS, displays various information,
from news and weather forecast to how much water is being spent each day or
how much a person weighs [4].
Another high-tech mirror on the list of currently available is Magic Memory
Mirror, also known as Memomi [5]. This mirror is being used as a replacement
for dressing rooms in shops that sell clothes. Customers are able to interact with
the mirror through the mobile application. The mirror gives them a 360◦view
of their outﬁts as well as the possibility to modify their looks by changing the
color or adding accessories to the outﬁt [6].
Aside from the above mentioned products, there are also smaller-scale
projects that involved piecing components together to achieve the functional-
ity of a smart mirror. These are mostly done as DIY projects and are used for
individual purposes [7,8].

DIY Smart Mirror
331
Max Braun’s bathroom mirror stands out from this group (DIY projects) [7].
This mirror does not oﬀer as many features as those that are being sold, but it
does serve a purpose of a smart mirror as it displays basic information a person
needs while in bathroom such as the weather, time and date, and news.
There are numerous other projects that involve building a custom-made
smart mirror. One of the main reasons for this undertaking are the prices of
available oﬀ-the-shelf products. With a bit of eﬀort and imagination, the same
result can be achieved and with much lower expenses. In Sect. 6 we will provide
a more detailed analysis of costs and performances of available smart mirrors
compared to our built-from-scratch solution.
3
Concept
The idea behind our smart mirror is to display information such as time, date,
weather, and list of tasks to be done on a mirror display. This is the basic
information we need not only in the morning, but throughout the day as well.
Making the most of the available time we have during the day is crucial when it
comes to being eﬃcient and productive, and this project is a great example of
how technology can make our lives simpler and ease our day-to-day routines.
The concept of a smart mirror has been around for several years. The inspi-
ration came from the Internet of Things (IoT) concept, which could be described
as an eﬀort to make everything smart. The goal behind this concept is to provide
people with technology that would make their lives simpler and ease their daily
routines. At its core, Internet of Things is about connecting devices over the
Internet in a way that enables the communication between users and applica-
tions on such devices [9].
Following this concept, hardware components that are necessary for the mir-
ror to be functional have been acquired. These include a Raspberry Pi controller
board [10], monitor, and see through mirror [11]. In Sect. 4 we give a detailed
description of how these components are put together and tested out.
4
Design
The design of the device is done in two parts. The ﬁrst part deals with designing
the box that is used as a container for the monitor, controller board, and mirror.
Both monitor and Raspberry Pi need to be placed inside the box and secured
to avoid any possible damage to any of the components while moving the box
around. Once these two components are ﬁxed, the mirror will cover the front
of the box. Any information that is to be displayed on the mirror will actually
be displayed on the monitor. The characteristics of the mirror are such that the
front side is reﬂective and acts as a mirror, while the back side is transparent,
so anything that is displayed on the monitor can be seen on the mirror surface.
Therefore, the mirror needs to be placed directly onto the monitor to ensure that
all of the information is visible. In addition to this, the box will have openings for

332
S. Kulovic and B. Ramic-Brkic
the controller board and monitor power cables. Furthermore, since Raspberry Pi
has several USB ports that might be needed for keyboard or mouse for manual
conﬁguration of the device, the box has a side opening. To avoid any possible
movements of the monitor and controller board that could cause damage to these
components, the box will have compartments for them to ensure they stay in
place. We used Creo Parametric software [12] for modeling. Initial model of the
box is shown in the ﬁgures.
Figure 1a illustrates the front view of the box. There are three compartments:
one for the monitor, one for the controller board, and one for ﬁxing the power
cables.
(a) Front view of the
box
(b) 3D model of the
box
(c) Front cover of the
box
Fig. 1. Box model
A 3D model of the box is given in Fig. 1b. In addition to the three compart-
ments, one can also see an opening on one side of the box. This opening will
allow access to the controller boards USB ports.
Figure 1c represents the front cover of the box. Once all components are
placed inside, the box will be sealed with this cover and thus keeping everything
inside of the box in one place.
The second part of the design is designing the application. Because the goal
is to have a mirror that is displaying certain information, the design has to be
such that the existing surface of the mirror, which covers the monitor, is used
in the most eﬃcient manner. The information that is to be displayed needs to
be arranged in such manner that the display area of the monitor is eﬃciently
utilized and that it does not stand in the way of one’s reﬂection. Thus, careful
planning of the design is an imperative. The best way to achieve this is to place

DIY Smart Mirror
333
these pieces of information in the corners of the display, leaving the mid part of
the display surface empty, so that the person’s reﬂection in the mirror is clearly
visible. The ﬁnal solution is given in the Fig. 2.
Fig. 2. Information displayed on the monitor
5
Development
Upon the completion of the design phase, the development phase begins. In
this phase, the application that generates the speciﬁed information is developed.
Prior to that, Raspberry Pi needs to be set up and conﬁgured so that it can
run the application without problems. The set up includes a fresh install of a
compatible operating system, which in this case is Raspbian. Once the operating
system is installed, Raspberry Pi needs to be conﬁgured to automatically run the
application on boot. This way, as soon as Raspberry Pi boots, the application is
started in a full screen while hiding all other system components and features.
These features can be accessed only when a keyboard is plugged in for debug-
ging purposes. Once the conﬁguration is done, the application is developed and
installed. The application will be developed according to the design speciﬁca-
tions. It will generate the speciﬁed information that include time, date, weather
forecast, and list of tasks. The background of the application will be black and
the font color will be white to ensure maximum reﬂection of the mirror. The
initial solution for the application is to develop a website. For the development
of the website, we will use Laravel framework [13]. The idea is to implement
a sign-up/login feature, so that the user has their own account and is able to
customize the mirror in the future. The user will enter their credentials during
the ﬁrst Raspberry Pi boot, and upon a successful login, they will be redirected

334
S. Kulovic and B. Ramic-Brkic
to the page where the relevant information is stored. The Raspberry Pi is con-
ﬁgured in such a way that, when it boots, it automatically launches the browser
and loads a speciﬁed website. In the same manner the website that will display
the information for this project will be launched. Aside from the sign-up/login
feature, the website also allows a user to, once signed in, add or delete tasks they
have set for themselves. These tasks are displayed on the page that is launched
on Raspberry Pi. Adding and removing tasks is done over a smart phone, tablet,
or laptop. Tasks are speciﬁc to users, so without being logged in it is impossible
to add or remove any task. In addition to this, a request to the server is sent
every thirty seconds to check if new tasks have been added or if some tasks have
been removed so that the list can be updated accordingly.
6
Cost-Eﬀectiveness Analysis
Products that already exist on the market and that are being sold are usually
overpriced considering the fact that the same result can be achieved by investing
some time and eﬀort. In Sect. 2, we discussed several diﬀerent products and one of
them, Cybertecture Mirror, was priced in thousands of US dollars, which limits it
from mass-production. There are other products that cost less than Cybertecture
Mirror. However, due to the lack of customization, the prices are still high. One
example is Perseus Mirror [14] which costs $249, but the dimensions of the
mirror are ﬁxed, so the customers who wishes to own a larger mirror would
not be satisﬁed with this one. Another smart mirror on the market is Smart
Touch Vanity Mirror [15] sold by Evervue. This mirror is more expensive than
the previous one and the prices range from $799 to more than $1,500, depending
on the user customization. These prices do not include the costs of shipping to
Bosnia and Herzegovina. All of the devices mentioned above are very similar in
terms of applications running on them. They all show the same basic information
and have similar features. However, this is a matter of software development and
not hardware. So, from this perspective, it is by far cheaper to invest into the
necessary hardware components and time and eﬀort into developing the software.
For example, the basic hardware components that are necessary for the smart
mirror device to be functional include the controller board which costs $37 [10],
the see-through acrylic mirror which costs around $27 [11], and the monitor,
which can be found in computer stores that sell used computer equipment for
under $50. The price of building the box for these components is under $20.
Therefore, the overall price is around $134, which is a bit over half the price
of the cheapest device mentioned above. This is in case all of the hardware
components need to be purchased. The costs of building our device are lower
as we already have the monitor, so the costs of our device will be around $84.
This is shown in the Table 1. All that is left is to invest time in developing the
software that will generate the desired content.

DIY Smart Mirror
335
Table 1. Cost-eﬀectiveness analysis
Mirror
Price ($) Advantages/Disadvantages
Perseus mirror
249
Fixed mirror dimensions
Smart touch vanity mirror 799–1,500 Customizable, but expensive
Smart mirror
134
Customizable and low cost
7
Conclusions and Future Work
This project started as an attempt to contribute in bringing smart homes and
devices to Bosnia and Herzegovina. Although the features this device oﬀers are
bare minimum of what it could oﬀer, there is room for improvement and further
development. At this time, the purpose of the displayed information on the
mirror is to save the time spent in the mornings in search for such information.
In the future, additional features will be added to the mirror, allowing it to be
more customizable and user friendly. Users will be more in control in terms of
selecting which type of information they wish to be presented with. They will also
be able to interact with the mirror by using their smart phones. Smart Mirror is
currently a hot topic. The purpose of this paper was to provide an overview of
currently available solutions and detailed instructions on how to build your own.
With enough time and eﬀort, this project could turn into a great opportunity
for further development and integration with other smart home components.
References
1. Ridden, P.: Cybertecture’s magical mirror is bursting with augmented information.
Newatlas.com (2017). http://newatlas.com/cybertecture-smart-mirror/20227/
2. Chan, A.: Samsung wins CES innovation award for smart window display—PSFK.
PSFK (2012). https://www.psfk.com/2012/01/samsung-smart-window-ces.html
3. Zax, D.: Samsung’s Smart Window. MIT Technol. Rev. (2017). https://www.
technologyreview.com/s/426662/samsungs-smart-window/
4. SERAKU Corporation, Ltd. Smart Washbasin (2017). http://smart-washbasin.
seraku.co.jp/english/about/index.html
5. Rittman,
E.:
‘Magic
Memory
Mirror’
changing
the
way
people
shop
for
clothing.
Kctv5.com
(2017).
http://www.kctv5.com/story/29070505/
magic-memory-mirror-changing-the-way-people-shop-for-clothing
6. Memomi MemoryMirror (2017). http://memorymirror.com/
7. My Bathroom Mirror Is Smarter Than Yours. Medium (2017). https://medium.
com/@maxbraun/my-bathroom-mirror-is-smarter-than-yours-94b21c6671ba#.
uq4h3aqa0
8. Cohen, E.: Introduction Smart Mirror Documentation. Docs.smart-mirror.io
(2017). https://docs.smart-mirror.io/
9. What
is
the
Internet
of
Things
(IoT)?—Deﬁnition
from
Techopedia.
Techopedia.com
(2017).
https://www.techopedia.com/deﬁnition/28247/
internet-of-things-iot

336
S. Kulovic and B. Ramic-Brkic
10. Amazon.com.:
Raspberry
Pi
3
Model
B
Motherboard:
Com-
puters
&
Accessories
(2017).
https://www.amazon.com/
Raspberry-Pi-RASPBERRYPI3-MODB-1GB-Model-Motherboard/
dp/B01CD5VC92/ref=sr 1 3?ie=UTF8qid=1490491421&sr=8-3&
keywords=raspberry+pi+3+model+b
11. Amazon.com.:
12”
x
24”
Acrylic
See-Through
Mirror,
1mm:
Home
&
Kitchen (2017). https://www.amazon.com/12-Acrylic-See-Through-Mirror/dp/
B01G4MQ3WQ?th=1
12. Creo Parametric 3D Modeling Software — PTC. Ptc.com (2017). http://www.ptc.
com/cad/creo/parametric
13. Otwell, T.: Laravel—The PHP Framework For Web Artisans. Laravel.com (2017).
https://laravel.com/
14. Meet Perseus, the next generation mirror. Meet Perseus, the next generation mirror
(2017). https://www.perseusmirrors.com/shop
15. Mirror, S.: Smart Touch Vanity Mirror. Evervuestore.com (2017). http://www.
evervuestore.com/smart-touch-vanity-mirror

Farm: Serious Game for Addressing
Child Obesity
Sena Bajraktarevi´c and Belma Rami´c-Brki´c(B)
School of Science and Technology, Hrasnicka Cesta 3A,
Sarajevo, Bosnia and Herzegovina
sena.bajraktarevic@stu.ssst.edu.ba, belma.ramic@ssst.edu.ba
Abstract. Obesity among young adults is an increasing world health
problem associated with harmful eﬀects on the health and general well-
being. Although this phenomenon is mainly connected to low and mid-
dle income countries such as Bosnia and Herzegovina, obesity levels in
developed countries such as UK and USA, are signiﬁcantly increasing as
well. In this paper we address the possibility of reducing the problem
through the use of serious games. Stimulating children through the fun
and interactive play environment has been shown as an eﬀective method
in increasing physical activity and energy expenditure during the play.
Using this method of playing a carefully designed serious game is one of
the ways to stimulate and help children change their behaviour towards
eating, physical activity and therefore, consequently, prevent and reduce
child obesity. In this paper we present a game which encourages a child
to move using Kinect motion sensors. The aim of the game is to move
through the virtual farm and cache the healthy food items appearing
on the sides. Food items are grouped in four categories across diﬀerent
levels of the game. In the preliminary study presented here, the eﬀect of
pleasure and user satisfaction was only tested.
Keywords: Educational games · Obesity · 3D
1
Introduction
Child obesity is considered as one of the most serious public health challenges
of the 21st century. The problem is prevalent in urban settings and in the past
two decades has increased at an alarming rate [1]. Solely in the United States,
the percentage has tripled since 1970s. According to World Health Organization
(WHO) data from 2015, the number of overweight children under the age of ﬁve,
is estimated to be over 42 million. Almost half of all overweight children under
the age of ﬁve live in Asia and one quarter lives in Africa [1].
c
⃝Springer International Publishing AG 2018
M. Hadˇzikadi´c and S. Avdakovi´c (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_32

338
S. Bajraktarevi´c and B. Rami´c-Brki´c
Obesity results from energy intake exceeding expenditure over the long term
[2]. For example, fruits and vegetables are declared as low energy dense food and
hence, been associated with weight loss [1]. Physical activity, as the major modiﬁ-
able component of energy expenditure, has also been associated with lower levels
of body fat in children [2,3]. So is the amount of time spent watching television
[4–8]. However, children diet and activity patterns are less than desirable [5].
According to Baranowski et al., obesity could potentially be reduced by
changing eating habits and introducing regular physical activities [4]. Behav-
iour change must be approached as a complex, multi-step process. Rather
than attempting to change behaviour directly, behavioural scientists attempt
to change mediators. It has been shown that participation in regular physical
activity reduces the risk of non-communicable diseases such as cardio-vascular
disease, diabetes, and cancer [4,5].
Reducing obesity level could be accomplished by using serious game whose
main goal is to educate players through a fun and interesting environment. The
aim of this paper is to present the development of such a game. The game “Farm”
promotes kids healthy lifestyle by making players ﬁnd and pick-up healthy food
such as carrots and apples. Through the use of Microsoft Kinect, players need to
actually walk through the farm in order to ﬁnd the fruits and vegetables given in
a task. Every action is accompanied with a message informing the player of the
beneﬁts and inﬂuences of each food item on the human organism. Such combined
actions will educate a child, inﬂuence their behaviour change towards particular
food items, and consequently aﬀect the child obesity level.
2
Related Work
The term “Serious Game” was ﬁrst introduced by Clark Abt in his book “Serious
Games” [9]. As a researcher in U.S. research lab, his main goal was to promote
games as tools for educating and training. According to him, “Games may be
played seriously or casually. We are concerned with serious games in the sense
that these games have an explicit and carefully thought-out educational purpose
and are not intended to be played primarily for amusement. This does not mean
that serious games are not, or should not be, entertaining.” If designed properly,
serious games have a great potential of completely engaging children in a given
task and therefore aﬀect their behaviour [8].
There are a number of studies supporting the idea of using video games as
a tool to aﬀect children nutrition habits and stress the importance of physi-
cal activity [2,3,10,11]. United State Department of Agriculture (USDA) most
recently started a project “Choose My Plate” and developed a supporting game
named “My Plate” [12,13].The aim is to help children ﬁnd healthy eating style

Farm: Serious Game for Addressing Child Obesity
339
and build it throughout the years and hence, increase the awareness about the
food they eat and drink knowing that the right mix can help them be healthier
now and in the future.
“FoodChamps.org” represents yet another company from USA that created
a game known as “Fruits and Veggie Color Champions” to help kids and their
family eat more fruits and vegetables at every occasions [11]. With so many
unhealthy eating options promoted in stores and via on-line media, it is diﬃcult
to get kids to eat healthy food. This game aims to teach children how to make
healthy meals through a fun and interesting approach. The game characters
are a combination of human and fruits, and have lovable names such as Big
Pauli (representative of the healthy snacks) and Great (representative of the
vegetables. In addition to design and naming of characters, the game has very
strong and powerful messages for kids.
Harvard University School of Public Health started a very successful web
based platform project titled “Kids Healthy Eating Plate” [10]. The content is
available in more than twenty languages, and in contrast to serious game and
some other projects, they are trying to combine awareness of the importance of
food items and their quality as well as the physical activity. Their slogan for
physical activity is “trade inactive sit-time to ﬁt-time”.
The main disadvantage of currently available games is that they only stimu-
late and increase awareness of the importance of movement, but do not actually
require it. The language in which they are developed is a second disadvantage,
limiting it only to, in majority of cases, an English-speaking region. The game
presented in this paper is unique for so many reasons: it consists of combination
of activities, it raises awareness about nutrition and it stimulates children to
do physical activity through a fun and interesting way. The game is written in
native (Bosnian) language with the possibility of easy transition to any language.
3
Game Design
Serious game “Farm” is designed to entertain players while attempting to modify
their behaviour related to eating and physical exercises. The game logic and its
structure are provided in the following Fig. 1.
This game serves as a visual guide in educating children about healthy eating
habits and support them in exercising. The type of food used throughout the
game represents best choice food for inspiring children’s decisions related to the
healthy meals and to emphasize physical activity as part of the equation for
health and well-being.

340
S. Bajraktarevi´c and B. Rami´c-Brki´c
The game presented here is created following the recommendations of the
nutrition experts from the Harvard School of Public Health. It shares the same
message focusing on diet quality and diversity bur also, requires physical activity
such as moving around, in order to complete given tasks and instead of, using
fancy equipment or gym.
Fig. 1. Game logic and structure
The environment of the game consists of a farmer and a colourful farm. The
game is composed of four levels, each based on speciﬁc group of food items. For
instance, ﬁrst level represents the vegetables, the second level represents fruits,
grains represent the third level and proteins are in the fourth level. Within each

Farm: Serious Game for Addressing Child Obesity
341
Fig. 2. The farm
level, a farmer needs to run through the farm in order to collect particular group
of items. Farmer is also presented with obstacles in a form of unhealthy food
items such as hamburgers. These items decrease the number of won points. Once
the farmer succeeds in winning adequate number of points, he is awarded with
an applause and the possibility to change the level. Each food item shown in
the game, is followed by a message explaining its importance and usefulness to
human body.
3.1
Game Production
The game was produced on a MacBook Pro computer (Figs. 3 and 4). The
model of a farm was taken with permission from Unity Access Store (see Figs. 2
and 6). The main focus of the game was the connection with the Microsoft
Kinect and therefore enabling the physical activity—running through the farm
(see Fig. 5). The game logic was added through Unity (see Fig. 4). The work in
Unity consisted of several parts: scripts that were created using C# for menus,
character as for the environment and game logic.
The exported game can be played as a stand-alone application on Kinect
sensor without the need of internet connection. Certain space must be provided
in front of the Kinect device for it to recognize the movement and for child itself,
to be able to run and pick particular food items.

342
S. Bajraktarevi´c and B. Rami´c-Brki´c
Fig. 3. The main character
Fig. 4. The game’s art style
4
Preliminary Feedback
The game has received positive initial feedback. Even though it was shown only
to a small selected group of participants, we have accomplished our goal. The
game did make participants move and they did want to continue playing it.
The colours used in the game were much more catching and pleasant for the
observers. After the game, participants felt the need for the healthy food items.

Farm: Serious Game for Addressing Child Obesity
343
Fig. 5. Picking the vegetables in one of the levels
Fig. 6. The farmer in the farm
5
Conclusions and Future Work
We strongly believe that giving children additional tools such as here presented
serious game Farm for learning about importance of healthy eating and increased
physical activity could signiﬁcantly contribute to reducing the overall level of
child obesity present in the world. The advantage of the serious game is its uti-
lization which can be at home with parents and siblings as well as kindergarten
or classroom with teachers and classmates. Therefore, the impact of the serious
game is transferable to others from child’s surrounding. The level of engage-
ment depends greatly on the quality of the game design and the creativity of
the designer. The design of serous game should be based on the most accurate
scientiﬁc evidence related to healthy nutrition and physical activities as well as
theories for behaviour changes.

344
S. Bajraktarevi´c and B. Rami´c-Brki´c
This paper presents a preliminary study. More time and work needs to be
devoted to the design, as well as on the expansion of given tasks and learn-
ing assignments such as creation of healthy dish following the recommendations
of the Kid’s Healthy Eating Plate. In order to test the eﬀectiveness of the pre-
sented game, more thorough study needs to be performed. The study will include
preschool and school children as well as their teachers and record the obesity
level over certain period of time.
References
1. WHO—World Health Organization. Global recommendations on physical activ-
ity
for
health
(2010).
http://www.who.int/dietphysicalactivity/publications/
9789241599979/en/. Accessed Mar 2017
2. PBH—Produce for Better Health Foundation (2017). http://www.foodchamps.
org/disclaimers/parentsinfo.html. Accessed Mar 2017
3. Bajraktarevic, S., Ramic-Brkic, B.: Kockica: developing a serious game for alpha-
bet learning and practising vocabulary. In: Advanced Technologies, Systems, and
Applications, pp. 349–358. Springer International Publishing (2016)
4. Baranowski, T., Lin, L.S., Wetter, D.W., Resnicow, K., Hearn, M.D.: Theory
as mediating variables: why aren’t community interventions working as desired?
Annal. Epidemiol. 7(7), 89–95 (1997)
5. Baranowski, T., Smith, M., Hearn, M., Lin, L., Baranowski, J., Doyle, C.: Patterns
in children’s fruit and vegetable consumption by meal and day of the week. J. Am.
Coll. Nutr. 16, 216–223 (1997)
6. Baranowski, T., Buday, R., Thompson, D., Baranowski, J.: Playing for real: video
games and stories for health-related behavior change. Am. J. Prev. Med. 34, 74–82
(2008)
7. Haskell, W.L., Lee, I.M., Pate, R.R., Powell, K.E., Blair, S.N., Franklin, B.A., Mac-
era, C.A., Heath, G.W., Thompson, P.D., Bauman, A.: Physical activity and public
health: updated recommendation for adults from the American College of Sports
Medicine and the American Heart Association. Med. Sci. SportsExerc. 39(8), 1423–
1434 (2007)
8. Schuller, B., Dunwell, I., Weninger, F., Paletta, L.: Serious gaming for behavior
change: the state of play. IEEE Pervasive Comput. 12(3), 48–55 (2013)
9. Goran, M.I., Treuth, M.S.: Energy expenditure, physical activity, and obesity in
children. Pediatr. Clin. North Am. 48(4) (2001)
10. Brown, S.J., Lieberman, D.A., Germeny, B.A., Fan, Y.C., Wilson, D.M., Pasta,
D.J.: Educational video game for juvenile diabetes: Results of a controlled trial.
Med. Inform. 22(1), 77–89 (1997)
11. My plate. United States Department of Agriculture (2017). https://www.
choosemyplate.gov/MyPlate. Accessed Apr 2017

Farm: Serious Game for Addressing Child Obesity
345
12. Healthy eating plate and healthy eating pyramid. Harvard University (2011).
https://www.hsph.harvard.edu/nutritionsource/healthy-eating-plate/.
Accessed
Apr 2017
13. My plate—match game. Dairy Council of California. Healthy Eating Made Eas-
ier (2017). http://www.healthyeating.org/Healthy-Kids/Kids-Games-Activities/
My-Plate-Match-Game.aspx. Accessed Apr 2017

Part III
Power Quality

Application of EMD and STFT Methods
in Analysis of Energization of an Unloaded
Overhead Line Under Different Operating
Conditions
Snežana Vujošević and Saša Mujović(&)
Faculty of Electrical Engineering, University of Montenegro,
Podgorica, Montenegro
sasam@ac.me
Abstract. Maintaining of voltage conditions recommended by EN 50160
Standard is ultimate aim related to power quality. This issue gets an additional
importance in era of modern power systems and smart grid environments.
Occurrence of voltage whose magnitude exceeds the highest voltage level
recommended for proper electrical equipment operation is known as overvolt-
age. Switching operation such as line energization can cause this type of dis-
turbance. The paper deals with switching overvoltages occurred during
energization of an unloaded overhead transmission line under different operating
conditions. Namely, this process can be performed under normal or fault con-
ditions. The latter means that some kind of fault has occured along the line and
exists at the moment of energization. Switching overvoltages have to be ana-
lyzed not only from the aspect of magnitude, but also from the aspect of the
existing harmonics in the harmonic spectrum. Accordingly, the Empirical Mode
Decomposition and Short time Fourier Transform were used. The obtained
results were compared to the calculated ones. In this way, one may get char-
acteristic harmonics values which occur during the energization process, what
would make a worthwhile contribution related to recognition of operating
conditions during energization. Also, a conclusion about applied methods efﬁ-
ciency is drawn.
1
Introduction
Proliferation of modern devices into power system signiﬁcantly contribute to awareness
raising about importance of power quality issue. Proper operation of personal com-
puters, ﬂuorescent lamps and other appliances based on power electronics components
is closely related to maintaining of voltage conditions recommended by EN 50160
Standard [1]. Strengthening of smart grid concept and involving of information and
communication technologies represent an additional impetus toward achieving of
prescribed grid parameters [2].
Voltage sags and dips, the appearance of harmonics, overload of transformers,
signiﬁcant neutral line currents and strong overheating of the lines are the typical
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_33

problems related to a poor voltage supply quality [3]. Detection of the above mentioned
problems is the ﬁrst step in their elimination [4].
This paper addresses energization of unloaded overhead lines. Energization process
is very often accompanied by switching overvoltages. They represent high magnitude
voltage surges of a short duration which strongly affects power system reliability, i.e.
power quality and has signiﬁcant impact on power system structure. The maximum
overvoltage, which depends on system parameters and network conﬁguration, can
reach up to twice the amplitude of the system voltage but higher values are not rarity
[5]. To that end, their occurrence may cause damage to the insulation or some parts of
the equipment and therefore, it is necessary to identify and eliminate their effects [6].
The instant of line energization for the most part affects the magnitude of occurred
switching overvoltage. The magnitude will take a higher value for the case of greater
difference between the grid and the line voltage at the instant of energization [7]. It
should also stress out that voltage at the load end of the line is higher than the voltage at
the source end of line due to Ferranti-effect [8]. It means that one may expect more
severe voltage conditions at the line’s load end.
Energization of an unloaded overhead line can be performed under normal or fault
conditions. The latter means that some kind of fault has occured along the line and
exists at the moment of energization. In relation with faults, asymmetric phase to
ground faults are very common occurrence [9]. Approximately, 70% of these faults are
single phase to ground faults, while 20% are double phase to ground faults. Knowing
that the very process of line energization is asymmetric it becomes clear of how
challenging task is to analyze the issue which the paper deals with.
Until recently, common approach in the switching overvoltage analysis was based
on consideration of the surge waveform maximum value. Accordingly, the values of
frequencies and harmonics that accompany the observed transient process were not
taken into account [10–13]. Bearing in mind a dramatic increase of sensitive electronic
devices in the load proﬁle of modern systems, such an approach is unacceptable. To
that end, digital signal processing methods are increasingly used in analyzing of
switching overvoltages and power quality disturbances in general [14–20].
The authors intention is to perform harmonic spectrum analysis during line ener-
gization under different operating conditions. This paper promotes the Empirical Mode
Decomposition (EMD) method, which is convenient for the nonlinear and nonsta-
tionary data analysis. The main innovation of the method is the introduction of intrinsic
mode functions based on the local properties of the signal [14–17]. Besides using the
EMD method, the analysis of energization process was performed by Short Time
Fourier Transform (STFT) method, which is a standard transformation for signals with
time-varying spectrum analysis [20]. The results obtained by the EMD and STFT
methods were compared with calculated ones and conclusions related to their efﬁciency
and characteristic harmonics bands that occur during line energization were drawn.
The paper is organized as follows. Section 2 presents method for analytical
determination of frequencies that appear during the energization process, as well as
EMD and STFT methods. Section 3 provides simulated voltage signals characteristic
for a line energization under different operating conditions. A harmonic analysis of the
signals by application of EMD and STFT methods is given in the Sect. 4. Conclusion
and Reference list are at the end of the paper.
350
S. Vujošević and S. Mujović

2
Methods for Harmonic Determination During Line
Energization
This section presents method for analytic determination of frequencies that occur as a
result of switching surges during energization process and gives an overview of digital
signal processing methods (EMD and STFT), which are used in the considered examples.
2.1
Analytical Determination of the Expected Frequencies
In this paper it is assumed circuit breaker pole discordance during line’s energization,
i.e. energization process is considered as asymmetric. Also, as it is mentioned before,
single phase to ground fault and double phase to ground fault are asymmetric occur-
rences. Since modeling of asymmetric processes need utilization of a three phase model,
the authors employed method of symmetrical components in order to simplify analysis.
During the process of energization of three-phase unloaded overhead transmission
lines, the traveling waves propagate along the line and their refraction and reﬂection
occurs. The time in which the surge wave arrives from the beginning to the end of line is:
s ¼ l 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
L  C
p
;
ð1Þ
for direct component, and:
s0 ¼ l 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
L0  C0
p
;
ð2Þ
for zero component, where l is the length of the line, whereas L, L0, C, C0 are lines
inductances and capacitances per unit length for direct and zero sequence, respectively.
It is well known that for the overhead transmission lines the propagation velocity of
the direct component of the traveling waves is very close to the speed of light. In the
three-phase presentation, the following frequency values are calculated:
• the refraction and reﬂection of the traveling waves (direct component) with velocity
value v along the unloaded transmission line with length l causes the frequency
value
f ¼ v
4l :
ð3Þ
• the refraction and reﬂection of the traveling waves (zero component) with velocity
value v0 along the unloaded transmission line with length l causes the frequency
value
f0 ¼ v0
4l :
ð4Þ
Application of EMD and STFT Methods in Analysis
351

• the transition of the system from one stationary state to another causes natural
frequencies of the system (direct and zero component), which can be calculated
based on the equivalent line T scheme [21]:
f ¼
1
2p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Le þ L l
2


 Cl
q

 ;
ð5Þ
f0 ¼
1
2p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Le0 þ L0 l
2


 C0l
q

 ;
ð6Þ
where Le is equivalent generator and transformer inductance.
From the Eqs. (1)–(6) follows that the accuracy of the results obtained analytically
depends on the knowledge of the input data for L and C, which are not always readily
available, and this makes the application more complex.
2.2
EMD Method
The EMD is a novel signal analysis tool which gives sharp identiﬁcation of embedded
signal components. It was introduced by N.E. Huang and it has become an important
tool for signal decomposition and analysis [14]. Recently, EMD has been successfully
applied to power quality monitoring, power line energization, as well as general signal
decomposition and radar signal processing problems.
The EMD decomposes a given signal x(t) into a sum of intrinsic mode functions
(IMF) through an iterative process called sifting. By deﬁnition, an IMF satisﬁes two
conditions:
1. The number of extrema and the number of zero crossings may differ by no more
than one.
2. The average value of the envelope deﬁned by the local maxima, and the envelope
deﬁned by the local minima, is zero.
Thus, locally, each IMF contains lower frequency oscillations than the previously
extracted one. The IMFs are obtained using the following algorithm (sifting process):
1. Identify all extrema of xðtÞ.
2. Interpolate between minima (maxima) obtaining envelopes elowðtÞ (resp. eupðtÞ).
3. Compute the average envelope mðtÞ ¼ eupðtÞ þ elowðtÞ


2.
4. Extract the detail dðtÞ ¼ xðtÞ  mðtÞ.
5. If dðtÞ is an IMF, extract it and replace xðtÞ with rðtÞ ¼ xðtÞ  dðtÞ,
If dðtÞ is not an IMF, continue sifting replacing xðtÞ with dðtÞ.
6. Repeat steps (1)–(5) until some of the stopping criteria are satisﬁed.
To guarantee that the IMF functions retain enough physical sense we have to
determine criterion for the sifting process to stop. The most widely used criterion is to
limit the size of standard deviation (SD), computed from the two consecutive sifting
results as:
352
S. Vujošević and S. Mujović

SD ¼
X
T
t¼0
imfðk1ÞðtÞ  imfkðtÞ


imf 2
ðk1ÞðtÞ
2
"
#
ð7Þ
The sifting stops when SD value comes between 0.2 and 0.3 (experimentally
determined threshold). By summing all IMF components with residual we obtain
xðtÞ ¼
X
N
i¼1
imfi þ rN
ð8Þ
Thus, we achieved a decomposition of the data into N empirical modes, and a
residue rN, which can be either the mean trend or a constant.
2.3
STFT Method
The STFT is introduced to overcome the problem of nonstationary signal analysis. It is
based on the assumption that for a short-time basis, signal can be considered as
stationary.
The main idea is introducing a window function which will truncate the analyzed
signal, and applying the Fourier Transform on the truncated parts. Time-frequency
representation is obtained by sliding the window along the signal.
The expression for the STFT is
STFT t; x
ð
Þ ¼
Z1
1
x t þ h
ð
Þw h
ð Þejxhda ¼ FT x t þ h
ð
Þw h
ð Þ
½
;
ð9Þ
where w(h) is the window function, whereas h represents the time variable.
The crucial drawback of this method is that the length of the window is related to
the frequency resolution. For any chosen window function the expression Dt  Df ,
where Dt is time and Df is frequency resolution, is always constant, i.e.
DtDf  1=4p
ð9Þ
Increasing the window length leads to improving frequency resolution, but it means
that the non-stationarities occurring during considered interval will be smeared in time.
3
Simulated Results of Voltage at the Load End of a Line
During Energization
An analysis of voltage characteristics during line energization is conducted by using of
simulated results. The single-phase model of the considered system, given in Fig. 1,
was taken as a base for development of Matlab Simulink model. Accordingly, it should
has modelled: e(t)—a time-dependent value of electromotive force; Re = Rg + Rtr and
Le = Lg + Ltr—equivalent generator and transformer resistance and inductance,
Application of EMD and STFT Methods in Analysis
353

respectively and overhead transmission line with parameters L, C and R, which rep-
resent its inductance, capacitance and resistance per unit length, respectively.
Simulations were performed with different lengths of overhead transmission lines.
Namely, the lines are generally categorized into the three groups with regard to their
physical length: short lines (if length is less than 80 km), medium-length lines (if
length is between 80 and 240 km) and long lines (for length greater than 240 km). We
have taken into account this division and we investigated following line lengths: 10, 30,
50, 100 and 500 km. Table 1 comprises parameters of the analyzed transmission lines.
In this table, Xe and Xe0 represent equivalent reactive generator and transformer
resistance and its zero sequence, respectively; Re0 stands for zero sequence of Re,
respectively; ZC and ZC0 are characteristic line impedance and its zero sequence,
respectively. For the sake of the paper’s brevity, the next analysis is related to line
length of 500 km. The line of this length is representative and all derived conclusions
have validity for lines of other lengths.
3.1
Line Energization Under Normal Conditions
The ﬁrst considered case is energization of an unloaded overhead line without fault
presence at the moment of energization. It is supposed a delay in closing the switch
contacts of individual phases, and therefore line energization process takes place
non-simultaneously. The process is asymmetric. Simulated voltage waveforms for all
three phases of an unloaded overhead line during energization under normal conditions
are presented in Fig. 2.
e(t)
1
2
i
L, C, R
B
Re
Le
Fig. 1. The single-phase model of the analysed system
Table 1. Parameters of the analysed overhead transmission lines
l (km) Xe (H) Xe0 (H) Re (X) Re0 (X) R (X) R0 (X) ZC (X) ZC0 (X)
10
63
16
4.4
0.7
0.33
0.48
370
740
30
63
16
4.4
0.7
0.33
0.48
370
740
50
63
16
4.4
0.7
0.33
0.48
370
740
100
201
91
4.4
0.7
0.069 0.219
321
640
500
100
70
8
7
0.021 0.170
273
600.6
354
S. Vujošević and S. Mujović

3.2
Line Energization Under Single Phase to Ground Fault
Single phase to ground fault arises when there is a low impedance connection between
one of the three phases and the ground. This is the most common type of fault in power
systems. Assuming single phase to ground fault on phase A, Fig. 3 depicts voltage
waveforms of phases B and C (healthy phases), during line energization process.
3.3
Line Energization Under Double Phase to Ground Fault
Double phase to ground fault represents a low ohmic connection between two phases
with existence of a ground contact. From assumption that double phase to ground fault
has occurred on phases B and C, the voltage waveforms for phase A is given in Fig. 4.
Fig. 2. Energization of an unloaded 500 kV overhead transmission line under normal operating
conditions
Fig. 3. Energization of an unloaded 500 kV overhead transmission line under single phase to
ground fault—voltage waveforms of phases B i C
Application of EMD and STFT Methods in Analysis
355

4
Voltage Signals Analysis by Using of EMD and STFT
In this section authors intention is to perform harmonic spectrum analysis of previously
presented voltage signals occurred during line energization. The analysis is conducted
by using of EMD and STFT methods. The extracted frequencies were compared with
obtained results using of an analytical approach.
The decomposition results (IMFs) of the voltage signal from Fig. 2 are presented in
Fig. 5, while Fig. 6 illustrates IMFs in frequency domain. All results relates to phase A
of the line (the same conclusions apply for other two phases).
Fig. 4. Energization of an unloaded 500 kV overhead transmission line under double phase to
ground fault—voltage waveforms of phase A
imf3
imf2
imf1
imf5
imf4
Fig. 5. IMFs for signal distinctive for a line energization under normal conditions. 1200 signal
samples are used as input to the EMD
356
S. Vujošević and S. Mujović

The signal was also analyzed by a direct application of the STFT method, and the
results are presented in Table 2. As regard the Table 2, it provides an overview of the
analytically calculated frequency values obtained on the basis of Eqs. (3)–(6) and
estimated frequency values, obtained by using of the EMD and the STFT methods. The
calculated values are taken as a comparison base for the estimated values, i.e. accuracy
assessment of the used methods.
A sign “–” in the Table 2 means that the marked frequency value was not detected
by using of EMD and STFT methods. For the sake of simplicity, all calculated (esti-
mated) frequencies are identiﬁed with numbers in their subscripts, rather than using
marks for direct and zero components.
The comparison between the results of the analysis of the voltage signal obtained
by the unloaded 500 kV overhead line energization under normal conditions and the
calculated ones shows that results obtained by the EMD method are close to the
calculated values, i.e. EMD provides better frequency estimation then STFT method.
FT[imf5]
FT[imf4]
FT[imf3]
FT[imf2]
FT[imf1]
Fig. 6. Fourier transform of the IMFs presented in Fig. 5
Table 2. Frequencies obtained by the EMD and STFT methods compared with the calculated
values for the line energization under normal conditions
Method
f1 (Hz) f2 (Hz) f3 (Hz) f4 (Hz) f5 (Hz)
Analytical 50
74.20
96.05
98
150
EMD
46.88
70.31
93.75
–
152.30
STFT
52.20
–
89.50
–
–
Application of EMD and STFT Methods in Analysis
357

The IMFs of the voltage signal from Fig. 3 are presented in Fig. 7, while Fig. 8
illustrates IMFs in frequency domain. Single phase to ground fault has occurred on
phase A and aforementioned ﬁgures contain results for healthy phase B of the line.
Fig. 7. IMFs for signal distinctive for a line energization under single phase to ground fault
Fig. 8. Fourier transform of the IMFs presented in Fig. 7
358
S. Vujošević and S. Mujović

Calculated and estimated frequency values for the considered case of presence of
single phase to ground fault during line energization process are presented in Table 3.
As in the previously analysed case, EMD has proved as a better solution for frequency
extraction than the STFT.
Figure 9 presents the voltage signal of phase A, decomposed into intrinsic mode
functions. The signal relates to line energization under double phase fault conditions (B
and C are faulty phases). The evaluation of frequencies extracted from the decomposed
voltage signal is presented in Fig. 10.
Table 3. Frequencies obtained by the EMD and STFT methods compared with the calculated
values for the line energization under single phase to ground fault
Method
f1 (Hz) f2 (Hz) f3 (Hz) f4 (Hz) f5 (Hz) f6 (Hz) f7 (Hz)
Analytical 50
89.24
92.92
185.62 292.13 336.77 620.67
EMD
48.83
–
94.60
204.47 –
338.75 622.56
STFT
48.90
–
90.60
–
–
332.40 –
Fig. 9. IMFs for signal distinctive for a line energization under double phase to ground fault
Application of EMD and STFT Methods in Analysis
359

For the voltage signal obtained by an unloaded overhead transmission line ener-
gization under double phase to ground presence, the EMD method also provides very
accurate results. Four out of ﬁve calculated values of the frequencies are extracted. In
this case, by direct application of the STFT method, only two frequency values have
been extracted (Table 4).
5
Conclusion
This paper addresses calculation and estimation of frequencies characteristic for an
unloaded overhead line energization under different operating conditions. It has been
shown that a line energization issue can be performed both under normal conditions
and under fault conditions. Related to faults, we have taken into account an asymmetric
phase to ground faults that can exist at the moment of line energization. In order to
achieve harmonic decomposition of voltage signals obtained by simulations for a line
Fig. 10. Fourier transform of the IMFs presented in Fig. 9
Table 4. Frequencies obtained by the EMD and STFT methods compared with the calculated
values for the line energization under double phase to ground fault
Method
f1 (Hz) f2 (Hz) f3 (Hz) f4 (Hz) f5 (Hz)
Analytical 50
91.43
177.23 312.11 375.52
EMD
48.83
88.50
164.80 354.10 –
STFT
48.89
88.89
–
–
–
360
S. Vujošević and S. Mujović

energization process, the EMD and STFT methods were used. Comparing the fre-
quency values estimated by using of the methods to calculated ones, it was concluded
that the EMD method is extremely reliable and that its application, with high precision,
can get the harmonic spectrum of various overvoltage signals.
References
1. EN50160 Standard: Voltage characteristics of electricity supplied by public distribution
systems. In: CENELEC (1994)
2. Farzanehrafat, A., Watson, N.R.: Power quality state estimator for smart distribution grids.
IEEE Trans. Power Deliv. 28(3), 2183–2191 (2013)
3. Chattopadhyay, S., Mitra, M., et al.: Electric Power Quality, pp. 1–3. Springer, London-New
York (2011)
4. Knežević, J.M., Katić, V.A.: The hybrid method for on-line harmonic analysis. Adv. Electr.
Comput. Eng. 11(3), 29–34 (2011)
5. IEC 60071-2 Standard: Insulation Co-ordination—Part 2. International Electrotechnical
Commission, Switzerland (1996)
6. Seyedi, H., Tanhaeidilmaghani, S.: New controlled switching approach for limitation of
transmission line switching overvoltages. IET Gener. Transm. Distrib. 7(3), 218–225 (2013)
7. Filipović-Gečić, B., Uglešić, I., Pavić, I.: Application of line surge arresters for voltage
uprating and compacting of overhead transmission lines. Electr. Power Syst. Res. 140, 830–
835 (2016)
8. Nagpal, M., Martinich, T.G., Bimbhra, A., et al.: Damaging open-phase overvoltage
disturbance on a shunt-compensated 500 kV-line initiated by unintended trip. IEEE Trans.
Power Deliv. 30(1), 412–419 (2015)
9. Popović Lj, M.: Algorithm for single phase-to-ground fault digital distance relay. IET Gener.
Transm. Distrib. 6(3), 226–232 (2012)
10. Davila, M., Naredo, J.L., Moreno, P., Ramirez, A.: Practical Implementation of a
transmission line model for transient analysis considering corona and skin effects. In:
Proceedings of IEEE Bologna Power Tech Conference, Bologna, Italy (2003)
11. Škuletić, S., Vujosević, S.: Possibilities for an analysis of switching overvoltages due to
three-phase faults tripping with a discrete method. In: Proceedings of 35th UPEC
Conference, Belfast, N.Irland, P.No. 2 (2000)
12. Ramirez, A.I., Semlyen, A., Iravani, R.: Modeling nonuniform transmission lines for time
domain simulation of electromagnetic transients. IEEE Trans. Power Deliv. 18(3), 968–974
(2003)
13. Vujosević, S., Škuletić, S.: Energization of an unloaded three-phase transmission line with
consideration of frequency dependent parameters, corona effects and dissipations of circuit
breaker switching moments. In: Proceedings of 40th UPEC Conference, Cork, UK, P.
No. 95 (2005)
14. Avdaković, S., Nuhanović, A., Kušljugić, M., Musić, M.: Wavelet transform applications in
power system dynamics. Electr. Power Syst. Res. 83(1), 237–245 (2012)
15. Panigrahi, B.K., Pandi, V.R.: Oprimal feature for classiﬁcation of power quality disturbances
using wavelet packet-based fuzzy k-nearest neighbor algorithm. IET Gener. Transm. Distrib.
3(3), 296–306 (2009)
16. Barros, J., Diego, R.I., De Apraiz, M.: Application of wavelets in electric power quality:
voltage events. Electr. Power Syst. Res. 88, 130–136 (2012)
Application of EMD and STFT Methods in Analysis
361

17. Guillen, D., Paternina, M.R., Zamora, A., et al.: Detection and classiﬁcation of faults in
transmission lines using the maximum wavelet singular value and Euclidean norm. IET
Gener. Transm. Distrib. 9(15), 2294–2302 (2015)
18. Maier, V., Pavel, G., Maier, C.D., Birou, I.: Correct application of the discrete Fourier
transform in harmonics. Adv. Electr. Comput. Eng. 8(1), 26–30 (2009)
19. Seyedtabaii, S.: Improvement in the performance of neural network-based power transmis-
sion line fault classiﬁers. IET Gener. Transm. Distrib. 6(8), 731–737 (2012)
20. Chakrapani, V., Prasad, C.D., Kumar, K.K.: Power quality disturbance analysis using short
time Fourier transform and S-transform. In: Proceedings of 2nd International Conference on
Innovations Electrical and Electronic Engineering, Pune, India, pp. 384–390 (2015)
21. Andersson, G.: Electric Power System-Electric Power Transmission and Distribution,
pp. 99–100. EEH—Power Systems Laboratory, Zurich (2009)
362
S. Vujošević and S. Mujović

Voltage Flickers as Voltage Quality Problem
in Industrial Sector
Ivan Ramljak1(&), Jurica Perko2, and Matej Znidarec3
1 P.U “Elektroprivreda HZ HB”, Mostar, Bosnia and Herzegovina
ivanramljak1985@gmail.com
2 Regional Energy Agency North, Koprivnica, Croatia
jurica.perko@rea-sjever.hr
3 Faculty of Electrical Engineering, Computer Science and Information
Technology Osijek, Osijek, Croatia
mznidarec@etfos.hr
Abstract. Very important parameter of product called electricity is voltage
quality. In recent times electricity became a commodity that has its price and it is
known that any commodity today must have an appropriate quality for its price.
That applies for electrical energy as well. Inﬂuence of loads based on nonlin-
earity, power electronic and renewable energy sources connected on distribution
network has great consequences on voltage quality. This paper deals with
voltage quality in industrial facilities. Industrial facilities connected on distri-
bution network can be various types. Those consumers set very high require-
ments for reliability and voltage quality. Supply disruption and poor voltage
quality leads to production losses in facilities which results with ﬁnancial losses
and dissatisfaction of facility owners. In this paper several measurements in
different facilities are taken and analyzed. Those measurements and their results
are presented. Focus is placed on worst voltage quality parameter that appears in
most measurement cases—voltage ﬂickers. Voltage ﬂickers might be caused due
to distribution network or facilities themselves. Causes and consequences of
those voltage ﬂickers are analyzed in continuation of this paper.
1
Introduction
EN 50160 is standard for voltage quality supply made by CELENEC (European
Committee for Electrotechnical Standardization). Standard gives quantitative charac-
teristics of voltage in normal conditions [1, 2]. The purpose of this standard is to
provide and describe the characteristics of voltage parameters in terms of permissible
deviations. EN 50160 describes voltage characteristics in terms of [1, 2]:
• frequency,
• waveform,
• symmetry and
• magnitude.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_34

Measurement of voltage quality supply according to EN 50160 requires specialized
instrument and measuring methods. This enables continuous monitoring of the fol-
lowing parameters: voltage in three phases, power frequency, total harmonic distortion
(THD), voltage unbalance factor, ﬂickers etc. [3]. According to EN 50160 measure-
ment period should be 7 days. Measurements are strictly deﬁned with IEC 61000-4-x
standards where methods for voltage quality measurement are described. Thus, IEC
61000-4-7 describes ﬂicker measurements and IEC 61000-4-15 harmonics measure-
ments [1]. Supply voltage parameter, statistical evaluation and compliance limits are
given in Table 1.
Table 2 shows harmonic limits according to EN 50160 [4].
2
Voltage Quality Measurements in Industrial Sector
In this paper voltage quality measurement is performed in seven different types of
industrial facilities for a period of one day to 14 days. Measurement duration was
conditioned with agreement of facilities owners and with process of production. Dif-
ferent types of industrial facilities are:
Table 1 Supply voltage parameter according to EN 50160 (0.4 kV voltage level)
Supply voltage parameter
Statistical evaluation
Compliance limit
Power frequency
95% of the time in 1 week
100% of the time in 1 week
50 Hz ± 1%
50 Hz + 4% to −6%
Supply voltage variations
95% of the time in 1 week
Uc ± 10%
Rapid voltage changes (and Flicker) 95% of the time in 1 week
Plt  1
Supply voltage dips
1 year
None givena
Short interruptions
1 year
None givenb
Long interruptions
1 year
None givenc
Temporary overvoltage
1 year
None given
Supply voltage unbalance
95% of the time in 1 week
<2%
Harmonic voltage
95% of the time in 1 week
95% of the time in 1 week
See Table 1
THD < 8%
Mains signalling voltage
95% of the time in 1 day
9% @ 100 Hz
1% @ 100 kHz
aIndicative value of 1000 dips/year provided
bIndicative value of “several hundreds” of short interruptions/year provided
cIndicative value of 50 long interruptions/year provided
Table 2 Harmonic limits according to EN 50160
Individual harmonics
THD 3
5
7
9
11
13
15
17
19
21
23
8.0% 5.0% 6.0% 5.0% 1.5% 3.5% 3.0% 0.5% 2.0% 1.5% 0.5% 1.5%
364
I. Ramljak et al.

1. Reinforcement mesh factory,
2. Plastic processing facility,
3. Huge project ofﬁce,
4. Printing ofﬁce,
5. Winery,
6. Wood industry and
7. Soap and detergent manufacture.
Some signiﬁcant electrical characteristics of industrial facilities such as type of
connection, connection and measurement point are shown below (Table 3).
Figure 1 shows measurement point in case of reinforcement mesh factory and
plastic processing facility. Figure 2 shows measurement point in other cases.
Table 3 Signiﬁcant electrical properties of industrial facilities
Facility
Power supply of
facility
Connection point
Measurement point
Reinforcement
mesh factory
XHE 10(20) kV
MV cable
connection
MV/LV
Distribution cabinet in
facility
Plastic
processing
facility
AL/Fe overhead
lines
Distribution
cabinet in LV
network
Distribution cabinet at
connection point in LV
network
Huge project
ofﬁce
AL/Fe overhead
lines
MV/LV
LV transformer cabinet in
substation
Printing ofﬁce
XHE 10(20) kV
MV cable
connection
MV/LV
LV transformer cabinet in
substation
Winery
XHE 10(20) kV
MV cable
connection
MV/LV
LV transformer cabinet in
substation
Wood industry
XHE 10(20) kV
MV cable
connection
MV/LV
LV transformer cabinet in
substation
Soap and
detergent
manufacture
XHE 10(20) kV
MV cable
connection
MV/LV
LV transformer cabinet in
substation
Voltage Flickers as Voltage Quality Problem
365

Main load types and measurement periods for all facility types are given as follows
(Table 4).
Fig. 1 Measurement point in case of reinforcement mesh factory and plastic processing facility
Fig. 2 Measurement point in case of huge project ofﬁce, printing ofﬁce, winery, wood industry
and soap and detergent manufacture
366
I. Ramljak et al.

3
Results of Voltage Quality Measurements in Industrial
Sector
Some basic results of voltage quality measurements in industrial sector are presented
below. Most measurements lasted for seven days, but some were shorter/longer.
Measurements were performed in Herzegovina region (B&H) and Slavonia region
(Croatia). Measurement instruments were different, but in compliance with EN 50160
and IEC standards. According to IEC 61000-4-30 2003, instruments used for mea-
surements were class A and B. Due to large amount of collected data summary, results
are expressed in table below (Table 5) in order to be clearly visible and easier to
compare. Satisfying results of voltage quality parameters are marked with √, and results
that are not in compliance with standard are marked with X.
Table 4 Main load types and measurement periods for all facility types
Facility type
Main load type
Measurement
period
Reinforcement
mesh factory
Asynchronous motors (machinery for reinforcement
mesh), welding machines, computers, air conditioning
system, lighting
10.07.2015.–
17.07.2015.
Plastic processing
facility
Asynchronous motors (plastic processing machinery),
heating appliances, air conditioning system, lighting
24.06.2011.–
24.06.2011.
Huge project
ofﬁce
More than 100 computers, UPS, printers, server room,
lighting, kitchen appliances, air conditioning system
22.05.2015.–
29.05.2015.
Printing ofﬁce
Asynchronous motors (printing machines, paper
processing machines, machines for paper coloring),
UPS, air conditioning system, computers, PLC,
lighting
06.09.2013.–
20.09.2013.
Winery
Asynchronous motors (production line), air
conditioning, computers, lighting
17.05.2006.–
24.05.2006.
Wood industry
Asynchronous motors (circular saw, other
woodworking machinery), lighting
26.11.2004.–
3.12.2004.
Soap and
detergent
manufacture
Asynchronous motors (production line), air
conditioning, lighting
19.04.2006.–
26.04.2006.
Table 5 Results of voltage quality measurements in industrial sector
PQ
Parameter
Reinforcement
mesh factory
Plastic
processing
facility
Huge
project
ofﬁce
Printing
ofﬁce
Winery Wood
industry
Soap and
detergent
manufacture
Frequency √
√
√
√
√
√
√
Voltage
√
X
√
√
√
√
√
Flickers
X
X
√
X
√
X
√
Voltage
dips
45
3,000
11
23
–
–
–
THD
√
√
√
√
√
√
√
Unbalance √
√
√
√
√
√
√
Voltage Flickers as Voltage Quality Problem
367

It is visible from Table 5 that in four of seven cases, voltage ﬂickers (Plt) are above
permissible value. In one case voltage values are not in permissible values according to
EN 50160. Also, number of voltage dips in one case is very high. Major problem in
analyzed cases were ﬂickers Plt. Connection point and power supply (cable or overhead
line) did not have inﬂuence on voltage quality parameters (excluding ﬂickers). Voltage
ﬂickers are analyzed in continuation and some solutions are presented.
4
Voltage Flicker Analysis
Voltage ﬂickers are above permissible value in four measurements. Flicker can be
explained as modulationoflight,whether periodicor intermittent, which mostusers can see
as a change in the light output, even under static circumstances [5]. Inﬂuence of voltage
ﬂickers onhuman can be observed as a disorientation, vertigo,and nausea—inducingeffect
of a strobe light ﬂashing at 1–20 Hz, approximately the frequency of human brainwaves
(the Bucha effect). The effects are similar to seizures caused by epilepsy (in particular
photosensitive epilepsy), but are not restricted to people with histories of epilepsy [6].
Flicker levels in excess of 1 (compatibility level at LV) can exist at HV/EHV without
leading to any complaints at these voltage levels. The key issues in this rationale are that
ﬂicker-producing loads may not be present at all voltage levels and that ﬂicker levels are
attenuated between voltage levels, particularly between HV/EHV and LV [7].
4.1
Case 1—Reinforcement Mesh Factory
Administrative building employees have complained about ﬂickering of lighting and
computer working process. Measurement was performed at distribution cabinet inside
facility area from which administrative building is supplied. It was noted that from
same cabinet (same supply cable) some industrial electrical machines are supplied too.
Flicker values are shown in Fig. 1. Figure 1 shows that values of voltage ﬂickers (Plt)
are much above permissible value (Plt < 1). Flickers exist in working days in working
hours as can be seen from Fig. 3.
Fig. 3 Flicker values in reinforcement mesh factory
368
I. Ramljak et al.

In Fig. 4 mean and maximum values of current in phase 1, and ﬂicker Plt at the
same phase are presented. It is visible that ﬂickers are higher as the difference between
maximum and mean value of current is greater.
Figure 5 shows mean and maximum values of current in phase 1, mean and mini-
mum values of voltage and ﬂicker Plt at the same phase. It can be seen that as difference
between maximum and mean value of current is greater the difference between mean
and minimum value of voltage is greater as well. Greater differences between mean and
minimum/maximum values of voltage/current imply greater ﬂickers.
Fig. 4 Mean and maximum values of current in phase 1, and ﬂicker Plt at the same phase
Fig. 5 Mean and maximum values of current in phase 1; mean and minimum values of voltage
and ﬂicker Plt at the same phase
Voltage Flickers as Voltage Quality Problem
369

It is clear in this case that industrial electrical machines supplied from the same
cabinet as administrative building imply great values of ﬂickers that have negative
inﬂuence on computers and lighting (ﬂickering)—administrative employees. Great
peak current (maximum current values) of those machines implies great voltage
ﬂickers. It is expressed in periods of working days/hours when machines work. After
separation of industrial and administrative building loads (separate supply cable),
complains of employees in administrative building have stopped.
4.2
Case 2—Plastic Processing Facility
Residents of area near plastic processing facility have complained about ﬂickering of
lighting in their houses. That facility had bought some new electrical machines for
processing of plastics. Flickering problems started with those new electrical machines.
Measurement was performed at distribution cabinet from which facility is connected. It
is the same cabinet from which before mentioned houses are supplied. Flicker values
are shown in Fig. 6. Figure 6 shows that values of voltage ﬂickers (Plt) are much above
permissible value (Plt < 1).
In Fig. 7 mean and maximum values of current in phase 2 are presented. It is visible
that difference between maximum and mean value of current is great.
Fig. 6 Flicker values in plastic processing facility
370
I. Ramljak et al.

After separation of plastic processing facility and houses (separate supply cable),
measurement was performed on the same cabinet from which now only houses were
supplied (Fig. 8). Flicker level is now satisﬁed.
4.3
Case 3—Printing Ofﬁce
Printing ofﬁce is situated in one industry area with some more facilities. All facilities
are connected from one MV/LV substation with two transformers. Figure 9 shows that
voltage ﬂickers exist no matter facility is working or not. Flicker values are indepen-
dent on rated current of facility.
It is obvious that complex of facilities supplied from the same substation has common
inﬂuence on ﬂicker level. Solution of this problem could be separation of supply points
(second MV/LV substation) or installation of VAR compensator. The main objectives of
Fig. 7 Mean and maximum values of current in phase 2
Fig. 8 Flicker values after separating houses power supply from plastic processing facility
Voltage Flickers as Voltage Quality Problem
371

VAR compensation are to increase the stability limit of the power system, to decrease
voltage ﬂuctuations during load variations and to limit overvoltages due to large distur-
bances [8].
4.4
Case 4—Wood Industry
In wood industry ﬂicker levels are a little above upper level given by EN 50160
(Fig. 10). This facility is supplied by own substation MV/LV. In order to deeper
Fig. 9 Mean voltage, mean current and ﬂicker Plt values for printing ofﬁce
Fig. 10 Flicker values in facility of wood processing
372
I. Ramljak et al.

analyse for ﬂicker reasons it is needed to take a look in short term ﬂicker
diagram.
In Fig. 11 mean and maximum values of current in phase 2, and ﬂicker (Plt) of the
same phase is shown. It is visible that ﬂickers are higher as well as the difference
between maximum and mean value of current is greater. The reason is variable (dy-
namic) work mode of electric loads in this facility. One of problem solutions could be
based on SVC technology, details in [8].
5
Conclusion
Results of voltage quality measurements in various industrial facilities are presented in
this paper. All measurements were performed on 0.4 kV voltage level. Obtained results
imply problems mainly with voltage ﬂickers. Those results are independent of facility
type. Main reason for voltage ﬂicker existence is variable (dynamic) work mode of
electrical loads in these facilities. Problem becomes more serious if some other con-
sumers (commercial buildings, homes) are supplied from the same distribution cabinet,
because they will feel ﬂickering that can be very uncomfortable for the human psyche.
Generally, solution would be separation of industrial (causes of ﬂickers) and other
supplies (commercial buildings, houses etc.). Also, in industrial areas, one of problem
solutions can be based on SVC technology.
Fig. 11 Mean and maximum values of current in phase 2, and ﬂicker Plt at the same phase
Voltage Flickers as Voltage Quality Problem
373

References
1. Ramljak, I.: Inﬂuence of PV plant connection on power quality in point of common coupling
in distribution network. Bosanskohercegovačka elektrotehnika 8 (2014) (in Croatian)
2. Perko, J.: Comparison of Grid Rules and Standard HRN EN 50160. Elektrotehničko društvo
Zagreb, E-biblioteka (in Croatian) (2010)
3. Markiewicz, H., Klajn, A.: Voltage Disturbances, Standard EN 50160, guide. Wroclaw
University of Technology and Copper Development Association, European Copper Institute
(2014)
4. Quality of supply standards: Is EN 50160 the answer? https://www.oasis-open.org/committees/
download.php/37248/Power%20Quality%20White%20Paper%20from%20Schneider.pdf.
Accessed Dec 2016
5. Understanding the lighting ﬂicker frustration. http://www.ledsmagazine.com/articles/print/
volume-12/issue-11/features/ﬂicker/understand-the-lighting-ﬂicker-frustration.html. Accessed
Jan 2017
6. Milankov, R., Čarnić, J.: Flickers—cause, impact on the environmental and mitigation. In:
22nd International Conference on Electricity Distribution—CIRED, Stockholm, Sweden
(2013)
7. Halpin, M., et al.: A review of ﬂicker objectives related to complaints, measurements, and
analysis techniques. In: 20th International Conference on Electricity Distribution—CIRED,
Prague, Czech Republic (2009)
8. De Kock, J., Strauss, K.: Practical Power Distribution for Industry. Elsevier (2004)
374
I. Ramljak et al.

Power Quality Field Measurements
on Photovoltaic System
Mia Lešić1 and Tatjana Konjić2(&)
1 Electricity Transmission Company, Elektroprijenos – Elektroprenos BiH a.d.
Banja Luka, Operational Area Tuzla, Tuzla, Bosnia and Herzegovina
2 Faculty of Electrical Engineering, University of Tuzla, Tuzla, Bosnia and
Herzegovina
tatjana.konjic@untz.ba
Abstract. Solar energy is the main driving force of climate cycles and all life
cycles on Earth. It is inexhaustible and it can be used in each country all over the
world. Bosnia and Herzegovina is in line with EU guidelines on mandatory
reductions of greenhouse gas emissions required to increase production of
electricity from the renewable energy sources. The main scope of this paper is to
investigate the power quality parameters measured during different periods and
weather conditions on the ﬁrst photovoltaic system “Eko Energija” connected to
the distribution power network in Bosnia and Herzegovina. The results of these
measurements are analysed and presented in this paper.
1
Introduction
Due to increasing environmental awareness, the legal regulations and international
agreements that require reduction of carbon emissions and improvement of energy
efﬁciency, there is need to increase the share of renewable energy in the total energy
balance of the community. Bosnia and Herzegovina is in line with EU guidelines on
mandatory reductions of greenhouse gas emissions required to increase production of
electricity from the renewable energy sources. Beside hydro power, solar energy is one
of the most suitable renewable energy sources in Bosnia and Herzegovina, due to
favourable natural conditions [1].
Following EU guidelines and recent worldwide trends, the use of photovoltaic
systems as a safe and clean source of energy from the sun has been increasing in Bosnia
and Herzegovina. The application of photovoltaic systems can be divided in 2 ﬁelds,
off-grid and grid-connected applications. Off-grid systems are being used to provide
power for remote loads and do not have power exchange with the grid. On the other
hand, grid-connected photovoltaic systems are used to provide power for local loads
and have the exchange of power with local utility grid [2].
Photovoltaic systems can enhance the operation of power systems by improving the
voltage proﬁle, but comparing with other renewable sources, photovoltaic systems still
have major difﬁculties and may have negative effect to the system in terms of power
quality [3]. Power quality measurements during different periods and weather
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_35

conditions have been performed on the ﬁrst photovoltaic system in Bosnia and
Herzegovina. The results are presented in this paper.
2
Photovoltaic Market Development
The history of photovoltaics (PVs) began in 1839 when Edmund Becquerel was able to
cause a voltage to appear during his experiment with metal electrode in a weak elec-
trolyte solution. Almost 40 years later, Adams and Day were the ﬁrst to study the
photovoltaic effect in solids. They were able to build cells made of selenium that were
1–2% efﬁcient. Russell Ohl patented the modern junction semiconductor solar cell in
1946 [2].
The ﬁrst practical photovoltaic cell was developed in 1954 at Bell Laboratories.
They used a diffused silicon p-n junction that reached 6% efﬁciency, compared to the
selenium cells that found it difﬁcult to reach 0.5%. Due to the high production costs,
solar cells found their ﬁrst commercial applications in the 1958 in space stations.
During the oil crisis in the 70-ies of the last century, when it was noticed that supplies
of fossil fuel are limited, solar cells found their use on Earth [2].
Europe has set off intense effort to deal with issues such as climate change, a
growing dependence on imported energy, ﬂuctuating oil and gas prices and increasing
energy consumption. European energy policy is built on sustainability, competitiveness
and security of supply through a series of measures such as promoting renewable
energy sources and energy efﬁciency. The European Union has committed to cut
greenhouse gas emissions, to decrease energy consumption due to increasing energy
efﬁciency and to increase share of renewable energy sources in total energy con-
sumption all by 20% until 2020 (“Target 20-20-20”) [4]. Final priority of these
European energy strategies is to achieve an energy efﬁcient Europe and to build
pan-European integrated energy market. Solar photovoltaic technology has proven in
recent years that, with the appropriate regulatory framework in place, it can be a major
contributor to reaching the European Union’s target of increasing share of renewable
energy sources by 2020.
Photovoltaic technology shows the potential to become a major source of power
generation for the world—with robust and continuous growth. That growth is expected
to continue in the years ahead as worldwide awareness of the advantages of PV
increases. At the end of 2009, the world’s PV cumulative installed capacity was
approaching 23 GW. It was 40.3 GW 1 year later and later in 2011, more than 70 GW
are installed globally and could produce 85 TWh of electricity every year. In 2012, the
100 GW mark was reached and by 2013, almost 138.9 GW of photovoltaics had been
installed globally. In only 5 years, from 2010 to 2015, the total global photovoltaic
capacity jumped over 450% from less than 41 GW. Looking back 10 years, photo-
voltaic’s development has been even more impressive—from 5 GW of total com-
missioned PV capacity at the end of 2005 the market has grown 45 times in just one
decade [5]. This amount of energy is sufﬁcient to cover the annual power supply needs
of over 45 million households. Evolution of global cumulative installed capacity from
year 2000 to 2015 is shown in Fig. 1.
376
M. Lešić and T. Konjić

After having scored the top position in the European Union in terms of new
installed capacities in 2011 and 2012, photovoltaics were in the second place in 2013
ranking, after wind systems. With more than 21 GW connected to the grid for pho-
tovoltaics and wind, these 2 renewable electricity sources together beat gas and all
other sources of electricity. If we count decommissioning (which remains marginal in
the PV sector—less than 40 MW were replaced by new capacities), wind and photo-
voltaics both come ahead of gas. All of the other sources, both renewable and con-
ventional, are far behind [5].
Solar energy is one of the most suitable renewable energy sources in Bosnia and
Herzegovina due to the natural conditions. Southern parts of Bosnia and Herzegovina
have mediterranean climate, while the north has a continental climate. Solar radiation is
about 1600 kWh/m2 in the southern parts of the country, while it is about 1250 kWh/m2
in the north [1]. An important characteristic is the number of sunny hours per year. The
number of sunny hours is 1900–2300 h in southern parts of the country, while there are
between 1800–2000 sunny hours in the northern part [6]. According to these data, it
can be estimated how much solar energy is applicable in an observed area.
According to Regulatory Commission for Electricity in Federation of Bosnia and
Herzegovina (FERK) data from year 2013, approximately 75 MW of distributed
renewable energy sources (small hydro and solar photovoltaic power plants) were
connected to the distribution network of Bosnia and Herzegovina.
Interest in the construction of photovoltaic power plants in Bosnia and Herzegovina
is noticeable. According to FERK, 65 requests for approval to build electric solar
power plants of different power (from 3.8 kW to 0.920 MW) were applied in 2013. By
2017, according to Operator for renewable energy sources and efﬁcient cogeneration in
Federation of Bosnia in Hercegovina, 224 photovoltaic plants have been built with
cumulative installed capacities of 75 MW.
Fig. 1. Evolution of global cumulative installed capacity 2000–2015 (MW) [5]
Power Quality Field Measurements on Photovoltaic System
377

3
“Eko Energija” PV System Connected to the Distribution
Network in Bosnia and Herzegovina
The ﬁrst on-grid photovoltaic system in Bosnia and Herzegovina has been commis-
sioned on 19th March 2012. The system is located on the rooftop of a gym in Kalesija
(Fig. 2a) consists from 520 solar modules (Fig. 2b), and 8 inverters (Fig. 3a, b), and is
directly connected to the power distribution network. Power of this photovoltaic system
is 120 kW, and the forecasted annual production 140 MWh of electricity. Information
on photovoltaic system in Kalesija is shown in Table 1. The inverter that is used to
connect system to the low voltage (LV) distribution network is SMA STP 1500 TL-10
type [7].
Figure 4a shows total electricity production of photovoltaic system “Eko Energija”
during hottest and sunniest month in 2012, August. Highest production was on August
13th, 2012, with amount of 850 kWh. Figure 4b shows total electricity production of
photovoltaic system “Eko Energija” during winter, in December 2012. Highest
Fig. 2. PV system “Eko Energija”—model (a), modules (b) [7]
Fig. 3. PV system “Eko Energija”—inverter (a), inverter screen (b)
378
M. Lešić and T. Konjić

production was on December 27th, 2012, with amount of 320 kWh. Production dia-
grams were taken directly from photovoltaic inverter.
Table 1. Information about ﬁrst PV system in Bosnia and Herzegovina
General
Location
Kalesija, B&H
Commissioned
2012
Solar generator
Solar module type
REC 240 PE
Number of solar modules
520  240 Wp
Maximum AC power
120 kW
Estimated annual production 140 MWh
Inverters
Inverter type
SMA STP 15000TL-10
Number of inverters
8
Topology
Transformerless
Fig. 4. Electricity production of the “Eko Energija” in August 2012 (a) and December 2012 (b)
Power Quality Field Measurements on Photovoltaic System
379

4
Power Quality Parameters Measurement
Power quality determines the ﬁtness of electrical power to consumer devices. Syn-
chronization of the voltage frequency and phase allows electrical systems to function in
their intended manner without signiﬁcant loss of performance or life. The term is used
to describe electric power that drives an electrical load and the load’s ability to function
properly. Without the proper power, an electrical device (or load) may malfunction, fail
prematurely or not operate at all. There are many ways in which electric power can be
of poor quality and many more causes of such poor quality power [8].
The quality of electrical power may be described as a set of values of parameters,
such as:
• Continuity of service,
• Voltage magnitude variation,
• Voltage and current transients,
• Harmonic content in the waveforms.
The main document dealing with requirements concerning the supplier’s side is the
EN 50160 Standard, which characterizes voltage parameters of electrical energy in
public distribution systems [8].
Since photovoltaic systems can produce a signiﬁcant impact on the power system
due to their electronic components, it is necessary to survey the power quality
parameters. Power quality measurements were performed on the PV system “Eko
Energija” several times, in different weather conditions, during different seasons.
Measurements were performed during summer (July 20 to July 24, 2012), during
autumn (October 12 to October 17, 2012) and several times during winter (December 6
to December 13, 2012 and December 28, 2012 to January 12, 2013). Measurement was
performed with Fluke 434 Three Phase Power Quality Analyzer (Fig. 5). The instru-
ment compares results with the European Standard, EN 50160:2004. Besides power
quality parameters, voltage and current waveforms were recorded, in different weather
condition. Connection of the instrument to the 3 phase system in shown in Fig. 6, and
the particular set up to the distribution cabinet of the PV system “Eko Energija” is
shown in Fig. 7.
Fig. 5. Fluke 434 three phase power quality analyzer [9]
380
M. Lešić and T. Konjić

Figure 8a, b show voltage and current waveforms recorded on October 12th, 2012
at 14:00:22 h, respectively. It is noticeable that voltage waveform could be described
by almost pure sin function, while that is not the case with current. Transformer less
inverter by which the analysed PV system is connected to the distribution network
needs to provide optimal operating point on the I-V curve for the system. During the
day, the working parameters of the system are changing. Varying position optimal
working point of PV systems is especially pronounced for devices without trans-
formers. The inverter that connects presented PV system to the grid has 2 devices to
track the maximum power point, and that is the reason for not having sine current
Fig. 6. Test set up [9]
Fig. 7. Power quality parameters measurement set up [10]
Power Quality Field Measurements on Photovoltaic System
381

Fig. 8. Voltage (a) and current (b) waveforms, October 12, 2012, cloudy [11]
Fig. 9. Voltage (a) and current (b) waveforms, December 6, 2012, cloudy [11]
Fig. 10. Voltage (a) and current (b) waveforms, January 12, 2013, sunny [11]
382
M. Lešić and T. Konjić

Fig. 11. Voltage (a) and current (b) waveforms, April 29, 2013, sunny [11]
Fig. 13. Harmonic components and THDI on cloudy (December 6, 2012) (a) and sunny (April
29, 2013) (b) weather [11]
Fig. 12. Voltage (a) and current (b) harmonic components and total harmonic distortion,
October 12, 2012 [11]
Power Quality Field Measurements on Photovoltaic System
383

waveform. Due to cloudy weather conditions, low amount of electricity production can
be noticed. Almost identical situation related to the voltage and current waveform is on
snowy and cloudy day, on the December 6, 2012 at 16:43:21 h (Fig. 9a, b,
Fig. 14. Power quality parameters (a) and events (b) according to EN 50160, July 20–July 24,
2012 [11]
Fig. 15. Harmonic components and total harmonic distrorsion (THD) for phase L1 (a), phase L2
(b) and phase L3 (c), July 20–July 24, 2012
384
M. Lešić and T. Konjić

respectively). When the weather was sunny on January 12, 2013 at 13:26:26, voltage
waveform is sine again (Fig. 10a), and the current is almost ideal sine waveform
(Fig. 10b), and there is greater electricity production than in previous cases. The
voltage and current waveforms on April 29, presented in Fig. 11a, b respectively are
Fig. 16. Third (a), ﬁfth (b) and seventh (c) harmonic component trend, July 20–July 24, 2012
Fig. 17. Power quality parameters (a) and events (b) according to EN 50160, October 12–
October 17, 2012 [11]
Power Quality Field Measurements on Photovoltaic System
385

very similar with those presented in Fig. 10a, b even though it was different season
(winter- spring).
It can be concluded that productivity of PV system does not depend on outage
temperature, but solar irradiation angle [11]. The main condition for good production is
solar irradiation that is higher during sunny days. It is not important if a sunny day is on
summer or any other seasons. It can be noticed that current on the sunny day of January
(Fig. 10b) was 92 A, and current on the sunny day of April (Fig. 11b) was 116 A while
during cloudy days of October (Fig. 8b) and December (Fig. 9b) current was 11 A and
1 A, respectively.
Figure 12a shows voltage harmonic components and total harmonic distortion
(THDU) and Fig. 12b shows current harmonic components and total harmonic dis-
tortion (THDI) measured on October 12, 2012. Fluke 434 Power Quality Analyzer
measures current harmonic component and THDI according to IEC 61000-4-30
Standard [9]. Current harmonic components are greater than voltage, but still in
allowed values. Figure 13a shows the harmonic components and Fig. 13b shows the
total harmonic distortion of current that PV system “Eko Energija” inputs into the
distribution network. Harmonic components were in each case less than the allowable
by the Standards, weather the wheatear is sunny or cloudy, and the dominance of ﬁfth
and seventh current harmonic is noticeable in both cases.
Fig. 18. Harmonic components and total harmonic distrorsion (THD) for phase L1 (a), phase L2
(b) and phase L3 (c), October 12–October 17, 2012
386
M. Lešić and T. Konjić

Figure 14a shows values of power quality parameters of PV system “Eko Energija”
measured in July 2012. The results are shown in the form of columns, which is suitable
to be compared with the EN 50160:2004 Standard. Looking from left to right the ﬁrst 3
groups of 3 columns represent the effective value of voltage, harmonics and ﬂicker,
respectively, for each phase individually. Subsequent columns represent dips, inter-
ruptions, rapid voltage changes, unbalance, frequency and mains signalling respec-
tively, each column summed up for all 3 phases. Height of column changes as the
parameter value moves away from the nominal value. If the value of any parameter is
greater than the prescribed, columns are about to change their colour from green to red.
Fluke 434 Power Quality Analyzer can also specify the exact time when there was a
disturbance such as ﬂicker, voltage dips or swells. Figure 14b shows events that
occurred related to the power quality. During this period of measurement, there have
been 2 voltage dips and 5 ﬂickers, but the summarized value for measuring time of any
of these disorders was not higher than the Standard allowable values.
Figure 15a–c show harmonic components and total harmonic distortion (THD) for
phases L1, L2 and L3 respectively. THD is for every phase less that 8%, which is
Standard allowed value. Dominant harmonic components are ﬁfth and seventh, both in
Standard allowed values. Third (a), ﬁfth (b) and seventh (c) harmonic component trend,
Fig. 19. Third (a), ﬁfth (b) and seventh (c) harmonic component trend, October 12–October 17,
2012
Power Quality Field Measurements on Photovoltaic System
387

for measuring period July 20–July 24, 2012 are shown in Fig. 16a (third), Fig. 16b
(ﬁfth) and Fig. 16c (seventh).
Power quality parameters measurements were preformed several times and the
results are presented in Figs. 17a, b, 20a, b, and 21a, b. During the second and third
measurement period (Figs. 17b and 20b, respectively) no disorder, neither from PV
system itself or the grid, was observed. During the ﬁnal measurement (Fig. 21b), there
have been 8 voltage dips and 4 ﬂickers, but the value of any of these disorders for
measuring time was not higher than the Standard allowable values (Figs. 18 and 19).
Figure 18a–c show harmonic components and total harmonic distortion (THD) for
phases L1, L2 and L3 respectively. THD is for every phase less that 8%, which is
Standard allowed value. Dominant harmonic components are ﬁfth and seventh, both in
Standard allowed values. Third (a), ﬁfth (b) and seventh (c) harmonic component trend,
for measuring period July 20–July 24, 2012 are shown in Fig. 19a (third), Fig. 19b
(ﬁfth) and Fig. 19c (seventh).
Fig. 20. Power quality parameters (a) and events (b) according to EN 50160, December 28,
2012–January 4, 2013 [11]
Fig. 21. Power quality parameters (a) and events (b) according to EN 50160, April 29–May 6,
2013 [11]
388
M. Lešić and T. Konjić

Photovoltaic system respects Standard given voltage criteria. Figure 22 shows
voltage unbalance for period July 20–July 22, 2012 (a) and October 10–October 17,
2012 (b). In both cases, voltage unbalance is 0,2% in it’s highest value, which is way
bellow limit of 2%. Example of measurement report from FlukeView—Power Quality
Analyzer Software is given in Fig. 23.
Analyzing the results of measuring the quality of electricity carried in all weather
conditions, it can be concluded that the PV system “Eko Energija” meets all regulations
set the EN 50160 Standard at point of connection to distribution network, and as such
Fig. 22. Voltage unbalance, July 20–July 24, 2012 (a), October 10–October 17 2012 (b)
Fig. 23. FlukeView—Fluke 434 power quality analyzer software measurement report, July 20–
July 24, 2012
Power Quality Field Measurements on Photovoltaic System
389

does not have global negative effect to the grid. Summarized results of all performed
measurements are shown in Table 2.
5
Conclusion
Global trends in energy are being characterized primarily by increased demand for
energy, the increase in prices of conventional energy sources, and the pursuit of
renewable energy sources. As a result of these initiatives, the rapid growth of the
installed capacity of photovoltaic systems can be seen. Installed capacity of PV systems
in the world doubles every 2 years, and photovoltaic systems are the leading tech-
nology of electricity production with the biggest growth trend.
In accordance with European Union guidelines Bosnia and Herzegovina is required
to increase electricity production from renewable sources. Recently there is increasing
integration of photovoltaic systems in the power system of Bosnia and Herzegovina.
The ﬁrst photovoltaic system with power of 120 kW was commissioned in March 2012
in Kalesija, Bosnia and Herzegovina. Due to the fact that photovoltaic systems and
renewable energy could have an impact on the quality of the distribution network,
measurement of power quality parameters were performed on photovoltaic system
“Eko Energija”. Comparing the measured values of power quality parameters with the
EN 50160 Standard, it can be concluded that observed photovoltaic system fully
complies with the regulations set by the applicable Standards at the point of connection
to the distribution network. Depending on the weather conditions the photovoltaic
system during operation produced different amount of harmonic components. How-
ever, the values of harmonic current components are negligible, and in any case do not
exceed the values prescribed by applicable Standards.
References
1. Vukmir, G., Stanisljevic, L., Cero, M., Cacan, M., Markovic, M., Rudez, M., Laganin, O.,
Kostic, R., Oprasic, S., Catovic, S., et al.: Initial National Communication of Bosnia and
Herzegovina Under the United Nations Framework Convention on Climate Change.
UNFCCC, Banja Luka, Bosnia and Herzegovina (2009)
2. Majdandzic, L.: Solarni sustavi. Graphis d.o.o., Zagreb, Croatia (2010)
Table 2. Measurement—summary
Measuring facility
Photovoltaic system Eko-Energija
Location
Kalesija, Bosnia and Herzegovina
Instrument
Fluke 434 three phase power quality analyzer EN 50160
First measurement
July 20–July 4, 2012
PASS
Second measurement October 12–October 17, 2012
PASS
Third measurement
December 6–December 13, 2012
PASS
Fourth measurement
December 28, 2012–January 1, 2013
PASS
Fifth measurement
April 29–May 6, 2013
PASS
390
M. Lešić and T. Konjić

3. Studija o priključivanju i radu distribuiranih izvora električne energije u elektroenergetskom
sistemu Crne Gore, Elektroinstitut Milan Vidmar, Ljubljana (2012)
4. European Commission: Energy 2020, a Strategy for Competitive, Sustainable and Secure
Energy. Publication Ofﬁce of the European Union, Belgium, Brussels (2010)
5. Schmela, M.: Global Market Outlook for Solar Power/2016–2020. SolarPower Europe,
Belgium, Brussels (2016)
6. Lesic, M.: Fotonaponski sistemi u srednjenaponskoj distiributivnoj elektroenergetskoj mrezi.
BSc, Faculty of Electrical Engineering, University of Tuzla, Tuzla, Bosnia and Hezregovina
(2012)
7. Verdenik, A., Kovacic, D., Dobrun, T.: Izgradnja prve solarne elektrane u Bosni i
Hercegovini. In: International
Conference Energa, June 2012, Tuzla, Bosnia and
Herzegovina (2012)
8. Markiewicz, H., Klajn, A.: Standard EN 50160—Voltage Characteristics in Public
Distribution Systems. Wroclaw University of Technology, Wroclaw, Poland (2004)
9. Fluke 434/435 Three Phase Power Quality Analyzer, Users Manual. Fluke Corporation,
Everet, Washington, USA (2006)
10. Ahmedic, J.: Tehnoekonomska analiza pogona fotonaponskih Sistema. MSc, Faculty of
Electrical Engineering, University of Sarajevo, Bosnia and Hezregovina (2016)
11. Lesic, M., Konjic, T., Kurtic, A., Tomljenovic, T.: Uticaj fotonaponskih sistema na kvalitet
električne energije u distributivnoj mreži, Neum. Bosnia and Herzegovina, 11. Savjeto-
vanje BH K Cigre (2013)
Power Quality Field Measurements on Photovoltaic System
391

Different Approaches for Analysis
of Harmonics Impact on the Transformer
Losses and Life Expectancy
Izudin Kapetanović1, Jasna Hivziefendić2, and Majda Tešanović1(&)
1 Faculty of Electrical Engineering, University of Tuzla, Tuzla,
Bosnia and Herzegovina
majda.tesanovic@untz.ba
2 Faculty of Engineering and Information Technologies, International Burch
University, Sarajevo, Bosnia and Herzegovina
jasna.hivziefendic@ibu.edu.ba
Abstract. Transformers represent the largest part of capital investment in
power system. In addition, power transformer outages have a considerable
economic impact on the operation of an electrical network [1]. Application of
nonlinear loads in recent decades, such as power electronic loads, caused higher
levels of harmonic. This kind of load can lead to creating heat, losses and
therefore cause destruction of insulation and aging of transformers. Load and
ambient temperatures are two important factors that inﬂuence the life of insu-
lation in transformers. The estimated load factors and ambient temperatures are
input to the IEC life consumption models to assess the consumed life of insu-
lation. It should be recognized that liquid-ﬁlled transformers may have different
load limitations than dry-type transformers and that the harmonic loading
practices should treat the two transformer types differently when necessary. In
this paper are presented different approaches for determination of losses and
estimation of life expectancy of transformers under harmonic loads. For this
purpose are used computational, analytical and measuring methods. The Finite
Element Method (FEM) is very sophisticated method for design and analysis of
electrical machines. FEM method could be used for estimation of parameters
and performances of machines during design process and before production
phase. If operator decided to change non-linear load of transformer with another
non-linear load with, for example, different THD factor, operator can calculate
losses using presented FEM and evaluate the effects of proposed non-linear load
on transformer losses and life time. Finally, the results obtained by using dif-
ferent methods are presented in this paper.
1
Introduction
Transformer, as an important part of the transmission and distribution system, plays an
important role in the safe operation of power grids [2]. In comparison with the
oil-cooled transformer, dry-type transformer shows a good performance in ﬁreproof
performance, mechanical properties, dielectric strength, anti-short-circuit ability,
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_36

heat resistance etc. Further, the feature of less pollution and convenient installation
make it widely used [3].
Existence of nonlinear loads in recent years, such as power electronic loads, caused
higher levels of harmonic. Increase in harmonic load current cause additional losses
and increases in winding hot spot temperature and stress on insulation, and ﬁnally
reducing the useful life of insulation and transformer capacity [4, 5]. To prevent these
problems, nominal capacity of transformers that will feed the nonlinear load, should be
reduced. Because of this fact, it is a reasonable concern about the increased use of such
equipment and power electronics components, almost in every electrical device.
Impacts of harmonic distorsions in distributive networks could not be immediately
visible.
Distortion of the system voltage THD mainly is well below 5%. The magnitudes of
the voltage harmonics components are small compared to the fundamental component,
rarely exceeding a level of 2–3%. This is determined by the low internal impedance of
most supply systems carrying harmonics. Therefore, neglecting the effect of harmonic
voltage and considering the no load losses caused by the fundamental voltage com-
ponent will only give rise to an insigniﬁcant error. This is conﬁrmed by measurements
[6, 7].
There are three effects that result in increased transformer heating when the load
current includes harmonic components.
1. Rms current: If the transformer is sized only for the kVA requirements of the load,
harmonic currents may result in the transformer rms current being higher than its
capacity;
2. Eddy-current losses: These are induced currents in a transformer caused by the
magnetic ﬂuxes.
3. Core losses: The increase in nonlinear core losses in the presence of harmonics will
be dependent under the effect of the harmonics on the applied voltage and design of
the transformer core [8].
The main purpose of this research effort is to quantify the increased losses due to
harmonics and estimate life expectancy of transformers under harmonic loads with
three different approaches: numerical, analytical and experimental.
Presented data gives the opportunity to the researchers to understand the effect of
the harmonics on transformer loss of life. The obtained data is sufﬁcient for the analysis
of the losses caused by load harmonics.
2
Transformer Losses
Losses are the main issue that faces any device in real life, so it is very important to
detect, quantify and reduce the losses as much as possible.
Transformer’s power losses can be divided into two main components: no-load
losses (hysteresis and eddy current losses) and load losses (ohmic heat losses and
conductor eddy current losses).
Different Approaches for Analysis of Harmonics
393

This can be expressed by equation:
PT ¼ PNL þ PLL
ðWÞ
ð1Þ
where,
PNL
are the no load losses.
PLL
are the load losses.
PT
are the total losses.
There are, however, other two types of losses in the above mentioned working con-
ditions, namely extra losses created by harmonic and unbalanced currents, respectively.
Since the greatest concern about a transformer operating under harmonic load
conditions will be for overheating of the windings, it is convenient to consider loss
density in the windings on a per-unit basis (base current is rated current and base loss
density is the I2R loss density at rated current) [9].
PLL ¼ Pdc þ PEC þ POSL
ðWÞ
ð2Þ
where,
Pdc
the losses due to load current and dc winding resistance.
PEC
the winding eddy losses.
POSL
other stray losses in clamps, tanks, etc.
DC loss is loss due to resistance of windings and increases with square of rms of load
current. The winding dc loss under harmonic condition are shown by equation [10]:
Pdc ¼ Rdc  I2 ¼ Rdc 
X
hmax
h¼1
I2
h;max
ð3Þ
The eddy current losses are generated by the electromagnetic ﬂux. They vary with
the square of the rms current and the square of the frequency (harmonic order h), [10]:
PEC ¼ PECR
X
h¼hmax
h¼1
Ih
IR

2
h2 ðWÞ
ð4Þ
where are
PEC
the winding eddy loss due to non-sinusoidal current
PEC- R
the winding eddy current loss under rated conditions
h
the harmonic order
Ih
the rms current at harmonic order h,
IR
the rms fundamental current under rated frequency and load conditions
The increased eddy current losses produced by a non-sinusoidal load current can
cause excessive winding losses and abnormal temperature rise.
394
I. Kapetanović et al.

The other stray losses are assumed to vary with the square of the rms current and
the harmonic frequency to the power of 0.8 [10]:
POSL ¼ POSLR
X
h¼hmax
h¼1
h0:8
Ih
IR
 2
ð5Þ
where are
POSL
the other stray losses in the structural parts due to nonsinusoidal current
POSL-R
the other stray losses in the structural parts under rated conditions
The factor of 0.8 has been veriﬁed in studies by manufacturers and others, and is
accepted in the standards.
For dry type transformers increased temperatures in structural parts of the trans-
former do not contribute to the raise of the winding hot spot temperature.
For oil type transformers the stray losses increased the oil temperature and the
winding hot spot temperature.
The simple method to calculate stray losses is a function of transformer rated
power. The total stray losses PSL are determined by subtracting Pdc from the load losses
measured during the impedance test, i.e.:
PSL ¼ PEC þ POSL ¼ PLL  Pdc
ðWÞ
ð6Þ
There is no test method to distinguish the winding eddy losses from the stray losses
that occur in structural parts.
Factors known as K-Factor, Total Harmonic Distortion (THD), Harmonic Loss
Factor (FHL) are very important indicators in evaluation of harmonics impact on
transformer losses and life time. When distribution transformer is operating under
harmonic load, the value of load losses is increased, while the no load losses increased
with a little amount. No load losses of a transformer under harmonic loads can increase
its operating cost.
Harmonic loss factor, FHL is a key indicator of the current harmonic impact on the
winding eddy loss and other stray loss. The harmonic loss factor is normalized to either
the fundamental or the rms current.
FHL for winding eddy current is the ratio of the total eddy current losses due to the
harmonics, to the eddy current losses at the power frequency. The FHL-STR is the ratio of
the other stray loss due to the harmonic to the other stray loss at power frequency. The
eddy current loss is increased by a factor of FHL and the other stray loss are increased
by a factor of FHL-STR in the presence of harmonics. The transformer load losses in
non-sinusoidal condition is [11]:
PLL ¼ Pdcrated þ FHLPECrated þ FHLSTRPOSLrated
ð7Þ
This factor is calculated by the following equations [11]:
Different Approaches for Analysis of Harmonics
395

FHL ¼
Phmax
h¼1
Ih
I1
h i2
h2
Phmax
h¼1
Ih
I1
h i2
ð8Þ
FHLSTR ¼
Phmax
h¼1
Ih
I1
h i2
h0:8
Phmax
h¼1
Ih
I1
h i2
ð9Þ
where,
FHL
harmonic factor for eddy current loss,
FHL-STR
harmonic factor for other stray loss
Various measuring devices permit calculations to be made in terms of the har-
monics normalized to the total rms current or to the ﬁrst or fundamental harmonic.
The per unit load losses are given by the expression below [11]:
PLLpu ¼ PLLratedðpuÞ2 
X
hmax
h¼1
Ih
I1
 2
ð10Þ
A portion of the stray loss is taken to be eddy-current loss. For dry-type trans-
formers the winding eddy loss is assumed to be [10]:
PEC ¼ 0:67PSL
POSL ¼ PSL  PEC
ð11Þ
The division of eddy-current loss and other stray losses between the windings is
assumed to be as follows [11]:
(a)
60% in the low voltage winding and 40% in the high voltage winding for all
transformers having a maximum current rating of less than 1000 A (regardless
of turns ratio).
(b)
60% in the low voltage winding and 40% in the high voltage winding for all
transformers having a turns ratio of 4:1 or less.
(c)
70% in the low voltage; winding and 30% in the high voltage winding for all
transformers having a turns ratio greater than 4:1 and also having one or more
windings with a maximum self cooled current rating greater than 1000 A.
The permissible transformers current in non-sinusoidal conditions is expressed as:
ImaxðpuÞ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
PLLratedðpuÞ
1 þ FHL  PECðpuÞ þ FHLSTRPOSLðpuÞ
s
ð12Þ
396
I. Kapetanović et al.

K-Factor calculation depends upon the fundamental current and the harmonic
current components, and it is an indicative value of the harmonic contents in the power
system.
The goal of method, used in Europe, is to estimate by how much a standard
transformer should be de-rated so that the total loss on harmonic load does not exceed
the fundamental design loss. This method is known as ‘factor K’. Factor K is a total
rating factor. The expression for “factor K” is presented bellow:
K ¼
1 þ
e
e þ 1
I1
I
 2X
N
h¼2
nq Ih
I1
 2
 
!
"
#0:5
ð13Þ
where:
e
the eddy current loss at the fundamental frequency divided by the loss due to a dc
current equal to the RMS value of the sinusoidal current, both at reference
temperature
h
harmonic number;
I
rms. of the sinusoidal current including all harmonics;
Ih
magnitude of the n-th harmonic;
I1
magnitude of the fundamental current;
q
an exponential constant that is dependent on the type of winding and frequency.
Typical values are 1.7 for transformers with round or rectangular cross-section
conductors in both windings and 1.5 for those with foil low voltage windings.
Percentage THD can be deﬁned in two different ways, as a percentage of the
fundamental component (%THDF, the IEEE deﬁnition of THD) or as a percentage of
the r m s (%THDR, used by the Canadian Standards Association and the IEC) [12].
The THD of a current waveform is calculated by taking the square root of the addition
of the squares of the harmonic currents, and dividing them by the fundamental current.
ITHD ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
I2
3 þ I2
5 þ I2
7 þ   


q
=I1
ð14Þ
In the formula, I3, I5, I7 etc. are currents of the respective order of harmonics and I1
is current at fundamental frequency.
3
Estimation of Transformer Life Expectancy
For estimation of transformer life reduction, aging rate of its insulation system has to be
considered. About 50% of life reduction of transformer is caused by thermal stress
caused by non-linear load. The term life expectancy of transformer usually refers to the
term aging of its isolation, and not on the service life. The normal life of the trans-
former considered a value of 180 000 h or 20.55 years [13].
Different Approaches for Analysis of Harmonics
397

(a) Oil transformer life expectancy estimation
The reduction of the life of insulation is expressed mathematically through Arhenius
law for aging of insulation [13]. The law is non linear and is represented by Eq. (15):
Life ¼ eðA þ B
TÞ
ð15Þ
where is
A, B
are constants
T
absolute temperature
The hot-spot temperature us the most important factor in life reduction of trans-
former. The hot spot temperature is calculated as [13]:
hHS ¼ hA þ DhHS ¼ hA þ hTO þ hg
ð16Þ
hTO ¼ hTOrated 
PLL þ PNL
PLLrated þ PNLrated

0:8
ð17Þ
hg ¼ hgrated 
PLL
PLLrated

0:8
ð18Þ
where is
hTO
top oil temperature
h
hottest spot conductor rise over top oil temperature
hA
ambient temperature
Life reduction and real life of a transformer can be expressed as follows [14]:
LifeðpuÞ ¼ 9:8  1018 expð 15000
hHS þ 273Þ
ð19Þ
Real life ¼ LifeðpuÞ  normalinsulation life
ð20Þ
(b) Dry-type transformer life expectancy estimation
The estimation of life expectancy for dry-type of transformers can be deﬁned on
the basis of the international standard IEC 60076-12 recommendations as [13]:
L ¼ a  e b
T
ð21Þ
where is
L
is life expectancy
a, b
coefﬁcient determined regarding temperature properties of insulation system
398
I. Kapetanović et al.

Hot-spot temperature of dry type transformers could be calculated by Eq. (21):
hHS ¼ ha þ DhHS
ð22Þ
where is
ha
ambient temperature,
DhHS
temperature gradient for load in Kelvin
For natural cooled transformers DhHS could be represented by equation [13, 14]:
DhHS ¼ DhHS;r I½ 2m
ð23Þ
where is
hHS;r
hot-spot temperature for load 1.0 (relative unit),
m
empirical constant
For forced cooled transformers it is necessary to take into account the temperature
adjustment for changes in resistance depending on temperature, and correction factor
for cooling system.
It should be recognized that liquid-ﬁlled transformers may have different load
limitations than dry-type transformers and that the harmonic loading practices should
treat the two transformer types differently when necessary.
4
Different Approaches for Determination of Harmonic
Impact
(a) 2D electromagnetic-thermal model of transofrmer
Experimental methods, combining data provided by measurements with analytical or
numerical methods, in order to provide efﬁcient models for the accurate representation
of certain transformer characteristics.
Laboratory dry-type transformer data (Table 1) were used for the numerical
determination of the distribution of the electromagnetic and temperature ﬁelds. The
results obtained from the numerical calculation in the next chapter have been compared
to the results obtained from laboratory measurements.
The dry-type transformer data are as follows:
Electromagnetic and temperature ﬁelds are deﬁned by equations [15–17]:
Table 1. Transformer data
Type
DP/0-9896
Nominal power
2,4 kVA
Nominal voltage, HV winding 500 V
Nominal voltage, LV winding 380 V
Frequency
50 Hz
Nominal primary current
3 A
Nominal secondary current
3,6 A
Different Approaches for Analysis of Harmonics
399

rxH ¼ rðTÞE
r lðH; TÞH
½
 ¼ 0
rxE ¼  @ l H;T
ð
ÞH
½

@T
r krT
ð
Þ  qc @T
@t þ qv ¼ 0
ð24Þ
This equation presents partial differential equation of non-stationary heat transfer,
where is:
T
function of temperature distribution in space and time,
c
speciﬁc heat capacity,
q
speciﬁc material density,
k
coefﬁcient of heat conduction,
qv
thermal capacity of eventually heat sources in determined point, and
t
time
All of these functions are functions of space and temperature.
A formulation of the quasi-static ﬁeld in the time domain using vector calculus is:
r 1
l rA


þ r @A
@t ¼ rr/ ¼ Js
ð25Þ
where is
A
magnetic vector potential
Js
current density vector
/
scalar potential
To consider quasi-static ﬁelds for eddy current calculations, a magneto-dynamic
formulation is to be used.
This equation can be simpliﬁed assuming sinusoidal ﬁeld variation at angular
frequency x or using phasor representation.
r 1
l rA


þ jxr A ¼ r V
l ¼ Js
ð26Þ
5
Results of Numerical, Analytical Calculation
and Measurements
(a) Numerical calculation results
Presented model provided information about important thermal data for prognosis,
simulation and analysis of the transformer operation.
400
I. Kapetanović et al.

Sources of electromagnetic and temperature ﬁeld are currents in the coils, Joules
losses which are consequence of current ﬂow through transformer coils. Numerical
calculation of temperature ﬁeld is realized using ﬁnite element method in CAD soft-
ware package. The experimental basis of those researches is the measurements on
dry-type transformer realized with thermo-vision camera. Results of ﬁnite element
method are shown on Fig. 1.
Results of numerical calculation of dry-type transformer are compared with results
obtained by measurements.
Results of magneto-dynamic simulation model of three phase dry type transformer
for symmetric and asymmetric voltages are:
For symmetric voltages calculated core losses are:
Full = 785.661 W
Hysteresis = 620.003 W
Foucault currents = 165.658 W (Eddy currents-also called Foucault currents)
Spectrum of current density in transformer secondary (S12) is shown on Fig. 2.
Fig. 1. Temperature distribution in the coils of dry type transformer during 12 000 s
Fig. 2. Spectrum of current density in transformer secondary (S12)
Different Approaches for Analysis of Harmonics
401

For asymmetric voltages core losses are:
Full = 793.264 W
Hysteresis = 626.004 W
Foucault currents = 167.261 W
On the basis of results obtained by numerical calculation it could be concluded that
core losses for asymmetric voltages are 1.25% greater then core losses for symmetric
voltages, what cause additional heating of transformer.
Results of analytical calculation of K factor for dry type transformer are based on
data obtained by calculation according to the standard BS 7821 Part 4. Results are
shown in the Table 2.
K2 ¼ 1 þ 0:091  11 064:1
ð
Þ
K2 ¼ 1:01
K ¼ 1:005
(b) Experimental measurements results
Duration of numerical calculation is 12 000 s. It was a long enough period of time for
transformer to reach steady state during experimental measurements. The same period
of time is used also for numerical calculation. The heating experiment will be stopped
when the transformer reaches steady state. This condition is reached, when the winding
temperature during the last 2 h does not rise more than 1 °C (Fig. 3).
Table 2. K factor calculation
Harmonic In (rms) In/I1
(In/I1)2
nq
nq  (In/I1)2
1
1
1
1
1
1
3
30.277
30.277 916.696
6.473
5933.77
5
29.376
29.376 862.98
15.426 13312.33
7
28.366
28.366 804.606
27.332 21991.5
9
27.048
27.048 731.594
41.900 30653.79
11
25.450
25.450 647.720
58.934 38172.73
P = 3963.596
P = 110 064.1
Faktor q = 1.7; e = 0.1 ! e/(1 + e) = 0.091
402
I. Kapetanović et al.

Thermivision image of transformer and diagram of winding temperature recorded
by thermovision camera are shown on Fig. 4:
Tcoil;num ¼ 49:69
C
Tcoil:measur ¼ 49
C
Relative error is:
@T% ¼
DT
Tcoil;measur
100% ¼ 1;408%
Relative error is:
@T% ¼
DT
Tcoil;measur
100% ¼ 2;878%
Fig. 3. Experimenal setup and diagram of temperature distribution in the center of the coil of
dry type transformer during 12 000 s
Different Approaches for Analysis of Harmonics
403

The power quality analyzer FLUKE 454 is used to log the harmonic data on the
faculty laboratory. Optical cable for USB and power log software was used to transfer
the data stored in the instrument to the computer. The logger was set to measure
quantities such as frequency, voltage, current, total harmonic distortion, active power,
reactive power, apparent power, voltage harmonic, current harmonic, true power factor
and displacement power factor. Results of measurement realised using instrument
FLUKE 454 are shown on following ﬁgures (Fig. 5).
(c) Analytical calculation results
Analytical calculation performed on the basis of value of current harmonics obtained
by numerical analysis, provided the value of the k factor = 1.005.
Data of the three-phase dry-type transformer needed for the calculation of har-
monics impact on transformer life are:
Fig. 4. Thermovision image and diagram of temperatures recorded by thermovision camera
404
I. Kapetanović et al.

Sn ¼ 2:4 kVA
Up ¼ 500 V
Us ¼ 380 V
Ip ¼ 3 A
Is ¼ 3:6 A
PLL ¼ 19:2 Westimated
PNLL ¼ 2:4 Westimated
Rp ¼ 2:2 X
Rs ¼ 1:5 X
According to [11, 12] calculation of transformer stray losses could be implemented
PI2RR ¼ 0:9069 PLL ¼ 15:24 W
PECR ¼ 0:0931 PLL ¼ 1:564 W
In a case of load represented by three diodes in series with the resistances, on the
transformer secondary, results of measurement obtained by FLUKE 454 showed a
harmonic spectrum, Table 3.
Fig. 5. Voltages, currents, CF, K factor and THD%
Different Approaches for Analysis of Harmonics
405

Calculation of temperatures and parameters necessary for calculation o hot spot
temperature and estimation of transformer life expectancy could be carried out in the
following manner:
ðI2RÞ2R2 ¼ 19:16 W
hg ¼ 10 19:16 þ 2:4  34:78
19:16 þ 2:4  1:564

0:8
¼ 26:94
C
hHS ¼ hA þ hg ¼ 25:5 þ 26:94 ¼ 52:45
C
For ambient temperature equal to 25.5 °C, result of numerical calculation is hHS =
49 °C.
Absolute and relative errors are:
DhHS ¼ 52:45  49 ¼ 3:45
C
dhHS ¼ 19:97%
Presented harmonic spectrum caused temperature rise of 20%. For estimation of
harmonic impact on transformer life it is necessary to know properties of insulation
system. For used laboratory transformer were not known characteristic parameters of
insulation system needed for estimation of transformer life expectancy.
If it is assumed that for this type of transformer winding temperature rise is 65 °C
for ambient temperature of 30 °C, using Eqs. (19–20), for assumed normal insulation
life of 40 years, real life of transformer under conditions represented in Tables 3 and 4
is 21,2 years. Excessive increase in temperature signiﬁcantly affects on the aging of the
insulation system and decrease transformer life
Table 3 Calculation of parameters for obtained current harmonic spectrum
h
(Ih/I1)
(Ih/I1)2
(Ih/I1)2h2
h0.8
(Ih/I1)2 h0.8
1
1
1
1
1
1
3
0.0043 0.0000185 0.000166 2.408 0.0000445
5
0.92
0.846
21.15
3.623 3.065
7
0.03
0.0009
0.044
4.743 0.004
9
0.02
0.0004
0.0324
5.799 0.0023
11 0.01
0.0001
0.0121
6.089 0.00068
1.847
22.24
4.072
406
I. Kapetanović et al.

6
Conclusion
In this paper a practical engineering analysis and methods have been proposed for
calculation of the additional losses under linear and nonlinear load conditions. The
results of laboratory experiments aimed to measure the harmonic distortion and their
effects on the efﬁciency of transformer. It is very clear that transformer’s efﬁciency
decreasing with increasing of harmonic distortion.
There are different technics for determination and monitoring of the transformer
temperature. In general, the methods are divided into direct methods (contact and
contact-less) or indirect methods that are based on calculations, modeling and
simulations.
Presence of harmonics caused rise of losses. Growth of losses caused increase in
temperature. Excessive increase in temperature signiﬁcantly affects on the aging of the
insulation system and decrease transformer life.
The assessment of the remained working life of transformer is based on the
knowledge of hot-spot temperature of dry type transformer’s windings.
Therefore, to prevent these problems, the transformer load capacity should be
reduced., under non-linear load currents.
The economic effects of harmonics are reduced life time of transformer and reduced
energy efﬁciency.
The simulation shows that higher loads caused reduction of transformer life time,
higher THD also caused reduction of transformer life time, due to the increase of
transformer losses and hot spot temperature.
This paper described the harmonic impact on transformer losses, and has intro-
duced a methodology based on FEM model, to predict satisfactorily the harmonic
impact on transformer. The methodology introduced in this paper, if implemented at
the design stage of transformers, may provide great services in reducing the losses.
References
1. Susa, D., Lehtonen, M., Nordman, H.: Dynamic thermal modeling of power transformers.
IEEE Trans. Power Deliv. 20(1), 197–204 (2005)
2. Ning, W., Ding, X.: Three-dimensional ﬁnite element analysis on ﬂuid thermal ﬁeld of
dry-type transformer. In: 2012 Second International Conference on Instrumentation,
Table 4 Calculation of losses for presented current harmonic spectrum
Type of losses Rated losses
(pu = 1)
Correction factor due to harmonics Total adjusted losses
PNLL
2.4
2.4
I2R
15.24
15.24
EC
1.564
22.24
34.78
OSL
0
4.072
Ptotal losses
19.2
52.42
Different Approaches for Analysis of Harmonics
407

Measurement, Computer, Communication and Control (IMCCC), pp. 516, 519, 8–10 Dec
2012
3. Li, Y., Li, L., Jing, Y., Li, S., Zhang, F.: Calculation and analysis of hot-spot
temperature-rise of transformer structure parts based on magnetic-thermal coupling method.
In: Electrical Machines and Systems (ICEMS), pp. 8–11 (2013)
4. Daut, I., Syafruddin, H.S., Ali, R., Samila, M., Haziah, H.: The effects of harmonic
components on transformer losses of sinusoidal source supplying non-linear loads. Am.
J. Appl. Sci. 3(12) (2006)
5. Taci, M.S., Sarul, M.H., Yıldırmaz, G.: The effects of the harmonic components upon
transformer active losses in case of (non)sinusoidal sources and (non)linear loads. In:
Proceedings of IEEE International Conference on Industrial Technology 2000, vols. 1, 2,
pp. 741–746, 19–22 Jan 2000
6. Driesen, J., Belmans, R., Hameyer, K.: Study of eddy currents in transformer windings
caused by non-linear rectiﬁer load. In: Proceedings EPNC 98 Circuits, Sept. 98, Liege,
Belgium, pp. 114–117 (1998)
7. Bishop, M.T., Baranowski, J.F., Heath, D., Benna, S.J.: Evaluating harmonic-induced
transformer heating. IEEE Trans. Power Deliv. 11(1), 305–311 (1996)
8. Sadati, S.B., Tahani, A., Darvishi, B., Dargahi, M., Youseﬁ, H.: Comparison of distribution
transformer losses and capacity under linear and harmonic loads. In: IEEE 2nd International
Conference on Power and Energy, 2008, PECon 2008, 1–3 Dec 2008, pp. 1265–1269 (2008)
9. Girgis, A., Makram, E., Nims, J.: Evaluation of temperature rise of distribution transformer
in the presence of harmonic distortion. Electr. Power Syst. Res. 20(1), 15–22 (1990)
10. IEEE Std C57-110-1998: Recommended Practice for Establishing Transformer Capability
When Supplying Non Sinusoidal Load Currents
11. IEEE Std C57-110-2008: Recommended Practice for Establishing Liquid Filed and Dry
Type Power and Distribution Transformer Capability when Supplying Non Sinusoidal Load
Currents
12. Ramos, V.A., Jr.: Treating harmonics in electrical distribution systems. In: Computer Power
and Consulting (1999)
13. IEEE Standard-Guide for Loading Dry-Type Distribution and Power Transformer, Standard
ANSI/IEEE C57.96–89 (1989)
14. Sadati, S.B., Yazdani-Asrami, M., Taghipour, M.: Effects of harmonic current content and
ambient temperature on load ability and life time of distribution transformers. Int. Rev.
Electr. Eng. 5(4), 1444–1451 (2010)
15. Perez, J.: Fundamental principles of transformer thermal loading and protection. In: 2010
63rd Annual Conference for Protective Relay Engineers (2010)
16. Swift, G.W., Molinski, T.S., Lehn, W.: A fundamental approach to transformer thermal
modelling—Part I Theory and equivalent circuit. IEEE Trans. Power Deliv. 16(2), 171–175
(2001)
17. Swift, G.W., Molinski, T.S., Bray, R., Menzies, R.: A fundamental approach to transformer
thermal modelling—Part II: Field veriﬁcation. IEEE Trans. Power Deliv. 16(2):176–180
(2001)
408
I. Kapetanović et al.

Voltage Sag Propagation Caused by Faults
in Medium Voltage Distribution Network
Tarik Hubana, Elma Begić, and Mirza Šarić(&)
Public Enterprise Elektroprivreda of Bosnia and Herzegovina, Mostar, Bosnia
and Herzegovina
{t.hubana,elma.begic,m.saric}@epbih.ba
Abstract. Underground cables are being increasingly constructed in modern
power distribution networks, however overhead lines are still present, making
the system more exposed to faults. Faults from medium voltage networks
propagate throughout the network and then experienced by low-voltage cus-
tomers as voltage sags. Voltage sags have become increasingly important when
considering the various power quality issues that cause inconvenience to cus-
tomers. For some customers voltage sags cause, especially high costs. In this
paper, different fault types at the medium voltage level and the sag propagated
throughout the network are analyzed. Asymmetrical faults can cause problems
for protection devices since they propagate differently throughout the distribu-
tion network, and distort the phase to ground voltages on the different voltage
levels. Experimental results obtained from the Bosnia and Herzegovina distri-
bution system show the signiﬁcance of appropriate voltage sag propagation
understanding. The power distribution companies should understand the voltage
sags experienced in their networks and develop strategies for decreasing the
inﬂuence of voltage sags. Thus, voltage sags analysis should be an important
part of power distribution planning and an important element in a comprehen-
sive power system analysis.
1
Introduction
The power system is a complex and very capital intensive system which requires
substantial investments in order to maintain predetermined quality standards and meet
future energy and capacity needs. While transmission networks typically consist of
overhead lines, MV networks consist of underground cables in urban areas and
overhead lines in rural areas. The fault frequency of a MV overhead line network can
be remarkably high. This is primarily due to construction and reclosing practices [1]. In
medium voltage distribution networks, voltage sags are mainly caused by power sys-
tem faults. A voltage sag is not a new phenomenon in power systems. Sages have
always been one characteristic of electric power distribution, but just until during the
past decade, they have been considered a signiﬁcant power quality problem. In the past
loads and customers were generally more immune to effects of voltage sags. Nowa-
days, more and more sag sensitive loads are connected to the network and thus sag
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_37

sensitive customers will experience remarkable economic loss. This is a good reason to
pay extra attention on the fault caused sags in distribution system, and to include them
into the planning system.
2
Distribution System
In Bosnia and Herzegovina medium voltage networks are fed from the 110 kV sub-
transmission system. Distribution network companies operate MV (35, 20 and 10 kV)
and LV (0.4 kV) networks which may be built as looped but operated radially. In
Bosnia and Herzegovina MV networks are neutral isolated or earthed via small
resistance. HV (110 kV) and MV networks provide supply to large customers, but the
vast majority of customers are connected to LV networks (Fig. 1).
In rural areas MV and LV networks consist of overhead lines while in urban areas
underground cables are typical. The real economic beneﬁt could most likely be gained
from industry as well as commercial and public service branch customers whose loads
are located in a compact area and could thus be protected with a lumped solution [2].
2.1
Isolated Neutral Point Medium Voltage Networks
Neural points of 10(20) kV networks were not earthed (isolated) at the beginning.
Today’s tendency is to abandon those systems due to their disadvantages. In such
systems, the neutral point is disconnected from the earth. The only connection with the
earth is achieved across ground level capacities of power lines and cables. During the
Fig. 1. Radial network topology in Bosnia and Herzegovina distribution system
410
T. Hubana et al.

earth fault, currents ﬂow through earth capacities of healthy phases (Fig. 2). Neglecting
draining and longitudinal impedances, the current at the fault point depends on the
capacitive current of the network and on fault resistance [3].
The phasor diagram shown in Fig. 2 is valid for so-called metallic earth faults. At
low values of fault resistance, the voltage of the neutral point is approximately the same
as the phase voltage. Advantages of the networks with isolated neutral point [3]:
• during earth fault, which statistically represents the most frequent fault, in the case
of relatively low capacitive current it results in self-extinction of the fault if it is a
transient fault, i.e. the fault line does not energize, and it positively affects the
quality of the electrical power supply
• because of the relatively low fault current, conditions for earthing implementation
of substation 20(10)/0,4 kV are basically not a problem
• simplicity and cost-effectiveness of the performance.
Disadvantages of networks with an isolated neutral point [3]:
• in networks with an isolated neutral point, intermittent overvoltages can occur, with
relatively high overvoltage factors, which can cause double earth faults in different
parts of the network
• internal overvoltages are much greater than in earthed networks
• fault detection is more difﬁcult than in earthed networks
• at higher capacitive currents there is no self-extinction of the currents of the tran-
sient earth faults.
3
Voltage Sags Caused by Medium Voltage Network Faults
According to IEEE standard 1159-1995, a voltage sag is deﬁned as a decrease to
between 0.1 and 0.9 p.u. in root mean square (rms) voltage at the power frequency for
durations of 0.5 cycles to 1 min [4]. Voltage sags have always been present in power
Fig. 2. Earth fault in the system with unearthed neutral point and phasor diagram
Voltage Sag Propagation Caused by Faults
411

systems, but only during the past decades customers have become more aware of the
inconvenience caused by them.
A power system fault is a typical cause of a voltage sags. Faults occur in trans-
mission (EHV), subtransmission (HV), medium-voltage (MV), and low-voltage
(LV) systems, and the sags propagate throughout the power system. The sag distri-
bution experienced by a low-voltage customer includes all these sags of different origin
[1]. The characteristics of voltage sags caused by faults originated within the distri-
bution network depend on several factors:
• fault characteristic (type, duration, resistance);
• distribution transformer connections;
• substation grounding;
• rating of DG units;
• design of the different protection systems;
• operating conditions and DG penetration level;
• fault and monitor locations [4].
Voltage sags can generally be characterized by sag magnitude, duration, and fre-
quency. Network impedances determine the sag magnitude. When considering sags
caused by faults, the protection practices specify the sag duration, and the fault fre-
quencies determine the number of voltage sags.
Most of the faults affecting LV customers occur in MV networks. A LV customer
experience sags caused by faults in the neighboring MV feeders and also via the HV
systems, from faults in the MV networks located behind the neighboring substations
[1]. Sags are most often caused by:
• Faults in Neighboring MV Feeders: MV networks are operated radially. The most
serious sags caused by MV faults are those in the neighboring feeders of the sag
sensitive customer.
• Faults behind Neighboring HV/MV Transformers: In the case of a fault behind a
neighboring HV/MV transformer, the sagged voltage will not collapse to a high
degree because the transformer impedance is now on the load side of the PCC.
Typically, sags caused by LV faults are not taken into account in a sag distribution
because LV faults are rare, one distribution transformer supplies only a small number of
customers and, thus, these sags have only a minor and very local impact on the overall
sag distribution, and fuses limit the fault current and thus also the voltage drop [1].
3.1
Voltage Sags Magnitude
In meshed transmission and subtransmission systems, the calculation of voltages is
based on Thevenin’s theorem and the network impedance matrix [5]. In radially
operated networks, the calculation can be simpliﬁed. The voltage sag at the substation
busbar can be calculated using the impedance divider principle [6]. The sagged voltage
of the substation busbar Usag is seen in the whole substation area supplied by this
busbar. Thus, the substation busbar represents the point of common coupling (PCC) for
the faults in the network in question and experienced by the customers downstream
(Fig. 3).
412
T. Hubana et al.

Usag ¼
ZL þ ZF
ZS þ ZT þ ZL þ ZF
US
ð1Þ
where US is the voltage before the fault. In three-phase short circuits, all three
phase-to-ground and phase-to-phase voltages sag to the same degree. In asymmetrical
faults, depending on the fault type, one, two or three phase-to-ground and
phase-to-phase voltages are sagged, raised or remain unchanged [7].
3.2
Voltage Sag Propagation
Although most customers are connected to LV networks, faults occur at all voltage
levels. The fault type, earthing practices, and transformer connections determine which
voltages are of interest when considering sags at the LV customer location. Sags caused
by symmetrical three-phase faults propagate without changes through transformers. In
the case of unsymmetrical faults, however, the transformer connections have a strong
effect. Again, sags caused by symmetrical three phase short circuit transfer from one
voltage level to another without changes, but for asymmetrical sags the voltages are
propagated through transformers according to the equations [1].
Usec ¼ PAP1UprI
ð2Þ
P ¼
1
1
1
1
a2
a
1
a
a2
2
4
3
5
ð3Þ
a ¼ 0:5 þ i
ﬃﬃﬃ
3
p
=2
ð4Þ
A ¼
Að1; 1Þ
0
0
0
1\a
0
0
0
1\  a
2
4
3
5
ð5Þ
Fig. 3. A circuit model for the voltage sag calculation in a radially operated MV network
Voltage Sag Propagation Caused by Faults
413

In (2), matrix P−1 transforms the phase-to-ground voltages to symmetrical compo-
nents, while matrix P does the opposite. The matrix A determines the transformer type.
The element A(1,1) depends on how the zero sequence component propagates through the
transformer. If the zero sequence current cannot penetrate both windings then A(1,1) is set
to zero. In a YNyn transformer with both neutrals earthed, A(1,1) = 1. The angle is
determined by the change in the positive sequence voltage. For example, for an Ynd11
type transformer, a = 30° [5].
3.2.1
Voltage Sag Propagation from MV to LV Systems
In distribution systems, Dyn11 is a common connection of a MV/LV transformer (for
example in Finland) [1]. This means that in (2) the element is zero and the angle. Further,
the phase-to-phase voltage on the MV voltage side is seen as a phase voltage on the LV
side. In the case of a symmetrical three-phase fault on the MV side, all three
phase-to-phase and phase voltages will collapse to the same degree and propagate
without changes to the LV side. In the example presented in Fig. 4a, a three-phase short
circuit cause a remaining voltage of 50%. In the case of a two-phase short circuit of fault
resistance, two of the phase voltages sag to a voltage of while the other remains
unchanged (Fig. 4b). Further, only one phase-to-phase voltage collapses to zero while
the other two phase-to-phase voltages decrease to 87% of the nominal voltage (Fig. 4c).
Thus, during two-phase short circuits on the MV side, only one phase voltage on the LV
side is substantially disturbed. When an earth fault occurs in a high impedance earthed
medium voltage distribution network, the neutral point voltage rises and the phase
voltages of the sound phases increase. If the fault resistance is 0 X, the phase voltages of
the sound phases reach the value of the phase-to-phase voltages. Because of the shift in
the neutral point, there is no change in the phase-to-phase voltages on the MV side.
Further, there will be no collapsed phase voltages on the LV side (Fig. 4d) [1].
4
Results and Discussion
In this paper a distribution system of the city of Mostar, Bosnia and Herzegovina, is
modeled in MATLAB Simulink simulation software. The system is fed from the
110 kV transmission network, over 35 and 10 kV distribution medium voltages, while
Fig. 4. Sagged p.u. voltages of a a three-phase short circuit, b a two-phase short circuit (phase
voltages), c a two-phase short circuit phase-to-phase voltages), and d an earth fault (phase
voltages) in an MV network with an unearthed neutral [1]
414
T. Hubana et al.

consumers are connected to the 0.4 kV low voltage network. 35 kV distribution net-
work mainly consists of overhead lines, while 10 kV network is in an urban area and
therefore made of underground cables. Simulation model shown in Fig. 5 is a three
phase model of the part of the Mostar distribution network.
The fault is simulated on the 35 kV overhead line, that feeds the entire consumption
area, and measurements are performed on the 35 kV overhead line, 10 kV busbar of
the 35/10 kV transformer and on the 0.4 kV busbar of the remote area 10/0.4 kV
distribution transformer. Single line short circuit fault (single line to ground—LG),
double line short circuit fault (line to line to ground—LLG), three line short circuit fault
(line to line to line to ground—LLLG) and single phase brake is simulated, and
measurements performed on different voltage levels are shown on Figs. 6, 7, 8 and 9.
Fig. 5. Simulation model of the analyzed distribution network
Fig. 6. Single line short circuit fault (LG) propagation trough distribution network
Voltage Sag Propagation Caused by Faults
415

As previously mentioned, in Eqs. 2, 3, 4, 5 transformer connection strongly affects
fault and voltage sag propagation throughout the grid. The analyzed system 35/10 kV
transformers have Yd5 connection type and isolated neutral point. 10/0.4 kV transformers
have Dy5 connection type and directly earthed neutral point. Neutral point treatment also
strongly affects thefault propagation. Figure 6showsa phase toground voltages onthe35,
10 and 0.4 kV voltage levels. In transmission line faults, roughly 65–70% are asymmetric
line-to-ground faults [8]. The single phase short circuit is simulated on the 35 kV overhead
line. In this scenario, phase A voltage at the fault point drops to 0, while phase B and C
voltages signiﬁcantly rise by a factor of approximately 1.7. Because of the transformer
connection, on the 10 kV busbar of the 35/10 kV transformer, phase voltages look
completelynormalwithaslight voltagesagofphaseC(0.943p.u.) andphaseA (0.97p.u.),
what lies within the permitted voltage boundaries. Similar situation happens on the 0.4 kV
busbar of the 10/0.4 kV transformer. Earthing and transformer connection combination
reacted very well in this fault scenario, without further fault propagation from the 35 kV
voltage level, but this can be a serious problem for the fault protection devices, where good
voltage values in 10 and 0.4 kV voltage levels can show the fake system status.
In case of two phase short circuit at the 35 kV overhead line, faulty phases (A and
B) voltages drop to zero, while the unaffected phase C voltage rises to approximately
1.5 p.u. It is interesting to see that at the 10 kV busbar of the 35/10 kV transformer,
phase to ground voltages of Phase B and Phase C have a 0.84 p.u. voltage with
opposite phase angle. At the 0.4 kV busbar of the 10/0.4 kV transformer, phase B
voltage has a value of 0.922 p.u., while the phases A and C have the same phase shift
with the value of 0.47 p.u. Voltage scenarios like this can lead to misconceptions,
Fig. 7. Double line short circuit fault (LLG) propagation trough distribution network
416
T. Hubana et al.

because the voltages measured at the 0.4 kV side can remind to the two phase short
circuit with resistance between phases A and C.
LLLG fault (three phase short circuit) is a symmetrical fault (as shown in Fig. 8),
and therefore propagated symmetrically throughout the network, as previously
explained. Because of that, LLLG faults are not this paper point of interest.
Fig. 8. Three line short circuit fault (LLLG) propagation trough distribution network
Fig. 9. Single line brake fault propagation trough distribution network
Voltage Sag Propagation Caused by Faults
417

Single line brake represents a serious fault in the system, since highly harmonic
voltage transients appear at the fault point. Figure 9 shows a Phase A interruption, at
the 35 kV overhead line, and this fault propagation throughout the network. The fault is
asymmetrical, and hence Phase B and C voltages induce voltage on the interrupted
Phase A. At the 10 kV busbar of the 35/10 kV transformer voltage transients are
dumped because of the large transformer inductivity. Phase shifting is again present, so
in this case the phase B is completely healthy (0.97 p.u.), while phases A and C have
voltage sag (0.49 p.u.). At the 0.4 kV busbar of the 10/0.4 kV transformer, phase A
and B voltage sag is present (0.8 p.u.), while phase C voltage drops to zero.
From previous ﬁgures it can be seen that transformer connection and neutral point
treatment strongly affect the fault and voltage sag propagated throughout the network.
Faulty phase voltages are alternating as passing through the different transformers. This
simulation shows only faults to ground in order to better demonstrate the fault
advancement trough network. Faults that occur over resistance are more damped and
therefore more demanding to detect. Damped faults that propagate as previously
mentioned faults can be a real problem for protection devices, so they need to be treated
with more attention.
5
Conclusion
Distribution system characteristics strongly affect sag propagation. It is shown that sag
distributions are highly dependent on network topology and transformer connections.
The sags experienced by an LV customers are caused by faults at all voltage levels. In
urban areas, overhead transmission line faults are an important cause of sags, since
underground cable faults in MV networks are rare.
Analyzed system presents an urban distribution system fed over 35 kV overhead
line, while 10 and 0.4 kV distribution networks are made of underground cables, and
hence more reliable. Because this system has a long MV overhead line feeder in use,
MV faults represent the main cause of sags.
This paper analyzed the sag propagation throughout a real distribution system.
Results showed that phase to ground voltages change their amplitude during fault
conditions, as passing through different connection distribution transformers. In some
cases (LG fault) this can be an advantage since the system efﬁciently damps the voltage
sag, but in other asymmetrical faults (LLG for example) phase to ground voltages
change their amplitude as changing the voltage levels. This can lead to protection
system confusion and endanger the whole system.
The results presented in this paper are relevant for the case of a typical distribution
system conﬁguration in Bosnia and Herzegovina, where the transformer neutral points
are resistance earthed, isolated and directly grounded in 35, 10, and 0.4 kV voltage
levels, respectively. Results further demonstrate that neutral point earthling conﬁgu-
ration across the system has a signiﬁcant inﬂuence at the voltage sag propagation.
There is still no unique approach to the neutral point earthing of medium voltage
distribution networks, considering that each method has its advantages and disadvan-
tages. From a voltage sag point of view, the meshed structure of a power system is a
disadvantage and a radial system would be preferred. An efﬁcient way to limit the sag
418
T. Hubana et al.

propagation is to split buses and substations in the supply path to limit the number and
length of feeders in the affected area, while sag-sensitive customers can be supplied by
their own transformers.
The voltage sag propagation and the origin of the majority of fault caused voltage
sags presented in this paper is valuable when planning fault and voltage sag mitigation
measures in different parts of the power system. Sag propagation analysis should be an
important part of system protection planning.
References
1. Heine, P., Lehtonen, M.: Voltage sag distributions caused by power systems faults. IEEE
Trans. Power Syst. 18(4), 1367–1373 (2003)
2. Heine, P., Pohjanheimo, P., Lehtonen, M., Lakervi, E.: Estimating the Annual Frequency and
Cost of Voltage Sags for Customers of ﬁve Finnish Distribution Companies. Helsinki
University of Technology, Helsinki, Finland (2000)
3. Ćućić, R., Komen, V., Živić Đurović, M.: Neutral point concept in distribution networks.
Eng. Rev. 28(2), 77–89 (2008)
4. IEEE: IEEE Recommended Practice for Monitoring Electric Power Quality. IEEE Std.,
pp. 1159–1995 (1995)
5. Martinez-Velasco, J.A., Martin-Arnedo, J.: EMTP model for analysis of distributed
generation impact on voltage sags. In: IPST—International Conference on Power System
Transients, Lyon (2007)
6. Bollen, M.H.J.: Fast assessment methods for voltage sags in distribution networks. IEEE
Trans. Ind. Appl. 32(6), 1414–1423 (1996)
7. Heine, P.: Voltage Sags in Power Distribution Networks. Helsinki University of Technology,
Helsinki (2005)
8. Agarwal, T.: elprocus.com [Online]. https://www.elprocus.com/what-are-the-different-types-
of-faults-in-electrical-power-systems/. Accessed 21 Feb 2017
Voltage Sag Propagation Caused by Faults
419

Architecture of the Modern Power Quality
Monitoring Systems with Installation
and Experiences Examples from BiH,
Croatia and Slovenia
Ivan Vlahović(&) and Ivo Novaković
Tectra d.o.o., Zagreb, Croatia
vlahovic@tectra.hr
Abstract. The quality of supply and reliability of the grid are main investments
reason in electrical grid. To keep the track of the power quality it’s necessary to
develop power quality monitoring system. Development of power quality sys-
tem requires deep analysis of customer grid to properly select points of instal-
lation and project with clear goals and requirements for monitoring system.
Implementation of the power quality system depends on the type of the electrical
grid and existing communication infrastructure. The new system need to be
compatible with all up to date technologies and provide data protection and
access control. Beside the basic power quality indices, it needs to offer inte-
gration with other utilities systems like SCADA and fault location systems to
improve response and benchmarking of the system.
1
Introduction
Modern power quality (PQ) systems need to give PQ indices of the electrical grid.
Main parts of the power quality system are measurement instruments (portable and
permanent), acquisition system, data storage and data representation.
Implementation of the PQ system depends on the type of the electrical grid. Typical
electrical grid consists of the transmission network, distribution network, power pro-
duction and power consumption (Fig. 1).
PQ systems installed by the distribution system operator (DSO) and transmission
system operator (TSO) differ in number of measuring places and concept of
installation.
The TSO usually control high voltage lines and substations with voltage levels
down to the 110 kV depending on the country and grid rules. The schematic of TSO
grid is shown on the Fig. 2.
The TSO’s PQ system is usually smaller than DSO’s PQ system because TSO need
to cover smaller number of points as their system is determined by the number of high
voltage (HV) substations and users connected directly to the HV network.
The DSO usually operates a network with the voltage levels up to 110 kV and need
to manage the low voltage grid with signiﬁcant number of small 10–20/0.4 kV
substations as shown in Fig. 3. The DSO should cover all common coupling points to
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_38

transmission
system,
all
medium
voltage
(MV)
substations
(voltage
levels >
10/20 kV), big distributed power sources (depending on the power and impact to the
grid) and statistically enough amount (more than 10%) of common coupling points at
low voltage with PQ monitors to get good picture of power quality status in their
network. Beside permanent monitoring system the DSO should use portable PQ
monitors to make campaign measurements and check the PQ in all parts of the network.
Fig. 1. Typical electrical grid
Architecture of the Modern Power Quality Monitoring
421

Fig. 2. Typical schematic of transmission grid
Fig. 3. Typical schematic of distribution grid
422
I. Vlahović and I. Novaković

2
Communication Infrastructure
Development of the modern PQ system depends on the communication infrastructure
in the grid. Transmission grid usually has communication network to all substations
and it’s easy to integrate the PQ monitors as the communication infrastructure is
already present.
Distribution grid is implied with the smart grid projects and utilities develop
modern communication infrastructure, but majority of low voltage and medium voltage
substations still doesn’t have adequate communication infrastructure. To avoid this
issue, it’s usual to use publicly available infrastructure, a GSM or 4G networks and
private radio networks. On the Fig. 4 is a model of communication network in utilities
with protection, electricity meters and PQ monitors.
3
Architecture of Power Quality Monitoring System
Modern power quality monitoring system needs to be compatible with “smart grid” and
have open and scalable structure which can support modern communication protocols
(HTTP/HTTPS, FTP/FTPS, IEC 61850, IEC104). The PQ system should work with
modern data interchange format (PQDIF and COMTRADE) and have scalability to
improve response and data availability. Beside the functional component it needs to
provide additional security protection to protect and secure data from unattended
access. The PQ system beside the main purpose needs to offer integration with other
utilities systems like SCADA and fault location systems to improve response and
Fig. 4. Utilities communication model
Architecture of the Modern Power Quality Monitoring
423

benchmarking of the grid (SAIDI, SAIFI). On the Figs. 5 and 6 are examples of
modern PQ system, PQview 3 and PQView 4 by Electrotek [1].
Fig. 5. Example of PQ monitoring system—PQview 4
Fig. 6. PQ system installed at ConEdision New York
424
I. Vlahović and I. Novaković

4
Instruments Used in the Power Quality Systems
Instruments used in modern power quality systems needs to be compliant with valid
regulations and standards. In the Europe, all PQ systems need to be compliant with
requests from the latest editions of EN50160 and IEC61000-4-30 norm.
PQube3 [2], example conﬁguration shown on Fig. 7, is example of the instrument
compliant with current requests of the IEC 61000-4-30:2015 and it records data
compliant with the requests of the EN50160:2010.
To be sure that instrument is compliant with the IEC 61000-4-30:2015 it needs to
have certiﬁcate of conformity with the IEC 61000-4-30:2015 and test report compliant
with the IEC 62586-1:2013 and IEC 62586-2:2013. If there is any doubts about
instrument compliance, the working groups, TC 77/SC 77A/WG 9 which works on the
IEC 61000-30 and TC 85/WG 20 which works on the IEC 62856, can give verdict
about instrument status.
5
Examples of Power Quality Monitoring System
Installations in BIH, Croatia and Slovenia
The PQ systems installed or used during pilot projects in Bosnia and Herzegovina,
Croatia and Slovenia compliant to the requests of modern PQ architecture:
• TSO Croatia
– HOPS PrP ZAGREB
– HOPS PrP OSIJEK
– HOPS PrP SPLIT
– HOPS PrP RIJEKA
Fig. 7. PQube3—example of conﬁguration
Architecture of the Modern Power Quality Monitoring
425

• DSO CROATIA
– HEP ODS ELEKTRODALMACIJA SPLIT
– HEP ODS ELEKTROSLAVONIJA OSIJEK
– HEP ODS ELEKTRA KARLOVAC
– HEP ODS ELEKTRA ČAKOVEC
• TSO SLOVENIA
– ELES
• DSO SLOVENIA
– ELEKTRO LJUBLJANA
– ELEKTRO CELJE
• INDUSTRIAL INSTALLATIONS
– ZABA (bank)
– REVOZ (car manufacturer)
• POWER GENERATION
– Dravske elektrarne Maribor d.o.o.
• Pilot project Bihac 2010
– Pilot project—power quality monitoring Bihać JP EP BIH ELEKTRODIS-
TRIBUCIJA BIHAĆ June–July 2010
• Pilot project Zenica 2011
– Pilot project—Testing of instruments and system for real time power quality
monitoring—ED
Zenica,
Location
TS
35/10
Nemila.
Client:
JP ELEKTROPRIVREDA BIH d.d.—Sarajevo
5.1
Pilot Project Bihac 2010
Pilot project in Bihac was done from the May 2010 to the July 2010 [3]. The three
Dranetz Encore 61000 SG monitors [4] were installed at JP EP BIH substations, Fig. 8.
The Encore series software was installed at ED Bihac headquarter, the communication
was done through the JP EP BIH and Elektroprenos BIH WAN network, Fig. 9.
426
I. Vlahović and I. Novaković

Fig. 9. Communication conﬁguration during Pilot project Bihac 2010
Fig. 8. Instrument and network topology for Pilot project Bihac 2010
Architecture of the Modern Power Quality Monitoring
427

5.2
Pilot Project Zenica—2011
The pilot PQ project in Zenica was result of the request from the JP EP BIH for
presentation of the Dranetz Encore PQ system with the PQView 3 software support.
The project was done from the September 2011 to the November 2011 [5]. The
instruments were installed at the Transformer station 35/10 kV Nemila, Fig. 10. Encore
series software and PQView 3 were installed at ED Zenica. Communication was done
through the JP EP BIH WAN network Fig. 11.
Pilot project show the advantages of continuous power quality monitoring systems
and proved that Dranetz Encore system with PQview can support all customer
requirements.
5.3
Power Quality System Installed at Croatian and Slovenian
Transmission System
Typical high voltage substation is shown on Fig. 12.
Fig. 10. Instrument and network topology for Pilot project Zenica 2011
Fig. 11. Communication conﬁguration during Pilot project Zenica 2011
Transmission system operators in Croatia (HOPS) and Slovenia covered almost all
(more than 90%) of their measuring places with PQ monitors [1].
TSO in Croatia have PQ system with class A monitors from two vendors, Schneider
and Dranetz with the PQview 3 as data archiving and analysis software, the architecture
of the system is shown on Fig. 13.
428
I. Vlahović and I. Novaković

Fig. 12. Typical transmission high voltage substation
Fig. 13. Architecture of power quality system at HOPS
Architecture of the Modern Power Quality Monitoring
429

The TSO in Croatia incorporates data from the SCADA and electricity metering
database together with the PQ data to get bigger picture of network condition during
analysis (Fig. 14).
The ELES covered complete system with the Dranetz Encore 61 series and with the
PQview 3 software (Fig. 15).
Fig. 14. Croatian transmission system
430
I. Vlahović and I. Novaković

5.4
Power Quality System Installed at HEP
ODS ELEKTRSOLAVONIJA OSIJEK
DSO in Osijek built the power quality system to cover renewable power sources
connected to the medium voltage grid (10 and 20 kV) and renewable power sources
with power higher than 100 kVA connected to the low voltage grid. The PQ system
consist of PQube Classic and PQube3 power quality monitors with the QubeView
software. On Fig. 16 is installation example (Fig. 17).
Fig. 15. Slovenian transmission system
Fig. 16. PQube3 installation example
Architecture of the Modern Power Quality Monitoring
431

6
Conclusions
Proposed approach is just brief introduction. Development of power quality system
requires deep analysis of customer grid to properly select points of the PQ measure-
ment and project with clear goals and requirements for monitoring system. In this
article, we showed four examples of PQ systems which proved the concept, permanent
installations in Croatia and Slovenia and pilot project showcases in Bosnia.
References
1. PQview User Group Meeting—presentation and materials from www.pqview.com
2. PSL PQube3 user guide
3. Report from the project Bihac (2010)
4. Dranetz Encore user guide
5. Report from the project Zenica (2011)
Fig. 17. Architecture of QubeView software
432
I. Vlahović and I. Novaković

Impact of Distributed Generation on Power
Quality in Medium Voltage Distribution
Networks
Alija Jusić1(&), Edin Jareb1, and Zijad Bajramović2
1 Public Enterprise Elektroprivreda of Bosnia and Herzegovina, Travnik, Bosnia
and Herzegovina
{al.jusic,e.jareb}@epbih.ba
2 Public Enterprise Elektroprivreda of Bosnia and Herzegovina, Sarajevo,
Bosnia and Herzegovina
z.bajramovic@epbih.ba
Abstract. This paper provides an analysis of the impact of mini hydropower
plant “Kordići” on the 20 kV distribution network of Bugojno area in terms of
power quality. Therefore, measurement and evaluation of the quality of the
supplied power was carried out at the point of common coupling between the
mini hydropower plant and the distribution network. Measurement was carried
out for the period from March 7 to March 18, 2016 and the data were evaluated
in accordance with EN 50160 standard.
1
Introduction
Electrical power supply has become a basic necessity in modern industrial society.
From the consumers’ point of view, distribution company needs to provide a very high
level of reliability of supply to every customer with increasing performance in terms of
continuity. Beside reliability and continuity, the quality of electrical energy has become
a strategic issue for distribution companies. This is so important that some companies
have departments to deal speciﬁcally with this matter (e.g. ELES in the Republic
of Slovenia). In 1985, the Commission of the European Communities states (directive
85/374) that electricity is to be considered a product.
At the deregulated energy market, where electricity is to be considered a product, in
search for competitiveness it is essential to provide not only reliable power supply but
also high power quality [1, 2].
Considering the fact that the future of Bosnia and Herzegovina’s energy market is
liberalization, it is advisable to take necessary steps as soon as possible. Each product
in a competitive market must meet certain criteria if we want the customers to buy it
and be satisﬁed.
Let us deﬁne power quality as a property of electrical energy at a certain point the
power system compared to the speciﬁed values. Reference values of technical
parameters are determined based on several years of experience gained from analyzes
of the electric power system and through agreements at the international level.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_39

Some of international organizations, which deﬁne the value of technical parameters
are: IEC (eng. International Electrotechnical Commission), CENELEC (eng. European
Committee for Electrotechnical Standardisation), IEEE (eng. Institute of Electrical and
Electronics Engineers), UNIPEDE (eng. International Union of Producers and
Distributors of Electrical Energy), CIGRE (fren. Conseil International des Grands
Réseaux Electriques—or eng. International Council On Large Electric Systems), ANSI
(eng. American National Standards Institute), etc. These organizations provide stan-
dards that deﬁne the reference (nominal) value of technical parameters, as well as the
tolerance of these values. If a particular parameter varies within the recommended
limits, it should not cause problems in the power system, i.e. interference or error in
functioning of end user’s equipment.
It is important to note that the international standards related to power quality are
constantly improving, and that the measurement and continuous analyse of power
quality in power systems are carried out around the world [1].
2
Power Quality—Characterization of Disturbances
The importance of certain characteristics of power quality changed with economic
development. At the beginning, major concern was availability of power supply and
that the voltage and frequency, if deviate from nominal values are within recommended
limits. At the time, voltage dips and similar disturbances did not have a major impact
on the concept of power quality.
With more advanced economic development, continuity of power supply, the
voltage value and frequency remain important parameters but emphasis shifts to the set
of characteristics nowadays known as a power quality [1, 3].
Power quality is the combination of current quality and voltage quality, involving
the interaction between the system and the load. Voltage quality concerns the deviation
of the voltage waveform from the ideal sinusoidal voltage of constant magnitude and
constant frequency. Current quality is a complementary term and it concerns the
deviation of the current waveform from the ideal sinusoidal current of constant mag-
nitude and constant frequency [4].
Measurement of power quality usually involves measuring:
• power-frequency variations
• voltage variations
• voltage ﬂuctuations
• rapid voltage changes (ﬂicker)
• voltage dips
• short interruptions
• long interruptions
• transients
• overvoltages
• voltage unbalance
• voltage swell
434
A. Jusić et al.

• harmonics
• interharmonics
• signalling voltages
Power frequency variations are deﬁned as the deviation of the power system fun-
damental frequency from it speciﬁed nominal value (e.g., 50 or 60 Hz).
Voltage variations are variations in the rms value or the peak value with an
amplitude of less than 10% of the nominal voltage.
Voltage ﬂuctuations are a series of voltage changes or cyclical or random variations
in the voltage envelope which are characterised by the frequency of variation and the
magnitude.
Flicker is a measure of the ﬂuctuations (repeated variations) in voltage. Flicker
makes light bulbs ﬂash or pulsate. These ﬂuctuations arise by frequent connection and
disconnection of loads, often in combination with a weak grid.
A voltage dip is a decrease of the normal voltage level between 10 and 90% of the
nominal rms voltage at the power frequency, for durations of 0,5 cycle to 1 min.
Interruptions are a special type of voltage dip to a few percentage of Uref (typically
within the range 1–10%). They are characterised by one parameter only: the duration.
Short interruptions last less than 1 min (extended to 3 min depending on network
operating conditions) and often result from tripping and automatic reclosure of a circuit
breaker designed to avoid long interruptions which have longer duration. Short and
long interruptions differ in both their origins and the solutions required to prevent or
reduce their occurrence.
Voltage disturbances lasting less than a halfcycle T (DT < T/2) are regarded as
transient.
Where voltage is applied to a device and the peak value exceeds the limits deﬁned
in a standard or speciﬁcation, this is an overvoltage.
Temporary overvoltages occur at power frequency.
Switching overvoltages are produced by rapid modiﬁcations in the network
structure (opening of protective devices, etc.).
Lightning is a natural phenomenon occurring during storms. A distinction is made
between direct lightning strike (on a line or structure) and the indirect effects of
lightning (induced overvoltages and increase in earth potential).
A three-phase system is unbalanced if the rms value of the phase voltages or the
phase angles between consecutive phases are not equal.
Voltage swell is momentary increase of the voltage, at the power frequency, outside
the normal tolerances, with duration of more than one cycle and typically less than a
few seconds.
Harmonics are voltage or current waveforms assume non-sinusoidal shape. The
waveform corresponds to the sum of different sine-waves with different magnitude and
phase, having frequencies that are multiples of power-system frequency.
Interharmonics are sinusoid components with frequencies which are not integer
multiples of the fundamental component (they are located between harmonics). The
remote control frequencies used by the power distributor are also interharmonics [2, 5, 6].
Impact of Distributed Generation on Power Quality
435

3
Impacts of Mini Hydropower Plant (MHP) on Power
Quality
This paper provides an analysis of the impact of mini hydropower plant on power
quality in a 20 kV distribution network. For this purpose, measurements of power
quality are carried out in accordance with EN 50160 for the period from March 7 to
March 18, 2016 and covered the period before the ﬁrst synchronization and mea-
surement during disconnection or reconnection of mini hydropower plant to the dis-
tribution network.
3.1
Basic Data on 20 kV Distribution Grid
Bugojno area is fed by a step-down substation TS 110/20/10 kV “Bugojno” with rated
power of 20/14/14 MVA. Area has 165 distribution substations, of which one-third
operates at 10 kV level while the remaining two-thirds operates at 20 kV level. The
part of the distribution system that operates at 20 kV is radial overhead system. MHP
“Kordići” is connected to overhead line DV 20 kV Gračanica. Total length of this
overhead line is 47 km. The feeder length is approximately 22,5 km and branch lines
with total length of approximately 24,5 km. DV 20 kV Gračanica supplies 34 distri-
bution substations with total rated power of 5360 kVA. In addition to newly connected
MHP “Kordici“, there is a mini hydropower plant “Vileška” already connected to one
of branch lines of DV 20 kV Gračanica with allowed power output of 410 kVA
(Fig. 1) [1].
3.2
Basic Data on Mini Hydropower Plant “Kordići”
Mini hydropower plant “Kordići” is equipped with two synchronous generators rated
power of 406 kVA each and one 20/0,4 kV 1000 kVA step-up transformer.
The medium voltage switchgear is 24 kV air-insulated, metal-clad with single
busbar and equipped with switching devices. Switchgear is placed in a special section
of powerhouse and consists of two parts. One section of the switchgear is the
responsibility of Public Enterprise Elektroprivreda of Bosnia and Herzegovina with
conﬁguration: ring-main, metering panel and bus-riser. Other section of the switchgear
is responsibility of investor and consists of bus-riser and transformer panel with a
protective relay. Also, switchgear is equipped with devices for accurate measurements
and protection (Fig. 2) [1].
3.3
Instrument Connection and Recording Procedure
In this section each step of measurement and recording procedure will be described in
details. Measurements were made by a portable three phase power quality analyser
METREL Power Master MI 2892, which is in Class A according to IEC 61000-4-30
(Fig. 3).
436
A. Jusić et al.

Measurement procedure with installed equipment is given below:
1. Preparing the instrument for measurement (time and date setting, releasing storage
for new measurement)
2. Measurement setup according to the nominal voltage, current etc.
3. Connection of instrument (see Table 1 and Fig. 4) and setting the recording time
interval of 10 min
Fig. 1. Single line diagram of overhead line DV 20 kV Gračanica
Impact of Distributed Generation on Power Quality
437

Fig. 2. Single line diagram of mini hydropower plant “Kordići”
Fig. 3. METREL Power Master MI 2892
438
A. Jusić et al.

4. Online measurements of power quality for eleven days (from March 7 to March 18,
2016)
5. Stop recording, disconnection of the instrument and data analysis.
3.4
Power Quality Measurement Results and Discussion
In this part of the paper will be presented the results of measurements of power quality
carried out in accordance with EN 50160 for the period from March 7 to March 18,
2016.
During the measurement period several events occurred in the 20 kV distribution
system as shown in Table 2.
Table 1. Installed equipment
No
Analyser
Measurement point
Voltage
Current
1.
METREL
Power
Master MI 2892
Mini hydropower plant
“Kordići”, medium voltage
switchgear
Metering
panel, 20 kV
busbar
DV 20 kV
Gračanica
feeder
Fig. 4. Connecting instrument to the existing current transformers in medium voltage system
Table 2. Events on the 20 kV network registered in period from March 7 to March 18, 2016
Voltage parameter Supply interruption Voltage dip Temporary overvoltage
Number of events
1
11
3
Impact of Distributed Generation on Power Quality
439

Power is measured in accordance with standard IEEE 1459-2010 and result is
presented on ﬁgure below (Fig. 5).
All current measurements represent RMS values and results are presented on ﬁgure
below (Fig. 6).
Measurement results of all three phase voltages for the period from March 7 to
March 18, 2016 are shown on ﬁgure below (Fig. 7). Measurement results illustrate
that voltage variations are within speciﬁed range of ±10% according to standard EN
50160 [7].
Fig. 5. Active power for the period from March 7 to March 18, 2016
Fig. 6. Three phase current RMS values for the period from March 7 to March 18, 2016
440
A. Jusić et al.

It is important to note that in accordance with the introductory provisions of
standard EN 50160, quality parameters of voltage during supply interruptions are not
included in the power quality assessment.
Figure 8 illustrates results of frequency measurement for the period from March 7
to March 18, 2016. Measurement results illustrate that power frequency variation is
within the limits of ±1% given in EN 50160 (49,5–50,5 Hz).
Fig. 7. Three phase voltages for the period from March 7 to March 18, 2016
Fig. 8. Power frequency for the period from March 7 to March 18, 2016
Impact of Distributed Generation on Power Quality
441

Figure 9 illustrates values of total voltage harmonic distortion (THDU) for all three
phase voltages for the period from March 7 to March 18, 2016 measured at metering
panel of medium voltage switchgear of mini hydropower plant “Kordići”.
Measurement results show that total voltage harmonic distortion (THDU) is sig-
niﬁcantly below the limit of 8% given in EN 50160.
Values of long-term ﬂicker severity (Plt) are shown on Fig. 10 for all three phase
voltages for the period from March 7 to March 18, 2016 measured at metering panel of
medium voltage switchgear of mini hydropower plant “Kordići”. Figure 10 illustrates
that ﬂicker severity is within the limits given in EN 50160 (Plt < 1) during the presence
of voltage at the point of measurement.
3.5
Impact of Disconnection and Reconnection of Mini Hydropower
Plant “Kordići” on Power Quality
In this part of the paper will be presented measurement results of power quality during
switching operations of mini hydropower plant “Kordići”.
Figure 11 illustrates values of total active power (Ptot) of mini hydropower plant
“Kordići”, measured at metering panel of medium voltage switchgear during switching
operations. Moment of disconnection and reconnection of the generator breaker is
clearly visible on the Fig. 11. Disconnection of the generator from the network
occurred on March 15th at 8:50 a.m. and reconnection at the same day at 11:20 a.m.
All three current RMS values before disconnection and after reconnection of mini
hydropower plant are shown on ﬁgure below (Fig. 12).
Fig. 9. Illustration of total voltage harmonic distortion (THDU) for the period from March 7 to
March 18, 2016
442
A. Jusić et al.

Figure 13 shows voltage variations of all three phase voltages during switching
operations. The graphic shows that the voltage at the point of measurement varies
approximately 1,68%, which is within the limits given in EN 50160.
Fig. 10. Long-term ﬂicker severity (Plt) for all three phase voltages for the period from March 7
to March 18, 2016
Fig. 11. Total active power (Ptot) of mini hydropower plant “Kordići” during switching
operations
Impact of Distributed Generation on Power Quality
443

Figure 14 shows the change of frequency during the observed period. Frequency is
a system parameter and disconnection of the generator can’t have an impact because of
its relative low power generation.
Figure 15 illustrates values of total voltage harmonic distortion (THDU) during
disconnection and reconnection of mini hydropower plant. Measurement results show
that total voltage harmonic distortion (THDU) during switching operations is below the
limit of 8% given in EN 50160.
Fig. 12. All three current RMS values during switching operations
Fig. 13. Phase voltages during switching operations
444
A. Jusić et al.

4
Conclusion
This paper analyzed the impact of mini hydropower plant “Kordići” on the 20 kV
distribution network of Bugojno area in terms of power quality. Therefore, measure-
ment and evaluation of the quality of the supplied power was carried out at the point of
common coupling between the mini hydropower plant and the distribution network.
Measurements of power quality were carried out in accordance with EN 50160 for the
period from March 7 to March 18, 2016 and covered the period before the ﬁrst
Fig. 14. Illustration of power frequency during the observed period
Fig. 15. Illustration of total voltage harmonic distortion (THDU) during switching operations
Impact of Distributed Generation on Power Quality
445

synchronization and measurement during disconnection or reconnection of mini
hydropower plant to the distribution network.
Considering the fact that requirements concerning voltage changes in EN 50160 are
not very rigorous i.e. the parameters of the supply voltage shall be within the speciﬁed
range during 95% of the test period, while the permitted deviations in the remaining 5%
of the period are much greater, we can safely say that the power quality measurement
results are substantially within the speciﬁed limits.
References
1. Jusic, A.: Analysis of the impact of the ﬁrst synchronization of MHP “Kordići” on the 20 kV
distribution network of Bugojno area. University of Sarajevo, Faculty of Electrical
Engineering (2016)
2. Ferracci, P.: Power Quality. Cahiers Techniques no. 199, Schneider Electric (2001)
3. Dugan, R.C., McGranaghan, M.F., Santoso, S., Beaty, H.W.: Electrical Power Systems
Quality, 2nd edn. The McGraw-Hill Companies (2004)
4. Tesařová, M.: Power Quality and Quality of Supply. Železná Ruda-Špičák, University of
West Bohemia, Czech Republic (2011)
5. Markiewicz, H., Klajn, A.: Voltage Disturbances, Standard EN 50160—Voltage Character-
istics of Public Distribution Systems. Wroclaw University of Technology (2004)
6. de Almeida, A., Moreira, L., Delgado, J.: Power Quality Problems and New Solutions. ISR—
Department of Electrical and Computer Engineering, University of Coimbra (2003)
7. European standard EN 50160: Voltage characteristics of electricity supplied by public
distribution systems
446
A. Jusić et al.

Part IV
Civil Engineering

3D Numerical Study of Sidewall Friction
Inﬂuence on Small Scale Reinforced Earth
Wall Behavior
Adis Skejić(&), Mladen Kapor, Senad Medić, and Đenari Čerimagić
Faculty of Civil Engineering, Institute of Geotechnical Engineering and Geology,
University of Sarajevo, Sarajevo, Bosnia and Herzegovina
askeja@live.com, {mladenkapor,senad_medic}@yahoo.com,
djenari.cerimagic@gf.unsa.ba.com
Abstract. Identiﬁcation of failure mechanisms and behavior of reduced scale
reinforced earth walls under footing pressure have been increasingly investi-
gated in recent period. Numerical and physical modeling is introduced in
research to generate data related to interaction of the particular components of
these walls. Certain assumptions must be made regarding the choice of
parameters for numerical model while physical modeling on scaled walls often
includes boundary conditions that can be different from full scale walls. One of
the model wall boundary conditions that does not exist for full scale walls is
sidewall friction. Its effect can be reduced by applying silicone grease before
wall construction. Since model walls are often made with transparent glass sides,
lubrication of inner surfaces of sidewalls reduces its transparency and disables
observation by advanced monitoring techniques. Inﬂuence of sidewall friction
was investigated by 3D numerical analysis in this study. Reduction of the
sidewall friction inﬂuence with increasing model width was conﬁrmed. Results
indicate that sidewall friction is not a decisive factor that governs behavior and
failure mechanism even for a relatively narrow model walls.
Keywords: Failure mechanism  Reduced scale reinforced soil walls
Sidewall friction  3D numerical modeling
1
Introduction
Understanding of soil-reinforcement interaction mechanisms can be signiﬁcantly
improved by observing displacement ﬁeld of reinforced earth walls surcharged at
the top. Examples of physical analyses of reduced scale walls can be found in works of
[1–6].
Table 1 gives an overview of wall dimensions and sidewall conditions used in
some of the mentioned studies. As illustrated, most of the physical models were built
with transparent sidewalls. Since transparency of the front sidewall is very important
for monitoring, it is generally not possible to apply grease at the inner sidewall surface.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_40

Glass-sand friction angles between 6° and 9° were reported in literature [7]. The results
of direct shear testing (150 mm  150 mm), conﬁrm that sand-glass contact friction
angles range between 10° and 15°, depending on the normal stress [8]. Consequently,
physical models performed with direct sand-glass contact could be inappropriate for
plane strain condition simulation. Additionally it is discussed that sidewall friction
cannot be eliminated without appropriate lubrication technique [9]. Note that a sidewall
lubrication technique can reduce the sidewall friction to about 0.05° [10]. There are
many examples of physical models performed without lubrication of sidewalls. For
example, [5] stated that sand-glass friction does not inﬂuence the failure mechanism
signiﬁcantly since failure mechanisms predicted by 2D limit equilibrium analysis were
in good agreement with failure mechanisms recorded by monitoring of colored sand
motion at large displacements.
Investigation of sidewall friction inﬂuence is often related to unreinforced sand
without additional load at the top. For example, [11] numerically investigated the
effects of sidewall friction in unreinforced models. The authors concluded that the
active earth pressure on a narrow model of retaining wall (width/height = 0.4) was
reduced by 30% for dilative sand with sidewall friction angle of usw= 15°. Increasing
the model width and backﬁll friction angle resulted with reduction of inﬂuence of
sidewall friction. It is experimentally measured that the reduction of active earth
pressure coefﬁcient Ka amounts to about 14% for unreinforced small-scale retaining
walls with sidewall friction angle of usw = 5.7° and width to height ratio equal to 0.5
[12]. Similar values were suggested by [13].
This paper investigates the inﬂuence of sidewall friction on small scale reinforced
earth wall behavior by 3D numerical modeling. Different wall geometries are analyzed
in order to deﬁne optimum geometry that enables construction of representative small
scale wall without lubrication of sidewalls.
Table 1. Characteristics of selected reinforced earth physical model walls
Investigation
Model
height-H
(cm)
Model
length-L
(cm)
Model
width-B
(cm)
B/H
ratio
Sidewall condition
Simonini and
Gottardi (2003)
60.0
120.0
40.0
0.66
Low friction glass
Anubhav and
Basudhar (2014)
45.0
71.0
55.0
1.22
Thin transparent ﬁlm
attached to perspex sheet
Xiao et al. (2016) 36.0
72.0
40.0
1.1
Low friction glass
Jacobs et al.
(2016)
100.0
100.0
45.0
0.45
Low friction glass
450
A. Skejić et al.

2
Physical Model
An instrumented small scale wall was used as benchmark for veriﬁcation of 3D
numerical model. Box dimensions and geometry of instrumented small scale wall is
shown in Fig. 1. The wall was built in a box with inside dimensions of 1.3 m (length),
0.5 m (width) and 0.8 m (height). Transparent sidewall is made of 5.1 cm thick glass,
while thin 4.0 mm glass is glued to the opposite stiff wooden wall side in order to
enable same, low friction characteristics at the interface. Sidewall surfaces were not
Fig. 1. a Box dimensions; b wall geometry
3D Numerical Study of Sidewall Friction Inﬂuence
451

lubricated since transparency was requested for digital image collection during footing
pressure application. Details of physical model wall construction and monitoring
details are not reported here since it is part of unpublished paper.
3
Numerical Model
3D numerical model (Fig. 2) was composed by using Pl axis 3D ﬁnite element program
[14]. Details of the numerical model are described for particular components which
include:
boundary
conditions,
backﬁll,
reinforcement,
facing-facing
interface,
facing-sand interface, soil-reinforcement interface, footing plate, construction phases
and loading program.
A horizontal restraint in x-direction was assigned to the right side of the model, and
the bottom boundary of the model was assumed ﬁxed in all directions. Interface ele-
ments are introduced between stiff sidewalls and backﬁll to account for friction.
Namely, direct use of interface strength reduction by applying reduction factor value
(Ri < 1) to backﬁll, signiﬁcantly reduces the normal stiffness of zero thickness element
(see [14, 15]).
This can result with displacements perpendicular to sidewall surface greater than
zero. In order to prevent this, additional material with high stiffness and small strength
is introduced for deﬁning side wall-sand interface. The parameters of this material are
given in Table 2 along with other model parameters. High stiffness of interface material
was selected as appropriate since direct shear test results indicate that the full sidewall
friction capacity is mobilized after relatively little deformation (only about 0.5 mm of
y
x
z
backfill
0.5 m
1.3 m
0.8 m
Facing 
elements
Stiff sidewall
footing
pressure (q)
0.15 m
Fig. 2. 3D numerical model
452
A. Skejić et al.

shear displacement is required to achieve this condition) [13]. Furthermore, both
large-scale and small-scale tests did not show any systematic reduction in usw at large
deformations.
Backﬁll sand soil is modeled by Hardening soil constitutive model [16]. Strength
and stiffness parameters are determined in accordance with conventional laboratory
tests. Poisson ratio and empirical m coefﬁcient are adopted according to suggested
values given by [17].
Reinforcement (metal grid) is modeled as linear elastic ideally plastic material.
Reinforcement stiffness is deﬁned as the ratio of force per unit width at 50% of ultimate
strength (Fult) and corresponding axial deformation measured during in-air tensile test
(EA = 1277 kN/m and Fult = 7,7 kN/m).
Soil-reinforcement interface was modeled as perfect bond with the same strength of
interface as backﬁll. This result is obtained by independent pullout tests performed in
small box with inside dimensions equal to 260 mm (length), 160 mm (width) and
250 mm (height). Reinforcement length and width were 200 mm and 155 mm
respectively. Other details and results of this test are not reported in this paper.
Facing blocks made of wood (thickness 1.8 cm) were modeled as linear elastic
solid elements (Table 2).
Facing block-block interface strength between two wooden plates is determined by
simple sliding test. The angle between plates and horizontal surface is incrementally
increased until sliding occurred. Angle at sliding is measured as u = 23.0°, what
deﬁnes the strength of interface for numerical modeling.
An independent shear test for deﬁning sand-wooden plate shear strength was not
performed here, since this value does not inﬂuence the results signiﬁcantly.
Backﬁll-facing block interface is modeled with constant value of strength reduction
coefﬁcient (Ri = 1.0), since similar value was suggested by [18] for wood-quartz sand
interface.
Table 2. Parameters of numerical model components
Parameter
Backﬁll sand
(R = 95%)
HS model
Facing
(linear elastic)
Sand glass interface
Mohr Coulomb
Unit weight, cd (kN/m3)
16.0
6.0
16.0
Internal friction angle, u (°) 46.0
–
1.0
Cohesion, c (kN/m2)
1.0
–
0.1
Dilatancy angle, w (°)
8.0
–
0.0
pref (kPa)
100.0
–
–
E50,ref (kPa)
17 000
1.1e7
50 000
Eoed,ref (kPa)
14 820
–
–
Eur (kPa)
96 000
–
–
K0 (–)
0.281
–
–
m (–)
0.5
–
–
Poisson ratio m(ur) (–)
0.2
0.2
0.4
3D Numerical Study of Sidewall Friction Inﬂuence
453

Construction phases are simpliﬁed to only one phase without modeling compaction
and temporary formwork.
According to model test setup, surcharge load is applied as uniform load at steel
plate (495/200/20 mm, unit weight, c = 78.5 kN/m3, Young’s modulus, Eref = 2e8 kPa
and Poisson ratio, m = 0.15). The load is applied according to surcharge program of
physical model wall. It is incrementally increased up to pressure that results with the
ultimate tensile load in reinforcement, since the reinforcement model is not capable of
simulating strain softening in tension.
4
Validation of Numerical Model Results
The comparison of measured and numerically predicted lateral displacements of facing
blocks and settlements of loaded plate are shown in Fig. 3. Vertical settlements are
given in stages until the yield force in reinforcement was reached and the lateral
displacements are shown for ﬁnal loading stage. It is useful to note that yield tension
force predicted by numerical model reached ultimate tensile resistance for meshes that
failed during physical model test for the limit vertical load, which is same as maximum
load used in testing.
-80
-70
-60
-50
-40
-30
-20
-10
0
physical model
numerical model
wall height [cm]
Δx [mm]
-30
-25
-20
-15
-10
-5
0
-5
(a)
(b)
-3
-1
0
100
200
300
physical model
numerical model
q [kPa]
Δy [mm]
Fig. 3. Comparison of measured and numerically predicted: a lateral displacements of facing
blocks at the ultimate stage; b settlements of loaded plate
454
A. Skejić et al.

5
Analysis of Sidewall Friction
Inﬂuence of sidewall friction was analyzed from two different points of view. The ﬁrst
considers footing pressure (q) versus settlements curve (Dy) and the second relates
values of maximum tension forces in reinforcements (Fx) and different footing pres-
sures (q).
All model walls had a wall height of 0.8 m. Total of 5 different model widths were
deﬁned in combination with three different sidewall-sand interface strengths. Values of
sidewall friction were selected as usw = 1°, usw = 10° and usw = 15°, which include
lower and upper limit of dense sand vs. glass friction at normal pressures for model
walls conditions [17]. An illustration of investigation plan is given in Fig. 4.
Sum of maximum tension forces calculated for all 4 reinforcements for different
sidewall friction angles (Fi,max, usw) was compared with forces predicted by model with
negligible sidewall friction (Fi,max, usw = 1°). This ratio is deﬁned by factor a:
a ¼ 1 
P Fi;max;usw
P Fi;max;usw¼1
ð1Þ
The reduction of tension forces in all reinforcements (a) with an increase in the
sidewall friction (uaw) is shown in Fig. 4b. It is shown that as the sidewall friction
angle increases, the reduction of reinforcement tension forces increases too. This is
expected, since sidewall friction component reduces the total lateral pressure. Reduc-
tion of the box width results in decrease of reinforcement tension force.
glass-sand
interface
(φsw=1º; 10º; 15º)
B = 0.125m; 0.25m; 0.5m; 
1.0m; 2.0m
H = 0.8m
footing
pressure (q)
wall facing
0
5
10
15
20
25
30
(b)
(a)
0
5
10
15
Reduction of reinfocement tenssion forces, α [%]
Sidewall friction angle, φSW [degrees]
B/H=0,15625
B/H=0,3125
B/H=0,625
B/H=1,25
B/H=2,5
Fig. 4. a Illustration of analyzed wall geometries; b results of numerical simulations
3D Numerical Study of Sidewall Friction Inﬂuence
455

Results indicate that the inﬂuence of sidewall friction can be signiﬁcantly dimin-
ished by increasing the box width. Namely, even a relatively narrow model (B/H =
0.625) with upper limit of glass-sand sidewall friction angle can keep the error in
tension force prediction lower than 10%.
6
Conclusion
Comparison of 2D and 3D analysis indicates that the same results can be achieved by
matching boundary conditions of models if interface parameters at sidewalls are cor-
rectly selected. It is found that the glass sidewall-sand friction does not inﬂuence the
reinforcement tension forces signiﬁcantly (<10%) if model width is larger than 0.6H.
References
1. Schlosser, F., Long, N.T.: Recent results in French research on reinforced earth. J. Constr.
Div. 100(10800 Proc Paper) (1974)
2. Wong, K.S., Broms, B.B., Chandrasekaran, B.: Failure modes at model tests of a geotextile
reinforced wall. Geotext. Geomembr. 13(6), 475–493 (1994)
3. Simonini, P., Gottardi, G.: The viscoplastic behavior of a geogrid-reinforced model wall.
Geosynth. Int. 10(1), 34–46 (2003)
4. Anubhav, A., Basudhar, P.K.: Footing on double-faced wrap-around reinforced soil walls.
Proc. Inst. Civil Eng.-Ground Improv. 167(2), 73–87 (2014)
5. Xiao, C., Han, J., Zhang, Z.: Experimental study on performance of geosynthetic-reinforced
soil model walls on rigid foundations subjected to static footing loading. Geotext.
Geomembr. 44(1), 81–94 (2016)
6. Jacobs, F., Ruiken, A., Ziegler, M.: Investigation of kinematic behavior and earth pressure
development of geogrid reinforced soil walls. Transp. Geotech. 8, 57–68 (2016)
7. Tatsuoka, F., Haibara, O.: Shear resistance between sand and smooth or lubricated surfaces.
Soils Found. 25(1), 89–98 (1985)
8. Ruiken, A., Ziegler, M.:
Zum Spannungs-Dehnungsverhalten des Verbundbaus toffs
„geogitterbewehrter Boden” (No. RWTH-CONV-144574). Lehrstuhl für Geotechnik im
Bauwesen und Institut für Grundbau, Bodenmechanik, Felsmechanik und Verkehrswasser-
bau (2013)
9. Wu, J.T., Adams, M.T., Nicks, J.E.: Discussion of Xiao C. et al., Experimental study on
performance of geosynthetic-reinforced soil model walls on rigid foundations subjected to
static footing loading. Geotext. Geomembr. 44(1), 81–94 (2016); Geotext. Geomembr. 6
(44), 891–893
10. Tatsuoka, F., Molenkamp, F.R.A.N.S., Torii, T., Hino, T.: Behavior of lubrication layers of
platens in element tests. Soils Found. 24(1), 113–128 (1984)
11. Jayasree, P.K., Rajagopal, K., Gnanendran, C.T.: Inﬂuence of sidewall friction on the results
of small-scale laboratory model tests: numerical assessment. Int. J. Geomech. 12(2), 119–
126 (2011)
12. Bransby, P.L., Smith, I.A.: Side friction in model retaining-wall experiments. J. Geotech.
Geoenviron. Eng. 101(ASCE# 11447 Proceeding) (1975)
456
A. Skejić et al.

13. Bathurst, R.J., Benjamin, D.J.: Preliminary assessment of sidewall friction on large-scale
wall models in the RMC test facility. In: The Application of Polymeric Reinforcement in
Soil Retaining Structures, pp. 181–192. Springer Netherlands (1988)
14. Brinkgreve, R.B.J.: PLAXIS—Finite Element Code for Soil and Rock Analyses (2002);
Hatami, K., Bathurst, R.J.: Numerical model for reinforced soil segmental walls under
surcharge loading. J. Geotech. Geoenviron. Eng. 132(6), 673–684 (2006)
15. Skejić, A.: Interface formulation problem in geotechnical ﬁnite element software. Electron.
J. Geotech. Eng. 17, 2035–2041 (2012)
16. Schanz, T., Vermeer, P.A., Bonnier, P.G.: The hardening soil model: formulation and
veriﬁcation. Beyond 2000 in computational geotechnics, pp. 281–296 (1999)
17. Benz, T.: Small-Strain Stiffness of Soils and Its Numerical Consequences, p. 193. University
of Stuttgart, Inst. f. Geotechnik (2007)
18. Acar, Y.B., Durgunoglu, H.T., Tumay, M.T.: Interface properties of sand. J. Geotech.
Geoenviron. Eng. 108(GT4) (1982)
3D Numerical Study of Sidewall Friction Inﬂuence
457

3D Modeling and Nonlinear Analysis Stability
of CWR Tracks
Sanjin Albinovic1(&), Samir Dolarevic2, and Dusan Marusic3
1 Faculty of Civil Engineering, Department of Roads, University of Sarajevo,
Sarajevo, Bosnia and Herzegovina
sanjin.albinovic@gmail.com
2 Faculty of Civil Engineering, Department of Materials and Structures,
University of Sarajevo, Sarajevo, Bosnia and Herzegovina
samir.dolarevic@gf.unsa.ba
3 Faculty of Civil Engineering, Architecture and Geodesy, University of Split,
Split, Croatia
dusan.marusic@gradst.hr
Abstract. Rail welding and forming continuous welded track provides many
beneﬁts in terms of exploitation and maintenance of railway lines, however there
remains the problem of ensuring stability and prevent lateral buckling of the
track. This problem is particularly expressed in areas with large temperature
differences in the summer and winter period (minimum and maximum tem-
perature) and tracks the maximum adjusted topographical conditions (applied to
the minimum curvature radius and maximum longitudinal slopes). In order to
solve this problem in the past have made numerous researches and developed
many of the analytical method for the analysis. Many of them due to the large
number of parameters are too complicated and are not applicable in engineering
practice so tended their simpliﬁcation, usually in a way that the calculations are
not taken into consideration material and geometric nonlinearity. To analyze the
stability of the CWR track in past several years used numerical models based on
the ﬁnite element method, and have been developed and various software
packages to analyze the stability of the track. Advantages of this method of
analysis of the stability of the track-and CWR are able to make the models
corresponding to the actual state of the track and the possibility of nonlinear
analysis. Also, for making models can be used commercial FE software for
nonlinear structural analysis, as will be shown in this paper.
Keywords: CWR track  Numerical models  Track buckling
1
Introduction
Classic (mechanical) rail joints are showed of many disadvantages in the use because in
places of these joints often is coming to damage of rails, sleepers and fastening sys-
tems, and a track irregularity. For these reasons, today at the railways are generally in
use welded joints with the formation of the so-called, continuous welded rail (CWR
track) (Fig. 1).
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_41

Advantages of continuous welded rails are that due the dynamic loads from passing
trains damages of rails and noise level are signiﬁcantly less than classic joints, while
passenger comfort is much higher, especially at higher speed trains.
The main problem in CWR track is ensuring stability of track.
In fact, in traditional joints allows changing the length of the rails due to temper-
ature changes.
In CWR track, during a changing temperature it is not possible to changing length
rails and consequently is coming to appearance large longitudinal forces in the rails that
can cause breakage or deformation of the rails.
Even though, from the aspect of safety of the railway trafﬁc is not desired any of the
above cases, is somewhat more unfavorable case of the deformation or lateral track
buckling.
To these phenomena usually coming in the summer period at extremely high tem-
peratures (greater than 40 °C) when the temperature in the rails can increase to 65 °C,
and as a result prevented by change in rails length, longitudinal force (pressure) can be
up to 1000 kN. This force may cause a lateral rail buckling and displacement sleepers
over the ballast (Fig. 2).
Fig. 1. Mechanical and welded joints [1]
Fig. 2. Typical lateral track buckling [2, 3]
3D Modeling and Nonlinear Analysis Stability
459

In order to prevent the occurrence of such a large longitudinal force is necessary to
implement appropriate action in designing and laying of CWR track (calculations of
temperature laying and welding of rails in CWR).
Since the greatest resistance to lateral buckling track provides a ballast (65–85% of
the total resistance to track buckling), it is usually done in a way that during the ballast
prism construction, provides installation a corresponding devices which increase the
lateral ballast resistance.
2
The Theoretical Assumption and Methods to Solving
the Problem of CWR Track Stability
There are many theories that have been offered as an explanation of the phenomenon of
track buckling, or to solve the problem of stability of the track. It is characteristic that
are recommended formulas for using are very different, in forms, and the results they
give.
In many theories of track stability, track grid is observing as a beam of inﬁnite
length in a homogeneous environment that gives resistance to longitudinal and lateral
movements of the beam. Axial pressure force Po which acting in such beam, before
deformation have constant intensity in each section of the beam.
In curved beam at one place, i.e. at place a local deformation in the horizontal
plane, regardless of the cause of deformation, the length of the beam in a deformed
shape is slightly larger than the original length of the beam. Also, there is a relaxation
of the stresses in the beam and the pressure force in the beam Po decreases of values
according to the value of P (P < Po).
Assuming that the stresses does not exceed the limit of elasticity, the longitudinal
deformation of the beam, i.e. the size of its elongation, can be determined according to
Hooke’s Law:
u ¼ P  Po
ð
Þ  1
E  x
;
ð1Þ
where are:
E—modulus of elasticity of steel (for rails),
Fig. 3. Deformation of beam [4]
460
S. Albinovic et al.

x—cross section of rails,
l—length of curved part (Fig. 3).
Because curved part still remains linked to beam as an entity, the local stress drop
will cause deformation of the straight parts of the beam that are directly linked with the
curved part.
The frictional resistance r is opposing to this deformation, which causes relaxation
of the straight parts of beam, i.e. gradually are reducing the pressure force and lon-
gitudinally displacement.
At a distance L from the ends of the curved part, longitudinal deformation becomes
zero, and the pressing force have intensity (values) Po.
As a result, total deformation curved part k is greater than lengthening u for 2z:
k ¼ u þ 2z;
ð2Þ
where z is longitudinal deformation of the straight parts of the beam.
The size of k can be determined as the difference between the curved part length
L1, and the length of the arc L:
k ﬃ
Z1=2
0
y0
ð Þ2dx ¼ u1 f1; 1
ð
Þ
ð3Þ
where the f arrow curved parts (Fig. 3). Consequently, Eq. (3) connecting the lateral
deformation express an arrow F and the longitudinal deformation. In the literature this
equation is referred to as “deformation” equation.
The main differences in the approaches to calculation stability of track, related
mainly to the different ways of determining the conditions of equilibrium, which is a
major problem with the calculation of stability CWR track.
Various researchers had a different approaches to solving this problem, so that
some of them used the expressions for potential energy elastically deformed beam,
others are introduced equivalent transverse loads, while some tried to directly integrate
differential equation of equilibrium. There are also used differential equation oscillation
beam and integral equations, various analogies, etc.
In the framework of these researches, in various ways is described the problem of
stability of the track and the most used is method of energy or methods of elastic
potential, while the solutions to the problem usually occurs by setting and solving
differential or integral equations of equilibrium.
In the literature of this period (1930–1965), the usual was a division on complex
solutions (M. Numate, M. A. Martine 1936, L. Sakmauer 1958, A. A. Krivobodrov
1952, E. Nemeždi, H. Zanden 1932, K. N. Mišćenko 1932), simpliﬁed solutions (G.
Majer 1937, G. Rubin 1955, J. Nemeždi – Nemeček 1931, R. Levi 1957, L. Sakmauer i
D. Bartleo 1961) and simpliﬁed a complex solution (K. Grinevald 1930, A. Bloh 1932,
K. N. Miščenko 1932, J. Nemeždi – Nemeček 1931) to the problem of stability of the
track [4].
Solutions that take into account the lateral and longitudinal deformation of the
beam, called the complex solution.
3D Modeling and Nonlinear Analysis Stability
461

Most of the so-called complex solutions is due to its complexity was practically
unusable, and therefore were made various simpliﬁcations.
The most common simpliﬁcations are:
• of track grid to seen as full steel beam of the bending rigidity in the horizontal and
vertical plane
• resistance mid transverse displacements of the beam in horizontal and vertical plane
is constant and evenly distributed along the length of the beam,
• beam in the ﬁrst, unstressed condition forms is the ideal straight line, or in the case
that are analysing the stability in curved track, ideally a circle.
With such an approach problem solving were developed many formulas for cal-
culating the stability of continuously welded rail.
Given the many simpliﬁcations and different approach to the problem and the
solutions obtained calculations by various formulas were different.
Comparison of computational and experimental results for a steady-state equilib-
rium after track buckling shows the matching results.
However, this cannot be proof of regularity of those theories, because the com-
parison of results obtained by formulas from mentioned theories with experimental
results, it is right only for the state of stable equilibrium, which occurs after the
completion of tracks buckling process.
The main cause of disagreement of the theory with reality is that not considered the
most important thing in the process of track buckling and that is the cause which leads
to the displacement track from the original position.
In order to displacement track from its initial equilibrium position is not necessary
to have impact any external lateral forces, than because of increase in the potential
energy of the system and axial forces of pressure. In addition, the problem is that is in
these calculations applied inadequate load schemes and supporting of track grids, the
initial track irregularities and misalignments are not taken into calculation, resistance
values for lateral and longitudinal track displacement are constants and etc.
Based on the exposed theory from this period can be concluded that the problem of
elastic stability of the track is not completely resolved, and in the coming period this
issue dealt with a number of experts in the various countries of the world.
In this period coming up to the ﬁrst use of computers to create models for analyze
stability of CWR tracks. The ﬁrst models to analyze the stability of CWR tracks that
have appeared at the beginning of the 60s of the last century were generally a beam
models (Bijl 1964, Kerr 1976–1980, Kish 1982–1985, Samavedam 1979–1993) or
two-dimensional rail-sleeper model (one sleeper with two rails—Ramesh 1985 and
Jackson with associates 1988) [5].
These methods are similar to the above analytical methods have many disadvantage
and the main disadvantage is that the ones could not take into calculation the various
nonlinear behavior of structural elements of the CWR track.
For these reasons, even during the 90s comes to the development of the ﬁrst
numerical models based on the ﬁnite element method which were used to analyze the
stability of CWR track (El-Ghazaly 1991, Hengstum i Esveld 1998) [5, 6].
Although compared to previous models, which were to simplify calculation con-
strained to horizontal and vertical level has been made certain progress because there
was a possibility to deﬁne the nonlinear behavior of structural elements of the track.
462
S. Albinovic et al.

On the basis of these models have been developed various software tools to analyze
the stability of the CWR track, and among the signiﬁcant are CWERRI1 and CWR
Buckle2 where they made some improvements, and based on the results of analyzes on
these models conducted UIC Committee ERRI D2023 and for different cases of the
condition and elements of the track structure, developed safety criteria for stability of
CWR track4 [7–9].
The model (Fig. 4) consists of the beam elements the length of 0.6 m (distance
between sleepers) that are actually two rails which are supported on a linear elastic
spring (Winkler model) which actually represent is a sleeper and the ballast.
Inordertomadethemorecrediblepresentationoftrackstructureshavebeendeveloped
three-dimensional model (Lim et al. 2003—Figs. 5 and 6) and the some special software
for nonlinear analysis of CWR track based on the ﬁnite element method [5].
Fig. 4. LONGSTAB (Model, which is an improved version of CWEERI model and developed
by C. Esveld and associates) model [9, 10]
Fig. 5. Cross section of three-dimensional track model [5]
1 The model was developed at TU Delft in the Netherlands by prof. C. Esveld and associates and based
on an earlier model called Prolis.
2 Model was developed (1996) in the company Foster and Miller (Kish and Samavedam et al.) for the
US Federal Railroad Administration (Federal Rail Administration of the United States of America—
FRA).
3 Committee for scientiﬁc research the worldwide railway organization (UIC).
4 Original name for document is UIC leaﬂet 720—R—“Laying and Maintenance of CWR Track”
Paris 2005.
3D Modeling and Nonlinear Analysis Stability
463

3
Numerical Model for Analyze Stability of CWR Track
Lately, it is tend to improving 3d model for analyzing the stability of CWR track on the
basis of experimental research and existing software developed for this purpose are
continuously upgraded. However, to analyze the stability of CWR track can be use and
commercial software for the structural analysis provided they have certain features.
In this paper will brieﬂy present the possibility of making 3D models for analyzing
the stability of CWR track using a commercial software “SAP 2000 nonlinear” [11].
This software has a wide variety of structural analysis and is necessary to select
appropriate type of analysis.
In this case, was chosen Direct-integration time-history analysis. That allows to
deﬁne and consider of material and geometric non-linearity in the calculation (large
displacement effects and P delta effects) as well as combinations and variations load [11].
Also, there is the possibility of continuing the previously executed nonlinear
analysis, taking into consideration the history of the nonlinear condition or stability
analysis of CWR track according to the second-order theory.
The model consists of the beam elements that representing the rails and sleepers,
and spring elements are provided showing interconnections between rail and sleeper or
between the sleepers and ballast (Fig. 7).
Fig. 6. Side view of three-dimensional track model [5]
Fig. 7. Cross section and side view of three-dimensional track model made in Sap 2000 [12]
464
S. Albinovic et al.

As the rails and sleepers are modeled as beam elements is necessary in the ﬁrst step
to deﬁne the characteristics of the material (steel for the rails and concrete or wood for
sleepers) and the cross-sections (Fig. 8).
This software is primarily designed for the analysis of structures and there are only
basic drawing tools, and it is not possible to draw a cross-section which is identical to
the real cross-section but there is a possibility of correction by coefﬁcients and through
adjustment of the real state.
After deﬁning of the basic characteristic for all beams elements (material and
cross-section), the next step in making a model is deﬁning the connections between
some elements.
When such numerical models are made, since all interactions between the elements of
the structure of the track represented by spring elements, the most importance is to select
the adequate type of springs (linear or nonlinear), and its stiffness in a speciﬁed direction.
Connection between the rails and sleepers which are in the reality is realized by
fastening systems in the model is made using non-linear elastic springs.
In this way a behavior of the fastening systems is simulated and stiffness of these
springs have been adopted on the basis of tests carried out on various types of fastening
systems and which are also used in the previously developed models [13, 14].
In the CWR track, the sleepers are situated on the crushed stone layer, which is
compacted with a certain force and which is called a ballast prism.
The correct execution of the ballast prism and its compaction, preventing the
displacements of sleepers in the longitudinal and lateral direction.
So, in the next step connections between sleepers and the ballast are simulated, and
there is necessary to deﬁne the resistance to lateral and longitudinal displacements
between sleepers and ballast.
Fig. 8. Material and cross section deﬁnition for rails in Sap 2000 [12]
3D Modeling and Nonlinear Analysis Stability
465

The value of resistance the lateral displacement which is using in analytical cal-
culations represent the maximum value, or assume that the resistance displacements of
sleepers in the ballast is linear.
Considering the great importance of the resistance in ensuring stability of the track,
were made many studies in the order to determine the values of the resistance.
Based on experiments, it is known that such resistance were not linear and limits
resilient displacement is in the interval of 0.05–0.4 cm [14].
Or in small movements and size of resistance are less, and its maximum value
reached in a displacement of about 10–15 mm, which then remain the constant (Fig. 9).
For deﬁning a behavior of the ballast, in the design model it is necessary to apply
non-linear spring by means which it is possible to simulate the non-linear character-
istics of the lateral resistance of sleepers with residual deformation, them to be
according to test results.
In the software package by which used for modeling and calculation there are
several options for deﬁning the behavior of non-linear springs and in this case, the best
spring type is the multilinear plastic spring (Fig. 10).
Also, in the same way, longitudinal ballast resistances is deﬁned which have higher
values of the lateral resistance and that is in the accordance with the testing results
(Fig. 11).
After completion of deﬁning elements, the model can be made in full length of the
track with all its geometric characteristics (straight or curve) and after deﬁning the load
to calculate the stability of the track to different scenarios (Fig. 12).
In the order to model validation necessary to make a comparison with the results of
experiments on test sections of track or made similar model implemented in UIC code
720. Of course, there is not the possibility of obtaining the identical results but devi-
ations in results should not be too large (Fig. 13).
Fig. 9. Diagram of lateral resistance between ballast and concrete sleepers [15]
466
S. Albinovic et al.

Fig. 10. Dialog box for deﬁne the stiffness and type of spring to simulate the resistance of lateral
displacement between the sleepers and the ballast [12]
Fig. 11. Dialog box for deﬁne the stiffness and type of spring to simulate the longitudinal ballast
resistance [12]
3D Modeling and Nonlinear Analysis Stability
467

Fig. 12. Model of straight track—deformation of track due to temperature stresses [12]
Fig. 13. The results of tests on an experimental rail section in Hungary [2]
468
S. Albinovic et al.

4
Conclusion
By improving the performance of computers and software tools for structural analysis
enabled a new approach to analyzing the stability of CWR track.
As shown in the this paper for modeling CWR track can be used and commercial
programs for structural analysis using the results of previously conducted research to
determine the basic characteristics of model elements.
After validation of the model, it is possible to analyze the stability of these various
loading and geometry of the tracks and at the same time to consider the different
material and geometric nonlinearity, misalignments and irregularities of the track, the
loss of connection elements and etc.
Even though, the modeling process requires more time than analytical calculation
method but in this way can be taken into calculation more factors that can lead to loss
of stability of the track.
Also, there is always the possibility of further development of the model in the
accordance with the results of new research, especially toward to the additional impact
of dynamic load due to the train passage.
References
1. https://en.wikipedia.org/wiki/Track_(rail_transport)#Continuous_welded_rail
2. http://boredomtherapy.com/railway-buckling/
3. https://teara.govt.nz/en/photograph/4536/twisted-railway-tracks-edgecumbe
4. Ignjatić, D.: Gornji stroj željeznica. Građevinski fakultet Univerziteta u Beogradu, Beograd
(1965)
5. Lim, N.-H., Han, S.-Y., Han, T.-H., Kang, Y.-J.: Parametric study on stability of continuous
welded rail track-ballast resistance and track irregularity. Steel Struct. 8, 171–181 (2008)
6. Coenraad, E.: Modern Railway Track, 2nd edn. Delft University of Technology,
MRT-Production (2001)
7. UIC CODE 720, Laying and Maintenance of CWR Track, International Union of Railways,
2nd edn., March 2005
8. Coenraad, E.: Improved Knowledge of CWR track, ERRI Committee D202, Paris (1998)
9. Kish, A., Samavedam, G.: Track Buckling Prevention: Theory, Safety Concepts, and
Applications. U.S. Department of Transportation/Federal Railroad Administration, Final
report, March 2013. Minneapolis, MN (2013)
10. LONGSTAB, Theoretical manual, Esveld Consulting Services (2004)
11. CSI Analysis Reference Manual for SAP2000—Computers and Structures, Inc., University
Avenue Berkeley, California 94704 USA (1995)
12. Albinovic, S.: Prilog istraživanju stabilnosti kolosijeka dugog šinskog traka (DTŠ-a). Ph.D.
thesis, Faculty of Civil Engineering, University of Sarajevo (2013)
13. van’t Zand, J., Moral, J.: Static and Dynamic Tests on Rail Fastening Systems, Report
7-97-118-2, Roads and Railways Research Laboratory, TU Delft, September 1997
14. Đorđe, M.S., Kolosek, N., Knjiga, T.: Beograd (1987)
15. van’t Zand, J., Moral, J.: Ballast Resistance Under Three Dimensional Loading, Report
7-97-103-4, Roads and Railways Research Laboratory, TU Delft, April 1997
3D Modeling and Nonlinear Analysis Stability
469

A Study of Speed on Two-Lane Roadways
Mehmed Bublin(&)
Faculty of Civil Engineering, Department of Roads and Transportation,
University of Sarajevo, Sarajevo, Bosnia and Herzegovina
bublin.mehmed@gf.unsa.ba
Abstract. Speed represents the most important functional characteristic of
roads. That is why the article attempts to give a view of the overall speed
generation procedure on main (trunk) roadways (magistralne ceste) and its
measuring methods. Also proposed is the back calculation approach in ana-
lyzing the conditions for main (trunk) road rehabilitation from the aspect of
bringing speed to conform with the legally set limits, but also road protection
from speed degradation due to elemental conditions taking place during road
exploitation.
1
Introduction
The article discusses important issues of deﬁning speed on two-lane roads.
Speed represents the most important functional characteristic signiﬁcantly affecting
travel time and costs, travel safety, capacity and level of road services.
That is why the driving dynamic parameters used in designing roads are important
for speed analysis, as well as the impact of road parameters on speed in roads that are in
use, and the state of speed on two-lane roads on the basis of signalization.
One may thus notice the complexity of the speed generation process, wherein one
should also be mindful of spontaneous processes pertaining to environmental factors
and trafﬁc safety factors, or frequent trafﬁc accidents.
2
Deﬁning Speed and Method of Measurement
Speed is generally deﬁned as the length of road covered during a unit of time, which is
usually indicated in kilometers per hour (km/h). Due to the fact that in a trafﬁc stream
the speed of particular vehicles may vary across a broad spectrum, in the analysis of
trafﬁc conditions on a road network, speed is usually construed to mean average speed.
Average speed (v) represents a ratio between the length of a particular road or
street, a section (segment) thereof, and the average time all vehicles within the observed
trafﬁc stream spent on the observed section of the road.
The general formula for calculating the average speed is as follows:
V =
L
Pn
i¼1 tin
¼
nL
Pn
i¼1 ti
,
ð1Þ
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_42

where are:
V
average stream speed (km/h)
L
length of road section (segment) (km)
ti
time of i-th vehicle’s travel along the section (h)
n
number of vehicles covered by the measurement of time they took to pass along
the observed section
Travel time stands for the total time a vehicle spends on the road.
2.1
Trafﬁc Stream Speed
There are different notions for the average stream speed that are applied in trafﬁc ﬂow
analyses, such as:
• Average running speed
Average running speed is a ratio between the length of the observed road section
and the time spent moving along the observed section. In the literature, the notion is
often (albeit wrongly) referred to as space mean speed.
• Average journey speed
Average journey speed is a ratio between the length of the observed road section
and the time the vehicle took to complete the journey along the observed road
section. Journey time includes the running time and time losses due to vehicle
stoppage.
• Space mean speed
Space mean speed is the mean speed of all vehicles within the trafﬁc stream of the
road section at the observed point in time.
• Time mean speed
Time mean speed is an arithmetic mean speed of all vehicles passing through the
observed road section over a given period of time.
Figure 1 shows a ratio between the time mean speed and space mean speed.
Fig. 1. Typical relations between time mean speed and space mean speed [1]
A Study of Speed on Two-Lane Roadways
471

Fundamental trafﬁc ﬂow parameters are as follows:
1. Density
Density is the number of vehicles per unit of length on a trafﬁc lane or road. Density
is expressed by the symbol q (veh/km).
Vg ¼ q
V ,
ð2Þ
where are:
g
ﬂow of vehicles (veh/h)
V
average ﬂow speed (km/h)
q
density (veh/km)
2. Distance headway
Distance headway is the distance between the heads of two successive vehicles in a
trafﬁc stream. Distance headway is expressed by symbol Sh (m/veh).
3. Interval headway
Interval headway is a time interval between the passage of two successive vehicles
through the observed road section. Interval headway is expressed by symbol th
(s/veh).
• Mathematical relations between two basic trafﬁc ﬂow parameters
Density on the basis of an average distance headway is calculated as follows:
g = 1000
Sh
veh
km


,
ð3Þ
Mean interval headway on the basis of mean distance headway and ﬂow speed is
calculated as follows:
th¼ Sh
V s/veh
ð
Þ,
ð4Þ
Analytical expressions of relations between the fundamental trafﬁc ﬂow parameters
are as follows:
q = gV,
ð5Þ
g = q
V and V = q
g ,;
ð6Þ
Diagrammatic interpretation of interdependence between the fundamental trafﬁc
ﬂow parameters per pairs V-g, V-q and q-g is shown in Fig. 2.
472
M. Bublin

2.2
Design Speed
The following types of speed are taken into account in designing roads:
Vv
driving speed is the actual speed of vehicle moving on the road,
Vdoz
permissible driving speed is the speed that is limited by legislation on a
particular road section,
Vput
travel speed represents an average driving speed on a particular road,
Vpl
planned/deﬁned travel speed represents the average driving speed, which
vehicles are supposed to have at the end of the period covered by the plan,
which represents a relevant speed to determine the dimensions of a normal
cross proﬁle, geometric and technical elements of the road,
Vpred
anticipated speed is a speed calculated for particular categories of road, based
on which to estimate road feature dimensions (a particular road section will
typically have one and the same anticipated speed),
Vpro
design speed is a speed of a moving vehicle in a free stream on a clean and
wet carriageway (free stream speed V85%), used as a designed speed to
analyze trafﬁc safety. Design speed cannot be lower than the anticipated speed
(Vpred), and its highest value must not exceed the highest legally permissible
driving speed on the road or a road section (Vdoz), and
Vrad
radial speed of moving—laterally, while switching lanes, and
Vr
designed speed is any speed used to determine or calculate technical road
features
2.2.1
Design Speed
Design speed Vpro is determined by analyzing road elements, situation plan and lon-
gitudinal proﬁle. The set speed must not exceed the maximum permissible speed on the
Fig. 2. Graphic interpretation dependencies of three basic parameters of trafﬁc ﬂow in ideal
conditions [1]
A Study of Speed on Two-Lane Roadways
473

given roadway. The analysis is performed for the roads belonging to the technical
groups A and B, where the anticipated speed exceeds 70 km/h. The following formulas
are in application:
For two-way roads with separate carriageways, where Vpred < Vdoz
Vproj ¼ Vpred þ 10 km/h for extremely curved routes, and
Vproj ¼ Vpred þ 20 km=h for straight-line routes,
For two-way roads with a single carriageway, where Vpred < Vdoz
Max Vproj ¼ Vdoz for a particular category of road, or max Vproj ¼ Vpred.
The difference between the design speed and anticipated speed must not exceed
20 km/h. If the difference is:
VprojVpred  20 km=h,
It is necessary to check the adopted levels of anticipated speed and increase them,
or reduce the design speed by correcting the route in order to have the difference be
within the scope as follows:
VprojVpred  20 km=h
2.2.2
Lateral Speed
Lateral speed is a speed while switching trafﬁc lanes. It depends on the width of trafﬁc
lanes, speed of driving and route stream, straight lines—curves.
The following factors are in play:
Mild lateral speed 0.7 m/s for Vv > 70 km/h, for heavy vehicles and curved
roadways, and
Acceptable lateral speed 10 m/s for Vv  70 km/h, for passenger vehicles and
straight-line roadways.
2.3
Speed Measurement/Recording
Speed recording may be performed by using following tools:
• Moving trafﬁc-monitoring vehicle
It is used to determine the speed recording route. The co-driver writes down into the
log/form the following data: route, direction of driving, weather conditions, day,
month of recording, initial recording, initial mileage, time of departure and time of
arrival, number of vehicles from the opposite direction under the PA, BUS, TV, AV
structure, number of vehicles overtaken by the trafﬁc-monitoring vehicle, number of
vehicles that overtook the trafﬁc-monitoring vehicle. It is based on these data that
one determines relevant indicators; stream speed and trafﬁc ﬂow.
474
M. Bublin

• Video camera
A camera is set vertically to the roadway at a distance of 15–20 m. Since video
recording made by a camera is not expressed in speed-characteristic units (km/h or
m/s), a pixel (video recording units) measuring unit conversion must be made to
eventually arrive at a speed unit. In order to make unit conversion, it is necessary to
know current relationship between the recording and a known length in the
recording.
• Counter
Automatic counters may record speed per thresholds, as follows; lower than
60 km/h, 60–80 km/h and higher than 80 km/h.
• Radar
Radar speed recording is most commonly used during road speed controls.
3
Speed Determination Procedures
3.1
Design Phase
Design speed Vpro is determined by analyzing road elements, situation plan and lon-
gitudinal proﬁle. The set speed must not exceed the maximum permissible speed on the
given roadway.
The basic road features that affect design speed are as follows: curve radius, lon-
gitudinal slope, trafﬁc lane width, tonnage and surface construction features, road
warping, presence of sight distance in excess of 450 m.
3.2
Road Safety
Road safety check is performed through geometry rectiﬁcation, situation plan and
longitudinal road proﬁle conformity analysis, surface water drainage, analysis of
capacity, speed, uncertainty factors, and speed model analysis (driving dynamics test)
and design geometry dimensioning. The largest number of trafﬁc accidents occur
exactly because of a superposition of multiple factors, including road-related factors,
such as unfavorable horizontal and vertical elements and spatial road features, road
surfacing features, drainage solutions, lighting, environment etc.
It is by checking all designed elements that one arrives at the extent of speed from
the safety aspect.
3.3
Speed According to Trafﬁc Signalization
The state of speed on roads in use is determined according to signalization. Apart from
the foregoing factors, the state of speed is also largely affected by road environment
factors, roadside development, connections, and especially the impact of local com-
munities and police, all of which affect speed limits signiﬁcantly. Figure 3 displays
speed according to trafﬁc signalization conditions on a main (trunk) road section.
Under the law, the maximum speed on main (trunk) roads (magistralne ceste) is
80 km/h, and minimal 60 km/h.
A Study of Speed on Two-Lane Roadways
475

Speed recording on main (trunk) roads has shown that in a large number of cases
there existed considerable speed limits, even down to 40 km/h or 50 km/h, without any
meaningful logical explanation.
3.4
Trafﬁc Flow Speed on Roads in Use
Studies of trafﬁc ﬂow speed on roads in use have been conducted by measuring speed
using
the
method
of
a
moving
trafﬁc-monitoring
vehicle,
camera
or
automatic-counter-determined speed.
The method used for determining speed was multivariate regression analysis.
Following are some of the studies conducted in our region:
D. Dmjanović tried to determine the impact of road elements on a trafﬁc ﬂow speed,
and came up with a joint effect equation [3]:
V = 34,37 log R þ 15,42 s.  0,029 Kn0,038 K47,80
ð7Þ
where are:
R
horizontal curve radius
S
trafﬁc lane width
Kn
speciﬁc curve turns
K
road section curvature
with the author’s note that satisfactory sight distance is required along the entire road
section.
B. Mazić et al. have come up with a correlation equation, which reads as follows
[3]:
Vt ¼ 67;71  6;63 R  2;39 KT þ 4;39 S  1;37 N  0;04 K
ð8Þ
where are:
R
technical road class
KT
terrain category
Š
carriageway width
Fig. 3. Speed according to trafﬁc signalization and conditions [2]
476
M. Bublin

N
average slope (%) and
KV
share of commercial vehicles BUS + TV + AV (veh/h)
R
0.82
Vehicle speed dependent on road elements is as follows [3]:
Up ¼ 65;89  6;632 R  2;39 KT þ 4;39 S  1;37 N;
ð9Þ
For cargo vehicles, the relevant correlation equation reads as follows [3]:
Vt ¼ 52;14 þ 2;92 Sk  0;059 Su  2;94 Un;
ð10Þ
R = 0,82
where are:
Vt
average speed of heavy cargo vehicles + bus (km/h)
Šk
carriageway width (m)
Su
deviation angle (0) and
Un
longitudinal slope (%)
For the needs of Ljubljana region development, there is a correlation equation that
reads as follows:
V ¼ 6;82 þ 8;9 S  62;85 D;
ð11Þ
Fig. 4. Basic trafﬁc ﬂow diagram for various heavy vehicle shares [5]
A Study of Speed on Two-Lane Roadways
477

R = 0,765
where are:
V
speed (km/h)
Š
road width (m)
D
roadside development percentage
The equation has been used for the anticipated speed and travel time from one zone
to another while testing various road network variants.
Ivan Lovrić has studied trafﬁc ﬂow speed models on out-of-town two-lane road-
ways, and came up with a trafﬁc ﬂow diagram for various heavy vehicle shares, as
shown in Fig. 4 [4].
4
Conclusions
The analysis of determining speed has pointed to the complexity of speed generation on
two-lane roadways.
In relation to the designed speed, signiﬁcant changes take place especially due to
trafﬁc accidents and spontaneous interventions in the road-gravitating area.
Road rehabilitation should start from an analysis of conditions for the existing state
of speed and the possibility of bring the speed down to the legally set level by affecting
a change of road parameters and hindering factors that emerged in the road-gravitating
area.
This is why back-calculation analyses are important before initiating road reha-
bilitation projects, and speed is one of the most signiﬁcant effects achieved by road
rehabilitation. Given a rather large pressure exerted by external factors in our condi-
tions (roadside construction, connection/access roads, presence of various facilities,
lobbying by local communities, inﬂuence of the police…), it is necessary to draft a
spatial plan for the special area of main (trunk) roads as a legal framework for regu-
lating interventions into the given road strip.
References
1. Mehmed, B.: Funkcionalne karakteristike saobraćajnica, Univerzitet u Sarajevu, Građevinski
fakultet, Sarajevo (2013)
2. Mehmed, B.: sa saradnicima: Metodologija za rangiranje prioriteta intervencija na magistral-
nim cestama Federacije BiH, Univerzitet u Sarajevu, Građevinski fakultet, Sarajevo (2009)
3. Džebo, S., Pozder, M., Mazić, B., Mandić, A.: Brzina vozila na dvotračnim putevima. In:
Second Serbian Road Congress, Beograd. Srpsko društvo za puteve “Via-Vita”, Beograd
(2016)
4. Lovrić, I.: Model brzina prometnog toka izvangradskih dvotračnih cesta. Doktorska
disertacija, Građevinski fakultet, Sveučilište u Mostaru, Mostar (2006)
478
M. Bublin

Adriatic-Ionian Road: Economic and Social
Potential of the Adriatic-Ionian Region
Predrag Sarkinovic1(&) and Naser Morina2
1 Civil Engineering, IPSA Institute, Sarajevo, Bosnia and Herzegovina
predrag.sarkinovic@ipsa-institut.com
2 Civil Engineering, Gnjilane, Republic of Kosovo
morina.n@hotmail.com
Abstract. General information about the Adriatic-Ionian road and its corridor
are presented in the paper. Furthermore, the impact of the road on the economic
and social development of a part of the southeast and the entire BiH by con-
necting it to the neighbouring countries is signiﬁcant, as explained in the text.
Keywords: Adriatic-Ionian region  Adriatic-Ionian route  Bosnia and
Herzegovina  Connecting the region  Social and economic development
1
Introduction
The route of the Adriatic-Ionian road is located along the shores of the Adriatic and
Ionian seas. The total length of the route from Trieste in Italy to Athens in Greece
would approximately measure 1100 km. Geographically, the route is situated along the
eastern coast of the Adriatic and Ionian seas—from Trieste in Italy to Athens in Greece.
The route is to be placed within the planned corridor: Trieste—Rijeka—Split—
Dubrovnik—Bar—Durres—Athens. The route traverses 7 countries: Italy (10 km),
Slovenia (30 km), Croatia (460 km), Bosnia and Herzegovina (90 km), Serbia and
Montenegro (120 km), Albania (370 km) and Greece (20 km).
The Adriatic-Ionian road is important for all the countries it traverses:
• Italy—it
is
a
continuation
of
the
Milan—Venice—Trieste
(Serrenissima)
mottorway.
• Slovenija—a relatively short section but it gains importance when combined with
the Ljubljana—Rijeka road and when it connects the Koper port and Rijeka.
• Croatia—since the major part of the Adriatic-Ionian route overlaps with the
Zagreb-Split motorway, Croatia’s position would be quite favourable if construc-
tion goes ahead, because the largest section of the route is already completed,
• Bosnia and Herzegovina—it would connect to Corridor Vc and stretch to the
border with Montenegro,
• Albania—this route would have a major impact on social development.
The position of the road is very favourable because all the Mediterranean countries
are located along the eastern coast of the Adriatic Sea. The motorway would connect
the south of the Balkans with Western Europe (Figs. 1 and 2).
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_43

Fig. 1. Adriatic-Ionian region [1]
Fig. 2. Adriatic-Ionian route [2]
480
P. Sarkinovic and N. Morina

2
Adriatic-Ionian Motorway Route Through BiH
From the perspective of Bosnia and Herzegovina, this road is important. The vicinity of
the Ploče Port (CRO), the Bar Port (MN) and Dubrovnik, via the Adriatic-Ionian road
and Corridor Vc Budapest–Osijek–Sarajevo, will surely contribute to the economic
development of the country and the area along the road.
Currently, the construction of Corridor Vc is in full swing in Bosnia and Herze-
govina. The route and the interchange where Corridor Vc and the Adriatic-Ionian road
will connect, are included in the Spatial Plan (Fig. 3).
According to the Spatial Plan, the Adriatic-Ionian road should connect to Corridor
Vc in the Čapljina area. The route continues further to the southeast towards Mon-
tenegro. There are three major towns in Bosnia and Herzegovina that will greatly
beneﬁt from the route. The towns are Stolac, Ljubinje and Trebinje. The route must
pass adjacent to the said towns. Neum, the only costal town which has a good con-
nection with Stolac, will thus also be connected to the Adriatic-Ionian road and Cor-
ridor Vc (Fig. 4).
All transport, economic and tourist potential coming from Western Europe and
Eastern Europe (Corridor Vc) would use this route to reach the Montenegro coast,
Albania and Greece. The route traverses one of the major agricultural regions which
supply most of BiH with produce. Such regions include the Ljubinje ﬁeld and Popovo
ﬁeld—major sources of produce for BiH. Several alternative routes in the said area
should of course be developed and the most favourable one chosen through
multi-criteria evaluation. Economic, spatial and social analyses should be considered.
Fig. 3. Planned roads corridor through BiH [3]
Adriatic-Ionian Road: Economic and Social Potential
481

3
Road Proﬁle
An analysis will show which road proﬁle would be the most cost-effective one.
The route may have standard motorway elements:
• Speed: 120 km/h,
• Max slope 4%,
• Lanes: 2  (2  3.75) + emergency lane 2.5 m,
• Central reservation 4 m.
Or it may have the properties of a road for motor vehicles:
• Speed: 100 km/h,
• Max slope 4% (5%),
• Lanes 2  (2  3.5) with no emergency lane,
• Central reservation 4 m.
One of the above two proﬁle will have to be chosen. The planned trafﬁc will surely
justify the investment and yield results.
The existing spatial constraints are as follows:
• Relief of the area with alternating mountainous and ﬂat terrain,
• Existing water courses and conﬂict points with them,
• Railway line,
• Existing settlements (smaller and larger-Stolac, Ljubinje, Trebinje),
• Connection to other areas.
Fig. 4. Planned connection between Adriatic-Ioninan route and corridor Vc [4]
482
P. Sarkinovic and N. Morina

4
Connection to the Surrounding Area to Which the Route
Naturally Gravitates
The route will have a huge impact on the development of surrounding areas. Such areas
primarily include:
• Trebinje, Ljubinje, Stolac,
• Neum,
• Dubrovnik and its surroundings,
• Montenegro coast.
Namely, via its connection with Corridor Vc Budapest–Osijek–Sarajevo–Ploče,
this road would attract heavy trafﬁc form Eastern Europe, while also ﬁnally solving the
matter of connecting Croatian Dalmatia with the south of Croatia (unless the Pelješac
Bridge is constructed). The inﬂuence of Dubrovnik and its surroundings in the tourist
sector is well-known, however, the inﬂuence of Neum should not be ignored and this
road would contribute to its prosperity. The existing Trebinje – Dubrovnik road which
crosses the Ivanica border should be modernised and a new border crossing should be
constructed, whereby the issue of ensuring a connection with Dubrovnik would be
solved. The construction of the Stolac–Neum road is underway, hence the Stolac
interchange would have its function in terms of a connection with Neum (most
importantly, Neum would be have a good road connection with central Bosnia).
5
Conclusion
It is important to say that this route does not cover only road transport, but also rail and
air transport. Interestingly, the air trafﬁc that is expensive to maintain, is much better
developed than road and rail trafﬁc. But that is a great advantage and signiﬁcant cost
savings but also a paradox. It should be stressed that this Corridor will be prosperous
and have a speciﬁc role in the future. Due to its unstable political situation, the future of
the entire region lies in its inter-connection and economic strengthening. Until such
time as this corridor is constructed, most of these areas will be isolated from the rest of
Europe.
References
1. http://www.adriatic-ionian.eu/about/the-adriatic-ionian-region
2. http://www.adriatic-ionian.eu/about/the-adriatic-ionian-region
3. http://www.jpautoceste.ba/
4. Main design motorrway on corridor Vc, section Pocitelj-Zvirovici, IPSA Institute, Sarajevo
(2011)
Adriatic-Ionian Road: Economic and Social Potential
483

Browninfo Methodology and Software
for Development of Interactive Brownﬁeld
Databases
Tijana Vujičić1(&), Dijana Simonović1, Aleksandra Đukić2,
and Maksim Šestić3
1 Faculty of Architecture, Civil Engineering and Geodesy, Department of Urban
Planning, University of Banja Luka, Banja Luka, Bosnia and Herzegovina
{tijana.vujicic,dijana.simonovic}@aggf.unibl.org
2 Faculty of Architecture, Department of Urban Planning, University of
Belgrade, Belgrade, Serbia
adjukic@rcub.bg.ac.rs
3 Maksim Šestić, INOVA Informatički Inžinjering, Ltd., Vidovdanska 2, 78 000
Banja Luka, Bosnia and Herzegovina
max@geoinova.com
Abstract. Brownﬁelds represent abandoned and disused sites, an exceptional
strategic
reserve
of
space
and
urban
resource
whose
potentials
are
under-recognised in Bosnia and Herzegovina. The initial step towards urban
renewal of these spaces is: their identiﬁcation, evaluation, assessment of
development potential and promotion. Interactive, GIS and web oriented digital
database of brownﬁelds is very useful tool for its promotion and visibility that
makes process of brownﬁeld regeneration more efﬁcient and faster. Therefore,
the paper presents the unique methodology for development of interactive
brownﬁeld databases and bundled software BrownInfo.
Keywords: Brownﬁelds  Database  Urban renewal
1
Introduction
Confronting again the problems found in the contemporary city in general, that is,
posing questions about the forms and limits of urban sprawl and the ways in which it
can be addressed, this paper opens a debate on contemporary trends of urban devel-
opment and regeneration, with a special emphasis on brownﬁeld sites. Although the
term brownﬁeld is nowadays widely used, it does not have a single widely recognised
deﬁnition. The most quoted deﬁnition of brownﬁelds was given by a research network
CABERNET (Concerted Action on Brownﬁeld and Economic Regeneration) according
to which this term implies surfaces that: (1) have been affected by former uses of the
site or surrounding land; (2) are derelict or underused; (3) are mainly fully or partly
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_44

developed urban areas; (4) require intervention to bring them back to beneﬁcial use and
(5) may have real or perceived contamination problems [1]. In principle, previous
deﬁnition determines the meaning of brownﬁeld sites. However, it is necessary to point
out that their deﬁnition is determined by complex local problem characteristics. Factors
of emergence, forms, patterns and dynamics of their transformation are conditioned by
local social, economic, political, cultural and historical context.
Brownﬁeld regeneration has been the focus of the urban debate, both theoretical
and practical, since the 1980s. A number of European countries and cities began the
process of rebuilding and revitalising their brownﬁeld sites at different times in the past,
while countries of the former Eastern Bloc have dealt with the problem of brownﬁeld
regeneration only since 2000. One characteristic of the approach to this problem is that
various methods have been used, which implies different approaches, those systemic,
which address invidual cases as the ﬁnal phase, as well as those concerned exclusively
with individual cases (top-down and bottom-up). Some developed countries have used
special incentives to encourage investors to invest in brownﬁelds. States or cities have
special programmes, projects and funds for the recycling of brownﬁeld sites. At the
same time, the procedures for obtaining the necessary documents and permits for
brownﬁeld regeneration have been simpliﬁed. In some cases, investors are exempt from
paying taxes, or they are sold or rented land at lower prices (sometimes free of charge),
on the condition the brownﬁeld is ﬁrst cleaned and put in good shape, a process which
includes decontamination.
In Bosnia and Herzegovina (B&H) brownﬁeld sites are not sufﬁciently recognised
as potentials for (urban) renewal and sustainable development, there is no systematic
approach towards solving problems neither at national nor at local level [2]. Therefore,
recognising these spaces as signiﬁcant resources, this paper promotes an ‘integrated
approach to the regeneration of brownﬁelds’ [3], offering an answer to the problem of
uncontrolled exploitation of urban resources, more precisely, deﬁning the new
methodology for its inventarisation. The proposed methods of collection, systemati-
sation, storage, presentation and update of brownﬁelds’ data (BrownInfo) represent an
interactive brownﬁeld database as a tool to increase the effectiveness of the planning
and management of spatial development, i.e. a tool which allows the continuous
monitoring of changes in space, thus contributing to the promotion of the principles of
sustainable development. ‘Integrated approach’ allows the reactivation of abandoned
urban construction land and the ﬁltering of the existing built structure of towns and
cities
on
the
basis
of
their
comparison
and
identiﬁcation
of
priorities
for
decision-making, in a manner which is especially sensitive to their social and cultural
impact.
Brownﬁeld regeneration encourages sustainable urban spatial development based
on the efﬁcient and rational use of urban spatial resources and helps check urban sprawl
as a major threat to greenﬁeld resources. In addition, an integrated approach ensures the
integration and systematisation of the major potentials of brownﬁelds (physical, social,
cultural, environmental and economic), while drawing attention to their importance for
the urban community. An important segment in brownﬁeld reactivation is making
efforts to awaken awareness of the importance and necessity of the process, especially
Browninfo Methodology and Software for Development
485

at the institutional level, as well as among the economic community. Brownﬁeld sites
need to be inventoried in order to be properly evaluated. A single brownﬁeld database
can facilitate the process of urban planning, allow the introduction of new forms of
informal planning and thus make the management of urban development more efﬁcient.
2
BrownInfo International Scientiﬁc Research Project
BrownInfo—Methodological Framework for Development of Brownﬁeld Databases is
a name of one-year scientiﬁc research project conducted by the Faculty of Architecture,
Civil Engineering and Geodesy, University of Banja Luka, in 2014 in cooperation with
national, regional and EU partners, and a total of 160 researchers from 12 countries
took part in its implementation [2, 4]. The institutions that participated in the project
came from a range of domains of professional activity: academic, expert and admin-
istrative—universities, research institutes, professional organisations, local develop-
ment agencies, local self-administration departments, and different actors from the
economic sector.
The subject of this research is a special category of urban space—brownﬁelds.
They are recognised as previously developed and/or built on areas which have inefﬁ-
cient utilisation and potential for urban renewal and sustainable development (Fig. 1)
[2]. In Bosnia and Herzegovina phenomenon of brownﬁeld spaces is not explored. The
problem and the potential of these spaces are not recognised, there is no ofﬁcial
deﬁnition or categorisation, and there is no clear vision of their regeneration neither at
national nor at local level [2, 3, 5]. Social, cultural, ecological and economic potentials
of brownﬁelds indicate importance of these spaces for urban community and necessity
for their regeneration. Brownﬁelds are outstanding strategic reserve and potential of
cities [6]. The initial step towards urban renewal of brownﬁelds is their identiﬁcation,
multicriteria evaluation of the existing situation, classiﬁcation, assessment of devel-
opment potential, and ﬁnally, its presentation with the aim to promote and raise
awareness about the problem and potential of neglected area [4]. Digital database have
the most important role in this process and can be considered as useful tool for
brownﬁeld promotion and visibility. It is important to build up consistent and effective
databases. Process of database creation needs a common methodology, in order to
make data compatible, useful and comparable. Apart from making the process of
brownﬁeld regeneration more efﬁcient, brownﬁeld databases also make governance and
management of brownﬁelds easier. In that context, this study supported the thesis that
the application of tested methodologies for exploring derelict and underused spaces,
applying and recommending by professional/scientiﬁc association and their adaptation
to speciﬁc local urban context, is a starting point and certain way for designing a
methodological apparatus for research of brownﬁeld spaces. Such a methodology,
which is harmonised with valid international conventions and practices and which aims
to survey and monitor derelict and underused spaces, would have the generally
applicable characteristics and would be a segment of a universal methodological
apparatus for construction of uniﬁed land information system. The construction and
testing of such a methodological framework is the focus of this project.
486
T. Vujičić et al.

This research was initiated with the aim of deﬁning universal standards of identi-
ﬁcation, inventory and classiﬁcation of brownﬁeld sites, in order to devise sets of
effective methods for the collection, systematisation, presentation and storage of
brownﬁeld-related data, to be used in future as a basis for decisions about starting
brownﬁeld regeneration projects. The main objectives of the BrownInfo research
project were: (1) devising methods and special tools for the development and estab-
lishment of a brownﬁeld database; (2) building a platform for future projects in the ﬁeld
of brownﬁeld regeneration on the territory of B&H and its entities; (3) forming and
developing research networks between countries in the region and wider Europe area in
order to enhance the possibility of future cooperation in the ﬁeld of brownﬁeld
regeneration; (4) educating the management and all potential participants/stakeholders
about the importance of brownﬁeld regeneration and the possibility of their future
development. The key outcomes of the BrownInfo project are: (1) a set of integrative
methods for establishing a brownﬁeld database—BrownInfo [2, 4]; (2) the BrownInfo
software package, speciﬁcally tailored to the BrownInfo set of methods, which enables
the creation of interactive brownﬁeld databases [2, 8]; (3) a scientiﬁc monograph,
which presents the results of the theoretical and applied investigations under the project
in the form of structured research and review papers [3]; (4) a manual intended for all
those directly or indirectly involved in spatial planning and design, as well as urban
brownﬁeld regeneration, offering speciﬁc guidance on the application of the devised
methods. It is intended for such key target groups as local administrative urban and
spatial planning services, development agencies, Land Registry, planning institutions
[2]; (5) the international academic and professional symposium ‘International Aca-
demic Conference BrownInfo 2014’ and the symposium proceedings [9]; (6) the
BrownInfo Pilot Project of the Banja Luka Business Zone (former Incel Industries) [8];
(7) a survey conducted among representatives of local administration departments,
development agencies and planning institutions of the Republic of Srpska (entity of
B&H), in order to determine the ʻvisibility’ of brownﬁelds in the context of urban
Fig. 1. Power plant in former industrial complex Incel in Banja Luka (B&H) [7]
Browninfo Methodology and Software for Development
487

management in the Republic of Srpska (RS) and to estimate the need for the estab-
lishment of a brownﬁeld information platform and the status of human and technical
resources that can be mobilised in its application [5]; (8) a survey conducted among
representatives of the business sector, which assessed their perception of brownﬁeld
sites for the goal of estimating the need for the establishment of a brownﬁeld infor-
mation platform and learning about the speciﬁc requirements of investors in relation to
the database contents [10].
3
Brownﬁeld Challenges
The transformation which the B&H has been going through, the restructuring of its
industry and reorganisation of its production, which also includes its public services,
have resulted in a large number of brownﬁeld sites, some of which are also in envi-
ronmentally unsustainable condition. As well as that, when it comes to the RS, the
phenomenon of brownﬁelds has not been sufﬁciently explored or their potential
carefully evaluated; no ofﬁcial deﬁnition and classiﬁcation are in use, and no single
formal approach to this rather complex problem is used by the national and local
authorities [3]. The professionals and academics working in the department of urban
planning and design recognise brownﬁeld sites as previously developed and/or
space/land built on which is evidently inefﬁciently used, but which has potential for
urban renewal and sustainable development [2, 3]. The social, cultural, environmental
and economic potential inherent to brownﬁelds indicates their importance for the urban
community and points out the necessity of their regeneration [11]. These abandoned
and disused sites represent an exceptional strategic reserve of space and urban resource
[6].
The ﬁrst step toward the reactivation of this resource concerns the formulation of a
strategy of brownﬁeld regeneration, integrated into the network of spatial regeneration
strategies of municipalities, regions and entities [6]. As an alternative to a society that
continuously grows and heavily depletes its natural resources, brownﬁeld reactivation
is an economical solution that supports sustainable urban development based on a more
efﬁcient exploitation of urban spatial resources. An essential prerequisite for the efﬁ-
cient management of spatial resources in the RS is the establishment of a single register
of brownﬁeld property. The compilation of such a register invariably includes identi-
fying, inventorying and evaluating the condition of brownﬁelds according to a number
of criteria. It will also allow their classiﬁcation, assessment of their development
potential and, ﬁnally, presentation in the media which meet the needs of modern
communication and information exchange [3].
The revitalisation of brownﬁelds in the RS is secondary to the aspirations of local
communities for economic recovery. The need to increase relative economic compet-
itiveness leads to rivalry between cities and municipalities, from which they emerge as
ʻwinners or losers,’ which further results in uneven spatial development [3]. In the
context of such a categorisation of municipalities and cities, the status of brownﬁelds is
best shown by Danilović et al., who states that there are three categories of munici-
palities, namely, (a) municipalities situated in commercial locations, close to the main
trafﬁc and utility infrastructure; brownﬁelds located in these municipalities stand a
488
T. Vujičić et al.

good chance of reuse; (b) municipalities located outside the main development axes,
but with enough economic development or other potential; in this case, it is necessary
to respond quickly, in order to reap the beneﬁts of brownﬁeld reuse as soon as possible
and make the municipality in question the economic leader among its neighbours; and
(c) municipalities that do not ﬁt into either of the previous two categories, which are
hardly likely to solve their economic problems and cannot hope for private investment,
but must rely on national and international funding schemes and pursue intelligent and
unconventional solutions instead [12].
In a situation where there is no clear or coherent brownﬁeld strategy at the regional
and national level, the fact that municipalities also lack in a clear vision and attitude
regarding the problem of the abandoned and underused facilities and land on their
territory is more than aggravating, since they suffer the most because of their existence
and disuse [5]. Additional instruments are needed for municipalities to manage their
territorial development, including the revitalisation of brownﬁelds, since those cur-
rently used are insufﬁcient. When local administrations enter the process of brownﬁeld
regeneration, they ﬁnd themselves trapped between the interests of market participants
on the one hand and the demands of higher authorities (entity/state), since brownﬁeld
management falls under their jurisdiction [5]. Property owners often make decisions
about investment guided by their own interests, and their ideas on how to bring a
brownﬁeld back into use is frequently a far cry from those of the local community [10].
Also, a chief obstacle to brownﬁeld reactivation is an unresolved ownership structure
or history.
In addition, there is also the problem of brownﬁeld inventory and mapping; as a
rule, municipalities do not have all the measurable data relevant for brownﬁelds [5],
which would allow data comparison between cities and municipalities for the purpose
of realising the nature and estimating the extent of the problems present, and possibly
their joint resolution. Municipalities can not make plans or estimates and predict the
effects of brownﬁeld reuse without a single brownﬁeld database or uniform standards
of data collection and systematisation. Also, if a common methodological framework is
not established, the data found in nominally identical documents will hardly be
comparable.
The inherited system of data collection, systematisation and storage is based on the
concepts and requirements as imposed by the rational/territorial system of spatial
planning, with the presentation of analogue data, ‘scattered’ and kept at different
places, as the main medium used by system. Most often they are where they are created
or in the possession of those for whom they are or were created. This situation results in
little access to information, and quite often in a lack of transparency. In general,
brownﬁeld data are not updated, and if they are, they are not available for general use,
which further excludes them from the category of commercially available urban space.
Their reuse is thus impossible or is additionally delayed, which also further complicates
the process of development of planning documents and prolongs their adoption. The
damages and losses made by such a disorganised system are tremendous and affect all
spheres of life of communities. In the context of transformation with the goal of
transition
to
a
market-oriented
economy,
the
emergence
of
the
concept
of
collaborative/strategic planning and the era of digitisation, there is a growing need to
Browninfo Methodology and Software for Development
489

innovate the collection and presentation of information in order to optimise access,
ﬂow and transparency, as well as increase the efﬁciency of use of spatial resources [3].
Key problems and challenges that brownﬁeld sites deal with, and have been
recognised through the research are [2, 3]:
• Existing stereotypes of brownﬁelds as unproﬁtable investment compared to
greenﬁeld locations and as inhibitors, rather than catalysts for local develop-
ment; ignoring the problems and spatial potentials of brownﬁeld locations at
national and local level of space management and seeing them as the inherited
burden avoided to be solved; advancement of such investment development model
that emphasises the development of greenﬁeld sites with the irrational expansion of
its territory and non-rational neglect of existing regional resources of narrow city
areas.
• Lack of development strategies, plans, programmes and projects focused on the
problem of brownﬁelds at national, regional and local level which leads to reckless
and partial activation of brownﬁelds; lack of regulatory role of the state in the
process of brownﬁeld regeneration; market actors and mechanisms that act as
regulators and starters of inventory, presentation and availability of brownﬁeld
areas.
• Lack of unique brownﬁeld database at the level of BiH and/or its entities;
unavailability of information on the space available for development and investment
(brownﬁeld and greenﬁeld), which is reﬂected in complex procedures of local
governments; the process of mapping based on the principle of partial inventory of
abandoned and unused spaces on different levels of the state, entity and local
governments as well as different sectors.
• Lack of unique methodological framework for creating databases of abandoned and
unused spaces; lack of uniformity of the database contents, partial sectoral approach
to mapping and data incompatibility; lag in relation to the expected standards of
modern age regarding the application of information and communication system in
the process of more efﬁcient management of information and space.
Solving the problem of brownﬁeld sites in RS is secondary to the processes directed
toward an economic recovery and consolidation at the state, entity and local level. The
activities of the key entity institutions with jurisdiction over the issue of brownﬁelds are
characterised by a fragmented approach, i.e. by their focusing on those aspects of the
problem that are within their jurisdiction [3]. What is missing is an integrated
cross-sectoral approach, which would enable a thorough and profound understanding
of the causes, consequences and impact of brownﬁelds on local communities, as well as
the listing of the many potential of brownﬁeld sites [5]. At the level of RS, several
individual initiatives that stand out are directly or indirectly related to brownﬁeld sites
which will be discussed further in the text.
A related project worth singling out implemented by an RS institution is the project
of inventory of the available production facilities of the Ministry of Industry, Energy
and Mining, whose goal was to better match the supply of and demand for production
facilities in the RS, i.e. facilitate the identiﬁcation of available sites/facilities by
potential investors interested in starting production and opening new jobs. The project
results are available at the web portal of the Ministry, and the database is built and
490
T. Vujičić et al.

updated through property owners registering individual sites or facilities as the
‘available production space’ [13].
In the context of this research, the most important project is the establishment of the
Republic of Srpska Spatial Information System (SISRS), implemented by the Ministry
of Spatial Planning, Civil Engineering and Ecology in cooperation with the World
Bank. The initial phase of the project, the system establishment, lasted from 2009 to
2011, and the system currently contains the Republic of Srpska Spatial Plan 2015.
Created according INSPIRE directive [14, 15] SISRS aimed to create a spatial data
infrastructure, in order to enable the exchange of environment-related spatial infor-
mation between public institutions and to ensure better public access to spatial infor-
mation within entity as well as around Europe. As such, SISRS is in line with the latest
standards in this ﬁeld which laid the foundations for its further development and
implementation. In this regard, RS is a step ahead of the other countries in the region
because its Spatial Information System was created in agreement with these standards.
However, one disadvantage of the SISRS is its limited accessibility due to a compli-
cated access granting procedure. Also, the system maintenance is underfunded, which
affects the quality and pace of update activities. Moreover, there has been a problem
with the division of competences in regard to the system management between the
competent institutions, the Ministry of Spatial Planning, Civil Engineering and Ecology
and the RS Administration for Geodetic and Property Affairs, manifesting as limited
and insufﬁcient exchange of information between them [2, 3]. Finally, both local
administrations and state institutions in the RS are either understaffed, or the current
staff is inadequately computer literate to create, update and share compatible spatial
data. The contribution of state and local administration employees to updating the
SISRS is also inadequate and insufﬁcient. Nevertheless, the SISRS is an extremely
important tool for the efﬁcient management of spatial development, and it is absolutely
necessary to invest in its development in the future.
Of impact on the solution of the problem of brownﬁelds, in terms of the legislation
adopted by local administrations, is the regional programme for the certiﬁcation of
municipalities and cities as places with a favourable business environment, Business
Friendly Cities [16].1 Business Friendly Certiﬁcation (BFC), implemented by Serbian
National Alliance for Local Economic Development (NALED) in South East Europe
(SEE), is a unique programme for improving the quality of services and information that
municipalities in the SEE region offer to companies. Since 2012, more than 60 munici-
palities of the SEE region have joined the BFC SEE Programme—26 of them being
certiﬁedas business-friendly, 10 of which are in BiH. Currently, another 14municipalities
in BiH are at different stages of the process of certiﬁcation. Municipalities’ interest in
1 The methods of the programme of certiﬁcation of cities and municipalities as business-friendly were
developed by Serbia’s National Alliance for Local Economic Development (NALED). The
certiﬁcation of municipalities and cities as business-friendly is a complex process that aims to
promote the work and decisions of local administrations, increase the quality of information and the
speed and quality of services provided by local government institutions to entrepreneurs in order to
create favourable conditions for the survival and growth of industry, as well as a more attractive
environment for direct investment. BFC SEE has been implemented since 2012 in Bosnia and
Herzegovina, Croatia, Macedonia, Serbia, and since 2016 in Montenegro.
Browninfo Methodology and Software for Development
491

becoming involved in BFC programme indicates that this will be an important trend in the
future. According to BFC criteria, very inﬂuential for achieving a favourable business
environment is the criterion of the existence of a database on brownﬁeld sites. In this
regard, BrownInfo methodological apparatus for the inventory of brownﬁeld sites can
facilitate and speed up the mapping process, and a unique database would allow the
comparability of data between different sites, municipalities and cities.
4
BrownInfo Methodology
The methodology for creating an interactive database of brownﬁelds—BrownInfo—is
universal instrument for efﬁcient mapping and presentation of derelict and unused
space in municipalities and cities. It is intended for institutions dealing with economic
development and the management of spatial resources at the local and national
level. Key target groups—consumers of BrownInfo methodology and software package
at the local level are administrative departments for urban and spatial planning,
development agencies, geodetic administration and planning institutions, while key
potential agents for establishing national databases of brownﬁeld sites are the Ministry
of Spatial Planning, Civil Engineering and Ecology, Chamber of Commerce, Republic
Administration
for
Geodetic
and
Legal-Property
Affairs
and
the
Investment-
Development Bank.
BrownInfo methodology offers ready-made tools for identiﬁcation and compre-
hensive evaluation of brownﬁeld sites and as such should help local communities [2]:
• to estimate the size of neglected and unused previously developed and built on sites
in the territory of their municipalities and cities, to estimate the damage and losses
stemming from their neglect and underutilisation as well as to identify multiple
potential that these areas possess
• to plan effectively, on the basis of comprehensive information on brownﬁeld areas,
strategic development and reconstruction of cities based on the principles of
sustainability
• to promote brownﬁelds as sites favourable for investment and development
• to make the information on open spaces, available for future development and
investment within the brownﬁeld zone, easily accessible to the public through a
web-oriented database.
The aim of mapping, by the means of BrownInfo methodology, is to collect all the
reference data on neglected and underutilised spaces that are often scattered through-
out various departments of local and state administration ofﬁces or are possessed only
by property owners.
4.1
Spatial Levels and Aspects of Brownﬁeld Analysis
Key components of BrownInfo methodology, that are complement and interconnected,
are aspects of analysis and spatial levels of brownﬁeld analysis. The presentation of
these areas can be carried out through two basic media: catalogue sheets and interactive
database [2].
492
T. Vujičić et al.

BrownInfo methodology, as the basic ‘spatial level of analysis’, recognises urban
micro elements: building, parcel and complex (Fig. 2). And the following speciﬁc
levels of analysis stand out: corridor level, level of trafﬁc facilities and level of
hydraulic structures and systems. A very important level of brownﬁeld analysis,
according to BrownInfo methodology, is ‘the level of building’. Given that, by deﬁ-
nition, brownﬁeld sites represent previously developed and/or built on areas, most often
buildings are essential and most important resources of brownﬁelds. Therefore, they
stand as the lowest and very important level of analysis.
BrownInfo methodology is conceived as a comprehensive analytical model which
integrates key aspects of the analysis of brownﬁeld sites (Fig. 2). General, regulatory,
economic, spatial, ecological, cultural and social aspects completely deﬁne issues of a
speciﬁc brownﬁeld site and also allow a comprehensive considering of its resources
and potentials [2]. Aspects can be processed and considered separately and inte-
grally. In this way, a multi-sectoral approach when gathering information in the pro-
cess of mapping is ensured, and the users are allowed to access the data in reference to
their domain of activity and interest. Integrated consideration through all the aspects
contributes to the creation of a comprehensive picture of the speciﬁc brownﬁeld site
and is usually required in cases of claims of locations for investment. Also, an inte-
grated approach to the analysis at the macro level (municipality, region and entity)
enables the creation of a comprehensive information base—a base for producing
development programmes and plans. Aspects are successively processed for each level
individually, whereby the level of reasonableness and analysis elaborateness of certain
aspects grow as spatial levels decrease. Thus, for buildings the most comprehensive
data sheet is deﬁned, while for the level of the complex, the level of elaborateness
decreases signiﬁcantly. Mapping on the ‘bottom-up’ principle allows the collection of
data and information which are transferred to higher levels of analysis. According to
BrownInfo methodology different aspects and spatial levels are mutually aligned ver-
tically and horizontally. In horizontal plane, various aspects are synchronised and
complemented on a particular individual spatial level (of a building, parcel or
Fig. 2. BrownInfo methodology—analysis aspects; spatial levels of analysis and levels of data
presentation [2]
Browninfo Methodology and Software for Development
493

complex), while in the vertical plane, for a certain individual aspect, compliance is
monitored through all spatial levels ranging from low to high ones.
If the methodology had its full application, or if the local governments accepted it
and carried out the mapping of brownﬁeld sites, it would be possible to integrate
databases from the local level into a regional bases or a single information system of
brownﬁelds in RS (Fig. 2). The importance of such conceived bases is multifaceted,
and primarily is reﬂected in creating a positive image of cities, regions and entities,
attracting investment and improving the business environment. A uniform analysis
model arising from BrownInfo methodology ensures consistency of the database and
allows comparison of brownﬁeld sites from different locations and cities on the same
set of criteria. This model allows an investor to easily, quickly and efﬁciently get the
necessary information, thereby speeding the process of selecting a location for
investment.
5
BrownInfo Software Platform
BrownInfo is the name of designed software component architecture of interactive,
urban-oriented, on-line database of brownﬁeld sites (Fig. 3). The concept of software
platform is based on the principles of:
• ‘managing’ brownﬁeld locations and associated feature properties
• ‘planning’ regeneration based on spatial information
• ‘designing’ with the help of geospatial plans and brownﬁeld analysis and
• ‘revitalising’ on the principle of project-planning information base (BIM).
Fig. 3. Concept of software platform. Source Authors
494
T. Vujičić et al.

When deﬁning the software architecture framework, the following basic techno-
logical standards have been met:
• implementation of ISO, OGC and CEN standards from the ﬁeld of spatial infor-
mation technology [14];
• software platform referentiality for the development of client applications, with
built-in GIS functions for the creation, collection, interpretation and analysis of
geospatial data;
• interoperability and processing with formats and services in the relevant external
systems;
• referentiality of platform for interactive access to projects, maps and data in real
time over the Internet and Intranet;
• referential or a rational system for translating graphical data from relational to
object model and vice versa (adjusted to the organised central base of GIS data in
the environment of Microsoft SQL Server technology);
• the possibility of software customisation that is supposed to satisfy the needs of a
wide range of users with programming tools in Autodesk, Microsoft Windows and
NET Framework environment.
BrownInfo software package provides for the developmentally adaptable, rational
and scalable target architecture suitable for integration into the existing information
system of local self-government [17] (Fig. 4). Architecture includes the following
system components:
• Client layer—CAD/GIS application layer for the creation, modiﬁcation and con-
version of graphical objects and associated attributes. The client layer is responsible
for geometric and logical validation of spatial data sets related to brownﬁeld sites, in
a manner that applies strict standards of classiﬁcation prescribed by the object
model;
• Data Server layer—basically GIS (spatial-enabled) server, which keeps a collection
of pro-centred and associated alphanumeric data on brownﬁelds, and on request
gives them to Client and Web Server layers. Data on the Data Server are stored in a
standardised entry format and leave space for the connection and integration with
other information systems. This layer provides mechanisms for: authentication and
protection of data and data access, long transactions, simultaneous work with dif-
ferent versions of data, etc.;
• Web Server layer—enables publishing and microanalysis of space-themed collec-
tions of brownﬁelds downloaded from the Data Server via the Internet and Intra-
net. It also provides a display of spatial data from other (heterogeneous), potentially
distant sources and collections.
Browninfo Methodology and Software for Development
495

It is necessary that all the geometric, topological and descriptive data of unique
BrownInfo spatial basis are placed within the logic model implemented in an envi-
ronment of RDBMS (Relational database management system), and according to the
speciﬁcations made by the Faculty of Architecture, Civil Engineering and Geodesy in
Banja Luka. This will open up the possibility that the information about area is fully
integrated with other data, and additionally provide the possibility of their use through
the applications that are not specialised in GIS.
Users of the system who are browsing the data and make various standard queries
and standard analysis upon them should be allowed to access the system via a
web-based application operating in an environment of standard GIS-web application
server. This system component provides fast and efﬁcient development of applications
used in everyday work by numerous interested users and/or concessionaires. This
allows, in addition to the use of alphanumeric applications and processes, combining
and analysing spatially oriented information.
On-site applications developed within the web-layers provide data exchange
between unique brownﬁeld spatial databases and ﬁeld services [18]. The users of the
mobile station can download the data set required to perform a speciﬁc ﬁeld work, and
after its completion put the collected data with all realised connections back into the
system.
5.1
Multi-layered Software Architecture
5.1.1
GIS Server
GIS Server is basically a Data Server responsible for storing, indexing and processing
BrownInfo spatial data. It stores data in a standard format suitable for distribution
Client and web system layers. BrownInfo software package is based on a platform that
provides support for most of the current Data Server conﬁgurations. Server has pri-
marily the function of storage of different spatial data collections (Spatial Data
Source), and is a host to specialised applications for acquisition and display of
Fig. 4. BrownInfo Client/Server CAD-GIS software platform. Source Authors
496
T. Vujičić et al.

alphanumeric data in the Web Browser applications of client computers. Spatial Data
Source is basically a source (collection) of spatial data on brownﬁelds. For the
maintenance and data update a creator of content (Content Manager) is in charge. The
stored data via LAN or WAN are transported to the Web Server (which is also called a
Content Provider since it handles with already prepared data). Web Server distributes
data collected from one or more sources of data to the end user (Fig. 5).
5.1.2
Web Server
Web Server is responsible for distribution of graphic processing, and remote updating
of central database. It enables you to combine one or more data sources, and display
resulting thematic maps and spatial queries. It is based on modern web server solution
Autodesk Infrastructure Map Server, and works on both Windows (IIS) and Linux
(Apache) platforms. It supports large number of heterogeneous data sources, ﬁle for-
mats and Data Server conﬁgurations. BrownInfo Web Server allows the client to view
or even deliver information without any special application platform (except the web
browser), using desktop or handheld computers. The end user of data, using a standard
web browser, accesses to Web Server and displays, modiﬁes or browses pre-prepared
topics. This is a two-way connection to the extent at which it is allowed by the creator
of content (Content Manager). Web Server distributes information through a standard
Fig. 5. Scheme of data processing within BrownInfo software platform. Source Authors
Browninfo Methodology and Software for Development
497

TCP/IP or HTTP protocol. Server administration is also done via the HTTP protocol,
which means that the administrator can be a remote computer wthin the LAN or even
the Internet (if they meet all safety requirements). This is particularly important from
the standpoint of concession possibilities in hosting spatial database.
5.1.3
Client Layer
Applicative solution on which is based the client layer is Autodesk Map 3D, with the
addition of Autodesk Raster Design upgrade, and operates independently of the server
platforms. The task is to create the basic thematic content: geometric conception of
brownﬁeld spatial entities, their attributes, and mutual topological relations. It contains
the following implemented programme modules:
• Module for import, display and conversion of existing vector processing
• Module for accurate mapping, respectively modiﬁcation and creation of spatial
content
• Module for managing digital project documentation
• Module for calibrating and planning raster geodetic plans and maps
• Module for creating and printing excerpts from the planning documents.
The starting point for the development of all tools is a graphical object of
brownﬁeld site associated with alphanumeric data (Fig. 6). This facility is mapped and
entered into the system only once, and later, only its properties are changed depending
on the context in which it appears.
6
BrownInfo Database on a Former Industrial Complex
Incel—Pilot Project
Pilot project for establishing an interactive database of brownﬁelds of Business zone
Banja Luka2 has been initiated by BrownInfo project team in order to check practically
and examine on a speciﬁc example the proposed framework of generally set
methodology for establishing an interactive database of brownﬁelds. Through practical
work and cooperation between different actors, a series of activities have been carried
out: from ﬁnding partners and funding sources, animating actors and training working
team, mapping, to software developing, establishing the base itself and its presenting
on the Internet. As part of the pilot project, based on an analysis of European examples
of brownﬁeld site bases and through practical work, a software platform tailored to the
speciﬁc
needs
of
BrownInfo
methodology
has
been
created. As
the
ﬁrst
software-supported methodology for the identiﬁcation, classiﬁcation and mapping of
brownﬁeld space, it laid the foundations of the modern and quality approach to the
brownﬁeld presentation not only in Bosnia and Herzegovina, but also in the
region. Interactive, GIS and web-oriented digital database of brownﬁeld sites, based on
2 The pilot project of the establishment of the Business Zone Banja Luka database—the former Incel
Industries, which was implemented as part of this research project, was greatly assisted by INOVA
informatički inžinjering Ltd. and Business Zone JSC Banja Luka.
498
T. Vujičić et al.

BrownInfo software platform and BrownInfo methodology, are extremely useful tools
for the promotion and visibility of brownﬁelds, or instrument that contributes to their
faster and more efﬁcient activation. The result of the pilot project is an interactive
web-accessible brownﬁeld database of Business zone Banja Luka, which includes 58
parcels and 110 buildings (Fig. 6) [8].
7
Possibilities of BrownInfo Platform Application
Key ﬁelds for BrownInfo platform application are identiﬁed in the following areas:
economic development, management of urban development, and ﬁelds of Real Estate
Cadastre.
In the context of economic development, interactive databases of brownﬁelds
designed by BrownInfo model, allow investors to quickly and easily ﬁnd a location for
investment and development. Uniform approach allows them to compare the different
locations on the same criteria and assessment of their suitability in relation to the
planned future use. On the other hand, in the context of growing tendency cities being
certiﬁed as Business Friendly Cities, cities with favourable business environment,
BrownInfo platform meets the highly inﬂuential criteria according to which the
cities/municipalities are required to possess a database of brownﬁelds.
In terms of overall management of local development and planning, either through
formal planning models (spatial plans of municipalities/cities, urban plans of cities) or
informal (integrated local development strategies), BrownInfo platform is an extremely
useful tool for mapping the development potential for their activating and putting into
operation. With minimal adjustments, BrownInfo methodology can be used not only
for mapping brownﬁeld but also greenﬁeld sites. Valid data base created by this model
will enable more efﬁcient management of the overall development of municipalities
and cities by directing part of the development pressures to the forgotten and neglected
brownﬁeld sites, which will directly contribute to sustainable development.
Fig. 6. Interactive brownﬁeld database of Business Zone Banja Luka [8]
Browninfo Methodology and Software for Development
499

As a model for recording and monitoring changes of space, BrownInfo platform can
be used as a means to achieve a more efﬁcient functioning of cadastre and real estate
market. The establishment of national and entity database of brownﬁeld (and green-
ﬁeld) sites would enable the openness of the local real estate market to foreign mar-
kets. In the modern era, in which awareness is the basis of decision-making, a uniﬁed
national database of areas available for future development and construction represent a
competitive advantage of the country and the means of attracting foreign investment.
8
Conclusion
More efﬁcient use of existing spatial resources, ecological recovery of the environment,
creation of new jobs and improvement of the local and regional economic development
are the basic principles of sustainable development and the imperative for effective
management of local governments. In this context, the project BrownInfo has great
signiﬁcance in starting regeneration of brownﬁeld sites from a ‘standstill’, opening the
debate on the effects of their neglect and drawing attention to the necessity of regen-
eration of these important spatial resources of entities and local communities. We
believe that spatial data are an equally important resource as space and plans them-
selves. We are aware that the era of digitisation, application of advanced planning
software, intensiﬁcation of the planning process to meet the needs of the changing
market and the dynamic global environment, and emergence of collaborative/strategic
planning has also given rise to the need to innovate the concept of information systems
to improve the access, ﬂow and transparency of information, and also to optimise the
use of spatial resources. In that respect, the need to establish an information platform of
brownﬁelds as vital spatial resources is an imperative, and the development of a
methodological instrumentarium for data analysis, collection, processing, systemati-
sation, storage and presentation as the necessary ﬁrst step toward its implementation.
Thereby, the results of the project will serve as a basis for a quality inventory of spatial
resources of municipalities and cities. Methodological apparatus for inventory of
brownﬁelds, BrownInfo, will facilitate and speed up mapping procedures, while the
unique access to the database will enable comparability of data among locations, cities
and municipalities. Databases resulting from the application of the proposed method-
ology will make brownﬁelds visible and enable more efﬁcient management of these
spatial resources at the entity and state level.
References
1. CABERNET: Sustainable brownﬁeld regeneration—CABERNET network report. Univer-
sity of Nottingham, Nottingham (2006)
2. Đukić, A., Vujičić, T. (eds.): BROWNINFO, Priručnik za uspostavljanje interaktivne baze
podataka braunﬁld lokacija (BROWNINFO—Handbook for Brownﬁeld Database Devel-
opment), Banja Luka: University in Banja Luka, Faculty of Architecture, Civil engineering
and Geodesy, Deutsche Gesellschaft fuer Internationale Zusammenarbeit (GIZ) GmbH,
INOVA informatički inžinjering Ltd. (2014)
500
T. Vujičić et al.

3. Đukić,
A.,
Simonović,
D.,
Vujičić,
T.
(eds.):
International
scientiﬁc
monograph
BROWNINFO. Toward a methodological framework for brownﬁeld database development,
Banja Luka: University of Banja Luka, Faculty of Architecture Civil Engineering and
Geodesy (2014)
4. AGGF: BrownInfo 2014. University of Banja Luka Faculty of Architecture, Civil
Engineering and Geodesy. http://aggfbl.org/browninfo2014/main.html (2014). Accessed
15 Feb 2017
5. Novaković, N., Preradović, Lj., Vujičić, T.: Survey—state and visibility of brownﬁeld in
Republic of Srpska conducted among representatives of local and national authorities, Banja
Luka: University of Banja Luka Faculty of Architecture, Civil Engineering and Geodesy
(2014)
6. Simonović, D., Novaković, N., Vujičić, T.: Towards a strategy of regeneration of urban
landscape: brownﬁelds as a strategic resource. In: Proceedings of I International Conference
“Ecology of urban areas”, Zrenjanin, RS, Faculty of Technical Sciences Mihajlo Pupin,
pp. 439–449 (2011)
7. Trifunović, A.: Power plant in former industrial complex Incel. Author, Banja Luka
8. AGGF and INOVA: Brownﬁeld database of business zone of Banja Luka. University of
Banja Luka, Faculty of Architecture, Civil Engineering and Geodesy and INOVA
informatički inženjering Ltd. www.geoinova.com/brf (2014). Accessed 16 Feb 2017
9. Đukić, A., Stanković, M., Milojević, B., Novaković, N. (eds.): In: BROWNINFO 2014
Proceedings of International Academic Conference, Banja Luka: University of Banja Luka,
Faculty of Architecture, Civil Engineering and Geodesy (2014)
10. Novaković, N., Preradović, Lj., Vujičić, T.: Survey—state and visibility of brownﬁeld in
Republic of Srpska conducted among representatives of the business sector, Banja Luka:
University of Banja Luka, Faculty of Architecture, Civil Engineering and Geodesy (2014)
11. Simonović, D., Vujičić, T.: Valuation and reactivation of the 20th-century industrial heritage
and its relevance for strengthening the cultural identity of the Republic of Srpska. In: Đukić,
A., Simonović, D., Vujičić, T. (eds.) BROWNINFO. Toward a Methodological Framework
for Brownﬁeld Database Development. Banja Luka, University of Banjaluka, Faculty of
Architecture, Civil Engineering and Geodesy, pp. 23–42 (2014)
12. Danilović, K., Stojkov, B., Zeković, S., Gligorijević, Ž., Damjanović, D. (eds.): Oživljavanje
braunﬁlda u Srbiji—Priručnik za donosioce odluka i profesionalce. Beograd, PALGO centar
(2008)
13. MIEM: AVAILABLE MANUFACTURING FACILITIES/SITES. Ministry of Industry,
Energy and Mining Government of Republic of Srpska. http://www.vladars.net/sr-SP-Cyrl/
Vlada/Ministarstva/mper/EEI_investicioni_potencijali/Pages/Slobodni_prostori.aspx (2014).
Accessed 10 Feb 2017
14. European Commission—Working Group Brönnimann, F., Šestić, M., González Pérez, P.A.,
Hugan, F., Magdalinski, N., Miserez, K., Pfafﬁnger, N., Ritschl, J., Schwarzbach, F.,
Vanbockryck, J., López Alós, A.: INSPIRE infrastructure for spatial information in Europe,
D2.8.III.6 Data speciﬁcation on utility and governmental. http://inspire.ec.europa.eu/
documents/Data_Speciﬁcations/INSPIRE_DataSpeciﬁcation_US_v3.0rc3.pdf
(2013).
Accessed 11 Feb 2017
15. European Commission: INSPIRE—Infrastructure for spatial information in Europe.
European Commission. http://inspire.ec.europa.eu/ (2007). Accessed 15 Jan 2017
16. NALED: Business friendly certiﬁcation South East Europe. National Alliance for Local
Economic Development (NALED). http://bfc-see.org (2012). Accessed 10 Feb 2017
Browninfo Methodology and Software for Development
501

17. Mijić, N., Preradović, D., Šestić, M.: Strategic development of infrastructure for spatial
information based on european INSPIRE directive. In: Proceedings of 5th Researching
Economic Development and Entrepreneurship in Transition Economies (REDETE) Confer-
ence, Belgrade (2016)
18. Mijić, N., Šestić, M., Koljančić, M.: CAD—GIS BIM integration—case study of Banja Luka
city center. In: Hadžikadić, M., Avdaković, S. (eds.) Advanced Technologies, Systems, and
Applications, Volume 3 of the book series. Lecture Notes in Networks and Systems (LNNS).
Springer, pp. 267–281 (2016)
502
T. Vujičić et al.

Rehabilitation of the Urban Road and Pristina
A.B Piles
Bujar Emra1(&), Naser Morin2, Mirsad Tarić1,
and Mirza Hadžimujović1
1 Civil Engineering Faculty, DUNP, Novi Pazar, Serbia
bujar_emra@hotmail.com
2 Civil Engineering, S.T.C., Gnjilane, Serbia
morina.n@hotmail.com
Abstract. Incoherent soil is stable as long as the angle of slope less than the
friction angle of the soil. So, in this type of soil slippage occurs only if the angle
of slope is greater than the internal friction and slippage occurs when at every
point of the sliding surface shear stresses due to the action of external force
reaches the shear resistance of the soil.
Keywords: Clay slope stability  Cohesion  Friction angle  Shear stress
Reinforced concrete piles
1
Introduction
Landslide that occurred in the village “Arbëria” (former Arbëria) in Pristina as a sudden
dramatic event on the slope that had previously been stable for a long time cause such
damage to the existing city road in the village. The cause of the landslide initiation—
landslides, the human factor, i.e. was undermining surface-deep swimming or digging
for laying the foundations skyscrapers (Fig. 1), above which was built the way for
settlement “Arbëria” (Figs. 2 and 3).
Given that called “sliding” means a very wide range of phenomena in shape, size
moving mass, mode, speed and other characteristics during the landslides identiﬁed and
described as follows: type sliding parts landslides, measuring landslide, landslide
activity, speed, type movable material and moisture content. So why landslide belongs
to the geological phenomenon.
According to the depth of the sliding surface of this landslide belongs to the type of
deep landslides (5–20 m), in this speciﬁc case h = 8.40 m. as can be seen from Fig. 1.
In order to landslides or avalanches could successfully rehabilitated, it is necessary
to remove the causes that have caused slippage, and this is done on the ground by us.
The critical height Hc [1] that slopes may have determined from the state of limit
equilibrium.
C ¼ c 
Hc  zc
cos 45  /
2


ð1Þ
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_45

Fig. 2. The position of the existing road that is damaged
Fig. 1. Visible damage to the road due to the negligence of the contractor
Fig. 3. Due to landslides, there was damage to the city road
while the second is the frictional force, which has an angle with the normal of the
sliding surface, then the weight of the soil mass will be:
504
B. Emra et al.

W ¼ ctl  1
2 H2
c  z2
c


 tan 45  /
2


ð2Þ
Since the force and W must be in balance, in this condition you get the formula for
the critical height that has the form:
Hc ¼ 2  c
ctl:
 tan 45 þ /
2


ð3Þ
while according to Terzaghi we have:
Hc ¼ 2:67  c
ctl
 tan 45 þ /
2


ð4Þ
Bearing in mind this is also the data from the geomechanical study done by the
“Institute of Earthquake Engineering and Engineering Seizmology (IZIIS) University”
Ss. Cyril and Methodius “Department: Natyral and Technological Hazards &
Geotechnics - PO Box 101, 1000 Skopje, Republic of Macedonia”, under the number
IZIIS 2006/31 from the date of November 2006 year. Done rehabilitation of landslides.
The rehabilitation was done in a way that is integrated with a length of 42 m retaining
wall of reinforced concrete, where the wall thickness at the apex is 40 cm. While at the
base of the foundation wall thickness is 80 cm, with a strong foundation. The wall is
H = 12 m. The task of the wall to provide a path that is in front of the wall and that
serves to comfortably work carried out for the construction of high-rises, which has
ﬂoors 2P + Pr + 6 with gauges basically 26  32 m. The wall is made of concrete
C-30/37 until the quality of ﬁttings S-500H.
In order to avoid the emergence of a new landslides during the rough construction
works, at the foot of the slopes in the contour of the building are built of reinforced
concrete piles class C-30/37, and the quality of ﬁttings C-500H, while diameter piles is
D = 80 cm, depth 16 m. Wheelbase of the reinforced concrete piles is cm. The piles
are placed at a distance of 32 m, following the perimeter of the building.
2
Assessment of the Permitted Payload PILES
Generally it is known that piles can be hovering, standing or combination of the two.
Assessment permitted capacity of piles based on formulas that are part of a theoretical
and empirical work. The best and most reliable results we would get if they had the
results of a centralized penetration experiments (CPT) or proof load. As we do not have
this data, then the permissible carrying capacity estimates calculate [2–4].
Ultimate bearing capacity of the pile is deﬁned as Qf :
Qf ¼ Qf;b þ Qf;s  W
ð5Þ
Rehabilitation of the Urban Road and Pristina A.B Piles
505

while limiting capacity base piles Qf ;b as:
Qf;b ¼ A  qb;f
ð6Þ
where are they:
Qf ;s—boundary capacity tread, W—net weight of the piles, qb;f —Voltage level of
the base of the piles due to its own weight of soil, A—surface base piles
A ¼ d2  p
4
¼ 0:82  3:14
4
¼ 0:5024
while,
qb;f ¼ c  Nc þ qb  Nq þ 0:5  c  D  Nc
ð7Þ
where:
c—cohesion of soil, Nc; Nq; Nc—Factors capacity, depending on the angle of
internal friction
Nc ¼ Nq  1


 ctg/
ð8Þ
Nq—factor due to the weight of the side layer of soil to a depth of foundation, is
expressed through the term:
Nq ¼ tg2 45 þ /
2


 eptg/
ð9Þ
Nc—factor under its own weight of soil below the soil wedge footings
Nc ﬃ1:8  Nq  1


 tg/
ð10Þ
Based on the geomechanical study, budget effective voltage level base pane is:
qb ¼ Ks 
X
c
0
i  hi
ð11Þ
where
• Ks—coefﬁcient of earth pressure at idle, i—this layer,
• c
0
i—volume weight, i—this layer of earth (kN/m3),
• hi—thickness, i—this layer of earth.
Ks ¼ 1  sin / ¼ 1  sin 20 ¼ 0:658
where is
506
B. Emra et al.

X
ci  hi ¼ 224:02 kN/m2
then:
qb ¼ Ks 
X
c
0
i  hi ¼ 0:658  224:02 ¼ 147:41 kN/m2
qb;f ¼ c  Nc þ qb  Nq þ 0:5  c  D  Nc ¼ 65  70 þ 147:41  12 ¼ 6318:92 kN/m2
value 0:5  c  D  Nc no impact on the capacity of the piles, so i ignored. Then we have:
Qf ;b ¼ Ab  qb;f ¼ 0:5024  6318:92 ¼ 3174:63 kN
Calculation of load capacity per layer piles:
Qf ;s ¼ As 
X
Si  hi
where
As ¼ D  p  1:0 ¼ 0:80  3:14  1:0 ¼ 2:512 m2
Si ¼ ci þ Ksi  rni  tg/i
X
Si  hi ¼ 771:824 kN/m2
Qf ;s ¼ As 
X
Si  hi ¼ 2:512  771:824 ¼ 1938:82 kN
And ﬁnally piles ultimate bearing capacity equal to:
Qf ¼ Qf ;b þ Qf ;s  W ¼ 3174:63 þ 1938:82  0:82  3:14
4
 12:0  25:00 ¼ 4962:73
The permitted capacity of the piles:
Qlej: ¼ Qf
Fs
¼ 4962:73
2:5
¼ 1985:09
2.1
Budget Stiffness of Resilient Spring
Determination of the equivalent relationships averaged modulus of elasticity of the
laminated ﬂoor [4]:
Ee ¼
P Ei  hi
P hi
ð12Þ
Rehabilitation of the Urban Road and Pristina A.B Piles
507

As we Geo-mechanical study did not provide data on the modulus of elasticity of
soil layers, then can be used known connection between the elastic modulus and
compression modulus of the soil.
Msi ¼
1  #
1  2  #
ð
Þ  1 þ #
ð
Þ  Esi
ð13Þ
where the # value Poasson-coefﬁcient, where in the absence of measured values can
adopt the value of # ¼ 1=3. When you adopt this value then the above expression
becomes:
Esi ¼ 2
3  Msi
For humus soil Ms = 1500 kN/m2, and Es iznosi: Es ¼ 2
3  1500 ¼ 1000 kN/m2.
For the last layer is Es ¼ 3460 kN/m2, while the coefﬁcient of horizontal soil
reaction to Vesić deﬁned as [5]:
Ks ¼ 0; 65
D

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Es
E  D4
Ix
12
s

Es
1  #2
ð
Þ
ð14Þ
where are they:
D (m)—diameter piles; E (kN/m2)—modulus of elasticity of the piles; Ix (m4)—
moment of inertia of the piles.
To repair adopted reinforced concrete piles where the selected compressive strength
of concrete: C-25/30, with E = 31.5 GPa.
then:
E  Ix ¼ 31:5  106  0:804  3:14
64
¼ 633 024 kNm2
while,
Ks ¼ 0:65
0:80 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
3460  0:804
633024
12
r

3460
1  0:3332
ð
Þ ¼ 3157:52 kN/m3
Matrix ﬂexibility and stiffness matrix piles:
u
v
/
8
<
:
9
=
; ¼
F11
F12
F13
F21
F22
F23
F31
F32
F33
2
4
3
5 
H0
V0
M0
8
<
:
9
=
;
ð15Þ
respecting
508
B. Emra et al.

u
v
/
8
<
:
9
=
; ¼
F11
0
F13
0
F22
0
F31
0
F33
2
4
3
5 
H0
V0
M0
8
<
:
9
=
;
ð16Þ
If we observe the spatial distribution then the matrix of order 5:
F
½  ¼
F11
0
0
0
F13
0
F22
0
0
0
0
0
F11
F13
0
0
0
F13
F33
0
F13
0
0
0
F33
2
66664
3
77775
ð17Þ
She inverse stiffness.
K
½  ¼
K11
0
0
0
K13
0
K22
0
0
0
0
0
K11
K13
0
0
0
F13
K33
0
K13
0
0
0
K33
2
66664
3
77775
ð18Þ
The vertical stiffness of the piles:
Settlements of piles due to the force P acting on the top of the pile according to
Poulos-in is given as [6]:
S ¼ P  Ip
Es  D
ð19Þ
D—diameter pile, Ip—factor inﬂuence to Poulos-in, which is deﬁned as:
Ip ¼ Io  Rk  Rh  Rv  Rb
ð20Þ
The values of these factors are given diagram in the book: “Pile foundation and
Design”—Poulos.
Io ¼ 0:08; Rk ¼ 0:15; Rh ¼ 1:00; Rb ¼ 1:00; RV ¼ 1:00
then
The vertical stiffness of the piles is obtained as the force that causes a unit
settlement:
S ¼ 1 ) KV ¼ P ¼ Es  D
Ip
ð21Þ
However, we have:
Rehabilitation of the Urban Road and Pristina A.B Piles
509

KV ¼ 3460  0:80
0:012
¼ 230 666:67 kN/m
Determination of horizontal stiffness using solutions of differential equation of
bending rod (Winkler’s hypothesis) [6].
E  I  d4v
dz4 ¼ Ks  D  v
ð22Þ
where are they:
v—horizontal scrolling; EI—ﬂexural rigidity; Ks—horizontal soil reaction. If the
axis of the measure from the top of the pile and if the length of the pile is designated l,
then the boundary conditions that correspond to a given differential equation and the
given problem:
1. z = 0; M(0) = 0; H (0) = 0
2. z ! 1; M(l) = 0; H (l) = 0
With the introduction of the mark:
k ¼
ﬃﬃﬃ
4
p Ks  D
4  E  I
ð23Þ
and if we look at the top of the piles works only horizontal force H0, then the solution
of differential equations given by:
v zð Þ ¼ 2  H0  k
Ks  D  ekz  cos k  z
ð24Þ
While the rotation / zð Þ, bending moments M zð Þ and transverse forces T zð Þ
obtained by differentiating as:
/ zð Þ ¼ 2  H0  k2
Ks  D
 ekz  cos k  z þ sin k  z
ð
Þ;
M zð Þ ¼ H0
k  ekz  sin k  z;
ð25Þ
T zð Þ ¼ H0  ekz  cos k  z  sin k  z
ð
Þ
Matrix elements of ﬂexibility are adequate displacement due to unit load P = 1.
Thus for H0 = 0 and z = 0 we get the F11 and F31 and the conditions M0 = 0 and z = 0
elements F13 and F33. Where the F13 = F31.
For H0 = 0 and z = 0, we have:
v 0
ð Þ ¼ 2  k
Ks  D ¼ F11
and
u 0
ð Þ ¼ 2  k2
Ks  D ¼ F31
510
B. Emra et al.

For M0 = 0 and z = 0:
v 0
ð Þ ¼ 2  k
Ks  D ¼ F11
and
u 0
ð Þ ¼ 4  k3
Ks  D ¼ F33;
while F22 is:
F22 ¼ 1
Kv
¼
1
230666:67 ¼ 4:335  106 kN/m
where:
k ¼ 0:162743 m1


;
Well it:
F11 ¼ 2  0:162743
3157:52  0:8 ¼ 12:8853  105 kN/m
F13 ¼ F31 ¼ 2  0:1627432
3157:52  0:8 ¼ 2:097  105 kN/m
F33 ¼ 4  0:1627433
3157:52  0:8 ¼ 6:825  106 kN/m
Matrix ﬂexibility and stiffness matrix piles U  800:
F
½  ¼
12:8853
0
2:097
0
0:4335
0
2:097
0
0:625
2
4
3
5  105 kN/m
ð
Þ
Finally, the stiffness matrix K
½  has a value of:
K
½  ¼
1
1:58484 
0:2709
0
0:9090
0
3:6559
0
0:9090
0
5:5858
2
4
3
5
T
105
¼
17093:208
0
57359:071
0
230679:44
0
57359:071
0
352451:982
2
4
3
5
3
Conclusions
Landslide is caused due to an imbalance between the external force, its own weight of
earthen mass and internal resistance, which was no longer sufﬁcient to counteract the
increased external forces. There fore, in this case we have a so-called sliding the base
Rehabilitation of the Urban Road and Pristina A.B Piles
511

where soft soil, wherein the value of internal friction is very small since the small depth
below surface layer of low resistance is ﬁrm ground. So to ensure the stability of the
slope and the existing road to be installed reinforced concrete piles along the existing
urban times.
References
1. Nikola, J.: Najdanović, Mehanika tla – Građevinska Knjiga, Beograd (1963)
2. Nonweiller, E., Geomehanika, I.: II i III. Građev. fak, Zagreb (1971)
3. Stojadinović, R.: Mehanika tla I deo. Naučna Knjiga, Beograd (1972)
4. Stojadinović, R.: Mehanika tla II, Naučna Knjiga, Beograd (1973)
5. Vujičić, C.: Fundiranje, Naučna Knjiga, Beograd (1991)
6. Chopra, A.K.: Dynamic of Structures, 2nd edn. New Jersey 07458
512
B. Emra et al.

Analysis of Economic Feasibility
and Usefulness of Asphalt Mixtures
of Recycled Asphalt in Relation to the New
Ones
Muamer Dubravac1(&), Edis Softić2, and Zlatan Talić1
1 Department of Construction, University of Zenica, Polytechnic University,
Zenica, Bosnia and Herzegovina
muamer.db@hotmail.com, zlatan.talic@divel.ba
2 Department of Construction, University of Bihać, Polytechnic University,
Zenica, Bosnia and Herzegovina
edis.softic@bih.net.ba
Abstract. This paper describes testing of the possibilities of application recy-
cled asphalt in the production of new asphalt mixtures. Recycled asphalt was
obtained by mechanical milling of the existing asphalt pavement in the recon-
struction of old asphalt pavements. Laboratory testing has researched conve-
nience in using recycled asphalt and it is also projected composition of asphalt
mix recycled asphalt and after that is made out trial production engineered of the
asphalt mix. The results have enabled to draw conclusions about the applica-
bility and the required proportion of recycled asphalt, production technology
and potential savings in the production of new asphalt mixtures.
Keywords: Recycled asphalt  Asphalt mixture  Production technology
Savings
1
Introduction
It is important to recognize that asphalt recycling is a powerful method to rehabilitate
pavements. When properly applied, it has long term economic beneﬁts—allowing
owner agencies to stretch their available funds while providing the traveling public
with a safe and reliable driving surface. RAP (Reclaimed Asphalt Pavement) ic
common name for the recycled asphalt material and it is accepted all over the world.
RAP (Reclaimed Asphalt Pavement), is the asphalt obtained by breaking, crushing or
milling of the existing old asphalt and contains about 95% of quality aggregates
wrapped bitumen. Recycling of asphalt in the world has known for many years ago.
The ﬁrst serious studies started at the end of the 70s by launching pilot projects in the
US. Since that time, the equipment manufacturing and construction industries have
been proactive in the development of asphalt recycling methods and technologies.
Development of mechanical milling machines, which have replaced the rippers and
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_46

machinery for heating and scraping, and that could asphalt gnaw without prior warming
and pollution, signiﬁcantly affected to the acceptance of recycling [1].
Society has become increasingly aware of the effects of all types of development on
the environment. Many countries have already enacted legislation which requires that
certain percentages of materials, particularly the ones used in roadway construction and
rehabilitation, must be recycled or include recycled materials. By demonstrating the
technical viability, the savings in energy and non-renewable natural resource (crude oil
and granular materials) and the cost savings associated with asphalt recycling, progress
towards one of society’s goals of environmentally responsible construction processes
will be achieved. It is noted, that asphalt pavements are presently the most commonly
recycled material in North America. Of the European countries, RAP has a high
percentage of exploits in Denmark, France, Sweden and Germany. Especially there is
interesting example in the Netherlands, where the proportion of old asphalt is 30–40%
in the production of asphalt mixtures. The Dutch are well known by Europe’s biggest
percentage recycling of various waste materials, because of high awareness of envi-
ronmental values and commitment that the surrounding soil is not used for different
landﬁll waste materials nor to exhaust the natural source material [2].
Sufﬁcient information is provided so that a rational decision can be made with
respect to the feasibility and/or cost beneﬁts of asphalt recycling. From that point,
detailed design issues will need to be addressed by those experienced in asphalt
recycling techniques prior to the ﬁnal project design, advertising, tendering or letting
and construction. In this paper it will be described laboratory studies conducted on
recycled asphalt in order to obtain the best characteristics of asphalt mixture and trial
production of the engineered asphalt mix [3].
2
Asphalt Recycling Methods
Various types of recycling are classiﬁed by the stirring place, temperature process. It is
also classiﬁed by the characteristics of the material that is recycled, the type of used
binder, etc. [4] Five broad categories have been deﬁned to describe the various asphalt
recycling methods. These categories are:
• Cold Planing (CP)
• Hot Recycling
• Hot In-Place Recycling (HIR)
• Cold Recycling (CR)
• Full Depth Reclamation (FDR).
2.1
Cold Planing (CP)
CP consists in the controlled removal of the existing asphalt to the desired depth. The
process allows the regulation of longitudinal road proﬁle and inclination control using
specially designed machines and it is illustrated in Fig. 1.
514
M. Dubravac et al.

The modern cold planer or milling machine has a large diameter rotary cutting
drum or “cutter/rotor/mandrel” housed in a “cutting chamber.” The cutter is equipped
with specially designed replaceable tungsten carbide cutting “teeth” or “tools” that
remove or “mill” the existing pavement. A small amount of water is used during the
milling operation to control the amount of dust generated and to extend the life of the
tools. The water is sprayed unto the tools by a number of nozzles in the cutting
chamber. Milling machines are self-power/self-propelled and of sufﬁcient size to
provide the traction and stability needed to remove the pavement surface to the
speciﬁed proﬁle and cross-slope. Most are equipped with automatic grade control
systems to mill to the speciﬁed elevations and grades. The RAP generated during the
CP operation is loaded onto haul trucks by the milling machine and removed from the
site. The reuse of the RAP, as a base aggregate or similar material, is a form of the three
“R’s” of recycling (reduce, recycle, and reuse) but the higher “value added” application
would be in CCPR or recycled mix [3].
2.2
Hot Recycling (HR)
The removed asphalt is transported to the factory for the production of asphalt where it
recycles the hot procedure. Hot recycling utilizes the heat-transfer method to soften the
RAP to permit mixing with the virgin aggregates and asphalt binder and/or recycling
agent. HR uses a heating RAP method which make it soft and facilitates the mixing and
bonding with a new engine and a binder (Fig. 2).
Fig. 1. Milling machine [3]
Analysis of Economic Feasibility and Usefulness
515

The produced mixture is transported to the place where it should be placed. After
spreading, the asphalt is compacted with rubber or steel roller. During transport mix is
losing heat and making allowed time for compaction shorten. Hot recycling of RAP
currently is the most widely used asphalt recycling method in the world [4].
3
Experimental Design and Procedure
The aim of this study was to determine the suitability and the required proportion of
recycled asphalt in the production of new asphalt mixtures in order to achieve the
required properties of mixtures and the potential savings in production.
4
Price Analyze of Asphalt Construction and Recycled
Asphalt
The ﬁrst analyzing phase is showing the rates prices of materials required for the
production of new asphalt construction. The price of new asphalt construction will be
compared with the price of asphalt construction of recycled asphalt (Table 1).
Fig. 2. Serial asphalt plant for HR procedure [2]
Table 1. Price of the materials for the production of new asphalt mixtures
Weight of the asphalt [kg/t]
1000 [kg/t]
Composition and price of the new asphalt [t]
Material
Composition [%/t] kg
Price BAM/kg Total price [BAM]
Aggregation
95%
950 0.013 BAM
11.88
Bitumen
5%
50 0.80 BAM
40.00
Total [BAM/t] 100%
1000
51.88
516
M. Dubravac et al.

4.1
Price of the Asphalt Mixture of Recycled Asphalt with the 1%
Cement Addition
The cost of asphalt construction materials of recycled asphalt with the addition of 0.5%
bitumen and 1% cement is shown in the following Table 2.
The cost of asphalt construction materials of recycled asphalt with the addition of
0.5% bitumen and 1% cement is shown in the following Table 3.
The cost of asphalt construction materials of recycled asphalt with the addition of
1.5% bitumen and 1% cement is shown in the following Table 4.
Table 2. Prices of asphalt construction materials of recycled asphalt with the addition of 0.5%
bitumen and 1% cement
Price of asphalt construction materials of recycled asphalt with the addition of 0.5%
bitumen and 1% cement
Material
Structure [%/t] kg
Price BAM/kg Total price [BAM]
Recycled asphalt
98.50%
985 0.011 KM
11.19
Concrete
1.00%
10 0.22 KM
2.20
Bitumen
0.50%
5 0.80 KM
4.00
Total [BAM/t]
100%
1000
17.39
Attitude strength [%] 80.20%
Table 3. Prices of asphalt construction materials of recycled asphalt with the addition of 1%
bitumen and 1% cement
Price of asphalt construction materials of recycled asphalt with the addition of 1%
bitumen and 1% cement
Material
Structure [%/t] kg
Price BAM/kg Total price [BAM]
Recycled asphalt
98.00%
980 0.011 BAM
11.14
Concrete
1.00%
10 0.22 BAM
2.20
Bitumen
1.00%
10 0.80 BAM
8.00
Total [BAM/t]
100%
1000
21.34
Attitude strength [%] 85.90%
Table 4. Prices of asphalt construction materials of recycled asphalt with the addition of 1.5%
bitumen and 1% cement
Price of asphalt construction materials of recycled asphalt with the addition of 1.5%
bitumen and 1% cement
Material
Structure [%/t] kg
Price BAM/kg Total price [BAM]
Recycled asphalt
97.50%
975 0.011 BAM
11.08
Concrete
1.00%
10 0.22 BAM
2.20
Bitumen
1.50%
15 0.80 BAM
12.00
Total [BAM/t]
100%
1000
25.28
Attitude strength [%] 88.70%
Analysis of Economic Feasibility and Usefulness
517

The cost of asphalt construction materials of recycled asphalt with the addition of
2% bitumen and 1% cement is shown in the following Table 5.
The cost of asphalt construction materials of recycled asphalt with the addition of
2.5% bitumen and 1% cement is shown in the following Table 6.
Dependency relationships in the price of new and recycled asphalt the addition of
1% of cement and with different amounts of bitumen is shown in the following diagram
(Fig. 3).
Table 5. Prices of asphalt construction materials of recycled asphalt with the addition of 2%
bitumen and 1% cement
Price of asphalt construction materials of recycled asphalt with the addition of 2%
bitumen and 1% cement
Material
Structure [%/t] kg
Price BAM/kg Total price [BAM]
Recycled asphalt
97.00%
970 0.011 BAM
11.02
Concrete
1.00%
10 0.22 BAM
2.20 BAM
Bitumen
2.00%
20 0.80 BAM
16.00 BAM
Total [BAM/t]
100%
1000
29.22 BAM
Attitude strength [%] 95.50%
Table 6. Prices of asphalt construction materials of recycled asphalt with the addition of 2.5%
bitumen and 1% cement
Price of asphalt construction materials of recycled asphalt with the addition of 2.5%
bitumen and 1% cement
Material
Structure [%/t] kg
Price BAM/kg Total price [BAM]
Recycled asphalt
96.50%
965 0.011 BAM
10.97
Concrete
1.00%
10 0.22 BAM
2.20
Bitumen
2.50%
25 0.80 BAM
20.00
Total [BAM/t]
100%
1000
33.17
Attitude strength [%] 91.50%
0 BAM
1,000 BAM
2,000 BAM
3,000 BAM
4,000 BAM
5,000 BAM
6,000 BAM
7,000 BAM
8,000 BAM
9,000 BAM
0 t
30 t
60 t
90 t
120 t
150 t
0,5% bitumen
1% bitumen
1,5% bitumen
2% bitumen
2,5% bitumen
New Asphalt
Fig. 3. Dependency relationships in the price of new and recycled asphalt the addition of 1% of
cement and with different amounts of bitumen
518
M. Dubravac et al.

4.2
Price of the Asphalt Mixture of Recycled Asphalt with the 1.5%
Cement Addition
The cost of asphalt construction materials of recycled asphalt with the addition of 0.6%
bitumen and 1.5% cement is shown in the following Table 7.
The cost of asphalt construction materials of recycled asphalt with the addition of
1.1% bitumen and 1.5% cement is shown in the following Table 8.
The cost of asphalt construction materials of recycled asphalt with the addition of
1.6% bitumen and 1.5% cement is shown in the following Table 9.
Table 7. Prices of asphalt construction materials of recycled asphalt with the addition of 0.6%
bitumen and 1.5% cement
Price of asphalt construction materials of recycled asphalt with the addition of 0.6%
bitumen and 1.5% cement
Material
Structure [%/t] kg
Price BAM/kg Total price [BAM]
Recycled asphalt
97.90%
979 0.011 BAM
11.13
Concrete
1.50%
15 0.22 BAM
3.30
Bitumen
0.60%
6 0.80 BAM
4.80
Total [BAM/t]
100%
1000
19.23
Attitude strength [%] 88.10%
Table 8. Prices of asphalt construction materials of recycled asphalt with the addition of 1.1%
bitumen and 1.5% cement
Price of asphalt construction materials of recycled asphalt with the addition of 1.1%
bitumen and 1.5% cement
Material
Structure [%/t] kg
Price BAM/kg Total price [BAM]
Recycled asphalt
97.40%
974 0.011 BAM
11.07
Concrete
1.50%
15 0.22 BAM
3.30
Bitumen
1.10%
11 0.80 BAM
8.80
Total [BAM/t]
100%
1000
23
Odnos Čvrstoća [%] 87.60%
Table 9. Prices of asphalt construction materials of recycled asphalt with the addition of 1.6%
bitumen and 1.5% cement
Price of asphalt construction materials of recycled asphalt with the addition of 1.6%
bitumen and 1.5% cement
Material
Structure [%/t] kg
Price BAM/kg Total price [BAM]
Recycled asphalt
96.90%
969 0.011 BAM
11.01
Concrete
1.50%
15 0.22 BAM
3.30
Bitumen
1.60%
16 0.80 BAM
12.80
Total [BAM/t]
100%
1000
27.11
Attitude strength [%] 84.90%
Analysis of Economic Feasibility and Usefulness
519

The cost of asphalt construction materials of recycled asphalt with the addition of
2.1% bitumen and 1.5% cement is shown in the following Table 10.
The cost of asphalt construction materials of recycled asphalt with the addition of
2.6% bitumen and 1.5% cement is shown in the following Table 11.
Dependency relationships in the price of new and recycled asphalt the addition of
1.5% of cement and with different amounts of bitumen is shown in the following
diagram (Fig. 4).
Table 10. Prices of asphalt construction materials of recycled asphalt with the addition of 2.1%
bitumen and 1.5% cement
Price of asphalt construction materials of recycled asphalt with the addition of 2.1%
bitumen and 1.5% cement
Material
Structure [%/t] kg
Price BAM/kg Total price [BAM]
Recycled asphalt
96.40%
964 0.011 BAM
10.95
Concrete
1.50%
15 0.22 BAM
3.30
Bitumen
2.10%
21 0.80 BAM
16.80
Total [BAM/t]
100%
1000
31.05
Attitude strength [%] 92.10%
Table 11. Prices of asphalt construction materials of recycled asphalt with the addition of 2.6%
bitumen and 1.5% cement
Price of asphalt construction materials of recycled asphalt with the addition of 2.6%
bitumen and 1.5% cement
Material
Structure [%/t] kg
Price BAM/kg Total price [BAM]
Recycled asphalt
95.90%
959 0.011 BAM
10.90
Concrete
1.50%
15 0.22 BAM
3.30
Bitumen
2.60%
26 0.80 BAM
20.80
Total [BAM/t]
100%
1000
35.00
Attitude strength [%] 92.80%
0 BAM
2,000 BAM
4,000 BAM
6,000 BAM
8,000 BAM
10,000 BAM
0 t
30 t
60 t
90 t
120 t
150 t
0,6% bitumen
1,1% bitumen
1,6% bitumen
2,1% bitumen
2,6% bitumen
New Asphalt
Fig. 4. Dependency relationships in the price of new and recycled asphalt the addition of 1.5%
of cement and with different amounts of bitumen
520
M. Dubravac et al.

4.3
Price of the Asphalt Mixture of Recycled Asphalt with the 2%
Cement Addition
The cost of asphalt construction materials of recycled asphalt with the addition of 0.7%
bitumen and 2% cement is shown in the following Table 12.
The cost of asphalt construction materials of recycled asphalt with the addition of
1.2% bitumen and 2% cement is shown in the following Table 13.
The cost of asphalt construction materials of recycled asphalt with the addition of
1.7% bitumen and 2% cement is shown in the following Table 14.
Table 12. Prices of asphalt construction materials of recycled asphalt with the addition of 0.7%
bitumen and 2% cement
Price of asphalt construction materials of recycled asphalt with the addition of 0.7%
bitumen and 2% cement
Material
Structure [%/t] kg
Price BAM/kg Total price [BAM]
Recycled asphalt
97.30%
973 0.011 BAM
11.06
Concrete
2.00%
20 0.22 BAM
4.40
Bitumen
0.70%
7 0.80 BAM
5.60
Total [BAM/t]
100%
1000
21.06
Attitude strength [%] 106.50%
Table 13. Prices of asphalt construction materials of recycled asphalt with the addition of 1.2%
bitumen and 2% cement
Price of asphalt construction materials of recycled asphalt with the addition of 1.2%
bitumen and 2% cement
Material
Structure [%/t] kg
Price BAM/kg Total price [BAM]
Recycled asphalt
96.80%
968 0.011 BAM
11.00
Concrete
2.00%
20 0.22 BAM
4.40
Bitumen
1.20%
12 0.80 BAM
9.60
Total [BAM/t]
100%
1000
25.00
Attitude strength [%] 118.30%
Table 14. Prices of asphalt construction materials of recycled asphalt with the addition of 1.7%
bitumen and 2% cement
Price of asphalt construction materials of recycled asphalt with the addition of 1.7%
bitumen and 2% cement
Material
Structure [%/t] kg
Price BAM/kg Total price [BAM]
Recycled asphalt
96.30%
963 0.011 BAM
10.94
Concrete
2.00%
20 0.22 BAM
4.40
Bitumen
1.70%
17 0.80 BAM
13.60
Total [BAM/t]
100%
1000
28.94
Attitude strength [%] 120.20%
Analysis of Economic Feasibility and Usefulness
521

The cost of asphalt construction materials of recycled asphalt with the addition of
2.2% bitumen and 2% cement is shown in the following Table 15.
The cost of asphalt construction materials of recycled asphalt with the addition of
2.7% bitumen and 2% cement is shown in the following Table 16.
Dependency relationships in the price of new and recycled asphalt the addition of
2% of cement and with different amounts of bitumen is shown in the following diagram
(Fig. 5).
Table 15. Prices of asphalt construction materials of recycled asphalt with the addition of 2.2%
bitumen and 2% cement
Price of asphalt construction materials of recycled asphalt with the addition of 2.2%
bitumen and 2% cement
Material
Structure [%/t] kg
Price BAM/kg Total price [BAM]
Recycled asphalt
95.80%
958 0.011 BAM
10.89
Concrete
2.00%
20 0.22 BAM
4.40
Bitumen
2.20%
22 0.80 BAM
17.60
Total [BAM/t]
100%
1000
32.89
Attitude strength [%] 120.40%
Table 16. Prices of asphalt construction materials of recycled asphalt with the addition of 2.7%
bitumen and 2% cement
Price of asphalt construction materials of recycled asphalt with the addition of 2.7%
bitumen and 2% cement
Material
Structure [%/t] kg
Price BAM/kg Total price [BAM]
Recycled asphalt
95.30%
953 0.011 BAM
10.83
Concrete
2.00%
20 0.22 BAM
4.40
Bitumen
2.70%
27 0.80 BAM
21.60
Total [BAM/t]
100%
1000
36.83
Attitude strength [%] 107.30%
0 BAM
1,000 BAM
2,000 BAM
3,000 BAM
4,000 BAM
5,000 BAM
6,000 BAM
7,000 BAM
8,000 BAM
9,000 BAM
0 t
30 t
60 t
90 t
120 t
150 t
0,7% bitumen
1,2% bitumen
1,7% bitumen
2,2% bitumen
2,7% bitumen
New Asphalt
Fig. 5. Dependency relationships in the price of new and recycled asphalt the addition of 2% of
cement and with different amounts of bitumen
522
M. Dubravac et al.

5
Conclusion
In our environment in all layers of pavement are almost exclusively used asphalt layers
(lar bearing and wearing course).
Based on the review of newly constructed roads at the Cantonal and Federal
Directorate of Roads and Public Enterprise “Roads of the Republic of Serbian” we can
see that there is no roads (not even a part of roads) built with recycled layers of existing
pavements and dilapidated. In terms of standard and technical regulations most of the
regional and main roads do not meet the criteria of capacity and quality of drainage,
because most of them made with lar bearing layer mainly asphalt limestone origin.
According to the results obtained by examining the optimal share of binders bitumen
and concrete can be concluded that the samples tested standards are satisfactory.
The highest index value of tensile strength of wet and dry samples are in samples of
recycled asphalt with a percentage share of concrete and 2% is splintered by: (0.7; 1.2;
1.7; 2.2; 2.7;)%. In this case, the largest value of the indirect tensile strength is in the
sample that was made with a percentage share of 2% of concrete and bitumen from
2.2% that have strength 120.4 that the percentage of 50% higher than the permissible
value is 80.
References
1. Aničić, D., Sense, K.: Civil Engineers on the Road to Europe, CARDS 2001 Project. The
European Union and the Faculty of Civil Engineering, Osijek (2004)
2. Dimter, S.: The Possibility of Application Waste Materials in Construction. 5-year Program of
Professional Training in the Construction Industry, Faculty of Civil Engineering (2005–2010)
3. Schwabe, Z.: Recycling asphalt pavement structures. Management of Transport Infrastructure,
Roads Days 2009, Zagreb, Sept 2009, pp. 189–239
4. Roberts, F., Kandhal, P., Brown, E.: Hot Asphalt Mixes, Materials, Design and Installation,
2nd edn., (translation from English). Croatian Society of Civil Engineers, Croatian
Construction Institute (2003)
Analysis of Economic Feasibility and Usefulness
523

Seismic Vulnerability, Damage
and Strengthening Evaluation of Historical
Buildings in Bosnia and Herzegovina
Mustafa Hrasnica(&)
Faculty of Civil Engineering, University Sarajevo, Patriotske Lige
30, 71000 Sarajevo, Bosnia and Herzegovina
hrasnica@bih.net.ba
Abstract. Bosnia and Herzegovina is situated in seismic active region of
South-East Europe, divided in seismic zones with PGA of 0.1–0.2 g, even 0.30–
0.35 g in some parts, assuming 500 years return period. Traditionally, historical
buildings were made of stone-masonry. They exhibit stiff behavior when
exposed to the seismic loading. In the case of stronger earthquake motion such
buildings could suffer substantial or heavy damages. Some structural elements,
as domes and arches, crack already by moderate earthquake but without the loss
of stability. Such buildings can be classiﬁed in vulnerability classes B and C
according to European Macroseismic Scale, where A stands for the weakest
seismic structures and F for those expected to have best seismic performance.
Damage assessment, retroﬁt and strengthening of these buildings are complex
tasks for structural engineers, because special attention has to be paid to con-
servator’s requests and conditions.
Keywords: Seismic  Damage  Masonry  Buildings
1
Introduction
The existing buildings in Bosnia and Herzegovina are traditionally built as masonry
buildings, which include most of historical buildings. After the World War II rein-
forced concrete structures prevail by the new erected buildings, but masonry structures
are further built with apply of new materials. If one wants to assess possible damages,
especially those caused by an earthquake, the existing older buildings are more vul-
nerable compared to the buildings constructed according modern technical codes.
Many historical building, which belong to national cultural heritage, were made of
stone-masonry. Generally they have robust and enduring structure and in the case of
stronger earthquakes could suffer substantial or heavy damages. Several strong earth-
quakes that happened in the few last decades underlined the importance of seismic
vulnerability assessment including evaluation of strengthening and retroﬁt measures.
The territory of Bosnia and Herzegovina is situated in active seismic region of
South-East Europe. Shown on the seismic intensity map of Bosnia and Herzegovina for
the reference return period of 500 years (Fig. 1) the greatest part of the country lies in
the zones of 7th and 8th intensity degrees according to MCS-scale or the new European
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_47

Macroseismic Scale EMS [1]. Relatively small part of the territory is situated in the
seismic intensity zone 9. Referred to peak ground acceleration (PGA), PGA between
0.10 and 0.20 g corresponds to the greatest part of the territory and even PGA of
0.30–0.35 g in smaller part of the country (see Fig. 1).
2
Damages Caused by Past Earthquakes in the Region
Several strong earthquakes hit the region of South-East Europe in the last few decades,
some of them in Bosnia and Herzegovina or neighborhood countries. They caused loss
of human lives, a lot of injured inhabitants of the hit areas and substantial damages to
the building structures. Some of the recent earthquakes, which caused damages of the
buildings, also inﬂuenced development of seismic codes for the whole Europe, are
listed below:
Fig. 1. Seismic zones in Bosnia and Herzegovina [4]
Seismic Vulnerability, Damage and Strengthening Evaluation
525

Earthquake in Skopje, Macedonia in 1963, seismic intensity 9th degree, after this
earthquake the ﬁrst modern seismic code was introduced.
Earthquake in Banja Luka, Bosnia and Hercegovina 1969, seismic intensity 8th–9th
degree, hypocenter was in the country.
Earthquake in Friuli, North-East Italy, seismic intensity 9th–10th degree, damage
analysis are used in development of European Macroseismic Scale EMS.
Earthquake on the Montenegro Coast in 1979, seismic intensity 9th degree, after
this earthquake the new seismic code was improved.
The most severe consequences after a strong earthquake are total or partial collapse
of the building structures, which were observed for Skopje and Banja Luka earthquake
(Figs. 2 and 3). Five-story masonry building without vertical R.C. conﬁning elements
could not withstand strong forces induced by the earthquake of the seismic intensity 9
and collapsed (Fig. 2). Similar observation can be conﬁrmed by failure of the corner
building (Fig. 3).
Common damages of masonry structures caused by earthquake are diagonal cracks
in the walls (Fig. 4). This type of damage on masonry structures was observed after
many earthquakes, from minor cracks after less severe ground motion to larger cracks
due to strong earthquakes, as illustrated on the ﬁgure below. This can lead to the
buckling of the damaged wall and collapse of the whole building. The reasons for the
diagonal cracks in the masonry walls are in their small resistance in tension. Due to the
high level of the horizontal forces induced by an earthquake a sort of truss resistance
mechanism is formed in the masonry walls or piers. The truss chords are ﬂoor
Fig. 2. Total collapse of unreinforced masonry building, Skopje 1963
526
M. Hrasnica

structures, which in the case of R.C. ﬂoors can transmit the horizontal forces in efﬁcient
way, while the diagonals are formed in the masonry wall itself and fall in tension. The
most of the existing masonry buildings in southeast Europe belong to the unreinforced
masonry structures and this tradition is preserved. The improvement is made by almost
regular built in of the vertical reinforced concrete conﬁning elements, which improve
overall structural ductility signiﬁcantly.
The other, also very frequent type of damage in masonry buildings due to the
seismic action is loss of connection between two mutually perpendicular walls in plan.
This is also shown on the Fig. 4, where partial loss of connection between two external
walls at the corner of building was observed.
Traditional stone masonry houses were very often built in the Mediterranean
region, where continuous seismic activity is permanently registered. Most of the houses
are built with wooden ﬂoors. The damages at one of such buildings due to Montenegro
earthquake are shown on the Fig. 5. The stone masonry wall collapsed, which caused
partial collapse of the ﬂoor and the roof structures, leading generally to heavy structural
damage of the building.
The masonry buildings are generally brittle structures, which show relatively sat-
isfactory behavior up to moderate seismicity. In that case most damages can be pre-
dicted and also repaired. But, exposed to very strong earthquakes most of the
traditional buildings suffer heavy structural damages, whose reparations are not
Fig. 3. Partial failure of the masonry building, Banja Luka 1969
Seismic Vulnerability, Damage and Strengthening Evaluation
527

Fig. 4. Diagonal crack in masonry walls and loss of corner wall [1]
Fig. 5. Stone masonry building with wooden ﬂoors, Montenegro earthquake [1]
528
M. Hrasnica

reasonable. Exceptions are important historical buildings. The advantage of the existing
masonry building is the structural regularity. Most of them have no large structural
eccentricity, viewed in the plan, or there is no important stiffness irregularity along the
height of the building.
3
Damage Assessment of Historical Buildings
Whole Mediterranean area belongs to seismic active regions, which was conﬁrmed by
past earthquakes and at the same time Mediterranean countries are rich in historical and
cultural buildings and monuments. Those buildings have great importance and value
for speciﬁc countries and their inhabitants. So they merit special care and protection.
This concerns in the same way the historical buildings in Bosnia and Herzegovina.
Besides the risk of seismic damages, a lot of them were damaged or even destroyed
during the last war.
Assessment of historical buildings presents speciﬁc problem considering the ways
they were built and the materials, which were used. The damages are sometimes
cumulated through many years and many causes, e.g. few moderate or stronger earth-
quakes. Another speciﬁc problem arises by reparation and necessary strengthening or
retroﬁt, for example to achieve earthquake resistance demanded by modern seismic
codes. Speaking about historical buildings and monuments the aim is to preserve and
reveal their aesthetic and historical values and to use original materials and original way
of construction, if possible. But, where traditional techniques prove inadequate some
modern construction and conservation techniques must be implemented. The same
problems occur with traditional construction materials. In order to provide necessary
resistance and ductility and fulﬁll the demands of new building codes the contemporary
building materials have to be carefully implemented in the structures of those buildings.
Many important principles for the assessment of historical buildings and monuments are
summarized in the Venice Charter.
Fig. 6. Schematic presentation of dome structural system and cracks in the dome masonry
structure
Seismic Vulnerability, Damage and Strengthening Evaluation
529

The large majority of all historical buildings are built as masonry structures, a lot of
stone masonry, but in some regions bricks masonry as well. They are traditionally built
as unreinforced masonry without conﬁning elements. In some regions timber con-
ﬁnement was used. Typical curved structural forms as domes, arches and vaults are
often part of historical buildings especially the religious one. As they are built as
unreinforced masonry structures, the historical buildings are relatively stiff and show
generally brittle behavior. So, the ﬁrst damages in form of cracks appear already by
moderate earthquakes on softer structural elements as domes and arches (Fig. 6), or
ceilings by wooden ﬂoors and on partition walls, if there are any. At the same time the
main structure, as dick walls and abutments, is in linear range of the behavior, with no
or almost no cracks. But, it’s generally not true for very strong earthquake motion.
Masonry structures have rather small resistance in tension and cracks open per-
pendicular to the direction of seismic forces. Typical example is schematically presented
on the following Fig. 6, where the cracks on the dome are opened orthogonal to ring
tension forces. The structural form of the dome as three-dimensional structure is shown
on the left side of the Fig. 6; the radial arches in compression and circumferential
tension. Masonry dome is usually set on the drum below and very often they are
interrupted with window openings. It cracks radially as shown on the right side of the
Fig. 6, forming ring of the arches. The illustration of the cracks in an old dome masonry
structure is on the Fig. 7.
Fig. 7. Radial cracks in the old dome masonry structure [11]
530
M. Hrasnica

4
Vulnerability Classiﬁcation and Damage Grades According
to EMS
Within European Macroseismic Scale [1] structural systems of buildings are classiﬁed
according to vulnerability classes depending on structural type. Vulnerability classes
are A–F, where class A is for the weakest seismic structures and class F for those that
are expected to have very good seismic performance (Table 1).
The classiﬁcation of damage degrees for buildings is given within European
Macroseismic Scale as well. Damage degrees are from 1 to 5 that means from irrelevant
damages or only damages of nonstructural elements that correspond to damage 1, to
destruction or even building collapse that corresponds to damage degree 5. Classiﬁ-
cation of damage to masonry buildings is made generally as follows:
• Grade 1: Negligible to slight damage (no structural damage). Hair-line cracks in
very few walls. Fall of small pieces of plaster.
• Grade 2: Moderate damage (slight structural damage). Cracks in many walls. Fall of
fairly large pieces of plaster.
• Grade 3: Substantial to heavy damage (moderate structural damage). Large and
extensive cracks in most walls, roof tiles detach.
• Grade 4: Very heavy damage (heavy structural damage). Serious failure of walls,
partial structural failure of roofs and ﬂoors.
• Grade 5: Destruction (very heavy structural damage). Total or near total collapse.
Table 1. Vulnerability classes for masonry structures according to EMS
Seismic Vulnerability, Damage and Strengthening Evaluation
531

Damage degrees depend also on earthquake intensity. The class of building
vulnerability, which depends on the structural type, can be related to damage degrees
[6, 7], which can be expected for different seismic intensities (Table 2). Within the
European Macroseismic Scale there are short descriptions of effects that could be
expected for the speciﬁc degree of seismic intensity.
Masonry buildings made of rubble stone or earth brick generally belong to vul-
nerability class A and already for 7th degree of seismic intensity serious damages can
be expected, including instability of walls or falling down of ceiling. Such buildings
have no many ﬂoors, usually ground ﬂoor and a story; they are situated in village, often
in inaccessible environment. There are masonry buildings constructed with bricks
produced in factory, but without vertical conﬁning elements. We speak about unrein-
forced masonry (URM) without conﬁnement. Older buildings have usually wooden
ﬂoors, while buildings built after World War II generally have R.C. ﬂoors. The ﬁrst
belong mostly to vulnerability class B where very heavy damages can be expected for
the earthquakes whose intensity corresponds to the seismic zone 8. Masonry buildings
with R.C. ﬂoors according to EMS classiﬁcation could stand heavy damages of the
structure including falling down of some walls for the intensity degree 9 and they
belong mostly to vulnerability class C.
Masonry buildings with reinforced concrete conﬁning elements, usually called
conﬁned masonry, are generally classiﬁed according to EMS in relatively low class of
vulnerability, class D. For 9th degree of seismic intensity signiﬁcant cracks can appear,
roof tiles detach, chimneys can fall down, but there should not be collapse of entire
walls. The advantage of conﬁned masonry is evident. After the new seismic codes were
introduced this became usual type of masonry building.
Most of the historical buildings, especially public and religious buildings, are built
of stone masonry. The conclusions made from Table 2 could be used to estimate
roughly seismic vulnerability of historical buildings as well. But, some of them have
rather speciﬁc structure, especially monumental buildings and they can not be directly
ordered into the table above. Also, it has to be mentioned that previous classiﬁcations
refer to some average design and constructed masonry buildings. In the cases of worse
construction heavier damages than those described must be taken in consideration.
Table 2. Damage grades of typical masonry buildings in Bosnia and Herzegovina
Type of masonry and R.C: wall buildings typical structures in
B&H
Seismic zone according to
EMS
Zone
VII
Zone
VIII
Zone
IX
Masonry buildings made of earth brick or ﬁeld stone
3–4
4–5
5
Unconﬁned masonry, older than approx. 60 years mostly with
timber ﬂoor structure
2–3
3–4
4–5
Unconﬁned masonry, younger than approx. 60 years with
reinforced concrete ﬂoors
2
2–3
3–4
Conﬁned masonry with R.C. ﬂoors, mostly newer masonry
buildings
1
2
2–3
532
M. Hrasnica

Presented vulnerability classiﬁcation of masonry buildings corresponds to relatively
regular structures, which should be noted as well.
5
Seismic Evaluation of Existing Buildings
The existing buildings are more vulnerable to seismic actions than those designed and
built according to modern seismic codes. Important percentage of existing buildings
represents masonry buildings, which include most of the historical monuments and
buildings. The prediction of their seismic performance is very important to assess
possible damages. The properties of those buildings and their typical damages were
analyzed in the previous chapters. They were also classiﬁed according their seismic
vulnerability and damage grades, which can occur for different levels of seismic
intensity.
The efﬁcient methods for evaluation of existing buildings are pushover analysis and
capacity spectrum method [2, 3, 5]. Assuming that the structure responds predomi-
nantly in the ﬁrst eigenmode, using nonlinear static procedure the capacity curve of the
building is developed. On the other side, earthquake demand is represented by design
spectrum (Fig. 8). The intersection point of two curves simulates the performance point
of the structure for the given conditions.
The both curves are presented in acceleration-displacement-response-spectrum
ADRS-format. On the Fig. 8 earthquake demand is represented by Eurocode 8 design
spectra for the elastic behavior. This procedure gives very good insight into the
structural behavior from the engineering point of view. Position of the structure
capacity curve regarding earthquake demand curve shows what kind of measures
should be undertaken to improve seismic performance of the building, if it’s necessary.
Retroﬁt of the structures concerns three basic structural properties: strength, ductility
0
2
4
6
8
10
12
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
spectral displacement Sd[m]
spectral acceleration Sa[m/s 2]
T=0.5s
T=1.0s
T=2.0s
EC8 ag=0.35g  
Structure capacity
Fig. 8. Structure capacity versus earthquake demand
Seismic Vulnerability, Damage and Strengthening Evaluation
533

and stiffness. Each of them is important for the desirable structural performance during
an earthquake. The strength is connected with minimum design capacity, ductility level
is important for dissipation of energy induced into the structure by an earthquake and
stiffness is important to limit deformations.
On the Fig. 9 the capacity curves for three different structures are compared
referring to earthquake demand. The ﬁrst curve, which does not intersect the demand
curve represent behavior of brittle structures with low ductility. This is typical for older
historical buildings constructed as robust unreinforced masonry structures. The fun-
damental period of such structures is usually in the range of high spectral acceleration.
If the capacity curve does not go over seismic demand the structure generally cannot
survive that earthquake intensity. On the contrary side there are very soft structures,
with high ductility but without minimum of required resistance. The capacity curve
also does not intersect the seismic demand, here in long period range. It is also example
of bad earthquake resistant design.
In order to achieve good earthquake resistant design the structure should have
required resistance and appropriate ductility level to assure dissipation of seismic
energy. Unreinforced masonry buildings and most of the historical buildings built in a
traditional way don’t fulﬁll these requirements. In the Table 2 (Chap. 4 of this paper)
the damage grades for different seismic zones are summarized. To decrease damage
grades of the traditional masonry and historical buildings it is obviously necessary to
improve their ductility, in the way that the capacity curve intersect the demand curve at
the reasonable level of horizontal deformations. There are many possibilities for repair
and retroﬁt of the masonry or reinforced concrete structures in order to improve their
seismic performance.
horizontal displacement
resistance
earthquake demand
buildings design according new codes 
bad earthquake resistant design
older historical buildings 
Fig. 9. Capacity of different structures regarding their earthquake resistance
534
M. Hrasnica

6
Historical Masonry Buildings in Bosnia and Herzegovina
Examples of three historic stone masonry buildings from Ottoman period in Bosnia and
Herzegovina [8–10] are brieﬂy discussed in this chapter. During the last war in Bosnia
and Herzegovina many historical buildings, valuable as important cultural heritage,
were damaged and even barbarically destroyed (Figs. 10 and 11).
The damages can be accumulated through the centuries. A good example is a
mosque situated in the old city of Sarajevo, one of the ﬁrst mosques with the dome
constructed during Ottoman period in Bosnia and Herzegovina (Fig. 12). Probably the
ﬁrst hair cracks had opened during some earthquake motion in the history and over
time they became larger. Most of the damage was concentrated in the mosque dome,
and partially in the walls. The cracks were typical for this type of the masonry
structures, going in meridian direction (see Fig. 12). The cracks width was mostly in
the range from 10 to 100 mm. They can be estimated as very large cracks. Even, few
displacements perpendicular to the dome surface between two portions of the dome
divided by the crack were observed. They were in the range from 30 to 120 mm.
Obviously, the mosque dome was substantially damaged. At the same time there was
no damage registered on the minaret. One can conclude that instead of the original
space type of the dome structure few plane arch structures were formed, which pro-
vided the roof stability, but not the functionality.
Fig. 10. Destroyed mosque building in Počitelj near Mostar
Seismic Vulnerability, Damage and Strengthening Evaluation
535

Repair and strengthening of historical buildings shown above was performed using
traditional and contemporary materials. For the mosque in Banjaluka, traditional
Fig. 11. Only foundation left after severe destruction, mosque in Banjaluka
Fig. 12. Schema of the observed cracks and crack detail, mosque in Sarajevo
536
M. Hrasnica

reinforcement is previewed in a lower part of the dome in the direction of ring forces,
as well as in minaret, where vertical reinforcement is built in masonry stones. Even
existing foundation structure was tightened and strengthened by reinforced concrete
jacketing on micro-piles. Carbon ﬁber strips (CFRP) was implemented for strength-
ening of the dome and minaret in Počitelj mosque (Fig. 13). The similar solution was
applied for the dome of the mosque in Sarajevo. But the cracks were injected with
special mortar mixture (Fig. 14).
7
Conclusions
Damage and seismic vulnerability assessment are important to make decision about
retroﬁt and strengthening of the existing buildings. Seismic evaluation and comparison
of structural capacity of existing buildings with seismic demand according to modern
Fig. 13. Strengthening solutions for minaret and dome, mosque in Počitelj
Fig. 14. Injection of cracks and built in of carbon strips, mosque in Sarajevo
Seismic Vulnerability, Damage and Strengthening Evaluation
537

seismic codes will result in the necessity of retroﬁt for the majority of existing masonry
buildings, especially for historical buildings. They are mostly made of stone masonry
and could suffer substantial or heavy damages in the case of stronger earthquake
motion, which means damage grades from 3 to 5 for seismic intensity degrees 7–9,
respectively. Examples of three typical historical buildings from the Ottoman period in
Bosnia and Herzegovina were presented. Seismic strengthening and repair had been
performed using traditional and contemporary materials and construction techniques.
References
1. EMS-98: European macroseismic scale. In: Grüntal, G. (ed.) European Seismological
Commission, Luxembourg (1998)
2. Freeman, S.A.: Development and use of capacity spectrum method. In: 6th U.S. National
Conference on Earthquake Engineering (1998)
3. Hrasnica, M.: Seismic analysis of buildings (in Bosnian), Faculty of Civil Engineering
University of Sarajevo, Sarajevo (2005)
4. Hrasnica, M.: Aseismic Structures (in Bosnian), Faculty of Civil Engineering, Sarajevo
(2012)
5. Hrasnica, M.: Response spectra for seismic evaluation of buildings (in Croatian).
J. Građevinar Zagreb 54 (2002) 11
6. Hrasnica, M.: Seismic vulnerability of tipical multi-storey buildings in Bosnia and
Herzegovina. In: Proceedings of NATO-Advanced Workshop, Sarajevo, 5–9 Oct 2008
7. Hrasnica, M.: Damage assessment of masonry and historical buildings in Bosnia and
Herzegovina. In: Ibrahimbegović, A., Zlatar, M. (eds.) Damage Assessment and Recon-
struction After War or Natural Disasters. Springer (2009)
8. Hrasnica, M., Zlatar, M., Kulukčija, S., Humo, M., Madzarević, M.: Seismic strengthening
and repair of typical stone masonry historical buildings in Bosnia and Herzegovina. In: 8th
International Masonry Conference, Dresden (2010)
9. Hrasnica, M., Zlatar, M., Kulukčija, S., Humo, M.: Damage assessment and seismic
strengthening of stone masonry historical buildings in Bosnia and Herzegovina. In: 14th
European Conference on Earthquake Engineering, Ohrid, Macedonia (2010)
10. Hrasnica, M., Medić S.: Seismic strengthening of historical stone masonry structures in
Bosnia Herzegovina. In: 15th World Conference on Earthquake Engineering, Lisboa (2012)
11. UNDP/UNIDO: Repair and strengthening of historical monuments and buildings in urban
nuclei. Building Construction Under Seismic Conditions in the Balkan region, vol. 6, Vienna
(1984)
538
M. Hrasnica

Analysis of Relations Between Freeway
Geometry and Trafﬁc Characteristics
on Trafﬁc Accidents
Marina Milenković(&) and Drazenko Glavić
Faculty of Trafﬁc and Transport Engineering, University of Belgrade,
Belgrade, Serbia
marina.milenkovic@sf.bg.ac.rs, drazen@via-vita.org.rs
Abstract. Modelling trafﬁc accidents on freeways is very signiﬁcant due to the
fact that accidents occurring on these roads are frequent and severe and cause
trafﬁc jams. Bearing this in mind, the objective of this paper was to analyze the
impact of freeway geometry and trafﬁc characteristics on the occurrence of
trafﬁc accidents with fatalities on freeways in Serbia. The paper analyzed trafﬁc
accidents with fatalities which had on the M-1 freeway with the total length of
394 km. A regression analysis was used in the analysis of data. The developed
models represent the one of the ﬁrst attempt of quantifying the mentioned
impacts on the freeways in Serbia. These models should contribute to the
improvement of safety of the existing freeways and designing new safer
freeways.
Keywords: Trafﬁc accidents  Road characteristics  Trafﬁc characteristics
Freeway
1
Introduction
Numerous studies have been conducted worldwide in order to determine the rela-
tionship between trafﬁc accidents and freeway geometry and trafﬁc characteristics. This
type of research has not been carried out in Serbia for freeway network, so this paper
represents the ﬁrst attempt to deﬁne these dependencies. So far, in local conditions the
impacts of road and trafﬁc characteristics on the occurrence of trafﬁc accidents have
been analyzed only for two-lane rural roads [1, 2].
The paper analyzed the road and trafﬁc characteristics of a 394.4 km-long section of
the M-1 freeway according to the old categorization, i.e. DP I-1 by the new catego-
rization. The following road characteristics were considered: radius of horizontal curve,
longitudinal gradient and density of access points. Trafﬁc characteristics analyzed in
the paper were: average annual daily trafﬁc and share of commercial vehicles in the
total trafﬁc ﬂow. The paper dealt with the trafﬁc accidents which had occurred in the
ten-year period (2004–2013).
The objective of this paper was to examine the impact of road and trafﬁc charac-
teristics on the occurrence of trafﬁc accidents with fatalities on freeways in Serbia.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_48

The main limitation of the paper is related to the data unavailability and their
structure and volume. When it comes to the freeway network in Serbia, the only
available data are the ones regarding fatalities on the M-1 freeway. In addition, one of
the paper limitations is the fact that the sample mostly consisted of the sections with
ideal characteristics (radius larger than 450 m, gradient less than 3%, etc.), so it was
impossible to examine the impact of unfavourable geometric elements on the occur-
rence of trafﬁc accidents with fatalities.
2
Literature Review
Chang [3] developed two trafﬁc accident prediction models for freeways in Taiwan.
The ﬁrst model was based on using a negative binomial regression, while the other was
founded on using artiﬁcial neural network. The analysis was conducted on the
373 km-long freeway using the data on trafﬁc accidents which had occurred in the
2-year period. The data on geometric freeway characteristics analyzed in the paper
were the number of lanes, lane width, horizontal curvature and gradient. Trafﬁc
characteristics analyzed were: average annual daily trafﬁc (AADT) for different vehicle
types, peak hour factor and trafﬁc distribution per lanes.
On the same freeway in Taiwan, Chang and Chen [4] analyzed trafﬁc accident
frequency using a tree-based model. The paper analyzed the data on trafﬁc accidents in
a 2-year period and the impact of geometric characteristics, trafﬁc characteristics and
weather conditions on the occurrence of trafﬁc accidents. Regarding geometric char-
acteristics, the study used the data on the number of trafﬁc lanes, horizontal curvature,
gradient and shoulder width; while trafﬁc characteristics used in the analysis were
AADT for different vehicle types, peak hour factor and trafﬁc distribution per lanes.
Christoforou et al. [5] focused on examining the effects of various trafﬁc parameters
collected in real time—at the moment and before the occurrence of various types of
trafﬁc accidents. Multivariate prediction models were based on 4-years of data for a
2.3 km-long freeway segment in France. A disaggregate approach was used where the
units of analysis were the accidents themselves (381 of them). The multivariate
regression model used in this study was based on the multivariate normal distribution.
The used trafﬁc data were ﬂow, speed and density. Accidents with unavailable data on
trafﬁc were also excluded. The results of the occurrence of trafﬁc accidence depending
on trafﬁc regime are presented in Fig. 1.
In their paper, Caﬁso and D’Agostino [6] presented the procedure of accident
analysis for freeway network providing a comparison between the conventional ana-
lytical techniques based on GLM (Generalized Linear Model) and a different approach
based on GEE (General Estimating Equation). The subject of the paper was a freeway
in Italy. The data for a 6-year period were used and collected for the total number of
652 sections. Only the data for accidents with fatalities and injuries were used in the
analysis. The following independent variables were deﬁned for each section: road
hazard, value of the gradient, the lack of corresponding cross slope on the observed
sections, variables related to the segment type showing the degree of the section
homogeneity and variables related to the curvature of homogeneous road elements.
540
M. Milenković and D. Glavić

In their study, Chang et al. [7] conducted the analysis of trafﬁc accident frequency
on freeways using a nonparametric model of analysis, i.e. Multivariate Adaptive
Regression Splines—MARS, which enabled examining the effect of various factors on
the occurrence of trafﬁc accidents. The analysis was conducted on the 373-km-long
freeway in Taiwan. The paper studied the impact of geometric freeway characteristics,
trafﬁc characteristics, as well as weather conditions on the occurrence of trafﬁc acci-
dents. The data were collected for a 2-year period. The data on geometric road char-
acteristics included the number of lanes, gradient and radius of horizontal curve, while
the analyzed trafﬁc parameters involved AADT of various vehicle categories and trafﬁc
intensity distribution over lanes. In addition to the above mentioned, the impact of
weather conditions was considered, i.e. air pressure, temperature, humidity, precipi-
tation and wind speed.
Chengye and Ranjitkar [8] analyzed the trafﬁc safety on the freeway by developing
accident prediction models that connected accident frequency with non-behavioural
factors which led to trafﬁc accidents, including trafﬁc conditions and operational road
characteristics and weather conditions. The study used the data on trafﬁc accidents
which occurred in the 7-year period on a 74-km long section of a freeway in New
Zealand. The dataset from the ﬁrst ﬁve years was used for developing models, while
the information for the following 2 years was utilized for testing the predictive per-
formance. In the paper, negative binomial regression models were developed for three
cases: ﬁrst for all freeway sections, then for rural and urban freeway sections separately
and ﬁnally for freeway sections without ramps and for freeway sections with an
entrance ramp and an exit ramp. The results showed that AADT and number of lanes
had the largest impact on the safety of trafﬁc participants.
Fig. 1. Distribution of trafﬁc accident types depending on trafﬁc regime
Analysis of Relations Between Freeway Geometry
541

3
The Analysis of Databases
In order to study the impact of road and trafﬁc characteristics on the occurrence of
trafﬁc accidents with fatalities, it was requisite to collect data on trafﬁc accidents and
data on trafﬁc load and road geometry.
3.1
The Database of Trafﬁc Accidents
The database on trafﬁc accidents contained the data regarding the type of trafﬁc
accidents, time-space distribution of trafﬁc accidents, as well as the information about
the carriageway state at the time of the accident and the main cause of the accident. The
data on the spatial distribution of trafﬁc accidents with fatalities were signiﬁcant for the
needs of this research. The spatial distribution of trafﬁc accidents was deﬁned
according to the deﬁned milepost markers, i.e. the kilometers of the mileposts. In the
studied 10-year period, the considered sections witnessed the total number of 246
trafﬁc accidents with fatalities.
3.2
The Database of Average Annual Daily Trafﬁc
The data on trafﬁc load were taken from the database of the Public Enterprise “Roads of
Serbia”. The trafﬁc load database contains the data on AADT, including both the total
number and the number according to basic vehicle categories.
3.3
The Database of Road Geometry
The data on road geometry were also taken from the database of the Public Enterprise
“Roads of Serbia”. This database consisted of several parts (charts) where the data on
the road cross section and on the horizontal and vertical route alignment were recorded
separately. The unit of observation of each database (chart) was a road segment. The
segment was deﬁned as part of the section with the constant value of the variable whose
effect was examined. Thus, the borders between the sections were set at the places
where the corresponding variable was altered.
3.4
The Integrated Database
On the basis of the individual databases, integrated databases were formed. The formed
integrated databases contained the data on trafﬁc accidents with fatalities and freeway
geometry and trafﬁc characteristics. The sections were divided into homogenous seg-
ments, while the borders between the segments were set at the places with the alter-
ations of one of the independent variables (average annual daily trafﬁc, share of
commercial vehicles in the trafﬁc ﬂow, longitudinal gradient or radius of horizontal
curve). These databases were formed for the purpose of analyzing the impact of several
independent variables on the dependent variable.
In order to form an integrated database it was necessary to assign a particular
number of trafﬁc accidents to each section (i.e. to determine how many accidents
occurred at each section).
542
M. Milenković and D. Glavić

The dependent variable was the number of trafﬁc accidents with fatalities per
kilometre per year, while the following variables were selected to be independent
variables: average annual daily trafﬁc, the share of commercial vehicles in the trafﬁc
ﬂow, radius of horizontal curve, longitudinal gradient and density of access points.
4
The Analysis of the Results
The paper includes the results which were conﬁrmed to have a statistically signiﬁcant
inﬂuence on the occurrence of trafﬁc accidents. Also, particular effects of the variables
were impossible to examine due to the fact that there was an insufﬁcient number of
sections with the critical values of the selected variable (for instance, the radius of the
curve was smaller than the critical radius, i.e. less than 450 m, only on two sections of
the chosen freeway, so it was impossible to evaluate the impact of radius on the
occurrence of trafﬁc accidents).
4.1
The Impact of the Average Annual Daily Trafﬁc on the Number
of Trafﬁc Accidents with Fatalities
In order to examine the individual impact of the average annual daily trafﬁc (AADT) on
the occurrence of trafﬁc accidents with fatalities, we developed the regression models of
the dependence of the number of trafﬁc accidents with fatalities on the annual average
daily trafﬁc. For rural freeway sections, the best-ﬁtting model was the following:
N ¼ 0;007 þ 3;29  PGDS1;5
ð1Þ
The obtained results show that on rural freeways the number of trafﬁc accidents
with fatalities per kilometre per year increases with the rise of the annual average daily
trafﬁc. This model explained 46.1% of the variance of the number of trafﬁc accidents
with fatalities per kilometre per year (R2 = 0.461). This model is graphically repre-
sented in Fig. 2. The obtained results are in accordance with the results obtained for
two-lane rural roads in Serbia [1] and ﬁndings of foreign studies.
Fig. 2. Model of dependence of the number of TA on AADT
Analysis of Relations Between Freeway Geometry
543

4.2
The Impact of the Share of Commercial Vehicles in the Total Trafﬁc
Flow on the Number of Trafﬁc Accidents with Fatalities
In addition to average annual daily trafﬁc, one of the signiﬁcant trafﬁc characteristics is
the share of commercial vehicles in the total trafﬁc ﬂow. Therefore, this paper devel-
oped the models of dependence of the number of trafﬁc accidents with fatalities per
kilometre per year on the share of commercial vehicles in the ﬂow, for both rural
freeway sections. The following model was the best-ﬁtting for rural sections:
N ¼ 0;073  1;106  KV3
ð2Þ
The results obtained in this study have shown that the increase of the share of
commercial vehicles in the total trafﬁc ﬂow on the rural freeway slightly reduces the
number of trafﬁc accidents with fatalities per kilometer per year (R2 = 0,007). How-
ever, this model explains only 0.7% of the variance of the response variable. (the
number of trafﬁc accidents with fatalities per kilometer per year). This model is
graphically represented in Fig. 3. Glavić et al. [1] for two-lane rural roads in Serbia also
have found that the total number of trafﬁc accidents decreases with the rise of the
percentage of commercial vehicles in the total trafﬁc ﬂow. The obtained results are
partially in concordance with the results of foreign studies, since the results of foreign
studies vary, i.e. some have determined that the increase of the share reduces the
number of trafﬁc accidents (as it is the case in this study), while others have concluded
the opposite—that the rise of the share increases the number of trafﬁc accidents.
4.3
The Impact of the Gradient on the Number of Trafﬁc Accidents
with Fatalities
The paper examined the impact of the gradient, as one of the road (geometric) char-
acteristics of the freeway, on the occurrence of trafﬁc accidents with fatalities per
kilometre per year. The sample included only the sections with the critical gradient
(gradient larger than 3%). The best-ﬁtting model for rural freeway sections was the
following:
Fig. 3. Model of dependence of the number of TA on %CV
544
M. Milenković and D. Glavić

N ¼ 0;113  1;635  e UN
ð
Þ
ð3Þ
The obtained results show that on rural sections the number of trafﬁc accidents with
fatalities per kilometre per year increases with the rise of the gradient. This model
explained 7% of the variance of the response variable (R2 = 0.069). This model is
graphically represented in Fig. 4 and the obtained results are in accordance with the
results of foreign studies. For two-lane rural roads in Serbia, Glavić et al. [1] have
found that the increase of roadway gradient reduces the total number of trafﬁc
accidents.
4.4
The Impact of the Density of Access Points on the Number of Trafﬁc
Accidents with Fatalities
Having in mind that a large number of foreign studies concluded that the density of
access points had a signiﬁcant inﬂuence on the occurrence of trafﬁc accidents, the paper
analyzed the impact of this variable on the number of trafﬁc accidents with fatalities.
The following dependence was determined for rural freeway sections:
N ¼ 0;0083 þ 0;0289  e
AP
1;825
ð
Þ
ð4Þ
The results acquired for rural sections of freeways show that the larger density of
access points increases the number of trafﬁc accidents with fatalities per kilometre per
year. This model explained 24.2% of the variance of the response variable
(R2 = 0.242). This model is graphically represented in Fig. 5. The obtained results are
in accordance with the results obtained for two-lane rural roads in Serbia [2] and also
with the ﬁndings of foreign studies.
Fig. 4. Model of dependence of the number of TA on the gradient
Analysis of Relations Between Freeway Geometry
545

4.5
The Combined Impact of the Average Annual Daily Trafﬁc
and Share of Commercial Vehicles in the Total Flow
on the Occurrence of Trafﬁc Accidents with Fatalities
Within the study, we developed a multivariate regression model of the dependence of
the number of trafﬁc accidents with fatalities on the average annual daily trafﬁc and
share of commercial vehicles in the total trafﬁc ﬂow on the rural freeway sections. The
following model was obtained for rural freeway sections:
Fig. 5. Model of dependence of the number of TA on the density of access points
Fig. 6. Multivariate regression model of the dependence of the number of TA on the density of
access points
546
M. Milenković and D. Glavić

N ¼ 0;0702 þ 7;013  KV  0;022=PGDS
ð5Þ
The results of this analysis have shown that the sections with greater average annual
daily trafﬁc and larger share of commercial vehicles in the total ﬂow have a larger
number of trafﬁc accidents with fatalities (Fig. 6).
5
Discussion
This paper includes key results of foreign studies and the summary of this project.
Namely, Chang et al. [7] reached the following key results:
• if AADT is less than 17,142 vplpd then it has no effect on the frequency of
accidents
• if AADT is greater than 17,142 vplpd but less than 30,264 vplpd, the trafﬁc accident
frequency will increase by 0.038 for each additional 1,000 vehicles per trafﬁc lane
• if AADT is larger than 30,264 vplpd, the trafﬁc accident frequency will increase by
0.046 for each additional 1,000 vehicles per trafﬁc lane
• if the gradient of the freeway section is less than 2.3%, then it has no effect on the
frequency of trafﬁc accidents;
• if the gradient is greater than 2.3% but less than 3%, the trafﬁc accident frequency
will rise by 0.858 for each gradient increase of 1%;
• if the gradient is greater than 3% but less than 5%, the trafﬁc accident frequency will
decrease by 1.404 for each gradient increase of 1%;
• if the gradient of the freeway section is larger than 5%, the trafﬁc accident frequency
will increase by 6.508 for each gradient increase of 1%.
The results of the research conducted in this study are mostly in accordance with
the results of foreign studies and also with the results obtained in local conditions for
two-lane rural roads [1, 2]. Namely, they showed that average annual daily trafﬁc, share
of commercial vehicles in the total ﬂow, gradient and density of access points had a
signiﬁcant effect on the occurrence of trafﬁc accidents with fatalities. It was determined
that on rural freeway sections the increase of the average annual daily trafﬁc, gradient
and density of access points led to a greater number of trafﬁc accidents with fatalities
per kilometre per year, while the increase in the share of commercial vehicles decreased
the number of trafﬁc accidents with fatalities per kilometre per year.
6
Conclusion and Recommendations for Future Research
6.1
Concluding Remarks
The acquired results showed that certain road and trafﬁc characteristics had a signiﬁcant
effect on the occurrence of trafﬁc accidents and that these freeway elements should be
taken into account when designing new freeways and improving the existing ones.
Also, there are numerous measures incorporating intelligent transport systems which
can contribute to safer and more efﬁcient functioning of trafﬁc on freeways. One of
Analysis of Relations Between Freeway Geometry
547

these measures is certainly ramp metering. In addition to improving the level of service,
the purpose of ramp metering is also to ensure safe manoeuvring when entering the
freeway.
6.2
Recommendations for Further Research
In order to develop reliable trafﬁc accident prediction models for freeways in the future,
the sample should be enlarged by collecting data for the total freeway network in
Serbia. Furthermore, in addition to accidents with fatalities, accidents with injuries and
accidents with property damage should be taken into consideration.
Moreover, the data related to the spatial distribution of trafﬁc accidents should be as
precise as possible. Registering the accidents with GPS coordinates would greatly
contribute to the precision of the data on the spatial distribution of TA. Therefore,
increasing the sample and collecting precise data would enhance the reliability of the
results.
Acknowledgements. The results obtained in this paper are the product research project of the
Ministry of Construction, Transport and Infrastructure of Serbia- titled: Improving the level of
trafﬁc safety on motorways’. The project is co-ﬁnanced by the Ministry of Construction,
Transport and Infrastructure of the Republic of Serbia and the Serbian society for roads
“Via-Vita” Beograd. The authors also gratefully acknowledge the support of representatives of
the Ministry of Construction, Transport and Infrastructure, Highway Institute and all of the
participants who have work on this project.
References
1. Glavić, D., Mladenović, M., Stevanovic, A, Tubić, V., Milenković, M., Vidas, M.:
Contribution to accident prediction models development for rural two-lane roads in Serbia.
PROMET Trafﬁc Transp. 28(4), 415–424 (2016)
2. Milenkovic, M., Glavic, D., Tubic, V., Trpkovic, A.: Evaluation of the impact of the road
characteristics on trafﬁc safety. In: Proceedings of the First International Conference
“Transport for Today’s Society”, Bitola, Makedonia (2016)
3. Chang, L.Y.: Analysis of freeway accident frequencies: negative binomial regression versus
artiﬁcial neural network. Saf. Sci. 43(8), 541–557 (2005)
4. Chang, L.Y., Chen, W.C.: Data mining of tree-based models to analyze freeway accident
frequency. J. Saf. Res. 36(4), 365–375 (2005)
5. Christoforou, Z., Cohen, S., Karlaftis, M.G.: Identifying crash type propensity using real-time
trafﬁc data on freeways. J. Saf. Res. 42(1), 43–50 (2011)
6. Caﬁso, S., D’Agostino, C.: Safety performance function for freeways using generalized
estimation equations. Proc. Soc. Behav. Sci. 53, 900–909 (2012)
7. Chang, L.Y., Chu, H.C., Lin, D.J., Lui, P.: Analysis of freeway accident frequency using
multivariate adaptive regression splines. Proc. Eng. 45, 824–829 (2012)
8. Chengye, P., Ranjitkar, P.: Modelling freeway accidents using negative binomial regression.
J. East. Asia Soc. Transp. Stud. 10, 1946–1963 (2013)
548
M. Milenković and D. Glavić

Artiﬁcial Neural Networks Application
in the Backcalculation Process of Flexible
Pavement Layers Elasticity Modulus
Ammar Saric(&) and Mirza Pozder
Faculty of Civil Engineering, Department of Roads and Transportation,
University of Sarajevo, Sarajevo, Bosnia and Herzegovina
{ammar.saric,pozder.mirza}@hotmail.com
Abstract. The mechanical properties of existing ﬂexible pavements determine
the remaining life of pavement and the moment when rehabilitation program
should be implemented. The calculation of these properties can be very difﬁcult,
time consuming and non-reliable process. To determine the structural capacity
of the pavement non-destructive testing equipment used. One of the most
commonly used NDT techniques is falling weight deﬂectometer (FWD) test.
Based on FWD measurements backcalculation process must be carried out in
order to obtain the modulus of elasticity of pavement layers. This can be done by
several methods, different in complexity and accuracy. Artiﬁcial neural net-
works can be successfully used for fast backcalculation process with training
based on synthetic deﬂection basin obtained with linear elastic theory.
Keywords: Flexible pavement  Backcalculation  Artiﬁcial neural networks
Training  Falling weight deﬂectometer  FWD
1
Introduction
Flexible pavements are affected by moving vehicles, climate and other environmental
factors. As a result of these factors, the pavement starts to deteriorate. In order to
prevent further deterioration, a maintenance program should be carried out at right time
and right places. Perhaps the most difﬁcult factor to determine is the remaining life of
the pavement. Many distresses can be seen by eye. In order to determine the remaining
life, the pavement should be analyzed structurally with material properties for each
layer in terms of elastic modulus, Poisson’s ratio and thickness of layers. For the
determination of the structural carrying capacity of the pavement, non-destructive
testing equipments are used. These are mainly Benkelman Beam, Dynaﬂect, road rater
and falling weight deﬂectometer (FWD). In such a process, the most important thing is
to analyze the collected data. A backcalculation procedure is carried out for backcal-
culating the elastic modulus for each layer that has an effect on the pavement life.
Generally, linear elastic and ﬁnite element based programs are used for backcalcula-
tion, but they are time consuming. An artiﬁcial neural network (ANN) approach is used
for the elimination of this drawback during the course of this research [1].
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_49

Flexible pavements are layerd systems, constructed by bituminous and granular
materials. It is very difﬁcult to model the complete ﬂexible pavement using traditional
methods, because an asphalt mixture has a visco-elastic behavior depends on loading
time. In these methods, especially in layerd elastic theory and ﬁnite element methods,
mathematical formulations are complex and based on non-realistic assumptions as in
equivalent layer thickness (ELT) method. All these complex procedures can be over-
came using artiﬁcial neural networks (ANN).
The goal of this research is application of trained artiﬁcial neural network for
backcalculating pavement layer moduli from FWD data. For this purpose, we use
measurements from 17 km long section of main road, in mountainous terrain. An
artiﬁcial neural network is trained using relative small volumes of synthetic test data
generated by layered-elastic model used in the conventional backcalculation program
WESLEA. These procedure shows fast and reliable calculation of pavement layers
elasticity modulus (E) from real FWD data.
2
Nondestructive Testing of Pavements
The structural integrity of roads is determined in large part by the load-deﬂection
properties of the concrete, asphalt, and soils that make up the pavement system. Those
properties can be measured in the laboratory; however, doing so requires that a portion
of the pavement system be destroyed in order to obtain material samples. The distur-
bance that results from the sampling process itself calls into question the accuracy of
the subsequent laboratory tests. Furthermore, the inherent heterogeneity of pavement
materials (especially in the base and subgrade) means that isolated samples are often
not representative of the pavement system as a whole. As an alternative to laboratory
testing, the structural properties of the pavement can be measured in situ using tech-
niques that fall under the general category of nondestructive testing (NDT). As the
name implies, NDT can be used to assess pavement ﬁtness without destroying the
pavement in the process. Unlike laboratory methods, there are no concerns with sample
disturbance because samples are not taken. Furthermore, the test results reﬂect the
properties of the pavement system over a broad area rather than at a single point. In
fact, to the extent that the device being used simulates an actual vehicle load, the NDT
techniques can actually represent full-scale tests of the pavement system [2].
The majority of the NDT methods currently used for the structural evaluation of
pavement systems are based on the same general principle: the structural integrity of a
pavement system is inversely proportional to the amount of surface deﬂection observed
under a given load. The primary differences between the various devices being used
lies in the nature of the loads they apply to the pavement. Accordingly, NDT devices
can be categorized according to load type as being either static, steady-state dynamic,
or transient dynamic devices [2].
Static devices measure the deﬂection response of the pavement to what is essentially
a static vertical load. This class of NDT devices includes the Benkelman Beam and the
La Croix Deﬂectograph. The NDT devices that fall into the steady-state dynamic cat-
egory measure the deﬂection response of the pavement to a low-frequency oscillatory
load. The Dynaﬂect, Road Rater, and WES 16-kip Vibrator are three such steady-state
550
A. Saric and M. Pozder

devices. The transient dynamic NDT devices apply an impulse load to the pavement and
record the resulting pavement deﬂection histories at several radial distances from the
load (Fig. 2). This experimental data is usually summarized by a “deﬂection basin” that
is constructed from the peak deﬂections at each measurement location (Fig. 1). The
stiffness of the various material layers in the pavement system are calculated from these
deﬂection basins through a process called backcalculation or inversion [2].
One of the most widely used NDT techniques is the Falling Weight Deﬂectometer
(FWD) test. An FWD test is performed by applying an impulse load to the pavement
via a circular plate and measuring the resulting pavement deﬂections directly beneath
the plate and at several radial offsets from the plate (Fig. 2).
Fig. 1. Typical deﬂection basin [3]
Fig. 2. Falling weight deﬂectometer (FWD) device [4]
Artiﬁcial Neural Networks Application in the Backcalculation
551

One advantage that these devices have over the steady-state devices is that they do
not need a static preload. Their force levels are achieved by the rapid, but controlled,
deceleration of a relatively light falling mass. As a result, pavement responses to the
heavy wheel loads of most interest can be obtained with an extremely light,
trailer-mounted device that can be towed by most conventional vehicles. Another
advantage of the transient dynamic devices is that they come closer than any of the
other device types to replicating the force histories and deﬂections produced by moving
vehicles. Bohn et al. (1972) showed that the load pulses produced by an FWD are
virtually identical in shape and duration to those produced by a truck moving at
40 km/h. They also showed that the magnitudes of the resulting deﬂections were
almost identical despite a signiﬁcant difference in their duration [2].
Benkelman Beam and Dynaﬂect give the information underneath the center of
circular mass (i.e. these devices give one deﬂection data in each measurement) whereas
the FWD gives the information in other six or more point which are away from the
circular plate. There are many types of FWDs which can apply similar loading.
The time of loading varies between 0,025 and 0,030 s; the applied loads vary between
6,7–156 kN. The loads are generally applied in a sinusoidal form. The loading time of
0,030 s represents duration of a load pulse produced by a wheel moving at a speed of
30 km/h [5].
A major limitation of existing techniques for backcalculating pavement layer
moduli from FWD results is that they are computationally inefﬁcient—they all involve
numerous repetitions of mathematically complex calculations. This makes the inver-
sion programs slow and tedious to use. Though the test itself takes only a minute or two
to run, the data analysis can take considerably longer. This can limit the usefulness of
FWD testing, especially for performing routine, periodic assessments of pavement
integrity such as would be needed for the FWD to be used as part of a pavement
management system [2].
2.1
Backcalculation of Pavement Layer Moduli
Backcalculation is the procedure that involves the calculation of theoretical deﬂection
under applied load using assumed pavement layer moduli [6]. The procedure followed
is that these calculated deﬂections are compared with measured deﬂections, the
assumed moduli are then adjusted in an iterative procedure until the theoretical, and
measured deﬂection basins reach an acceptable match (Fig. 3). This implies that
knowledge of the existing layer thicknesses and the behavior of the pavement materials
is required [7].
The fundamental principles of backcalculation procedures are based on pavement
theories such as the multi-layer elastic theory and plate theory. The most often used
multi-layer elastic theory was simpliﬁed using Odemark’s equivalent thickness
assumptions. Basically, materials are assumed to be homogeneous, isotropic, and linear
elastic, even though they are often far from reality. Various programs were developed to
facilitate the layer moduli backcalculation of a more practical multi-layered system [8].
552
A. Saric and M. Pozder

The purpose of backcalculation is to consider the derived moduli as representative
of the pavement response to load and can be used to calculate stresses and strains in the
pavement structure for analyses purposes [7].
The main problems any classical backcalculation procedure faces are convergence,
accuracy, and the number of layers in the backcalculation program. The selection of the
seed moduli controls the convergence of the backcalculation procedure to pavement
moduli that minimizes the mean square error (of the objective function) between the
measured deﬂection and the backcalculated deﬂection using the backcalculated moduli.
It is known that more than one solution could satisfy the objective function criterion in
the backcalculation of the pavement moduli due to the multimodal nature of the
backcalculation search space where many local optima exist. In turn, arrival at local
optima will lead to “inaccurate” pavement moduli that can be as much as twice the
“accurate” value. On the other hand, the maximum number of layers that can be used in
any existing backcalculation program can handle at most ﬁve layers with recommen-
dations to use three layers to reduce the error associated with the backcalculation
process. In some cases, increasing the number of layers in the backcalculation process
is desirable to obtain more representative variation of the moduli with depth [9].
Backcalculation analysis approaches may be classiﬁed as follows [10]:
Fig. 3. Schematic diagram representing process of backcalculation
Artiﬁcial Neural Networks Application in the Backcalculation
553

• Simpliﬁed methods,
• Gradient relaxation methods,
• Direct interpolation methods.
These approaches have been used to develop many software applications which
actually can reasonably accomplish backcalculation from FWD test results using dif-
ferent assumptions of the elastic layered systems. Simpliﬁed and direct interpolation
approaches are not popular because the typical numerical routines that are used for
backcalculation may not properly iterate the moduli as the local minimum for the
solution for a system can be numerous and global optimization may be required. These
methods also pose the possibility of inaccurate solutions if the pavement layer prop-
erties are not in accord with the assumptions made. However, in spite of the drawbacks,
the problem if formulated correctly leads to very reasonable solutions [10].
Gradient relaxation methods are the most popular ones due to their nonlinear
behavior in formulation of the algorithm. They employ mathematical models to
describe the pavement condition. The process is to use a set of seed moduli (from
experience or known values for standard layers) to determine deﬂections from a for-
mulated model for the problem in hand and then to compare the estimated value with
the experimental values from FWD testing [10].
Flexible pavement layer moduli calculations can be performed using several
well-known software programs among which MODULUS, EVERCALC, ELMOD are
the most commonly used ones. MODULUS and EVERCALC were developed by the
Texas Transportation Institution and the Washington State Department of Transporta-
tion (WSDOT), respectively. WESLEA, a layered elastic solution platform by US Army
Corps of Engineers included in MODULUS, performs the forward calculation for
building a database of computed deﬂection basin. This database is compared with
measured deﬂections using a pattern search routine to determine the layer moduli in the
pavement system. Flexible pavements with up to four unknown layers can be processed
using MODULUS. Similar to MODULUS, EVERCALC also uses an iterative approach
incorporating WESLEA as the forward engine to calculate deﬂection basin based on a
given set of layer moduli. Using an optimization technique known as Augmented
Gauss-Newton algorithm, EVERCALC can provide evaluations of layer moduli for up
to ﬁve layer pavement structures. Unlike EVERCALC and MODULUS, that use the
WESLEA elastic layered program, ELMOD, another commonly used backcalculation
software program, uses the Odemark equivalent thickness approach [10].
3
Artiﬁcial Neural Networks
An Artiﬁcial Neural Network (ANN) is an information processing paradigm that is
inspired by the way biological nervous systems, such as the brain, process information.
The key element of this paradigm is the novel structure of the information processing
system. It is composed of a large number of highly interconnected processing elements
(neurons) working in unison to solve speciﬁc problems [11].
554
A. Saric and M. Pozder

Neural networks learn by example. They cannot be programmed to perform a
speciﬁc task. The examples must be selected carefully otherwise useful time is wasted
or even worse the network might be functioning incorrectly. The disadvantage is that
because the network ﬁnds out how to solve the problem by itself, its operation can be
unpredictable [11].
The ﬁrst appearance of the ANN concept in the literature is due to McCullough and
Pits (1943), who deﬁned a working model of a neuron. Their neuron is binary
(true/false) threshold unit that ﬁres if the sum of its inputs exceeds a certain threshold.
Despite the apparent simplicity of this model, McCulloch and Pitts showed that any
ﬁnite logical expression could be encoded by a network of these binary logic units [2].
Later on, many researchers concentrated their attention on the learning ability of
humans and its modelling (Hebb 1949, Rosenblatt 1985, Widrow and Hoff 1960,
Kohonen 1972, Anderson 1972, Hopﬁeld 1984, Rumelhart, Hinton, and Williams
1986). The biggest progress in ANN development was introducing of nonlinear neuron
by Hopﬁeld (1984) forms an afﬁne combination of the inputs and the weights, then
transforms the result using a nonlinear function to obtain the desired output:
yj = g sj
 
ð1Þ
where sj is afﬁne combination of the inputs and the wights:
sj¼
X
i wijx jbj
ð2Þ
and the function g(), which is called the transfer function, is usually taken to be the
sigmoidal logistic function (Figs. 4 and 5):
g x
ð Þ¼
1
1 + ex
ð3Þ
Fig. 4. Sigmoidal logistic function [2]
Artiﬁcial Neural Networks Application in the Backcalculation
555

In 1986, Rumelhart, Hinton, and Williams solved last problem for creating ANN
structure like we know today, i.e. create methods available for training multi-layered
networks, instead of single-layer networks. They developed a learning algorithm
known as backpropagation which was developed for a fully-connected, multi-layer,
feed-forward network with nonlinear neurons of the type developed by Hopﬁeld. In
such a network, each neuron ﬁrst accepts one input from each of the neurons in the
layer above it, then forms an afﬁne combination of those inputs and its own weights,
applies the nonlinear transfer function to that afﬁne combination, and ﬁnally makes the
result available to every neuron in the layer below it (Fig. 6) [2].
Fig. 5. Schematic drawing of a multi-layer feed-forward network [2]
Fig. 6. Measured deﬂection on example section
556
A. Saric and M. Pozder

The backpropagation algorithm provides a way to adjust the weights of hidden
neurons based on the error at the output layer. It does this by propagating the output
errors backward through the network. This backpropagation of the errors tells the
hidden neurons two things: ﬁrst, how strongly they are connected to each output
neuron, and second, the error at each output neuron. If a hidden neuron is weakly
connected to an output neuron with a large error or strongly connected to an output
neuron with a small error, it should not have to modify its weights substantially.
Conversely, if a hidden neuron is strongly connected to an output neuron with a large
error, it should bear the brunt of the modiﬁcations needed to correct that error [2].
It is reported by Lawrence (1993) that the consensus of opinion is that
back-propagation is the best general-purpose model and probably the best at general-
ization. A good understanding of feedforward networks and bac-propagation is
therefore essential when applying ANNs [7].
3.1
Application of Artiﬁcial Neural Networks for Backcalculation
The ANN modeling consists of two steps. First step is to train the network; second is to
test the network with data which were not used for training. The processing of adaption
of the weights is called learning. During the training stage the network uses the
inductive-learning principle to learn from a set of examples called the training set [1].
Synthetic deﬂection basins are generated using linear elastic theory in WESLEA
program with inputs of layer thickness, E-moduli, Poisson ratios, and the deﬂection
basin from measurements in seven points (0, 300, 600, 900, 1200, 1500 and 1800 mm).
The pavement structure of Type 1 pavements for which synthetic deﬂection basins
were created, is shown in Table 1. Elasticity modulus of subgrade and thickness of
granular base (values in parentheses) used for ANN training are higher than usually
values for ﬂexible pavement. This is because of geographical location (mountain ter-
rain) of example road section which has thicker and stronger pavement due to resis-
tance to frost. A total of 266 synthetic deﬂection basins were created of which 241 were
used as the training set and 25 as the test set. Relative small number of used synthetic
basins shows large capabilities of ANN and accuracy backcalculation process even if
we have limited input data set.
Table 1. Characteristics of pavement structure Type 1 used in ANN
No Layer
Modulus of elasticity (MPa) Thickness (cm)
Poisson’s ratio
1
Asphalt
1.000–10.000
5–17
0,35
2
Granular Base 300–900
25–45 (50,60,90) 0,35
3
Subgrade
40–150 (380)
∞
0,35
Standard deviation
1
Asphalt
2535,57
3,52
–
2
Granular Base 196,88
16,79
–
3
Subgrade
64,61
–
–
Artiﬁcial Neural Networks Application in the Backcalculation
557

Numerous ANNs were created to test various network architectures (i.e. combi-
nations of neurons and hidden layers). Multilayer Normal Feedforward ANNs with a
backpropagation learning rule and sigmoid transfer functions were used for all the
ANNs. Several hidden layer conﬁgurations were tested and a summary of these ANNs
is shown in Table 2. For each network created the Root Mean Square Error (RMSE)
using Eq. 1 and also are reported in the Table 2:
RMSE =
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n
XN
i¼1
dci - dmi
dmi

2
s
0
@
1
A  100%
ð4Þ
where are:
dci
ith calculated (ANN predicted) deﬂection
dmi
ith measured (theoretical, synthetic) deﬂection,
n
total number of deﬂections
Optimal architecture of ANN (i.e. number and order of hidden layers and number
of neurons), based on calculated RMSE, for four hidden layers is 25-20-15-10 and for
ﬁve hidden layers is 25-20-15-10-5. ANN with latter architecture is used for elasticity
modulus prediction of example road section.
FWD measuring on the ﬁeld is obtain with 50 kN load via circular plate width
300 mm, on approximately every 250 m of selected section (Fig. 6). A total of 31
FWD measurements are used for prediction of elasticity modulus of layers with pre-
vious trained ANN.
In this way, we obtain mechanical properties of pavement in exploitation which is
necessary for rehabilitation program. Backcalculation using ANN is faster than other
known methods, for example FEM, which is much more time consuming and requires
additional data.
Table 2. Summary of results for created ANN
No Inputs
Outputs
Hidden layers geometry RMSE (%)
E1
E2
E3
1
20-20-15
8,87 5,47
3,87
2
20-15-10
8,87 5,47
3,87
3
20-15-10-5
7,86 4,56
2,41
4
d1 (asphalt) E1 (asphalt)
20-15-10-5-5
7,99 4,27
3,70
5
d2 (base)
E2 (base)
25-20-15-10
7,39 3,78
2,10
6
D0–D1800
E3 (subgrade) 10-15-20
15,27 5,66 13,71
7
10-15-20-15
14,38 4,86 10,84
8
20-15-20
9,37 4,83
4,33
9
20-15-20-10
9,74 4,31
5,64
10
25-20-15-10-5
5,84 3,63
2,02
558
A. Saric and M. Pozder

4
Conclusion
This research shows that ANN can be successfully trained to backcalculate elasticity
modulus from measured deﬂection basins. Although training set was relative small, in
compare to some other similar studies, backcalculation process has sufﬁcient accuracy.
What is more important, process is very fast and reliable.
Synthetic deﬂection basins are generated using linear elastic theory, which has
some restrictions and does not properly describe true pavement behavior. Improvement
can be using a theory that does accommodate nonlinear and stress dependent behavior
of materials. This way will result in improved performance of the ANN. An ANN’s
ability to solve problems is independent of the complexity of the problem or method
used to generate data. Alternative methods (dynamic backcalculation, ﬁnite element
analysis) can therefore be used to generate data. These methods will improve the
modelling of the pavement structure, but are more mathematically complex and need
much more time for problem solving.
References
1. Saltan, M., Tigdemir, M., Karasahin, M.: Artiﬁcial neural network application for ﬂexible
pavement thickness modeling. Turk. J. Eng. Environ. Sci. 26 (2002)
2. Meier, R.W.: Backcalculation of Flexible Pavement Moduli from Falling Weight
Defelctometer Data Using Artiﬁcial Neural Networks. Technical Report for U.S. Army
Corps of Engineers, Washington (1995)
3. Beltran, G., Romo, M.: Assessing artiﬁcial neural network performance in estimating the
layer properties of pavements. Ingeniería e Investigación 34(2) (Bogota, Colombia) (2014)
4. http://www.strata.com.br/falling-weight-deﬂectometer/. Accessed 19 Dec 2016
5. Saltan, M., Terzi, S.: Backcalculation of pavement layers using artiﬁcial neural networks.
Indian J. Eng. Mater. Sci. 11 (2004)
6. Ullidtz, P., Coetzee, N.F.: Analytical Procedures in Nondestructive Testing Pavement
Evaluation, Transportation Research Record 1482 Transportation Research Board. Wash-
ington, D.C. (1995)
7. Bredenhann, S.J., van de Ven, M.F.C.: Application of artiﬁcial neural networks in the
back-calculation of ﬂexible pavement layer moduli from deﬂection measurements. In:
Proceedings of the 8th Conference on Asphalt Pavements for Souther Africa (CAPSA’04),
12–16 September 2004, Sun City, South Africa (2004)
8. Lee, Y.H., et al.: Study of backcalculated pavement layer moduli from the LTPP database.
Tamkang J. Sci. Eng. 13(2) (2010)
9. Pan, E., et al.: An Efﬁcient and Accurate Genetic Algorithm for Backcalculation of Flexible
Pavement Layer Moduli. Ohio Department of Transportation, Department of Civil
Engineering The University of Akron, Akron, OH (2012)
10. Tutumluer, E., Sarker, P.: Development of Improved Pavement Rehabilitation Procedures
Based on FWD Backcalculation. NEXTRANS Project, USDOT Region V Regional
University Transportation Center Final Report (2015)
11. Nielsen, M.A.: Neural Networks and Deep Learning, Chapter 1. Determination Press (2015)
Artiﬁcial Neural Networks Application in the Backcalculation
559

BIM Project Execution Planning Suited
for Road Infrastructure Pilot Project in Bosnia
and Herzegovina
Saša Džumhur1(&), Žanesa Ljevo2, and Jasmina Marić1
1 IPSA Institute LLC Sarajevo, Sarajevo, Bosnia and Herzegovina
{sasa.dzumhur,jasmina.maric}@ipsa-institut.com
2 Faculty of Civil Engineering, University of Sarajevo, Sarajevo, Bosnia and
Herzegovina
zanesahandzar@yahoo.com
Abstract. Being aware of the “BIM boom” in architecture, engineering and
construction industry, a team of experts from the University Sarajevo Faculty of
Civil Engineering and from IPSA INSTITUTE LLC Sarajevo decided to
develop a scenario adopted to the current public procurement practice for road
infrastructure projects in Bosnia and Herzegovina and accordingly to implement
a worldwide recognized BIM project planning methodology that will lead to a
successful BIM Pilot Project development. BIM Project Execution Planning
suited for road infrastructure pilot project in Bosnia and Herzegovina provides
technology, process and policy requirements for object-oriented modelling—1st
BIM stage applicable in local conditions/environment.
Keywords: BIM  Planning  Pilot project  Bosnia and Herzegovina
Infrastructure
1
Introduction
Building Information Modeling (hereinafter: BIM) is not only a tool in construction, it
is a new powerful concept for planning and realization in our complex world. BIM has
the potential to bring more intelligence into the construction sector. Using BIM in the
Bosnia and Herzegovina has not yet signiﬁcant, while in the United States of America
(USA) and Canada more than 35% of companies have been applying BIM over 6 years,
and only 14% are beginners who use it 1 or 2 years, in Europe 47% companies have
been applying BIM for 1–2 years and 41% for 3–5 years [1].
The European Union (EU) Directive 2014/24/EU1 on Public Procurement states
that “for public works contracts and design contests, Member States may require the
use of speciﬁc electronic tools, such as of building information electronic modelling
tools or similar”. Bosnia and Herzegovina, as a potential EU candidate country, will be
obliged to transpose all EU Directives into its legislation.
1 Ofﬁce Journal of the European Union, Directive 2014/24/EU of the European Parliament and of the
Council of 26 February 2014 on Public Procurement and repealing Directive 2014/18/EC, Article 22
(4), Volume 57, Publication ofﬁce of the European Union, March 28 2014.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_50

Having in mind the above stated, but also being aware of the “BIM boom” in
architecture, engineering and construction industry, a team of experts from the
University Sarajevo Faculty of Civil Engineering and from IPSA INSTITUTE LLC
Sarajevo decided to develop a scenario adopted to the current public procurement
practice for road infrastructure projects in Bosnia and Herzegovina and accordingly to
implement a worldwide recognized BIM project planning methodology that will lead to
a successful BIM Pilot project (Pilot Project Scenario).
It is important to notice that the BIM Pilot Project is non-commercial and its only
purpose is educational. So, for the developed Pilot Project Scenario, the chosen Public
Procurement method was the Design-Tender-Build where the Public Client (i.e.
“Public Enterprise Roads of Federation of Bosnia and Herzegovina”) requested 1st
stage of project development (readiness to apply for the planning permit) for the
Kiseljak bypass road.
The ﬁnal aim of the Pilot Project is not only to improve coordination and collab-
oration between different design disciplines but also to introduce a different approach to
road infrastructure project implementation to the Public Client, thereby offering a lot of
potential beneﬁts to be achieved (e.g. to recognize and at the earliest project stage avoid
issues that could cause extra costs and delays in the following project stages).
Based on the experiences of other countries (mainly the USA), we can claim that
the transition to BIM technology is not matter only an upgrade of software, but requires
a certain organizational changes in the project team [2].
Figure 1 provides a collaborative BIM project initiation workﬂow and the intention
is to show not only steps required in earliest planning stage of a BIM project but also to
pinpoint strong connection between BIM project execution planning and BIM col-
laboration goals. Moreover, this paper is structured so to keep in line with the depicted
processes workﬂow, starting with the BIM Pilot project scope and going all the way to
the BIM project initiation.
Upon review of major BIM guides in use worldwide, the Pilot Project team agreed
that Penn State University BIM Project Execution Planning Guide [3] is the best choice
when it comes to creation of project-speciﬁc BIM Execution plans. Of course, some
adaptations to local conditions and the developed Pilot Project Scenario (as described
in the paper) were required.
The Key Features of the BIM Execution Plan Guide are:
• Clear methodology for establishing project goals and mapping the execution
process;
• Clariﬁcation of projects roles, participants and collaboration processes;
• Deﬁnitions of BIM Functions;
• Guidance and worksheets for process maps and information exchange.
Association of major Belgian contractors’ BIM deﬁned practical guidelines related
to digital exchanges within the classical contract procedure context (“Design-Tender-
Build”). “Building Information Modelling—Belgian guide for the construction
industry” providing information about BIM, its use and the speciﬁc roles/actors that
have to be taken into account and incorporated in a BIM process as well as general
requirements related to BIM collaboration, document sharing and data management.
Moreover, the Belgian guide provides generic BIM protocol and process map
BIM Project Execution Planning Suited for Road Infrastructure
561

representing the traditional contract by phase. Therefore, this publication helped the
Pilot project Scenario developers to suite BIM Project Execution Planning Method-
ology for road infrastructure pilot project in Bosnia and Herzegovina.
Finally, “CORPORATE BIM STANDARD For Infrastructure Projects Using
Autodesk Infrastructure Design Suite” provide general requirements to BIM models,
requirements to levels of development (LOD) with base LOD speciﬁcation for
infrastructure projects, Model Progression Speciﬁcation for development an aggregated
model, naming conventions, spatial location and coordination, roles and responsibili-
ties as well as collaborative BIM data sharing platform/common data environment.
2
The BIM Pilot Project Execution Planning Methodology
The implementation of each human action is in jeopardy when the planning process is
neglected or in worst case bypassed. When it comes to BIM project execution, a
lacking planning process usually necessitates extra resources, including extra time. In
addition, when it comes to the implementation of road infrastructure projects in Bosnia
and Herzegovina most problems are more or less related to a lack of information
management. As a result, there is a lack of collaboration and coordination between
Public Clients and designers that causes different types of errors, which is then reﬂected
Fig. 1. Collaborative BIM project initiation workﬂow [5]
562
S. Džumhur et al.

through higher costs and more time required for project implementation than it was
originally expected.
The BIM Pilot Project started in January 2017 when the involved actors agreed the
following project Mission: to promote and popularize BIM in implementation of
Bosnia and Herzegovina road infrastructure projects. The stated Mission was
amended with the following statement: Usage of BIM tools for better collaboration,
communication and information exchange as well as more efﬁcient resource usage in
the conceptual design phase of road infrastructure projects.
The actors involved are experts from two renowned Bosnia and Herzegovina
institutions: University Sarajevo Faculty of Civil Engineering and IPSA INSTITUTE
LLC Sarajevo as well as a representative of Public Client—“Public Enterprise Roads of
Federation of Bosnia and Herzegovina” (P.E. Roads of FBH).
“To effectively integrate BIM into the project delivery process, it is important for
the team to develop a detailed execution plan for BIM implementation. A BIM Project
Execution Plan (BIM Plan) outlines the overall vision along with implementation
details for the team to follow throughout the project. The BIM Plan should be
developed in the early stages of a project; continually developed as additional partic-
ipants are added to the project; and monitored, updated, and revised as needed
throughout the implementation phase of the project. The plan should deﬁne the scope
of BIM implementation on the project, identify the process ﬂow for BIM tasks, deﬁne
the information exchanges between parties, and describe the required project and
company infrastructure needed to support the implementation [3].”
The BIM Pilot Project Scope
Basic input for the BIM project execution planning is the project scope providing
basic information and enabling BIM manager to develop BIM strategy (including
technology requirements), create BIM processes and workﬂows as well as BIM
implementation standards and protocols. The basic BIM Pilot Project information are
listed below.
Project name: Applying BIM in the 1st stage of Kiseljak Bypass project devel-
opment (BIM Pilot Project);
Project location: Bosnia and Herzegovina; Federation of Bosnia and Herzegovina;
Middle Bosnia Canton, Municipality Kiseljak, Kiseljak urban area;
Contract type/delivery method: Public procurement procedure for road infras-
tructure planning, design and construction in Federation of Bosnia and Herzegovina
—“Design-Tender-Build” public procurement procedure. For the developed Pilot
Project Scenario, the Public Client (P.E. Roads of FBH) requested 1st stage of Kiseljak
bypass road project development (readiness to apply for the planning permit);
Brief project description: Bypass road of urban area Kiseljak is a new link
connecting Trunk Road M5 (Gromiljak—Kiseljak—Blažuj) and Regional Road R443
(Kiseljak—Visoko). Trunk road M5 passes through urban area Kiseljak. Along the M5
road alignment there are many commercial and residential buildings generating intense
trafﬁc ﬂow. In addition, unfavorable technical elements of the M5 road alignment
contribute to low level of service through Kiseljak urban area. Municipality Kiseljak
have recognized the problem and included Kiseljak bypass project both in its Strategy
BIM Project Execution Planning Suited for Road Infrastructure
563

of development and Spatial plan deﬁning approx. 1.5 km long corridor of the Kiseljak
bypass road.
Additional project information: For the 1st stage of Kiseljak bypass road project
development the Project owner requested detailed analysis of the following spatial
planning documentation: Spatial plan of Bosnia and Herzegovina; Spatial plan of
Middle Bosnia Canton and Spatial Plan of Municipality Kiseljak. According to
Rulebook on basic safety conditions for public roads including road furniture and
constructions the trafﬁc analysis is obligatory providing technical parameters for design
and dimensioning of carriageway construction elements. Also, according to the same
Rulebook, the preliminary analysis of level of services is required. Upon the analysis of
spatial limitations the Contractor will develop three alternatives, including one from
Spatial Plan of Municipality Kiseljak. The multi criteria analysis (MCA) will include
technical/operational, ﬁnancial, environmental and spatial criteria. The Project owner
together with Municipality Kiseljak authorized representatives will evaluate the best
rated variant from the MCA compare the results of all variants and approve the ﬁnal
variant to be developed through the following project stages.
Organizational roles/stafﬁng for the BIM Pilot Project is taken from “CORPO-
RATE BIM STANDARD For Infrastructure Projects Using Autodesk Infrastructure
Design Suite” (BIM STANDARD), as follows: BIM Manager is responsible for
Developing the corporate BIM strategy, Best practice/research, Creating BIM pro-
cesses and workﬂows, Creating and supporting BIM standards and protocols, BIM
implementation, Training strategy and BIM Execution Plan; BIM Coordinator
responsibilities include BIM Execution Plan, Training, Auditing the project data and
modelling principles, Participating in multidisciplinary coordination meetings as well
as Content creation and distribution, content quality control; BIM Modeler/BIM
Author is responsible for discipline-speciﬁc working on different parts of the project
with skill and experience in BIM software.
Technology Framework, Modelling Standards and Communication Protocols
One of the key BIM concepts is to integrate and visualize data from various sources
rather than being locked in by a single all-encompassing model and a single software
vendor. BIM is an effective tool in improving certain key aspects of the delivery of
construction projects. The success criteria of BIM: cost was the one most positively
inﬂuenced by the implementation of BIM followed by time, communication, coordi-
nation improvement and quality. The negative beneﬁts or challenges of implementing
BIM implementation are relatively fewer, and most of them are focused on software or
hardware issues [4].
According to [5] BIM capability stages (see Fig. 2) may be used to measure the
BIM capability maturity of organizations, teams and other macro organizational scales.
In that light, the actors involved in BIM Pilot Project recognized need to introduce
technology, process and policy steps required for the “object-based modelling” stage.
BIM can also be seen as a software platform allowing to coordinate or combine the
work of different stakeholders into one Building Information Model. A Building
Information Model, is a three dimensional (3D) object-oriented model with embedded
information. It means that it is a three dimensional representation of the building in
which all the elements that compose the buildings are considered as “objects”
564
S. Džumhur et al.

connected to each other. Each object has a unique identiﬁer and relates information
about its geometry and its properties [6].
Technology/software framework when it comes to BIM project execution planning
includes identiﬁcation of software solutions (BIM tools), possibilities for information
exchange/interoperability (BIM platform) and selection of common data environment
(BIM environment). Managing information is essential for the BIM project collabo-
ration, so all relevant information must be shared in such a way to be available timely
and in right format to all interested parties. Key for interoperability lies in ﬁnding a
common language that enable organization, classiﬁcation, identiﬁcation and sharing of
information.
Having in mind the BIM Pilot Project scenario developed to support the project
mission, the BIM pilot project team recognized Autodesk A360 supported with
Autodesk Cloud provides excellent common data environment for implementation of
scenario developed for the BIM Pilot Project enabling collaboration in the cloud.
Furthermore, the project team agreed Autodesk Infraworks 360 is the best solution for
the pilot Project BIM platform supporting not only various data source types import
(e.g. 3D models, LandXML Files, Raster Files, Shape Deﬁnition Files etc.) but also
interoperability with BIM tools required for the later project stages (e.g. NAVIS-
WORKS, AUTODESK 3D CIVIL and so on). Finally, common software solutions for
word processing (MS Word), calculations (MS Excel), spatial data analysis (ESRI
ARC GIS) and so on will support BIM Pilot Project execution.
When it comes to Modelling standard establishment, the BIM Pilot Project team
agreed to use BIM STANDARD respecting the fact it was written with reference to
ISO/TS 12911:2012 “Framework for building information modeling (BIM) guidance”
and BS 1192:2007 “Collaborative production of architectural, engineering and con-
struction information/Code of practice”.
At last, but not the least, there are communication procedures as well as intellectual
property issues that should be deﬁned through protocols/procedures in such a way to
support reliable and efﬁcient information exchange. Having in mind that developed Pilot
Project Scenario includes just the 1st stage of Kiseljak Bypass project development, the
BIM pilot project team also agreed to include all required protocols/procedures (e.g.
information exchange protocol, archiving procedures and so on) in a comprehensive
BIM PLAN.
Fig. 2. BIM capability stages [5]
BIM Project Execution Planning Suited for Road Infrastructure
565

3
The BIM Pilot Project Execution Planning Process
Project Execution Planning Process is the ﬁnal step before the BIM Pilot Project
initiation and the ﬁnal product is the BIM PLAN. Since the pilot project scenario is
developed for the “object-based modelling” stage the project team agreed that
“Employer’s Information Requirements”2 include the previously agreed common data
environment (CDE) as well as BIM tools and BIM STANDARD to be applied.
Having in mind that the development of the BIM Plan by its nature is a collabo-
rative process, the BIM pilot project team also agreed to follow four step planning
procedure developed by The Pennsylvania State University Computer Integrated
Construction Research Program (see Fig. 3). The procedure consist of four main steps:
1. Identify BIM Goals and Uses;
2. Design BIM Project Execution Process;
3. Develop Information Exchanges;
4. Deﬁne Supporting Infrastructure for BIM Implementation.
BIM Dictionary deﬁnes Model Uses as “expected or intended project deliverables
expected from generating, collaborating-on and linking 3D models to external data-
bases”. The BIM Pilot Project BIM uses are: Conceptualization/existing conditions
modelling allowing the initial investigation of design possibilities and spatial
requirements; 3D Coordination through a BIM model to reduce/eliminate ﬁeld
Fig. 3. The BIM project execution planning procedure [3]
2 A document/s clarifying the employer’s requirements during services’ procurement. Employer’s
Information Requirements (EIR) may include levels of modelling detail, training/competence
requirements, ordinance systems, exchange formats or other employer-mandated processes,
standards or protocols (http://bimdictionary.com).
566
S. Džumhur et al.

conﬂicts and visualize project; Cost Estimation—A Model Use representing how 3D
models are used to generate feasibility studies and compare different budgetary options;
Visual Communication where 3D models are generated or enhanced for the purposes
of communicating visual, spatial or functional qualities.
“The Project Speciﬁc BIM Use Process Maps shall contain a detailed process plan
that clearly deﬁnes the different activities to be performed, who will perform them, and
what information will be created and shared with future processes” [3]. Figure 4
presents Level One Team Process Overview Map with four BIM Pilot Project Uses and
ﬁve BIM information exchanges included, of which two internal (D 1.0: Inputs for
Preliminary BIM Model and D1.1: Inputs for Coordination BIM Model) and three
external (D1.2: Deliverable for Design Review; D1.3: Inputs for Feasibility Study and
D2.0 Inputs for Planning Permit). Project Phase (e.g. Preliminary Design), responsible
party (e.g. BIM Modelers), BIM Use (e.g. Conceptualization) and corresponding BIM
Model (Preliminary BIM Model) are deﬁned for each Process in the Overview Map
(see Fig. 4).
Moreover, Requirements for exchange (Authoring party, Receiver and Expected
Content) are deﬁned for each BIM information exchange (INFO exchange). So, for
example, Table 1 gives Requirements for INFO exchange D1.2.
Collaboration strategy is based on adopted BIM STANDARD, deﬁning CDE as a
means of allowing information to be shared efﬁciently and accurately between all
members of the project team, and enables multi-disciplinary design teams to collabo-
rate in a managed environment. The ﬁgure below illustrate the BIM Pilot Project
environment/CDE structure. Autodesk A360 together with Autodesk Cloud provides
Fig. 4. BIM pilot project level one team process overview map
BIM Project Execution Planning Suited for Road Infrastructure
567

excellent CDE for implementation of developed BIM Pilot Project scenario (enabling
collaboration in the cloud) also allowing electronic communication between all project
stakeholders (Fig. 5).
Table 1. Requirements for INFO exchange D 1.2
Authoring party
BIM coordinator
Receiver
Project owner/external auditor
Expected
content
Deliverables for reviewer (3D Coordinated BIM model, Collision analysis
etc.)
Fig. 5. Common data environment structure [7]
568
S. Džumhur et al.

Information models are developed in stages (see Fig. 4). Completion of BIM model
is determined by the level of development according to the Level of Development
(LOD) speciﬁcation deﬁned in BIM STANDARD for each model development stage.
For example, Preliminary Design Stage (deﬁned in adopted BIM STANDARD) is
characterized by low LOD, so corresponding Preliminary BIM model contains only the
basic elements on which the development will be carried out at later stages.
By adoption of BIM STANDARD, the BIM Pilot Project team implicitly adopted
Autodesk Infrastructure Design Suite software to be used for the BIM Pilot Project
execution. More precisely, Autodesk Infraworks 360, AutoCAD Civil 3D and Auto-
desk Navisworks Manage will be the main BIM tools during the BIM Pilot Project
execution.
The BIM Pilot Project team developed Project Folder Structure and Naming
Conventions as recommended in BIM STANDARD (recommendations are based on
BS1192:2007 Standard). The metric system and “MGI Balkans Zone 6” coordinate
system are adopted as standard for the BIM Pilot Project.
The BIM Pilot Project Coordinator is put in charge of the internal quality control
including:
• Document management (matching with adopted BIM STANDARD of ﬁle formats,
software versions, model elements coding as well as model elements, ﬁles, layers
and styles naming conventions);
• Model ﬁles update check (checking both content of model and model accompa-
nying tables);
• Visual check and Element Validation (to ensure there are no unintended model
elements and that there are no undeﬁned or incorrectly deﬁned elements.);
• Interference/Clash check (through collision analysis).
Having in mind previously deﬁned the BIM Pilot Project purpose, but also
respecting project Mission, the deliverables are suited to serve for education, promotion
and popularization of BIM in Bosnia and Herzegovina. Consequently, BIM PLAN
together with all developed BIM models and model accompanying ﬁles will eventually
be presented to public bodies and students on a specialized BIM workshop.
4
Conclusions
BIM Project Execution Planning suited for road infrastructure pilot project in Bosnia
and
Herzegovina
provides
technology,
process
and
policy
requirements
for
object-oriented modelling—1st BIM stage applicable in local conditions/environment.
Developed BIM PLAN enables BIM process mapping based on previously deﬁned
BIM goals and uses as well as corresponding information exchange requirements and
supporting infrastructure for BIM implementation.
By applying BIM Project Execution Planning methodology and process, the par-
ticipants in a road infrastructure project development can achieve higher productivity
and quality of work, have control over all information and changes within a project
early on to identify possible errors, provide more detailed and precise analysis of the
project, collaborate interactively with other participants and thus achieve a high level of
BIM Project Execution Planning Suited for Road Infrastructure
569

communication, identify problems early in the project, make cost estimates faster and
more accurately.
The ﬁnal BIM Pilot Project intention is not only to promote and popularize BIM in
Bosnia and Herzegovina but also to start process of setting guidelines and adopting
standards for (road infrastructure projects) BIM implementation.
References
1. Kolarić, S., Pavlović, D., Vukomanović, M.: Developing a methodology for preparation and
execution phase of construction project. Org. Technol. Manag. Constr. 7(1) (2015)
2. Post, N.M.: Building Team Views Technological Tools as Best Chance For Change (2010)
3. Computer Integrated Construction Research Program. BIM Project Execution Planning Guide
—Version 2.1. The Pennsylvania State University, University Park, PA, USA (2011)
4. Bryde, D., Broquetas, M., Volm, J.M.: The project beneﬁts of building information modelling
(BIM). Int. J. Proj. Manag. 31 (2013)
5. Succar, B., Sher, W., Williams, A.: An integrated approach to BIM competency acquisition,
assessment and application. Autom. Constr. 35 (2013)
6. Building Information Modelling—Belgian Guide for the Construction Industry. Version: 1.0,
p. 8 (2015)
7. Corporate BIM Standard: For Infrastructure Projects Using Autodesk Infrastructure Design
Suite. TEMPLATE, Revision 1.01, p. 16 (2015)
570
S. Džumhur et al.

Demographic Analysis Using Modern GIS
Software Tools—Case Study of the Republic
of Srpska (Bosnia and Herzegovina)
Nikolina Mijic1(&) and Jovo Ateljevic2
1 Faculty of Earth Science and Engineering, Institute of Geophysics and
Geoinformatics, University of Miskolc, Miskolc, Hungary
nikolinamijic7@gmail.com
2 Faculty of Economics, University of Banja Luka, Banja Luka,
Bosnia and Herzegovina
jovo.ateljevic@gmail.com
Abstract. This chapter introduces latest location software tools and techniques
throughout the process of collecting, mapping, storing and representing statis-
tical data of the Republic of Srpska (Bosnia and Herzegovina). Inherent in
Geographic Information Systems (GIS) data is information on the attributes of
features as well as their spatial distribution. Spatial analysis more than often uses
methods adapted from conventional analysis to address problems in which
spatial location is the most important explanatory variable. Besides subsequent
raw data processing, this information is used to create geographical maps that
can be observed both visually and spatially. Geospatial statistical analysis helps
extract additional information from collected geodata that might not be so
obvious simply by looking at the numbers information such as how attribute
values are distributed, whether there are spatial trends in the data, spotting
outliers (extreme high or low values), or whether the features form certain
spatial patterns. In principle there is no limit to the complexity of spatial analytic
techniques that might ﬁnd some application in the world, and might be used to
tease out interesting insights and support practical actions and decisions. In
reality, some techniques are simpler, more useful, or more insightful than others,
and the contents of this paper reﬂect that reality. Provided GIS-based software
tools and suggested workﬂows enable feature mapping and/or importing
ready-made feature geometry from various external sources, e.g. web mapping
services (WMS), web feature services (WFS)—among the others. Tools enable
spatial data storage instantiation and maintenance capabilities using common
data store types such as ﬁle-based (e.g. SQLite, Esri SHP, Autodesk SDF…) and
relational DBMS (Microsoft SQL Server, Oracle Database, PostGIS…).
Keywords: GIS  Software  Demography  Spatial analysis  Statistic methods
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_51

1
Introduction
The recent trends in the development of geo-demographic studies show signiﬁcant
growth in their number and use of increasingly sophisticated tools. The considerable
amount of demographic statistical information calls for their systematization using the
mathematical methods and the application of computer software.
Characteristic feature of the modern cartographic tools is the wide use of the
computer and GIS technologies. For processing spatial data, the Environmental Sys-
tems Research Institute (ESRI) has created the ArcGIS package, which allows visu-
alising data, performing mathematical calculations, spatial and geostatistical analysis in
2D and 3D dimensions. Maps have a special place in GIS. The process of making maps
with GIS is much more ﬂexible than are traditional manual or automated cartography
approaches. It begins with database creation. Existing paper maps can be digitized and
computer-compatible information can be translated into the GIS. The GIS-based car-
tographic database can be both continuous and scale free. This allows the creation of
map products which are centered on any location, at any scale, and showing selected
information symbolized effectively to highlight speciﬁc characteristics. The charac-
teristics of atlases and map series can be encoded in computer programs and compared
with the database at ﬁnal production time. Digital products for use in other GISs can
also be derived by simply copying data from the database. In a large organization,
topographic databases can be used as reference frameworks by other departments.
Geospatial analysis concerns what happens where, and makes use of geographic
information that links features and phenomena on the Earth’s surface to their locations.
It extends traditional statistics on two fronts—ﬁrst, it seeks to map the variation in a
data set to show where unusual responses occur, instead of focusing on a single typical
response. Secondly, it can uncover “numerical spatial relationships” within and among
mapped data layers, such as generating a prediction demographic maps.
Related software application also help easily summarize or average scattered data
for certain regions or feature categories, then create outputs such as new visualizations
or alphanumeric reports. Related stylization techniques include customized symbolical
elements (depicting totals using symbol size and count), pie charts (illustrating relative
magnitudes or frequencies of attribute values), bar graphs (representing raw values
distributed by attribute series) and many more.
2
Methods of Demographic Spatial Analysis
Demographic analysis is predicated on the accurate and systematically recorded pop-
ulation data for the area. The methods, which can be used for demographic analysis are:
(1) complete periodic census enumeration, (2) continuous population registration, and
(3) obtaining population data by estimation [1].
The methods of statistical data analysis fall into multivariate data analysis and
univariate data analysis. Univariate statistical techniques represent a variety of basic
descriptive statistics and include different methods of analysis. The most important
methods of unvariate statistical data analysis are: nearest neighbor analysis, reﬁned
nearest neighbor analysis, K-function, weighted K-function, space-time Knox, spatial
572
N. Mijic and J. Ateljevic

autocorrelation, autocorrelograms, and variograms [2–4]. Spatial econometric mod-
elling, geostatistical and spatial general linear modelling are concerned with modelling
the relationship between one response variable of particular interest and others that may
explain its spatial variation. Multivariate statistics provide the ability to analyze
complex sets of data. Common methods of modelling multivariate data include
descriptive statistics, multivariate statistical analysis, multivariate spatial correlation,
Clustering, Geostatistical, Spatial econometric modelling, factorial ecology and spatial
general linear modelling [4–6].
2.1
Statistical Analysis of Spatial Data
Under spatial analysis, the focus is a spatial data set i.e. a data set in which each
observation is referenced to a site or area (geographical location). Much of demo-
graphic data is collected in spatial context and requires statistical analysis for inter-
pretation. Methods of analyses of spatial data include data description, map
interpolation, exploratory data analyses (descriptive statistics), explanatory analyses
and conﬁrmatory data analyses [2]. There are several reasons why spatial analysis is
key to integrated assessment framework. First, there is a strong link between humans
and their environment. Spatial analysis techniques and methods help to incorporate
spatial elements so that develop a clearer picture of this human/environment link.
Second, for population study, different group’s outcomes require varying spatial res-
olution of population model. Last but not least, people’s actions and activities are
spatial. Spatial statistics summarize and describe numerically a variety of spatial pat-
terns. Spatial statistics fall into three categories: Point pattern analysis, spatial auto-
correlation, and Geostatistics (include descriptive spatial statistics) [7].
Descriptive statistics addresses itself to summarizing in brief form the information
contained in a distribution. They are used to describe the basic features of the data in a
study. Descriptive statistics are divided into basic descriptive statistics (these are
aspatial which include central tendency (shows the trend in the distribution and include
mean, median, and mode) and dispersion (shows the extent of dispersion about the
central tendency, three common measures of dispersion, the range, the standard
deviation). The entropy is an index of uncertainty representing in a quantitative way
how well we can predict which value a random variable will take on [8, 9].
The section dealt with a simple set of numbers, made no reference to geographic
location, x, y coordinates or anything spatial. The distribution of point features can be
described by frequency, density, geometric center, spatial dispersion, and spatial
arrangement.
Geometric center: The geographical properties of a point pattern are characterized
by the geometric center, the dispersion, and spatial arrangement [9]. This shows how
points are distributed over the map area and whether the points are near each other, near
the center of a demarked area (say occupied by certain ethnic group) or clustered near
the corner.
Spatial Mean: This statistic locates the “center of mass” of the data. If you con-
sider the area of study as being a thin plate (of zero mass), and the value of each data
point as being a point mass on the plate, the spatial mean is the location you’d have to
Demographic Analysis Using Modern GIS Software Tools
573

put your ﬁnger under to have the plate balance on it. Each coordinate of the spatial
mean is computed separately, using the grouped data mean formulae:
x ¼
P fixi
P fi
y ¼
P fiyi
P fi
ð1Þ
If the spatial mean of the data is signiﬁcantly different from the geographic center of
the region, this indicates a non-uniform distribution. If the individual x and y coor-
dinates of the data points are not known, the spatial mean can be estimated, at the cost
of introducing measurement error, by the following procedure: Impose a rectangular
grid on the area. Sum the frequencies (i.e. data point values) in each column along the
x-axis and row along the y-axis.
Standard Distance: This is the two-dimensional equivalent of the standard devi-
ation, and is a common measure of dispersion. Large value of the standard distance
means that the points are relatively scattered, while a small value means they are
relatively clustered.
sD ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
s2
x þ s2
y
q
ð2Þ
2.2
Multivariate Analysis
Multivariate analysis is explored in this research as it has a big family of techniques and it
is promising for GIS demographic data in planning. The family of techniques can assist
in the accomplishment of (1) description (ﬁnd patterns of relationships where the human
eye and even univariate or bivariate statistics fail to do so), (2) explanation (other
techniques have special capabilities to explain or to help explain relationships for
instance, (3) prediction and (4) control (the techniques themselves will not institute
control, but their application can help a decision maker develop cause-and-effect rela-
tionships and models, which will ultimately enhance their ability to control events to
some degree).
As data sets for demographics are always big sized, a factor analysis, which is an
interdependence technique (i.e. the variables are left in a single set, rather than being
divided into two sets) is desired to study the interconnections (correlations) among the
variables), helps in assessment of performance or attitude of different variables, i.e.
grouping variables which tend to lead to same conclusion, ending up with few factors.
2.3
Data Presentation and Visualization Techniques
Presentation techniques cover different approaches to the visualisation and presentation
of demographic datasets. These approaches are designed around the concept of data
spaces, and present the user with a series of tables, pictures, or graphics, each
describing a data space, and with tools to explore the synergy between data spaces data
presentation and visualisation.
For pictures and graphic for demographic presentation, the task is relating people to
where they actually occur on the ground. In most cases this means mapping the
574
N. Mijic and J. Ateljevic

distribution of people in terms of their place of residence. Traditional this has been
accomplished in many ways: using data tables, data pictures and graphics (symbols for
location e.g. circle, cubes), Choroplethic maps, cartograms or Lorenz curves [10, 11].
3
Spatial Analysis and Modeling Using GIS Software Tools
Phenomena in the real world can be observed in three “modes”, namely, spatial,
temporal and thematic. The spatial mode deals with variation from place to place, the
temporal mode deals with variation from time to time (one slice to another) and the
thematic mode deals with variation from one characteristic to another (one layer to
another). All measurable or describable properties of the world can be considered to
fall into one of these modes, viz. place, time and theme, however, an exhaustive
description of all three modes at the same time is not possible with today’s technology
[12]. Therefore, in reality, when observing real-world phenomena, we usually hold one
mode “ﬁxed”, vary one in a “controlled” manner, and “measure” the third; holding
geography ﬁxed and varying time gives longitudinal data, while holding time ﬁxed and
varying geography gives cross-sectional data.
3.1
Geographical Data Analysis and Its Elements
GIS and spatial analysis have enjoyed a long and productive relationship over the past
decades [13–15] the origins of spatial analysis lie in the development in the early 1960s
of quantitative geography and regional science [8]. GIS has been seen as the key to
implementing methods of spatial analysis, making them more accessible to a broader
range of users, and hopefully more widely used in making effective decisions and in
supporting scientiﬁc research. Any statistic we can think of to describe the data then
automatically has geographic properties and as a result can be placed on map for visual
processing. It has been argued [13] that in this sense the relationship between spatial
analysis and GIS is analogous to that between statistics and the statistical packages.
Meanwhile, the term spatial analysis is reserved for methods that either investigate
patterns in spatial data or seek to ﬁnd relationships between such patterns and the
spatial variation of other attributes, or for methods of spatial modeling. The former type
of spatial analysis, however, as a statistical spatial analysis or simply spatial statistics, is
currently poorly represented in the technology of GIS. This type of analysis would
include such areas as nearest neighbour methods and K-functions, Kernel and Bayesian
smoothing methods, spatial autocorrelation, spatial econometric modelling, and spatial
general linear models.
Types of spatial analysis [8]
include: Single layer operations (GIS procedures
which correspond to attribute queries, spatial queries, and alternations of data that
operate on a single data layer), Multiple-layer operations which useful for manipula-
tion of spatial data on multiple data layers, Spatial modeling which involves the
construction of explanatory and predictive models for statistical testing, Point pattern
analysis that deals with the examination and evaluation of spatial patterns and the
processes of point features, Network analysis, designed speciﬁcally for line features
organized in connected networks, typically applies to transportation problems and
Demographic Analysis Using Modern GIS Software Tools
575

location analysis, Surface analysis deals with the spatial distribution of surface infor-
mation, Grid analysis involves the processing of spatial data in a spatial, regularly
spaced form, spatial overlay, Boundary analysis, Proximity analysis, and Buffer
analysis
The analysis of spatial order and spatial association requires the following three
elements of spatial information: (1) Location: the exact location of every spatial feature
must be available, (2) Attribute data provides important information about the prop-
erties of the spatial features under consideration, and (3) Topology is deﬁned as the
spatial relationships between map features [8, 16]. From [8, 17, 18] GISs are indis-
pensable for spatial analysis because of their ability to integrate all the three elements of
spatial on formation in locally consistent manner. A database management system
handling only attribute is best useful for aspatial statistical analysis. A computer system
capable of handling location and attribute data but not topological elements is suitable
for automated cartography, but not spatial analysis. A typical automated cartography
system provides mapping functions for organization and presentation of spatial
information. In a spatial relationships among map features can be effectively processed
only by using GIS that provides the functionality to handle all three types of elements.
3.2
GIS Spatial Analysis Techniques
Spatial analysis is in many ways fundamental to the effective use and further
exploitation of GIS in many different applied contexts like in demography. Spatial
analysis and modeling of human spatial systems is now rapidly emerging as a new
grand challenge area, for the late 1990s [19]. Although from a GIS perspective, the
analysis and modeling tasks only become important once GIS has become an estab-
lished technology. It is apparent that this post-GIS revolution era has been reached and
that the focus of research attention is now moving on from Geographic Information
Handling to Geographic Information using with the obvious greatly increased emphasis
on creating appropriate analysis and modeling functionality [20]. In spatial analysis,
what is being looked for are geographically localized patterns at disaggregated level
and it is these features that are often of greatest interest and also the hardest to ﬁnd.
While there is a general consensus that the lack of spatial analysis functionalities in
current GIS seriously limits the usefulness of GIS as a research tool to analyze spatial
data and relationships [21–24] there is no agreement about what kinds of spatial
analysis techniques and methods are most relevant to GIS environments. The most
important GIS relevancy criteria that spatial data analysis (SDA) tools should ideally
attempt to meet may be summarized as follows:
• A GIS able SDA tool should be able to handle large and very large numbers (from
a few tens to millions) of spatial objects without difﬁculties, and thus meet the
large-scale data processing needs in GIS.
• GIS relevant SDA techniques should be sensitive to the special nature of spatial
information.
• The most useful GIS able SDA techniques and models will be frame indepen-
dent (i.e. invariant under alternative spatial partitioning of a study region).
576
N. Mijic and J. Ateljevic

• GIS relevant SDA should be a safe technology (i.e. the results should be reliable,
robust, resilient, error and noise resistant, and not based in any important way on
standard distributions).
• GIS able SDA techniques should be useful in an applied sense, (i.e. focus on spatial
analysis tasks that are relevant to GIS environments).
• The results of SDA operations should be mappable to afford understanding and
insight, since GIS is a highly visual and graphics oriented technology.
From above it can be seen that there is no agreement about what kinds of spatial
analysis techniques and methods are most relevant to GIS environments for speciﬁc
ﬁelds like demographic data analysis.
4
Demographic Analysis Using GIS Tools
GIS has three components map, database and spatial analysis. First two are in most
cases used and easily incorporated in demographics. When comes to spatial analysis,
although GIS are not directly set out be used for demographic analysis, the task is being
explored so that demographic spatial analysis can be fully carried out in GIS. Demo-
graphic data have some properties that make it difﬁcult to deal as they are intended to
explain or manage the behavior of individuals or groups, the position and the
boundaries of demographic phenomena cannot be directly determined through obser-
vation or measurements, the phenomena are linked to people and their activities
therefore, their distribution over space is often extremely uneven and heterogeneous,
and these phenomena are not permanent but transient [25]. Also because of the very
heterogeneous sources of population data, a variety of integration problems can occur:
missing positional information, inconsistent classiﬁcations and methodologies, differ-
ent spatial units, different levels of aggregation (“resolution”), thematic and spatial data
gaps, and different time references. Thus GIS Demographic analysis problems can be
outlined as:
• The problem of demographic georeferencing i.e. how demographic data be geo-
coded efﬁciently to represent characteristics, carry out spatial analysis and modeling
• Demographic characteristics like gender, marital status, etc. have two differentiating
values like female/male, single/married. Although we are interested in representing
such characteristics, there is need also to have their quantities at the same.
• Which data structure in 2D, 2.5, and 3D can accomplish demographic spatial
analysis and modeling; is it a raster based or vector based GIS.
GIS provides planners with more demographic data processing, exploratory, and
manipulation in spatial terms, this has been due to GIS effect on spatial statistical
analysis, which has led to broadening of process of hypothesis testing [26]. Thus
providing way for GIS Demographic spatial analysis basing on spatial statistical
analysis much more ﬂexible as it can be noted from the Fig. 1, a step has been added to
the traditional approach of hypothesis guided inquiry, and most steps have been
expanded to include more opportunities to asses data from different vantage points. The
Demographic Analysis Using Modern GIS Software Tools
577

added step, data manipulation, presents planners with opportunities to use larger
samples, view data over a series map scale, and generally to be in a stronger position to
carry out statistical tests on demographic data by means of simulations.
With GIS approach the planner being able to carry out interactive spatial data
analysis making the human factor more directly in the exploration of data [27] and
thereby gain richer insight than possible with traditional rigid and static display. Where
the user can delete data points, highlight subsections of the data, establish links
between the same data points in different graphs, and rotate, cut through, and project
higher-dimensional data.
Fig. 1. Traditional and GIS approaches to demographic analysis
578
N. Mijic and J. Ateljevic

4.1
Demographic Spatial Informatics
As planning broadens its focus toward the determinants of population living conditions
at household level, GIS can perform several functions in demographic spatial infor-
matics (DSI). In demographic population analysis a more important precondition is a
systematic, integrated approach to geocoding all population-based data systems. With
routinely geocoded databases, GIS can fulﬁll many roles in DSI. Functions include:
• An interactive environment for the spatial display of demographic data; a laboratory
for the development and dissemination of neighborhood/community demographic
indicators; a tool for integrating disparate data records by location; a vehicle for
displaying results of analyses from databases merged by automated record linkage;
a platform for testing hypotheses concerning the demographic determinants in
planning process, or associations between determinants of population location and
utilization of facilities; and as a vehicle for facilitation of population program
planning, evaluation, and community-based decision making.
• GIS demographic analysis to be used in planning, to facility in showing how the
population interacts with other features of urban form through combining popula-
tion data with other layers in planning process at disaggregated spatial level.
• Use GIS for demographic updating as it is quite difﬁcult to use other techniques
which can be applied to other contents in the plan, for example building changing
can be updated using satellite images which is not the case with other population
data.
• With GIS using population as the common layer having common geographical
location for all other entities/components in planning process to achieve a com-
prehensive out put where proposals reinforce each other to further public interest. In
order to achieve that without great distortion or harmful side-effects of magnitude
sufﬁcient to out weigh the gains achieved, use GIS, which can combine the different
specialties, make simulations of action and outcome and combine all with the
population according to demographic characteristics.
4.2
Presentation of Demographic Data in GIS
Having to put demographics in GIS there is need to look at how the different entities
will be represented in order to be able to manipulate them. The Table 1 highlights how
the demographic characteristics can be represented using the GIS primitives (points,
lines, area, and polygons). The columns illustrate four classes of geographical phe-
nomena namely point, line, area, and surfaces. The rows illustrate four stages in rep-
resentation process. Demographic characteristics exist in the real world, georeferencing
them provides the link with digital objects, which may be used to represent their
locations, then GIS provides the manipulation tools for the creation of new objects and
visualisation techniques are applied to each case. Between these stages are the data
collection and entry, data manipulation, and data output transformations.
The stages to carry out GIS demographic spatial analysis follow Fig. 2. Geocoding
refers to the process of associating a data point with a geographic location based on
some form of address. This address need not necessarily be a street or mailing address,
but can be any key identiﬁer of a particular location, such as the name of a place or the
Demographic Analysis Using Modern GIS Software Tools
579

Table 1. Presentation of demographic data in GIS
Point
Line
Area
Surface
Real world entity Individual persons Street/road
residence
Building/zone of
living
Population
density
Data collection and entry
Digital object
Personal
coordinates
Street
coordinates
Building/zone
boundary
TIN or 3D-DM
Data manipulation
Manipulation
technique
Nearest neighbor
analysis
Topological
analysis
Areal
interpolation
Slope analysis
Boundary
generation
Centroid
generation
TIN creation
Surface
generation
Surface
generation
Analysis of
surface form
Data output transformations
Visualisation
technique
Point mapping
Line mapping
Choropleth
mapping
Tin mapping
Multivariate
display
Line
cartograms
Areal cartograms
Grid mapping
Convert to 3D
Convert to 3D
Convert to 3D
Convert to 3D
Fig. 2. GIS demographic analysis procedure
580
N. Mijic and J. Ateljevic

lot and block number of a property parcel. Geocoding is the mechanism that allows
using addresses to identify locations on a map [28]. It is the process of inputting spatial
data in the GIS by assigning geographical coordinates to each point, line, and area
entity [29].
GeoProcessing is a way to create new data based on themes in a view. In most cases
alter the geometric properties of the features in a dataset while controlling some aspects
of how its attribute data is handled [30]. Linking attribute utilizes the linkage with
tables. A table lets you work with data from a tabular data source in ArcView. This
facility is exploited to bring tabular demographic data from SPSS and Microsoft Access
Database where it is analyzed into ArcView as tables. Then this data from the tables is
added to maps, and symbolize, query and analyze this data geographically.
As a GIS geared towards the analysis and/or representation of the population
demographic characteristics should adopt one of one of the three approaches [31]: First
is the Individual level approach, in which data are held relating to every individual
person in the population, second is areal aggregation approach is the most common and
includes the convectional choropleth census mapping and the third option begins with
the assumption that demographic phenomena of interest to the analysis are essentially
continuous over space, and attempts are to reconstruct this continuity.
4.3
GIS Spatial Modeling
Spatial modeling of individuals can help us understand dynamic population level
processes such as spatial similarity and habitat availability and preferences. Spatial
patterns are the result of spatial processes. These processes can be described, measured,
and evaluated using various methods of point pattern analyses.
GIS software like ArcView GIS can associate spatial and aspatial attributes, for this
to be done at micro level further modeling are needed. Starting with the conceptual
model where the demographic components of the individual persons are outlined, then
mathematical models in order to operationalize conceptual models formulated by
representing them with mathematical constructs and ﬁnally with the possibilities of
scale models to organize mathematical models so that the real world features can be
represented [32]. All this is done in GIS using a combination of raster and vector
presentation of spatial elements to lead to spatially disaggregate models that are able to
over come some of the disadvantage of zonal models. This type of analysis is termed
micro simulation [33] and there are four ﬁelds in which GIS can support micro tech-
niques of analysis and modeling: storage of spatial data, generation of new data using
analytical tools such as overlay or buffering, disaggregating of data using appropriate
micro simulation algorithms, and visualisation.
Individual level databases are frequently encountered, but these are generally
structured
according
to
attribute
characteristics
rather
than
geographic
ones.
A non-geographic DBMS may offer the ability to extract all members in a particular
building by searching for the building in the address ﬁeld of the database, it will not
facilitate any form of explicitly spatial query, and can not support queries involving
concepts such as adjacency, connectivity or spatial coincidence and can not access the
attribute geographically. In this micro level analysis for demographic data, having the
Demographic Analysis Using Modern GIS Software Tools
581

demographic characteristics in the database then using the database management
capabilities to retrieve any data according to set query.
Considering the disadvantages and shortcoming of the other approaches like areal
aggregation approach although it is the most common and includes the convectional
choropleth census mapping and the census type data are generally available in this
form. The inherent assumption of any such model is that geographic space is divided
into internally homogeneous zones, with all change occurring across zone boundaries.
This is a fundamental ﬂaw, as neither the attributes characteristics (age, sex, race, etc.)
not the distribution of population can reasonably be expected to be uniform within any
arbitrarily deﬁned areal unit. Although total counts and summary statistics for the
attributes deﬁned will be correct, this information is impossible to interpret up to
individual level precisely. Thus analysis fallacy will always be present in that data
which have been through any transformation involving aggregation, as the original
detail cannot be retrieved by any computation.
The main attraction of individual-level databases is that they facilitate ad hoc
aggregation, allowing the design of areal units to suit analytical requirements. Since the
all issue of GIS demographic spatial analysis is based on geographical referencing,
question at this point is the location chosen for the georeferencing of individuals.
5
Experimental Research—Case Study of the Republic
of Srpska (Bosnia and Herzegovina)
Demographic data are collected on the ﬁeld, in 2013. Data which are collected, were
recorded in the data base MS Excel. Considering the way population (demographic)
data is collected using ﬁeld survey, which is the main source of data, this data has to be
georeferenced (geocoded) before it is analyzed in GIS. Among the information
recorded is the place of residence mostly building number; thus building has been
adopted in this investigation as the georeferencing spatial unit to provide a way to
disaggregate demographic analysis and this can be easily combined with other spatial
analysis.
Area of Republic of Srpska is divided in four different regions: region of Banja
Luka, region of Doboj and Bijeljina, region of Sarajevo and Zvornik and region of
Trebinje and Foca. Difference between them is shown on the Fig. 3.
For demographic spatial analysis, it was used a base map of Bosnia and Herze-
govina. In this case it was used cadastral map, with all municipalities in Republic of
Srpska. Cadastral map has a border for each municipality in Republic of Sprska. It was
analyzed and created a map for each region in Republic of Srpska, separately. The
stages to carry out demographic spatial analysis in GIS using ArcView GIS as an
example involves getting a base map, in this case the base map used is that of the
cadastral GIS of the study area, on base map overlay the building layer on which all the
spatial analyses are based. The population analysis is ﬁrst carried out in database using
Microsoft Access as the database management system and also in SPSS as statistical
analysis software. Population data from these packages is either exported from these
packages using the export functions or it is imported into ArcView GIS using the
accessing tabular data capabilities of ArcView like the SQL connection. After
582
N. Mijic and J. Ateljevic

importing population ﬁles they are georeferenced to the buildings as the reference
spatial units using ArcView geocoding styles. After ﬁnishing this process of linking a
data from Microsoft Access and creating database which has been imported in Arc-
View, every region has got his own number of settlements, area and other attributes
which are important for demography of Republic of Srpska. Exported maps are created
in ESRI ArcView and they are shown on the Figs. 4, 5, 6 and 7 for each region
separately.
In this map we can notice that municipality Banja Luka has the biggest number of
settlement population in the region and the smallest number of population have
municipalities Petrovo, Ribnik, Istocni Drvar, Kupres, Jezero, Krupa na Uni and
Kostajnica. On the Fig. 5 it was shown number of settlement population in the region
of Doboj and Bijeljina.
On the Figs. 6 and 7 it was shown a number of settlement population in the region
of Sarajevo and Zvornika and also in the region of Trebinje and Foca.
In this maps it can be seen which municipalities are developed and which one have
tendency to grow up. To aid in spatial planning the geocoded population is spread
randomly within the boundaries of the polygons (buildings or zones). Also using
different ArcView tools it was analyzed a number of born and deceased people in this
regions, and also it was calculated a population growth for each municipality in
Republic of Srpska. Population growth was presented on the Fig. 8 for region of Banja
Luka and number od born and deceased was shown on the map (Fig. 9).
Fig. 3. Regions of the Republic of Srpska
Demographic Analysis Using Modern GIS Software Tools
583

Fig. 4. Number of settlement population—region of Banja Luka
Fig. 5. Number of settlement population—region of Doboj and Bijeljina
584
N. Mijic and J. Ateljevic

Fig. 6. Number of settlement population—region of Sarajevo and Zvornik
Fig. 7. Number of settlement population—region of Trebinje and Foca
Demographic Analysis Using Modern GIS Software Tools
585

On the map (Fig. 9) it can be seen that in some municipalities in the region of Banja
Luka there is much more people who are deceased then born. Also the same situation is
in other regions of Republic of Srpska. On the Fig. 10 it was shown the situation of
population growth for the region of Doboj and Bijeljina and on Fig. 11 a map of born
and deceased people in this region.
In this region there is a lot of deceased people much more than born, and also a
population growth is negative. That means that this region doesn’t have a young
population. People are moving in another region more developed or abroad. The same
Fig. 8. Population growth—region of Banja Luka
Fig. 9. Number of born and deceased—region of Banja Luka
586
N. Mijic and J. Ateljevic

situation is in the region of Sarajevo and Zvornik and region of Trebinje and Foca. On
the Figs. 12 and 13 are shown charts and maps for the region of Sarajevo and Zvornik.
Region which has after Banja Luka the best geographical location have also very
low population growth. The most people in the region of Trebinje and Foca are old and
they have negative population growth. This is shown on the Figs. 14 and 15.
Fig. 10. Population growth—region of Doboj and Bijeljina
Fig. 11. Number of born and deceased—region of Doboj and Bijeljina
Demographic Analysis Using Modern GIS Software Tools
587

Demographic analysis of population in Republic of Srpska has shown that in the all
regions there is much more old people then young. Also during this analysis it was
shown that the cities with good geographical location are having a large number of
people but municipalities which are small and undeveloped in them there is only old
Fig. 12. Population growth—region of Sarajevo and Zvornik
Fig. 13. Number of born and deceased—region of Sarajevo and Zvornik
588
N. Mijic and J. Ateljevic

population. Population growth is almost negative in all regions, that means that in
Republic of Srpska there is much more deceased than born people. This fact is very bad
for all municipalities in Republic of Srpska.
Fig. 14. Population growth—region of Trebinje and Foca
Fig. 15. Number of born and deceased—region of Trebinje and Foca
Demographic Analysis Using Modern GIS Software Tools
589

6
Conclusion
In this article it was described method and techniques for demographic analysis of
population and population growth in Republic of Srpska (Bosnia and Herzegovina).
Process of collecting data was very long, and all analysis took a lot of time. Creating
data base, and linking the data with all municipalities in Republica Srpska was a part of
this very difﬁcult process. For creating demographic maps in ArcGIS it was necessary
to have cadastral map of Republic of Srpska with borders of each municipality.
Demographic analysis has shown that a lot of people are leaving small and
undeveloped municipalities and move in developed regions or cities, in some cases
abroad.
References
1. Chapin, F.S.: Urban Land Use Planning. University of Illinois Press, USA (1965)
2. Haining, R.: Spatial Data Analysis in Social and Environmental Sciences. Cambridge
University Press, UK (1990)
3. Cressie, N.: Statistics for Spatial Data. Wiley, New York (1991)
4. Fotheringham, A.S., Charlton, M.: GIS and Exploratory Spatial Data Analysis: An Overview
of Some Research Issues (1994)
5. Plane, A.D., Rogerson, A.P.: The Geographical Analysis of Population with Application to
Planning and Business. Wiley, New York (1994)
6. Trevor, C.B.: In: Fotheringham, S., Rogerson, P. (eds.) (1994)
7. Cressie, N., Chan, H.N.: Spatial modelling of regional variables. J. Am. Statist. Assoc. 84,
393–401 (1989)
8. Chou, Y.-H.: Exploring Spatial Analysis in Geographic Information Systems. On Word
Press, Santa Fe (1997)
9. Willemain, T.R.: Statistical Methods for Planners. The Massachusetts Institute of
Technology (1980)
10. Hornby, F.W., Jones, M.: An Introduction to Population Geography. Cambridge University
Press, UK (1984)
11. Witherick, M.E.: Population Geography. Longman Group, UK (1990)
12. Molenaar, M.: An Introduction to the Theory of Spatial Object Modelling for GIS. Taylor
and Francis Ltd (1998)
13. Goodchild, M.F.: A spatial analytical perspective as geographical information systems. Int.
J. Geogr. Inf. Syst. 1, 327–334 (1987)
14. Goodchild, M.F., Haining, R.P., Wise, S., et al.: Integrating GIS and spatial analysis:
problems and possibilities. Int. J. Geogr. Inf. Syst. 6, 407–423 (1992)
15. Fotheringham, A.S., Rogerson, P.: Spatial Analysis and GI. Taylor and Francis, London
(1994)
16. Burrough, P.A.: Principles of Geographical Information Systems for Land Resources
Assessment. Oxford University Press (1986)
17. DeMers, N.M.: Fundamentals of Geographic Information System. Wiley, New York, USA
(1997)
18. Heywood, I., Cornelius, S., Carver, S.: An Introduction to GIS. Longman, New York (1998)
19. Openshaw, S.: Making geodemographic more sophisticated. J. Market Res. Soc. 31, 111–
131 (1989)
590
N. Mijic and J. Ateljevic

20. Openshaw, S.: Learning to live with errors in spatial databases. In: Goodchild, M., Gopal, S.
(eds.) The Accuracy of Spatial Databases. Taylor and Francis, London (1989)
21. Goodchild, M.F.: A spatial analytical perspective as geographical information systems. Int.
J. Geogr. Inf. Syst. 1, 327–334 (1987)
22. Openshaw, S.: Location-Allocation Techniques: Practical Methods for Spatial Planning,
Planning Out Look (1991)
23. Fischer, M., Nijkamp, P.: Geographic Information Systems and Spatial Analysis (1992)
24. Anselin, L., Getis, A.: Spatial statistical analysis and geographic information systems. In:
Fischer, Nijkamp (eds.) (1993)
25. Gerland, P.: Socio-economic data and GIS: datasets, databases, indicators and data
integration issues. In: Paper Presented at the UNEP/CGIAR, Arendal III Workshop on Use
of GIS in Agricultural Research Management, Norway (1996)
26. Getis, A., Ord, J.K.: The Analysis of Spatial Association by Use of Distance Statistics,
Geographical Analysis (1992)
27. Anselin, L.: Interactive techniques and exploratory spatial data analysis. In: Longley, A.P.,
Goodchild, M.F., Maguire, D.J., David (eds.) (1999)
28. Chrisman N.: Exploring Geographic Information Systems. Wiley (1997)
29. DeMers, N.M.: Fundamentals of Geographic Information System, 2nd edn. Wiley, New
York, USA (2000)
30. ESRI: ArcView 3D Analyst: 3D Surface Creation, Visualisation and Analysis. Environ-
mental System Research Institute Inc., Redlands, California (1997)
31. Martin, D.: Representing the socioeconomic world. Papers Reg. Sci. Assoc. 70, 325–335
(1991)
32. Craglia, M., Harlan, O.: Geographic Information Research: Trans-Atlantic Perspectives.
Taylor and Francis, London (1999)
33. Wegener, M.: Spatial models and GIS. In: Caraglia, M., Harlan, O. (eds.) Geographic
Information Research: Trans-Atlantic Perspectives. Taylor and Francis, London (1999)
Demographic Analysis Using Modern GIS Software Tools
591

Estimation of Peak Flood Discharge
for an Ungauged River and Application of 1D
Hec-Ras Model in Design of Water Levels
Emina Hadžić(&), Ajla Mulaomorević-Šeta, Hata Milišić,
and Nerma Lazović
Faculty of Civil Engineering, Department of Water Resources,
University of Sarajevo, Sarajevo, Bosnia and Herzegovina
{eminahd,hata.milisic,nerma.ligata}@gmail.com,
ajla.mulaomerovic@gf.unsa.ba
Abstract. Flood is one of the most serious environmental problems we face. It
is impossible to construct a totally secure system of ﬂood control, but is nec-
essary to take all measures to reduce damage. The occurrence of high water level
in the river bed, most commonly coincide with heavy rain, melting snow or both
phenomena together. To construct hydraulic structures, it is necessary, among
other data, to have information on high waters of different return period. As a
large number of rivers in Bosnia and Herzegovina do not have continuous
hydrological monitoring, high water cannot be deﬁned on statistical approach. In
this, empirical expressions are applied.This paper presents the application of
rational methods for determining peak ﬂood discharge of Lepenički stream,
sizing river beds and determine water level along part of riverbed using software
package Hec-Ras.
Keywords: Peak ﬂood discharge  Ungauged basin  Time of concentracion
Determine of water level  HEC RAS  Lepenicki stream
1
Introduction
Flood is one of the most serious environmental problems we face. The occurrence of
high water in the river bed, most commonly coincide with heavy rain, melting snow or
both phenomena together. Beside this, peak ﬂood discharge can be caused by extreme
events on basin caused various factors: landslides in artiﬁcial or natural lakes, demo-
lition of dams or dikes, improper handling evacuation devices, opening the barrier
occurred collecting ice or driftwood, etc. Spatial distribution and magnitude of peak
ﬂood discharge depend on the season, and the dry and wet periods within the seasons.
Soil moisture also has a great inﬂuence on the size of the high water, [1]. Frozen or
saturated soil, cause runoff coefﬁcient whose values is almost 1.0. Dry land, on the
other hand, absorbs the fallen precipitation and thus greatly reduces surface runoff.
Whether for protection from ﬂoods, sizing hydraulic, or other objects, or of allo-
cating water resources, it is necessary to know the magnitude of peak discharge that can
be expected in the future. Setting the magnitude of peak ﬂood water for sizing object
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_52

depends on its relevance, and the effects of any possible ﬂooding would have on the
wider community. Therefore, the magnitude of peak must be determined with high
certainty. It can be done in a natural, undisturbed state, after which it can be trans-
formed to future-built state. Peak discharge can be determined in several ways
depending on available data. It is certain that the characteristics regime of watercourses
have a signiﬁcant impact on the mode of ﬂood control.
Water regime of natural watercourse represents the spatial and temporal distribution
of water, described by water level h (cm) and ﬂow Q (m3/s). There are different
approaches for determine peak ﬂood discharge, depending on the available data. In
case of gauged basins, water regime of the watercourse (Q (x, y, t), h (x, y, t)) is
obtained from measurements of the gauges. On the other hand, in ungauged basin,
water regime is obtained indirectly-using precipitation data and physical characteristics
of the basin (for the determination of Q (x, y, t)), and the use of hydraulic calculations
(to determine h (x, y, t)).
2
Rational Method for Peak Flood Discharge Determination
Peak ﬂood discharge determination, using high intensity rains, is carried out by rational
method. This method is widely adopted for calculating the maximum discharge on
small catchment areas, i.e. the catchment area less than 50 km2 and for the precipitation
duration up to 2 h. Maximum discharge (Qmax) occurs when the entire catchment area
begins to take part in the runoff, i.e. when the precipitation duration of the rain (tk) is
greater or equal to the time of concentration (Tc). The method was developed on the
assumption that the intensity of the precipitation is constant and uniformly distributed
over the basin [2]. Another assumption is that precipitation of return period (T) prod-
ucts maximum discharge of the same return period (1):
Qmax T
ð Þ ¼ Fsl  ib T
ð Þ  g T
ð Þ
ð1Þ
where is:
Fsl—catchment area,
ib(T)—precipitation intensity of T (year) return period,
η—runoff coefﬁcient or percentage (part) of rainfall that caused runoff (under
assumption that the duration of precipitation is equal to the time of concentration).
Assumptions for rational method are: (i) The duration of the rain must be equal or
greater than time of concentration; (ii) method gives maximum discharge but not whole
hygrogram (in this case it does not constitute a limitation, as the maximum discharge
value is relevant for river bad regulation); (iii) Discharge value exhibit linear rela-
tionship to precipitation intensity which may not be the case; (iv) constant intensity
over the basin area and throughout the duration of precipitation are assumed (which is
usually the case concerning both rain of short duration and small catchment areas);
(v) runoff coefﬁcient is function of return period.
Estimation of Peak Flood Discharge for an Ungauged River
593

Although the method is quite simple, choosing parameters catchment areas is very
important. Empirical expression for determining concentration time, i.e. the relevant
precipitation duration based on certain parameters of catchment areas, is determined
based on a map scale of 1:25,000. On the basis of the map, there are parameters which
are deﬁned: borders and its catchment area, watercourse slope, slope and length of
hillslide, etc. (Figs. 1 and 2, Table 1).
Fig. 1. Topographic map with marked catchment border areas Žički and Lepenički stream, [3]
Table 1. The parameters of the basin with speciﬁc topographic maps required for calculation of
the peak discharge, [3]
Parameter and unit
Lepenički
stream
Catchment area Fsl (ha)
204
Maximum elevation of basin Kmax (a s l)
876
Source elevation Kizv (m n m)
590
Outlet elevation Kmin (m n m)
545
Length of slope measured from the farthest point in the basin to the
watercourse L (m)
1400
Slope of hillslide (%)
20,1
Watercourse length Lp (m)
1087
Slope between maximum and outlet elevation (watercourse slope)
Ib = (Kizv −Kmin)/Lp  100 (%)
4,60
Weighted channel slope M (%)
3,57
594
E. Hadžić et al.

2.1
Time of Concentration Tc
Time of concentration can be determined as the sum of the time takes precipitation to
reach the river and travel time through watercourse to the output proﬁle. For small
basins, with the dominant ﬂow on the slopes, estimation of concentration time through
can be done using Hathaway’s term (Eq. 2).
tpad ¼ 1; 44 
Lpad  n

0;467  S0;235
ð2Þ
where is:
tpad—travel time for reaching the river (min)
Lpad—slope length (m)
S—slope of hillslide (m/m)
N—dimensionless roughness factor.
540
550
560
570
580
590
600
0
200
400
600
800
1000
1200
Altitude (mn.m)
Distance (m)
Uzdužni profil Lepeničkog potoka
Bruto pad toka Ib
Uravnati pad M
Fig. 2. Longitudinal section of Lepenički stream with watercourse slope and weighted channel
slope, [3]
Table 2. Values of roughness factor n, [1]
Type of surface
n
Smooth impervious
0.02
Smooth bare-picked soil
0.1
Poor grass, row crops or moderately rough bare soil 0.2
Pasture
0.4
Deep litter or grass
0.8
Estimation of Peak Flood Discharge for an Ungauged River
595

Dimensionless roughness factor is analogous to the Manning roughness coefﬁcient
(Table 2). However, for a speciﬁc type of substrate its value is signiﬁcantly higher than
the ratios in open channels ﬂow. In this case, its value is adopted 0.80 (coniferous
forests, dense grass).
Travel time through watercourse can be expressed as:
tvod ¼ L
v
ð3Þ
Where is:
tvod—length of principal watercourse (min)
L—length of principal watercourse (m)
V—water velocity (m/s).
Velocity through the river is adopted on the basis of Table 3 by adopting an upper
limit and hilly terrain (2.5 m/s).
Finally, concentration time is:
Tc ¼ tvod þ tpad
ð4Þ
In this way, it was determined that concentration time is about 60 min for Lepe-
nički stream.
2.2
Determination of Effective (Net) Rainfall
To determine the relevant precipitation for maximum discharge in smaller catchments,
intensities of the so-called heavy rains is absolutely necessary, [2]. Heavy rains mean
the short-term rain of signiﬁcant height whose duration is of several minutes to several
hours. Figure 3 shows the ib-tk-T diagram (i—gross precipitation intensity, tk—per-
cipitation duration, T—return period) for rain gauge station of Sarajevo. Rain gauge
station of Sarajevo is the closest station to Lepenički stream.
Table 3. Approximate ﬂow rate according to the characteristics of watercourses and relief, [2]
Watercourse and relief properties Water velocity v (m/s)
Stream with water depth up to 1 m Other watercourse
Watercourse in wetlands
0,3–0,5
0,4–0,8
Watercourse in lowland
0,8–1,2
1,0–1,5
Watercourse in hilly terrain
1,5–2,5
2,0–2,5
596
E. Hadžić et al.

Effective rainfall intensity is estimated by runoff coefﬁcient, i.e. using equation:
ie T
ð Þ ¼ ib T
ð Þ  g
ð5Þ
where is:
ie—effective precipitation intensity,
g—runoff coefﬁcient,
ib—gross precipitation intensity (shown in Fig. 3).
Factors inﬂuencing the runoff coefﬁcient are: initial losses, retention (accumulation)
of water in depressions in the basin, soil characteristics, the degree of soil saturation,
the intensity of precipitation and hydrogeological characteristics of the catchment area.
Runoff coefﬁcient is expressed in function of return period, characteristic of slope
and land cover, and adopted values are: from 0,35 (for return period of 2 years) to 0,52
(for return period of 100 years) for Lepenički stream. Adopted values correspond to the
values given in Table 4 for hilly terrain.
0
100
200
300
400
500
600
10
20
30
40
50
60
Intenzitet bruto kiše ib (l/s,ha)
Trajanje kiše tk (min)
T=100
T=50
T=20
T=5
T=2
ib = (251,22∙T0,30) / tk0,60
Fig. 3. ib-T-tk for Sarajevo rain gauge station, [2]
Estimation of Peak Flood Discharge for an Ungauged River
597

Having deﬁned catchment area, estimated runoff coefﬁcient and concentration time,
precipitation gross intensity, peak discharge is calculated using Eq. 6. The results of
calculated peak discharge in function of return period are presented in Table 5.
Qmax T
ð Þ ¼ Fsl  ib T
ð Þ  g T
ð Þ
ð6Þ
For determined peak discharge value, using the software HecRas, water levels for
natural and regulated channel are deﬁned.
Table 4. Runoff coefﬁcient values in term of return period and the characteristics of the land
cover [4, 5]
Landcover
Return period
2
5
20
50
100
Constructed area
Asphalt
0.73 0.77 0.86 0.90 0.95
Concrete
0.75 0.80 0.88 0.92 0.97
Green areas (cemetery, parks)
0–2%
0.32 0.34 0.40 0.44 0.47
2–7%
0.37 0.40 0.46 0.49 0.53
More than 7% 0.40 0.43 0.49 0.52 0.55
Undeveloped area
0–2%
0.31 0.34 0.40 0.43 0.47
2–7%
0.35 0.38 0.44 0.48 0.51
More than 7% 0.39 0.42 0.48 0.51 0.54
Meadows
0–2%
0.25 0.28 0.34 0.37 0.41
2–7%
0.33 0.36 0.42 0.45 0.49
More than 7% 0.37 0.40 0.46 0.49 0.53
Forest
0–2%
0.22 0.25 0.31 0.35 0.39
2–7%
0.31 0.34 0.40 0.43 0.47
More than 7% 0.35 0.39 0.45 0.48 0.52
Table 5. Calculated peak discharge Qmax(T) (m3/s) for Lepenički stream as a function of return
period T (years), for adopted precipitation duration tk (min), [3]
Qmax(T) Return period T (years)
17.87
100
14.15
50
5.76
5
3.93
2
598
E. Hadžić et al.

3
Modeling of Lepenički Stream Water Level
3.1
Hec-Ras Model
HEC-RAS (Hydrological Engineering Centre - River Analysis System) is a
one-dimensional hydraulic modelling program based on 4 types of analysis in rivers:
Steady ﬂow models, Unsteady ﬂow models, Sediment transport models and Water
quality analysis. It allows simulating ﬂow in natural riverbeds or artiﬁcial channels to
determine the water level being its main goal develop ﬂood studies and determine
ﬂoodable areas. The program was developed by the US Department of Defense, Army
Corps of Engineers in order to manage the rivers, harbors, and other public works
under their jurisdiction; it has found wide acceptance by many others since its public
release in 1995 [6].
It is widely used in one-dimensional ﬂow main parameters calculations in case of
steady and unsteady river ﬂow regimes. These parameters are essential in the analysis
of various hydraulic engineering problems including the determination of the effect of
hydraulic structures on the upstream and downstream channels; the estimation of ﬂood
plain; the analysis of the capacity of river; the monitoring of the depth at any point in
river; the choice of implantations sites of hydraulic structures (such as dams, pumping
stations etc.), [6].
3.2
Computation of Hydraulic Parameters
Water level calculation from one cross section to the next was based on the solution of
the one-dimensional energy Eq. (6) with an interactive procedure called the standard
step method [6]
Z2 þ Y2 þ a2  V2
2
2g
¼ Z1 þ Y1 þ a1  V2
1
2g
þ he
ð6Þ
where Z1, Z2 are elevations of the main channel inverts (m), Y1, Y2 are depths of water
at cross sections (m), V1, V2 are average velocities (total discharge/total ﬂow area)
(m/s), a1, a2 are velocity weighting coefﬁcients, g is gravitational acceleration (m/s2),
he is energy head loss (m).
he ¼ LSf þ C a2  V2
2
2g
 a1  V2
1
2g


ð7Þ
where: L is weighted reach length (m), Sf is representative friction slope between two
adjacent sections, C is contraction/expansion loss coefﬁcient.
Estimation of Peak Flood Discharge for an Ungauged River
599

3.3
Boundaries Conditions
Boundary conditions are necessary to deﬁne the starting water depth at upstream or
downstream end. In a sub critical ﬂow regime, boundary conditions are only required at
the downstream end of the river system and the computation starts from downstream to
upstream end. If a supercritical ﬂow regime is going to be calculated, boundary con-
ditions are only necessary at the upstream end of the river system and the computation
starts from upstream to downstream end. If a mixed ﬂow regime calculation is going to
be determined, both downstream and upstream boundaries conditions are required at all
open ends of the river system. There are three kinds of boundaries conditions (critical
depth, normal ﬂow depth, or a given depth downstream of the channel), but only one is
needed [6].
In this paper, HEC-RAS 4.10 was utilized for hydraulic analysis. The basic data
requirements for simulation are: geometric data, cross section geometry, reach lengths,
Manning’s roughness coefﬁcients, contraction and expansion coefﬁcients, steady ﬂow
data, boundary condition, ﬂow regime (Figs. 4 and 5) In this study, the ﬂow regime is
supposed subcritical.
The next step after creating river geometry is to specify the discharges values, ﬂow
regime and boundaries conditions to perform the calculations. In this study, the steady
ﬂow component has been used. As a boundary condition obtained water depth at the
most downstream proﬁle is used.
Further, only the results of modeling are presented. Based on the input parameters,
and the predetermined value Qmax(T = 100), using the Schezy-Manning equation,
normal depth (hn) in trapezoidal river bad is obtained.
Q ¼ 1
n  A  R2=3  I1=2
ð8Þ
• Normal depth hn = h = 1,641 m,
• Cross section area A = 5,6432 m2,
• Wetted perimeter O = 5,46 m,
• Hydraulic radius R = 0,768 m,
• Average velocity v = 4.152 m/s.
Calculation of water depth in the regulated channel from km 0 to km
00 + 0 + 197.84 is preformed. Input data for hydraulic calculation of the Lepeničkog
stream, for the purpose of regulating the ﬂow, are peak discharge of 100 return period
(which is Qmax(T = 100) = 18.0 m3/s) and water level elevation on the upstream
boundary of the channel (Fig. 4).
For a given discharge, ﬂow characteristics including water surface proﬁles, energy
grade line, water surface elevation, ﬂow velocity, ﬂow area, wetted perimeter, Froude
number, top width and energy slope have been computed (Table 6).
600
E. Hadžić et al.

Table 6. Presentation of results for regulated channel obtained using HEC-RAS software, [3]
Reach
River
Sta
Proﬁle
Q
Total
(m3/s)
Min Ch
El (m)
W.S.
Elev
(m)
Crit
W.S.
(m)
E.G.
Elev
(m)
E.G.
slope
(m/m)
Vel
Chnl
(m/s)
Flow
area
(m2)
Top
width
(m)
Froude
# Chl
Uzvodni
dio
194.79
PF1
18.00
552.03
553.67
553.89
554.55
0.030108
4.16
4.33
4.28
1.32
Uzvodni
dio
179.49
PF1
18.00
551.54
553.16
553.40
554.07
0.031555
4.23
4.25
4.24
1.35
Uzvodni
dio
162.49
PF1
18.00
551.01
552.64
552.87
553.54
0.031063
4.21
4.28
4.26
1.34
Uzvodni
dio
160.44
PF1
18.00
550.35
551.69
552.21
553.37
0.070993
5.74
3.14
3.68
1.98
Uzvodni
dio
160.43
PF1
18.00
549.65
550.84
551.51
553.29
0.117793
6.94
2.59
3.37
2.53
Uzvodni
dio
144.74
PF1
18.00
549.18
550.69
551.04
551.85
0.043504
4.77
3.77
4.01
1.57
Uzvodni
dio
125.14
PF1
18.00
548.59
550.27
550.45
551.08
0.026833
3.98
4.52
4.37
1.25
Uzvodni
dio
104.34
PF1
18.00
547.97
543.62
549.83
550.49
0.029712
4.14
4.35
4.29
1.31
Uzvodni
dio
95.14
PF1
18.00
547.70
543.36
549.56
550.21
0.028919
4.09
4.40
4.31
1.29
Uzvodni
dio
95.13
PF1
18.00
547.00
543.31
548.86
550.12
0.078269
5.95
3.02
3.62
2.08
Uzvodni
dio
85.14
PF1
18.00
546.71
543.23
548.57
549.35
0.041596
4.69
3.83
4.04
1.54
63.14
PF1
18.00
546.04
547.72
547.90
548.53
0.026958
3.99
4.51
4.37
1.25
(continued)
Estimation of Peak Flood Discharge for an Ungauged River
601

Table 6. (continued)
Reach
River
Sta
Proﬁle
Q
Total
(m3/s)
Min Ch
El (m)
W.S.
Elev
(m)
Crit
W.S.
(m)
E.G.
Elev
(m)
E.G.
slope
(m/m)
Vel
Chnl
(m/s)
Flow
area
(m2)
Top
width
(m)
Froude
# Chl
Uzvodni
dio
Uzvodni
dio
47.14
PF1
18.00
545.56
547.21
547.42
548.07
0.029478
4.12
4.36
4.30
1.31
Uzvodni
dio
29.14
PF1
18.00
545.02
546.66
546.88
547.54
0.029888
4.15
4.34
4.29
1.31
Uzvodni
dio
25
PF1
18.00
544.90
546.13
546.44
547.35
0.049577
4.89
3.68
3.00
1.41
Uzvodni
dio
24.9
PF1
18.00
544.90
546.13
546.44
547.35
0.049577
4.89
3.68
3.00
1.41
Uzvodni
dio
15
PF1
18.00
544.90
546.44
546.44
547.21
0.026752
3.89
4.62
3.00
1.00
Uzvodni
dio
14.9
PF1
18.00
544.90
546.44
546.44
547.21
0.026752
3.89
4.62
3.00
1.00
Uzvodni
dio
0
PF1
18.00
543.54
544.52
545.08
546.42
0.091605
6.11
2.95
3.00
1.97
602
E. Hadžić et al.

Calculated water levels in regulated riverbed for Qmax(T = 100) = 18 m3/s, are
presented in Table 6.
4
Conclusion
On ungauged basin, if it’s necessary to perform some speciﬁc control works in the
river, empirical method for determining maximum discharge is applied. The problem is
choosing the best method, since the value of ﬂush water of the same basins can
signiﬁcantly vary. Usually, empirical equations are valid only for the area for which
Fig. 4. Schematic presentation of regulated part of Lepenički stream with location of cross
section proﬁles, [3]
Fig. 5. Inserting geometric characteristic of P9 cross section proﬁle of regulated riverbed of
Lepenički stream in HEC-RAS, [3]
Estimation of Peak Flood Discharge for an Ungauged River
603

they are derived, and the selection of the most appropriate formula is largely dependent
on the experience of engineers. Main disadvantage of experiential equations is the fact
that they generally cannot cover all the factors important for runoff.
In this paper, rational method is used to determine peak discharge as a function of
return period. For many years, this method has found its place in the hydrologic
practice of Bosnia and Herzegovina. For obtained discharge, water levels of regulated
part of Lepenički stream are deﬁned using HEC-RAS software.
References
1. Hrelja, H.: Inženjerska hidrologija. Građevinski fakultet u Sarajevu (2007)
2. Hrelja, H.: Analiz kiša kratkog trajanja za potrebe deﬁniranja oticanja sa urbanih površina,
Zavod za hidrotehniku Građevinskog fakulteta u Sarajevu (1984)
3. Hadžić, E. idr.: Glavni projekat regulacije Lepeničkog potoka. Građevinski fakultet u
Sarajevu (2017)
4. Kosatdinov, S.: Bujični tokovi i erozija. Šumarski fakultet u Beogradu (2008)
5. Žugaj, R.: Velike vode malih slivova. skripta Zagreb (2010)
6. US Army Corps of Engineers: Hydrologic Engineering Center, HEC-RAS River Analysis
System, Hydraulic Reference Manual, V. 4.1.0 (2010)
604
E. Hadžić et al.

Experimental Study on Behavior of Reinforced
Concrete Beam Subjected to Cyclic Loading
Edhem Živalj1(&), Asad Kadić1, Senad Medić2, and Muhamed Zlatar2
1 Faculty of Civil Engineering, University of Sarajevo, Fabing d.o.o., Sarajevo,
Bosnia and Herzegovina
edhemzivalj@gmail.com, asad_kadic@live.com
2 Faculty of Civil Engineering, Institute for Materials and Structures, University
of Sarajevo, Sarajevo, Bosnia and Herzegovina
senad_medic@yahoo.com, zlatar.muhamed@gmail.com
Abstract. Experimental investigations that have been conducted by a number
of researchers have shown that the conﬁnement of concrete using an appropriate
arrangement of transverse reinforcement results in a signiﬁcant increase in both
the strength and ductility of conﬁned concrete. The increase of strength and
ductility of concrete due to conﬁnement have a signiﬁcant impact on the
load-bearing capacity and ductility of reinforced concrete elements subjected to
bending. The experimental testing includes twelve beams reinforced with same
longitudinal reinforcement and different transverse conﬁning reinforcement. The
aim of this research was to investigate the effects of conﬁnement on strength and
ductility of the beam under cyclic loading. The experiments resulted in hys-
teresis loops showing the force-displacement dependence. Beams with different
constitutive laws for concrete and reinforcement and with differently deﬁned
cross-sections were modeled using SAP 2000 software. Results of the experi-
mental tests and numerical models were compared.
Keywords: Beam 
Strength 
Ductility 
Conﬁnement 
Transverse
reinforcement  Cyclic experiment
1
Introduction
In the design of structures, it is often required to carefully shape the potential plastic
hinge zone in order to avoid unannounced failure of load-bearing elements or collapse
of whole buildings. To this end, it is necessary to ensure the redistribution of bending
moments from more stressed to less stressed bearing area.
Generally, the ductile behavior of the cross-section (local ductility) causes a ductile
response of the whole structure (global ductility). Section tends to behave in a ductile
manner if tensile reinforcement yields before concrete starts to crush. The combination
of signiﬁcant compressive stress and reinforcement area (over reinforced section)
prevents cross-sectional curvature and leads to brittle or unannounced failure.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_53

In order to achieve the desired ductility in the area of beam plastic hinge the most
important is to provide sufﬁcient transverse reinforcement (stirrups). The role of
transverse reinforcement is not only to resist shear forces but also to conﬁne concrete,
i.e. to prevent transverse deformation. Conﬁnement has a signiﬁcant impact on the load
bearing capacity and ductility of reinforced concrete elements subjected to bending.
Theoretical M-N-j curve can be obtained if constitutive laws of concrete and
reinforcing steel are known. The concrete cover is unconﬁned and it is assumed to be
ineffective upon reaching the ultimate compressive stress of concrete. On the other
hand, the triaxially compressed (conﬁned) area of concrete cross-section continues to
bear even for very large strains. The effect of compressive normal force is twofold: it
increases load bearing capacity and reduces ductility.
The aim of this research is to experimentally and numerically analyze the effects of
different transverse reinforcement layouts on the behavior of concrete beams subjected
to cyclic loading. A total number of tested beams is twelve and testing was conducted
using specially designed auxiliary steel structure in the laboratory of the Institute for
Materials and Structures, Faculty of Civil Engineering, University of Sarajevo.
Dimensions of beams are 20  20  200 cm and in terms of the static system, the
beam is a cantilever. Girders are divided into four groups with three identically rein-
forced beams. All beams have the same longitudinal reinforcement. The difference
between groups is in the form and arrangement of transverse reinforcement. The ﬁrst
group is reinforced according to EC 2 [7] where low ductility is assumed. The second
and the third group of beams are reinforced according to the rules of EC 8 for DCM
(medium class ductility) [4, 5], while transverse reinforcement in the fourth group was
carried out spirally according to the ACI guidelines.
2
Experiments
This experimental testing included twelve reinforced concrete beams (see Fig. 1a)
statically modeled as a cantilever. Beams were divided into four groups as follows: S1,
S2, S3 and S4, and each group included three identical reinforced beams, marked with
symbols A, B and C.
The layout of deﬂectometers with testing apparatus is shown in Fig. 1b. The lower
part of apparatus is a steel structure used to clamp the cantilever, while the upper part
consisted of a press used for application of cyclic loading. Sixteen deﬂectometers were
used during the experimental testing. All deﬂectometers were connected via signal
ampliﬁer HBM Spider 8 to the computer and software HBM Catman 5 was used to
analyze the data.
606
E. Živalj et al.

Concrete class of beams is C25/30 and reinforcing steel used is BSt 500S. All
beams were reinforced with the same longitudinal reinforcement, 8 / 14 and
arrangement of transverse reinforcement is shown in Fig. 2.
Fig. 1. (a) Geometry of typical beam; (b) Layout of deﬂectometers and testing apparatus
S1
S2
S3
S4
Fig. 2. Arrangement of transverse reinforcement and cross-section of columns S1, S2, S3, and
S4
Experimental Study on Behavior of Reinforced Concrete Beam
607

2.1
Testing Program and Results of the Experiment
Testing of beams was conducted at the Institute for Materials and Structures, Faculty of
Civil Engineering, University of Sarajevo. Prior to experiments, a thin coating made of
white paint was applied on beams in order to make the development of cracks better
noticeable. Deﬂectometers were placed on the beam after painting, as well as the press,
used to apply the load, i.e., impose displacement on the top of the beam (see Fig. 3).
The load was applied cyclically, and each cycle was performed three times (see
loading program in Fig. 4). In the ﬁnal phase, the column is pushed monotonically up
to 100 mm. Typically obtained hysteresis curve that relates horizontal force and dis-
placement is shown in Fig. 5.
Development of cracks was recorded during testing and ﬁnal layout of cracks on
beam S3A is represented in Fig. 6.
Fig. 3. Testing of S2B beam
-50
-30
-10
10
30
50
13:12:00
13:26:24
13:40:48
13:55:12
14:09:36
14:24:00
Displacement Δ [mm]
Time t  [min]  
Strain VS Time _ DF 100  (S3A)
DF 100
Fig. 4. Loading protocol for S3A beam
608
E. Živalj et al.

3
Numerical Modeling
Beams consist of three different materials: conﬁned concrete, unconﬁned concrete
(cover) and reinforcement.
Nonlinear behavior within the cross-section is reﬂected on the nonlinear behavior
of the whole structure. This is especially true for structural elements such as frames,
where internal forces are concentrated on the element ends. Hence, it is largely
accepted in nonlinear analysis of frames that nonlinear behavior can be concentrated in
a single cross-section (plastic hinge). Integration of normal stresses in ﬁbers along the
height of cross-section with respect to deformations yields moment—curvature (M–j)
diagram [6].
However, if analysis with localized nonlinearity in plastic hinges is applied, M-Ø
(moment-rotation) relation must be deﬁned instead of M-j relation. Rotation is deﬁned
by Ø = lp * j, where lp is the length of the plastic hinge. The length of plastic hinge
-40
-20
0
20
40
-90-80-70-60-50-40-30-20-10 0 10 20 30 40 50 60 70 80 90100110
Δ [mm]
F  [kN]
Hysteresis - DF 100
(S3A)
Hysteresis
Fig. 5. Hysteresis curve for S3A beam
Fig. 6. Final layout of cracks on S3A beam
Experimental Study on Behavior of Reinforced Concrete Beam
609

varies considerably among the authors and can only be estimated. According to the
EC8 guidelines [4], the length of the critical area lcr (plastic hinge) can be calculated
according to the formula:
lcr ¼ hw
ð1Þ
where:
hw
—depth of the beam.
Moment-curvature (M-j) relation is very important for determining the resistance
to earthquakes, as well as ductility and ability to redistribute cross-sectional forces [8].
In reinforced concrete structures, the cross-sectional curvature j results from com-
pressive straining of concrete ec and elongation of steel es in the following manner:
j ¼ ec þ es
h
ð2Þ
In order for nonlinear deformation of concrete and reinforcement to be realized in
the plastic hinge area, reinforcement must be reliably anchored to the foundations. The
foundation structure must not rotate, otherwise the displacement of the column will not
come from deformations of cross-sections.
3.1
r-e Diagrams for Concrete and Reinforcing Steel
Linear r-e diagrams for concrete are largely used for calculating cross-sectional forces
in reinforced concrete structures while the reinforcement is ignored. However, the
actual behavior of materials (concrete, reinforcement) described by r-e diagrams is
nonlinear and adequate constitutive relations should be used for accurate determination
of cross-sectional forces. Mander’s model [1] for concrete and Simple model [1] for
reinforcing steel (r-e diagrams) were used in this paper.
According to Mander [9], conﬁned and unconﬁned concrete have different
compressive stress and strain limits due to the presence of conﬁning reinforcement.
The Simple model for reinforcing steel is deﬁned by four zones: elastic, ideally plastic,
hardening and softening zone (Fig. 7).
Fig. 7. Mander models for conﬁned and unconﬁned concrete and simple model for reinforcing
steel [1]
610
E. Živalj et al.

Figure 8 shows the comparison of Mander conﬁned r-e diagrams for differently
reinforced columns S1, S2 and S3, and Mander unconﬁned r-e diagram. The increase
of bearing capacity and strain in the cross-section, relative to the amount of longitudinal
reinforcement as well as to the amount and arrangement of conﬁning reinforcement, is
noticeable.
Figure 9 shows the comparison of r-e diagrams of conﬁned and unconﬁned
concrete according to EC2 for differently reinforced beams S1, S2 and S3. As with
Mander r-e diagrams, an increase of bearing capacity and strain is signiﬁcant.
0
5
10
15
20
25
30
35
40
45
50
0.000
0.005
0.010
0.015
0.020
0.025
Stress [N/mm2]
Strain [‰]
UNCONFINED
CONFINED S1
CONFINED S2
CONFINED S3
Fig. 8. Mander r-e diagrams for conﬁned and unconﬁned concrete (comparison of differently
reinforced beams S1, S2 and S3)
0
10
20
30
40
50
60
70
0.000
0.010
0.020
0.030
0.040
0.050
0.060
Stress [N/mm2
Strain [‰]
UNCONFINED CONCRETE
ACCORDING TO EC2
CONFINED CONCRETE
ACCORDING TO EC2 FOR
BEAM S1
CONFINED CONCRETE
ACCORDING TO EC2 FOR
BEAM S2
CONFINED CONCRETE
ACCORDING TO EC2 FOR
BEAM S3
Fig. 9. Idealized r-e diagrams for conﬁned and unconﬁned concrete according to EC2
(comparison of differently reinforced beams S1, S2 and S3)
Experimental Study on Behavior of Reinforced Concrete Beam
611

Simple model for reinforcing steel is shown in the Fig. 10.
3.2
Beam Analysis Using SAP 2000
In time history analysis, direct integration of equations of motion is performed. Since
displacements were imposed very slowly in the experimental campaign, dynamic time
history analysis reduces to static cyclic analysis.
Frame or line element is used for modeling of beams. Material nonlinearity of frame
elements is introduced by plastic hinges deﬁned with M-Ø curve at selected points. For
the integration of plastic deformations (strains and curvatures), it is necessary to deﬁne
the length of the plastic hinge. The length of the plastic hinge in this paper is taken
according to EC8 and amounts to 20 cm.
Modeling of beams was done using the software package SAP 2000. Two beam
models were made with frame elements [10]. The cross-section of the ﬁrst model was
created using section designer. The cross-section is divided into three areas and each
area is deﬁned by different constitutive laws of concrete (conﬁned and unconﬁned r-e
diagram according to Mander) and reinforcing steel (Simple model). Cross-section of
the second model is deﬁned in the usual way using the constitutive law of concrete (r-e
diagram for nonlinear analysis of force and deﬂection according to EC2) and rein-
forcing steel (r-e diagram of reinforcing steel according to EC2) (Fig. 11).
0
100
200
300
400
500
600
700
0
0.05
0.1
0.15
0.2
Stress [N/mm2]
Strain [‰]
SIMPLE MODEL FOR
REINFORCING STEEL
Fig. 10. Simple r-e diagram for reinforcing steel
Fig. 11. Deﬁning of the cross-section [SAP 2000]
612
E. Živalj et al.

Plastic hinge is deﬁned using M-Ø diagram, which was obtained by multiplying
M-j curve with the height of plastic hinge. Nonlinear properties of the plastic hinge
placed at the beam ﬁxed end were assigned using “ﬁber P-M2-M3” hinge (Fig. 12).
Time record of imposed displacements on the beam top of was used for
Time-History analysis. Typically, time record was given for 109 steps with an interval
of 25 s, which means that the experiment lasted 2,700 s in total (Fig. 13).
Fig. 12. Schematic representation of the beam model, static system and location of link element
(plastic hinge) [SAP 2000]
Fig. 13. Deﬁnition of time-history function [SAP 2000]
Experimental Study on Behavior of Reinforced Concrete Beam
613

For the hysteresis loop, which represents dissipation (loss) of energy due to the
deformation and is used to describe the behavior of a certain material, Takeda hys-
teresis model was selected (Fig. 14). Takeda model is a sophisticated model that takes
into account the stiffness degradation and makes a distinction between big cycles—
hysteresis loops and small cycles. [3]. Also, Takeda model gives a good description of
the behavior of reinforced concrete.
When maximum forces (support reaction at the ﬁxed end) obtained using numerical
model are compared against the experiment, there are no signiﬁcant differences
(Fig. 16). The maximum force of 31.76 kN was obtained using the numerical model
(Fig. 15) while the maximum force of 35.36 kN was obtained in the experiment (Fig. 5).
The dependency of force and displacement can be clearly seen on hysteresis loop.
Fig. 14. Takeda hysteretic model
Fig. 15. Hysteresis loop [SAP 2000]
614
E. Živalj et al.

Figure 16 compares hysteresis loops obtained experimentally and numerically for
the S3B beam. r-e models for conﬁned and unconﬁned concrete according to Mander
were used as constitutive laws of concrete and Simple r-e diagram for reinforcing
steel. It may be noted that the curves match very well. The reason for small deviations
should be sought in the imperfections of the testing apparatus.
Figure 17 compares hysteresis loops obtained experimentally and numerically for
the S3B beam, which was modeled using a r-e diagram of concrete for nonlinear
analysis of forces and deﬂections according to EC2 and r-e diagram for reinforcing
steel according to EC2 (BSt 500 S).
-40.00
-30.00
-20.00
-10.00
0.00
10.00
20.00
30.00
40.00
-100-90-80-70-60-50-40-30-20-10 0 10 20 30 40 50 60 70 80 90100
Δ [mm]
F  [kN]
Numerical model (BEAM S3B)
EXPERIMENT
Fig. 16. Comparison of hysteresis loops for S3B beam (rc-ec according to Mander and rs-es
according to “Simple” model)
-40.00
-30.00
-20.00
-10.00
0.00
10.00
20.00
30.00
40.00
-90 -80 -70 -60 -50 -40 -30 -20 -10 0
10 20 30 40 50 60 70 80 90
Δ [mm]
F  [kN]
Numerical model (BEAM S3B)
EXPERIMENT
Fig. 17. Comparison of the hysteresis loops for S3B beam (rc-ec according to EC2 and rs-es
according to EC2)
Experimental Study on Behavior of Reinforced Concrete Beam
615

Hysteresis loops for two different numerical models are compared in Fig. 18.
Overlapping of curves obtained for “simple” and “Section Designer” model is perfect
and both numerical models provide same results.
4
Conclusion
In the case of particularly important constructions, design load can be chosen such that
the structure remains elastic. On the other hand, the load level can be controlled and
reduced. The classic concept of load level reduction is based on the permission of
non-linear structural response which is evident in controlled structural damage
(cracking). Damages should occur in carefully constructed zones (plastic hinges)
so that adequate load-bearing capacity and ductility of load-bearing elements is
provided [11].
Furthermore, in order to enhance the structure quality, a so-called “silent reserve”
(ductility) is important. The total load-bearing capacity of the cross-section and the
entire structure cannot always be accurately estimated from the allowable stress, or
even from the coefﬁcient of safety. Namely, not only load bearing capacity but also
ductility is of crucial importance for dissipation of energy released by earthquake
ground motion. The occurrence of brittle fracture is worse than the occurrence of the
so-called announced fracture or if the fracture is preceded by signiﬁcant plastic
deformations. Therefore, relying solely on the strength criteria may not always be fully
reliable, if general safety is not taken into account [2].
In accordance with the cyclic testing method, the main result of the experiment is
represented through a force-displacement diagram (hysteresis curve). The difference
between the hysteresis curves of differently reinforced beams with transverse rein-
forcement is almost nonexistent. The absence of normal force means that pure bending
occurs in the cross-section and the longitudinal reinforcement, which is the same in all
beams, provides resistance to pure bending. The effect of cross-sectional conﬁnement,
in this case, is negligible.
-40
-30
-20
-10
0
10
20
30
40
-90-80-70-60-50-40-30-20-10 0 10 20 30 40 50 60 70 80 90100110
[mm]
F  [kN]
Numerical model (BEAM S3B)
CONFINED CROSS-SECTION
Numerical model (BEAM S3B)
"SIMPLIFIED CROSS-SECTION
Δ
Fig. 18. Comparison of hysteresis loops of S3B beam which is modeled with two differently
deﬁned cross-sections
616
E. Živalj et al.

Occurrence and development of cracks on the beams were also monitored and
mapped during the experiment. It was observed that the formation of cracks occurred as
expected and that the shape of cracks in the plastic hinge area is regular.
Experimentally obtained hysteresis loops largely coincide with both numerical
models created in SAP 2000. The reason for small deviations are the imperfections of
the testing device. During testing, a slight lift of the column base is observed. The lift
was not taken into consideration in numerical analysis, i.e. it was assumed that the
beam base is completely clamped.
Based on test results and ﬁndings presented above, with the aim of providing better
insight into the beam ductility and occurrence of plastic hinges at the beam base, it is
necessary to conduct additional tests with variable compressive forces. Furthermore, in
terms of numerical calculation, the beam could be modeled using nonlinear layered
shell element in SAP 2000 and the results of hysteresis loops should be compared with
the results presented in this paper.
References
1. CSI technical notes: Material stress-strain curves (2008)
2. Jokanović, O.: Principi kvalitetnog projektovanja konstrukcija. Građevinski fakultet
Univerziteta u Sarajevu
3. Hrasnica, M.: Potresno/zemljotresno inženjerstvo—predavanja na doktorskom studiju na
Građevinskom fakultetu u Sarajevu. Građevinski fakultet Univerziteta u Sarajevu (2011)
4. Eurocode 8: Design of structures for earthquake resistance—part 1: general rules, seismic
actions and rules for buildings (2003)
5. Folić, R.: EC 8: Projektovanje seizmički otpornih konstrukcija. Građevinski fakultet
Univerziteta u Beogradu (1997)
6. Kwak, H.-K., Kim, S.-P.: Nonlinear analysis of RC beams based on moment-curvature
relation (2000)
7. Hasanović, V.: Proračun AB konstrukcija prema EC2. Građevinski fakultet Sarajevo (2002)
8. Hrasnica, Z., Medić, S., Begić, S.: Design of RC buildings for different ductility classes
according to the now EC8. 1st ECCES. Geneve, Switzerland (2006)
9. Mander, J.B., Priestley, M.J.N., Park, R.: Theoretical stress-strain model for conﬁned
concrete (1988)
10. Medić, J.: Seminarski rad: Nelinearna analiza vitkih armiranobetonskih zidova pomoću
prorama SAP 2000. Građevinski fakultet Sarajevo (2014)
11. Hrasnica, M.: Aseizmičko građenje. Građevinski fakultet Univerziteta u Sarajevu (2012)
Experimental Study on Behavior of Reinforced Concrete Beam
617

Landﬁll Leachate Management—Control
and Treatment
Amra Serdarevic(&)
Faculty of Civil Engineering, Department of Sanitary and Environmental
Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina
amra.serdarevic@gf.unsa.ba
Abstract. Sanitary landﬁlls have been the most popular methods of municipal
solid waste disposal for the last decades, all over the world, but waste man-
agement policy has been greatly turned toward waste minimizing and reuse.
Incineration and energy recovery play an important role in waste reduction and
energy conversion. Sanitary landﬁlls, however, still exist and will continue to be
used for solid waste and residue disposal in many countries. The designs of
landﬁll leachate treatment and landﬁll closure requirement is one of the major
engineering challenge for environmental compliance. The main issue is related
to the question: How to select a method for landﬁll leachate treatment which
will be in line with relevant regulations and with reasonable cost and operation
complexity? Which one is a right for particular site? Bosnia and Herzegovina is
facing nowadays with implementation of solid waste management project
throughout the country, which includes issues related to the landﬁll leachate
treatment. This paper presents leachate containment and treatment as well as a
brief overview of the subject issue in Bosnia and Herzegovina.
Keywords: Landﬁll leachate  Recirculation  Chemical characterization
Quantity  Treatment options
1
Introduction
Leachate produced in a landﬁll is a liquid which has percolated through the disposed
waste, ﬂushing up suspended and soluble materials that originate from or are products
of the degradation of the waste (Fig. 1) [1]. The production of the leachate is an
important environmental problem. Many factors interact with process of production of
variable quantity and quality of leachate from the landﬁlls. These factors are: annual
precipitation, runoff, inﬁltration, evaporation, transpiration, ambient temperature, waste
composition and density, initial moisture content and depth of the landﬁll. Also, sta-
bilization of solid waste placed in a sanitary landﬁll and the quality of leachate are
principally the results of physical, chemical and biological processes. Landﬁlls gen-
erally contain a highly inhomogeneous mixture of materials, which include both a very
high organic component as well as soluble mineral substances. Depending on the
substances present, appropriate wastewater technology must be applied. Before con-
sidering the design of any leachate management system it is important to consider the
objectives that are to be achieved. Some of them are: control of type and quantity of
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_54

waste which is allowed to be disposed, minimization of active cells and open area for
disposal, avoidance contact or damage of the bottom liner of the landﬁll, keeping the
drainage system in function, etc. [1].
2
Landﬁll Leachate Generation—Quality and Quantity
Knowledge of the landﬁll leachate generation is a prerequisite for the planning of a
leachate management strategy. An assessment of the leachate generation rate cannot be
prepared in the absence of a plan of waste disposal. An understanding of the potential
for the leachate generation and quality is essential at the conceptual design stage. Water
balances are used to assess likely leachate generation volumes. Parameters used include
waste volumes, input rates and absorptive capacity, effective and total rainfall, inﬁl-
tration and other site parameters.
A control of leachate quantity and quality is the basis for stabile, long-term landﬁll
and leachate treatment plant operation. Proper characterization of leachate, supported
by studies (bench test or pilot plants) is helpful to select a reliable leachate treatment
facility that can effectively accommodate variable inﬂuent characteristic.
Fig. 1. Water balance at a sanitary landﬁll [1]
Landﬁll Leachate Management—Control and Treatment
619

2.1
Leachate Quantity—the Water Table of Landﬁlls
Leachate production is the result of precipitation, evaporation, surface runoff, inﬁl-
tration, storage capacity etc. (Fig. 1). According to the literature, [2] volume of the
leachate could be roughly calculated as percentage of the precipitation, depending of
the waste compaction. Leachate volume could be estimated for:
• low compacted landﬁll as: 25–50% of precipitation,
• high compacted landﬁll as: 15–25% of precipitation.
For example, a design of leachate treatment plants in Germany for the basic pre-
cipitation of the 750 mm/a assumes calculation of leachate volume as follows:
• low compacted landﬁll: 5–10 m3/(had),
• high compacted landﬁll: 4–5 m3/(had).
Leachate quantity is possible to calculate, in accordance with mayor inﬂuent
factors, by water balance equations or hydraulics calculation. There are software
available on the market for calculations and prediction production of leachate. Models
for calculations are developed with an objective to simplify simulation of the different
scenarios (HELP, USA Army corp.) [1].
As the landﬁll design progresses, the calculations should be reﬁned. As a minimum,
a simple water balance calculation should be undertaken twice per year, to check
whether there has been any increase in leachate production. The calculation should be
of the following form [3]:
Lo ¼ ER  A
ð Þ þ LW þ IRCA þ ER  Alag




 a.W
½

ð1Þ
where:
Lo = leachate produced (m3)
ER = effective rainfall (use actual rainfall (R) for active cells) (m)
A = area of cell (m2)
LW = liquid waste (also includes excess water from sludge) (m3)
IRCA = inﬁltration through restored and capped areas (m3) (In areas that have been
temporarily capped/restored an inﬁltration rate of 20–30% of the effective rainfall
(ER) should be used. Inﬁltration in restored areas would be in the range 2–10% ER
in case scenario for a geosynthetic clay liner cap. Inﬁltration into restored areas
(A restored) should be calculated using site speciﬁc information.)
Fig. 2. Relationship between waste density and the absorptive capacity of waste [3]
620
A. Serdarevic

Alag = surface area of lagoons (m2)
a = absorptive capacity of waste (m3/t): for the waste density of 0.65 t/m3 the waste
is capable of absorbing a further 0.1 m3 water per tonne of waste before leachate is
generated. This absorptive capacity falls to about 0.025 m3 water per tonne of waste
for waste densities of 1 t/m3 (Fig. 2)
W = weight of waste deposited (t/a)
Further adjustments of the water balance may include consideration of factors such
as moisture losses via landﬁll gas and waste fermentation.
2.2
Leachate Composition
The principal organic content of leachate is formed during the breakdown processes. The
composition of landﬁll leachate differs depending on the type of waste stored, changes in
time as the degradation of the waste continues inside the landﬁll. It is normally measured
in terms of Biochemical Oxygen Demand (BOD), Chemical Oxygen Demand (COD) or
Total Organic Carbon (TOC). The degradation process is generally divided into ﬁve
successive stages, namely: (a) aerobic, (b) hydrolysis and fermentation, (c) acetogenesis
(d) methanogenic and (e) aerobic phase. These processes are dynamic, each stage being
dependent on the creation of a suitable environment by the preceding stage [3].
Due to exothermal processes inside the landﬁll, the temperature of the leachate is
usually higher than typical groundwater in the area. Landﬁll leachate is usually quite
turbid, has a very strong odour and a brownish colour.
From one to two years after the waste disposal on the landﬁll, the initially aerobic
decomposition processes transfer to anaerobic processes. Initially, anaerobic decom-
position consumes only short-chain fatty acids. Concertation of an organic compounds
entering the leachate are quite high. As the waste degradation increases, anaerobic
decomposition progresses to a methane production. In addition to a range of soluble
nitrogen and sulphur compounds, sulphates and chlorides, the leachate also contains a
high degree of persistent organic pollutants [4].
Typical landﬁll leachate composition of leachate from domestic waste at various
stages of waste decomposition is presented in Table 1 (results are in mg/L except
pH-value).
Explanation of the leachate categories shown above:
(A) Recently disposed domestic waste, in the active “acid-forming” stage of anaerobic
decomposition, with rapid production of readily degradable organic materials
such as fatty acids [5].
(B) Relatively aged waste in latter stages of stabilization, containing a lower pro-
portion of biodegradable organic materials (as indicated by the low ratio of BOD:
COD), but with continuing biological activity as shown by the concentration of
ammoniacal nitrogen, [5].
(C) and D) Average concentrations of biochemical inﬂuenced leachate components in
Germany [6].
Composition of the leachate, as already above mentioned, is very volatile due to the
many reasons, such maturity of the landﬁll, type of the landﬁll, waste composition and
Landﬁll Leachate Management—Control and Treatment
621

quantity, waste management option (recycling, incineration, etc.). A very important
parameter is biodegradable organic matter (COD/BOD5) which decreases rapidly with
the ageing of the landﬁll. Landﬁll leachate presents lower biodegradability in com-
parison with urban wastewater but in regard to the nitrogen compounds there is higher
concentration of nitrogen in TKN and NH4-N form, usually as results from the
biodegradation of proteins and amino acids present in waste. Decision for removal
before treated efﬂuent discharge depends strongly on country legal requirements.
Presence of heavy metals (cadmium, chromium, iron, etc.) is directly related with waste
origin and concentration varies with landﬁll age. In normal conditions heavy metals are
not a problem in municipal solid waste landﬁlls, but salts could be signiﬁcantly present
in landﬁll leachate. Salts (chlorides, carbonates, sulphate, etc.) are responsible for
buffer capacity of leachate. This factor is important in the design of treatment solutions
concerning pH variations [4, 6].
2.3
Reduction and Collection of Leachate
In last decades, the best practice is to operate landﬁll sites on the basis of containment
to prevent leachate polluting groundwater and also to avoid problems of landﬁll gas
migration. Such solutions allow accumulation of ﬁltrate at the base of the landﬁll.
Hence there is a need to remove the leachate from the base of the site and to treat it in
an environmentally acceptable manner. Drainage system for collecting leachate must
Table 1. Leachate composition [5, 6]
Parameters
Leachate A
Leachate B
Leachate C Leachate D
Recent wastes Aged wastes Acid phase Methanogenic phase
pH-value
6.2
7.5–8
6
8
COD
23 800
1 160
22000
3000
BOD5
11 900
260
13000
180
TOC
8 000
465
7000
1300
Fatty Acids
(as C) 5 688
5
12
Ammoniacal-N
790
370
750
750
Oxidises-N
3
1
0.5
0.5
tot.P
6
6
o-phosphate
0.73
1.4
Chloride
1 315
2 080
2100
2100
Sodium
(Na)
960
1 300
1350
1350
Magnesium
(Mg)
252
185
470
180
Potassium
(K)
780
590
1100
1100
Calcium
(Ca)
1 820
250
1200
60
Iron
(Fe)
540
23
780
15
Nickel
(Ni)
0.6
0.1
0.2
0.2
Copper
(Cu)
0.12
0.03
0.08
0.08
Zinc
(Zn)
21.5
0.4
5
0.6
Lead
(Pb)
0.4
0.14
0.09
0.09
622
A. Serdarevic

be arranged on the landﬁll base and on the sloping areas. The leachate collecting
system should include the following components: drainage layer (blanket) constructed
of either natural granular material (sand, gravel) or synthetic drainage material (e.g.
geonet or geocomposite), perforated pipes for leachate collection, protective ﬁlter layer
over the drainage blanket, leachate monitoring points and leachate collection tank for
equalization before its inﬂow into the leachate treatment plant.
The two ﬁnal surface sealing systems that must be applied according to the regu-
lations of the German “TA Siedlungsabfall” [7], are shown in Fig. 3.
3
Leachate Treatment
Conventional design criteria for sewage wastewater treatment is not appropriate for
landﬁll leachate treatment design. Main challenges for leachate treatment are related to
discharging limits and providing a stable treatment plant operation. The system should
be ﬂexible for the different situation at the landﬁll and for quick adjustments of treatment
steps due to the changes in leachate quality. Main classiﬁcation of the technical solution
and technology classiﬁcation for leachate management and treatment on the site or at the
leachate treatment plant (LTP) could be divided into following groups:
1. Leachate treatment on the site or transfer to the central wastewater treatment plant:
leachate lagooning and recirculation into the landﬁll body or at the surface, com-
bined leachate with the domestic sewage system and treatment at the wastewater
treatment plant.
2. Biological processes: different combinations of the aerobic and anaerobic processes.
3. Chemical and physical processes: chemical oxidation, adsorption on activated
carbon, chemical precipitation, coagulation, ﬂocculation, air striping.
Fig. 3. Landﬁll top and base sealing systems [7]
Landﬁll Leachate Management—Control and Treatment
623

4. Membrane processes: reverse osmosis (RO), membrane combination with biolog-
ical treatment (MBR), ultraﬁltration (UF), nanoﬁltration (NF).
5. Thermal processes: evaporation.
3.1
Leachate Recirculation and Biological Co-Treatment Leachate
with Sewage
System of leachate recirculation into the landﬁll body could be consider like sustain-
able system of leachate management. Leachate is kept on the source, and it could
facilitate an increase of the process of waste degradation and increase the quality and
quantity of the landﬁll gas. The potential problem could be stability of the landﬁll body
and clogging of the system for inﬁltration of the leachate. The operation scheme of
leachate recirculation issues is showed on Fig. 4.
Literature recommends that quantity of leachate for re-inﬁltration ranges between
0,5 m–2 mm/day (0.5–2 L/m2, day). Based on experience, a landﬁll area of 2,5 ha is
required for re-inﬁltration of 50 m3/day of liquid (concentrate). Each landﬁll and
re-inﬁltration system is a different case and the re-inﬁltration costs shouldn’t be
disregarded.
Co-treatment leachate with municipal wastewater (sewage) testing shows that this
could be a possible options for leachate treatment. Those results could be also con-
cluded from the worldwide usage of wastewater treatment plants (WWTP) with dif-
ferent biological technologies applied [4]. However, there is a need to take into account
following: increase of load (especially regarding the ammoniac, BOD5, COD or salts,
variation in the quantity, incidental contaminants, etc.).
Fig. 4. Scheme of the steps of implementation of the leachate recirculation
624
A. Serdarevic

Combination of these wastewaters could be treated with the same systems. If the
leachate is added without any other changes of the treatment process the load increases
and therefore in most cases the efﬂuents and sludge production also increase. The
increase could reduce the nitriﬁcation rate dramatically with the result of ammonia
toxicity. This means that with leachate addition the same load conditions must be kept
as without leachate addition. Restriction could be necessary if sewage should be used
as a carbon source for denitriﬁcation of nitrogen of both wastewaters—sewage and
leachate from old landﬁlls with low organics.
Also for many other wastewaters the nutrient requirement of the biomass must be
considered. The increase of nondegradable leachate components (residual COD,
halogens, metals) is a function of dilution reduced by additional precipitation and/or
adsorption effects. Inﬂuence of the heavy metals in non-treated leachate is relatively
low.
Overall biological co-treatment of sewage and sanitary landﬁll leachate is a proven
technology and operates well, if the treatment plant is carefully designed and operated.
Experiences on the existing WWTP have shown that the operation of plants for mix of
sewage and leachate are much more stable with less failures than for a separate leachate
treatment plant (LTP) [7].
3.2
Biological Treatment
Biological treatment processes are the most common practice for wastewater as well as
for leachate treatment, despite of the fact that the organic biodegradability of the old
leachate is usually very low. Ratio between carbon concentration and nutrients is
possible to be improved with usage of external source of carbon (e.g. methanol).
Biological treatment could be divided into anaerobic and aerobic treatment pro-
cesses [8]:
(a) Aerobic processes:
• aerated lagoons,
• aerobic biological different treatments with activated sludge (ASP; SBR),
• rotating biological contactors (RBC),
• trickling ﬁlter or some non-conventional technics (reed beds, wetlands).
(b) Anaerobic processes:
• anaerobic biological treatment,
• anaerobic ﬁlter,
• anaerobic sludge bed reactor (UASB).
Aerated lagoons is a relatively simple leachate treatment system. The basic idea is
that the retention time of the leachate is long enough so that as many bacteria can
develop per time. Long retention times are necessary in order to degrade also the
medium degradable organic fraction and because of low temperatures. The mainte-
nance and operation costs are relatively low. Aeration and mixing guarantee BOD
degradation and odour minimisation.
Landﬁll Leachate Management—Control and Treatment
625

Beneﬁts of the aerated lagoons are in low sensitivities to load variation and process
stability, but main disadvantage is low efﬁciency. Aerated lagoons can be combined
with nonconventional treatment system (reed beds, etc.) but problems of this solutions
are low efﬁciency and large area for lagoons or plants.
Biological aerobic treatment with activated sludge is the most common treatment
for leachate. Beside BOD5-reduction the nitriﬁcation of ammonium is a very important
process for activated sludge plants. Nitrogen elimination becomes more and more
important with aging of landﬁll and the increasing of reduction of BOD in the landﬁll
body. The treatment of such leachates are very complicated. The pH of these leachates
can be in the range of 8, 0–8, 3 but, if the ammonium is converted to nitrate the pH
decreases as a result of alkalinity destruction. Overall a very careful operation and
pH-control is necessary to get low ammonium efﬂuent values [7].
The detention time in activated sludge plants can be considerably shorter than in
aerated lagoons. The reason is that the sludge content (amount of bacteria) can be
controlled to a certain degree and is several times higher than in aerated lagoons. The
beneﬁts of implementation are nitrogen elimination, as well as maximum biological
reduction of COD and BOD. Disadvantages are process sensitivity and high investment
and operation costs [9].
Anaerobic leachate treatment is an effective process but the remaining BOD5-and
COD-efﬂuents are still high. After the anaerobic treatment step the leachate has to be
treated to ﬁnal efﬂuent standards by different technologies like aerobic, physical—
chemical or membrane processes.
To reduce the high nitrate content in leachate efﬂuent and to stabilize pH-conditions
in activated sludge plants a denitriﬁcation step could be helpful. Leachate from the
methanogenic phase could only be denitriﬁed with addition of carbon sources as acetic
acid, methanol etc.
The main advantage of the anaerobic treatment process is the low energy
requirement, because no oxygen has to be supplied. Technical anaerobic processes
need adequate temperatures and have to be designed appropriately. The process is also
very sensible, for this reason the plant has to be operated adequately.
Summary of Biological Treatment
Biological treatment processes are very effective methods to reduce biodegradable
organics as BOD5 and majority part of COD. Also from leachates with low organics
and BOD5-COD-ratio <0.2 the COD could be removed by biological treatment up to
50%. It is also an effective method to oxidize ammonium to nitrate and if necessary to
reduce ammonium by nitriﬁcation and additional denitriﬁcation to nitrogen gas (N2).
The decreasing elimination rates during period with low water temperatures especially
for ammonium reduction are a disadvantage as well as sensitive and high investment
and operation cost. Negative effects of this technology is high operation cost for
chemicals and necessity of the next steps in treatment for salts, hard COD,
3.3
Physical-Chemical Processes
Physical processes usually have larger ﬂexibility but do not eliminate main contami-
nants. Rest contaminants are transferred to sludge. Physical processes are usually
626
A. Serdarevic

design for pre-treatment, to reduce suspended solids, or remove oil or sand in the
inﬂuent, and preserve next process and equipment like membrane.
Coagulation/Flocculation/Precipitation could be used for elimination of some
percentage of the organic compounds. Although this technology is rather cheap it is not
used frequently because of the addition of chloride and sulphate into the leachate
efﬂuent. Also, the problem is the sludge and cost for additional treatment of this sludge.
Chemical oxidation and adsorption processes. During the last decades chemical
oxidation and adsorption on active carbon processes were developed and applied at
different locations. Combination of chemical oxidation (Fenton process with oxidation
with hydrogen peroxide (H2O2) or ozone) and ultraviolet light (UV) reach high oxi-
dation rates for leachate COD and AOX.
The negative side of this technology is that the oxidation agents are very expensive
and the oxidation of all organics is sometimes not convenient. In addition leachate
contains so many components which still remain in the mixture of the landﬁll leachate.
During chemical oxidation not all organics are oxidized to carbon dioxide and water.
Some organics are only partly oxidized often to biological degradable components.
These new biodegradable organics must be reduced by biological treatment. To reduce
these relatively low concentrations a ﬁxed ﬁlm reactor is an effective solution. Bio-
logical pre-treatment with nitriﬁcation of biodegradable components could decrease
costs of treatment.
Chemical oxidation is a very expensive step. In many cases a combination of
chemical oxidation and activated carbon ﬁlters (partly as biological ﬁlters) could reduce
the costs. But as results of oxidation signiﬁcant amount of chemical sludge should be
treated and that also raise total expenses. This technology using mostly of iron or
aluminium salts for coagulation/precipitation and polymers for ﬂocculation. This is
proven technology to remove and separate suspended solids, colloidal particles, and
heavy metals in chemical sludge with contaminants which should be treated with
subsequent steps. Main disadvantages are high demanding operation with high oper-
ation cost and high production of chemical sludge, with limited efﬁciency.
Air stripping, Air stripping is a process by which a wastewater, is brought into
contact with air, so that undesirable volatile substances present in the liquid phase can
be released and carried away by the gas. Process is usually undergoing in counter-
current packed towers. In some cases ammonium is removed by air stripping process,
but air stripping is also very expensive and very complex technology for operation.
3.4
Membrane Technology
Reverse Osmosis and membrane technology are frequently applied in the last decade
for leachate treatment (Fig. 5).
Reverse osmoses is a separation process with two ﬂuid ﬂows—one low polluted
permeate ﬂow and one high polluted concentrate ﬂow. The solution for recirculation is
not preferable due to the high amount of concentrate, as well as degradation of the
concentration of contaminants on the leachate. In the Germany, in last decade, testing
the evaporation of the concentrate is dominant approach as an environmental friendly
solution. The operation cost depends of the quantity of the concentrate.
Landﬁll Leachate Management—Control and Treatment
627

This technology is well designed for leachate treatment, but there is a possibility of
membrane clogging with high operation costs (energy, ﬁlters cartage is necessary to very
often replace, sand ﬁlters, chemical for operation and cleaning, membrane replacement).
But, with RO it is possible to reach very strong limits for discharge in an open stream,
with relatively simple operation and easy replacement or extension of the plants.
With new developments in membrane productions and application in domain of
wastewater treatment it is possible to combine membrane and biological treatment for a
treatment of very complexity leachate water. With post membrane ﬁltration it is pos-
sible to get very low polluted permeates with exception of the acetic phase leachate. In
that case biological pre-treatment is strictly necessary. Membrane biological reactor
(MBR) is the combination of the biological treatment with a high concentration of
mixed liquor suspended solids (MLSS) and separation by using micro, ultra or
nanoﬁltration. MBR systems require smaller reaction tanks and do not need any ﬁnal
settling tank or sludge concentration tank. The MBR technology is proven all over the
world for the last decades and this technology is continuously being improved. The
limitations like costs for spare parts or membrane cartridges decrease in last decade due
to the development and increase costs of membrane production and increased number
of installed units.
3.5
Combinations of the Available Technology for LTP
Current treatment facilities for the treatment of leachate mainly consist of several of the
above mentioned treatment methods to meet the limited concentrations for the efﬂuent
(Fig. 6).
One of possible combination of the available technology for leachate treatment
options is shown on Fig. 6. It includes biological treatment, mechanical treatment by
Fig. 5. Membrane technology—principle (left) and RO disk module (right) [10]
628
A. Serdarevic

ultraﬁltration, and treatment with active carbon ﬁlters and reverse osmosis using disc
tube module technology.
The numbers of available techniques and technologies are applied worldwide. The
major engineering challenges are related to the high and variable concentration of
pollutants. Technologies for landﬁll leachate treatment are compared by efﬁciency for
removal of some characteristic parameters for leachate and it is presented in Table 2. It
is obvious that some techniques are not suitable for one parameter, but are very efﬁcient
for the others. Which combination is the best for some leachate treatment? It primarily
depends of the quantity and quality of the leachate. Pilot plant tests of the combination
Fig. 6. Combined biological treatment + tertiary treatment: MBR + AC [9]
Table 2. Process efﬁciency [9]
Processes/Parameter
TSS BOD5 COD TN NH4-N Heavy
metals
AOX Salts
Biological treatment
–
+
(+)
(+) (+)
(−)
(−)
–
Adsorption/activated carbon
–
(−)
+
–
–
(−)
+
–
Sedimentation/ﬂocculation
(+)
(−)
(−)
–
–
(+)
(−)
–
Filtration/membrane ﬁltration
(ultraﬁltration)
+
(−)
(−)
(−) –
(+)
(−)
–
Reverse osmoses
(+)
+
+
+
(+)
+
+
+
Air stripping
–
(−)
(−)
–
+
(−)
(+)
–
Chemical oxidation
–
(−)
+
(−) (+)
–
(+)
–
Evaporation
+
+
+
(+) (−)
+
+
+
−Not appropriate
+Appropriate
(−)Low efﬁciency
(+)Appropriate with limitation
Landﬁll Leachate Management—Control and Treatment
629

of appropriate technologies should conﬁrm that chosen combination will eliminate
contaminants from the leachate.
4
Practice in the Leachate Treatment in BiH
Waste management strategy in BiH, adopted in 2003, is based on the regional concept
of the waste disposal on the sanitary landﬁlls, with envisioned implementation of all
waste hierarchy actions according to the directive EU.
Sanitary landﬁlls are opened in several regions/cities (Sarajevo, Zenica, Banja
Luka, Bijeljina). For most existing landﬁll, technical documentation for reconstruction
or closure activities have been prepared, but generally in BiH the waste management
issues are still very complex. During the last few years the situation with waste
management has been improving and some studies for new solution for the waste reuse,
recycle, energy recover or landﬁlling with leachate treatment consideration have been
developed. The objective is to design, construct and test the regional centres for waste
management in accordance with legislation in Bosnia and Herzegovina and relevant EC
Directives.
Regarding the leachate treatment in BiH, only two LTPs are in operation: leachate
treatment plants in Banja Luka and Bijeljina. The both treatment plants are designed for
the revers osmoses (RO). Small production of the leachate (approx. 30 m3/day) is an
advantage for quality RO operation.
The Sarajevo sanitary landﬁll is the biggest and oldest landﬁll in the country. It is
already existing for over than 50 years, and at the end of the last century, the former
landﬁll in Sarajevo has been remediated into the sanitary landﬁll.
The problem of leachate water is still an outstanding issue at the Sarajevo sanitary
landﬁll. It has been present at the Sarajevo landﬁll for a long time, but unfortunately
there has not been serious effort in ﬁnding a sustainable solution. Membrane bioreactor
(MBR) with submerge membrane was installed at the site and started with operation in
2011. But unfortunately, after very short period of time, due to the mechanical dam-
ages, plant operation has stopped. Activities on the revitalisation of the treatment plan
and monitoring of the leachate are undergoing.
Some techniques which are cutting of the edge and which could be applicable for
the Sarajevo sanitary landﬁll are presented in January 2017 to the representatives of
authorities, experts and engineers from this ﬁeld. The proposed solutions for the
treatment of the leachate from Sarajevo sanitary landﬁll are based on the biological
treatment and membrane separation technique. Amount of the leachate is approx.
500 m3/day, and quality is typical for old landﬁlls, methanogenic leachate.
The proposed technology is based on the last state of the art in this domain.
Two-stages reverse osmoses and MBR combination with ultra-ﬁltration, nanoﬁltrations
and activated carbon adsorption were considered as a possible solution. The activities
on the leachate monitoring and pilot plant set up on the site are undergoing. Depending
on the results of the monitoring and operation results of the pilot plants the decision of
the appropriate technology will be made. For the selection of concept of a leachate
treatment plant several aspects have to be taken into account:
630
A. Serdarevic

• limited discharge concentrations,
• low demand of resources,
• low demand of energy,
• low generation of residues, sludge, salts, heavy metals, hazardous contaminants,
• low environmental impact,
• economical efﬁcient operation,
• simplicity.
However, this approach is still considered to be quite costly for BiH circumstances
due to high operational costs ranging between 2, 5 and 6 EUR per cubic meter.
5
Conclusions
Presented overview of the leachate control, collection and treatment emphasizes
complexity and importance of this highly important environmental issue. Nowadays
many treatment plants are under operation throughout the world, so there are many
experiences regarding the technology and the efﬂuent quality. In some cases the
treatment of leachate resulted in increasing operation problems in opposite to the
treatment of other wastewaters. It is necessary to test different wastewater treatment
processes to select the most effective and economic systems depending on the speciﬁc
discharge limits and the speciﬁc conditions of a landﬁll. Decision and chosen treatment
process should not only be based on efﬂuent values and maintenance but also on the
production of residuals with new pollution potential and demand of energy and
chemicals.
References
1. Serdarevic, A.: Upravljanje cvrstim otpadom ( Solid Waste Management) Faculty of Civil
Engineering, University of Sarajevo, p. 350 (2016)
2. Rettenberg, G.: Landﬁll practice-DAAD seminar. Faculty of Sarajevo University of Sarajevo
(2006)
3. EPA Ireland, Landﬁll manuals—Landﬁll site design, Environmental Protection Agency
(2000). https://www.epa.ie/
4. https://www.ifas-hamburg.de/pdf/leachate.pdf
5. https://www.leachate.co.uk/main/leachate-chemistry-testing/landﬁll-leachate-composition/.
Accessed 10 Jan 2017
6. Ehrig, H.-J.: Leachate quality. In: Christensen, T.H., Cossu, R., Stegmann, R. (eds.) Sanitary
Landﬁlling: Process, Technology and Environmental Impact, pp. 213–230. Academic Press,
London, UK (1989)
7. TASi-Technische Anleitung zur Verwertung, Behandlung und sonstigen Entsorgung von
Siedlungsabfällen (Dritte Allgemeine Verwaltungsvorschrift zum Abfallgesetz) vom 14. Mai
1993 (BAnz. Nr. 99a vom 29.05.1993). http://www.bmub.bund.de/ﬁleadmin/bmuimport/
ﬁles/pdfs/allgemein/application/pdf/tasi_ges.pdf. Accessed 10 Feb 2017
8. Tchobanoglous, G., Franklin, L.B., Stensel, H.D., Wastewater Engineering Treatment and
Reuse. 4th edn., p. 1819. Metcalf & Eddy, Inc. McGraw-Hill Education (2003)
Landﬁll Leachate Management—Control and Treatment
631

9. ISWA (2015): Leachate Management-Landﬁll Training Workshop, ISWA World Congress,
Antwerp BEL. https://www.iswa2015.org/assets/ﬁles/downloads/Leachate_Management_-_
Landﬁll_training_ISWA_WGL_7_sep_2015.pdf. Accessed 20 Jan 2017
10. ROTREAT Abwasserreinigung GmbH, Austria (2017). https://www.rotreat.at.˙Accessed 01
Feb 2017
632
A. Serdarevic

Modeling Strategies for Masonry Structures
Senad Medić(&) and Mustafa Hrasnica
Faculty of Civil Engineering, Institute for Materials and Structures, University of
Sarajevo, Sarajevo, Bosnia and Herzegovina
senad_medic@yahoo.com, hrasnica@bih.net.ba
Abstract. Three strategies in the ﬁnite element analysis are used for modeling
masonry structures which, depending on complexity, pertain to micro-, meso- or
macro-models.
Discrete
particle
models,
as
well
as
combined
ﬁnite
element/discrete models, are employed for analysis of heterogeneous masonry
elements. Specially dedicated to the analysis of masonry panels and piers is an
equivalent frame or macro-element approach. Different application ﬁelds exist
for each model type. TNO Diana 10.1 is employed for modeling of wallets
exposed to compression and unconﬁned unreinforced walls loaded in shear.
Numerical results are veriﬁed against the data obtained from experimental
research program performed at the Faculty of Civil Engineering in Sarajevo.
TREMURI, PFC2D and FEM/DEM are used to describe other modeling
techniques.
Keywords: Masonry  Finite element modeling  Discrete
particle  Macro-element
1
Introduction
Masonry is a composite material that consists of units and mortar. Experimental
investigation of masonry is essential in understanding structural behavior, however,
numerical modeling can complement experimental research and provide new insights.
Masonry structures are usually analyzed by ﬁnite elements (FEM) and, based on the
level of detail, computational strategies are traditionally divided into following cate-
gories: micro-, meso- or macro-modeling techniques. The second group of modeling
strategies pertains to discrete element method (DEM) or a ﬁnite and discrete element
combination (FEM/DEM). Specially dedicated to the analysis of masonry panels and
piers is an equivalent frame (macro-element) approach. One modeling strategy cannot
be preferred over the other since different application ﬁelds exist for each model type.
In this paper, special attention is paid to numerical models developed in ﬁnite
element package TNO DIANA 10.1 [1]. Attention will be given to meso-model with
combined cracking-shearing-crushing interface [2] and macro-model denoted as
Engineering Masonry model [3].
The developed models were veriﬁed against the experimental data obtained from
the tests performed in the laboratory of Institute for materials and structures, Faculty of
Civil Engineering, University of Sarajevo. Mechanical properties of masonry compo-
nents (brick, mortar, interface) were determined using appropriate specimens [4]. Wall
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_55

compressive strength and modulus of elasticity were determined on so-called wallets
(51.4  12  63 cm). Unreinforced unconﬁned masonry walls were built in full scale
(233  237  25 cm) and reduced scale cca. 1:2 (100  100  25 cm). Walls were
loaded in cyclic shear under constant vertical pressure or pushed monotonically.
Discrete element modeling will be brieﬂy described using PFC2D [5], combined
ﬁnite element and discrete element modeling using FEM/DEM [6] and macro-element
modeling TREMURI [7]. Different modeling strategies are shown in Fig. 1.
2
Micro-Modeling
A detailed analysis of masonry, denoted as micro-modeling, should represent all
constitutive elements of masonry structure which includes units, mortar and unit/mortar
interface. Mortar and brick units are discretized by continuum elements with corre-
sponding failure criteria. Discontinuity in displacement ﬁeld is introduced by interface
elements between mortar and unit which should account for potential cracks. However,
due to the complexity, micro models are rarely used.
This approach is suited for small structural elements with a particular interest in
strongly heterogeneous states of stress and strain in order to provide a better under-
standing of the local behavior. The primary aim of micro-modeling is to closely rep-
resent masonry from the knowledge of the properties of each constituent and the
interface. The necessary experimental data must be obtained from laboratory tests in
the constituents and small masonry samples [2]. In Fig. 2 shows a 3D model of the
tested wallet and typical displacement pattern of the mortar joint when the wallet is
exposed to compression. Even though the loading is uniaxial, the stress state in
masonry wallet (prism) is three dimensional due to different mechanical properties of
brick and mortar.
Realistic values of elastic moduli for the more compliant mortar versus the stiffer
brick units lead to lateral tension in the brick under far-ﬁeld compression. Simply, the
mortar layer is attempting to squeeze out laterally introducing lateral tension in the
brick unit in exchange for lateral conﬁnement of the mortar layer. On the other hand,
the mortar layer is subjected not only to axial but also to lateral compression whereby
the triaxial conﬁnement signiﬁcantly increases the mortar strength due to internal
Discrete particle
model
Fig. 1. Different modeling approaches [8, 9]
634
S. Medić and M. Hrasnica

friction. Consequently, it is not the weak mortar that fails in compression, but the brittle
brick which fails in bilateral tension [10].
3
Meso-Modeling
Wall geometry is slightly simpliﬁed in meso-models since mortar joint and
mortar-brick contact are homogenized (lumped) into single interface (discontinuous)
element while the units are expanded in order to keep the geometry unchanged.
Masonry is thus considered as an assembly of bricks bonded by potential fracture/slip
lines at the joints. Additionally, an interface can be introduced at the middle of each
brick in order to allow cracking. Poisson effect of mortar which induces biaxial tension
in brick units is omitted. Hence, meso-models cannot consider all possible failure
modes [2].
3.1
Meso-Modeling in Diana 10.1
The interface material model, also known as the ‘Composite Interface model’, is
appropriate for simulation of fracture, frictional slip as well as crushing along material
interfaces, for instance at joints in masonry. Usually, the brick units are modeled as
linear elastic, or viscoelastic continua, while the mortar joints are modeled with
interface elements, which obey the nonlinear behavior described by this combined
cracking-shearing-crushing
model
[1,
2].
Figure 3
shows
the
elements
of
a
meso-model, with an additional interface element in the unit middle [1].
A plane stress interface model was formulated by Lourenco [2]. It is based on
multi-surface plasticity, comprising a Coulomb friction model combined with a tension
cut-off and an elliptical compression cap (Fig. 4). Softening acts in all three modes and
is preceded by hardening in the case of the cap mode. The interface model is derived in
terms of the generalized stress and strain vectors.
Fig. 2. Micro-model of masonry wallet (left) and typical displacement pattern of mortar joint
due to compression (right)
Modeling Strategies for Masonry Structures
635

Fig. 3. Meso-model of masonry [1]
Fig. 4. Multi-surface plasticity for combined cracking-shearing-crushing interface model [1]
Fig. 5. Displacements in Y direction of the compressed wallet
636
S. Medić and M. Hrasnica

Figure 5 shows a displacement ﬁeld of a wallet modeled with the described
interface and numerical and experimental results are compared in Fig. 6.
4
Macro-Modeling
Macro-models are applicable when the structure is composed of solid walls with
sufﬁciently large dimensions so that the stresses and strains can be averaged over a
macro-length. In the large and practice-oriented analysis the knowledge of the inter-
action between units and mortar is, generally, negligible for the global structural
behavior. These models approximate heterogeneous masonry wall by single material
and discretization is independent of brick layout, i.e. units, mortar and unit-mortar
interface are smeared out in the continuum. This is clearly a phenomenological
approach, which means that the material properties must be determined from masonry
tests of sufﬁciently large size under homogeneous stress state. A complete
macro-model must reproduce an orthotropic material with different compressive and
tensile strengths along the material axes as well as different inelastic behavior for each
material axis. It is not surprising that only a few macro-models have been implemented
due
to
the
intrinsic
complexity
of
introducing
orthotropic
behavior.
The
macro-modeling technique is practice-oriented due to the reduced time and memory
requirements as well as user-friendly mesh generation. This type of modeling is most
valuable when a compromise between accuracy and efﬁciency is needed [2].
However, it should be stressed that the failure mechanism of a masonry wall is very
complex and different failure modes can occur separately or combined which is not
easy to capture. Numerical problems may occur in the case of strong localization with a
distinguished macro-crack. In macro-models, the cracking is smeared along a certain
characteristic length which is not a case in reality. Figure 7 shows possible failure
modes for a typical shear wall exposed to vertical compression and horizontal (racking)
force [11].
0
100
200
300
400
0
1
2
3
4
5
6
7
0
1
2
3
4
Force [kN]
Stress [N/mm2]
Deformation [‰]
Wallet Compression Test
Wallet W2 Experiment
Wallet 2 Model Discrete
Fig. 6. Experimental versus numerical results
Modeling Strategies for Masonry Structures
637

4.1
Macro-Modeling in Diana 10.1
Engineering Masonry model is an orthotropic total-strain continuum model with
smeared cracking and it can be used with membrane or shell elements [3]. The model is
capable of simulating compression, tensile and shear failure modes and it can crack in
the X-horizontal bed joint, the Y-vertical head-joint as well as diagonally in the form of
staircase. In comparison with the Total Strain Crack model [1, 3], the Engineering
Masonry model deﬁnes unloading behavior in compression more realistically (Fig. 8
left). Additionally, Coulomb friction failure criterion is included in the model (Fig. 8
Fig. 7. Typical failure modes of a shear wall [11]
Fig. 8. Engineering masonry model: crushing behavior (left) and shear behavior (right) [3]
Fig. 9. Cracking behavior of Engineering masonry model [3]
638
S. Medić and M. Hrasnica

right). The tensile behavior is deﬁned linear loading and softening curves shown in
Fig. 9.
Fig. 10. Rocking of the reduced scale wall
-200
-150
-100
-50
0
50
100
150
200
-18
-14
-10
-6
-2
2
6
10
14
18
Horizontal force [kN]
Displacement at the top [mm]
Experimental hysteresis vs. push over curve 
Fig. 11. Experimentally determined hysteresis versus numerically obtained pushover curve
Modeling Strategies for Masonry Structures
639

Some characteristic results are shown in Figs. 10 and 11. The tested wall exhibited
rocking due to low vertical precompression and the experimental and numerical results
match well.
5
Discrete Models
When severe geometrical and material nonlinearity occurs in masonry structures, such
as fracture or crushing, it can be difﬁcult to consider the damage and discontinuity of
the material with ﬁnite element methods. Therefore, discrete element method (DEM) is
an alternative to solve the problem of fragmentation with a description of the medium
as an assembly of discrete elements [12]. In 1971, Cundall proposed DEM using
contact and shear slip constitutive relationships, which was a pioneering work mainly
suitable for mechanical analysis of discontinuous bodies [13]. Thereafter, various
algorithms were developed for DEM analysis, for example [5].
The DEM model needs a few material properties because the response complexity
arises from the fact that collections of simple things behave—collectively—in com-
plicated and completely different ways. Particles are basically linear elastic, but the
response can be nonlinear or it shows dilation related to mean stress, the transition from
brittle to ductile behavior, hysteresis, etc. Furthermore, the DEM model naturally
exhibits localization (fractures in a brittle solid, shear bands in a granular material)
which in contrast might be problematic for a model with mesh. The weak point of the
discrete approach lies in the determination of aforementioned material properties
(normal and shear stiffnesses, damping coefﬁcients, interparticle bond). They can only
be obtained from macroscopic experiments calibrating the input parameters after
experimental results [9].
6
Combined Modeling Techniques
A method which combines ﬁnite element and discrete element method is called
FEM/DEM [6]. The method uses advantages of both approaches. Namely, the frag-
mentation process (strong discontinuity between elements) is modeled employing
DEM, and the deformation in the element inside is modeled using FEM.
The combined ﬁnite-discrete element method simulation comprises a large number
of particles which are represented by a single discrete element that interacts with
discrete elements close to it. Each discrete element has its own ﬁnite element mesh
which is used to analyze the particle deformability. The material non-linearity
including elastic hysteresis, fracture and fragmentation of discrete elements is con-
sidered through contact elements which are implemented within the ﬁnite element
mesh. The main processes included in the FEM/DEM method are contact detection,
contact interaction, ﬁnite strain elasticity as well as fracture and fragmentation [14, 15].
Figure 12 shows a gradual fragmentation of the tested wallet. It can be noticed that
the cracking pattern is mesh dependent since the triangular elements are employed. In
reality, the cracking of the wallet occurs along vertical lines perpendicular to the
640
S. Medić and M. Hrasnica

loading direction due to biaxial tension in bricks. Figure 13 shows the crack pattern of
the shear wall which matches quite well with the experimental results.
Fig. 12. Gradual fragmentation of the tested wallet [16]
Fig. 13. Experimental and numerical crack pattern for the tested shear wall [16]
Modeling Strategies for Masonry Structures
641

7
Macro-Element Modeling
Complete 3D models of unreinforced masonry structures can be obtained assembling
2-node macro-elements, representing the non-linear behavior of masonry panels and
piers. This modeling strategy has been implemented in the TREMURI program with
non-linear static and dynamic analysis procedures [7]. By means of internal variables,
the macro-element considers both the shear-sliding damage failure mode and its evo-
lution, controlling the strength deterioration and the stiffness degradation, and rocking
mechanisms, with toe crushing effect (bending mode).
This approach is suitable for analysis of structures as a whole [17]. Figure 14 shows
a typical shearing deformation at the ground ﬂoor and the obtained pushover curve [17].
Fig. 14. Failure mechanism and pushover curve for masonry structure [17]
642
S. Medić and M. Hrasnica

8
Conclusion and Ongoing Work
Numerical models were created using different classes of software. Wallets and walls
were simulated with macro-models using engineering masonry material model
(smeared
cracking
type
of
model)
and
meso-models
using
combined
cracking-shearing-crushing material model. Some numerical results are compared
against the experimental data regarding the wallets and shear walls.
Finite
element
modeling
strategies
using
engineering
masonry
model
for
macro-models
and
combined
cracking-shearing-crushing
material
model
for
meso-models can quite well simulate the behavior of masonry structures presented in
this paper. Further model upgrades that will include RC jacket strengthening installed
on wall sides are currently in development stage.
Macro-element or equivalent frame models can be used quite efﬁciently for the
analysis of the complete structures, however, only basic failure modes can be con-
sidered in the model.
Discrete and combined models are expected to become more popular in future since
all materials are discontinuous at a certain scale, but these are still conﬁned to the
research community.
References
1. TNO Diana BV: DIANA—User’s Manual. Material library, Delft, The Netherlands (2016)
2. Lourenço, P.B.: Computational Strategies for Masonry Structures. Delft University Press,
The Netherlands, Delft (1996)
3. Schreppers, G.J., Garofano, A., Messali, F., Rots, J.G.: DIANA Validation Report for
Masonry Modelling. DIANA FEA BV and TU Delft, Delft (2016)
4. Hrasnica, M., Ademović, N., Medić, S., Biberkić, F.: Experimental in-plane cyclic response
of unreinforced masonry walls versus strengthened walls using jacketing, brick and block
masonry. In: Proceedings of the 16th International Brick and Block Masonry Conference,
pp. 26–30. CRC Press, Padova, Italy, June (2016)
5. ITASCA, PFC2D—User Manual. http://www.itascacg.com/pfc2d
6. Munjiza, A.: The combined ﬁnite-discrete element method. Wiley (2004)
7. Lagomarsino, S., Galasco, A., Penna, A., Cattari, S.: TREMURI Program—Seismic
Analysis Program for 3D Masonry Buildings (2008)
8. Meskouris, K., Butenweg, C., Mistler, M., Kuhlmann, W.: Seismic behaviour of historic
masonry buildings. In: Proceedings of the 7th National Congress on Mechanics. Hellenic
Society for Theoretical and Applied Mechanics, Chania, Crete 24–26 June (2004)
9. Medic, S.: Discrete Element Method Using Particle Flow Code for 2D problems,
iNDiS-Planning, Design, Construction and Building Renewal. University of Novi Sad,
Faculty of technical sciences, Novi Sad, Serbia (2012)
10. Hrasnica, M., Ademovic, N., Novak, B., Kurtovic, A., Biberkic, F., Medic, S.: Cyclic shear
tests on URM and strengthened masonry walls and its modeling. In: 2nd European
Conference on Earthquake Engineering and Seismology. European Association for
Earthquake Engineering, Istanbul, Turkey (2014)
11. Page, A.W.: Unreinforced masonry structures—an Australian overview. In: Paciﬁc
Conference on Earthquake Engineering. Melbourne (1995)
Modeling Strategies for Masonry Structures
643

12. Gu, X.L., Zhang, H., Jia, J.Y., Li, X., Chen, G.L.: Multi-scale analysis of masonry structures
based on discrete method, brick and block masonry. In: Proceedings of the 16th International
Brick and Block Masonry Conference. CRC Press, Padova, Italy, 26–30 June (2016)
13. Cundall, P.A.: A computer model for simulating progressive large scale movements in
blocky rock systems. Proc. Symp. Rock Fract., Nancy (1971)
14. Smoljanović, H., Živaljić, N., Nikolić, Ž.: A combined ﬁnite-discrete element analysis of dry
stone masonry structures. Eng. Struct. 52, 89–100 (2013)
15. Smoljanović, H., Nikolić, Ž., Živaljić, N.: A combined ﬁnite-discrete numerical model for
analysis of masonry structures. Eng. Fract. Mech. 136, 1–14 (2015)
16. Nastić, N.: Fundamentals and application of discrete element method. Master thesis, Faculty
of Civil Engineering, University of Sarajevo (2015)
17. Simonović, G.: Computational models for 3D analysis and seismic assessment of existing
buildings. PhD Thesis. Faculty of Civil Engineering, University of Sarajevo (2014)
644
S. Medić and M. Hrasnica

Modern Geodetic Technologies As a Basis
of the Design and Planning
Marina Davidović(&) and Tatjana Kuzmić
Faculty of Technical Sciences, Department of Civil Engineering and Geodesy,
University of Novi Sad, 21000 Novi Sad, Republic of Serbia
d.marina92@yahoo.com, cukili13@gmail.com
Abstract. Traditional way of collecting spatial data leaves the dominant role to
modern technologies, especially when it comes to mass storage of spatial forms.
Modern technologies of data collection and the obvious development of
geo-information technologies provide a wide range of information and indicate
that the moment of transition from conventional methods to introducing new
technologies for making digital topographic maps, especially when it is needed
to keep track of objects of cultural and historical signiﬁcance has come. This
paper presents the advantages, characteristics and the components of modern
surveying technologies, such as digital photogrammetry, GPS, LIDAR and
UAV systems. Also, a software tool is presented-MicroSurveyCAD, which is
used to process point clouds, in different variants.
Keywords: Point cloud  Digital photogrammetry  GPS  LIDAR  UAV
1
Introduction
Preparation of preliminary studies, as well as projects with different levels of detail and
quality, requires updated topographic and geodetic base that would be created as soon
as possible and that would be prepared to use new computer technologies. Existing
substrates, cadastral maps, topographic maps and other graphic documentation, with its
promptness usually do not meet users’ needs. In response to the increasingly complex
demands of architects in terms of spatial data, there is a signiﬁcant development in the
ﬁeld of geo-information technologies. Traditional way of collecting spatial data leaves
the dominant role to modern technologies, especially when it comes to mass storage of
spatial forms. The revolutionary development of satellite positioning technology and
remote sensing sensors does in favor of this conclusion.
Creating a digital archive of signiﬁcant spatial forms provides the ability to protect
data from deterioration over time. Laser scanning technology has a revolutionary
application in the formation of a digital archive of objects of cultural signiﬁcance, in
order to precisely document their condition and registration of all possible damages that
could be effectively recovered in the event of the collapse or destruction of any kind.
Due to the ongoing work on the construction of the highway Belgrade—South Jadran,
we have witnessed the involvement of citizens, non-governmental organizations and
environmental organizations in order to preserve the oak old six centuries, located in
Savinac near Gornji Milanovac, in the middle of the future route. Modern technologies
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_56

of data collection and the obvious development of geo-information technologies pro-
vide a wide range of information and indicate that the moment of transition from
conventional methods to introducing new technologies for making digital topographic
maps, especially when it is needed to keep track of objects of cultural and historical
signiﬁcance has come. The projected targets for the transition to the modern working
mode in the ﬁeld of spatial information technology relate to obtaining quality digital
base for making all kinds of projects, and providing substantial savings in money, time
and resources, but with mandatory initial investment. The introduction of modern
technologies in the processes of spatial planning is necessary in order to enter in step
with contemporary global trends.
2
Modern Geodetic Techologies
Modern geodetic technologies (Fig. 1) involve:
• Digital photogrammetry;
• GPS;
• LIDAR;
• UAV.
2.1
Digital Photogrametry
Photogrammetric methods belong to the very numerous methods when looking at the
collection of massive amounts of data about objects and phenomena on the physical
surface of the Earth [1]. The 3D coordinates of points on an object surface are
determined based on overlapping images with camera position and orientation infor-
mation known as exterior orientation. In order to determine these parameters suc-
cessfully, it is necessary that at least three control points exist in the overlapping area of
GPS
Modern geodetic 
technologies 
LIDAR
Unmanned 
aerial vehicle
Digital photo-
grammetry
Fig. 1. Modern geodetic technologies
646
M. Davidović and T. Kuzmić

photography. The parameters of interior orientation that describe the principal point
location and calibrated focal length of the camera have to be known [2].
With the classical approach, the overlapping images were recorded according to the
stereographic principle. The 3D measurements of the object points in the stereo model
were performed by a human operator after orientation of a stereo model using analog or
analytic stereo plotter. All these procedures have been replaced in digital photogram-
metry thanks to advances in electronics, computers and information technology [2].
Based on the photos it is possible to:
• determine the coordinates of the points on surveyed objects;
• create maps and plans;
• establish orthorectiﬁed photos;
• make a digital terrain model and digital 3D model of objects.
Of course, all these elements are determined without contact with the appropriate
object or surface, and according to a predetermined accuracy. Figure 2 presents a high
resolution digital camera that is often used for photogrammetric recording.
2.2
GPS
GPS stands for NAVSTAR GPS (Navigation System with Time And Ranging Global
Positioning System). It is a system that allows us to position ourselves in space. GPS is
a passive system based on communication with the satellites (users receive information
from satellites without the possibility of sending information to the satellite). GPS
system uses a constellation of 24 satellites and consists of three segments—space,
control and user.
GPS (Fig. 3) is based on measuring the length, which is called the trilateration
method. Distance measurement is achieved by specifying the duration of the signal
travel or by phase difference (comparison of two signals). Accordingly, two different
Fig. 2. Example of camera used in photogrammetry [3]
Modern Geodetic Technologies
647

types of measurement exist: code and phase. The phase measurements are much more
precise, until the code measurements are used mainly for navigational purposes.
Beneﬁts of GPS systems are [4]:
• three-dimensional positioning;
• independence from the weather conditions and the possibility of measuring at any
time of day or night;
• line-of-sight visibility between survey stations is not necessary;
• high precision;
• rapid data processing and quality control.
LIDAR and UAV represent the latest innovation in the digital display of the Earth’s
surface. Since that, they will be discussed with more details.
Fig. 3. GPS device [5]
648
M. Davidović and T. Kuzmić

3
LIDAR
LIDAR stands for Light Detection And Ranging. LIDAR is accepted method for
generating precise and direct-georeferenced spatial data about characteristics of the
surface of the Earth. This method allows higher data accuracy, precision and density
[6]. What makes it especially attractive is the high spatial and temporal resolution of the
data, and the possibility of observing the atmosphere and the scope of coverage of the
heights from ground to more than 100 km altitude [7].
The basic working principle of LIDAR (Fig. 4) is based on determining the dis-
tance from the sensor to the object in space by means of laser light. The technology is
based on the collection of three different sets of data. Position of sensor is determined
using a GPS phase measurements in a regime of relative kinematics, while the use of an
inertial measurement unit (IMU) provides known orientation. The last component is a
laser scanner. Laser sends an infrared beam to the ground and it is reﬂected back to the
sensor. After the ﬁeld work operation, the data are processed, after which the polar
coordinates for each point on the surface from which the laser beam had refused are
obtained. It is essential to focus the laser beam in a certain direction, which is done in
most LIDAR systems by using a mirror that oscillates or rotates. This system is the
most ﬂexible in terms of layout and density of points on the ground. This results in a
range of laser pulses, which scans the surface of the ﬁeld by one line. The mirror
oscillates perpendicularly to the direction of ﬂight, and as a result of movement of the
aircraft, scan lines that are not perpendicular to the direction of ﬂight, but a little
slanting, are obtained. All the lines are mutually connected to each other, in the form of
the letter “Z” and so form the strip of scanned surface ﬁeld.
Fig. 4. LIDAR working mode [8]
Modern Geodetic Technologies
649

LIDAR is present on the market since the mid-nineties. Its adaptation to the con-
servative mentality of geodetic community met with great resistance. It can be seen
through the emergence of new technologies, new modes of learning, manipulating the
vast amount of data and new formats; and so the acceptance of the ﬁnal product took a
really tough time. LIDAR today represents in the world one of the most modern
technologies that is used in the survey and preparation of topographic maps for dif-
ferent purposes [9]. It makes possible fast, efﬁcient and economically feasible data
collection based on the principles of geodesy. LIDAR can be used in many areas as a
substitute for conventional methods of surveying and aerial photogrammetry. The user
speciﬁes what type of system he will use. IMU (inertial unit) is used to calculate a
correction due to the tilt, yaw and the like.
Characteristics of LIDAR scans are: high density of dots in the scanning process
(formation of a cloud of points), high-resolution digital photos and a high level of
accuracy. LIDAR can work in almost all conditions, but does not work well in areas
with snow and in case of fog and rain. It is very difﬁcult to ﬂy at precisely deﬁned path
in a windy area.
A LIDAR system generates a huge amount of points, so consequently signiﬁcant
resources are required to handle these data and their processing. Under the “vast
amount of points” is meant up to several hundred million dots. These points are related
to the most diverse objects such as treetops, cars, buildings, houses, etc. A large
number of users request information relating just to the surface of the terrain (digital
terrain model), while there are those who are looking for an area deﬁned by vegetation
cover which includes trees and buildings together (digital surface model). It is therefore
essential to apply intelligent methods of data processing and to remove unwanted
information using various methods of ﬁltering.
Figure 5 provides an overview of point clouds obtained by the LIDAR. The data
are loaded into the software PointTools. As can be seen, the user is provided the
Fig. 5. Cloud of points
650
M. Davidović and T. Kuzmić

coordinates of each point from the cloud—x, y, and height. Each point can be accessed
individually. The ﬁgure clearly shows the road surface, curb, vegetation along the way,
artiﬁcial objects, and so on.
MMS (Mobile Mapping System), as subcategory of LIDAR creates new opportu-
nities because it can quickly collect enormous amounts of high precision, georefer-
enced spatial data and transform them into a three-dimensional model, rich in
information. MMS involves the implementation of laser scanning technology com-
bined with high-precision navigation system to allow 3D scanning of roads, buildings
and trees from a moving vehicle. The system uses several laser scanners, each of them
running about 10000 measurements per second [10].
With the appropriate software solutions, MMS can automate key processes such as
the creation or extraction of model surfaces, road signs, urban elements, curbs,
pedestrian crossings and road geometry and increase the proﬁtability of the mapping
process. It also provides integration with the most popular Geographic Information
Systems in terms of cartographic databases and applications.
The mobile mapping systems are commonly ideal for mapping the corridors and is
therefore center on trafﬁc routes or intersections. As MMS can record under normal
trafﬁc speed—there is no need for road closures and warnings about security. That is,
the recording can be carried out smoothly.
Combining the laser scanning data with video and photographic data, point cloud in
color and 3D model is obtained. The results can be used and displayed in many
formats. Some of them are: LAS, ASCII, XYZ, POD, PTS, etc. DEM (Digital Ele-
vation Model) is a continuous mathematical model representing the surface of the
Earth. Height is a function of positional coordinates [11]:
H ¼ f y; x
ð
Þ or H ¼ f ðu; kÞ
It is important to point out that there are two types of DEM: Digital Surface Model
(DSM), i.e. model that represents the Earth’s surface with all the natural and artiﬁcial
objects on Earth, including houses, buildings, vegetation; and Digital Terrain Model
(DTM), i.e. model that represents the “naked” Earth’s surface without vegetation and
man-made objects. During the preparation of topographic plans and in various stages of
design, both models are often used.
In order to obtain the DTM it is necessary to classify points in three categories by
applying intelligent algorithms. Point belongs to the Earth surface, object, or vegeta-
tion. Without going into greater details, the principle of classiﬁcation is as follows [12]:
• Identiﬁcation of the points on the principle the ﬁrst and the last of similar height.
Based on the identiﬁed points polygons are created.
• All the points of the last echo that fall within the detected polygons and have a
similar height in the ﬁrst and the last echo probably belong to the object.
• All the points of the ﬁrst echo falling into detected polygons, and have a signiﬁ-
cantly different height from the last echo probably belong to the vegetation.
Model is created based on the points that are classiﬁed as items belonging to the
ﬁeld. This model represents the DTM.
Modern Geodetic Technologies
651

Laser scanning has an important application in topographic mapping. Dr. Edward
Jaselskis in 2003. conducted a comparison research on the assessment of the accuracy
of the method of least squares to model surfaces formed on the basis of LIDAR data
and the total station. The result of the research is that by applying different methods the
accuracy changes of 1.2%, indicating that LIDAR is a very efﬁcient method for
forming the surface [13].
Matti Vaaje in 2011. explored the beneﬁts of use MMS in observing topography
and altitude changes along the river corridor. Vehicles used were small, rigid hull—
inﬂatable boat and trolleys designed to be drawn by an individual. The results of the
research period (one year) showed that MMS provides accurate and precise detection of
changes. However, careful control of systematic errors should be done. He also noted
that the scanning of the area parallel to it resulted in lower accuracy compared to the
record made at right angles to the topography [13].
3.1
Generation of Topographic Maps Based on Point Cloud
Another software tool that can be used to process point clouds is MicroSurveyCAD.
This program creates a database that contains the location of points, descriptions,
distances and various other information for their connection. The database is stored in a
ﬁle with the extension MSZ. The program creates certain ﬁles to provide ﬁnding a
particular entity and accessing based on their identiﬁers.
This software is based on CAD platform that enables using many tools for
extracting points, lines and other entities directly from point clouds.
MicroSurvey CAD can work with a created point cloud using LIDAR, but also with
simple formats, such as ASCII. Point cloud opens in a separate window—Point Cloud
Line Work. Although this program opens separately, each step of work in this window
could be simultaneously saved in the main window, i.e. drawing.
Fig. 6. Point cloud shown by the option ‘gray scale’
652
M. Davidović and T. Kuzmić

The format that is recommended for work with large data sets is Leica PCI format.
In this software is possible to open LAS format (version 1.0–1.3) directly, but it is
recommended to convert to the PCI format.
Figure 6 shows a point cloud in MicroSurvey CAD, in gray-scale version.
Point cloud can also be displayed in a different way using option ‘elevation
mapping’ which expresses the heights of the cloud points (Fig. 7).
Fig. 7. Point cloud shown by the option ‘elevation mapping’
Fig. 8. Surface model through a combination of TIN and GRID structure
Modern Geodetic Technologies
653

The digital surface model can be created through a combination of TIN and GRID
structure. TIN structure means that point cloud triangles are formed, while grid means
forming a square elements. The combination of these two structures is obtained by the
model as shown in Fig. 8.
Figure 9 is a digital surface model that can be created from point clouds using the
appropriate tools within the software. TIN model is used, from point clouds to generate
a large number of small triangles, that enable a faithful presentation of the Earth’s
surface, is formed.
The ability of simultaneously work with the point cloud and digital terrain models
and surfaces, make this software modern. This software is the right choice for those
who want to use point cloud for simulation and animation creating.
Deﬁning accuracy of LIDAR system faces a number of challenges. There is no
guarantee that the laser will hit the target center or control point. Also, the trace size of
the laser ray increases with distance from the scanner and could be a few centimeters
and greater. Therefore, it is sometimes unknown from where exactly the returned ray
has reﬂected. The accuracy of each point will depend on the scanned object geometry.
The accuracy of point cloud can be deﬁned as absolute and relative. Relative means
deﬁning accuracy compared to some other measured point, while absolute accuracy
means the accuracy with which the point cloud is positioned in the coordinate system.
Currently, the common practice is to perform a comparison of points with the same
‘control’ points (points within the cloud that were collected independently of the
scanner). Here arises the uncertainty whether the observed point is compared to the
control point that suits her.
To reduce this uncertainty, the control points are placed on a typical sites that can
be easily distinguished and observed within a cloud.
Fig. 9. Surface model through TIN structure
654
M. Davidović and T. Kuzmić

The accuracy of data obtained by LIDAR can be separated into two components—
vertical and horizontal. Vertical accuracy is much easier to determine. Figure 10a
shows that the accuracy of the vertical component can be easily determined by the ratio
of control points and a plane which can be formed by three scanned points. Figure 10b
shows that in the same situation, when determining the horizontal component, there is a
problem with which scanned point should control point be connected and whether it
should be compared with any of them.
Ray and Graham proposed in 2008 a method by which the horizontal accuracy is
determined based on the reﬂected pulses intensity. However, this method had certain
drawbacks, but it has been worked on its improvement.
4
Unmanned Aerial Vehicle
UAV (Unmanned Aerial Vehicle) also belongs to the modern surveying technologies,
and lately more often is used for the design and planning (Fig. 11). As the name
suggests, this “instrument” is not managed by man, but uses an aerodynamic force that
provides aircraft lifting. It can ﬂy autonomously, and remotely can be managed. It can
Fig. 10. Vertical and horizontal accuracy
Modern Geodetic Technologies
655

be expendable or recoverable. UAV is capable for a controlled, sustained ﬂight level
[10]. UAV delivers an alternative to the acquisition of semi-aerial platform corre-
sponding for a relatively small region of interest (<5000 ha) [14]. Recording result with
the UAV system is a dense point cloud.
Generally, these complex systems include ground stations and other elements next
to the aircraft. There are different types of unmanned aerial vehicles, but for research,
recording and development in surveying and geomatics, also commercial aircrafts are
used. It can be used as a mapping platform. These platforms are equipped with systems
for photogrammetric measurement including small or medium video cameras, camera
systems for thermal and infrared light, multispectral cameras, sensors for a cameras
range and LIDAR sensors in the air. In conditions where there is no accurate DEM,
statistical error for determining the theoretical accuracy is calculated. The absolute
geometric accuracy of the recording results, using GIS approach with different soft-
ware, cameras and GCP schemes is shown in the tables below [15]. Table 1 shows the
geometrical accuracy obtained using the S100 camera.
Table 2 shows the geometrical accuracy obtained using the NEX7 camera.
Fig. 11. UAV [10]
Table 1. Geometrical accuracy of camera S100 [15]
GCP (VHRS)
GCP (Recent)
Plan (m) Height (m) Plan (m) Height (m)
PCI Geomatics
11,099
28,775
9,815
22,312
PCI Geomatics (Calib. data)
2,742
7,530
1,333
5,197
Agisoft
1,016
1,610
0,863
1,274
656
M. Davidović and T. Kuzmić

UAV is a valuable source of information for inspection, monitoring, mapping and
3D modeling. New applications in the short and close range domain are introduced as a
cheaper and more convenient than aerial. In recent years there has been more use of
UAV in geodesy. The biggest advantage of UAVs versus manual systems is that UAVs
can ﬂy, survey in inaccessible areas, such as mountains, deserts, areas of earthquakes,
volcanoes, ﬂoods, accidents (war zones). In the ﬂight up to 200 m of the surface
recording, there is no need for speciﬁc weather conditions (clouds will not prevent the
mission).
The result of UAV surveying (Fig. 12) is similar to the result of air pilot mapping
system. The entire project consists of:
• preparation phase (choice of appropriate drones and projection parameters);
• ﬂight plan (ﬂight parameters and ﬂight mode);
• quality data check (for unmanned ﬂight).
• determining the trajectory and data processing (position and orientation);
• the ﬁnal product (quality 3D result, referring to the DSM, DTM, contour lines..).
Table 2. Geometrical accuracy of camera NEX7 [15]
GCP (VHRS)
GCP (Recent)
Plan (m) Height (m) Plan (m) Height (m)
PCI Geomatics 1,993
4,304
1,842
3,232
Agisoft
1,117
2,232
1,081
0,967
Fig. 12. Result of UAV surveying [10]
Modern Geodetic Technologies
657

Basically, there are three aspects for UAV data processing (shown in Table 3). The
combination of them is used to obtain topographic maps with the best geometrical
accuracy [15]:
Topographic maps, as surveying results, are becoming priorities in the development
of a detailed spatial planning around the world. For faster processing of these maps, the
integration of air and alternative technologies is convinient. A compilation of forms,
including planimetric objects (2D) and digital terrain models (3D) is integrated in the
provision of digital elevation model (DEM) as the main activity of topographic map-
ping. DEM can be used for topographical maps creation at large scale, 1:10 000. To
obtain a map at large-scale, integration with UAV data is used, particularly for ensuring
greater absolute height accuracy.
Thus, compared to conventional surveying techniques (total station), where sur-
veying expert a priori chooses characteristic points on the object, the advantage of
UAVs and laser scanning is that the points are recorded with a predetermined reso-
lution (density) where measurement quality are geometrical elements recorded with
much more points than traditional methods. UAV becomes a discovery that can
combine high resolution data acquisition with a relatively simple and low-cost platform
compared to conventional air companies. Flying at low altitude, the UAV recordings
with geospatial objects with high level of detail and in the all colors mode, are
obtained. Generating point clouds, mesh (correct or incorrect points network),
depending on the vertical and horizontal resolution laser scanner, is formed. Network
size is directly chosen by an expert who process the ﬁnal measurements, and that is the
result of measurement resolution and points density that will be achieved after ﬁltering.
Thus, modern technological collecting processes and spatial data processing enables
the 3D display of spatial forms (terrain and buildings) in full-color mode [16]. Virtually
all recent geo-information systems have integrated module for 3D visualization that
enables 3D objects positioning in relative and absolute model or coordinate system.
Many of them have some additional beneﬁts such as the possibility of drawing the
Table 3. UAV data processing [15]
658
M. Davidović and T. Kuzmić

building height, ﬂight simulation over the digital terrain model, and so on. In sur-
veyings in Indonesia, the 8 themes/layers involved in obtaining a topographic map are
speciﬁed:
coasts,
hypsography
hydrography,
geographic
names,
administrative
boundaries, transportation, public facilities, buildings and land cover [15].
5
Conclusion
Topography, or the terrain surface provides many important, basic information for a
variety of applications. Scientists use topographic maps to study the plants, conduct
geological analysis, etc. It was noted that the topographic data form the basis for
determining the water level and ﬂooding surface areas. Topographic maps are also used
in applications for aircraft navigation. Also, layers in Google Earth are constantly
updated based on topographic maps. Today, the infrastructure development requires
fast and constant production of updated topographic maps and digital surface models
with different accuracy levels.
The method choice for data collection is based on user needs, accuracy require-
ments and level of details, the period in which it is necessary to create the appropriate
product, economic opportunities and available resources. Surveying profession is now
facing challenges in terms of the period in which it is necessary to create the right
product. The realization products process is required to be created in time, within
budget and satisfy the needs for which it is intended and, of course, that it is the
appropriate quality.
In modern times there is an intensive technology and software development that
enables experts to respond adequately to the tasks and survive in the market.
Of course, great progress through the data integration and databases development
has been achieved. This allows the joint use of data collected by different methods and
gathering data of signiﬁcantly better quality.
Observing all of this, it can be concluded that the time in which traditional and
conventional methods increasingly lose value and are suppressed by time and progress,
has come.
In particular, a lot of time is spent on the data collection and its postprocessing in
the ofﬁce. Today, the efﬁciency of modern methods has reached such a level that for a
very short period of time can raise large data amounts, which were previously
impossible. Also the data processing, analysis and interpretation has become digitized
and extremely simpliﬁed.
References
1. Sturzenegger, M., Stead, D.: Close-range terrestrial digital photogrammetry and terrestrial
laser scanning for discontinuity characterization on rock cuts. Eng. Geol. 106(3–4), 163–182
(2009)
2. Yastikli, N.: Documentation of cultural heritage using digital photogrammetry and laser
scanning. J. Cult. Herit. 8(4), 423–427 (2007)
3. https://www.aerialarchives.com/UltraCam.htm. Accessed 15 Jan 2017
Modern Geodetic Technologies
659

4. Chrzanowski, A., Szostak-chrzanowski, A.: Deformation monitoring surveys—old problems
and new solutions. Reports Geod. 2(87), 85–103 (2009)
5. https://www.dreamstime.com/royalty-free-stock-images-geodetic-gps-image12112739.
Accessed 21 Jan 2017
6. Jamie Carter, L.B., Schmid, K., Waters, K., Brian Hadley, J.H., Mataosky, R.: Lidar 101 : an
introduction to lidar technology, data, and applications. NOAA Coast. Serv. Cent. 76 (2012)
7. Forestry Commission: Introduction to Lidar 1–18 (2012)
8. https://overheid.vlaanderen.be/DHM-Ondersteuning. Accessed 04 Feb 2017
9. Ninkov, T., Sušić, Z., Ninkov, J.: Savremene metode prikupljanja i obrade podataka u
geodeziji kao mogućnost pružanja novih servisa na tržištu usluga
10. Modern Acquisition Technology of Spatial Data as a Basis of Environmental Engineering
and Planning Projects, FIG Congress (2014)
11. Ninkov, T., Bulatović, V., Sušić, Z., Vasić, D.: Moderne tehologije prikupljanja podataka
kod projektovanja saobraćajnih i linijskih struktura i objekata; Nov 2013
12. Ninkov, T., Bulatović, V., Sušić Z.: Primena laserskog skeniranja kod projektovanja linijskih
struktura i objekata (2008)
13. Williams, K.E.: Accuracy Assessment of LiDAR Point Cloud Geo-Referencing, Jun 2012
14. Tampubolon, W., Reinhardt, W.: UAV data processing for large scale topographical
mapping. Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci.—ISPRS Arch. 40(5), 565–
572 (2014)
15. Tampubolon, W., Reinhardt, W.: UAV Data Processing for Large Scale Topographical
Mapping
16. Ninkov, T., Bulatović, V., Sušić, Z., Vasić, D.: Application of Laser Scanning Technology
for Civil Engineering Projects in Serbia
660
M. Davidović and T. Kuzmić

Nonlinear Analysis of Hyperelastic Membrane
Rasim Šehagić1(&) and Senad Medić2
1 Calypso Promet d.o.o., Bosanska Krupa, Bosnia and Herzegovina
rasimsehagic@gmail.com
2 Faculty of Civil Engineering, Institute for Materials and Structures, University
of Sarajevo, Sarajevo, Bosnia and Herzegovina
senad_medich@yahoo.com
Abstract. When analyzing structures with large displacements, the difference
between initial and current conﬁguration cannot be ignored. Most of the
structural design problems can be solved with the assumption that the dis-
placements will be small enough, with a possible error less than 1%. In this
paper, a membrane submitted to small and large displacements will be analyzed.
The stress-strain relationship is described as linear elastic and neo-Hookean
hyperelastic. We will show the difference in results when the membrane is
submitted to large displacements and why is it important to use the accurate
description in constitutive models. The simplest constitutive model for geo-
metric nonlinear analysis of membranes is hyperelastic neo-Hookean which is
an extension to large deformations of the usual linear elastic model. All geo-
metric nonlinear and linear elastic analyses were performed in MATLAB.
Keywords: Geometric nonlinearity  Linear elastic  Hyperelastic  Membrane
MATLAB
1
Introduction
Difference between the initial and the deformed conﬁguration cannot be ignored for
structures submitted to large displacements and rotations. For the proper choice of
analysis type one should estimate if the structural deformations are large or not and the
most common way for this is the engineering intuition. In this paper, we will analyze
hyperelastic membrane (as well as linear elastic) for which we certainly know that it
will, for adequate load intensity, have large deformations. The most common hyper-
elastic material used in structural engineering is an elastomer (like a rubber) which has
large strains at small loads. We will compare the response of a membrane described
with different stress-strain relationships, to prove the importance of proper constitutive
law in the modeling of structures. First, we will describe hyperelastic and linear elastic
stress-strain relationships and their ﬁnite element deﬁnitions. Then, we will analyze the
response of membrane for two different constitutive lays and assess the differences.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_57

2
Hyperelastic Material Model—Neo-Hookean Material
Hyperelastic material is an elastic material, which means that hyperelastic body returns
to the original shape after the forces have been removed. However, the difference with
respect to linear elastic materials lies in the deﬁnition of the stress-strain law. Namely,
the stress-strain relationship is derived from a strain energy density function and it’s
nonlinear
for
hyperelastic
materials
(Fig. 1).
Hyperelastic
materials
are
also
Cauchy-elastic, which means that the stress is determined by the current state of
deformation, and not the path or history of deformation.
There are many hyperelastic material laws. The simplest model is neo-Hookean
which is an extension to large deformations of the Hooke’s law. The model for
compressible materials is based on the following strain energy function:
w ¼ 1
2 kðln detF
ð
ÞÞ2  l ln detF
ð
Þ þ 1
2 lðTr c
ð Þ  3Þ
ð1Þ
F ¼
@x
@x0
@x
@y0
@x
@z0
@y
@x0
@y
@y0
@y
@z0
@z
@x0
@z
@y0
@z
@z0
0
B
B
@
1
C
C
A; c ¼ FTF
ð2Þ
where F is the deformation gradient and k and l are two material parameters known as
Lamé’s constants. They are related to the usual Young’s modulus E and Poisson’s
ration m in the following way:
k ¼
mE
ð1 þ mÞð1  2mÞ ; l ¼
E
2ð1 þ 2mÞ
ð3Þ
The 3  3 matrix c = FTF is known as the right Cauchy-Green tensor. We also
need the left Cauchy-Green tensor b = FFT.
By differentiating the strain energy density function with respect to the right
Cauchy-Green tensor, the second PK (Piola-Kirchhoff) and the Cauchy stress tensors
can be written as follows [2].
Fig. 1. Linear elastic and hyperelastic material behavior [3]
662
R. Šehagić and S. Medić

bS ¼ kðlnðdetFÞÞc1 þ lðI  c1Þ
ð4Þ
br ¼
k
detF lnðdetFÞI þ
l
detF ðb  IÞ
ð5Þ
where I is a 3  3 identity matrix.
3
Finite Element Formulation—Linear Elastic Membrane
When 2D and 3D domains are analyzed, isoparametric ﬁnite elements are usually
employed to construct the ﬁnite element approximations. That means that the chosen
ﬁnite element mesh is obtained by the corresponding mapping from the parent element.
Mapping is made using shape functions Na(f, η), where f and η are natural coordinates
(Fig. 2).
For interpolation functions for 4-node element are:
Na n; g
ð
Þ ¼ 1
4 1 þ nan
ð
Þ 1 þ gag
ð
Þ; na ¼ 1; ga ¼ 1; a ¼ 1; 2; 3; 4
ð6Þ
,
N1 n; g
ð
Þ ¼ 1
4 1 þ n
ð
Þ 1 þ g
ð
Þ
N2 n; g
ð
Þ ¼ 1
4 1  n
ð
Þ 1 þ g
ð
Þ
N3 n; g
ð
Þ ¼ 1
4 1  n
ð
Þ 1  g
ð
Þ
N4 n; g
ð
Þ ¼ 1
4 1 þ n
ð
Þ 1  g
ð
Þ
8
>
>
<
>
>
:
ð7Þ
The displacement ﬁeld of the isoparametric ﬁnite element is constructed by
employing the same shape functions as those used for the element geometry
representation
uh
i n; g
ð
Þ ¼
X
4
a¼1
Na n; g
ð
Þde
a ) uh n; g
ð
Þ ¼
X
4
a¼1
Na n; g
ð
Þde
a
ð8Þ
Fig. 2. Quadrilateral isoparametric ﬁnite element with 4 nodes and its parent element
Nonlinear Analysis of Hyperelastic Membrane
663

where de
a are the nodal displacement values. The corresponding strain ﬁeld approxi-
mation can then easily be obtained as
eh n; g
ð
Þ ¼
X
nen
a¼1
Bade
a
ð9Þ
Ba ¼
@Na
@x1
0
0
@Na
@x2
@Na
@x2
@Na
@x1
2
664
3
775
ð10Þ
The element load vector is computed by numerical integration. To that end, the
isoparametric mapping is used in order to replace the element integration domain by the
parent element.
fe
a ¼
Z
Xe Na x
ð Þb x
ð ÞdV
¼
Z
þ 1
1
Z
þ 1
1
Ne
a n; g
ð
Þbe n; g
ð
Þj n; g
ð
Þdndg
¼
X
nin
i¼1
X
nin
j¼1
Na ni; gj


be ni; gj


j ni; gj


wiwj
ð11Þ
where j n; g
ð
Þ ¼ @x
@n n; g
ð
Þ @y
@g n; g
ð
Þ  @x
@g n; g
ð
Þ @y
@n n; g
ð
Þ is the determinant of the
Jacobian matrix of the isoparametric coordinate transformation and wi, wj are weights
of the numerical integration in f and η directions respectively.
The element stiffness matrix is computed in the same manner [1].
Ke
ab ¼
Z
Xe BT
a x
ð ÞCBb x
ð ÞdV
¼
Z
þ 1
1
Z
þ 1
1
BT
a n; g
ð
ÞCBb n; g
ð
Þj n; g
ð
Þdndg
¼
X
nin
i¼1
X
nin
j¼1
BT
a ni; gj


CBb ni; gj


j ni; gj


wiwj
ð12Þ
4
Finite Element Formulation—Hyperelastic Membrane
The ﬁnite element equations can be established in a manner similar to that used for
linear problems. The initial geometry is mapped to the master element using the
appropriate interpolation functions [3]:
664
R. Šehagić and S. Medić

x0 n; g; 1
ð
Þ ¼
X
i
Nix0
i ; y0 n; g; 1
ð
Þ ¼
X
i
Niy0
i ;
z0 n; g; 1
ð
Þ ¼
X
i
Niz0
i
ð13Þ
where x0, y0, z0 are coordinates of an element in the initial conﬁguration.
The Jacobian matrix of the mapping can be obtained by direct differentiation:
J ¼
@x0
@n
@x0
@g
@x0
@1
@y0
@n
@y0
@g
@y0
@1
@z0
@n
@z0
@g
@z0
@1
0
B
B
@
1
C
C
A;
detJ ¼ Det½J
ð14Þ
The new nodal coordinates are obtained by adding the currently known nodal
displacements:
xi ¼ x0
i þ ui;
yi ¼ y0
i þ vi;
zi ¼ z0
i þ wi
ð15Þ
The mapping for this known geometry is written using the same interpolation
functions:
x n; g; 1
ð
Þ ¼
X
i
Nixi; y n; g; 1
ð
Þ ¼
X
i
Niyi; z n; g; 1
ð
Þ ¼
X
i
Nizi
ð16Þ
The deformation gradient can be established as follows:
F ¼
@x
@x0
@x
@y0
@x
@z0
@y
@x0
@y
@y0
@y
@z0
@z
@x0
@z
@y0
@z
@z0
0
B
B
@
1
C
C
A;
@x
@x0
@x
@y0
@x
@z0
0
B
@
1
C
A ¼ JT
@x
@n
@x
@g
@x
@1
0
B
@
1
C
A
. . .
ð17Þ
Final linearized virtual work expression:
Z Z
V0
Z
BSBT þ BF
TCFBT
h
i
dV0Dd
¼ 
Z Z
V0
Z
BF
TSdV0 þ
Z Z
V0 Nq0dA0 þ
Z Z
V0
Z
Nb0dV0
ð18Þ
or
kc þ ks
ð
ÞDd ¼ ri þ rq þ rb
ð19Þ
where kc is the current stiffness matrix, ks is geometric stiffness matrix, ri is the
equivalent nodal load vector due to stresses in the current conﬁguration, rq is the
equivalent nodal load vector due to the surface forces, and rb is the equivalent nodal
load vector due to body forces.
For hyperelastic material, second PK stress tensor and elasticity tensor can be
written as:
Nonlinear Analysis of Hyperelastic Membrane
665

bS ¼ kðlnðdetFÞÞc1 þ lðI  c1Þ
ð20Þ
Ci;j;k;l ¼ kc1
i;j c1
k;l þ l ln detF
ð
Þ c1
i;k c1
j;l þ c1
i;l c1
kj


ð21Þ
where I is a 3  3 identity matrix and k and l are two material parameters known as
Lamé’s constants. They are related to the usual Young’s modulus E and Poisson’s
ration m.
k ¼
mE
ð1 þ mÞð1  2mÞ ; l ¼
E
2ð1 þ 2mÞ
ð22Þ
All components of elasticity tensor are deﬁned in [2].
5
Practical Example
We will analyze linear elastic and hyperelastic membrane with dimensions shown in
Fig. 3. The type of material that can be described as hyperelastic is an elastomer, so we
will say that elastomer would behave in a similar manner.
For both stress-strain relationships, the membrane is submitted to forces which will
produce small and large displacements. In the ﬁrst case, we will use F1 = 1 and F2 = 1
and for the second case F1 = 150 and F2 = 150. We assume that E = 1000, m = 0.25
and h = 0.1. For simplicity, we will use only one ﬁnite element. All models were
formed in MATLAB [4] (Figs. 4 and 5).
Fig. 3. Membrane dimensions
666
R. Šehagić and S. Medić

u1;h
v1;h
u2;h
v2;h
u3;h
v3;h
u4;h
v4;h
2
66666666664
3
77777777775
¼
0
0
0:079
0:007
0:079
0:007
0
0
2
66666666664
3
77777777775
m
½ ;
u1;le
v1;le
u2;le
v2;le
u3;le
v3;le
u4;le
v4;le
2
66666666664
3
77777777775
¼
0
0
0:079
0:007
0:079
0:007
0
0
2
66666666664
3
77777777775
m
½ ;
u1;h  u1;le
v1;h  v1;le
u2;h  u2;le
v2;h  v2;le
u3;h  u3;le
v3;h  v3;le
u4;h  u4;le
v4;h  v4;le
2
66666666664
3
77777777775
¼
0
0
0
0
0
0
0
0
2
66666666664
3
77777777775
m
½ 
where u1,h, u2,h, u3,h, u4,h are horizontal nodal displacements of hyperelastic membrane,
v1,h, v2,h, v3,h, v4,h are vertical isoparametric ﬁnite elements displacements of hyper-
elastic membrane, u1,le, u2,le, u3,le, u4,le are horizontal isoparametric ﬁnite elements
displacements of linear elastic membrane and v1,le, v2,le, v3,le, v4,le are vertical
isoparametric ﬁnite elements displacements of linear elastic membrane.
Fig. 4. Deformed hyperelastic membrane, small displacements
Fig. 5. Deformed linear elastic membrane, small displacements
Nonlinear Analysis of Hyperelastic Membrane
667

When the structure is submitted to forces that will produce small displacement it is
not important whether the stress-strain relationship is described as linear-elastic or
hyperelastic. The displacement ﬁeld for both cases is practically the same.
rxx;h
ryy;h
sxy;h
2
64
3
75 ¼
10:44
1:89
0:20
2
64
3
75;
rxx;le
ryy;le
sxy;le
2
64
3
75 ¼
5:13
0:90
0:10
2
64
3
75 for ðn ¼  1ﬃﬃﬃ
3
p ; g ¼  1ﬃﬃﬃ
3
p ; w ¼ 1:0Þ
rxx;h
ryy;h
sxy;h
2
64
3
75 ¼
10:44
1:89
0:20
2
64
3
75;
rxx;le
ryy;le
sxy;le
2
64
3
75 ¼
5:13
0:90
0:10
2
64
3
75 for ðn ¼  1ﬃﬃﬃ
3
p ; g ¼ 1ﬃﬃﬃ
3
p ; w ¼ 1:0Þ
rxx;h
ryy;h
sxy;h
2
64
3
75 ¼
9:89
0:33
0:20
2
64
3
75;
rxx;le
ryy;le
sxy;le
2
64
3
75 ¼
4:86
0:20
0:10
2
64
3
75 for ðn ¼ 1ﬃﬃﬃ
3
p ; g ¼  1ﬃﬃﬃ
3
p ; w ¼ 1:0Þ
rxx;h
ryy;h
sxy;h
2
64
3
75 ¼
9:89
0:33
0:20
2
64
3
75;
rxx;le
ryy;le
sxy;le
2
64
3
75 ¼
4:86
0:20
0:10
2
64
3
75 for ðn ¼ 1ﬃﬃﬃ
3
p ; g ¼ 1ﬃﬃﬃ
3
p ; w ¼ 1:0Þ
Regarding stresses in the structure, we can see that stress-strain description is
important, because, no matter that the displacement are the same, the difference in
stresses can be up to 100% for small membranes submitted to the forces of small
intensity (Figs. 6 and 7).
Fig. 6. Deformed hyperelastic membrane, large displacements
668
R. Šehagić and S. Medić

u1;h
v1;h
u2;h
v2;h
u3;h
v3;h
u4;h
v4;h
2
66666666664
3
77777777775
¼
0
0
9:21
0:58
9:21
0:58
0
0
2
66666666664
3
77777777775
m
½ ;
u1;le
v1;le
u2;le
v2;le
u3;le
v3;le
u4;le
v4;le
2
66666666664
3
77777777775
¼
0
0
5:89
0:54
5:89
0:54
0
0
2
66666666664
3
77777777775
m
½ ;
u1;h  u1;le
v1;h  v1;le
u2;h  u2;le
v2;h  v2;le
u3;h  u3;le
v3;h  v3;le
u4;h  u4;le
v4;h  v4;le
2
66666666664
3
77777777775
¼
0
0
3:32
0:05
3:32
0:05
0
0
2
66666666664
3
77777777775
m
½ 
When the structure is submitted to forces that produce large displacement, as we
can see, stress-strain relationship is very important because the difference in dis-
placement ﬁeld can be signiﬁcant. In our case, it is cca.40%.
rxx;h
ryy;h
sxy;h
2
64
3
75 ¼
1132:10
87:90
25:80
2
64
3
75;
rxx;le
ryy;le
sxy;le
2
64
3
75 ¼
770:45
135:67
15:59
2
64
3
75 for ðn ¼  1ﬃﬃﬃ
3
p ; g ¼  1ﬃﬃﬃ
3
p ; w ¼ 1:0Þ
rxx;h
ryy;h
sxy;h
2
64
3
75 ¼
1132:10
87:90
25:80
2
64
3
75;
rxx;le
ryy;le
sxy;le
2
64
3
75 ¼
770:45
135:67
15:59
2
64
3
75 for ðn ¼  1ﬃﬃﬃ
3
p ; g ¼ 1ﬃﬃﬃ
3
p ; w ¼ 1:0Þ
rxx;h
ryy;h
sxy;h
2
64
3
75 ¼
1025:50
19:40
25:80
2
64
3
75;
rxx;le
ryy;le
sxy;le
2
64
3
75 ¼
729:20
15:61
30:67
2
64
3
75 for ðn ¼ 1ﬃﬃﬃ
3
p ; g ¼  1ﬃﬃﬃ
3
p ; w ¼ 1:0Þ
rxx;h
ryy;h
sxy;h
2
64
3
75 ¼
1025:50
19:40
25:80
2
64
3
75;
rxx;le
ryy;le
sxy;le
2
64
3
75 ¼
729:20
15:61
30:67
2
64
3
75 for ðn ¼ 1ﬃﬃﬃ
3
p ; g ¼ 1ﬃﬃﬃ
3
p ; w ¼ 1:0Þ
When the structure is submitted to forces of high intensity, that produce large
deformations, we have big difference in stresses as we have in displacements. The
difference is precisely the same, cca.40%
Fig. 7. Deformed linear elastic membrane, large displacements
Nonlinear Analysis of Hyperelastic Membrane
669

6
Conclusion
As we can see after making the detailed analysis of linear-elastic and hyperelastic
membrane with the same dimensions and submitted to the same intensity of forces, the
right choice for a stress-strain relationship is very important. First of all, we have to
estimate the displacement of the structure. For small displacements, it is the right
decision to neglect effects of large deformations and rotations. Probably, the mistake
we make is less than 1%. But, if we estimate that the displacements are large than we
have to make the right choice of stress-strain relationship. If we analyze the structure
submitted to such forces that produce large displacements and neglect the inﬂuence of
large rotations and deformations, we can make unacceptable mistakes.
References
1. Ibrahimbegović, A.: Nonlinear Solid Mechanics. Media B.V, Springer Science-Business
(2009)
2. Bhatti Asghar, M.: Advanced Topics in Finite Element Analysis of Structures. Wiley (2006)
3. Jankel, R.: Analysis of Hyperelastic Materials with MECHANICA, Presentation for the 2nd
SAXSIM, Rev 1.0. Technische Universität Chemnitz, 27 Apr (2010)
4. MathWorks: http://www.mathworks.com/support/tech-notes/1100/1109.html
670
R. Šehagić and S. Medić

Quality Factors of Process and Products
in Construction Projects
Žanesa Ljevo1(&), Saša Džumhur2, and Selena Grizić1
1 Faculty of Civil Engineering, University of Sarajevo, Sarajevo, Bosnia and
Herzegovina
zanesahandzar@yahoo.com, selena-g93@hotmail.com
2 Dipl. Ing, IPSA Institute LLC Sarajevo, Sarajevo, Bosnia and Herzegovina
sasa.dzumhur@ipsa-institut.com
Abstract. Construction industry, as a project-oriented industry, represents an
important sector in any economy development and the society in whole. As
such, it is also the driving force of development of a wide range of economic
activities. In this paper will be shown the results of study about quality of
process (project management), quality of product (delivered building) and the
factors affecting them. According to their importance, the factors for phase such
as conception from the perspective of all respondents in BH are involvement,
team work, and quality policy.
Keywords: Project  Process  Quality  Delivered product  Construction
1
Introduction
According to the report Stanich group 29.6% projects cost overruns for 51–100%,
35.5% projects time overruns for 101–200%, the project changes occurring in 39.1%
projects for 75–99% compared to the initial. The average across all companies is 189%
of the original cost estimate, 222% of the original time estimate. For challenged pro-
jects, more than a quarter were completed with only 25–49% of originally-speciﬁed
features and functions [1].
An estimated 20% of the current Gross Domestic Product (GDP) in the world today
appertains to the project management (PM). However, the legal norms in Bosnia and
Herzegovina still do not acknowledge the project management sector.
Although the quality of the process affects the quality of the deliver product, there
are currently insufﬁcient knowledge on the quality of project management processes
that directly affect the quality of the constructed object.
The project management in B&H is not accepted and acknowledged widely within
this industry, primarily due to the current legal regulations. Construction companies in
B&H are not at an enviable stage of project management in both building and civil
engineering sectors.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_58

This paper will show the steps and results of research PM and quality in con-
struction industry of Bosnia and Herzegovina.
The ﬁrst step was to deﬁne the phases and sub-phases of the project in construction
(PM model), then found the key quality factor of the process (project management) and
factors of the product quality (to deliver a product that is built building). Then made
factor analysis for key factor of quality, and factor of the product quality for each phase
of project.
2
Overview of Literature
Every project can be viewed though certain phases. As per PMI [2] phases occurring
within projects, depending on the type of enterprise, are as follows: ﬁrst—conception
and initializing—deﬁning project, identifying needs and capabilities, preparing alter-
natives and project organization, second—planning and design—preparation of the
initial plans and scethe, creation the detailed design and completed plan, third—exe-
cution and realization—implementation and co-ordination of all the activities and
resources toward the project execution, fourth—ﬁnalization and completion—perfor-
mance of ﬁnal activities and tasks in orer to achomplish the project’s purpose and meet
the goals. One of the analysis of the project phase is: planning, speculate, explore,
adapt, close by model agile project management [3].
Based on review of the gathered literature and the interviews conducted, bases for
establishment of characteristic model for project management in BH was developed.
This model is founded on 4 basic phases with all sub-phases and steps involved, but for
next steps we need ﬁrst three phase, which will be presented in Fig. 1. Four basic
phases are conception, deﬁnition, execution and guaranteed due-date for completion.
These phases are characteristic not only for B&H but for any project’s life-circle. The
phase of the project closure could also be added, as it is described as a critical phase of
the project’s life-circle in the studied literature. As all the interview participants had
omitted this project phase, it has not been included in this model formulation. The key
importance of this model is that it presents a quick and effective, yet simple, sample of
project realization from its very beginning to its completion. It also allows each step to
be evaluated and therefore corrected to ensure the most positive impact of each phase
on the ﬁnalized project. Each participant, regardless of the phases they are involved in,
can then recognize the steps they need to take to complete their portion of the job
effectively within the predicted period/due-date and expense budget [4].
672
Ž. Ljevo et al.

In some cases the quality factors are associated or compared with the factors that
affect the safety management of construction projects, but did not ﬁnd any information
on the impact of these factors in different phases of the construction project. After the
review and analysis of literature, it was found that different authors in different
researches do not deﬁne equally factors affecting quality management i.e. quality
factors. In the literature found different number of quality factors, some authors found
in 26, 9 or 6 factors [5–7]. After selection factors with university professors, in the next
step
of
research
eleven
quality
factors—variable
(1-planning
and
control,
2-involvement, team work, 3-expertise, knowledge, 4-customer satisfaction, 5-top
Management
commitment,
6-communication,
7-continuous
improvement,
•IDEA
•RESEARCH
•CONCEPTUAL TECHNICAL 
DOCUMENTATION
•REQUIREMENTS OF DELIVERY AREA
•INVESTMENT STUDY
•FEASIBILITY STUDY
•INSURANCE FINANCE
CONCEPT
•FINAL PROJECT AND TASK
•BASIS FOR DESIGNING
•SELECTION OF DESIGN ENGINEER
•PRODUCTION AND MONITORING OF TECHNICAL 
DOCUMENTATION
•APPROVAL TO THE DOCUMENTATION
•TENDER FOR CONSTURCCTION
•CHOICE OF CONTRACTORS AND CONTRACTING
•PROVISION OF GUARANTEES
•PROJECT PLAN CONSTRUCTION
•DESIGN FOR CONSTRCTION ORGANIZATION
DEFINING AND PLANING
•INTRODUCTION OF CONSTRUCTOR
•PRELIMINSRY WORK
•CONSTRUCTION
•PARTERE
•UNINSTALLING CONSTRUCTION SITE
•TEHNICAL REVIEW
•RECEIVING HEND AND FINAL CALCULATION
•DELIVERI AND INSTAL. OF EQUIPMENT
• USE PERMINT
EXECUTION
Fig. 1. Project phases and sub-phases. Adapted [4]
Quality Factors of Process and Products in Construction Projects
673

8-coordination between project participants, 9-quality policy, 10-availability of
resources, 11-supplier Quality Management) of the process were involved (project
management) [8].
Analyzing the factors of product quality, and included a legal obligation, the factors
described in the literature and supplemented with a model that shows the success of the
project through ﬁve dimensions with 27 indicators, but is observed only indicators
relating to product quality through customer satisfaction in phases [9–11].
3
Research Methodology and Results
After reviewing the literature, analysis and consultation with university professors,
were selected key quality factors, and quality products factors. There was based on a
survey conducted in Bosnia and Herzegovina, followed by the analysis of the results,
and making conclusions. In the questionnaires the phases of concept, deﬁning and
planning, execution, monitoring and control (which runs in parallel with the
performance/execution s phase) as well as phases that are present in the construction
projects were taken into account, but were used only ﬁrst three phases. The impact of
the factors that were taken into account had obvious presence in all the phases, so they
were not analyzed in the closing phase of the project. The data obtained by analyzing
the literature and from questionnaires were used to analyze.
The ﬁrst part of the questionnaire contained questions that were related to the key
factors of quality, whose importance was evaluated according to the Likert scale of
assessment (1 not at all important; … 6—most important). It is used an even number of
grades in order to avoid neutral grades. The questionnaire was sent via e-mail by the
means of the web application docs.google.com in the period April–November (BH:
April-July), and there were 8.52%—80 ﬁlled out questionnaires.
The interviews were made with 91 investors of construction project, civil engineers
or architects (investors—34,1%, designers—35,8%, contractors—30,1%) about factors
of product quality in the different phases of project, used the Likert scale assessment
(1–6).
Before further analysis, tested the data (factors project management process) for
reliability using Cronbach’s coefﬁcient alpha a (the reliability 0,7). Thus, every value
above threshold indicated a reliable measurement. The alphas for each perspective are:
all perspectives a = 0.875, investors a = 0.738, contractors a = 0.879, project man-
agers—designers a = 0.925 for the quality factors. The alphas were computed sepa-
rately for each of the groups. Results show that all of the groups had favourable scores.
In order to examine whether there is a difference between the perspectives of
respondents the nonparametric Kruskal-Wallis test was used, and if signiﬁcant differ-
ences in the level of signiﬁcance (0.05) in perspectives between the samples were
encountered then the Mann-Whitney nonparametric test was used, which shows which
pairs of the population differ and which is identical to the parametrical t-test, which
takes into account two samples. Afterwards, the ﬁndings were checked in the program
for statistics SPSS 16.0. and conﬁrmed difference between perspective for quality
factor—top management v2 = 10.352 (Table 1). The Mann-Whitney discovered that
674
Ž. Ljevo et al.

the investors signiﬁcantly differed from the contractors and project managers, values of
0.002 and 0.017 respectively (Table 2).
All participants (according to the management perspectives) believe that the key
factors are most important for the phase of performance/execution and according to
respondents from BH are: coordination between project participants, availability of
resources, and communication (Fig. 2). Respondents in Bosnia and Herzegovina for
the concept phase consider of most important 2, 9 and 4, the phase of deﬁning and
planning: 4, 1 and 5, performance/execution: 8, 10, 6, for monitoring and control: 9,
11, 7.
Used factor analysis to describe variability among observed, correlated variables
(key quality factors) in terms of a potentially lower number of unobserved variables
called new factors.
First, correlation is made between anti-image matrix where there is a partial cor-
relation coefﬁcients, and the main diagonal are MSA (a measure of the adequacy of the
sample—speciﬁc measures of sampling adequacy).
Table 2. Results Mann-Whitney test for quality factor—top management
Number Mean rank
Investors
24
32.31
31.52
Contractors
27
20.39
26.41
Project managers 28
29.54
22.20
Inv./Cont.
Inv./PM. Cont./PM
Sig.—p < 0.05
0.002
0.017
0.493
Fig. 2. Importance of key factors in project phases from the perspective of all respondents from
BH
Table 1. Results Kruskal-Wallis test for quality factor—top management
Number Mean rank v2
Investors
24
51.33
10.352
Contractors
27
32.80
10.352
Project managers 28
37.23
10.352
Quality Factors of Process and Products in Construction Projects
675

Table 3. Anti-image correlation matrix
1
2
3
4
5
6
7
8
9
10
11
Anti-image Covariance
1
0.491
−0.098
−0.089
0.109
−0.082
−0.109
−0.023
−0.037
−0.025
−0.033
0.130
2 −0.098
0.607
−0.105
0.073
−0.151
−0.036
−0.040
0.003
−0.005
0.036
−0.070
3 −0.089
−0.105
0.410
−0.003
0.046
0.086
−0.037
−0.168
0.034
−0.047
0.006
4
0.109
0.073
−0.003
0.611
−0.263
−0.074
−0.119
0.009
−0.038
−0.043
0.010
5 −0.082
−0.151
0.046
−0.263
0.564
−0.060
0.085
−0.021
−0.003
−0.070
0.038
6 −0.109
−0.036
0.086
−0.074
−0.060
0.389
−0.069
−0.117
−0.007
−0.044
−0.031
7 −0.023
−0.040
−0.037
−0.119
0.085
−0.069
0.455
−0.046
−0.149
0.007
−0.096
8 −0.037
0.003
−0.168
0.009
−0.021
−0.117
−0.046
0.251
−0.088
−0.029
−0.046
9 −0.025
−0.005
0.034
−0.038
−0.003
−0.007
−0.149
−0.088
0.559
−0.014
−0.042
10 −0.033
0.036
−0.047
−0.043
−0.070
−0.044
0.007
−0.029
−0.014
0.752
−0.134
11
0.130
−0.070
0.006
0.010
0.038
−0.031
−0.096
−0.046
−0.042
−0.134
0.762
Anti-image Correlation
1
0.872a −0.179
−0.198
0.199
−0.156
−0.248
−0.048
−0.106
−0.048
−0.054
0.212
2 −0.179
0.889a −0.212
0.119
−0.259
−0.074
−0.076
0.009
−0.008
0.053
−0.104
3 −0.198
−0.212
0.812a −0.006
0.096
0.214
−0.086
−0.524
0.072
−0.085
0.010
4
0.199
0.119
−0.006
0.731a −0.448
−0.151
−0.226
0.024
−0.064
−0.064
−0.015
5 −0.156
−0.259
–0.96
−0.448
–0.765a −0.128
–0.167
−0.056
−0.005
−0.108
–0.058
6 −0.248
−0.074
0.214
−0.151
−0.128
0.878a −0.164
−0.374
−0.014
−0.081
−0.058
7 −0.048
−0.076
−0.086
−0.226
0.167
−0.164
0.891a −0.135
−0.295
0.012
−0.164
8 −0.106
0.009
−0.524
0.024
−0.056
−0.374
−0.135
0.847a −0.235
−0.066
−0.106
9 −0.048
−0.008
0.072
−0.064
−0.005
−0.014
−0.295
−0.235
0.915a −0.021
−0.065
10 −0.054
0.053
−0.085
−0.064
−0.108
−0.081
0.012
−0.066
−0.021
0.936a −0.177
11
0.212
−0.104
0.010
0.015
0.058
−0.058
−0.164
−0.106
−0.065
−0.177
0.837a
aMeasures of Sampling Adequacy-MSA
676
Ž. Ljevo et al.

Table 3 shows that the MSA is from 0.731 to make 0.936, what is a strong and very
strong correlation, however, is a collection suitable for the applicability of factor
analysis.
Than was made the Kaiser-Meyer-Olkin Measure of Sampling Adequacy indicates
the proportion of variance in the variables that might be caused by underlying factors
(Table 4). High values (close to 1.0) indicate that a factor analysis may be useful and if
the value is less than 0.50, the results will probably not be very useful. Bartlett’s test of
sphericity indicates that the variables are unrelated and therefore unsuitable for struc-
ture detection. Values of signiﬁcance level less than 0.05 indicate that a factor analysis
may be useful.
80.1% of the variance related to a variable 8 (coordination between project par-
ticipants), 78.2% of the variance related to a variable 5 (top Management commitment)
is a common variance with other variables.
Factor model with three factors described 65.7% of the basic set of variables
(Table 5). Having identiﬁed a number of factors, determined by the matrix of the factor
structure of secreted factors. The matrix factor structure contains the factor loadings,
which depict the correlation coefﬁcients between the secreted factors and variables.
Factor loadings indicate the importance of each variable for each factor.
Factor 1 has high positive loading for variable 1 (planning and control), where the
load is 0.841, 3 (expertise, knowledge …), where the load is 0.754, variable 2/load
0.716, variable 6/load 0.548, and variable 8/load 0.703. Name factor 1 is the planning
and participants. Factors 2 are variables: variable 7/load 0.668, variable 9/load 0.606,
variable 10/load 0.440, variable 11/load 0.814. And factor 3 are: variable 4/load 0.833,
variable 5/load 0.819. Some factors have small factor loads, but they are all taken into
account because they are important for further research.
For the design phase of 62.6% of respondents said that customer satisfaction in the
end phase (project task—scope, concept, structure is aligned with the business plan) the
key factor of product quality, secured ﬁnancing 59.3% of the respondents considered
the factor of product quality, because if you are not insured planned ﬁnances there is a
change of vision, ideas, and at the end original goals. In the phase of deﬁning and
planning 57.1% is considered a key factor in the quality of customer satisfaction in the
end phase (development of new processes and technologies), and 42.9% that it is a
complete technical documentation (main project, a study of safety at work … and other
legally required documentation). For the phase of performing/execution 52.7% is
considered a key factor in the quality of customer satisfaction in the end phase (project
proﬁtability, efﬁciency/harmonization with the scope and schedule/, the contribution
Table 4. Kaiser-Meyer-Olkin and Bartlett’s test
Kaiser-Meyer-Olkin measure of sampling
adequacy
0.855
Bartlett’s test of sphericity Approx. Chi-Square 359.058
df
55
Sig.
0.000
Quality Factors of Process and Products in Construction Projects
677

Table 5. Total variance explained
Component
Initial eigenvalues
Extraction sums of squared loadings
Rotation sums of squared loadings
Total
% of
variance
Cumulative
(%)
Total
% of
variance
Cumulative
(%)
Total
% of
variance
Cumulative
(%)
1
4.925
44.771
44.771
4.925
44.771
44.771
3.067
27.882
27.882
2
1.201
10.922
55.692
1.201
10.922
55.692
2.317
21.067
48.949
3
1.103
10.032
65.724
1.103
10.032
65.724
1.845
16.775
65.724
4
1.845
16.775
65.724
5
0.793
7.207
72.931
6
0.697
6.339
79.270
7
0.536
4.869
84.139
8
0.486
4.421
88.560
9
0.425
3.859
92.420
10
0.346
3.141
95.561
11
0.313
2.849
98.410
678
Ž. Ljevo et al.

for future projects), and 42.6% that it is carried out technical inspection and won the
occupancy permit and 29.7% that was delivered without defects and deﬁciencies.
After that the ranking factors of the product (Relative Importance Index—RII)
supplemented with legally acts for each phase. Where is: Rw—the sum of grades given
to each factor; A—max. assessment grade for each factor; N—total number of
respondents [12].
RII ¼
P w
A  N
ð1Þ
RII is in the interval 0  1—when used in ordinal grading scale in research many
researchers advocate this method of ranking; as RII is higher the quality factor is
considered to be more important.
In the design phase (through certain measures) factors are: customer satisfaction in
the end phase (RII = 0.918), secured the funding of (RII = 0.894), the handover
without faults and defects (RII = 0.819), drafted an operational plan (RII = 0.780). In
the phase deﬁning and planning measured factors through certain measures: customer
satisfaction at the end of phase (RII = 0.897), obtained urban and building permit
(RII = 0.855), complete technical documentation (RII = 0.855), designed and planned
budget (RII = 0.841).
4
Discussion and Conclusion
Legal regulation in B&H does not prescribe anything related to project management in
the construction industry, so this is left to the companies to apply (methods, techniques
and tools) and see if they may ﬁnd it useful for the realization of the projects in which
they participate. The importance of each quality factor in the phases of the project for
the participants in the project has not been extremely different among the respondents
in Bosnia and Herzegovina. A factor analysis of the variables grouped into three
groups, factor model with three factors described 65.7% of the basic set of variables.
Factor 1 called planning has high positive loading for variable planning and control,
expertise, knowledge …, involvement, team work, communication, and coordination
between project participants. Factor 2 has high positive loading for variable continuous
improvement, quality policy, and availability of resources. And factor 3 are: customer
satisfaction and top management commitment.
The next steps for research are to deﬁne measures for each variable (project
management process and products of the each phase), and the same analysis on the
direct observation of the projects, which will be the input for modeling results.
Using the model of research conducted by Shenhar [11], for the design phase of the
factor “customer satisfaction at the end of phase”, taken into account that the terms of
reference (scope, concept, structure) in conformity with bussnis plan (procnjena
resources and expected revenues, in order to know the proﬁtability or predicted a
proﬁt). In the phase of deﬁning factor “customer satisfaction at the end of phase’ means
Quality Factors of Process and Products in Construction Projects
679

the development of new processes and technologies”. Customer satisfaction at the end
of phase’ performance/execution involves the viability of the project, the efﬁciency
(compliance with the scope and schedule), the contribution for future projects.
References
1. Standish Group 2015 Chaos Report. http://www.infoq.com/articles/standish-chaos-2015,
Accessed 20 Dec 2016
2. Project Management Institute: A guide to the project management body of knowledge. In:
PMBOK® Guide—Fifth Edition. Pennsylvania, USA (2013)
3. Moran, A.: Managing Agile: Strategy, Implementation, Organization and People. Springer
(2015)
4. Ljevo, Ž., Vukomanović, M.: Characteristic project management model found in construc-
tion companies of Bosnia and Herzegovina. Technical Gazette, pp. 689–696 (2013)
5. Ogwueleka, A.C.: Review of safety and quality issues in the construction industry.
J. Construct. Eng. Project Manag. Korea, 42–48 (2013)
6. Kanagi, K.: Critical factors of quality management used in research questionnaires: a review
of literature. Sunway Acad. J. 5, 19–30 (2008)
7. Lusihine, T.W., Hoonakker, P.L.T.: Integrated quality and safety management systems in
construction. In: Proceedings of the 12th Annual Construction Safety Conference. Chicago,
IL, USA (2002)
8. Ljevo, Ž., Vukomanović, M., Rustempašić, N.: Assessing the inﬂuence of project
management on quality in the construction industries of Bosnia and Herzegovina and
Croatia. In: 12th International Conference Organization, Technology and Management in
Construction Conference Proceedings (2015)
9. Taylor, B.W., Russell, R.S.: Operations Management: Quality and Competitiveness in a
Global Environment, 5th edn. Hoboken, NJ, Wiley (2006)
10. Zakon o gradnji (građenju), (“Službene novine FBiH’’, broj 55/02)
11. Shenhar, A.J., Dvir, D.: Reinventing Project Management: The Diamond Approach to
Successful Growth and Innovation. Harvard Business Scholl Press (2007)
12. Larsen, J.K., Shen, G.Q., Lindhard, S.M., Brunoe, T.D.: factors affecting schedule delay,
cost overrun, and quality level in public construction projects. J. Manag. Eng. (2016)
680
Ž. Ljevo et al.

Static and Dynamic Indicators for Composite
Bridges
Naida Ademovic(&)
Faculty of Civil Engineering, Department of Structures and Materials,
University of Sarajevo, Sarajevo, Bosnia and Herzegovina
naidadem@yahoo.com
Abstract. Standard non destructive static and dynamic testing of bridges after
their reconstruction serves as indicators regarding the capacity of the structure
and their durability. Several steel concrete composite girder bridges were
investigated and compared. Measurement from the static tests were used to
make some correlation between the stiffness of the structure and dynamic
properties. From the investigations it was clear that different truck weight has a
direct inﬂuence of the dynamic characteristics, requiring a standardization of
testing vehicle. Temperature inﬂuence was more than indicative.
Keywords: Composite steel concrete  Girder bridges  Dynamic properties
Stiffness  Frequency  Deﬂection  Temperature effects on natural frequencies
1
Introduction
Nondestructive load testing is an effective approach to measure the structural response
of a bridge under various loading conditions and to determine its structural integrity.
This load-test program integrates an optical surveying system, a sensor dilatation
measurement on steel and concrete parts of the structure and deﬂection analysis by the
use of Inductive Displacement Transducer. The bridge is exposed to static and dynamic
loading in order to evaluate the behavior of the bridge. The actual response of a bridge
to loads is usually better than what the theory dictates [1]. Factors that contribute to the
load capacity difference include unintended composite action, load distribution effects,
participation of parapets, railings, curbs, and utilities, material property differences,
unintended continuity, participation of secondary members, effects of skew, portion of
load carried by deck, and unintended arching action due to frozen bearings [1, 3].
Load testing in Bosnia and Herzegovina is deﬁned by [2] and represents as an
“effective means of evaluating the structural response of a bridge.” The purpose of
conducting load testing on existing bridges is to evaluate their structural response
without causing damages. In this respect, as load testing is usually conducted in a
non-destructive manner resulting that it may be deﬁned as such [3].
Every testing procedure is in one segment the same and in the other speciﬁc for
each bridge. For each bridge a clear program needs to be set with clear testing
objectives and load conﬁgurations, selection and placement of instrumentation, anal-
ysis technique, evaluation and comparison of test results and analytical results [3].
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_59

2
Static and Dynamic Tests
Static load testing of bridges is conducted by the utilization of the trucks of certain
weigh depending on the span and width of the bridge. In testing a bridge various
structural elements need to be examined. Strain or deﬂection-transducer gages are
placed at critical locations in order to measure the deﬂection of the bridge upon which
the strength of these elements can be determined. Survey instruments are used to
measure the deﬂection on the asphalt layer, while deﬂection of the composite steel
concrete girders is done by Inductive Displacement Transducer positioned underneath
the girder. Stresses in the steel and concrete elements of the superstructure are obtained
from the strain gages measurements glued on the structure elements.
For static testing the bridge was incrementally loaded up to the design live load in
order to induce maximum effects. Previously analytical model was done in order to
compare the experimental results with the calculations at each conducted load step. At
the ﬁnal stage the measured data (deformations, strains-calculated stresses) was com-
pared with the obtained analytical results and adequate conclusions and recommen-
dations were given.
Dynamic load testing is performed by exciting the vibration of the bridge and by
measuring its properties after the excitation has ceased. There are several ways to excite
the bridge: eccentric rotating masses, impact of a heavy weight and passage of a loaded
truck. The most realistic is the movement of the vehicles. By the dynamic loading tests
controlling parameters of the dynamic behavior of the bridges are determined (the
fundamental vibration frequency, the dynamic ampliﬁcation factor and the logarithmic
decrement). These quantities are relatively easy to obtain experimentally, and can give
valuable information for the exploitation and maintenance of the bridge (Burdet and
Corthay 1995).
For dynamic loading one truck passed over a plank of 5 cm thick with different
speeds. This plank is used to represent the effect of deterioration of the pavement and in
this way it causes the excitation of the bridge. By varying the speed of the truck on the
bridge, the full range of trafﬁc speeds was investigated. Additionally, different trucks
were used, which indicated the change in the dynamic characteristics of the bridge.
This clearly indicated the inﬂuence of the truck structure interaction, connected to the
different weight of the trucks. This is one of the elements that has to be taken into
account during the calculations as well as the need for standardized trucks for dynamic
testing is required in order to eliminate this inﬂuence. The inﬂuence of the temperature
on the dynamic characteristics was investigated as well indicating a clear dependency
of frequency upon temperature.
3
Conducted Experimental Tests
3.1
Comparison of Static Values
Static and dynamic analysis was conducted on ﬁve composite steel concrete bridges.
Detailed comparison of the analytical and experimental results is given in the
682
N. Ademovic

individual reports of each bridge [4–6]. The bridges composed of either one or several
simple beams from 11 to 36.75 m.
Modeling of all bridges was done with the application of the program Tower [7].
All the bridges were modeled as 3D structures, static and dynamic calculations were
done, giving internal forces, stresses and deﬂections for symmetric and nonsymmetrical
loading phases and comparison was done with the measured values on the site [4–6].
Characteristic cross section of only two the bridges is illustrated in (Fig. 1) for
Bridge over Krivaja and in (Fig. 2) for the bridge over Sapna river. Details regarding
other bridges can be found in [4–6].
Figure 3 shows the static testing of the bridge.
As the analyzed bridges have different spans, lengths and height of the super-
structure and in that respect the maximum loading to which the structures were exposed
was different. In order to be able to make adequate comparisons correction of the
deﬂection was done in respect to the geometrical dimensions of the bridge and loading.
The deﬂection was corrected utilizing the formula:
Fig. 1. Typical cross section of the bridge and elevation view of the bridge in Krivaja [4]
Fig. 2. Typical cross section of the bridge and elevation view of the bridge in Zvornik [6]
Static and Dynamic Indicators for Composite Bridges
683

vk¼ Fk
Fi
L3
kBi
L3
i Bk
ð1Þ
where:
v
deﬂection
F
loading
L
span of the bridge
B
width of the bridge
i
index for real value
k
index for referent value
From the conducted comparison it is evident that for all the bridges the measured
values are lower than the calculated ones indicating that the constructed bridge has a
higher stiffness in respect to the calculated one, raising the safety factor to a certain
amount. It is interesting as well to note that the stressed in concrete have not reached
not even 50% of the calculated value. This is a clear indication that concrete is most
probably made of a higher quality than what is taken in the calculation as per the
design. This could be checked by taking out the concrete cylinders from the bridge and
conducting compression tests, or better with the application of some Non-destructive
tests like Schmidt rebound hammer test and the ultrasonic pulse velocity test. However,
in conducting such analysis special care should be taken into account [8–10]. Com-
bined NDT methods (also known as SonReb method) yield better estimations than
single NDT methods. The results also show that the SVMs model is more accurate than
Fig. 3. Static testing of the bridge in Zvornik [6]
684
N. Ademovic

Table 1. Comparison of calculated and measured static values of different bridges
Bridge
L/H
Stresses in steel
(MPa)
Difference (%)
Stresses in
concrete (MPa)
Difference (%)
Deﬂection
v (mm)
Difference (%)
Cal.
Meas.
Cal.
Meas.
Cal.
Meas.
Krivaja
23.25
36.27
30.24
16.6
−1.29
−0.20
84.5
13.98
12.81
8.4
Zvornik
20.00
49.60
41.70
15.9
−0.98
−0.35
64.3
14.55
13.69
5.9
Donja Bioča
23.00
35.41
26.56
25.0
−0.88
−0.36
59.0
7.52
7.15
4.9
Static and Dynamic Indicators for Composite Bridges
685

the statistical regression model [10, 11]. Stress in the steel is lower in the average of
around 19% indicating as well good mechanical characteristics of the steel. In general it
can be stated that the constructed bridge has a higher stiffness and better mechanical
characteristics compared to the designed values (Table 1).
3.2
Comparison of Dynamic Values
Theoretical dynamic analysis was conducted which consists of bridge modeling and
determination of the theoretical dynamic parameters which are than compared with the
measured values on the site [12]. The main dynamic characteristics being: frequency,
the dynamic ampliﬁcation factor and the logarithmic decrement. As well the modal
parameters are often sensitive to changing environmental conditions such as temper-
ature, humidity, or excitation amplitude. Environmental conditions can have as large an
effect on the modal parameters as signiﬁcant structural damage, so these effects should
be accounted for before applying damage identiﬁcation methods. This is something that
should be monitored in the structural health monitoring systems.
Examination of the natural frequency and temperature data from the continuous
monitoring system revealed that natural frequency and temperature were strongly
correlated and that the relationship was nonlinear [13]. In order to grasp this effect, the
dynamic testing was done during different weather conditions, day and night, and at
temperature of 5 and 22°C.
Figure 4 shows the track passing over the 5 cm plank at Zvornik bridge and the
position of the accelerometers.
Figure 5 shows the vertical excitation of the bridge when the truck passed having
the speed of 20 km/h is shown in Fig. 5a. Finally the calculated power spectra using
the FFT is shown in Fig. 5b for different temperatures.
Discrete Fourier Transform (DFT) and Fast Fourier Transform (FFT)
In practice, the response function {x(t)} is recorded for a ﬁnite time duration at {N}
discrete points that are evenly spaced by the sampling scheme and digitized by the
analogue-to-digital conversion process. Assuming the record is periodic about the
length of the sample, the Fourier Transform can be estimated as a ﬁnite series with
discrete points at (t = tk) and (k = 1, N) [14].
x(tkÞ  ðxkÞ¼ 1
2 a0 þ
XN=2 or N1
ð
Þ=2
N¼1
ðancos 2pnk
N
þ bnsin 2pnk
N Þ
ð2Þ
and the coefﬁcients are deﬁned by,
an ¼ 1
N
XN
k¼1 xkcos 2pnk
N
ð3Þ
bn ¼ 1
N
XN
k¼1 xksin 2pnk
N
ð4Þ
a0 ¼ 2
N
XN
k¼1 xk
ð5Þ
686
N. Ademovic

The Fast Fourier Transform (FFT) is and optimization of the DFT where the
method requires N to be an integral power of two (2) thereby reducing the execution
time to compute the DFT of the response time history. The modal parameters of the
system are then estimated from Fourier spectra generated from these relationships.
Here only the result form the Zvornik bridge will be given as the same trend is
observed in the other two bridges as well. It was clearly see that the identiﬁed natural
frequencies increase as temperatures decrease, which is in consistency with the
experimental results done by other researchers [14–19]. The difference in the ﬁrst
frequency is from 5, 5–6, 2%, and the second frequency in the range form 7, 2–9, 4%
(Fig. 5). No such temperature dependence was observed for the identiﬁed damping
ratios or mode shapes.
The concept of a dynamic ampliﬁcation factor (DAF) is used to describe the ratio
between the maximum load effect when a bridge is loaded dynamically, and the
maximum load effect when the same load is applied statically to the bridge. As stated in
[20] generalized DAF value was applied to the worst static load case for a given bridge.
This is a conservative approach since DAF depends on the length of the bridge and
ignores many signiﬁcant bridge and truck dynamic characteristics. However, as this is
(a)
(b)
-2,00
-1,50
-1,00
-0,50
0,00
0,50
1,00
1,50
2,00
0,00
5,00
10,00
15,00
20,00
25,00
30,00
35,00
40,00
45,00
Acceleration [m/sec2]
Time [s]
Accelelogram-Verical excitation -speed20km/h
Series1
ACC106
2,19
3,61
2,06
3,35
0
0,01
0,02
0,03
0,04
0,05
0,06
0,07
0,08
0,09
0,1
0
5
10
15
20
25
Power spectra [m/sec2^2]
Frequency [Hz]
Power Spectra
Temp 5C
Temp 22C
Fig. 5. a Accelelogram b frequency in dependence of temperature [6]
Fig. 4. a Dynamic testing of the bridge in Zvornik b position of the accelerometers [6]
Static and Dynamic Indicators for Composite Bridges
687

currently enforced rule calculations were done obeying these rules. More complex
calculations would be an advantage and this is planned to be done in the future [3].
Table 2 shows the valued of the dynamic characteristics of different bridges. It is
interesting to note that the difference between the calculated and measured second
frequency is higher in respect to the ﬁrst mode. However, the calculated and experi-
mental results are in a rather good agreement, indicating that there are no damages on
the structures. And the consistency between the stiffness and frequency are quite
obvious.
The ﬁrst mode of all three bridges was of a pure ﬂexural shape. During the testing
of the bridge in different temperatures the modes did not change. The calculated modes
of the three bridges are presented in Fig. 6.
Table 2. Comparison of calculated and measured dynamic values of different bridges
Bridge
Frequency
1 [Hz]
Difference
[%]
Frequency
2 [Hz]
Difference
[%]
Dynamic
ampliﬁcation
factor (DAF)
Difference
[%]
Cal.
Meas.
Cal.
Meas.
Cal.
Meas.
Krivaja
3.83 3.54
7.6
4.51 4.13
8.4
1.11
1.095
1.4
Zvornik
2.29 2.34
2.1
3.32 3.81
12.9
1.176 1.16
1.4
Donja
Bioča
5.72 5.57
2.6
6.78 6.05
10.8
1.24
1.20
3.2
Fig. 6. a Krivaja bridge b Zvornik bridge c Donja Bioča bridge [4–6]
688
N. Ademovic

4
Conclusion
This paper presents static and dynamic load testing of composite steel concrete girder
bridges in Bosnia and Herzegovina. It is clear from the analysis of all bridges that the
stiffness and material quality is of a higher degree in respect to the modeled structure
where the data from the design of the bridge was taken into account. Stresses in the
steel reached maximum 75% from the calculated values, while stresses in the concrete
were rather low. Dynamic characteristics of the structure (frequency and DAF) showed
excelled correlation between the experimental and numerical results. This all indicates
that the bridges at the moment do not have any defects. Examination of the natural
frequency and temperature data that natural frequency and temperature were correlated.
No such temperature dependence was observed for the identiﬁed damping ratios or
mode shapes.
References
1. NCHRP-234: Manual for Bridge Rating Through Load-testing, National Cooperative
Highway Research Program, Research Results Digest, Number 234. Transportation
Research Board, Washington, DC, USA (1998)
2. BAS U.M1.046: Bridge Load Testing Guidelines for Design, Construction and Audit on
Roads of the Road Directive of the Federation of B&H and Roads of Republica Srpska
(2005)
3. Ademovic, N.: Assessment of bridge performance by load testing after reconstruction. In:
COST TU 1406, Quality Speciﬁcations for Roadway Bridges, Standardization at a European
level, pp 20–21. Delft, The Netherlands (Oct 2016)
4. Report on the bridge load testing over the Krivaja River on the Regional road R-467
Zavidovići-Olovo Situation at km 51 + 700. Institute for Materials and Structures, Faculty of
Civil Engineering, University of Sarajevo, no. 497-2/14 (Jan 2015) (in Bosnian language)
5. Report on the bridge load testing on the road Ilijaš-Donja Bioča. Institute for Materials and
Structures, Faculty of Civil Engineering, University of Sarajevo, no. 383-4/13 (Nov 2013)
(in Bosnian language)
6. Report on the bridge load testing over river Sapna in Zvornik. Institute for Materials and
Structures, Faculty of Civil Engineering, University of Sarajevo, n0. 04-1-1435-294-4/16
(Nov 2016) (in Bosnian language)
7. Tower 7, program for static and dynamic analysis of structure, Radimpex * http://www.
radimpex.rs (2015)
8. Helal, J., Soﬁ, M., Mendis, P.: Non-destructive testing of concrete: a review of methods.
Spec. Issue Electr. J. Struct. Eng. 14(1), 97–105 (2015)
9. Hajjeh, H.R.: Correlation between destructive and non-destructive strengths of concrete
cubes using regression analysis. Contemp. Eng. Sci. 5(10), 493–509 (2012)
10. Shih, Y.-F., Wang, Y.-R., Lin, K.-L., Chen, C.-W.: Improving non-destructive concrete
strength tests using support vector machines. Materials 8, 7169–7178 (2015)
11. Hannachi, S., Guetteche, M.N.: Application of the combined method for evaluating the
compressive strength of concrete on site. Open J. Civ. Eng. 2, 16–21 (2012)
12. Paultre, P., Proulx, J., Talbot, M.: Dynamic testing procedures for highway bridges using
trafﬁc loads. J. Struct. Eng. 121(2), 362–376 (1995)
Static and Dynamic Indicators for Composite Bridges
689

13. Moser, P., Moaveni, B.: Environmental effects on the identiﬁed natural frequencies of the
dowling hall footbridge pp. 1–42. https://pdfs.semanticscholar.org/196b/2d3792ad9d1fbe
7c5c2462481c916ec93ce5.pdf
14. Ewins, D.J.: Modal Testing: theory, practice and application, 2nd edn. Research Studies
Press Ltd (2000)
15. Cross, E., Worden, K., Koo, K.Y., Brownjohn, M.W.: Modelling environmental effects on
the dynamic characteristics of the Tamar suspension bridge. In: Proceedings of the
IMAC-XXVIII, pp. 21–33. Jacksonville, Florida, USA, 1–4 February 2010
16. Balmes, E., Corus, M., Siegert, D.: Modeling thermal effects on bridge dynamic responses,
pp 1–8. http://www.sdtools.com/pdf/IMAC06_thermal.pdf
17. Farrar, C., Doebling, S., Cornwell, P., Straser, E.: Variability of modal parameters measured
on the Alamosa Canyon Bridge. In: Proceedings of SPIE, The International Society for
Optical Engineering, vol. 3089, pp. 257–263 (1997)
18. Alampalli, S.: Inﬂuence of in service environment on modal parameters. In: Proceedings of
the 16th International Modal Analysis Conference, pp. 111–116. Santa Barbara, California
(1998)
19. Hu, W.-H., Mountinho, C., Magalhaes, F., Caetano, E., Cunha, A.: Analysis and extraction
of temperature effect on natural frequencies of a footbridge based on continuous dynamic
monitoring. In: Proceedings of the 3rd International Operational Modal Analysis Confer-
ence, pp. 55–62. Portonovo, Italy (2009)
20. Rule book on technical normative for determination of the loads on bridges. Ofﬁcial Gazette,
Belgrade (1991) (in Serbian language)
690
N. Ademovic

The Criteria for the Control of Condition
of Railway Lines in FB&H
Mirna Hebib-Albinovic1(&) and Sanjin Albinovic2
1 Public Enterprise Railways of the Federation of Bosnia and Herzegovina,
Sarajevo, Bosnia and Herzegovina
hebib16@hotmail.com
2 Faculty of Civil Engineering, Department of Roads, University of Sarajevo,
Sarajevo, Bosnia and Herzegovina
sanjin.albinovic@gmail.com
Abstract. Construction of the railway infrastructure is a very complex activity
and requires a large ﬁnancial investment. For this reason, always tends to build
systems that have a long lifetime. A precondition for this, other than a quality of
construction work and installation of high-quality material is continuous and
proper maintenance. If we disregard the obvious and unexpected irregularities,
which are observed by visiting the railway and requiring emergency and urgent
actions to maintain the railway and treatment of the same, measuring driving is
the most important instrument for identifying irregularities of track geometry
and track condition, and railway capacity in terms of planning activities to
maintain the railway and the planning works. According to the concept of
measuring driving means the recording geometrical parameters the railway,
customized railway vehicles for this purpose, which we call the measuring
vehicle. Measuring vehicles carry out recording while driving on the track at
reduced speed and under speciﬁc axial load. The geometry of tracks is one of the
basic parameters, based on which it determines the speed of trains and rolling
stock, and therefore the level of security in railway transport and line capacity.
Keywords: Measuring driving  Track condition  Maintenance  Track
geometry
1
Introduction
There are different systems for measuring the track geometry and the processing data
obtained by measurement.
The measuring vehicles until a decade ago, the track geometry mostly measuring
based of three point (the principle of chord) system of the measurement.
Measuring track geometry on railway lines in the FB&H, in this way are period-
ically carried out until 2008, and measurements were carried out using a measuring
vehicle type “FMK-004” and then by the measuring vehicle “FMK-007” that works on
the principle of non-contact measurement (Fig. 1).
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_60

Considering the various approaches to the measurement, in order to data previously
carried out measurements were comparable with the results of the new measurements,
it was necessary to do the analysis and determine the differences in the results.
For this purpose, measurements were carried out a simultaneously, with measuring
vehicles “FMK-004” and “FMK-007” on the same section, while taking like a basis the
data of measurements which carried out with the measuring vehicle “FMK-004” on the
same section in the earlier (basic) time.
Is deﬁned by statistical set whose basis constitute the sections where the completed
a common measurements (where are taken into consider only those qualifying sections
of 500 m for which had existed “FMK-004” and “FMK-007” qualifying “SAD”1
value), and in which a made the comparative analysis.
On the basis of the analysis are determined by the correction value “K”2 that should
apply to “SAD” values obtained from data “FMK-007”.
Corrected values can be compared with previous measurement data supplied by the
geometric measurement system “FMK-004”.
In Fig. 2 shows a graphic distribution of the value “SAD” (blue line) and corrected
“SAD” values (red line) of the measurement system “FMK-007” and the “SAD” values
of the measuring system “FMK-004” (green line).
Fig. 1. Track geometry and clearance gauge proﬁle measuring vehicles FMK-004 & FMK-007 [4]
1 Qualiﬁcation of the general track condition by applying several principles in the concrete case
applied the territorial principle and qualifying length of 500 m, while the general condition of track
geometry provides for a qualifying number of “SAD” which takes into account six geometric
parameters
(stability
right,
stability
left,
direction
right,
direction
left,
superelevation_A,
superelevation_B).
2 K = 1.08.
692
M. Hebib-Albinovic and S. Albinovic

From the diagram (Fig. 2) can see how the distribution corrected “SAD” value
measurement system “FMK-007” is very well adjusted to the distribution of “SAD”
value measurement system “FMK-004”.
In the last few years there has been further progress in the development of mea-
suring vehicles where with the non-contact measurement and installed video surveil-
lance systems and laser scanning line and tends to increase the recording speed.
In Fig. 3 shows the new type of vehicle measuring “FMK—8” and its measurement
control room.
Also, there are other types of measuring vehicles from different manufacturers
(Fig. 4) are also working on similar principles but in any case during the performance
measurement driving with new types of vehicles is necessary to establish a correlation
with the results of previous measurements in order to able to compare the results.
Using modern types of measuring vehicle for carrying out measuring driving, the
managers of infrastructure gets a lot of information about of the track geometry, rails
damages and possible dangerous places on the railway line which enables a better
planning works of maintenance tracks.
Fig. 2. Comparative analysis and conversion results of simultaneously measurement [5]
Fig. 3. Track geometry measuring vehicle (FMK-008) and their measurement control room [4]
The Criteria for the Control of Condition of Railway Lines
693

2
Speciﬁc Criteria for Measuring Driving in the Area
of Railway Management ŽFB&H
The data obtained by the measuring driving, i.e. the recording of tracks by the mea-
suring vehicles processed on the basis of criteria that are described by Railways of
FB&H in “Uputstvo 339” (Guidelines 339) [1]. Results of are obtained in the form of a
diagram (graph) and numerical reports.
Diagrams provide a graphical representation of certain elements the track geometry
in certain proportions, and on them are registered and other necessary data for con-
nection to diagrams and the situation on the terrain (chainage of tracks, objects, etc.)
(Fig. 5).
Fig. 4. Plasser and Theurer track geometry measuring vehicles and EM 160 vehicle for
non-contacting measurement under load in real time at up to 160 km/h [6]
Fig. 5 Detail of track geometry graph [4]
694
M. Hebib-Albinovic and S. Albinovic

Numerical reports include information on the track condition which is based on
computer analysis (individual and ﬁnal reports).
Registered irregularities that directly endanger trafﬁc safety must be removed
immediately, or as soon as possible.
For registration of individual parameters in measuring driving is adopted the fol-
lowing [1]:
• objects on the line are registered in special graphic symbols,
• the driving speed of 1 mm corresponds to a speed of 4 km/h (measured from the
zero line in the diagram),
• acceleration (horizontally) is recording in 1:1,
• arrows of left and right rails showing the size of the arrows in the scale of 1:4 for
tendon (the measuring base) of 10 m,
• curvature radius can be obtained in the form R = 3125/a,
• track superelevation is deﬁned as the ratio of height of rails in the 1:5, and is
measured from the zero line,
• cant of track is register for the base of the measuring 3,50 m in 1:1,
• track gauge was recorded as the deviation of the normal width of 1.435 mm (ex-
tension or narrowing of the track),
• track stability for measuring base of 5 m, is registered in the scale of 1:1.
Table 1. Allowed values of deviations for some geometrics parameters for the admission of new
railway lines and general overhaul [1]
Classiﬁcation of lines parameter
C
I
II III IV
1 Track gauge Gauge widening
3 3
5
8
2
Gauge tightening 3 3
3
3
3 Superelevation of track (Short
base 3.50 m)
4 6
7
9
4 Track direction
2 5
8
10
5 Cant of the track
2 4
5
5
6 Track stability
2 4
5
5
Table 2. The size of limit values of parameters of track geometry [1]
Classiﬁcation
of lines parameter
I
II
III
IV
V > 100 km/h
100  V > 80
80  V  60
V < 60
A
B
C
A
B
C
A
B
C
A
B
C
1
Track
gauge
Gauge
widening
3
10
20
3
15
25
5
25
30
8
25
35
2
Gauge
tightening
3
3
3
3
4
6
3
4
8
3
5
10
3
Superelevation
of track (short
base 3.50 m)
4
7
10
6
8
12
7
10
15
9
14
18
4
Track direction
2
5
10
5
10
20
8
20
30
10
25
40
5
Cant of the track
2
4
8
4
6
10
5
8
15
5
8
15
6
Track stability
2
5
10
4
8
15
5
8
20
5
10
20
The Criteria for the Control of Condition of Railway Lines
695

Tables 1 and 2 show the limit values and allowed deviations for some parameters
of track geometry.
It is important to mention the following:
• limit of values are changing in relation to the category of railway,
• all analysis shall be adopted for a length of 1000 m,
• reports are compiled in the form of ﬁnally and individual reports.
Misalignments in the track by the above-mentioned parameters are divided into
three categories:
• Rank A—values at which it is not necessary to plan and execute the works,
• Rank B—values that should be planned works to eliminate misalignments,
• Rank C—misalignments that endanger trafﬁc safety and must be removed
immediately.
Evaluation of the state of the track depends on the number of individual
misalignments (A, B, C) per unit, which is usually 1 km.
The situation can be assessed (evaluated) as:
• very good (up to 10 m misalignments in the group B and 0 misalignments in the
group C, or < 10/0 B/C)
• good (up to 50 m misalignments in the group B, and up to 10 m misalignments in
the group C, or < 50/10 B/C)
• sufﬁcient (up to 250 m misalignments in the group B, and up to 25 m misalign-
ments in the group C, or < 250/25 B/C)
• poor (more than 250 m misalignments in the group B and more than 25 m
misalignments in the group C, or  250/25 (B/C)
The so-called “weak places” on the line, like as switches places, the area of the
level crossings, bridges, the insulated section and etc. must be separately analyzed.
After the measuring of driving and analysis, are conduct a works on determining
the occurrence of misalignment, and then being accessed to intervention and ﬁx the
same.
Measuring driving is the basic process by which builds strategy maintenance of
railway lines, because without continuous monitoring is not possible to timely provide
resources and materials for works on the railroad.
According to current regulations in B&H testing the geometric conditions of the
track by measuring driving should be carried out:
1. as a part of regular maintenance on the lines of 120 km/h—4 times per year, on all
lines ﬁrst order regardless of the speed, three times a year.
2. in receipt of completed works on the general overhaul of railways, reconstruction
and the admission of new railway lines.
Basic requirement for measuring driving is that measuring driving are not carry out
at temperatures lower than −5 °C or above +40 °C. If to these temperatures coming
during the recording, then are recording must be interrupted and continued with the
appearance of the allowable temperature.
696
M. Hebib-Albinovic and S. Albinovic

Other tests and reviews provided “Pravilnik 314” (Regulations 314) [2] which are
intended to carry out special mechanization are:
• control embedded rail (defectoscopy),
• testing of wear rails,
• testing dimensions,
• recording and testing the contact network.
Improvement of measurement techniques and to record reduced to a minimum, and
it is possible that all forward-mentioned tests and measurements tracks carried out in
same measuring car in during one recording (Figs. 6, 7 and 8).
Fig. 6 Ultrasonic measuring bogie on the “SDS” train [4]
Fig. 7 Rail proﬁle measuring head on the “SDS” train [4]
The Criteria for the Control of Condition of Railway Lines
697

3
Measuring Driving According to “EN 13848” [3]
In terms of control of track geometry, according to “EN 13848” primarily relates to the
completely modern measurement technique, that includes measuring vehicle with video
surveillance for different segments of the railway infrastructure (construction, electrical
energy, signaling) and the entire rail zone, as well as non-contact recording of all
parameters of the track, including GPS optical measurement.
One of the basic advantages of joint recording is that it gives an insight into
common action recorded parameters, as well as the possibility of their superimposition
in causing damage to the track and a rail capacity, which in practice is most often the
case.
It’s also cost-effectiveness of the procedure is greater, because it captures a larger
number of parameters simultaneously.
Modernization of railway measuring vehicles also has an impact on the interna-
tional standardization of measuring driving.
In “EN 13848” are no longer talking about a strict limits when it comes to “defects”
identiﬁed in the measuring driving but it comes thresholds of tolerance that enable
timely intervention to maintains the tracks.
The thresholds are given in three levels:
• Level 1-standby threshold,
• Level 2-threshold interventions,
• Level 3-threshold safety margin.
Even though in “EN 13848” focus also a put on the above mentioned parameters of
railway track (cant and super elevation of the track, stability and etc.) much more
dedicates itself to attention video (CCTV) surveillances (recording) of railway lines,
installations and objects on the track, recording speed and acceleration, and recordings
related to the contact network (Figs. 9 and 10).
Fig. 8 Mechanized eddy current measuring probes [4]
698
M. Hebib-Albinovic and S. Albinovic

This approach to measuring driving is predicted in order to increase the percentage
of interventions within maintains of railway lines which are planned on the basis of the
results of measuring driving and the other side decrease the percentage of interventions
on the basis of observation of people directly involved in maintenance of the railway.
As a result of this approach, the railway managers could be timely made a plan of the
efﬁcient maintenance of the tracks.
4
Conclusion
Until now, the measuring driving exclusively being used for the diagnosis of “weak
places” on the railway line and urgent and timely interventions to eliminate them. In the
future, measures driving should be one of the main elements on the basis of which will
Fig. 9 Clearance gauge measurement data [4]
Fig. 10 System for video surveillance of railway zone and laser scanner (360°) for measuring
the distance in “Plasser & Theurer” measuring vehicles [7]
The Criteria for the Control of Condition of Railway Lines
699

be planning maintenance, investment and management of railway infrastructure.
Modernization of measuring vehicles in the sense of recording a large number of
parameters and adjustment of terrain conditions and special requirements of railway
administrations largely contributes to this trend.
References
1. Pravilnik 314—Pravilnik o održavanju gornjeg stroja pruga pruga JŽ, Beograd (1989)
2. Uputstvo 339—Uputstvo o jedinstvenim kriterijumima za kontrolu stanja pruga na mreži JŽ,
Beograd (1988)
3. EN 13848-6:2014 Railway applications. Track. Track geometry quality. Characterization of
track geometry quality
4. MÁV Central Rail and Track Inspection Ltd., MAV_CRTI_Ltd_EN_prospectus. http://www.
mavkfv.hu
5. Evaluating of measurement of track geometry on the lines ŽFBH in the year 2008—ﬁnal
report, MÁV Központi Felépítményvizsgáló KFT (2008)
6. https://www.plassertheurer.com/en/machines-systems/measuring-work-em160.html
7. Oberlechner, G., et al.: Tračnička mjerna vozila za beskontaktno mjerenje i procjenu podataka
dobivenih mjerenjem Željeznice 21. HŽ Infrastruktura d.o.o., Zagreb (2014)
700
M. Hebib-Albinovic and S. Albinovic

The New Topographic Information System
and Establishing the Basic Topographic
Database of the Federation of Bosnia
and Herzegovina
Slobodanka Kljucanin(&)
Faculty of Civil Engineering, Department of Geodesy, University of Sarajevo,
Sarajevo, Bosnia and Herzegovina
slobodanka63@yahoo.com
Abstract. The creation of topographic maps is not a traditional activity in
Bosnia and Herzegovina. Making such maps were under the jurisdiction of the
military Socialist Federative Republic of Yugoslavia (SFRY). Due to the
technology used today and the digital spatial data, on the one hand, and com-
pliance with the INfrastructure for SPatial Information (INSPIRE) directive,
which is a requirement for entry into the European Union, on the other hand, it
was necessary to make strategic decisions about the development of cartography
in FB&H. So are some of the strategic decisions of the adoption of the new
topographic information system and the establishment of basic topographic
database 1:10000 (TTB). In this article will be talk about the new topographic
information system FB&H, and data collection methods and procedures for the
transfer of data that have been applied for the purposes of establishing TTB. It
also speaks about the proposal visualization TTB data.
Keywords: Topographic information system  INSPIRE  TTB  Data
collection methodology  Procedures for data transfer  Data visualization
1
Introduction
Modern technologies have changed the paradigm of cartography, and today it is
focused on building a system for establishing topographic databases, while maps, both
digital and analog are only forms of expression and communication with customers.
Methodologies and processes of making topographic databases and cartographic
products have changed accordingly.
Following the recommendation of the Topographic development strategy of FB&H
(from 2014), a benchmark was set for the Basic topographic database at 1:10000
(hereinafter TTB) of FB&H. This scale is one of the basic state map scale used in the
former country (SFRY). The Topographic development strategy of FB&H (hereinafter
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_61

Strategy) recommends visualization of spatial data in the form of topographic maps
divided to basic purpose maps and special purpose maps. Basic maps have the fol-
lowing scale: 1:10000, 1:50000 and 1:250000, and special maps: 1:25000, 1:100000
and smaller than 1:250000.
Following the establishment of the basic topographic database of FB&H, it is also
intended to create appropriate algorithms for model generalization1 and cartographic
data generalization, for the purpose of visualizing maps at a smaller scale. Model
generalization encompasses following processes: selection of object classes, selection
of certain objects according to the attributes and appearance context, geometry change
(areas in lines, areas in points, lines in points), ﬁlling blanks that are result of selection
(preservation of topology), simpliﬁcation of networks (roads, waters) and geometry
smoothness. Cartographic generalization implies, in addition to the model-based gen-
eralization the above methods, the following procedures: displacement of objects, text
positioning and application of map speciﬁc displays [2]. In order for TTB to be
effectively implemented, it is necessary to develop and adopt:
• speciﬁcation for TTB
• a digital collection of symbols for basic and special map scales
• a connection to the database of real estate cadaster (BPKN)
• a connection to a certain set of data of other subjects (water management, forestry,
roads, etc.)
• a connection to the register of geographical names
• a connection to the address model and
• a connection to other registers2 [4].
After accepting the Strategy by the Federal Administration for Geodetic and Real
Property Affairs (hereinafter FGU) a new Topographic Information System (TIS) of
FB&H was created. TIS was created in compliance with INSPIRE speciﬁcations, with
certain deviations in order to comply the legal regulations of FB&H.
A digital collection of characters was created in order to visualize the basic
mechanisms of topographic maps. The connection to BPKN is deﬁned using the
methodology for the establishment of TTB based on the pilot project “TTB
Bosnia-Podrinje Canton”. The connections with datasets with other operators (water
management, forestry, roads, etc.) and other registers have not yet been realized, as the
infrastructure of spatial data (SDI) in the Federation is yet to be established. Sharing
information is the basic objective of the SDI, which is mentioned in the Strategy on the
infrastructure of spatial data of the Federation of Bosnia and Herzegovina.
1 Model generalization is deﬁned as the controlled data reduction in the spatial, thematic and temporal
sense [1].
2 Basic registers or Authentic Registrations are those administrations, which are essential for the public
sector [3].
702
S. Kljucanin

2
Topographic Information System of the Federation
of Bosnia and Herzegovina
The data model deﬁnes the concepts of concern as a collection of object classes, the
hierarchical classiﬁcation of the concepts, the mutual association between the concepts
and their cardinality. It also contains the deﬁnition of the attributes (names and types)
and the constraints associated with the data [5].
Testing the Topographic data model TDM—object catalog V.1.3.1, published on
12.03.2012, indicated that there were signiﬁcant omissions of the model. Therefore, a
new information system was created, one that will fully be in compliance with the
INSPIRE speciﬁcations. Creating this model took about a year. Since the task was
demanding, i.e. it was necessary to create a topographic model which will be detailed
enough for the purposes of TTB and which can be used to visualize data in different
scales (using the model-based and cartographic generalization), we started the study
valid INSPIRE speciﬁcations on topics that are of interest to the content of TTB. By
studying the speciﬁcations, it was concluded that the speciﬁcations were done in great
detail, and that it is necessary to select only those speciﬁcations, packages and object
classes necessary for the establishment of TTB and topographic data visualization for
the scale of 1:10,000. Therefore, the following INSPIRE speciﬁcations and Packages
were selected:
1. INSPIRE Data Speciﬁcation for the spatial data theme Geographical Names
(Geographical Names)
2. INSPIRE Data Speciﬁcation for the spatial data theme Buildings (BuildingsBase,
BuildingsExtendedBase, Buildings2D, Buildings3D, BuildingsExtended3D)
3. INSPIRE Data Speciﬁcation for the spatial data theme Hydrography (Hydro-base,
Hydro-Network, Hydro-Physical Waters)
4. INSPIRE
Data
Speciﬁcation
for
the
spatial
data
theme
Land
Cover
(LandCoverVector)
5. INSPIRE
Data
Speciﬁcation
for
the
spatial
data
theme
Land
Use
(ExistingLandUse)
6. INSPIRE Data Speciﬁcation for the spatial data theme Utility and Government
Services
(Common
Utility
Network
Elements,
Electricity
Network,
Oil-Gas-Chemicals Network, SeweNetwork, TermalNetwork, WaterNetwork,
Telecommunications Network)
7. INSPIRE Data Speciﬁcation for the spatial data theme Elevation (Eleva-
tionGridCoverage, ElevationVectorElements, ElevationTIN)
8. INSPIRE Data Speciﬁcation for the spatial data theme Transport Networks
(Common Transport Elements, RoadTransportNetwork, RailwayTransportNet-
work, CableTransportNetwork, WaterTransportNetwork, AirTransportNetwork)
9. INSPIRE Data Speciﬁcation for the spatial data theme Administrative Units
(AdministrativeUnits, MaritimeUnits)
10. INSPIRE Data Speciﬁcation for the spatial data theme Sea Regions (SeaRegions).
The New Topographic Information System
703

After selecting the appropriate speciﬁcations, the focus moved to selecting the
application schema with appropriate object classes and packages (e.g. INSPIRE
Application Schema Buildings2D, V.3.0; Package-Building2D, class Building) as
shown in Table 1.
In addition to the INSPIRE speciﬁcations, it used the real estate cadaster data
model, i.e. application scheme—Catalog of objects 1.3.1 package—Geodetic points
(Table 2).
Table 1. Application Schema Buildings2D, v.3.0
INSPIRE Application Schema
Buildings2D, V.3.0
Package
Buildings2D
Class
Building
Name
Building
Subtype
Building
Deﬁnition
Building is a closed building above and/or below the ground, which is used or is
intended for people, animals or things or for the production of economic goods.
Buildings refer to any structure in permanent construction or built on that site
Stereotype «featureType»
Attribute: geometry2D
Name
2D geometry
Value type
BuildingGeometry2D
Deﬁnition
2D or 2,5D geometric representation of
the building
Description Note: Multiple geometric representations
are possible (i.e. with the surface, and a
point)
Multiplicity 1
Limitation: Building parts
shall be 2D
Natural
language
Part of the building should be presented
using the BuildingPart type—package
Buildings2D
OCL
inv: self.parts- > oclIsKindOf
(Buildings2D::BuildingPart)
Limitation:
singleReferenceGeometry
Natural
language
Only one 2D geometry attribute must be
a reference geometry, i.e.
referenceGeometry of the attribute must
be ‘true’
OCL
inv:self.geometry2D- > select
(referenceGeometry = true)- > size
() = 1
Table 2. Application schema—catalog of objects 1.3.1
Speciﬁcation
Application schema
Real estate cadastre data model Catalog of objects 1.3.1
Package
Geodetic points
704
S. Kljucanin

We also developed an appropriate code list, i.e. Value List, and this example can be
seen in Table 3.
3
The Methodology and Procedures for the Purpose
of Establishing and Maintaining the Basic Topographic
Database
The proposed data model for the basic topographic database is written in English,
abiding by the INSPIRE speciﬁcations (to avoid loss of information because of
translation or to avoid confusion due to inadequate translation). On the other hand, the
model on its own does not recommend data sources to be used. It is a well-known fact
that the Federation of B&H does not have an established Spatial Data Infrastructure
(SDI), so it cannot collect data as expected by the INSPIRE Directive. Therefore, we
initiated the development of methodologies and procedures for the purpose of estab-
lishing and maintaining the basic topographic database (hereinafter—Methodology).
Table 3. Value list—CrossingTypeValue (type of intersection/crossing), related to application
schema of hydro-physical waters
Application
Code list
INSPIRE Application
Schema ‘hydro—
physical waters’
CrossingTypeValue
Title
Type of intersection/crossing
Deﬁnition
Types of artiﬁcial crossing of physical watercourse
Expansion possibility
No
Identiﬁer
http://inspire.ec.europa.eu/codelist/CrossingTypeValue
Value
Accepted values for this code list include only the ones provided in
the below table
Aqueduct
Name
Aqueduct
Deﬁnition
A an pipe or artiﬁcial channel that is
designed to transport water from a
remote location, usually by gravity,
intended for drinking water,
agricultural and/or industrial use
Bridge
Name
Bridge
Deﬁnition
A structure that connects two positions
and is used to cross the barrier surface
Culvert
Name
Culvert
Deﬁnition
A closed channel that transfers the
water stream within routes
Siphon
Name
Siphon
Deﬁnition
A pipe which is used to transfer liquids
from one level to another, using the
pressure of the ﬂuid for upward thrust
The New Topographic Information System
705

Therefore, the Methodology contains a chapter dedicated to establishing and creating
TTB i.e. it was established that for the purpose of creating a basic topographic database
it is necessary to download the data from the databases of real estate cadaster (BPKN)
for: cadastral parcels, buildings and other structures on them, waters and buildings on
them, roads, railways, airports, cable cars, trafﬁc on water, land use, geographic pur-
poses, the administrative boundaries, relief, vegetation, utility lines and geodetic points,
which should go through selection and cartographic generalization. In addition to
downloading data from BPKN, to supplement the necessary facilities of TTB we need
to vectorize the data with a digital orthophoto plan using the 1:5000 scale (DOP5), and
download the rest of the data, if necessary, from other institutions which are respon-
sible for spatial data (e.g. statistical data, etc.).
The Methodology established that in order to maintain the TTB, one needs to do the
following:
(a) change the geometry of existing objects, incurred as a result of routine and
emergency maintenance or other measures. It is essential that owners of such data
(if it is legally regulated for example with an agreement) allow access to data as
soon as the work/measurement is done.
(b) change the geometry of newly constructed objects or parts of newly built objects.
It is essential that owners of such data (if it is legally regulated for example with
an agreement) allow access to data as soon as the work/measurement is done.
(c) until an agreement with data owners is made, on using their data, the following
data needs to be entered:
a. Downloaded data collected for the purposes of BPKN
b. Vectorization of objects with digital orthophoto M = 1:5000 [6].
3.1
Pilot Project “TTB of the Bosnia-Podrinje Canton”
In order to lay down the procedures that are necessary to retrieve data and to create
certain rules, which will enable easier user operation of TTB and their use of data, a
methodology was created based on a pilot project called “TTB of the Bosnia-Podrinje
Canton”.
For the purposes of these methodologies, we inspected which BPKN object classes
and code lists could be copied into TTB (7 object classes and 15 code lists). Bitemporal
schemes were created for the purpose of transferring data. The number of data in
question can be seen in Table 4.
Table 4. Statistical data of downloaded BPKN data for the purpose of TTB
Polygonal geometry
1 Buildings
20042
2 Method of use
108564
Point geometry
1 Geodetic points
3232
2 Toponyms
4993
3 Symbols of expanded content 448
706
S. Kljucanin

One could copy the existing data using two methods:
1. the
mapping
1:1
(BPKN.KAT_GEODETSKA_TOCKA_TACKA
to:
TTB.
GeodetskaTockaTacka; BPKN.KAT_TOPONIM in TTB.NamedPlace and BPKN.
KAT_ZGRADA to TTB.BuildingPart2D)
2. the
mapping
1:n
(BPKN.KAT_LINIJA
to:
TTB.BuildingExtended3D.Build-
ingsExtendedBase, TTB.LandCoverVector.LandCoverDataset, TTB.CommonUtil-
ityNetworkElements.Duct, etc.)
Screenshot of downloaded data can be seen in Fig. 1. After mapping the selected
object classes BPKN, we started with the examination of the topology rules, carto-
graphic generalization and visualization of the topographic map in scale 1:10,000.
3.2
Topology and Topological Rules TTB
Topology is a branch of mathematics that deals with a continuous surface and its
mapping. In his book of Algebraic Topology, Massey uses the term topology when
deﬁning the continuity of space. His deﬁnition of topology is: “A surface representing a
topological space X is a continuous copying p: X* ! X such that follows the following
conditions: Each point x 2 X is connected to arc with an open U neighbor, but so that
each arc component p−1(U) can be topologically copied into U over p. (i-e. homeo-
morphisms property—copying 1:1).
Fig. 1. Display of the LandUse, buildings and administrative unit packages
The New Topographic Information System
707

Topological space is deﬁned by a geometric object model, which consists of basic
geometry classes—points, curves, surfaces and geometric collection. Each geometric
object occupies a certain position in an associated spatial reference system. In addition
to basic geometric classes there are 0, 1, and 2-dimensional collection classes—mul-
tiPoint, MultiLineString, MultiPolygon, used for geometric modeling. MultiCurve and
MultiSurface are abstract super-classes [7].
Topological rules TTB are deﬁned by the Topographic Information System (TIS).
Topological relations of two spatial objects are based on their speciﬁc geometry and
topological properties which may, in principle, be explored with reference to types of
operations deﬁned with ISO 19107 (or methods as speciﬁed in EN ISO 19125-1).
Table 5 shows examples of topological rules, for only two INSPIRE speciﬁcations
found in TIS, which are mandatory in TTB.
3.3
Data Visualization TTB—Scale 1:10000
After the TTB topology it continued with model/cartographic data generalization.
Generalization is one of the most important steps in creating the maps because it
enables deﬁnition of selected dataset quantity and geometric quality as well as the form
in which those data will be shown on the map. Large amounts of data from the real
estate cadastre database can be shown, especially considering the fact that basic
topographic maps are created in large scales (1:5000 and 1:10 000). According to valid
cartographic regulations, map content generalization is performed for scales smaller
than 1:7000 which, in our case, means that generalization scope is different for OTK5
(partial generalization) and OTK10 (complete generalization) [8].
Table 5. INSPIRE speciﬁcations, package in TIS, and where you can ﬁnd the topological rules
INSPIRE speciﬁcation
Package in TIS
Topological rules
INSPIRE data
speciﬁcation for the
spatial data theme
geographical names
Geographical names
http://inspire.ec.europa.eu/
documents/Data_
Speciﬁcations/INSPIRE_
DataSpeciﬁcation_GN_v3.1.
pdf. See chapter geometric
representation on pp. 13–14
INSPIRE data
speciﬁcation for the
spatial data theme
buildings
BuildingsBase,
BuildingsExtendedBase,
Buildings2D, Buildings3D,
BuildingsExtended3D
http://inspire.ec.europa.eu/
documents/Data_
Speciﬁcations/INSPIRE_
DataSpeciﬁcation_BU_v3.
0rc3.pdf. See chapter geomet-
ric representation on pp. 50–
51 (building 2D); p. 149…
(building 3D) and annex
D-city model
708
S. Kljucanin

Next step was visualization geodata from TTB. When creating cartographic rep-
resentations and the products it is essential to take into account the way the database
contents will be interpreted. Cartographic visualization of intergated geospatial data
makes them clear, obvious and easy to use, and allows their better and faster distri-
bution and availability [9]. As this concerns a topographic database, we used
topographical sign for the visualization, that are used as the primary means of com-
munication between the cartographers and map users (see Fig. 2).
Digital signs that were used for TTB data visualization are unofﬁcial—or rather, the
Collection of characters that is used is not an ofﬁcial document of the Federal
Administration for Geodetic and Property Affairs. The Collection of characters used
was elaborated in the ﬁnal thesis written at the University of Sarajevo, Faculty of Civil
Engineering—Department of Geodesy.
This Collection of characters, all cartographic signs were graphically portrayed and
accompanied by textual interpretation of each character [10].
4
Conclusion
Establishing the TTB and using the new TIS is of an enormous signiﬁcance for car-
tography in the Federation of B&H, primarily because it is an entirely independent
product of the Federal Administration for Geodetic and Property Affairs. On the other
hand, it abandons the classical approach of data collection by using the photogram-
metric method of drawing map content, but instead it uses an approach of establishing
the basic topographic database based on TIS, which is fully compliant with INSPIRE
speciﬁcations.
Fig. 2. The visualized data using TTB collection of topographical characters
The New Topographic Information System
709

Using this methodology and the pilot project resulted in a series of questions that
must be addressed at the Federation level, such as:
1. Visualization of contours for the entire Federation of B&H
2. forming a register of geographical purposes
3. formalize the Collection of topographical characters for the purpose of TTB data
visualization
There is no doubt that by working further with TIS and completing the TTB data,
will result in a number of projects that will have to be done in order to facilitate the
work of future TTB users. One can also expect easier maintenance of TTB if the Spatial
Data Infrastructure (SDI) is properly implemented which will enable access to and
downloading of existing data including the contents of TTB.
In any case, by adopting a new version of TIS and by establishing TTB, we made a
huge step forward in the ﬁeld of cartography in the Federation B&H.
References
1. Weibel, R.: Three essential building blocks for automated generalization. In: Mueller, J.-C.,
Lagrange, J.-P., Weibel, R. (eds.) GIS and Generalization Methodology and Practice,
pp. 56–69. Taylor & Francis, UK (1995)
2. Biljecki, Z., Rapaić, M., Tonković, T.: Conceptual solution for the military geoinformation
system for the Croatian Ministry of Defense, no. 5, pp. 14–23. Cartography and Geoinfor-
mation, Zagreb, Croatia. http://hrcak.srce.hr/index.php?show=clanak_download&id_clanak_
jezik=7040 (2006). Accessed 19 Jan 2017
3. Bakker, N.J.: TOP10NL as Basic Register Topography Cadastre. Land Registry and
Mapping Agency, Apeldoorn, Netherlands (2005)
4. Ključanin, S., Poslončec-Petrić, V., Ponjavić, M., Karabegović, A., Landek, I: Development
Strategy of Ofﬁcial Cartography of the Federation of Bosnia and Herzegovina. Expertise.
Geometrika, Ltd. Grude, Bosnia and Herzegovina (2014)
5. Stroter, J.E.: Towards One Domain Model and One Key Register Topography. TU Delft
(research carried out at ITC), the Netherlands. http://www.gdmc.nl/publications/2009/One_
key_register_topography.pdf (2017). Accessed 4 Apr 2017
6. Modrinić, Z.: The Methodology and Procedures for the Purpose of Establishing and
Maintaining the Basic Topographic Database. Expertise. Geometrika d.o.o. Grude, Bosnia
and Herzegovina (2016)
7. Ključanin, S.: Interoperable cartographic databases. Master’s thesis, University of Sarajevo,
Faculty of Civil Engineering—Department of Geodesy, Sarajevo, Bosnia and Herzegovina
(2006)
8. Dinar, I., Ključanin, S., Poslončec-Petrić, V.: Large Scale Topographic Maps Generalisation
and Visualization Based on New Methodology, no. 3, pp. 189–198. Geodetski list 2015,
Zagreb, Croatia (2015)
9. Jovanović, J.: Cartographic Visualisation and Information Society, no. 46, pp. 59–68.
Geografski razgledi, Skopje, Macedonia. http://www.igeograﬁja.mk/MGD/Razgledi-46-
2012/05-%20Jasmina%20Jovanovic.pdf (2012). Accessed 6 Apr 2017
10. Družić, D.: Considering the existing collections of topographical indications scale of 1:5000
and 1:10000, and giving suggestions for a new digital collection of characters. Master thesis,
University of Sarajevo, Faculty of Civil Engineering—Department of Geodesy, Sarajevo,
Bosnia and Herzegovina (2015)
710
S. Kljucanin

The Regression Model for Assignment
of Diverted Trafﬁc to Planned Bypass Road
Suada Dzebo(&)
Faculty of Civil Engineering, Department of Roads and Transportation
Engineering, University of Sarajevo, Sarajevo, Bosnia and Herzegovina
suada.dzebo.gf@gmail.com
Abstract. This paper presents a simpliﬁed model of trafﬁc assignment to the
planned bypass road. The purpose of such model is to provide to the planners a
tool for simple, fast and inexpensive way to estimate the expected trafﬁc volume
on the planned bypass road by using data that can be obtained relatively quickly.
Thus, simpliﬁed model may have application in the prefeasibility and feasibility
studies, conceptual designs, as well as the development of spatial plans where it
is required to reserve the corridor of the bypass and the like. The model is
deﬁned by regression analysis. Sources of data, used in this paper, were feasi-
bility trafﬁc studies of eight cities in Bosnia and Herzegovina.
Keywords: Bypass road  Trafﬁc  Submodel  Planning  AADT
1
Introduction
The difﬁculties that planners face every day in drafting feasibility studies of bypass
roads in Bosnia and Herzegovina have initiated the topic of this paper. Examining the
trafﬁc studies that are made for the purpose of deﬁning and evaluating alternative
solutions of bypass roads in Bosnia and Herzegovina, it has been shown that many
studies in the absence of time, funds and the necessary data were drafted based on the
assumed growth rate of trafﬁc volume, assumed values of the diverted trafﬁc and so on.
Also, we are faced with the fact that Bosnia and Herzegovina has not implemented
population census from 1991 to 2013, making it very difﬁcult to establish trends in
socio-economic indicators.
If the data is not of sufﬁcient quality, then it can be more reliable to make a
prediction with simpliﬁed and “rougher” models [1]. It is well known that data scat-
tering can make their prediction less reliable. Therefore, when choosing a simpliﬁed
model, should prefer the variables that can be predicted with greater conﬁdence
interval.
The most common approximation take into account only two factors in the route
choice, that is the “time” and “ﬁnancial costs”. Actually, ﬁnancial costs are propor-
tional to the length of route. It is evident, that the travel time is a dominant factor in the
process of the route choice for the trafﬁc in urban areas. Outram and Thompson [2]
compared behaviour of drivers related to travel time or the route length. They found
that the combination of travel time and route length provide the best explanation for the
route choice, but also that this can explain only 60–80% of the route choice in practice.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_62

Since other factors in selecting the route are less important, the unexplained part is
attributed to factors such as differences in perception, inaccurate information about the
route costs or errors made during simpliﬁcation.
2
Why We are Building Bypass Roads?
The answer to the question can be found in the difﬁculties caused by the trafﬁc in the
central part of the cities.
The most common problems are:
• increase the time of travel,
• decrease the trafﬁc safety,
• exhausted capacities for urban development,
• heavy trafﬁc that passes through the centre of the town,
• decreased possibility for development of transport network,
• the appearance of bottlenecks, and
• increased pollution and noise.
Aiming to resolve above mentioned problems it is necessary to properly plan the
content and development of the city to get to the optimal solution. Construction of
bypass road in this case is much more than a trafﬁc diversion.
In fact, it is often thought that bypass roads are used only for the relocation of
heavy and transit trafﬁc out of the city in order to solve trafﬁc jams. Unfortunately, the
examples show that the relocation of transit could not solve trafﬁc jams in the cities [3].
Experiences have shown that decrease of the trafﬁc jams resulting from the construction
of the bypass road usually last for a very short time.
From the aspect of expansion and conceptual development of the city, bypass road
has a great signiﬁcance. Actually, the proper planning should ensure that the bypass
road is in function of the city development as much as in the trafﬁc function. In order to
take the speciﬁed role, the bypass road should be located on the acceptable distance
from the centre, and should have the transverse links that will allow its use for origin—
destination (I-D) trafﬁc. It is necessary to determine the optimal position of the bypass
road by multi criteria evaluation, taking into account technical, transport, spatial and
environmental criteria.
Otherwise, the result is often limited or without change in total trafﬁc levels, and it
is often associated with improved travel opportunities for local residents and access for
downtown businesses.
3
Data Sources and Methodology of Research
The main sources of data were Bypass Feasibility Studies of the following cities:
Bihac, Cazin, Kljuc, Kalesija, Gorazde, Bugojno, Donji Vakuf and Livno drafted in
2013 (Fig. 1) [4–11].
712
S. Dzebo

The mentioned studies were sources for trafﬁc data, data of the socio-economic
indicators, and the data of transportation forecasts.
Based on data taken from the studies of the above mentioned cities, a series of
simulations of trafﬁc load were made on their modelled transport networks by con-
ventional method, and for different sizes of the trafﬁc load under the following
assumptions:
1. All bypasses are ranked as the main roads.
2. The assumed design speed on the bypasses is 80 km/h.
3. Since the cities are relatively small (up to 40,000 inhabitants—urban part), all the
bypasses have only entrance and exit to the existing road, without additional
connections.
4. Equilibrium method of trafﬁc assignment was used in trafﬁc simulation.
The German software PTV Visum, which is a comprehensive, ﬂexible software
system for transportation planning, modelling of transport demand and network data
management was selected for the implementation of research.
The Equilibrium method is one of the most widely used algorithms in the trans-
portation planning and trafﬁc research. The equilibrium assignment is based on the
Wardrop’s ﬁrst principle, which states that no driver can unilaterally reduce his travel
costs by shifting to another route.
The required number of the trafﬁc simulations on existing and new transport net-
work of the cities was performed for each city. After each simulation the results were
collected, that is:
1. AADT—Annual Average Daily Trafﬁc on the planned bypass road (AADTbyp),
2. max.AADT on the existing road before building of bypass (AADTcur),
3. AADT on the screen line through the city without bypass (AADTscrin),
Fig. 1. Map of subject cities in Bosnia and Herzegovina
The Regression Model for Assignment of Diverted Trafﬁc
713

4. number of connections on the current road (ncur),
5. length of the existing road (lcur),
6. length of the planned bypass road (lbyp),
7. total travel time on the existing road (Tcur), and
8. number of total movements on the network (Nsum).
The total number of 74 simulations was completed on the existing, and 74 simulations
on a new transportation network. All data were divided into two groups by systematic
sampling, wherein 75% of the data were selected for model formulation, and the
remaining 25% for model validation. Following data collection, they were processed
and statistically analysed. Based on that variables that did not meet the assumptions of
regression analysis or do not have a signiﬁcant impact on the researched variable
AADT were rejected.
A multi regression analysis method was used for modelling using “Statgraphics
Centurion”1 software. Variables that did not have a normal distribution, were trans-
formed by the roots function (Table 1).
The table above shows a summary of statistics for each of the selected data vari-
ables. It includes measures of the central tendency, measures of variability and mea-
sures of shape. The standardized skewness and standardized kurtosis, which are used to
determine whether the sample comes from a normal distribution are of particular
interest. Values of these statistical values outside the range of −2 to +2 indicate
signiﬁcant departures from normality, which would tend to invalidate many of the
statistical procedures normally applied to this data.
In this case, measures of standard skewness and kurtosis for all transformed data are
within target range of −2 to +2.
After detailed analysis and rejection of some independent variables, the following
variables were taken further in analysis: AADTbyp, AADTcur, lcur and lbyp.
In an effort to simplify the model as much as possible, two new variables were
created from the above selected variables, i.e: AADTbyp/lbyp i AADTcur/lcur. This way,
the problem was reduced to model of simple regression. These variables were also
normalized by the root function.
Statistics of such deﬁned variables are shown in the Table 2.
In the Table 2, measures of standard skewness and kurtosis for the all transformed
data are in target range of −2 to +2.
Table 3 shows Pearson product moment correlations between each pair of vari-
ables. These correlation coefﬁcients range between −1 to +1 and measure the strength
of the linear relationship between the variables. The second number in each location of
the table is a P-value which tests the statistical signiﬁcance of the estimated correla-
tions. P-values below 0.05 indicate statistically signiﬁcant non-zero correlations at the
95.0% conﬁdence level.
Statistical indicators from the previous tables (Tables 2 and 3) show that the newly
created variables have a normal distribution and a very good, statistically signiﬁcant
(P < 0.05), mutual correlation of 0.95.
1 http://www.statgraphics.com/.
714
S. Dzebo

Table 1. Summary statistics
Count Average Standard deviation Coeff. of variation Minimum Maximum Range
Stnd. skewness Stnd. kurtosis
sqrt(AADTbyp)
54
75.838 25.058
0.330
28.231
127.523
99.291
0.106
−1.076
sqrt(AADTcur)
54
115.458 27.357
0.237
69.397
177.865
108.468
0.980
−0.991
sqrt(AADTscrin) 54
121.125 28.593
0.236
71.162
180.950
109.788
0.973
−1.033
sqrt(ncur)
54
2.446
0.547
0.223
1.414
3.464
2.050 −0.117
0.735
sqrt(lcur)
54
79.334 21.817
0.275
53.113
116.056
62.943
0.942
−1.891
sqrt(lbyp)
54
76.393 22.434
0.294
41.364
113.380
72.016
0.406
−1.473
sqrt(Tcur)
54
73.058 30.438
0.417
26.533
130.042
103.509
0.982
−1.485
log(Nsum)
54
174.836 40.283
0.230
96.587
293.012
196.425
1.971
1.204
The Regression Model for Assignment of Diverted Trafﬁc
715

3.1
Sample-Size Determination
The endeavour to obtain sufﬁciently precise estimates of the basic set of parameters, by
examining only a part of it, is the whole point of application of the samples method [12].
When choosing a sample size, it is necessary to take into account the following
factors:
• the degree of precision we want to achieve,
• the degree of variability of the data in the basic set,
• the amount of costs necessary for formation of the sample,
• the time required to form the sample, and
• the size of the basic set.
The optimum sample size is the sample that, with a minimum engagement of time
and resources, provides satisfying and sufﬁciently accurate results.
A sample which is less than 30 units is deﬁned as a small one and the one greater
than 30 units as a large sample. Before the sample size is determined it is necessary to
decide what degree of reliability (probability) is desired to make conclusions and what
maximum error can be accepted.
If the estimate’s error is expressed in units of measure in which is the observed
variable, then the sample size is calculated according to:
n0 ¼ z  r
d
h
i2
ð1Þ
where:
n0
Sample size
r
Standard deviation population estimated using the sample
Table 2. Summary statistics
Count
Average
Standard
deviation
Coeff. of
variation (%)
Minimum
Maximum
Range
Stnd.
skewness
Stnd.
kurtosis
sqrt
(AADTbyp/
lbyp)
54
1.082
0.456
42.11
0.275
2.117
1.843
0.760
−0.925
sqrt
(AADTcur/
lcur)
54
1.549
0.476
30.75
0.598
2.567
1.969
−0.137
−0.835
Table 3. Correlation coefﬁcients
sqrt(AADTbyp/lbyp) sqrt(AADTcur/lcur)
sqrt(AADTbyp/lbyp)
0.946
Correlation
0.000
P-Value
sqrt(AADTcur/lcur)
0.946
0.000
716
S. Dzebo

z
Reliability coefﬁcient of estimation, and
d
Acceptable error estimates in absolute value
In case of 90% interval of conﬁdence, from the table of normal distribution, it
follows that z = 1.69, and as acceptable error is taken 8% from the mean of the sample,
i.e. d = 6.05.
In this case the sample size is:
n0 ¼
1:69  25:14
6:05

2
¼ 49
ð2Þ
The following illustration shows the test power curve in relation to the mean value
of the sample (Fig. 2).
Parameter to be estimated: normal mean.
Desired tolerance: ±8.0% when mean = 75.58.
Conﬁdence level: 90.0%.
Sigma: 25.14 (to be estimated).
The required sample size is n = 49 observations.
This procedure determines the sample size required when estimating the mean of a
normal distribution. Assuming that the standard deviation of the normal distribution
equals 25.14, then 49 observations are required to estimate the mean of the sample within
error of ±8.0% (assuming the mean is around 75.58) with 90.0% conﬁdence level.
4
Research Results
As a result of the research, a “double reciprocal” model was obtained of which the
general form is:
Power Curve
alpha = 0.1, sigma = 25.14, n=49
61
66
71
76
81
86
91
TrueMean
0
0.2
0.4
0.6
0.8
1
Power (1 - beta)
Fig. 2. Test power curve
The Regression Model for Assignment of Diverted Trafﬁc
717

Y ¼
1
a þ b
X
ð3Þ
The selected model has the highest coefﬁcient of determination and the lowest
standard error that satisﬁes all the assumptions of regression analysis.
The equation of the ﬁtted model is:
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
AADTbyp
lbyp
s
¼
1
0:410414 þ
2:16232
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
AADTcur
lcur
p
ð4Þ
where:
AADTbyp
Annual Average Daily Trafﬁc on the planned bypass (veh/day)
AADTcur
Annual Average Daily Trafﬁc on the existing road (veh/day)
lbyp
Length of the planned bypass (m)
lcur
Length of the existing road (m)
There are two newly formed variables in the ﬁnal Eq. (4): AADTbyp/lbyp and
AADTcur/lcur. From the aspect of their physical meaning, it is evident that the newly
formed variables represent the number of vehicles per unit length of the road, which
practically represents the density of trafﬁc ﬂow on the two observed roads.
In this case, the resulting model can be interpreted as the ratio of the trafﬁc density
on the planned bypass road and existing road where increase of the trafﬁc density on
the existing road increase the trafﬁc density on the planned bypass.
The regression coefﬁcients obtained by the analysis are given in the next table as
the original output of the program Statgraphics Centurion (Table 4):
Analysis of Variance is shown in the next table (Table 5):
Table 4. Regression coefﬁcients
Parameter Standard estimate coefﬁcients Standard error T statistic P-value
Constant
−0.410414
0.0696837
−5.88967 0.0000
Slope
2.16232
0.103415
20.9092
0.0000
Table 5. ANOVA—Analysis of Variance
Source
Sum of squares Df Mean square F-ratio P-value
Model
7.2936
1
7.2936
437.20 0.0000
Residual
0.767405
46 0.0166827
Total (corr.) 8.06101
47
718
S. Dzebo

• r = 0.95121—correlation coefﬁcient,
• R2 = 90.48%—coefﬁcient of determination,
• Radj
2
= 90.27%—adjusted coefﬁcient of determination,
• Standard Error of Est. = 0.129162,
• Mean absolute error MAE = 0.106956
• Durbin-Watson statistic DW = 0.453323 (P = 0.0000)
The output shows the results of ﬁtting a double reciprocal model to describe the
relationship between sqrt(AADTbyp/lbyp) and sqrt(AADTcur/lcur).
Since the P-value in the ANOVA table is less than 0.05, there is a statistically
signiﬁcant relationship between sqrt(AADTbyp/lbyp) and sqrt(AADTcur/lcur) at the
95.0% conﬁdence level.
The R-Squared statistic indicates that the model as ﬁtted explains 90.48% of the
variability in sqrt(AADTbyp/lbyp). The correlation coefﬁcient equals 0.95121, indicating
a relatively strong relationship between the variables. The mean absolute error
(MAE) of 0.106956 is the average value of the residuals.
The standard error of the estimate shows the standard deviation of the residuals to
be 0.129162. This value can be used to construct prediction limits for new observations
(Table 6).
The Table 6 shows the predicted values for sqrt(AADTbyp/lbyp) using the ﬁtted
model. In addition to the best predictions, the table shows:
1. 95.0% prediction intervals for new observations
2. 95.0% conﬁdence intervals for the mean of many observations
The prediction and conﬁdence intervals correspond to the inner and outer bounds
on the graph of the ﬁtted model (Fig. 3).
The Durbin-Watson (DW) statistic tests of the residuals determine if there is any
signiﬁcant correlation based on the order in which they occur in the data ﬁle. Since the
P-value is less than 0.05, there is an indication of possible serial correlation at the
95.0% conﬁdence level. The relationship between empirical and regression values is
shown in the following graph (Fig. 4).
Considering that in our case there is a series of time intervals which are not of
primary importance for the research, but through the time sections simulated increase
of trafﬁc volume, this phenomenon has no major impact on the adopted model.
Table 6. Prediction limits
95.00%
95.00%
Predicted Prediction
limits
Conﬁdence
limits
X
Y
Lower
Upper
Lower
Upper
0.5979 0.3119
0.2821 0.3487 0.2922 0.3345
2.5669 2.3151
1.4283 6.1067 2.0090 2.7312
The Regression Model for Assignment of Diverted Trafﬁc
719

If we replace sqrt(AADTcur/lcur) with kG, and give it the name “coefﬁcient of the
city”, the Eq. (4) takes the form:
AADTbyp ¼
kG
0:410414  kG þ 2:16232

2
lbyp
ð5Þ
where the “coefﬁcient of the city” is:
kG ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
AADTcur
lcur
r
ð6Þ
Fig. 3. Plot of ﬁtted model
Fig. 4. Graph—observed versus predicted
720
S. Dzebo

From Eq. (5) it can be concluded that the AADT on the planned bypass depends on
the “coefﬁcient of the city” and length of the planned bypass road.
4.1
Testing and Validation of the Proposed Regression Model
To check the reliability of the proposed model, additional test was conducted with the
data that were not used in the model creation.
Checking the validity of the prediction makes it possible to examine whether the
model, determined on the one set of data, is viable when it is applied to the second set
of data.
In general, the validation is implemented on the basis of data obtained by mea-
suring in situ, wherever it is possible. However, in situations where this kind of
measurement is not feasible, researchers often use the results obtained by simulation
models as references for testing the new analytical models.
Thus, 25% of the data of the whole sample is set aside for testing and validation,
while 75% was used to form the model.
For this purpose two measures of efﬁciency were used: the average absolute error
(AAE) and the mean absolute percentage error (MAPE) deﬁned by the following
equations:
AAE ¼ 1
n
X
n
1
f i
DTs  f i
DTm


ð7Þ
MAPE ¼ 1
n
X
n
1
f i
DTs  f i
DTm
f i
DTs


ð8Þ
where:
n
Number of data
fDTs
AADT planned bypass obtained by simulation
fDTm
AADT planned bypass obtained by model
For the adopted model these values are:
• AAE = 0.137
• MAPE = 0.123 = 12.3%
The calculated average absolute error is 0.137, while the average absolute per-
centage error is 12.3%. Given that this is a trafﬁc prediction on the planned level, the
results can be considered acceptable.
Figure 5 represents a graphical comparison of simulated and modelled values in the
coordinate system where their relationship is compared with the line y = x, which
represents an ideal matching results.
The Regression Model for Assignment of Diverted Trafﬁc
721

5
Conclusion and Recommendation
This research shows that such simpliﬁed model may have application in the prefea-
sibility and feasibility studies, conceptual designs, development of spatial plans where
it is required to reserve the corridor of the bypass, at the preliminary calculation
capacity and level of service, for calculating economic feasibility, as well as other
impacts of the planned road.
The resulting model can be interpreted as the ratio of the trafﬁc density on the
planned bypass road and existing road, where increase of the trafﬁc density on the
existing road increase the trafﬁc density on the planned bypass.
There is a new variable kg, named “coefﬁcient of the city”, and it is equal to sqrt
(AADTcur/lcur). Namely, it depends of the ratio of annual average daily trafﬁc and route
length of the existing road through the city. In this case one can calculate annual
average daily trafﬁc on the planned bypass road (AADTbyp) using formula (5), and it
depends of coefﬁcient of the city and length of the planned bypass road.
The absolute percentage error for this calculation is 12.3%, and it could be said that
it is acceptable up to 20% in case of preliminary projects and studies.
For the future research it is recommendable to analyse scenarios of bypass roads
with one or more transverse links in the middle. In the next phase it would be advisable
to expand research to larger cities and to apply the same analysis to them too.
References
1. Alonso, W.: Predicting best with imperfect data. J. Am. Inst. Plan. 34(4), 248–255 (1968)
2. Outram, V., Thompson, E.: Driver’s perceived cost in route choice. In: Proceedings of 6th
PTRC Summer Annual Meeting, London (1978)
3. Collins, M., Weisbrod, G.: Economic Impact of Freeway Bypass Routes in Medium Size
Cities. Department of Economic Development, Roanoke, Virginia, SAD (2000)
4. IPSA Institute, Sarajevo, Studija izvodljivosti za obilaznicu Bihać. JP Ceste FBiH, Sarajevo
(2013)
0.00
0.50
1.00
1.50
2.00
2.50
0.00
0.50
1.00
1.50
2.00
2.50
model 
simulation 
Fig. 5. Comparison of modelled and simulated results obtained by regression model
722
S. Dzebo

5. IG “Banja Luka”, Studija izvodljivosti za obilaznicu Bugojno. JP Ceste FBiH, Banja Luka
(2013)
6. IG “Banja Luka”, Studija izvodljivosti za obilaznicu Cazin. JP Ceste FBiH, Banja Luka
(2013)
7. TRAFFICON DOO Zagreb, Studija izvodljivosti za obilaznicu D. Vakuf. JP Ceste FBiH,
Zagreb (2013)
8. IG “Banja Luka”, Studija izvodljivosti za obilaznicu Goražde. JP Ceste FBiH, Banja Luka
(2013)
9. INCOPROM DOO, Banja Luka, Studija izvodljivosti za obilaznicu Ključ. JP Ceste FBiH,
Banja Luka (2013)
10. Institut, I.P.S.A.: Sarajevo, Studija izvodljivosti za obilaznicu Kalesija. JP Ceste FBiH,
Sarajevo (2013)
11. Institut, I.P.S.A.: Sarajevo; Integra Mostar, Studija izvodljivosti za obilaznicu Livno. JP
Ceste FBiH, Sarajevo (2013)
12. Fazlović, S.: Statistika: Deskriptivna i inferencijalna analiza. Denfas, Tuzla (2006)
The Regression Model for Assignment of Diverted Trafﬁc
723

Transient Vibrations of Railway Track
Elements and the Inﬂuence of Support
Conditions
Emina Balic(&) and Ciaran McNally
School of Civil Engineering, University College Dublin, Belﬁeld, Dublin 4,
Ireland
{emina.balic,ciaran.mcnally}@ucd.ie
Abstract. An analysis of the dynamic characteristics of railway track elements
is presented within the paper. A short review of ballast condition inﬂuence on
dynamics of railway track is provided. Ballast stiffness degradation and lack of
ballast support are the severities that are taken into account. In addition results
from transient vibration testing of in situ railway track elements are presented.
Field testing focused on two track sections: one on an embankment with timber
sleepers, the other close to a railway junction with concrete sleepers and the
bedrock close to the surface. The impact hammer testing technique was utilized
for acquiring the transient vibration data; corresponding accelerance functions
are obtained for several railseats. The comparisons are made between acquired
accelerance functions for sleepers from three different locations. Comments
about results repeatability for tested railseats are also given. A link between
obtained accelerance function and condition of supporting ballast layer is given.
1
Introduction
A great amount of information can be found about the integrity of railway track
elements by exciting the rails and sleepers. However, the track is embedded into a
coarse granular material that is further supported by a subgrade layer and underlying
soils. This implies that the inﬂuence of railway substructure cannot be neglected when
analysing the obtained data. This paper presents a part of an ongoing research project
on the assessment of railway substructure that combines the study of transient vibra-
tions of railway track elements with certain condition of substructure layers.
The condition of substructure layers affects the dynamic response of the track
system in the lower frequency range, usually below 300 Hz [1–4]. When frequency
response functions are obtained either on top of the railhead or at railseat for different
sleeper support and ballast condition, distinct dominant frequencies can be noticed
[1–3, 5]. For a well supported railway track the obtained FRF on top of the railhead
shows a ﬁrst dominant frequency under 250 Hz [2, 3]. This frequency range corre-
sponds to vibration of rail on a sleeper supported by ballast [2, 3]. Differences in
plotted FRFs for frequency range 0–100 Hz have been seen for different ballast
material, heights of ballast and existence of under ballast mats [1]. The plotted FRFs
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_63

differed both in magnitudes and dominant frequencies, where higher magnitudes were
obtained for stiffer track support but with lower dominant frequencies [1]. If FRFs are
obtained on top of the sleeper end, a single dominant frequency is noticed at about
110 Hz for good sleeper support [6]. When, the sleepers FRFs are measured in areas
prone to poor support several dominant frequencies are showed at 110, 300, 585 and
900 Hz [6]. If ballast support under the sleeper is completely lost, a highly damped
peak occurs at around 120 Hz in plotted sleeper accelerance function [5]. The mag-
nitude of the FRFs when measured on the sleeper is increased if it’s measured in areas
that have poor ballast support [7] and by increasing the number of poorly supported
sleepers [5].
This work presents an addition to already published works related to study of
dynamical characteristics of railway track elements for different substructure condition.
First, a short review about inﬂuence of ballast condition on dynamics of railway track
elements is given. Ballast stiffness degradation and lack of ballast support are the
severities that are taken into account. Features such as free vibrations of railway
sleeper; displacement, loading and acceleration of sleeper; displacement, loading of rail
are analysed. Second, acquired in situ accelerance functions of railway track from three
different location of unused railway line are plotted. Comments about obtained fre-
quencies for concrete and timber sleeper with different support conditions are given.
Emphasis is given to railseats position. In addition, repeatability and attenuation of
accelerance functions were analysed in terms of reached sleeper-ballast contact.
2
Ballast Condition and Its Effect on Dynamics of Railway
Track Elements
Owing to repeated trafﬁc loads, the ballast layer underneath the railway track in
transversal and longitudinal direction becomes unevenly distributed with varying
stiffness or even pockets at sleeper-ballast contact occur due to differential settlements.
These present ballast layer severities will signiﬁcantly alter not just displacements and
loading of track elements but also dynamic characteristics of railway track for different
frequencies bandwidths. Hence, constant monitoring of dynamic behaviour of railway
track elements for various ballast condition can enable a link between certain ballast
condition and dynamic response of track elements. Following two chapters give a short
review of ballast condition inﬂuence on behaviour of railway track elements. First,
stiffness degradation in ballast bed is reviewed mainly for ballast-sleeper dynamic
interaction. Second, more extreme case such as complete loss of ballast support
underneath the sleeper is considered. Numerical studies and in-ﬁeld measurements
have been taken into account.
2.1
Ballast Stiffness Degradation
Several numerical and experimental studies have analysed sleeper/ballast dynamic
interaction for various condition of ballast bed such as improper packing/tamping [8],
stiffness degradation [7, 9, 10], wet ballast [11] and fouled ballast [12, 13]. The works
Transient Vibrations of Railway Track Elements
725

showed that ballast layer stiffness usually affects the dynamics of railway sleeper in the
lower frequency range. First two rigid body mode shapes with corresponding natural
frequencies are the most affected with any alteration in ballast layer. Some changes are
evident also in the ﬁrst ﬂexural bending mode, whereas the inﬂuence on higher bending
modes can be almost neglected.
In practice we can ﬁnd that ballast stiffness is usually not equally distributed
underneath the railway sleeper, hence we can have regions of different ballast stiffness
underneath the sleeper. In studies, either experimental or numerical, it was proved that
rigid body mode shapes are the most inﬂuenced when ballast stiffness degradation is
considered. When one portion of ballast layer underneath the sleeper has been changed
with ﬁner particles in an experimental setup, greatest reduction in natural frequencies of
10.3% occurred for ﬁrst rigid body mode shape [10]. In addition, mode shape curve of
the second mode shape was affected and bending took place instead of rotation [10].
The numerical studies showed that if more regions of unequal ballast stiffness are
considered underneath the sleeper, the reduction in resonant frequencies is greater
[7, 10, 12]. Furthermore, the higher degree of stiffness degradation the greater alteration
in resonant frequencies occurs [7]. In addition, the studies showed that transverse mode
shape diminishes and rotation happens instead with some amount of bending. If ballast
stiffness is more degraded in middle regions, ﬁrst translational mode shape shows more
bending [10].
The effect of improper ballast packing/tamping in regions underneath the railseats
was introduced by nonlinear distribution of support stiffness in the FE model of
sleeper/ballast interaction [8]. The effective zone of ballast support underneath each
railseat was considered to have uneven stiffness distribution from 0 to 100% and from
100 to 200%. For translational mode, high degree of softening prevailed for uneven
ballast stiffness reduction, whereas very low hardening degree occurs when ballast
stiff-ness increase. For rotational mode, the hardening effect is greater than softening.
On the other side the ﬁrst bending mode went through very low hardening and soft-
ening, whereas higher ﬂexural modes are almost insensitive to non-linearity in support
condition. Modal displacements tend to be higher for regions with lower ballast stiff-
ness for translational mode, whereas for rotation and ﬁrst ﬂexural mode the maximum
occurs in regions with higher ballast stiffness.
Kaewunruen et al. [11] showed the effect of wet/dry ballast on free vibration
characteristics of sleeper/ballast interaction. For the frequency range of interest
(0–1.6 kHz), the inﬂuence of wet ballast was evident for lower resonant frequencies and
corresponding damping values. The ﬁrst two bending modes had an increase in both
resonant frequencies and damping. This mode had the greatest change in damping for
wet conditions and it was 37% higher when compared for dry condition. However, this
trend didn’t continue for higher frequencies. Third, fourth and ﬁfth resonant frequency
for wet condition were reduced whereas corresponding damping values increased for the
ﬁfth and third mode, while for the fourth mode the damping value decreased.
Field measurements done by Zakeri et al. [13] showed that bending moments of the
sleeper were greater in the condition of contaminated ballast with sandy particles. The
increase in bending moments was by 28%. The sandy contamination didn’t affected
that much the pressure distribution underneath the sleeper. Again, it remained maxi-
mum underneath the railseats.
726
E. Balic and C. McNally

2.2
Gaps/Pockets or Lack of Ballast Support
Due to uneven ballast stiffness distribution along the railway sleeper and along the
railway track, the ballast layer is prone to uneven settlements and as a consequence
gaps with loss of sleeper-ballast contact will occur. The occurrence of gaps and lack of
ballast support produces different effects on the unsupported sleeper than the other
elements of railway track. The main characteristics of railway track that are altered due
to lack of ballast support are the following: resonant frequencies and mode shapes of
unsupported sleeper; displacements and loading of unsupported sleeper; rail dis-
placements and loading; wheel/rail contact force; loading and displacements of adja-
cent supported sleeper.
Several numerical studies showed that the two rigid body resonant frequencies are
the most affected when gaps at sleeper-ballast contact are formed at different locations,
and with varying extent along the sleeper [14–16]. The resonant frequencies will
decrease further as more sleeper-ballast gaps develop. The ﬁrst resonant frequency will
decrease up to 70% of its initial value [15]. For symmetric occurrence of pockets
underneath the sleeper, the ﬁrst and second body mode shape remain translation and
rotation, whereas for unsymmetrical pockets along sleeper length a certain degree of
dynamic instability is noticed where rigid body mode shapes are combination of both
translation and rotation [15, 16].
In addition to the study of individual sleeper-ballast interaction, several numerical
studies and ﬁeld/experimental measurements have been performed to study the effect of
unsupported sleeper within railway track. Field measurements have shown that for a
well supported railway track although the speed and loading increases, the accelera-
tions of railway sleeper won’t exceed 5 g [17]. In zones where the track is poorly
supported, sleeper acceleration can go above 10 g and the railway track will exhibit
greater movements [17, 18].
The effect of unsupported sleeper on the railway track is enhanced, if we consider
the train speed; size of the gap between the sleeper and ballast; increasing number of
unsupported sleeper; spacing between sleepers. An increase in wheel/rail normal load is
accelerated when all these effects are taken into account. For a 3D multi-body system
formulation that takes into account effect of rail, sleeper and ballast ﬂexibility on
creepages, creep forces and wheel/rail contact formulations, the wheel rail normal load
increased by 30% for one unsupported sleeper [19]. The wheel/rail normal load
increases rapidly as the gap between sleeper and ballast increases, increasing the
number of hanging sleeper up to ﬁve or six and with increased train speed [20]. For a
car-body with three axle bogies there was a signiﬁcant increase in displacements,
bending moments for track with two unsupported sleepers and the values were higher
by 100 and 300% respectively [21]. For a train speed up to 320 km/h the railseat
counterforce is increased by almost 4 times [5]. Furthermore, the result showed that
unsupported sleepers will lead to relatively large additional dynamic interaction force
on the track particularly on high speed tracks [5].
The size of loose sleeper gaps also have great effect on the bending fatigue of rail
welds and it was shown by [22] that in the case of two loose sleeper with gap of
2.0 mm the fatigue life of rail welds is estimated to be half as long as that in the case of
no loose sleeper. The increased gap size of 1.2 mm and sleeper spacing of 75 cm have
Transient Vibrations of Railway Track Elements
727

increased rail displacements by 24 and 22% respectively [23]. Furthermore, if the
number of unsupported sleeper goes from 1 to 9 with increased gap size the rail
displacements are increased by 74% [23]. Sleeper-ballast loads are increased by 200%
if the sleeper ballast gap is 2.0 mm [24].
The existence of unsupported sleepers will cause redistribution of loading away
from the unsupported sleeper to more well supported sleepers [24]. Hence, loading and
displacements of adjacent sleepers will be maximised. When the number of unsup-
ported sleepers is increasing, the dynamic effects are visible in regions that are adjacent
to the unsupported section. With increasing vehicle speed and increasing the number of
hanging sleepers, the sleeper acceleration, sleeper displacement, railseat counter force,
wheel-rail force, ballast top surface pressure are increasing [19]. So for speed of
320 km/h the adjacent sleeper displacement increase by factor of 4, the sleeper
acceleration is raised 8 times, while the ballast acceleration is raised 6 times [5].
A discrete numerical model has shown that effect of unsupported sleeper and rail
displacement increase was continued up to three adjacent well supported sleepers [23].
The maximum support forces in the adjacent sleepers have risen by 28.5% for the track
with one sleeper while this value went to 48.5 for the track with two unsupported
sleepers [21]. The well supported sleeper will experience increase in loading of 83% if
the adjacent sleeper has gap of 1 mm [24]. The gap size of 1 mm between sleeper and
ballast will increase the contact force at the adjacent sleeper up to 70% and the dis-
placement of adjacent sleeper up to 40% [25]. When a three car body is considered in
numerical analysis the support forces for adjacent sleeper were increased by almost
50% for two unsupported sleepers [19].
3
Transient Vibration Testing of Railway Track Elements:
Field Measurements
Transient vibrations of railway track elements are analysed by utilizing impact hammer
testing method. Impact hammer testing method represents a mobile, self-supporting
non-destructive method. It enables a fast system for getting the dynamic track
parameters without major trafﬁc disruptions [26].
Several experimental and ﬁeld works have utilized impact hammer testing for study
of behaviour of railway track. Impact hammer testing in railway environment has been
utilized for study of following features: free vibration characteristics of sleeper/ballast
interaction [10, 11, 27–29]; receptance and accelerance functions of rail [1, 2, 26, 28];
receptance and accelerance function of rail joints [30]. This section contains a
description of impact hammer testing method with equipment setup and necessary
procedure for acquiring the data of interest according to Balic et al. [28]. Description of
track sites with site conditions is also given.
3.1
Track Sites
Vibration properties of railway track and its elements are studied at three locations that
had different support conditions. First track site was on an inactive railway line and it
was located in Nobber, county Meath. The railway track is placed on an embankment
728
E. Balic and C. McNally

2.0 m high. The railway track consisted from UIC 54 rails with 1600.2 mm (5ft and
3in). The railway sleepers were timber made from oak or pine wood with dimensions
265  130  2615 mm. The tested track site in Nobber can be seen on Fig. 1.
The second test site was located in town of Wellingtonbridge in County Wexford,
Ireland. The railway line was closed for passenger trafﬁc in 2010. The railway track is
placed on an embankment. The studied track section consists of UIC 54 rails with
1600.2 mm (5ft and 3in) gauge and timber sleepers (see Fig. 2).
Fig. 1. Railway track in Nobber, County Meath, 13/05/2016.
Fig. 2. Test site 1 in Wellingtonbridge, County Wexford, 18/05/2016 [28].
Transient Vibrations of Railway Track Elements
729

The third test site was also located in town of Wellingtonbridge in County Wex-
ford, Ireland. The railway line was closed for passenger trafﬁc in 2010. The railway
track section consists of UIC 54 rails with 1600.2 mm (5ft and 3in) gauge and concrete
sleepers type TIR6. This test site was located in vicinity to rail junction and approx.
200 m away from Wellingtonbridge train station (see Fig. 3).
3.2
Site Conditions
Reports of geotechnical and geophysical investigations were available for test locations
in Wellingtonbridge [31]. The ﬁrst test track is laid on a 4.5 m high embankment, and
the embankment is made of ﬁll material identiﬁed as medium dense silty sand with
occasional lenses of clay and sand. The ﬁll is underlain with a very stiff clay layer
which is then underlain by relatively soft natural ground consisting of silt/clay marine
deposits followed by a layer of dense sand. The ballast layer was almost 1.0 m high
and was consisted of sand and gravel (particle sizes between 0.06 and 60 mm).
For the second test location in Wellingtonbridge the Geological Survey of Ireland
(GSI) [32] digital data shows the bedrock to be located on the surface and is identiﬁed
as karst.
3.3
Test Setup
The equipment setup consisted from compact chassis DAQ-9178 with module NI-9205
and module NI-9234 from National Instruments; instrumented sledge hammer (model
PCB 086D50 of 5.5 kg) from Piezotronics; eight MEMS (micro electro-mechanical
system) DC accelerometers range ±5 and ±2 g.
The response was picked up by uniaxial accelerometers that were placed on top of
tested elements. For concrete surface the accelerometers were installed with special
Fig. 3. Test site 2 in Wellingtonbridge, County Wexford, 18/05/2016 [28].
730
E. Balic and C. McNally

aluminium brackets, epoxy and high performance tape, whereas for wooden surface the
accelerometers were screwed directly onto timber. The vibration signals from six
accelerometers were digitized by NI-9205 module that was further connected to
Compact DAQ-9178 chassis. The force signal from the hammer was recorded by
NI-9234 module that was also plugged into Compact DAQ-9178 chassis.
3.4
Test Overview
Impact hammer testing on railway sleepers included two different tests: testing of
timber railseats and testing of concrete railseats. Testing was carried out by exciting the
sleeper with the instrumented excitation hammer and by recording subsequent vibration
components [26]. The example of produced force by instrumented hammer and
recorded acceleration can be seen in Fig. 4.
Timber and concrete sleepers were excited at railseats in the vicinity of the posi-
tioned accelerometer. Left and right railseats were tested and for every excitation point
three impacts were performed. The accelerometer conﬁguration for tested railseats can
be seen in Fig. 5. The railseat position is referred to location on top of the sleeper very
close to clips and under rail pads.
Obtained force and acceleration data in time domain is further processed to get data
in frequency domain. For this purpose, measured signals went through following signal
analysis approaches:
• data detrend: removing linear trends close to 0 Hz by subtracting the mean value
from time records;
• in the frequency domain: power and cross spectral density of force and acceleration
data by using cpsd function in Matlab [33]. Calculation of force auto power
spectrum revealed the frequency spectrum that is excited with instrumented hammer
[34];
• in the frequency domain: accelerance function is obtained as ratio of cross spectrum
between force and response, divided by the autospectrum of the force [34–36].
In order to check the quality of plotted accelerance functions corresponding
coherence functions are plotted. Coherence presents degree of linearity between output
and input signal, i.e. how much of the output signal in plotted accelerances is related to
input signal.
Fig. 4. Force and acceleration obtained by impact hammer test.
Transient Vibrations of Railway Track Elements
731

4
Accelerance Functions of Railway Track Elements. Results
and Discussion
Timber sleepers were tested on two embankments, in Nobber and Wellingtonbridge.
First, testing was done in Nobber and corresponding data is acquired. Second, impact
hammer testing and data acquiring is done on an embankment in Wellingtonbridge.
Third, concrete sleeper is tested in Wellingtonbridge on a rail junction. The resulting
normalized accelerance function, normalized autopower force spectrum and coherence
plots of left and right railseat for timber and concrete railseats are shown within
following sections.
4.1
Timber Railseats in Nobber
The plotted results for timber sleeper in Nobber differ for both railseats (see Fig. 6).
Plotted force auto power spectrum shows a frequency roll of 130 dB for the right
railseat (up to frequency of 450 Hz), whereas for the left railseat the roll of is 150 dB
(up to frequency of 350 Hz).
Generally both railseats are dampened in the lower frequency range of interest. In
the range of force spectrum frequency roll of, accelerance function for right railseat is
more dampened with very wide frequency bandwidth of almost 250 Hz. For this
railseat, no noticeable peaks can be detected under 300 Hz. Regards the left railseat, for
the force frequency roll of, one small peak can be detected close to 100 Hz, the curve
then continues with highly dampened response from 120 to 230 Hz and ends with a
third noticeable peak at 280 Hz. Plotted accelerance magnitudes were higher for right
railseat throughout the whole frequency spectrum.
Most of the signal in plotted accelerances is due to applied impact what is shown by
coherence plots for both railseats. These can be seen from values of obtained
Fig. 5. Testing of concrete and timber railseats.
732
E. Balic and C. McNally

coherence, where for good signal correlation coherence is 1, whereas for bad corre-
lation, coherence value goes close to 0.
For higher frequencies after the frequency roll off, for both railseats more dis-
tinctive peaks occurred, but accelerances for higher frequency range are out of scope of
this paper.
Highly damped accelerances for both railseats in lower frequency range can be
related to condition of timber sleeper. Timber sleeper was rotten and an opaque sound
was heard during excitation. Since it was completely immersed in the embankment,
imparted hammer energy dissipated very quickly to supporting embankment. Hence,
free vibration modes couldn’t get excited.
4.2
Timber Railseats in Wellingtonbridge
The plotted results for timber sleeper in Wellingtonbridge differ for both railseats (see
Fig. 7). Plotted force auto power spectrum shows a frequency roll of 120 dB (up to
frequency of 250 Hz) for the right railseat, whereas for the left railseat the roll of is
90 dB (up to frequency of 400 Hz).
In the range of force frequency roll of, accelerance function for right railseat shows
noticeable peaks at 70 and 100 Hz, then a very wide crest continues from 150 to
200 Hz. Regards the left railseat, for the force frequency roll of, several peaks can be
detected, but the most distinguishable with a narrow bandwidth is at 100 and 230 Hz.
Fig. 6. Obtained results for timber railseat in Nobber.
Transient Vibrations of Railway Track Elements
733

Plotted accelerance magnitudes were higher for right railseat throughout the whole
frequency spectrum.
In the lower frequency range, under 250 Hz for right railseat and under 400 Hz for
left railseat, the peaks on accelerance functions show that transient vibration modes of
railseats occur and that the response is not completely dampened by bal-last.
The plotted coherence plots for both railseats show that most of the signal in plotted
accelerances is due to applied impact. These can be seen from values of obtained
coherence, where for good correlation coherence is 1, whereas for bad correlation,
coherence value goes close to 0.
For both railseats, accelerance curves and auto force spectrum became noisier in
frequency range above the frequency roll off. After the frequency roll of, the coherence
values have dropped under 1.0.
4.3
Concrete Railseat in Wellingtonbridge
The plotted results for concrete sleeper in Wellingtonbridge differ for both railseats (see
Fig. 8).
Plotted force auto power spectrum for right railseat can be divided into three
regions according to frequency roll of, i.e. ﬁrst region of 75 dB roll of (up to 300 Hz);
second region of 50 dB roll of (from 300 to 730 Hz); and third region that corresponds
to noise after 730 Hz. Similarly, plotted force autospectrum for left railseat can be
Fig. 7. Obtained results for timber railseat in Wellingtonbridge.
734
E. Balic and C. McNally

divided in three regions: ﬁrst region of 90 dB roll off (up to frequency of 225 Hz);
second region of 50 dB roll of (from 225 to 650 Hz); and third region that corresponds
to noise (frequencies after 650 Hz).
The plotted accelerance functions within the ﬁrst region of force frequency roll of
are of interest for both railseats, i.e. the frequencies under 200 and 300 Hz. For both
railseats the plotted accelerance functions are extremely dampened where no visible
frequency dominations can be extracted. This shows us that the concrete sleeper is
extremely embedded and supported by the ballast layer that dampens the railseat
vibrations. The dampened railseat vibrations fall in the frequency range corresponding
to sleeper movement on supporting ballast. Plotted accelerance magnitudes were higher
for right railseat throughout the whole frequency spec-trum.
For both railseats accelerance curves became very noisy in the frequency range
after third region of force frequency roll of (above 650 and 730 Hz). These are in
accordance with coherence plot, where coherence values have dropped under 1.0.
5
Conclusions
An analysis of transient vibrations of railway track and its elements is given within the
paper. The ﬁrst part of the paper gives a short review about inﬂuence of ballast stiffness
degradation and lack of ballast support. Emphasis is given to features such as free
Fig. 8. Obtained results for concrete railseat in Wellingtonbridge.
Transient Vibrations of Railway Track Elements
735

vibrations of railway sleeper; displacement, loading and acceleration of sleeper; dis-
placement and loading of rail.
The stiffness degradation or lack of ballast support affects the lower resonant fre-
quencies with greatest reductions. Also, modal displacements are inﬂuenced, where
translational mode shapes can diminish and bending will happen instead. The changes
in frequencies and mode shapes after the ﬁrst ﬂexural mode can be neglected.
For more extreme case of ballast condition, such as complete loss of its support on
sleeper-ballast contact, the track deterioration process is even more enhanced. This has
been seen from increase in main features such as: displacements and loading of
unsupported sleeper; rail displacements and loading; wheel/rail contact force; loading
and displacements of adjacent supported sleeper.
In the second part of the paper acquired in situ accelerance functions of railway
track are shown. These were obtained from three different location of unused railway
lines and they were plotted for left and right railseat position. For the lower frequency
range of interest for all three locations, output signals were correlated to input signal
from the hammer. These can be seen from coherence plots, where coherence value was
ﬂat and equal to 1.0.
For the lower frequency range, the majority of distinctive frequencies are noticed
for the timber railseat on an embankment in Wellingtonbridge. In other words, transient
vibration modes have been excited and the response is not completely dampened by
ballast. On the other hand, the concrete sleeper in Wellingtonbridge and timber sleeper
in Nobber are very well imparted and supported by ballast and embankment materials.
Timber sleeper in Nobber was rotten and an opaque sound was heard during excitation.
Since it was completely immersed in the embankment, imparted hammer energy dis-
sipated very quickly to supporting embankment and free vibration modes couldn’t get
excited. Although the concrete sleeper in Wellingtonbridge was in better condition, its
vibrations were extremely dampened due to good sleeper/ballast contact. Hence,
supporting ballast layer dampened the railseat vibrations in frequency range that cor-
responds sleeper movement on supporting ballast.
Knowledge about how the track features vary with support condition is extremely
beneﬁcial in assessment of track integrity. This paper gives a contribution to studies of
changes of transient vibration of railway track due to varying support condition. If
affected track features could be measured then track sections with poor support could
be identiﬁed and localized more easily. All of this puts forward the need to ensure
constant monitoring and adequate maintenance of railway track so that occurrence of
track with poor support is suppressed.
Acknowledgements. The research presented in this paper was carried out as part of the Marie
Curie Initial Training Network (ITN) action FP7-PEOPLE-2013-ITN. The project has received
funding from the European Union’s Seventh Framework Programme for research, technological
development and demonstration under grant agreement number 607524. The authors also express
gratitude to Irish Rail (Iarnród Éireann) for their cooperation and providing access to railway site.
736
E. Balic and C. McNally

References
1. Arlaud, E., D’Aguiar, S.C., Balmes, E.: Receptance of railway tracks at low frequency:
numerical and experimental approaches. Transp. Geotech. 9, 1–16 (2016)
2. Kaewunruen, S., Remennikov, A.M.: Field trials for dynamic characteristics of railway track
and its components using impact excitation technique. NDT E Int. 40(7), 510–519 (2007)
3. Grassie, S., Cox S.: The dynamic response of railway track with unsupported sleepers. Proc.
Inst. Mech. Eng. Part D J. Automobile Eng. 199(2), 123–136 (1985)
4. Berggren, E.G., Kaynia, A.M., Dehlbom, B.: Identiﬁcation of substructure properties of
railway tracks by dynamic stiffness measurements and simulations. J. Sound Vib. 329(19),
3999–4016 (2010)
5. Zhu, J., Thompson, D., Jones, C.: On the effect of unsupported sleepers on the dynamic
behaviour of a railway track. Veh. Syst. Dyn. 49(9), 1389–1408 (2011)
6. Thompson II H., Sussmann, Jr T., Stark, T., Wilk S.: Non-invasive monitoring of track
system gaps. In: Proceedings of railway engineering-2015 conference, Center JAVT (2015)
7. Balic, E., McNally, C.: Assessment of railway substructure using impact hammer testing. In:
Pombo, J. (ed.) In: 3rd International conference on railway technology: research,
development and maintenance. Stirlingshire, Cagliari (2016)
8. Kaewunruen, S., Remennikov, A.M.: Effect of improper ballast packing/tamping on
dynamic behaviors of on-track railway concrete sleeper. Int. J. Struct. Stab. Dyn. 7(01), 167–
177 (2007)
9. Lam H, Wong M. Railway ballast diagnose through impact hammer test. Procedia Eng. 14,
185–194 (2011)
10. Lam, H.F., Wong, M., Yang, Y.: A feasibility study on railway ballast damage detection
utilizing measured vibration of in situ concrete sleeper. Eng. Struct. 45, 284–298 (2012)
11. Kaewunruen, S., Remennikov, A.M.: Experimental determination of the effect of wet/dry
ballast on dynamic railway sleeper/ballast interaction. J. Test. Eval. 36(4), 1–4 (2008)
12. Hu, Q.: Bayesian ballast damage detection in consideration of uncertainties from
measurement noise and modelling error. Ph.D. thesis, City University of Hong Kong
(2015). Hong Kong, China
13. Zakeri, J.A., Abbasi, R.: Field investigation of variation of loading pattern of concrete
sleeper due to ballast sandy contamination in sandy desert areas. J. Mech. Sci. Technol. 26
(12), 3885–3892 (2012)
14. Rezaei, E.: Vibrations of partly supported concrete railway sleeper. MSc thesis, Linkoping
University (2010). Linkoping, Sweeden
15. Rezaei, E., Dahlberg, T.: Dynamic behaviour of an in situ partially supported concrete
railway sleeper. Proc. Inst. Mech. Eng. Part F J. Rail Rapid Transit. 225(5), 501–508 (2011)
16. Kaewunruen, S., Remennikov, A.: Investigation of free vibrations of voided concrete
sleepers in railway track system. Proc. Inst. Mech. Eng. Part F J. Rail Rapid Transit. 221(4),
495–507 (2007)
17. Wilk, S.T., Stark, T.D., Rose, J.G.: Evaluating tie support at railway bridge transitions. Proc.
Inst. Mech. Eng. Part F J. Rail Rapid Transit. 230(4), 1336–1350 (2016)
18. Shi, J., Chan, A.H., Burrow, M.P.: Inﬂuence of unsupported sleepers on the dynamic
response of a heavy haul railway embankment. Proc. Inst. Mech. Eng. Part F J. Rail Rapid
Transit. 227(6), 657–667 (2013)
19. Recuero, A., Escalona, J., Shabana, A.: Finite-element analysis of unsupported sleepers
using three-dimensional wheel and rail contact formulation. Proc. Inst. Mech. Eng. Part K
J. Multibody Dyn. 225(2), 153–165 (2011)
Transient Vibrations of Railway Track Elements
737

20. Zhang, S., Xiao, X., Wen, Z., Jin, X.: Effect of unsupported sleepers on wheel/rail normal
load. Soil Dyn. Earthq. Eng. 28(8), 662–673 (2008)
21. Mosayebi, S.-A., Zakeri, J.A., Esmaeili, M.: Effects of train bogie patterns on the mechanical
performance of ballasted railway tracks with unsupported sleepers. Proc. Inst. Mech. Eng.
Part F J. Rail Rapid Transit. 0954409716664932 (2016)
22. Ishida, M., Akiko, K., Ying, J.: Inﬂuence of loose sleeper on track dynamics and bending
fatigue of rail welds. Q. Rep. RTRI 40(2), 80–85 (1999)
23. Zakeri, J.A., Fattahi, M., Ghanimoghadam, M.M.: Inﬂuence of unsupported and partially
supported sleepers on dynamic responses of train and track interaction. J. Mech. Sci.
Technol. 29(6), 2289–2295 (2015)
24. Stark, T., Wilk, S., Thompson II H., Sussmann, Jr T.: Effect of unsupported ties at transition
zones. In: Proceedings of railway engineering-2015 conference (2015)
25. Lundqvist, A., Dahlberg, T.: Load impact on railway track due to unsupported sleepers.
Proc. Inst. Mech. Eng. Part F J. Rail Rapid Transit. 219(2), 67–77 (2005)
26. De Man, A.: Determination of dynamic track properties by means of excitation hammer
testing. Rail Eng. Int. 25(4) (1996)
27. Kaewunruen, S., Remennikov, A.: Experimental simulation of the railway ballast by resilient
materials and its veriﬁcation by modal testing. Exp. Tech. 32(4), 29–35 (2008)
28. Balic, E., Hester, D., McNally, C.: Impact hammer testing of a railway track. In: Proceedings
of bearing capacity of roads, railways and airﬁelds BCRRA, Athens (2017)
29. Sadeghi, J.: Field investigation on dynamics of railway track pre-stressed concrete sleepers.
Adv. Struct. Eng. 13(1), 139–151 (2010)
30. Oregui, M., Molodova, M., Nunez, A., Dollevoet, R., Li, Z.: Experimental investigation into
the condition of insulated rail joints by impact excitation. Exp. Mech. 55(9), 1597–1612
(2015)
31. Geotechnical investigative report. Gavin & Doherty Geosolutions Ltd (2015)
32. Geological Survey of Ireland (2016). https://www.gsi.ie/
33. Signal Processing with Matlab. In: MathWorks I, MathWorks Inc, Pamphlet (2014)
34. Avitabile, P.: Experimental modal analysis. Sound vib. 35(1), 20–31 (2001)
35. Ewins, D.: Modal testing: theory, practice and application (mechanical engineering research
studies: engineering dynamics series) (2003)
36. Dossing, O.: Structural testing, part 2 modal analysis and simulation. In: Kjær, B. (eds.)
Brüel & Kjær, Denmark, pamphlet (1988)
738
E. Balic and C. McNally

Part V
Information and Communication
Technologies

Smart Parking System Based on Arduino SD
Card Ajax Web Server
Edin Mujčić(&), Una Drakulić, and Merisa Škrgić
Faculty of Technical Engineering Bihać, University of Bihać, Bihać, Bosnia and
Herzegovina
{edin.mujcic,merisa.skrgic}@gmail.com,
una.94@hotmail.com
Abstract. The modern age implies a vehicle per capita which leads to many
problems in trafﬁc. Some of the problems of increased number of vehicles on the
road are: an increase of trafﬁc accidents, impeded trafﬁc, large losses of time in
ﬁnding a suitable place for parking vehicles, etc. This paper is based on problem
of ﬁnding a suitable place for parking the vehicle because a waste of time is one
of the things that modern people cannot afford. For large parking areas, such as
huge shopping centers, drivers always hardly ﬁnd a free parking place, espe-
cially during peak days such as holidays and free working days. This paper
describes the smart parking system which is based on two technologies, Arduino
and Ajax, i.e. control using a web server. This system uses ultrasonic sensors
HC-SR04 which are connected to the Arduino board and which are used to ﬁnd
the free and taken parking places in the parking area. The characteristics of
smart parking system include detection of free parking places, reservation and
display of available parking places. The goal is to reduce the losing of time for
divers while searching for free parking place and reducing the ﬂow of trafﬁc in
the parking lots.
Keywords: Smart parking system  Ultrasonic sensor HC-SR04  Arduino
Ajax  Detection  Microcontroller
1
Introduction
Trafﬁc crowds, caused by too many vehicles are a huge problem in all major cities.
Almost every cosmopolitan city in the world suffers from trafﬁc congestion, which
causes drivers frustration especially when searching for a parking place. Solving such a
problem or even trying to alleviate it will certainly offer several beneﬁts, such as
reducing drivers’ frustration and stress by saving time and fuel, and reducing gas
emissions, which in turn, will affect levels of pollution [1].
The problem of parking vehicles grows exponentially with the increasing number
of vehicles. Searching allowed parking place for most drivers is very difﬁcult to ﬁnd
especially during the striking hours in day.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_64

The term “smart parking” is broadly deﬁned as the integration of technologies to
streamline the parking process—from dynamic place availability information to sim-
pliﬁed payment methods [2].
Smart parking system has the purpose of facilitate and speed up parking vehicles for
drivers to designated places, i.e. specially constructed parking or closed garage which
also can be part of the parking system.
The current smart parking systems only obtain the availability information of
parking places from deployed sensor networks, and simply publish the parking
information to direct drivers. However, since these systems cannot guide the drivers to
their desired parking destinations, even sometimes make the situation worse, they are
not “smart” enough [3].
In urban areas, one-third of cars which have reached their destination and are
circling around looking for a parking place thus leading to problems like pollution and
trafﬁc congestion. In a recent survey, researchers have found that in one year, car
cruising for parking created the equivalent of 38 times trips around the world, burning
1.7 litre of fuel and producing 730 tons of CO2. So, it is essential to control the air
pollution using a robust parking system that will be used for the reservation of parking
spot as well as parking spot allocation in on spot resource allocation scenario.
Reservation can be made using multiplatform tools such as Android application, iOS
application, Windows applications or even Web Portal allowing user to have a hassle
free parking experience. Recommender system will help user ﬁnd the right parking lot
closest to his location [4].
In this paper we design and implement the prototype of smart parking system using
Arduino SD card Ajax Web server. In paper is explained how to set up an Arduino with
Ethernet shield to work as a web server.
2
Smart Parking System
Smart parking system helps drivers ﬁnd a free parking place for their vehicles. Using a
variety of methods to detect whether there is a parked car, i.e. signal drivers for the
available parking place.
2.1
The Methods Used in Smart Parking System
Depending on the purpose and scope of the project for parking there are two types of
methods used in smart parking system. The ﬁrst method is Vision-Based Method that
uses so-called Closed—Circuit Television or CCTV. It has a camera that is responsible
for more than one parking place and image editing software that will detect the status of
the parking place. The second method is Sensor–Based Method and uses a single
sensor for each parking place [5]. Various factors affect the choice of types of sensors.
These factors are: the size of the parking, the reliability of sensors, adapting to changes
in the environment and price [6]. Sensors can be divided into Intrusive Sensors and
742
E. Mujčić et al.

Non-intrusive Sensors. Intrusive sensors must be installed directly on the surface of the
road. These sensors include magnetometers, pneumatic tubes, inductive sensors,
piezo-electric cables and others. Non-intrusive sensors only require attaching on the
ceiling or on the ground. The infrared and ultrasonic sensors are categorized as
non-intrusive sensors [7]. Ultrasonic sensors can be used for counting vehicles and
assessment of availability of each parking place [8]. The advantages of ultrasonic
sensors are low cost and easy installation. Disadvantage of ultrasonic sensor is sensitive
to temperature changes and strong air turbulence. This paper is used ultrasonic sensor
HC-SR04.
2.1.1
Ultrasonic Distance Sensor HC-SR04
Ultrasonic sensors, known as sonars, belong to group of sensors for measuring dis-
tances, and is shown in Fig. 1.
They are used to measure the distance based on the velocity of propagation of
ultrasonic waves through the air. The working principle of these sensors as shown in
Fig. 2 is based on the reﬂection of sound wave from the object to which it encounters.
The transmitter emits fast, short ultrasonic signal and measures the time that elapses
from of sending up to receiving ultrasonic signals [9].
Fig. 1. Ultrasonic distance sensor HC-SR04
Smart Parking System Based on Arduino SD Card
743

Ultrasonic sensors for measuring distances are used to measure the distance
between the sensor and the object that is in front of him. These sensors have good
precision and easy to use. Ultrasonic sensors allows non-contact measurement of the
distance in the range from 2 to 400 cm with precision measurements of about 3 mm in
optimal conditions. The ultrasonic ranging module HC-SR04 consists of an ultrasonic
transmitter, an ultrasonic receiver and control electronics.
To calculate the distance it is necessary to measure the length of the return pulse.
Distance is obtained by the equation [9]:
d ¼ T  v
2
ð2:1:Þ
where is: d—distance, T—the return pulse duration in seconds, v—speed of sound
340 m/s.
In Table 1 are listed the parameters of ultrasonic sensor HC-SR04. Beneﬁts for
which is very often used are simplicity, low weight, low power consumption, low cost
and excellent properties at avoiding obstacles. The biggest disadvantages are large
beam width, low speed of sound, attenuation, large minimum distance measurements,
limited the maximum distance, specular reﬂection, etc. [9].
Fig. 2. Working principle for ultrasonic distance sensor HC-SR04
Table 1. The parameters of the ultrasonic sensor HC-SR04 [10]
Operating voltage
5 V
Min range
2 cm
Operating current
15 mA
Measuring angle
15º
Operating frequency
40 kHz
The control pulse
10 us
Max range
4 m
Echo signal
The pulse length is proportional to
the distance
744
E. Mujčić et al.

2.2
The Types of Smart Parking Systems
Currently in the world are several types of smart parking systems. Parking systems are
implemented in an open place (such as shopping centers) and indoors (such as
underground garages) and thus the availability of places can be determined before
entering the garage or parking. These types of systems reduce trafﬁc and air pollution,
reduce the time required to search for a free parking place. The next part of the paper
describes the types of parking systems.
Types of smart parking system are:
1. Parking guidance system
2. Smart payment system
3. E-parking system
4. Automated parking system
5. Transit-based system
Parking guidance system is designed to provide parking guidance in real time.
Disadvantage of parking guidance system is that at the same time more drivers go to
the same accessible parking place, thus creating congestion on arrival and use of
parking places becomes unbalanced [11].
Smart payment system allows for quick and convenient payment. They are used all
contact methods (debit, credit card), contactless methods (smart cards, RFID cards) and
mobile devices for communication parking system and the user. When driver pay
parking place on the Internet, he reserve a parking place in the parking lot, and that
parking place is than reserved. The modern way of parking is done via smart phones,
when user send a message to reserve a parking place and make parking payments [12].
E-parking uses advanced methods for easier parking reservations. The driver can
inquire about the availability of reserved parking place at a given destination and pay
on the way out. The system is accessed via mobile phones or the Internet [13].
Automated parking system represents mechanical computer-controlled system that
allows users to operate their vehicles. Automated parking system allows efﬁcient use of
expensive and limited parking place. The advantage of the automated parking system is
the efﬁcacy and safety of vehicles. Using visual navigation is carried out strict control
over the movement of vehicles across the parking lot. It is important that the ﬂow of
vehicles divided into departure and arrival. The driver monitors all parking rules,
monitors the work at the entrance, billing and so on. The essential characteristics of the
automatic counting of vehicles, various identiﬁcation methods, ﬂexible system of tariffs
for different users, removing the possibility of theft, control of entry and exit of
vehicles [14].
Transit-based systems provides information on parking lots and public transport
timetable. The main purpose is to encourage drivers to park their vehicles and use buses
or trains for transport. This in turn will reduce trafﬁc congestion, pollution and fuel
consumption [15]. These types of parking systems make it easier for drivers to ﬁnd a
parking place, and each in its own way has good and bad sides. The logic for smart
parking is essential for optimization and better development of the project, starting
from the programming sequence. The basis of optimizing the programming scenarios
park with a view of the display panel, entering the parking lot, parking, automatic
Smart Parking System Based on Arduino SD Card
745

change of information about the owner of the place at all times and exiting the parking
lot. Flowchart event for parking is reﬂected in the programming code on the physical
platform such as the Arduino development environment, used in this paper.
3
Designing Smart Parking System
This chapter describes design of smart parking system. Smart parking system is crated
using two Arduino boards with microcontroller ATMega2560, which are programmed
in Arduino IDE. On one Arduino board is connected an Ethernet shield that is used to
establish a connection to the web server. On second Arduino board are connected
electronic components used in designing smart parking system: 12x LEDs, 7x ultra-
sonic sensors HC-SR04, servo motor and switch button. Figure 3 shows the block
diagram of Web based parking system.
From Fig. 3 we see that users are using the global network of Internet to access
server located on SD card in ethernet shield. For programming, powering and pro-
cessing received signals using an ethernet shield is used an Arduino board with an
ATMega2560 microcontroller (see Fig. 4). Once the connection has been established,
on the created website the user can see which place on the parking lot is occupied and
which is not. The user selects a free parking place and reserves it. A reserved place for
all users is displayed as busy (on the website and on the parking display).
Users who do not make parking reservations using the Web server come to the
parking lot where at the entrance they have an overview of busy, reserved and available
parking places. Parking on one of the available parking places the place becomes busy.
Functional block diagram of designed parking system is shown in Fig. 4.
Fig. 3. Block diagram of web based parking system
746
E. Mujčić et al.

As mentioned above, the ehternet shile is used together with the Arduino board
with microcontroller Atmega2560. Programming the web server is done using HTML,
CSS, JavaScript, Ajax. Because of the size of the memory and the number of different
ways to access the web server is placed on the SD card. Due to the speed of access from
different locations, it is desirable to make the website as simple as possible with a small
amount of data. For parking control is used an additional Arduino board with
Atmega2560 microcontroller. Arduino board is controling ultrasonic HC-06 sensors
that detect vehicle presence on the parking lot. Ultrasonic sensors transmit an ultrasonic
signal, generated by using a microcontroller, which is repelled from the barrier if it
exists. The rejection signal is applied to the Arduino board using an ultrasonic sensor.
In the microcontroller, the received signal is processed and forwarded to the display
and to the ehternet shild with the Arduino board with the Atmega2560 microcontroller.
This gives users a complete insight in the state of the smart parking.
Testing the designed smart parking system
Testing designed smart parking system described is by example from the arrival the
vehicle, parking the vehicle to the time when the vehicle leaves the parking lot, as
shown in Fig. 5.
Fig. 4. Functional block diagram of designed system
Smart Parking System Based on Arduino SD Card
747

In Fig. 5a driver comes to parking lot. When the driver decides to park in the
parking lot ﬁrst driver sees a display board which consists of label for parking places
and LEDs, red and green, which indicate the driver status of parking places. If place is
available, the driver moves the parking lot and encounters a ramp that automatically
raises.
After driver found a parking place which is free for the vehicle needs to be parked
in the appropriate parking place. When the vehicle is parked, the sensor will detect that
parking place is no longer free and then will change the situation on the display board,
Fig. 5b. When the driver decides to leave the parking lot, it is necessary to remove the
vehicle from the parking place and to move towards the exit of the parking lot where
the ramp rises after push on button. On the other hand, the website is a real-time view
of activity in the parking lot, i.e. display status for parking places, shown in Fig. 6.
Fig. 5. Testing the designed system
748
E. Mujčić et al.

Figure 6a shows the state of parking place at the time described in Fig. 6b when the
vehicle enters the parking No.5. Figure 6 shows the state of parking place, i.e. changes
in the display board. The web server is also enable to reserve parking place so that on
the display board the red LED turn on for a reserved parking place.
4
Conclusion
In this paper is described way to design smart parking system using Arduino board with
ATMega2560 microcontroller attached Ethernet shield with SD card. Control of smart
parking system, turning on diodes depending on the state of HCSR04 sensor, is con-
trolled by Web server. Based on the signiﬁcantly shorten time for ﬁnd a free parking
place that is enabled from the view on the panel for free and busy parking, we conclude
that the proposed smart parking system can alleviate trafﬁc congestion caused parking
searching and reduce the amount of trafﬁc volume searching for parking.
References
1. Muftah, F., Fernstrom M.: Investigation of smart parking systems and their technologies. In:
Thirty Seventh International Conference on Information Systems, Dublin 2016 (View
30.07.2015)
2. Halleman, B.: Europe’s space program (parking space, naturally). Trafﬁc Technology
International, February/March, pp. 46–4 (2003) (View 30.07.2015)
3. Wang, H., He, W.: A reservation-based smart parking system. In: Computer Communica-
tions
Workshops
(INFOCOM
WKSHPS),
2011
IEEE
Conference
on,
Shanghai,
pp. 690– 695 (2011). https://doi.org/10.1109/INFCOMW.2011.5928901 (View 30.07.2015)
4. Faiz Ibrahim, S., Patrik Nirnay, J., Saideep Pradeep, B., Omkar Pradip, K., Nikhilkumar
Shardoor B.: Smart parking system based on embedded system and sensor network. Int.
J. Comput. Appl. (2016) (View 30.07.2015)
Fig. 6. Testing the designed smart parking system: a Web server, b designed system
Smart Parking System Based on Arduino SD Card
749

5. Yamada, K, Mizuno, M.: A vehicle parking detection method using image segmentation,
Electron. Commun. Jpn. Part III Fundam. Electron. Sci. (2001) (View 30.07.2015)
6. Masaki, I.: Machine-vision systems for intelligent transportation systems. IEEE Intell. Syst.
Appl. (1998) (View 30.07.2015)
7. Scheeling, J.: Car Park Monitoring System. University of Queensland (2002) (View
30.07.2015)
8. Mimbela, L.E.Y., Klein, L.A.: A Summary of Vehicle Detection and Surveillance
Technologies used in Intelligent Transportation Systems, Southwest Technology Develop-
ment Institute (SWTDI) at New Mexico State University (NMSU) (2000) (View 30.07.2015)
9. http://www.lejla-bm.com.ba/PDS Robotika/MR_1_Senzorika_Mob_robotika_s.pdf (View
25.07.2015)
10. http://www.saperel.com/HC-SR04%20Senzor%20razdaljine (View 26.07.2015)
11. https://en.wikipedia.org/wiki/Parking_guidance_and_information (View 30.07.2015)
12. http://sec.cs.ucl.ac.uk/users/smurdoch/talks/ccc08tamper. (View 30.07.2015)
13. http://www.essi.co.in/e-parking.aspx (View 30.07.2015)
14. http://olympiad.ibu.edu.ba/assets/userﬁles/members/pdf/ensarvlahovljak-bosnian-science-
olympiad.pdf (View 30.07.2015)
15. http://tsrc.berkeley.edu/sites/default/ﬁles/UCD-ITS-RR-06-19.pdf (View 30.07.2015)
16. https://startingelectronics.org/tutorials/arduino/ethernet-shield-web-server-tutorial/ethernet-
shield-tutorial-summary/ (View 30.07.2015)
750
E. Mujčić et al.

Implementation of Audiological Measurements
at Persons with Hearing Impairment
Ferid Soﬁtć and Jasna Čošabić(&)
International University Travnik, Travnik, Bosnia and Herzegovina
feridsofticbl@gmail.com, jasnacosabic@live.com
Abstract. The paper presents the processing of audiological measurements
with the corrective characteristics at persons with hearing impairment. Char-
acteristic points on frequency axes are especially analysed by using a special
software by manufacturers of hearing aids. Apart from analyses of audiological
measurements, the psychological effect on users by the corrections has been
evaluated as well. The level of correction is determined by audiological meth-
ods, matematical methods and electronic hearing aids. A special accent has been
put on children in school environment.
Keywords: Hearing Audiogram Frequency Characteristic Implementation
1
Introduction
For persons with hearing impairment, it is neccessary to enable the audibility of sound
ﬁles with preservation of fundamental articulation. During sound transmission a person
receives the information on objects that are out of his reach. Frequency range which is
signiﬁcant for human, entails three decades. Information on sound pressure differ from
the conditions of real hearing and do not make difference between the use of various
types of hearing aids: behind-the-ear, in-the-ear or in-the-canal. Each modelling of
change of pressure p(t) in hearing range includes the determination of dependance of
pressure in the range that entails space coordinates p(x, y, z) in a constant time moment.
The methods of hearing examination may be divided into subjective and objective
ones. The characteristics of hearing at the level of subjective impression (so-called
subjective psychological or biophysical characteristic of sound) as loudness, tone pitch
and timbre equally correspond to objective values: volume, frequency of spectral
components. The result of treatment by otorhinolaryngological proceedings would be a
facilitated hearing in personal and work surroundings. Communication without the use
of hearing aid in everyday environment facilitates a psychological attitude in inter-
communication and coordination.
Social protection is essential when speaking of persons with hearing impairment as
voulnerable groups, and should be addressed when coping with any form of social
exclusion [1]. Among the most signiﬁcant population are the children of school age. If
the cure has not been achieved, then the hearing aids are inevitable. By early and proper
approach in children with hearing problems, and with active work, possibilities for
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_65

adequate growth and development of children are being created, so they could integrate
into everyday life. The nowdays education model through inclusion is also one of the
possibilities for a normal development.
2
Audiogram Modelling
For mild defect the threshold of hearing is at the level lower then 20 [dB], medium
defect carry the threshold of hearing from 25 to 40 [dB], while severe deffects up to 60
[dB]. Hearing impairment of 80–90 [dB] is considered as complete deafness [2–5]. In
order that deffects are compensated, it is neccessary that corrected frequency charac-
teristic has such a form of frequency dependance of sound ampliﬁcation that, together
with resonant characteristic of dependency of ampliﬁcation it should reach a decibel
level which is neccessary for normal listening. Frequency range for persons with
hearing impairment, the ear of whom feels important changes of pressure as a sound, is
between 200 [Hz] and 8 [kHz] (Fig. 1). For persons with hearing impairment, diag-
nostic is made by audometres by determining the lower limit of auditory ﬁeld
depending on the frequency and intensity of sound. On the grounds of such recorded
tonal audiogram of a person with hearing impairment, a curve of targeted ampliﬁcation
is projected.
As a result of audiometrics, the deviation of auditory limit from standard values is
being established. Audiogram is presented by separate diagrams for left and right ear.
One of diagnostically processed audiograms from the audiogram database is presented
on Fig. 2.
For projecting of corrective characteristic, the neccessary ampliﬁcation is being
calculated using the mathematical model. Mathematical modeling of level of hearing
limit of sound in [dB] in the function of frequency f was presented as L(f) in [3–5].
Computer programmes for modelling and recording of audiograms are based on
determination of lines of isoperception. Coefﬁcients of polynomic modelling depend on
subjective perception and are modelled according to subjective perception.
Fig. 1. Frequency characteristic of lower limit of auditory ﬁeld for two persons
752
F. Soﬁtć and J. Čošabić

An audiologist uses additional standardized targeted curves of ampliﬁcation in
order to reach the neccessary level of ampliﬁcation correction in [dB]. A range of
corrective relations such as NAL-R (The National Acoustic Laboratories) and Pogo II
(Prescription of Gain/Output) correction level.
3
Corrections of Individual Frequency Ear Characteristic
In medicine praxis it is well known that an additional correction of ampliﬁcation is
needed [3–5]. In everyday communication out of personal environment, persons with
hearing impairment use a hearing aid, but with the same frequency characteristic which
is used in programmed recording of music or speach signal. The equation for modelling
of human ear is very complex, so the use of modelling by electronic circuits is an
advantage, because it may point to the connection in a modelled system.
With the use of package MATLAB R14 the mathematical dependance is being
analysed. Values are determined by using:
• matrix f ¼ ½f1; f2; . . .fn for display of observed values;
• matrix L ¼ ½L1; L2; . . .Ln for display of measured values.
Using the matrix of frequency F and sound intensity L, the polynom approximation is
being applied by using the command polyﬁt as follows:
p ¼ polyfitðF; L; nÞ
ð1Þ
where:
p ¼ an; an1; . . .a1; a0
½

matrix representing coefﬁcients of polynom of line n, by which the modelling of
characteristic is performed.
Fig. 2. Example of a real audiogramme
Implementation of Audiological Measurements
753

The equation for modelling sound intensity L is:
Lðf Þ ¼
X
n
i¼1
Y
k6¼i
ðf  fkÞ
ðfi  f kÞ  Li
ð2Þ
For computer analyses Sound Forgeis being used, as well as Audacity® 1.3. The
quality of recording adjustment depends on the choice of speed of measurement
(Quality-Sampling-Sample Rate). Frequency of measurement for a good quality of
music signal is 44100 [Hz], while for the measurement of speach signal value of 8000
[Hz] to 16 [kHz] is sufﬁcient.
Important information for an audiologist is obtained by analyses of recorded
spectral characteristic which shows levels of signal ampliﬁcation depending upon the
frequency which directly shows the success in hearing correction. Such a processing of
a sound ﬁle (*.wav) in a domain of ampliﬁcation, enables changing of signal level
from –N to +N [dB] [6–8] (Fig. 3).
4
Experimental Results
Very often, it is insisted that hearing aids of various manufacturers achieve the greatest
possible ampliﬁcation within certain range of frequency, which, certainly is not the
only feature of a good adjustment. The articulation, because of psychological effect,
must be within the allowed range. An important information for an audiologist is the
analyses of recorded spectral characteristic. It is convenient to use the family of
characteristics of transfer function of ear model. Graphic equilizer at available pro-
grammes is analogous to adjustment of subrange with a special software Connex.
Therefore the picture 4 presents the position of the slider at frequency subrange of
graphic equilizer at individual frequency ear characteristic with hearing impairment.
Results of application of equilizer are obtained on the grounds of audiological
measurments which determine the form of convenient characteristic of hearing aid. On
Fig. 3. Spectral characteristic
754
F. Soﬁtć and J. Čošabić

the grounds of measurement, results of several patients with standard hearing
impairment, range of neccessary ampliﬁcation of signal is from 10 [dB] at frequency of
200 [Hz] to 25 [dB] at frequency of 1 [kHz] [4–8] (Fig. 4).
Time forms of signals directly show relative values of weakened and corrected
signal. Original signal and weakened signal and corrected signal in a time range are
presented at Fig. 5.
Good results in the range from moderate impairment to more severe deafness are
achieved for the range of neccessary ampliﬁcation up to 27 [dB]. Having in mind that
deviation of individual frequency characteristic of ear up to −10 [dB] comparing to an
ideal curve is being tolerated, it may be concluded that satisfactory range of ampliﬁ-
cation is 17 [dB].
Fig. 4. Equilizers on hearing aids according to Connex
Implementation of Audiological Measurements
755

5
Application of Aids and Psychological Effects to User
The problem of audibility and its acceptance is the most prominent at youth. It is
considered that the earlier the hearing impairment occurs in the child’s life, the more
serious the effects on the child’s development, which may be to a great extent elevated
by earlier identiﬁcation of the problem [9].
Integration into society and their accomodation to intercommunication is a very
complex process. In this process a great role is attributed to experts in this area, parents
and educators, depending on the child age. Problem of integration of children with
development impairment in the process of education in BaH is a very pronounced. The
research has shown that prevailing opinion is that these children should attend special
institutions for educcation with precisely deﬁned curricula, under the supervision of
professional staff [10]. However, there is an issue of social integration of persons who
spent signiﬁcant part of their life in some sort of isolation. In order to aviod such
problems, efforts are made nowdays towards inclusion, and integration of children with
impairment to regular education process. The same happens with children who have
hearing impairment. The sense of hearing is very important for proper growing up. The
application of proper hearing aids, enables them to grow up together with their peers.
However, a lot of work in that regard is inevitable. Apart from regular education, work
on speech development and communication in general is important, and it is performed
in specialised institutions. One should have in mind that there are no two children with
the same result obtained.
Fig. 5. Original, weakened and corrected signal in a time range
756
F. Soﬁtć and J. Čošabić

Therefore, apart from being focused solely to provision of devices, the overall
process of addressing the hearing impairment issue should focus also to social inclusion
of people with disabilities, in order that social equity and participation is enabled [11].
Isolation or any kind of exclusion should be avoided, as well as their perception of
being different. Children with signiﬁcant hearing impairment often report feeling iso-
lated, friendless and unappy in school. Communication difﬁculties often leading to
social isolation and poor self concept [9]. Certainly, due to inability to hear and study in
the envoronment where they live, special forms of behavior are possible. Nowadays,
work with children who have hearing problem is at a great extent facilitated from the
educator point of view. Education of teachers to work with children with hearing
impairment is practically not existing. One of adequate solutions is a good computer
literacy. It is clear that teaching staff must think wider then the average everyday tasks,
must cooperate actively at all ﬁelds, to be team workers with the aim of development of
child that has hearing imparment. Strict grammar control, experssion in writing and
patience when communicating are some of the solutions. With the application of
speciﬁc software packages it is possible to more easily engage in education and work
with children.
Proper teaching methods open a possibility for the whole group to be integrated,
and they are a signiﬁcant factor in development of other children, both in a sphere of
accepting diversities and cooperation and team work sense. With the adequate
approach towards children, a constructive environment for everone’s work is created,
unlike
destructive
environment
which
may
boost
inferirority
complexes
and
overexclusiveness.
In order that a hearing problem is partially solved, hearing aids are being used.
Hearing aids also have direct impact to associated psychological disturbances [11, 12].
There are various types of hearing aids: behind-the-ear, in-the-ear, and in-the-canal. In
the experimental analyse, hearing aid Prisma 2 was used, and settings in frequency
subranges of measurmenets were done according to special software Connex. After
recording of audiogramms, a correctional curve of ampliﬁcation is being recorded to
the hearing aid.
6
Conclusion
Results of measurments and experiments are presented in a graphic form. Mathematical
models are applied, by which analysing of greater number of parameters is enabled. By
recording an audiogram, a curve of ampliﬁcation is being recorded to the hearing aid.
Thus, audibility with a certain degree of articulation is enabled. Spectral characteristic
shows levels of signal ampliﬁcation depending upon the frequency which directly
shows the success in the ear correction. It is convenient to use the family of charac-
teristics of transfer function of ear model. Good results in the range from moderate
impairment to more severe deafness are achieved for the range of necessary ampliﬁ-
cation of up to 27 [dB]. Having in mind that deviation of individual frequency char-
acteristic of up to −10 [dB] comparing to an ideal curve is tolerated, we may conclude
that the satisfactory range of ampliﬁcation is 17 [dB]. A person with hearing impair-
ment is psychologically isolated. One of the ways to deal with that is education
Implementation of Audiological Measurements
757

inclusion. Therefore, persons, and especially developing children with hearing
impairment are enabled to equally participate in everyday life. Active work of experts,
parents, educators is very important and must be present even after the ﬁnalizing of a
formal education. Education process of correct communication may be enriched and
directed towards the whole group attending the class. The richness of inclusion and
learning is not aimed only to a child with a hearing impairment, but also to a child with
no impairment, because it learns to cooperate, to work in team and to accept diversities.
References
1. Rimmerman, A.: Social Inclusion of People with Disabilities. Cambridge University Press,
New York (2013)
2. Annass, S.J., Rassul, M.O.A.: Electronic modeling of human ear. Int. J. Eng. Innov.
Technol. IJEIT, 5(6) (2015). ISSN:2277-3754
3. Xin J: Ear Modeling and Sound Signal Processing. Department of Mathematics and ICES,
University of Texas at Austin, Austin 78712, math.uci.edu/*jxin
4. Softić, F., Bundalo, Z., Blagojević, Ž.: Frequency Correction of Sound Files For Listening
Without Using Hearing Aid Devices. In: 2nd Mediterranean Conference on Embedded
Computing MECO—2013, Budva, Montenegro pp. 266–269
5. Softić, F., Bundalo, Z., Blagojević, Ž., Stjepanović, A.: Listening of sound ﬁles using mobile
phones without hearing aid devices for persons with damaged hearing. In: Proceedings of
International Scientiﬁc Conference ERK2011, pp. 83–86, Portoroz, Slovenia (2011)
6. Softić, F., Bundalo, Z., Blagojević, Ž., Stjepanović, A.: Correction of ear amplitude
characteristic damages causedby natural aging. In: INDEL 2012, Banjaluka (2012)
7. Fastl, H., Zwicker, E.: Psycho-acoustics, Facts and Models. Springer, Heidelberg (2007)
(Amazon.com)
8. Softić, F., Bundalo, Z.: Modeling, analysis and correction of hearing system characteristics.
In: Proceedings of International Scientiﬁc Conference ERK2009, Portorose, Slovenia,
pp. 82–85 (2009)
9. Johnson, C.D., Benson, P.V., Seaton, J.B.: Educational Audiology Handbook. Singular
Thomson Learning, USA (1997)
10. D. Đipa, S. Fazlić, „Znanje, stavovi i iskustva sa djecom sa smetnjama u razvoju, Rezultati
kvantitativnog istraživanja“, Prism Research, Sarajevo, juni 2013. (Internet, access October
2016.)
11. Hogan, A., Phillips, R.: Hearing Impairment and Hearing Disability: Towards a Paradigm
Change in Hearing Services. Routledge Taylor & Francis Group, London and New York
(2016)
12. Stephens, D., Jones, L.: The Effects of Genetic Hearing Impairment in the Family. Wiley,
England (2006)
758
F. Soﬁtć and J. Čošabić

Analysis of Techno-Economic Proﬁtability
on the Example of Construction of an Optical
Suburban Access Network in Srebrenica
Mujo Hohzic(&), Anis Maslo, and Evin Skaljo
BH Telecom Bosnia and Herzegovina, Sarajevo, Bosnia and Herzegovina
{mujo.hodzic,anis.maslo,edvin.skaljo}@bhtelecom.ba
Abstract. The realization of the access network Srebrenica is planned in two
phases. In the ﬁrst phase, the FTTH P2P (Fiber to The Home Point to Point)
network for 400 potential customers was built. Realization of the phase two are
planning to include the extension of the existing network for additional 500
customers. Realization of the phase two depends on the requirements of new
users. In this analyse we have compared the costs of two different technologies
for phase two: FTTH P2P and FTTH PmP—GPON (Fiber to The Home Point to
multi Point-Gigabit Passive Optical Networks). The analysis has shown that
using GPON technology reduces costs by 23% compared to P2P. The advan-
tages of P2P technology on PmP GPON regarding the offered bandwidth per
customer did not come to the fore because the radio link from Sarajevo to
Srebrenica is limited.
1
Introduction
In Srebrenica, there was a built-in FTTH P2P network. The ﬁnal project implemen-
tation was planned in two phases. The ﬁrst phase involved the construction of a FTTH
network with a capacity of 400 ﬁber optics. The second stage envisages the upgrading
of the ﬁrst phase with 500 new ﬁber optics.
The principles for building an access network (AN) used in Europe can not be fully
applied to BiH space [1]. The past war has inﬂuenced the user’s commitment when
choosing a telecom operator. The increase in the consumption of multimedia services
across different media and services deﬁnes broadband access as an essential parameter
of quality of life. The Digital Agenda The EU (Europen Union) sets the strategic
framework for the availability of internet connectivity across the state level. In BiH, the
focus of all telecommunication service providers is focused on densely populated urban
areas. In these areas, smaller infrastructure investments can build a new generation of
telecommunication access network (NG AN FFTH) for a larger number of users.
An analysis of the technical-economic characteristics of the construction of
broadband access networks of the new generation, relates to the analysis of the dif-
ferences in the cost of building urban and rural access networks. Land conﬁguration,
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_66

level of building construction, population density determine the building principles.
They determine, for example, whether underground cable ducts or aerial optical cable
systems are used as a corridor for access to the user. All analyzes show that the
overhead aerial network is cheaper than the underground approximately 20%.
The analysis of economic parameters for constructing the access networks of the
next generation of practical network is divided into three categories.
1. The ﬁrst category are settlements where the broadband access service offers at least
two providers. In these areas there is competition between telecom operators.
2. The other categories are settlements that have only one broadband access provider
that has practically a monopoly in that settlement.
3. The third category of settlement is a settlement that still does not have broadband
Internet access.
Analyzing access networks built over the past twenty years can be ascertained the
following: Infrastructure in urban networks was built using underground cable pipes.
Access networks in suburban areas older than 2006 have an underground pipe only in
the primary part of the access network. They are so constructed that they have
underground cable pipes in the primary part of the access network. Rural poorly
populated settlements have similar access networks as the suburban. It is important to
point out that the practical construction of FTTH access networks in BiH starts with
2015 for all types of settlements. The technology of building an FTTH network PtP or
PmP based on the GPON protocol depends primarily on the requirements of the
investor [2].
2
The Process of Realization of AN Srebrenica Construction
2.1
Existing Condition
The broadband service in BiH is provided by four dominant telecom operators. In the
FBiH dominant are BH Telecom, HT Eronet and partly Telemach, and in the RS, M:
TEL is the dominant operator. Telemach is a cable operator providing only a service of
a ﬁxed triple play service. In addition, there are several smaller alternative cable
operators.
In the city of Srebrenica the existing infrastructure is owned by M: TEL. This is an
old copper access network with a long corded copper wire that can not provide
broadband services. It was built in the 70s and 80s of the past century, as shown in
Fig. 1. This network is devastated and repairs have made many extensions. As such, it
is not suitable for providing broadband services. In addition to this network, an
alternative operator provides cable television based on satellite transmission. The
quality of this service is limited in time, and the speed of Internet trafﬁc can not be
considered like broadband.
760
M. Hohzic et al.

According to the 2015 census, around 15,000 inhabitants are registered in the
Srebrenica area. Many of them live and work abroad or in the Federation of BiH.
3
Construction of FTTH Network Srebrenica
The construction of the new network was realized with the capacity of 400 ﬁber optic
—Phase I-Fig. 1. In case of need it is possible to extend the network to a maximum of
900 ﬁbers. This project is designed to solve the narrow city core that would cover about
1200 residential-business facilities. As there are three TO (Telecom Operators) oper-
ators, the estimated penetration is 30%.
Realization of the FTTH P2P network, project Srebrenica had two segments.
• Installing the connection path
• Installation of the access network.
Building a connecting telecommunication path: The investor did not have a
built-in ﬁber optic link near Srebrenica.
That is why the connection to the core network has generated a radio relay link. The
existing RR link (Sarajevo-Srebrenica) is expanded with the capacity required for the
access network.
Fig. 1. Scheme of existing copper and newly built FTTH networks
Fig. 2. The RR (Radio Relay) link Scheme of Sarajevo-Srebrenica
Analysis of Techno-Economic Proﬁtability on the Example
761

For the part from the RR node to the active node-CO (Central Ofﬁce) link with an
optical cable in length of 300 m. This is presented in Fig. 2.
Building an access network: As far as the industry is not developed, it is estimated
that potential users are not solvent. Therefore, the construction of the access network is
planned in two phases. In the ﬁrst phase, a network of 400 ﬁber optics was built.
If the user requests, then the existing capacity will be extended for the new 500
ﬁbers-Phase 2. However, if we started using PmP-GPON, it would not be necessary to
plug in new optical cables. If the user requests that, then the existing capacity for the
new 500 ﬁbers-Phase 2 will be extended. However, if we started using PmP-GPON, it
would not be necessary to plug in new optical cables. It is enough to mount splitters in
the splice box and thus multiply the network capacity.
Network Concept: From CO to each building there is a CI (Cable Infrastructure)
consisting of 2xﬁ50 mm tubes and PVC or concrete frames—Fig. 3. In each of the
PVC tubes, there are three micro tubes with micro cables that are retracted. Along the
street in the urban part of town are distributive street cabinets-Fig. 4. In the suburban
area (individual objects) instead of street cabinets-Fig. 5, we used boxes with splice
tapes mounted on a wooden pole-Fig. 6.
Fig. 3. Scheme construction of cable sewerage
Fig. 4. Schema of access network elements
762
M. Hohzic et al.

Fig. 5. Street cabinet with splice casstte for distribution
Fig. 6. Woden pole with splice casstte for distribution
Analysis of Techno-Economic Proﬁtability on the Example
763

These users are connected to the network using a self-contained optical ﬁber cable
(air cab). We have chosen this method because topology of ground is unfavorable for
construction works.
On this way we have reduced the costs by about 20% for this part of the access
network and the quality of the total access network has remained unchanged and
equally functional.
3.1
Limiting Factors and Problems in Project Realization
• The ﬁnancial resources planned for this project were limited. Therefore, the scope
of coverage and network capacity is limited.
• Insufﬁcient information on customer’s solvency. All users have taken a service that
requires max 10 Mbit/s.
• Adaptation of project realization according to user’s geographic location. The
number of concentrate with wooden poles increased and the network coverage
increased.
• Using retractable cables and micro cables, with different ﬁber diameters. Difﬁcult
connection of cables of different cross section.
3.2
Comparing Investments to Expand Existing Srebrenica Network
(P2P and PmP)
Construction of this network is based on penetration of the user about 30%. Current
P2P network is built in Srebrenica with a capacity of about 400 ﬁbers. If we want to
extend the existing network to full capacity (900 ﬁbers), the investment must increases.
For this network expansion by P2P technology, it is necessary to provide the material,
to draw new cables, to make connection optical ﬁber cables, to measure the cables and
to create technical documentation. This increases the network extension costs by about
25% in P2P technology.
If this network was made with PmP-GPON technology, the ﬁrst stage in invest-
ments (400 ﬁbers) would be approximately identical to P2P. If we want to extend the
network with GPON technology, we only invest in installing splitters into distribution
cabinets (Street Cabinet, box in the building, box on a wooden pillar). These invest-
ments are negligible. Using the splitter 1/16 we provide the user with 78 Mbit/s. This is
enough for customer service in the coming period. Using the spliter 1/8 we can provide
transmission speed over 150 Mbit/s. GPON also provides great ﬂexibility in the
network.
Below we will further analyze the costs of two types of access networks, P2P and
PmP-GPON [3]. Many factors determine the choice of technology: the type of set-
tlement, the development perspective, the user’s solvency. Using micro-cables, micro
tubes and aerial ﬁber optic cables, savings of about 10% were achieved at the start for a
complete network of 400 ﬁber optics. This saving is the same for both technologies.
However, there are limitations in realizing this project: RR link capacity and modem
speed. Therefore, the advantage of P2P technology will not emerge in this case.
Estimated time to realize this extension with P2P technology is 90 days.
764
M. Hohzic et al.

The main advantage of PmP is the ability to expand without investing in the
primary part of the access network. Respectively, network extensions do not require a
detailed study of the current state of affairs, user’s habits, user’s solvency, and user’s
penetration. The PmP does not require a detailed study of the number of end users, as
simply increasing the number of splitters results in a multiple increases in distribution
point capacity. The time to realize the PmP GPON technology extension is 10 days.
Building this network using P2P technology has created problems at the beginning.
Some users have not planned the main project. For these users, it was convenient to
increase the capacity cables from the CO to the splice boxes. This required modiﬁ-
cations to the project during its implementation.
Comparison (P2P and PmP) of additional investments in the case of extending the
existing network for a new capacity of 500 ﬁbers is given in Table 1.
• Life time: P2P technology provides very high transmission speeds. With GPON
technology, the transmission speed is limited by the choice of splitters. Splitter 1/16
enables speed of 155 Mbit/s, Splitter 1/8 enables speed of about 300 Mbit/s.
• Civil work: No new construction work is required for any technology.
• New home installation: It is necessary to make installation to the user for both
technologies.
Table 1. Comparison of PM Srebrenica expansion investments (P2P/PmP-GPON). Phase 2
Actions
Investment P2P
(%)
approksimately
FTTH P2P Investment
GPON (%)
FTTH GPON
Comment
Life time
Long
Midle*
Civil work
No
No
New home
instalation
1
Yes
1
Yes
New materijal
11
Cable, clip 1
Splitter
New construct
works
12
Yes
No
New
documentation
1
Yes
No
Expanded
capacity
network
500 ﬁbers
More than 1200
Spliter
1/4, 1/8,
1/16
Rate
user/investment
Midle
Good
Terminal
equipment
limit
100 Mbit/s
Limit 100 Mbit/s
Transmission
speed
Very high
High > 156 Mbit/s
Total cost
increase
25
2
Analysis of Techno-Economic Proﬁtability on the Example
765

• New material: For P2P technology, additional material should be provided: optical
cables, plugs, markers. For G PON technology, only splitters should be provided.
• New construct works: For P2P technology, it is necessary to install new cables with
the appropriate equipment. For GPON it was just to connect splitters in distribution
boxes.
• New documentation: P2P requires new documentation. For GPON it is only nec-
essary to complete the existing documentation with the splitter capacity.
• Expanded capacity network: With P2P technology we provide 500 potential users.
With GPON we provide the technical ability for additional 1200 potential users.
This network capacity can be increased, but users will have a lower speed of 300
Mbit/s.
• Rate user/investment: The investment per user is better using GPON than P2P.
• Terminal equipment: The modem speed is limited to 100 Mbit/s. P2P technology
can provide higher speeds than GPON if high speed modems are used.
• Transmission speed: P2P technology provides faster network speed compared to
PmP GPON. The transmission speed at GPON deﬁnes the split level. However, as
modem speed is limited to 100 Mbit/s, this advantage will not be considered.
• Total cost increase: For new 500 ﬁbers, P2P technology requires additional 25%
investment. GPON technology requires additional 2% investment.
Because of the limitation of the RR transmission and the limitation of the modem
speed, the advantage (transmission rate) of the P2P relative to GPON technology is in
this case negligible [4].
4
Conclusion
P2P technology is better than PmP-GPON, but this advantage has not come to the fore
in this project. The reason is this: RR link limitation (RR Sarajevo-Srebrenica), net-
work card/modem speed limitation and non-solvent users. All involved service users in
Srebrenica use a broadband service that requires 10 Mbit/s.
To extend the existing network, P2P technology requires additional material, new
upgrades, which increases the investment by 25%. GPON technology delivers great
ﬂexibility in the network and does not require any additional material and work, except
splitters. With this technology, costs increase by 2% and network capacity increases by
300%. That is, CAPEX (Capital Expenditures) is the same for both technologies, but
OPEX (Operating Expenses) P2P is about 20 times higher in PmP-GPON technology.
This is the main advantage that has emerged in Srebrenica, i.e. The ability to
expand without additional investment into the primary part of the access network. This
ﬂexibility allows faster design without detailed research of the area. P2P technology
requires the above mentioned. Faster and simpler projects shorten the time of con-
struction and increase the speed of exploitation, respectively ARPU (Average revenue
per user) is more better with PmP technology.
766
M. Hohzic et al.

References
1. BONE: Report on Y2 activities and new integration strategy, Europe (2009)
2. Sigurdsson, H.M.: Tehno-Economics of rezidential Brodband Deployment, Kongens Lyngby
(2007)
3. Chatz, S.: Techno-economic study of high–splitting ratio PONs and comparison with
conventional FTTH-PONs/FTTH-P2P/FTTB and FTTC deployments. In: Optical Fiber
Communication Conference and Exposition (OFC/NFOEC) (2011)
4. Skaljo, E.: A cost effective topology in ﬁber to the home point to point networks based on
single wavelength bi-directional multiplex. FOAN, Brno (2015)
Analysis of Techno-Economic Proﬁtability on the Example
767

Synchronization Between Arduino Based
Appliance and MATLAB Application
Adnan Felić and Edin Mujčić(&)
Faculty of Technical Engineering, University of Bihać, Bihać,
Bosnia and Herzegovina
{edin.mujcic,felic.adnan13}@gmail.com
Abstract. Microcontrollers have usages in various types of processes and
automation systems. What makes them so convenient is their programmability,
low-cost, and a big number of possible extensions. Microcontroller ATMega328
with Arduino board is common when it comes to learning and experimenting.
Liquid Crystal Displays are often used with microcontrollers, and in this project
one with 16  4 dimensions is used. The programming logic of a project is built
in Arduino IDE. Standard 4  4 keypad with 8 output pins is used with only
one pin with some modiﬁcations in connection. The device can be controlled by
the application built in MATLAB environment. The application and the device
are communicating by Serial communication and are in synchronization. In this
way, the device can be used directly, or by the application at the same time.
Keywords: Microcontroller  LCD display  Arduino  Calculator  Serial
communication  MATLAB  Graphical user interface
1
Introduction
Modern technology cannot be conceived without the usage of microcontrollers from
children’s toys to advanced scientiﬁc projects. The usual misconception is that
microcontrollers and microprocessors are the same thing. The main difference is that, in
order to make microprocessor usable, many components must be added, and that is not
the case with microcontrollers [1]. Microcontrollers contain one or more CPU, mem-
ory, I/O peripherals, etc., and there is no need for other external components [1, 2].
Microcontrollers are widely used, programmable, low-cost, and resistant electronic
components which are often described as a small computer on a “single integrated
circuit” [2]. The best-known manufacturers of microcontrollers are Intel, Atmel,
Microchip, Toshiba, etc. Arduino boards are one of the most affordable boards on the
market. With them one can easily start working with microcontrollers [3]. LCD dis-
plays that are ideal for displaying text are character displays [4]. They come in various
sizes, and the one used in this project is 16  2. These displays are the most inex-
pensive and simplest, and have 16 pins for connection with microcontroller [5]. Matrix
keypads are often used in projects that need some kind of data entry, and use com-
bination of rows and columns of switches to provide button states [5, 6]. MATLAB
environment is one of the most commonly used in engineering world, and when it
comes to this project, a GUI is created for device control.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_67

2
ATmega328 Microcontroller and Arduino Boards
ATmega328 microcontroller is Atmel’s product and is a low-power CMOS 8-bit
microcontroller based on the AVR enhanced RISC architecture [3]. It has a lot of
features and some of them are: advanced RISC architecture, 32 Kbytes of In-System
Programmable with Read-While-Write capabilities, 1 Kbytes of EEPROM, 2 Kbytes
SRAM, etc. [3, 7]. ATmega328 is a microcontroller of Arduino board used in this
paper. Figure 1 is showing pinout of a board used in this paper.
Arduino is a company, project that designs microcontroller kits since 2005 and it is
a thriving industry, supported by a large community of people [5, 8]. Arduino kits are
widely used from individuals, students, hobbyists, to educational institutions [9]. It is
an open source project. The one used in this paper is a version called Croduino and it is
shown in Fig. 1. It is based on Arduino Nano.
Croduino board has all the features that any other Arduino board gives. It has 22
I/O pins, 8 analog, and 14 digital pins, and 6 of 14 digital are PWM. Figure 1 is
showing some limitations of the board itself when it comes to current and power.
The Arduino project has its own IDE written in Java which is originated from the
IDE for languages Processing and Wiring [7]. Syntax for writing Arduino code is based
on C and C++. The main structure must have a setup and loop functions [9]. A program
made in Arduino IDE is called sketch.
Fig. 1. Arduino ATmega328 pinout [10]
Synchronization Between Arduino Based Appliance
769

3
LCD Display and Matrix Keypad
Liquid Crystal Displays are the most inexpensive display modules for Arduino.
Character displays are very easy to use, and can be purchased in various sizes. 16  2
us used in this paper. Connection of 16  2 LCD with Arduino board is shown on
Fig. 2.
Variable resistor is used for display contrast adjustment since this display has a
backlight. Pins for communication with LCD are D0–D7, RS, E and RW. Only 6 pins
are necessary for standard communication: RS, EN, D7, D6, D5 and D4.
Since this paper is based on Calculator as controlled device, some kind of module
that will produce inputs was needed. Matrix keypad used in this paper is 4  4 keypad
and has 8 connection pins for 16 buttons. It has thin design, good performance, and it is
great as a human interface component in projects. In order to provide different states to
microcontroller, four rows and four columns are used. In order to connect matrix
keypad in this paper we use one analog pin instead of 8 pins. Every row has its own
resistance, and so do the columns. Figure 3 is showing (a) the appearance of a matrix
keypad, and (b) wiring of a keypad used in this paper.
OneWireKeypad library is used for keypad control. This library lets a user to deﬁne
resistances of rows and columns, and calculates range of analog values for every
button. When a button is pressed, microcontroller receives an analog value of resis-
tance. In that way, it is determined which key has been pressed. As it was mentioned
before, the main problem that is to be controlled in this paper is a calculator that uses
matrix keypad, as well as four switches with pull-down resistors that are used for extra
operations.
Fig. 2. Connecting LCD to Arduino
770
A. Felić and E. Mujčić

4
Synchronization Between Arduino Based Appliance
and MATLAB Application
Arduino appliance in this paper is a calculator with four basic operations built in 4  4
matrix keypad, and 4 additional operations (square, square root, sine, cosine) added
with switches and pull-down resistors. Calculator logic is written in Arduino IDE, and
all of it is set on the microcontroller. Keys from matrix keypad are recognized by
methods from included library, and work of switches is stabilized with debounce
method that is dealing with switch bounce problem. In order for calculator to work
well, after keys are recognized, ﬂags, if-statements and methods are implemented.
Figure 4 is showing the block diagram of behavior of this paper work.
Data entry can be realized through keypad, additional four switches, or graphical
user interface built in MATLAB environment. As soon as microcontroller receives the
key, part of the program logic is done and variable that contains current operation is
ﬁlled. The same variable is shown on the LCD display, as well on the graphical user
interface on the computer. MATLAB application is using timer method as a back-
ground task. This method is reloading every 0.05 s, reading from serial port, and
showing result on GUI. Every button on GUI is using previously opened serial port and
writing a character in it. Figure 5 is showing (a) wiring and (b) appearance of a
calculator.
Graphical user interface is built in MATLAB environment as mentioned before.
Figure 6 is showing appearance of GUI.
Figure 7 is showing the whole system in work including both the calculator and the
GUI application.
Fig. 3. Matrix keypad a appearance and b wiring
Synchronization Between Arduino Based Appliance
771

Fig. 4. Block diagram of project usage
Fig. 5. a Wiring of the calculator b appearance of the calculator
772
A. Felić and E. Mujčić

Fig. 6. Graphical user interface
Fig. 7. The calculator and the GUI application in work
Synchronization Between Arduino Based Appliance
773

5
Experimental Results
As it was explained through usage block diagram, user can send data through calculator
itself, or through GUI. On Fig. 8 ways of data entry and results of testing are shown on
three images. On the Fig. 8a, user is typing “cos(0)” on calculator, and “+sin(90)” on
GUI, and the equal button is pressed on the calculator. Figure 8b is showing result of
typing only on the calculator, and on Fig. 8c equal button is pressed on GUI and
operation is synchronously continued to the previous. It can be seen that both calcu-
lator’s display and GUI are showing the same results.
6
Conclusion
Experimental analysis has shown that synchronization between built appliance and
MATLAB application works well. The speed of Arduino looping caused minimum
problems with key recognition from keypad by the app, but it did not create any fatal
error nor stop the application from working. Synchronization can be done between any
other appliance through either guided, or unguided medium with serial communication.
References
1. Verle, M.: PIC mikrokontroleri. Mikroelektronika, Beograd (2008)
2. Microcontrollers. https://goo.gl/Zklv3J
3. Atmel 8-bit AVR Microcontrollers ATmega328/P. https://goo.gl/IsEQYm
4. Character LCDs. https://goo.gl/qaene9
5. Boxall, J.: Arduino Workshop. No starch press, San Francisco (2013)
6. 4  4 Matrix Membrane Keypad. https://goo.gl/Hg8L2G
Fig. 8. a Combined usage b calculator only c GUI only
774
A. Felić and E. Mujčić

7. Hari Sudhan, R., Ganesh Kumar, M., Udhaya Prakash, A., Anu Roopa Devi, S., Sathiya, P.:
Arduino ATMega-328 Microcontroller. Int. J. Innov. Res. Electr. Electron. Instrum. Control
Eng. 3(4) (2015)
8. Arduino. https://goo.gl/po3v8
9. Louis, L.: Working principle of Arduino and using it as a tool for study and research. Int.
J. Control Autom. Commun. Syst. (IJCACS) 1(2) (2016)
10. Croduino Basic2 Pinout. https://goo.gl/bHeq0s
Synchronization Between Arduino Based Appliance
775

Transmission of Two Optical Signals Through
the Fibber in Opposite Directions Using PLC
Splitters—Practical Measurements
Mujo Hodzic1(&), Edvin Skaljo1, Nermin Suljanovic2,
and Aljo Mujcic2
1 BH Telecom, Sarajevo, Bosnia and Herzegovina
{mujo.hodzic,edvin.skaljo}@bhtelecom.ba
2 Faculty of Electrical Engineering, University of Tuzla
Bosnia and Herzegovina, Tuzla, Bosnia and Herzegovina
{nermin.suljanovic,aljo.mujcic}@untz.ba
Abstract. In this study, the use of PLC (Planar Lightwave Circuit) splitter in
bi-directional WDM optical systems instead of the standard WDM (Wavelength
Division Multiplex) splitters is theoretically described and practically conﬁrmed
in real telecommunications environment. The transmission of two optical signals
in opposite direction over a single mode ﬁber is tested at slightly spaced
wavelengths. The insertion and separation of opposite optical signals are real-
ized using two 1:2 PLC splitters. The measurements are successfully completed
for two different lengths of optical ﬁbers (8055 and 61091 m). The proposed
solution is cost-effective and ﬂexible, with a wide temperature range covering a
broad band of operating wavelengths.
1
Introduction
The well-known method for the optical link capacity enhancement is WDM (Wave-
length Division Multiplexing) system [1]. It is founded on the utilization of signal
transmission at different wavelengths over a ﬁber. In particular, with the development
of DWDM (Dense WDM) technologies, the number of channels transmitted along a
single ﬁber has risen up to several hundreds of channels, spanning long distances [2].
The implementation of systems with a large number of channels requires laser trans-
mitters with a highly narrow spectrum. Production of high-quality laser equipment is
extremely expensive. Wider line-width of laser reduces the complexity of circuits for
controlling the wavelength of the laser and thus a reduction in price. On the other hand,
standard FP (Fabry Perot) and DFB (Distributed Feedback) lasers that operate at the
most frequently used wavelengths (1310 and 1490 nm) are becoming more affordable
at reasonable prices. Nowadays we are facing with the better degree of utilization of the
already installed optical cables.
There are many ﬁbers, already installed in communication networks, which are
used for one-way or bi-directional WDM transmission. By using the signal transmis-
sion in both directions at slightly spaced wavelengths, the capacity and usability of the
optical ﬁber could be increased. The two-way transmission can be achieved using PLC
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_68

splitters at both terminals. In such way, a much higher utilization of networks with
minimal investment in additional elements is provided. The technical and economical
analysis intends to ensure a maximum usability of the network elements with low
investments.
The theoretical description of propagation of two light waves in opposite direction
is described in Sect. 2. In next section, the scheme of measuring two independent
optical systems with PLC splitters in opposite directions is presented. The measurement
results are presented in Sect. 4.
2
Transmission of Two Light-Waves in Opposite Directions:
Theoretical Considerations
2.1
Interference of Two Light Electromagnetic Waves
The interaction between two light-waves propagating in opposite directions is analyzed
in [3] and it has been shown that this interaction may be neglected. Therefore, we
assume that two light electromagnetic waves propagating in opposite directions in a
single ﬁber (Fig. 1) do not cause interference and as the result the transmission of two
digital streams has no interaction.
2.2
Reﬂected Light as a Noise
According to previous statement, the main requirement for a successful transmission of
two opposite signals is that the levels of reﬂected lights (Light 1R and Light 2R) from
the input of the ﬁber (Fig. 2) are deep under the levels of the useful signals coming
from the other sides of the optical ﬁber (Light 1). In such way, the correct reception of
the signal coming from the other side is not disrupted.
Fig. 1. The transmission of two light signals in opposite directions over an optical ﬁber
Fig. 2. Reﬂected light as noise
Transmission of Two Optical Signals Through the Fibber
777

Reﬂected signal light 1R is an undesired signal (noise) in the course of reception
and detection of the signal light 2. Also, reﬂected signal light 2R is an undesired signal
(noise) in the course of reception and detection of the signal light 1. In digital com-
munications, the noise manifests as a bit error, also known as BER (Bit Error Rate). In
the process of digital systems designing, an allowed BER is deﬁned in advance so that
a usually allowed BER is 10−9, 10−10 and, rarely, 10−12 [4]. If an allowed BER is
deﬁned it means the alloved level of noise is also deﬁned.
2.3
PLC Spliter
The introduction of lights into the ﬁber may be achieved by using many types of
couplers. In this study, an usage of PLC splitter is proposed as coupler of light signals.
Thanks to the development of PON networks these splitters with high performance
are widely used. Currently the PLC splitters with reﬂection loss better than −55 dB,
wavelength range of 1280–1625 nm, temperature range of −40–(+80) °C and low price
are commercially available [5, 6].
3
Measurement Setup
In order to validate the method proposed in the paper, the following measurements are
completed:
• BER measurements
• Measurement of power of reﬂected signal
• Measurement of spectra of the reﬂected signal.
3.1
Measurement of BER
The measurement of BER is performed on two independent digital optical systems
operating over a single ﬁber in accordance with the scheme shown in Fig. 3.
Two ﬁbers are taken from the same cable that were previously connected in the
loop at the far side of cable. On this way, both splitters are located in the same room at
near side of cable. The actual optical path is a double cable length, plus the length of
patch cables.
Digital optical system (DOS) 1 is realized by using the optical path that goes from
the transceiver Tx1 to port 1 of splitter 1, and continues through the ﬁber passes to the
splitter 2 (point 4). The signal from the Port 1 of splitter 2 (point 5) is forwarded to the
receiver Rx1. Similarly, the digital optical system 2 is realized by using the opposite
direction. The same is true for DOS 2.
778
M. Hodzic et al.

BER is measured using two independent instruments (BERT—BER tester) brand
Acterna EDT 135. BER is measured for several different conﬁgurations that are pre-
sented in Table 1.
3.2
Measurement of Power of Reﬂected Signal
In the process of measuring the reﬂected power, the digital optical system 2 was
switched off and disconnected. Instead of the receivers Rx2 the optical power meter
(OPM) is connected (Fig. 4).
3.3
Measurement of Spectra of the Reﬂected Signal and the Useful Signal
The measurement of the power spectrum of the optical signal is conducted by: Optical
Spectrum Analyzer-OSA as a part of JDSU’s Scalable Multitest Platform instrument
Fig. 3. Measurement setup for BER
Fig. 4. Measurement of the reﬂection signal levels
Transmission of Two Optical Signals Through the Fibber
779

Device MTS 8000 Number 8696, Module 830SA Number D-0054. Results are viewed
by: Fiber Trace Viewer Unicode JDSU 1994-2009 [4, 7].
For the purposes of this study, the measurement was performed in the way that the
digital optical system 1 was active, the digital optical system 2 transmitter was switched
on, while spectrum analyzer was connected instead of the receiver Rx2 (Fig. 5).
4
Results and Discussion
In order to validate the method proposed in the paper, the following measurements are
completed using described test optical system:
Fig. 5. Power measurement of reﬂected signals together with useful signal
Table 1. Parameters of the system for ﬁve types of measurements
Measur-ement
setup
System Bit
rates
[Mb/s]
Type
of
laser
Wavelen-gth
[nm]
Optical
Power of
Transmit-ted
signals
[dBm]
Sensitiv-ity
[dBm]
Length
of
optical
path
[m]
1
1
2488
DBF
1307,2
−3
–27
8 055
2
8
FP
1307,1
−13,5
−28,7
2
1
2488
DBF
1307,4
0
−26,7
8 055
2
2488
DBF
1309,3
−3
−18
3
1
2488
DBF
1306,9
0
–27
8 055
2
2488
DBF
1309
0
−27
4
1
2488
DBF
1552,3
−3
−27,1
61 091
2
2488
DBF
1550,1
−3
–27,1
5
1
8
FP
1305,17
−13,5
–28,7
28 340
2
8
FP
1305,09
−13,5
−28,7
780
M. Hodzic et al.

• Measurement of the bit error;
• Measurement of the reﬂection (Reﬂection measurements)
• Recording of the signal spectrum at characteristic system points;
4.1
Results
1. Measurements of BER were done repeatedly in accordance with recommendations
G 821 and G 826 [7]. Each measurement lasted for a minimum of 24 h. No bit error
(Fig. 6) was recorded in any of the measurements shown in Table 1 (BER = 0).
2. The reﬂected signal power measured by OPM (Optical Power Meter) is −36.6 dBm
if the transmitted signal power is 0 dBm. It is exactly due to the use of splitters and
connectors, with the stated characteristics necessary for the implementation of the
proposed method.
3. The measurement of the optical signal spectra using spectrum analyzer [4, 8]:
• In this study all the wavelengths of the lasers used were measured by spectrum
analyzer and their values are given in Table 1.
• The spectrum of DBF laser from the measurement setup 1 of the transmitter Tx1
is shown in Fig. 7.
• The spectrum of FP (Fabry Perot) laser which was used in the measurement
setup 1 of the System 2 at the input of the receiver Rx2 is shown in Fig. 8.
• The spectrum of signals with the measurement setup 1 of the system 1 at the
input of the receiver Rx1, while lasers were switched on in both systems, is
shown in Fig. 9.
Fig. 6. The measurement results of BER
Transmission of Two Optical Signals Through the Fibber
781

Fig. 7. Spectrum of optical signal emitted from DFB laser used in measurement setup 1 system
1 (Table 1)
Fig. 8. The spectrum of FP laser used at the measurement setup 1 system 2 (Table 1) at the
input of the receiver Rx2 while the system 1 transmitter is switched off
782
M. Hodzic et al.

4.2
Discussion
All the measurements were done on real systems in the company BH Telecom d.d.
Sarajevo. These measurements are performed for different signal wavelengths, different
lengths of optical ﬁbers and different transmitting/receiving levels of the signal.
All measurements were made using the ﬁber-optic cables that have been in oper-
ation. For ease of measurement, we are one in the same ﬁber cable shorted patch cable.
The table T1 is listed the length of the optical path as the two lengths of cable plus
patch (l = 2L + patch).
The BER was measured using instruments, which were placed on the side of both
receivers: Rx1 and Rx2.
The proposed measurement methodology was deployed using two instruments
Acterna EDT-135. Each instrument measures the bit error of the signal at the level of 2
Mbit/s, according to the recommendations ITU-T G.821 and G826. BERT 1 device was
connected to the tributary electrical port of transmission system 1 and BERT 2 device
was connected to the tributary electrical port of transmission system 2. The power of
signal (reﬂected and useful) was measured by OPM-TV (Optical Power Meter).
The measurement of the signal spectrum was performed for each system and for
each measurement setup in Table 1. However, due to some limitations in the study, we
only presented results for the measurement setup 1.
Fig. 9. The spectrum of signals for measuring setup 1 of system 1, at the input of the receiver
Rx1 while lasers are switched on in both systems
Transmission of Two Optical Signals Through the Fibber
783

5
Conclusion
Transmission of two optical signals over single ﬁber in opposite directions is theo-
retically proved and experimentally conﬁrmed. The introduction of two signals into the
ﬁber was performed by using PLC splitters on both sides of ﬁber as a communication
media. This splitter is the key element of proposed system due to its wide wavelength
range and low reﬂection loss.
The measurements for four different scenarios, based on commercially available
components, are completed. These measurements are focused on BER, reﬂection loss
and power spectrum density. The duration of BER measurements was longer than 24 h.
The successful transmission of two optical signals on the same wavelength in opposite
directions is achieved with no bit error. Also, the other measurement requirements in
accordance with standard G826 and 821 are successfully fulﬁlled. The measured
reﬂected signal at both sides of communication system was under −36 dBm and there
was no impact on digital optical transmission.
Comparing to existing Bi-direction WDM solutions the proposed system cover
wide
band
of
wavelengths
from
1280
to
1610 nm.
The
system
is
also
temperature-stable working in wide range of temperatures from −40 to 80 °C. In the
proposed system the PLC splitter introduces additional attenuation of 3 dB. This
attenuation should be compensated by other components in the long distance systems.
References
1. Miki, T., Ishio, H.: Viabilities of the wavelength-division-multiplexing transmission system
over an optical ﬁber cable. IEEE Trans. Commun. 26(7), 1082–1087 (1978)
2. Liu, J., Zhou, Y., Li, F., Xu, Z.: The second-order interference between laser and thermal
light. EPL 105, 64007 (2014). http://www.epljournal.org
3. Ou, Z.Y.J.: Multi-Photon Quantum Interference. Springer (2007)
4. Alpert, A.: Understanding ITU-T Error Performance Recommendations. http://www.jdsu.com
5. Molina-Fernández, I.: Planar lightwave circuit six-port technique for optical measurements
and characterizations. J. Lightwavetehnol. (6) (2005)
6. Tee, D.C.: Numerical investigation on cascaded 1  3 photonic crystal power splitter based
on asymmetric and symmetric 1  2 photonic crystal splitters designed with ﬂexible
structural. In: (C) Optics Express, 6 Oct 2014, vol. 22, no. 20, p. 24254. OSA (2014). https://
doi.org/10.1364/OE.22.024241
7. ITU 1988, 2006: Error performance of an international digital connection forming part of an
integrated services digital network. The International Telegraph and Telephone Consultative
Committee—CCITT, G-821 (1988)
8. http://www.jdsu.com/en-us/Test-and-Measurement/Products/ﬁeld-network-test/ﬁber/Pages/
default.aspx#.VL7d9C7L-J8
784
M. Hodzic et al.

Soft to Hard Data Transformation Using
Uncertainty Balance Principle
Migdat Hodzic(&)
FENS, EEE Department, International University of Sarajevo, Sarajevo,
Bosnia and Herzegovina
mhodzic@ius.edu.ba
Abstract. The paper advances our on going work in the area of uncertainty
alignment and transformation between fuzzy (soft, human generated, possi-
bilistic) and random (hard, machine generated, probabilistic) data. As reported in
our previous papers, the Uncertainty Balance Principle was deﬁned to express
uncertain data vagueness as represented by a fuzzy data models, with a non
uniqueness of related random data distributions. The underlying assumption is
that both fuzzy and random data are described in terms of the same independent
uncertain variables. The connection between fuzzy and random data is done via
cumulative rather than probability density functions. In this paper we clarify and
extend our previous work whereas an initial fuzzy distributions (membership
functions) are supplied and the aim is to determine corresponding and related
random distributions. The next step in this analysis will focus on Bayesian data
mining to determine random distributions from a given large set (data base) of
soft data modeled as fuzzy (triangular, trapezoidal or other convex) distribu-
tions. This work has been inspired by an ever increasing need to fuse human and
machine data in order to perform decision making procedures. Areas of appli-
cations include Bank Risk Assessment in ﬁnancial industry as well as Command
and Control Integration in defense industry and any other applications where
soft and hard data fusion is required.
1
Introduction
In studies of uncertain phenomena, several methods are employed. Two most widely
used are random and fuzzy data approaches. They are typically described in terms of
random and fuzzy distributions [1]. These two methods look at the uncertainty from
different points of view. In literature one can ﬁnd various terms for fuzzy data, such as
possibilistic, soft and subjective [2], as opposed to random called probabilistic, hard
and objective [3]. These terms are somewhat arbitrary and there are authors who used
probability distributions to represent subjective information [4–6]. Similarly, other
authors used fuzzy sets and possibility distributions to describe objective imprecise
information either about constants or about probability distributions [7]. Historically,
probability is deﬁned in the context of some physical measurement and mathematically
in terms of probability axioms by Kolmogorov [4, 8, 9]. On the other hand fuzzy,
possibilistic [10] approach relates to some intuitive uncertain notion (often of human
nature) of an underlying uncertain event with some conﬁdence (presumption) levels
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_69

deﬁned. Often in fuzzy data there is no reference, at least not directly, to any experi-
ment or hard measurement. It is more representation of our conﬁdence level in an
uncertain phenomenon. If a need arises for fuzzy-random data fusion, [1, 3], each
distribution is typically handled separately for a speciﬁc problem at hand. To our
knowledge no rigorous mathematical methodologies exist for a practical uncertainty
alignment between two types of data. In classic fuzzy references [1, 11], various
algebraic operations on fuzzy data are described, as well as the methods as how to
combine fuzzy and random data in meaningful ways. One approach is to deﬁne hybrid
data which retains both fuzzy and random properties of original data. One can deﬁne
random fuzzy data where fuzzy distribution argument is “randomized” according to a
probabilistic distribution density. Or, one can consider fuzzy random data where the
value of random distribution density is fuzziﬁed according to fuzzy distribution. There
was very extensive development last two decades, [12–38, 9, 39–42] in the area of
“random fuzzy sets” and “fuzzy random variables”. Neither is the focus of our paper.
The subject of our paper is to consider fuzzy (possibilistic) to random (probabilistic)
uncertainty transformation (i.e. starting with fuzzy and generating random data, or vice
versa) using very basic properties of fuzzy and random distributions. We assume an
underlying uncertain variable to be both possibilistic and probabilistic. In our approach
we employ three step methodology:
(I)
Decompose any possibilistic (fuzzy) distribution (PosD) via cumulative proba-
bilistic distributions (ProCD) deﬁned as random event probabilities of the
standard form P(X  x).
(II)
Determine speciﬁc probabilities which implement ProCDs using some related
probability density functions (ProPD) if they can be deﬁned.
(III)
Associate speciﬁc probabilities with application speciﬁc probabilistic events.
The result of our approach is that any fuzzy distribution can be thought of as an
interplay (union, intersection) of two or more probabilistic events. The opposite applies
as well, i.e. given two or more random events their interplay can be interpreted as a
fuzzy event. We believe our approach can bring about new avenues in aligning fuzzy
and random data, in particular in very important area of soft-hard (human-machine)
data fusion [3]. In our previous papers [43, 44], we presented the basics of our
uncertainty alignment methodology. This paper extends and clariﬁes these results for
all transformation cases. The paper is organized as follows. Section 2 summarize our
basic idea for fuzzy to random transformation. Section 3 describes several possible
methods for this transformation clarifying and extending our previous results [43, 44].
In Sect. 4 we review and extend our previous results on Uncertainty Balance Principle
as an universal principle which ties fuzzy and random distributions via very simple
relationship. In this Section we present the main results in a simpliﬁed and clearer form.
In Sect. 5 we recall basic description of Zadeh Consistency Principle [10] which
loosely ties two descriptions of an uncertain data, i.e. fuzzy and random. We further
develop this concept with more precise and clearer intuitive presentation. Also new
graphical methodology is introduced to present both possibilistic and probabilistic data
in the same diagram. Section 6 presents numerical optimization methodology to cal-
culate probabilities from given fuzzy distribution. Section 7 presents a list of future
research to follow, and Conclusion is given in Sect. 8, followed by key References.
786
M. Hodzic

2
Fuzzy to Random Decomposition
In this section we further elaborate on and develop our methodology [43, 44] for
Fuzzy-to-Random (Soft-to-Hard) decomposition and transformation.
2.1
Note on Soft-Hard Data
One of the practical motivations for this work is to have a methodology to deal with
(i) soft and hard data fusion for a variety of applications, and (ii) use fused data to
enhance decision making process. Table 1 has an intuitive summary of attributes found
in literature on soft (human) and hard (sensor or machine) generated data. Other views
on what is hard and soft and when to apply fuzzy versus random are possible as well
[4–7].
For our purposes in this work we think of hard data as produced by some mea-
surements or an experiment, whereas fuzzy data represent our perception of the same or
related event prior to the measurements or experiment.
2.2
Fuzzy Distribution Examples
We consider any normalized PosD function PX(x) = P(x) which is numerically
equivalent to fuzzy membership function µ(x), [10]. Figure 1 shows a typical triangular
PosD, and Fig. 3 show additional PosD functions (trapezoidal, left half trapezoidal,
right half trapezoidal and general convex).
Table 1. Intuitive soft and hard data designations
Soft data (Human)
Hard data (Machine)
Subjective, valuation
Objective, measure
Before experiment or measurement After experiment or measurement
Qualitative
Quantitative
Possibility theory
Probability theory
Fuzzy models
Random models
Fuzzy Distribution P(x) = µ(x)
Cummulative Distribution F(x)
x
a            
b            
c
Triangular 
Π(x) = µ(x)
1
Fig. 1. Triangular fuzzy number (TFN)
Soft to Hard Data Transformation
787

2.3
General Transformation Description
Figure 2 indicates our main idea, a very simple one, as to how to approach decom-
posing a triangular PosD function, using some related (uniform) ProCD functions. The
idea comes from (i) an intuitive similarity between rising half of a PosD and some
related ProCD distribution, and (ii) the fact that both have maximum values at 1. We
believe these two points warrant further investigation. Later in this paper we relate
ProD functions as well, as they are derivatives (if they exist, [45]) of the corresponding
ProCD functions. From Fig. 2 we formally write:
PðxÞ ¼ F1ðxÞF2ðxÞ
ð1Þ
which indicates how PosD is “decomposed” via a pair of to-be-deﬁned ProCD’s.
Note that F1(x) and F2(x) could be generated by two different random “experiments”,
whereas as F1(x) reaches 1 (certain event), F2(x) starts to cast some increasing “doubt”
about F1(x), until the two cancel completely. By repeated application of (1) one can
decompose multi modal PosD. Other PosD’s in Fig. 3 can be decomposed as in (1) as
well. First three hint decomposition using uniform, and the last two some non uniform
ProCD’s. Recall that any F(x) is a probability of an event A = {X  x} as given in
[10], i.e. F(x) = P(X  x) = P. It is critical we assume that the uncertain variable X is
both probabilistic and possibilistic (let us call it ProPos variable X), as it takes a
speciﬁc value X = x. For the purpose of this paper we recall basic probabilistic relation
related to a union of two related random events A1 and A2:
PðA1 [ A2Þ ¼ P A1
ð
Þ þ P A2
ð
Þ  PðA1 \ A2Þ
ð2Þ
x
a            
b             c
x
a             b             c
F2(x)
F1(x)
1
1
Fig. 2. ProCD functions F1(x) and F2(x)
788
M. Hodzic

or in simpliﬁed notation as:
P1 þ 2 ¼ P1 þ P2P12
ð3Þ
with P(A1) = P1, P(A2) = P2, P(A1 \ A2) = P1*2 (an intersection of two random
events), and P(A1UA2) = P1+2 (a union of two random events). We use (2) to imple-
ment F(x), as it carries a “seed” of random non uniqueness.
3
Decomposition Methods
In this section we extend and summarize key details of decomposition methods
described in our papers [10], whereas an unimodal P(x) is presented as a difference of
a pair of ProCD’s, as given in (1). All distributions have a common ProPos independent
a     b         c
d  
Trapezoidal
x
a    
b          
Left Half Trapezoidal
x
a     
b                   
Right Half Trapezoidal
x
x
a           b         c
Π(x)
Convex
1
1
Π(x)
Π(x)
1
1
Π(x)
Fig. 3. Various PosD functions
Soft to Hard Data Transformation
789

variable X. With no loss of generality we assume PosD to be a triangular one, or TFN
(Triangular Fuzzy Number) as in Fig. 1. All probabilities in (3) are of the form P
(A) = P(X  x) to conform to F(x) deﬁnition. We consider two situations, namely
either two or four random events generated to decompose P(x) for ProPos variable
X = x.
3.1
Two Random Events Decomposition
We can try to decompose P(x) in several ways using a pair of related random events
and corresponding probabilities.
3.1.1
Decomposition Method 1 (DM1)
An “obvious” choice for decomposition of an unimodal TFN to some random distri-
bution is:
PðxÞ ¼ F1ðxÞF2ðxÞ ¼ P1P2
ð4Þ
with F1(x) = P(A1) = P1 and F2(x) = P(A2) = P2. This assumes subtracting one
probability from another, or existence of negative probability. We would need to
consider so called “signed” probability distribution which can take both negative and
positive signs. There are many accounts on negative probability in the literature [11,
12, 45]. In the case of (5) above and to avoid negative probability we interpret it as:
PðxÞ þ P2 ¼ P1
ð5Þ
Per (4) above and Fig. 2 P(x) rising part is represented by P1 and falling part by
P1 −P2 which is equal to 1 −P2 when P1 becomes a certain event, or P(x) + P2 = 1.
Hence P2 can be interpreted from P(x) + P*(x) > 1, [10, 15] as:
PðxÞ [ P2
ð6Þ
The main issue with the above method is that it does not produce non unique
probabilities, still this method may work in some simple cases. See Table 2.
3.1.2
Decomposition Method 2 (DM2)
We proceed by observing an equivalent form of Eq. (3), P1+2 −P2 = P1 −P1*2 and
rewriting (1):
Table 2. Decomposition method 1
x
A1
A2
A1*2 and A1+2
0  x < a P1 = 0
P2 = 0
P1*2 = 0, P1+2 = 0
a  x < b P1 = P P2 = 0
P = P1, P1*2 = 0, P1+2 = P1
b  x < c P1 = 1
P2 = 1−P P = 1 −P2, P1*2 = P2, P1+2 = 1
c  x
P1 = 1
P2 = 1
P1*2 = 1, P1 + P2 −P1*2 = 1
790
M. Hodzic

PðxÞ ¼ F1ðxÞF2ðxÞ
ð7Þ
¼ P1P12 ¼ P1 þ 2P2
ð8Þ
Note that both terms in (8) are standard (unsigned) probabilities which are positive
and less than 1. The terms −P1*2 and −P2 are not negative probabilities per the deﬁ-
nition of signed probabilities, they are just being subtracted (operated on) from two
larger probabilities than themselves. Hence this way we avoid negative probability
issue. We have two possibilities here.
General Case. Here (7) and (Equ8) hold but F1(x) 6¼ P1 and F2(x) 6¼ P1*2 plus
F1(x) 6¼ P1+2 and F2(x) 6¼ P2. The idea is that an intersection of two events A1 \ A2
can produce probability non uniqueness, with P1 ﬁxed. The events A2 and A1 \ A2
and their probabilities P2 and P1*2 are to be determined. Different A2 can produce the
same intersection A1 \ A2 for a common A1. First note that P1+2 and P1 satisfy an
obvious condition:
PðxÞ  P1 þ 2 and P1  1
ð9Þ
A little reﬂection on basic probability axioms and set theory brings us to:
0  P2  1  PðxÞ  PðxÞ
ð10Þ
and P*(x) is a complementary PosD (6). We also have related condition:
0  P12  1  PðxÞ  PðxÞ
ð11Þ
One can consider the interplay of non unique P1 and non unique P2 in P1*2 pro-
ducing “fuzziness” on rising side of P(x) in Fig. 1. Falling side has no random non
uniqueness (Table 3), and probability P2 is complementary to P(x).
Simpliﬁed Case. Here (9)–(11) still hold but for rising side we choose F1(x) = P1
and F2(x) = P1*2, and for falling side F1(x) = P1+2 and F2(x) = P2. In this case con-
ditions (9)–(11) simplify and we have an equivalent to TM1 in Table 1 with no random
events non uniqueness.
Table 3. Decomposition method 2
x
A1
A2
A1*2 and A1+2
0  x < a P1 = 0
P2 = 0
P = 0, P1*2 = 0, P1+2 = 0
a  x < b
Rising
P1 non unique P2 non unique P = P1 −P1*2
P1+2 = P1 + P2 −P1*2, P1*2
b  x < c
Falling
P1 = 1
P2 = 1 −P
P = 1 −P2 = 1 −P1*2
P1*2 = P2, P1+2 = 1
c  x
P1 = 1
P2 = 1
P = 0, P1*2 = 1, P1+2 = 1
Soft to Hard Data Transformation
791

3.1.3
Decomposition Method 3 (DM3)
We proceed by using (1) and (3) and write:
F1ðxÞ ¼ P1 þ 2 ¼ P1 þ P2P12
ð12Þ
for the rising part of P(x). Next note that the constraints are:
0  P1  PðxÞ; 0  PðxÞP1  P2  PðxÞ
ð13aÞ
0  P12  P1  PðxÞ
ð13bÞ
which is also equivalent to:
0  P1; P2; P12  PðxÞ
ð14Þ
A1 is chosen ﬁrst and A1 \ A2 and A2 follow. At the top of the rising side we have
P1+2 = 1, P1 = 1 and P2 = P1*2 = p for some 0  p  1, and A2 is a subset of A1.
The falling part is equivalent to DM1 and DM2. Table 4 has a summary.
Note that all the methods using just a pair of related random events A1 and A2 only
produce random non uniqueness for the rising part of the distribution P(x). The falling
part is modeled by ﬁxed probability only.
3.2
Four Random Events Decomposition
This method looks at rising part via a pair of random events, A1 and A2, and falling part
with another pair of random events, A3 and A4. The idea is to produce random non
uniqueness on both sides of PosD P(x). As stated earlier random events (A1, A2) and
(A3, A4) are generated in different “experiments”.
3.2.1
Decomposition Method 4 (DM4)
The rising side is equivalent to DM2 and it culminates with F1(x) = 1 i.e. P1+2 = 1,
P1 = 1 and P2 = P1*2 = p, for some 0  p  1, and A2 is a subset of A1. To produce
probability non uniqueness on the falling side we introduce additional pair of random
events A3 and A4 so that P(x) is equal to:
Table 4. Decomposition method 3
x
A1
A2
A1*2 and A1+2
0  x < a P1 = 0
P2 = 0
P = 0, P1*2 = 0, P1+2 = 0
a  x < b
Rising
P1 non unique P2 non unique P = P1+2 = P1 + P2 - P1*2
P1, P2 and P1*2 (13)
b  x < c
Falling
P1 = 1
P2 = 1 −P
P = 1 −P2 = 1 −P1*2
P1*2 = P2, P1+2 = 1
c  x
P1 = 1
P2 = 1
P = 0, P1*2 = 1, P1+2 = 1
792
M. Hodzic

PðxÞ ¼ F1ðxÞ  F2ðxÞ ¼ 1  F2ðxÞ
¼ 1  P3  P34
ð
Þ ¼ 1  P3 þ 4  P4
ð
Þ
ð15Þ
which has an equivalent form as:
PðxÞ þ F2ðxÞ ¼ PðxÞ þ P3 þ 4  P4
ð
Þ
¼ PðxÞ þ P3  P34
ð
Þ ¼ 1
ð16Þ
An alternative form is P(x) + P(x)*  1, where
PðxÞ   P3P34; PðxÞ   P3 þ 4P4
ð17Þ
is complementary PosD of P(x) for the falling part. The probability constraints are as
in (13a) and (14) with new events A3 and A4:
0  P3 þ 4; P3  PðxÞ
ð18Þ
0  PðxÞP3  P4  PðxÞ
ð19Þ
0  P34  PðxÞ
ð20Þ
Random event A3 is chosen ﬁrst and A3 \ A4 and A4 follow with their corre-
sponding probabilities. See Table 5 for a summary. Note that the role of events A1 and
A2 can be reversed, and similarly for A3 and A4, i.e. either pair of events can be chosen
ﬁrst. In general there may be some TBD probabilistic connection between events (A1,
A2) and (A3, A4). At this point we assume all the events to be independent and “timed’
apart from each other as in Fig. 1.
3.2.2
Decomposition Method 5 (DM5)
The rising part is equivalent to DM3 and it culminates with P1+2 = 1, P1 = P2 =
P1*2 = 1 or P1 = 1 and P2 = P1*2 = p for some 0  p  1, and A2 is a subset of A1.
The falling part corresponds to:
Table 5. Decomposition method 4
x
A1, A3
A2, A4
A1 versus A2, A3 veresus A4
0  x  a P1 = 0
P3 = 0
P2 = 0
P4 = 0
P = 0, P1*2 = 0, P1+2 = 0
P3*4 = 0, P3+4 = 0
a  x  b
P3 = P4 = 0
P1 non unique P2 non unique P = P1 −P1*2
P1+2 = P1 + P2 −P1*2, P1*2 (11)
b  x  c
P1 = P2 = 1
P3 non unique P4 non unique P = 1 −(P3 −P34)
P3+4 = P3 + P4 −P3*4, P3*4 (20)
c  x
P1 = 1
P3 = 1
P2 = 1
P4 = 1
P = 0, P1*2 = 1, P1+2 = 1
P3*4 = 1, P3+4 = 1
Soft to Hard Data Transformation
793

PðxÞ ¼ 1  F2ðxÞ ¼ 1  P3 þ P4  P34
ð
Þ
ð21Þ
which is equivalent to P(x) + F2(x), or:
PðxÞ þ P3 þ 4 ¼ PðxÞ þ P3 þ P4  P34
ð
Þ ¼ 1
ð22Þ
An alternative form is P(x) + P(x)*  1, where
PðxÞ  P3 þ P4  P34
ð23Þ
is complementary of P(x) for the falling part. The probability limits are as in (9), (19)
and (11) with new events A3 and A4:
PðxÞ  P3 þ 4 and P3  1
ð24Þ
0  P3 þ 4 and P2  1  PðxÞ  P  ðxÞ
ð25Þ
with A3 chosen ﬁrst and A3 \ A4 and A4 follow see Table 6 for a summary. Note that
the role of events A1 and A2 can be reversed, and similarly for A3 and A4, i.e. either
pair of events can be chosen ﬁrst. We assume independence among various events.
3.3
Probability Constraints
Based on the methods in this Section we conclude that there are two types of proba-
bility constraints:
Type 0 Constraints
Rising:
P1 ¼ PðxÞ
ð26aÞ
Falling:
P2 ¼ 1  PðxÞ
ð26bÞ
Type 1 Constraints (Rising or Falling)
PðxÞ  P1 þ 2; P1  1
ð27aÞ
Table 6. P(x) transformation method 5
x
A1, A3
A2, A4
A1 vs. A2, A3 vs A4
0  x < a P1 = 0
P3 = 0
P2 = 0
P4 = 0
P = 0, P1*2 = 0, P1+2 = 0
P3*4 = 0, P3+4 = 0
a  x < b
P3 = P4 = 0
P1 non unique P2 non unique P = P1 + P2 −P12
0  P1 + P2 - P12  1
b  x < c
P1 = P2 = 0
P3 non unique P4 non unique P = 1 −(P3 + P4 −P34)
P3+4 = P3 + P4 - P3*4, P3*4
c  x
P1 = P3 = 1
P2 = P4 = 1
P = 0, P1*2 = 1, P1+2 = 1, P3*4 = 1, P3+4 = 1
794
M. Hodzic

0  P2  1  PðxÞ  P  ðxÞ
ð27bÞ
0  P12  1  PðxÞ  P  ðxÞ
ð27cÞ
Type 2 Constraints (Rising or Falling):
0  P1  PðxÞ
ð28aÞ
0  PðxÞP1  P2  PðxÞ
ð28bÞ
0  P12  P1  PðxÞ
ð28cÞ
or:
0  P1; P2; P12  PðxÞ
ð29Þ
Instead of P1, P2, P1*2 and P1+2, we could have used P3, P4, P3*4 and P3+4
depending on which side of P(x) we operate on (see Table 6). We use these constraints
in the next section to deﬁne total probability change and Uncertainty Balance Principle
that comes out of that.
3.4
Decomposition Methods Summary
Table 7 has a summary of all the decomposition methods and corresponding proba-
bility constraints.
3.5
Half Trapezoidal Decomposition Methods
Note from the above transformation methods that left half trapezoidal PosDs can be
decomposed using just a pair of related random events for there is no falling part which
would require additional pair of random events. For the right half trapezoidal distri-
bution we would need a certain event with probability 1 to start with, plus a pair of
random events to reduce presumption level down to 0.
Table 7. Methods summary
Probability constraints
Method Rising side Falling side No. of random events
DM1
Type 0
Type 0
2
DM2
Type 1
Type 0
2
DM3
Type 2
Type 0
2
DM4
Type 1
Type 2
4
DM5
Type 2
Type 1
4
Soft to Hard Data Transformation
795

4
Uncertainty Balance Principle Summary
The main goal of this section is to clarify and extend our previous results on Uncer-
tainty Balance Principle (UBP) reported in [44] and produce usable and practical results
to relate fuzzy (soft) and non unique (hard) random data.
4.1
Summary of General Results
From [44] and Sect. 2.3, for unimodal P(x) we have the following general result, per
Fig. 1.
Theorem 1 Any unimodal P(x) can be decomposed as a difference of a pair of
ProCD:
PðxÞ ¼ F1ðxÞF2ðxÞ
ð30Þ
for any uncertain simultaneously possibilistic and probabilistic variable X = x ■
In terms of speciﬁc probabilities, various methods are described in Sect. 3. Using
(31) we have:
Corollary 1 Any unimodal P(x) of Theorem 1 can be further expressed as:
Rising side of P(x)
DM2 & DM4 :
PðxÞ ¼ P1P12 ¼ P1 þ 2P2
ð31Þ
DM3 & DM5 :
PðxÞ ¼ P1 þ 2
ð32Þ
Falling side of P(x):
DM2 & DM3 :
PðxÞ ¼ 1P2
ð33Þ
DM4 & DM5 :
PðxÞ ¼ 1P3 þ 4
ð34Þ
for any uncertain ProPos variable X = x ■
Next we introduce the following:
Deﬁnition 1 Total probability range is deﬁned as:
DP1 þ 2 ¼ DP1 þ DP2  DP12
ð35Þ
with:
DPi ¼ P Ai
ð
ÞMP Ai
ð
Þm; i ¼ 1; 2
ð36Þ
DP12 ¼ PðA1 \ A2ÞMPðA1 \ A2Þm
ð37Þ
deﬁned as probability range for the indicated events, with “M” maximum, and “m”
minimum value ■
One can also write (33) using (34)–(35) as:
796
M. Hodzic

DP1 þ 2 ¼ ½P A1
ð
ÞM þ P A2
ð
ÞMPðA1 \ A2ÞM
 ½P A1
ð
Þm þ P A2
ð
ÞmPðA1 \ A2Þm
ð38Þ
or:
DP1 þ 2 ¼ PðA1 [ A2ÞM  PðA1 [ A2Þm
ð39Þ
representing a probability range of union of two evens A1 and A2.
We now restate our main result [45] which relates a fuzzy distribution and a
probability range for some related random event probabilities.
Theorem 2 Type 1 probability constraints result in presumption-invariant and
x-invariant universal fuzzy-random Uncertainty Balance Principle (UBP):
PðxÞ þ DP1 þ 2 ¼ 1
ð40Þ
P  ðxÞ  DP1 þ 2
ð41Þ
for any uncertain ProPos variable X = x ■
The above Theorem 2 expresses PosD as a difference of a pair of variable random
events A1 and A2 probabilities. The key feature of Theorem 2 is that it holds for any x
and any presumption level of P(x). The name UBP indicates that a PosD P(x) and
some TBD and related random ΔP1+2 add up to a certainty expressed by “1”. Yet
another interpretation is that numerical value of complementary PosD P * (x) is
always greater or equal to a total range of probabilities for two related random events
union. Loosely speaking Theorem 2 indicates that “fuzziness” can be interpreted as
“randomness” range. Proof is presented in [45]. Next we have:
Corollary 2 From Theorem 2, any unimodal PosD derivative dP(x)/dx can be
expressed as a universal fuzzy-random Uncertainty Change Law (UCL):
dPðxÞ=dx ¼ dðDP1 þ 2Þ=dx
ð42Þ
dP  ðxÞ=dx  dðDP1 þ 2Þ=dx
ð43Þ
for any uncertain ProPos variable X = x ■
Simply stated, (42) says that the change in fuzzy distribution is the opposite of
probability change. Figure 4 summarize Theorem 2 and Corollary 2. The diagrams are
universal for any P(x) with Type 2 probability constraints.
Type 2 Constraints produce:
Theorem 3 Type 2 probability constraints result in presumption-invariant and
x-invariant universal fuzzy-random Uncertainty Balance Principle (UBP):
PðxÞ ¼ DP1 þ 2
ð44Þ
P  ðxÞ þ DP1 þ 2  1
ð45Þ
Soft to Hard Data Transformation
797

for any uncertain ProPos variable X = x ■
Corollary 3 From Theorem 3, any unimodal PosD derivative dP(x)/dx can be
expressed as a universal fuzzy-random Uncertainty Change Law (UCL):
dPðxÞ=dx ¼ dðDP1 þ 2Þ=dx
ð46Þ
dP  ðxÞ=dx   dðDP1 þ 2Þ=dx
ð47Þ
for any uncertain simultaneously possibilistic and probabilistic variable X = x ■
Equation (46) states that the change in fuzzy distribution is aligned with probability
change, the opposite of (42). Hence Type 1 and Type 2 constraints produce different
decomposition results. Theorem 3 and Corollary 3 are presented in Fig. 5. The dia-
grams are universal for any P(x) with Type 2 probability constraints. Note that The-
orem 1 and Corollary 1 indicate numerical algorithms for calculating various
probabilities for a given PosD P(x), and Theorems 2 and 3 state probability constraints.
Corollaries 2 and 3 state PosD and ProCD rate changes.
We also note that Theorems 2 and 3 can be used in “reverse”, namely to express
some given range of probabilities in terms of “an equivalent” PosD P(x). As we stated
earlier in this paper, we consider PosD as representing an uncertainty before some
related “experiment” and ProD as representing an uncertainty after the “experiment”.
Hence fuzziness turns into randomness after this experiment, and before it, it is looked
as a non unique randomness. Why would randomness turn into fuzziness is less clear,
but it could be related to the existence of some “hidden” random distributions. We will
look into this problem in future research, in addition to related comments in Sect. 5. At
the end of this Section we state an intuitive notion which states that a possibility of an
uncertain event is always bigger than its probability (i.e. before and after an “experi-
ment” or possibility comes before probability):
∆P1+2
1
Π(x)
dΠ/dx
d(∆P1+2)/dx
1
Fig. 4. Theorem 2 and Corollary 2
∆P1+2
1        Π(x)
dΠ/dx
d(∆P1+2)/dx
1
Fig. 5. Theorem 3 and Corollary 3
798
M. Hodzic

ProD  PosD
ð48Þ
5
Consistency Index
5.1
Basic Deﬁnitions
We recall Consistency Principle (Index) between fuzzy and random variable x deﬁned
in Zadeh’s classic paper [10] as:
CX ¼
X
P xi
ð ÞP xi
ð Þ ¼ P1P1 þ P2P2 þ    þ PNPN
ð49Þ
where P is over i = 1,…, N, P(xi) = Pi, P(xi) = Pi and variable x is a ProPos variable
consisting of the same number of choices in an X interval of interest. If there is a
precise point wise match, Pi = Pi, then:
CX ¼
X
P2
i ¼
X
P2
i
ð50Þ
In the spirit of (41) and per UBP Theorem 3 we have, with simpliﬁed notation
ΔP1+2(xi) = ΔP(xi):
Deﬁnition 1 Consistency Index C and its complementary index C* are deﬁned as a
sum of the point wise products:
C ¼
X
C xi
ð Þ ¼
X
DP xi
ð ÞP xi
ð Þ
ð51Þ
C ¼
X
C  xi
ð Þ ¼
X
DP xi
ð ÞP  xi
ð Þ
ð52Þ
across the range of xi for non zero values of both ΔP(xi) and P(xi), where C(xi) =
ΔP(xi)P(xi) and C*(xi) = ΔP(xi)P*(xi) are individual products, with i = 1,…, N ■
For our purposes in this paper we also have, not in the spirit of (42), but following
Theorem 2, an alternative Index which expresses a (numerical) level of agreement
between ProCD and PosD functions of ProPos uncertain variable X:
Deﬁnition 2 Consistency Index CS is deﬁned as a sum of the point wise sums:
CS ¼
X
CS xi
ð Þ ¼
X
½DP xi
ð Þ þ P xi
ð Þ
ð53Þ
across the range of xi for non zero values of both ΔP(xi) and P(xi) where CS(xi) = ΔP
(xi) + P(xi) are individual sums, with i = 1,…,n ■
5.2
ProPos Consistency Square
Consistency Index (CI) carries an intuitive observation that reducing the possibility of
an event tends to reduce its probability. The opposite may not hold. Table 7 summarize
these intuitive notions which we put in a precise relationships in this paper. The symbol
Soft to Hard Data Transformation
799

“!” indicates an implication. Figure 6 is a graphical representation of Table 7 where
shaded areas indicate “consistency” area. Let us call this diagram ProPos Consistency
Square (PPCS). We will also relate these diagrams to Deﬁnitions 2.1 and 2.2. CI may
be useful when possibility is known about uncertain event but not the probability. Our
paper expands this idea and its applicability via Uncertainty Balance Principle
(UBP) which produces variable probability from given possibility.
Using Fig. 6a and b, Tables 8 and 9 can be reinterpreted and made more precise in
terms of probabilistic and possibilistic differences, as shown next in Tables 10 and 11.
Note a complementary nature of the entries in Tables 10 and 11. For example
“complement” of Table 11 ﬁrst row is equivalent to Table 10 s row and likewise,
complement of Table 11 s row is equivalent to Table 10 ﬁrst row, i.e.:
LowPOS


 ! LowPRO



¼ HighPRO ! HighPOS
ð54Þ
and
Low             
High   Probability
Low                 
High   Probability
1
High
Low
Posibility
Posibility
1
High
Low
(a)
(b)
Fig. 6. a (Table 8) b (Table 9)
Table 8. From probabilistic to possibilistic
Probabilistic
Possibilistic
Low probability !
Not necessarily low possibility
High probability ! High possibility
800
M. Hodzic

HighPOS


 !
High  Low
ð
ÞPRO
h
i

¼ LowPRO ! Low  High
ð
ÞPOS
ð55Þ
Note also that the above intuitive analysis implies (48), i.e. PosD  ProD. In our
speciﬁc case this will also imply P(xi)  ΔP(xi) as we show bellow, once we employ
Sects. 2 and 3 decomposition methods.
From Theorems 2 and 3 and Deﬁnition 2.1 we have Fig. 7, showing four ProPos
consistency triangles within ProPos Square. These triangles are essential in our Sects. 2
and 3 decomposition methodology.
Note that one ProSquare Square applies for rising portion of P(x), and another one
for falling portion. Due to symmetricity in P and P* terms in Theorems 2 and 3, Fig. 7
can be also drawn with P* instead of P, and with ΔP* instead of ΔP, by simply
replacing the two. At this point we need to relate ProPos square to Type 1 and Type 2
constraints from Sect. 4.2. This will allow us to deﬁne corresponding Consistency
Principles.
5.3
Consistency Index Optimization
5.3.1
Type 1 Constraints
We now proceed with ﬁnding any extreme point inside the ProPos square which can be
used for the purposes of ﬁnding decomposition probabilities of Sects. 2 and 3 for the
PosD P(x). We recall (43) and write for a speciﬁc point x = xi:
C1 xi
ð Þ ¼ DP xi
ð ÞP xi
ð Þ
ð56Þ
Using Theorem 2 we further write:
Table 9. From possibilistic to probabilistic
Possibilistic
Probabilistic
Low possibility !
Low probability
High possibility ! Not necessarily high probability
Table 10. From probabilistic to possibilistic
Probabilistic Possibilistic
LowPRO !
(Low-High)POS = ΔPOS
HighPRO !
HighPOS
Table 11. From possibilistic to probabilistic
Possibilistic Probabilistic
LowPOS !
LowPRO
HighPOS ! (High-Low)PRO = ΔPRO
Soft to Hard Data Transformation
801

C1 xi
ð Þ ¼ ½ 1  P xi
ð Þ
ð
P xi
ð Þ ¼ P xi
ð Þ  P2 xi
ð Þ
ð57Þ
Recalling second term in (4) we also have:
C1 xi
ð Þ  P  xi
ð ÞP xi
ð Þ
ð58Þ
Next, ﬁrst derivative dC1(xi)/dP(xi) = 1 −2P(xi) = 0 and second derivative
d2C1(xi)/[dP(xi)]2 = −2 produce C1(xi) maximum:
CMAX
1
xi
ð Þ ¼ 1=4
ð59aÞ
at:
P xi
ð Þ ¼ 1=2
ð59bÞ
Figure 8a and b show C1(xi) as the upper shaded rectangular area, for small and
large values of P(xi) respectively.
C1(xi) also corresponds to the ﬁrst entry in Table 11 i.e. LowPRO implies
(Low-High)POS = ΔPOS. This conﬁrms that Theorem 2 also corresponds to the intuitive
notion of the second entry in Table 9. Similarly upper shaded area in Fig. 8b corre-
sponds to C1(x). It also corresponds to the second entry in Table 11, i.e. HighPRO
implies HighPOS. To illustrate how to determine probabilities P1 and P2 we recall (12)
for DM3 and DM5, and conclude that C1
MAX(xi) = ¼ is obtained for P(xi) = ½ and in
turn for P(xi) = P1+2 = ½ = P1 + P2 – P1*2. Hence any combination of P1, P2, and P1*2
producing P1+2 = ½ also maximizes C1(xi). This fact can be used when implementing
real SW system for P(xi) decomposition. Per (51) the total Consistency Index C1 is
sum over i = 1, 2,…, N:
Posibility Π
Probability ∆P
Π = ∆P
Π + ∆P = 1            
Π + ∆P ≥1
Π ≥∆P
Π + ∆P ≤1
Π ≥∆P
Π + ∆P ≥1
Π ≤∆P
Π + ∆P ≤1
Π ≤∆P
Fig. 7. ProPos square breakdown
802
M. Hodzic

C1 ¼
X
C1 xi
ð Þ ¼
X
DP xi
ð ÞP xi
ð Þ ¼
X
½P xi
ð Þ  P2 xi
ð Þ
ð60Þ
and it applies for Type 1 constraints.
5.3.2
Type 2 Constraints
We again recall (44) and write:
C2 xi
ð Þ ¼ DP xi
ð ÞP xi
ð Þ
ð61Þ
Using Theorem 3 we further write:
C2 xi
ð Þ ¼ P xi
ð ÞP xi
ð Þ ¼ P2 xi
ð Þ
ð62Þ
In this case C2(xi) maximum is obtained as:
CMAX
2
xi
ð Þ ¼ 1
ð63aÞ
at:
P xi
ð Þ ¼ 1
ð63bÞ
Figure 8a shows C2(xi) as the lower shaded rectangular area, for small values P(xi).
It also corresponds to the ﬁrst entry in Table 10, hence. LowPOS implies LowPRO. This
conﬁrms that our Theorem 3 also corresponds to the intuitive notion represented by the
ﬁrst entry in Table 8. Similarly lower shaded area in Fig. 8b corresponds to C2(xi).
It
also
corresponds
to
the
second
entry
in
Table 11,
i.e.
HighPOS
implies
(High-Low)PRO = ΔPRO. This conﬁrms that Theorem 3 corresponds to the intuitive
notion represented by the ﬁrst entry in Table 8. Note that we can interpret C1(xi) and
C2(xi) as some generalized “transformation” energy. Based on (46) and (50), and Fig. 9
we have Table 12.
Deﬁnition 3 Total Consistency Index CT(x) is deﬁned as the sum:
Γ1
1 
1/2
Π
Γ2
1 
Γ 1
Γ 2
1
∆P
1/2     Probability              
1/2   ∆P
Probability   
Posibility
Posibility
1
Π
(a)
(b)
Fig. 8. a Small P, C1 > C2 b High P, C1 < C2
Soft to Hard Data Transformation
803

CTðxÞ ¼ C1ðxÞ þ C2ðxÞ
ð64Þ
and it captures both small and large ProPos values ■
The immediate and simple result follows.
Theorem 4 Total Consistency Index CT(x) is:
CTðxÞ ¼ PðxÞ  P2ðxÞ þ P2ðxÞ ¼ PðxÞ
ð65Þ
6
Probability Optimization Problem
For each Decomposition Method and one of the corresponding probability constraint
type, numerical optimization problem is as follows:
Given:
1. PosD P(x)
2. Type 1 or Type 2 probability constraint
3. Consistency Principle C1 and C2
4. One algebraic equation with given P(x) and two unknowns P1 and P2
Determine:
P1
1 -Π
P2
Π = 0.1  0.25
0.5
0.8
Fig. 9. Type 1 constraints P2 versus P1
Table 12. C1(x) versus C2(x) comparison
C1(x) versus C2(x) PosD P(x)
Comment
C1(x) > C2(x)
0  P(x)  1/2 More energy in C1(x)
C1(x) = C2(x)
P(x) = 1/2
Same “Balanced” energy
C1(x) < C2(x)
1/2  P(x)  1 More energy in C2(x)
804
M. Hodzic

5. Choose one of the probabilities, for example P1 using some optimization rationale
6. The second probability follows from the algebraic equation
At this point we recall (i) Theorem 1, Corollary 1 as well as (ii) Theorems 2 and 3.
Hence we relate P(x) decomposition methods and probability constraints which must
be satisﬁed.
Corollary 4 Given PosD function P(x), it is decomposed using one of the methods of
Corollary 1, and it must obey UBP of Theorem 2 or Theorem 3, depending on
probability constraints type for any uncertain ProPos variable X = x ■
Next we look into Type 1 and Type 2 constraints to determine unknown
probabilities.
6.1
Probability Optimization
6.1.1
Type 1 Constraints
Given P(x) and Type 1 constraints, for dependent random events A1, A2 we have:
PðxÞ ¼ P1  P12 ¼ P1  P2=1P1 ¼ P1 1  P2=1


ð66Þ
For independent events A1 and A2 we have:
PðxÞ ¼ P1 1  P2
ð
Þ ¼ P1P
2
ð67Þ
The numerical optimization problem consists of a set of Type 1 probability con-
straints and one algebraic Eq. (66) or (67) with two unknowns, P1 and P2.
In the spirit of Tables 9 and 11, we separate the optimization problem into two, for
small and for large values of P(x).
Small P(x) Optimization Problem: Given small P(x) < 0.5, determine proba-
bilities P1 and P2 such that (66) or (67) is satisﬁed and the difference between two
probabilities is the smallest, i.e.:
Min Abs P1P2
ð
ÞSubject to ð66Þ or ð67Þ
ð68Þ
This way we come the closest to Table 11 ﬁrst entry LowPOS ! LowPRO.
Large P(x) Optimization Problem: Given large P(x) > 0.5, determine proba-
bilities P1 and P2 such that (66) or (67) is satisﬁed and the difference between two
probabilities ranges from the smallest to the largest, i.e.:
Min Abs P1P2
ð
ÞSubject to ð66Þ or ð67Þ
ð69aÞ
Max Abs P1P2
ð
ÞSubject to ð66Þ or ð67Þ
ð69bÞ
This way we come the closest to Table 11 s entry HighPOS ! (High-Low)PRO.
6.1.2
Type 2 Constraints
Given P(x) with Type 2 constraints we have for dependent A1 and A2:
Soft to Hard Data Transformation
805

PðxÞ ¼ P1 þ P2  P12 ¼ P1 þ P2  P2=1P1 ¼ P1 1  P2=1


þ P2
ð70Þ
For independent events A1 and A2 we have:
PðxÞ ¼ P1 1  P2
ð
Þ þ P2 ¼ P1P
2 þ P2
ð71Þ
The numerical optimization problem at this point consists of a set of Type 2
probability constraints and one algebraic Eq. (69a) or (70) with two unknowns, P1 and
P2 (Fig. 10).
We again separate the optimization problem into two, for small and for large values
of P(x). For small we repeat (68) and for large we repeat (69a, 69b) both with “Subject
to (70) or (71)”. The other comments related to Table 11 are equivalent.
7
Future Research
In this paper we continued our development of Uncertainty Balance Principle reported
earlier in (5) for possibilistic to probabilistic decomposition and transformation. The
intended use, besides purely theoretical development, is practical issue of Data Fusion,
such as soft and hard data fusion used in many decision making problems in practice.
There are several remaining issues to solve, in particular:
1. How to use Consistency Index in optimizing probabilities
2. Determining probabilities for some practical applications such as banking risk
calculation
3. Building soft and hard data bases for speciﬁc applications
4. Determining probabilities using Big Data methodologies
5. Producing commercial optimization SW
Also, we will look into an opposite problem in more details, i.e. transformation of
probabilistic (hard) data into possibilistic (soft) data. One can argue that this is of no
P1
Π = 0.1  0.25  0.5
0.8 0.85
P2
Fig. 10. Type 2 constraint P2 versus P1
806
M. Hodzic

practical use, due to better information content of hard data. We believe there are some
interesting issues in this context as well.
8
Conclusion
In this paper we continue our earlier development of Uncertainty Balance Principle in
the context of possibilistic to probabilistic transformation. In particular we clariﬁed
some earlier results, presented them in much simpler form, as well as extended them.
We continue employ the most basic properties of random and fuzzy distributions for
this research, starting from fuzzy PosDs decomposed as a combination of probabilistic
cumulative distribution functions, ProCDs, rather than probabilistic density functions,
ProPD’s, which may not always exist. We also extended a notion of Consistency Index
and gave it more speciﬁc mathematical and practical description. A new notion of
ProPos Consistency Square is deﬁned which combines in one graphical view both
possibilistic and probabilistic quantities. We believe this to be an elegant and useful
way to simultaneously look into both type of uncertain data. Also, simple optimization
scheme is presented to choose various probabilities for small and large values of
possibilistic distribution.
References
1. Kaufmann, A., Gupta, M.M.: Introduction to Fuzzy Arithmetic, Theory and Applications.
Reinhold, Van Nost (1985)
2. Zadeh, L.A.: Possibility theory and soft data analysis. Selected papers by LotﬁZadeh,
pp. 515–541 (1981)
3. Jenkins M.P. et al.: Towards context aware data fusion: modeling and integration of
situationally qualiﬁed human observations to manage uncertainty in a hard-soft fusion
process. Inf. Fus. 21, 130–144 (2015)
4. de Finetti, B.: Theory of Probability, 2 vols. Wiley, Inc., New York (1974)
5. Bernardo, J.M.: Reference analysis. Handbook of statistics 25, 17–90 (2005)
6. Jaynes, E.T.: Bayes methods: general background. In: Justice, J.H. (ed.) Maximum-Entropy
& Bayesian Methods in Applied Statistics. Cambridge U. Press (1986)
7. Couso I. et al.: Random Sets and Random Fuzzy Sets as Ill-Perceived Random Variables. An
Introduction for Ph.D. Students and Practitioners. Springer (2014)
8. Doob, J.L.: Stochastic Processes. Willey (1953)
9. Feller, W.: An Introduction to Probability Theory and Its Applications, vol. 1 and 2. Willey
(1950)
10. Zadeh, L.A.: Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets Syst. 1, 3–28 (1978)
11. Zimmermann, H.J.: Fuzzy Set Theory and its Applications, 4th edn. Kluwer Academic Publ.
(2001)
12. Dubois, D.: Possibility theory and statistical reasoning. Institut de Recherche en
Informatique de Toulouse, May 2006
13. Dubois, D., Prade, H.: Possibility theory probability theory and multiple valued logics: a
clariﬁcation. Annal. Math. Artif. Intell. 32, 35–66 (2002)
Soft to Hard Data Transformation
807

14. Dubois, D., Prade, H., Smets, P.: New semantics for quantitative possibility theory. In: 2nd
International Sympoium on Imprecise Probabilities and Their Applications, Ithaca, New
York (2001)
15. Dubois, D., Prade, H.: Unfair coins and necessity measures: towards a possibilistic
interpretation of histograms. Fuzzy Sets Syst. 10, 15–20 (1983)
16. Dubois, D., Prade, H.: Fuzzy sets and statistical data. Eur. J. Oper. Res. 25, 345–356 (1986)
17. Dubois, D., Prade, H.: The mean value of a fuzzy number. Fuzzy Sets Syst. 24, 279–300
(1987)
18. Dubois, D., Prade, H.: Possibility Theory. Plenum Press, New York (1988)
19. Dubois, D., Prade, H.: When upper probabilities are possibility measures. Fuzzy Sets Syst.
49, 65–74 (1992)
20. van der Helm, R.: Towards a clariﬁcation of probability, possibility and plausibility: How
semantics could help futures practice to improve. Foresight 8(3), 17–27 (2008)
21. Agarwal, P., Najal, H.S.: Possibility theory vs possibility theory in fuzzy measure theory.
P. Agarwal Int J Eng. Res. Appl. 5(5), 37–43 (2015)
22. Coolen F.P.A. et al., Imprecise probability. In: International Encyclopedia of Statistical
Science. Springer (2010)
23. Zadeh, L.A.: Is there a need for fuzzy logic? Elsevier. Info. Sci. 178, 2751–2779 (2008)
24. Zadeh, L.A.: Fuzzy sets. Inf. Control 8, 338–353 (1965)
25. Bellman, R., Zadeh, L.: Decision-making in a fuzzy environment. Manag. Sci. 17(4) (1979)
26. Eschenbach, W.: Triangular Fuzzy Numbers and the IPCC, Feb 2012
27. Creating Membership Functions (PID, Fuzzy Logic Toolkit) LabVIEW, National Instru-
ments (2011)
28. Kaur, B., Bala, M., Kumar, M.: Comparitive analysis of fuzzy based wildﬁre detection
techniques. Int. J. Sci. Eng. Res. 5(7) (2014)
29. Şentürk, S.: Fuzzy regression control chart based on a-cut approximation. Int. J. Comput.
Intell. Syst. 3(1), 123–140 (2010)
30. Onuwa, O.B.: Fuzzy expert system for malaria diagnosis. Orient. J. Comput. Sci. Technol. 7
(2), 273–284 (2014)
31. Babashamsi, P., Golzadfar, A., Yusoff, N.I., Ceylan, H., Nor, N.G.: Integrated fuzzy analytic
hierarchy process and VIKOR method in the prioritization of pavement maintenance
activities. Int. J. Pavement Res. Technol. (2016)
32. Garibaldi, J.M., John, R.I.: Choosing membership functions of linguistic terms (2016)
33. Raufaste, E., Neves, R.D.S.: Empirical evaluation of possibility theory in human radiological
diagnosis. In: Prade, H. (ed.) 13th European Conference on Artiﬁcial Intelligence. Wiley
(1998)
34. Iancu, I., Mamdani, A.: Type fuzzy logic controller. In: Dadios, E. (ed.) Fuzzy logic—
controls, concepts, theories and applications. In Tech, Mar (2012)
35. Yang, M.S., Liu, M.C.: On possibility analysis of fuzzy data. Elsevier, Fuzzy Sets Syst. 94,
171–183 (1998)
36. Narukawa, Y., Torra, V., Gakuen, T.: Fuzzy measure and probability distributions: distorted
probabilities (2016)
37. de Cooman, G.: Possibility theory 1, the measure- and integral-theoretic groundwork.
Universiteit Gent, Vakgroep Elektrische Energietechniek (1996)
38. Liu, B.: Why is there a need for uncertainty theory? J. Uncertain. Syst. 6(1), 3–10 (2012)
39. Mauris,
Gilles:
Possibility
distributions:
a
uniﬁed
representation
of
usual
direct-probability-based parameter estimation methods. Int. J. Approx. Reason. 52, 1232–
1242 (2011)
40. Sanchez, L., Casillas, J., Cord, O., Jose del Jesus, M.: Some relationships between fuzzy and
random set-based classiﬁers and models. Int. J. Approx. Reason. 29, 175–213 (2002)
808
M. Hodzic

41. Shapiro, A.F.: Fuzzy random variables. Insur. Math. Econ. 44, 307–314 (2009)
42. Sentuik, S.: Fuzzy regression control chart based on a-cut approximation. Int. J. Comput.
Intell. Syst. 3(1), 123–140 (2010)
43. Hodzic, M.: Fuzzy to random uncertainty alignment. Southeast Eur. J. Soft Comput. 5(1),
58–66 (2016)
44. Hodzic, M.: Uncertainty balance principle. PEN 4(2) (2016)
45. Zadeh, L.A.: The concept of a linguistic variable and its application to approximate
reasoning. Inf. Sci. Part I: 8, 199–249, Part II: 8, 301–357; Part III: 9, 43–80 (1975)
Soft to Hard Data Transformation
809

Programming and Experimental Analysis
of MELFA RV-2SDB Robot
Edin Mujčić(&), Sabina Lonić, and Mersa Muminović
University of Bihać, Bihać, Bosnia and Herzegovina
{edin.mujcic,darkieftw,mersa.muminovic}@gmail.com
Abstract. With the development of technology and science, it became clear
that robots are starting to have a major role in the modern world. Regardless of
whether they’re meant to do hard work or just entertain, robotics has changed
human life for the better. What makes a robot different from a man is that a robot
doesn’t have the ability to make decisions by itself, so it needs to have a set of
instructions previously written by a man. The topic of this paper is programming
and experimental analysis of MELFA RV-2SDB robot’s work. The content of
this paper is based on development of robotics as a science branch, the structure
of a robot, as well as controlling one, with its kinematics and dynamics. That is
followed by an explanation of controlling and programming a MELFA
RV-2SDB robot. Programming has been done with CIROS Studio tool using
MELFA BASIC V programming language. Practical part consists of work
analysis and programming of the mentioned robot. Its task is to move LEGO
cubes from one place to another, composing a desired form as the result.
Keywords: Robot  MELFA RV-2SDB  CIROS studio  MELFA BASIC V
1
Introduction
Fast development of science has lead man to new discoveries and inventions, which
would make life easier for humans, including robots. Robots are considered as machines,
which take the role of performing jobs and various tasks that are too hard or dangerous for
a man. They have proven very useful in almost every ﬁeld of industry known to man
today. To be able to control a robot, a man must ﬁrst be thoroughly aware of the robot’s
structure, its abilities, as well as the expanse in which it’s placed, and can then move onto
programming the robot. In case of MELFA robots, the most common programming
language is MELFA BASIC V, while the programming is done in CIROS Studio [1].
Robotics as a term is a branch of engineering, which includes mechanical, electrical
and computer science, amongst others [2, 3]. Robotics is in charge of design, con-
struction and operations of robots, as well as computer systems needed to control them,
information processing and sensory feedback.
The goal of this technology is to develop a machine that would serve as a substitute
for a human. There are a lot of dangerous environments in which people work, so to
minimalize risks to the human life, scientists developed machines that are able to do the
same work a man does, for e.g. bomb detection and deactivation. Today’s robots are
capable of replicating some of the behavior typical for a human, such as walking,
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_70

lifting, speech, cognition and many others. They can be made in any shape, so the idea
behind making robots similar to humans lies in the attempt to get people to accept and
get accustomed to robots in their everyday life. The word robotics is derived from the
word robot, which comes from Slavic word “robota”, that translates to “labour” [4, 5].
With the development of technology and science, it became clear that robots are
starting to have a major role in the modern world. Regardless of whether they’re meant
to do hard work or just entertain, robotics has changed human life for the better.
There are a lot of types of robots, since their application ﬁeld is unlimited in today’s
world, but all robots share three similarities when it comes to their construction: metal
construction, electrical components and computer programming code. Metal construc-
tion is a frame, or a shape designed to achieve a particular task, electrical components
purpose is to power and control the machinery, and the computer programming code are
actually instructions which the robot follows. For example, a robot whose purpose is to
travel across heavy terrain, such as dirt or mud, would need caterpillar tracks which
represent the metal construction, power in form of electricity to be able to use those
tracks, and lastly the computer programming code which would tell it to move.
2
Robots and Types of Robots
There are many deﬁnitions of robots, but in general we can say robots are machines that
can be programmed by a computer and are capable of carrying out complex series of
actions automatically. Robots are divided by generations, from the period they were
invented, up until today [6].
Null generation robots were robots that couldn’t be reprogrammed, and they didn’t
have a control unit. These robots could’ve been programmed only once, and are used as
a replacement in manual labor, such as industrial welding robots, shown in the Fig. 1.
Fig. 1. Industrial welding robots
Programming and Experimental Analysis
811

First generation robots have the ability to manage joints, using an independent
actuating system. These robots have all the necessary sensors, but very limited intel-
ligence taking in consider that every part is separately programmed and managed. They
are used in areas where great precision isn’t needed.
Second generation robots work in accordance with the environment in which they
are, and have the ability to adapt in accordance with sensors that they have. They also
exchange information using the same sensors. Also, they can make logical decisions:
yes and no.
Third generation robots are highly intelligent robot systems, and they are most
similar to human’s characteristics and behavior. They have the ability to make deci-
sions in various situations, as well as learn new behavior.
Robots can be used for military purposes, like weapon systems that provide cal-
culations for accurate predicted ﬁre, or autonomous ﬁghter jets and bombers, for
medicinal purposes, such as surgical robots, rehabilitation robots and bio-robots, as
well as social and other purposes.
3
MELFA RV-2SDB Robot
Mitsubishi MELFA industrial robots are manufactured to work with high speed and
relatively high accuracy while working, combining different technology methods. They
are divided according to usage, which means some of them are made to perform
complex operations, while others are made to perform under high speed or precision.
MELFA RV-2SDB is a vertical robot type, shown in Fig. 2 above, whose purpose
is to perform complex operations. This type of robot has 6 degrees of freedom [1, 7, 8].
It uses CR1DA-700 controller. Supply voltage for this controller is AC 180–230 V.
Installed power required is 0.5 kW and the range of operating frequencies is 50–60 Hz.
Parts of MELFA RV-2SDB are shown in Fig. 3 MELFA RV-2SDB robot has a
Fig. 2. MELFA RV-2SDB robot
812
E. Mujčić et al.

capacity of 256 programs. Programming of MELFA is done using MELFA-BASIC IV
or MELFA-BASIC V programming language.
The robot can be manually controlled using Teaching Pendant (TP) controller,
which is connected to the previously mentioned CR1DA controller. Teaching Pendant
has a button to stop the robot even when it’s in automatic mode, if an error happens.
4
Programming of the MELFA RV-2SDB Robot
As previously mentioned, these types of robots are programmed using MELFA-BASIC
IV or MELFA-BASIC V programming language, and CIROS Studio interface. CIROS
Studio [9, 10] is an integrated development environment (short IDE), for Mitsubishi
robots, that supports fast and easy generation of MELFA-BASIC III/IV/V or
MOVEMASTER COMMAND programs. After various tests and optimization, pro-
gram is uploaded onto the robot, using a direct connection between the computer and
the robot via network or serial port. CIROS Studio has supervision over the robot while
it performs the uploaded code, as well as visualization of movement in 3D graph. The
interface of the program is shown in Fig. 4.
Fig. 3. Parts of MELFA RV-2SDB
Programming and Experimental Analysis
813

Numbers in Fig. 4 are as follows:
1. visual display of the virtual robot
2. RCI explorer, contains programs, variables, parameters etc.
3. Program editor
4. Messages, warnings, errors
The most common instructions of MELFA BASIC V [11] are as follows:
• DLY—wait of 0.5 s before executing next instruction,
• SERVO ON—turns on the servo,
• SERVO OFF—turns off the servo,
• HOPEN—opens the hand,
• HCLOSE—closes the hand.
It’s necessary to use instruction DLY before instruction to open the hand (HOPEN)
and before closing the hand (HCLOSE).
In order for the robot to do a task it’s designed for, there are motion instructions,
such as basic motion instructions, instructions for circulatory motion, instructions for
relative motion, and instructions for continued motion. Next to mentioned instructions,
there are also various instructions for management of I/O data, interrupts and similar.
The control of the MELFA robot is done with predeﬁned order of action. Con-
trolling is done via CR1DA-700 and TP controllers. There are two ways of control,
manual and automatic. Automatic control is running previously written code, while
manual refers to setting positions. For both types of programming, positions must be
inserted and saved via TP controller.
Fig. 4. CIROS studio
814
E. Mujčić et al.

For our experimental analysis (see Fig. 5), 7 positions of robot’s trajectory has been
used.
Using JOG mode, it’s necessary to manually place the robot in the desired starting
position and save that position using the TP controller’s options, and repeat that for all
positions needed in the movement trajectory.
After saving the positions, the code is written in MELFA BASIC V programming
language [11], which enables the robot to move over the predeﬁned positions. Robot is
programmed so that it moves with different speed between different positions, so from
P2 to P3, it will move with slower speed so it could precisely grab the LEGO cube,
meanwhile between positions P1 and P4, or P4 and P5, it has a curved motion and
greater speed.
5
Experimental Analysis of MELFA RV-2SDB
In this paper it is explained how MELFA RV-2SDB robot moves objects from one
place to another (see Fig. 6). Taking in consider that this type of robot it limited to the
size of the object, LEGO cubes are used, with their width being 3.7 cm, while the
length of the object is irrelevant. It is necessary to mention that the better grip the robot
has on an object, it will be easier for it to move it without major deviations.
Fig. 5. Positions and trajectory of the robot
Programming and Experimental Analysis
815

Two boards are needed, one for primary position of cubes, and the other as des-
tination. In this paper it is shown how MELFA RV-2SDB rearranges LEGO cubes
from one position to another.
Fig. 6. Robot moving a cube
Fig. 7. Starting positions of the cubes
816
E. Mujčić et al.

On the starting position (Fig. 7), the cubes are arranged with enough spacing in
between to enable unobstructed work for the robot. All cubes have their marked
positions, which are saved as positions, as previously explained. Robot takes one cube
at a time, in a predeﬁned order, and composes them on the destination board.
On the destination board (Fig. 8), there are ﬁxed wooden dowel pins on which the
ﬁrst two cubes are placed, to prevent slipping or other errors the robot could make
during the arrangement.
After the conducted experimental analysis, we can come to the conclusion that even
though the task looks easy to do, it is in fact quite hard for the robot. This is because it
requires high precision to compose the cubes. The hand of the robot is not ﬂexible so
even the slightest deviation, in millimeters, can cause inability of the robot to compose
the cubes.
Fig. 8. Destination board, ﬁnal appearance
Programming and Experimental Analysis
817

6
Conclusion
Based on experimental analysis of the work of MELFA RV-2SDB robot, it can be
concluded that this robot can be used for very complex and precise tasks, performed
under high speed. In this paper, the robot successfully managed to do the task of
composing the desired form. It can be concluded that MELFA RV-2SDB robot can be
used for a wide range of tasks, which demand both high precision and speed.
References
1. Mitsubishi Electric: MELFA Robots, Industrial Robot, Standard Speciﬁcations Manual
(2011)
2. Robotics https://en.wikipedia.org/wiki/Robotics
3. Karabegović, I., Dolček, V.: Robotika. Bihać (2002)
4. Čapek, K.: Rossumovi Univerzální Roboti (1921)
5. Asimov, I.: I, Robot. USA (1950)
6. Robot (2017). https://en.wikipedia.org/wiki/Robot
7. Papcun, P., Jadlovsky, J.: Mathematical model of robot Melfa RV-2SDB. Emergent Trends
in Robotics and Intelligent Systems, Advances in Intelligent Systems and Computing, vol.
316 (2015)
8. CIROS
studio
(2017).
http://www.festo-didactic.com/ov3/media/customers/1100/ciros_
studio_manual_1.pdf
9. CIROS studio (2017). http://www.ciros-engineering.com/en/products/virtual-engineering/
ciros-studio/
10. Daniel B. (FESTO DC-EC): Melfa-Basic V Handbook (2013)
11. CR1DA (2017). http://int76.ru/upload/iblock/dd9/dd9262a792dc5b443f6632d020cb6416.
pdf
818
E. Mujčić et al.

A Customizable Embedded WebRTC
Communication System
Edin Pjanić1(&) and Sanjin Lišić2
1 Faculty of Electrical Engineering, University of Tuzla, Tuzla, Bosnia and
Herzegovina
edin.pjanic@untz.ba
2 Bicom Systems, London, UK
sanjinlisic@outlook.com
Abstract. In this paper we present a WebRTC communication system com-
posed of a web phone and a SIP proxy as part of the Reticulum project. We
combine it with Raspberry Pi to produce a platform that makes communication
more accessible and portable. The proxy part is integrated with SimpleFSM, a
Ruby domain speciﬁc language, in order to simplify development of custom
telecom services by modeling complex call ﬂows as ﬁnite state machine models.
Results of testing we have conducted on different hardware platforms show that
Reticulum has good performance and responsiveness, even on the old Raspberry
Pi 1.
1
Introduction
Communication applications involving different devices (computers, personal digital
assistants, tablets, mobile phones) and exchanging voice, video and data are becoming
increasingly popular in the last decade. Viber, WhatsApp, Skype, Facebook Messen-
ger, Google Talk, Google Hangouts are examples of such services. Those services
connect to a new physical communication interface that appeared in the form of IP
phones using Voice over IP (VoIP) technologies and a range of communication pro-
tocols, either standard or proprietary.
With the spread of VoIP, personal computers (PC) became one of the focal points
for which new types of software—softphones were developed [1]. Besides proprietary
applications for aforementioned services, there is a range of free and/or open-source
softphones such as Jitsi, Blink, Linphone, X-Lite and others.
Main advantages of these solutions are simple maintenance and upgrade. It means
that, instead of buying a new IP phone device or having to do a complex ﬁrmware
upgrade every time a new technology or protocol update appears, it is enough to install
a new software version.
Additionally, in the last ﬁve years there is a dramatic increase in development and
sales of smartphones [2]. Proliferation of mobile based solutions [3] was the next step
in evolution of VoIP based systems. Several applications pushing the “mobile ﬁrst”
philosophy became available, such as Viber and WhatsApp.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_71

With the advent of WebRTC [4, 5], even software limitations disappear. We no
longer need to install a dedicated software application for communication. Using a web
browser, we can access a WebRTC web application and, utilizing a webcam and
microphone, communicate with anyone, anywhere as long as we are using the same
application. Linux, Mac OS and Windows, the three leading PC platforms, as well as
leading mobile platforms, support WebRTC.
There are a number of open-source frameworks and libraries for development of
WebRTC based solutions that utilize already existing VoIP technologies and protocols.
Some of them are JSSIP [6] and Sipml5 [7] for client VoIP applications, and OverSIP
[8] and Webrtc2sip [9] as corresponding server side solutions. All these frameworks are
well standardized and support a wide range of standard communication services.
However, they have a large code base and complex maintenance, conﬁguration and
deployment.
In this paper we present a WebRTC based communication system that can be
deployed on Raspbery Pi devices, credit-card sized single-board computers. The pre-
sented system, that we called Reticulum, is a complete solution with client and server
components that provides a lightweight communication platform. Besides a fully
conﬁgurable web application part of the system, in order to implement a range of
telephony services with speciﬁc call ﬂows, the VoIP part can be customized by uti-
lizing a simple specialized language built for that purpose.
Additionally, the presented communication system provides interoperability with
JSSIP and Sipml5 based clients.
The paper is organized as follows: in the following section we present technologies
used in development of Reticulum and its main functionalities. After that, we described
SimpleFSM [10], a simple domain speciﬁc language (DSL) used to customize Retic-
ulum. The last sections contain performance testing methodology and results, as well as
conclusion and ideas for further work.
2
Technologies Used
VoIP communication is based on two types of protocols: signaling and transport.
Signaling protocols are used for initiation and control of communication sessions
between peers. The most widely used communication protocol, especially in the
telecom industry, is the Session Initiation Protocol (SIP). SIP is a text protocol deﬁned
in RFC 3261 [11] with a syntax similar to the Hypertext Transfer Protocol (HTTP) [12]
developed and used for establishing and managing multimedia IP sessions.
Transport protocols are used for media exchange via audio/video streams with
parameters negotiated by utilizing a signaling protocol. In SIP based communication
systems, Real-time Transport Protocol (RTP) [13] is the most common transport
protocol.
However, web browser based applications are not able to directly utilize SIP, RTP
or any protocol other than HTTP and HTTP based protocols. Hence, WebSocket
protocol [14], an upgrade of HTTP, is used for transport of signaling and media.
Another constraint for web based applications is ECMAScript programming language
820
E. Pjanić and S. Lišić

[15], well known as JavaScript, the only standard and widely supported scripting
language in web browsers.
In order to support faster and easier development of multimedia and communica-
tion applications in web browsers, WebRTC [16] was developed as a browser inte-
grated solution. WebRTC is a free, open project that provides browsers and mobile
applications with real-time communications (RTC) capabilities via simple APIs. The
WebRTC initiative is a project supported by Google, Mozilla and Opera, amongst
others.
Web based solutions require a machine for hosting. By hosting it on a credit
card-sized single-board computer like Raspberry Pi we have greater mobility and
ﬂexibility [17]. To make it available, it is enough to plug it into a simple Local Area
Network (LAN) like those present in a residential, small business ofﬁce or academic
setting or even in the ﬁeld. With that in mind, we developed Reticulum.
3
Reticulum Main Features
The main goal of Reticulum project is to provide an intuitive, simple, lightweight and
portable communication system that is fully contained on a single device, such as
Raspberry Pi.
We have made Reticulum available as an open-source project on Github (https://
github.com/GrimmKull/Reticulum). It is a self-contained solution that can be deployed
on any PC platform, including Raspberry Pi devices. Besides PC platforms, this project
can be deployed on a range of server solutions to make it available worldwide.
Reticulum system is composed of two parts: a SIP proxy and a SIP client.
Reticulum proxy is a SIP proxy developed in Ruby programming language with
added functionality of SIP registrar utilizing a MySQL database. Reticulum proxy was
envisioned in 3 basic layers. The ﬁrst layer is the transport layer that uses WebSocket
server to provide a connection to our WebRTC based Reticulum web phone. The other
two proxy layers are SIP parser and SIP stack deﬁned by RFC 3261 [11] and RFC 7118
[18].
The other component, Reticulum web phone, is a WebRTC based SIP client
implemented as a static web page that is served by Reticulum proxy over HTTPS. It is
implemented in JavaScript. Reticulum web phone incorporates a complete graphical
user interface (GUI) that provides user registration to the system and establishment of
peer to peer audio-video calls with other users.
The other possibility for peer to peer communication through Reticulum system is
using a supported third party SIP client application. At the time of writing this paper,
Reticulum is compatible with JSSIP and Sipml5 based SIP clients.
The ﬁrst version of Reticulum was presented in [19]. At that stage, Reticulum
proxy was a standard SIP registrar and SIP proxy without any form of customization. If
one wants to implement a simple telecom service, such as call screening or other
deﬁned in RFC 5359 [20], it would be very complicated because he would have to
master the Reticulum architecture and change its source code in several different Ruby
ﬁles.
A Customizable Embedded WebRTC Communication System
821

In order to speed up and simplify the development process of communication
applications and services, as well as to introduce an opportunity for their customization,
we integrated a simple domain speciﬁc language (DSL) [10] for SIP communication
systems. The concepts of this approach were proposed in [21].
4
A DSL Approach
SIP protocol is a stateful protocol, hence a SIP application could be naturally modeled
as a ﬁnite state machine (FSM). This is the reason we incorporated the Sim-
pleFSM DSL within Reticulum.
SimpleFSM is implemented as an internal DSL using an embedded implementation
pattern [22–24] and is primarily designed for SIP communication systems, but can be
used for modeling an FSM for any domain.
SimpleFSM was developed in order to support the following requirements:
• FSM can have arbitrary number of states,
• unlimited number of transitions can be speciﬁed,
• state transition can be conditional,
• an action can be invoked on entering a state and/or exiting a state,
• an action can be executed on an event,
• events can receive an arbitrary number of arguments which are sent to all related
actions during the event processing.
Syntax of SimpleFSM DSL is simple. It supports state and transition deﬁnitions
that include deﬁnitions of events the FSM accepts as well as speciﬁcation of actions
executed on certain events. The DSL is developed as an internal DSL and does not
require any parser or other facility in order to be used in Ruby applications.
FSM actions are modeled using Ruby methods. This makes the FSM model
compact and clear. The state machine that is implemented in the class is deﬁned within
the block of code after the fsm keyword. FSM state transitions are deﬁned within the
transitions_for statement. The arbitrary number of transitions for any state can
be speciﬁed using the event statement.
In [25] we demonstrate an application of the SimpleFSM DSL to SIP communi-
cation systems. In particular, we combine the SimpleFSM DSL with SIP servlets in
order to simplify modeling of SIP call ﬂows inside an application. With that in mind,
we integrated SimpleFSM into Reticulum in order to apply a similar technique.
5
Utilizing SimpleFSM DSL
Instead of complex if-else, switch-case or other programming structures, application
logic of SIP proxy can be modeled using SimpleFSM DSL, which is far more elegant.
The FSM events that represent the received SIP messages are speciﬁed in the
format sipXXX, where XXX represents the SIP request method extracted from the
message, in case the received message is a SIP request. Similarly, a sipRE-
SPONSE_YYY event, where YYY represents a SIP response code or SIP response
822
E. Pjanić and S. Lišić

code class, is used to notify the FSM that a SIP response message was received.
Furthermore,
the
FSM
can
accept
events
sipREQUEST_ANY
and
sipRE-
SPONSE_ANY if it is required to process any received request and/or response inside a
certain application state that are not already provided within the transition deﬁnitions.
Examples of events that can be handled by the FSM inside the Ruby SIP controller are:
• sipINVITE—for INVITE request,
• sipACK—for ACK request,
• sipCANCEL—for CANCEL request,
• sipRESPONSE_200—for SIP response with status code 200 (OK),
• sipRESPONSE_2xx—for any SIP responses with status code beginning with 2
(success responses),
• sipREQUEST_ANY—for any SIP request,
• sipRESPONSE_ANY—for any SIP response.
Before writing FSM model for SIP messages processing using SimpleFSM DSL, in
reality we should probably deﬁne the state machine using a state diagram ﬁrst. Fig-
ure 1. depicts an FSM model of the Reticulum SIP proxy that implements registrar,
proxy and call screening service. Initial state is ‘waiting’. Circles with T mark the end
of a dialog. The following source code illustrates modeling of a SIP proxy that
implements registrar, proxy and call screening service [20] using SimpleFSM DSL:
Fig. 1. FSM model of the reticulum registrar, proxy and call screening service
A Customizable Embedded WebRTC Communication System
823

The proxy code is written in a separate Ruby ﬁle for convenience and loaded
(required) during Reticulum startup. It is good to note again that all actions in the FSM
model are methods that have to be deﬁned in the Ruby code. Many methods that are
common in SIP message processing are already deﬁned in the project.
Code in an fsm block represents an FSM model for processing of messages within
a SIP dialog that can potentially include several SIP transactions. Every SIP dialog
maintains its own FSM state. In Reticulum, state is saved in an abstraction called
context. Context is an extension of SIP session. A context includes SIP sessions and
824
E. Pjanić and S. Lišić

corresponding transactions for easier maintenance of all messages that are related to
each other. For example, one context lives from the initial INVITE to the ﬁnal 200 OK
after BYE, the other from initial INVITE to ﬁnal unsuccessful response etc. Examples
of SIP dialogs that correspond with a certain context are depicted in Fig. 2.
6
Performance Testing
6.1
Testing Methodology
Since we have a limited inﬂuence on the WebRTC media stack, we could only test
performance of the Reticulum SIP stacks. Thanks to the modular design of the web
phone media and transport components, for testing purposes, a replacement imple-
mentation was added to keep the web phone functional and at the same time remove the
WebRTC media stack inﬂuence on the SIP stack implementations. Another reason for
this approach is the fact that WebRTC performance is bound to the client machines on
which we have no inﬂuence.
Using the server side JavaScript with Node.js and its cluster module, we imple-
mented a test application that is also available on Github (https://github.com/
GrimmKull/araneola). As depicted in Fig. 3, it utilizes the processor cores to spawn
parallel workers and test the Reticulum implementation on both web phone and proxy
Fig. 2. Examples of SIP dialogs and context lifetime
A Customizable Embedded WebRTC Communication System
825

sides. When launching the test, the user has to deﬁne the proxy endpoint and the
number of calls that need to be made. We presume that a sufﬁcient number of SIP users
have been added to the registrar. One job is deﬁned and created for every call. All jobs
are divided equally among workers. For each job, a pair of Reticulum web phones is
created, connected to proxy and registered if the connection was successful, by issuing
a corresponding command using the web interface. After all the registrations have been
completed a command is issued to start the calls so that each “A phone” calls its pair,
the “B phone”. Each call is auto answered and auto-hangup after 10 s. During this
process phone state, request, response, transaction and call information are gathered.
After all transactions associated with these calls are terminated the user initiates a report
that contains all the aforementioned information.
6.2
Test Results
The testing methodology in [19] was applied to Reticulum hosted on Raspberry Pi 1
(RPi 1) and 2 (RPi 2) to determine the minimal number of supported concurrent calls
and responsiveness. We have also tested server deployments on a Digital Ocean Virtual
Private Server (VPS) and Scaleway bare metal C1 instance for comparison. Testing
was performed from the same PC using 6 parallel workers corresponding to the 6
available processor cores. The PC is based on 3 GHz AMD Phenom II 1075T pro-
cessor with 16 GB RAM.
During testing we found that the limit of Raspberry Pi 1 hardware is about 100
concurrent calls which we used as our baseline. Attempts to make more calls caused
either transport or stack issues. Transport issues were in form of WebSocket connection
timeouts and resets. The stack issues manifested themselves in the form of BYE
transaction timeouts. On the slower, Raspberry Pi 1, transaction timeouts were more
prevalent since it was not capable of processing that many SIP requests at the same
time. If a system managed to avoid the BYE transaction timeouts, the next segment that
Fig. 3. Test application worker generating a phone pair
826
E. Pjanić and S. Lišić

would give in the stress tests would be the transport layer with the connection timeouts
and resets.
We performed the same tests on the system with integrated SimpleFSM that
modeled registrar and proxy and merged results with the previous ones. Test results for
call duration minimum (min), maximum (max), mean and standard deviations (std) on
different hardware platforms are shown in Table 1. Values are represented here in
seconds. The same data are depicted in Fig. 4.
Automated test terminates a call after 10 s. Every second above that value is caused
by the delays in transport and server side processing.
Taking into account the results of the tests, we conclude that implementation on
Raspbery Pi 2 shows the best performance of all embedded environments as expected.
However, the implementation of Reticulum SIP proxy with incorporated Sim-
pleFSM DSL offers great level of simplicity and elegance for developing custom
telecom services. Hence, although its message processing time is slightly longer than
the basic implementation of Reticulum SIP proxy due to FSM events processing, we
ﬁnd it as a better choice for such applications.
Table 1. Call duration for different hardware platforms (in seconds)
Platform
Min
Mean Max
Std
Raspberry Pi 1
28.76 34.00 41.18 4.23
Raspberry Pi 2
10.64 15.40 16.87 0.88
SimpleFSM (RPi2) 12.85 17.32 17.46 0.50
Scaleway
10.53 13.79 14.51 0.72
Digital Ocean
10.47 11.06 11.81 0.27
Fig. 4. Comparison plot for call duration in ms
A Customizable Embedded WebRTC Communication System
827

7
Conclusion
In this paper we present Reticulum, a WebRTC based VoIP solution with the main goal
of making a lightweight, self-sufﬁcient communication platform. The additional goal
for this platform is that it has to be customizable in order to be able to implement
custom telecom services. Customization is provided by incorporating SimpleFSM DSL
into Reticulum. When deployed on Raspbery Pi device, it becomes a customizable
embedded WebRTC communication system that is lightweight and portable. When
conﬁgured, it is enough to plug it into a simple local area network (LAN) like those
present in a residential, small business ofﬁce or academic setting or even in the ﬁeld,
and use any networking capable device with a web browser for communication.
Such system could be modeled as an FSM in order to implement custom telecom
services such as services described in RFC 5359 [20].
We have shown that Reticulum system is capable of providing a communication
platform for at least 100 concurrent calls with minimal investment of time and money.
In the near future it is expected that even more web browsers will support the
WebRTC standard. It is also expected for Raspberry Pi to again be included among the
Chrome (Chromium) release targets. This would allow Reticulum to be used on a wider
range of platforms and devices.
Future work will include support for all WebRTC enabled browsers as well as
support for SIP messaging and conference calls. Another possible research direction
will be development of deployment platform for Raspberry Pi Reticulum solutions.
References
1. Kundan Singh, H.S.: Peer-to-Peer Internet Telephony using SIP. Department of Computer
Science, Columbia University
2. Ericsson: Ericsson Mobility Report (2016)
3. Sinnreich, H., Johnston, A.B.: Internet Communications Using SIP, 2nd edn. Wiley
Publishing Inc. (2006)
4. Manson, R.: Getting Started with WebRTC. Packt Publishing Ltd. (2013
5. Johnston, A.B., Burnett, D.C.: WebRTC APIs and RTCWEB Protocols of the HTML5
Real-Time Web, 2nd edn. DigitalCodex LLC (2013)
6. JSSIP: Versatica. http://jssip.net/
7. SIPml5: Doubango Telecom. https://www.doubango.org/sipml5
8. OverSIP: Versatica. http://oversip.net
9. Webrtc2sip: Doubango Telecom. https://www.doubango.org/webrtc2sip/
10. Pjanić, E., Hasanović, A.: SimpleFSM—a domain speciﬁc language for SIP communication
systems—Part I: language description. Elektrotehnički vestnik 78(4), 223–228 (2011)
11. Fielding, R., et al.: Hypertext Transfer Protocol—HTTP/1.1. RFC2616. https://www.ietf.
org/rfc/rfc2616.txt. June 1999
12. Rosenberg, J., Schulzrinne, H., Camarillo, G., Johnston, A.: Session initiation protocol.
RFC3261. https://www.ietf.org/rfc/rfc3261.txt. June 2002
13. Schulzrinne, H., Casner, S., Frederick, R., Jacobson, V.: RFC 3550: RTP: A Transport
Protocol for Real-Time Applications. Technical Report, IETF (2003)
14. Fette, I., Melnikov, A.: WebSocket. IETF. https://www.ietf.org/rfc/rfc6455.txt. Dec 2011
828
E. Pjanić and S. Lišić

15. ECMAScript 2016 Language Speciﬁcation—ECMA-262 Standard: Ecma International
(2016)
16. Bergkvist, A., Burnett, D.C., Jennings, C., Narayanan, A., Aboba, B.: WebRTC. W3C
WebRTC Working Group. https://www.w3.org/TR/webrtc/. May 2016
17. Raspberry Pi: Raspberry Pi Foundation. https://www.raspberrypi.org
18. Castillo, I.B., Villegas, J.M., Pascual, V.: RFC7118. IETF. https://www.ietf.org/rfc/rfc7118.
txt. Jan 2014
19. Lišić, S., Pjanić. E.: Reticulum—WebRTC communicator and SIP proxy implemented on
Raspberry Pi platform. In: Proceedings of the Twenty-ﬁfth International Electrotechnical and
Computer Science Conference ERK 2016. 19–21 Sept. 2016, Portorož, Slovenia, Ljubljana.
IEEE Region 8, Slovenian Section IEEE, vol. B, pp. 59–62
20. Donovan, S., Cunningham, C., Sparks, R., Summers, K., Johnston, A.: Session Initiation
Protocol Service Examples, RFC 5359, Request for Comments (2008)
21. Pjanić, E., Hasanović, A., Suljanović, N., Mujčić, A., Zajc, M.: Metaprogramming
approaches to ﬁnite state machine modeling for SIP applications. In: MELECON 2010, The
15th IEEE Mediterranean Electrotechnical Conference, 25–28 April 2010, Valletta, Malta.
IEEE https://doi.org/10.1109/MELCON.2010.5476022
22. Mernik, M., Heering, J., Sloane, A.M.: When and how to develop domain-speciﬁc
languages. ACM Comput. Surv. 37(4), 316–344 (2005)
23. Cuadrado, J., Molina, J.: A model-based approach to families of embedded domain-speciﬁc
languages. IEEE Trans. Software Eng. 35(6), 825–840 (2009)
24. Gunther, S., Haupt, M., Splieth, M.: Agile engineering of internal domain-speciﬁc languages
with dynamic programming languages, In: Proceedings of the 2010 Fifth International
Conference on Software Engineering Advances (ICSEA), pp. 162–168 (2010)
25. Pjanić, E., Hasanović, A.: SimpleFSM—a domain-speciﬁc language for SIP communication
systems—Part II: application to SIP Servlets. Elektrotehnički vestnik 78(5), 293–297 (2011)
A Customizable Embedded WebRTC Communication System
829

Part VI
Mechatronics, Robotics and Embedded
Systems

BCIs for Electric Wheelchair
Dalibor Đumić1(&) and Jasmin Kevrić2
1 International Burch University, Splitska 53, Sarajevo, Bosnia and Herzegovina
dalibor.djumic@stu.ibu.edu.ba; mentalibor@hotmail.com
2 International Burch University, Francuske Revolucije Bb, 71210 Ilidza,
Bosnia and Herzegovina
jkevric@ibu.edu.ba
Abstract. This paper presents signiﬁcantly facilitated ways of controlling an
electric wheelchair using the power of the human brain for persons getting
motor neuron disease (MND) and the difference in efﬁciency and accuracy
between. The proposed BCI was developed in .NET framework which uses
NeuroSky Mindwave’s single dry electrode as the only way of communication
with the interface. BCI developed in .NET framework shows the direction
control of wheelchair in all four directions: left, right, forward, and backwards.
The results of the proposed BCI in controlling the wheelchair are promising, and
the advantages are compared to some existing BCI wheelchair systems. The
application of the proposed BCI systems would signiﬁcantly help people with
motor disabilities to have an improved style of living with more autonomy.
Keywords: Brain computer interface (BCI)  MATLAB GUI  .NET
framework  Motor neuron disease (MND)  NeuroSky Mindwave Mobile
1
Introduction
Electric wheelchairs are always considered as one of the most important aids for the
elderly and the physically impaired patients. There are patients who are not able to
control an electric wheelchair by conventional methods. Especially, these people can
only use the eyes and brain to exercise their willpower. In the context, brain-controlled
wheelchairs are a great aid especially suitable for the paralyzed patients that are not
able to operate the electric wheelchair completely. The motivation of the project is to
facilitate assistance in controlling the wheelchair in realistic environments for the
paralyzed patients. It includes the development of BCI software with navigation
strategies to enable the patients to move wheelchairs efﬁciently and easily.
When the patients have suffered from MND, their muscle starts to gradually waste
and become weak until their body is frozen. The most known type of MND is called
Amyotrophic Lateral Sclerosis (ALS).
It is also called Lou Gehrig’s disease because the famous American baseball star
Lou Gehrig died of this disease. Their words, swallowing and respiration are dys-
functional until respiratory failure and death. Anyone can suffer from this disease, but
more common in people aged from 40 to 70 years old. The development of the disease
is rapid and ruthless. Generally, the average life expectancy of survival is between 2–5
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_72

years after onset. Because sensory nerves have not been violated, it does not affect the
patient’s intelligence, memory or feeling.
There are a lot of interfaces between human and machines to utilize input devices
such as keyboards, joysticks or mouse. Recently, a number of biological signals have
been utilized as hand-free interfaces, named brain-computer interface (BCI) to
machines like electromyogram (EMG) and electroencephalograph (EEG). The para-
lyzed patients cannot operate objects or communicate their needs so this is a reason to
design a BCI system and a drive circuit by using the paralytic patient’s brain waves to
control electric wheelchairs and to help them move freely in their daily life.
Back in 1973, the idea of BCI was ﬁrst presented by Vidal [1]. To deﬁne, a
brain-computer interface (BCI) is a system that enables the communication without any
movement possible [2]. This is the reason why severely disabled individuals may ﬁnd a
BCI very promising communication system. Generally, BCI must satisfy the following
four conditions [3]:
• depends on direct measures of central nervous system (CNS) activity,
• delivers user feedback,
• functions in real-time, and
• depends on intended control.
For disabled people, robots are usually used to do their work without human
interaction. In wheelchair applications, the wheelchair is usually controlled with the
conventional input tools such as joystick, mouse, voice and keyboard, but these input
methods can be used by people without disabilities. However, these input methods are
not easy to perform because they can cause more complexity in controlling the
wheelchair for elderly and disabled people. This is main reason why Arrows System
has been developed as shown in Fig. 1.
This system has a purpose to eliminate the standard methods of control and to
provide a direct connection between the human brain and external devices. The
activities of electrical signals are measured and recorded from the scalp by using EEG
device [4].
Fig. 1. Development of Arrows System
834
D. Đumić and J. Kevrić

2
Literature Overview
Sections 3−5 are focused on giving details about development of the Arrows System
and its behavior in operation mode. In reference section contains the discussion about
the comparison of this BCI and the one presented in [11], that is BCI developed in
MATLAB GUI. Also, Sect. 6 provides conclusion and possible improvements in future
work.
3
Proposed Methodology of Arrows System
The proposed Arrows System consists of three modules:
• BCI System module
• .NET Framework module and
• Electric Wheelchair hardware module.
3.1
BCI System Module
Biomedical technology and its sensing are very important in a health care. NeuroSky
Mindwave headset consists of a single-channeled dry electrode which is used to
measure the EEG signals (in this paper to detect an Eye blink) located on the FP1
frontal lobe of the scalp which is indicated on Fig. 2.
Table 1 gives the speciﬁcations information of NeuroSky Mindwave headset [6].
Fig. 2. Location of the FP1 frontal lobe on the scalp indicated by red circles [5]
BCIs for Electric Wheelchair
835

The measured signals are preprocessed by ThinkGear chip which is placed in the
headset and sent to the PC over Bluetooth connection [7]. Diagram of the NeuroSky
MindWave Mobile EEG Headset is shown in Fig. 3 where sensor tip is actually single
dried electrode. The device has the power switch and the device is supplied by a single
battery. The device is ﬂexible which is making it usable for every person.
The ThinkGear device provides several relevant information through developed
NeuroSky algorithms which provide the foundation of a universe of applications that
can be built to optimize brain health, education, alertness and overall function.
Table 1. NeuroSky Mindwave’s speciﬁcation
EEG system
Mindwave mobile
Sensor type
Dry
Bandwidth
3–100 Hz
A/D resolution 12 bits
Channels
1
Sampling rate
512 Hz
Transfer
Wireless (Bluetooth)
Weight
90 g
Battery life
8 h
Fig. 3. NeuroSky Mindwave Mobile EEG Headset diagram [8]
836
D. Đumić and J. Kevrić

With this headset is possible to measure attention through Attention algorithm,
mediation through Mediation algorithm, blink strength through Blink Detection algo-
rithm and also it is possible to get raw data or EEG theta data which is usually constant
at some time. However, these algorithms are implemented in ThinkGear SDK which
can be used by any application and are available on the ofﬁcial website of the sensor.
The Blink Detection algorithm works by detecting a peak between two raw data
from the sensor at the moment of eye blinking. When detected, it then measures how
strong is that peak and by it calculates the blink strength. It happens because eye blinks
are the artifacts i.e. some noise for these data. Figure 4. shows the plots of raw value
and its behavior in the moment of eye blinking.
The Blink Detection algorithm detects a user’s blinks. A higher number indicates a
stronger blink, while a smaller number indicates a weaker blink, so blink strength
values is obtained in value which varies from 0 to 255. The frequency of blinking is
correlated with nervousness of human body. Figure 5. shows how strong are normal
reﬂexive eye blinks.
Fig. 4. The raw output from NeuroSky
Fig. 5. MATLAB analysis of blink strengths
BCIs for Electric Wheelchair
837

3.2
.NET Framework Module
.NET Framework requests using ThinkGear SDK to access all data obtained from
ThinkGear chip sent through Bluetooth port. .NET framework simply communicates
with Bluetooth port and uses algorithms which are contained in ThinkGear SDK, and
therefore it is easy to get access to the Blink Detection algorithm [9].
Fig. 6. Arduino Nano [10]
Fig. 7. Proposed electric wheelchair
838
D. Đumić and J. Kevrić

3.3
Electric Wheelchair Hardware Module
The electric wheelchair was borrowed from the local hospital on the short time for the
test purposes. There was no possibility to disassemble it and test it in real environment.
The only interface for controlling on the wheelchair is placed on the handle in the form
of joystick and a microcontroller was used to control this joystick by servo motors.
Arduino Nano was used to control the servo motors. It is based on the ATmega328.
Servo motors are placed around the joystick of the electric wheelchair [10]. This
platform is shown on Fig. 6.
Fig. 8. Windows 10 tablet
Fig. 9. Schematic description of the proposed system
BCIs for Electric Wheelchair
839

Fig. 10. Flowchart of the Arrows Application
840
D. Đumić and J. Kevrić

Their purpose is to control the movement of the wheelchair by capturing the
command received. The proposed electric wheelchair is shown in next ﬁgure, Fig. 7.
The hardware is connected by Universal Service Bus (USB) communication to the
tablet which is run by Windows 10. The Windows 10 tablet is shown on Fig. 8.
The schematic description of the proposed system is shown in next ﬁgure, Fig. 9.
4
Design Flow
This section describes the Arrows System in steps. Figure 10 shows the ﬂowchart of
system’s application. At the beginning, the NeuroSky headset is turned on, which is
gathering the neuro signals. NeuroSky Mindwave captures the signal and sends to
ThinkGear Chip for preprocessing the raw EEG data. After the analysis and conversion
of the data into digital, digital signals are sent over Bluetooth connection to the tablet and
application developed in .NET framework gets the digital signals using ThinkGear SDK.
The user has to connect to the electric wheelchair by connecting the USB cable to
the laptop and pressing the CONNECT button in the application. The next step is
realization of connection between the NeuroSky Mindwave headset and the .NET. That
is done by clicking the CONNECT button which calls the functions from
ThinkGear SDK [3] to connect the Neurosky Mindwave and enables the Blink
Detection algorithm. After few seconds, application is ready to use.
Fig. 11. The ﬁrst appear of the Arrows Application
BCIs for Electric Wheelchair
841

There are two groups of direction, each of which gets sequentially highlighted. One
group has two directions: upward and backward, and other has left and right. After
choosing a group by a blink, the only thing is to choose the desired direction by one
more additional blink and then the electric wheelchair will go into the desired direction.
The time of driving depends on the user. In other words, the wheelchair will stop
when user blinks again. After user blinks again, the wheelchair will wait for the user to
choose desired group of directions.
The pseudo code will show how the usage of this proposed prototype is very easy
to use because the interaction with the wheelchair is available only through eye
blinking.
system.print(“Follow the instructions.”);
setTheBlinkStrengthLevel();
if (connectedToWheelchair ())
{ 
if (connectedToNeuroSky ())
{ 
while ((!closed()) || (!connectedToNeuroSky()) || (!connectedToWheelchair())
 
 
{ 
chooseTheGroupOfDirections(); 
if (blinked == true) 
 
 
{ 
chooseTheDirection(); 
if (blinked == true) 
 
 
 
{ 
moveWheelchairInChosenDirection();
if (blinked == true)
 
 
 
 
{ 
stopTheWheelchair();
 
 
 
 
} 
 
 
 
} 
 
 
} 
 
} 
else
 
{ 
error(“NeuroSky is not properly connected. Check the 
connectivity.”)
 
} 
} 
else 
{ 
error(“Wheelchair is not properly connected. Check the con-
nectivity.”);
}
842
D. Đumić and J. Kevrić

However, at the beginning there might be some unexpected issues such as Neu-
rosky Mindwave headset being turned off, battery of the NeuroSky Mindwave headset
being low, improper connection between the electric wheelchair and the tablet and
similar things.
There is an extra option: BlinkStrength Value which allows the Arrows System to
set the blink strength to the desired value. For instance, if user wants to control the
electric wheelchair by stronger blinks, it’s necessary to set the BlinkStrength value to
the value bigger than 100. The similar is for weaker blinks whereas user has to set the
BlinkStrength value lower than 40.
Fig. 12. Highlighting of groups of the directions
BCIs for Electric Wheelchair
843

5
Experimental Result
This section shows the design of the Arrows Application in the operation mode.
Figure 11 shows the ﬁrst appear of the Arrows Application.
Figure 12 shows how groups of directions are being highlighted for one second and
the user just needs to choose a group by a blink.
Figure 13 shows how user can easily choose desired directions by a simple blink.
The directions are also being highlighted for one second.
At the end, when user chooses a direction, the electric wheelchair will start to drive
in desired direction which is shown on Fig. 14 and will stop until the new blink is
registered from the user.
Figure 15 shows how the BlinkStrength value can be changed by simply moving
the cursor at the track bar.
Fig. 13. Highlighting of the direction
844
D. Đumić and J. Kevrić

Firstly, for the hardware purpose the prototype was tested on the robotic car and
there were no problems. Then the prototype was tested by three random persons in
closed environment i.e. building on the real wheelchair and the results appealing were
really great, especially because of the accuracy which is shown on Table 2. Since the
wheelchair was borrowed on short time from the local hospital, there was no possibility
to test it in real environment.
Fig. 14. Driving the electric wheelchair in desired direction
Fig. 15. Changing the BlinkStrength Value
BCIs for Electric Wheelchair
845

6
Conclusion and Future Work
BCI developed in MATLAB GUI [11] shows the practical implementation of the motor
imagery BCI system where EEG signals were recorded for two motor imagery tasks:
right direction and left direction. The advantage of the BCI proposed in this study is its
applicability in almost every environment. It does users’ full concentration, it is also
user friendly and can be applicable for other purposes as well. On the other hand, the
MATLAB GUI adopted for BCI [11] required full concentration of the user which may
not always be possible due to the external factors. However, the MATLAB GUI is
based upon machine learning for detecting user’s thoughts or intentions, and as such
may be more suitable for the application in stroke rehabilitation. However, the weak
spot of this BCI is that it requires very high concentration on the imagery tasks in real
environment in order to control the wheelchair, which is impossible to do because the
environment may be dangerous at any moment and it would effect on the performance
on the above mentioned BCI. The Arrows System has eliminated that spot and
moreover, it made the usage of BCI easier by using only one input command—eye
blinking.
The Arrows System has a goal to facilitate daily life for elderly and disabled
people. The proposed method uses the Blink Detection algorithm as the control signal.
The prototype model of this system could not be developed without simple NeuroSky
technology which is very effective and the most important thing, inexpensive. Why
inexpensive? It only needs a microcontroller, a tablet with pre-installed Windows 10, a
software application and the EEG device. NeuroSky EEG device is easy to use, very
practical and inexpensive. It doesn’t require MATLAB software.
The future work may consider adding the options for controlling a speed of the
electric wheelchair that would certainly improve the performance of Arrows System.
Adding the speed controls would really help in real-life scenarios and in situations
when user feels the need for speed.
Finally, the ideal product would consist of hardware only. Instead of the tablet, the
LatterPanda [12] would be built inside of the wheelchair. It’s speciﬁc because it is
much cheaper than tablet, has preinstalled Windows 10, preinstalled Arduino IDE and
built-in microcontroller which is used for communication with the controller of the
wheelchair. As interface, a small touch display with HDMI port would be placed on the
handle of the wheelchair so that the user would easily follow the instruction from this
display, in the end, the wheelchair would be very simple for the use and more
Table 2. The results from testing prototype
Trials (N) Desired directions (N) Accuracy (%)
Person 1 20
18
90
Person 2 26
23
88.46
Person 3 32
31
96.87
Total:
78
72
92.3
846
D. Đumić and J. Kevrić

inexpensive because it would exclude the usage of the tablet. All of the hardware
except the display would be hidden inside of the handle of the wheelchair. When the
wheelchair are not controlled, the user can use the Internet, listen to music through
available 3.5 mm jack port, that is, the wheelchair would be completely smart.
References
1. Vidal, J.J.: Toward direct brain-computer communication. Ann. Rev. Biophys. Bioeng. 2,
157–180 (1973)
2. Wolpaw, J.R., Birbaumer, N., McFarland, D.J., Pfurtscheller, G., Vaughan, T.M.: Brain–
computer interfaces for communication and control. Clin. Neurophysiol. 767–791 (2002)
3. Pfurtscheller, G., Allison, B.Z., Brunner, C., Bauernfeind, G.: The hybrid BCI. Front.
Neurosci. 4(30) (2010)
4. Rani, B.J.A., Umamakeswari, A.: Electroencephalogram-based brain controlled robotic
wheelchair. Indian J. Sci. Technol. 8(S9), 188–197 (2015)
5. EEG: Overview of EEG, neurologic labs. http://neurologiclabs.com/neuromonitoring/eeg/
6. NeuroSky Mindset Instruction Manual. NeuroSky, Inc. 19 June 2009
7. NeuroSky BrainWave Signal (EEG) of NeuroSky, Inc. http://frontiernerds.com/ﬁles/
neuroskyvs-medical-eeg.pdf
8. MindWave Diagram: NeuroSky Inc. http://support.neurosky.com/kb/mindwave/mindwave-
diagram
9. ThinkGear SDK for .NET/Manuals/. http://developer.neurosky.com/docs/doku.php?id=
thinkgear.net_sdk_dev_guide_and_api_reference
10. Arduino Nano. https://www.arduino.cc/en/Main/arduinoBoardNano
11. Kevric, J., Subasi, A.: The impact of MSPCA signal de-noising in real-time wireless brain
computer interface system. SE Eur. J. Soft Comput. 4(2), 1859–2233 (2015)
12. LatterPanda, documentation. http://www.lattepanda.com/product-details/?pid=2
BCIs for Electric Wheelchair
847

Brainiac’s Arm—Robotic Arm Controlled
by Human Brain
Dalibor Đumić(&), Mehmed Đug, and Jasmin Kevrić
International Burch University, Francuske Revolucije bb, Splitska 53,
71210 Ilidza, Sarajevo, Bosnia and Herzegovina
{dalibor.djumic,mehmed.dug}@stu.ibu.edu.ba,
jkevric@ibu.edu.ba
Abstract. This paper shows electroencephalograph (EEG) controlled robotic
arm based on Brain–computer interfaces (BCI). BCIs are systems that enable
bypassing conventional methods of communication (i.e., muscles and thoughts)
and provide direct communication and control between the human brain and
physical devices using the power of the human brain. The main goal of the
project work is to develop a robotic arm that can assist the disabled people in
their daily life and by it make their work independent on others.
Keywords: Robotic arm  BCI  Human brain  Disabled people
1
Introduction
Robotics is the science of robots, their design, development and implementation. It
covers the ﬁelds of informatics (especially artiﬁcial intelligence), Electrical and
Mechanical Engineering. These advancements manage robotized machines (robots for
short) that can replace people in hazardous situations or assembling forms, or look like
people in appearance, conduct, as well as discernment.
A considerable lot of today’s robots are propelled by nature, adding to the ﬁeld of
bio-enlivened mechanical autonomy. The idea of making machines that can work
self-sufﬁciently goes back to established times, however examine into the usefulness
and potential employments of robots did not become considerably until the twentieth
century. Throughout history, it has been habitually expected that robots will one day
have the capacity to copy human conduct and oversee undertakings in a human-like
style.
Today, robotics is a quickly developing ﬁeld, as mechanical advances keep;
inquiring about, planning, and building new robots ﬁll different useful needs, whether
locally, monetarily, or militarily.
Numerous robots are worked to do tasks that are perilous to individuals, for
example, defusing bombs, discovering survivors in shaky destroys, and investigating
mines and wrecks. Mechanical technology is likewise utilized as a part of STEM
(Science, Technology, Engineering, and Mathematics) as an educating help.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_73

2
Literature Overview
The most information is collected from the Internet because it’s the biggest source of
the available knowledge for students and young professionals. All references except
[1–5] are from the Internet and they are used in research in order to develop and make
Braniac’s arm, whereas literatures [6–10] provide necessary information related to
servo motors, robotics and biomedical signals. The robotic hand was built from
InMoov 3D printed parts [11] which will be explained in in coming sections.
Sections 3–6 are focused on giving details about research and making of the
Brainiac’s arm. Section 7 contains results obtained with the Brainiac’s arm. Also,
Sect. 8 provides conclusion.
3
Design of Brainiac’s Arm
Robotic arms are new innovations in technology and they are used to perform different
tasks in industry, military, space exploration and so on. Robotic arm is a mechanical
arm that has similar functions as human arm and it is usually programmed.
The 3D printed robotic parts are taken from the InMoov project, which are open
source parts designed by Gael Langevin [11].
In this paper is shown how 3D printed robotic arm can be programmed for brain
controlling. The robotic arm in this project is in normal size and it is built for inter-
action with users (Fig. 1).
Braniac’s arm is constructed of several parts such as:
• 3D printed parts of ﬁngers,
• a pipe and
• a bed with servo motors (Fig. 2).
Fig. 1. 3D design of robotic arm [1]
Brainiac’s Arm—Robotic Arm Controlled by Human Brain
849

A ﬁst of Brainiac’s arm is consisting of the 3D printed ﬁnger which are together
assembled. The ﬁst is then connected to a pipe, then the pipe will be connected to the
bed for servo motors (Fig. 3).
Servo motors will be connected with ﬁngers by simple strings and they will move
according to the desired direction. The bed is consisted of six servo motors where ﬁve
of them will move the ﬁngers and one servo motor will move arm up and down
(Fig. 4).
Fig. 2. Brainiac’s arm
Fig. 3. A ﬁst of Brainiac’s arm
850
D. Đumić et al.

4
Biomedical Signals
Biomedical signals are phenomenon that carries information relative to one or more
biological systems involved. Obviously, biomedical signals can be found at different
observation scales: as an example, at the level of a functional organ (heart, brain, liver,
kidneys, etc.), at a system level (cardiovascular, central nervous, endocrine-metabolic
systems, etc.), but also at the level of the cell or even at a subcellular level, as indicated
in the previous section, as well as in higher dimension systems, as in the case of the
quantitative study of morbidity, mortality, and the mechanisms of propagation of an
epidemic disease inside a certain population [6].
4.1
Electroencephalograph (EEG)
An electroencephalograph (EEG) is the recorded electrical activity generated by the
brain. In general, EEG is obtained using electrodes placed on the scalp with a con-
ductive gel. In the brain, there are millions of neurons, each of which generates small
electric voltage ﬁelds. The aggregate of these electric voltage ﬁelds creates an electrical
reading which electrodes on the scalp are able detect and record. Therefore, EEG is the
superposition of many simpler signals. The amplitude of an EEG signal typically
ranges from about 1–100 uV in a normal adult, and it is approximately 10–20 mV
when measured with subdural electrodes such as needle electrodes [7].
4.2
NeuroSky Mindwave Headset
NeuroSky has developed a dry sensor system for consumer applications of EEG
technology. The NeuroSky system consists of dry electrodes and a specially designed
electronic circuit for the dry electrodes (Fig. 5).
The Attention Meter algorithm indicates the intensity of mental “focus” or “at-
tention”. The value ranges from 0 to 100. The attention level increases when a user
focuses on a single thought or an external object, and decreases when distracted. Users
can observe their ability to concentrate using the algorithm [2].
Fig. 4. Positions of servo motors in the bed of Braniac’s arm
Brainiac’s Arm—Robotic Arm Controlled by Human Brain
851

5
Proposed Methodology of Brainiac’s Arm
The proposed hardware mechanic system consists of four modules:
• 3D printed model of Brainiac’s arm
• Arduino Uno microcontroller
• Servo motors
• Bluetooth module
5.1
3D Printed Model of Brainiac’s Arm
This Brainiac’s arm is made almost entirely of 3D printed parts that are snapped
together. The ﬁst of Braniac’s arm is assembled from the 3D printed parts. There are
ﬁve ﬁngers and each of them has six parts. Also there is a palm which is constructed of
three parts by two bolts. A ﬁshing line is pulled through these parts and the ends of
these lines are connected to ﬁve servo motors whose will pull or stretch the ﬁngers.
The index and middle ﬁnger are connected to the main part of palm; the thumb is
connected to the main part of palm with a bolt. The two other parts of palm (green and
purple part in Fig. 6) are connected to the main part of palm with a bolt. Then the little
ﬁnger and the ring ﬁnger are connected to the other parts of palm (Fig. 7).
Fig. 5. NeuroSky mindwave headset [8]
852
D. Đumić et al.

Fig. 6. 3D design of Brainiac’s arm [3]
Fig. 7. 3D printed Brainiac’s arm
Brainiac’s Arm—Robotic Arm Controlled by Human Brain
853

5.2
Arduino Uno Microcontroller
Arduino is an open source microcontroller development board based on easy to use
hardware and software. The Arduino Uno is a microcontroller that works on the
ATmega328. The ATmega328 has 32 KB and it has 2 KB of SRAM and 1 KB of
EEPROM. Arduino Uno has 14 digital input/output pins 6 of them are PWM outputs
(Pulse Width Modulation, or PWM, is a way of getting analog results with digital
means), 6 analog inputs, a power jack, a USB connection, a 16 MHz crystal oscillator,
an ICSP header, and a reset button [4] (Fig. 8).
5.3
Servo Motors
Servo motors are not something new in technology they have been around for a long
time and they are used in many applications. They are very energy efﬁcient and they are
small in size that is why they are used in many applications. They are also used to
operate remote-controlled or radio-controlled an example of that are toy cars, robots
and airplanes. Because of their properties servo motors are used in industrial appli-
cations, robotics, in-line manufacturing, pharmaceutics and food services [12] (Fig. 9).
5.4
Bluetooth Module
In the project Bluetooth module is used to connect with NeuroSky Mindwave headset.
HC-05 is used as Bluetooth module and it’s based on the Cambridge Silicon Radio
BC417 2.4 GHz Bluetooth Radio chip. This is a complex chip which uses an external 8
Mbit ﬂash memory [13] (Fig. 10).
Fig. 8. Arduino uno microcontroller and its parts [5]
854
D. Đumić et al.

Fig. 9. MicroServo 90 servo motor and its dimensions [14]
Fig. 10. HC-05 bluetooth module [15]
Brainiac’s Arm—Robotic Arm Controlled by Human Brain
855

6
Pseudo Code and Design Flow of Braniac’s Arm
6.1
Pseudo Code
Pseudo code for using robotic arm as a grabber:
checkBluetoothConnection(); 
if (connection == true) 
{ 
Serial.print(“Connected”)
 
curAttention = readAttenetion(); 
 
switch(curAttention) { 
case stage1: 
 
quarterCloseFist(); 
 
break; 
case stage2: 
 
halfCloseFist(); 
 
break; 
case stage3: 
 
threeQuartersCloseFist(); 
case stage4: 
 
fullCloseFist(); 
 
break; 
default: 
openFist(); 
break; 
} 
} 
else  
{ 
Serial.print(“Connection failed..”)
connectAgain();  
} 
6.2
Design Flow
Figure 11 describes the design ﬂow of the Brainiac’s arm. It shows a loop which shows
how the ﬁst of the arm is closing according to the measured concentration of a human
brain and it never stops to work until a user turns off it.
856
D. Đumić et al.

Fig. 11. NeuroSky mindwave headset
Brainiac’s Arm—Robotic Arm Controlled by Human Brain
857

7
Results
The result is the Braniac’s arm which can be controlled using brain signals in other
words reading the attention of a human brain in order to move ﬁngers of the Brainiac’s
arm.
As attention increases, ﬁngers form a ﬁst. There are four levels of attention and
every level has set angles for each servo motor. The attention is measured from 0 to 100
and the angles of rotation are different for every ﬁnger which is connected on each
servo motor. Index ﬁnger of Brainiac’s arm will be took as an example, where the
difference between stretched and bent ﬁnger is 120°.
Every level of attention has its desired position. The starting position of servo
motor that is connected to index ﬁnger is 30° and when the attention is grater then 30 (it
moves into second level of attention) the motor moves to 60° that is desired position for
the second level of attention. For every ﬁnger, it is different so there was a need
calculate the rotation of the servo motor and match the right angle with the right level
of attention.
8
Conclusions
The interesting thing about Brainiac’s arm is that it can be used in different ﬁelds. First
of all, it can be used by people with disabilities, people who are lacking a physical
body. Using brain, they can control their missing part of body [9, 10]. The advantage is
that they will train their brain and attention. Similar work has been done in [9, 10]
where authors used EEG signals for controlling the arm. In [9] was developed a Java
Application for tracking the signals and there was given visual feedback to user. Also
in the [9] force sensors are used with the arm, in our work we didn’t implement force
sensors. Our next stage will be the implementation of force sensors which will give us
much more control of the robotic arm. The difference between our work and [9, 10] is
that we used much cheaper equipment for performance of similar task. In [10] two
software framework were presented in order to control a 5 degree of freedom robotic
and prosthetic hand. In this paper we developed our software for the control of robotic
arm. In [9, 10] EPOC headset was used for tracking brain signals, which is much better
equipment than our Neurosky because it has more electrodes which means it can
measure brain activity in several points whereas ours is focused on one point of
human’s head. If we compare our solution with the [9, 10] solutions, they have more
precise results but we have simpler and cheaper solution.
References
1. Credit for Fig. 1. https://previews.123rf.com/images/aliencat/aliencat1010/aliencat 101000166/
7972773- 3D-render-of-a-robot-arm-pointing–Stock-Photo-mechanical.jpg
2. Neurosky, EEG Biosensors, EEG sensor, Algorithms. http://neurosky.com/biosensors/eeg-
sensor/algorithms/
858
D. Đumić et al.

3. Credit
for
Fig. 6.
https://cdn.thingiverse.com/renders/bc/e4/7d/c6/02/InMoov_render_1_
preview_featured.jpg
4. Arduino Board Uno. https://www.arduino.cc/en/Main/ArduinoBoardUno
5. Credit for Fig. 8. http://1.bp.blogspot.com/-OX_oDoo_HzQ/VjDtqegKGUI/AAAAAAA
AAGE/ TlMyj9AtkCI/s1600/arduino_uno_components.jpg
6. Cerutti, S., Marchesi, C.: Fundamentals of biomedical signal processing and introduction to
advanced methods. In: Advanced Methods of Biomedical Signal Processing. Wiley-IEEE
Press (2011)
7. Brain Wave Signal (EEG) of NeuroSky, Inc. 15 Dec 2009
8. Credit
for
Fig. 5.
https://cdn.shopify.com/s/ﬁles/1/0031/6882/t/12/assets/MindWave-
headset.jpg?2137080139183096221
9. Deo, S., Sharma, R., Kumari, K., Pawar, S.G.: Mind controlled robotic arm using EEG
classiﬁcation of neurons as per expressive and cognitive suite. Int. J. Innov. Res. Comput.
Commun. Eng. (2016)
10. Elstob, D., Secco, E.L.: A low cost EEG based BCI prosthetic using motor imagery. Int.
J. Inf. Technol. Convergence Serv. (2016)
11. Langevin, G.: InMoov project. http://inmoov.fr/
12. Jeegnesh, S., Pandya, U.: Seminar paper DC servo motors. http://www.slideshare.net/
Jags176/dc-servo-motor-46641455
13. Bluetooth-HC05-Modules-How-To, Arduino-info.wikispaces.com. https://arduino-info.
wikispaces.com/BlueTooth-HC05-HC06-Modules-How-To
14. Credit for Fig. 9. https://c2.staticﬂickr.com/8/7500/27730883760_1dcc4de4dc_o.png
15. Credit for Fig. 10. https://arduino-info.wikispaces.com/ﬁle/view/BT_HC05_Annotated.jpg/
549484826/1083x729/BT_HC05_Annotated.jpg
Brainiac’s Arm—Robotic Arm Controlled by Human Brain
859

Identiﬁcation of Parameters for Robot PUMA
560
Dejan Jokić1(&) and Slobodan Lubura2
1 International Burch University, Sarajevo, Bosnia and Herzegovina
dejan.jokic@ibu.edu.ba
2 Faculty of Electrical Engineering, University of East Sarajevo, Sarajevo,
Bosnia and Herzegovina
slobodan.lubura@etf.unssa.rs.ba
Abstract. Design of FPGA based controller for robot PUMA 560 requires
knowledge of a large number of robot parameters. Considering the fact that
robot manufacturer has not published the parameters, many scientists presented
their own parameter results. Considerate variability in reported results was
observed in values for mass and center of mass and therefore it was recognized
as a matter of scientiﬁc interest.
Keywords: PUMA 560  Center of mass  FPGA
1
Introduction
Robot PUMA 560 is an old industrial robot and probably the best mathematically
described robot. Since it is technologically outdated, it has not been used in industry for
a long time but due to a large number of publications about it, it found new application.
Nowadays it is frequently used in laboratories throughout the world for education
purposes as well as scientiﬁc research. Design of new controller became imperative, so
many researchers designed new controllers: PC—based [1–5], FPGA—based [6], etc.
Great insight into the deviation of the parameters is best observed in the paper [7] and
in one of the tables presented there [7] “Degree of variability in reported values of the
PUMA 560 parameters. NSD is normalized standard deviation, REV is ratio of
extremal values” (Table 1).
The greatest deviation is observed for the values of mass and centers of mass.
Measuring the mass of segments and establishing the parameters for centers of mass
can be performed in two ways. The ﬁrst, and more precise one, is disassembly of
segments, their measuring and modelling in CAD tools. Considering the fact that it
requires specialized tools, this method was abandoned.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_74

2
Analysis of Available Parameter Values
from the Literature
It is important to mention that the manufacturer of robot PUMA 560 never published
robot parameters due to business secrecy and incorporated military components. For
that reason, many researchers tried to identify accurate parameter values, but they never
reached agreement. Differences were so great that one should ask whether there had
been more different versions of robot PUMA 560. In course of his research on the robot
author Peter Corke found the suggested robot parameters and published them in the
paper [7]. The highest level of disagreement was upon masses of segments and gravity
centers. In Table 2 are presented values of masses of segments m and in Table 3 values
for gravity centers Lc.
Due to obvious disagreement of the authors upon certain parameters of the robot it
is necessary to perform measuring of parameters for the robot used. The most precise
manner of calculating gravity center for the robot is dismantling the robot into seg-
ments, their weighting and modeling in CAD tools. However, due to lack of dedicated
tools for robot PUMA 560, this method was dismissed. The second suggested method,
which was eventually used, was indirect measuring and calculation of the parameters.
Use of this method implies bringing the adequate torque to the robot which would
compensate for gravity of the segments. In accordance with the values of DC motor
current, which is necessary to keep the segments in unfavorable horizontal position,
Table 2. Masses of segments for robot PUMA 560 [7]
Param Armstrong Paul81 Tarn
m1
–
4.43
13.00
m2
17.40
10.20
22.40
m3
4.80
4.80
5.00
m4
0.82
1.18
1.20
m5
0.35
0.32
0.62
m6
0.09
0.13
0.16
Table 1. Degree of variability in reported values of the PUMA 560 parameters [7]
Param
Table NSD (%) REV
Kinematics
2
2.7
1.20
Link mass
3
29
2.93
Center of gravity
4
124
10.70
Moments of inertia
5
66
4.52
Motor torque
7
11
1.53
Armature inertia
9
29
2.06
Friction parameters 11
40
4.42
Identiﬁcation of Parameters for Robot PUMA 560
861

parameters of the segments could be calculated, with the assumption that the third
segment is smaller version of the second segment. There are grounds for this
assumption because, besides visual observation, both segments use motors as
counter-weights. The second segment has signiﬁcantly stronger motors which drive
second and third axis and the third segment contains motors for driving fourth and ﬁfth
axis. Following steps in determination of parameters m2, m3, Lc2 and Lc3 would have
been determining of work space (work volume) and current which creates gravity
compensation torque (Figs. 1 and 2).
3
Workspace Boundary Determination for Basic Robot
Conﬁguration
After the robot had been put into function, certain disagreement was found in joint
space values found in available literature and presented in this paper. Encoders on
robot’s motors were used for deﬁning joint space for each robot axis. Placing the robot
in ﬁnal positions and reading the number of pulses enabled determination of workspace
for ﬁrst three robot axes (basic robot conﬁguration). In Table 4 are presented experi-
mental results of the performed measurement and calculated values of resolution per
single degree of joint movement, as well as total joint space.
Table 3. Gravity centers of robot segments for robot PUMA 560 [7]
Param Armstrong Paul81 Tarn
sx1
–
0
0
sy1
–
80
4
sz1
–
0
−309
sx2
68
216
103
sy2
6
0
5
sz2
−16
−26
−40
sx3
0
0
20
sy3
−70
−216
−4
sz3
14
0
14
sx4
0
0
0
sy4
0
0
−3
sz4
−19
−20
−86
sx5
0
0
0
sy5
0
0
−1
sz5
0
0
−10
sx6
0
0
0
sy6
0
0
0
sz6
32
10
3
862
D. Jokić and S. Lubura

Minimum deviation between measured and observed values presented in Table 5
was noted for performed measuring of joint precision for the ﬁrst three axis.
However, signiﬁcant deviation was observed between measured joint space
(Table 4) and the one provided in [8]. The reason behind it may be the use of different
ﬁnal position limiters or their damage (the robot tested uses rubber limiters).
4
Measuring Gravity Inﬂuence and Gravity Compensation
For determination of center of mass position Lc2 and Lc3 for second and third axis
(which are inﬂuenced by gravity) were used experimental methods which include
simple measuring of motor current and it is fully justiﬁed because there is no unani-
mous opinion of the authors on its values (Tables 2 and 3). Namely, parameters for
center of mass were obtained on the basis of the measured necessary torque which
keeps the axis in prescribed position (which compensates for the gravity force which
inﬂuences that particular robot conﬁguration). Information that has to be kept in mind
in the course of measuring the motor current is the information about the segment angle
which is determined by counting the pulses from encoder.
4.1
Results of Measuring the Current for Second and Third
Axis Motor Drive
Motor drive current for second and third axis which creates the torque necessary for
gravity compensation was measured with analog amperemeter (accuracy class: 0.5).
Table 4. Joint space and resolution
Joint no. No. of pulses—total Joint position precision Joint space
1.
59645
0.00581°
346.7°
2.
77447
0.00413°
320°
3.
46174
0.00694°
320.6°
Table 5. Encoder resolution [8, 9]
Joint Encoder resolution Counts/motor rev. Gear ratios Joint position precision
1.
250
1000
62.61
0.0058°
2.
200
800
107.36
0.0042°
3.
250
1000
53.69
0.0067°
4.
250
1000
76.01
0.0047°
5.
250
1000
71.91
0.0050°
6.
250
500
76.63
0.0094°
Identiﬁcation of Parameters for Robot PUMA 560
863

For second and third axis there were six different positions selected, where inﬂu-
ence of gravity was the greatest. In Fig. 1a, b are presented the results of measuring the
current of the second motor in vertical and horizontal position of the third axis.
In Fig. 2 are the measurement results for the third axis for the highest gravity
inﬂuence.
There is signiﬁcant difference in motor current required for gravity compensation of
the third axis in left and right position. The difference noted is the sign of difference in
worn out motor brushes and redactor gears.
4.2
Determination of Parameters for Third Robot Axis
Parameter Lc3 was determined on the basis of the below presented block-scheme in
Fig. 3 and motor and reductor coefﬁcient (which slightly differ from one author to
another [7]).
Il=1.2 A
Id= -1.2 A
(a) 
Il=1.9 A
Id= -1.8 A
(b) 
Fig. 1. Measuring of the current for second axis drive at a vertical and b horizontal position of
the third axis
 Il= 0.67 A
 Id= -0.52 A
Fig. 2. Measuring of the current for third axis drive
864
D. Jokić and S. Lubura

For the measured current values and entered motor and reductor values the fol-
lowing applies: Il = 0.67 A, Id = −0.52 A, Km ¼ 0:24 Nm
A , Kr ¼ 53 and K = 1. For the
reductor s ¼ Krsm applies, where Kr is reductor gear ratio and sm is the value for motor
torque. The value of torque for both currents, i.e. for both left and right position, is
obtained from the parameters:
sl ¼ 0:67 A
½   0:24 Nm
A


 53  8:5 Nm:
ð1Þ
sd ¼ 0:52
j
j A
½   0:24 Nm
A


 53  6:61 Nm:
ð2Þ
According to the equation for gravity compensation torque G3 [10]:
G3 ¼ m3gLc3 sinðh2 þ h3Þ
ð3Þ
where h2 ¼ 0; and h3 ¼ 90; sinðh2 þ h3Þ ¼ 1 applies so, for the mean value of torque
for both position G3 ¼ m3gLc3 ¼ 7:5 Nm (4) applies
where m3 = 5 kg and g ¼ 9:81 ms2
According to the above mentioned data we conclude that Lc3 = 0.154 m, which is
within the range of values provided by various authors [7].
4.3
Determination of Parameters for Second Robot Axis
Considering the fact that the third axis is by appearance and location of the motor
second robot axis but smaller in size so we may assume that Lc2 = Lc3 applies. In
Fig. 4 is block-scheme of third robot axis.
For parameters: Il = 1.9 A, Id = −1.8 A, K = 1, Kr = 107, Km ¼ 0:24 Nm
A
it is
applied for both positions, left and right:
Amplifier 
(driver)K
Motor
Km
Reductor
Kr
I
τ
PUMA 560
robot arm
q2 
τm
G2
Fig. 4. Block-scheme of second robot axis
Amplifier
(driver) K
Motor
Km
Reductor
Kr
I
τ
PUMA 560 
robot arm
q3 
τm
G3
Fig. 3. Block-scheme of third robot axis
Identiﬁcation of Parameters for Robot PUMA 560
865

sl ¼ 1:9 A
½   0:24 Nm
A


 107  48:79 Nm
ð4Þ
sd ¼ 1:8
j
j A
½   0:24 Nm
A


 107  46:22 Nm
ð5Þ
And according to the equation for gravity compensation of the second axis torque G2
[10]:
G2 ¼ ðm2gLc2 þ m3gL2Þ cos h2 þ m3gLc3 sinðh2 þ h3Þ
ð6Þ
where h2 ¼ 0° and h3 ¼ 90, cos h2 ¼ 1 and sinðh2 þ h3Þ ¼ 1 is applied, so it follows
that
G2 ¼ m2gLc2 þ m3gL2 þ m3gLc3
ð7Þ
Parameters are [24]: m2 = 10.2 kg, m3 = 5 kg, L2 = 0.43 m, Lc3 = Lc2 = 0.154 m
G2 ¼ 10:2 kg
½
  9:81 m
s2
h i
 0:154 m
½  þ 5 kg
½
  9:81 m
s2
h i
0:43 m
½  þ 0:154 m
½ 
ð
Þ
and the result is as follows:
G2 ¼ 44 Nm:
ð8Þ
5
Conclusion
In this paper was presented the problem of determining the values of parameters for
centers of mass of the second and third axis where the highest variability of reported
values was observed for different authors. The suggested concept of determining the
values was measuring the values of motor current and calculation of torque necessary
for gravity compensation. The result obtained in (8) has less than 10% deviation in
comparison to the experimental results presented in (4) and (5). It is important to
mention that the damage to motor gears caused by use signiﬁcantly inﬂuences the error
observed which could be seen in differences in values of the current measured in left
and right position, necessary for compensation in second and third axis. Accurate
parameter values may be obtained only through disassembling the robot and measuring
the parameters. This method was dismissed due to lack of dedicated tools. This paper
contains materials used in one part of the author’s dissertation [10].
866
D. Jokić and S. Lubura

References
1. Farooq, M., Wang, D.: Implementation of a new PC based controller for a PUMA robot.
J. Zhejiang Univ. Sci. A 8(12), 1962–1970 (2007)
2. Becerra, V.M., Cage, C.N.J., Harwin, W.S., Sharkey, P.M.: Hardware retroﬁt and computed
torque control of a puma 560 robot updating an industrial manipulator. IEEE Control Syst.
24(5) (2004)
3. Katupitiya, J., Radajewski, R., Sanderson, J., Tordon, M.: Implementation of a new PC
based controller for a PUMA robot. In: Proceedings of the 4th Annual Conference on
Mechatronics and Machine Vision in Practice (1997)
4. Costescu, N., Lofﬂer, M., Zergeroglu, E., Dawson, D.: QRobot—a multitasking PC based
robot control system. In: Proceedings of the IEEE Conference on Department of Electrical
and Computer Engineering Clemson University (1998)
5. Jokić, D., Lubura, S., Đorđević, G.: Projektovanje i realizacija kontrolera za robot PUMA
560, Infoteh-Jahorina, vol. 9, Ref. A-22, pp. 105–109, Mar 2010
6. Piltan, F., Sulaiman, N., Marhaban, M.H., Nowzary, A., Tohidian, M.: Design of
FPGA-based sliding mode controller for robot manipulator. Int. J. Robot. Autom. (IJRA)
2(3) (2011)
7. Corke, P.I., Armstrong-Helouvry, B.: A search for consensus among model parameters
reported for the PUMA 560 robot. In: 1994 Proceedings of the IEEE International
Conference on Robotics and Automation, San Diego, CA (1994)
8. Peter, I.: Corke: The Unimation Puma servo system. CSIRO Division of Manufacturing
Technology Preston, Australia (1994)
9. http://en.wikipedia.org/wiki/Unimation. Accessed Jan 2014
10. Dejan, J.: Realizacija upravljačkog okruženja za robot PUMA 560. Doktorska disertacija.
ETF I, Sarajevo (2016)
Identiﬁcation of Parameters for Robot PUMA 560
867

System for Distributed Measurement
of Ambient Conditions in Homes
Kenan Husić and Tarik Uzunović(&)
Faculty of Electrical Engineering, University of Sarajevo, Sarajevo, Bosnia and
Herzegovina
tuzunovic@etf.unsa.ba
Abstract. Measurement of ambient conditions in homes is one of the main
preconditions for their control what represents one of the steps toward the
complete implementation of smart homes. The objective of this study is to
implement a system for distributed measurement of temperature, illuminance
and humidity. In this work, all parameters to be measured will be analyzed, and
applied sensors will be described. In order to have better insight in the measured
parameters, measurements on different locations are made and all data are sent to
one place. One of the main purposes of the project is to accomplish low price of
the implemented system. In addition, a simple software application is made, and
it allows visualization of the measurement results and their statistical analysis.
1
Introduction
This work is just beginning of a project that we can call smart home implementation.
Realization of a smart home is important because it makes people live more com-
fortable and it minimizes user’s intervention in home settings.
Implementations of similar systems were reported in the literature. In [1], the
authors presented a solution for building a smart home. Arduino platform is utilized to
make standard home devices behave as smart things. In the presented solution, the
authors are monitoring home conditions employing different sensors, and also con-
trolling several actuators. ZigBee communication protocol is used to establish inter-
action between smart things and several Arduino platforms. In addition, there is also a
web application based on cloud service, which provides storage for the measurement
results and also computing resource. All sensors and actuators are connected to the
cloud through one Arduino device. Therefore, a problem with that device may make
the whole system dysfunctional.
An interesting and effective system is proposed in [2]. Android based smart home
application is developed, and used to communicate via Internet with micro web-server
consisting of Arduino Mega device and Arduino Ethernet shield. This web server is
connected to sensors and actuators in smart home. The operation of the whole system
heavily depends on the server operation. Additionally, the sensors and actuators are
connected directly to the server, meaning that they have to be in its vicinity.
A similar system to the previously mentioned that combines ZigBee and X10
technology for home automation purposes is implemented in [3]. These two tech-
nologies are used to connect home devices to a central controller, which has the same
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_75

structure as in [2]. It is again Arduino Mega with Ethernet shield. It again communi-
cates via internet with an Android application. It is also possible to have local access to
the central node using direct wireless connection. The system includes task scheduling
which takes into account instantaneous power targets and also cost targets.
A low cost home control and monitoring system is introduced in [4], and the same
system is discussed more in detail in [5]. Arduino based web server is implemented on
central node in order to communicate with an Android mobile application. Similarly to
the system from [2], there is direct connection of sensors and actuators to the central
node, which means that sensors and actuators need to be placed close to the central node.
The project described in this paper is dealing with measurement of different
ambient parameters on various locations. The locations could be inside several rooms,
or several apartments in a building being monitored. After collecting data from the
measurement nodes in the realized system, all data is being sent to one place that is in
this work a server on the Internet, which can be easily accessed through a web page.
The obtained measurement results are sent via wireless communication. One of the
objectives is to make the overall system cheap but functional. An advantage of this
system compared to the mentioned ones is that each node has direct connection to the
server. Therefore, the measurement nodes can be distributed on arbitrary locations and
dysfunctionality of one node does not affect the whole system, since the measurements
from other nodes are still available. The system presented here has been tested for two
nodes, but it can be easily expanded to have a greater number of nodes.
2
Hardware Description of the Implemented System
In this section, the hardware of the realized system is described in detail. The section starts
with overall system description, and then the measurement nodes are discussed in detail.
2.1
Overall System Description
In this work, a system with two measurement nodes is implemented and it is repre-
sented in Fig. 1. Structure of the nodes is depicted in Fig. 2.
Measurement
node 1
Measurement
node 2
ADSL Wi-Fi
modem router
Wi-Fi channel
Wi-Fi channel
ThingSpeak
platform
Internet
Fig. 1. Overall system representation
System for Distributed Measurement
869

At each node temperature, illuminance and humidity are being monitored. The used
sensors are: LM35 [6], photoresistor and DHT11 [7]. All three sensor are connected to
Arduino Uno board and collected data is placed on microcontroller. The data are then
sent to ThingSpeak platform available through a website thingspeak.com. Using the
platform, one can collect large amount of data. The data are sent in form of messages
and stored in channels. Each message can contain measurements from up to eight
sensors, and one can send approximately 8200 messages per day if the free license is
used. Three parameters to be measured are selected due to their importance for com-
fortable living. Temperature is one of the most often controlled parameters in homes.
Making the control automatic requires temperature measurement. Humidity is espe-
cially important for plants, and therefore it is important to know whether it is inside
some desirable limits. Having illuminance at some desired level is highly desired, and it
can be also applied for lighting control, which then also has signiﬁcant economic
impact.
One of the most important advantages of this system is its low price. Moreover, due
to usage of very common electronic components it can be easily implemented on other
places. In addition, the system can be easily expanded for measurement of other
variables.
A very important part of this system is ThingSpeak platform accessible through its
webpage. It is IoT (Internet of Things) platform that collects data and stores data from
sensors on cloud. The platform gives possibility for analysis and visualization of
different data. It is also possible to extract data using webpage in Matlab and in CSV
format. The platform represents the central node to which all data is collected and it
accepts data that are being sent by Arduino, Raspberry PI, ESP 8266 module, and other
hardware components.
2.2
Measurement Node
The main component of each node is Arduino Uno board. It is “brain” of the node,
which acquires measurement signals from the sensors, processes them and sends them
to ThingSpeak platform. The board is powered through USB connector from external
5 V power source. Arduino Uno is now widely used processing platform, supported by
large community which created many useful libraries which allow fast and efﬁcient
code development.
The second component of the each node is the temperature sensor. In this work
LM35 [6] sensor is used. It is selected for its low price, reliability and linear
temperature-voltage characteristics. The sensor has only 3 pins, and output voltage is
linearly proportional to a measured temperature.
There is an issue with these sensors when there is cycling of air directly on them.
Then situation is getting challenging because for example if you put hot air fan directly
to LM35 sensor while it is reading data, it makes incorrect reading, it measures 0 °C
and temperature should be at least 70 °C. This unexpected situation can last up to
40 min before the sensor starts to work normal again. When no physical movement of
the sensor exists, there is no cycling of air, and sensor is supplied with a voltage from 4
to 20 V, it is working satisfactory. It is important to mention that output voltage of this
sensor is linear with change of temperature, so its sensitivity is +10 mV/°C.
870
K. Husić and T. Uzunović

Output voltage of LM35 is connected to analog input of Arduino device, read as
10-bit data, and measured temperature is calculated by the following equation
T C
ð
Þ ¼ 5V  reading
1024  1000  0:1
C
mV
ð1Þ
Second component of each node is sensor DHT11. It is slightly more expensive
than temperature sensor. However, two sensors are actually combined on one chip,
temperature sensor and humidity sensor. In this work, it was used in combination with
DHT library that is available for Arduino users to be utilized in combination with this
sensor. The sensor is powered by 5 V. The problem that exists with this sensor is that it
has relatively big delay in measuring temperature of around 10 s, and for humidity
measurement it is approximately 5 s. Therefore, one cannot directly compare tem-
perature measurement of DHT11 and data obtained from LM35. Due to this issue, it
was decided to use LM35 for getting information about temperature. DHT11 is pro-
viding measurement information in the form of digital data, and it is being acquired by
Arduino using one of its digital inputs.
Third component for each node is sensor for measurement of illuminance. A pho-
tocell (photoresistor) is utilized, and the main reason was its low price. Photoresistor is
capable of providing reliable data, and it has small measurement delay. The sensor is
powered with 5 V. In order to make it work correctly, it is necessary to make voltage
divider, and for that purpose one 10 kΩ resistor is utilized. The output voltage Vo is
measured on 10 kX resistor. Then, the photoresistor resistance is calculated as
RPhotocell ¼ 50  103  10  103  Vo
Vo
:
ð2Þ
Fig. 2. Structure of measurement nodes
System for Distributed Measurement
871

The measured illuminance is then
Ev lx
ð Þ ¼
500
RPhotocell
:
ð3Þ
Fourth component for each node is Wi-Fi module ESP826. The module is used for
sending measurement data to ThingSpeak. The module works with 50 mA current that
is supplied by Arduino. In the module’s manual it is stated that it may require up to
160 mA once it is sending the data. However, the practical experiments showed that it
works satisfactory even with 50 mA, even though it sometimes fails to send a package
due to limited input current. However, this solution was selected since it does not
require an additional circuit capable to provide a higher current. It is highly important
to disconnect RX and TX from Arduino while uploading a program to Arduino. After
upload, they have to be connected again.
3
Software of the System
Software of the system, written for Arduino platform, is represented in Fig. 3. Besides
standard Arduino libraries, it is necessary to include DHT.h library which is used for
obtaining measurement results from DHT11. Several variables are deﬁned, along with
delay between two consecutive transfers of data to ThingSpeak platform and baud rate
for serial communication. Since DHT.h can be used for DHT11 or DHT22 sensor, it is
necessary to specify which type is utilized. In order to communicate with ThingSpeak,
its IP address has to be speciﬁed. After that mode of operation for ESP8266 is set (in
this work, station mode), it is indicated whether multiple connections to platform will
exist. Multiple connections are necessary if several ESP8266 modules from measure-
ment nodes will be sending data, like in this study. Then, wireless connection to
ThingSpeak is established. The next step is to obtain signals from sensors, process
them and send measurement data to ThingSpeak platform.
4
Results and Discussion
The presented system was tested in home environment. The obtained results are
depicted in Fig. 4 for one measurement node. Besides the measured parameters, one
can show different statistical parameters in graphical windows (for example, average,
median, sum, etc.). It can be observed that measured temperature is varying pretty fast.
The reason is high sensitivity of LM35, and limited resolution of Arduino’s ADC
which is only 10 bits. Therefore, one bit of ADC reading corresponds to to almost
0.5°C. The shown results are taken with Td = 24 s.
If one analyzes the system, the following advantages can be listed:
• Low investment for components
• Sensors which can be integrated with Arduino platform are chosen
• Components have small dimensions, and they can be combined to a small box
• System can work with battery supply
872
K. Husić and T. Uzunović

• Existence of libraries for all sensors on Arduino platform
• Collected data can be extracted easily in Matlab or CSV format
Include DHT.h library
Define necessary variables for sensors and Wi-Fi module
Define time delay Td for sending data to web page
Define baud rate for serial communication
Define type of DHT sensor
Define string with IP address of ThingSpeak 
Define mode for ESP8266 
Define type of connection
Establish connection to ThingSpeak
Collect data from LM35
Collect data from DHT11
Collect data from photoresistor
Write collected data in a package so the package can be 
sent to ThingSpeak by ESP8266
Send data
Delay for Td
Fig. 3. Software component of the system
System for Distributed Measurement
873

One the other hand, the following disadvantages can be found:
• Used sensors have low class of accuracy
• Sensors are inﬂuenced by air cycling and movement
• System needs high current, Wi-Fi module consumes up to 160 mA during sending
• Only up to around 4100 measurements per day can be collected from one sensor,
since two channels are used for two nodes, and in total about 8200 messages can be
sent per day
Some of the listed disadvantages could be overcome. Sensors with better accuracy
could be used instead of applied ones. Inﬂuence of the air cycling and movement could
be avoided by placing the sensors on appropriate locations and making them stationary.
The realized system has been tested for power consumption while a node was powered
by 9 V 500 mAh battery. With delay between two consecutive transfers to ThingSpeak
equal to 16 s, the node was able to operate for around 3 h. When the delay was 60 s,
the total operation time was almost 10 h. Therefore, setting the delay to higher values
could open possibility to power the nodes by batteries. This could be justiﬁed since the
ambient conditions do not change very fast in normal conditions. In addition, there is a
possibility to put ESP8266 to sleep mode while it is not transferring data and that can
further reduce the consumption. Even though number of measurements for one sensor
is limited, it is not a huge constraint, since the limit is high and also other cloud service
could be used in the future. Thus, the system can be further improved and it will be part
of our future work.
Fig. 4. Results collected on ThingSpeak
874
K. Husić and T. Uzunović

As stated in the introduction, an important advantage of the proposed system
compared to those mentioned in that section is that each node has its own direct
connection to ThingSpeak platform. Therefore, the measurement nodes can be dis-
tributed on arbitrary locations and dysfunctionality of a node does not affect the whole
system, since the measurements from the other node are still available. The system can,
of course, be implemented to have a larger number of measurement nodes.
5
Conclusion
The introduced system represents well performing measurement platform for moni-
toring of ambient conditions. Main advantages of the system are low price and small
dimensions. Possibility of exporting measured data also gives big importance of this
system over other systems. In the future, the aim is to remove recognized disadvantages
of the system and to combine a control system with this measurement system, which is
even step closer to smart home.
References
1. Soliman, M., Abiodun, T., Hamouda, T., Zhou, J., Lung, C. H.: Smart home: integrating
internet of things with web services and cloud computing. In: 2013 IEEE 5th International
Conference on Cloud Computing Technology and Science (CloudCom), vol. 2, pp. 317–320
(2013)
2. Kumar, S.: Ubiquitous smart home system using android application. Int. J. Comput. Netw.
Commun. 6(1), 33–43 (2014)
3. Baraka, K., Ghobril, M., Malek, S., Kanj, R., Kayssi, A.: Low cost arduino/android-based
energy-efﬁcient home automation system with smart task scheduling. In: 2013 Fifth
International Conference on Computational Intelligence, Communication Systems and
Networks (CICSyN), pp. 296–301 (2013)
4. Piyare, R., Lee, S.R.: Smart home-control and monitoring system using smart phone.
In: ICCA 2013, ASTL, vol. 24, pp. 83–86 (2013)
5. Piyare, R.: Internet of Things: ubiquitous home control and monitoring system using android
based smart phone. Int. Jo. Internet Things 2(1), 5–11 (2013)
6. Texas Instruments: LM35 Precision Centigrade Temperature Sensors Datasheet [Online].
http://www.ti.com/product/LM35/datasheet (2016)
7. D-Robotics: DHT11 precise temperature and humidity sensor datasheet [Online]. http://www.
micropik.com/PDF/dht11.pdf (2010)
System for Distributed Measurement
875

Automatic SAR Target Recognition and Pose
Estimation. Part 1. Geometric Methods
for Pose Estimation
Tarik Namas and Migdat Hodžić(&)
FENS, EEE Department, International University of Sarajevo, Sarajevo, Bosnia
and Herzegovina
mhodzic@ius.edu.ba
Abstract. The paper presents geometrical methodology of determining pose
angle of an object against SAR images stored in a data base. The images are
separated by a pose angle (less than 2°), obtained from an air surveillance plane
at a depression angle and at a distance, producing SAR images of 1  1 foot
pixel resolution. The data base is public (US Government) for academic and
defense research. The data base consists of 100s of commercial and military
vehicles, as well as building structures. In this paper, we focus on three targets
which have symmetric geometry, each of different size and shape. In addition,
we also generate several synthetic targets of various symmetric shapes to serve
as the ideal test cases. The analysis is based on simple geometrical considera-
tions as well as edge and surface area determination using number of SAR
signatures pixels. The overall methodology aims at signiﬁcantly reducing data
base search space to produce an effective and fast computationally viable search
algorithm. The result is very accurate target pose angle determination of the
order of 96–98% precision. Once the pose angle is determined, that can be
further used to determine target type as well, described in “Automatic SAR
Target Recognition and Pose Estimation, Part 2. Statistical Methods for Target
Recognition”.
1
Introduction
Historically, radars have been used to estimate the range, directions and velocities of
targets, [16, 17, 18]. These parameters alone are not sufﬁcient to recognize a target. The
advances in the radar imaging led to the development of the synthetic aperture radar
(SAR), and high resolution radar (HRR). SAR systems produce large amount of data
when operated in complex environments [1]. If implemented for intelligence, surveil-
lance and reconnaissance (ISR), such large data would require enormous human efforts,
hence, to gain the maximum amount of efﬁciency from deploying SAR imaging in real
time operations an Automatic Target Recognition (ATR) system should be used [2]. For
the purposes of reducing the search space in a template based ATR we present in this
work a detailed geometrical pose angle analysis of the ground targets signatures. The
geometrical analysis of targets pose angles serves for reducing error in recognition
process as well. In our previous papers, [4, 5] we described MSTAR Data Base in more
details [8, 13]. This paper is organized as follows. After the introduction, Sect. 2 shows
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_76

the signatures formation of the data. Then Sects. 3–5 discuss different geometric esti-
mations of the target’s pose angle. Those include, a set of nonlinear equations, line
ﬁtting and corners method. Section 6 discusses combining the previous mentioned
methods and shows some results. Finally, conclusion and references are presented.
2
Target Signatures
The images of the targets in the real data base consists of 2D matrices that contain grey
level values based on the targets shape and pose angles. This is shown in Figs. 1 and 2.
Fig. 1. SAR images of some ground targets [8]
Fig. 2. The ﬁrst 4 target chips of the T72-A04 tank in two depression angles. Source [8]
Automatic SAR Target Recognition and Pose Estimation
877

Our proposed method uses two 1-D spatial signatures as an essential component of
the template. The ﬁrst step in template preparation is to ﬁnd the two 1-D signatures for
each target image. The mean values of the grey-level pixels of the image data are used.
The process of ﬁnding the means of grey-level values rows and columns wise are found
according to Eq. (1) [6] is illustrated in Fig. 3.
li ¼ 1
N
X
N
j¼1
Ai;j;
lj ¼ 1
N
X
N
i¼1
Ai;j
ð1Þ
where:
li; lj
the mean values of the gray level of the pixels’ column and row respectively
N
the length of the columns and rows of the images (square images are used here)
Ai;j
the gray scale value of the pixel at location (i,j) of the image
The two signatures associated with a target chip are shown in Fig. 4. Other
parameters can be considered for these signatures, like the variance of the grey-scale
levels row and column wise. A template database of these vectors for all of the targets
of interest is prepared and stored for the purpose of ATR. Figure 5 shows the images of
the T72-A04 tank as it rotates from 0° pose angles with respect to the sensor to 90°
pose angles.
Fig. 3. Illustration of ﬁrst type of signature formation using the mean of the image
878
T. Namas and M. Hodžić

Fig. 4. Four different 1-D vector signatures of a single target
Fig. 5. T72-A04 targets in the ﬁrst quadrant with dep. angle of 15° [8]
Automatic SAR Target Recognition and Pose Estimation
879

3
Synthetic Data Base
As a preliminary benchmark for testing the suggested method, and before going
through the MSTAR data, we generated a dataset to mimic the real dataset. The basic
idea behind this artiﬁcial dataset is to have a road map on the direction with which our
research should go. With the improvement of sensor technology, parts of our synthetic
data set will not deviate a lot from data collected through advanced sensors. The data
set consisted of 3 types of images, namely; ideal, quasi-ideal and random with three
different targets. An example of the synthetic data targets at position 30, is shown in
Fig. 6.
Even though the synthetic targets cannot be used for conﬁrmation purposes of the
results and for reporting, they can be used for algorithm testing and ﬁne-tuning.
Fig. 6. Random synthetic target at position 30 (39.4°) and their signatures
880
T. Namas and M. Hodžić

4
Non-linear Equations
Our aim is to determine the target type and pose through a set of estimations. In every
estimation, we assume that we are closing on the target and getting a better estimation
of its type and pose angle. The ﬁrst estimation is the geometrical. In this estimation, the
main goal is to approximate the pose angle of the target while assuming that such a
pose angle is possible in all four quadrants, [4, 5]. Although the principle is the same
for both synthetic and real targets, this estimation for real targets showed to be a subtle
matter as discussed in the following.
4.1
Synthetic Targets
To address the geometric analysis in synthetic targets we use Fig. 7 (left) which
illustrates a simpliﬁed target geometry in (x, y) coordinates using an angled target
signature between 0° and 90°. Note that not every target will be in a perfect rectangular
shape but this is very good approximation for asymmetric target if we are interested in
its maximum (length) and minimum (width) dimensions. From Fig. 7 we easily obtain
the following set of three nonlinear algebraic equations which summarizes geometric
relationships between three key geometric features of the target signature, i.e. width,
length and pose angle. These equations need to be solved for X-s given Y-s, when we
acquire “real time” target as well as for justiﬁcation of their use for the known data base
targets.
X1 cos ðX3Þ þ X2 sin ðX3Þ ¼ Y1
X1 sin ðX3Þ þ X2 cos ðX3Þ ¼ Y2
X1X2 ¼ Y3
ð2Þ
where:
X1, X2
the target’s length and width respectively
X3
the target’s pose angle (constrain is that X3 between 0° and 90°)
Y1, Y2
the target’s projections on x-axes and y-axes respectively
Y3
the target’s area (number of pixels it occupies)
In order to calculate Y1 and Y2 we need to perform further analysis of the target
signatures. Namely, we will need to “clean up” or “pre-ﬁlter” target signatures by
eliminating all zero or near zero amplitudes right before the target signature exhibits
signiﬁcant spatial amplitude, starting from the left side. We can call these signature
leading zeroes.
Similarly, we will clean the trailing zero on the right side of the signature. Equa-
tion (2) can be solved for a “real time” target once acquired by the sensor platform. The
trust region [9] with dogleg method is used to solve this system of non-linear equations.
It is used because it is a dedicated method for non-linear equations, as well as it is the
default algorithm in MATLAB nonlinear equations solver. The solutions X1 and X2
correspond to the length and width of the rectangular target, and X3 is corresponding
pose angle estimate. The initial solution is generically presented in 1st quadrant (0°–
90°). Geometric symmetry of target shown in Fig. 7 (right). The real quadrant on the
Automatic SAR Target Recognition and Pose Estimation
881

other hand is determined in a second step where we calculate 1st order statistics. This
will act as an initial estimate of pose and will identify the target type as well. Further
use of other statistics will improve the estimate. The following example illustrates the
approach. We chose a random synthetic target at 224th position in the synthetic target
data base. Figure 8 (upper) shows the synthetic target while Fig. 8 (lower) shows the
Fig. 7. (left) Simpliﬁed target geometry, (right) targets symmetry in 4 quadrants
Fig. 8. (upper) Synthetic target at position 224, (lower) signature of the synthetic target Y1, Y2,
can be found by inspection
882
T. Namas and M. Hodžić

target’s signatures on x and y directions. After solving the equations for X1, X2, and
X3 we get the ﬁrst estimate of the target geometry and pose angle. This information
determines the target position at 50 (1st quadrant), out of 274 in the circle. Due to
possible errors, different number of targets from the data base around the ﬁrst geometric
estimate are chosen for comparison purposes and for second stage estimation. The real
quadrant (4th in this case) is determined by analyzing three more signatures mirrored
across horizontal, vertical and diagonal directions with respect to target 50, and situated
in quadrants 2, 3 and 4 Fig. 7 (right) (Table 1).
4.2
Real Targets
The results for real targets are found by substituting Y1 which is found from the
horizontal signature of the target’s image, Y2 which is found from the vertical signature
of the target’s image, and Y3 which is the number of pixels in the target’s image into
Eq. (2). Finding Y1, Y2 and Y3 using synthetic target’s is not a hard task due to the
fact that all surroundings of the target are 0 gray level pixels. When trying to apply this
method to real targets the ﬁrst obstacle encountered is the fact that the target’s signature
and the target representing image are almost without any zero gray level pixels.
A representation of real target and its signature can be seen in (4) and (5) (Fig. 9).
Various methods are suggested and tested to determine the input parameters to
Eq. (2) when real targets are in question. Two methods described in the following
subsections are used for determining parameters Y1 and Y2, which will we refer to as
the projections of the target on x and y axes.
4.2.1
Dynamical Spatial Threshold
The dynamical spatial threshold is a simple and fast method where the signatures are
ﬁrstly normalized, then a threshold is assigned accordingly. The target’s signature
begins and ends with ﬁrst and last pixels whose normalized gray level equal to the
threshold value. An illustration is shown in Fig. 10. The threshold values are chosen to
be as the mean of the means of the gray level values for that particular image. Mean of
the means is found by calculating the means of the gray level column or row wise, then
calculating the mean of the resulting vector.
Table 1. Target pose estimation example
X1 (Width)
8.98 ft
X2 (Length)
31.06 ft
X3 (Pose angle) 65.63°
Position no.
50
Automatic SAR Target Recognition and Pose Estimation
883

4.2.2
One Dimensional HAAR Threshold
Another method for assurance of the values of Y1 and Y2 that were found by the
dynamical spatial threshold is a one-dimensional HAAR transform. The idea of using
HAAR transform comes from the fact that wavelets are good in detecting frequency
changes in signals. The sudden change in the signatures amplitude in both rising edge
and descending edge is equivalent to change in frequency, from low to high frequency.
The Haar transform is the simplest among the wavelet transformation functions from
the space or time domain to a local frequency domain. It has the advantages of being
simple for implementation and the ability of extracting local features. There are two
functions that play the primary role in wavelet analysis, the scaling function and the
wavelet function [7]. A 1-D Haar transform can be used to detect such frequency
changes, hence, can be used to determine the projections of the target on x-y axis as
show in Fig. 11, where the signature from previous example is used and the 1-D Haar
method is used to detect the change in frequencies in the signature. Any of the previous
two methods can be used to determine the projections of the target or both methods can
be combined to achieve the required outcome. It’s important to mention here that
ﬁnding the projections of the target in terms of high precision is not very crucial, a
Fig. 9. Real target and its signatures
884
T. Namas and M. Hodžić

tolerance of one or two pixels is acceptable because the surroundings of the targets at
that pose are going to be included in the search space.
4.2.3
Real Target Data
The comparison between real targets and synthetic targets shows that real targets suffer
from large amount of deviation from the expected values. That is due to various short
comings of the real target’s data itself. Figure 12 shows four different targets, while
Fig. 10. Illustration of dynamical spatial threshold
Fig. 11. Determining the projection of the target through HAAR coefﬁcients
Automatic SAR Target Recognition and Pose Estimation
885

Figs. 13, 14, 15, and 16 show the difference between real, quasi-ideal synthetic, and
real-like target images in terms of x, y projection as they rotate by 360°.
Fig. 12. Four targets that are rotated and their x-y projection is shown in next 4 ﬁgures
0
50
100
150
200
250
300
Target position
10
15
20
25
30
35
40
45
Projection of target in pixels = feet
x-y projection for real target
x-projection
y-projection
Fig. 13. x-y projections of a real target T72 as it rotates by 360° (Fig. 12a)
0
50
100
150
200
250
300
Target position
5
10
15
20
25
30
35
Projection of target in pixels = feet
x-y projection for real target
x-projection
y-projection
Fig. 14. x-y projections of a real target D7 as it rotates by 360° (Fig. 12b)
886
T. Namas and M. Hodžić

The next step is to determine the number of pixels within a target data image as the
input parameter Y3 to our non-linear Eq. (2). Two methods are suggested in this thesis
to check for the number of pixels within a target image.
4.2.4
Dynamical Threshold Value
The ﬁrst method in detecting the number of pixels, is a simple and direct method where
a threshold value is used for each image to determine whether the pixel is a part of the
target image or not. The threshold level could be the same threshold level used in
determining the Y1 and Y2 projections in dynamical spatial threshold or any other
threshold value that can be calibrated according to requirements of the data base or
sensor conditions. Examples of the results for the suggested method are shown in our
previous papers [4, 5]. The image in Fig. 17a is transformed into binary image and
different results are shown in (b), (c) and (d) (Fig. 18).
It’s clear from the results in the above ﬁgures that various threshold levels will
result in different pixel counts of the target image. An automatic threshold level can be
used based on Otsu’s method [10]. This threshold result is a more realistic outcome as
shown in [4, 5].
0
50
100
150
200
250
300
Target position
10
15
20
25
30
35
Projection of target in pixels = feet
x-y projection for quasi ideal target
x-projection
y-projection
Fig. 15. x-y projections of a quasi-ideal synthetic target as it rotates by 360° (Fig. 12c)
0
50
100
150
200
250
300
Target position
10
15
20
25
30
35
Projection of target in pixels = feet
x-y projection for real like target
x-projection
y-projection
Fig. 16. x-y projections of a real-like target as it rotates by 360° (Fig. 12d)
Automatic SAR Target Recognition and Pose Estimation
887

4.2.5
Polygon Area Method
As seen from the previous images, the dynamical threshold value showed various
problems, sometimes by excluding points which are within the target image, or by
adding points from the clutter or the noise surrounding the target. So, a polygon area
method is tested to check if better results can be achieved. A polygon is assigned to
catch up most of the area of the target image, this is achieved by determining eight
vertices on the target image and connecting them, then calculating the area covered by
the polygon which is formed by the vertices. The choice of eight vertices is a pro-
gramming related decision, more or less vertices can be chosen. Figure 19 shows the
results of a polygon area for the T72 target at position 1, 40 and 140 respectively.
Fig. 17. Different threshold values and results of converting an input image to binary
Fig. 18. Binary image of input image of Fig. (17) using Otsu’s method
Fig. 19. Polygon area method
888
T. Namas and M. Hodžić

The area of the polygon containing the target’s data is given by (3), [11]
areapolygon ¼ x1y2  y1x2
j
j þ x2y3  y2x3
j
j þ    þ x7y8  y7x8
j
j
2
ð3Þ
where points x1, x2, …, x8 are the x-axis coordinates of the polygon’s vertices and y1,
y2, …, y8 are the y-axis coordinates of the polygon’s vertices. The true dimensions of
the T72 tank are 9.53 m in length with gun, while its 6.95 m without the gun, and
width is 3.59 m, since our sensor has a resolution in feet, we list the dimensions in feet;
length with gun: 31 ft 3 in, length without gun 22 ft 10 in, width 11 ft 9 in [3]. In other
words, we can estimate the area of the tank in images to be either round ð22:83 
11:75Þ ¼ 268 ft2 without the gun or with gun included it would be around
ð31:25  11:75Þ ¼ 379 ft2. When applying the last two methods for calculating the
area of the target images, we see that dynamical threshold value with automatic
threshold level using the Otsu method shows values of area closer to the area calculated
without the gun, namely 268 pixels, while the area using the polygon method gives
results closer to the area with the gun included. Figure 20 shows the area of the T72
tank measured by both methods as its rotated with 360° within the data base. We can
see from Fig. 20 that the automatic threshold method is more consistent with its
measurement of pixels’ number, but less around 20% of the pixels’ number. On the
other hand, the polygon method seems to catch some correct values, but adds more as
well when it’s not necessary. That can be explained by the fact that the gun sometime is
clear in the image data while in other times it’s not that clear.
0
50
100
150
200
250
300
target position
0
100
200
300
400
500
600
number of pixels
Automatic threshould
Polygon Method
Real area
Fig. 20. Number of pixels for T72 target at various poses measured by automatic threshold and
polygon area methods
Automatic SAR Target Recognition and Pose Estimation
889

For comparison, the number of pixels for the Zil truck is shown in Fig. 21 after
being calculated using both methods. The original dimensions of the Zil truck are
7.25 m in length and 2.44 m in width, which makes its area in pixels equal to 190
pixels. It can be noticed that the same pattern is repeated i.e. the automatic threshold is
consistent with some 20–50% loss of pixels, while the polygon method is spiky around
the correct value.
To ﬁnd the angle using Eq. (2) we use the outcome of one of the projection
methods and one of the number of pixels’ methods as inputs, then solve the system of
nonlinear equations. The results of the ﬁrst 70 positions which lay in ﬁrst quadrant of
the T72 truck are shown in Fig. 22. Those results were obtained using a modiﬁed
number of pixels, 20% were added to automatic threshold method while 15% were
0
50
100
150
200
250
300
target position
0
100
200
300
400
500
600
700
800
900
1000
number of pixels
Automatic threshould
Polygon Method
Real area
Fig. 21. Number of pixels for ZIL 131 target at various poses measured by automatic threshold
and polygon area methods
0
10
20
30
40
50
60
70
80
90
The position of the target in degrees
0
10
20
30
40
50
60
70
80
90
The estimated position of the target in degrees
Using automatic threshold
Using polygon area
Correct values
Fig. 22. Results of ﬁnding pose angles of the ﬁrst 70 targets of T72 using Eq. (2)
890
T. Namas and M. Hodžić

taken away from the polygon area method. From the ﬁgure, we can see that solving
Eq. (2) gives good estimation of the targets pose angle if the target is rotated between
30° and 70°, outside these two ends, the method estimates with much larger error, i.e.
more than 10°. Due to the large difference between the estimated angle and real one
outside the 30°–70° interval. For the 0°–90° range, two more methods are suggested to
estimate the pose angle of the target.
5
Line Fitting
The line ﬁtting [14] approach ﬁts two different straight lines that pass through the
target’s image along rows and columns of brightest pixels. The straight lines ﬁtted
provide a good estimate of the targets pose angle as one of the straight line ﬁtted with
respect to x projection while the other with respect to y projection. The results showed
that the ﬁtted lines are orthogonal which is consistent with the target’s geometry (the
real targets are symmetrical rectangular objects in 2-D). The least square methods for
straight line ﬁtting is used, where from the straight-line equation given by y = ax + b
the coefﬁcients a and b for best line ﬁt using least square method are found by the given
normal equations:
a P x2
i þ b P xi ¼ P xiyi
a P xi þ nb ¼ P yi
a ¼
n P
xiyi
ð
ÞP
xi P
yi
n P
x2
i
ð
ÞP
xi P
xi
b ¼
P
x2
i
P
yiP
xiyi P
xi
n P
x2
i
ð
ÞP
xi P
xi
ð4Þ
The summation goes up to n, where n is the number of points in the data set to be
ﬁtted, while i, is the summation index.
In order to perform line ﬁtting, three options are available to be used: (1) use the
position of pixels with maximum gray level values of column or row wise, or (2) use
the mean value of the positions of the brightest pixels and ﬁnally (3) use the position of
pixels after edge detection of the image or after converting the image to binary image.
From Fig. 23 we see two target images at two different positions (position 35 and
position 13) and the ﬁtted lines that approximate the pose angle of the target projected
on the x axis or from the point view of the sensor. We notice as well from the images
that the ﬁtted lines are approximately perpendicular, which asserts the assumption that
this method can be used to estimate the pose angle of the targets. In Fig. 23, the red
points are the points on the right edge of the target’s binary image. The blue points are
the points on the left edge of the target’s binary image. Right and left in this context is
with respect to the sensor. The results of applying the line ﬁtting method to ﬁnd the
pose of the various input images are shown in Figs. 24 and 25. With some exceptions,
the method, in contrast to the system of nonlinear algebraic equations, can estimate the
position of the target without large deviation in the 0°–20° and 70°–90° intervals.
Automatic SAR Target Recognition and Pose Estimation
891

Fig. 23. Line ﬁtting illustration for the estimation pose estimation using binary image
0
10
20
30
40
50
60
70
80
90
100
The position of the target in degrees
0
10
20
30
40
50
60
70
80
90
100
The estimate position of the 
target in degrees
Pose angle estimate
Correct values
Fig. 24. Results of ﬁnding pose angle of the ﬁrst 70 targets of T72 using curve ﬁtting
0
10
20
30
40
50
60
70
80
90
100
The position of the target in degrees
0
10
20
30
40
50
60
70
80
90
100
The estimate position of the 
target in degrees
Pose angle estimate
Correct values
Fig. 25. Result of ﬁnding pose angle of the ﬁrst 70 targets of the D7 using curve ﬁtting
892
T. Namas and M. Hodžić

6
Finding Corners Method
A third method for estimating the pose angle of the targets involves ﬁnding three
corners of the target and the distance between these three corners. This is valid because
of the fact that a naked eye estimation of the target’s pose angle is not difﬁcult, that is
clear from a simple look at the images. This method depends on some image pro-
cessing techniques; namely, the gray image is converted to binary, then a search is
initialized to ﬁnd the corners, this is shown Fig. 26.
The calculation of the angle is achieved by ﬁnding the tangent inverse of the slope
of a line with the x-axis. The line is the longer one among the two that connects
green-blue and green-red points from the ﬁgure above. Some issues arise due to noise
in the images and due to some hurdles in converting the gray image to binary.
The main two problems are (i) the wrong estimation of the side of the triangle that
is used for angle estimation corners and (ii) the out of target region corner due to noise.
Fig. 26. Illustration of the corners method for pose angle estimation of targets
Fig. 27. Illustration of the drawbacks of the corners method for pose angle estimation because
of wrong placement of the corners
Automatic SAR Target Recognition and Pose Estimation
893

Examples of such problems are shown in Figs. 27 and 28 respectively. It can be seen
that the corners found are not the right corners and the angle calculated due to that will
never be the correct one. To overcome these issues, some modiﬁcations are introduced
to the code used for corners extraction. The modiﬁcations include refusing the result in
case the distance between the corners are less than a certain value, as well as refusing
the result in case the corners found are isolated pixels. The pseudo-code for this
correction is given as follows (Fig. 29).
After applying the correction algorithm for the case shown in Fig. 27, we see the
new result in Fig. 30. The use of the corner method to estimate the poses of the ﬁrst 70
Fig. 28. Corner outside of the target’s region
Fig. 29. Problematic corners correction
894
T. Namas and M. Hodžić

targets of the T72 is shown in Fig. 31. It is obvious that the method catches close
estimates of the poses but with some offset.
7
Geometric Estimation of the Pose
The methods described in the previous sections to estimate the pose of the target within
image data do not always agree in the interval from 0° to 90°, which is the interval of
interest due to the symmetry of the target poses. The fundamental idea is to determine
the pose angle within the ﬁrst quadrant for the investigated target. Then take the
symmetrical poses in the remaining three quadrants. To do so, we found that the most
appropriate approach is to give certain weights to the pose angles found according to
the method they were found by. Then to apply these weights in determining the pose
estimate. For more efﬁciency and consistency, a rule based decision making algorithm
Fig. 30. Corrected ﬁgure
0
10
20
30
40
50
60
70
80
90
100
The position of the target in degrees
0
10
20
30
40
50
60
70
80
90
100
The estimate position of the target in degrees
Pose angle estimate
Correct values
Fig. 31. Finding the pose angle of the ﬁrst 70 targets of the T72 using the corners method
Automatic SAR Target Recognition and Pose Estimation
895

regarding the angles is proposed [12]. This is due to the fact that it is possible for one
suggested method to ﬁnd a pose angle to be at 65° while other method to ﬁnd the same
target at pose angle of 30°. This large difference of results indicates that methods do not
have the same estimation of pose angles for same targets’ poses. Hence, a third method
is used as well, and a decision-making rule based approach is used to decide which
methods should be considered for the pose estimation. Another aiding tool for esti-
mating the correct pose is to use the mirror image of the input target image around the
y-axis. In ideal situation, the mirror image of the target should hold the same pose but
in another quadrant. See Examples next.
7.1
Decision Making Rules for Pose Estimation
A decision-making approach using set of rules is used to determine which method is
going to be dominant in determining the pose angle estimation. The rules can be
divided into two categories; (1) rules to determine the role of the mirror image, (2) rules
to determine the role of the whole method.
1. Rules related to mirror image
Figure 32 shows two input target images and their mirror images around y-axis. If
we apply any of the methods to ﬁnd the pose angle, we will have the mirror target
image with 90° shift. For example, if the estimate of the pose angle at position 100° is
131°, the estimate for the mirror image would be 41°. Therefore, some rules are needed
to decide which value of the error image is going to be used.
2. Rules to determine the method
The previous discussion of geometric methods showed that a method is performing
better than others in certain intervals. Hence, rules should be used to decide which
Fig. 32. Input targets and their mirror images
896
T. Namas and M. Hodžić

method to be used. If all will be included, then they would have different weights
within a certain interval. Both the rules and weights which are described in the fol-
lowing subsections are determined by inspecting the amount of error obtained by a
certain method. For example, in Figs. 24 and 25, it is clear that the amount of error is
highest in the middle area of the range, therefore the method is given less weights in
that range.
7.2
Fine-Tuning by Weights
After determining the methods which will be considered for pose estimation the
methods are given weights based on experimental observations and the results shown
in Sects. 3–5. The weights and their corresponding angles for each method are listed in
Table 2.
Determining the geometrical pose estimate in terms of procedure is summarized as
follows:
1. The pose estimate is found for the input image and for the mirror of it by three
methods described in Sects. 3–5.
2. Rules are applied for determining the role of the mirror image and which method
from 3 to 5 should be considered.
3. Spline curve ﬁtting [15] is used determine the weights for the found poses by every
method in case that method is not excluded by step 2.
4. The ﬁnal weights are found from the spline function, and Eq. (5) is applied to
calculate the pose (angle) value.
Table 2. Weights given to each angle estimate range for different method
Angle range in degrees Weights
Nonlinear equations Curve ﬁtting Corners method
0–9
0
1
1
10–19
0.01
1
0.9
20–29
0.5
0.9
0.8
30–39
0.5
0.3
0.8
40–44
0.8
0.2
0.9
45–49
1
0
0.95
50–54
0.8
0.2
0.9
55–59
0.5
0.3
0.8
60–69
0.5
0.9
0.8
70–79
0.01
1
0.9
80–90
0
1
1
Automatic SAR Target Recognition and Pose Estimation
897

anglepose ¼ k1 ðp1Þ þ k1 ðp1mÞ þ k2 ðp2Þ þ k3 ðp2mÞ þ k4 ðp3Þ þ k5 ðp3mÞ
2k1 þ k2 þ k3 þ k4 þ k5
ð5Þ
5. The weights k1 through k5 are assigned to each type of estimation, while p1, p2 and
p3 are the estimated poses using the three different methods described in Sects. 3–5.
6. An example of the weights given to poses found by nonlinear equation method is
shown in Fig. 33. As it can be seen, more weights are given to the middle of the
interval as this method shows better results. The same is done for the remaining two
methods, and we can use the data in Table 2 as a reference.
0
10
20
30
40
50
60
70
80
90
estimated pose
-0.2
0
0.2
0.4
0.6
0.8
1
weight at the given pose
Fig. 33. Weights using spline interpolation for the estimation of poses using nonlinear equations
Fig. 34. Combined methods and ﬁnal geometric pose estimation for T72 between 0°–90° range
898
T. Namas and M. Hodžić

When applying the combination of methods with all the details in terms of decision
making rules and weights of pose angles, the results for the ﬁrst quadrant are shown in
Fig. 34 for the T72 target. Figure 35 shows the results for the quasi ideal synthetic
target. It can be seen that both results follow same trend, with slightly better estimation
in case of the synthetic target.
8
Conclusion
In this paper, we presented the geometrical pose estimation SAR targets, as the ﬁrst
pose estimation method in several estimations that will take place to reach the ﬁnal
target recognition process. This estimation aims to reduce the number of targets within
the templates for further search purposes. Foremost, three different approaches were
suggested, namely; pose estimation by solving a set of nonlinear equations, pose
estimation by line ﬁtting of the target image pixels’ intensities, and the estimation of the
pose using the corners of the target’s image. Eventually the combination of the three
methods and weighted results were used for ﬁnal pose estimation. All the estimation is
assumed to be in the ﬁrst quadrant due to symmetry in four quadrants. With a tolerance
of 10°–15° for the ﬁrst estimate, this would mean a reduction by 66% of computation
requirements. Since a 15° tolerance indicates 30° per quadrant, which is equal to 23
poses in one quadrant out of 68. That is a 34% of the total targets to be investigated.
The poses chosen in four quadrants as an output from this stage will be used as search
space for the next stage for target recognition.
Fig. 35. Combined geometric pose estimation for quasi ideal synthetic target between 0°–90°
range
Automatic SAR Target Recognition and Pose Estimation
899

References
1. Callant, J.F.: Automatic target recognition for synthetic aperture radar. R. Can. Air J. 2(3),
8–18 (2013)
2. Dudgeon, D.E., Lacoss, R.T.: An overview of automatic target recognition. Linclon Lab. J. 6
(1), 3–10 (1993)
3. Foss, C.: Jane’s Armour and Artillery 2005–2006. Janes Information Group, Coulsdon
(2005)
4. Hodzic, M., Namas, T.: Spatial Analysis of Target Signatures. In: International Conference
on Information, Communication and Automation Technologies (ICAT), 2015 XXV,
Sarajevo (2015)
5. Hodzic, M., Namas, T.: Pose estimation methodology for target identiﬁcation and tracking.
In: International Conference on Systems, Control, Signal Processing and Informatics,
Barcelona, Spain (2015)
6. Leon-Garcia, A.: Probability, Statistics, and Random Processes for Electrical Engineering.
Pearson, New Jersy (2008)
7. Merry, R.: Wavelet Theory and Applications. A Literature Study. Eindhoven University of
Technology, Eindhoven (2005)
8. MSTAR online database (US Air Force). www.sdms.afrl.af.mil (n.d.)
9. Nocedal, J., Wright, S.: Numerical Optimization. Springer, New York (2006)
10. Otsu, N.: A threshold selection method from gray-level histograms. IEEE Trans. Syst. Man
Cybern. 9(1), 62–66 (1979)
11. Page, J.: Math Open Reference. http://www.mathopenref.com/coordpolygonarea.html (n.d.)
12. Ross, T.: Fuzzy Logic With Engineering Applications. Wiley, Singapore (2010)
13. Sandia National Laboratory. http://airborneisr.sandia.gov/Evolve/ (n.d.)
14. SARmap: SARmap the earth observation gateway. http://www.sarmap.ch/ (2009)
15. Sauer, T.: Numerical Analysis. Pearson, Fairfax, USA (2012)
16. Schumaker, L.: Spline Functions: Basic Theory. Cambridge University Press, Cambridge
(2007)
17. Skolnik, M.I.: Introduction to Radar. MGraw-Hill, New York (1980)
18. Wolff, C.: Grundlagen der Radartechnik. http://www.radartutorial.eu (2016)
900
T. Namas and M. Hodžić

Automatic SAR Target Recognition and Pose
Estimation. Part 2. Statistical Methods
for Target Recognition
Migdat Hodžić(&) and Tarik Namas
FENS, EEE Department, International University of Sarajevo, Sarajevo, Bosnia
and Herzegovina
mhodzic@ius.edu.ba
Abstract. The paper presents a second part of the paper: Automatic SAR
Target Recognition and Pose Estimation, in which we analyze ATR, automatic
target recognition. The ﬁrst part deals with pose angle determination and target
data base search space reduction using a variety of geometrical methods. Both
papers use US Government MSTAR, public target data base released for aca-
demic research and development. SAR target images are separated by a small
pose angle (between 1° and 2°). They are obtained form an air surveillance
moving plane platform at a certain depression angle and at a certain distance
which all produce SAR images of 1  1 foot pixel resolution. The objects
(targets) data base consists of 100s of commercial and military vehicles, as well
as wall and building structures. We focus on three typical targets which have
symmetric geometry, each of different size and shape. In addition to these three
real targets we also generate several synthetic targets of various symmetric
shapes to serve as the ideal test cases. The target recognition analysis is based on
simple ﬁrst and second order statistics including correlation and stochastic
processes independence analysis. This analysis is done both in spatial as well as
corresponding frequency domain. The overall methodology aims at signiﬁcantly
reducing computational time which is of order of fractions of a second. The end
result is very accurate target type determination algorithm of the order of 97–
99% precision. Once the pose angle (Part 1) and ATR (Part 2) are determined,
this information can be used for target tracking. The methodologies developed
in this work can be also applied to other objects, such as facial recognition.
Other applications are in analysis and recognition of sound effects which may be
useful to police and home land security applications.
1
Introduction
Historically, radars have been used to estimate the range, directions and velocities of
targets [1]. These parameters alone are not sufﬁcient to recognize a target. The
advances in the radar imaging led to the development of the synthetic aperture radar
(SAR), and high resolution radar (HRR). SAR systems produce large amount of data
when operated in complex environments [2]. If implemented for intelligence,
surveillance and reconnaissance (ISR), such large data would require enormous human
efforts, hence, to gain the maximum amount of efﬁciency from deploying SAR imaging
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_77

in real time operations an Automatic Target Recognition (ATR) system should be used.
An ATR system classiﬁes a target without user intervention. ATR systems generally
use image processing techniques to achieve the goal of target recognition. The phase of
target detection and its discrimination within a clutter of various targets has been
investigated and solutions have been suggested [3]. However, the issue of target
recognition or classiﬁcation for real time operations is still an active problem to be
solved. This is due to low optical resolution of SAR images which results in very
complex features extraction [2]. In Part 1 of this paper we described estimating pose
angle of the target using variety of geometrically based methods. Pose estimation is one
step in ATR process and it is important for follow up target tracking and data fusion as
well. Part 2, this paper, describes the remaining step in ATR process, i.e. determining
type of the target. Both steps, pose and target type, are based on using US
Army MSTAR data base with a number of military as well as commercial targets. In
our previous papers [4] and [5], we described MSTAR Data Base in more details. This
paper is organized as follows. Section 2 discusses target spatial analysis. Section 3
describes signature wavelet analysis, Sect. 4 gives ATR implementation details and
Sect. 5 describes target identiﬁcation algorithm. The results are presented in Sect. 6
and Conclusion in Sect. 7.
2
Spatial Analysis
This section describes signatures spatial analysis and various correlations to match an
input target data to the database of the targets at the poses found by the geometrical
analysis of that input data [5]. The signatures of two targets (synthetic and T72) are
shown in Fig. 1. Both targets are at pose position 1.
0
10
20
30
40
50
60
70
Spatial distance in pixels (1 pixel = 1 ft )
0
0.1
0.2
0.3
0.4
Mean value of gray levels of
           target image
Spatial signatures T72
x-direction signature
y-direction signature
0
10
20
30
40
50
60
70
Spatial distance in pixels (1 pixel = 1 ft )
0
0.05
0.1
0.15
0.2
0.25
Mean value of gray levels of 
                target image
Spatial signatures synthatec target
x-direction signature
y-direction signature
Fig. 1. Signatures of two targets at pose position 1
902
M. Hodžić and T. Namas

Signature statistical parameters as described in [4] and [5] are found for the input
target signature and compared to database of parameters with poses found in geo-
metrical analysis (Part 1) of the ATR. Figure 2 shows the process. Various statistical
parameters are tested and the ones which are more relevant or that give better results are
chosen to be used for the classiﬁcation of targets. These include [6]:
1. Maximum value of the signature vector x
2. The location of the maximum in the signature
3. The mean of the signature
4. The median of the signatures (M)
5. The variance of the signatures (r2)
6. The standard deviation (r)
7. Sum of squares of the deviations from the means and it’s given by
8. Max/Min ratio
9. The energy of the signature, xxT
10. Skewness, a measure of the asymmetry of the data around the sample mean
11. Kurtosis, a similar measure to skewness. However, it describes the probability
distribution of the data. It is based on the fourth moment of the data
12. Harmonic mean, the reciprocal of the mean of the reciprocals of the data set.
The Table 1 shows the above statistical parameters of the indicated X and Y
signatures. Not all of these parameters are useful for the ATR, hence, some of them will
be neglected.
Fig. 2. Process of comparison of statistics of input signatures with database signatures
Automatic SAR Target Recognition and Pose Estimation
903

Table 1. Statistical parameters of two signatures
0
10
20
30
40
50
60
70
Spatial distance in pixels (1 pixel = 1 ft )
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Mean value of gray levels of target image
Spatial signatures T72
x-direction signature
y-direction signature
Statistics
x signature
y signature
Max
0.3583
0.2746
Max location
32
26
Mean
0.0928
0.0928
Median
0.0518
0.0520
Var
0.0060
0.0048
Std
0.0778
0.0694
SS
0.3809
0.3033
Max/Mean
3.8605
2.9587
Energy
0.9322
0.8546
Skewness1
1.6355
1.1622
Skewness0
1.6750
1.1902
Kurtosis1
4.7976
3.2075
Harmonic mean
0.0609
0.0603
904
M. Hodžić and T. Namas

A set of correlation coefﬁcients between the input signatures and the database
signatures are also used for the matching of targets. The four major correlation coef-
ﬁcients used are:
1. Pearson Correlation Coefﬁcient (PCC) [6].
PCC measures linear dependence between two variables and is constrained to the
interval [−1, 1]. PCC has the following properties:
Positive linear correlation is found by positive values of PCC
• The negative PCC indicate a negative linear correlation
• The value of 0 indicates linear independence
• The stronger the correlation in any direction the closer is the absolute value of PCC
to 1.
It is important to note that PCC is a measure of linear association, hence, a 0 valued
PCC does not indicate the lack of correlation, it only shows the lack of linear corre-
lation. Another version of the PCC is called the reﬂective correlation coefﬁcient,
(RCC) which is applied if the data deviated from the mean value. For the signatures
shown in [4, 5] which represents the x signatures of the T72 at position 1 and 2, we
calculate PCC and RCC. The values are found to be PCC = 0.7372, RCC = 0.8999.
2. Spearman Correlation Coefﬁcient (SCC)
SCC is calculated similarly as PCC, but it applies to ranked data. The SCC measures
the strength of monotonicity between two data sets. We calculate SCC for the same two
signatures shown in [4, 5], and it equals 0.6475. On the other hand, for the data set
shown in Fig. 4 the PCC(x,y) is 0.745 while the SCC(x,y) is 0.998, this is due to the
fact that the data is monotonically increasing, and SCC can indicate this property very
well.
0
10
20
30
40
50
60
70
Spatial distance in pixels (1 pixel = 1 ft )
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Mean value of gray level s of target image
Spatial x-signature of T72 at position1
0
10
20
30
40
50
60
70
Spatial distance in pixels (1 pixel = 1 ft )
0
0.05
0.1
0.15
0.2
0.25
Mean value of gray levels of target image
Spatial x-signature of T72 at position 2
Fig. 3. The x-projections of signatures of T72 at positions 1 and 2 for correlation calculation
Automatic SAR Target Recognition and Pose Estimation
905

3. Kendall Rank Correlation Coefﬁcient (KCC) [7]
KCC is another indicator for the ordinal association betwes en two data sets. The
difference between KCC and SCC that the ﬁrst is less affected by the distance between
the ranks. Its commonly known as Kendall tau coefﬁcient. The KCC has similar
properties as SCC in terms of interpretation:
• Identical data give correlation value of 1.
• Total disagreement between the ranks corresponds to value of −1.
• Monotonically independent variables give values close to zero.
For the purpose of comparison between various correlation results, the data set in
Fig. 4 is shown. As data x increases data y increases but not in a linear way. The KCC
of the two signatures given in Fig. 3 is 0.4554 while for the x, y paired data given in
Fig. 4 the KCC is 0.9880.
4. Brownian Distance Covariance (BDC) [8] and [9].
Finding relationship between various variables is one of the main objectives in data
analysis, in other words ﬁnding dependence among variables [10]. The previous cor-
relation measures work very well for linear or monotonically related data. However,
they become useless when the data is related in a non-linear way or is in clusters.
Another obstacle that arises in using traditional correlation measures, is the fact that
the two vectors x, and y have to be of the same length. If they are not of the same
length, an exhausting process of ﬁnding all possible pairwise correlation would be
required [10]. In [8] the authors proposed distance covariance, which measures the
degree of relationship between two vectors, while the relationship is not limited to
certain form. A detailed elaboration can be found in [9], where a distance covariance
and Brownian covariance are shown to be equivalent (Table 2).
0
1
2
3
4
5
6
x data
-12
-10
-8
-6
-4
-2
0
2
4
y data
Fig. 4. Illustration of ranks of x, y paired data for comparison of correlation values
906
M. Hodžić and T. Namas

5. Cross-correlation and Autocorrelation.
The cross-correlation is the measure of similarity between two signals, and autocor-
relation of the signal itself [11]. The cross-correlation between the two signatures in
Fig. 3 is shown in Fig. 5 while the auto correlation is shown in Fig. 6. From the
previous ﬁgures, we see that there is a similarity between the cross correlation of two
adjacent signatures, i.e. targets with small rotation angle and autocorrelations of these
signatures. That allows the usage of cross-correlation vectors for target estimating
either by using mean square error between the cross-correlation and autocorrelation or
by using some set of correlation measures between the cross-correlation and autocor-
relation of the signatures. To illustrate the last idea, we will use the random synthetic
target and quasi ideal synthetic targets and their signatures as shown in Fig. 7.
Table 2. Correlation results comparison using various correlation methods
Correlation
Signatures in Fig. 3
(x, y) in Fig. 4
Reﬂected
0.899
0.3465
Pearson
0.737
0.745
Spearman
0.648
0.998
Kendall
0.455
0.988
Brownian
0.612
0.499
-20
-15
-10
-5
0
5
10
15
20
Lags (displacement) of samples
-0.5
0
0.5
1
Magnitude of cross-correlation
Fig. 5. Cross-correlation from Fig. 3
Automatic SAR Target Recognition and Pose Estimation
907

The input target is at position 25 (pose 33). The extracted signatures of the target
are cross-correlated with targets signatures at poses 24, 25 and 26 as well as a signature
of a different type of target at pose 25. The results of the calculated cross-correlation are
compared to autocorrelations of those targets at poses 24, 25, and 26. The results are
tabulated in Table 3.
-20
-15
-10
-5
0
5
10
15
20
Lags (displacement) of samples
-0.5
0
0.5
1
Magnitude of auto-correlation
auto-correlation of signature 1
auto-correlation of signature 2
Fig. 6. Autocorrelation from Fig. 3
Fig. 7. Illustration of cross-correlation and autocorrelation in target identiﬁcation
908
M. Hodžić and T. Namas

The results of the example show that the target at pose 26 from the quasi ideal
targets is the best candidate to match the input target, and it’s clear that the target which
does not belong to the input family of targets is the least likely to be a candidate. This
particular result was expected due to the nature of pixel distribution and its difference
between random and quasi-ideal synthetic targets. In more random cases as the case of
real targets we expect that the cross correlation could be used as a parameter for our
ATR and would results in estimates close to input target.
The 2-D spatial analysis and 2-D correlation of images include three main
parameters that can be calculated from the gray scale images; and those are:
1. Image entropy
This is a parameter that is used mostly in image compression tools; it describes the
amount of information to be coded for an image when compression is applied. Gen-
erally, images with low areas of contrast have low values of entropy; hence, com-
pression is easier, while images with a lot of contrast areas have high entropy which
indicates harder compression i.e. larger ﬁle size. Reference [12] has the detailed
description. Although, at ﬁrst glance, the entropy seems to be an intuitive parameter
which can be correlated to the image, but calculation of entropy of various images
shows that it is not that simple to anticipate an entropy of an image from the image
itself. Entropies of three synthetic targets are calculated are shown in Fig. 8.
2. 2-D—Correlation
Measures the similarity between two matrices, and in our case between two images. It
ﬁnds application in checking water marks of images and also has other image pro-
cessing applications [13]. The 2-D correlations of two examples of targets are shown in
Fig. 9.
Table 3. Results of an example where the cross-correlation is used with synthetic targets
With target
0.001/MSE
of sig1
0.001/MSE
of sig2
RCC
of sig1
RCC
of sig2
Total
Best
candidate
At pose 25
0.7160
0.4498
0.9976
0.9969
3.1604
2
At pose 24
0.5056
0.5737
0.9965
0.9977
3.0736
3
At pose 26
1.2189
0.4578
0.9986
0.9970
3.6723
1
At pose 25
other target
0.0378
0.0194
0.9439
0.9056
1.9067
4
Automatic SAR Target Recognition and Pose Estimation
909

3. Difference measures
The difference measures include: Root mean square (RMS) error, total error and
Hilbert Schmidt distance. The RMS error is a 2-D parameter similar to the root mean
square error of a vector, [12]. The total error is a value that sums the total differences in
pixels’ gray values between two images, and Hilbert Schmidt distance sums the total
square differences. After calculating the three values for the input and data in Fig. 9, it
can be seen that the RMS error and the Hilbert Schmidt distance can be used as
parameters for similarity measures between images, while the total error is not a good
choice, since positive and negative errors cancel each other to produce a false positive
of certain match. Calculations of the three parameters are in [4, 5] (Table 4).
Fig. 8. Various synthetic images and their entropies
Fig. 9. 2-D correlation illustration
Table 4. RMS error, Hilbert-Schmidt distance and total error for images in Fig. 9
Input and data image 1
Input and data image 2
Root mean error
0.0167
0.023
Hilbert Schmidt distance
68.43
94.34
Total error
−49.87
−1.64
910
M. Hodžić and T. Namas

3
Frequency (Wavelet) Analysis
To overcome certain issues with Fourier transform, the multiresolution analysis was
founded in late 1980s [14]. The motivation behind multiresolution analysis is that some
features at certain resolution may not be captured hence another resolution could be
used to capture such features. One of the imaging techniques in this analysis is wavelet
transform. There is a large number of wavelets, but we are interested in Haar transform
[5] due to its simplicity. Assuming basic Haar transformation understanding, we apply
it on the SAR signatures. The two input signatures of a T72 at pose 40, and their 6
levels Haar transform are shown in Fig. 10. The discrete two-dimensional Haar
transform ﬁnds applications in image compressions, denoising and restoration.
Transform is performed on 2-D matrices to result in a same size 2-D matrix of four
blocks. One block contains the coefﬁcients that represent an approximation of the
original matrix, another block contains the coefﬁcients that are related to the horizontal
details of the matrix, the third and fourth blocks contain the coefﬁcients of the vertical
and diagonal details respectively [15]. The 2-D DWT is computed through the
implementation of two types of ﬁltering on the input data, namely: low pass and high
pass ﬁlters. Shahbahrami in [15] suggested different algorithm approaches to achieve
the 2-D DWT. The 2-D DWT provides various approximations and various details for
every input image [16]. The number of approximations and details depends on the level
of the DWT.
Results whether in terms of approximations or in terms of details could be inves-
tigated to be used for feature extraction in form of vectors or matrices. These vectors
and matrices are then tested for matching conditions between input targets and data
base of targets. To illustrate that possibility, Fig. 11 shows an input target image and its
2 level 2-D DWT.
The details and the approximation of the two-level 2-D DWT can be treated in two
different ways; one way is to form signature vectors similar to the input signature
vectors from the approximation part, that is shown in Fig. 12. This results in shorter
vectors that are expected to be similar to original signatures of the target. The second
way which is not a novelty, it was discussed in previous works, to convert the details
matrices into vectors of lengths which will depend on the level of decomposition. In the
case of 2 level decomposition and input images of 64 pixels by 64 pixels the details
vectors would be of length 256 pixels as shown in Fig. 13.
Automatic SAR Target Recognition and Pose Estimation
911

0
10
20
30
40
50
60
70
0
0.1
0.2
x-projection input signature and it's 6 levels 
                   1D haar transforms
0
10
20
30
40
50
60
70
-0.5
0
0.5
0
10
20
30
40
50
60
70
-0.5
0
0.5
0
10
20
30
40
50
60
70
-0.5
0
0.5
0
10
20
30
40
50
60
70
-0.5
0
0.5
0
10
20
30
60
70
-0.5
0
0.5
0
10
20
30
40
50
40
50
60
70
-1
0
1
0
10
20
30
60
70
0
0.2
0.4
y-projection input signature and it's 6 levels 
               1D haar transforms
0
10
20
30
60
70
-0.5
0
0.5
0
10
20
30
60
70
-0.5
0
0.5
0
10
20
30
60
70
-0.5
0
0.5
0
10
20
30
60
70
-0.5
0
0.5
0
10
20
30
60
70
-0.5
0
0.5
0
10
20
30
40
50
40
50
40
50
40
50
40
50
40
50
40
50
60
70
-1
0
1
Fig. 10. 2 input signatures and their 6 levels 1-D Haar transform
Fig. 11. 2-level 2-D Haar decomposition of an input target
-40
-30
-20
-10
0
10
20
30
40
Column location centered at 0
0
0.1
0.2
0.3
Gray level values
x-projection signature
Original signature
Signature using 2D DWT
-40
-30
-20
-10
0
10
20
30
40
Row location centered at 0
0
0.1
0.2
0.3
0.4
Gray level values
y-projection signature
Original signature
Signature using 2D DWT
Fig. 12. New signatures from the DWT approximation
912
M. Hodžić and T. Namas

These various vectors can be used in terms of correlation or mean square error to
measure their match to other targets. Selecting the appropriate parameters for com-
paring the input data with the database is an essential part in the process of ATR
design. We present three types of parameters used for the purpose of target estimation
in the spatial domain, the same parameters are used in frequency (Haar) domain as
well. The parameters are categorized as follows: (i) Statistical parameter which are
calculated from the 1-D signatures, (ii) Correlation and Autocorrelation parameters
which are calculated from the 1-D signatures, and
(iii) Two dimensional parameters calculated from the images pixels. Applying 1
and 2 dimensional Haar transform produces vectors and matrices, where the previously
described parameters can be repeated for the purpose of matching in frequency domain.
As real-time application is one of the goals here, it is crucial to have parameters that are
easy to calculate and compare as well as programming requirements in any platforms.
4
ATR Implementation
In this section, we discuss the implementation of the geometric target analysis and the
various parameters described to achieve the goal of ATR. We list the results obtained
by our proposed ATR on different combinations of synthetic data and MSTAR data-
base. The targets which are chosen for testing the suggested algorithm are: The T72
tank, the D7 CAT bulldozer, and the Zil truck. They are chosen from the database of
15° depression. These targets are used as reference, while the T72 tank of a different
version at 17° depression is used as an input (test). However, the difference in
depression angle as well as the version of the tank introduces more randomness and
Fig. 13. The details of the 2-D DWT two-level converted to vectors
Automatic SAR Target Recognition and Pose Estimation
913

probability of error. A sample of the input target and database targets are shown in
Fig. 14. T72 at pose 100 is chosen as an example, and the ﬁrst step is to extract the
signatures from the input target. Then the geometric analysis is applied. The results are
described below. The function dimXY we implemented in MATLAB takes the input
image and returns the X and Y projections, which is in this case was 35 pixels and 23
pixels (Fig. 15).
Next step is to estimate the possible poses of the target by assuming it is in the ﬁrst
quadrant at the beginning, then using symmetric property for other poses for the
remaining three quadrants. This is achieved by the function sigPos. The function uses
nested functions discussed in Part 1 paper for pose estimation. The results of the
previous step are summarized in the Table 5.
Fig. 14. Sample of input target and database targets
0
10
20
30
40
50
60
70
Column locations
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
Mean of gray level values
           column wise
x-projection signature
0
10
20
30
40
50
60
70
Rows locations
0
0.05
0.1
0.15
0.2
Mean of gray level values
            row wise
y-projection signature
0
10
20
30
40
50
60
70
Column locations
0
0.05
0.1
0.15
Mean of gray level values 
         column wise
x-axis projection using methods in 4.3.1 and 4.3.2
0
10
20
30
40
50
60
70
Rows locations
0
0.05
0.1
0.15
0.2
Mean of gray level values
              row wise
y-axis projection using methods in 4.3.1 and 4.3.2
Fig. 15. Signatures of input target, the x and y projections using methods from geometric
analysis of the target (part 1 of this work)
914
M. Hodžić and T. Namas

We should note that the true value of the pose of the input target is
100 ð1:314Þ ﬃ131. This angle is 180  131 ¼ 49 in 1st quadrant which is
equivalent to pose 37, while the ﬁrst estimate of the angle is 55 and that is equivalent
to pose 42. That gives a pose error of 5 poses within the ﬁrst quadrant.
To have a visual perspective of the process, Fig. 16 shows the input target, its
assumed version in ﬁrst quadrant, and the ﬁrst geometric estimation based on the angle
found for the three targets within the search space. The sigPos function gives a list of
possible poses of the input target in all four quadrants by adding number of poses to the
right and left of the ﬁrst geometric estimate and use symmetry to assume the same
poses in all quadrants. The number of added targets to the left and right of initial
estimate, depends on how accurate the geometric estimation is. In this particular
example, we added nine poses to the left and nine to the right in each quadrant. This
makes a total of 76 possibilities for each target to be checked rather than 274 the
number of original data base (Table 6).
Table 5. Summary of results of example 1 for angle estimation
Method of pose estimate
Angle of image
Angle of mirror image
Area
64
64
Curve ﬁtting
65
20
Corners
52
49
Final result after fuzzy rules and weights
55
Fig. 16. Geometric estimation—process visualization
Automatic SAR Target Recognition and Pose Estimation
915

This is an excellent amount of reduction in calculations. Next step is to apply
spatial statistics and correlation tests between the input target and the targets of the
database at the 76 poses highlighted.
Applying spatial statistics code to the input and the 76 poses from sigPos produces
large amount of results. Tabulating them would occupy large space of this paper,
hence, the total results for the ﬁve parameters at the possible 76 poses are shown in the
following ﬁgures. From these the maximum of summation of correlation in Fig. 17, the
minimum of summation of differences from Fig. 18, the minimum of the summation of
mean square errors in Fig. 19, the minimum of the summation of the Hilbert Schmidt
distance in Fig. 20, the maximum of the summation of the 2-D correlation in Fig. 21,
and ﬁnally the maximum of the correlation of the cross-correlation and autocorrelation
of the signatures from Fig. 22. All give a certain tag to refer to target type, and the
respected tags for targets are shown in Table 1. The target type with most number of
tags is target type 1 from the list; hence, the ﬁrst part where other targets are eliminated
is achieved.
Table 6. All poses in 4 quadrants and the list of possible poses for T72 at pose 100
Q1
Q2
Q3
Q4
1
25
49
69
93
117
138
162
186
206
230
254
2
26
50
70
94
118
139
163
187
207
231
255
3
27
51
71
95
119
140
164
188
208
232
256
4
28
52
72
96
120
141
165
189
209
233
257
5
29
53
73
97
121
142
166
190
210
234
258
6
30
54
74
98
122
143
167
191
211
235
259
7
31
55
75
99
123
144
168
192
212
236
260
8
32
56
76
100
124
145
169
193
213
237
261
9
33
57
77
101
125
146
170
194
214
238
262
10
34
58
78
102
126
147
171
195
215
239
263
11
35
59
79
103
127
148
172
196
216
240
264
12
36
60
80
104
128
149
173
197
217
241
265
13
37
61
81
105
129
150
174
198
218
242
266
14
38
62
82
106
130
151
175
199
219
243
267
15
39
63
83
107
131
152
176
200
220
244
268
16
40
64
84
108
132
153
177
201
221
245
269
17
41
65
85
109
133
154
178
202
222
246
270
18
42
66
86
110
134
155
179
203
223
247
271
19
43
67
87
111
135
156
180
204
224
248
272
20
44
68
88
112
136
157
181
205
225
249
273
21
45
89
113
137
158
182
226
250
274
22
46
90
114
159
183
227
251
23
47
91
115
160
184
228
252
24
48
92
116
161
185
229
253
916
M. Hodžić and T. Namas

0
50
100
150
200
250
Possible pose
6
6.5
7
7.5
8
8.5
Correlation valuses summation
With target 1
With target 2
With target 3
Fig. 17. Summation of correlation values of the signatures (input and possible poses)
0
50
100
150
200
250
Possible pose
0
5
10
15
20
25
30
35
Statistics differences summation
With target 1
With target 2
With target 3
Fig. 18. Summation of statistics differences of the signatures (input and possible poses)
0
50
100
150
200
250
Possible pose
0
1
2
3
4
5
6
7
8
9
Summation of mean square error
10 -3
With target 1
With target 2
With target 3
Fig. 19. Mean square error between input signatures and database signatures at possible poses
Automatic SAR Target Recognition and Pose Estimation
917

0
50
100
150
200
250
Possible pose
120
140
160
180
200
220
240
260
280
300
Hilbert Schmidit Distance
With target 1
With target 2
With target 3
Fig. 20. Hilbert Schmidt distance difference between input and targets at possible poses
0
50
100
150
200
250
Possible pose
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
2D correlation between images
With target 1
With target 2
With target 3
Fig. 21. 2-D correlation between input target and targets at possible poses
0
50
100
150
200
250
Possible pose
1.6
1.65
1.7
1.75
1.8
1.85
1.9
1.95
2
cross correlation of signatures
With target 1
With target 2
With target 3
Fig. 22. Correlation of cross-correlation of input signature with signatures at possible poses
918
M. Hodžić and T. Namas

Form various analysis of targets it was clear that parameters 1 and 4 from Table 7
are the best indicators and mostly conformal with ﬁnal target decision, so a special
treatment of those parameters would make sense, and it would be discussed later when
the general results are presented. Next is to estimate the pose of the target and to
achieve this goal, a second set of tests are performed, with the exclusion of the other
target types (seer Part 1 paper using spatial and Haar signatures). Figure 27 shows an
image and its trimmed version. The results for the T72 target at pose 100 are in Fig. 23.
From Fig. 23, we see that the maximum correlation summation for both spatial and
frequency signatures takes place at pose 96. The distance tests on the other hand
include the Hilbert Schmidt distance and image entropy difference for both input image
and its trimmed version around the target. Those tests and their results as shown in
Fig. 24 suggest a target pose at 230. In distance measures, we are interested in the
minimum distance. We can see that distance measurements give a minimum distance
summation at pose 230. It can be noted that the differences using Hilbert Schmidt
distance is in orders of tens and in some cases, it can exceed 100 units of difference
(pixel gray level value squared). While the entropy difference values are in the orders of
ones. Hence comparing them directly does not reﬂect the true weight of each, so, either
amplifying the effect of entropy or attenuating the effect of distance should be con-
sidered. Alternative is to have them separated as different measures.
Table 7. Summary of tags for target elimination for the T72 at pose 100 example
Parameter
Target tag
1. Correlation
3
2. Statistics differences
1
3. Mean square error
2
4. Hilbert Schmidt distance
1
5. 2-D correlation
1
6. Cross correlation
1
0
50
100
150
200
250
Possible poses
0
5
10
15
20
25
30
35
Summation of correlation values
Corr of sigs.
Corr of trim. sigs.
Sum of corr of Haar
Summation
Fig. 23. The differences using Hilbert Schmidt distance at possible pose angles
Automatic SAR Target Recognition and Pose Estimation
919

The two results that we found are position 96 and position 230. Although we are
trying to achieve pose estimation of high accuracy, the reality is that difference in
depression angles of 17° and 15° between input target and database images will always
results in some shift. We will assume 5 poses as tolerance would be an acceptable
tolerance since 5 poses are equivalent to less than 7° out of 360°, which is less than 2%
of error in pose estimation. The input target and the two candidate results are shown in
Fig. 25.
A look at the estimated poses shows the effect of symmetry on target images, where
targets that are separated by 180° can be detected for a single pose, hence, a further step
is needed to determine which pose is the most likely to be the best ﬁt to the input’s pose.
Several ideas can be implemented for this last step. The simplest one is to choose a
number of poses (4 in this example) around the two distinct estimates and correlate the
input target signatures with signatures of these poses and look for the highest summa-
tion. This is the method that was applied here, and it was found that the ﬁnal pose is at
position 96, with an error of 4 poses, which corresponds to 1.5% error or 98.5%
accuracy. Another idea that can be used as well, i.e. to compare binary form of the input
image with the binary form of poses around the pose estimate as shown in Fig. 26. From
the 2-D correlations, although there is a very small difference of correlation sum
0
50
100
150
200
250
Possible poses
10
20
30
40
50
60
70
80
90
Distance values
2D Dis.
2D Trim. Dis.
Summation
Fig. 24. The differences using Hilbert Schmidt distance at possible pose angles
Fig. 25. Comparison between poses found after pose estimation phase
920
M. Hodžić and T. Namas

between the upper images and lower ones, 3.833 versus 3.8065, still the ﬁrst group is
better, hence the target is chosen to be at pose 96. A third option is to apply a 2-D Haar
analysis and use the results of the approximations or diagonal details or both to compare
the input image to database images around the candidate poses. In this example using
three level Haar 2-D decomposition and approximations and comparing them after
reshaping their matrix form to vector form we calculated summation of reﬂation cor-
relation coefﬁcient of 2.6298 for pose 96 and 2.6291 for pose 230.
5
Target Type Identiﬁcation
For implementing the algorithm and testing it on all input positions of the T72 tank,
three different approaches can be tested and that is mainly for comparison reasons. The
ﬁrst one is by using the geometric target analysis. The advantage of this approach as
discussed before is to reduce the amount and time of computation processes, as it tests
for an input image up to 120 positions (30 positions in every quadrant). The drawback
of this approach is the possibility that a target position might not be detected at ﬁrst
estimate, which could cause large drift in pose estimate if not a complete miss iden-
tiﬁcation of the target. The tests can be performed based on various possibilities, the
ﬁrst one is input target image size, so the algorithm can be applied on an image chip of
64 by 64 pixels or on smaller chips that are the same the size of a trimmed version of
the input image, or a binary version of the image and its trimmed version. On the other
hand, a second group of possibilities and their combinations can be used as criteria for
correlation and statistics differences, for example, the maximum of the summation of
correlation coefﬁcients at possible poses, or the highest number of correlation coefﬁ-
cients at possible poses which are above the mean of all correlation coefﬁcients, and
ﬁnally the maximum of correlation coefﬁcients. Figure 27 shows an illustration of
those possibilities.
Fig. 26. Binary images around the ﬁrst two estimations
Automatic SAR Target Recognition and Pose Estimation
921

The results of applying ATR algorithm for target type detection with the geo-
metrical analysis for the 274 samples of T72 test group is shown in Fig. 28, as can be
seen 8 positions are missed out of 274 and classiﬁed mistakenly to be the either ZIL
131 or CAT bulldozer.
The average time required for identifying one target without code optimization,
using MATLAB is about 0.5 s. This time can be reduced with code optimization and if
it was to be implemented in real time system another programming environment could
be used to reduce the time even more. When applying the suggested ATR without
geometrical analysis, rather, to compare the input target with the whole databases, the
amount of T72 target’s recognized is reduced but with an acceptable general result.
Figure 29 shows that 18 poses were missed in terms of target type. The average time
required for identifying one target without code optimization and without geometrical
analysis is about 3 s. The third approach is to sample out the database and compare the
Fig. 27. Illustration of possible tests options data inputs
0
50
100
150
200
250
Position of target
0
0.5
1
1.5
2
2.5
3
Target type
1 - T72, 2 - Zil131, 3 - CAT bolldozer
Fig. 28. Target type identiﬁcation for T72 with geometrical analysis
922
M. Hodžić and T. Namas

samples of the database with the input target, the results showed similar outcome as the
whole database in terms of target identiﬁcation, as shown in Fig. 30, but in terms of
processing time it took an average of 0.7 s for target identiﬁcation.
To summarize the results for the target type identiﬁcation the following setups were
implemented for testing, and the confusion matrices for all the setups are shown below
for (i) Using geometrical analysis, (ii) Using samples of reference data, and (iii) Using
entire reference data, as applied to:
1. Synthetic targets (same, and different input and reference data)
2. The input target is chosen from within the reference target database (as in 1 above)
3. The input target is chosen from a different database than the reference target
database (Tables 8, 9, 10, 11, 12, 13, and 14).
0
50
100
150
200
250
Position of target
0
0.5
1
1.5
2
2.5
3
Target type
1 - T72, 2 - Zil131, 3 - CAT bolldozer
Fig. 29. Target type identiﬁcation for T72 without geometrical analysis
0
50
100
150
200
250
Position of target
0
0.5
1
1.5
2
2.5
3
Target type
1 - T72, 2 - Zil131, 3 - CAT bolldozer
Fig. 30. Target type identiﬁcation for T72 using sampled database
Automatic SAR Target Recognition and Pose Estimation
923

Table 8. Results of same synthetic targets (IR: Identiﬁcation Rate)
Geometrical
analysis
IR (%) Sampled data
IR (%) All data
IR (%)
T1
T2
T3
T1
T2
T3
T1
T2
T3
T1 274
0
0 100
274
0
0 100
274
0
0 100
T2
0 274
0 100
0 263
11 95.9
0 274
0 100
T3
0
0 274 100
0
0 274 100
0
0 274 100
Table 9. Results of different synthetic targets (IR: Identiﬁcation Rate)
Geometrical
analysis
IR (%) Sampled data
IR (%) All data
IR (%)
T1
T2
T3
T1
T2
T3
T1
T2
T3
T1 274
0
0 100
274
0
0 100
274
0
0 100
T2
0 264
10
96
0 263
11 95.9
0 263
7 97.4
T3
0
0 274 100
0
0 274 100
0
0 274 100
Table 10. Same MSTAR targets
Geometrical
analysis
Rate
(%)
Sampled data
Rate
(%)
All data
Rate
(%)
T72_64 CAT Zil
131
T72_64 CAT Zil
131
T72_64 CAT Zil
131
T72_64 274
0
0 100
274
0
0 100
274
0
0 100
CAT
0
274
0 100
7
267
0
97
0
274
0 100
Zil131
0
0
274 100
44
15
215
78
0
0
274 100
Table 11. Different T72 input example 1
Geometrical analysis
Rate
Sampled data
Rate
All data
Rate
T72_32
T72_32
T72_32
T72_64
266
97%
256
93%
256
93%
CAT
6
17
17
Zil131
2
1
1
Table 12. Different T72 input example 2
Geometrical analysis
Rate
Sampled data
Rate
All data
Rate
T72_07
T72_07
T72_07
T72_64
270
99%
273
99.6%
269
98%
CAT
0
0
0
Zil131
4
1
5
924
M. Hodžić and T. Namas

In terms of pose determination, we can look at the targets and references from
various perspectives. We investigated three options within the geometrical analysis
1. The input target and reference are exactly from the same database.
In case all the database is used as a reference, while the input image from the same
database, then we expect a 100% correct pose estimation, this result is trivial since all
poses are included in the analysis, however, it is prone to some error if geometrical
analysis is implemented even if the same database is used for input and reference, that
is due to the fact that some poses can be initially missed. Figure 31 shows the results of
pose estimation for both mentioned cases with the T72—A64 version.
Table 13. Different T72 input example 3
Geometrical analysis
Rate
Sampled data
Rate
All data
Rate
T72_07
T72_07
T72_07
T72_32
267
97%
266
97%
264
96%
CAT
4
3
5
Zil131
3
5
5
Table 14. Different T72 input example 4
Geometrical analysis
Rate
Sampled data
Rate
All data
Rate
T72_64
T72_64
T72_64
T72_32
262
96%
266
97%
268.00
98%
CAT
10
6
4
Zil131
2
2
2
0
20
40
60
80
100
120
140
Pose
0
20
40
60
80
100
120
140
Pose estimation
Pose estimate: input and refrence from same 
                         DB without GA
0
20
40
60
80
100
120
140
Pose
-1
-0.5
0
0.5
1
Error
Pose estimate: error
0
20
40
60
80
100
120
140
Pose
0
20
40
60
80
100
120
140
Pose estimation
Pose estimate: input and refrence from same
                         DB with GA
0
20
40
60
80
100
120
140
Pose
-1
-0.5
0
0.5
1
1.5
2
Error
Pose estimate: error
Fig. 31. Pose estimation results same database for input and reference
Automatic SAR Target Recognition and Pose Estimation
925

2. The input target and reference are from the same database with some noise
introduced.
This situation would represent a fairer comparison platform, since the input target in
real life is always the same, the conditions of the measurement would change due to
weather or other circumstances, so some pixels’ values would change, not the actual
shape of the target. To simulate the situation, 2 random numbers were introduced for
each pixel within the image, and a threshold of 0.33 is used to introduce a change in a
pixel. With 0.5% threshold of for adding or removing 0.33 to the pixel. The threshold
values are chosen on a tentative basis, they can be chosen on some experimental or
simulated data which is not the scope of this work. Figure 32 shows input target image
and it is randomly modiﬁed version.
By applying pose estimation on a modiﬁed image within the same database ref-
erence and with geometrical analysis included we get very good results, with minimum
96% of correct pose estimation, while the remaining 4% or less, can have a bad pose
estimation of up to 15% and we noticed that this error takes place mainly near the
vertical positions. This result makes sense since pixels’ change on vertical positions
means a lot in terms of rough estimate at the beginning (Fig. 33).
Fig. 32. Original and modiﬁed images for pose estimation
0
50
100
150
Pose
0
20
40
60
80
100
120
140
Pose estimation
Pose estimate: modified input with GA
0
50
100
150
Pose
-2
-1
0
1
2
3
4
Error
Pose estimate: error
Fig. 33. Example of pose estimation with modiﬁed input images (noise introduced)
926
M. Hodžić and T. Namas

3. The input image and reference data are of same type, but not same database.
This option is an outcome of data availability, since there is only one set of available
targets at same depression and pose angles, other sets of same target type can be used
for reference. The same sets where used for target identiﬁcation. The results of pose
estimation are shown in the following three ﬁgures with the assumption that identi-
ﬁcation is achieved. A mean error of pose 2 and 4 posses, with spikes of maximum
errors in the range of 10 or 14 poses. This is achieved within the 180° ambiguity,
which our results showed that it cannot be solved without considering the 3-D model
of the data. So, this ambiguity is left for a further type of research in the future
(Figs. 34, 35, and 36).
0
20
40
60
80
100
120
140
Pose
0
20
40
60
80
100
120
140
Pose estimation
Pose estimate: two different target sets
Estimation
Real Pose
0
20
40
60
80
100
120
Pose
-15
-10
-5
0
5
10
15
Error
Pose estimate: error
Fig. 34. T72_64 input, T72_32 as reference (max error 12, mean error 3.75)
0
20
40
60
80
100
120
140
Pose
0
20
40
60
80
100
120
140
Pose estimation
Pose estimate: two different target sets
Estimation
Real Pose
0
20
40
60
80
100
120
Pose
-15
-10
-5
0
5
10
15
Error
Pose estimate: error
Fig. 35. T72_32 input, T72_64 as reference (max error 14, mean error 3,93)
Pose
00
20
20
40
40
60
60
80
80
100
100
120
140
120
140
Pose
0
20
40
60
80
100
120
Pose estimation
Pose estimate: two different target sets
Estimation
Real Pose
-15
-10
-5
0
5
10
15
Error
Pose estimate: error
Fig. 36. T72_07 input, T72_64 as reference (max error 12, mean error 2.8)
Automatic SAR Target Recognition and Pose Estimation
927

6
Results and Analysis
In this work, three target databases were chosen for determining an input target type
within SAR images, due to the nature and aims of the work, no separation of dataset
was made as test and training or input and reference. The input or test targets came
from a similar target type but different database. This is because the algorithm should
ﬁnd both target type and target position. If some targets are chosen for tests, that means
their position would not be found correctly. Yet, for generality and consistency in the
testing experiments the case where the target comes from the same database was done
and reported in two different cases; without noise introduced and with noise introduced.
The suggested ATR algorithm showed good identiﬁcation results, as well as speed
requirements for possible real-time implementation. The tests were performed using
MATLAB 2016a, on an Intel(R) Core™i7-4510U CPU @ 2.0 GHz processor with
8 GB RAM, but with redundant code, which implies the time results represent worst
case scenario for the given setup. Results are summarized as follows (Table 15).
In terms of pose estimation of the targets, the worst case we recorded in cases of
different target input and reference database with 89% or 11% off pose, for one half of
the circle. The effect of symmetry was not discussed in terms of pose estimation, since
it requires a different set of test tools (Table 16).
We also compare our results with other reported in the literature. Traditional
template SAR recognition was developed by the MIT Lincoln laboratory with ATR in
mind. Pose estimate and execution time were not often considered. In this work we tie
the three parameters. Table 17 lists some of the results reported in the literature for
both, template based and non-template based recognition. Note that different datasets
by different researches where used, but we are reporting the part where T72 SAR
images were used.
Table 15. Processing time for target type recognition
Data included
Average time per target (s)
All data
3.5
Sampled data
0.75
Geometrical analysis
0.5
Table 16. Pose estimate worst cases and means of off poses
Input target
Reference target
Worst case (off by) (%)
Mean (off by) (%)
T72_32
T72_64
11
4
T72_07
T72_64
9
2
T72_64
T72_32
11
3
928
M. Hodžić and T. Namas

7
Conclusion
In this paper, the results of applying the suggested ATR algorithm on MSTAR data that
include T72 tanks, Zil truck and D7 Caterpillar bulldozer were presented. An overall
detection average of 96.65% is achieved, as well as pose information with worst case
being 89% accurate within time average of l second. The results outperform large
amount of the previous reported results in literature and among the very few that
consider performance time and emphasis pose detection as well. Processing time
improvements are expected when using dedicated DSP application and hardware which
gives better results for real-time deployment. The template based ATR beneﬁts from
the great improvements in memory capacitates and CPU computation speed which
were main limiters of such algorithms before. The template which is based on
extracting two 1-D vectors from each target image data is not a model template, rather
it used other targets as a reference, which suggests a future research topic of building a
general template of vectors as a reference for multiple targets. Future research may
involve other datasets not limited to military, rather, civilian objects and vehicles.
References
1. Skolnik, M.I.: Introduction to Radar. MGraw-Hill, Boston (1980)
2. Callant, F.: Automatic target recognition for synthetic aperture radar. R. Can. Air J. 2(3),
8–18 (2013)
3. Dudgeon, D.E., Lacoss, R.T.: An overview of automatic target recognition. Linclon Lab. J. 6
(1), 3–10 (1993)
Table 17. Comparison of results with other template and non-template algorithms
Reference
T72
recognition
rate
Pose estimation
No. of
targets
2015, Hu and Li, [17]
98.97%
NA
3
In 2011, M. Liu, Y. Wu
and Q. Zhao [18]
94.8%
NA
3
2013, Kaiqi, Wenguang
and Zuowei [19]
93.72%
NA
3
1999, Novak, Owirka and
Weaver [20]
96.4%
95% (with 180° ambiguity)
10
2001, Liao et al. [21]
(Using HRR)
92.1%
NA
10
2008, Gomes, Brancalion
and Fernandes [22]
94% (T62
not T72)
NA
5
2013, Shaw, Paul and
Williams [23]
94%
A priori pose used with
assumption target is being tracked
4
In this paper
96.65%
89% (with 180° ambiguity)
03
Automatic SAR Target Recognition and Pose Estimation
929

4. Hodzic, M.I., Namas, T.: Spatial analysis of target signatures. In: International Conference
on Information, Communication and Automation Technologies (ICAT), 2015 XXV,
Sarajevo (2015)
5. Hodzic, M., Namas, T.: Pose estimation methodology for target identiﬁcation and tracking.
In: International Conference on Systems, Control, Signal Processing and Informatics,
Barcelona, Spain (2015)
6. Moore, D.: The Basic Practice of Statistics. W.H. Freeman and Company, New York (2010)
7. Abdi, H.: The Kendall rank correlation coefﬁcient. In: Encyclopedia of Measurement and
Statistics. SAGE Publications, Thousand Oaks, USA (2007)
8. Szekely, G., Rizzo, M., Bakirov, N.: Measuring and testing dependence by correlation of
distance. Ann. Statist. 35, 2769–2794 (2007)
9. Szekely, G., Rizzo, M.: Brownian distance covariance. Ann. Appl. Statist. 3, 1236–1265
(2009)
10. Cowley, B., Vinci, G.: Summary and discussion of: Brownian distance covariance. Statist.
J. Club 36, 1–23 (2014)
11. Mitra, S.: Digital Signal Processing: A Computer-Based Approach. McGraw-Hill, Colum-
bus, USA (2010)
12. Gonzalez, R., Woods, R.: Digital Image Processing. Pearson, New Jersey (2007)
13. Gonzalez, R., Woods, R., Eddins, S.: Digital Image Processing Using MATLAB. Pearson,
New Jersy (2009)
14. Mallat, S.: A theory for multiresolution signal decomposition: the wavelet representation.
IEEE Trans. Pattern Anal. Mach. Intell. 11(7), 674–693 (1989)
15. Shahbahrami, A.: Algorithms and architectures for 2D discrete wavelet transform.
J. Supercomput. 62(2), 1045–1064 (2012)
16. Instruments, N.: Advanced Signal Processing Toolkit Manual. National Instruments
Corporation, Austin, USA (2014)
17. Yifeng, H., Bin, L.: Automatic SAR target recognition based on two-dimensional locality
preserved maximum information projection. In: 8th International Symposium on Compu-
tational Intelligence and Design, Hangzhou, China (2015)
18. Ming, L., Yan, W., Quan, Z., Lu, G.: SAR target conﬁguration recognition using locality
preserving projections. In: Proceedings of 2011 EEE CIE International Conference on Radar
(2011)
19. Liu, K., Wang, W., Sun, Z.: Recognition of SAR image based on combined templates. In:
IEEE International Conference on Imaging Systems and Techniques (IST), Beijing, China
(2013)
20. Novak, L.M., Owirka, J.G., Weaver, L.A.: Automatic target recognition using enhanced
resolution SAR data. IEEE Trans. Aerosp. Electron. Syst. 35(1), 157–175 (1999)
21. Liao, X., Runkle, P., Jiao, Y., Carin, L.: Identiﬁcation of ground targets from sequential
HRR radar signatures. In: IEEE International Conference on Acoustics, Speech, and Signal
Processing, Salt Lake City, USA (2001)
22. Gomes, P.J.P., Brancalion, F.B.J., Fernandes, D.: Automatic target recognition in synthetic
aperture radar image using multiresolution analysis and classiﬁers combination. In: IEEE
Radar Conference, Rome, Italy (2008)
23. Shaw, A.K., Anindya, P., Williams, R.: Eigen-template-based HRR-ATR with multi-look
and time-recursion. IEEE Trans. Aerospace Electron. Syst. 49(4), 2369–2385 (2013)
930
M. Hodžić and T. Namas

Part VII
BEM/MRM Formulation for
Engineering Applications

Numerical Analysis of Screw Compressor
Rotor and Casing Deformations
Ermin Husak1(&), Ahmed Kovacevic2, and Sham Rane2
1 Technical Faculty, University of Bihac, Bihac, Bosnia and Herzegovina
erminhusak@yahoo.com
2 School of Mathematics, Computer Science and Engineering, Centre for
Compressor Technology, City, University of London, London, UK
A.Kovacevic@city.ac.uk
Abstract. Performance and reliability of screw compressors is highly depen-
dent on their operational clearances. Compressor structural parts including rotors
and the casing are affected both by pressure and temperature of the working ﬂuid
to which they are exposed. The standard approach when simulating performance
is to neglect these deformations and assume rigid compressor elements. In this
paper a numerical solution which combines the solution of ﬂuid ﬁeld from
Computational Fluid Dynamics (CFD) and Finite Element Analysis (FEA) of
solid elements is used to calculate deformations of the compressor elements. The
temperature ﬁeld obtained from CFD is extracted and applied to the surface of
the solid parts where it was averaged in time and served as boundary conditions
for solid body calculations. The FEM analysis performed in ANSYS showed
encouraging results which can be used for analysis of changes in compressor
clearances.
Keywords: Computational ﬂuid dynamics (CFD)  Finite element analysis
(FEA)  Screw compressor  Deformations
1
Introduction
Screw compressor are positive displacement machines that comprises pair of helically
geared rotors contained in a casing as shown in Fig. 1. Relative motion of rotors and
casing causes change in the volume of the compressor working chamber which
increases the pressure and causes the compression process [1, 2]. Clearances must exist
between rotating and stationary parts allow relative motion between rotors and casing.
That in turn provides leakage gaps between rotating and stationary elements. The
compressed ﬂuid leaks through these leakage gaps and inﬂuences the efﬁciency of a
screw compressor. The size of clearances is affected by the deformation of structural
elements caused by pressure and temperature. If the compressor rotors deform due to
the increase in temperature, there is a risk of contact between the rotating and stationary
elements and therefore risk of damage or complete failure of a compressor. In order to
avoid that contact, designers increase compressor clearances. This however causes
higher leakage losses and in turn increase in the working chamber temperature which in
turn further deforms the rotors [3]. Therefore it is desired to minimize the clearance.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_78

Due the fast development of manufacturing technologies in the past several decades it
is now possible to manufacture screw compressor parts with high precision. Screw
compressor rotors can now be produced at an economic cost with tolerances as small as
5 lm while casing bores can be manufactured with repeatability of 2 lm [4]. This gave
possibility to manufacture screw compressor with low level clearances and avoid
previously mentioned malfunctions.
Despite the advanced manufacturing capabilities give a deterministic framework to
the design process of screw compressors the thermodynamic process during screw
compressor operation signiﬁcantly affects changes in clearances. The increase in
pressure and temperature will cause screw compressor parts to deform. Which of these
two parameter will inﬂuence more on deformation depends mostly on the compressor
type, i.e. is it oil free or oil injected [4]. Oil free screw compressors are mostly designed
for low pressures of up to 3 bar but due to lack of cooling of the compressed gas, they
usually have high discharge temperatures. In this case pressure loads are much less
signiﬁcant than the temperature loads.
Temperature loads in oil free screw compressors cause signiﬁcant size and shape
deformations which cause to clearance level changes. Typically, the interlobe and
radial clearances on the discharges side of the compressor will reduce. To determine
values of clearances under the working conditions especially under temperature
changes, several steps have to be performed. Typically, the ﬁrst step is to perform CFD
calculations which will provide temperature distribution on the boundary of rotors and
casing. Second step is thermal analysis which gives temperature load distribution in
rotors and casing. Third step is structural analysis which gives deformations of rotors
and casing. In order to perform such a process, due to complexity of screw compressor
Male rotor
Female rotor
Casing
Suction side
Discharge
Fig. 1. Design of twin screw compressor
934
E. Husak et al.

geometry simpliﬁcation have to be taken into account. Different authors suggest dif-
ferent approach to solve this problem.
Sauls et al., 2006 proposed to separately calculate temperature ﬁeld in the com-
pressed gas and FEA analysis [5–7]. This process requires serious effort and time to
perform calculation and transfer results in different software which is measured in
months and is not practical for industrial use. Kovacevic et al., 2002 proposed use of
Computational Continuum Mechanics (CCM) in which ﬁnite volume method is used to
calculate ﬂuid ﬂow and solid structure, all in the same solver [4]. The results showed
excellent results. The process is suitable for research but still requires signiﬁcant effort
to be implemented in industry. One dimensional analytical approach for local defor-
mations proposed by Buckney et al., 2014 is applicable in industry as it is fast but takes
into account some assumptions which cannot be always generally accepted [3].
It is important to emphasize that during thermodynamic process in screw com-
pressor heat will transfer not just to the rotors and casing but to some other parts of
compressor too. This means other parts will expend under temperature loads in the
convenient ratio. Clearances mostly depend of the geometrical values of the rotors and
casing and in this analysis all other deformations will be neglected.
This paper presents an integrated approach which uses results from chamber model
and applies averaged temperature for the numerical analysis of screw compressor rotors
and casing deformation using commercial solver ANSYS in order to estimate change in
clearances. For this analysis oil free screw compressor with 3–5 “N” rotors showed in
Fig. 2 is used.
2
Thermal Analysis of Screw Compressor Rotors and Casing
Chamber models are often used to calculate compressor performance and determine
temperature change within the working chamber in time, i.e. with the rotation of rotors
[1]. Such models are fast and reliable. Screw compressors are positive displacement
machines in which change of the pressure is caused by the change in volume. Therefore
it is reasonable to assume that parameters of the ﬂuid trapped in the chamber such as
pressure and temperature are the same for the entire chamber in one time instant. They
change with rotation of the rotors in time as function of change in volume. Therefore,
Fig. 2. Oil free screw compressor with 3–5 “N” rotors
Numerical Analysis of Screw Compressor
935

pressure and temperature values calculated by use of chamber model can be assigned to
the part of the rotors and casing depending on the rotation angle. The computer code
SCORG developed at City, University of London allows grid generation for CFD and
calculation of preliminary thermodynamics. The grid generated in SCORG is exported
to ANSYS—CFX for temperature and pressure calculations. State of ﬂuid temperatures
obtained from the chamber model and CFD simulation for one speciﬁc position can be
seen in the Fig. 3.
Figure 3 shows temperatures of ﬂuid trapped in each chamber next to the rotors and
casing surfaces for speciﬁc rotor angle. The temperature distribution over rotor and
casing surfaces changes with the change in the rotational angle. Similar results are
achieved by multi-chamber thermodynamic model built in SCORG. These results are
used for calculation of rotor and casing deformations.
Fig. 3. Temperatures assigned to rotors (left) and casing (right) for speciﬁc rotor angle value
Fig. 4. Averaged temperature distribution on rotor (left) and casing (right)
936
E. Husak et al.

Due the cyclic characteristic of motion identical temperature distribution over sur-
faces are repeated after full cycle for rotors and after every lobe passing for casing. These
temperatures are averaged along the rotors and casing which is shown in Fig. 4 [3, 5].
Averaged temperatures are than used as boundary conditions in steady state thermal
analysis of rotors and the casing. Figure 5 shows temperature loads on male and female
rotors after steady state thermal analysis. For this analysis ANSYS software for steady
state thermal analysis has been used. Temperature load values are changed from around
27–180 °C for rotors and casing but with different distribution over surfaces. Figure 6
shows temperature loads on casing.
Temperature values for the casing are highest around discharge port. Once the
thermal steady state analysis has been performed it is possible to start with structural
analysis of screw compressor rotor and casing under temperature load.
Fig. 5. Temperature loads on female (left) and male (right) rotors
Fig. 6. Temperature loads on casing
Numerical Analysis of Screw Compressor
937

3
Structural Analysis of Rotors and Casing
Computation of rotors and casing deformation has been carried out by Finite Element
Method (FEM) in ANSYS software. Numerical analysis for each part is performed
separately. Solid bodies of rotors and casing are divided into ﬁnite elements where
female rotor comprises 185789 elements, male rotor 156228 elements and casing
50172 elements. Temperature loads are taken from steady state thermal analysis from
the previous step. Rotors are restrained at bearings.
Figure 7 shows deformations of the female and male rotors under temperature
loads. Deformations are enlarged 130 times to be visible in the ﬁgures. From the results
it can be seen that deformations are increased from suction side to the discharge side
which means that the largest deformations are on the discharge side where temperature
ﬁeld has the highest values.
Values of the maximal deformations of 100 µm on the male and female rotors are
signiﬁcant. This means that projected clearances are changed. If only the rotor defor-
mations are taken into account, this compressor would have contact between the rotors
and the casing as well as between the rotors. In reality both screw compressor which
rotors and casing will deform. Therefore the deformation of the casing needs to be
included is the analysis.
Fig. 7. Enlarged deformations of female (left) and male (right) rotors
Fig. 8. Enlarged deformations of casing
938
E. Husak et al.

Figure 8 shows the casing deformation. Deformation for casing are also enlarged
130 times to be visible.
As expected, the deformations of the casing are signiﬁcantly different than the
rotors deformations. The casing is not deformed symmetrically around the axis like in
the rotors case which can be seen from the Fig. 8. The largest deformations values for
casing are around 200 µm and it occurs in the discharge zone. This result shows that
casing expansion will make some free space for the expansion of rotors. The critical
point is the top of the casing on the discharge side which sees deformations lower than
the expansion of the rotors. However due to slight change in the position of rotor axes
at the discharge the compressor elements will not come in contact and the compressor
will continue working correctly.
The analysis shows that different regions have different levels of deformations.
Deformations in the suction zone are signiﬁcantly lower than in discharge zone which
inﬂuences clearances to change differently along the rotors and casing.
4
Conclusion
A full 3-D simulation has been carried out to determine the ﬂow ﬁeld within the oil free
screw compressor. The temperature to which rotors are exposed has signiﬁcant inﬂu-
ence on the change in operating clearances. Changes in the working clearances are
consequence of the deformation of rotor and casing. It is shown that the results from
CFD calculation or from chamber model can be used to average the temperature on the
rotor surface and could be used for mapping the boundary conditions for the FEA
analysis. The FEA analysis shows that due to the different nature of the deformation of
rotors and casing, the operational clearances in different regions of the compressor will
change differently. The work is continuing to deﬁne reliable and fast method for
analysis of the effect of the clearance change on the performance of screw compressor.
References
1. Stosic, N., Smith, I., Kovacevic, A.: Screw Compressors, Mathematical Modelling and
Performance Calculation. Springer, New York (2005)
2. Kovacevic, A., Stosic, N., Smith, I.: Screw Compressors, Three Dimensional Computational
Fluid Dynamics and Solid Fluid Interaction (2006)
3. Buckney, D., Kovacevic, A., Stosic, N.: Accounting for local thermal distortions in a chamber
model for twin screw compressors. In: 22nd International Compressor Engineering
Conference at Purdue, 14–17 July 2014
4. Kovacevic, A., Stosic, N., Smith, I.: The inﬂuence of rotor deﬂection upon screw compressor
performance. In: Conference on Screw Type Machines VDI-Schraubenmachinen, Dortmund,
Germany, September 2002, pp. 17–28 (2002)
5. Sauls, J., Powell, G., Weathers, B.: Transient thermal analysis of screw compressors, Part I:
Use of thermodynamic simulation to determine boundary conditions for ﬁnite element
analyses. In: International Compressor Engineering Conference at Purdue, 17–20 July 2006
Numerical Analysis of Screw Compressor
939

6. Weathers, B., Sauls, J., Powell, G.: Transient thermal analysis of screw compressors, Part II:
Transient thermal analysis of a screw compressor to determine rotor-to-housing clearances. In:
International Compressor Engineering Conference at Purdue, 17–20 July 2006
7. Powell, G., Weathers, B., Sauls, J.: Transient thermal analysis of screw compressors, Part III:
Transient thermal analysis of a screw compressor to determine rotor-to-rotor. In: International
Compressor Engineering Conference at Purdue, 17–20 July 2006
940
E. Husak et al.

Electric Field Calculation on Surface
of High-Voltage Transmission Line
Conductors
A. Carsimamovic1(&), A. Mujezinovic2, S. Carsimamovic2,
Z. Bajramovic2, and M. Kosarac1
1 Sarajevo, Bosnia and Herzegovina
a.carsimamovic@nosbih.ba
2 Faculty of Electrical Engineering, University of Sarajevo, Sarajevo,
Bosnia and Herzegovina
Abstract. This paper presents a calculation of the value of the electric ﬁeld on
the surface of the conductors and in the immediate vicinity of the high voltage
transmission line of 400 kV level. For the calculation of the values of the
electric ﬁeld, the CSM method is used. Verifying the established model was
carried out by comparing the calculation results with results of measurements of
the electric ﬁeld at a height of 1 m above ground level. Measurements of the
value of electric ﬁeld lines below the height of 1 m above ground were carried
out over the years in Bosnia and Herzegovina. The measurements and calcu-
lations were carried out on 400 kV lines of standard dimensions and reduced
dimensions. Using the established model, values of the electric ﬁeld on the
surface of the conductors and in their immediate vicinity were calculated. The
calculated values of the electric ﬁelds are used to determine the corona onset
voltage gradients.
1
Introduction
The corona discharge is the self-sustained discharge around conductors and occurs
when the voltage gradient on the surface of conductors reaches a threshold value which
is deﬁned as the corona onset voltage gradient. For the calculation of the values of the
electric ﬁeld, the Charge simulation method (CSM) is used [1]. Main factor to inﬂuence
the corona onset voltage gradient is voltage levels. Increased voltages in the
high-voltage electric network of Bosnia and Herzegovina causes increase in the value
of voltage gradient on surface of conductor and exposure of human being to electric
ﬁeld. Also, power frequencies overvoltage in the high-voltage electric network causes
higher power losses due to AC corona. In the 400 kV electric network of Bosnia and
Herzegovina during the last few years there were registered a signiﬁcant the appearance
of the power frequency overvoltage, especially in the regime of small loads [2–8].
Independent System Operator in Bosnia and Herzegovina operates with electric net-
work in accordance with European Network of Transmission System Operators for
Electricity (ENTSO-E), Grid Code [9] and IEC 60038 [10]. The greatest inﬂuence are
loads on the 400 kV electric network below the natural capacity (loads do not exceed
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_79

20% of thermal capacity), as well as very limited opportunities for compensation of
surplus reactive power in electric network of Bosnia and Herzegovina. Duration of
these power frequency overvoltage are signiﬁcant (that last continuously for several
hours, even days) during the non-working days and night time and represent a potential
risk to the operation of power system with possibility of breakdown of insulation,
which can cause outage of elements of the system and its stability and supply con-
sumers with electricity. It should be noted that voltages above the allowed values
adversely affecting the insulation level of equipment, shortening its lifetime, and at the
same time increase power losses due to AC corona.
2
Power Frequency Overvoltage in Bosnia and Herzegovina
Electric Power System
In analyzed part of the Bosnia and Herzegovina power system (substation SS Sarajevo
10) in the last few years are registered increased values of power frequency overvoltage
with long duration, Table 1 [11]. The power frequency overvoltage in nodes of the
Bosnia and Herzegovina power system typically happens during night, mostly in the
minimum load regime. The power frequency overvoltage may occur throughout the
year, but they are the most likely to happen in fourth, ﬁfth and sixth months of the year.
In considered work regime, most lines of Bosnia and Herzegovina power system are
loaded bellow the natural transmission power that is 550 MW for 400 kV grids that
causes production of signiﬁcant capacitance charge. In addition to the domestic pro-
duction of capacitive power charge, the interconnection grid with the neighboring
countries regularly exports reactive power of 80–100 MVAr. This causes that the
voltage in some 400 kV nodes in Bosnia and Herzegovina power system goes above
the allowed values [11]. The voltage values at substation 400 kV SS Sarajevo 10 in the
period of one year from 1st of January to 31st of December 2016 (8760 h) are shown in
Fig. 1 [11].
Figure 2 represents a diagram of duration time in % of voltage values in SS
Sarajevo 10 from the highest to the lowest voltage values in the considered period
(2013–2016) [11].
Figure 2 shows that the voltage reach a value of 434.16 kV (2013), 437.4 kV
(2014), 433.62 kV (2015) and 436.36 kV (2016), thus 3.37% (2013), 4.14% (2014),
3.24% (2015) and 4.13% (2016) higher values than the allowable voltages for nominal
system voltages of 400 kV. Also, it can be observed that the voltages in SS Sarajevo 10
Table 1. Duration time in % when voltage is higher than the maximum allowed value [11]
SS Sarajevo 10
Umax (kV) Duration time in % when U > Um
2013 (02.06.2013.) 434,16
41
2014 (18.05.2014.) 437,40
66
2015 (12.01.2015.) 433,62
38
2016 (15.05.2016.) 436,36
65
942
A. Carsimamovic et al.

during 2013–2016 were almost 41% (2013), 66% (2014), 38% (2015) and 65% (2016)
of time above the permissible voltage value according to Grid Code [9]. This over-
voltage is harmful for the equipment insulation with negative impact on power quality.
Fig. 1. Diagram of 400 kV voltages in SS Sarajevo 10 in 2016 [11]
Fig. 2. Diagram of duration of 400 kV voltages in SS Sarajevo 10 during 2013–2016 [11]
Electric Field Calculation on Surface
943

At the same time power frequency overvoltage increases power loss due to AC corona.
Therefore, it is necessary to perform an analysis of voltage gradient on surface of
central and outer conductors.
3
Case Study
Coaxial arrangement of energized stranded conductors of overhead transmission lines
SS Sarajevo 10—SS Sarajevo 20 and SS Tuzla 4—SS Visegrad, single-circuit with
horizontal conﬁguration are shown in Fig. 3. Stranded conductor of bundle ACSR
2  485/63 mm2 is taken. Aluminum wire number is 54, steel wire number is 7.
Complete diameter of sub-conductor d is 30.42 mm. Diameter of aluminum wire is
3.38 mm. Number of outer strand is 24 with bundle spacing of 400 mm.
Basic dimension of analyzed overhead lines are app. 21 m (horizontal distance
between outer phases) related to rated voltage 400 kV was built in 1970s, Fig. 3a.
overhead line SS Tuzla 4—SS Visegrad was built in 1988 with reduced horizontal
distance between outer phases app. 17 m, Fig. 3b.
4
Calculation Method
The voltage gradient around a stranded conductor is required for calculating corona
inception
voltage
gradient.
The
calculation
of
voltage
gradients
of
every
sub-conductors of bundles are performed.
Fig. 3. Three phase horizontal conﬁguration of 400 kV transmission lines with dimensions,
a SS Sarajevo 10—SS Sarajevo 20, b SS Tuzla 4—SS Visegrad
944
A. Carsimamovic et al.

(a) Charge simulation method (CSM)
CSM is very commonly used for calculation of electric ﬁeld on surface and near surface
of transmission line conductors. The conductors-ground plate structure conductor’s
corona inception voltage gradient calculation model was investigated. Three phase
horizontal conﬁguration of 400 kV transmission line are presented, Fig. 4.
Method used in this paper is based on the principle that the surface charge on the
every conductor can be replaced by a set of the ﬁctitious point charges. Phasor of
electrical potential in arbitrary point P(y,z), caused by n factitious point charges can be
calculated by using superposition principle, as follows:
uðy; zÞ ¼
1
2pe0
X
n
i¼1
qi  ln 1
ri
 
ð1Þ
where u is phasor of the electrical potential, qi phasor of the i-th ﬁctious charge, ri is
the Euclidian distance between i-th ﬁctitious point charge with coordinates (yi,zi) and
arbitrary point P(y,z) and n is the number of ﬁctitious point charges. From Eq. (1) can
be obtained phasor of electrical potential at some point in the case of a homogenous
inﬁnite space. The impact of soil can be taken into account by using a complex
coefﬁcient, and the Eq. (1) take the following form:
uðy; zÞ ¼
1
2pe0
X
n
i¼1
qi  ln 1
ri
 
þ C  ln
1
r0
i
 


ð2Þ
where ri′ is the Euclidian distance between i-th image of the ﬁctitious point charge with
coordinate (yi′,zi′) and arbitrary point P(y,z), C is complex reﬂection coefﬁcient that can
be calculated as [12, 13]:
Fig. 4. Three phase horizontal conﬁguration of 400 kV transmission line
Electric Field Calculation on Surface
945

C ¼ j  x  e0  c þ j  x  e0  ers
ð
Þ
j  x  e0 þ c þ j  x  e0  ers
ð
Þ
ð3Þ
where x is angular frequency, c soil conductivity and ers is relative permittivity of the
soil. The electrical conductivity of the soil is signiﬁcantly larger than the electrical
conductivity of the air, and the reﬂection coefﬁcient is [12, 13]:
C  1
ð4Þ
Therefore, phasor of the electric potential in arbitrary point P(y,z) can be calculated
using Eq. (2) [14, 15]. For the calculation of the phasor electric potential at a arbitrary
point of space, phasor of ﬁctitious point charge must be known. In this paper
high-voltage transmission lines are modeled using n ﬁctitious point charges qn at every
outer strand of sub-conductor and placed uniformly distributed in the semicircle inside
the every wire, with radius rk less than the radius of the wire (rq = 0.75  rw), Fig. 5 [12].
Test points Pn are placed at the wire surface, Fig. 5. Number of n is chosen equal 6.
To determine the phasor of the ﬁctitious point charges it is assumed that the values
of phasor electric potential on phase conductor is equal to phase voltage and values of
phasor electric potential of ground wire is equal zero. The values of phasor of all
ﬁctitious point charges can be determined using the following matrix equation:
½P  fqg ¼ fug
ð5Þ
where [P] is matrix of potential coefﬁcient, fqg is vector of unknown point charge
phasors and fug is vector of electric potential phasors. Dimension of matrices and
phasors in matrix term (5) depends of the number of the ﬁctitious point charges and the
Fig. 5. Arrangement for twin-bundle cylindrical stranded conductor and charge representation
[12]
946
A. Carsimamovic et al.

number of test points. From the matrix Eq. (5) are determined phasors of the ﬁctitious
point charges and from the Eq. (2) can be determined potential at any point. Vector of
electric ﬁeld intensity can be expressed from the phasor of electric potential by using
following equation:
E!ðy; zÞ ¼ ruðy; zÞ
ð6Þ
The individual components of the electric ﬁeld vector in an arbitrary point of space
with coordinates (y,z) can be determined using following equations:
Eyðy; zÞ ¼  @u
@y ¼
1
2pe0

X
n
i¼1
qi 
y  yi
r2
i
 y  yi
r
02
i


ð7Þ
Ezðy; zÞ ¼  @u
@z ¼
1
2pe0

X
n
i¼1
qi 
z  zi
r2
i
 z  zi
r
02
i


ð8Þ
Fig. 6. Distribution of calculated the space electric ﬁeld strength around sub-conductor surface
of 400 kV overhead transmission line SS Sarajevo 10—SS Sarajevo 20; a electric ﬁeld
distribution around central phase stranded conductor; b electric ﬁeld distribution near tip of outer
strand for central phase stranded sub-conductor; c electric ﬁeld distribution around outer phase
stranded conductor; d electric ﬁeld distribution near tip of outer strand for outer phase stranded
sub-conductor
Electric Field Calculation on Surface
947

The total effective value of the electric ﬁeld in a arbitrary point of space with
coordinate (y,z) is:
Eðy; zÞ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Eyðy; zÞ

2 þ Ezðy; zÞ

2
q
ð9Þ
(b) Calculation of the electric ﬁeld around the sub-conductor surface
Distribution of calculated the space electric ﬁeld strength around the sub-conductor
surface of overhead transmission lines SS Sarajevo 10—SS Sarajevo 20 and SS Tuzla
4—SS Visegrad are shown in Figs. 6 and 7.
Owing to the shielding effect of bundle conductors, the inside voltage gradient
strength of every sub-conductor surface is lower than outside voltage gradient. Peak
points appear at the every outside strand. The voltage gradients of outside phase
conductors are higher than central phase conductor. The average value and the max-
imum value of the voltage gradient at the surface of the conductors of transmission line
Fig. 7. Distribution of calculated the space electric ﬁeld strength around sub-conductor surface
of 400 kV overhead transmission line SS Tuzla 4—SS Visegrad; a electric ﬁeld distribution
around central phase stranded conductor; b electric ﬁeld distribution near tip of outer strand for
central phase stranded sub-conductor; c electric ﬁeld distribution around outer phase stranded
conductor; d electric ﬁeld distribution near tip of outer strand for outer phase stranded
sub-conductor
948
A. Carsimamovic et al.

with reduced dimensions (400 kV OHL SS Tuzla 4—SS Visegrad) are lower than the
transmission line with standard design (400 kV OHL SS Sarajevo 10—SS Sarajevo
20).
5
Calculation Method
For the veriﬁcation of the presented mathematical model the results of measurement
values of electric ﬁeld under high-voltage transmission lines SS Sarajevo 10—SS
Sarajevo 20 and SS Tuzla 4—SS Visegrad were used. There are 400 kV lines with
horizontal conﬁguration of conductors as shown in Fig. 3. The measurements of the
values of electric ﬁeld at the height of 1 m above ground according to recommenda-
tions [10, 16] were performed. Measurements were performed in the middle of the
span. At these points the highest values of the electric ﬁeld are expected, because of
high voltage line closets to the ground. Due to the symmetrical shape of horizontal
conﬁguration of conductors electric ﬁeld measurements were performed from the
middle phase conductor up to a distance of 25 m in one direction. The electric ﬁeld
measurements were performed by using 3D sensor. Comparisons of measured and
calculated results of the electric ﬁeld are given in Fig. 8.
It can be noted that the measured results and calculation results match well.
6
Conclusion
The paper presents the mathematical model based on CSM method for calculation of
the electric ﬁeld on the surface of the conductors and in the immediate vicinity of the
high-voltage transmission line of 400 kV level. Compared transmission lines are with
different dimensions of horizontal conﬁguration. 400 kV OHL SS Sarajevo 10—SS
Sarajevo 20 is with standard dimensions, but 400 kV OHL SS Tuzla 4—SS Visegrad is
Fig. 8. Comparison of measured and calculated results; a SS Sarajevo 10—SS Sarajevo 20;
b SS Tuzla 4—SS Visegrad
Electric Field Calculation on Surface
949

with reduced dimensions. The model is veriﬁed using measured values of the electric
ﬁeld under transmission lines at the height of 1 m above the ground. Comparison of
measured and calculated results showed that they match well. The average value and
the maximum value of the voltage gradient at the surface of the conductors of trans-
mission line with reduced dimensions (400 kV OHL SS Tuzla 4—SS Visegrad) are
lower than the average value and the maximum value of the voltage gradient at the
surface of the conductors of transmission line with standard design (400 kV OHL SS
Sarajevo 10—SS Sarajevo 20).
References
1. Carsimamovic, A., Mujezinovic, A., Carsimamovic, S., Bajramovic, Z., Kosarac, M.,
Stankovic, K.: Analyzing of AC Corona discharge parameters of atmospheric air. In: The 6th
International Conference on Sustainable Energy Information Technology (SEIT 2016),
Madrid, Spain (2016)
2. Measuring of electrical and magnetic ﬁelds. Faculty of Electrical Engineering Sarajevo and
Dalekovod Zagreb, Report dated 22.10.2007, Sarajevo (2007)
3. Measuring of electrical and magnetic ﬁelds. Faculty of Electrical Engineering Sarajevo and
Dalekovod Zagreb, Report dated 08.07.2014, Sarajevo (2014)
4. Measuring of electrical and magnetic ﬁelds. Faculty of Electrical Engineering Sarajevo,
Report dated 03.08.2014, Sarajevo (2014)
5. Measuring of electrical and magnetic ﬁelds. Faculty of Electrical Engineering Sarajevo,
Report dated 13.09.2015, Sarajevo (2015)
6. Carsimamovic, S., Bajramovic, Z., Rascic, M., Veledar, M., Aganovic, E., Carsimamovic,
A.: Experimental results of ELF electric and magnetic ﬁelds of power system in Bosnia and
Herzegovina. In: International Conference on Computer as a Tools (EUROCON 2011),
Lisbon, Portugal (2011)
7. Bajramovic, Z., Carsimamovic, S., Veledar, M., Hadzic, S., Carsimamovic, A.: Temporary
power frequency overvoltages in 220 kV and 400 kV transmission network. In: CIGRE C4
Colloquium on Power Quality and Lightning, 13–16 May 2012, Sarajevo, Bosnia and
Herzegovina (2012)
8. Carsimamovic, A., Mujezinovic, A., Carsimamovic, S., Bajramovic, Z., Kosarac, M.,
Stankovic, K.: Measuring of voltages and ELF electric ﬁelds of high-voltage network in
Bosnia and Herzegovina. In: Proceedings of International Symposium on Electromagnetic
Compatibility (EMC Europe 2014), 1–4 September 2014, Gothenburg, Sweden (2014)
9. Independent System Operator in Bosnia and Herzegovina: Grid Code, Sarajevo (2016)
10. IEC 60038: IEC Standard Voltages, Edition 7.0, 2009-06
11. Independent System Operator in Bosnia and Herzegovina: Identiﬁcation of Unallowable
Voltages in Bosnia and Herzegovina Transmission Network, Sarajevo (2014–2017)
12. Mujezinovic, A., Carsimamovic, A., Carsimamovic, S., Muharemovic, A., Turkovic, I.:
Electric ﬁeld calculation around of overhead transmission lines in Bosnia and Herzegovina.
In: Proceedings of International Symposium on Electromagnetic Compatibility (EMC
Europe 2014), 1–4 September 2014, Gothenburg, Sweden (2014)
13. Vujevic, S., Sarajcev, P., Botic, A.: Computation of the overhead power line electromagnetic
ﬁeld. In: 16th International Conference on Software, Telecommunications and Computer
Networks (SoftCOM 2008), pp. 27–31, Split, Croatia (2008)
950
A. Carsimamovic et al.

14. Abdel-Salam, M., Abdel-Aziz, E.Z.: Corona power loss determination on multi-phase power
transmission lines. Electr. Power Syst. Res. 58(2), 123–132 (2001)
15. Modric, T.: Computation and Measurement of the Electric and Magnetic Fields Generated
by Power Lines and Substations. University of Split, Faculty of Electrical Engineering,
Mechanical Engineering and Naval Architecture, Split (2012)
16. CIGRE WG C4. 203: Technical Guide for Measurement of Low Frequency Electric and
Magnetic Fields near Overhead Power Lines (2008)
Electric Field Calculation on Surface
951

Calculation and Measurement Analysis
of Transformer Station Low-Frequency
Electromagnetic Fields in the Framework
of Legislation on Protection from Non-ionizing
Radiation
Hidajet Salkić1(&), Adnan Muharemović2, and Nerdina Mehinović3
1 Public Enterprise of Electric Utility, Sarajevo, Bosnia and Herzegovina
h.salkic@elektroprivreda.ba
2 NOS BiH—Independent System Operator, Sarajevo, Bosnia and Herzegovina
3 Faculty of Electrical Engineering, Tuzla, Bosnia and Herzegovina
Abstract. Modern researches of electromagnetic ﬁelds (EMF) are based on the
concept that complicated theoretical investigations are resulting in appropriate
design solutions, and they are almost exclusively developed as applied resear-
ches. Generally, two directions are presented. The ﬁrst one is based on the
simpliﬁcation of numerical calculation models, and the second one is based on
models of objectiﬁed physical measurements in hard conditions. In both cases,
however, the result is the same objective, which can be summarized as follows:
create the optimal variant of solving the impact of the EMF, both in existing and
new power facilities. The paper presents the analysis of numerical calculation of
the EMF of distribution substation and detailed operational measurement pro-
gram, which includes all measurements in stationary state with measurement
location. The analysis of calculations and measurements results must indicates
the possible dangerous places and gives the recommendations of measures for
elimination of electromagnetic inﬂuences or their reduction to an acceptable
level, according to the Regulations.
Keywords: Electromagnetic ﬁelds  Electromagnetic ﬂux density
Occupational exposure  Increased sensitivity
1
Introduction
In time when all segments of work in the electric power industry are increasingly faced
with the adoption of various decisions, the idea to explore and analyze the allocation of
this research through relevant and scientiﬁcally recognized methods, represents an
important attempt to objectify modern engineering and has a special applicative value.
Developed methodology of calculating the low-frequency electromagnetic ﬁelds of
power facilities is the basis for the further development of power facilities design,
redesign of existing power facilities and normative regulation of electromagnetic
compatibility in the segment of low-frequency electromagnetic ﬁelds. A special orig-
inal scientiﬁc contribution of performed researches is reﬂected in developing the
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_80

original calculation methodology of three-dimensional low-frequency electromagnetic
ﬁelds of power facilities for any complex geometry and developing the original
methodology of power facilities modeling for numerical calculation of electric and
magnetic ﬁelds. This will produce proposals and measures to reduce electromagnetic
ﬁelds that are build on by the calculation methodology to achieve electromagnetic
compatibility in the designing and redesigning stage of power facilities. Public opinion
has been formed by inadequate reporting of mass media about the issue of the impact of
the EMF, the inability of scientists to determinate, with certainty, that there is abso-
lutely no risk, and declarations of scientiﬁc and government institutions that further
researches are needed, as well as unproved claims about the existence of a conspiracy
that would hide health risks from EMF of operating frequency. What is needed, is a
broad-based joint effort of physicists, biologists, medical scientists and engineers for
expanding and deepening knowledge about the interaction between the EMF and the
human body on macroscopic and static level [4]. After construction of the facility, it is
necessary to perform the calculation and measurement of electric and magnetic ﬁelds
levels from the authorized legal person. Calculation and measurement report is an
integral part of the documentation of the facility technical inspection and it is used for
obtaining the usage permit of the structure. Measurements have to certify levels of
electric and magnetic ﬁelds in accordance with the calculation in earlier report, in
which the designer determined the dimensions of the facility. According to the study of
CENELEC (European Committee for Standardization in Electrical Engineering), for
the European Commission, there are over 130 laws, regulations, standards and rec-
ommendations in the area of protection. The recommendations of the European
Council, Parliament of European Union for directives, the British National Commis-
sion for magnetic radiation NRPB-UK, the International Commission on Non-ionising
Radiation Protection ICNIRP, American governmental organization for environmental
protection ACGIH, the US National Council on Radiation Protection and Measure-
ments NCRP, the Institute of Electrical and Electronics Engineers IEEE are of par-
ticular importance. All standards for protection against electromagnetic ﬁelds are
prescribing the basic limits (basic restrictions) and reference limit values (reference
levels) for the amount of time-dependent electromagnetic ﬁelds and areas of profes-
sional exposure and increased sensitivity [1]. These guidelines are issued on the basis
of scientiﬁc researches and knowledge about the harmful impact of electromagnetic
radiation on the health of people. The European Union has issued a directives
2004/40/EC, 2008/46/EC and Recommendation 1999/519/EC related to the minimum
requirements for human protection from health risks and their safety. The same rec-
ommendation gave the levels of electric and magnetic ﬁelds that are recommended for
the maximum permissible levels in the legal framework within each country individ-
ually. Thus, the international organization for the protection of non-ionizing radiation
(International Commission on Non-ionizing Radiation Protection - ICNIRP) in 2010
has issued the guidelines for permitted values of intensity of electromagnetic ﬁelds at
low frequencies, for the area of occupational exposure for electric ﬁeld of 10 kV/m and
magnetic induction of 1000 lT [2]. These are limits within condition of the basic
limitation of current density in the human body to the amount of 2 mA/m2, at fre-
quency of 50 Hz. Areas with occasional stay of people are deﬁned as areas of occu-
pational exposure and for these areas a permissible value of electric ﬁeld intensity is
Calculation and Measurement Analysis
953

5 kV/m and for magnetic induction is 200 lT. Comparing the Regulations of indi-
vidual countries there can be seen a great variety of different approaches, which can be
classiﬁed, according to the study ENCONET, in three groups: a lighter approach (based
on the Guidelines of the ICNIRP and conﬁrmed effects such as in Austria, Germany,
England, all without the precautionary principles), moderate approach (based on the
precautionary principle for example in Slovenia and Croatia) and a radical approach
(characterized by intensive use of the precautionary principle, such as in Switzerland
with the limit value of the magnetic induction of 1 lT for areas of increased sensitivity
in the vicinity of power facilities). It should be kept in mind that the radiation is emitted
not only during the operation, but also in non-operational state. As signiﬁcant source of
EMF is deﬁned the source whose EMF in one of two quoted areas of protection reaches
at least 10% of the threshold ﬁeld level for that frequency.
2
Background Theory
When calculating the low-frequency electromagnetic ﬁelds ELF (Extra Low Fre-
quency) in and around power facilities, urban areas, in stationary modes, in order to
obtain levels of electric and magnetic ﬁelds in space where people stay temporarily or
permanently, within the spectrum of electromagnetic non-ionizing radiation which will
initiate adoption of the Law on the protection of non-ionizing radiation, it is necessary
to:
1. deﬁne the sources of low-frequency electromagnetic ﬁelds in power facilities
clearly,
2. perform modeling of power facility elements, close facilities and people,
3. develope and set mathematical methods for the calculation of low-frequency
electromagnetic ﬁelds in areas where computer equipment has to be stored and
where people stay permanently or occasionally,
4. perform numerical calculation of low-frequency electromagnetic ﬁelds according to
mathematical methods on particular power facility,
5. check the validity of established calculation methodology by implementation of
measurements on selected power facility,
6. analyze the results of calculation and measurements and point out to the potentially
vulnerable locations and give proposals for elimination of electromagnetic inﬂu-
ences or their reducing to acceptable level, in accordance with international stan-
dards for protection of exposure to low-frequency electromagnetic ﬁelds,
7. initiate the adoption of legislation on the protection of the environment from
harmful effects of low-frequency electromagnetic ﬁelds, as type of non-ionizing
radiation.
Basics of macroscopic electromagnetic theory of electromagnetic ﬁelds are the nec-
essary background for the calculation of vector values of electric and magnetic ﬁelds in
conductive and dielectric media, such as air inside and outside the power facility [3].
Applied differential and integral equations and quasi static conditions in the form of
negligence the shift current or retardation of ﬁelds are essential for the calculation of
low-frequency electromagnetic ﬁelds, because then the electric and magnetic ﬁelds can
954
H. Salkić et al.

be calculated separately [4]. Calculations of EMF for power facilities can be performed
by software package EFC-400, which allows the simulation in three-dimensional space
[5]. Suppose that power source is presented by the line conductors that look like thin
wires (dimension of the intersection is ignored). Conductor from which the current
ﬂows into the space is presented over straight-line segments. Calculation of electric and
magnetic ﬁelds at points in space that are located far from the line source (element of a
meshed grounding) is performed through the distribution of current or potential. The
value of current that is uniformly distributed on the segment of the line conductor is
determined based on the voltage drop between the end points that restrict the segment.
The impedance of the segment of line conductor is taken into account as well. To
determine the potential at some point in space, as result of the existence of the source
line segment, the method of mirrors is used. Phasor of electric potential at point in
space is obtained by applying the superposition theorem as the ﬁnal sum of the
potential caused by elementary, time-varying charges on the surface of the conductor.
A total value of potential in some point in space, caused by uniformly distributed
current in the segment of thin line conductor, can be calculated according to the
equation:
u ¼
1
4 p e
Z
l0
uðr0Þdl0
r  r0
j
j þ
1
4 p e
Z
l00
qðr00Þdl00
r  r00
j
j
ð1Þ
where are:
qðr0Þ
phasor of line charges density of the original conductor (A/m)
qðr00Þ
phasor of line charges density of the conductor in the mirror (A/m)
r  r0
j
j
distance between observed point and phasor of line charges density of the
original conductor (m)
r  r00
j
j
distance between observed point and phasor of line charges density of the
conductor in the mirror (m)
It is necessary to discretize the Eq. (1) by discretizing the ﬁeld of source with
unknown distribution, for example, density of line charge by using appropriate com-
bination of N linear, independent fundamental functions. In that case, the discretization
of conductor length on N segments and the discretization of observed points are
connected. The conductor division into segments of ﬁnite lengths and approximate the
unknown distribution of ﬁelds with the appropriate number of fundamental functions is
obtained in the form of the following expression:
ql0ðr0Þ ¼
X
N
j¼1
a0
jq0
j
and
ql00ðr00Þ ¼
X
N
j¼1
a00
j q00
j
ð2Þ
where are:
ql0
fundamental function on segment j of original conductor
ql00
fundamental function on segment j of conductor in the mirror
Calculation and Measurement Analysis
955

For observed segment constants a0
j ¼ 1 and a00
j ¼ 1 are selected while in other
segments they are 0. Considering that constant of the segment is selected as funda-
mental function and that high accuracy is needed, a number of segments per conductor
is increased so, the length of the longest segment does not exceed 1 m. In that case the
Eq. (1) can be presented as:
q ðr) ¼
1
4 p e
X
N
j¼1
Z
Dl0
j
a0
jq0
jðr0Þdl0
r  r0
j
j þ
1
4 p e
X
N
j¼1
Z
Dl00
j
a00
j q00
j ðr00Þdl00
r  r00
j
j
ð3Þ
Since, q0 ¼ q00 the expression (3) has N unknown values on the right side of the
equation. In order to solve the Eq. (3), N observed points in space with known potential
that are corresponding to powered conductor shave been selected. It establishes a
system of N equations with N unknown values and it is deﬁned in matrix form as:
½u ¼ ½M½q
ð4Þ
where the elements Mi;j of matrix system ½M represent the potential of the observed
point i, located on the conductor surface with current density qj. For solving matrix
equation Gauss-Seidel’s method is used [6]. When the approximation of current density
on the conductors is obtained, the vector-phasor conservative components of the
electric ﬁeld intensity at the observed point with position vector r can be determined by
using the equation:
EðrÞ ¼
1
4 p e
X
N
j¼1
Z
Dl0
j
a0
j q0
jðrÞðr  r0Þdl0
r  r0
j
j3
þ
1
4 p e
X
N
j¼1
Z
Dl00
j
a00
j q00
j ðrÞðr  r00Þdl00
r  r00
j
j3
ð5Þ
In the three-dimensional calculation, vector of the electric ﬁeld is in each point
elliptically polarized, i.e. the peak of the vector E describes an ellipse in time. The
cause of elliptical polarization is the phase shift between phases in a multi-phase
system. Such polarization can occur in single-phase systems as well, due to the pres-
ence of multiple ﬁeld sources which have a mutual phase shift. Depending on the
geometry and currents in the conductors elliptical polarization can vary from linear to
circular. Each of three components have a different size and phase shift:
ExðtÞ ¼ Exmax cos ðxt þ uÞ
EyðtÞ ¼ Eymaxcos ðxt þ uÞ
EzðtÞ ¼ Ezmaxcos ðxt þ uÞ
ð6Þ
Vector of the electric ﬁeld is elliptically polarized and it rotates in time. For electric
ﬁeld presentation the effective value (RMS) of absolute value of the electric ﬁeld is
used according to:
956
H. Salkić et al.

Eef ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
T
ZT
0
E2
xðtÞ þ E2
yðtÞ þ E2
zðt)
h
i
v
u
u
u
t
ð7Þ
Distribution calculation of magnetic ﬂux density is performed by method based on
the application of Biot-Savart’s law for the induction of ﬁnite-length, straight stream-
line and the law of superposition. Magnetic ﬂux density at any point in the space can be
calculated by superimposing of the contributions of each conductor in which current
ﬂows. The spatial position of conductors segments, their currents and phase angles,
represents inputs for the magnetic ﬂux density calculation in the desired points of the
space. The direction of magnetic ﬂux density vector is determined by the unit vector, in
cylindrical coordinate system, connected to the observed segment. Since, the position
of segments in the space is different, and thus the directions of induction vector, it is
necessary to decompose a vector of magnetic ﬂux density into components in the
direction of each coordinate axis of the global system that is not tied to a particular
segment. The direction of magnetic ﬂux density vector is vertical to the boundary plane
and deﬁned as:
B ¼ l0
4 p
Z
lj
I dl x ðr  r0Þ
r  r0
j
j
ð8Þ
The total amount of magnetic ﬂux density vector, caused by currents of N seg-
ments, can be obtained by adding the contributions of all segments:
BðtÞ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
X
N
i¼1
Bx:iðtÞ
 
!2
þ
X
N
i¼1
By:iðtÞ
 
!2
þ
X
N
i¼1
Bz:iðtÞ
 
!2
v
u
u
t
ð9Þ
where Bx,i (t), By,i (t), Bz,i (t) are the components of magnetic ﬂux density at seg-
ment i. Vector of magnetic ﬂux density is also elliptically polarized and rotates in time.
The effective value (RMS) of magnetic ﬂux density is used for magnetic ﬁeld pre-
sentation according to:
Bef ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
T
ZT
0
B2
xðt) þ B2
yðt) þ B2
zðt)
h
i
v
u
u
u
t
ð10Þ
3
Calculation of Electromagnetic Fields in EFC–400
Calculations of EMF for TS 10(20)/0.4 kV are performed with software package
EFC-400, which allows the simulations in three-dimensional space. Transformer rated
power is 630 kVA. Medium-voltage switchgear consists of three conductive ﬁelds and
Calculation and Measurement Analysis
957

one transformer ﬁeld, 24 kV, VDA 24-3VT. Low-voltage switchgear has transformer
ﬁeld and 4 cable outages up to 400 A and compensation ﬁeld. Distribution switchgear
(equipment) type VDA 24-3VT (dimension 1100  830  1350 + 600 mm) is com-
pact, metal-enclosed, SF6 insulated block. It is produced for voltages 12 kV and
24 kV. It is used for power distribution in substations up to 630 (1000) kVA, 10(20)/
0.4 kV. All parts and elements of the main current circuit are placed into
gas-impermeable metal housing and to each other as well as to the housing are isolated
with gas SF6. Low-voltage switchgear is located in the area with medium-voltage
switchgear and constructed in the form of standardized switchgear, modiﬁed for the
speciﬁc denouement. Low-voltage switchgear +N1, dimension 600  340  (1940
+ 110) mm, is designed as a whole, comprised of the supply transformer ﬁeld and
ﬁelds with outages. Bus system is designed with ﬂat copper conductors 3  2
 (50  10) + 1  (50 + 5) mm. Low-voltage switchgears +N2 and +N3, each
measuring 600  340  (1940 + 110) mm, are designed as whole. Low-voltage
switchgear +N4, measuring 600  340  (1940 + 110) mm, is designed as a whole,
comprised of the supply ﬁeld and outages ﬁelds. Medium-voltage and low-voltage
switchgears are located inside the building, as it is shown in the disposition of the
substation. According to the given projects a substation 10(20)/0.4 kV (Fig. 1) is
modeled in the software package EFC-400. At the same time the maximum possible
transformer current load is adopted. The value Im on the primary side is 18.2 A. The
calculation is conducted with the value of 20 A which is on the side of safety. At the
secondary side maximum current load is 909 A with rated voltage of 0.4 kV. A load of
909 A rarely appears, but the calculations are performed with this value so, they are on
the side of safety. It follows that the maximum current load of low-voltage side of the
transformer station is divided into 4 outages and it is 227 A per outage. Calculation was
conducted with a value of 230 A per outage which is on the side of safety.
Medium-voltage and low-voltage switchgears are located inside the building of sub-
station, as it is shown in the substation disposition. Results of calculation of electric
ﬁeld intensity and magnetic ﬂux density, at a height of 1 m above the ground are
shown in Figs. 2, 3, 4, 5 and 6. The results show that the electric ﬁeld intensity at
Fig. 1. Two-dimensional and three-dimensional view of substation disposition in EFC-400
958
H. Salkić et al.

Fig. 2. Distribution of electric ﬁeld intensity—continuous distribution
Fig. 3. Distribution of magnetic ﬁeld density at height of 1 m—isoline display
Fig. 4. Distribution of magnetic ﬁeld density at height of 1 m—continuous distribution
Calculation and Measurement Analysis
959

outside parts of the substation falls below the value of 1 kV/m, which is twice lower
than the limit value for areas of increased sensitivity. The area that is signiﬁcantly
larger than the described is separated and marked with a special warning signs. Also, in
this area is installed wiring and equipment so, during normal operation, the movement
of persons is prevented, and it can be expected that the facilities in which the substation
is located will acting as an obstacle and will reduce the electric ﬁeld intensity [6].
Magnetic ﬂux density, already at a very short distance from the transformer, falls
below the value of 20 lT which is well below the value of 40 lT as it is prescribed in
the Regulations for the areas with increased sensitivity. Values above 40 lT occur
inside the substation and that belongs to the area of professional exposure. Since the
calculation was made with the current value of 230 A, which rarely occurs in normal
Fig. 5. Detailed show of distribution of magnetic ﬁeld density around the transformer and
switchgears—isoline display
Fig. 6. Detailed show of distribution of magnetic ﬁeld density around the transformer and
switchgears—continuous distribution
960
H. Salkić et al.

operation, it can be expected that these values will be lower. Within facility the value of
magnetic ﬂux density does not exceed 100 lT, which is the limit for the areas of
professional exposure, except within the installed equipment, what cannot be avoided
because of the structure. Figures 6 and 7 show detailed overview around transformer
and reactors where it is visible that values above 100 lT occur exclusively within the
transformer equipment. From this presentation, it is evident that values above 100 lT
occur only within the equipment which cannot be avoided because of their construc-
tion. Also, outside the housing of the same equipment, magnetic ﬂux density drops to
very low values (10 lT at a distance of 3 m), so there is no danger of magnetic ﬁeld
density above 40 lT. At Figs. 7, 8, 9 and 10 are given the distribution of electric ﬁeld
intensity and magnetic ﬂux density for different transversal contours.
Fig. 7. Distribution of electric ﬁeld intensity along the y axis for direction x = 3 in EFC-400
Fig. 8. Distribution of magnetic ﬁeld density along the y axis for direction x = 3 in EFC-400
Fig. 9. Distribution of electric ﬁeld intensity along the y axis for direction x = 5 in EFC-400
Calculation and Measurement Analysis
961

4
Measurement of Electromagnetic Fields
During the preparations for measuring of electromagnetic ﬁelds in TS 10 (20)/0.4 kV
twelve (12) measuring points are selected, at which the highest levels of the electric and
magnetic ﬁelds are expected. Listed twelve measurement points are drawn in the
measurement situation (Fig. 11).
When determining the measurement points it was taken care of the fact that in the
facility environment there are residential buildings (area of increased sensitivity).
Therefore, these areas are included in the measurement. Visiting the ﬁeld showed that
the measurement situation corresponds to the actual state of development of the
respective substation in the ﬁeld [7]. After locating of measurement points measuring
instruments were veriﬁed (AC Field meter Model 238A-1, Monroe Electronics,
Fig. 10. Distribution of magnetic ﬁeld density along the y axis for direction x = 5 in EFC-400
Fig. 11. Situation of measurement
962
H. Salkić et al.

Table 1. Measurement results of electromagnetic ﬁeld
Mark of the
calculation
point
High of the
calculation
point above the
ground (m)
Electric
ﬁeld
intensity
(kV/m)
Evaluation of the
measuring uncertainty
(k = 2) (kV/m)
Magnetic
ﬂux density
(µT)
Evaluation of
the measuring
uncertainty
(k = 2) (µT)
Calculation results compared to the
prescribed maximal levels for the
zone (PE/IS—
acceptable/unacceptable)
1
1.6
< 0.1
0.092
0.47
0.010
PE—acceptable
2
1.6
< 0.1
0.092
0.44
0.010
PE—acceptable
3
1.6
< 0.1
0.092
0.31
0.010
PE—acceptable
4
1.6
< 0.1
0.092
0.63
0.010
PE—acceptable
5
1.6
< 0.1
0.092
7.76
0.036
PE—acceptable
6
1.6
< 0.1
0.092
0.52
0.010
PE—acceptable
7
1.6
< 0.1
0.092
0.36
0.010
PE—acceptable
8
1.6
< 0.1
0.092
0.28
0.010
PE—acceptable
9
1.6
< 0.1
0.092
0.61
0.010
IS—acceptable
10
1.6
< 0.1
0.092
23.9
0.036
IS—acceptable
11
1.6
0.2
0.092
10.15
0.010
IS—acceptable
12
1.6
0.45
0.092
2.81
0.036
IS—acceptable
Calculation and Measurement Analysis
963

50–1000 Hz, and a plate-shaped probes and Triaxial ELF magnetic ﬁeld meter Model
4090, FW Bell, 40–400 Hz, 3 coils U74, 0.43 mm2) and it was found that they are
ready for usage. Collected climatic conditions were: temperature (30.9 °C (in the
facility), 31.8 °C (in the environment of the facility)), relative humidity (23.5% (in the
facility), 22.9% (in the environment of the facility)), meteorological conditions (sunny).
After that, the measurements above the ground, which corresponds to an area of the
chest (1.6 m), for area of occupational exposure and area of increased sensitivity were
conducted. Instrument for measuring of electric and magnetic ﬁelds consists of sen-
sitive device (probe) and the element that determines (and usually shows) the effective
values (rms) of the ﬁeld. Measurements can be presented in three-dimensional or
uniaxial form. Three-dimensional device is performing the effective measurements of
ﬁelds along three orthogonal axes simultaneously and determines the resulting ﬁeld
regardless of the orientation of the probe in the space. Review of the measurement
situation shows that the measurement points 1 to 8 belong to the area of increased
sensitivity, while measurement points 9 to 12 belong to the area of occupational
exposure. Comparing the measurement results (Table 1) of electric ﬁeld intensity and
magnetic induction with limited values prescribed by Regulation on protection from
electromagnetic ﬁelds shows that values of electric and magnetic ﬁelds in the area of
increased sensitivity (measuring points 1 to 8) are far lower than the maximum per-
mitted (Emax = 2 kV/m and Bmax = 40 lT), and that the measured values of electric
and magnetic ﬁelds in the area of occupational exposure (measuring points 9 to 12) are
also lower than the maximum permitted (Emax = 5 kV/m, Bmax = 100 lT). When
implementing metering the electric ﬁeld intensity readings on the instrument were
lower than the smallest notch of measuring scale on the smallest measuring range (0–
5 kV/m) or lower than 0.1 kV/m, which is result of the fact that the electric ﬁeld
outside the facility building is already at very low level and is more and more weaker
by moving away from it. Taking into consideration the fact that magnetic ﬂux is
proportional to the current (load power), and considering common loads of substation
in normal operation, it is estimated that in normal operation (TS loaded with an average
of 50% of rated power) maximum amounts of magnetic induction in accessible parts of
the building will not exceed the prescribed limit of 100 lT in the area of occupational
exposure, and 40 lT in area of increased sensitivity (in environmental of transformer
station, measuring points 1 to 8).
5
Conclusion
Strategy of development and organization of protection against non-ionizing radiation
by calculation, evaluation and measurement in the environment of radiation sources, by
reducing the levels of radiation, time-limited human exposure to non-ionizing radiation
and apply of personal and mutual protection of people has to be developed for sure.
Applying the precautionary principles and norms for limiting the exposure to
low-frequency electromagnetic ﬁelds would make a huge step in preventing the dis-
ease, and the improvement and advancement of health of the population. Activating the
proposals and adoption of legislation and recommendations at the national level about
the health dangers, during exposing to low-frequency electromagnetic ﬁelds of
964
H. Salkić et al.

high-power and long-term effects, would be done mainly for the population as a whole.
This paper presents the calculations of electric and magnetic ﬁelds levels in accordance
with European legislation. It is evident that, designers adherence to standards and
regulations, all the problems related to increased levels of electric and magnetic ﬁelds
around power infrastructure can be avoided. The results of calculations and measure-
ments show that the values of the electric ﬁeld intensity does not exceed the value of
0.5 kV/m. The values are relatively low because the maximum voltage level, with
which the calculation have been performed, was 24 kV. Also, the elements installed in
the substation are shielded, so the electric ﬁeld is signiﬁcantly lower. The values of the
electric ﬁeld intensity in real operation will be even lower because the entire facility is
additionally enclosed by outdoor building. From presented calculations and measure-
ments results it is visible that the value of magnetic ﬂux density, almost in the
immediate vicinity of the transformer, is lower than the limited values prescribed for
the areas of professional exposure. At greater distances that value is falling to much
lower values. At a distance of 2 m from the transformer magnetic ﬂux density value
falls to value below 20 lT, which is much lower than the value of 40 lT, that is
prescribed in the Regulations for areas of increased sensitivity. Since, much wider
space than described is enclosed by substation building and visibly marked, no addi-
tional protective measures have been needed. Therefore, it can be concluded that TS 10
(20)/0.4 kV will emit electric and magnetic ﬁelds whose values will be lower than the
permissible limits prescribed by the Regulation on protection against electromagnetic
ﬁelds. Since, the calculations are performed with the values that are greater than the
values that occure in normal operation, it can be concluded that calculations are on the
side of safety and expected values of electric and magnetic ﬁelds in normal operation
are less than limited values prescribed in the Regulation on protection from electro-
magnetic ﬁelds. Results obtained by calculation give a satisfactory correspondence
with the experimental data, which indicates that the introduction and development of
such calculations for the practical needs related to the design and reconstruction of
existing substations is reasonable [8]. Seen from an economic point of view, it is
possible to achieve signiﬁcant cost savings, as in this way the need for expensive
experimental measurements and overhauling is reduced [9]. For evaluation of ﬁelds
distribution both procedures are needed, as they are mutually complementary and thus
they are enabling the safe evaluation of ﬁeld value. Presented mathematical model,
calculation and visual three-dimensional distribution of electric and magnetic ﬁelds
present a realistic assumption for the research of interaction of electromagnetic ﬁelds and
human bodies on macroscopic and static level by ﬁnding certain optimization criteria in
order to create new technological and procedural solutions and design methods [10].
References
1. Poljak, D.: Teorija elektromagnetskih polja sa primjenama u inžinjerstvu. Školska knjiga
Zagreb (2014). ISNB 978-953-0-30885-5
2. Poljak, D.: Izloženost ljudi neionizacijskom zračenju. Kigen (2006). ISBN 953-6970-25-2
Calculation and Measurement Analysis
965

3. Kapetanovic, I., Madzarevic, V., Muharemovic, A., Salkic, H.: Exposure to low frequency
magnetic ﬁelds of a transformer station. IJESSE Int. J. Electr. Syst. Sci. Eng. 1(2), 120–128
(2008). ISSN 2070-3953
4. Human exposure to electromagnetic ﬁelds low-frequency (0–10 kHz). European prestandard
ENV 50166–1, European Commitee for Electrotechnical Standardization (1995)
5. Poljak, D.: Advanced modeling in computational electromagnetic compatibility. Wiley
(2007). ISBN/EAN: 9780470036655
6. Salkic, H., Softic, A., Muharemovic, A., Turkovic, I., Klaric, M.: Calculation and
measurement of electromagnetic ﬁelds. Electromagnetic Radiation. In: Bashir, S.O. (ed.)
p. 288. Publisher InTech (2012). ISBN 978-953-51-06395 https://doi.org/10.5772/2009
7. Muharemovic, A., Madzarevic, V., Salkic, H., Turkovic, I., Mehinovic, N.: Calculation of
low-frequency magnetic ﬁeld distribution of a transformer station in stationary state. Int.
Rev. Model. Simul. (IReMoS) (2009). ISSN 1974-9821, Cd-Rom ISSN 1974-983X
8. Salkic, H., Madžarevic, V., Muharemovic, A., Hukic, E.: Numerical solving and
experimental measuring of low frequency electromagnetic ﬁelds in aspect of exposure to
non-ionizing electromagnetic radiation. In: The 4th International Symposium on Energy,
Informatics and Cybernetics: EIC 2008 in the Context of the 12th Multi-conference on
Systems, Cybernetics and Informatics: WMSCI 2008. Orlando, Florida, USA, vol. 2,
pp. 257–262 (2008)
9. Salkic, H., Madzarević, V., Klaric, M., Mehinovic, N.: Calculation and measuring of
quasi-static electromagnetic ﬁeld in electric facilities. In: EMF 2009 8th International
Symposium on Electric and Magnetic Fields from Numerical Models to Industrial
Applications. Mondovì, Italy, pp. 26–29 (2009)
10. Salkic, H., Madzarevic, V., Hukic, E.: Calculation and measuring of low-frequency electric
ﬁeld distribution of 10(20)/0.4 kV, 630 kVA transformer station. In: 43rd International
Universities Power Engineering Conference (UPEC2008). University of Padova, Depart-
ment of Industrial Engineering, University of Cassino, Padova, Italy Sept (2008)
966
H. Salkić et al.

On the Use of Boundary Element Method
for Cathodic Protection System Modeling
Adnan Mujezinović1(&), Sanja Martinez2, and Slobodan Milojković3
1 Faculty of Electrical Engineering, University of Sarajevo, Sarajevo,
Bosnia and Herzegovina
adnan.mujezinovic@etf.unsa.ba
2 Faculty of Chemical Engineering and Technology, University of Zagreb,
Zagreb, Croatia
3 Faculty of Electrical Engineering, University of East Sarajevo, East Sarajevo,
Bosnia and Herzegovina
Abstract. Metallic installations placed in the soil or sea water (such as metallic
pipelines) are submissive to the processes of corrosion. Corrosion is the
destruction of the metal which is placed in electrolytes. One of the most often
used technique for protection of underground or underwater metallic infras-
tructures against corrosion is cathodic protection systems. Design of any
cathodic protection system requires determination of the electric potential and
current density distribution on the electrode surfaces. This paper presents
numerical method based on the direct boundary element method for calculation
of distribution of electric potential and current density on electrode surfaces with
nonlinear polarization characteristics. Presented numerical method was used to
calculate parameters of one illustrative example of cathodic protection system.
Keywords: Cathodic protection system  Boundary element method
Newton–Raphson method  Nonlinear boundary conditions
1
Introduction
Corrosion is the destruction of the metal caused by exposure to and interaction with its
environment. Previous research has shown that the greatest percentage of damage
caused by corrosion can be prevented by the use of modern protection techniques. One
of most effective method for protection of the metallic surface against corrosion is
application of the cathodic protection system in combination with passive protection
[1–3]. Adequate design of any cathodic protection system requires determination of
distribution of electrical potential and current density on the electrode surfaces. The
purpose of cathodic protection system modeling is to determine the efﬁciency of the
cathodic protection system. For the cathodic protection system, it is considered to be
efﬁcient when the value of the electrical potential at each point of the protected object is
lower than the minimum protection potential [4]. Also, current density distribution on
the surface of protected object should be homogenous [5].
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_81

Since for evaluation of the efﬁciency of the cathodic protection system electric
potential and current density distribution on the electrode surfaces is required, direct
boundary element method is the most suitable numerical method. By using boundary
element method only boundaries of the domain is need to be discretized and there is no
need for discretization of inﬁnite boundaries [6]. In this paper the methodology of
cathodic protection system parameters calculation using the boundary elements method
is presented.
2
Mathematical Model
Electric potential distribution of the cathodic protection system can be calculated by
using Laplace partial differential equation for the static current ﬁeld [1]:
r  c  r  u
ð
Þ ¼ 0
ð1Þ
where u is electric potential, c is conductivity of the media where cathodic protection
system is placed and symbol r is Nabla operator.
By applying Green’s identity on the Laplace partial differential Eq. (1), integral
equation of the static current ﬁeld can be obtained as follows:
c q
ð Þu q
ð Þ þ
Z
C
u p
ð Þ  ~n  rG p; q
ð
Þ
ð
Þ dC ¼
Z
C
G p; q
ð
Þ  ~n  ru p
ð Þ
ð
Þ dC
ð2Þ
where q is observation point, p is ﬁeld source point, c(q) is constant, u(q) is the electric
potential of the observation point q, u(p) is the potential of the ﬁeld source point p, G
(p, q) is the Green’s function, C is the boundary of the domain and ~n is normal unit
vector.
In order to calculate normal current density, Ohms law must be introduced, as
follows [7]:
~n  ru p
ð Þ ¼  j p
ð Þ
c
ð3Þ
where j(p) is normal component of the current density.
By applying Eq. (3) on the Eq. (2), the integral ﬁeld equation takes following form:
c q
ð Þ u q
ð Þ þ
Z
C
u p
ð Þ  ~n  rG p; q
ð
Þ
ð
Þ dC ¼  1
c
Z
C
G p; q
ð
Þ  j p
ð Þ dC
ð4Þ
The previously given integral ﬁeld Eq. (4) is valid for internal Dirichlet and
Neumann problems. This further means that the integral Eq. (4) in this form is
968
A. Mujezinović et al.

applicable for cathodic protection system modeling of bounded geometry such as a
cathodic protection system of the internal wall of the pipelines and the inner wall of the
tanks. In order to model cathodic protection systems placed in inﬁnite and semi-inﬁnite
domains, Gauss boundary condition must be satisﬁed [8, 9]:
Z
C
j p
ð ÞdC ¼ 0
ð5Þ
This condition can be satisﬁed by adding a constant potential u∞to the right side of
the Eq. (4), as follows:
c q
ð Þ u q
ð Þ þ
Z
C
u p
ð Þ  ~n  rG p; q
ð
Þ
ð
Þ dC ¼  1
c
Z
C
G p; q
ð
Þ  j p
ð Þ dC þ u1
ð6Þ
The introduction of unknown potential u∞in Eq. (4) provides a conservative
current ﬂow between anode and cathode surface [10].
2.1
Boundary Conditions
In order to obtain a unique solution of electric potential and current density distribution
of the cathodic protection system, it is necessary to set appropriate boundary conditions
at the electrode surfaces. For modeling the cathodic protection system it is necessary to
set boundary conditions at the anode/electrolyte and cathode/electrolyte boundaries.
Boundary conditions on the electrode surfaces are inﬂuenced by electrochemical
reactions which take place on the electrode surfaces. These boundary conditions are
assigned by the appropriate analytical (empirical) functional relations between current
density and electrical potential and these functional relationships are nonlinear due to
the nature of the electrochemical reactions.
Electrochemical reactions of metal dissolution, oxygen reduction and hydrogen
separation simultaneously occur of the cathode surface [11]. Therefore, the boundary
condition on the cathodic surface consists of three parts as follows:
jc ¼ j0; Fe  10
uuFe
bFe  jlim; O2  j0; H2  10

uuH2


bH2
ð7Þ
where jc is total current densities for cathode surface, j0,Fe current density corre-
sponding to the metal dissolution reaction, jlim,O2 is the limiting current density of
oxygen reduction jH2 is current density corresponding to the reaction of hydrogen
separation, u is potential difference of interface metal/electrolyte uFe, and uH2 are
equilibrium potentials of corresponding electrochemical reactions and bFe, and bH2 are
Tafel slopes of metal dissolution and hydrogen separation, respectively.
On the Use of Boundary Element
969

Electrochemical reactions that occur on the anode surface are metal dissolution and
oxygen reduction. Therefore, the boundary condition i.e. the polarization characteristic
of the galvanic anode surface has the following form:
jA ¼ jO2 
10
uuA
bA  1


ð8Þ
where uA is equilibrium potential of galvanic anode and ba is Tafel slope of galvanic
anode.
3
Boundary Element Method
In the numerical solution of the integral ﬁeld equation by the boundary element
method, it is necessary to discretize the electrode surface on the M boundary elements.
After the discretization of the electrode surface, the integral Eqs. (5) and (6) are solved
over each boundary element. For each boundary element, integral equations can be
written in the following form:
c qi
ð Þ u qi
ð Þ þ
X
M
k¼1
Z
Ck
uk p
ð Þ  ~n  rG p; qi
ð
Þ
ð
Þ dCk
¼  1
c 
X
M
k¼1
Z
Ck
G p; qi
ð
Þ  j p
ð Þ dCkþ u1
ð9Þ
X
M
k¼1
Z
Ck
j p
ð Þ dCk ¼ 0
ð10Þ
where M is total number of boundary elements.
In order to solve the previous integral equations it is necessary to approximate the
current density, electric potential and geometry by shape functions. In this paper, the
Lagrange quadratic polynomial functions have been used for the approximations.
Using the coordinate transformation and the Gauss-Legendre quadrature, the integral
Eqs. (9) and (10) can be written in the form of a matrix equation [12]:
Hcc
Hca
1
Hac
Haa
1
0
0
0
2
4
3
5 
uc
ua
u1
8
<
:
9
=
; ¼
Gcc
Gca
Gac
Gaa
Ac
Aa
2
4
3
5 
jc
ja


ð11Þ
where [H] and [G] are matrix of coefﬁcients, {uc, ua, u∞}T is matrix vector of
unknown potentials and {jc, ja}T is vector matrix of current densities. Indexes c and
a represents cathode and anode surface, respectively.
970
A. Mujezinović et al.

Equations (7) and (8) represent the boundary conditions of electrode surfaces. As it
can be noted, these boundary conditions are nonlinear, therefore matrix Eq. (11) is
nonlinear and can be solve by using iterative techniques. In this paper for solution of
matrix Eq. (11) iterative Newton—Raphson technique proposed in [9, 10] was used.
First step of used Newton-Raphson technique is expansion of the vector current density
in the Taylor series. Then vector of current densities (cathodes and anodes) in n-th
iteration can be calculated by using following equation:
j
f gn¼
j
f gn1 þ
@j
@u


n1
Du
f
gn
ð12Þ
where [∂j/∂u] is Jacobean matrix and {Δu}is vector of electric potential increments in
n-th iteration and can be calculated as:
Du
f
gn¼
uc
ua
u1
8
<
:
9
=
;
n

uc
ua
u1
8
<
:
9
=
;
n1
ð13Þ
Including Eqs. (12) and (13) in matrix Eq. (11) solution of electric potential increments
vector in n-th iteration can be calculated [10]:
H
½   G
½ 
@j
@uS


n1

	
 Du
f
gn¼ G
½  j
f gn1 H
½  u
f gn1
ð14Þ
First step of each iteration of presented Newton—Raphson technique is calculation
of the electric potential increment vectors. These vectors are further used in Eqs. (12)
and (13) for calculation of vector of current densities and electric potentials in same
iteration. Iterative procedure stops when all components of potential increment vectors
are lower than permissible error.
In order to accelerate calculation, GMRES iterative technique was used at each
iteration of the Newton–Raphsnon method on Eq. (14) for calculation of the vector of
potential increments [13].
4
Case Study
Previously presented mathematical model was used for calculation of galvanic anode
cathodic protection system parameters. Geometry of analyzed cathodic protection
system is given on the Fig. 1.
On the Use of Boundary Element
971

Analyzed cathodic protection system is placed in homogeneous electrolyte and is
composed of the three galvanic anodes placed at an equal distance from the cathode.
Distance of the anodes center from center of cathode was d = 10 (m) and electrolyte
conductivity was c = 0.04 (S/cm). Radius of all anodes was ra = 10 (cm) and radius of
cathode was rc = 500 (cm). Parameters of the polarization characteristics for both
anode and cathode surfaces are listed in Table 1.
The results of the calculation of the electric potential and current density distri-
bution on the cathode and anodic surfaces of example given on Fig. 1 are presented
below.
Electrolyte
Cathode
rc
Anode 1
ra
d
Anode 2
ra
d
Anode 3
ra
d
0°
90°
180°
270°
0°
90°
180°
270°
Fig. 1. Geometry of analyzed cathodic protection system
Table 1. Value of polarization parameters
Parameter Value
Parameter Value
j0Fe
0.882 (mA/cm2)
uH2
−3184 (mV) versus CSE
uFe
−2575 (mV) versus CSE bH2
501 (mV/dec)
bFe
924.5 (mV/dec)
jO2
1 (mA/cm2)
jlimO2
0.530 (mA/cm2)
ua
−1100 (mV) versus CSE
jH2
0.0685 (mA/cm2)
ba
600 (mV/dec)
972
A. Mujezinović et al.

On Fig. 2 results of the electric potential distribution is given while on Fig. 3
results of current density distribution is given for cathode surface. From given results it
can be noted that electric potential on the cathode surface is lower on the areas that are
closer to anodes then other parts. Also, in these areas current density is higher (in
absolute sense). Therefore it can be concluded that cathode surface that is closer to
anodes are more protected then the other parts of the cathode surface.
Fig. 2. Electric potential distribution on the cathode surface
Fig. 3. Current density distribution on the cathode surface
On the Use of Boundary Element
973

On Fig. 4 results of the electric potential distribution is given. On Fig. 5 results of
current density distribution is given for anode surface (Anode 1). Since, all three
anodes are symmetrical placed, it is analyze results of one anode. From given results it
can be noted that electric potential on the anode surface is higher on the areas that are
closer to cathode surface then other parts. Also, in these areas current density is higher.
Therefore it can be concluded that anode surface that is closer to cathode have higher
current consumption and this area is more exposed to corrosion process.
5
Conclusion
In this paper, mathematical models based on the boundary element method for cathodic
protection system analysis and design has been presented. Presented mathematical
model takes into account nonlinear effects that occur on the electrodes surfaces.
Fig. 4. Electric potential distribution on the anode surface
Fig. 5. Current density distribution on the anode surface
974
A. Mujezinović et al.

Application of the presented model was demonstrated on one example of one galvanic
anode cathodic protection system.
References
1. Lazzari, L., Pedeferri, P.: Cathodic Protection, 1st edn. Polipress, Milano (2006)
2. Morgan, J.H.: Cathodic Protection Design. NACE International, Houston, TX (1993)
3. DeGiorgi, V.G.: Corrosion basics and computer modeling. In: Industrial Applications of the
Boundary Element Method, Computational Mechanics, pp. 47–79 (1993)
4. Muharemovic, A., Zildzo, H., Behlilovic, N., Turkovic, I.: Numerical model for calculation
of parameters of cathodic protection system with galvanic anodes. In: Proceedings of the
XXII International Symposium on Information, Communication and Automation Technolo-
gies (ICAT), Oct 2009
5. Muharemović, A., Zildžo, H., Letić, E.: Modelling of protective potential distribution in a
cathodic protection system using coupled BEM/FEM method. In: Proceedings of the 30th
International Conference on Boundary Elements Method and Other Reduction Methods,
BEM/MRM, Slovenia, Maribor, July 2008
6. Martinez, S.: Evaluation of the uniform current density assumption in cathodic protection
system with close anode to cathode arrangement. Mater. Corros. 61(4), 338–342 (2010)
7. Mujezinović, A., Martinez, S., Muharemović, A., Turković, I.: Application of the coupled
BEM/FEM method for calculation of cathodic protection system parameters. Int. J. Comput.
Methods Exp. Meas. 5(5), 659–666 (2017)
8. Brebbia, C.A., Telles, J.C.F., Wrobel, L.C.: Boundary Element Techniques Theory and
Applications in Engineering. Springer (1984)
9. Santiago, J.A.F., Telles, J.C.F.: On boundary elements for simulation of cathodic protection
system with dynamic polarization curves. Int. J. Numer. Methods Eng. 40(14), 2611–2627
(1997)
10. Santiago, J.A.F., Telles, J.C.F.: A solution technique for cathodic protection system with
dynamic boundary conditions by the boundary element method. Adv. Eng. Softw. 30(9),
663–671 (1999)
11. Mujezinović, A., Muharemović, A., Turković, I., Muharemović, A.: Calculation of the
protective current density distribution of a cathodic protection system with galvanic anodes
in terms of double-layer electrolyte. In: Proceedings of the 34th International Conference on
Boundary Elements Method and Other Reduction Methods, BEM/MRM, Croatia, Split
(2012)
12. Riemer, D.P.: Modeling cathodic protection for pipeline networks, PhD Theses, University
of Florida, USA, Florida (2000)
13. Mujezinovic, A., Turkovic, I., Muharemovic, A., Martinez, S., Milojkovic, S.: Modeling of
the galvanic anode cathodic protection system with dynamic polarization characteristics. Int.
J. Chem. Chem. Eng. Syst. 1, 95–100 (2016)
On the Use of Boundary Element
975

Part VIII
Robotics and Biomedical Engineering

Research and Development of New Generation
Service Robots for Medial Application
Isak Karabegovic(&)
Technical Faculty, University of Bihac, 77000 Bihać, Bosnia and Herzegovina
isak1910@hotmail.com
Abstract. The development of new technologies has contributed to the
development of robotic technology. The greatest contribution to the develop-
ment of robotic technology was made by information technology and sensor
technology. Parallel to their development, robotic technology evolved, as well.
The development of industrial robots was followed by the development of
service robots. The development of information technology and advancement in
sensor technology and servo-drives resulted in over 300 different types or
prototypes of service robots for non-production applications. Service robots are
designed to be used in professional and service workplaces, and to be used in all
areas of daily life. One of the application areas of service robots is medicine.
Since medicine can be classiﬁed as science, research, and human discipline, so
can application of service robots present scientiﬁc contribution through huge
application of variety of robotic devices for different purposes in different
technological achievements in various ﬁelds of medicine. These robotic devices
are used as replacement for missing limbs, perform complex surgical proce-
dures, serve patients in hospital rooms, perform laboratory tests to diagnose the
disease, and help in rehabilitation after stroke. This paper will explain the
representation of service robots in medicine, as well as their application in
different areas of medicine. An analysis was conducted of investment in
development and research of robotic technology, with scenario of development
of robot revolution to the creation of intelligent robots.
Keywords: Digital technology  Service robot  Application of service robots
Medicine  Logistics  Rehabilitation
1
Introduction
In the last 20 years there was a development of digital technologies that are, with
integration with other technologies, implemented in all aspects of human life, as well as
unavoidable in the implementation of robotic technology, both industrial and service
robotics. We are at the beginning of the fourth industrial revolution because many
strategies of industrial development of countries in the world, that are considered
technological advanced countries, advocate the transformation of industrial production
by combining digital technology and communication technology (the Internet) with the
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_82

conventional industry, with service robotics being included in the development strat-
egy. Let us take the example of Germany, which published its strategic program
expected by 2020 for the ﬁrst time in Hanover in 2011 under the title “Industry 4.0”,
while in 2012 in the USA the Advanced Manufacturing Partnership Managing Com-
mittee, provided the recommendations for positioning the United States in the
long-term leadership in advanced technology. Similarly, in 2013 the Government
Ofﬁce in the UK announced scientiﬁc strategy named the Future of Manufacturing: A
new era of opportunity and challenge for the UK, whereas China aims to become
technologically most developed country in the world with their strategy “Made in
China”. Here we have to single out Japan, because the Japanese Government imple-
ments its strategy called “robotic revolution”, which aims to develop from the second
to the fourth robotic revolution. Today, robotic devices in medicine are used in neu-
rosurgery, orthopedics, endoscopy, surgery, radiation spot, colonoscopy, serving the
patient, but also to replace missing limbs, perform delicate surgical procedures, deliver
neuro-rehabilitation therapy for patients that have experienced a stroke, teach children
with disabilities to learn, allow visits to patients and conduct or supervise remote
operational procedures (telesurgery) with assistance of robotic technology which is
operated by the doctor remotely from his ofﬁce, hospital or home, and perform a
growing number of other similar tasks relating to health. With the development of
information technology, new materials, and robotic technology, there is now an
opportunity to change and overcome the problems that previously were impossible to
be solved, but also to improve orthopedics and help patients to recover quickly and
return to the normal state prior to their injuries. In the mid 80s there was a sudden
increase in the production of medical robots due to the growing demands of the market
for new innovations, new production technologies and services. Nowadays, service
robots have an important place in medicine. The beneﬁts of these robots in a revolu-
tionary clinical practice are numerous: facilitate medical processes with guiding
instruments, the application of diagnostic equipment and tools for diagnosis and
therapy, increase of safety and overall quality of the operation, better patient care,
education and staff training is performed through simulation, and promotion of the use
of information in diagnosis and therapy. Robotics represents the future of modern
medicine, and in the developed western countries it has long been an absolute trend in
surgery, rehabilitation, diagnostics and logistics of medical institutions. In support of
robotic surgery goes the fact that the surgery is performed several times more accurate
and with much smaller incisions than traditional surgery. In the nearby future (as
expected by 2025) robots and people need to be mutually linked, so that they can work
and communicate with each other via the internet platform (IOT) [1–10]. The robots
will be networked so they can communicate among themselves and make decisions.
Robotic technology represents a challenge for research because a number of areas is
included in the research, as shown in Fig. 1.
980
I. Karabegovic

When it comes to service robotics, Fig. 1 indicates that it is impossible to skip
social, economic and technical implications. Service robotics research area is wide and
it ranges from the design process, business model of service robots, service robots
diffusion, acceptance of continuous treatment, their standardization for the sake of their
implementation, to the integration of digital and ICT technology. In the ﬁeld of
research it is necessary to take economic implications since their implementation will
cause the simpliﬁed and cheaper operation that affects economic outcome, but also the
technical implication since it is necessary to develop a service robots for everyday use
which are more easily handled, and can be used by ordinary people without much
knowledge of ICT. There are many examples of why we must take into account the
social implications in the research. We will mention one of the biggest. Most countries
have a problem with the older population (who retired) and are of poor health condition
[11–13]. The best examples of retired population are in Japan and the USA, as indi-
cated in Fig. 2.
Fig. 1. Service robotics research area
0
10
20
30
40
50
60
70
1995
2000
2005
2010
2015
2020
2025
2030
2035
2040
%  Penzionisana radna snaga
JAPAN
USA
Fig. 2. The percentage of retired workforce in the USA and Japan
Research and Development of New Generation
981

Based on Fig. 2, we conclude that the percentage of retired people in the USA and
Japan is increasing each year, which gives us the right to conclude that these are elderly
or persons with disability and in need of some help. The most prominent situation is in
Japan, followed by the USA, where in 2040 Japan will reach approximately 60% of the
retired workforce, while in the USA the percentage will amount to 44%. Almost every
older person binds two healthy people who have to take care of them. In order to avoid
the above facts it is necessary to develop service robotics, i.e. robots that will replace
persons and will be helpful to every older person, not only to help but also for the
rehabilitation and treatment of people in need.
2
The Development of Robot Technology Supported
by Digital Technology and ICT
Robots are for the ﬁrst time used in the automation of manufacturing processes in the
60s of the last century. As is well known robots perform the dirty, dangerous and
boring jobs in strictly controlled and projected area to prevent injuries to workers or
other operation conducted nearby. If required to perform other tasks, the ﬁrst genera-
tion robots are difﬁcult to adjust because new tasks require re-programming, and
therefore it is the matter of rigid automation. The development of digital and advanced
technologies, as well as innovation in production processes represents a challenge for
the development of robot technology [2–6, 14–19]. Among other advanced tech-
nologies, robotic technology represents one of the cornerstones of the fourth industrial
revolution. The convergence of digital and other technologies, primarily referring to the
sensor technology, have inﬂuenced the development of robotic technologies, as indi-
cated in Fig. 3.
As Fig. 3 shows, the ﬁrst generation of industrial robots presents the ﬁrst robotic
revolution that occurred in the 60s and 70s of the last century, and it is the industrial
automation that is most represented in the automotive industry, and many other pro-
duction processes. Digital technologies, ICT technologies, sensor technology and man
Fig. 3. The development of robotic technology from the ﬁrst to the fourth robotic revolution
982
I. Karabegovic

advanced technologies changed this situation and lead to the development of
second-generation robots, which are more sensitive and safe to operate and do not
require a fence to separate from the workers. Development of robot technology has
contributed to the fourth industrial revolution “Industry 4.0”, since strategies of leading
industrial nations are aimed to the complete automation of production processes and
“intelligent automation”, which leads to robotic technology and fourth robot revolution
when all robots will be intelligent robots. The second generation of robots has created
new requirements for increased productivity, and surpassed the industrial robots of the
ﬁrst generation. There are many reasons for this and some are: making robots easier to
program and use, enhanced ability to manipulate (can perform diverse range of tasks),
reduced size and cost of the robot, robots are working in a wide range of dynamic
environment and work alongside to workers. In this way, we have a signiﬁcant
improvement in the management of the robot, which gives the robot the possibility to
assume a greater variety of performance tasks. In the twentieth century listed tech-
nologies prepared our path to the development of artiﬁcial intelligence, dealing with
numerical problem solving related to intelligence. The development of digital tech-
nology and ICT gives us the possibility to realize, with more or less success, things that
we observe in real time. In order to develop and fully implement artiﬁcial intelligence
we need to understand the knowledge of functioning of human intelligence, such as the
process of learning, thinking, perception, consciousness etc. In other words, we must
know the three pillars of functionality of the intelligence on which to build artiﬁcial
intelligence, such as: the ability to learn, the ability to abstract thinking and the ability
to cope in the new situation, which is nowadays researched in the various research
institutes worldwide. This brings us to fully intelligent robots and intelligent systems or
fourth robotic revolution (Figs. 3 and 4). Challenges for the development of robot
Fig. 4. Technological trends in the development of robotics per service robots
Research and Development of New Generation
983

technology from the ﬁrst generation industrial robots to service robots, or most
important technology shaping the future ability of robots, lie in the direction of three
technical areas, as shown in Fig. 4. The ﬁrst technical ﬁeld is the cognition that gives
robot the ability to perceive, understand, plan and move in the real world. This feature
improves robot cognitive abilities so that the robot can operate independently in dif-
ferent complex environments [1, 2, 4, 6]. Other technical ﬁeld is manipulation which
gives robot precise control and a preparedness to manipulate objects in its environment.
The third technical ﬁeld is interaction and presents one of the most important areas
because it enables the robots to learn and collaborate with people. In addition, it
improves interaction robot–human being for verbal and non-verbal communication.
The robot has the ability to observe and imitate man and learn from that experience. An
absolute condition for the operation of robots with people in the neglected environment
is safety. Robots and artiﬁcial intelligence today live with each other, so it is difﬁcult to
imagine a modern robot that is not a kind of artiﬁcial intelligence. As with artiﬁcial
intelligence, robots, androids and fusion of all three life forms also raise the question
what if they get out of control? According to one of the robot/AI experts, Hans
Moravec, by 2040 robots will become as smart as men, and we are sure that many of
them will become smarter than some people. Unlike pessimistic-paranoid predictions,
Moravec is not worried. He believes that robots and artiﬁcial intelligence will actually
extend the life of man, and improve the quality of life in general. For lay people, it is
difﬁcult to assess which of the scientists is right; the truth is that some of the possi-
bilities and theories are worrying, but we understood that by reading some of the great
works of science ﬁction. As it seems, the work of evolution has led man nearly to the
degree that he can build an intelligent being like himself! The whole matter is now far
advanced and is probably impossible to control, and perhaps we just need to try to turn
it in our favor. As we noted, and ending the text of the artiﬁcial intelligence, the only
real danger come from the man who perhaps (incorrectly) used his time, by destroying
nature, waging war and sowing hatred. On the other hand, some of science ﬁction
works have shown that the co-existence of artiﬁcial intelligence/robots/androids and
humans is possible, but only under the condition that humans also make progress
together with these creatures. In any case, a century in which we ﬁnd ourselves has
already brought a considerable amount of scientiﬁc excitement, and there are actually
rare people that do not perceive this outcome as positive. We live in a time that will
undoubtedly be remembered for many things in the distant future, and it would be
shameful that we are not aware of it in present time. During the 1980s and early 1990s,
advances in new technologies, sensor technologies, computers and servo-drive devel-
oped a hundreds of different types of service robots for non-production applications.
The development of robot technology cannot be stopped, including service robotics,
which is highly signiﬁcant for use in all branches of medicine: serving patients,
rehabilitation, and disinfection of premises to performing surgical procedures. In order
to see to what extent they are represented we need to conduct an analysis of the
representation of service robots in health care facilities.
984
I. Karabegovic

3
The Representation of Service Robots in Medical
Institutions Worldwide
In order to conduct the analysis of the representation of service robots it is necessary to
have statistical data. The statistical data for the above analysis of the number of
industrial robots were taken from the International Federation of Robotics (IFR), the
UN Economic Commission for Europe (UNECE) and the Organization for Economic
Cooperation and Development (OECD) [6, 7, 20–23] (Fig. 5).
The total representation of service robots for professional use increases annually, so
in the last seven years it has increased by four times, with about 11.000 service robot
units represented in 2009 it increased to 41.000 service robot units in 2015. The reason
for such an increase in the application of service robots for service activity is the
implementation of digital technologies, ICT and other advanced technologies in robotic
technology which led to the development of many applications for the implementation
of various tasks. If we observe the representation of service robots by branches we see
that until the last two years the highest representation was for defensive purposes,
followed by agriculture. The third place was held by logistics which in the last two
years was in the ﬁrst place thanks to the fourth industrial revolution. The reason for this
is that all technologically advanced countries in the world have adopted strategies to
fully automate production processes and to come up with “intelligent automation” that
cannot be done without logistics service robots, which in return reﬂected their appli-
cation. In the fourth place is the representation of service robots in medical institutions
with a slight increasing trend every year. This trend is dictated by the strategies of the
developed countries, but also the market because the market takes interest in service
robots that can be easily implemented, achieve good performance, easy to maintain and
replace workers in the production process which will bring proﬁt to companies and
enable them to be competitive in the market. This is not the case with medical facilities,
although in recent years they are increasingly developing such applications that are
progressively being used in medical institutions. The development of robot technology
in the future will be such that every household will have a service robot for one or more
different purposes. Investing in research and development is aimed at the so-called
0
10.000
20.000
30.000
40.000
50.000
2009 2010 2011 2012 2013 2014 2015
0
5000
10000
15000
20000
2009201020112012201320142015
Logist.
Defence
Field
Medical
(a) Total representation
(b) Representation per areas
Units
Units
Fig. 5. The representation of service robots for professional use for the period 2009–2015
Research and Development of New Generation
985

social (service) robots (so far industrial and military robots had the advantage), as well
as service robots for application in medical institutes for different purposes. Conﬁr-
mation to this conclusion is given by the ﬁnances that are invested in certain types of
robots in the world, as shown in Fig. 6a, as well as investment in China up to 2021 in
Fig. 6b.
Based on the image Fig. 6a, we conclude that there is a continuous increase of
ﬁnancial investment in the development of the service robotics, so that investments in
2025 will reach the amount of approximately 66.4 billion dollars. The largest part of
this amount relates to the development and deployment of personnel and service robots
70%, while a smaller part relates to the production processes in the industry about 30%.
Using abundant labor force China has achieved the astonishing economic success in
recent decades. The adopted strategy “Made in China 2025” will lead China among the
technologically most developed countries in the world. In China, the percentage of
retirees in the coming decades is going to increase signiﬁcantly, and it is assumed that
by 2025 the population older than 60 years of age will increase to slightly more than
30% of the total population, because the percentage in China today is 12%. In order to
meet the growing health needs of an aging population, China is investing in the
development and implementation of service robotics (Fig. 6b), where they have two
scenarios and it is anticipated that by 2021 they will invest about 400 million dollars
(according to the Report GCIS in 2016). Certain applications of service robots in the
medical institutions are already in function, such as surgical robots that achieve more
precise and less invasive procedures. Robots will transform secondary care, tertiary
care, primary care, home and community care. Also, there are intensive applications of
service robots for rehabilitation of patients in China, and it is expected that there will be
an enormous increase in the representation of the service robotics in all segments of
society in China and mostly in medical institutions. Investment in the development of
robot technology shown in the diagrams in Fig. 6, conﬁrm that digital technologies,
ICT technologies and other advanced technologies lead us in the direction of building
“intelligent factory”, which is the ultimate goal of the fourth industrial revolution, i.e.
0
100
200
300
400
2016 2017 2018 2019 2020 2021
I Scenario
II. Scenario
(a) Investment worldwide
(b) Investment in China
Milion $ 
Fig. 6. Financial investment in various types of robots in the years to come worldwide and in
China
986
I. Karabegovic

the society organized in such manner that the service activities in all segments will be
performed by “intelligent” service robots.
4
The Role of Service Robots in Medical Institutions
To help you understand the beneﬁts that service robots of second, third and fourth
robotic revolution bring in all segments of society in the world we will illustrate several
applications of medical service robots that are today changing the image of general
health system. For example: in terms of logistics within the medical institutions, which
are used to supply the patient with food, medicine, supplies, waste disposal, etc.,
rehabilitation service robots, different surgical systems, in orthopedics, in endoscopy, in
spot ventilation, in colonoscopy, serving patients, perform delicate surgical procedures,
delivery of neuro-rehabilitation therapy for patients who have experienced stroke, teach
children with disabilities in learning, allow visits to patients, guiding and monitoring
operating procedures remotely (telesurgery). Let us state some of the examples of
service robots that assist patients and elderly, such as two service robots displayed in
Fig. 7 [24–26].
Figure 7 depicts two service robots that help elderly to communicate with outside
world and medical staff, when needed. The ﬁrst robot named ‘Giraffe’ which is remote
controlled, with wheels to move, the camera and the monitor, allows two-way video
call similar to “Skype”, while management and control is performed via computer. The
other robot developed by “Anybots Inc.” allows them to communicate with their
patients and thus eliminate the long-term home visits. All these robots are focused to
help the elderly and patients in their treatment. The third robot is a little robot called
“MABU” that is communicating with the patient, and the conversation can last a
minute, two, ﬁve or ten, depending on the patient and what he wants to talk. The same
robot has touch sensitive monitor on the front so that it can be used to display infor-
mation. Using robots with patients is much more practical, which is also supported by
psychologists who have studied this relationship for decades, due to information they
obtain by phone, tablet, or computer. In addition to these robots, there are already
several surgical system developed that are now being used in medical facilities, of
which we mention a few: the precision of the robot is far greater than man’s, post
Fig. 7. Mobile service robots for elderly to communicate with them and to the outside world
Research and Development of New Generation
987

operational period is far shorter when surgery is performed by the robot than the
surgeon, etc. [27–29]. We show a couple of applications for rehabilitation of patients
who have had fractures or strokes and have experienced cancellation of motor functions
of certain muscles. Few applications of service robots for rehabilitation are shown in
Fig. 8.
As already mentioned people who survive a stroke may experience partial or
complete motor paralysis of one or both sides of the body. Reinstatement of motor
control of the upper extremities in these people is very important for their independence
and improvement of life quality. In the beginning of recovery, patients are often unable
to perform functional movements, and they need some form of assistance. This
assistance comes from the therapist, which presents tedious and monotonous task and
service robots are purposefully designed for that purpose, depending on the extremities.
The advantage of service robots for rehabilitation is to be able to assist in the execution
of movements in a way that is repeatable and can be adapted to the needs of the patient.
Another important segment is that these systems offer the possibility of turning virtual
reality and video games in the process of rehabilitation of the patient. In Fig. 8, on the
left and right side we can see two construction service robots for rehabilitation, or
restoring the feet and hands that have lost motor service. In the middle of Fig. 8 there is
service robot “Exoskeleton” that gives paralyzed patients, or if patients had spinal cord
damage and lost functional ability of normal walking, ability to walk. In recent years, a
series of applications of rehabilitation robots was developed, that are already in
operation in medical facilities. Digital technology, ICT technology and sensor tech-
nology have helped the development of robotic technology that has resulted in the
development of various applications for service robots that are used in medical insti-
tutions, thereby changing the picture of the general health of the world in a positive
way.In this paper, we were not able to list and describe all service robots, but we will
mention the service robots that are used for logistics in medical institutions and are
already largely in function and used for the various purposes. One of construction of
such service robots is shown in Fig. 9.
Fig. 8. Service robots for patient rehabilitation and recovery
988
I. Karabegovic

Figure 9 shows service robot “TUG” that delivers and transports all the necessary
objects for patient care: food, medication, post, laboratory results, necessary material,
bed linen, sterilization of instruments, garbage disposal, etc. Service robots are com-
pletely independent in logistics for a speciﬁc location, and all control and monitoring is
done from a single location in the information system, as shown in Fig. 9. This method
of logistics in medical facilities is very complex and demanding, and creates signiﬁcant
savings, efﬁciency and satisfaction among workers in medical institutions. The intro-
duction of such system for the logistics in medical institutions has a number of beneﬁts:
improved utilization of space through the reduction of room for storage of materials,
improved safety of workers and labor cost reduction, increased productivity, ability to
work in the existing building construction of medical facilities, because they work with
the existing interior design, these systems do not require new infrastructure building,
and ultimately they reduce the time of delivery of certain material.
5
Conclusion
The development of digital technologies, ICT technology, sensory technology, and
new materials has led to the development of robotic technology, so as to develop a
second generation industrial robots, but also to developed many applications for service
robots that are used for various purposes, most interesting one being their applications
in medical institutions. The introduction of the service robotics in the medical insti-
tutions, starting from logistics, surgical system, a system for diagnosing, a system for
radiation therapy, a system for rehabilitation and assistance for the elderly, facilitates
general health to become better and gives it a dimension of safety and quality. The
world is continually investing in research and development of service robots that will
be used to help the population to make their lives better and improved, so that in the
future we will develop robots that will be able to judge and make decision indepen-
dently, which marks the time of fourth robotic revolution and intelligent robots. As we
have seen service robots are already represented in medical institutions and in all
aspects of medical care and protection of human population. However, this is not
enough because the research and development of new applications is in constant
progress. The development of artiﬁcial intelligence with the rest of the mentioned
technologies goes in the direction of creating humanoid service robots that will be able
Fig. 9. Service robots for logistics in medical institutions [26]
Research and Development of New Generation
989

to communicate with people, work with them and be of beneﬁt and assistance nec-
essary to man when performing certain tasks. While developed countries in the world
adopt their development strategies towards digital technology, ICT and other advanced
technologies in production processes in order to obtain “intelligent factory”, Japan as a
country has a strategy for the development of robot technology that aims to reach the
next-generation robots as soon as possible. The reason for this is that population in
Japan has high percentage of older people and this number will increase in the future,
because every older person who is sick or aged ties three healthy persons to himself. In
order for these three people to be relieved, the goal is to replace them with humanoid
service robots. Service robots have already been of great help to medical institutions,
and in the coming period, this help will increase so that it will give a better service and
be more proﬁtable compared to today.
References
1. Guang-Zhong, Y.: The next robotic industrial revolution, manufacturing robotics, robotics
and autonomous systems (RAS). UK-RAS Manuf. Rev. 2–17 (2015–2016). ISSN
2398-4422. www.ukras.org (2016)
2. Sulavik, C., Portnoy, M., Waller, T.: How a New Generation of Robots is Transforming
Manufacturing. Manufacturing Institute USA, Sept 2014. Gaithersburg, USA, pp. 1–13
(2014)
3. Bunse, B., Kagermann, H., Wahister, W.: Industrija 4.0. Smart Manufacturing for the Future,
Germany Trade & Invest, Berlin, Germany (2015)
4. Robotics 2020 Strategic Research Agenda for Robotics in Europe, Produced by euRobotics
aisbl, Draft 0v42 11/10/2013, pp. 25–43. http://www.eurobotics-project.eu
5. Howe, R., Matsuoka, Y.: Robotics for surgery. Annu. Rev. Biomed. Eng. 1, 211–240 (1999)
6. Karabegović, I., Doleček, V.: Servisni roboti. Društvo za robotiku Bihać, Bihać (2012)
7. Doleček, V., Karabegović, I.: Robotika. Tehnički fakultet Bihać, Bihać (2002)
8. Buchmeister, B., Friscic, D., Palcic, I.: Impact of demand changes and supply chain’s level
constraints on bullwhip effect. Adv. Prod. Eng. Manag. 8(4), 199–208 (2013)
9. Dev Anand, M., Selvaraj, T., Kumanan, S., Ajith Bosco Raj, T.: Robotics in online
inspection and quality control using moment algorithm. Adv. Prod. Eng. Manag. 7(1), 27–38
(2012)
10. Dev Anand, M., Selvaraj, T., Kumanan, S.: Fault detection and fault tolerance methods for
industrial robot manipulators based on hybrid intelligent approach. Adv. Prod. Eng. Manag.
7(4), 225–236 (2012)
11. Karabegović, I., Karabegović, E., Husak, E.: Application of service robots in rehabilitation
and support of patients. Časopis Medicina ﬂuminensis 49(2), 167–174 (2013)
12. Karabegović, I., Karabegović, E., Husak, E.: Service robot application for examination and
maintaining of water supply, gas and sewage systems. Int. J. Eng. Res. Dev. 2(4), 53–57
(2012)
13. Karabegović, I., Husak, E., Đukanović, M.: Applications intelligent systems-robot the
manufacturing process. In: 19th Conference Information Technology—IT 2014, pp. 177–180.
Faculty of Electrical, Engineering University Montenegro, Žabljak (2014)
14. Wolka, D.W.: Roboter sisteme, Technishe Universität des Saarlandes im Stadtwald (1992)
15. Freund, E., Stern, O.: Robotertechnologie I. Institut für Roboterforschung, Dortmund (1999)
990
I. Karabegovic

16. Buess, G., Schurr, M., Fischer, S.C.: Robotics and allied technologies in endoscopic surgery.
Arch. Surg. 135, 229–235 (2000)
17. Smith, J., Bray, W.L.: Robotics in Urologic Surgery. New York (2008)
18. Vanja Brozović: Medical Robots. In Tech (2008)
19. Tony Hyland: Scientiﬁc and Medical Robots. Black Rabbit (2007)
20. World Robotics: United Nations, IFR Statistical Department, c/o VDMA Robotics +
Automation, New York and Geneva (2015)
21. World Robotics: United Nations, IFR Statistical Department, c/o VDMA Robotics +
Automation, New York and Geneva (2014)
22. World Robotics: United Nations, IFR Statistical Department, c/o VDMA Robotics +
Automation, New York and Geneva (2013)
23. World Robotics: United Nations, IFR Statistical Department, c/o VDMA Robotics +
Automation, New York and Geneva (2010)
24. http://robots.net/. Accessed 2 Feb 2017
25. https://blog.universal-robots.com/how-service-robots-are-delivering-hope-and-helping-
patients-rehabilitate. Accessed 3 Feb 2017
26. https://www.youtube.com/watch?v=sMGENBEhYTM. Accessed 14 Feb 2017
27. https://blog.centerforinnovation.mayo.edu/. Accessed 3 Feb 2017
28. https://www.ﬁtness-gaming.com/news/health-and-rehab/armeo-power-robotic-training.html.
Accessed 5 Feb 2017
29. http://www.intouchhealth.com. Accessed 5 Feb 2017
30. http://www.ifr.org/news/ifr-press-release/ifr-round-table-on-the-future-of-robotics-153/.
Accessed 30 Jan 2017
31. http://www.spectrum.org/automaton/robotics/home-robots/where-are-the-eldercare-robots.
Accessed 2 Feb 2017
Research and Development of New Generation
991

Rheological Models and Numerical Method
for Simulation of Blood Flow
Ejub Džaferović1(&), Muris Torlak1, Almin Halač1,
and Amra Hasečić2
1 Univerzitet u Sarajevu - Mašinski fakultet, Vilsonovo šetalište 9,
71000 Sarajevo, Bosnia and Herzegovina
dzaferovic@mef.unsa.ba
2 JP Elektroprivreda BiH d.d. Sarajevo, Vilsonovo šetalište 15, 71000 Sarajevo,
Bosnia and Herzegovina
Abstract. This paper describes a method and results of computational ﬂuid
dynamics (CFD) simulation of blood ﬂow. Material properties of blood are
assumed to be constant, homogeneous and isotropic. Blood is regarded as vis-
coplastic liquid, where two different rheological models are applied: Bingham
and Casson model. Plastic viscosity and yield stress are given as a function of
hematocrit. The ﬂow regime is considered as laminar. Having applied rheo-
logical models to time dependent balance equations of mass and momentum
conservation given in integral form, a ﬁnite-volume method is used for dis-
cretization. Discretization results in a set of systems of linearized algebraic
equations which are solved individually, for blood velocity components and
blood pressure, at every time step within a considered time interval. The method
is applicable to domains of arbitrary shapes and unstructured computational
meshes. The examples presented include: (a) pulsatile viscoplastic ﬂow in a pipe
representing a simpliﬁed blood vessel, where the solutions obtained with the two
rheological models are compared to the numerical and analytical solution
obtained with Newtonian liquid, as well as (b) blood ﬂow in aorto-renal
bifurcation and carotid artery branch. In analysis of the ﬂow in the branch, three
geometric models are tested: idealized bifurcation with the branch angle of 60°
and 90°, and a realistic shape of the bifurcation.
Keywords: Blood ﬂow  Viscoplastic liquids  Rheology  CFD simulation
Finite-volume method
1
Introduction
Investigation and understanding of the blood ﬂow dynamics in blood vessels have
gained considerable importance in the recent years. Hemodynamic properties,
including velocity, pressure and wall shear stress, play an important role in research on
vascular systems. Occlusion as well as obstruction and blockage of the ﬂow frequently
lead to disease, damage and fatal consequences. They arise especially in vessels of
complex geometric shapes and/or in vessels with relatively high blood ﬂow rates,
speciﬁcally, in coronary, carotid, abdominal and femoral arteries, where also the
aforementioned ﬂow properties exhibit strong spatial and temporal variations.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_83

Understanding relations between these ﬂow properties’ variations, the blood ﬂow and
vessel behaviour may help ﬁnding appropriate treatment and therapy.
Practically, blood ﬂow is always time-dependent, and its dynamic nature may cause
important differences as compared to usually analysed steady-state ﬂows. Generally, it
can be regarded as incompressible and laminar. Under circumstances, it may also be
considered as turbulent, such as in the ascending aorta, in the branch regions of large
arteries, in narrowed parts of blood vessels or around heart valves. However, its main
distinction from usual liquid ﬂows found in natural systems and engineering applica-
tions is non-Newtonian behaviour. Unlike in commonly found ﬂuids like water, oil,
milk or gasoline, the dependence of shear stress and shear strain rate is not linear.
This paper presents a mathematical model of blood ﬂow which takes into account
its non-Newtonian behaviour using two different rheological models for viscoplastic
ﬂuids: Bingham and Casson model, as well as a numerical method for its solution using
ﬁnite-volume discretization.
2
Mathematical Model and Numerical Method
Mathematical model emanates from conservation laws of continuum mechanics and
includes conservation of mass, conservation of linear momentum and conservation of
space (the latter is applied in cases where moving or deformable walls are calculated)
[1, 2]. The conservation laws apply to all ﬂuids, and herewith they are also applicable
to blood ﬂows.
Bingham model describes bilinear relation between the shear stress and the strain
rate, and can be written in form which delivers explicit expressions for dynamic vis-
cosity of the liquid:
l ¼
l0 þ
s0
2
ﬃﬃﬃﬃﬃ
II _D
p
for
1
2 Td : Td


[ s2
0
1
for
1
2 Td : Td


 s2
0
(
ð1Þ
Casson model delivers a non-linear relation of shear stress and strain rate, and is
found to be appropriate to describe rheological behaviour of blood:
l ¼
1ﬃﬃﬃﬃﬃ
II _D
p
l2
0II _D

0:25 þ
ﬃﬃﬃ
s0
p
ﬃﬃ
2
p
h
i2
for
1
2 Td : Td


[ s2
0
1
for
1
2 Td : Td


 s2
0
8
<
:
ð2Þ
_D¼ 1
2 grad v þ grad v
ð
ÞT
h
i
where l0 is the plastic viscosity, s0 is the initial yield stress needed to initiate the ﬂow,
II _D is the second invariant of the shear strain rate tensor D, Td is the deviatoric part of
the stress tensor, and v is the velocity vector.
Material properties of blood are assumed to be constant (except the dynamic vis-
cosity which is solution dependent, and herewith it may be variable in space and time),
Rheological Models and Numerical Method
993

homogeneous and isotropic. The plastic viscosity and the yield stress are given as a
function of hematocrit h [3]:
l0 ¼
l0p
1  h=100
ð
Þ2;5 ; s0 ¼ 0:00625h
ð
Þ2;
ð3Þ
where l0;p is the viscosity of the blood plasma.
The part of space under consideration is divided into a set of adjacent,
non-overlapping cells of polyhedral shapes, building thus an unstructured numerical
mesh. The adopted constitutive relation is applied to the conservation equations of
mass and linear momentum, written in integral form for each cell in the numerical
mesh. A ﬁnite-volume discretization described by Demirdžić and Muzaferija [1] and
Ferziger and Perić [2] is then applied to convert the time-dependent integro-differential
equations into a set of non-linear algebraic equations. At every time step within a
considered time interval, the set is separated into subsystems of equations for each
solution variable: velocity components and pressure. Temporary decoupling and lin-
earization of the subsystems are performed within an iterative procedure, which also
accommodates implementation of non-linear viscosity nature described by Eqs. (1) and
(2) [4], as well as velocity-pressure linkage employing SIMPLE algorithm [5], and the
subsystems are solved sequentially in turn. Upon a convergence criterion is reached,
the solving process proceeds to the next time step.
3
Examples
3.1
Pulsatile Viscoplastic Flow in a Pipe
Time-dependent, pulsatile ﬂow through a 40 mm long pipe with diameter of 4 mm is
calculated. Uniformly distributed axial velocity across the pipe is prescribed at the inlet,
while its temporal variation is given in form of a sine function:
u tð Þ ¼ u 1 þ sin 2pt
T


:
ð4Þ
This form of the sine function implies that the prescribed inlet velocity is
non-negative, i.e. there is no backﬂow at the inlet.
The mean axial velocity value of 0.135 m/s and pulsation period of T = 0.2 s are
speciﬁed. The ﬂow is calculated for a Newtonian liquid whose dynamic viscosity is
0.0032 Pas, as well as for a Bingham and Casson ﬂuid, whose plastic viscosity is also
0.0032 Pas and the initial yield stress is 0.0375 Pa. A structured computational mesh
with 150 cells in axial and 20 cells in radial direction is generated.
Figure 1 shows distribution of the axial velocity of developed ﬂow across the pipe
diameter at four distinct instants of time, calculated for Newtonian ﬂuid ﬂow and
compared to the corresponding analytical solution of Womersley [4]. Agreement of the
results is evident, with maximum deviation of less than 5%, implying a good accuracy
of the base model setup.
994
E. Džaferović et al.

Figure 2 shows comparison of the developed ﬂow solutions obtained using
non-Newtonian models with that obtained using the Newtonian one. The axial velocity
distribution across the pipe diameter at four distinct instants of time is displayed again.
In the ﬁrst half of the pulsation period, agreement of the investigated models is
apparent. In the second half, differences are remarkable. Particularly, the differences
obtained with Casson model in the central part of the pipe (considerably lower axial
velocity) at the time t/T = 0.875 are noticeable. In a number of published works, blood
ﬂow is, probably for simplicity reasons, simulated as Newtonian or Bingham ﬂuid,
although its rheologic behaviour is better described by Casson model. The here pre-
sented results clearly indicate that the choice of rheological model strongly affects the
pulsating ﬂow solution, and may trigger inconclusive ﬁndings if the model is not
adopted appropriately.
In addition to that, the backﬂow near the wall is detected in the third quarter of the
pulsation period, and herewith the wall shear stress becomes negative at the location of
the extracted proﬁle, even though the inlet velocity is non-negative. Such a dynamic
behaviour may cause additional dynamic load to the vessel wall.
numerical soluƟon
(a)
(b)
(c)
(d)
Fig. 1. Axial velocity proﬁles of a Newtonian liquid in a rigid pipe at different instants of time:
a t/T = 0.125; b t/T = 0.375; c t/T = 0.625 and d t/T = 0.875
Rheological Models and Numerical Method
995

3.2
Pulsatile Blood Flow
3.2.1
Idealized Bifurcation at Two Different Branch
Angles—Aorto-Renal Branch
The model setup is adapted according to the experimental conditions described by
Tokunori et al. [6], where a simpliﬁed model of aorto-renal branch is studied. Diameter
of the abdominal aorta is 20 mm, while the diameter of the renal artery is 6 mm. Flow
velocity is assumed to have parabolic distribution at the inlet. Temporal variation of the
inlet velocity is described using the following periodic function:
u tð Þ ¼ 0:15 þ Ui  0:15
ð
Þ sin 2pt
T
ð5Þ
where the mean velocity is Ui = 0.6718 m/s and the pulsation period is T = 0.75 s. The
outlet ﬂow rates through the abdominal and the renal aorta are split in proportion of
91:09, respectively. Blood is modeled as a Bingham ﬂuid, where the values in the
constitutive relation s0 and l0 are obtained from the assumed hematocrit value for
normal blood h = 43% and the blood plasma viscosity l0;p = 0.00125 Pas [3].
Fig. 2. Axial velocity proﬁles of a non-Newtonian liquid in a rigid pipe at different instants of
time: a t/T = 0.125; b t/T = 0.375; c t/T = 0.625 and d t/T = 0.875, calculated using two
different rheologic models: Casson (magenta) and Bingham (yellow) ﬂuid, and compared to
Newtonian ﬂuid solution (blue)
996
E. Džaferović et al.

Two different branch angles are considered: 90° and 60°, see Figs. 3 and 4.
In the ﬁrst case, separation and recirculation on the upstream side of the renal artery
are observed, which is also reported in experimental results [6]. The ratio between the
maximum backﬂow velocity and the maximum main stream velocity is 0.4, while this
ratio according to the experimental results is 0.3 ± 0.1.
In the latter case, with the branch angle of 60°, the recirculation is rather thin and
small, so that it can be practically neglected.
3.2.2
A Realistic Shape of Bifurcation of Carotid Artery
Simulation of blood ﬂow in a realistic shape with complex geometry is demonstrated in
the case of carotid artery. Blood vessel walls have irregular surface shape with rela-
tively large variations of the cross section area. In addition to that, the here considered
parts of the carotid artery form a branch. Blood vessel diameter at the inlet, as well as
the diameters at the both outlets are assumed to be 6.2 mm. The same periodic
inlet-velocity condition is used as in the case of aorto-renal branch. The outlet sections
are deﬁned as zero-pressure boundaries. Blood is modelled as Casson ﬂuid with the
same l0 and s0 values as in the previous case.
Figure 5 shows distribution of the velocity magnitude as well as pressure distri-
bution over the blood vessel wall. Strong separation and recirculation of the blood ﬂow
from the vessel walls is seen in the regions of abrupt expansion of the cross-section
area. Also negative pressure values are obtained in the regions of strong contraction.
separation region
Fig. 4. Velocity vectors in longitudinal section of a simpliﬁed model of aorto-renal branch at
two different branch angles 90° and 60°
Fig. 3. Numerical mesh in idealized bifurcation at the branch angle 90° (left) and 60° (right)
Rheological Models and Numerical Method
997

4
Conclusions
In this work, a mathematical model for computational ﬂow analysis with two different
constitutive relations for viscoplastic ﬂuids is introduced, solved using ﬁnite-volume
method, and applied to several examples of pulsating ﬂow such as those appearing in
blood vessels.
Numerical solutions of the ﬂow in a simple pipe obtained with the Newtonian
model and with the two models of viscoplastic ﬂuid are compared to the analytical
solution obtained for Newtonian ﬂuid. The comparison conﬁrms applicability and
plausibility of the implemented models. The non-Newtonian models are also applied to
geometric domains of complex shape arising in aorto-renal branch and in a branch of
carotid artery.
Numerical solution provides detailed insight into the blood ﬂow structure indicating
the regions with signiﬁcant wall pressure and wall shear stress changes which may
directly lead to blood vessel damages. It also detects the blood ﬂow separation and
recirculation. These regions are supposed to be the places where occlusion by plaque
may develop.
References
1. Demirdžić, I., Muzaferija, S.: Numerical method for coupled ﬂuid ﬂow, heat transfer and
stress analysis using unstructured moving meshes with cells of arbitrary topology. Comput.
Methods Appl. Mech. Eng. 125, 235–255 (1995)
2. Ferziger, J.H., Perić, M.: Computational Methods for Fluid Dynamics, 3rd edn. Springer,
Berlin, Heidelberg (2003)
3. Fung, Y.C.: Biomechanics—Mechanical Properties of Living Tissues, 2nd edn. Springer,
New York (1993)
4. Džaferović, E.: Interakcija viskoplastičnog ﬂuida i viskoelastičnog čvrstog tijela – numeričko
modeliranje. Ph.D. thesis, Univerzitet u Sarajevu, Mašinski fakultet (2002)
separation regions
Fig. 5. Blood ﬂow in a realistic bifurcation model of a carotid artery. Distribution of velocity
magnitude in the longitudinal section plane (left) and vessel wall pressure (right) at the instant of
time t/T = 0.25
998
E. Džaferović et al.

5. Patankar, S.V., Spalding, D.B.: A calculation procedure for heat, mass and momentum
transfer in three-dimensional parabolic ﬂows. Int. J. Heat Mass Transf. 15(10), 1787–1806
(1972)
6. Tokunori, Y., Yasuo, O., Akihiro, K., Hiroyoshi, T., Osamu, H., Katsuhiko, T., John, M.L.,
Kim, H.P., Cristopher, J.J.H., Caro, C.G., Fumihiko, K.: Blood velocity proﬁles in the human
renal artery by doppler ultrasound and their relationship to atherosclerosis. A T. Vasc. Biol.
16, 170–177 (1996)
Rheological Models and Numerical Method
999

Heuristic Optimization Methods in Industrial
Robotics
Ermin Husak(&) and Isak Karabegović
Technical Faculty, University of Bihać, 77000 Bihać, Bosnia and Herzegovina
erminhusak@yahoo.com, isak1910@hotmail.com
Abstract. In industrial robotics is signiﬁcant numbers of problems that can be
optimized which can improve functioning of industrial robots. During the time
signiﬁcant number of optimization methods have been developed in optimiza-
tion with all their advantages and disadvantages. In this paper are presented
genetic algorithm, particle swarm optimization and ant colony as heuristic
optimization methods that can be used in industrial robotics.
Keywords: Optimization  Industrial robotics  Heuristic methods  Population
methods
1
Introduction
Each stage in design of industrial robots and application of robots gives possibilities to
implement optimization. There are different examples as optimizations of gearbox size and
arm lengths from an acceleration capability perspective [1] or trajectory optimizations [2].
Heuristic optimization methods are methods that not demand strict logical postu-
lates but even in that case can ﬁnd good optimal solution or objective function. But in
some cases solution can be far from optimal. These methods are divided on methods
with deterministic character and stochastic character. In methods with stochastic
character are mostly consider population methods such are Genetic Algorithm, Particle
Swarm Optimization and Ant Colony Optimization [3, 4].
Population methods are based on some characteristic and behaviour of biological
systems, molecular systems, neurobiological systems, systems of swarms and insects etc.
Genetic algorithm method is established at principles of natural genetic and natural
selection.
Ant colony optimization method is based at cooperative behave of real ant colonies.
Particle swarm optimization method is established at behaving groups of living
creatures as are swarm of insects, ﬂock of birds, swarm of bats etc.
Population methods have been interesting for researcher in last 30 years. Genetic
algorithm and application of this method on optimisation for different problems is the
most published method of population methods. Mostly because it is oldest of these
methods and good results that gives for different application. Ant colony optimization
and particle swarm optimization are methods published recently but attracted a lot
of attention. Population methods differently than local methods and also classical
gradient methods not use starting solutions but rather starting population which ensures
that optimisation process not ﬁnish in local optimum to fest.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_84

2
Genetic Algorithms
Genetic algorithms were suggested as optimization method by American scientist John
Henry Holland in beginning of seventies in last century [3]. This method is based on
theory that in nature survive only ﬁttest. Fittest are the ones with the best genetic
material which they inherit from the previous generations. Process of optimization
begins with a deﬁning of size of ﬁrst generation which is random. Generations can have
from several to several hundred units. Units are in this case chromosome. In mostly
cases size of generation is kept constant. From ﬁrst generating should be extracted next
generation with same number of units. There are several procedures and operations that
have to be done (Fig. 1).
First ﬁtness value has to be determined for every unit by criteria of the ﬁtness
function. Fitness function can be same as objective function. But in some cases not.
Founded by results of ﬁtness values from every unit there are different methods for
selecting units for being parent for next generation. One of frequently used is
Roulette-wheel selection showed by Fig. 2.
Evolution 
search
Fittnes 
evaluation
Crossover 
Selection
Mutation
Solution 
Coding 
Objective function
Evolution operators
Specific knowledge 
Problem 
Fig. 1. Genetic algorithm optimization process
20%
11%
5%
15%
9%
38%
1 
2 
3 
4 
5 
6 
Wheel 
Fitness 
value 
Pointer 
Units 
Fig. 2. Roulette-wheel selection
Heuristic Optimization Methods in Industrial Robotics
1001

Fitness value of every unit can be presented in percentage as a share of total sum for
all ﬁtness values. Units with higher value of parentage has bigger chance to be selected
on roulette wheel to be a parent.
After selection who will be the parent, next step is reproduction. Reproduction is
like in real life getting children. For this step recombination as operator is used [3, 5, 6].
That means that we have to take part of genetic material from the father and from the
mother side (Fig. 3).
To avoid to optimization process fast converge to solution mutation operation can
be implemented. Mutation means changing one or several genes in chromosome string
(Fig. 4).
After new generation has been formed same procedure is repeated. Optimization
process ends when number of generation is reached or there is no improvement in the
ﬁtness functions.
3
Particle Swarm Optimization
Particle swarm optimization is typical example of population methods which method of
searching in space of solutions is strictly stochastic [7]. This method was presented by
James Kennedy and R. C. Eberhart in 1995. Characteristic behaviour of the ﬂock of the
Parents
Children
Fig. 3. Crossover operator
Mutation of part of 
chromosome
Fig. 4. Mutation operator
1002
E. Husak and I. Karabegović

birds and their behaviour during searching for the food was used to deﬁne algorithm of
this optimization method which shows Fig. 5.
Bird during searching for the food try to adjust position in the ﬂock in a way that
not to just follow the ﬂock rather to move on own experience but always adjusting to
ﬂock movement. In this case bird using own experience and experience of the whole
ﬂock in searching for the food. In following several ﬁgures it can be seen how birds
changing position in each iteration of population bird moving.
For the ﬁrst generation all ﬁtness value have to be calculated for every bird position
(Fig. 6). Fitness value depends how bird is close to the food source. Depending of the
best position which one of the birds have and position of the others birds new position
for new generation will be calculated in which birds are moving (Fig. 7).
Fig. 5. Flock of the birds and ﬁshes
The nearest 
particle (bird)
Food 
Searching space
Fig. 6. First generation
Heuristic Optimization Methods in Industrial Robotics
1003

As can be seen from the Fig. 6 all birds start to moving to the food source which
mean that process of optimization are coverage. Finally because most of the birds are
near the food source which is optimum value it is question how to stop the optimisation
process.
In case of partial swarm optimisation method, optimization can be over by setting
number of generations or differences in values in objective functions (Fig. 8).
4
Ant Colony Optimization
ACO—Ant Colony Optimization is technique which deﬁnes how ants always ﬁnd the
shortest path between food source and anthill. Algorithm for this optimisation method
was presented by Alberto Colorni, Marco Dorigo and Vittorio Maniezzo where they
test it on the travelling salesman problem [4].
During of ant’s motion, ants deposit the pheromone trail. When other ants ﬁnd this
trail in most the cases ant will follow the trail. In Fig. 9 it can be seen how ant will ﬁnd
smoothest path if they are moving in both directions.
Fig. 7. Random generation
Fig. 8. Final generation
1004
E. Husak and I. Karabegović

Different paths between anthill and food source that every ant has to pass in one
iteration is named Tour. Number of tours can be used as criteria for ﬁnishing opti-
mization. Every ant has to pass his path by calculated probability. Probability which is
used for determining what of the path ant must pass depends by pheromone intensity of
the trail. Path with the best ant or ant which path is the shortness will have high level of
the pheromone trail compared with other trails which in next generation inﬂuences that
this path be selected by larger number of ants what will inﬂuence that optimization
process converge.
5
Conclusion
Population methods are the stochastic methods and have some advantages over gra-
dient methods especially from optimization problems that are not convex. In industrial
robots problems there are several problems which are not convex like optimization of
time trajectory planning and where this methods shows good results. Genetic algorithm
have possibility to be programed for constrained problems by equality and inequality
constrains separately instead of particle swarm optimization and ant colony which
constrains must be implemented by using penalty function.
References
1. Pettersson, M., Design optimization in industrial robotics, methods and algorithms for drive
train design. Linköping Studies in Science and Technology. Dissertations, No. 1170, 2008
2. Heim, A., Stryk, O., Trajectory optimization of industrial robots with application to computer
aided robotics and robot controllers. J. Math. Program. Oper. Res. (1999)
3. Holland, J.H.: Adaptation in Natural and Artiﬁcial Systems. MIT Press, Cambrige (1975)
Fig. 9. Optimal path for ants
Heuristic Optimization Methods in Industrial Robotics
1005

4. Colorni, A., Dorigo, M., Maniezzo, V.: Distributed optimization by ant colonies. In:
Proceedings of ECAL91-European Conference on Artiﬁcial Life, Paris, France. Elsevier
Publishing, pp. 134–142 (1991)
5. Brezočnik, M.: Uporaba genetskega programiranja v inteligentnih proizvodnih sistemih.
Univerza v Mariboru, Fakulteta za strojništvo, Maribor, ISBN 86-435-0306-1 (2000)
6. Syswerda, G.: Uniform crossover in genetic algorithms. In: Third International Conference on
Genetic Algorithms and Their Applications, pp. 2–9. Morgan Kaufmann, San Mateo, CA
(1995)
7. Gao, M., Ding, P., Yang, Y.: Time-optimal trajectory planning of industrial robots based on
particle swarm optimization. In: Fifth International Conference on Instrumentation and
Measurement, Computer, Communication and Control (2015)
1006
E. Husak and I. Karabegović

Impedance Control in the Rehabilitation
Robotics
Zlata Jelačić(&)
Faculty of Mechanical Engineering Sarajevo, University of Sarajevo,
71000 Sarajevo, Bosnia and Herzegovina
jelacic@mef.unsa.ba
Abstract. Physical interactions between patients and therapists during reha-
bilitation have served as motivation for the design of rehabilitation robots, yet
there is a lack in fundamental understanding of the principles governing such
human-human interactions. Review of the literature posed important open
questions regarding sensorimotor interaction during human-human interactions
that could facilitate the design of human-robot interactions and haptic interfaces
for rehabilitation. The goal is to use the leading principles of the human-human
interaction in order to deﬁne a way in which people could be in contact with
robots in a more intuitive and biologically inspired way. The proposed hybrid
impedance control solves the robot–environment contact problem and offers a
possible solution for the rehabilitation robot interaction problem.
1
Introduction
Of the three forms of human-human interaction, behaviour deﬁned by Jarasse [1],
cooperative interactions predominate in rehabilitation (i.e. education and assistance).
Here, leader-follower or teacher-student roles are predeﬁned. However, the vast
majority of the prior research involving human-human interface has focused on sen-
sorimotor collaboration. Here, two individuals share a common goal, but have no
pre-deﬁned roles.
While sensorimotor cooperation and education serve as a motivation and basis for a
large number of rehabilitation interventions by therapist and robot, there is almost no
literature which covers the question of the interaction force between the individuals in
those two contexts. For example, therapists often provide haptic feedback to the
patients by physically guiding their movements during the exercise. However, there is
little known about the way these forces should be applied in order to stimulate motor
learning (so-called cooperative sensorimotor education). In the research which focussed
on the forces applied by the therapists during sensorimotor training, Galvez [2] con-
cluded that experienced therapists each apply signiﬁcantly different forces during
training on the patients with spinal injuries, which resulted in the different leg kine-
matics. Ikeura [3] also examined the force characteristics during the sensorimotor
assistance task, which consisted of lifting and moving a light object over a distance of
15 cm. As in the research of Galvez [2], the aim of the experiment wasn’t the con-
tribution of the interaction force to the task performance, but the characterisation of the
forces in order to develop a controller to mimic this behaviour [3].
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_85

In future research it can be useful to check if the increased interaction force
stimulates improvements in the motor performance during dyad in contrast to the solo
performance. In order to further explore the role of the increased interaction force
between human partners and how it can inﬂuence the motor performance, one can think
of a simple experiment in which several conditions could be examined. In each state the
interaction force between the human partners is systematically different. The motor
performance of each state can then be compared in order to examine the role of the
increased
interaction
force
which
have
been
identiﬁed
during
human-human
interaction.
In comparison to the work of Galvez [2], which has focused on the characterisation
of the interaction force, the vast part of the research work in the ﬁeld of the sensori-
motor cooperation focuses on the performance improvement while ignoring the
interaction forces between the patient and the therapist or the trainer. In general,
sensorimotor cooperation or physical guidance has shown that there is an improvement
in the performance of the simple motor task, but these improvements usually aren’t
lasting and cannot be extrapolated to the individual overall performance. The appli-
cation of the continuous haptic guidance can undermine the development of the error
detection and the correction process, leading to a so-called haptic addiction as a way to
improve the performance of the motor task. Continuous haptic guidance can also limit
the variability of the movement, which is considered crucial in the motor learning
because it lets the participants explore the space of the possible movements. Perfor-
mance improvement generalisation from the combined to the individual task can be
more efﬁcient in the case of complex motor tasks which are more challenging and
especially if the security aspects are a priority. In those circumstances the haptic
guidance through cooperative sensorimotor education may increase patient conﬁdence
and limit safety concerns as well as provide individuals the opportunity to identify not
only the most effective but also the most efﬁcient movement strategies.
In the case of the rehabilitation the role therapist–patient is well deﬁned. Hence, the
appropriate way to analyse the problem of the rehabilitation robotics is through sen-
zorimotor cooperation. This research will focus on the contact problem analysis and
implementation of manipulator cooperation in the rehabilitation robot control design.
Currently there is a lack in framework and experimental data describing how humans
physically interact, much less how therapists guide patients during rehabilitation. As a
result, there is not a solid scientiﬁc understanding of how this can best be achieved or
what principles robotics should implement when designing robots for the purpose of
rehabilitation.
2
Review of the Current Rehabilitation Robotics
2.1
Target Population and Rehabilitation Robots
People with disabilities due to neurological damage, such as strokes and spinal cord
injuries, require therapeutic rehabilitation and assistance. Some of them need support
for performing daily living activities through assistive devices. A stroke occurs when
poor blood ﬂow to the brain results in death of cells. A spinal cord injury is an injury to
1008
Z. Jelačić

the spinal cord resulting in a change, either temporary or permanent, to the cord’s
normal motor, sensory, or autonomic functions. Muscular dystrophies are inherited
myogenic disorders characterized by progressive muscle wasting and weakness of
variable distribution and severity. Other neurological damages include cerebral palsy
and Parkinson disease that is characterized by poor motor control. These types of
neurological damages need physical therapy for mobility and occupational therapy for
daily living activities and work skills.
Rehabilitation is related to reaching and maintaining the level of disabilities. The
World Health Organization [4] deﬁnes rehabilitation of people with disabilities as a
process aimed at enabling them to reach and maintain their optimal physical, sensory,
intellectual, psychological and social functional levels. Disability is the outcome of the
interaction between health conditions and environmental and personal factors. Reha-
bilitation robotic devices are robots that are developed for providing rehabilitation
assistance but lack either the required number of programmable axes or a degree of
autonomy.
2.2
Trends of Rehabilitation Robots
Rehabilitation robotic devices can be divided into two categories: therapeutic reha-
bilitation robotic devices [5–7] and assistive robotic devices [8, 9]. Therapeutic robotic
devices play an important role in maintaining and/or improving the functional recovery
of a patient in hospitals. Assistive robots for daily living activities could be included in
rehabilitation robots in broad sense. Figure 1 represents the classiﬁcation of rehabili-
tation robots based on the purpose of use and the part of the body. The horizontal axis
corresponds with the purpose of use, such as therapeutic rehabilitation robots and
Fig. 1. Classiﬁcation of rehabilitation robots based on purpose of use and part of body (Source
WHO [4])
Impedance Control in the Rehabilitation Robotics
1009

assistive robots. Currently, therapeutic robotic devices have a larger market share than
assistive robotic devices. The vertical axis corresponds with the part of the body. The
major focus is on the upper and lower extremities.
The trends observed in commercialized rehabilitation robots are as follows. First, a
product lining, that offers several related products for sale individually, is constructed
for several related products. Some products are related to the stage of a patient’s
disease: acute, sub-acute, and chronic. The Armeo therapy concept includes Armeo-
Power, ArmeoSpring and ArmeoBoom [10]. The various rehabilitation devices for
mobility include Erigo for early rehabilitation and mobilization of patients, Lokomat
for intensive locomotion therapy and Andago for overground training [11]. First Mover
and G-EO are rehabilitation robotic devices for rehabilitation of diseases that are at an
acute stage and subacute stage, respectively [12] in lower extremities. Other examples
of rehabilitation robotic devices are related to the parts of the body: hands, arms and
legs, e.g., Motorika [13], Hocoma [11], and Reha Technology [12] for the upper and
lower extremities, and Tyromotion [14] for the ﬁngers, upper extremities and trunk. In
addition, affordable products, such as Lokomat Nano [11] and NexStep [12], and
premium products, such as Lokomat Pro [11] and G-EO [12], are available or will be
available in the market.
Second, low inertia and lightweight are important issues in rehabilitation robotic
devices. A high inertia can be an obstacle for weak people with disabilities in the lower
extremities [15, 16], ﬁngers [17] and upper extremities [18, 19]. For example, a high
inertia provides resistance to upper extremity exercises and thus persons with weak
muscle strength and coordination can be effectively trained under gravity compensation
on the part of body. The novel device, Proﬁcio, is an example of such a low inertia
robotic device [20].
Third, the purpose of the devices has shifted from therapeutic use to personal use,
i.e., to support daily living activities at home. Robotic devices designed for personal
use at home have a larger market share than those designed for therapeutic use in
hospitals or facilities. Several advanced techniques are being developed: balance [21],
upper limb movements classiﬁcation [22], fall detection [23] and wearable real-time
activity tracker [24]. Those techniques could be used at home as well as hospitals. The
market will exponentially increase if every disabled person has a personal robotic
device at home. For example, the market for lower exoskeleton robotic devices has the
potential to grow to a similar size as that of manual wheelchairs. There is increasing
competition between three types of leg exoskeletons, i.e., Rewalk [25–27], Ekso [28],
Indego [29, 30]. A training program for physical therapists, occupational therapists and
people with disabilities is an important issue for novel robotic devices, such as an
overground wearable exoskeleton at home, because of the difﬁculty of supervision or
assistance of therapists. In comparison to standalone systems, such as a treadmill and a
body weight support module-based robotic exoskeleton in hospitals or facilities, an
overground wearable exoskeleton at home needs more effort to train usage, i.e., sus-
taining balance.
Fourth, ease of use is an important aspect of the rehabilitation robotic devices. It has
been proven that in the case of wearable devices a quick on-and-off feature stimulates
patients to train more regularly. End-effector based robotic devices have a quicker on
and off feature than exoskeletal devices.
1010
Z. Jelačić

Other trends observed in rehabilitation robots are hardware and software
improvements including impedance control, assist-as-needed and intention reading for
dexterous interaction. Multi institutional trials of robotic devices can provide evidence
regarding the beneﬁts of such devices. Robotic modules can develop a synergy by
co-operating with different modules such as a treadmill, a body-weight supporter and
various sensors. Irrespective of whether the original purpose is to develop a rehabili-
tation device or not, the technology can be applicable to other areas such as assisting
disabled and elderly people and industrial and military use.
However, the vast majority of the currently available robotic rehabilitation devices
are passive or semi-passive and do not offer a proper replacement of the
therapist-patient interaction during training. For the rehabilitation to be efﬁcient, the
development of the rehabilitation robotics needs to focus on the human-human inter-
action and contact forces present during rehabilitation with an experienced therapist.
This lack of attention has inspired current research.
3
The Robot–Environment Interaction Problem
The second half of the twentieth century witnessed the rise of robotic and automated
systems in industry. In the last decade or so, with the advance of computers and with
the reduction of the costs of related hardware and software, robots ﬁnd use in different
ﬁelds such as agriculture, underwater and recently in households. Robots used in the
industry were initially only able to execute simple tasks (e.g. pick and place) due to the
lack of advanced sensing capabilities. Thanks to the progress in sensor technologies,
robotic systems are becoming more intelligent, and the environments in which they can
operate are gradually shifting from static towards dynamic ones. Examples of such
sensors are tactile and force sensors and vision systems (e.g. a camera). As robotic
systems become widespread in different domains, they are expected to execute diverse
and challenging tasks. In order to successfully accomplish them, such systems should
be robust and safe and they should demonstrate a sufﬁcient level of ﬂexibility.
A robotic or an automated system is considered to be robust, if it is capable to operate
under varying operating conditions without changing its initial structure. Robots used
in industrial applications should be safe in the sense that they should neither damage
themselves nor the objects present in their environments. Furthermore, for applications
concerning the robots working nearby human beings, the safety of the humans has to be
insured. Flexibility of a robotic system is its ability to be reassigned quickly and easily
in the case of changing manufacturing demands. One particular class of the afore-
mentioned systems are robotic manipulators, which are mechanisms composed by a
chain of rigid bodies (i.e. the links) connected by joints [31].
In this paper the focus is the control of the contact tasks based on the aforemen-
tioned terms of robust and ﬂexible control. An important part of the manipulation tasks
which include the physical interaction between the manipulator and the environment
are the so-called contact tasks. In order to successfully perform these tasks, manipu-
lators should improve the sensory capabilities (e.g. registration of the force during
interaction). In the following paragraphs, the control of the contact task and the
cooperative manipulation will be explained in detail.
Impedance Control in the Rehabilitation Robotics
1011

3.1
Control of Contact Tasks
Following the successful applications of manipulators in tasks where physical inter-
action with the environment is not the main intention, such as spot-welding, spray
painting and palletizing, it has become logical to start investigation of robot applica-
tions in contact tasks. Control of contact tasks has been investigated in the last three
decades [32, 33], with a particular desire to enhance autonomy of manipulators
operating in unstructured (or semi-structured) environments. In a structured environ-
ment, conﬁguration of objects with which the robot interacts is known precisely,
unintended collisions do not occur, the ambient conditions (e.g. lighting, temperature)
do not vary signiﬁcantly, etc. Applications like spot-welding and spray painting can be
executed using pre-planned motion proﬁles. Robot control strategies that consider only
desired motion proﬁles are less suitable to utilize in unstructured environments. This is
due to the fact that the success of this strategy depends heavily on accurate modelling
of the manipulator and the environment. Any modelling error or uncertainty eventually
results in less accurate motion planning and consequently unexpected contact
forces/moments may arise. In classical motion control, high bandwidth servo control
designs are used to increase robustness against modelling and parameter uncertainties
and disturbances. However, when both the manipulator and the environment are very
stiff, the contact forces/moments can reach very high values causing damage of either
one or both.
Risks of damage can be reduced if the manipulator can comply with the environ-
ment, i.e. if it can modify its response based on the contact force/moments. Compliant
behaviour can be achieved either by mechanical design or by analogue/digital control
or both. Grinding (see Fig. 2a), polishing, deburring and mechanical assembly (see
Fig. 2b) are examples of industrial applications that require a manipulator to be in
contact with the environment most of the time. Manipulators in domestic applications
(i.e. home robotics), that attract ever increasing attention in the recent years, are also
used to execute contact tasks such as wiping surfaces (see Fig. 2c) and opening doors
(see Fig. 2d).
Contact forces can be actively controlled in two different ways, indirectly or
directly [6]. Indirect schemes use motion control as an implicit mean to regulate the
contact forces whereas direct schemes utilize explicit force feedback loops [32].
Indirect techniques are impedance (or admittance) control [34] and stiffness (or com-
pliance) control (a simpliﬁed type of impedance control) [35]. The direct techniques
include hybrid motion/force control [36], inner/outer motion/force control [37] and
parallel position/force control [38]. Detailed modelling of the environment can be
avoided if indirect schemes are used, however the position tracking performance can
deteriorate [32]. Among the direct methods, hybrid motion/force control is quite
common, where its success depends on whether explicit constraint equations deﬁning
the environment geometry exist [33]. Another challenge for hybrid controllers is to
establish contact with the environment in a stable way [39].
In a complete contact task there are three phases, free motion, contact motion and
the transition phases. As the name suggests the ﬁrst one refers to the case where the
manipulator moves in spaces free of obstacles, the second one is related to motion
along certain surfaces, whereas the last one considers the transitions to and from free
1012
Z. Jelačić

and contact motion phases which involve impact phenomena [40]. An important
problem associated with the control of contact tasks is the transition phase in which the
manipulator comes from free motion into contact with the environment. Successfully
completing the transition phase is important to execute a complete contact task.
There are many factors that affect the robustness and performance of a manipulator
in contact tasks. These are in general related with availability of different sensory
feedback data, and knowledge of models of the manipulator and the environment.
Many variants of indirect and direct contact force control algorithms can be found in
the control literature. Control algorithms for contact tasks can be classiﬁed as
non-model (or model free) based, model-based, adaptive, robust and robust-adaptive
schemes. This classiﬁcation is made based on several criteria. First, the properties of
the dynamic model of the manipulator used in the stability analysis are determined.
Whether, how and to what extent such a model is used in the control design is
investigated. Second, the mechanical properties of the end-effector used in this model
are determined. This is done by checking whether the effect of compliance, be it due to
a force sensor or another source such as a soft cover is included to the model. Third,
they are classiﬁed into categories depending on the way they deﬁne the desired tra-
jectories
(known-distorted-modiﬁed)
and/or
they
decompose
the
task
space
(a) Grinding [41]
(b) Piston insertion [42]
(c) Table wiping [43]
(d) Robot opening a door of a microwave
[44] 
Fig. 2. Examples related to control tasks [41–44]
Impedance Control in the Rehabilitation Robotics
1013

(estimated-measured online-identiﬁed online) [45]. Next, the mechanical and geometric
properties of the environment the manipulator is supposed to make contact with are
classiﬁed. This classiﬁcation is done based on whether the environment is modelled as
compliant or (idealized) rigid one. After that special attention is given to the rotational
parameterizations. This is important for contact tasks that consider not only contact
forces but also contact moments (or torques) since characterization of rotational contact
parameters (e.g. stiffness) is not as straightforward as for translational ones [19]. Next
considerations are type of measurements/estimations and whether the effect of mea-
surement noise or estimation error is taken into account in the control design and
stability analysis. The achieved stability results are classiﬁed regarding free and contact
motion and transition phases of the contact task.
In the last decade, there has been an increasing interest on intentionally introducing
mechanical compliance in the design of manipulators for service applications. This is
driven by the desire to increase safety, to damp the impact forces and to provide a better
force/torque transmission to the manipulators’ joints by reducing the effects backlash,
dry friction, etc. Examples of such designs that can be found in the literature are series
elastic actuators (SEA) and variable stiffness/damping/impedance actuators (see [46]).
These devices usually have additional internal control loops to regulate the torques
delivered to the joints, or joint stiffness/damping/impedances.
3.2
Cooperative Manipulation
Multi-arm robotic systems are a popular subject of active research in recent years [47,
48]. These systems become required due to limited payload capacity of single arm
systems in certain tasks and need for additional equipment (e.g. ﬁxtures) besides the
single arm manipulators. Practical examples are heavy payload transportation and
ﬁxtureless multi-part assembly in industry and in space, or folding of cloths and
preparing meals in the domestic domain (see Fig. 3). Cooperative manipulators can
have signiﬁcant advantages compared to a single robot. If multiple manipulators are
used to carry a heavy or large payload, for example, the weight can be distributed
among several smaller and cheaper robots and the payload can be handled more safely.
Mechanical assembly, an important process in many industries (e.g. automotive), can
be performed faster and ﬂexibly. Special ﬁxtures whose main purpose are to support
certain parts of the assembly are often used in this process. With the help of multiple
manipulators where one or more play the role of the ﬁxture, the number of special
ﬁxtures can be reduced or ultimately their use can be eliminated completely. In many
cooperative tasks, the manipulators grasp a common object and also bring it into
contact with the environment. Some examples are scribing, painting, grinding, pol-
ishing, contour following and object aligning.
Although in cooperative manipulation usually a commonly grasped object is
considered, in [49] a distinction is made between non-coordinated (i.e. each arm per-
forms different tasks), coordinated (i.e. each arm performs a different part of the same
task) and bimanual (in the case of two manipulators) manipulation. The analysis and
control of the ﬁrst class is the same as in the case of individual manipulators for which
there is an abundance of literature.
1014
Z. Jelačić

Cooperative manipulation tasks can be categorized according to the grasp points as
ﬁxed and non-ﬁxed [49]. In the ﬁrst case, it is assumed that the object is rigidly
attached to the manipulators, thus the contact constraints are bilateral. In the latter case,
relative motion between the object and the manipulators is possible, thus the contact
constraints are unilateral. Although contact cannot be broken when considering bilat-
eral constraints, it is still possible to model holding an object using grippers with ﬁxed
grasp points if the object has speciﬁc features (e.g. the ear of a coffee mug). In such a
case contact can be broken if the grippers are opened.
Different control laws have been developed for cooperative manipulators such as
master/slave, hybrid position/force, input-output and input-state linearization, impe-
dance and passivity based control. In master/slave control, one manipulator (master) is
motion controlled and in charge of imposing the desired motion of the object, whereas
the others (slaves) are force controlled and required to follow the motion imposed by
the master. Problems such as the requirement for the slave(s) to be sufﬁciently com-
pliant and how to assign the roles of master and slave(s) to the manipulators dynam-
ically for certain tasks are commonly found in literature [52]. Hybrid position/force
control is one of the ﬁrst non-master/slave control algorithms used for cooperative
manipulation. It considers transforming the motion and force variables of the
end-effectors of the manipulators into object motion and internal/external forces, such
that they can be controlled separately. The drawbacks of this controller are related to
the incorrect use of orthogonality and contact compliance.
(a) Robot Rose preparing a meal [44]
(b) Robot PR2 folding towels [50]
(C) Motoman SDA10 assembling a chair [51] 
Fig. 3. Examples related to cooperative manipulation [50, 51]
Impedance Control in the Rehabilitation Robotics
1015

Input-output and input-state linearization are model-based compensation methods
which realize a decoupled linear system that can be controlled using well known linear
techniques. Using this technique, controllers have been designed in the joint space and
in the operational (or task) space. In [53] a reduced order dynamical model is obtained
by constraint elimination which is used to design a controller that decouples the force
and motion controlled degrees of freedom.
Impedance control is another very widely used technique for controlling cooper-
ative manipulation tasks. Its use in cooperative manipulation can be categorized into
three groups; by enforcing an impedance relationship between the grasped object and
the external environment or by enforcing an impedance relationship between the
grasped object and the manipulators or a combination of both. The approaches in the
ﬁrst category consider controlling the external forces that arise from the contact of the
grasped object with the environment and usually require the knowledge of object
accelerations (either by measurement or by estimation) and an object’s inertial
parameters. For the second type, the emphasis is on controlling the internal forces of
the grasped object instead of controlling the external forces. Only very few geometric
parameters of the object are required for the operation of this type of controllers,
knowledge of object inertial parameters are usually not required. The last category
considers controlling both the internal forces of the object and contact forces between
the object and an external environment. The impedance controllers for regulating
internal forces and external forces are combined in [54, 55]. Besides the previously
mentioned approaches, passivity based or adaptive or robust control algorithms for
cooperative manipulators also exist in the literature.
3.3
Cooperative Manipulation
Multi-arm robotic systems are a popular subject of active research in recent years [47,
48]. These systems become required due to limited payload capacity of single arm
systems in certain tasks and need for additional equipment (e.g. ﬁxtures) besides the
single arm manipulators. Practical examples are heavy payload transportation and
ﬁxtureless multi-part assembly in industry and in space, or folding of cloths and
preparing meals in the domestic domain (see Fig. 3). Cooperative manipulators can
have signiﬁcant advantages compared to a single robot. If multiple manipulators are
used to carry a heavy or large payload, for example, the weight can be distributed
among several smaller and cheaper robots and the payload can be handled more safely.
Mechanical assembly, an important process in many industries (e.g. automotive), can
be performed faster and ﬂexibly. Special ﬁxtures whose main purpose are to support
certain parts of the assembly are often used in this process. With the help of multiple
manipulators where one or more play the role of the ﬁxture, the number of special
ﬁxtures can be reduced or ultimately their use can be eliminated completely. In many
cooperative tasks, the manipulators grasp a common object and also bring it into
contact with the environment. Some examples are scribing, painting, grinding, pol-
ishing, contour following and object aligning.
Although in cooperative manipulation usually a commonly grasped object is
considered, in [49] a distinction is made between non-coordinated (i.e. each arm
1016
Z. Jelačić

performs different tasks), coordinated (i.e. each arm performs a different part of the
same task) and bimanual (in the case of two manipulators) manipulation. The analysis
and control of the ﬁrst class is the same as in the case of individual manipulators for
which there is an abundance of literature.
Cooperative manipulation tasks can be categorized according to the grasp points as
ﬁxed and non-ﬁxed [49]. In the ﬁrst case, it is assumed that the object is rigidly
attached to the manipulators, thus the contact constraints are bilateral. In the latter case,
relative motion between the object and the manipulators is possible, thus the contact
constraints are unilateral. Although contact cannot be broken when considering bilat-
eral constraints, it is still possible to model holding an object using grippers with ﬁxed
grasp points if the object has speciﬁc features (e.g. the ear of a coffee mug). In such a
case contact can be broken if the grippers are opened.
Different control laws have been developed for cooperative manipulators such as
master/slave, hybrid position/force, input-output and input-state linearization, impe-
dance and passivity based control. In master/slave control, one manipulator (master) is
motion controlled and in charge of imposing the desired motion of the object, whereas
the others (slaves) are force controlled and required to follow the motion imposed by
the master. Problems such as the requirement for the slave(s) to be sufﬁciently com-
pliant and how to assign the roles of master and slave(s) to the manipulators dynam-
ically for certain tasks are commonly found in literature [52]. Hybrid position/force
control is one of the ﬁrst non-master/slave control algorithms used for cooperative
manipulation. It considers transforming the motion and force variables of the
end-effectors of the manipulators into object motion and internal/external forces, such
that they can be controlled separately. The drawbacks of this controller are related to
the incorrect use of orthogonality and contact compliance.
Input-output and input-state linearization are model-based compensation methods
which realize a decoupled linear system that can be controlled using well known linear
techniques. Using this technique, controllers have been designed in the joint space and
in the operational (or task) space. In [53] a reduced order dynamical model is obtained
by constraint elimination which is used to design a controller that decouples the force
and motion controlled degrees of freedom.
Impedance control is another very widely used technique for controlling cooper-
ative manipulation tasks. Its use in cooperative manipulation can be categorized into
three groups; by enforcing an impedance relationship between the grasped object and
the external environment or by enforcing an impedance relationship between the
grasped object and the manipulators or a combination of both. The approaches in the
ﬁrst category consider controlling the external forces that arise from the contact of the
grasped object with the environment and usually require the knowledge of object
accelerations (either by measurement or by estimation) and an object’s inertial
parameters. For the second type, the emphasis is on controlling the internal forces of
the grasped object instead of controlling the external forces. Only very few geometric
parameters of the object are required for the operation of this type of controllers,
knowledge of object inertial parameters are usually not required. The last category
considers controlling both the internal forces of the object and contact forces between
the object and an external environment. The impedance controllers for regulating
Impedance Control in the Rehabilitation Robotics
1017

internal forces and external forces are combined in [54, 55]. Besides the previously
mentioned approaches, passivity based or adaptive or robust control algorithms for
cooperative manipulators also exist in the literature.
4
Manipulator and Contact Dynamics Model
In this section, the relevant background information about manipulator modelling and
the environment with which it interacts is presented. The kinematic and dynamic
equations are introduced for a non-redundant serial manipulator in contact with a
compliant environment. The manipulator possesses 6 independent degrees of freedom
such that the 3 positions and 3 orientations can be speciﬁed for the end-effector, hence
the robot arm is non-redundant.
4.1
Manipulator Kinematics and Dynamics
The links of the manipulators are assumed to be rigid while the joints exhibit no
ﬂexibility. The end-effector position pe ¼ R3 and orientation represented by the rota-
tion matrix Re 2 SO(3) of the manipulator are related to the joint variables, h 2 R6 via
the forward kinematics map (i.e. pe(h), Re(h)). A common way to derive this map is by
using Denavit-Hartenberg convention. The end effector velocities are related to the
joint velocities by the geometric Jacobian,
ve ¼ JðhÞ_h
ð4:1Þ
where:
ve ¼
_pT
e
xT
e


; _pe 2 R3; xe 2 R3
are the translational and the angular velocities expressed w.r.t. the base frame,
respectively. The translational and angular accelerations w.r.t. the base frame follow
from (4.1) as,
_ve ¼ JðhÞ_h þ _JðhÞ_h
ð4:2Þ
where:
_ve ¼
€pT
e
_xT
e

T:
It is common to use a suitable parameterization for the orientation of the
end-effector with less parameters than the rotation matrix (which has 9 parameters) in
order to reduce computational complexity for the implementation of the control
algorithm and trajectory planning. A minimal parameterization of the three dimensional
rotation group SO(3) requires 3 parameters. Examples of such representations are Euler
angles and exponential parameterization. A problem with minimal parameterizations is
that they cannot be both global (in the sense of a 1–1 map between the parameters and
1018
Z. Jelačić

the rotation matrix) and non-singular. Ad hoc solutions such as redeﬁning the inertial
frame or switching to a different parameterization can in principle deal with these
singularities. Unit quaternions, a non-minimal parameterization, are selected to com-
pute the orientation error, since they are computationally efﬁcient and can properly
represent a large range of orientation angles. Given the rotation matrix Re, its four
parameter singularity-free representation is given by the following unit quaternion,
qe ¼
ge;
eT
e

T 2 S3
ð4:3Þ
with:
ge ¼ cos ce
2 ; ee ¼ be sin ce
2
where:
ce 2 R; be 2 S2:
being the rotation angle and unit vector of an equivalent angle/axis representation of the
rotation matrix satisfying the unit norm constraint
g2
e þ eT
e ee ¼ 1
ð4:4Þ
and
ge  0
when
ce 2 p; p
½
:
The time derivative of the unit quaternion qe is related to the spatial angular
velocity xe by,
_qe ¼
_ge
_ee


¼ 1
2 TðqeÞxe ¼ 1
2
eT
e
Eðge; eeÞ


xe ¼ 1
2
eT
e
geI3  SðeeÞÞ


xe
ð4:5Þ
where S is a skew-symmetric matrix and the following relations
ETðge; eeÞEðge; eeÞ ¼ I3  eeeT
e
ð4:6Þ
TTðge; eeÞTðge; eeÞ ¼ I3
ð4:7Þ
Tðge; eeÞTTðge; eeÞ ¼ I4  qeqT
e
ð4:8Þ
are satisﬁed. The relation between the rotation matrix and unit quaternion is given by
the Rodrigues’ formula,
Reðge; eeÞ ¼ ðg2
e  eT
e eeÞI3 þ 2eeeT
e þ 2geSðeeÞ
ð4:9Þ
Impedance Control in the Rehabilitation Robotics
1019

where:
Reðge; eeÞ ¼ Reðge; eeÞ
which follows from the fact that S3 is a double cover of SO(3) (i.e. the map from
quaternions to rotation matrices is two-to-one).
4.2
Environment Model
Consider the end-effector making contact with an elastic surface described by,
uðpeÞ ¼ h
ð4:10Þ
where uðÞ is an at least twice differentiable function in its domain of deﬁnition and
h parameterizes the surface deﬂection (or deformation).
The surface is assumed to be sufﬁciently smooth and convex. Common examples of
such surfaces are planar,
uðpeÞ ¼ nTðpe  poÞ
where
pe ¼ pe;x
pe;y
pe;z
½
T; n 2 R3; po 2 R3
are the Cartesian coordinates, normal to the plane and the offset of the plane from the
origin,
uðpeÞ ¼ pe  po
k
k  ro
where
po 2 R3; ro 2 R [ 0
represent the position of its center and radius. For a frictionless and compliant surface,
the normal component of the contact force exerted by the end-effector on the surface is
modeled as,
fe;n ¼
keh
if
h  0
0
if
h [ 0

ð4:11Þ
where ke characterizes the stiffness of the surface. This model is known in the literature
as the Kelvin model. The complete contact force vector exerted by the end-effector on
the surface is given by,
fe ¼ nðpeÞfe;n
where
nðpeÞ ¼
@uðpeÞ
@pe

T
ð4:12Þ
1020
Z. Jelačić

where n(pe) is the normal direction of the surface. The effect of contact friction can also
be included in the model by considering the tangential components of the contact force
fe ¼ nðpeÞfe;n þ tðpeÞfe;t
ð4:13Þ
where t(pe) represent the tangential directions of the surface and fe,t the friction force
which depends on the magnitude of the normal force (i.e. |fe,n|) and velocity of the
end-effector. For the case h  0, the robot is in contact with the environment (i.e.
contact motion phase) and the surface is assumed frictionless. The assumption on the
absence of contact friction is solely motivated by theoretical reasons and the effect of
contact friction is left as a perturbation to the nominal closed-loop system.
4.3
Impedance Controller for Cooperative Manipulation
Over the past decades, several control algorithms have been proposed for cooperative
manipulators. The algorithms that can control both forces and motion can be divided
into hybrid position/force control schemes and impedance/admittance control schemes.
In the hybrid control schemes, the coordination space is decoupled into motion and
force controlled directions, using a predeﬁned and ﬁxed selection matrix. Unexpected
contact in motion controlled directions can lead to damage of the object and manip-
ulators, since the force in these directions is not controlled. In the impedance control
schemes the dynamic relation between the forces and motion of the system is taken into
account. Using impedance control, the task can be executed without leading to contact
instability in the absence of precise knowledge of the contact directions. However, to
achieve a satisfactory level of position/force tracking performance, the precise
knowledge of contact directions is still required.
A cascade controller for cooperative manipulation is introduced. The block diagram
of this controller is shown in Fig. 4. A motion controller is at the lowest level of this
scheme. The inner motion control loop is added to improve the tracking performance.
The reference for this motion control law is obtained from an impedance relationship
driven by the internal force error. The desired trajectory for each manipulator is
obtained from the kinematic constraints between the object and the respective
manipulator. Finally, the desired object motion is obtained from the external force
based impedance controller.
Fig. 4. Control architecture of the cooperative manipulators
Impedance Control in the Rehabilitation Robotics
1021

5
Discussion and Conclusion
For cooperative manipulation tasks, it is important to simultaneously control the motion
of the system comprised of the manipulators and the object, the interaction forces
between the object and the manipulators (internal forces) and the contact forces
between the object and the environment (external forces).
The proposed hybrid control, consisting of the hybrid position/force control and
impedance control, takes the previously described major contact problem issues into
account. It is a control law suitable for implementation in unstructured environments.
A cascade control algorithm, based on the cooperative manipulation, is designed for
the rehabilitation robot control. With the proposed control algorithm for cooperative
manipulation, the motion, internal and external forces of the object can be controlled.
Using impedance relationships, a commanded object trajectory is converted into ref-
erence trajectories for the motion controllers of the manipulators such that the desired
internal and contact forces can be achieved.
The next step towards the implementation in the rehabilitation would be to
experimentally map the applied forces during the rehabilitation training and implement
it in the proposed control algorithm.
6
Future Recommendations
The desire to develop robots that can physically interact with humans in intuitive and
biologically inspired ways has revealed a vast ﬁeld of human-human physical inter-
action that has only begun to be studied. It may be advantageous to design robots that
can be optimized to the functioning of the human nervous system. However, much
work is to be done in human-human sensorimotor interactions before any general
principles of cooperative sensorimotor control can be ﬁrmly established. As a ﬁrst step,
some areas of potentially fruitful investigation to reveal principles of sensorimotor
cooperation in human-human interface that are directly relevant to the design of
physical interfaces and control schemes for rehabilitation robots have been identiﬁed.
Speciﬁcally, new experimental paradigms should be developed that can address
open questions of how motor redundancy, varying skill level, speciﬁc role assignment
and reliance on haptic feedback play a role in the haptic cues and physical interactions
between individuals. In particular, more attention in the speciﬁc areas of sensorimotor
assistance and sensorimotor education could provide some guidance in the design of
haptic interfaces and controllers for rehabilitation robots. Possible motor tasks that
could be used in such paradigms include but are not limited to: handshake, partner
dance, sawing, carrying objects, leading an individual with visual impairment, and as a
more direct clinical task, therapist-patient interactions during rehabilitation.
While several of these tasks have served as motivation for a number of
human-human interface studies, there is a basic lack understanding of the magnitude of
forces used in these tasks. Additionally, how interaction forces contribute to task
performance, or provide a channel for communicating information about motor per-
formance, intent, and skill remains unknown. The identiﬁcation of general principles of
sensorimotor interaction between human partners may also be applicable to an
1022
Z. Jelačić

alternative perspective of human-robot-interaction in rehabilitation. While the appli-
cation of human-robot interaction in rehabilitation has traditionally focused on the
robot-patient interaction, future work may also wish to consider human-robot inter-
action in rehabilitation from the perspective of a robot interacting with a therapist.
The nature of the challenges presented by the interaction between robot and ther-
apist are likely to be different from those of robot-patient interactions, principles
derived from the study of human-human interaction may serve to inform such inter-
actions
as
well.
Overall,
the
identiﬁcation
of
guiding
principles
that
drive
human-human sensorimotor interactions have the potential to further the design, con-
trol and use of rehabilitation robots that can physically interact with humans in intuitive
and biologically inspired ways, thereby enhancing rehabilitation outcomes.
References
1. Jarrassé, N., Charalambous, T., Burdet, E.: A framework to describe, analyze and generate
interactive motor behaviors. PLoS ONE 7, 1–13 (2012)
2. Galvez, J.A., Kerdanyan, G., Maneekobkunwong, S., Weber, R., Scott, M., Harkema, S.J.,
Reinkensmeyer, D.J.: “Measuring Human Trainers” skill for the design of better robot
control algorithms for gait training after spinal cord injury. In: Proceedings of the IEEE
Conference on Rehabilitation Robotics, pp. 231–234 (2005)
3. Ikeura, R., Morita, A., Mizutani, K.: Variable-damping characteristics in carrying an object
by two humans. In: Proceedings of the IEEE International Workshop on Robot and Human
Communication, pp. 130–134 (1997)
4. Rehabilitation, World Health Organization. http://www.who.int/topics/rehabilitation/en/.
Accessed 19 Feb 2016
5. Díaz, I., Gil, J.J., Sánchez, E.: Lower-limb robotic rehabilitation: literature review and
challenges. J Robot. 759–764 (2011). https://doi.org/10.1155/2011/759764
6. Lum, P.S., Burgar, C.G., Shor, P.C., Majmundar, M., Van der Loos, M.: Robot-assisted
movement training compared with conventional therapy techniques for the rehabilitation of
upper-limb motor function after stroke. Arch. Phys. Med. Rehabil. 83(7), 952–959 (2002)
7. Basteris, A., Nijenhuis, S.M., Stienen, A.H., Buurke, J.H., Prange, G.B., Amirabdollahian,
F.: Training modalities in robot-mediated upper limb rehabilitation in stroke: a framework
for classiﬁcation based on a systematic review. J Neuroeng Rehabil. 11, 111 (2014)
8. Romer, G.R.B.E., Stuyt, H.J.A., Peters, A.: Cost-savings and economic beneﬁts due to the
assistive robotic manipulator (ARM). Conf. Proc IEEE Rehabil Robot. 1, 201–204 (2005)
9. Bemelmans, R., Gelderblom, G.J., Jonker, P., de Witte, L.: Socially assistive robots in
elderly care: a systematic review into effects and effectiveness. J. Am. Med. Dir. Assoc. 13
(2), 114–120 (2012)
10. Armeo therapy. https://www.hocoma.com/usa/us/products/armeo/. Accessed 19 Feb 2016
11. Hocoma product overview, Hocoma. https://www.hocoma.com/usa/us/products/. Accessed
19 Feb 2016
12. Reha technology product. http://www.rehatechnology.com/products.html. Accessed 19 Feb
2016
13. Motorika product, Motorika. http://www.motorika.com/?categoryId=90219. Accessed 19
Feb 2016
14. Tyrosolution, Tyromotion. http://tyromotion.com/en/products. Accessed 19 Feb 2016
Impedance Control in the Rehabilitation Robotics
1023

15. Knaepen, K., Beyl, P., Duerinck, S., Hagman, F., Lefeber, D., Meeusen, R.: Human-robot
interaction: kinematics and muscle activity inside a powered compliant knee exoskeleton.
IEEE Trans. Neural Syst. Rehabil. Eng. 22(6), 1128–1137 (2014)
16. Alamdari, A., Krovi, V.: Robotic physical exercise and system. Biomed. Eng. Lett. 6(1–9), 9
(2016)
17. (ROPES): A cable-driven robotic rehabilitation system for lower-extremity motor therapy.
In: Conference Proceedings of the ASME International Design Engineering Technical
Conferences and Computers and Information in Engineering Conference, vol. 1, pp. 1–10
(2015)
18. Li, J., Zheng, R., Zhang, Y., Yao, J.: iHandRehab: an interactive hand exoskeleton for active
and passive rehabilitation. Conf. Proc. IEEE Rehabil. Robot. 1, 1–6 (2011)
19. Casadio, M., Sanguineti, V., Morasso, P.G., Arrichiello, V.: Braccio di Ferro: a new haptic
workstation for neuromotor rehabilitation. Technol. Health Care 14(3), 123–142 (2006)
20. Huang, F.C., Patton, J.L., Mussa-Ivaldi, F.A.: Negative viscosity can enhance learning of
inertial dynamics. Conf. Proc. IEEE Rehabil. Robot. 1, 474–479 (2009)
21. Proﬁcio, Barrett Medical. http://www.barrettmedical.com/. Accessed 19 Feb 2016
22. Jung, H., Han, J., Kim, C.Y., Chun, K.J., Jung, D., Kim, J.S., Lim, D.: Characteristics of
center of body mass trajectory and lower extremity joint motion responded by dynamic
motions of balance training system. Biomed. Eng. Lett. 5(2), 92–97 (2015)
23. Biswas, D., Cranny, A., Rahim, A.F., Gupta, N., Maharatna, K., Harris, N.R., Ortmann, S.:
On the data analysis for classiﬁcation of elementary upper limb movements. Biomed. Eng.
Lett. 4(4), 403–413 (2014)
24. Parra-Dominguez, G.S., Snoek, J., Taati, B., Mihailidis, A.: Lower body motion analysis to
detect falls and near falls on stairs. Biomed Eng Lett. 5(2), 98–108 (2015)
25. Jensen, U., Leutheuser, H., Hofmann, S., Schuepferling, B., Suttner, G., Seiler, K.,
Kornhuber, J., Eskoﬁer, B.M.: A wearable real-time activity tracker. Biomed. Eng. Lett. 5
(2), 147–157 (2015)
26. Lajeunesse, V., Vincent, C., Routhier, F., Careau, E., Michaud, F.: Exoskeletons’ design and
usefulness evidence according to a systematic review of lower limb exoskeletons used for
functional mobility by people with spinal cord injury. Disabil. Rehabil. Assist. Technol. 4,
1–13 (2015)
27. Benson, I., Hart, K., Tussler, D., van Middendorp, J.J.: Lower-limb exoskeletons for
individuals with chronic spinal cord injury: ﬁndings from a feasibility study. Clin. Rehabil.
30(1), 73–84 (2016)
28. Asselin, P., Knezevic, S., Kornfeld, S., Cirnigliaro, C., Agranova-Breyter, I., Bauman, W.A.,
Spungen, A.M.: Heart rate and oxygen demand of powered exoskeleton-assisted walking in
persons with paraplegia. J. Rehabil. Res. Dev. 52(2), 147–158 (2015)
29. Kozlowski, A.J., Bryce, T.N., Dijkers, M.P.: Time and effort required by persons with spinal
cord injury to learn to use a powered exoskeleton for assisted walking. Top. Spinal Cord Inj.
Rehabil. 21(2), 110–121 (2015)
30. Hartigan, C., Kandilakis, C., Dalley, S., Clausen, M., Wilson, E., Morrison, S., Etheridge, S.,
Farris, R.: Mobility outcomes following ﬁve training sessions with a powered exoskeleton.
Top. Spinal Cord Inj. Rehabil. 21(2), 93–99 (2015)
31. Waldron, K., Schmiedeler, J.: Kinematics. In: Siciliano, B., Khatib, O. (eds.) Springer
Handbook of Robotics, pp. 9–33. Springer, Berlin, Heidelberg (2008)
32. Siciliano, B., Villani, L.: Robot Force Control, volume 540 of The Springer International
Series in Engineering and Computer Science. Springer US (1999)
33. Villani, L., de Schutter, J.: Force control. In: Siciliano, B., Khatib, O. (eds.) Springer
Handbook of Robotics, pp. 161–187. Springer (2008)
1024
Z. Jelačić

34. Hogan, N.: Impedance control: an approach to manipulation: Part I—Theory. J. Dyn. Syst.
Meas. Contr. 107(1), 1–7 (1985)
35. Salisbury, J.K.: Active stiffness control of a manipulator in Cartesian coordinates. In: 19th
IEEE Conference on Decision and Control including the Symposium on Adaptive Processes,
vol. 19, pp. 95–100 (1980)
36. Raibert, M.H., Craig, J.J.: Hybrid position/force control of manipulators. J. Dyn. Syst. Meas.
Control 103(2), 126–133 (1981)
37. de Schutter, J., Van Brussel, H.: Compliant robot motion II. A control approach based on
external control loops. Int. J. Robot. Res. 7(4), 18–33 (1988)
38. Chiaverini, S., Sciavicco, L.: The parallel approach to force/position control of robotic
manipulators. IEEE Trans. Robot. Autom. 9(4), 361–373 (1993)
39. Yoshikawa, T.: Force control of robot manipulators. In: Proceedings of IEEE International
Conference on Robotics and Automation (ICRA), vol. 1, pp. 220–226 (2000)
40. Brogliato, B.: Feedback control. In: Nonsmooth Mechanics, Communications and Control
Engineering, pp. 397–461. Springer, London (1999)
41. Technology Research News. Cooperative robots share the load, Feb 2002. http://www.
trnmag.com/Stories/2002/021302/Cooperative_robots_share_the_load_021302.html
42. Stanford University. The Stanford Assistant Mobile Manipulator (SAMM). http://robotics.
stanford.edu/*ruspini/samm.html
43. Urbanek, H., Albu-Schaffer, A., Van Der Smagt, P.: Learning from demonstration: repetitive
movements for autonomous service robotics. In: Proceedings of IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS), vol. 4, pp. 3495–3500 (2004)
44. Robot Rose: Robot rose media footage. http://robot-rose.com/media/
45. Karayiannidis, Y., Doulgeri, Z.: Robot contact tasks in the presence of control target
distortions. Robot. Auton. Syst. 58(5), 596–606 (2010)
46. Vanderborght, B., Albu-Schaeffer, A., Bicchi, A., Burdet, E., Caldwell, D.G., Carloni, R.,
Catalano, M., Eiberger, O., Friedl, W., Ganesh, G., Garabini, M., Grebenstein, M., Grioli,
G., Haddadin, S., Hoppner, H., Jafari, A., Laffranchi, M., Lefeber, D., Petit, F., Stramigioli,
S., Tsagarakis, N., Van Damme, M., Van Ham, R., Visser, L.C., Wolf, S.: Variable
impedance actuators: a review. Robot. Auton. Syst. 61(12), 1601–1614 (2013)
47. Caccavale, F., Uchiyama, M.: Cooperative manipulators. In: Siciliano, B., Khatib, O. (eds.)
Springer Handbook of Robotics, pp. 701–718. Springer, Berlin, Heidelberg (2008)
48. Uchiyama, M.: Chapter 1 Multi-arm robot systems: A survey. In: Chiacchio, Pasquale,
Chiaverini, Stefano (eds.) Complex Robotic Systems. Lecture Notes in Control and
Information Sciences, vol. 233, pp. 1–31. Springer, Berlin Heidelberg (1998)
49. Smith, C., Karayiannidis, Y., Nalpantidis, L., Gratal, X., Qi, P., Dimarogonas, D.V., Kragic,
D.: Dual arm manipulation—a survey. Robot. Auton. Syst. 60(10), 1340–1353 (2012)
50. Maitin-Shepard, J., Cusumano-Towner, M., Lei, J., Abbeel, P.: Cloth grasp point detection
based on multiple-view geometric cues with application to robotic towel folding. In: IEEE
International Conference on Robotics and Automation (ICRA), pp. 2308–2315 (2010)
51. Yaskawa Motoman Robotics. Motoman sda10 assembling a chair. http://www.motoman.
com/industries/furniture-ﬁxtures.php
52. Uchiyama, M., Dauchez, P.: Symmetric kinematic formulation and nonmaster/slave
coordinated control of two-arm robots. Adv. Robot. 7(4), 361–383 (1992)
53. Koivo, A.J., Unseren, M.A.: Reduced order model and decoupled control architecture for
two manipulators holding a rigid object. J. Dyn. Syst. Meas. Control 113(4), 646–654 (1991)
54. Caccavale, F., Chiacchio, P., Marino, A., Villani, L.: Six-dof impedance control of dual-arm
cooperative manipulators. IEEE/ASME Trans. Mechatron. 13(5), 576–586 (2008)
55. Caccavale, F., Villani, L.: Impedance control of cooperative manipulators. Mach. Intell.
Robot. Control 2, 51–57 (2000)
Impedance Control in the Rehabilitation Robotics
1025

Comparison of Numerical and Experimental
Results of Measuring Vehicle Movement
Kinematic Parameters Integrated
into Advanced Mechatronic Systems
Semir Mehremić1 and Isak Karabegović2(&)
1 Faculty of Mechanical Engineering, University “DžemalBijedić”, Mostar,
Bosnia and Herzegovina
2 Technical Faculty, University of Bihać, Bihać, Bosnia and Herzegovina
isak1910@hotmail.com
Abstract. According to statistical data only 3% of trafﬁc accidents are caused
by technical reasons (vehicles), while 97% of accidents are caused by human
factors (driver). Apart from unadequate technical validity of the vehicle, main
causes of trafﬁc accidents are wrong decisions of the driver, improper estimates
of circumstances or attention deﬁcit.Introduction of mechatronic systems of
driver assistance can signiﬁcantly inﬂuence the improvement of safety in trafﬁc.
Analysis of kinematic parameters of vehicle movement is shown in this work
(path, displacement, velocity, acceleration) and their monitoring and measuring
in real conditions. The analysis was made on the theoretical model of vehicle
movement, as well as on experimental mechatronic system especially designed
for the purposes of this work. In order to avoid problems in describing complex
spatial vehicle movement, and the system of larger number of the mass, inter-
connected by visco-elastic elements, one of the simplest theoretical models was
used for the needs of this work—the bicycle model. Analysis of experimental
measurements and comparison with theoretic results was done in order to
consider the possibilities of integration of such systems into advanced drivers
assistance mechatronic systems (ADAS). Experimental mechatronic system
which was exhibited in the work is one of the examples how to use intelligent
systems (sensors, computer, controllers etc.) for automatisation of any process,
not only in car industry, and with the purpose of reducing human decision
making factor to the minimum.
1
Introduction
Until 1960, radio was the only signiﬁcant electronic part in the car. All other functions
of the cars were completely mechanical or electrical. There was no intelligent security
systems, in addition to augmenting the bumper and structural elements for protecting
passengers in the event of an accident. Safety belts, introduced in the early 1960s, were
aimed at improving the safety of passengers and were to completely mechanically
actuated. All systems are power controlled by the driver and/or other mechanical
systems management. Highly automated passenger vehicles, integrated into an
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_86

intelligent environment will in future play a key role in improving road safety.
According to European statistics of accidents, about 2/3 of a severe accident the out-
come could have been avoided. A signiﬁcant part of these accidents are accidents
occurring combination of a number of circumstances, of which more than half of its
roots in the lack of attention. Forecasts suggest that without signiﬁcant improvements
in road safety, trafﬁc accidents will be the second largest cause of death by 2030 [1].
The main objective of this paper is to analyze the impact of the introduction of
mechatronic systems help the driver to increase trafﬁc safety, or scientiﬁc proof that
electronic networked systems for measuring and monitoring the kinematic parameters
of the vehicle (displacement, velocity, acceleration) can signiﬁcantly reduce the impact
of the driver in making key decisions and indirectly decrease the rate of trafﬁc accidents
caused by human factorsie. systems for measuring kinematic parameters integrated into
an intelligent environment increases control of the vehicle and direct impact on the
improvement of trafﬁc safety.
2
Vehicle Modeling
There are a number of models for the analysis of vehicle handling in literature [2–6].
Their structure ranges from simple such as bike model [3, 7] to the very complex
spatial models [2]. Describe the complex spatial motion of the vehicle, and the system
comprising a plurality of masses which are connected by elastic-damping element is a
major problem. At the same time, can certainly lead to certain errors so that the
obtained results are quite confused and practically unusable. Therefore, in recent years,
make attempts to process generation of differential equations of the vehicle automates.
In this regard, developed an increasing number of software packages.
In order to avoid problems in describing the complex spatial motion of the vehicle,
and the plurality of system weight, interconnected by elastic-damping elements, for the
purpose of this paper will use the simplest theoretical model of the vehicle—the bicycle
model.
The ﬁrst step in carrying out the kinematic model is the introduction of
non-holonomic constraints. Non-holonomic constraints does not limit the set of pos-
sible states of the mechanical system in conﬁguration space. However, if such
restrictions exist in the system, then some of the coordinate system is not possible to
independently vary, i.e. the changes affect the other coordinate system. In other words,
non-holonomic constraints affect the way the transition from one point to another in the
conﬁguration area (e.g. Parallel parking a car). It is important to note that the existence
of non-holonomic constraints does not reduce the number of degrees of freedomsystem.
Non-holonomic constraints in this case include the assumption that there is no traction
of the wheels [8–20].
where is:
(x, y)
coordinates of the position of center of gravity CG of the vehicle indicated by
Fig. 1,
h
orientation of the vehicle relative to the axis x,
u
angle of rotation of the front wheels relative to the longitudinal axis of the
vehicle.
Comparison of Numerical and Experimental Results of Measuring
1027

Using the coordinate system of the vehicle, as shown in Fig. 2, where the u-section
is deﬁned as the longitudinal part of the vehicle and the w-section is vertical in the
section.
_x ¼ vu cos h  vw sin h
ð2:1Þ
_y ¼ vu sin h þ vw cos h
ð2:2Þ
vu i vw—speed along the vehicle center of gravity axis u and w, respectively.
The dynamic equation of the vehicle include several assumptions:
• does not have the force of friction between the wheels and the body (chassis) of the
vehicle,
• the rear wheels of vehicles moving in the direction of orientation of the vehicle,
Fig. 1. Bicycle model which describes the movement of vehicles
Fig. 2. The coordinate system of the vehicle
1028
S. Mehremić and I. Karabegović

• no traction control,
• operation of traction vehicles is reduced to the action in the center between the rear
wheels of the vehicle.
The forces that exists in the performance of dynamic equations of motion of the vehicle
are shown in Fig. 3.
The obtained dynamic equations:
_vu ¼ vw _h  FF
m sin u þ FD
m
ð2:3Þ
_vw ¼ vu _h þ FF
m cos u þ FD
m
ð2:4Þ
€h ¼ aFF cos u
J
 bFR
J
ð2:5Þ
where is:
m
mass of the vehicle,
J
moment of inertia of the vehicle around the center of gravity,
FD
vehicle traction force (applied to the rear, along the axis of the vehicle),
FF; FR
resultant lateral forces on the front and rear wheels respectively.
Taking into account that it is:
X ¼ xyhvuFDu
½
0
ð2:6Þ
we get a system of dynamic equations of motion of the vehicle:
Fig. 3. Driving forces on the vehicle
Comparison of Numerical and Experimental Results of Measuring
1029

_x ¼
cos h  b tan u
l
sin h


vu
_y ¼
sin h þ b tan u
l
cos h


vu
_h ¼ tan u
l
vu
_vu ¼ vu _u mb2 þ J


tan u þ l2 cos u
ð
Þ2FD
ð2:7Þ
_FD ¼ f vu; FD
ð
Þ
_u ¼ f u
ð Þ
3
Analysis of Theoretical Model of Vehicle Movement
On the previously shown the theoretical model of the vehicle carried is the analysis of
the semicircular path of the vehicle to kinematic parameters that characterize the
motion of the vehicle (velocity, acceleration).
The dimensions of the model, as well as the characteristics of its parts (length, the
distance between the front and rear axles) are taken on the basis of the system for
experimental testing of kinematic parameters of the vehicle.
For the experiment used car brand Ford Focus 1.8 TDCI, with 85 kW. Data were
taken from www.auto-data.net [21–30].
Input data used at the theoretical model are:
b ¼ 1036 mm
a ¼ 1579 mm
L ¼ 2615 mm
J ¼ 2400 kgm2
m ¼ 1500 kg with two passengers
ð
Þ:
For solving differential equations of motion, the software package Maple 8 was
used.
In order to verify the theoretical model, differential equations are solved by the
inclusion of the above information.
If the theoretical model of the vehicle whose equations of motion given by (2.7) in
Sect. 2. Classiﬁes the known values (characteristics of the vehicle), by varying the
angles h and u in dependence on the current position of the vehicle when the
approximately semi-circular path, returns a time dependence of velocity and acceler-
ation of movement of this type.
We can see a linear dependence of the speed (Fig. 4) which is in reality difﬁcult to
achieve, and in which we will see in experimental researches presented in later sec-
tions. We can conclude that the theoretical model somewhat accurately describes the
1030
S. Mehremić and I. Karabegović

actual movement of the vehicle, and that would have been even closer to real condi-
tions it would be necessary to introduce a whole line of parameters that we are in this
discussion for simplicity avoid.
Figure 5 shown again linear acceleration changes, as well as in the previous con-
siderations. This is not the case in practice, where they meet with a range of parameters
that directly and indirectly inﬂuence the oscillations of the vehicle kinematic param-
eters (roughness, friction, wheel slip, engine vibration, etc.).
Fig. 4. Time-Velocity dependance for approximately semicircular movement (20 km/h)
Fig. 5. Accelerograms(x-axis and y-axis) for approximately semicircular movement (20 km/h)
Comparison of Numerical and Experimental Results of Measuring
1031

4
Experimental Analysis of Kinematic Parameters
of the Vehicle Movement
4.1
Description of the Experiment
An experimental system is developed for purposes of experimental research. It consists
of the car on which were carried out experimental measurements of the kinematic
parameters
and
of the
measuring
chain,
which
consists
of capacitivetriaxial
accelerometer ASC 5631, measuring—enhancer device SPIDER 8 which is at one end
connected to the sensors and at another end with a computer, using LPT port (Fig. 6).
Mechatronic system comprises sensors kinematic motion parameters located within
the vehicle on which the measurements are made, together with other components of
the measurement chain. Carried the variation of kinematic parameters of the vehicle)
and the monitoring and measurement of the real conditions. For the purpose of this
work was monitored and measured changes in kinematic parameters when the vehicle
drove on approximately semicircular path length of 112.49 m.
Measurement of kinematic parameters were measured on the polygon which is a
plan view shown in Fig. 7. The appearance of polygon is displayed using Google maps
application and length of the trajectory on which they are carried out experimental
measurements are measured.
Fig. 6. Triaxial capacitive accelerometer positioning in the car and SPIDER 8
Fig. 7. The appearance of polygons displayed using Google maps application
1032
S. Mehremić and I. Karabegović

4.2
The Measurement Results and Analysis
Appearance semicircular path on which are carried out experimental measurements is
shown earlier in Fig. 7. We were carried out experimentfor vehicle velocity of
20 km/h.
By analyzing the diagrams in Fig. 8, we can generally conclude that the biggest
change in the acceleration in the y axis occur in those time intervals for which the
changes of acceleration in the x axis minimum. Also, worth and inversely proportional
to the situation. Such changes of kinematic parameters of movement are expected due
to the curvature of the path by which the wheel.
Fig. 8. Accelerograms in the direction of x-axis and y-axis
Fig. 9. Accelerogram in the direction of z-axis
Comparison of Numerical and Experimental Results of Measuring
1033

It is obvious that these are only minor changes in acceleration in the direction of the
z-axis (Fig. 9). These changes are a result of engine vibration and are in the range of
about 0.35 m/s2 Fig. 10.
Comparing the diagram increase speed with accelerogram in the direction of x-axis
and y-axis, we can conclude that the increase in speed to the maximum value of time
corresponding to the rise acceleration.
5
Conclusion
In this paper numerical and experimental analysis of the kinematic parameters of the
vehicle (trajectory, displacement, velocity, acceleration) was shown and the monitoring
and measurement of the real conditions.
Numerical analysis was performed on a theoretical model of the vehicle which is
taken for the model bicycle.
Analysis of the theoretical model of the vehicle by the semi-circular path shows
that, based on this model can monitor the absolute value of the kinematic parameters as
well as their changes, but here we have diagrams’ “imperfections” that are a conse-
quence of the above-described simpliﬁcation of theoretical models.
Mechatronic sensor system that is designed for experimental analysis provides
precise information on the kinematic parameters of the vehicle from which it is clear
that changes speed and acceleration during the completion of the experiment the
Fig. 10. Changes of the velocity and distance of vehicles in motion by approximately
semicircular path
1034
S. Mehremić and I. Karabegović

straight line motion of the vehicle does not have a pronounced characteristic variability
despite rapid changes in acceleration as well as the components of acceleration higher
frequencies have no signiﬁcant impact on the value of acceleration.
Also, the timings of change of the kinematic parameters of the vehicle indicate the
presence of a number of factors that affect the movement of the vehicle such as engine
vibrations, roughness, various types of friction, imperfections paths, etc.
References
1. United Nations Economic Commission For Europe: Consolidated Resolution On Road
Trafﬁc (2010)
2. Demić M.: Teorija kretanja motornih vozila. Tehnički fakultet u Čačku (1999)
3. Ellis, J.R.: Vehicle Dynamics. Bussines Books, London (1973)
4. Gillespie, T.: Fundamentals of Vehicle Dynamics. SAE (1990)
5. Milliken, W., Milliken, D.: Race Car Dynamics. SAE (1995)
6. Miroslav, D., Jovanka, L., Constantinos, S.: Formalizovano modeliranje upravljivosti
motornih vozila. Vojnotehničkiglasnik Ministarstvaodbrane Republike Srbije, 48, br.2, str.
157–166 (2000)
7. Stojić, B.: Drumskavozila, deo: Teorijakretanjavozila. Fakultettehničkihnauka u Novom-
Sadu (2010)
8. Prof. Dr. Sc. Velagić, J.: Mehatronika, Elektrotehničkifakultet Sarajevo, predavanja
(2012/2013)
9. Sommer, C., Dressler, F.: Vehicular Networking. Cambridge University Press (2015)
10. Ćućuz, N., Rusov, L.: Dinamika motornih vozila. Privredni pregled, Beograd (1973)
11. Moret, E.N.: Dynamic Modeling and Control of a Car—Like Robot. Faculty of the Virginia
Polytechnic Institute and State University (2003)
12. Tvetmarken, S.A.: Analyzing Motions of Unicycles and Car-Like Vehicles. NTNU—
Trondheim (2012)
13. Bouchner, P.: Driving Simulators for HMI Researches. PhD Thesis, Czech Tehnical
University, Faculty of Transportation Sciences, Prague
14. Hejtmánek, P., Čavoj, O., Porteš, P.: Evaluation of vehicle handling by a simpliﬁed single
track model. Electron. Tech. J. Technol. Eng. Logist. Transp. VIII(2) (2013)
15. Karabegović, I.: Kinematika. TehničkifakultetUniverziteta u Bihaću, Bihać (2004)
16. Karabegović, Isak: Dinamika. Svjetlost, Zavodzaudžbenike i nastavnasredstva, Sarajevo
(1997)
17. Road Fatalities in the EU Since 2001: CARE (EU road accident database) (2010)
18. Adaptive Integrated driver-to-vehicle interface, AIDE, publikacije. http://www.aide-eu.org/
pdf/
19. European Transport Safety Council: A Challenging Start towards the EU 2020 Road Safety
Target
20. Karabegović, I., Husak, E., Đukanović, M., Karabegović, E., Mahmić, M.: Primjenasenzora
u mehatroničkimsistemimazaodržavanjevozilanakolodvoru, SONT 2014.—savjetovanje o
novim tehnologijama 06.05.2014, Šibenik
21. http://www.auto-data.net/en/?f=showCar&car_id=7377
22. Mehremić, S., Isić, S., Karabegović, I.: Experimental researches of kinematic parameters in
advanced mechatronics systems in order to increase safety when driving a vehicle. In: 3rd
International Conference “New Technologies NT-2016” Development and Application, 24–
25 Apr 2016, Tehnološki park INTERA, Mostar, Bosna i Hercegovina
Comparison of Numerical and Experimental Results of Measuring
1035

23. Palkovics, L., Fries, A.: Intelligent electronic systems in commercial vehicles for enhanced
trafﬁc safety. Veh. Syst. Dyn. 35, 227–289 (2001)
24. Duysinx, P., Bruls, O., Collard, J.-F., Fisette, P., Lauwerys, C., Swevers, J.: Optimization of
mechatronic systems: application to a modern car equipped with a semi-active suspension.
In: 6th World Congresses of Structural and Multidisciplinary Optimization Rio de Janeiro,
30 May–03 June 2005, Brazil
25. http://www.haveit-eu.org/displayITM1.asp?ITMID=6&LANG=EN
26. http://www.asc-sensors.de/en/products/accelerometers.html
27. https://www.hbm.com/de/2313/spider-8-pc-basierte-messdatenerfassung/
28. https://jr-international.fr/hq-inverter-24-230-v-300-w-with-usb,,,_INV300WU24F_itm_
english.html
29. Rosén, E., Sander, U.: Pedestrian fatality risk as a function of car impact speed. Autoliv
Research, Wallentinsvägen 22, 447 83, Vårgårda, Sweden. http://www.autoliv.com/
ProductsAndInnovations/Documents/Research%20Papers/1.%20RosenSander.pdf
30. http://de.bosch-automotive.com/en/parts/parts_and_accessories/electronics_and_accessories/
parking_aid/parking_aid_1
31. Prof. Dr. Bošnjak, I.: Inteligentnitransportnisustavi 1, Sveučilište u Zagrebu (2006)
32. http://magazine.volkswagen.com/
1036
S. Mehremić and I. Karabegović

Application of Mechatronic System
in the Automation Hydroforming Process
Edin Šemić1(&), Safet Isić1(&), and Edina Karabegović2(&)
1 Faculty of Mechanical Engineering, University of Mostar, Mostar,
Bosnia-Herzegovina
edin.semic@bih.net.ba, safetisic@gmail.com
2 Faculty of Technical Engineering, University of Bihać, Bihać,
Bosnia-Herzegovina
edina-karabeg@hotmail.com
Abstract. This paper gives a brief overview of the design, layout and func-
tionality of components for hydroforming automation process of thin-walled
elements. The present system includes a high pressure pump, electromagnetic
directional control valve, a two-part die, the measuring system with pressure and
feed sensors and supporting software for measurement and analysis, and feed-
back to the valve. Hydroforming process of thin-walled elements involves the
production of pieces, usually a very complex shape, from tubes and sheets.
Typical examples of the use of this process is the automotive, aerospace
industry, manufacturing of medical equipment, and home appliances. One of the
important factors for the performance of the process is a technological devel-
opment of system that includes automation and process control. Mechatronic
systems are characterized by certain level of autonomy. They react autono-
mously to changes in the environment and working conditions, using algorithms
for control. They are ﬂexible and simple. The modiﬁcation of the system may be
quick and easy. In practice, the combination of hydroforming and mechatronics
occurs as a mechatronic hydroforming system. The mechatronic hydroforming
system is an innovative approach to hydroforming of thin-walled elements
wherein the process automates and controlled via software.
Keywords: Hydroforming  Design  Automation  Mechatronic system
Sensors
1
Introduction
Hydroforming process is one of the cold forming process and it is used mainly for the
manufacture of geometrically complex shape body of the tube or sheet. Hydroforming,
as an alternative process, offers the possibility of production of various complex forms
and products from tubes or sheets. Using ﬂuid under high pressure, the achieved results
show great technical and production potential. This process becomes interesting for
manufacturers and wider application. For the modeling, simulation and testing com-
ponents of mechatronic systems for hydroforming, it is used SolidWorks and software
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_87

solution Motion Manager. After designing the components of mechatronic system, it
was made the simulation (animation) of principles of designed mechatronic system for
manage and control the hydroforming process of welded metal sheets in experimental
conditions [1–8].
2
The Design of the Components of the System
The increasing automation of complex systems requires the use of components that are
able to collect and transmit information relevant to the manufacturing process. The
design of mechatronic system is conditioned by the fast changes in the global industry,
which inevitably leads to rapid changes in technology and the conditions of their
application in the real system. Designing a system represents the development process
from an idea to ﬁnished product and requires additional veriﬁcations and validations,
which conﬁrms the proper operation of the components as a whole, and functionality of
the system in working conditions [5, 7].
workpiece 
lower die 
upper die 
preform in the die 
pump and reversible hydraulic valve 
Fig. 1. The design of the preform, tool, pressure system (pump) and reversible hydraulic valve
1038
E. Šemić et al.

The components of the mechatronic system includes: a preform—welded sheets of
a certain shape and size, a tool—the lower and upper die, the device to achieve the
working pressure—pump with storage tank and connectors, the device for measuring
the registered values, and the electronic device for the control and management of the
process-experiment. Figure 1 given design of preform, tool, pressure system (pump)
and reversible hydraulic valve.
The workpiece is composed of two identical metal sheets which are welded and
have on one side an extension with a connector for the pump. The tool is composed of
two parts (matrix) which are identical and assembled with screws. To regulate the ﬂow
of ﬂuid (oil) in the system, the valve is electrically driven. In this type of valve, the
solenoid controls the operation of ﬂuid ﬂow direction.
For the measurement, control and regulation designed system, the sensors have a
signiﬁcant role. The task of the sensor (Fig. 2) is converting the physical value (e.g.,
pressure, feed) into an electrical signal, which is read by the observer and/or the
instrument. After that is processing of the assumed information [3, 8]. System man-
agement is processing the informations from the sensors according to preset algorithm,
to run the processes in a desired manner.
The pressure sensor is strain gauges sensor. The displacement sensor working on
the inductive principle. To obtain information from the pressure and displacement
sensor, it was used measuring and ampliﬁer device “SPIDER 8”, manufactured by
HBM, Germany. The electronic device communicates with external devices via the
parallel (LPT) port on computer. Figure 3 give the design of the measuring and
electronic device with the laptop.
Fig. 2. The design of the pressure and displacement sensor
Application of Mechatronic System
1039

These model components, incorporated in one whole, represent 3D display of
designed models of mechatronic systems for hydroforming of welded sheets, Fig. 4.
3
Simulation of the Working Principle of Designed
Mechatronic System
For the modeling and simulation of mechatronic system, it is used a software Solid-
Works that uses Motion Manager, as an integrated software solution for creating
animations of SolidWorks assemblies. The resulting animations are a short ﬁlm
Fig. 3. The design of the measuring ampliﬁer and electronic device with laptop
Fig. 4. Model of mechatronic system for hydroforming welded sheets
1040
E. Šemić et al.

consisting of multiple frames or images displayed in sequence, in order to achieve
appropriate action and movement of the components of the mechatronic system
designed to simulate the real conditions [5, 7]. Simulation principles of designed
models of mechatronic systems is described through a few steps, Fig. 5.
Step 1. Order for start – start of performing the processes, at the same time activating the pump 
and electronic device by the computer (the program).
Step 2. The electronic device sends a signal (green) to the reversible valve in order to take a new 
position for the leakage of oil into the die. 
Step 3. The suppression of the oil under pressure to the system for shaping (the die) and the 
pressure sensor, where the pressure sensor senses the working pressure of oil at the entrance of 
the workpiece (the tool) and sends a signal to a measuring amplifier device SPIDER 8.
Fig. 5. Work simulation of mechatronic system for hydroforming welded sheets
Application of Mechatronic System
1041

Step 4. Displacement sensor registers the beginning of the spread the sheets (deformation) due to
effects of oil under pressure, and begin the process of creating and recording the change in the
spread sheet to achieve maximum displacement, and sends a signal to measuring amplifier
device "SPIDER 8".
Step 5. Measured values of pressure and displacement,registered by "SPIDER 8", are sent further 
to store the laptop (software memory). Hydroforming process is performed by monitoring and 
storing the data value of the working oil pressure and displacement.
Step 6. Formation of the workpiece is completed. The signal was send to the controller for the 
completion of the hydroforming process. A signal is sent to the reversiblevalve to take a new 
position for diverting the oil to the tank.
Fig. 5. (continued)
1042
E. Šemić et al.

The measured values of the process parameters, with the generation time intervals,
are stored in computer memory for subsequent processing and analysis.
4
Conclusion
The application of hydroforming process is increasingly important in industrial pro-
duction, which requires a wider exploration and processes and systems analysis. It was
done design of mechatronic system for this purpose and for improve the quality and
performance of experimental research using the automatic control of process and
process conditions.
Evaluation and veriﬁcation of the simulated model was made by performing the
process of hydroforming welded metal sheets for the experimental conditions. The
results will be used for modeling and optimization of the parameters and the application
in real conditions of execution of the process.
References
1. Matuško, J., Kolonić, F.: Design of Mechatronic Systems, Faculty of Electrical Engineering
and Computer Science, Lectures, Zagreb (2014)
2. Velagić, J.: Mechatronics, Faculty of Electrical Engineering, Sarajevo, Lectures (2012/2013)
3. http://www.tsrb.hr/meha/index.php?option=com_content&task=view&id=51&Itemid=1
4. Horvat, J.: Mechatronic didactic table. Final Paper, Technical College in Bjelovar, September
2016
5. Šemić, E.: The development of mechatronic systems for hydroforming of thin-walled
elements, Master work, Faculty of Mechanical Engineering in Mostar (2017)
6. Sing, H.: Fundamentals of Hydroforming. SME (2003)
7. Šemić, E., Karabegović, E.: The design of mechatronic systems for experimental
hydroforming of thin-walled elements. In: 3rd International Scientiﬁc Conference New
Technologies NT-2016 Development and Application, Mostar, May 2016
8. http://www.slideshare.net/tehnologije/senzori
Step 7. Endof the hydroforming process. The oil is returned to the tank. The tool is free to open 
and remove shaped piece. 
shaped piece
Fig. 5. (continued)
Application of Mechatronic System
1043

Modeling of Mechatronic Systems of Robot
Fingers
Sanela Hrnjica(&)
Technical Faculty, University of Bihac, 77 000 Bihać, Bosnia and Herzegovina
hrnjicasanela393@gmail.com
Abstract. In last few years there has been a very large increase in application
of robotics in everyday life. Among the most developed robots are certainly
humanoid robots, those kind of robots are similar to humans due to the large
number of possibilities. For humanoid robots mechatronic system of ﬁngers is
very important, starting from the mechatronic system of one ﬁnger. In this paper
it is also described mechatronic systems for robots with two, three, four and ﬁve
ﬁnger, the most complicated are certainly the ﬁve ﬁngered robots because they
are the most similar to human hand and they have the largest number of degrees
of freedom.
Keywords: Robotics  Humanoid robots  Mechatronic systems  Robot
ﬁngers  Degrees of freedom
1
Introduction
Thanks to the availability of advanced structures, improved actuators, minimized
electronic elements and sensors, the conformity of robotic structures is no longer
considered a problem. In fact, the design of advanced robotic systems ranges from a
classical concept to precision and rigid structures that are often heavy and very com-
plex, towards lighter and more ﬂexible structures with increased performance,
mechanically simpliﬁed, which lead to cost savings. There is currently a robotic rev-
olution in Japan that aims to create robots of the same man. Particular attention is
drawn to humanoid robots, they have high ﬂexibility and can be applied in the human
environment. Humane robots, such as humanoid robots, are expected to achieve
communication and coexistence with people. Human beings with the environment
usually communicate with their hands, so robots need hands to interact with the object.
The robotic arm can be divided into three categories: a mechanical handle, a hand with
special intent and a universal arm. Humanoid robots have a universal form of arm, that
form of hand tries to imitate the structure of the human hand. Up until now, several
research institutes have been able to develop several types of robotic arms that are
imitation of human hands in performing certain operations. The robot hand has
advanced functions that can be applied in several different ﬁelds since robots become
irreplaceable in space expeditions and other people’s dangerous jobs. However, this is
not enough to realize the movement of human hands, due to the lack of adequate
numbers of degrees of freedom. That is why a ﬁve-ﬁngered robotic arm developed to
match the size, shape and number of degrees of freedom to the human hand. Through
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_88

this paper, several different robot systems will be analyzed starting with the robotic arm
design, a three-ﬁnger mechatronic system, and a ﬁve-ﬁnger mechatronic system.
2
Development of Robot Fingers
The human hand is undoubtedly one of the most complex tools of nature. Scientists
study the characteristics and special features of this evolutionary design for years. The
results of these researches are used and implemented for the development of a robotic
arm. Rapid progress in the development of a robotic hand that mimics human move-
ment has advanced to developing a delicate handle with ﬁngers and thumbs. Advances
in microelectronics and micromechanics make it possible to produce hand-operated
ﬁngers with hand-to-hand controls and human-to-human joints. Some of the features
used in hand construction and hand design are that the ﬁnger is part of an anthropo-
morphic robotic arm based on the concept of structural and functional integration of a
robotic arm, so it is assumed that some of the functional hand components will be
contained inside the arm. The internal structure of the ﬁngers is endoskeleton so that
there are also outer layers as in the biological model of the human hand to increase
contact, adaptability and shaping the grip. The movement is enabled with linear
actuators, the ﬁnger must have sensors, the mechanical structure must be compact with
endoskeletal design and preferably simple, easy to produce and connect and bargain
and reliable. The kinematic ﬁnger structure must be compliant with manipulation tasks,
meaning that it must have at least 3 controlled degrees of freedom per ﬁnger (Fig. 1).
3
Robots with One Finger
Based on the observation of the human ﬁnger’s muscles, a robot’s ﬁnger was formed.
The muscles distributed on each ﬁnger phalanx play an important role for multiple
actions, for ﬂexing and prolonging the ﬁnger. The characteristic ﬁnger areas are shown
in the following illustration (Fig. 2):
The slider can be moved along the length, and the two sliders on each wrist are
connected to the joining rod. Therefore, by changing the position of the slider, the
desired angle of the wrist can be achieved (Fig. 3).
joints
tendons
Fig. 1. Fingers with most important parts
Modeling of Mechatronic Systems of Robot Fingers
1045

Sliders can be moved along the link, and the joint angle is ﬁxed. This provides a
different common moment and an additional factor to optimize the common moment on
each wrist (Fig. 4).
Joint 3
Joint 2
Joint 1
Fixed part
Connecting rod
Point on the slider
Top of the finger
Fig. 2. Finger with it’s parts
Fig. 3. Reaching the desired angle
Fig. 4. Longitudinal movement of the slider without moving the corners
1046
S. Hrnjica

In the manipulator’s ﬁnger structure there are small ultrasound motors, small in size
and weight. The ﬁnger force is very small due to the low thrust of the ultrasonic motor.
In order to overcome the ﬂaws of the manipulator’s ﬁnger, a solution for the installation
of brushless dc engines is designed. Such a ﬁnger manipulator consists of four links
with three hinges. Linear shifts can be achieved by minimizing dc-driven brushless
brush heads. Also, the sliders can be pushed off by a spring. The connecting rods are
used to connect the two skeletons around the wrist. The motor has a transmission ratio
of 1:125, 6 mm in diameter and a maximum torque of 0.0190 Nm (Figs. 5, 6).
slider 
Fig. 5. Front view of the ﬁnger
Anatomical 
structure of finger
Fig. 6. Comparison of the robot’s ﬁnger and the human ﬁnger
Modeling of Mechatronic Systems of Robot Fingers
1047

The robotic and man’s anatomy in the anatomical structure are very similar, and
they have been driven by the development of the ﬁnger robots so that they can do as
many operations as man performs and run (Fig. 7).
On the ﬁgure are shown different parts of a robot ﬁnger, those parts are:
• “distal interphalangeal joints” (DIJ or DIP), those between the second and third
(distal) phalanges
• “proximal interphalangeal joints” (PIJ or PIP), those between the ﬁrst (also called
proximal) and second (intermediate) phalanges
• The metacarpophalangeal joints (MCP) are situated between the metacarpal bones
and the proximal phalanges of the digits
4
Robots with Two Fingers
Robots with two ﬁngers are actually robots with two grips. The mechanism that drives
these grips is optimized so that two different contact regions are obtained. The ﬁrst
region is the one at the foot of the ﬁnger, the second region is the one at the tip of the
ﬁngers. The boundary between the two neighboring regions is the point of balance
(Fig. 8).
Apart from the mechanism used in each of the ﬁngers, the grips also rely on the
special clutch architecture between them. This is actually mechanically designed so that
these two ﬁngers can move in co-operation with each other (Fig. 9).
The design of a gripper must take into account several aspects of the components
and the system together with the peculiarities of a given application or a multi-task
purpose. Strong constraints for the gripping system can be considered lightness, small
dimensions, rigidity, multi-task capability, simplicity and lack of maintenance. These
Fig. 7. Parts of a robot ﬁnger
1048
S. Hrnjica

design characteristics can be achieved by considering speciﬁc end-effectors or grippers.
In the last case a two-ﬁnger gripper corresponds to the minimum number of ﬁngers and
the minimum complexity of an hand [1].
5
Robots with Three Fingers
The robot arm consisting of three articulated ﬁngers has one ﬁnger located in front of
the other two ﬁngers. Each one has three wrists (three phalanxes per ﬁnger). Some of
the hands with three ﬁngers have lower number joints than having a motor that allows
Region of fingertips
Equilibrium line
Region of finger pedestal
Fig. 8. Showing the ﬁngers mechanism with two different regions
Fig. 9. Ways of gripping
Modeling of Mechatronic Systems of Robot Fingers
1049

the ﬁngers to automatically adapt to the shape of the catch and thus facilitates the
control of the grips. There are four operating modes available with three-ﬁngered
hands. The ﬁrst mode is the basic mode and it is possible to capture objects of different
shapes, the other mode is widespread with that mode being mostly captured by large
dimension objects, the third mode is mode where only the tips of the ﬁngers are
captured by the objects, these are usually smaller objects and situations where needed
the precision of capturing, the fourth mode is so called scratch mode, so very small
objects are captured and usually only two ﬁngers take part in it (Fig. 10).
There are two ways of catching the ﬁngers, one is only with the toes of the ﬁngers,
then the object manages to stay between the ﬁngers because of the friction between the
ﬁngers and the object, the other way is when the ﬁngers are around the object, it is
recommended to use second mode because of the stability of the catch (Fig. 11).
Three ﬁnger grippers offer good centering possibilities for the adjustment of the
work piece on the gripper axis which is difﬁcult to realize with astrictive systems. This
is easy to achieve for work piece with axial symmetry but somewhat more difﬁcult for
prismatic work piece For a four-point contact the prehension forces act in two axial
directions. Unfortunately not every work piece can be handled in this manner and an
alternative is the three-point design for grippers. They can move along a curved path or
along a straight line towards the centre [2].
Fig. 10. Fingers operating modes
1050
S. Hrnjica

6
Robots with Four Fingers
Four-ﬁngered robots have one thumb and the other three ﬁngers, these three ﬁngers
each consist of three rigid connections, and the phalanxes are associated with three
joints. The thumb is similar to the other three ﬁngers except that it consists of two rigid
connections, while designing a four-ﬁnger robotic hand looks more like an adult man’s
face. The electronics hardware design consists of three main parts of a four-armed
robotic arm, a control unit, and a backward units. Each ﬁnger is connected to the DC
motor and the DC motor is controlled with the control unit. The kinematics of a robot
describes the relationship between the motion of the joints and the end effector. As for a
robotic hand, kinematic analysis can be speciﬁed as ﬁnding the relationship between
the joints and the position and orientation of the ﬁngertip. For serial manipulators,
forward kinematics is straightforward and the inverse kinematics is relatively difﬁcult.
In solving the inverse kinematics issues, various methods have been proposed. The
most commonly used methods are D-H transformation matrix method, 20 vector
algebra method, 21–23 geometric method, 24, 25 dual-number matrix method, 26, 27
screw coordinates method, 28, 29 exponential product method, 30 and qua-ternion
algebra method. As for the metamorphic mechanism, most research is concentrated on
the issue of mobility and representation of topology. However, only a few references
especially investigate kinematics of metamorphic mechanism. As a kind of hybrid
mechanism, there is no uniﬁed kinematic analysis method for the metamorphic robotic
hand to follow [3] (Fig. 12).
The reconﬁgurable palm is a closed-loop chain with two DOFs. Once two of the
joint angles are speciﬁed, the remaining joints will be also determined due to the
Fig. 11. The ways of catching
Modeling of Mechatronic Systems of Robot Fingers
1051

geometric constraints of the special closedloop chain. The geometric constrains of the
palm can be displayed by investigating the kinematics of the palm [3].
7
Robots with Five Fingers
Five-ﬁngered robots are most like human hands and can best help people in various
areas, an example of such a hand being the Shadow Dexterous Arm. The Shadow
Dexterous Hand is an advanced humanoid robotic handheld system that provides 24
degrees of freedom to reproduce the kinematics and man-mindedness as much as
possible. It is designed to provide comparable output power and precision of movement
as a human arm. Shadow Hand systems were used for exploration, manipulation, neural
control, industrial quality control, and handling of hazardous materials. Shadow
Dexterous Hand is a standalone system—everything is started and activated through
Arm and forearm. Shadow Dexterous Hand Set includes:
• control systems
• software
• PC
• Power
• collective reading (If needed)
• documentation and training (Fig. 13)
Fig. 12. Structure of four-ﬁngered hand
1052
S. Hrnjica

The hand is designed so that it is similar in shape and size to a typical male hand,
and to reproduce as much as possible the kinematics and abilities of the human hand,
the ﬁngers are all of the same length. The thumb has 5 degrees of freedom and 5 joints.
Each ﬁnger has 3 degrees of freedom and 4 joints (Fig. 14).
The distal joints of the ﬁngers are connected in a manner similar to the human
ﬁngers so that the angle of the central wrist is always greater than or equal to the angle
of the distal wrist. The small ﬁnger has an extra hinge in the palm that allows it to
counteract the thumb.
A microcontroller or microcontroller is used for all the ﬁngers, except for the thumb
to use a different microcontroller [4].
Fig. 13. Shadow hand
Modeling of Mechatronic Systems of Robot Fingers
1053

8
Conclusion
In this new era robotics has one of the most important roles, as the robots with all their
capabilities began to change people in doing dangerous jobs and began to use them as
everyday aids. As for the robotic arms, the idea was to achieve a degree of freedom as a
man’s arm, and the approach was development of a ﬁve-ﬁngered robotic hand, also two
auxiliary ﬁngers could be added to that hand, and with that we can have a hand with
seven ﬁngers. Certain robotic advances are expected, both industrial robots and service
robots.
Fig. 14. Hand with ﬁve ﬁngers (joints and motors)
1054
S. Hrnjica

References
1. Cuadrado, J., Naya, M.A., Ceccarelli, M., Carbone, G.: An optimum design procedure for
two-ﬁnger grippers
2. Krishnaraju, A., Ramkumar, R., Lenin, V.R.: Design of three ﬁngered robot-gripper
mechanism
3. Gao, Z., Wei, G., Dai, J.S.: Inverse kinematics and workspace analysis of the metamorphic
hand (2014)
4. http://www.shadowrobot.com/wp-content/uploads/shadow_dexterous_hand_technical_
speciﬁcation_E1_20130101.pdf
Modeling of Mechatronic Systems of Robot Fingers
1055

Application of Service Robots for Disinfection
in Medical Institutions
Aladin Begić(&)
Technical Faculty, University of Bihac, 77 000 Bihać, Bosnia and Herzegovina
aladinzizy@gmail.com
Abstract. Service robots are increasingly present in all ﬁelds of medicine. This
paper presents a review of the service robots in medicine with an emphasis on
service robots for disinfection in medical institutions. It is shown and described
how more and more disinfectant service robots are contributing to a very simple,
fast and effective disinfection in medical institutions. Work of the service robot
with all necessary components for its function as well as its good and bad sides
are in details elaborated and clariﬁed. The aim is to demonstrate the application
and use these service robots in medical institutions. Use of these service robots
reduces the risk of infection, cost of traditional cleaning and disinfection, and
most importantly acquires conﬁdence and security in medical facilities.
Keywords: Service robots  Medical facilities  Disinfection  Infection
1
Introduction
At each visit hospitals or clinical centers we try to leave everything, if possible. This is
because of the danger of new bacteria is high. It is mostly MRSA (methicillin-resistant
Staphylococcus aureus), C. diff. (Clostridium difﬁcile), VRE (Vancomycin-resistant
enterococci) and of new pathogens such as MERS (respiratory syndrome). These
microorganisms are resistant to antibiotics, and commonly referred to as “superbugs”.
As a result of infection by these pathogens often involve considerable pain and suf-
fering and many deaths. These infections are major problems and signiﬁcant costs of
modern health sector. Cleaning and disinfection are expensive and are not effective
enough due to inaccessible areas. Since there is no way to force people to disinfect
hands remains to introduce robots to disinfect[1].
The robots, which will be described in this paper is an attempt to reduce the risk of
hospital infections. There are many ways of transmitting infections, and studies have
shown that the greatest cause of contact surface such as: remote control, door handles
or cabinets, a button to call for help, etc. UV-C disinfection robot provides an eco-
nomical and effective measure in limiting the spread of bacteria. When bacteria are
exposed to UV-C light of their DNA absorbs light energy and causes cell damage that
prevents new infecting others. The robot is controlled by the medical staff and the time
for which disinfect a room is 10–15 min depending on the size of the room. When
operating the robot nobody should be present in the room because the UV-C light
damages eyesight and adversely affects organism people.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_89

Studies have shown that this method of disinfection kills more than 70% of bacteria
compared to the traditional way, so it is necessary to introduce these robots in hospital
for prevention, reducing the spread of infections and reduce the cost of treatment of the
same.
2
UV Light
UV-C has been a proven technology for disinfecting air, water and instruments for over
a century. Niels Finsen was awarded the Nobel Prize for Medicine in 1903 for being
the ﬁrst to use “light therapy” to treat disease with direct disinfection of skin. By the
1930s, UV had come into common use throughout hospitals for air and water treatment
and by WWII, UV was in widespread use in processing plants, water treatment
facilities, and anywhere microbial contamination was a concern. UV gained fame in the
1950s for helping to eradicate TB before fading in use in the 1960s with the prolif-
eration of antibiotics and chemical disinfectants [2, 3].
Now with the current focus on solving the crisis of rising multidrug resistant
organisms (MDROs), healthcare acquired infections (HAIs), and treatment costs, UV
has once again risen to the top of the list in the war against superbugs. After 100 years
in healthcare use, UV has found a new application in hospitals providing surface
disinfection of patient rooms, bathrooms, operating rooms (ORs), equipment rooms,
and mobile devices[4].
The sun is by far the strongest source of ultraviolet radiation in our environment.
Solar emissions include visible light, heat and ultraviolet (UV) radiation. Just as visible
light consists of different colours that become apparent in a rainbow, the UV radiation
spectrum is divided into three regions called UVA, UVB and UVC. As sunlight passes
through the atmosphere, all UVC and most UVB is absorbed by ozone, water vapour,
oxygen and carbon dioxide. UVA is not ﬁltered as signiﬁcantly by the atmosphere.
Particularly at wavelengths around 260–270 nm, UV breaks molecular bonds
within microorganismal DNA, producing thymine dimers that can kill or disable the
organisms [3].
• Mercury-based lamps emit UV light at the 253.7 nm line.
• Ultraviolet Light Emitting Diodes (UV-C LED) lamps emit UV light at selectable
wavelengths between 255 and 280 nm.
• Pulsed-xenon lamps emit UV light across the entire UV spectrum with a peak
emission near 230 nm [3] (Fig. 1).
The relatively long-wavelength UVA accounts for approximately 95% of the UV
radiation reaching the Earth’s surface. It can penetrate into the deeper layers of the skin
and is responsible for the immediate tanning effect. Furthermore, it also contributes to
skin ageing and wrinkling. For a long time it was thought that UVA could not cause
any lasting damage. Recent studies strongly suggest that it may also enhance the
development of skin cancers.
Application of Service Robots for Disinfection
1057

3
Service Robots in Medicine
Robots have broad application in healthcare. Such robots include roving machines
mounted with ipads to provide physician tele-presence, surgical assistance robots such
as the Da Vinci system, drones for delivery of emergency or other medical equipment,
assistive and therapeutic robotic devices used to increase the individual’s capability or
rehabilitate, empathic robots used in the care of the older or physically/mentally limited
individual, and industrial robots such as those used to sterilize patient rooms or for
supply delivery. Other robots are in research and development stage now and still other
applications of robotics in healthcare are being considered for the future. The world of
service robots is in its infancy [5, 6] (Fig. 2).
Fig. 1. Comparison of UV LED and mercury lamp
Fig. 2. Service robots in medicine
1058
A. Begić

Robots and robotics have entered the healthcare arena in a dramatic manner [5].
Countless needs are being addressed in new and different ways, and sometimes, for the
ﬁrst time! Robots, already, have wide-ranging healthcare applications within surgery,
ambulation in the disabled, hospital operations, neuro-muscular rehabilitation, and
emotional care and aging care, to name a few. Robust, exciting research of new
applications of robotics in healthcare is thriving [5].
As robots take care of our more intimate needs, such as personal caregiving, human
to robot and robot to human interactions will become a central focus of study and
philosophical discussion. There is much unknown regarding the ultimate acceptability
of robots in intimate settings, or at work. Comfort with robots may depend on multiple
variables, such as the individual, culture, particular application, or industry. Trust is at
the core of the use of autonomous robots in healthcare, and safety must be proven [7].
Once the qualiﬁcations of optimal design, answered needs, safety, and trust are met,
“the sky is the limit” for robots and robotics in health [5]!
4
Service Robots for Disinfection
To solve the hospital disinfection problem, several design requirements had to be
imposed. There are a myriad of design requirements for a commercial robotic disin-
fector and even more if the robot is to operate in a hospital. The robot produces UV
light in a hospital room and in 5 min it can drastically reduce the germs in room [8]
(Fig. 3).
The device is run when the room is empty after a patient discharge and terminal
cleaning. The xenon bulb in the device will pulse for 5 min disinfecting an area around
the device. During this time the user stays outside the room. UVC light cannot go
through safety glasses, walls or windows. However with prolonged exposure UVC
Fig. 3. Position of the robot in the room
Application of Service Robots for Disinfection
1059

could damage eyes so always run the robot in an empty room. For additional safety
there is an orange cone that stays outside of the room and guards at the door as well as
caution signs for the door. Inside the room the gray cone watches the entrance to the
room and detects motion. Should motion be detected during the pulsing of the light the
gray cone will turn the device off you will use the device after you’ve ﬁnished cleaning
a room but before the bed is made. For most rooms treating the bathroom with the UVC
light ﬁrst will save time because it is possible to work in the room while it is treating
the bathroom (Fig. 4).
Like common illnesses for which humans ingest antibiotics, each pathogen that
causes the most common healthcare associated infections has a known dosage,
speciﬁcally a UVC dosage, at which it is deactivated or terminated. UVC disinfection
dosage is a function of total intensity of UVC light and the length of exposure. Using
higher intensity UVC light decreases the time needed to reach the appropriate dosage.
Likewise, using lower intensity UV light lengthens the amount of time needed to reach
the right UV dosage to kill dangerous pathogens [3].
What that means is that running a UV robot for less time than needed to achieve the
germ-killing UV dosage enables HAI-causing pathogens to survive, creating the
opportunity for patients to become seriously ill. The best way to prevent under-dosing,
is to use UV robots that automatically measure room conditions in real time to calculate
the power and time needed to achieve the right dosage.
With germ-killing UV light robots, there’s no guessing when disinfection is
achieved. This patent pending SmartDosage UV technology incorporates proprietary
algorithms that automatically adjust UVC dosage and treatment time as the robot
operates, ensuring effective, complete treatment irrespective of variables such as room
size, layout, furnishings, and environmental characteristics.
Figure 5 shows a robot which is used for disinfection of surgical theaters. From this
we conclude that their use in medical institutions is represented in all departments.
UVC robots provide hospitals, nursing homes and other critical care environments with
the assurance that dangerous pathogens like Clostridium difﬁcile (C. diff), Acineto-
bacter and M.R.S.A., to name a few, are attacked before the next patient occupies the
room [7].
Fig. 4. Room disinfection process
1060
A. Begić

There is a large database of UVC effect on various organisms, from bacteria and
viruses to fungi and spores. Standard term used effective dose is deﬁned as the UVC
light required to inactivate 90% of a given population (also called log1). Effective dose
is deﬁned as H UVC power  time/irradiated area (Ws/m2). For example, E. coli
requires a dose 30 Ws/m2 while the standard fungi like Aspergillus niger requires a
dose of 1320 Ws/m2.
Headgear-mers virus is one of RNA viruses and are categorized together should
dose between 15–400 Ws/m2. Examples of well-known RNA virus doses are 110 Ws
poliovirus/m2, Newcastle disease 15 Ws/m2 SARS 226 Ws/m2. The assessment mea-
sured in the same region as high SARS can calculate real-time inactivity like to achieve
90% [3, 9].
5
Robot “IRIS 3200 m”
IRIS 3200 m is the most powerful system for disinfecting UV light in the world. His
continuous UVC generate up to 20 times the UVC output tested xenon pulse system
and three times the power of other constant light competitors.
The patented PowerBoost technology enables iris 3200 m UVC robot to provide
the whole room disinfection one position in far less time than any other unit on the
market, but unlike most of the competition, attacking shadows where many of the
harmful organisms are staying (in some cases many months).
Fig. 5. Disinfection process of surgical theaters
Application of Service Robots for Disinfection
1061

Built Patent SmartDosage, together with the patented box Balance and PowerBoost
technology allow iris 3200 m UV lighting system of disinfection for automatically
measuring conditions the room environment, such as the size of the room, temperature
and humidity in order to determine in real time the appropriate dose, time, number and
power of the lamp is required for complete disinfection of all while providing maxi-
mum power permitted in the United States for the production of germicidal UVC
energy.
IRIS 3200 m is ideal for the hospital which require the maximum disinfection least
amount of time. Managed by easy-to-use, wireless handheld Steri-Strip controller, iris
3200 m germ killing robot frees Environmental Services staff perform other tasks while
the system is disinfected the whole room in one procedure.
Beneﬁts Iris 3200 m UV light disinfection system are:
• Faster treatment room
• Higher productivity
• More effective treatments
• Highly pathogenic kills prices
• Whole-room treatments
• One placement
• One treatment
Fig. 6. Robot IRIS 3200 m
1062
A. Begić

6
Xenex Germ-Zapping Robots
High intensity ultraviolet light is produced by xenon ﬂash lamps across the entire
disinfecting spectrum known as UV-C. This UV-C energy passes through the cell walls
of bacteria, viruses and bacterial spores. The DNA, RNA and proteins inside the
microorganism absorb this intense UV-C energy. Xenex Full Spectrum UV-C provides
four mechanisms of damage against pathogens (Fig. 6).
Pathogens are vulnerable to UV-C light damage at different wavelengths depending
on the organism. Xenex’s Pulsed Xenon lamps produce a ﬂash of Full Spectrum
germicidal light across the entire disinfecting spectrum (from 200 to 320 nm) delivered
in millisecond pulses [10, 11].
The primary types of cellular damage caused by Pulsed Xenon UV are photohy-
dration (pulling water molecules into the DNA that prevents transcription), photo-
splitting (breaking the backbone of the DNA), and photodimerization (improper fusing
of DNA bases), all of which prevent cell replication. Additionally, photo crosslink-
ing causes cell wall damage and can cause cell lysis, an irreversible form of cell death.
Disinfecting across the entire spectrum helps prevent pathogens from repairing them-
selves [11] (Fig. 7).
Fig. 7. Four mechanisms of damage against pathogens
Application of Service Robots for Disinfection
1063

There are cheaper ways to generate disinfecting UV light. For example, mercury
lamps have been used to disinfect surfaces and liquids for decades, and the bulbs are
only about $100. However, they are 25,000 times less intense than a Xenon bulb and
the disinfection process can take hours, making them impractical for hospital use.
LEDs could also provide cheaper UV light, but they are also far less intense than
Xenon bulbs, according to Hart [5, 11] (Fig. 8).
Advantage of UV light is that it kills germs without the use of chemicals. But
surfaces need to be cleaned of ﬂuids and dust ﬁrst. UV light adds a ﬁnal layer of
protection. Xenex shine will destroy anything we can’t see [5]. When produced arti-
ﬁcially here on Earth, UV-C rays can be blocked by a barrier as thin as a plastic bag.
The robot comes with a motion sensor that turns itself off if it senses movement in the
room while it is at work [3, 9].
7
Conclusion
Study showed that a “no-touch” semi-automated system, the UV light, was effective in
substantially reducing the heterotrophic bacterial and MRSA burden on high-touch
surfaces in rooms vacated by MRSA-positive patients. UVC disinfection may add to
the armamentarium against HAI’s without risking the adaptive genetic resistance
incurred by pharmaceutical weapons. Implementation including training personnel to
operate the device is minimal, and time spent cleaning was not increased. Because there
Fig. 8. Xenex germ-zapping robot with LED xenon bulb
1064
A. Begić

were separate cycles for bathroom and living room, the surface reduction in aerobic
colony counts may be better than with other UV systems; a head-to-head comparison of
UV area disinfection devices may be warranted.
References
1. Karabegović, I., Doleček, V.: The role of service robots and robotic systems in the treatment
of patients in medical institutions. In: International Symposium on Innovative and
Interdisciplinary Applications of Modern Technology (IAT), Eight Days BHAAAS-a u
BiH—Hotel “SUNCE” Neum, Bosnia and Herzegovina, Neum (2016)
2. https://blue-ocean-robotics.com/uv-disinfection/
3. Introduction to UV Disinfection. Trojan UV (2012). Accessed 24 May 2012
4. https://en.wikipedia.org/wiki/Medical_robot
5. http://www.cbsnews.com/news/germ-zapping-robot-combats-hospital-infections/
6. Karabegović, I., Doleček, V.: Servisni roboti. Društvo za robotiku Bihać, Bihać (2012)
7. https://bmcinfectdis.biomedcentral.com/articles/10.1186/1471-2334-14-187
8. Rosoff, M.S.: Robotic Doorknob Disinfector
9. Ultraviolet Disinfection Guidance Manual for the Final Long Term 2 Enhanced Surface
Water Treatment Rule. United States Environmental Protection Agency, Washington, DC,
November 2006. Accessed 30 Jan 2011
10. https://www.intechopen.com/books/service_robot_applications
11. Serenus Biotherapeutics to Launch Xenex UV Disinfection Robot at IPNET-K Conference
2015 in Naivasha, Kenya
12. Karabegović, I., Karabegović, E., Husak, E., Mahmić, M.: Application of service robots for
safety and protection of health. In: 4th International Professional and Scientiﬁc Conference,
19–22 September 2012, Zadar, Croati, pp. 171–178 (2012). ISBN 978-953.7343-59-0
Application of Service Robots for Disinfection
1065

Detection of Parkinson’s Disease by Voice Signal
Fatima Mašić(&), Mehmed Đug, Jasna Nuhić, and Jasmin Kevrić
International Burch University, Sarajevo, Bosnia and Herzegovina
fatima_masic@live.com, mehmed.djug@outlook.com, jasna.
nuhic96@gmail.com, jasmin.kevric@ibu.edu.ba
Abstract. Detection of Parkinson Disease by Voice Signal is based on non-
invasive method for disease detection. Here we used Speech Dataset of sound
records which has been shown as most effective up to now. In order to detect
presence of disease by using different classiﬁers. At the end accuracy of each of
them have been calculated and compared.
Keywords: Parkinson disease  Telemonitoring  Voice signal processing
1
Introduction
Worldwide neurological disorders affect people profoundly and claim lives at an epi-
demic rate, with Parkinson’s disease (PD) as second most common neurodegenerative
disorder after Alzheimer’s [1]. Neurodegenerative disorder of central nervous system
that causes partial or full loss in motor reﬂexes, speech, behavior, mental processing,
and other vital functions is PD [2]. It is progressive disease, where symptoms get worse
with time and PD progression cannot be stopped; patient’s life can prolong by phar-
maceutical and surgical intervention which can mitigate the effect of some of the
symptoms [1].
Clinicians have devised a number of methods to quantify PD symptom severity,
and the most widely used metric is the Uniﬁed Parkinson’s Disease Rating Scale
(UPDRS), which reﬂects the presence and severity of symptoms (but does not measure
their underlying causes). Monitoring PD progression is critical because this enables
improved patient-directed treatment [1].
PD monitoring require many shortcomings:
(1) Frequent physical presence of patient in the clinic is required, which carries
logistical and ﬁnancial difﬁculties for patient and their carers, especially as disease
improves [1].
(2) Expert clinical staff availability is required for testing and assessment of the
patient’s symptoms in order to determine the UPDRS score [1].
(3) Assessment of UPDRS is subjective so there is disagreement between different
expert clinical raters on the reported scores (inter-rater variability) [1, 3–6].
(4) Accommodation of patients and allocation of expensive human resources is costly
for national health systems, which need to provide facilities for it [1].
(5) UPDRS examination lasts for more than 2 h, which is time consuming (for
assessing PD severity both “off” and “on” medication) [1].
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_90

Currently, for all these reasons PWP (People with Parkinson’s) will only have
UPDRS assessed once every three to 6 months because of the resource scarcity
available to patient, carers, and clinical staff. Compelling solution to accurately and
efﬁciently follow PD progression at more frequent intervals with less cost and minimal
waste of resources is frequent remote monitoring. An emerging option in general
medical care, potentially affording reliable, cost-effective screening of PWP, and
potentially alleviating the burden of frequent, and often inconvenient, visits to the clinic
is noninvasive telemonitoring. This way national health system relives from excessive
additional workload, by increasing the accuracy and decreasing the cost of clinical
evaluation of the subject’s condition [1].
PD have been linked to speech disorders [7–9], with strong supporting evidence of
degrading performance in voice with PD progression [10, 11]. The purpose of tele-
monitoring ﬁts ideally with speech signals, because they are non-invasive, can be
self-recorded, and are easy to obtain from a subject who is not expected to perform any
special kinds of actions in order to record his voice. In the research community differ-
entiating PWP from healthy controls using speech has attracted interest [1, 10, 12, 13].
2
Parkinson’s Disease: Mechanisms, Symptoms
and Diagnosis
The underlying cause of PD is unknown [14], but the symptoms are caused by sub-
stantial dopaminergic neuron reduction, leading to dysfunction of the BG which
mediates motor and some cognitive abilities [15]. The dopaminergic cells assist in
neurotransmission; consequently their decline leads to malfunction of the CNS which
can no longer co-ordinate muscle movements appropriately and delicately [1].
The clinically discernible symptoms of the disease appear when around 60% till
80% of the dopaminergic cells are dead [16]. The disease has progressed considerably
and at that point it is late to act on the degradation. The disease’s progressive evolution
involves dopaminergic loss where appear gradually more intense symptom, for
example it can be loss of muscle control and tremor [1].
The major symptoms of the disease are movement disorders, tremor and rigidity.
A common appearance is vocal damage [17, 18] and it is met between 70 and 90%
PWP [18–20]. In addition, it might be one of the ﬁrst indicators [21] and 29% of
patients think of it as one of their most noteworthy impediments related with the
disease [20]. Regularly, the symptoms at ﬁrst show up singularly (on either the right or
left side, showing that dopaminergic loss is more expressed in the BG of the brain
hemisphere) however in time continue respectively [1].
There is no general agreement for diagnosing a patient with Parkinson disease,
which leads to the wrong diagnosis [14, 22]. There are three criteria for diagnosing PD
and they are tremor, bradykinesia and rigidity [23]. If the patient fulﬁlls two of the three
criteria then it can be said that he has PD [24]. Moreover, if the person is known to
experience the ill effects of essential tremor, in that case there need to be all three
criteria satisﬁed for making the PD diagnosis [1, 24].
Detection of Parkinson’s Disease by Voice Signal
1067

Because of the utilization of pharmacopathological control (tranquilize treatment of
PD), the mean lifetime of PWP malady has expanded essentially over the earlier
decades. Presently, it is assessed that a patient determined to have PD at 62 years old is
relied upon to live for around 20 more years [22]. Pharmaceutical (mixes of levodopa
and different specialists) and surgical intercessions, for example, Deep Brain Stimu-
lation (DBS) [25] are archived to enhance motor functionality and decrease tremor,
deferring illness movement and offering sensibly great personal satisfaction [15]. The
effect of treatment on speech is uncertain [1, 9, 26].
During inhalation and exhalation (inspiration and expiration phases) the lungs
expand and collapse, and the air ﬂows into and out of the lungs. The process of
inspiration and expiration is controlled by a muscle call diaphragm. Diaphragms also
expands and collapse according to the process of breathing. According to this study, the
respiratory muscle control have threat to be damaged in PWP, which somewhat clar-
iﬁes why those subjects frequently neglect to have the capacity to create prolonged
vocal effort, by correlation with age and sexual orientation with the healthy speakers.
Accordingly of the examination of our dataset, in parallel to the outcomes revealed in
the writing, it is found that sustained vowels, compared to short sentences and isolated
words, carry more PD discriminative information. Using acoustic tools and aperiodic
vibrations in the voice, voice disorders can be measured [1] (Fig. 1).
3
Extraction of Features
There is no single “right” strategy for extraction of features, because any tool for signal
processing can be used combined with time series analysis tool to extract information
from speech signals which can be useful for medical applications [1].
In order to have measures robust for common speech signal analysis, the extraction
of features is made from voice samples (i.e. to have more precise view on the situation
with no confounding factors and without assumptions). Computation of fundamental
frequency is required for dysphonia measures. The precise assessment of the
Fig. 1. Schematic diagram of the major parts involved in the production of speech
1068
F. Mašić et al.

fundamental frequency is critical for characterization of speech signals [26], that dif-
ﬁculty was reason for developing different algorithms [1].
Extraction of features from voice samples used in studies are:
• Jitter and Jitter variants—Jitter focus on instabilities of the oscillating pattern
produced by the affected vocal folds (affected with PD) quantifying the change in
fundamental frequency [1].
• Shimmer and shimmer variants—Shimmer focus on instabilities of oscillating pat-
tern produced by the affected vocal folds quantifying the change in amplitude [1].
• Harmonics to Noise Ratio (HNR) and Noise to Harmonics Ratio (NHR)—Incom-
plete vocal fold closure gives as result increased noise because of turbulent airﬂow.
HNR and NHR evaluate the difference of noise signal and actual signal [1].
• Linear Predicting Coding Coefﬁcients (LPCC)—Quantify deviations of the forecast
of the present data sample as a function of the previous samples. In pathological
voices this deviation is relied upon to be bigger [1].
• Mel Frequency Cepstral Coefﬁcients (MFCC)—PD inﬂuences the articulators
(vocal tract) notwithstanding the vocal folds, and the MFCCs endeavor to analyse it
independently from the vocal folds [1].
• Glottal to noise excitation (GNE)—Using nonlinear energy and energy concepts for
volume of noise in speech [1].
• Detrended Fluctuation Analysis (DFA)—Quantify the random self-similarity of the
noise induced by turbulent airﬂow [1].
• Recurrence Period Density Entropy (RPDE)—Quantify the random segment of the
deviation of vocal fold periodicity [1].
• Pitch Period Entropy (PPE)—In speech it is extremely hard to manage stable pitch
because of incomplete vocal fold closure. PPE measures the impaired control of
balanced out pitch [1].
• Wavelet measures—Quantify deviations in (acquired utilizing any estimation
algorithm) [1].
• Empirical mode decomposition excitation ratio (EMD-ER)—Signal to noise pro-
portions utilizing EMD-based energy, entropy and nonlinear energy [1].
• Vocal fold excitation ratio (VFER)—Using energy, entropy and nonlinear energy
concepts for volume of noise in speech [1].
The wavelet dysphonia measures characterized here lessen the underlying vector
space with components equivalent to the length of the shape, to a decreased space
equivalent to the quantity of computed features [1].
4
Classiﬁers
From examined classiﬁers in Weka, which is machine learning software to solve data
mining problems, we used:
• Ibk—Identify the nearest neighbor of a given observation vector from among a set
of training vectors is conceptually straightforward with n distance calculations to be
Detection of Parkinson’s Disease by Voice Signal
1069

performed. However, as the number n in the training set becomes large, this
computational overhead may become excessive.
• Logistic—determine an outcome by statistical method for analyzing a dataset in
which there are one or more independent variables. The outcome is measured with a
dichotomous variable (in which there are only two possible outcomes) [27].
• Multilayer Perceptron—feed forward artiﬁcial neural network model that maps sets
of input data onto a set of appropriate outputs. An MLP consists of multiple layers
of nodes in a directed graph, with each layer fully connected to the next one [28].
• Random Forest—combination of tree predictors such that each tree depends on the
values of a random vector sampled independently and with the same distribution for
all trees in the forest [29].
• SMO—Can be regarded as the extreme limit of chunking. For SMO it is possible to
ﬁnd an analytic solution to the smallest possible QP problem. The time complexity
of this system is also subquadratic, and the space complexity (memory requirement)
linear [30].
5
Results and Analysis
Training and test data are consisted of 26 features from frequency, pulse, amplitude,
voicing, pitch and harmonicity parameters and 1 class feature. In training data set we
removed UPDRS feature as it directly indicates if patient has PD.
From examined classiﬁers in Weka, best accuracy have Random Forest, IBk and
Multilayer Perceptron.
In tables below “Accuracy” results are for train data consisted of 26 samples
(1 sustained vowel “a”, 1 sustained vowel “o”, 1 sustained vowel “u”, 10 numbers from
1 to 10, 4 short sentences and 9 words) for each of 40 patients (20 healthy and 20 PWP)
and test data consisted of 6 samples (3 sustained vowel “a” and 3 sustained vowel “o”)
which were said by 28 PD patients (Fig. 2).
Fig. 2. Accuracy of different classiﬁers
1070
F. Mašić et al.

Testing results of IBk
IBk
Test option
Accuracy (%)
Accuracy1 (%)
Training set
80.10
60
Supplied test set
64.29
79.76
Cross validation
67.40
45
Percentage split
66.10
37.04
0.00%
20.00%
40.00%
60.00%
80.00%
100.00%
Training set
Supplied test
set
Cross
validaƟon
Precentage
split
IBk
IBk Accuracy
IBk Accuracy1
Testing results of Random Forest
Random Forest
Test option
Accuracy (%)
Accuracy1 (%)
Training set
100.00
100
Supplied test set
54.76
64.29
Cross validation
70.29
55
Percentage split
71.19
48.15
0.00%
50.00%
100.00%
150.00%
Training set
Supplied test
set
Cross
validaƟon
Precentage
split
Random Forest
Accuracy
Accuracy1
Testing results of Multilayer Perceptron
Multilayer Perceptron
Test option
Accuracy (%)
Accuracy1 (%)
Training set
76.54
92.50
Supplied test set
57.14
61.90
Cross validation
66.44
62.50
Percentage split
65.54
70.37
Detection of Parkinson’s Disease by Voice Signal
1071

0.00%
20.00%
40.00%
60.00%
80.00%
100.00%
Training set
Supplied test
set
Cross
validaƟon
Precentage
split
MulƟlayer Perceptrone
Accuracy
Accuracy1
“Accuracy1” stands for train and test data explained before with just leaved sus-
tained vowel “a” samples.
Generally “Accuracy” has higher results while using Cross validation and Per-
centage split test options. Because huger training set in that case is used.
6
Conclusion
While using Supplied test set option higher percentage were for “Accuracy1”, because
we used samples of same kind. And we get even 79.76% accuracy for disease pre-
diction. Which is enough to conclude that detection of precision disease is possible by
using voice signal. But must be more explored to get higher accuracy which is needed
in order to have remote monitoring which will much ease following of PD progression
by this noninvasive technology.
References
1. Sakar, B.E., Erdem Isenkul, M., Okan Sakar, C., Sertbas, A., Gurgen, F., Delil, S., Apaydin,
H., Kursun, O.: Collection and analysis of a parkinson speech dataset with multiple types of
sound recordings. IEEE J. Biomed. Health Inform. 17(4), 828–834 (2013)
2. Tsanas, A.: Accurate telemonitoring of Parkinson’s disease symptom severity using
nonlinear speech signal processing and statistical machine learning (2012)
3. Rajput, A.H., Rozdilsky, B., Rajput, A.: Accuracy of clinical diagnosis in parkinsonism, a
prospective study. Can. J. Neurol. Sci. 18(3), 275–278 (1991)
4. Hughes, A.J., Daniel, S.E., Blankson, S., Lees, A.J.: A clinicopathologic study of 100 cases
of Parkinson’s disease. Arch. Neurol. 50, 140–148 (1993)
5. Ramaker, C., Marinus, J., Stiggelbout, A.M., van Hilten, B.J.: Systematic evaluation of
rating scales for impairment and disability in Parkinson’s disease. Mov. Disord. 17, 867–876
(2002)
6. Post, B., Merkus, M.P., de Bie, R.M.A., de Haan, R.J., Speelman, J.D.: Uniﬁed Parkinson’s
disease rating scale motor examination: are ratings of nurses, residents in neurology, and
movement disorders specialists interchangeable? Mov. Disord. 20(12), 1577–1584 (2005)
7. Darley, F.L., Aronson, A.E., Brown, J.R.: Differential diagnostic patterns of dysarthria.
J. Speech Hear. Res. 12, 246–269 (1969)
8. Gamboa, J., Jimenez-Jimenez, F.J., Nieto, A., Montojo, J., Orti-Pareja, M., Molina, J.A.,
Garcia-Albea, E., Cobeta, I.: Acoustic voice analysis in patients with Parkinson’s disease
treated with dopaminergic drugs. J. Voice 11, 314–320 (1997)
1072
F. Mašić et al.

9. Ho, A., Bradshaw, J.L., Iansek, R.: For better or for worse: the effect of Levodopa on speech
in Parkinson’s disease. Mov. Disord. 23(4), 574–580 (2008)
10. Harel, B., Cannizzaro, M., Snyder, P.J.: Variability in fundamental frequency during speech
in prodromal and incipient Parkinson’s disease: a longitudinal case study. Brain Cognit. 56,
24–29 (2004)
11. Skodda, S., Rinsche, H., Schlegel, U.: Progression of dysprosody in Parkinson’s disease
over time, a longitudinal study. Mov. Disord. 24(5), 716–722 (2009)
12. Little, M.A., McSharry, P.E., Hunter, E.J., Spielman, J., Ramig, L.O.: Suitability of
dysphonia measurements for telemonitoring of Parkinson’s disease. IEEE Trans. Biomed.
Eng. 56(4), 1015–1022 (2009)
13. Lang, A.E., Lozano, A.M.: Parkinson’s disease—ﬁrst of two parts. New Engl. J. Med. 339,
1044–1053 (1998)
14. Singh, N., Pillay, V., Choonara, Y.E.: Advances in the treatment of Parkinson’s disease.
Prog. Neurobiol. 81, 29–44 (2007)
15. Bernheimer, H., Birkmeyer, W., Hornykiewicz, O., Jellinger, K., Seitenberger, F.: Brain
dopamine and the syndromes of Parkinson and Huntington. J. Neurol. Sci. 20, 255–425
(1973)
16. Hanson, D., Gerratt, B., Ward, P.: Cinegraphic observations of laryngeal function in
Parkinson’s disease. Laryngoscope 94, 348–353 (1984)
17. Ho, A., Iansek, R., Marigliani, C., Bradshaw, J., Gates, S.: Speech impairment in a large
sample of patients with Parkinson’s disease. Behav. Neurol. 11, 131–137 (1998)
18. Logemann, J.A., Fisher, H.B., Boshes, B., Blonsky, E.R.: Frequency and coocurrence of
vocal tract dysfunctions in the speech of a large sample of Parkinson patients. J. Speech
Hear. Disord. 43, 47–57 (1978)
19. Hartelius, L., Svensson, P.: Speech and swallowing symptoms associated with parkinson’s
disease and multiple sclerosis: a survey. Folia Phoniatr. Logop. 46, 9–17 (1994)
20. Duffy, J.R.: Motor Speech Disorders: Substrates, Differential Diagnosis and Management,
2nd edn. Mosby, New York (2005)
21. Rajput, M., Rajput, A., Rajput, A.H.: Epidemiology (Chapter 2). In: Pahwa, R., Lyons, K.E.
(eds.) Handbook of Parkinson’s disease, 4th edn. Informa Healthcare, USA (2007)
22. de Rijk, M.C., Rocca, W.A., Anderson, D.W., Melcon, M.O., Breteler, M.M.B.,
Maraganore, D.M.: A population perspective on diagnostic criteria for Parkinson’s disease.
Neurology 48, 1277–1281 (1997)
23. Rajput, A.H., Rozdilsky, B., Ang, L., Rajput, A.: A signiﬁcance of Parkinsonian
manifestations in essential tremor. Can. J. Neurol. Sci. 20, 114–117 (1993)
24. Benabid, A.L., Chabardes, S., Mitrofanis, J., Pollak, P.: Deep brain stimulation of the
subthalamic nucleus for the treatment of Parkinson’s disease. Lancet Neurol. 8, 67–81
(2009)
25. Larson, K., Ramig, L.O., Scherer, R.C.: Acoustic and glottographic voice analysis during
drug-related ﬂuctuations in Parkinson’s disease. J. Med. Speech Lang. Pathol. 2, 211–226
(1994)
26. https://www.medcalc.org/manual/logistic_regression.php
27. Breiman, L.: Randm Forests. Statistics Department, University of California Berkeley (2001)
28. Campbell, C., Cristianini, N.: Simple Learning Algorithms for Training Support Vector
Machines. University of Bristol, UK
29. Sapir, S., Ramig, L., Spielman, J., Fox, C.: Formant centralization ratio (FCR): a proposal
for a new acoustic measure of dysarthric speech. J. Speech Lang. Hear. Res. 53, 114–125
(2010)
30. Christensen, M.G., Jakobsson, A.: Multi-pitch Estimation, Synthesis Lectures on Speech and
Audio Processing, Morgan & Claypool Publishers (2009)
Detection of Parkinson’s Disease by Voice Signal
1073

Modeling of Sensors for a Mechatronic
Systems of an Robotic Arm
Silmija Ferizović(&)
Technical Faculty, University of Bihać, 77 000 Bihać, Bosnia and Herzegovina
silmija.ferizovic@hotmail.com
Abstract. Modeling of sensors for a mechatronic systems of an robotic arm is
being considered as a very complex matter, whom is being approached through
several aspects. Anatomy of a human skin and structure of a most complex
human sense, sense of touch, is being in details analized. Analogy between a
human sense of touch and achievement of a robotic sense of touch is being
pointed up. Application of which components made a robotic sense of touch
realistic is also very important to mention. Sensors applicated in robotics are
considered as a primary components who contributed that robots can have
ability to feel things and “to know” how to properly react on stimuli from the
environment. Work principle of sensors and sensors in general are also in details
explained. Emphasis is placed on a tactile perception which is giving us
informations about mechanical properties and position of objects, and on tactile
sensors which are mimicking sensitivity of a human ﬁngers and are placed in a
robotic arm.
Keywords: Finger  Robotic arm  Mechatronic system  Sensor  Robot
1
Introduction
The sensor or transducer is a device that measures the physical quantity (for an
example: temperature, humidity, pressure, engine speed) and is converting it into a
signal suitable for further processing (most commonly into an electrical signal).
Applied in robotics, sensors allow the robot to adequately perceive and act on its
environment. Various divisions of sensors can be found in literature, from very simple
to complex ones. They are commonly classiﬁed by: speciﬁcations characteristics, ways
of detection, type of conversion, the materials of construction and application. The
sensors represent an essential element on which mechatronic system of an robotic arms
is based on. In practice, sensors of position and environment sensors are being used as
an indispensable element. Sensors of position include: resolvers, encoders and poten-
tiometers and environment sensors include: force sensors, tactile sensors, ultrasonic
sensors and robotic vision.
Position sensors provide reliable information about position of the robot. Their
main purpose is to provide non-stop information about the sizes which deﬁne the state
of the robot.
Robot control aims that the robot is converted from one position to the other and
thereby is describing the certain path (trajectory). In order to implement this aim in
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_91

robots are installed environment sensors. One of the most important environmental
sensors are tactile sensors. Tactile perception provides information about the position
and orientation of objects, their mechanical properties (weight, elasticity, ﬂexibility)
and physical conditions (humidity, temperature). Tactile sensors are mimicking the
sensitivity of human ﬁngers and are located in the robotic hand. They have the task to
control the system with information about the position of the object that is in the
robotic hand (location and orientation), its shape, the distribution of forces acting on it
and sliding objects (speed and direction). On the base of this information robot can
identify the object from the default class, determine the mechanical properties
according to the degree of deformation of the object, correct force which acts on it and
change the orientation for the successful performance of a speciﬁc task.
2
Anatomical Analysis of the Human Hand
Touch is the most complex of the ﬁve human senses, including the sequences of the
various elements of nerves and senses. This complex machine allows people to identify
things about objects with which they come into contact, such as: temperature, mass
distribution, shape, texture, characteristics of form objects and similar. We can also
extract data relating to changing conditions caused by our interaction with them, such is
the force of hugs, estimate initial moment when there will be a slide, even gravitational
effects and the effects of inertia. The hand has a very delicate and complex structure.
The muscles and joints of the hand allow a large range of movement and precision.
Different forces are also distributed in the hand in the best possible way. However, the
hand is also very vulnerable: tendon, nerve ﬁbers, blood vessels and very thin bones are
all placed under the skin and are protected by a thin layer of muscle and fat. Only the
Fig. 1. Anatomy of a human hand [8]
Modeling of Sensors for a Mechatronic Systems
1075

palm is protected by a strong pads (aponeurosis) for powerful grip. Our hands daily
undergo a series of dangerous situations and come into contact with dangerous tool that
can seriously hurt and cause problems to our hands. Human hand is consisted of:
bones, joints, ligaments, tendons, muscles, nerves and blood vessels. Human hand with
wrist has 27 bones. Ligaments are solids bone tissue that connect and are the com-
pounds of bones or joints. Tendons connect muscles to bones, nerve endings transmit
stimuli to the brain and back and blood vessel are supplying the human hand with
blood (Fig. 1).
3
Robotic Hand
The hand uses plastic components that are modeled to mimic human bones,
with crocheted ligaments, stringy tendons and rubber skin layered on top. Servo motors
pull cables to copy the movement of muscles in a real hand. This kind of robotic hand
is developed at the University of Washington, robotic department [3] (Fig. 2).
German researchers have built an anthropomorphic robot hand that can endure
collisions with hard objects and even strikes from a hammer without breaking into
pieces. The DLR hand has the shape and size of a human hand, with ﬁve articulated
ﬁngers powered by a web of 38 tendons, each connected to an individual motor on the
forearm. The main capability that makes the DLR hand different from other robot hands
is that it can control its stiffness. The motors can tension the tendons, allowing the hand
to absorb violent shocks [2] (Fig. 3).
To change its stiffness, the DLR hand uses a following way. The joints of each
ﬁnger [photo below] are driven by two tendons, each attached to one motor. When the
motors turn in the same direction, the joint moves, when they turn in opposite direc-
tions, the joint stiffens [2] (Fig. 4).
A sense of touch is enabled to robotic hand by sensors.
Fig. 2. Robotic hand developed at the University of Washington, robotic department [3]
1076
S. Ferizović

4
Sensors Modeling
In the broadest deﬁnition, a sensor is an electronic component, module, or subsystem
whose purpose is to detect events or changes in its environment and send the infor-
mation to other electronics, frequently a computer processor. A sensor is always used
with other electronics, whether as simple as a light or as complex as a computer.
Sensors are used in everyday objects such as touch-sensitive elevator buttons (tactile
sensor) and lamps which dim or brighten by touching the base [1].
Fig. 3. DLR robotic hand [2]
Fig. 4. Finger of a DLR robotic hand [2]
Modeling of Sensors for a Mechatronic Systems
1077

5
Tactile Sensors
A tactile sensor is a device that measures information arising from physical interaction
with its environment. Tactile sensors are generally modeled after the biological sense
of cutaneous touch which is capable of detecting stimuli resulting from mechanical
stimulation, temperature, and pain (although pain sensing is not common in artiﬁcial
tactile sensors) [5] (Figs. 5, 6, and 7).
The BioTAC prototype system has the form factor of a ﬁnger tip and features 3
types of sensors for providing tactile feedback. The primary pressure sensor consists of
an array of electrodes covered in a conductive ﬂuid which lies between an exterior
membrane of textured skin and the interior electronic housing. When the outer mem-
brane comes into contact with an object the conductive ﬂuid is displaced producing
large changes in the electrical impedance detected by the electrodes in the array. This
change in impedance at each of the electrode locations can then be used to estimate the
magnitude and location of the force in contact with the Bio-TAC.
In order to provide additional feedback for textured surfaces, the conductive ﬂuid is
also coupled to a pressure sensor which detects sounds waves generated by the
vibration of the ﬂuid as it moves over the surface. As different textured surfaces
produce unique vibration patterns, the Bio-TAC can identify a given texture from its
associated vibration waveform. Just in case that wasn’t enough, a thermsistor is
included to detect temperature changes at the outer membrane [6] (Fig. 8).
Dr. Ing. Risto Kõivadeveloped a compact tactile sensor ﬁngertip with embedded
electronics using a Laser-Direct-Structuring (LDS) process (Fig. 9). Thanks to this
Fig. 5. Robotic hand with tactile sensors [4]
1078
S. Ferizović

structuring process, the tactile sensor was designed to be added to free-form PCB
surfaces with the remarkable possibility of creating very ﬁne structures, down to
100 lm. The resistive sensing working principle was achieved using conductive metal
tracks as electrodes and conductive foam or rubber as the sensor material. The possible
sensor materials can also be composites, such as elastomer foam or rubber with added
carbon particles or conductive fabrics. Therefore the coupling between the ﬂex-printed
PCB and the elastomeric sensing material allowed the authors to produce a tactile
sensor deformable up to a small radius and with almost arbitrary 3D free-form shapes.
The signal digitalization was obtained by converting into a voltage the resistance
measured between the two electrodes, or an electrode and a common ground-plane
Fig. 7. Section of aSynTouch BioTac—multimodal tactile sensor [6]
Fig. 6. The SynTouch BioTac a multiomodal tactile sensor modeled after the human ﬁngertip
[7]
Modeling of Sensors for a Mechatronic Systems
1079

shared by all tactiles of the sensor array. In particular, a simple and constant pull-up
resistor was attached to a constant power supply (voltage divider circuit). The voltage
at the junction of the resistors was sampled by an analog-to-digital converter (ADC),
therefore the data were provided in a digital form for either transmission and further
signal processing. The measurement range could be easily shifted by varying the
pull-up resistor value. In particular, higher resistances allowed the measurement of
lower applied pressures, however higher signal-to-noise was detected and the maxi-
mum measurable applied strain was limited. To reduce the signal-to-noise ratio, the
author directly integrated both the circuitry for analog voltage measurements and the
digital communication into the ﬁngertip. In particular as ADC, they developed a
programmable module in the ﬁngertip so that the data acquisition was managed in
terms of high protocol conﬁgurability thus leading to good adaptability to different
hardware systems [10].
Tactile sensor based on silicon piezoresistors able to independently detect the shear
stress in the two axial components. It consisted of an array of vertical piezoresistive
cantilevers standing orthogonally to each other, thus able to detect the directions and
Fig. 9. Compact tactile sensor ﬁngertip with embedded electronics [10]
Fig. 8. An example of a robotic hand [9]
1080
S. Ferizović

the magnitudes of the applied shear stresses. The cantilevers were prepared by
microfabrication technology from a silicon on insulator (SOI) wafer. The horizontal
cantilevers were then vertically aligned with the help of a magnetic ﬁeld which interacts
with a Ni magnetic layer previously deposited on the SOI surface. To maintain the
cantilevers standing without the magnetic ﬁeld, parylene-C was vaporized on their
surface. Finally the whole cantilever array was embedded in an elastic and ﬂexible
PDMS support and tested under shear stresses in the X- and Y-directions (Fig. 10). In
this way, the standing cantilevers follow the elastic deformation of the PDMS material.
The authors successfully reported the detection of a DR/R resistance variation for the
−5.0 to 5.0 kPa shear stresses applied in vertical direction to the cantilever. The
measured sensitivity was 20 times higher than that obtained for the shear stress applied
in parallel direction. This result showed that the sensor was able to detect and distin-
guish one axial component of applied shear stress, with a 10% error occurred in the
magnitude measurement of the shear stresses. The authors also proposed, as a future
improvement on the sensor’s sensitivity, to change the elastic material [10].
Fig. 10. Tactile sensor based on silicon piezoresistors [10]
Modeling of Sensors for a Mechatronic Systems
1081

6
Conclusion
The effort of the human race from the beginning to the present time is to make life as
far as possible. With the development of technology and techniques sometimes heavy
and inconceivable without high-power jobs, are now easily achievable. From the
beginning of existence people are imitating nature and discovering inventions which
are enchanting with their ingenuity and efﬁciency. One such invention is the invention
of a humanoid robot, which never tires and performs precisely and accurately. This
type of robot is designed to mimic the structure of a human being. Its precision is
conditioned largely by installing sensors. One of the most important components of the
development of service robots is the development and continuous improvement of the
sensor, as key components of the development of these robots. Great effort and con-
siderable resources are being invested in the advanced world in the development and
deployment of sensors. Contrary disclosed, in Bosnia and Herzegovina are very few
resources and effort allocated to robotics and the development of modern technology.
Our teachers and we are working to change the current state and to awake the con-
sciousness of our people about the importance of this scientiﬁc area.
References
1. https://en.wikipedia.org/wiki/Sensor
2. http://spectrum.ieee.org/automaton/robotics/humanoids/dlr-super-robust-robot-hand
3. https://www.geekwire.com/2016/uws-robot-hand-comes-creepily-close-to-human-
functionality/
4. https://www.shadowrobot.com/products/dexterous-hand/
5. https://en.wikipedia.org/wiki/Tactile_sensor
6. https://www.medgadget.com/2011/11/techtouch-a-look-under-the-hood-of-an-advanced-
tactile-sensor.html
7. http://spectrum.ieee.org/automaton/robotics/robotics-hardware/startup-spotlight-syntouch
8. https://www.youtube.com/watch?v=zyl6eoU-3Rg
9. http://www.pressureproﬁle.com/oem-robotics/?locale=en_gb
10. http://www.mdpi.com/1424-8220/14/3/5296/htm
1082
S. Ferizović

Part IX
Mechanical Engineering

Modelling the Micro Coaxial Helicopter
Želimir Husnić(&)
The Boeing Company, Chicago, USA
zhusnic@yahoo.com
Abstract. In this paper, we investigated the ﬂight dynamics of a micro coaxial
helicopter and developed a simpliﬁed model for autonomous ﬂight control
system design purpose. The multivariable tracking and H2 control theory are
employed to design the ﬂight control system that would achieve some basic
ﬂight manoeuvres like hover, level straight or circular ﬂight, and altitude
changes, etc.
Keywords: Micro coaxial helicopter  Autonomous ﬂight control system
Abbreviations and Acronyms
u
Component linear velocity in x-axis
v
Component linear velocity in y-axis
w
Component linear velocity in z-axis
p
Helicopter angular velocity along x-axis (rate of roll)
q
Helicopter angular velocity along y-axis (rate of pitch)
r
Helicopter angular velocity along z-axis (rate of yaw)
/
Roll angle (Euler angle for roll (x-axis))
h
Pitch angle (Euler angle for pitch (y-axis))
w
Yaw angle (Euler angle for yaw (z-axis))
x
Linear position of the helicopter in x-body axis
y
Linear position of the helicopter in y-body axis
z
Linear position of the helicopter in z-body axis
X, Y, Z
External forces acting at the helicopter center of gravity
Xu
Force along the x-axis acting at upper rotor
Xl
Force along the x-axis acting at lower rotor
Xpl
Force along the x-axis acting at fuselage
Yu
Force along the y-axis acting at upper rotor
Yl
Force along the y-axis acting at lower rotor
Ypl
Force along the y-axis acting at fuselage
Zu
Force along the z-axis acting at upper rotor
Zl
Force along the z-axis acting at lower rotor
Zpl
Force along the z-axis acting at fuselage
L
External moment along the x-axis
M
External moments along the y-axis
N
External moments along the z-axis
Note The Boeing Company is not associated with this paper.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_92

Lu
Moment along the x-axis acting at upper rotor
Ll
Moment along the x-axis acting at lower rotor
Lpl
Moment along the x-axis acting at fuselage
Mu
Moment along the y-axis acting at upper rotor
Ml
Moment along the y-axis acting at lower rotor
Mpl
Moment along the y-axis acting at fuselage
Nu
Moment along the z-axis acting at upper rotor
Nl
Moment along the z-axis acting at lower rotor
Npl
Moment along the z-axis acting at fuselage
Ix
Moment of inertia about the body x-axis
Iy
Moment of inertia about the respectively body y-axis
Iz
Moment of inertia about the respectively body z-axis
Ixy
The product of inertia
G
Weight of helicopter
m
Helicopter mass
g
Gravitational acceleration (gravity)
A
Plant state-space internal dynamics matrix
B1
State-space input disturbance matrix
B2
State-space input control matrix
C2
State-space observation matrix
C1u
Control output gain matrix (matrix)
D1lu
Identity matrix
D22
State-space output disturbance matrix
D
Input feed-through matrix
D12d
Controlled output gain matrix
w1
Exogenous input vector to the system
x
The state vector
y
Measured output vector
u
Control input vector
z
Regulated signal vector
z1
Output-tracking error to be minimized
z2
Control-input constraints
m
Measurement noise
Q
State-weighting matrix
R
Input-weighting matrix
dth
Altitude (thrust)
dlat
Roll control input
dlon
Pitch control input
dyaw
Yaw control input
1086
Ž. Husnić

1
Introduction
Rotorcraft have signiﬁcant advantages over ﬁxed wing aircraft when the vehicle is
required to remain stationary (hover) or to ﬂy in a constrained space. The
miniature/micro coaxial conﬁguration helicopter is chosen over the single-rotor con-
ﬁguration in this study due to its maneuverability, versatility, stability and easiness to
control. Unlike the single-rotor helicopter which requires a tail rotor’s side force to
compensate for the reactive moment of the single main rotor, the coaxial-rotor heli-
copter has its rotors’ reactive moments compensating each other directly in their axis of
rotation. The main reason that makes a coaxial helicopter so special is because it uses
two contra-rotating rotors to compensate each other’s gyro-effect torque. Without a tail
rotor, which accounts for 10–12% of total power, the coaxial helicopter can devote all
the power in developing lift to improve the power efﬁciency [1]. In addition, the two
rotor speeds can be controlled individually to create a gyro differential to control the
heading direction of the vehicle. Therefore, rudder control is not required.
Although the dynamics of conventional helicopters of regular size has been
described in the literature, the micro size and special conﬁguration of the coaxial
helicopter have altered certain details of the ﬂight dynamics model [2]. We will start the
modelling with the classical rigid-body equations. The speeds in any direction are
assumed to be so small that the aerodynamic forces can be approximately represented
by linear friction forces with constant friction coefﬁcients. In addition to the friction
forces and disturbances, the forces and moments act on the body of the helicopter
include those from the gravitation and the two coaxial rotors [3].
The conventional aircraft body frame is adopted in the paper. As shown in Fig. 1,
the body frame using three Cartesian axes is ﬁxed to the helicopter, in which the x-axis
points forward along the nose, the y-axis points out along the right wing, and the z-axis
points down. The rotation around the x-axis is roll, the rotation around the y-axis is
pitch, and the rotation around the z-axis is yaw. The attitude of the helicopter is
described by Euler angles.
Fig. 1. Body-ﬁxed axis system used in helicopter dynamics analysis
Modelling the Micro Coaxial Helicopter
1087

The conﬁguration, dynamics, and control actuators are described in other section of
this paper. The micro coaxial helicopter ﬂight control problem will be formulated as a
multivariable tracking and H2 control problem based on the ﬂight dynamics model
obtained in this work. Also, paper will demonstrates computer simulations and the
conclusion.
2
Description of the Micro Coaxial Helicopter
2.1
Introduction
A fully functional ﬂight control is an absolute necessity for micro coaxial helicopter.
A typical coaxial rotorcraft control system (swashplate, pitch links, etc.) is quite
complex. Aerodynamic symmetry is the most important feature of the coaxial heli-
copter. It enhances its controllability and stability substantially. Aerodynamic sym-
metry of the coaxial conﬁguration is provided by the lack of reactive moment on the
airframe, relatively close upper and lower rotors and their beneﬁcial mutual effect,
which results in little difference in their thrusts when balanced. Rotors’ side forces
directed in different directions balance each other with their lateral moment, which
emerges due to their separation, being insigniﬁcant. Thanks to the lack of the tail rotor,
the coaxial rotor helicopter is not subject to the constant effect of the alternate side
force. The coaxial design ensures a smooth combination of efﬁcient control and
aerodynamic damping, which provides good controllability. The two rotors are the
most important mechanisms in this helicopter because they generate most of the forces
and torques applied to helicopter body. The upper rotor of the helicopter is not linked to
any servo, so a mechanical stabilizer is used to induce cyclic pitch control of the rotor
when it senses the inclination of the fuselage.
Upper Rotor is equipped with an adjustable linkage between the Stabilizer Flybar
and Upper Main Rotor. This linkage allows you to adjust the tracking of the upper main
rotor blade for smoother and more stable ﬂight performance. The stabilizer bar with
upper rotor forms Hiller control system. It has the effect of changing in reaction to
helicopter tilt to slow and stabilize tilt motion. In absence of aerodynamic forces and
external moments, the ﬂy-bar behaves as a gyroscope, maintaining its orientation
relative to inertial space. In a hover, the ﬂy-bar angle is zero.
The lower rotor is linked to two servos, which control the helicopter to pitch and
roll. Therefore, we can control the cyclic pitch of the bottom rotor to command the
helicopter to pitch and roll. The left and right servos control the lateral and longitudinal
cyclic pitch of the bottom rotor respectively. The rotational velocity of rotors is con-
trolled by two different motors. The front motor changes blade speed of the top rotor,
while the back motor controls the bottom rotor. The parameters relating the angular
velocities of the helicopter to the lateral and longitudinal motion, as well as the effects
of the velocities “u” and “v” on the moments experienced were identiﬁed. The
velocities are affected by gravitational force acting on the aircraft due to the change in
the lateral and longitudinal position. This effect was modelled externally through Euler
angles.
1088
Ž. Husnić

2.2
Dynamics
The forces and torque are balanced when helicopter is in stable condition. Unbalance of
force will result in linear acceleration, while unbalance of torque will result in angular
acceleration. In hovering force balance is achieved when the sum of the thrust from two
main rotors equals the gravitational force. Also, all forces and torques in all directions
sum to zero. At hover force directions are concentric with the rotor shaft. Because it is a
rigid body, the helicopter’s position and orientation in body coordinates will always be
zero [4].
However, the velocity and acceleration expressions are greatly simpliﬁed by using
these coordinates. The aerodynamic interaction between the rotor and the fuselage is
neglected. Terms such as Ixy and Iyz are zero due to the symmetry of the helicopter with
respect to the x-z plane. Although Ixz is nonzero, because the helicopter is not sym-
metric with respect to the x-y plane, it is typically much smaller than the other terms.
The offset between the rotor axis and the helicopter’s center of gravity is expected to be
zero. It is assumed that the helicopter’s center of gravity is in-line with the rotor axis.
Force equilibrium along the body axis, the external forces and moments have been
discussed in literature.
Summary is shown in equations below.
Force equilibrium along the body axis (Figs. 2 and 3).
X ¼ mvr þ mwq þ Gsinh þ m_u
Y ¼ mwp þ mur  Gcoshsin; þ m_v
Z ¼ muq þ mvp  Gcoshcos; þ m _w
L ¼ Ix _p  Iy  Iz


qr
M ¼ Iy _q  Iz  Ix
ð
Þpr
N ¼ Iz_r  Ix  Iy


pq
The external forces and moments
X ¼ Xu þ Xl þ Xpl
Y ¼ Yu þ Yl þ Ypl
Z ¼ Zu þ Zl þ Zpl
L ¼ Lu þ Ll þ Lpl
M ¼ Mu þ Ml þ Mpl
N ¼ Nu þ Nl þ Npl
Modelling the Micro Coaxial Helicopter
1089

Fig. 2. Helicopter forces and moments during ﬂight
Fig. 3. Coaxial helicopter forces and moments
1090
Ž. Husnić

2.3
Control Inputs
The control system provides external inputs to servo and motor to generate cyclic pitch,
collective pitch, and rotor speed. These controls cause forces and torques to be applied
to the helicopter and result in helicopter movement.
(a) Yaw Control
Yaw control can be performed by varying the difference in rotational speed
between the two rotors, creating a torque applied to the fuselage. Helicopter will turn to
the left with increase the speed of the lower rotor while decreasing speed of the upper
rotor. Helicopter will turn to the right with increase the speed of the upper rotor while
decreasing the speed of the lower rotor.
(b) Thrust Control
Thrust (altitude) control can be accomplished by varying the RPM of the rotors
equally. Thrust is adjusted by varying the rotors RPMs equally.
When the speed of rotor blades increase—helicopter climb.
When the speed decrease helicopter descend.
(c) Pitch Control
Pitch (longitudinal pitch) is controlled by varying the angle of the rotor blades as
they go around (tilting rotor back and forth). It does by varying the angle of the rotor
blades as they go round, tilting the rotor back and forth. When the longitudinal input
applied to the system the rear servomotor will push the swashplate upward/downward
tilting the rotor back and forth. When the pitch control input pitch nose of the helicopter
downward and helicopter moves forward; the rear servo motor, on the left hand side
looking from tail, will push the swashplate upward. When control input pitch the nose
of the helicopter upward, and helicopter moves backward; the rear servo motor will
pull the swash plate downward.
(d) Roll Control
When moved left or right the rotor tilts in that direction and the helicopter banks
and rolls. Roll is controlled by varying the angle of the rotor blades left or right. When
the lateral input applied to the system the forward servomotor will push the swashplate
upward/downward. The roll control input left will roll helicopter to the left, the forward
servo will push the swashplate upward. The roll control input right will roll helicopter
to the right, the forward servo will pull the swashplate downward.
Because it is a rigid body, the helicopter’s position and orientation in body coor-
dinates will always be zero; however, the velocity and acceleration expressions are
greatly simpliﬁed by using these coordinates. The aerodynamic interaction between the
rotor and the fuselage is neglected. The standard rigid body dynamical equation will be
used to model the motion of the helicopter in its environment. The offset between the
rotor axis and the helicopter’s center of gravity is expected to be zero. It is assumed that
the helicopter’s center of gravity is in-line with the rotor axis. In order to develop a
coupled body/rotor dynamics model, a hybrid model is used in which the rotor and
Modelling the Micro Coaxial Helicopter
1091

body quasi-steady dynamics are combined. The inputs are directly included in the rotor
dynamics.
The micro coaxial helicopter forces and moments with direct control inputs could
be partitioned as following
Z ¼ muq þ mvp þ m _w þ Vthdth
L ¼ Ix _p  Iy  Iz


qr þ Elatcdlat
M ¼ Iy _q  Iz  Ix
ð
Þrp þ Dmlondlon
N ¼ Iz_r  Ix  Iy


pq þ Kyawdyaw
Identifying stability and control derivatives from ﬂight test data can be used to
provide accurate linear models for control law design or in the estimation of handling
qualities parameters.
Our principal interest at this time is the application to simulation model validation.
Rotor force and moment derivatives are closely related to individual thrust and
ﬂapping derivatives. Many of the derivatives are strongly nonlinear functions of
velocity, particularly the velocity derivatives themselves. The derivatives are also
nonlinear functions of the changes in downwash during perturbed motion, and can be
written as a linear combination of the individual effects, as in the thrust coefﬁcient
change with advance ratio.
There are three approaches to estimating stability and control derivatives: analytic,
numerical,
backward-forward
differencing
scheme
and
system
identiﬁcation
techniques.
The system identiﬁcation approach seeks to ﬁnd the best overall model ﬁt and, as
such, will embody the effects of any nonlinearities and couplings into the equivalent
derivative estimates [5, 6]. The states are no longer perturbed independently; instead,
the nonlinear model, or test aircraft, is excited by the controls so that the aircraft
responds in some optimal manner that leads to the maximum identiﬁability of the
derivatives. The derivatives are varied as a group until the best ﬁt is obtained.
Note that the damping and input control derivative values are determined
experimentally.
3
Modeling in State Space
3.1
Model Analysis
For in-depth analysis and controller design, it is convenient to work with a closed-form
mathematical model of a system. A good mathematical model is one that will
approximate the system responses to given inputs in its region of validity with
acceptable accuracy.
A state space approach was chosen due to the resulting computationally friendly
and intuitive model. If the basic structure of the model is linearizable then a vast choice
of optimal control algorithms. The linear model is also found to be well suited for
simulation purposes.
1092
Ž. Husnić

The model that includes a linearized 6-DoF MIMO state space realization of the
aircraft dynamics about a user deﬁned trim point. The basic structure of the model can
be deﬁned in the well-known linear state space form.
The generalized plant, G, is the system to be controlled by the controller, K.
Together G and K form what is referred to as the closed loop system. The problem
is characterized by a desire for the plant to follow, or track, some reference command r
(t). This is done by creating an appropriate control input to the generalized plant, u(t),
based on the measured output of the plant, y(t), while simultaneously trying to mini-
mize the inﬂuence of disturbances on the plant, d(t), and measurement noise within the
plant, n(t). The effectiveness of K is assessed by its ability to minimize the norm of the
controlled output vector, z(t).
This vector is typically divided into two parts; the output-tracking error, z1(t), and
the control input constraints, z2(t).
The servomechanism formulation can be carried out using output-tracking regu-
lation theory in conjunction with H2 optimization schemes.
The generalized plant G can be described by the equations
_x tð Þ ¼ Ax tð Þ þ B1w1 tð Þ þ B2u tð Þ
y tð Þ ¼ C2x tð Þ þ v tð Þ þ D22u tð Þ
The problem is formulated as a tracking control problem where the reference signal
is a step input. The tracking/regulator control problem has been intensively studied in
literature. In addition to the internal model principle that guarantees zero steady-state
tracking error, in this paper optimal H2 control theory is employed to optimize the
transient response and the robustness subject to control input constraints. The effective
implementation of a full control system for maneuvering in-ﬂight munitions requires
many elements.
In this paper, we shall discuss a proposed method of modelling. This preliminary
work allows for advancement toward controlling the vehicle ﬂight path, which can be
utilized once the ﬂight dynamics have been analysed and an appropriate system-wide
controller has been designed. In this sense the work presented here is of critical
importance in implementing the full dynamic control system.
Fig. 4. The nominal closed-loop system
Modelling the Micro Coaxial Helicopter
1093

3.2
Controller Design
Controller development throughout this work is founded on a very speciﬁc servomech-
anism control scheme, based on closed-loop multi-variable output-tracking regulation and
H2 optimized feedback control. The controlled output vector can be deﬁned as
z tð Þ ¼
z1 tð Þ
z2 tð Þ


¼
C1u
0


x tð Þ þ
0
D12d


u tð Þ
Note that this formulation and claim are derived for a linear system model. When
systems behave linearly and match the linear model about which such regulation gains
are deﬁned, zero steady-state tracking error is indeed achievable using this control
method [7, 8]. However, for poorly modelled systems, or nonlinear systems whose
behaviour digresses from the region about which dynamics were linearized (e.g. aircraft
operating under adverse conditions), this approach must be extended to be effective in
practice.
The problem now is to ﬁnd a controller K as illustrated in Fig. 4 so that,
(i)
the closed-loop system is internally stable,
(ii)
both tracking error and alignment error are zero at steady state,
(iii)
the performance index is minimized.
This can be achieved using the structure of the controller shown in Fig. 5.
The ﬁnal step in assembling the tracking controller K is to combine the exogenous
input estimator, the regulator parameters W and U, and the state feedback gain F.
As long as the closed-loop system is internally stable and matches the plant model
well, steady state regulation will take place if W and U are chosen to satisfy equations.
For the stabilizable and detectable system deﬁned by A, B2, C2, let
x ¼ x  W hR
u ¼ u  U hR
By substitution, the system can now be expressed as,
Fig. 5. Generalized structure of a servomechanism output-tracking control regulator
1094
Ž. Husnić

_x tð Þ ¼ Ax tð Þ þ B1w tð Þ þ B2u tð Þ  WWRWR tð Þ þ AW þ B2U  WZR
ð
ÞhR tð Þ
z1 tð Þ ¼ C1ux tð Þ þ C1uW þ D1lu
ð
ÞhR tð Þ
Setting coefﬁcients of the hR term to zero
AW þ B2U  WZR ¼ 0
C1uW þ D1lu ¼ 0
yields
_x tð Þ ¼ Ax tð Þ þ B1w tð Þ þ B2u tð Þ  WWRWR tð Þ
z tð Þ ¼
C1u
0


x tð Þ þ
0
D12d


u tð Þ
According to the regulator theory, zero steady-state tracking error is achievable
through by designing the controller parameters W and U to satisfy these equations.
Output regulation gains U and W are calculated in Mathematica. Control output
gain matrix (matrix [4  12]) has 4 inputs to track and 12 states in state vector.
The state feedback gain, F, is generated based on weighting matrices chosen by the
designer that balance the cost of transient performance and input power. For nonlinear
systems and other systems operating under o-nominal design conditions, this gain may
also determine overall system stability.
Design of the state feedback gain begins with the selection of the aforementioned
weighting matrices. In the state feedback we assume that the whole state x can be
measured and therefore it is available for control.
State Feedback Gain F will implement full state feedback and omit the estimator
construction (Fig. 6).
Fig. 6. Controller schematic—Simulink model
Modelling the Micro Coaxial Helicopter
1095

We will design state feedback by using nominal values of parameters and we will
try to choose controlled output gains matrices.
State feedback Gain F is obtained from the solution of the algebraic Riccati
equation.
The Gain F is computed as follows
F ¼ R1BTX
where
X is the positive semi-deﬁnite stabilizing solution of the following continuous time
algebraic Riccati equation,
ATX þ XA  XBR1BTX þ Q ¼ 0
The algebraic Riccati equation determines the solution of the Linear Quadratic
Regulator problem (Matlab command lqr).
F
½  ¼ lqr A; B; Q; R
ð
Þ
4
Computer Simulation and Results
4.1
Summary
This chapter details the development of high reliability six degree-of-freedom mathe-
matical and simulation models, and the design of a stability augmentation system for
the micro helicopter.
We use the MatLab-Simulink 6DoF Aerospace Blockset to implement Euler angle
representation of six-degrees-of-freedom equations of motion. The 6DoF (Euler
Angles) block considers the rotation of a body-ﬁxed coordinate frame about an
Earth-ﬁxed reference frame. The origin of the body-ﬁxed coordinate frame is the center
of gravity of the body, and the body is assumed to be rigid, an assumption that
eliminates the need to consider the forces acting between individual elements of mass.
The Earth-ﬁxed reference frame is considered inertial, an excellent approximation that
allows the forces due to the Earth’s motion relative to the “ﬁxed stars” to be neglected.
A linear controller is designed, using eigenstructure assignment, following guidelines
outlined in the literature [9].
Though the observed response appears to be nonlinear and complex, it is arguably
possible that a single linear model could capture these dynamics and repeat them in
simulation.
1096
Ž. Husnić

To test this possibility, a single linearized model of the form in Equations was
created about the nominal trim condition deﬁned in Table 1.
A linear controller is designed, using eigenstructure assignment, following guide-
lines outlined in the literature.
Weighting matrices Q and R are used to deﬁne the cost function.
This section will provide general guidelines on how to choose Q and R.
The design procedure for ﬁnding the feedback is:
• Select design parameter matrices Q and R.
• Solve the algebraic Riccati equation.
• Find the state feedback gain F.
Note that output regulation gains U and W are solved in Mathematica. The Matlab
routine that performs numerical procedure for solving the algebraic Riccati equation is
“lqr (A, B, Q, R)”.
Most common choice of weighting matrices Q and R are diagonal matrices.
In general one has to ﬁnd a good ratio between the values of R and Q.
For the micro coaxial helicopter simulation we found out by trial and error method
a good values of R and Q.
According to the regulator theory, zero steady-state tracking error is achievable by
designing the controller parameters W and U to satisfy following equations.
AW þ B2U  WZR ¼ 0
C1uW þ D1lu ¼ 0
Table 1. Trim conditions for multi-linear simulation
Trimmed states
Velocity in body axis u = 0.1 m/s
v = 0.1 m/s
w = 0.1 m/s
Rotation rates
p = 0.1 m/s2 q = 0.1 m/s2 r = 0.1 m/s2
Euler orientation
roll = 0
pitch = 0.1
yaw = 0
Modelling the Micro Coaxial Helicopter
1097

U ¼
0
0
0
7;93456  1012
0
0
0
2;31341  1017
0
0
0
3;11551  1012
0
0
0
6;22775  1014
2
6664
3
7775
W ¼
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
0;000277193
0
0
0
0;999764
0
0
0
1;00026
0
0
0
10;0002
0
0
0
0;00239802
0
0
0
0;00237625
0
0
0
0;000537392
0
0
0
1;00002
0
0
0
1
2
666666666666666666666664
3
777777777777777777777775
State Feedback Gain F will implement full state feedback and omit the estimator
construction. State feedback Gain F is obtained from the solution of the algebraic Riccati
equation. The state feedback is designed by using parameters and matrices Q and R.
R matrix is calculated in the Matlab with the D12d shown below.
Where D12d is controlled output gain.
R ¼ DT
12dD12d
D12d ¼
55
0
0
0
0
0;0035
0
0
0
0
0;005
0
0
0
0
0;055
2
6664
3
7775
The micro helicopter has a fast time domain response due to its small size and is it’s
sensitive to small changes of parameters and matrices. The damping and input control
derivative values are determined experimentally.
How to choose Q matrix is important part of the state feedback gain design.
To improve the design, we try various Q matrices, and for each Q we
(i)
compute state feedback F
(ii)
simulate the closed loop system
(iii)
plot the state trajectories and evaluate them
The weighting factors will affect the computation result of the state feedback gain
F. These parameters allow the tuning of each participation in the cooperative track
seeking.
1098
Ž. Husnić

In the following we provide some guideline on adjusting these parameters for best
system performance.
The micro helicopter has a fast time domain response due to its small size and is it’s
sensitive to small changes of parameters and matrices.
Most common choice of weighting matrices Q and R are diagonal matrices. It was
successful for linear simulation. The six DoF model simulation with diagonal matrices
Q and R are explained in literature and this work.
For the micro coaxial helicopter six DOF simulation we found out by trial and error
method, a good values of Q. Depending on how Q and R are selected, the closed-loop
system will exhibit a different response.
The factors of Q matrix additional adjustment with weighting factors from matrix A
as follows.
rate1 = A(1,:); rate2 = A(2,:); rate3 = A(3,:); rate4 = A(4,:);
4.2
Simulation Results
The simulation was performed using the individual models.
Results with the Flight Gear animation screen snapshots for the 6 DOF simulation
of ﬂight are shown in Figs. 7 and 8.
After tuning of the weighting factors and derivatives, results of ﬂight simulation are
shown in Fig. 8.
5
Conclusions
System identiﬁcation techniques as used in full-scale helicopters have been success-
fully applied to model-scale micro coaxial unmanned helicopter. Micro coaxial heli-
copters seem to be particularly well suited to identiﬁcation.
The identiﬁed model should be well suited to ﬂight control design, handling quality
evaluation, and simulation applications.
Fig. 7. Snapshot at beginning of ﬂight and during the ﬂight
Modelling the Micro Coaxial Helicopter
1099

The dynamics of the helicopter is strongly inﬂuenced by the speciﬁc design of
micro-size helicopters and coaxial helicopter design. A novel implementation of
output-tracking regulation for the control of was developed. The reference input that
the regulator tracks is step input. The step component imparts a desired aerodynamic
load to guide the along its ﬂight path. This work is relevant because it allows the micro
coaxial helicopter to be controlled more smoothly than allowed by existing approaches.
The generalized development for this system is provided in this work, and speciﬁc
application for a six degree-of-freedom (6DoF) dynamics model are developed and
simulated.
The controller is shown to be extremely effective at safely ﬂying the micro coaxial
helicopter. These values show that the plant is stable at the described condition. These
results validates that the nominal controller may be successfully designed and
implemented.
The results obtained from the 6-DoF dynamic model are encouraging. The
development of the dynamic model is being done on a step-by-step basis, and the work
will continue until a model good enough to be used for control system design is
obtained. The forward ﬂight condition is challenging, but reasonable results are
Fig. 8. Six DOF model response
1100
Ž. Husnić

obtained by using experimental methods and the simulations carried out in
SIMULINK.
Good results of system identiﬁcation with a high quality instrumentation and an
optimal integration of the sensor information would further improve the results. It is
hoped that there will continue to be rapid progress made in the data acquisition and
parameter identiﬁcation processes, as well as in the 6-DoF dynamic modelling effort.
References
1. Petrosyan, E.: Aerodynamics of Coaxial Conﬁguration Helicopters. Moskva Poligon Press
(2004)
2. Padﬁeld, G.: Helicopter Flight Dynamics, 2nd edn. Blackwell Publishing, Oxford, UK (2007)
3. Raptis, I., Valavanis, K.: Linear and Nonlinear Control of Small-Scale Unmanned
Helicopters. Springer Science, New York (2011)
4. Velez, C.M., Agudelo, A., Alvarez, J.: Modelling, simulation and rapid prototyping of an
unmanned mini-helicopter, AIAA 2006-6737. In: AIAA Modelling and Simulation
Technologies Conference and Exhibit, Colorado (2006)
5. Mettler, B., Kanade, T., Tischler, M.: System Identiﬁcation Modelling of a Small-Size
Unmanned Helicopter Dynamics, American Helicopter Society 55th Forum. Montreal,
Quebec, Canada (1999)
6. Kim, S.K., Tilbury, D.M.: Mathematical modelling and experimental identiﬁcation of a model
helicopter. In: AIAA-98-4357, American Institute of Aeronautics and Astronautics (1998)
7. Chang, B.C., Salman, M.: Active coning compensation for control of spinning ﬂying vehicles.
In: IEEE Conference, Yokohama, Japan (2010)
8. Chang, B.C., Bajpai, G., Kwatny, H.G.: A regulator design to address actuator failures. In:
Proceedings of the 40th IEEE Conference on Decision and Control, vol. 2, pp. 1454–1459
(2001)
9. Husnić, Ž.: Micro Coaxial Helicopter Controller Design. Ph.D. Thesis, Drexel University,
Philadelphia, PA, US (2014)
Modelling the Micro Coaxial Helicopter
1101

Effects of Primary Measures in Combustion
Chamber on Co-ﬁring of Coal with Woody
Biomass
N. Hodžić1, S. Metović1(&), and A. Kazagić2
1 Faculty of Mechanical Engineering Sarajevo, University of Sarajevo,
Vilsonovo setaliste 9, 71000 Sarajevo, Bosnia and Herzegovina
hodzic@mef.unsa.ba
2 Elektroprivreda BiH d.d. - Sarajevo Power Utility, Vilsonovo setaliste 15,
71000 Sarajevo, Bosnia and Herzegovina
Abstract. In this work results of laboratory research of co-ﬁring coals from
Middle Bosnian basin with waste woody biomass are presented. Pulverized
combustion under various temperatures and various technical and technological
conditions is performed using previously formed fuel test matrix. This is pri-
marily related to the different mass ratio of fuel components in the mixture, the
overall coefﬁcient of excess air and the application of different primary measures
in the combustion chamber. The results and analysis of the emissions of com-
ponents of the ﬂue gases are presented and discussed, which are predominantly
related to the emission of CO2, CO, NOx and SO2. In addition to determining the
impact of fuel composition and process temperature on the values of the
emissions of components of the ﬂue gas, it is shown that other primary measures
in the combustion chamber are resulting in more or less positive effects in terms
of reducing emissions of certain components of the ﬂue gases into the envi-
ronment. This particularly stand out measures such as exponentiation of com-
bustion air and the use of natural gas as the additional fuel: the emission of NOx
is reduced from a level of 837 mg/mn
3 in conventional combustion to 710 mg/mn
3
using exponentiation of combustion air, or for more than 15%, and using natural
gas as additional fuel it is reduced to 450 mg/mn
3, or for more than 45%. Overall,
the effects of the primary measures in the combustion chamber are mutually
compared and quantiﬁed. In addition, the inﬂuence of applied primary measures
in the combustion chamber on the behavior of ash in the co-ﬁring process is
analyzed. In this way, evaluation the fouling and slagging tendencies of the ash
under given technical and technological conditions of co-ﬁring coal with waste
woody biomass is given.
Keywords: Co-ﬁring  Coal  Biomass  Emissions  Ash
1
Introduction
The multidisciplinary scientiﬁc and professional public has been involved over the last
30 years, more than ever, in analyzes and ﬁnding solutions to the increasingly negative
impact of emissions of ﬂue gas components generated by the burning of fossil fuels.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_93

These analyzes and projections for the future shorter and/or longer periods have
resulted and will continue to produce measures and recommendations expressed
through conventions, directives and protocols, such as the Kyoto Protocol of 1997.
This protocol deﬁnes guidelines that would, after the prolongation of the Protocol’s
validity by 2020, reduce greenhouse gas emissions as a cause of global warming,
primarily carbon dioxide CO2 emissions that have been enormously increased for the
last six decades, as well as other undesirable ﬂue gas components such as NOx and
SO2. In relation to the above, various scenarios of temperature increase on Earth up to
year 2100, out of which a critical scenario with a 2 °C increase in temperature is set—
Fig. 1 [1].
There are signiﬁcant coal reserves in Bosnia and Herzegovina. These are lignite and
brown coal, while there is no stone coal. According to the latest estimates, the balance
and exploitation reserves of coal in Bosnia and Herzegovina are about 4.5  109 t, of
which about 40% refers to brown coal and about 60% to lignite. However, the quality
of coal in Bosnia and Herzegovina is signiﬁcantly different from one basin to another,
and even from one mine to another within the same mining basin [2]. The basic
characteristics of these coals are: low heating value, high mineral mass and moisture
content and poor reactivity.
Though the reconstruction and modernization of thermoblocks in EP BiH (Public
Enterprise Electric Utility of Bosnia and Herzegovina) has been achieving certain
results in the sense of improving energy efﬁciency, due to the obsolete technology of
existing thermoblocks and relatively low heating value of domestic coals, the speciﬁc
CO2 emission from thermal power plants (TPP) is still high compared to modern
thermoblocks in Europe and the world. Figure 2 is a diagram of the change of speciﬁc
CO2 emissions from TPPs of EP BiH in the period 2001–2013—relative emission
reduction of almost 15% as a result of signiﬁcant ﬁnancial investments—but still high:
about 1140 kg/MWh. In addition, the ﬁring of coal mixtures in TPP Kakanj in these
conditions results in high emissions of other pollutants, eg. NOx emissions are typically
in the range of 700–1000 mg/m3
n and SO2 even over 8000 mg/m3
n, under the reference
conditions of 6% O2 in dry ﬂue gases [3].
The above mentioned emission values exceed the limits prescribed by the Ordi-
nance on emission limit values for thermal power plants (Ofﬁcial Gazette of FBiH
No. 4/13) and the Directive 2010/75/EU—Industrial Emissions Directive (IED).
Fig. 1. CO2 emissions and earth temperature growth scenarios [1]
Effects of Primary Measures in Combustion Chamber
1103

According to the results of the research carried out at the Mechanical Engineering
Faculty in Sarajevo under the ADEG project, biomass could be the second most
signiﬁcant renewable energy source after hydro potential—it is estimated that the total
annual technical energy potential of biomass remains in BiH is more than 33 PJ [4],
which is equivalent to more than 3 million tons of BIH lignite.
Due to the need for further reduction of CO2 emissions, in current research in the
world focus is especially on exploring these phenomena in the combustion of various
coal blends and co-ﬁring coal with different types of biomass. The application of one or
more primary measures at the same time in the combustion chamber/combustion zone
is an inevitable technical and technological setting. These studies have resulted in the
introduction of co-ﬁring in 230 TPPs in the EU.
In addition to the various technical solutions for the use of biomass in co-ﬁring, the
signiﬁcant introduction of different types of biomass (waste biomass, energy crops) and
gaseous fuels (natural gas, biogas) into the combustion process with coal as the base
fuel of the thermal power plant, depending on availability and economic eligibility,
represents the multi fuel concept (MFC).
Reduction of NOx emissions, compared to conventional combustion systems, can
be adequately achieved by using burners of newer generations (vortex Low-NOx
burners), over ﬁre air supply (OFA), staging primary fuel supply and using additional
fuel (e.g. natural gas, reburning).
2
Aim of the Work and Fuel Test Matrix
The subject of this paper is co-ﬁring of coal with waste woody biomass, ie. the
determination of the characteristics, behavior and characteristic phenomena of this
co-ﬁring in different ambient and technological conditions. This applies primarily to the
application of primary measures in the combustion chamber aimed at reducing emis-
sions (for example, different mass and/or energy partaking of component fuels in the
mix, different temperature conditions/regimes, gradual combustion by zonal fuel and/or
1329
1135
800
900
1000
1100
1200
1300
1400
2001
2003
2005
2007
2009
2011
2013
kg/MWh
Fig. 2. Speciﬁc CO2 emission in thermoblocks of EP BiH in the period 2001–2013 [3]
1104
N. Hodžić et al.

air supply, additional combustion using natural gas—reburning technology). This is
achieved by combusting mixture of component fuels in the experimental plant: an
automatically controlled coal and biomass pipe reactor, where it is possible to achieve
process temperatures ranging from ambient temperatures up to 1560 °C and varying the
excess air coefﬁcient with the possibility to vary air and fuel distribution. In this way, it
is possible to determine the inﬂuence of different primary measures in the combustion
chamber on co-ﬁring coal with woody biomass, with particular reference to emissions
of pollutants, e.g. CO, CO2, NOx and SO2.
The results of the exploration of the co-ﬁring coals from Middle Bosnian mining
basin (label U) with waste woody biomass (spruce and beech sawdust in weight ratio 1:
1, label B). In the regimes with reburning technology, gas was used as an additional
fuel (label P)—Table 1 [5].
Co-ﬁring of coal with woody biomass was carried out at process temperatures of
1350, 1400 and 1450 °C with staging air supply and excess air coefﬁcient of 1.20.
During test regimes, the process temperature, fuel consumption, air ﬂow (primary,
secondary/tertiary and OFA), as well as the composition of ﬂue gas (O2, CO, CO2, NO,
NO2, NOx and SO2) were measured. In addition, sampling of deposits, slags and ashes
from selected test regimes and sampling sites enabled the determination of the burnout
degree. Based on the measured and analyzed samples of combustion products, the
characteristics of the co-ﬁring process are determined, for example: the efﬁciency of
applying one or more primary measures in the combustion chamber to the emissions of
ﬂue gas components, the propensity of ash to fouling boiler heating surfaces.
3
Results and Discussion
It has been found that NOx emissions during co-ﬁring are at the emission levels of the
combustion of coal blends without biomass, there is practically no change in this
emissions by increasing the fraction of woody biomass in the blend. The average
difference in NOx emissions depending on the mode of combustion air supply is
250 mg/m3
n: for the classic air intake the emission is 942 mg/m3
n, while it is 692 mg/m3
n
Table 1. Test fuel matrix
No. Fuel/label
Weight ratio (%m) or
energy content ration
(%e)
1.
U
U 100%
%m
2.
B
B 100%
%m
3.
U95B5
U:B=95:5
%m
4.
U93B7
U:B=93:7
%m
5.
U90B10
U:B=90:10
%m
6.
UB(95)P5
UB(95):P=95:5
%e
7.
UB(95)P10 UB(95):P=90:10 %e
Effects of Primary Measures in Combustion Chamber
1105

for the staging air intake, Fig. 3. On the other hand, the sulfur content decreases
proportionally with the fraction of the woody biomass in the mixture. Although SO2
emission results are indented, it can still be concluded that the SO2 emissions decrease
with the increase of the biomass fraction in the mixture. On the other hand, based on
the results of the measurements it can be concluded that SO2 emissions do not prac-
tically depend on the mode of the air intake—the emissions are high and at process
temperature of 1350 °C are on an average of about 5300 mg/m3
n, Fig. 3.
The total and net CO2 emissions due to combustion of woody biomass as renew-
able fuel are shown in Fig. 4. By increasing the fraction of woody biomass, the
emission of the net CO2 emission is linearly reduced and it is 0.233 kg/m3
n with 10% of
biomass in the mixture. Some more increased CO emissions during co-ﬁring coal with
biomass compared to ﬁring only coal, especially at 10% biomass fraction, can be
1000
2000
3000
4000
5000
6000
250
500
750
1000
1250
1500
U100
U95B5
U93B7
U90B10
eSO2,
mg/mn3
eNOx,
mg/mn3
Temperatura: 1350 °C
NOx: 0,90/1,20
NOx: 0,95/1,20
NOx: 1,20/1,20
SO2: 0,90/1,20
SO2: 0,95/1,20
SO2:1,20/1,20
Fig. 3. NOx and SO2 emissions during co-ﬁring of coal with woody biomass at process
temperature of 1350 °C
0
50
100
150
200
250
300
0
0,1
0,2
0,3
0,4
0,5
0,6
U100
U95B5
U93B7
U90B10
eCO,
mg/mn3
eCO2,
kg/mn3
Temperatura: 1350 °C
CO2: 0,90/1,20
CO2*: 0,90/1,20
CO2: 1,20/1,20
CO: 0,90/1,20
CO:1,20/1,20
Bruto emisija CO2
Neto emisija CO2
Fig. 4. CO2 and CO emissions during co-ﬁring of coal with woody biomass at process
temperature of 1350 °C
1106
N. Hodžić et al.

related to granulation of the biomass that is not mechanically treated (minced)—a
certain fraction of biomass particles with size of up to 4 mm, Fig. 4.
With the increase in combustion temperature the NOx and SO2 emissions also
increase. Thus, average NOx emissions at temperatures of 1400 and 1450 °C: 750 and
800 mg/m3
n respectively, and SO2 emissions under these conditions: about 5000 and
5200 mg/m3
n respectively. Even in these test regimes, it has been conﬁrmed that NOx
emissions are practically unchanged with the increase of the biomass fraction in the
mixture and that the SO2 emission decreases proportionally with the increase of the
biomass fraction in the mixture—Fig. 5.
The dependence of NOx and SO2 emissions from the place of introduction of OFA
air into the furnace or reactor is shown in the following ﬁgure. Speciﬁcally, here are the
results of co-ﬁring coal with 7% woody sawdust at 1400 °C and with different options
for introducing OFA air into the reaction pipe: OFA 1 at a distance of 1 m from the
burner outlet, then OFA 2 at a distance of 1.3 m and the OFA 1 and 2 option—
simultaneous OFA air intake at both levels. The results show that the NOx emissions, in
this case, are smallest when the OFA air is introduced at position 1, while it is slightly
higher in the case of position 2. When the OFA air supplied at both positions (1 and 2)
in the same time, it can be concluded that the emissions NOx are practically at the same
level as when using position 2 for OFA air intake. In any case, these emissions—no
matter which of the positions for OFA air intake is used—are signiﬁcantly lower than
emissions without OFA air. On the other hand, using OFA air does not affect the SO2
emissions and is practically equal to emissions without OFA air—in this case the
average is over 4800 mg/m3
n, Fig. 6.
Regarding the possibilities of reduction of NOx emissions, the previous results of
research and knowledge are of key importance for the design of future but also possible
reconstruction of existing boilers, in particular the furnaces for the choice of a
place/zone of staging air intake in the combustion zone. This certainly has an impact on
the design and choice of boiler accessories.
1000
2000
3000
4000
5000
6000
250
500
750
1000
1250
1500
U100
U95B5
U93B7
U90B10
eSO2,
mg/mn3
eNOx,
mg/mn3
Temperatura: 1400 °C
NOx: 0,90/1,20
NOx: 0,95/1,20
NOx: 1,20/1,20
SO2: 0,90/1,20
SO2: 0,95/1,20
SO2:1,20/1,20
Fig. 5. NOx and SO2 emissions during co-ﬁring coal with woody biomass at process
temperature of 1400 °C
Effects of Primary Measures in Combustion Chamber
1107

The effects of applying 10% of the basic fuel stage to the NOx emission depending
on the process temperature, for the fuels of different composition, are presented in
Fig. 7. For example, at a temperature of 1450 °C for coal (U100), this emission
decreased from 892 to 694 mg/m3
n or more than 22% relative to the emission at the
same temperature, quantity and mode of air supply but without fuel staging. However,
it should be noted that in these test regimes there was a signiﬁcant increase in CO
content in ﬂue gases [6, 7], for example U100: 686 mg/m3
n at 1350 °C and 140 mg/m3
n
at 1450 °C.
The use of natural gas as an additional fuel (reburning technology) results in an
additional reduction in NOx emissions relative to the emissions previously presented
while applying staged basic fuel intake into combustion zone.
The NOx emission during co-ﬁring coal with biomass (U95B5) when applying
different primary measures in furnace, including the efﬁciency of these measures by
0
1000
2000
3000
4000
5000
700
800
900
1000
1100
1200
1,20/1,20
OFA 1
OFA 2
OFA 1 i 2
eSO2,
mg/mn3
eNOx,
mg/mn3
U93B7: t=1400 °C
NOx: 0,90/1,20
NOx: 0,95/1,20
NOx: 1,20/1,20
SO2: 0,90/1,20
SO2: 0,95/1,20
SO2:1,20/1,20
Fig. 6. NOx and SO2 emission dependence of the OFA air introduction place
500
600
700
800
900
1000
1300
1350
1400
1450
1500
eNOx,
mg/mn3
Temperatura, °C
10%m
U100 -clasic
U100
U95B5
U90B10
K70B20Z10
Fig. 7. Impact of the temperature on NOx emissions with staging combustion of fuel with
different composition
1108
N. Hodžić et al.

reducing this emissions compared to the emission with conventional combustion is
presented in Fig. 8—the results refer to the temperature of 1450 °C [8].
Efﬁciency of primary measures by staging the base fuel and using natural gas as
additional fuel is weakening with rising process temperatures. Namely, the efﬁciency of
the basic fuel staging at 1450 °C is 21% compared to 32% at 1350 °C, while the
efﬁciency of natural gas utilization for those temperatures is 39% and 46% respectively.
Thereby, the difference in efﬁciency of the application of combustion air staging for the
observed combustion temperature is the smallest [9].
Generally, by adding wood biomass to the coal mixture, the base and acid numbers
of this mixture increases. With the fraction of woody biomass in the mixture up to 10%,
the base and acid number ratio is slightly increasing in relation to the coal mixture, so
practically the coal-biomass mixture in the spreadsheet (RB/K–t) remains in the zone
with strong tendency to fouling/slagging [5].
4
Conclusion
Air staging:
• NOx emissions in co-ﬁring coal with waste woody biomass is at the emission level
at combustion of coal blend. There is practically no change in this emission with the
change in the fraction of woody biomass in the mixture. The average difference in
NOx emissions depending on the mode of combustion air is 250 mg/m3
n: for the
classical air intake the average emission is 942 mg/m3
n, while for the leveled air
intake it is 692 mg/m3
n.
• The SO2 emission is slightly reduced by increasing the fraction of woody biomass
and practically does not depend on the mode of combustion air intake—the emis-
sions are high and generally over 5000 mg/m3
n.
• By increasing the fraction of woody biomass in the mixture, the CO2 net emission is
989
823
782
602
0
17
21
39
0
20
40
60
80
100
120
0
200
400
600
800
1000
1200
Clasic
Air staging 
(OFA)
Fuel + air 
staging (OFA)
Reburning 
(OFA)
e, %
eNOx,
mg/m n3
U95B5: t=1450 °C
10%e
10%m
Δ
Fig. 8. Efﬁciency of primary measures at temperature of 1450 °C: fuel U95B5
Effects of Primary Measures in Combustion Chamber
1109

proportionally reduced, which at the 10% biomass in the blend is 0.233 kg/m3
n.
• The NOx emission also depends on the location of the OFA air in the combustion
chamber. So the NOx emission is smallest when the OFA air is supplied at 1/3 of the
length of the reaction tube/combustion chamber). The location of the OFA air intake
(positions 1 and 2) does not affect the SO2 emission value.
Fuel staging:
• In the case of combustion with a fuel staging NOx emission is reduced—this
reduction is proportional to the part of the fuel that is introduced into the reaction
zone afterwards. In the case of fuel staging of 10%m and temperature of 1350 °C,
the reduction of NOx emissions is about 100 mg/m3
n or 15% on average with respect
to emissions for combustion without fuel staging.
• For combustion with staged base fuel intake signiﬁcant increase in CO content in
ﬂue gases appears, for example U100: 686 mg/m3
n at 1350 °C and 140 mg/m3
n at
1450 °C. This phenomenon requires ﬁnding the optimum solution that will make
the combustion process economically viable and environmentally acceptable
regarding the levels of NOx and CO emissions. This implies ﬁnding the best ratio of
the staged fuel brought into the combustion zone (%) on the one side and the place
of introduction of the staged fuel in comparison to the primary combustion
zone/burner on the other side.
Reburning technology:
• Combustion with natural gas as additional fuel results in an additional reduction in
NOx emissions—this reduction is proportional to the fraction of natural gas that is
introduced into the reaction zone at a later stage. For example, when coal is co-ﬁred
with biomass under these conditions (UB (95) P10) and at temperature of 1450 °C,
the measured emission is 602 mg/m3
n, which is 30.5% less than emission with
U100, that is 866 mg/m3
n.
• In combustion regimes with natural gas, the CO emission is very low, especially
during co-ﬁring at temperatures above 1400 °C, where this emission is virtually
negligible: below 5 mg/m3
n.
Evaluation of the efﬁciency of primary measures in the combustion chamber—
the effects of the application of primary measures in combustion chamber for fuels
U100 and U95B5 for process temperatures ranging from 1350 to 1450 °C:
• By using natural gas in combustion of coal U100 at temperature of 1450 °C, the
emission of NOx decreased to 615 mg/m3
n or by almost 50% compared to the
emission of conventional combustion: 1154 mg/m3
n.
• Efﬁciency of primary measures by staging the base fuel and using natural gas as
additional fuel weakens with rising process temperatures. Efﬁciency of the staging
basic fuel at 1450 °C is 20.9% compared to 31.9% at 1350 °C, while the efﬁciency
of natural gas utilization for those temperatures is 39.1% and 46.2% respectively. In
doing so, efﬁciency of the application of staging combustion air is practically the
same for both observed combustion temperatures.
1110
N. Hodžić et al.

• Despite the increase of the base and acid number ratio with the addition of woody
biomass to the coal mixture, it has been shown that utilization of wood biomass in
the process gives even positive effects in the formation of more abundant deposits
of ash.
References
1. Data base VGB, 06/2010, 09/2012
2. Uticaj kvaliteta uglja na troškove proizvodnje električne energije i cijenu uglja; Studija;
Naručilac: JP Elektroprivreda BiH d.d. Sarajevo, Izvršilac: Mašinski fakultet Sarajevo i
Rudarski institut d.d. Tuzla, Sarajevo (2014)
3. Hodžić, N., Kazagić, A., Smajević, S.: Motiv za uvođenje u praksu kosagorijevanja uglja i
drvne biomase u TE Kakanj - iskustva iz probnog pogona podržana laboratorijskim
istraživanjima, 12. Savjetovanje bosansko-hercegovačkog komiteta Cigré, Neum, Bosna i
Hercegovina (2015)
4. Advanced Decentralised Energy Generation Systems in Western Balkans - ADEG, Projekt
FP6, National Technical University of Athens, Institut IVD Stuttgart, Fakultet Strojarstva i
Brodogradnje Zagreb, Mašinski fakultet Sarajevo, Institut Vinča, IST Portugal, voditelj
projekta ispred MF Sarajevo: A. Lekić, podprojekat: Biomasa, voditelj podprojekta: I.
Smajević, član istraživačkog tima: N. Hodžić, i dr. (2004–2007)
5. Hodžić, N.: Istraživanje kosagorijevanja uglja i biomase usmjereno na smanjenje emisija
primarnim mjerama u ložištu, Doktorska disertacija, Mašinski fakultet Univerziteta u
Satajevu, Sarajevo (2016)
6. Rozendaal, M.: Impact of Coal Quality on NOx Emissions from Power Plants. Delft
University of Technology, Delft (1999)
7. Wang, Y., Wang, X., Hu, Z., Li, Y., Deng, S., Niu, B., Tan, H.: NO emissions and
combustion efﬁciency during biomass co-ﬁring and air-staging. Bio Resour. 10(3), 3987–
3998 (2015)
8. Hodzic, N., Kazagic, A., Smajevic, I.: Inﬂuence of multiple air staging and reburning on NOx
emissions during co-ﬁring of low rank brown coal with woody biomass and natural gas. Appl.
Energy 168, 38–47 (2016)
9. Hodžić, N., Metović, S., Džaferović, E., Kazagić, A.: Woody Sawdust and Miscanthus
co-ﬁring—towards more sustainable solution for low-rank coal—a lab-scale investigation. In:
8th European Combustion Meeting (ECM 2017), 18–21 April 2017, Dubrovnik, Croatia
(2017)
Effects of Primary Measures in Combustion Chamber
1111

A Heat Exchanger with Finned Tube
and Phase-Change Material
for Thermal-Energy Storage: Effect of Gravity
and Orientation
Muris Torlak(&) and Nijaz Delalić
Univerzitet u Sarajevu – Mašinski fakultet,
Vilsonovo šetalište 9, 71000 Sarajevo, Bosnia-Herzegovina
torlak@mef.unsa.ba
Abstract. A device for storage of thermal energy in form of latent heat is
presented. The device is intended to be used either as portable or as stationary
unit for storage of energy stemming from intermittent sources (such as solar
power plants) or from sources with variable energy prices, to dampen temper-
ature ﬂuctuations (for example, in heating systems), or for waste heat recovery.
Delayed and/or remote use of energy from the storage unit can increase overall
energy efﬁciency and contribute to cost savings. In the device tested in this
study, water is adopted as heat transfer ﬂuid (HTF) both for charging and
discharging of thermal energy, and its temperature is deﬁned appropriately to the
investigated process. A phase-change material (PCM) is used as storage medium
due to relatively large amount of energy to be stored at relatively narrow tem-
perature range. Heat transfer rate within the storage device turns out to be
decisive for its proper performance. Low thermal conductivity of the PCM may
be a drawback. Hence, a design solution based on heat exchanger with ﬁnned
tube is adopted to increase the heat transfer rate from HTF to PCM and vice
versa. The measurements of temperature on vertically oriented storage unit have
conﬁrmed advantage of the ﬁnned design, as compared to the plain, smooth
tube. However, in vertical orientation the heat transfer rate may be a conse-
quence of the convective effects. This paper addresses the effect of storage unit
orientation and herewith related effect of gravity on the heat transfer regime
possibly triggered by natural convection, and consequently their effect on the
phase change process. The overall performance of the horizontally oriented
storage unit is compared with the vertically oriented one.
Keywords: Energy storage  Phase-change materials (PCM)  Heat exchangers
Heat transfer
1
Introduction
In the recent years, storage of energy, and speciﬁcally thermal energy storage (TES),
has gathered a considerable signiﬁcance in the world-wide research and engineering
development, as well as in practical solutions and implementations. TES may bring a
number of beneﬁts in various applications, for example: (a) use of energy stemming
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_94

from intermittent sources, such as solar power plants, to provide continuous energy
supply, (b) use of energy from sources with variable energy prices, such as electrically
driven heat pumps, to reduce the operating costs, (c) to dampen temperature ﬂuctua-
tions, for example in various HVAC systems to support uniform workload or promote
more appropriate thermal comfort, or (d) for waste heat recovery. Delayed and/or
remote use of energy from the storage unit can increase overall energy efﬁciency and
contribute to cost savings.
Depending on speciﬁc goals and operating conditions, two basic methods of TES
can be used: sensible-heat and latent-heat storage [1, 2]. Sensible-heat storage can be
employed using cheap and easily accessible materials, such as water. They are practical
where large dimensions and large temperature differences are acceptable. High tem-
peratures achieved in the storage medium could cause relatively large energy losses,
and thus require appropriate thermal insulation.
On the other hand, latent-heat storage can be used to store relatively large amounts
of thermal energy at narrow temperature ranges. Use of latent heat means that the
storage material exhibits phase change. Typically, solid-liquid phase change is applied.
One of the important decisions in design of a latent-heat TES is choice of the
storage material. Both the amount of energy to be stored (accordingly to the latent heat
of the material) as well as the phase change temperatures have to be adopted in
accordance with the requirements of the system in which the storage is to be imple-
mented. Most of the used materials, however, suffer from low thermal conductivity
which degrades heat transfer process.
In the previous studies we investigated different design solutions: a shell-and-tube
heat exchanger [3], a cylindrical heat exchanger with an internal helically coiled pipe
for HTF [4, 5], as well as a shell-and-tube heat exchanger with smooth and ﬁnned tube
[6]. Using experimental techniques and/or numerical computational ﬂuid dynamics
(CFD) simulations we assessed the design methods and solutions to increase the heat
transfer rate by modiﬁcation of the heat transfer surface (contact surface between HTF
and PCM) and by convection in the liquid phase. This paper presents results of
experimental investigation of the change of orientation of a TES-unit, and its conse-
quent inﬂuence of gravity and herewith related natural convection in the liquid phase
onto the heat transfer.
2
Problem Description and Experimental Setup
The thermal-energy storage unit built and tested in this study is illustrated in Fig. 1.
A copper tube is placed axially in a bottle-like transparent plastic container. The
remaining interior of the container is ﬁlled with the PCM for storage of thermal energy.
For that purpose, sodium acetate trihydrate is used. Typical physical properties of the
PCM used are shown in Table 1. The commercially available sheep wool produced by
Wool Line® is used as thermal insulation due to its superior insulating properties [7].
A Heat Exchanger with Finned Tube and Phase-Change
1113

Energy charging and discharging occurs through the HTF tube, from the point T1
to the point T2, and from the interior outwards. Water is used as the HTF, which is
heated upstream the storage unit by an electric heater with a thermostat. The average
ﬂow rate of HTF is about 0.3 l/min which results in Re-number value of about 700,
implying thus laminar ﬂow regime in the HTF tube. In the previous study [6] it was
shown that a copper tube with the copper ﬁns mounted on its outer side increases the
heat transfer rate into or from the PCM, as compared to a corresponding tube with plain
outer surface. Reduction of the container volume due to the ﬁns is relatively small and
is not regarded as relevant.
In this work, the effect of orientation and gravity is analyzed, monitoring temper-
ature distributions and their temporal variations in vertical and horizontal case. Tem-
perature variations are measured at four positions (see points T1 to T4 in Fig. 1) using
type-K thermocouples (Nickel-Chromium/Nickel). Sampling period of temperature
measuring is 10 s.
Fig. 1. Thermal-energy storage unit: a vertical and b horizontal conﬁguration with indicated
positions of the measurement points
Table 1. Basic material properties of sodium acetate trihydrate
Sodium-acetate-trihydrate
Melting point
58 °C
Latent heat
226–270 kJ/kg
Density of solid phase at 20 °C
1450 kg/m3
Density of solid phase in powder form
900 kg/m3
Density of liquid phase
1280 kg/m3
Speciﬁc heat capacity of solid phase at 25 °C 2790 J/kg  K
Speciﬁc heat capacity of liquid phase
3000 J/kg  K
Thermal conductivity of solid phase
0.7 W/K  m
Thermal conductivity of liquid phase
0.4 W/K  m
1114
M. Torlak and N. Delalić

3
Results and Discussion
Figure 2 displays the temperature histories recorded on the vertically oriented storage
unit recorded at the measurement points, as shown in Fig. 1 (left).
After a relatively fast temperature increase, which takes about half an hour, the
phase-change process in the measurement points T3 and T4 is observed. This is
Fig. 2. Vertically oriented storage unit with the measured temperature histories
Fig. 3. Horizontally oriented storage unit with the measured temperature histories
A Heat Exchanger with Finned Tube and Phase-Change
1115

characterized by the ﬂat part of the temperature curve and the very slow temperature
increase at continuously supplied heat. The phase change takes nearly 1 3/4 h. After the
melting is ﬁnished, the temperature increases due continuous heat supply. In the liquid
phase, temperature T3, located at the half of the container height, becomes larger than
at the bottom (T4). This may be caused by two effects: (i) natural convection inﬂuenced
by gravity—warmer liquid phases ﬂows upwards and transfers the heat, (ii) the ﬂow
along the tube develops so that change in the water velocity proﬁle across the tube
diameter and possible transition to turbulence may increase the heat transfer rate in the
tube interior. Since this temperature relation between the two points is observed only in
the liquid phase, it is more probable that it is caused by natural convection.
Cooling process is activated about 2.5 h after the beginning. Heating of the HTF is
reduced by thermostat regulation. Temperature of the HTF is lower than the PCM
melting point. The HTF ﬂows through the copper tube, triggering a quick temperature
decrease and herewith solidiﬁcation in the PCM. Solidiﬁcation occurs at nearly the
same temperature as the melting temperature. A slight trend deviation of temperature
histories at the points T3 and T4 is observed. While the temperature curve for the point
T4 is practically ﬂat during the solidiﬁcation, at the point T3 subcooling of PCM and a
slight subsequent temperature increase can be seen. With further cooling the solidiﬁ-
cation at the points T3 and T4 is ﬁnished after about 1.5 h, whereupon the temperatures
in solid phase continue to decrease. Temperature at the bottom region (T4) is lower
than in the upper part (T3) due to the lower HTF temperature at the inlet (on the bottom
side) during the cooling.
Figure 3 shows the temperature histories measured at the same points of the hor-
izontally placed storage unit. After its rotation, the point T3 is on the lower side and the
point T4 is on the upper side. The trends of the temperature curves are similar to those
obtained in the vertical orientation. However, there are some quantitative differences.
According to the ﬂat parts of the temperature curve, both melting and solidiﬁcation
are shorter than in the case of vertical orientation. This can be attributed to slightly
higher temperatures of the HTF: in this case between 75 and 79 °C, while in the
previous case it was in the range between 72 and 76 °C. At the point T4 melting is
completed earlier than at the point T3. In addition to that, while in the liquid phase, the
temperatures at the point T4 are higher than those at the point T3. This ﬁnding is
different from the previous case, and it is primarily addressed to the higher HTF
temperature at the inlet region during the heating process. It is questionable if natural
convection arising in the cross-section of the horizontal storage unit makes any con-
tribution to this temperature difference.
During solidiﬁcation temperature at T4 shows nearly ﬂat behavior, however the
temperature level is lower than at the point T3. This might by caused by the increased
thermal resistance on the top side of the container. Here, after solidiﬁcation is started,
mushy and solid phase shrink ﬁlling the lower part of the container due to gravitation,
and leaving its upper part empty, so that an air gap between the measurement point T4
and the PCM is created.
1116
M. Torlak and N. Delalić

4
Conclusion
A device for thermal energy storage is built and tested. Thermal energy is stored in
latent form within a PCM placed in a heat exchanger. The PCM used in this work has
relatively low thermal conductivity which slows down the heat transfer throughout the
PCM and herewith degrades the storage effect. The heat transfer is promoted by
appropriate heat exchanger design with ﬁnned tube.
It is shown that change of orientation in relation with gravity force affects heat
transfer by natural convection, and herewith changes the temperature distribution in the
PCM as well as dynamics of its temporal variation.
The characteristic temperature levels depend on the melting point of the PCM used
as well as on the external conditions (primarily, on the inlet temperature of the HTF
during the charging and discharging process, and partly on the ambient temperature).
In addition to that, in horizontal conﬁguration the liquid phase separates from the
top surface leaving thus empty space on the upper side. Due to this and natural
convection the heat transfer is not axisymmetric around the HTF tube which is a
drawback regarding efﬁciency of the heat transfer inside the storage unit.
References
1. Mehling, H., Cabeza, L.F.: Heat and Cold Storage with PCM. Springer, Berlin, Heidelberg
(2008)
2. Dincer, I., Rosen, M.A.: Thermal Energy Storage—Systems and Applications, 2nd edn.
Wiley (2011)
3. Torlak, M., Teskeredžić, A., Delalić, N.: Modeling and simulation of heat storage in
phase-change materials based on computational ﬂuid dynamics. In: Proceedings of the 17th
International Research/Expert Conference on Trends in the Development of Machinery and
Associated Technology TMT 2013, Istanbul, Turkey, September 2013, pp. 405–408 (2013)
4. Torlak, M., Delalić, N., Duraković, B., Gavranović, H.: CFD-based assessment of thermal
energy storage in phase-change materials (PCM). In: Proceedings of the Energy Technologies
Conference ENTECH’14, Istanbul, Turkey (2014)
5. Delalić, N., Delalić, B., Torlak, M.: Phase-change material heat exchanger with coil tubes—
experiment and numerical analysis. In: Proceedings of the 19th International Research/Expert
Conference on Trends in the Development of Machinery and Associated Technology, TMT
2013, Barcelona, Spain, July 2015, pp. 217–220 (2015)
6. Torlak, M., Delalić, N.: Latent-heat thermal-energy storage in heat exchanger with plain and
ﬁnned tube. In: Proceedings of the 3rd World Congress on Mechanical, Chemical, and
Material Engineering (MCM’17), Rome, Italy, 9–10 June 2017, paper no. HTFF-160 (2017)
7. Bericht ueber die Pruefung des Isolationsmaterials aus natuerlicher Wolle. Technical report
no. 5.4-146-01/17, Institut za građevinarstvo, građevinske materijale i nemetale – GIT, Tuzla,
Bosnia-Herzegovina (2017). http://www.wool-line.ba/
A Heat Exchanger with Finned Tube and Phase-Change
1117

Using of Remote Sensing Methods in Wind
Analysis in Conditions of Local Wind Bora
and Complexity of Terrain
Elvir Zlomušica1(&), Suad Zalihić1, Miralem Čampara2,
and Ramiz Zaimović2
1 Univesity “Džemal Bijedić” of Mostar, Mostar, Bosnia and Herzegovina
elvir.zlomusica@unmo.ba
2 IMPRO IMPEX d.o.o. Mostar, Mostar, Bosnia and Herzegovina
Abstract. More accurate assessment of wind potential in conditions of local
wind Bora and complexity of terrain requires performing of measurement
campaigns at higher levels and in several measuring places. Last measurements
in Bosnia and Herzegovina were performed by using of 80 m high measurement
mast, which seems measurement expensive, due to high costs of procurement
and installation of equipment and its maintenance during the measurement
period. For these reasons, it is effective to use different measuring methods. The
analysis and comparison of collected measurement data from the measurement
mast and the LIDAR remote sensing device (Windcube v2 FCR), as well as the
behavior of the equipment itself in the complex terrain and wind Bora conditions
at location Hrgud near the town of Stolac is presented in this paper. Ten minutes
average time intervals were used for comparision of wind speed and direction. It
can be concluded that the behavior of the LIDAR under harsh local weather
conditions was well. The LIDAR uncorrected wind speed was in general lower
than the wind speed measured by the cup anemometer at the same height. The
comparison of the data of measurement methods provides reliable information
on the wind speed within the considered altitude range.
Keywords: Wind potential  LIDAR  Bora  Complexity of terrain
1
Introduction
Although the Bora wind has been researched for many years (much more in Croatia
and less in Bosnia and Herzegovina), the research of Bora characteristics performed
over the last 10 years has shown unreliable estimates of wind energy potential in cases
where measurements were made in levels below the height of wind generator. The
existence of large differences in wind characteristics at individual locations, for
directions and heights were discovered. The reasons are primarily conditioned by the
characteristics of the Bora wind, but also by the characteristics of terrain in the areas
where the Bora blows.
Bora is a speciﬁc local wind that blows on the eastern coast of the Adriatic Sea (the
area between Trieste in Italy and Skadra in Albania). It is usually north or north-east
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_95

direction and often achieve stormy strength. It appears in areas where cold air, coming
from the land, ﬂows along a mountain barrier (the mountains of the Dinaric Massif) and
accelerates on the opposite side. Mountains, especially those higher, represent the main
corridors of air ﬂow and have a signiﬁcant inﬂuence on the ﬂow directions and the
intensity of speeds.
According to the international standard IEC 61400-12-1 [1] such terrains are
classiﬁed as complex terrains, or as terrains with a mean inclination angle of ﬂow
greater than ±15° and with higher turbulence intensity. The classiﬁcation of terrains
according to its complexity is quite complex and it is still in development.
The used classiﬁcation parameters are only orientational and they are introduced to
distinguish the levels of terrain complexity. The complex terrain can be divided into
three basic subcategories: moderately complex, complex and highly complex terrain.
According to EWA (European Wind Atlas) [2], the classes of complexity derived from
the roughness classes and terrain types, and their combinations. The terrains in Bosnia
and Herzegovina are mostly classiﬁed as complex terrains.
Detailed knowledge of the wind resource is necessary in the developmental and
operational stages of a wind farm site [1]. As wind turbines continue to grow in size,
measurement masts for mounting cup anemometers (the standard procedure for wind
resource assessment) have become much taller, and much more expensive.
The LIDAR (LIght Detection And Ranging) is ground-based and can work over
one hundred of meters, sufﬁcient for the tall wind turbines. The use of LIDAR in
complex terrain is very attractive for wind site assessments since a grinding installation
of a high mast can be avoided. The measurement campaigns in some projects showed
very promising results [3–7]. Some strengths of the LIDAR are: relatively easy to
deploy, still some ﬁngering with cables and tubes, installed by one or two person(s) in
half a day, withstanding harsh climatic conditions, low power consumption and no
noise, while some weaknesses of the LIDAR are: uncertainty of accuracy of wind
speed data in complex terrain, very expensive high-tech instrument, affected by rain
and low clouds.
However, to the present day it is not recommended to use a LIDAR as a stand alone
instrument for accurate wind measurements. More validation studies and comparisons
are needed and data retrieval algorithms (vertical wind speed, turbulence) have to be
improved. Furthermore, the assumption of a homogeneous ﬂow ﬁeld used by the
LIDAR technology has to be considered in the data analyses, especially in complex
terrain. In the next period the standard [1] is expected to be changed, and a new
standard will include remote sensing techniques like the LIDAR.
The aim of the work is to compare the wind measurements from commercial
LIDAR instrument against an instrumented mast, in complex terrain, where many wind
farms are now being installed worldwide, as well as equipment behavior under harsh
meteorological conditions at the locality of Hrgud (southeast of Bosnia and
Herzegovina).
This equipment has been used ﬁrst time in Bosnia and Herzegovina. Measurements
were performed during the summer-fall period of 2013 in the duration of 4 months.
Using of Remote Sensing Methods in Wind Analysis
1119

2
Method and Equipment
A 4-months measurement campaign with LIDAR Windcube v2 and 78 m mast, which
has provided also long term data, for evaluating the remote sensing instruments, was
performed. The LIDAR was connected directly to the electrical grid via the local power
line. However this power line has been hit by lightings several times during the
measurement campaign, which has destroyed the 220–24 V LIDAR convertor.
2.1
Site Description
The measurement site is located in mountainous area Hrgud, approximately 35 km
southeast of the city of Mostar or 5 km east from the town of Stolac, 10–11 km west
from the town of Berkovići. The area can be categorized as complex, with altitudes
varying between 960 and 1110 m asl (above sea level). The surface of area is
approximately 5 km2, Fig. 1. A southeast–northwest fault delimiting the plateau is
characterised with a very steep slope, which have a signiﬁcant inﬂuence at the wind
ﬂow at the site. The terrain is characterized by karsts with small meadows, bushes and
low forest vegetation. The area is mainly used for non-industrial grazing.
Site conditions parameters for the met mast height 77.5 m are: max 10-min.
measured wind speed is 31.4 m/s, max 3-s. measured wind speed is 38.7 m/s, annual
mean temperature is 10.2 °C, annual min. temperature is −10 °C and annual mean air
density is 1.083 kg/m3 [8].
2.2
Measurement Conﬁguration
The instruments were installed at a height 1098 m asl. The instruments site coordinates
are: X = 6502 939, Y = 4772 697 of Gauss Kruger projection, according to the
Fig. 1. Appearance of the Hrgud site with location of measurement equipment
1120
E. Zlomušica et al.

resolution of the GPS device. The instruments were located on 100 m high hill about
1.5 km North of a 1000 m deep and 2 km wide canyon of Bregava River. The hill is
about 1 km long and 100 m wide, oriented E-W.
The LIDAR was positioned approximately 1.5 m from measuring mast, Fig. 2. The
sensor height in the LIDAR is 1 m above mast ground level. Therefore 1 m shall be
added to the entered heights to get the actual measuring height. The LIDAR mea-
surement started on 23 August 2013. The system consists of a Windcube v2 LIDAR,
set up to measure the windspeed at 10 different heights. The LIDAR is powered by
220 V supplied from the commercial grid. Data from the LIDAR shall be used as
supplement to data from Hrgud mast (as long term data), in order to give a better
assessment of the wind conditions on the site.
LIDAR Windcube v2 equipped with FCR (Flow Complexity Recognition) for
direct wind measurements in complex terrain was used in this campaign. The height
range of this instrument is from 40 to 200 m, data sampling rate is 6 s. The mea-
surement conﬁguration and measurement periods of the met mast and the LIDAR are
shown in Table 1.
Fig. 2. The measurement site Hrgud, LIDAR and mast
Using of Remote Sensing Methods in Wind Analysis
1121

3
Results
3.1
Analyse of the Data Availability
The analyzed period started 23 August 2013 and it ended 19 December 2013. The time
series of the wind speed measured with the top cup anemometer at 77.5 m and the
LIDAR without and with FCR mode at 77 m are displayed in Fig. 3.
The LIDAR measurements were interrupted several times during measuring cam-
paign. Much more LIDAR incorrect wind speed data (non-corrected for the terrain
effect) were collected than FCR corrected data.
Table 1. Measurement conﬁguration
Measurement
height of wind
speed (m)
Measurement
height of wind
direction (m)
Measurement
period
Mast
cup anemometers, Thies
Classic, wind vanes Thies
Compact
30; 55; 55; 75;
77.5
53 and 75
30 July 2012–
31 December
2013
LIDAR
Windcube v2
44; 54; 64; 74; 77;
79; 89; 119; 129;
159
44; 54; 64; 74; 77;
79; 89; 119; 129;
159
23 August
2013–19
December 2013
0
5
10
15
20
25
30
1. sep.
1. okt.
1. nov.
1. dec.
Mast,  z=77.5m
Lidar No FCR, z=77m
LIDAR FCR, z=77 m
Fig. 3. Time series of the wind speed measured with the top cup anemometer (blue) and the
LIDAR without FCR correction (red)
1122
E. Zlomušica et al.

3.2
Comparison of the Measured Wind Speeds
The comparison of the measured wind speeds between the cup anemometer at 77.5 m
and the uncorrected LIDAR measurements and the FCR corrected LIDAR data at
77 m, respectively, for the same dataset is performed. Dataset including uncorrected
and FCR corrected LIDAR wind speed data with an availability more than 80%.
The incorrect LIDAR wind speed measurements underestimate the cup anemometer
by 3% on average. The FCR corrected LIDAR wind speed measurements overestimate
the cup anemometer by 1.7% on average. The correlation coefﬁcient is higher for the
FCR corrected data than for the incorrect measurement. Also, in case of uncorrected
LIDAR wind speed measurements, the wind speed deviations Dv are negative and
increase with speed intensity, but in case of FCR corrected LIDAR wind speed mea-
surements the deviations Dv are positive and their increasing was not noticed, Fig. 4.
Fig. 4. Wind speed deviation Dv measured by LIDAR at 77 m versus to the cup anemometer at
77.5 m (blue: LIDAR without FCR corrections; green: LIDAR with FCR corrections)
Using of Remote Sensing Methods in Wind Analysis
1123

Analysis of wind speed deviations Dv according to wind directions has shown, that
in case of LIDAR measurements without FCR corrections, the deviations are greater in
directions of the southern than the northern winds, what can be explained by the
inﬂuence of terrain topography. In case of LIDAR measurements with FCR correc-
tions, the dependence of deviations Dv from the wind directions was not observed,
Fig. 5.
4
Conclusions
The LIDAR incorrect wind speed was in general lower than the wind speed measured
by the cup anemometer at the same height by about 3%. The FCR corrected wind
speeds were higher than the cup anemometer wind speed by about 1.7%.
The LIDAR incorrect wind directions compared well to the wind vane and the FCR
correction had no signiﬁcance inﬂuence on this comparison.
Fig. 5. Wind speed deviation Dv measured by LIDAR at 77 m versus to the wind vane at 75 m
(blue: LIDAR without FCR corrections; green: LIDAR with FCR corrections)
1124
E. Zlomušica et al.

It can be concluded that the LIDAR functioned “relatively well” in complex
conditions of the terrain and wind characteristics of Bora. For more relevant observance
of Bora characteristics and behavior of the equipment in a complex location like this
one, it is necessary to carry out measurements in a longer period of time and in different
seasons.
Acknowledgements. This work was performed under project Measurement wind program for
the RS, Bosnia and Herzegovina. The authors are grateful for the considerable technical support
from the staff of the COWI A/S Lyndgby, Denmark and Impro Impex do.o. Mostar, B&H.
References
1. International Standard. IEC 61400-12-1 Ed. 1. Power performance measurements of
electricity producing wind turbines (2005)
2. Troen, I., Petersen, E.: European Wind Atlas. Risø National Laboratory, Denmark (1989)
3. Albers, A., Janssen, W., Mander, J.: Comparison of LIDARs, German test station for remote
wind sensing devices. In: German Wind Energy Conference, DEWEK, Bremen, Germany
(2008)
4. Bingöl, F., Mann, J., Foussekis, D.: Lidar performance in complex terrain modeled by WASP
engineering. In: European Wind Energy Conference and Exhibition, EWEC, Marseille,
France (2009)
5. Bourgeois, S., Cattin, R., Locker, I., Winkelmeier, H.: Analysis of the vertical wind proﬁle at
a BORA—dominated site in Bosnia based on SODAR and ZephIR LIDAR measurements. In:
European Wind Energy Conference and Exhibition, EWEC, Brussels, Belgium (2008)
6. Bourgeois, S., Cattin, R., Winkelmeier, H., Locker, I.: CFD Modeling of the vertical wind
proﬁle and the turbulence structure above complex terrain and validation with SODAR and
LIDAR measurements. In: European Wind Energy Conference & Exhibition, EWEC,
Marseille, France (2009)
7. Krishnamurthy, R., Boquet, M., Machta, M.: Turbulence Intensity Measurements from a
Varity of Doppler LIDAR. In: European Wind Energy Conference & Exhibition, EWEA,
Barcelona, Spain (2014)
8. Impro-Impex, COWI.: Hrgud—12 Months Wind Study and Site Conditions Report (2013)
Using of Remote Sensing Methods in Wind Analysis
1125

Computational Study of Pulverized Coal
Combustion in Boiler—Unit 7 of TPP Kakanj
Adnan Ðugum1(&) and Kemal Hanjalić2
1 Mechanical Engineering Faculty, University of Sarajevo, Sarajevo, Bosnia and
Herzegovina
djugum@gmail.com
2 Delft University of Technology, Delft, The Netherlands
Abstract. Computational
simulations
and
mathematical
modelling
have
become an indispensable tool for analysis, solution, prediction and optimization
of processes in various industries. We report on the simulation of combustion of
dust-coal in a boiler Unit 7 (230 MWe) of Thermal Power Plant (TPP) Kakanj,
BiH. This boiler was chosen for the study because of the high temperature
corrosion detected on its side membrane walls in the furnace diffuser, suspected
to be caused due to non-optimal distributions of velocity, temperature and
composition of ﬂue gases, especially of oxygen, in the wall-adjacent regions. The
numerical simulation were carried out using Ansys Fluent CFD software with the
comprehensive combustion model based on the RANS approach for two-phase
ﬂow of dispersed coal particles in multi-component gaseous medium, with
standard chemical kinetics and radiation models. In order to obtain realistic
boundary and input conditions for the simulation of combustion in the boiler, the
precursor simulations were performed, ﬁrst for ﬂow of pulverized coal in a
double-swirl burner, and the results interpolated to 24 burner exits into the boiler.
Although computationally much more demanding, imposing the inlet data from
the precursor simulations led to improved, more realistic mixing of coal dust and
oxidant, and consequently to the combustion conditions closer to the real ones.
The simulations conﬁrmed the occurrence of adverse conditions prone to cause
the corrosion. The results of both simulations are presented and discussed.
Keywords: Combustion  CFD  Double-swirl burner  Coal dust ﬁred boiler
1
Introduction
Computational ﬂuid dynamics (CFD) of pulverized coal combustion in real-scale
thermal power plants is a prominent example of the potential of numerical simulations
of complex industrial problems, which makes it possible to gain valuable information
needed for design, analysis, operation and optimization. While measurements remain
indispensable for providing reference data, they are limited to local point or plane
domain, and constrained by other factors, including the costs. In contrast, the CFD
offers much more detailed insight into the full complexity of the processes by providing
full, time-dependent three-dimensional ﬁelds of all relevant properties—ﬂuid velocity,
temperature, species concentration, which are neither inaccessible to measurements nor
acquirable through experience.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_96

Electricity in Bosnia and Herzegovina is generated mainly in coal-ﬁred TPPs.
Although the installed hydro-power capacity is close to that of TPPs, some 80% of
electricity and almost all domestic and industrial heating comes from coal-ﬁred plants,
thus putting the coal processing and combustion efﬁciency in the focus of the power
sector of BiH.
The boiler Unit 7 of the TPP Kakanj is chosen for the present analysis because of
the persisting problem with high temperature corrosion in the side membrane walls of
the furnace diffuser. Its cause is suspected to be in the lack of oxygen in the adjacent
ﬂue gases due to inadequate distributions of velocity, temperature and ﬂue gases
composition. In order to better predict the process of combustion, which is highly
dependent on mixing of coal dust and air, a precursor CFD simulation of the ﬂow
through one burner has been performed, and the results of that simulation are then
extrapolated to all 24 burners and used as the inlet data to the boiler.
2
Mathematical Modelling
The air-particle mixture ﬂow through the burner and combustion in the boiler furnace
was simulated with the Ansys Fluent software using the RANS approach for two-phase
ﬂow of dispersed coal particles in multi-component gaseous medium, with standard
chemical kinetics and radiation models. It solves the transport equations for the con-
tinuous phase, and coupling of the pressure and velocity ﬁeld is done by SIMPLE
algorithm. Realizable high-Re number k-e model is used for prediction of turbulence.
The model contains additional terms accounting for swirl, adverse pressure gradients,
separation and recirculation. Enhanced wall treatment is applied for wall boundary
conditions. The radiation is modelled by the ﬁrst-order differential algorithm P1 based
on the expansion of the radiation intensity in terms of ﬁrst order spherical harmonics.
Superheaters at the top part of the furnace are approximated as porous media with
ﬁxed-value heat sink.
The Lagrangian approach has been used to calculate motion of the coal dust par-
ticles, meaning that velocities and heat transfer to particles are tracked continuously
along their trajectories. The effect of turbulent ﬂuctuations on the particle motion is
taken into account by Discrete Random Walk method, with the aerodynamic drag as
the only force acting on the particle. Mutual interaction between particles is neglected.
Particle sizes are determined from the sieve analysis of the coal dust, and their dis-
tribution deﬁned by the Rosin Rammler regression. The inﬂuence of particles on
the gas phase is accounted for by the particle-in-cell (PSIC) method.
The combustion of coal particles is divided into four concurrent and sequential
phases modelled separately: particle heating, devolatilization, volatile combustion and
char burnout. The temperature of the particle is determined by the energy conservation
equation, taking into account convective and radiation heat transfer, heat consumed
during the devolatilization and heat transferred due to char burnout. Devolatilization is
described by two competing rates model. Combustion of volatiles and char burnout are
modelled by irreversible chemical reactions with Arrhenius equation using the
empirical parameters for the determination of its kinetics.
Computational Study of Pulverized Coal Combustion in Boiler
1127

3
Computational Results and Discussion
3.1
Coal Dust Swirl Burner
Coal dust burners installed at the unit 7 boiler of TPP Kakanj is of the double swirl
type, meaning that the primary air, carrying coal dust particles, and secondary air are
passing through vanes imposing the swirls of the same direction, which are then mixed
in the diffuser type mouth of the burner. Toothed rim is installed at the end of the
primary air annular conduit, before entering the mixing zone, in order to impose
additional turbulisation to the ﬂow, such enabling better mixing of the coal dust with
the air.
Due to a very complex geometry of the burner, its computational modelling is
usually avoided in performing CFD of the combustion in pulverized coal ﬁred boilers,
imposing uniform or very simple velocity, temperature, turbulence and particle dis-
tribution proﬁles at burner inlets, such oversimplifying a very complex ﬂow at the
entrance to the combustion chamber. In order to avoid that controversial simpliﬁcation,
the CFD of the burner is performed in this work, such providing more realistic inlet
values to the combustion chamber. Details of the CAD model of the burner can be seen
in Fig. 1.
The burner has 3 air inlets, core air ( _mcore ¼ 0:2657 [kg/s], Tcore = 330 [°C]),
primary air carrying coal dust particles ( _mprim ¼ 3:8123 [kg/s], Tprim = 90 [°C]), and
secondary air ( _msec ¼ 3:8123 [kg/s], Tsec = 330 [°C]). Core air enters the burner
through the mid tube, while primary and secondary air pass through the ducts equipped
with vanes oriented in the same swirl direction. At the burner inlets, fully developed
turbulent ﬂow velocity and turbulent proﬁles are imposed.
Numerical grid consists of 321,000 hexahedral control volumes, which are clus-
tered towards the outlet. The dimensionless wall distance y+ was kept over 20 in the
majority of CV-s, allowing to apply the standard wall functions, though the enhanced
wall function option was selected to account for viscous effects in case that the ﬁrst
wall-adjacent y+ is less than 11.5. A cross-section through the numerical grid is shown
in Fig. 2.
secondary air
primary air
core air
Fig. 1. 3D CAD model of the burner, full computational domain, view on the core, primary and
secondary air inlets, and front view
1128
A. Ðugum and K. Hanjalić

The process, considered as steady, was computed by “false”-time-marching with a
time step as small as Δt = 110−5 [s] to ensure better computational stability and to
avoid non-physical peak velocities due to strong swirling and the effect of particles on
the continuous phase ﬂow ﬁeld. The QUICK differencing scheme is used for dis-
cretisation of the convective terms and the second-order central differencing for the
diffusion terms.
The coal particle size distribution (PSD) from the sieve analysis is given in Table 1.
The Rossin Rammler regression parameters corresponding to the sieve analysis, for ﬁve
different particle diameters with the adopted maximum and minimum values Dmax =
2  10−4 [m], Dmin = 1  10−5 [m], the mean the diameter DN = 8.04  10−5 [m]
and spread factor n = 0.932. The number of the particles tracked was 4,800 with 2 tries
within the Discrete Random Walk method, what corresponds to 9,600 coal dust
particles.
Coal particles velocities at the primary air inlet are set to the value of the mean
velocity, vp = 12.844 [m/s], temperature to Tp = 90 [°C] and the ﬂow rate of particles
to _m = 2.083 [kg/s]. Air properties, speciﬁc heat cp, thermal conductivity k and
dynamic viscosity l are set to be temperature dependant using polynomial interpola-
tion. Values of the polynomial coefﬁcient for cp are taken from [5], and values of the
coefﬁcients for k and l are calculated from the discrete data from [6]. Some results of
the burner calculation are shown in 2D middle cross section in Fig. 3.
Fig. 2. Numerical grid of the burner, with inlets, outlet and section at the entrance to the boiler
Table 1. Sieve analysis of the coal dust particles
Sieve screen size [lm]
1000 500
200
90
Passed
Cumulative residue [%] 0.00
0.43 9.32 33.24 57.01
Computational Study of Pulverized Coal Combustion in Boiler
1129

The results of the burner calculation at the cross section that corresponds to the inlet
to the computational domain of the boiler (cross section marked in Fig. 2) are shown in
Figs. 4, 5 and 6.
The results are in line with the conclusions from the experimental study of
the double swirl burner [4]. Namely, these authors report annular recirculation zone
between the two peaks of annular velocities, which can be seen in Fig. 4, and report
two annular peaks of volume particle concentrations, and very low particle ﬂow in the
central zone. Also, particle diameters close to the central zone were always less than in
the other zones, what can be seen in Fig. 6, where red dots represent particles of the
smallest diameter, and black of the biggest one.
3.2
Boiler
The boiler of the Unit 7 in the TPP Kakanj is designed for the nominal steam mass ﬂow
of 740 [t/h] what corresponds to 230 [MWe] at the turbine shaft. It is equipped with 24
Fig. 3. Velocity magnitude and coal particle concentration at the middle cross section
Fig. 4. Velocity magnitude and z-velocity component at the boiler inlet cross section
1130
A. Ðugum and K. Hanjalić

pulverized coal burners at front and back side, situated in 2 rows (at vertical positions
zb1 = 12.35 [m], and zb2 = 14.85 [m]). It also has 16 over ﬁre air (OFA) nozzles above
the combustion chamber, also at front and back side in the radiation zone (zOFA = 22.5
[m]), for staged combustion. Neighbouring burners mainly have opposite swirl direc-
tion (apart from one pair of burners which have the same swirl direction). Model of the
burner and surface detail of the numerical grid, as seen from inside of the combustion
chamber, are given in Fig. 7.
Numerical grid of the boiler consists of 2,733,184 hexahedral cells clustered toward
the walls. Dimensionless wall distance y+ of centre of the wall-nearest cells is kept over
20 (maximum around 600) to permit use wall functions.
The positions of burner exits correspond to the inlet into the boiler shown in Fig. 2.
Since the numerical grids at those corresponding sections are different, the values of
ﬂuid phase variables (velocity components, temperature, turbulent kinetic energy and
dissipation) in the computational nodes of the burner section are interpolated to the
nodes of the numerical grid of the boiler at the burner inlets using separate computer
Fig. 5. Air temperature and coal dust concentration at the boiler inlet cross section
Fig. 6. Coal dust particles distribution at the boiler inlet cross section
Computational Study of Pulverized Coal Combustion in Boiler
1131

code written in Matlab. The ﬂow rate is increased by the content of the moisture in the
coal, which is deduced from the particles mass.
Although various authors report modelling of discrete phase by tracking up to
10,000 particles [7], in this simulation 900 particles per burner are used, what amount
to 21,600 particles for the entire boiler. Positions, temperatures and velocities of par-
ticles represented by vector, used at the inlets to the boiler, are shown on the right of
Fig. 6. Their properties are obtained by imposing uniform particles distribution at the
inlet of the burner, releasing them with the frozen temperature and velocity ﬁeld and
tracking them through the burner, and the sample is taken from the corresponding
section. The sizes are the same as for the calculation of the burner. Boundary condition
for the discrete phase is set to reﬂection at inlets, in order to avoid possible escape of
particles through the small recirculation zone, and loss of the coal-dust mass.
For the P1 radiation model, the wall emissivity was 0.6, particle scattering factor
0.9 and particle emissivity 0.6 [1]. As for the boundary condition for the energy
equation, the constant temperature of Tcc = 1457 [K] was set at the wall in combustion
chamber, Trc = 1178 [K] in radiation zone, and Tsh = 873 [K] in other parts [2].
Three superheaters are situated at the top of the boiler and are modelled as porous
media with pressure loss coefﬁcient in y-direction Cy = 70 [m−1], and in x and z di-
rection Cx = Cz = 0.5 [m−1] as proposed in [8]. Heat absorption in heat exchangers are
set to ΔHSH3 = ΔHSH4 = −31 [kW/m3] and ΔHSH5 = −19.3 [kW/m3], [2].
The composition of coal used in this study is given in Table 2. Lower heating value
is recalculated for dump-free coal dust. The process of combustion is modelled with
three equations, combustion of pseudovolatile, whose molecule is derived from coal
proximate and ultimate analysis (Eq. 3.1), char combustion (Eq. 3.2) and combustion
of CO (Eq. 3.3). The content ratio of XCO/XCO2 in Eq. (3.1) is set to 3.428, as proposed
Fig. 7. Boiler model and detail of the numerical grid
1132
A. Ðugum and K. Hanjalić

in [9]. Kinetic parameters for devolatilization, volatile combustion, char and CO oxi-
dation are taken from [1] and listed in Table 3.
3:99 C 8:76 H 2:34 O 0:1485 N 0:1148 S þ 3:58 O2 !
3:09 CO þ 0:9 CO2 þ 4:38 H2O þ 0:0742 N2 þ 0:1148 SO2
ð3:1Þ
Cs þ 0:5 O2 ! CO
ð3:2Þ
CO þ 0:5 O2 ! CO2
ð3:3Þ
Flow of mixture of gases is modelled by solving conservation equations describing
convection, diffusion, and reaction sources for each component species. Speciﬁc heat
cp, thermal conductivity k and dynamic viscosity l of each of the species are set to be
temperature dependant using polynomial interpolation. Values of the polynomial
coefﬁcient for cp are taken from [5], and values of the coefﬁcients for k and l are
calculated from the discrete data from [6]. Some result of the boiler calculation are
given in Figs. 8, 9 and 10.
Horizontal cross sections in Figs. 8 and 9 correspond to the positions marked with
z1−5 in Fig. 10. It can be seen in Fig. 9 that there is no oxygen at the side walls in the
zone where high temperature corrosion is detected (between the exit from the com-
bustion chamber z2 = 17.37 [m] and OFA nozzles level z3 = 22.5 [m]), whereas it is
present at the front and back walls, while content of the volatiles on the side walls is
much higher than at front and back walls. The temperature at those walls is also higher
than at front and back walls, as well as the velocity of the ﬂuid phase, which is directed
towards positive z direction. That implies that a hot mixture of volatiles and ﬂue gases
from combustion chamber, where oxygen is exhausted due to sub-stochiometric
combustion, are ﬂowing over the walls creating oxygen-free atmosphere at very high
temperatures and presence of volatiles, what creates conditions for occurrence of the
high temperature corrosion.
A more precise quantiﬁcation of the calculation results is given in Fig. 10, where
the selected quantities are displayed at various cross sections along two lines in y di-
rection (red lines), positioned at one quarter and middle of the domain in x direction
Table 2. Coal composition
Proximate analysis [%]
Ultimate analysis [%]
Lower heating val.
Moisture Ash
Volatiles Fixed C C
H
O
N
S
Hd [kJ/kg]
12.04
36.98 29.9
21.08
35.42 2.64 11.2 0.62 1.1 13.198
Table 3. Kinetic parameters for devolatilization, gas-phase reactions and char oxidation
Devolatilization—two competing rates
Volatile combustion
Char oxidation
CO oxidation
Av1
Ev1
[J/kmol]
Av2
Ev2
[J/kmol]
Avol
Evol
[J/kmol]
AC
EC
[J/kmol]
ACO
ECO
[J/kmol]
2  105
1.046  108
1.3  107
1.674  108
2.56  1011
1.081  108
0.0053
8.37  107
8.83  1014
9.98  107
Computational Study of Pulverized Coal Combustion in Boiler
1133

Fig. 8. Boiler CFD calculation results—temperature and velocity magnitude
Fig. 9. Boiler CFD calculation results—oxygen and volatiles content
1134
A. Ðugum and K. Hanjalić

(marked as x1/4 and x1/2), and two lines in x direction (blue lines), positioned at one
quarter and middle of the domain in y direction (marked as y1/4 and y1/2). It is noted that
measurement of the oxygen content at side walls [3] showed a small content of oxygen
(0.02 to 0.03) in the vicinity of wall at position z = 17.3 [m] which then disappears
further from the wall (0.5 and 1 [m] distance), while at position z = 22 [m] mea-
surements showed zero oxygen content in all three equidistant measuring points, and
all four measurement depths (0 to 1 [m]).
The average measured content of CO2 at walls is between 0.15 and 0.163, while
calculation overestimates those values to between 0.20 and 0.22.
4
Conclusion
CFD simulations of the double swirl coal dust burner and boiler were presented in this
work, where the results of the ﬁrst simulation have been used as the inlet boundary
condition for the second one. Although experimental data for the burner do not exist,
the calculation results are consistent with the trends of the measured data observed in
the similar type of the burner. Results of the boiler simulation showed the lack of
oxygen in the regions next to the side membrane walls, what is consistent with the
measurements of oxygen in those regions, although the zone lacking oxygen is
probably overpredicted in its lower part. The selected model overpredicts carbon
dioxide content at walls for some 25% in comparison to the measured data.
Combining two simulations into one, although computationally more demanding,
appears to be a good route for prediction of complex phenomena related with com-
bustion of pulverized coal in industrial boilers. The expected adverse trends in the
boilers have been predicted by the selected models with signiﬁcant accuracy, although
Fig. 10. Boiler calculation results at various positions in boiler
Computational Study of Pulverized Coal Combustion in Boiler
1135

the appropriate data for the kinetic parameters of devolatilization and combustion for
the coal considered do not exist. Such analysis of Bosnian coals would surely lead to
better modelling of their combustion.
References
1. Ranade, V.V., Gupta, D.F.: Computational Modelling of Pulverized Coal Fired Boilers. CRC
Press (2015)
2. Steinmuller: Instandsetzung Kraftwerke, Installation of Wall Air System for Boiler 7/ Unit 5
od the Kakanj Power Plant, Documentation Rev. 0, 2011
3. Mašinski fakultet Sarajevo: Određivanje koeﬁcijenta viška zraka u isparivaču kotla bloka 7
(230 MWe) u TE Kakanj - Kakanj, Izvještaj s prve faze ispitivanja (2010)
4. Jing, J., Li, Z., Zhu, Q., Chen, Z., Wang, L., Chen, L.: Inﬂuence of the outer secondary air
vane angle on the gas/particle ﬂow characteristics near the double swirl ﬂow region. Energy
36, 258–267 (2011)
5. FLUENT 6.3 Users Guide, Fluent Inc. (2006)
6. Svehla, R.A.: Estimated viscosities and thermal conductivities of gases at high temperatures,
NASA Technical Report R-132 (1962)
7. Chernetskiy, M.Y., Dekterev, A.A., Burdukov, A.P., Hanjalić, K.: Computational modeling
of autothermal combustion of mechanically-activated micronized coal. Fuel 135, 443–458
(2014)
8. Bris, T., Cadavid, F., Caillat, S., Pietrzyk, S., Blondin, J., Baudoin, B.: Coal combustion
modelling of large power plant for NOx abatement. Fuel 86, 2213–2220 (2007)
9. Vascellari, M., Cau, G.: Sviluppo e validazione di Modellistica di processi di Gassiﬁcazione e
combustione con Ricircolo dei prodotti in diverse Tipologie di reattori, Report RSE/2009/157,
Ente per le Nuove tecnologie, l’Energia e l’Ambiente - ENEA (2009)
1136
A. Ðugum and K. Hanjalić

The Application of Advanced Technologies
in Industrial Practice
Edin Ibrahimovic(&)
3Din d.o.o, Tesanj, Bosnia and Herzegovina
3din@3din.biz.ba
Abstract. This paper representing application of the 3D technologies in engi-
neering practice and also some theoretical basics of these technologies. Refer-
ences in this paper are examples from engineering department of the engineering
service company “3Din d.o.o.” and partners from area of design, 3D scanning,
3D printing, and reverse engineering.
Keywords: 3D print  3D scanning  3D inspection reports  Reverse
engineering
1
Introduction
Process of development of new products in last 10 years is rapidly speedup. That is
enabled by new and enhanced old computer technologies in engineering practice. In
those technologies are CFD simulations which are in last year’s more user friendly and
thanks to new powerful computer conﬁgurations faster than before 10 years. There is
also rapid prototyping of the complex geometries. For complex geometries, process of
the prototyping was very long and expensive. Now thank to the 3D printing even most
complex geometry can be produced in very short period of time and with small costs.
One part of this paper will have for topic practical usage of 3D printing in engi-
neering service 3Din d.o.o. and similar engineering services in BIH. Beside develop-
ment of new products, very often, there is need for production of old products which
don’t have any technical documentation. If those products have complex geometry or if
they are complex moulded of forged parts, for production is necessary to have 3D
model for producing moulds or dies on modern CNC machines. That must be done
with reverse engineering. That can be done on several ways but in recent period of time
reverse engineering is done by technology of 3D scanning. In this paper will be
presented the 3D scanning technology application in engineering services of the
company 3Din doo. Also, small theoretical part about this technology.
2
Technology of 3D Printing
The origins of 3D printing can be traced back to 1986, when the ﬁrst patent was issued
for stereo lithography apparatus (SLA). This patent belonged to one Charles (Chuck)
Hull, who ﬁrst invented his SLA machine in 1983 [1]. Hull went on to cofound 3D
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_97

Systems Corporation—one of the largest and most proliﬁc organizations operating in
the 3D printing sector today.
3D Systems’ ﬁrst commercial RP system, the SLA-1, was introduced in 1987 and
following rigorous testing the ﬁrst of these system was sold in 1988. As is fairly typical
with new technology, while SLA can claim to be the ﬁrst past the starting post, it was
not the only RP technology in development at this time, for, in 1987, Carl Deckard,
who was working at the University of Texas, ﬁled a patent in the US for the Selective
Laser Sintering (SLS) RP process. This patent was issued in 1989 and SLS was later
licensed to DTM Inc, which was later acquired by 3D Systems. 1989 was also the year
that Scott Crump, a co-founder of Stratasys Inc. ﬁled a patent for Fused Deposition
Modeling (FDM)—the proprietary technology that is still held by the company today,
but is also the process used by many of the entry-level machines, based on the open
source RepRap model at 2004, that are proliﬁc today. The FDM patent was issued to
Stratasys in 1992 (Fig. 1).
Short overview of the history 3D printing:
1986: Stereo lithography taken up by Charles Hull
1987: First SLA-1 machine
1988: ﬁrst SLS machine by DTM Inc then buy by 3D system
1992: FDM patent to Stratasys
1995: Z Corporation obtained an exclusive license from the MIT
2004: RepRap open source FDM project.
Currently most used technologies of the 3D printing are FDM, SLS, DLP (Stere-
olithography). Before explanation of the different technologies we will explain process
of preparation of the 3D model for 3D printing. A so-called slicer takes a3D model
(most often in .STL format) and translates this model into individual layers. It then
generates the machine code that the printer will use for printing.
Fig. 1. 3D printing products
1138
E. Ibrahimovic

3D printers can be either controlled through a small on-board control screen or
through a (USB) interface with a computer or through both. User interface/control
software allows a user to send a machine code ﬁle from the computer to the 3D printer,
change some parameters on run time (e.g. speed, ﬂow and temperature), and move the
print head manually around the x/y/z axis.
Some programs, like the Netfabb engine, combine the functionality of a slicer and a
user interface/control software. In addition programs like Netfabb engine, can add STL
editing, repairing, merging and some simple 3D modeling.
On the market there is lot of freeware slicers software’s, but there are also software
which comes with some 3D printers and they are so called closed systems, because you
can make ﬁle for the printer only with that software like ZORTRAX.
Fused deposition modeling (FDM) [2] is an additive manufacturing technology
commonly used for modeling, prototyping, and production applications. It is one of the
techniques used for 3D printing. FDM works on an “additive” principle by laying down
material in layers; a plastic ﬁlament or metal wire is unwound from a coil and supplies
material to produce a part. Fused deposition modeling: 1—nozzle ejecting molten
material, 2—deposited material (modeled part), 3—controlled movable table. See
Fig. 2.
The research application of CFD modeling in order to determine the capacity of the
screen, determining the ﬂow rate based on hydraulic and geometrical parameters of this
screen types, represents a challenge and still insufﬁciently explored scientiﬁc area,
which has great importance in engineering practice. See Fig. 3.
Fig. 2. FDM (Fused deposition modeling)
The Application of Advanced Technologies in Industrial Practice
1139

Selective laser sintering (SLS) [3] is an additive manufacturing (AM) technique that
uses a laser as the power source to sinter powdered material (typically metal), aiming
the laser automatically at points in space deﬁned by a 3D model, binding the material
together to create a solid structure.
Schematic representation of the process: a moving head (a) selectively binds (by
dropping glue) the surface of a powder bed (e); a moving platform (f) progressively
lowers the bed and the solidiﬁed object (d) rests inside the unbinded powder. New
powder is continuously added to the bed from a powder reservoir (c) by means of a
leveling mechanism (b) (Fig. 4).
Fig. 3. SLS (Selective laser sintering)
Fig. 4. a Inkjet 3D printing; b DLP (Stereo lithography)
1140
E. Ibrahimovic

Schematic representation of Stereo lithography; a light-emitting device (a) (laser or
DLP) selectively illuminate the transparent bottom (c) of a tank (b) ﬁlled with a liquid
photo-polymerizing resin; the solidiﬁed resin (d) is progressively dragged up by a
lifting platform (e).
3
Technology of 3D Scanning
Today are in use two non-contact technologies for 3D scanning with light emission
(structural light and laser) and with photogrammetry. There are also CT technologies,
for inner structures of scanned parts, but this technology is too expensive for daily
usage in engineering practice (Fig. 5).
Photogrammetry is the science of making measurements from photographs, espe-
cially for recovering the exact positions of surface points. Photogrammetry is as old as
modern photography, dating to the mid-19th century and in the simplest example, the
distance between two points that lie on a plane parallel to the photographic image
plane, can be determined by measuring their distance on the image, if the scale (s) of
the image is known (Fig. 6).
Fig. 5. 3D scanning
The Application of Advanced Technologies in Industrial Practice
1141

Projecting a narrow band of light onto a three-dimensionally shaped surface pro-
duces a line of illumination that appears distorted from other perspectives than that of
the projector, and can be used for an exact geometric reconstruction of the surface
shape (light section) (Fig. 7).
A faster and more versatile method is the projection of patterns consisting of many
stripes at once, or of arbitrary fringes, as this allows for the acquisition of a multitude of
samples simultaneously. Seen from different viewpoints, the pattern appears geomet-
rically distorted due to the surface shape of the object.
Measurement of distance is done by the triangulation principle. Accuracy of those
systems are dependent of the different parameters like temperature, vibration, humidity,
surface reﬂectivity, and also from resolution of the cameras used in 3D scanners.
Fig. 6. Photogrammetry
Fig. 7. The projection of patterns with many stripes
1142
E. Ibrahimovic

3.1
Examples of the 3D Printing and Scanning Application
in the Industry of Bosnia and Herzegovina (BiH)
In this part will be introduced few examples of the usage 3D technologies in BIH.
Because of small BiH market, number of engineering services which are providing
services of 3D printing and scanning in industrial area is small. Author of this paper
started with 3D print at 2014 from pure curiosity, what after growth into serious
engineering work and with 3D scanning at 2015.
Because for 3D scanning in reverse engineering and metrology is necessary to
invest into equipment, that part of usage of 3D technology is under development.
In BIH there are two companies which have their own 3D scanners, and author is using
services of 3D scanning outside BIH.
Fig. 8. Example of reverse engineering of the casting parts
Fig. 9. Example 2: Reengineering of current product, company poly gracanica BIH
The Application of Advanced Technologies in Industrial Practice
1143

Example 1: Reverse engineering of the casting parts: For 3D scanning is used
GOM ATOS triple scan, and for reverse engineering CREO 3.0. After reverse engi-
neering model is adopted for casting with Solid Works. Accuracy of the scanned model
was ± 0.05 mm.
Example 2 & 3: Reengineering of current product, Company Poly Gracanica BIH:
presented on next (Figs. 8, 9, 10 and 11).
Fig. 10. Example 3: Reengineering of current product, company poly gracanica BIH
1144
E. Ibrahimovic

4
Conclusion
Even we have application of the 3D technologies in Bosnia and Herzegovina that is still
on low level of usage. One of the reasons is that there is no developments of new
products, and also bad estimation of costs. Because this is relatively new technology
and companies have wrong picture that is expensive technology. May be price for some
service is high, but it can give huge beneﬁts, e.g. the one single scan can save millions
trough process of quality reclamation.
Scanning of molds for injection molding can also save huge amount of many if
after scanning we ﬁnd that wear of mold is not too high and that we can extend life of
the mold. There is lot other examples. Problem in BIH is creating industrial environ-
ment which accept this not like costs, but like competitive advantage.
Also there is whish of the companies that wants to have in their ownership all
equipment even ROI is smaller than amortization time. BIH companies don’t have
strategy to use external services until they ﬁnd that they have big problem. Currently in
BIH there is place for one metrology center which will provide services to the
Fig. 11. Example 4: Design check: Company Artisan Tesanj, BiH
The Application of Advanced Technologies in Industrial Practice
1145

companies where utilization of that equipment will be relatively high. Price for
industrial scanners are from 30 to 150 TEUR.
Application of the 3D printing and 3D scanning on the Bosnia and Herzegovina
market is on low level, but there is potential for growth. Growth can be achieved
through customer requests for 3D inspection reports, or trough development of own
products in BIH companies.
References
1. http://www.3ders.org/3d-printing/3d-printing-history.html
2. https://en.wikipedia.org/wiki/Fused_deposition_modeling
3. https://en.wikipedia.org/wiki/Selective_laser_sintering
1146
E. Ibrahimovic

The Causes and Consequences of Deﬁcit
in Nominal Temperature of Reheated Steam
After Implementation of Primary Measures
for NOx Reduction on the Boiler OB-650
Nedim Ganibegović(&) and Amel Mešić
Tuzla, Bosnia and Herzegovina
nedim.ganibeg@gmail.com
Abstract. The most recent general reconstruction of the Block 6 of the Thermal
Power Plant Tuzla resulted in dual effect on energy efﬁciency of the Block itself.
This article deals with an analysis of modernization projects that has been done
on coal boiler and steam turbine and possible consequences on boiler perfor-
mances. The modern project solution, which has been implemented on the coal
combustion system, has enabled the application of low emission concept of
combustion and reduction of ash slugging on walls of combustion furnace. As
those were the main goals of reconstruction, they have been successfully
achieved. However, after reconstruction has been completed, the total heat load
distribution on the surface of the boiler reheater is not able to provide enough
heat to achieve the nominal temperature value of reheated steam at lower loads,
particulary in exploitation conditions case which deviate from the calculated and
guaranteed conditions. Accordingly, this article will provide a review of two
possibilities of temperature increasement of reheated steam that leaves the
boiler. That includes two situations:
a. Increase the heating surface on the boiler reheater to the possible limits,
b. The use of recirculated cold waste gas for regulation of the temperature of
reheated steam.
1
Introduction
According to the reports by the International Energy Agency (IEA) production and
consumption of the electricity is growing rapidly, and all trends show that fossil fuels,
especially coal, will remain the main source of energy for a long time [1]. The concept
of energy efﬁciency, conservation of natural resources and environmental friendly work
encourage the use of various technologies for reduction of the emission and resource
consumption [2]. Reconstruction works of the thermo-block 6 carried out in 2012,
resulted with a boiler, which has a different combustion concept and a new
high-pressure turbine with a larger number of working stages. Combustion process was
changed due to the fact that modern design solution gives better results in lower
emission of pollutants (NOx) and they also have better control of ash slugging. New
combustion concept is based on new design of burners (Fig. 1), which includes
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_98

shortening of their height and their vertical displacement to the bottom, with larger
velocities of the pulverized coal and secondary air that comes out from the burners
nozzles. Of course, that new implemented construction also implicates reduction of the
combustion temperature in burners area.
Although the realized design solution enables the successful application of the
combustion concept with low emission and low intensity of ash slugging on the furnace
walls, it was also expected that the modern solution implemented would also have the
following advantages, such as:
• Reduction the amount of injected water for cooling of heater and reheater
• Upkeeping the design efﬁciency of the boiler  89%
• Upkeeping the design productivity of the coal mills
• Upkeeping stability of combustion process of the pulverized coal in limits 60–100%
of productivity of the boiler
• Upkeeping the nominal temperature of the primary and secondary steam in limits
75–100% of productivity of the boiler.
After reconstruction and optimization works of the thermo block 6 of the Thermal
Power Plant Tuzla, it is recorded that nominal temperature of reheated steam on the
Fig. 1. Burners design
1148
N. Ganibegović and A. Mešić

outlet of boiler can be reached only for boiler loads bigger than 90%, while on lower
boiler loads (<75%), this value is lower than 520 °C. More detailed analysis showed
that required increase of efﬁciency of the high pressure turbine would cause temper-
ature drop on the outlet of turbine in relation to the original designed state. Previous
conclusion is a completely realistic due to the fact that new high pressure turbine has
larger number of working stages which now use higher pressure drop.
2
Measurement Results
Guarantied measurements conducted in July 2013, after previous regulation and
optimization of the boiler, has shown that guarantied parameters were satisﬁed,
excluding the temperature of the reheated steam for boiler capacity below 550 Mg/h,
which is also shown in the Table 1 and diagram below (Fig. 2).
Diagram on Fig. 2 shows functional dependence of the boiler capacity from tem-
perature of reheated steam and as it is shown that temperature of reheated steam is
lower for capacity below 630 Mg/h, which effects on the efﬁciency of the whole block.
Table 1. Calculation of temperature of the reheated steam with corrections [3]
Description
Unit
Regime I
Regime II
Regime III
For guarantied measurements of boiler
Measured boiler capacity
Mg/h 641.0
544.0
489.6
Boiler capacity with correction +2.6%
Mg/h 657.6
558.1
502.3
Project temperature on the inlet of reheater
for measured boiler capacity with
correction
°C
320.0/320
318.5/319
314.0/315.5
Measured temperature on the inlet of the
reheater
°C
317.3
304.5
291.4
Difference
°C
2.7/2.7
14.0/14.5
22.6/24.1
Correction Dtulaz
°C
1.9/1.9
7.6/7.8
11.0/11.4
Measured temperature on the outlet of the
reheater
°C
539.5
524.72
518.5
Temperature on the outlet with correction
Dtulaz
°C
541.5/541.4 532.32/532.52 529.5/529.9
Tolerance of temperature measuring Dttol
°C
3
2.96
2.95
Temperature with tolerance Dtulaz + Dttol
°C
544.4
535.48
532.85
Guarantied temperature
°C
540 ± 5
540 ± 5
540 ± 5
Tolerance of temperature measuring Dttol = 0.3 + 0.005t
The Causes and Consequences of Deﬁcit
1149

3
Analysis of Facts that Cause Deﬁcit in Nominal
Temperature of Reheated Steam
After analysis of thermo-block work and optimization set up of work parameters, it was
concluded that there are several facts that cause deﬁcit in nominal temperature of
reheated steam. The main facts are:
a. Lower ash slugging on walls of furnace;
b. Large suction of false air in mills region;
c. Increase degree of efﬁciency of the high pressure turbine;
d. Reduction of surface of the secondary reheater 1A.
It is important to emphasize that each fact, mentioned above, causes temperature
deﬁcit on its own speciﬁc way. For example, new implemented coal combustion
system has enabled lower ash slugging on walls of furnace, which is very useful for
boiler efﬁciency, because in that case, we have lower averaged degree of deposits on
furnace walls, especially in upper part. In the other hand, degree of deposits will be
lower than the one in control thermic calculation, which will cause higher heat transfer
in furnace area and higher heat transfer will cause deﬁcit in temperature of ﬂue gases on
the outlet. Of course, lower temperature of ﬂue gases on the outlet of furnace will
automatically cause deﬁcit in nominal temperature of reheated steam. As it can be seen
from previous facts, the same new implemented combustion solution has dual effect on
the overall efﬁciency of the system.
Second main reason that causes temperature deﬁcit in nominal temperature of
reheated steam is large suction of false air in mills region. This suction results in an
increase of temperature of the ﬂue gases in main combustion area. That air suction
disables proper spreading ofﬁring area, which is crucial for achievement of low emission
concept, and achievement of designed distribution of total heat load on boiler heating
surfaces. This problem is particularly evident in cases that involve lower boiler loads.
As it has already mentioned, third main fact that cause deﬁcit in temperature of
reheated steam is increase of efﬁciency of the high pressure turbine with increasing the
number of working stages. Higher number of working stages increase the inner
510
520
530
540
550
500
520
540
560
580
600
620
640
660
Temperature of reheated steam [°C]
Capacity of the boiler [t/h]
Guaranteed temperature
Measured temperature
With the correcƟon of the inlet temperature
Fig. 2. Functional dependence of the boiler capacity from temperature of the reheated steam [3]
1150
N. Ganibegović and A. Mešić

efﬁciency for 2–4%, which is suitably for turbine efﬁciency. Larger number of stages
also means that the turbine usees larger pressure drop now, which automatically
includes lower temperature of secondary steam. It is determinate that in whole area of
guarantied load of boiler (100–75%), measured temperature of secondary steam is
lower for 2–15 °C in comparison to the designed and guarantied working conditions of
boiler, which are taken over from the original boiler design. Of course, as it is shown in
case before, differences between designed and measured temperature values are more
obvious with lowering of boiler load.
Fourth fact that cause deﬁcit in temperature of reheated steam is reduction of
surface of the secondary reheater 1A (Fig. 3). It was considered that lower injection of
water for steam intercooling would enable reduction of surface of the secondary
reheater 1A. Due to the previous constatation, its surface is reduce for 12%, or 1470 m2
compared to the base state of boiler. As well, surface reduction of the secondary
reheater 1A should also provide better velocity distribution of ﬂue gases at the tran-
sition area between ﬁrst and second boiler part, and reduce abrasive effects on piping
system.
4
Analysis of the Possibilities of Increasing the Temperature
of the Reheated Steam and Their Consequences
In previous section, main facts that cause temperature deﬁcit were considered. In this
section, we will considere possibilities for increasing the temperature of reheated
steam. That implies these two fallowing cases:
Fig. 3. Position of some boiler heat exchange surfaces [4]
The Causes and Consequences of Deﬁcit
1151

a. Upgrade of the reheater 1A to the boundary which is limited with useful space in
boiler, therefore the upgrade will be done from 4860 to 7240 m2.
b. Increased use of recirculation of the cold ﬂue gases for regulation of the temperature
of the reheated steam.
Both these possibilities have increasement of temperature of the reheated steam as a
result, but also they have some weakness that should be considered. In the Table 2, the
most inﬂuential parameters on speciﬁc heat consumption (such as temperature of
reheated steam, amount of injected water for cooling of the reheated steam, the cal-
culation of increase of the reheated steam ﬂow resistance, and decrease in the boiler
thermal efﬁciency) are analysed due to the increase of the recycled ﬂue gases ﬂow.
Calculated results are presented for the construction with upgraded reheater 1A, and for
construction with recycling of the cold ﬂue gases. Symbol “+” indicate growth of the
speciﬁc heat consumption, while symbol “−” indicates its reduction.
Upgrade of the reheater would cause the increase of maneuvering control of the
reheating from 94–100% to 80–100%, but it would also cause the increase of amount
of the injected water for intercooling (depending on the quality of the fuel—from 5 to
20 Mg/h), which would cause increase of the steam ﬂow resistance.
Upgrade of additional installations for recirculation of the cold ﬂue gases would
increase the maneuvering control of the reheating from 94–100% to 87–100%. But, this
solution would cause the increase of energy consumption and would also reduce the
thermal efﬁciency of the boiler for 0.4–0.5 percentage point. Results of this calculation
are also presented in the Fig. 4, in functional dependence of block load.
As it is shown in the Fig. 4, upgrade of reheater 1A cause the increase of speciﬁc
heat consumption of block on loads bigger than 187 MW, while on lower loads this
upgrade shows some beneﬁts. Increased recirculation of cold ﬂue gases causes increase
-60%
-40%
-20%
0%
20%
40%
60%
80%
130
150
170
190
210
Change in heat consumpƟon of 
the block[%]
Boiler power[MW]
Upgrade of 1A
Increased use of recirculated cold ﬂue gases
Fig. 4. Change in heat consumption of the block due to the upgrade of reheater 1A, and due to
the increased use of recirculated cold ﬂue gases [3]
1152
N. Ganibegović and A. Mešić

Table 2. The most inﬂuential parameters on speciﬁc heat consumption [3]
Boiler state
Boiler with upgraded reheater 1A
Boiler with larger recirculation of
ﬂue gases
Boiler capacity
Mg/h
650
550
450
400
650
550
450
400
The percentage of load
%
100
84.6
69.2
61.5
100
74.6
69.2
61.5
Temperature of primary steam
°C
540
540
534
530
540
540
529
Temperature of reheated steam
°C
540
540
525.5
520
540
528
517
511
Growth of temperature of the reheated steam
°C
0
9
21.5
23
0
7
13
14
Amount of injected water in the reheated steam
Mg/h
18
3
0
0
6
0
0
0
Growth of injected water in the reheated steam
Mg/h
15
3
0
0
3
0
0
0
Pressure drop in the reheater
bar
0.07
0.13
0.19
0.22
0.02
0.06
0.1
0.12
Boiler efﬁciency drop
%
–
–
–
–
0.4
0.45
0.5
0.5
Change of the heat consumption—increase the
temperature of the reheated steam
%
0.00
−0.25
−0.59
−0.63
0.00
−0.19
−0.35
−0.38
Change of the heat consumption—increase of injected
water in the reheated steam
%
0.57
0.114
0
0
0.114
0
0
0
Change of the heat consumption—pressure drop in the
reheater
%
0.028
0.052
0.076
0.088
0.008
0.024
0.04
0.048
Change of the heat consumption—boiler efﬁciency drop
%
–
–
–
–
0.4
0.45
0.5
0.5
Summarized change of heat consumption
%
0.598
−0.42
−0.510
−0.539
0.522
0.283
0.229
0.185
The Causes and Consequences of Deﬁcit
1153

of heat consumption for all loads, primarily due to the reduction of the thermal efﬁ-
ciency of the boiler, which makes this solution useless.
Previous solution, as it can be seen in Table 2, would cause increase of coal
consumption from 0.15 Mg/h (for lower loads) to 0.74 Mg/h (for bigger boiler loads).
It is obvious that this form of modernization leads to losses, instead of bringing
expected beneﬁts. On the other side, increase of surface of the reheater 1A causes lower
coal consumption for lower loads, for about 0.5 Mg/h, and increase of coal con-
sumption for bigger loads, above 83%, for about 0.85 Mg/h.
In order to determinate ﬁnal economic effect of proposed reconstructions that
includes upgrade of the reheater 1A, it is assumed that annual work of the block is
7800 h, and it is considered that we have several different loads:
a. Working power 215 MW-5200 h, working power 150 MW-2600 h (averaged
annual
power
193.3 MW)—result:
coal
consumption
growth
for
2860 Mg/annually;
b. Working power 220 MW-3900 h, working power 140 MW-3900 h (averaged
annual power 180 MW)—result: coal consumption growth for 1870 Mg/annually;
c. Working power 220, 200, 180, 160, 140 for 140 MW (averaged annual power
180 MW)—result: coal consumption growth for 310 Mg/annually;
d. Working power 200 MW-3650 h, working power 140 MW-4150 h (averaged
annual power 168 MW)—result: the amount of coal consumption does not change;
e. Working power 187 MW (averaged 164.5 MW)—3700 h of working over
187 MW (averaged 205 MW) and 4100 h (averaged annual power 185 MW)—
result: the amount of coal consumption does not change;
Based on previous analysis, we can conclude that upgrade of reheater of 1A would
produce economic income, only if the block would worked with power under 187 MW
more than half of the annual working period, so this upgrade is unproﬁtable.
5
Conclusion
Deﬁcit in temperature of secondary steam after high pressure turbine, when block
works on lower loads, is the main fact that prevents reaching the nominal temperature
of the reheated steam on the outlet of boiler. Analysis of consequences, for both
possibilities of increasing the temperature of reheated steam, has shown that for both
solutions, in conditions of optimal work of boiler, is produced bigger annual coal
consumption in comparison to the current state. Due to the fact that all goals haven’t
been achieved with previous reconstruction works, the main question, that has been on
topic, is how to improve boiler performances with current state. Main measures that
imply lowering of temperature deﬁcit of the reheated steam can be differentiated in two
groups that include technical end exploitation measures.
Change in exploitation conditions of the boiler can produce increase of temperature
of reheated steam, and the main conditions are:
1154
N. Ganibegović and A. Mešić

• increase of degree of deposit in furnace area, which will cause lower heat transfer,
• use of coal with lower quality,
• reduction of efﬁciency of the high pressure turbine can also cause increase of
temperature of the reheated steam.
On the other hand, the main technical measures for increase of temperature of
reheated steam are:
• better sealing of boiler, so we can reduce penetration of false air in boiler area,
• on the lower boiler loads, we can increase share of recirculated cold ﬂue gas,
• increase of surface area of the reheater 1A.
As it can be seen from this paper, this type of reconstruction works requires
extremely complex design methodology. It is recommended that, before any recon-
structive works, it is necessary to use all alternative design methods such as numerical
and experimental modeling, which will provide adequate detailed data about the
behavior of the power plant [5].
References
1. U.S. Energy Information Administration, International Energy Outlook 2016 with Projections
to 2040, May 2016
2. Crnomarković, N.Đ.: Prilog modeliranju prostorne distribucije zračenja u ložištu kotla za
sagorijevanje ugljenog praha. Ph.D, Beograd (2012)
3. Energotechnika-Energorozruch S.A.: Primjedbe i zaključci nakon više nego godišnje
eksploatacije kotla Bloka 6 u TE Tuzla, Internal report, Gliwice, June 2014
4. Technical power plant documentation, Power Plant Tuzla, Block 6 (2013)
5. Energotechnika-Energorozruch S.A.: Detaljne preporuke za upravljanje (eksploataciju kotla)
kotlom bloka 6–223 MW u TE Tuzla (nakon rekonstrukcije I optimizacije Sistema
sogorijevanja), Gliwice, Aug 2013
The Causes and Consequences of Deﬁcit
1155

Transition of a Conventional Power Utility
to Achieve 2050 RES and Carbon Cut
Targets—EPBIH Case Study
A. Kazagić, M. Musić, E. Redžić(&), and A. Merzić
Elektroprivreda BiH d.d.—Sarajevo, Power Utility, Vilsonovo Setaliste 15,
BiH—71000 Sarajevo, Bosnia and Herzegovina
a.kazagic@epbih.ba
Abstract. Conventional power utilities are faced today with challenges in
terms of sustainable development and decarbonisation. Different measures are
used to achieve the set targets, i.e. increase of RES, increase of energy efﬁ-
ciency and CO2 cut. In this work, a method for achieving CO2 cut targets has
been demonstrated on a real power system, i.e. EPBiH power utility, operating
in Bosnia and Herzegovina. EPBiH power utility is based today on a mix of
coal and hydro, while there is a great potential for RES projects; hydro, wind
and biomass. So it is possible to sustainably exploit the available capacity, to
drastically reduce the environmental impact of the power sector, for example
by use of biomass and expansion of cogeneration in existing coal-based power
plants. Also, for new thermal power units to be commissioned and being
necessary for consumption coverage once when generation from RES is low,
the environmental requirements are fulﬁlled. Those units are planned to be
CHP which would additionally contribute to the decrease of overall emissions
on district heating coverage area. Also, the efﬁcient use of fuel will be max-
imized in accordance with the best available techniques, while biomass
co-ﬁring will signiﬁcantly contribute to the set goals. The model used in this
work took into account the security of supply, competitiveness and sustain-
ability, considering environmental, economic and social indicators. Three
scenarios with different CO2 cut rates by 2050 have been considered, while a
range of measures have been applied to achieve the set decarbonisation targets.
All scenarios involve signiﬁcant increase of RES, from current 18 to 43% in
LOW CO2 cut, through 62% in MID CO2 cut, up to 76% in HIGH CO2 cut
scenario. As compared to 1990 level, CO2 emissions in those scenarios are
decreased for 55, 65 and 80%, respectively. In all cases, attention was paid to
maintaining the necessary level of stability of the power system and security of
supply. SCA and MSA have been performed for sustainability assessment.
SCA results show that HIGH CO2 cut scenario has advantage compared to
MID CO2 cut and LOW CO2 cut scenarios, when environmental and economic
OPEX indicators are considered. However, when only investment is consid-
ered, LOW CO2 cut scenario is preferable. Although electricity demand is met
in all scenarios, it was found that an increase in investment in generation
facilities that will contribute reducing the costs of operation and maintenance,
as well as reducing CO2 emissions, in the long run will be multiply rewarded,
when comparing LOW CO2 cut with HIGH CO2 cut scenario. It was conﬁrmed
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_99

by MSA analysis, which showed that no matter which relation between
weighting factors is considered, HIGH CO2 cut scenario is preferable.
Keywords: CO2 emissions  CO2 cut targets  Renewable energy sources
Power utility  Power generation  Sustainability
1
Introduction
Power generation system mainly based on coal is not compatible with international
climate targets, [1]. Problems which power sector based on coal faces today are con-
ducted with high CO2 emissions comparing to lower CO2-intensive energy resources,
particularly various renewable energy sources (RES). Furthermore, criteria of power
generation cost efﬁciency give more advantages to the RES options (wind, solar,
hydro) as compared to fossils. Environmental issue and consequent stronger require-
ments posed to fossil-fueled power plants in relation to reduction of SO2, NOx and
dust emissions, according to Large Combustion Plant Directive (LCPS) and Industrial
Emissions Directive (IED), along with increasing price of CO2 taxes, signiﬁcantly
contribute to the above mentioned negative trend of cost effectiveness of conventional
fossil-fueled power plants. On the other hand, safe, reliable and sustainable energy
supply is becoming one of the greatest challenges for the World, [2, 3]. Key decisions
have to be taken to drastically reduce carbon dioxide (CO2) emissions to meet energy
objectives for 2020 and ﬁght climate change. However, Europe’s energy systems are
adapting too slowly. Currently, only 45% of European electricity generation is based
on low-carbon energy sources, mainly nuclear and hydropower. Parts of the European
Union (EU) could lose more than a third of their generation capacity by 2020 because
of the limited life time of these installations. This means replacing and expanding
existing capacities, ﬁnding secure non-fossil fuel alternatives, adapting power systems
to RES and achieving a truly integrated internal energy market, [2]. EU and other
industrialized countries are aiming to achieve 80–95% cuts in GHG emissions by 2050,
as stated in Energy roadmap 2050, [4]. For the power sector, a CO2 reduction range of
between 54 and 80% was proposed by 2030 compared to 1990 levels. It was analyzed
in details in Power Perspectives 2030, [5], to response what is required between today
and 2030 to remain on a pathway to a decarbonized power sector by 2050. For power
industry, efforts are needed to increase the uptake of RES, high-efﬁciency cogeneration,
district heating and cooling, [6, 7].
Although signiﬁcant efforts have been undergone and are still ongoing to provide
perspectives for Clean Combustion Technologies (CCT), [8], the power system of EU
indisputably goes towards an energy transition, driven by the relevant EU legislation
launched and forced by the need to reduce carbon emissions and to mitigate climate
change [9]. From the other side, the growing number and amount of renewables in the
supply mix create transmission imbalances that need to be managed, [10]. China has
shown growing interest in market-based CO2 pricing and is expected to establish a
nationwide emission trading system by 2016, [11]. Thus, coal will even reach its
maximum use in the next few years. Many research works in recent period include
different model-based analyses on sustainable decarbonisation and increased share of
Transition of a Conventional Power Utility to Achieve 2050
1157

RES in the energy sector. Furthermore, the security of supply is stressed out in many
papers, e.g. [6, 7]. In [12], a range of energy services and criteria for assessing
developing countries’ energy service needs and priorities were deﬁned. RES integration
has been emphasized to be one of the main paths towards a sustainable energy future,
[13, 14]. In [15] energy planning tools have been applied for a speciﬁc case study in
order to identify optimal investment plans in new renewable and fossil generation
capacity with the goal of achieving signiﬁcant CO2 emissions reduction. Also, in [16]
the development of future smart electricity systems has been pointed out and applied to
Tokyo area. Multicriteria sustainability assessment (MSA) has been applied in [17, 18]
to support the power plant selection between considered options as function of the
highest sustainability index. Multicriteria approach was also used for analyzing RES
options in Tassos region, Greece, reported in [19]. A power utility generation portfolio
optimization model in terms of its sustainability as function of speciﬁc targets on RES
share in 2030, including comparative analyses between single criteria analysis
(SCA) and MSA, has been recently proposed by authors in [20]. Other available works
relate to developed countries and/or regions, e.g. EU region [21, 22, 23], or the Korean
electricity sector [24]. In this work, an innovative optimization methodology for sus-
tainable decarbonisation of a real power system, considering set decarbonisation targets
for 2050, has been demonstrated on JP Elektroprivreda BiH d.d.—Sarajevo (EPBiH), a
typical conventional coal-based power utility operating in SEE. Countries characterized
by high share of coal based electricity generation and consequent high CO2 emissions,
as the case demonstrated here, are faced with new challenges. In that context, the
transition model towards a sustainable decarbonized power system in 2050 for such
power utilities is proposed here.
2
Methodology Used
Method of power generation portfolio optimization, in terms of CO2 cut targets for the
year 2050, has been proposed in this work. Security of supply is considered, as well as
sustainability of the applied CO2 reduction measures, including environmental, eco-
nomic and social aspects. Optimization method has been demonstrated on a real power
system, EPBiH, a typical conventional coal-based power utility, operating in South
East Europe. Three basic scenarios of the generation portfolio development have been
analyzed to achieve CO2 cut targets in 2050. LOW scenario means 55% CO2 cut
compared to reference 1990, MID scenario relates to 65% CO2 cut, while HIGH
scenario relies on 80% CO2 cut. Scenarios include different RES share increase from
the current level of 18% up to 76% in HIGH CO2 CUT scenario in 2050, in combi-
nation with different CO2 reduction measures in TPP. Those measures include
replacement of existing power units by new, high efﬁcient and ﬂexible clean coal
power plants, replacement of a certain amount of coal by biomass, expansion of use of
CHP and ﬁnally, within one of the options considered, application of CCS technology.
In the work, sustainability assessment is applied, considering environmental and
economic resources as well as social criteria, to investigate effects to the sustainability
within different CO2 cut options deﬁned. MSA and SCA have been applied.
1158
A. Kazagić et al.

3
Power System Under Consideration
Power utility EPBiH is a typical power utility in South-East Europe. Annual electricity
generation is approximately 8,000 GWh and it comes from two coal-ﬁred TPP, i.e. TPP
Tuzla (1  110 MW + 2  200 MW + 1  225 MW) and TPP Kakanj (2  118
MW + 1  235 MW), three large hydro power plants (HPP), i.e. HPP Neretva
(6  30 MW + 2  57 MW + 3  70 MW), with a small participation from small
HPP (sHPP), approximately 1%. Both TPP use domestic low-rank coal, and consume
approximately 6,500,000 tons of coal per year. The current generation capacity structure
of 70:30% in favor of TPP provides some advantages like safe and reliable supply, but
further penetration of RES into the generation portfolio is a commitment in order to
contribute to the long-term sustainable development plans of the company and to
comply with the European targets for reduction of GHG emissions as well as pollutant
emissions. EPBiH supplies electricity to near 750,000 consumers in B&H, via its dis-
tribution network operated by the EPBiH´s distribution company, organized in ﬁve
regional distributive parts. Annual production of heat energy, generated in cogeneration
power units of TPP Tuzla and TPP Kakanj, is approximately 400 GWh, [20].
Total annual emission of CO2 in year 1990 was 9.500.000 t. Today the situation is
more favorable, given that the six blocks with the lowest efﬁciency are decommis-
sioned and all other existing coal-based power units have been modernized in period
between 2002 and 2013. Consequently, energy efﬁciency of EPBiH’s TPPs was
increased for 30% compared to the 1990 level; from 24% up to the current averaged
31%. Annual CO2 emission is at 6,500,000 t, on average for the last seven years, which
is 31.5% decrease compared to the reference 1990 level. The share of RES in total
power generation of EPBiH is at 20%, with close variations depending on annual
hydrological conditions.
Considering the planned consumption growth as well as exhausted life time and
low efﬁciency of TPP units, new generation facilities are planned to be built. Com-
missioning of new unit in TPP Tuzla (TPPTU7—450 MW) is planned for 2021, and in
TPP Kakanj (TPPKU8—300 MW) in 2023. The power demand ﬁgure also includes
cogeneration expansion for heating purposes, both in TPP Kakanj and in TPP Tuzla.
For further reduction of CO2 emissions, co-ﬁring coal with biomass is planned at all
existing EPBiH´s TPP; projected to use of 7% w of biomass for average operation of
3.000 h per year, [20]. Also, for new TPP units, higher amount of biomass is planned
to be co-ﬁred, up to 20% th of total fuel. Planned RES projects of EP BiH up to 2030
comprise new 1051 MW installed capacity in hydropower plants, 560 MW in wind
power plants (WPP) and new 20 MW in photovoltaic power plants (PVPP). Despite
signiﬁcant contribution to the CO2 emissions cut, additional projects have to be real-
ized in order to achieve CO2 cut targets by 2050.
4
Options Under Consideration
Three different scenarios have been analyzed, with different portfolio structures, in
order to deﬁne measures to be taken to achieve deﬁned CO2 cuts:
Transition of a Conventional Power Utility to Achieve 2050
1159

(a) 55% CO2 cut compared to 1990 level—low CO2 cut scenario (LOW CO2 CUT)
(b) 65% CO2 cut compared to 1990 level—medium CO2 cut scenario (MID CO2
CUT)
(c) 80% CO2 cut compared to 1990 level—high CO2 cut scenario (HIGH CO2 CUT).
In order to meet demand while maintaining exports level, deﬁned CO2 cut targets
by 2050 can be achieved by introducing new RES capacities, replacement of old TPP
units by new ones with higher efﬁciency, expansion of district heating systems,
replacing the certain amount of coal with biomass (share depending on analyzed
scenario) and application of CCS system when to be commercialized.
For all scenarios under consideration, decommission year for all TPP units is the
same, where all old remaining units will be decommissioned by 2031. Also, in all
scenarios commissioning of Unit 7 in TPP Tuzla (TU7—450 MW) is in 2021 and Unit
8 in TPP Kakanj (KU8) in 2023. Available potential and technical possibilities were the
base for assessment of amount of installed capacity in renewables by 2050. The
measures taken in each scenario are listed below:
LOW CO2 CUT assumes:
• Commission of Unit 7 in TPP Tuzla (TU7—450 MW) in 2021 and Unit 8 in TPP
Kakanj (KU8) in 20232
• Commission of new Unit 8 TPP in TPP Tuzla (TU8—450 MW) and Unit 9 in TPP
Kakanj (KU9) in 2031
• Co-ﬁring of biomass with coal on all operating TPPs after 2020, (share of biomass
approx. 4% of the total amount of coal annually)
• New RES capacity: 950 MW in HPPs, 150 MW in sHPPs, 550 MW in WPPs,
90 MW in solar power plants (SPPs).
MID CO2 CUT assumes:
• Commission of Unit 7 in TPP Tuzla (TU7—450 MW) in 2021 and Unit 8 in TPP
Kakanj (KU8) in 2023
• Commission of new Unit 8 TPP in TPP Tuzla (TU8—450 MW)
• On all operating TPPs after 2020, co-ﬁring of biomass with coal (share of biomass
up to 20% of the total amount of coal annually)
• New RES capacity: 1200 MW in HPPs, 190 MW in sHPPs, 800 MW in WPPs,
150 MW in SPPs.
HIGH CO2 CUT assumes:
• Commission of Unit 7 in TPP Tuzla (TU7—450 MW) in 2021 and Unit 8 in TPP
Kakanj (KU8) in 2023
• On all operating TPPs after 2020, co-ﬁr of biomass with coal (share of biomass up
to 20% of the total amount of coal annually)
• New RES capacity: 1700 MW in HPPs, 250 MW in sHPPs, 950 MW in WPPs,
300 MW in SPPs.
HIGH CO2 CUT scenario assumes CO2 emission reduction up to 80%, by
increasing the share of electricity produced from RES. This scenario is the most
challenging in terms of security of supply because of relatively high penetration of
1160
A. Kazagić et al.

wind and solar energy. However, considering very high level of penetration of HPP and
planned pumped-storage HPPs, along with expected commercialization of batteries for
electricity storage, the required level of stability and security of power system can be
maintained throughout the period under consideration. The effect of this portfolio
structure on the electricity price as well as positive ﬁnancial effects of CO2 reduction
are in detail analyzed within MSA.
5
Results Analysis
5.1
LOW CO2 Cut Scenario Results
Current state and the results for the years 2030 and 2050 are presented in Table 1.
For this scenario, the emission level is relatively low, and the CO2 cut compared to
1990 is signiﬁcant due to applied CCS technology for new TPP to be commissioned
after 2030 (Figs. 1 and 2).
Table 1. Results for LOW CO2 CUT scenario
Year
2014
2035
2050
Total electricity generation [GWh]
7 175
11 366
14 801
TPP electricity generation [GWh]
5 787
8 311
8 780
HPP electricity generation [GWh]
1 617
2 659
4 531
sHPP electricity generation [GWh] 74
313
614
WPP electricity generation [GWh]
0
674
1 124
Solar electricity generation [GWh]
0
2
130
Biomass electricity gen [GWh]
0
266
266
Share of RES [%]
22.6
32
43
CO2 emission [t]
6 631 954 3 919 079 4 276 346
CO2 emission cuta [%]
30.2
58
55
aCompared to value in 1990.
0
5000
10000
15000
2006 
2009
2012 
2015 
2018
2021 
2024
2027
2030 
2033
2036 
2039 
2042
2045 
2048
Electricity generaƟon 
[GWh]
Fig. 1. Electricity generation in EP BiH: realization in period 2006–2014, projection for 2015–
2050
Transition of a Conventional Power Utility to Achieve 2050
1161

In this scenario, in 2030 CO2 emission level is greater than today, which is the
consequence of operating of four TPP units which are decommissioned in year 2031. In
that year, CO2 emissions sharply drop for 45%, having a slightly raising trajectory
afterwards due to net efﬁciency decrease through the considered period, despite rela-
tively constant generation from TPP (Fig. 3). It was assumed that biomass would be
co-ﬁred with coal in amount of 7% w running 3000 h on annual basis on all TPPs. The
share of RES is continuously raising up to 43% in 2050. Avoided CO2 emissions seem
to decrease, but it is a relative indicator and a consequence of the annual reduction of
CO2 emissions.
5.2
MID CO2 Cut Scenario Results
In this scenario, despite a still relatively high installed capacity in TPP, to meet higher
utilization of RES and their commitment, CO2 emissions cut is 65% in 2050, see
Table 2.
In 2031, when all old units stopped operation, emissions have fallen signiﬁcantly
from 7.6 million tons to 3.75 million tons. Higher utilization rate of biomass in this
scenario (up to 20% on all units after 2020), contributes to a CO2 emissions reduction
in amount of 850 thousand tons. Despite the same utilization rate of TPP units and their
0.0
20.0
40.0
60.0
80.0
0.0
2000.0
4000.0
6000.0
8000.0
10000.0
LOW CO2 CUT
MID CO2 CUT
HIGH CO2 CUT
% RES
GWh
TPP
HPP
sHPP
WPP
SPP
Biomass
RES share
Fig. 2. Electricity generation by sources and RES share for considered scenarios
-4000000
-2000000
0
2000000
4000000
6000000
8000000
10000000
tonnes CO2 per year
LOW CO2 CUT
avoided LOW CO2 CUT
 MID CO2 CUT
avoided MID CO2 CUT
HIGH CO2 CUT
 avoided HIGH CO2 CUT
Baseline emissions
Fig. 3. Projections of annual CO2 emissions and avoided CO2 emissions for all scenarios
1162
A. Kazagić et al.

net efﬁciency decrease over time, CO2 emissions are continuously falling due to growth
of annual biomass share (see Fig. 3).
5.3
HIGH CO2 Cut Scenario Results
This scenario assumes the highest share of RES, with the dominant role of HPPs in
order to assure stability and security of the considered power system, providing bal-
ancing power for intermittent energy sources like wind and solar, see Table 3. There
are also few pumped-storage HPPs planned, which contribute maintaining the satis-
factory level of the BOP. CO2 emissions are decreasing for the complete period under
consideration due to the higher share of biomass co-ﬁred with coal planned. That share
will rise up to 20% in year 2050.
Table 2. Results for MID CO2 CUT scenario
Year
2014
2035
2050
Total electricity generation [GWh]
7 175
11 366
14 801
TPP electricity generation [GWh]
5 787
6 361
5 533
HPP electricity generation [GWh]
1 617
3 061
4 981
sHPP electricity generation [GWh] 74
400
814
WPP electricity generation [GWh]
0
850
1 674
Solar electricity generation [GWh]
0
35
200
Biomass electricity gen. [GWh]
0
630
1 408
Share of RES [%]
22.6
44
62
CO2 emission [t]
6 631 954 3 751 673 3 334 459
CO2 emission cuta [%]
30.2
60
65
aCompared to value in 1990
Table 3. Results for HIGH CO2 CUT scenario
Electricity generation [GWh] 2014
2035
2050
TPP
5 787
4 073
3 580
HPP
1 617
4 569
6 681
sHPP
74
695
1 114
WPP
0
1 295
1 974
solar
0
239
500
biomass
0
399
892
Share of RES [%]
22.6
64
76
CO2 emission [t]
6 631 954 2 056 971 1 868 193
CO2 emission cuta [%]
30.2
78.3
80.3
aCompared to value in 1990
Transition of a Conventional Power Utility to Achieve 2050
1163

Comparing MID CO2 CUT and HIGH CO2 CUT scenario, the share of electricity
generated from biomass is roughly the same, but in this scenario, it contributes to a
higher level of emission decrease. The reason is a higher share of biomass participation
(20%) on two TPP units without CCS technology applied, that effects the decrease of
CO2 emissions in an amount of 465 thousand tons, Fig. 3.
5.4
Comparison of Considered CO2 Cut Scenarios
Table 4 shows installed capacities in EPBiH power system in 2050 for the three
considered scenarios. As can be seen some thermal capacities planned in low CO2 cut
scenario would not be built in MID and HIGH scenarios while they would be replaced
mostly by new HPP and WPP.
Accordingly, power generation from fossils (coal), which in low scenario accounts
8.7 TWh in 2050, would be drastically decreased for HIGH CO2 cut scenario, falling
down to 3.5 TWh. In the same time, electricity generation from hydro and wind will be
increased signiﬁcantly for HIGH CO2 cut scenario, reaching 57% share of hydro and
15% share of wind in total electricity generation in 2050 for HIGH CO2 cut scenario,
see Fig. 2.
Consequently, in HIGH CO2 cut scenario, annual CO2 emissions in 2050 falling
down below 2,000,000 tons, which is more than double lower emissions compared to
LOW CO2 cut scenario, see Fig. 3.
In relation to the emissions in 1990, 80% of CO2 reduction is achieved in HIGH
CO2 cut scenario, unlike the 55% cut in LOW CO2 cut scenario.
6
Sustainability Analysis
6.1
Single Criteria Analysis
Sustainability indicators, set in the sustainability assessment model ﬂow chart in Fig. 1,
have been calculated for all three CO2 cut scenarios. Environmental indicator
(EI) considered for different CO2 cut scenarios in this analysis is CO2 indicator
(kg/MWh), which presents total CO2 emissions in tones generated in EPBiH power
utility in period 2016–2050, or more exactly released from EPBiH’s power plants,
divided by total electricity (MWh) generated by EPBiH power plants in period
Table 4. Comparison of installed capacity in MW in 2050 for all scenarios
LOW CO2 CUT MID CO2 CUT HIGH CO2 CUT
TPP
1500
1200
750
HPP
950
1200
1600
sHPP
150
190
250
WPP
550
800
950
Solar
100
150
300
1164
A. Kazagić et al.

2016–2050. SO2 indicator and NOx indicator (kg/MWh) are assessed to be of
neglected inﬂuence taking into account that all remaining and new TPP of EPBiH
operating in considered period, from 2018 to 2050, will be complied with IED, that
yields comparable SO2 and NOx indicators for all scenarios considered. As the most
important economic indicators (EcI) for this analysis, Investment indicator (EcICA-
PEX) and Indicator of energy costs (EcIOPEX) are considered, while the latest taking
into account ﬁxed and variable O&M costs, including fuel costs, costs of desulphur-
ization (DeSOx) and costs of denitriﬁcation (DeNOx). CO2 tax indicator (EcICO2tax)
is considered as a particular economic indicator.
Economic aspect is assessed using levelized cost of energy (LCOE) for all facilities
in generation portfolio of EPBiH power utility. LCOE is calculated according to the
relationship between the plant’s total costs (€) (OPEX and CAPEX) and total electricity
production (MWh), both of which are worked out in terms of the plant’s economic
lifetime. In calculation of OPEX, recommended factors of ﬁxed and variable opera-
tional costs for new facilities given in [25] have been used in analysis, adopted in this
case to the situation of EPBiH power utility. CO2 tax costs have been estimated on the
base of CO2 tax costs projections given in [3, 5].
Thus, if SCA is applied, by simple mutual comparison of indicators among all three
options of CO2 cut scenarios under consideration, it can be noted that, from the
environmental aspect—considering indicators of CO2 emissions, HIGH CO2 cut has an
advantage over MID and LOW CO2 cut scenarios. Moreover, considering operational
costs indicator (EcIOPEX) and CO2 tax indicator, HIGH CO2 cut scenario is preferable
over remaining two scenarios. The same can be stated if the total costs aggregated for
the period 2016–2050 for scenarios under consideration are compared, see Table 5.
However, from the aspects of investments (EcICAPEX), HIGH RES is not a preferable
option. The presented example shows that, within SCA, the selection of the optimal
option for the power system depends exclusively on selected criteria. Consequently,
subjectivity of decision makers in decision making could be expressed if only SCA is
applied.
6.2
Multicriteria Sustainability Assessment
Within the MSA performed in this paper, according to the standard MSA procedure
described and used in [17–20], equal weighting factors have ﬁrst been assigned to the
Table 5. Estimation of the costs and CO2 emissions for considered CO2 CUT scenarios in
EPBiH, aggregated for period 2016–2050
CO2 CUT Scenario
LOW CO2 cut
MID CO2 cut
HIGH CO2 cut
Indicator
Units
CO2 emiss.
t
188 632 395
174 714 243
139 275 183
CAPEX
EUR
6 547 432 128
7 266 347 002
8 286 646 708
OPEX
EUR
94 638 529 158
93 332 850 442
85 629 901 022
CO2 tax costs EUR
6 331 136 280
5 806 506 207
4 459 821 952
Total costs
EUR
107 517 097 566 106 405 703 651 98 376 369 682
Transition of a Conventional Power Utility to Achieve 2050
1165

group of environmental and economic indicators, as a basic case. Following the pro-
cedure of MSA, values of weighting factors and vectors of speciﬁc criteria, which
actually present normalized sustainability indicators values, together with general index
and ranking the options for this basic case, are given in Table 6. Generally, obtained
results of MSA improve SCA results. In principle, assigning equal importance to EI
and EcI, option of HIGH CO2 CUT has proved to be the preferable option, see Table 7.
In that case option of LOW CO2 CUT is ranked on last position.
Wide range of values of weighting factors in relation to the basic weighting factors
distribution have been investigated within sensitivity analysis of MSA, see Table 7.
Giving an advantage to environmental criteria over economic criteria, option HIGH
CO2 CUT is positioned as preferable, see Table 7. Giving an advantage to economic
criteria over environmental criteria, even with 100% of preference, HIGH CO2 CUT
scenario is still preferable and shows to be the most sustainable option between all
considered CO2 cut scenarios in case of EPBiH power utility.
7
Conclusions
Conventional power utilities are faced with challenges in terms of sustainable devel-
opment and decarbonisation. Different measures are used to achieve set targets, i.e.
RES share increase and energy efﬁciency improvements. A method for reaching CO2
cuts targets has been demonstrated on a real power system, power utility EPBiH. The
Table 6. Weighting factors, speciﬁc criteria vectors, General index and ranking of the options,
Case PwiEI:PwiEcI = 0.5:0.5
SI
wi
LOW CO2 cut MID CO2 cut HIGH CO2 cut
EICO2
0.50
1
0.926
0.738
EcICAPEX 0.1666 0.790
0.877
1
EcIOPEX
0.1666 1
0.986
0.905
EcICO2tax
0.1666 1
0.917
0.704
Q
0.965
0.926
0.804
Ranking
3
2
1
Table 7. Sensitive analysis of General index of sustainability for the options considered
Q
LOW CO2 cut MID CO2 cut HIGH CO2 cut
Case: EI:EcI = 0.50:0.50 0.965
0.926
0.804
Case: EI:EcI = 0.55:0.45 0.969
0.926
0.797
Case: EI:EcI = 0.60:0.40 0.972
0.926
0.791
Case: EI:EcI = 0.45:0.55 0.962
0.927
0.811
Case: EI:EcI = 0.00:1.00 0.930
0.927
0.870
Ranking
3
2
1
1166
A. Kazagić et al.

model took into account the security of supply, competitiveness and sustainability
considering environmental, economic and social indicators. Three scenarios with dif-
ferent CO2 cut rate have been considered, where a range of measures have been applied
to achieve set decarbonisation targets by 2050. All scenarios involve increase of RES
share, from current 18 to 43% in LOW CO2 cut, through 62% in MID CO2 cut, up to
76% in HIGH CO2 cut scenario. As compared to 1990 level, CO2 emissions in those
scenarios are decreased for 55, 65 and 80%, respectively. In all cases, attention was
paid to maintaining the necessary level of stability of the power system and security of
supply.
SCA and MSA have been performed for sustainability evaluation. SCA results
show that HIGH CO2 cut scenario has advantage compared to MID CO2 cut and LOW
CO2 cut scenarios, when environmental and EcIOPEX indicators are considered.
However, when only investment is considered, LOW CO2 cut scenario is preferable. If
this is not considered for the entire period up to 2050 and beyond, and analysed all the
beneﬁts of reducing emissions and the high share of energy from RES, this could lead
investors to the wrong lane. Although electricity demand is met in all scenarios, it is
obvious that an increase in investment in generation facilities that will contribute
reducing the costs of operation and maintenance, as well as reducing CO2 emissions, in
the long run will be multiply rewarded, when comparing LOW CO2 cut with HIGH
CO2 cut scenario. It was conﬁrmed by MSA analysis, which showed that no matter
what relation between weighting factors is considered, HIGH CO2 cut scenario is
preferable, both from the environmental and economical aspect, since economic
indicator is the sum of CAPEX, OPEX and CO2 fees indicators.
The fact is that Bosnia and Herzegovina has a great potential of RES, and that it is
possible to sustainably exploit the available capacity to drastically reduce the envi-
ronmental impact of the power sector. Also, for new TPP units to be commissioned and
being necessary for consumption coverage once when generation from RES is low, the
environmental requirements are fulﬁlled. Those units are planned to be CHP which
would additionally contribute to the decrease of overall emissions on district heating
coverage area. Also, the efﬁcient use of fuel will be maximized in accordance with the
best available techniques, while biomass co-ﬁring will signiﬁcantly contribute to the
ultimate goal.
Acknowledgements. Authors would like to thank EPBiH power utility for the outsourced real,
measured data.
References
1. IEA Statistics, CO2 Emissions from Fuel Combustion—Highlights, International Energy
Agency, 2014 Edition
2. Commission, European: Energy 2020. European Union, Brussels, Belgium (2011)
3. European Climate Foundation: Energy Roadmap 2050. European Union, Brussels, Belgium
(2010)
Transition of a Conventional Power Utility to Achieve 2050
1167

4.
Wang, R.Z., Yu, X., Ge, T.S., Li, T.X.: The present and future of residential refrigeration,
power generation and energy storage. Appl. Therm. Eng. 53(2), 256–270 (2013)
5. European Climate Foundation: Power Perspectives 2030. European Union, Brussels,
Belgium (2011)
6. Boston, A.: Delivering a secure electricity supply on a low carbon pathway. Energy Policy
52, 55–59 (2013)
7. Blum, H., Legey, F.L.L.: The challenging economics of energy security: Ensuring energy
beneﬁts in support to sustainable development. Energy Econ. 34, 1982–1989 (2012)
8. Carpenter, A.M.: R&D Programmes for Clean Coal Technologies, CCC/244, ISBN
978-92-9029-566-2, IEA Clean Coal Centre, October 2014
9. IPCC, Climate Change 2013: The Physical Science Basis, Contribution of Working Group I
to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change,
Cambridge University Press, Cambridge, United Kingdom and New York, NY, USA (2013)
10. Sorknæs, P., Lund, H., Andersen, A.N.: Future power market and sustainable energy
solutions—the treatment of uncertainties in the daily operation of combined heat and power
plants, Appl. Energy, 144, 15 April 2015, 129–138
11. Li, Y., Lukszo, Z., Weijnen, M.: The implications of CO2 price for China’s power sector
decarbonization. Appl. Energy, 146, 53–64 15 May 2015
12. Karakosta, C., Askounis, D.: Developing countries’ energy needs and priorities under a
sustainable development perspective: A linguistic decision support approach. Energy.
Sustain. Dev. 14, 330–338 (2010)
13. Gouveia, J.P., Dias, L., Martins, I., Seixas, J.: Effects of renewables penetration on the
security of Portuguese electricity supply. Appl. Energy 123, 438–447 (2014)
14. Lund, H.: Renewable energy strategies for sustainable development. Energy 32(6), 912–919
(2007)
15. Pina, A., Silva, C.A., Ferrão, P.: High-resolution modelling framework for planning
electricity systems with high penetration of renewables. Appl. Energy 112, 215–223 (2013)
16. Zhang, Q., Mclellan, B.C., Tezuka, T., Ishihara, K.N.: An integrated model for long-term
power generation planning toward future smart electricity systems. Appl. Energy 112, 1424–
1437 (2013)
17. Begic, F., Afgan, N.: Sustainability assessment tool for the decision making in selection of
energy system—Bosnian case. Energy 32(10), 1979–1985 (2007)
18. Begic, F., Afgan, N., Kazagic, A.: Multi-criteria sustainability assessment—a tool for
evaluation of new energy system. Int. Scientif. J. Therm. Sci. 11(18), 43–53 (2007)
19. Mourmouris, J.C., Potolias, C.: A multi-criteria methodology for energy planning and
developing renewable energy sources at a regional level: a case study Thassos. Greece,
Energy Policy 52, 522–530 (2013)
20. Kazagić, A., Merzic, A., Redzic, A., Music, M.: Power utility generation portfolio
optimization as function of speciﬁc RES and decarbonisation targets EPBiH case study.
Appl. Energy (Elsevier) 135, 694–703 (2014)
21. Capros, P., Tasios, N., De Vita, A., Mantzos, L., Paroussos, L.: Model-based analysis of
decarbonising the EU economy in the time horizon to 2050. Energy Strat. Rev. 1, 76–84
(2012)
1168
A. Kazagić et al.

22. Gracceva, F.: F., Zeniewski, P., A systemic approach to assessing energy security in a
low-carbon EU energy system. Appl. Energy (2014). https://doi.org/10.1016/j.apenergy.
2013.12.018
23. Lind, A., Rosenberg, E., Seljom, P., Espegren, K., Fidje, A., Lindberg, K.: Analysis of the
EU renewable energy directive by a techno-economic optimisation model. Energy Policy 60,
364–377 (2013)
24. Parka, N.-B., Yunb, S.-J., Jeona, E.-C.: An analysis of long-term scenarios for the transition
to renewable energy in the Korean electricity sector. Energy Policy 52, 288–296 (2013)
25. Annual energy outlook 2015, Levelized Cost and Levelized Avoided Cost of New
Generation Resources, U.S. Energy Information Admin
Transition of a Conventional Power Utility to Achieve 2050
1169

The Effects of the Measuring and Consumption
of Thermal Energy Based Billing System
in District Heating Systems of Bosnia
and Herzegovina
N. Harbaš(&)
Energy Expert, Sarajevo, Bosnia and Herzegovina
nihadharbas@gmail.com
Abstract. The purpose of this paper is show opportunities for strengthen
national capacities towards low carbon emission development which could be
achieved through developing capacity to formulate, mobilize ﬁnance and
implement activities in Bosnia and Herzegovina. Removing key informational,
institutional, social, ﬁnancial and market, and technical barriers is critical to
paving the way for investment for the enormous low-cost energy supply
improvement and GHG emission mitigation in the district heating systems.
1
Introduction
The energy intensity of a country is deﬁned as the energy usage per unit of GDP
produced on a PPP (purchasing power parity) basis. Similarly energy intensity of a
particular sector can also be deﬁned, but in such a case the GDP contribution would be
only from the respective sector. In order to plan a measurements and consumption of
thermal energy/heat activity, it would be essential to determine the baseline values and
the targets, so as to determine the level of reduction which could be achieve.
The purpose of measurements and consumption of thermal energy/heat in the
system of district heating of BiH is to strengthen national capacities towards low
carbon emission development which could be achieved through developing capacity to
formulate, mobilize ﬁnance and implement investments in Bosnia and Herzegovina.
The ﬁrst step of the transition to a low emission development path is the estab-
lishment of a low emission development strategy. Bosnia and Herzegovina has
developed a draft Climate Change Resilient and Low Emission Development Strategy
(hereinafter referred to as the Strategy, which will, if adopted allow access to the fast
start ﬁnancing committed by developed countries in Copenhagen to support developing
countries in implementing those activities. As a result, the country will be enabled to
make informed policy investment decisions that reduce GHG emissions, reduce pov-
erty, are inclusive, create new employment opportunities and green jobs and move
societies towards long term sustainability. Mainstreaming the climate change into core
development processes will be achieved.
The development of methods for measurements and consumption based milling of
thermal energy/heat in the system of district heating of BiH is innovative and as there
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_100

are lot of countries in the world with already developed this methods. Additionally,
each activity will have afﬁrmative effect on other sectors in country, notably poverty
reduction, green jobs, disaster risk management governance and gender equality.
In order to achieve goals deﬁned above this paper will present possibilities for
reducing emissions, achieving sustainable growth and healthy living conditions, while
setting an example for country—wide intervention. This be will achieved through
implementation of those activities, which will utilize economic instruments to support
the mitigation, adaptation and reduction of vulnerability in the face of climate change
and promote the protection, preservation and restoration environment; ensuring the
sustainable utilization of natural resources and generating economic beneﬁts to those
who implement these actions.
2
Implementing Entity and Overview of Relevant Policies
and Strategies
Consumption based billing is introduced locally in small scale throughout Bosnia and
Herzegovina. Still large portion of building/users are without individual heat metering.
There are no reliable data on installed apartment level heat meters and heat allocators at
the end users. It can be assumed that consumption-based billing for buildings is rep-
resented around 20% (in residential sector).
Adopted Law on Consumer Protection (in 2006) states that the supplied energy is to
be paid in accordance with consumption itself rather than by square meter as is now the
case. This prelude to individual heat metering is a major priority in terms of better
quality of service and better management of district heating system.
The Law on Production, Distribution and Supply of Thermal Energy has not yet
been adopted at the entity level, although its adoption has been proposed by a number
of relevant strategies (ESSBiH Module 9, 2008; the Energy Sector Strategic Plan and
Development Programme of FBiH, 2009; SESRS, 2010; and the Climate Change
Adaptation and Low-Emission Development Strategy, 2013). The Law should regulate
the production, distribution and supply of thermal energy, the rights and obligations of
service providers, and the rights and obligations of thermal energy consumers.
In 2013, three very important laws related to energy efﬁciency and renewable
energy sources came into effect in the Republic of Srpska. These laws are expected to
contribute substantially to the further development of district heating systems. They
are: (1) the Law on Spatial Planning and Construction, transposing the provisions of
Directive 2010/31/EC—Energy Performance of Buildings Directive into the legislation
of the Republic of Srpska; (2) the Law on Energy Efﬁciency, transposing the provisions
of Directives 2006/32/EC—Energy End Use Efﬁciency and Energy Services Directive
and 2010/30/EC—Energy Labelling Directive into the legislation of the Republic of
Srpska; and (3) the Law on Renewable Energy Sources and Efﬁcient Cogeneration,
transposing the provisions of Directives 2009/28/EC—Directive on the Promotion of
the Use of Energy from Renewable Sources and 2004/08/EC—Directive on the Pro-
motion of Cogeneration into the legislation of the Republic of Srpska.
In the Federation of BiH, transposition and implementation of EPBD is under
jurisdiction of the Federal Ministry of Physical Planning, based on the Law on physical
The Effects of the Measuring and Consumption of Thermal Energy
1171

planning and land utilization (“Ofﬁcial Gazette of FBiH”, No. 2/06, 72/07 and 32/08).
Under this framework, secondary legislation on methodology for calculation of energy
performance of buildings, energy audits of buildings and energy certiﬁcation of
buildings has been adopted and started with implementation. In 2013, the Federation of
BiH adopted the Law on Use of Renewable Energy Sources and Efﬁcient Cogenera-
tion, transposing the provisions of Directives 2009/28/EC—Directive on the Promotion
of the Use of Energy from Renewable Sources and 2004/08/EC—Directive on the
Promotion of Cogeneration into the legislation of the Federation of BiH. A Draft Law
on Energy Efﬁciency is currently being prepared. This law should provide for the
implementation of the provisions of Directives 2006/32/EC—the Energy End-Use
Efﬁciency and Energy Services Directive, 2010/30/EC—Energy Labelling Directive,
and 2010/31/EC—the Energy Performance of Buildings Directive (along with the Law
on Spatial Planning and Land Use of the Federation of BiH).
As part of the newly introduced Directive 2012/27/EC—Energy Efﬁciency
Directive (EED), all EU member countries are obliged to regulate consumption-based
billing with appropriate legislation by 5 June 2014. In order to implement
consumption-based billing in apartment buildings or multi-purpose buildings, instal-
lation of appropriate measuring devices is mandatory by 31 December 2016 (Article 9).
In the future, heat, cooling and hot water must be billed according to actual con-
sumption at least once a year. Consumption-based billing must be carried out at the
latest by 31st of December 2014, in case the meters are already installed (Article 10).
All of these laws will have a signiﬁcant impact on the future implementation of the
consumption based billing model in BiH, as well as customers demand/desire to pay
only for what they use.
3
Overview of the District Heating Systems of Bosnia
and Herzegovina
The position of BiH is between the continental and Mediterranean climates, and such
climate conditions in the most of the BiH territory require great consumption of thermal
energy. Thermal energy for heating is obtained partially through district heating sys-
tems in towns, while other consumers (buildings and households which are not con-
nected to the said systems) make their own heating arrangements. The average share of
central heating in BiH is around 30%, district heating 12%, own boiler rooms 11% and
self-contained central heating 6%. Around 70% of apartments are heated only by room
heaters/furnaces. Households with no heating comprise only a very small percentage
(around 0.7%). On the territory of BiH, district heating systems are mostly concentrated
in large towns. Before the last war, most of this population used thermal energy
supplied through the district heating system. Due to long-term neglect of maintenance
of these systems, and due to their age (it is estimated that heating plants and accom-
panying equipment are between 20 and 25 years old), these systems operate with low
efﬁciency. After the war, there has been some reconstruction of the existing systems
but, according to available data, more signiﬁcant reconstruction has been done only in
Sarajevo. In most of the other systems, there has only been some necessary recon-
struction, so these systems have signiﬁcant losses, which in some cases reach 60% of
1172
N. Harbaš

all losses. The data, published in the Municipal Network for Energy Efﬁciency
(MUNEE) programme, funded by USAID, show that central heating companies in BiH
face difﬁculty in billing for delivered heating energy. The high level of non-payments
makes it impossible to adequately maintain the existing systems and, particularly, to
invest in system upgrades. In addition, the Law on Consumer Protection stipulates that
energy delivered to the consumer should be metered and not charged based on the
surface (m2) of apartments. The implementation of the Law has been reduced and
applies only to individual cases. There are also no plans or deadlines for the intro-
duction of thermal energy metering at the consumers’ end of the system. Thermal
energy supply companies in RS rely on their own boiler plants. They use heavy fuel oil
and coal, except in Pale, Sokolac and Prijedor, where in addition to coal, a certain
amount of biomass is used, and in Zvornik, where natural gas is used. In FBiH, some
thermal energy supply companies do not have their own plants to produce thermal
energy but obtain it from local thermal energy units (most often, thermal power plants).
Compared to other towns, heating in Sarajevo has some special aspects, because the
construction of the gas network enabled development of a ﬂexible heating system,
consisting of a series of individual networks, and the use of small efﬁcient boiler units.
Other buildings such as education institutions, health centers (hospitals and clinics),
state institutions (courts, police), catering establishments and other similar institutions
normally have their own plants for thermal energy production, which use fuel oil or
coal as a source of energy (in Sarajevo Canton, it is normally natural gas). Almost all
companies for central supply of thermal energy use heat exclusively for space heating
but not for preparation of warm potable water.
Overview of the companies for production and distribution of thermal energy in
BiH are shown in Table 1.
Based on available data, following table gives overview of district heating systems
consider in this paper (Table 2).
4
Analysis of Introducing Metering and Consumption Based
Billing System in Bosnia and Herzegovina
The consumption based billing and setting up of an adequate metering system have to
be understood as one combined measure. Namely, currently at most district heating
systems in BIH there are no metering systems installed within the ﬁnal consumer.
Moreover, there is no heat energy/consumption metering system at all within the entire
network system.
Based on the ﬁndings and available data it is assumed that in the DHS’s the
consumption-based billing is already represented in approx. 20% of the total residential
sector. For the rest of 80% (approx. 104,219 dwellings or 6,001,703 m2 of the heated
area), considering the technical context of BiH’s heating distribution system, it can be
concluded that two types of metering systems are required in order to be able to provide
all current users with consumption based billing, transparency and easy costs calcu-
lation understanding:
The Effects of the Measuring and Consumption of Thermal Energy
1173

• Namely, due to the fact that in multifamily buildings the distribution pipes (as
vertical distribution systems) in most cases goes through consumer’s dwellings,
heat cost allocators have to be installed in each apartment on each heating
body/radiator. In addition to the heat costs allocators applied to every heating
body/radiator and the identiﬁcation by their type and thermal capacity, every
building supplied by a district heating company should have a general heat meter at
the entrance. Based on the total consumption indicated by this general heat meter,
the consumed heat energy is allocated among the individual ﬂats. These types of
systems, represents approx. 80% of the total residential sector without consumption
based billing introduced, which is approx. 98,618 dwellings or 5,659,905 m2 of the
total heated area.
• Within all other residential buildings approx. 20% of the total residential sector
without consumption based billing introduced, it is technically reasonable to apply
heat meters (calorimeters) in order to measure the monthly energy consumption.
Basically, it can be concluded that all stand-alone family houses and multifamily
buildings with one-pipe heating systems should have a calorimeter, which is
approx. 5,674 dwellings or 341,798 m2 of the total heated area).
Table 1. Companies for production and distribution of thermal energy in BiH
Company name
City
Public company “Komunalac” Ltd
Banovići
Public company “Rad”
Lukavac
“Centralno grijanje” d.d.
Tuzla
Public company “Toplane”
Breza
Public company “Grijanje” d.o.o.
Kakanj
Public company “Toplana” d.d.
Tešanj
Public company “Grijanje” d.o.o.
Zenica
KJKP “Toplane Sarajevo” d.o.o.
Sarajevo
JKP “Vik”
Konjic
JKP “Sana” d.o.o.
Sanski Most
“Toplana” a.d.
Banja Luka
Javno preduzeće “Gradska Toplana” d.o.o. Bijeljina
“Metal Brod” d.d.
Bosanski Brod
“Gradsko grijanje” o.j.d.p
Čelinac
“Toplana Derventa” a.d.
Derventa
“Gradska Toplana” o.d.j.p.
Doboj
Public company “Toplana” a.d.
Gradiška
Public company “Toplana”
Istočno Sarajevo
Public company “Gradska Toplana”
Pale
JKP “Centralna Toplana” a.d.
Prijedor
Public company “Gradska Toplana”
Sokolac
Public company “Zvornik stan” a.d.
Zvornik
1174
N. Harbaš

The table indicate the installation costs of the two heat measuring systems, com-
pared through a representative residential dwelling which have 4 pcs of heating bodies
(Table 3).
The investment costs for a heat cost allocator and calorimeter measuring system are
almost equal (about 930 KM per dwelling, excluding VAT for cost allocator systems,
and about 910 KM for calorimeter systems).
Table 2. Overview of analyzed district heating systems in BiH based on available data in BiH
Unit Residential/dwelings Public
Commercial Multi
apartment
building
Housing
Sarajevo
pcs
50.215
2.498
50.215
m2
2.871.890
445.292
2.871.890
Zenica
pcs
22.200
600
22.200
m2
1.000.000
372.000
1.000.000
Tuzla
pcs
19.075
142
2.066
17.168
1.908
m2
1.564.140
1.407.726
156.414
Grijanje
Kakanj
pcs
3.035
290
1972,75
1.062
m2
156.070
47.731
101445,5
54.624
“RAD”
Lukavac
pcs
2.700
2.160
540
m2
135.000
22.000
12.000
108.000
27.000
“Toplana”
Banja Luka
pcs
20.000
650
19.000
1.000
m2
1.078.000
1.024.100
53.900
Toplana
ODJP
“Doboj”
pcs
7.130
493
5704
1426
m2
350.000
98.000
280.000
70.000
“Toplana”
Prijedor
pcs
3.500
1.500
2.800
700
m2
201.999
75.041
161.599
40.400
“Gradske
toplane” Pale
pcs
647
4
68
453
194
m2
37.030
8.054
3.182
25.921
11.109
JP Toplana
Tešanj
pcs
572
72
400
172
m2
46.000
27.000
32.200
13.800
Toplane
Banovici
pcs
1.200
118
1.200
0
m2
62.000
26.000
62.000
0
Eko Toplane
Gračanica
pcs
303
18
51
212
91
m2
56.000
39.200
16.800
Total
pcs
130.274
2.834
5.667
123.273
7.001
Total
m2
7.502.129
528.346 607.954
7.074.881
427.247
The Effects of the Measuring and Consumption of Thermal Energy
1175

5
Overview of the Related Measures and Resulting Effects
As described in previous section, there are two types of measures which corresponds to
metering systems that are required to provide all current users with consumption based
billing.
Based on the above tables which indicate the installation costs and ﬁnancial
parameters of the two heat measuring systems, compared through a representative
residential dwelling and calculated with following parameters:
• Average annual energy consumption per dwelling
160 kWh/m2a
• Market price per MWh of heating energy in DHS
102 KM/MWh (VAT excl.)
The following can be calculated:
• Total annual energy consumption for heating in all dwellings
which requires consumption based billing—960,272 MWh
6; 001; 703 m2  160 kWh=m2a ¼ 960; 272 MWh
• Total annual costs of heating energy in DHS—97,947,791 KM
960; 272 MWh  102 KM=MWh ¼ 97; 947; 791 KM
Table 3. Investment costs of allocator and calorimeter heat measuring systems for a
representative dwelling in BIH
Investment
costs excl.
VAT
Investment costs per
dwelling in multi
apartment building excl.
VAT
Investment
costs per
family house
excl. VAT
[KM/pcs]
pcs
[KM/pcs]
[KM/pcs]
Thermostatic valves
90
4
360
360
Heat cost allocators—
vertical system
80
4
320
0
Calorimeters—
horizontal systems
and where technically
feasible
550
1
0
550
Allocator system
(including general
calorimeter)
5,000
1
250
0
Total investment costs
per dwelling
930
910
1176
N. Harbaš

According to experience from similar projects in Eastern Europe it is expected that
the average annual cost savings, due to switching to consumption based billing, amount
20% (in some cases savings up to 30% have been achieved) resulting with average cost
savings of 188 KM per dwelling. Based on the initial investment costs and the resulting
savings both measuring system investments result with acceptable ﬁnancial indicators
(Tables 4 and 5).
Resulting annual energy savings amounts up to 192,054 MWh/a or 1,8
MWh/dwelling.
Emission factor for district heating system of Bosnia and Herzegovina is 0.26939
tCO2/MWh1 thermal energy produced by district heating system, the total emission
reduction amounts to 51,738 tCO2/a.
192; 054 MWh=a  0:26939 tCO2=MWh ¼ 51; 738 tCO2=a
6
Conclusion
This paper has been prepared for reducing GHG through introduction of metering and
consumption based billing system in district heating systems of Bosnia and
Herzegovina.
Table 4. Financial indicators for heat measuring systems investment per dwelling
Allocator system (including
general calorimeter)
Calorimeter
system
Investment [KM]
930
910
Expected savings due to consumption
based billing [KM]
188
188
Discount rate [%]
7
7
Payback period [years]
5.0
4.8
NPV [KM]
1,332
1,352
IRR [%]
19.66
20.13
Table 5. Financial indicators for heat measuring systems total investment
Investment [KM]
96.811.833
Expected savings due to consumption based billing [KM] 19.589.558
Discount rate [%]
7.39
Payback period [years]
4.9
NPV [KM]
104,588,322.61
IRR [%]
19.68
1 “Guidelines for Conducting an Energy Audit in Buildings”; USAID 3E, UNDP BiH, GIZ;
November 2011, Bosnia and Herzegovina, page 58.
The Effects of the Measuring and Consumption of Thermal Energy
1177

All European countries have set up mandatory Law on Consumer Protection (in
2006) states that the supplied energy is to be paid in accordance with consumption
itself rather than by square meter as is now the case. This prelude to individual heat
metering is a major priority in terms of better quality of service and better management
of district heating system.
The paper is based on a thorough economic assessment and analysis of the
cost-effectives of climate change mitigation actions in the energy supply segment of the
residential sector with a speciﬁc focus on introduction of metering and consumption
based billing system in district heating systems.
Based on the ﬁndings and available data it is assumed that in the DHS’s the
consumption-based billing is already represented in approx. 20% of the total residential
sector. For the rest of 80% (approx. 104,219 dwellings or 6,001,703 m2 of the heated
area), considering the technical context of BiH’s heating distribution system. There are
that two types of metering systems in order to be able to provide all current users with
consumption based billing, transparency and easy costs calculation understanding:
allocator and calorimeter measuring system.
Based on the above ﬁgures which indicate the installation costs and ﬁnancial
parameters of the two heat measuring systems, compared through a representative
residential dwelling and calculated with following parameters:
• Average annual energy consumption per dwelling
160 kWh/m2a
• Market price per MWh of heating energy in DHS
102 KM/MWh (VAT excl.)
Total annual energy consumption for heating in all dwellings which requires
consumption based billing amount 960,272 MWh. Total annual costs of heating
energy in DHS amount 97,947,791 KM.
Applying installation of metering and consumption based billing system in district
heating systems, estimated savings amounts of 20%, which means energy savings of
192,054 MWh, GHG reduction of 51,738 tCO2/a and 19.589.558 KM money savings
with total investment of 96.811.833 KM.
Implementation of proposed measure will contribute in achieving BiH target in
increasing energy efﬁciency set by NEEAP and GHG emission reduction as well as
local employment and sustainable development.
1178
N. Harbaš

CFD as an Enginer’s Tool for Investigation
of Large-Scale-Flow-Phenomena “at Land,
Sea and Air”
M. Muhasilović1(&), B. Širok1, K. Ciahotny2, and M. O. Deville3
1 University of Ljubljana - Mechanical Engineering, Askerceva 6,
Ljubljana, Slovenia
muhasilovic@gmail.com
2 VSCHT, Czech Institute of Chemical Technology, Prague, Czechia
3 EPFL - Swiss Federal Institute of Technology, STI-ISE-LIN, Station 9,
CH-1015 Lausanne, Switzerland
Abstract. Along with prospective fashion of solving the engineering-tasks (if
development
would
allow
us
a
such
approach
while
answering
the
technology-questions of some project in future), goes alternatively corrective
performance of those engineering tools, used to compensate for needed changes
in some already existing technical solution. A unique chance to compare the
needs of trafﬁc-intentions (while setting the modern road communications
through the southern of Bosnia and Herzegovina) verses natural occurrences in
the atmosphere (such is a strong north-wind in this geographic region) offers the
high-way-section Pocitelj-Zvirovici. Exactly in such cases (and before the actual
construction of an engineering-construction is realized) “for the sake” of
prospective—and certainly self-sustainable engineering—the tool of CFD (the
tool for performing the Computational Fluid Dynamics) was applied to engaged
the problem. Again, in the same moment while solving prospectively (through
application of the CFD-based research prospectively in) several technical issues
on submarine tidal-turbines in bay-area of Swansea in Wales (UK) for electricity
production, a corrective way of engineering is requested to research on both
imagined ﬁre-scenarion in road-tunnel of Vranduk, close to Zenica (Bosnia) as
as well as in case of investigative observing the (large-scale) combustion of the
ﬂare-stack in the reﬁnery in town of Lendava (Slovenia). Coming from the both
steady-state
(k-epsilon
turbulence-treatment)
and
time-dependent
applied
CFD-based explorations (Large-Eddy Simulation in Smagorinsky-Lilly mode),
the unexpected and unwanted strong gaseous ﬂows were detected and the results
coming out of this are explained and discussed.
… to my late Mom, stroke by the destiny …
1
Introduction
Observing more closely the functioning of our civilization with it’s industry and it’s
economy, one can always notice our urge to perform the energy conversion in a such a
way where it´s primary ﬂux we try to convert to that energy-form that will have most
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_101

positive impact on our modern society. We certainly always do succeed in those
energy-transformation processes—hence the overall progress. However, rather frequent
accidents in areas of our lives that ought to be governed with great technological
development and human experience are forcing for continuous scientiﬁc engineering
research.
The on-growing population (of some 7 Billion of us) on our planet is requesting
even better procedures and machines that will provide almost ﬂuent energy-conversion
to that (maybe universal) ﬁnal energy-form, which is most suitable for our use.
According to the geographic characteristic, in some parts of our world, the renewable
energy “route map” outlines ambitious targets for 50% renewables by 2025. In Europe,
in some of it´s coastal regions, subject to tidal ranges of the order of 13 m and tidal
ﬂows in excess of 3 m/s, is thus in an ideal position to signiﬁcantly contribute to the
targets to proceed with electricity-gaining in renewable fashion. Tidal stream energy is
an emerging energy sector and a relatively small number of devices are at various
stages of development. However, before such demonstration devices or energy-arrays
and converter-parks can be applied at a larger scale, a number of engineering
research-scenarios must be obtained to ensure their safe and environmentally respon-
sible deployment.
This paper describes the engagement of engineer´s tool which is based on
numerical modeling. In one case, the CFD-based (Computational Fluid Dynamics)
research-work has been undertaken prospectively on an area of ocean that could be
used for the deployment of an imagined tidal stream turbine for electricity production.
In another investigation-case, this paper aims to reveille the scientiﬁc work that is put
into the context of exploration of the large-scale ﬁre accidents and to provide so an
overview of the surveying and modelling required for the industrial safety.
Therefore for (CFD-based, prospective and investigative) demonstration of
tidal-turbine was “set” into the Bristol Channel (Welsh coast, UK) because of its high
tidal ﬂows and vicinity to both British and ECPTE electricity-grid.
Again—in correlation with electricity distribution, the employment of corrective
CFD-based
exploration
of
unwanted
thermal
load
within
the
object
of
a
power-transforming station (produced by ENERGOINVEST, Sarajevo for the ﬁnal
user in Al Taweelah, UAE) offers an overview of the research carried out.
2
Objects of Interest and Applied CFD-Based
Research-Modes
Large scale ﬁres, occurring as accidents in many sectors of our society [1], became
object of investigation of a great interest, whose post-research-studies are giving
enough conclusions for the guidelines of their suppression [2, 3]. Field-model codes
that have been engaged in CFD-research [4], do report on good capability of these
numerical approaches, used in handling the gaseous reacting ﬂows in enclosed objects.
In spite of the ﬁrst hardware limitations, a decade a half ago, allowing computational
domains with few thousands cells [5, 6], satisfying results were accomplished in
attempts of both validating [7–12] of software tools and aimed CFD-prognoses for
particular explored cases of these ﬂuid-ﬂow phenomena [13, 14]
1180
M. Muhasilović et al.

2.1
Numerical Approach—Treatment of Turbulence—Mathematical
Model in This Study
For turbulence-modelled conservation equations, for mass and momentum, in case of
time-averaged k-e turbulence model (a CFD-mode that was applied in this study) the
governing integro-differential equations must be discretised in both space and time [15,
16]. Together with the equations of state for an ideal gas, form here a closed set of coupled
equations. These are again discretised and solved on a three-dimensional, ﬁnite-volume
Cartesian mesh. In choosing the numerical method we [8] rely on the standard of the ﬁnite
volumes [9, 16, 17]. The spatial discretisation of time-independent equations employed a
segregated solution method. Thelinearised equations result in a system of linear equations
for each cell in the computational domain, containing the unknown variable at the cell
centre as well as the unknown values in surrounding neighbour cells. This mechanism for
a scalar transport equation [8] is also used to discretise the momentum equations as well;
in the same mode for the pressure ﬁeld (if face mass ﬂuxes were known) and the velocity
ﬁeld will be obtained in same way as well. In case that the pressure ﬁeld and face mass
ﬂuxes are not known, FLUENT uses a co-located scheme, whereby pressure and velocity
are both stored at cell centres. A need for interfacial values includes an application of an
interpolation scheme to compute pressure and velocity out of cell values. The integration
over the arbitrary volume (a cell in a computational domain) can be performed yielding
the discretised through an arbitrary surface of a face. Performance of those equations
express the state for each other cell in the computational grid. This again will result in a set
of algebraic equations with a sparse coefﬁcient matrix. In this way the segregated solver is
handling “the updating” of a single variable ﬁeld by considering all the cells of the domain
at the same time, solving the governing equations sequentially (segregated one from
another). Subsequently, the next ﬁeld of another variable will be solved by again con-
sidering entire cells at the same time, etc. The computational loop for the converged
solution had about 5500 iterations.
3
Results and Discussion
Power-transformer stations that provide end-form of electricity level for application in
industry are actually equipped with robust and capable system for extinguishing the
unwanted ﬁre-accident that may occur. However, the safety “philosophy” is based on
“late” powering up of such (usually CO2—based) extinguishing gas and pumping it
into the cavern of the object where power-transformer is built in. In such ﬁrst 10 s the
object, it´s construction elements must sustain the large-scale thermal load and in that
way satisfy the safety-requirements. CFD-based method, replacing expensive and
hard-to-perform experiment did show also this time to be a reliable tool in the ﬁeld of
scientiﬁc engineering (Figs. 1, 2, 3).
CFD as an Enginer’s Tool for Investigation
1181

Concrete-spalling, a phenomenon that can be observed after some devastating
ﬁre-accidents, happens in cases when Reinforced-Concrete construction (RCC) is
exposed to a (large-scale) ﬁre more than 12 min in the temperature ﬁelds of a 725 K
and above. In ﬁrst 10 s (according to the safety-procedures) of an established accidental
ﬁre in the object of power-transformer, the developed temperature ﬁelds are high
indeed, but the RCC will withstand this unwanted thermal load, since the automatic-
extinguishing mechanism is engaged in propped time, where CFD-tool was able to
provide insight of this important aspect of the safety analysis.
4
Future Work
Our natural geographic characteristics and natural occurrences are always surrounding
trafﬁc-objects [18–20, 14]. Evaluations [5, 11, 13, 21] and suggestions [12] due to the
atmospheric movements do present potential safety risk and invite for CFD-based
exploring in large-scale fashion. Such investigations, due to the ever-stronger software
and hardware tools [6, 22, 23] are performed not only through the physical measuring
[10], and scaled testing [7], but also more frequently by applying the CFD-based
approach [6]. The latter research-approach [4] did ﬁnd application in wind-exploring
[24, 25] and trafﬁc-safety [26] offering very satisfying results accomplished in attempts
“prospective engineering” for particular explored cases of ﬂuid phenomena [27].
Computational domain
25m x 56m2
Fig. 1. Computational domain of the power station
1182
M. Muhasilović et al.

4.1
The Explored Bridge
The cross-section shapes of this high-way viaduct are distinguished as ones between
the major carrier-pylons and as the bridge-crown-shapes that are mounted onto the
“bridge-legs” of this trafﬁc steady-object. Standing under the angle of ca 3.1° the
road-treks of this bridge have the bow-length of 954 m and their arch-radius is 983 m
(Figs. 4, 5).
Going
partly
over
the
river-bed
and
partly
over
the
terrain-valley,
the
highway-bridge is demonstrating it´s highest section to be of 96 m. The wide-range
between the six major pylons is set to 147 m.
1.611.723 cells
759.646 cells
Fig. 2. Testing the solution-independency on grid-density
CFD as an Enginer’s Tool for Investigation
1183

4.2
Computational Domain
The area in which the computation with applied mathematical model approach and
additional numerical discretisation was performed is the very volume, that a ﬂuid can
take, without the walls, where the solid-body was the shape of the explored
road-bridge. Therefore the computational domain of the section was set to be
30 m  22 m  14,5 m.
The mesh of this computational domain (Figs. 6 and 7) is characterised through
hexahedral cells of a random structure. In this case a denser grid was also applied in the
area around the zones where particular mechanical-ﬂuid phenomena are expected,
Fig. 3. CFD-based research on withstanding the thermal-load within the cavern of the
power-transformer station
1184
M. Muhasilović et al.

Fig. 4. The cross-section of the crown in the free-air as well as the pylon-crown
Fig. 5. The constructive disposition of the high-way bridge
Thermal-transient bridge-
body and road-elements 
wall, adiabatic
Computational domain: air
T=293 K
p=1015,25 hPa
Open bounda-
ries
T=293K
p=1015,35 hPa
Turbulence
Varying 
wind -velocities v=10m/s 
20m/s 30m/s  40m/s 
Fig. 6. The unstructured hexahedral mesh is applied over highway-bridge
CFD as an Enginer’s Tool for Investigation
1185

having so more grid-points to support the major occurrences. Such unstructured hex-
ahedral mesh (sized here to 350 mm) was installed in such zones of whole computa-
tional domain Fig. 8.
The bridge-body and the road-elements were in the computational domain deﬁned
as non-adiabatic walls. The ﬂuid-domain is air, with the ambient conditions and no
ﬂuid-movement, but from the side of expected wind-strokes. The computational
ﬂuid-sides were designed as opened pressure boundaries.
Fig. 7. The meshing-detail over the highway-bridge—here, demonstrating the solution around
the road-fence
Fig. 8. The meshing-detail of the bridge-segment
1186
M. Muhasilović et al.

5
Conclusion
There is always a legitimate argument over application of CFD-based research and this
again in sense of such discussion: why any simulation should be applied for scientiﬁc
engineering purpose? Developed in late 1960-ies [2], compiled in modern computa-
tional codes, these software-mechanisms have fully came to engagement in late
1990-ies due to the great progress in development of hardware that was ﬁnally capable
of serving this powerful CFD-programs [3]. Such investigations, due to the
ever-stronger software and hardware tools [6, 22, 23] are performed not only through
the physical measuring [10], and scaled testing [7], but also more frequently by
applying the CFD (Computational Fluid Dynamics)-based approach [6]. The latter
research-approach [4] did ﬁnd application in wind-exploring [24] and trafﬁc-safety [26]
which is the research-pathway of the work presented in this paper, offering very sat-
isfying results accomplished in attempts “prospective engineering” for particular
explored cases of ﬂuid phenomena [27].
All of research attempts that have been brought up into the CFD-community, do
report on good capability of the numerical approaches used in handling the large-scale
(reactive) ﬂows in various shapes of (industrial and trafﬁc) infrastructure.
Therefore the aim of the study performed, was to demonstrate capability of
numerical modeling that presents today´s modern simulation-codes, as one of the major
parts of the CFD-based engineer´s tool.
References
1. Viulleumier, F., Weatherill, A., Crausaz, B.: Safety aspects of railway and road tunnel:
example of the Lötschberg railway tunnel and Mont-Blanc road tunnel. In: The 28th ITA
General Assembly, Sydney, Australia (2002)
2. Hanjalic, K., Launder, B.E.: A Reynolds stress model of turbulence and its application to
thin shear ﬂows. J. Fluid Mech. 52, 609–638 (1972)
3. Hanjalic, K., Launder, B.E.: Modelling Turbulence in Engineering and the Environment
(2011)
4. Kareem, A.: Numerical simulation of wind effects: a probabilistic perspective. J. Wind Eng.
Ind. Aerodyn. 96, 25 (2008)
5. Kwon, S., Lee, H., Lee, S., Kim, J.: Mitigating the effects of wind on suspension bridge
catwalks. J. Bridge Eng. 18(7), 8 (2013)
6. Cheng, L.H., Ueng, T.H., Liu, C.W.: Simulation of ventilation and ﬁre in the underground
facilities. Fire Saf. J. 36(6), 597–619 (2001)
7. ZhouI, L., Ge, Y.: Wind tunnel test for vortex-induced vibration of vehicle-bridge system
section model. J. Braz. Soc. Mech. Sci. Eng. 30(2) (2008)
8. Muhasilovic, M., Deville, M.: Tunnel-Curvatire´s inﬂuence on the propagation of the
consequences of large-scale accidental ﬁre—a CFD-investigation. Turk. J. Eng. Environ.
Sci. 31, 391 (2007)
9. Peric, M., Ferziger, J.H.: Computational Methods for Fluid Mechanics. Berlin, Springer
(2001)
10. Modic, J.: Porocilo o meritvah pri pozernem preizkusu v cestnem tunelu SENTVID.
Mechanical Engineering, University in Ljubljana, Ljubljana, Slovenia (2008)
CFD as an Enginer’s Tool for Investigation
1187

11. Newland, D.E. Vibration of the London Millennium Footbridge: Part 1—Cause. Department
of Engineering, University of Cambridge, Cambridge CB2 1PZ, UK (2002)
12. Newland, D.E.: Vibration of the London Millennium Footbridge: Part 1—Cure. Department
of Engineering, University of Cambridge, Cambridge CB2 1PZ, UK (2002)
13. Chen, A., You, Q., Zhang, X., Ma, R., Zhou, Z.: Aerodynamic problems of a super-long
span cable-stayed bridge. In: IABSE Symposium. Lisbon, Portugal (2005)
14. Bowers, P., Boneck, J.: Charleston’s Bridges Cross Troubled Waters. Charleston City Paper,
Charleston, South Carolina, USA (2012)
15. http://www.ﬂuent.com
16. Versteeg, H.K., Malalasekera, W.: An Introduction to Computational Fluid Dynamics.
London, Longman Group Ltd (1995)
17. Hirsch, C.: Numerical Computation of Internal and External Flows. Chichester Brisbane
Toronto New York, Wiley (1988)
18. Hembre, D., Otto, O., Payette, J., Pingree, G.: The 50th Anniversary of the Golden-Gate
Bridge. Laboratory for Construction Technology, Graduate School of Design, Harward
University, Cambridge, Massatchussets, USA (1988)
19. Simiu, E., Vickery, P., Kareem, A.: Relation between Safﬁr–Simpson Hurricane Scale Wind
Speeds and Peak 3-s gust speeds over open terrain. J. Struct. Eng. 133 (2007)
20. http://www.travelandleisure.com/articles/worlds-scariest-bridges
21. Martin, T., MacLeod, I.A.: The Tay Rail Bridge disaster revisited. In: Proceedings of the
Institution of Civil Engineers, p. 5 (2004)
22. Muzaferija, S., Gosman, D.: Finite-volume CFD procedure and adaptive error control
strategy for grids of arbitrary topology. J. Comput. Phys. 138(2) (1997)
23. Svaic, S., Boras, I., Andrassy, M.: A numerical approach to hidden defects in thermal
non-destructive testing. J. Mech. Eng. 53(3), 165 (2007)
24. Kwon, D.-K., Kijewski-Correa, T., Kareem, A.: E-analysis of high-rise buildings subjected
to wind loads. J. Struct. Eng. 134, 1139 (2008)
25. Kijewski-Correa, T., et al.: Validating wind-induced response of tall buildings: synopsis of
the Chicago full-scale monitoring program. J. Struct. Eng. 132(10) (2006)
26. Chen, X., Kareem, A.: Identiﬁcation of critical structural modes and ﬂutter derivatives for
predicting coupled bridge ﬂutter. J. Wind Eng. Indus. Aerodyn. 96, 14 (2008)
27. Chen, X., Kareem, A.: Revisiting multimode coupled bridge ﬂutter: some new insights.
J. Struct. Eng. 132(10) (2006)
1188
M. Muhasilović et al.

Smart LEG Control System Optimization
Haris Dindo, Zelimir Husnic, Remzo Dedic(&), and Adisa Vucina
Faculty of Mechanical Engineering and Computing, University of Mostar,
Matice hrvatske bb, 88000 Mostar, Bosnia and Herzegovina
remzo.dedic@sve-mo.ba
Abstract. This paper presents Smart Leg, an active robotic prosthesis that
enables people with an above or below knee amputation to perform different
types of motions that normally require power in lower limb joints. Our design
integrates advanced prosthetic and hydraulic technology with the state-of-the-art
machine learning algorithms capable of adapting the working of the prosthesis
to the optimal gait and power consumption patterns, and which therefore pro-
vide means to customize the device to a particular user. We outline optimization
of the hydraulic installation and control system that ensure stable and natural
gait for the end user.
1
Introduction
Today in the world there are a considerable number of upper leg amputees, who lost his
legs in trafﬁc accidents, armed conﬂicts, accidents at work or for certain types of
diseases. Loss of leg represents for such persons trauma, and the continuation of life
brings numerous restrictions. Therefore, it is very important to such people returning to
normal life as much as possible.
One of the most important activities of the movement. Walking existing AK
prosthesis is in a good solved, while climbing the stairs in a natural way, not yet. This
activity is not possible without active joints in the knees and ankles, and these are again
necessary actuators and external power source. Today in the world, according to [1],
there are 27 institutions that are trying to solve this problem.
2
Current State of the Above-Knee Prostheses
According to available data, there are several prototype solutions with actively driven
joints.
Varol and Goldfarb [2] presented AK prosthesis pneumatic-powered knee and
ankle (Fig. 1). Prosthesis can recognize user intent for standing or walking.
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2_102

Sup et al. [3] presented AK prosthesis that exercise standing or walking on ﬂat
ground by normal biological speed (Fig. 2). Prosthesis consumes 66 W and the patient
may tumble 5.2 km between the two battery charging.
Fig. 1. AK prosthesis pneumatic driven [2]
Fig. 2. AK prosthesis [3]
1190
H. Dindo et al.

Brackx et al. [4] developed a prototype passive foot in which extra energy is
achieved by the operation of the mechanism and spring (Fig. 3).
Au et al. [5] developed shin-powered prosthesis (Fig. 4), which allows walking on
level ground and the descent down the stairs.
Bionic leg [6] is result of the project of Rehabilitation Institute of Chicago
involving Vanderbilt University, the University of Rhode Island, and University of
New Brunswick (Fig. 5).
Fig. 3. Passive foot [4]
Fig. 4. Shin prostheses [5]
Smart LEG Control System Optimization
1191

3
Smart Leg
Smart Leg is prototype of active control prosthesis with actively driven knee and ankle
joints [7]. Hydraulic linear actuator generate movement joints and generate sufﬁcient
force to overcome all the moments that occur when climbing stairs. Artiﬁcial foot is
designed to be achieved passively bend in the toe, which has achieved yet another
improvement to the natural leg (Fig. 6).
3.1
Hydraulics Control System
The Prosthesis Hydraulics System main goal is to develop power system including
appropriate control system. The Hydraulics System need to enable desired movements
Fig. 5. The bionic leg [6]
Fig. 6. The prosthesis smart leg
1192
H. Dindo et al.

without collisions, and to be safe and able to withstand forces while working in real
operating condition. Discuss will be focus on the hydraulic and control systems opti-
mization. The knee and ankle actuators are designed according to load and motion
requirements with optimum size and weight. Hydraulic system must include appro-
priate control system and regulative elements for directing ﬂuid as well as its distri-
bution towards hydraulic actuators.
Requirements for control system:
• collect data of motion and position at the knee and ankle,
• identify a force/pressure during the contact a prosthesis with the ground,
• control and predict the leg motion/gait during the stairs climbing based on the
sensors input.
Hydraulic system scheme is shown in Fig. 7.
Motion and position demand comes from an intelligent control that would recog-
nize demand for beginning and end of request for the hydraulics actuators movement.
The controller compares the signal from the feedback sensor with an input demand to
determine the error, and produces a command signal to the servo-valve. The control
valve adjusts the ﬂow of pressurized ﬂuid to move the hydraulics actuator until the
desired position is achieved.
Fig. 7. Hydraulic system scheme
Smart LEG Control System Optimization
1193

A typical Electro Hydraulic Control system include position or velocity demand
signal, electronic servo controller, servo-valve, actuator and sensors to provide feed-
back signal.
Proposed Electro Hydraulic Control Closed Loop is shown in Block diagram
(Fig. 8).
3.2
Hydraulics Servo Valves
The power system requirement specify load, stroke, and ability to control speed,
position and force during entire operating range. In order to satisfy all these require-
ments, it would be the best choice to use a hydraulics servo valve. The servo valve is an
integral part of the controlled hydraulic system. The valve regulates ﬂow and direction
of the hydraulic cylinder. The most important parameters for the Servo Valve selection
are pressure, load, ﬂow and type of ﬂuid. The servo valve responds to command signals
generated by the software and processed by the controller. In order to achieve the best
system performance, it is needed to choose right servo valve for given load and
hydraulic cylinder. The basic steps for the servo valve sizing include computing of
required frequency response and selection of pressure and ﬂow. As for selection
regarding pressure and ﬂow, recommendation is to select the valve size to obtain
approx. 30% system pressure drop across the valve at maximum velocity or ﬂow. If the
drop across the valve is too small, then a ﬂow change will not take place until the valve
is nearly closed.
For instance, the system ﬂow requirement is Qc = 0,67 l/min (0,176 gpm or
0.6775 in3/s) and pressure requirement is pp = 2080 psi (14,3 MPa), including all
leakages and energy losses. These requirements represent the ﬂow and pressure that the
hydraulic system shall provide at hydraulics cylinders input ports. In case when
pressure at cylinders ports is pc = 1500 psi (10,3 MPa) and pressure drop is Δp =
580 psi (4.0 MPa).
The valve no-load ﬂow we could get using following relationship
Qp ¼ Qc
ﬃﬃﬃﬃﬃ
pp
pc
r
ð3:1Þ
Qp ¼ 0:808 in3=s 0:21 gpm; 0; 795 l/min
ð
Þ
Fig. 8. Block diagram
1194
H. Dindo et al.

where is
Qc
loaded ﬂow (ﬂow at valve outlet ports or hydraulic cylinder ﬂow) (in3/s),
Qp
no-load ﬂow (in3/s),
pc
pressure at cylinder (psi),
pp
pump pressure (psi),
Δp
pressure drop (psi).
The Servo Valve rated ﬂow Qr at 1,000 psi pressure drop will be determine with
relationship Qr = 1,1Qp. As Fig. 9. (right) shows one unit is mounted on the belt acting
as a local reference system, while other units are mounted one on each link forming a
leg kinematic chain. Figure also shows local reference systems. For the purpose of
validation, we have compared outputs of our system with that of trakSTAR, a com-
mercial 6DoF tracking system, and we have found negligible differences as depicted in
Fig. 10. All angles are referred to a shared global coordinate system.
The Servo Valve selection will be done using pressure and rated ﬂow Qr above. In
addition to pressure and ﬂow, it is necessary to perform further the Servo Valve
selection with concern related to the system dynamic response. Every component in the
electro-hydraulic control system has a natural frequency. Recommendation is to
compute hydraulic cylinder and load resonant frequency. Also, need to determine
frequency for servo valve (the servo valve catalogue provide this information). Com-
ponents with natural frequency far higher than the lowest natural frequency in the
system will have minimum impact on the system dynamic characteristics. The natural
frequency of the servo-valve should be approx. 3x higher than the natural frequency of
the hydraulic cylinder-load system. Also, the controller response should be fast or
approx. 10 times of the lowest natural frequency in the servo system. For instance, the
hydraulic cylinder natural frequency calculation. The hydraulic cylinder—load system
will be consider as simpliﬁed mass-spring model, where ﬂuid on each side of the piston
head acts as spring.
x ¼
ﬃﬃﬃﬃ
k
m
r
ð3:2Þ
k ¼ F
d
ð3:3Þ
F ¼ p A
ð3:4Þ
p ¼ DVb
Vo
ð3:5Þ
d ¼ DV
A
ð3:6Þ
k ¼ bA2
Vo
ð3:7Þ
Smart LEG Control System Optimization
1195

xcy ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
bA2
Vom
s
ð3:8Þ
where is
x
frequency (rad/s),
k
spring rate (constant),
d
spring deﬂection (m),
b
bulk modulus of ﬂuid (kPa),
m
moving (effective) mass (kg),
A
area of piston head (m2),
F
axial load (kN),
P
pressure (kPa),
Vo
volume of trapped ﬂuid (m3).
The actuator, ﬂuid and load data are given as follows:
A = 0,201 * 10−3 (m2); Vo = 1,5 * 10-5 (m3); b = 1,5 * 106 (kPa); m = 100 (kg);
xcy ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
bA2
Vom
s
¼ 201 rad/s
ð3:9Þ
Frequency in hertz (Hz).
Relationship between the angular frequency (rad/s) and frequency in hertz (Hz) is
given by relation:
fHz ¼ xrad=s
2p
¼ 32 Hz
ð3:10Þ
The hydraulic cylinder—load system natural frequency fHz needs to be compare
with the servo valve frequency. The servo-valves selected, for given system in this
work, has frequency response of 160 Hz at 90° phase lag point (90° phase lag is when
the phase lag between the input current and output ﬂow reaches 90°). The servo valve
frequency is acceptable for the cylinder-load system explained in this paper. The servo
valve chosen for given system satisﬁes criteria related to natural frequency, pressure
and ﬂow.
4
Sensors and Intelligent Control Subsystem
The “smart” adjective of our system refers to the ability of the robotic prosthesis to
automatically adapt to the user’s current needs and to maximize its comfort in the
everyday life. Such a prosthesis has a number of advantages over traditional
above-knee prosthesis. This prosthesis becomes intelligent prosthesis by incorporating
control system, several sensors and appropriate processors. For instance, motion would
be based on the ﬁrst step of the stairs position of the foot and knee. Processor would
1196
H. Dindo et al.

“remember” step and it would repeat the same step and positions. The climbing would
became more rapid, easy and more natural. To achieve that goal, we have equipped the
prosthesis with a custom made motion capture system. It has a twofold objective: (1) to
provide reference signals to the control system and (2) to continuously collect data for
machine learning model training. Our capture system is made of four low-cost iNemo
inertial units 5 shown in Fig. 9 (left). Each iNEMO inertial module integrates different
types of MEMS (microelectromechanical sensors) and provides either raw linear,
angular and magnetic data from the embedded 225 three-axes accelerometer, gyroscope
and magnetometer, or on-board processed Euler angles (roll-azimuth-elevation) with
respect to a global reference system. As Fig. 9 (right) shows one unit is mounted on the
belt acting as a local reference system, while other units are mounted one on each link
forming a leg kinematic chain. Figure also shows local reference systems. For the
purpose of validation, we have compared outputs of our system with that of trakSTAR,
a commercial 6DoF tracking system, and we have found negligible differences as
depicted in Fig. 10. All angles are referred to a shared global coordinate system. We
have created a library of different parametric motor programs (normal gait, climbing,
kicking, etc.). Our high-level control system acts as a ﬁnite-state machine by permitting
the prosthesis to commute from one program to another based on external events.
Fig. 9. (left) iNemo 9DOF inertial unit by ST microelectronics, (right) disposition of sensors
and local reference systems for the wearable motion capture system in a typical data acquisition
session
Fig. 10. Comparison of angles acquired with trakSTAR and iNemo solutions
Smart LEG Control System Optimization
1197

Our motion capture system is used both to tune the parameters of the gate function
for that particular user, and based on the information from the sound leg, as well as to
infer the motor intention of the user in order to adapt to and to switch to the most
appropriate motor program in the timely manner. The former problem is solved by
well-known statistical methods for parameter ﬁtting from noisy data [8]. For the latter
we have devised a machine learning algorithm that classiﬁes patterns of motion into
one of motor programs mentioned above. Without going into further details, we assume
that the sensor measurement trajectories—in a given temporal window—can be rep-
resented as the output of a linear time invariant system. We describe such set of
trajectories by a Hankel matrix, which embeds the observability matrix of the LTI
system generating the set of trajectories. The use of Hankel matrices avoids the burden
of performing system identiﬁcation while providing a computationally convenient
descriptor for the dynamics of a timeseries. For the recognition of actions, we use two
off-the-shelf classiﬁers: nearest neighbour (NN) and support vector machines (SVM),
in cross-subject validation. For further detail please refer to [9].
5
Conclusion
In this paper we have an active robotic prosthesis that enables people with an above- or
below-knee amputation to perform different types of motions that require power in
lower limb joints such as slope walking or stairs climbing. Our prototype, called Smart
Leg, combines advanced mechanical design with artiﬁcial intelligence-based adaptive
control solutions. While the former aims at maximizing the functionality of the pros-
thesis form the engineering point of view (power consumption, stability, etc.), the latter
is rather concerned with its usage. The adoption of machine learning algorithms cap-
able of learning and reproducing optimal motor patterns provides an increased comfort
for the end user and permits to customize the device to its particular needs.
References
1. Windrich, M., Grimmer, M., Christ, O., Rinderknecht, S., Beckerle, P.: Active lower limb
prosthetics: a systematic review of design issues and solutions. In: Robotics: Science and
Systems 28 Jun 2016. Berlin, Germany (2013)
2. Varol, H.A., Goldfarb, M.: Real-time Intent Recognition for a Powered Knee and Ankle
Transfemoral Prosthesis. ICORR 2007, pp. 16–23
3. Sup, F., Varol, H.A., Mitchell, J., Withrow T.J., Goldfarb, M.: Self-contained powered knee
and ankle prosthesis: initial evaluation on a transfemoral amputee. In: IEEE International
Conference on, Rehabilitation Robotics, ICORR 2009, vol. 1945–7898, pp. 638–644 (2009)
4. Brackx, B., van Damme, M., Matthys, A., Vanderborght, B., Lefeber, D.: Passive ankle-foot
prosthesis prototype with extended push-off. Int. J. Adv. Robot. Syst. 10(101) (2012)
5. Au, S., Berniker, M., Herr, H.: Powered ankle-foot prosthesis to assist level ground and
stair-descent gaits. Neural Netw. 21(4), 654–666 (2008)
6. Bionic Leg Article. http://www.dailymail.co.uk/sciencetech/article-2227785/Chicago-man-
ZacVawterclimbs-skyscraper-bionic-leg.html#ixzz2xGXJ47YW%29.
Accessed
12
Apr
2014
1198
H. Dindo et al.

7. Rupar, M., Dedic, R., Vucina, A.: Ankle joint design for the new hydraulic above the knee
prosthesis development. In: 3rd International Scientiﬁc Conference of Engineering, MAT
2014. Mostar, Bosnia and Herzegovina (2014)
8. Trevor, H., Tibshirani, R., Friedman, J.: The Elements of Statistical Learning: Data Mining,
Inference, and Prediction. Springer Series in Statistics, 2nd edn. (2009)
9. Dindo, H., Presti, L., La Cascia, M., Chella, A., Dedic, R.: Hankelet-based action
classiﬁcation for motor intention recognition. Submitted to Robotics and Autonomous
Systems (2016) (preprint available at request)
Smart LEG Control System Optimization
1199

Author Index
A
Ademovic, Naida, 681
Albinovic, Sanjin, 458, 691
Ateljevic, Jovo, 571
Avdaković, Samir, 3, 18, 203
B
Bajraktarević, Sena, 337
Bajramović, Zijad, 145, 433, 941
Balic, Emina, 724
Bandić, Lejla, 30
Begić, Aladin, 1056
Begić, Elma, 44, 409
Bosović, Adnan, 78
Bublin, Mehmed, 470
C
Čampara, Miralem, 1118
Carsimamovic, A., 941
Carsimamovic, S., 941
Catic, Tarik, 321
Causevic, Emina, 256
Čerimagić, Đenari, 449
Ciahotny, K., 1179
Čišija-Kobilica, Nejra, 18
Čošabić, Jasna, 751
Čučuković, Jasmina, 120
Čurić, Mirza, 299
D
Dacić-Lepara, Sabina, 170
Davidović, Marina, 645
Dedic, Remzo, 1189
Dedović, Maja Muftić, 203
Delalić, Nijaz, 1112
Delić-Zimić, Amina, 215, 223
Demir, Armin, 178, 189
Deville, M.O., 1179
Dindo, Haris, 1189
Dolarevic, Samir, 458
Drakulić, Una, 741
Dubravac, Muamer, 513
Đug, Mehmed, 848, 1066
Ðugum, Adnan, 1126
Đukić, Aleksandra, 484
Đumić, Dalibor, 833, 848
Džaferović, Ejub, 992
Džananović, Izet, 3
Dzebo, Suada, 711
Džumhur, Saša, 560, 671
E
Emra, Bujar, 503
Eydi, Ehsan, 249
F
Felić, Adnan, 768
Ferhatbegović, Šeila Gruhonjić, 3, 102
Ferizović, Husnija, 133
Ferizović, Silmija, 1074
G
Gadžo, Naida, 215
Ganibegović, Nedim, 1147
Glavić, Drazenko, 539
Grizić, Selena, 671
H
Hadžić, Emina, 592
Hadžić, Senad, 133
Hadžijahić, Nasiha, 178, 189
Hadzikadic, Mirsad, 307
Hadžimujović, Mirza, 503
Hadžimuratović, Semir, 133
Halač, Almin, 992
Hanjalić, Kemal, 1126
Harbaš, N., 1170
Hasečić, Amra, 992
© Springer International Publishing AG 2018
M. Hadžikadić and S. Avdaković (eds.), Advanced Technologies, Systems,
and Applications II, Lecture Notes in Networks and Systems 28,
https://doi.org/10.1007/978-3-319-71321-2
1201

Hebib-Albinovic, Mirna, 691
Hidić, Faruk, 120
Hivziefendić, Jasna, 30, 392
Hodžić, Migdat, 785, 876, 901
Hodzic, Mujo, 776
Hodžić, N., 1102
Hohzic, Mujo, 759
Hrasnica, Mustafa, 524, 633
Hrnjica, Sanela, 1044
Hubana, Tarik, 44, 55, 409
Husak, Ermin, 933, 1000
Husić, Kenan, 868
Husnić, Želimir, 1085, 1189
I
Ibrahimovic, Edin, 1137
Isanovic, Dalila, 234
Isić, Safet, 1037
J
Jahić, Admir, 89
Jareb, Edin, 433
Jelačić, Zlata, 1007
Jokić, Dejan, 860
Jusić, Alija, 145, 433
K
Kadić, Asad, 605
Kapetanović, Izudin, 392
Kapor, Mladen, 449
Karabegović, Edina, 1037
Karabegović, Isak, 979, 1000, 1026
Karić, Almin, 89
Kazagić, A., 1102, 1156
Kevrić, Jasmin, 299, 833, 848, 1066
Kljucanin, Slobodanka, 701
Konjhodžić, Edin, 265
Konjić, Tatjana, 89, 375
Kosarac, M., 941
Kovacevic, Ahmed, 933
Krnic, Medina, 282
Kulović, Ismet, 120
Kulovic, Sadeta, 329
Kuzmić, Tatjana, 645
L
Lavić, Zedina, 170
Lazović, Nerma, 592
Lešić, Mia, 375
Lišić, Sanjin, 819
Ljevo, Žanesa, 560, 671
Lonić, Sabina, 810
Lubura, Slobodan, 860
M
Macić, Dino, 67
Marić, Jasmina, 560
Martinez, Sanja, 967
Marusic, Dusan, 458
Mašić, Fatima, 1066
Maslo, Anis, 759
McNally, Ciaran, 724
Medić, Senad, 449, 605, 633, 661
Medjedovic, Dzejla, 249
Mehinović, Nerdina, 952
Mehremić, Semir, 1026
Mekic, Emina, 249
Merzić, A., 1156
Mešić, Amel, 1147
Metović, S., 1102
Micic, Ljubisa, 271
Mijic, Nikolina, 571
Milenković, Marina, 539
Milišić, Hata, 592
Milojković, Slobodan, 967
Morina, Naser, 479
Morin, Naser, 503
Muharemović, Adnan, 952
Muhasilović, M., 1179
Mujcic, Aljo, 776
Mujčić, Edin, 741, 768, 810
Mujezinović, Adnan, 941, 967
Mujović, Saša, 349
Mulaomorević-Šeta, Ajla, 592
Muminović, Mersa, 810
Musić, Mustafa, 78, 155, 1156
N
Namas, Tarik, 876, 901
Nawaz, Mohammad Asif, 307
Novaković, Ivo, 420
Nuhić, Jasna, 1066
P
Pantić, Vojislav, 133
Perko, Jurica, 363
Pjanić, Edin, 819
Pozder, Mirza, 549
R
Radic, Dejan, 243
Radosavac, Veselin, 271
Ramić-Brkić, Belma, 282, 321, 329, 337
Ramljak, Ivan, 363
Ramović, Aiša, 78
Rane, Sham, 933
Redžić, Elma, 1156, 155
1202
Author Index

S
Salkić, Hidajet, 952
Saric, Ammar, 549
Šarić, Mirza, 30, 44, 55, 67, 409
Sarkinovic, Predrag, 479
Šehagić, Rasim, 661
Selmanovic, Elmedin, 249
Šemić, Edin, 55, 1037
Serdarevic, Amra, 618
Šestić, Maksim, 484
Simonović, Dijana, 484
Širok, B., 1179
Skaljo, Edvin, 776
Skaljo, Evin, 759
Skejić, Adis, 449
Škrgić, Merisa, 741
Soﬁtć, Ferid, 751
Softić, Edis, 513
Suljanovic, Nermin, 776
T
Tahirović, Alma Ademović, 155
Talić, Zlatan, 513
Tarić, Mirsad, 503
Terzić, Lejla, 78
Tešanović, Majda, 392
Torlak, Muris, 155, 992, 1112
Trešnjo, Dino, 155
U
Udovićić, Mirna, 290
Uzunović, Tarik, 868
V
Vlahović, Ivan, 420
Vucina, Adisa, 1189
Vujičić, Tijana, 484
Vujošević, Snežana, 349
Z
Zaimović, Ramiz, 1118
Zalihić, Suad, 1118
Živalj, Edhem, 605
Zlatar, Muhamed, 605
Zlomušica, Elvir, 1118
Znidarec, Matej, 363
Author Index
1203


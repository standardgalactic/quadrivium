1 
Reinforcement Learning:  
Predicting Rewards 
Image Source: Wikimedia Commons 
2 
Reinforcement Learning 
Agent 
Environment 
State 
ut 
Reward 
rt 
Action 
at 
Image Source: Wikimedia Commons 

3 
Early Results: Pavlov and his Dog 
F Classical (Pavlovian) 
conditioning experiments  
F Training: Bell Food 
F After: Bell  Salivate 
F Conditioned stimulus 
(bell) predicts future 
reward (food) 
Image: Wikimedia Commons; Animation: Tom Creed, SJU 
4 
Predicting Delayed Rewards 
F How do we predict rewards delivered some time after a 
stimulus is presented? 
F Given: Many trials, each of length T time steps 
F Time within a trial: 0  t  T with stimulus u(t) and reward 
r(t) at each time step t  (Note: r(t) can be zero for some t) 
F We would like a neuron whose output v(t) predicts the 
expected total future reward starting from time t 
trials
t
T
t
r
t
v





0
)
(
)
(



5 
Learning to Predict Future Rewards 
F Use a set of synaptic weights w(t) and predict 
based on all past stimuli u(t): 
 
 
F Learn weights w() that minimize error: 
 
 
)
(
)
(
)
(
0






t
u
w
t
v
t
2
0
)
(
)
(











t
v
t
r
t
T


Yes, BUT future rewards are not yet available! 
(Can we minimize this using 
gradient descent and delta rule?) 
)
(t
v
)
(t
u
)1
( 
t
u
)
0
(
u
)
0
(
w
)
(t
w
)
(T
w
(Linear filter!) 
6 
Temporal Difference (TD) Learning 
F Key Idea: Rewrite error function to get rid of future terms: 
 
 
 
F Temporal Difference (TD) Learning: 


2
2
1
0
2
0
)
(
)1
(
)
(
)
(
)
1
(
)
(
)
(
)
(
t
v
t
v
t
r
t
v
t
r
t
r
t
v
t
r
t
T
t
T


































)
(
 
)]
(
)1
(
)
(
[ 
)
(









t
u
t
v
t
v
t
r
w
Expected future reward 
Prediction 
 
Minimize this using 
gradient descent! 

7 
Predicting Future Rewards: TD Learning 
Stimulus at t = 100 and reward at t = 200 
Prediction error  for each time step 
(over many trials) 
Image Source: Dayan & Abbott textbook 
8 
Possible Reward Prediction Error Signal in the 
Primate Brain 
Dopaminergic cells in Ventral Tegmental Area (VTA) 
Before Training 
 
 
After Training 
Reward Prediction error? 
No error 
)]
(
)1
(
)
(
[
t
v
t
v
t
r



)1
(
)
(
)
(



t
v
t
r
t
v
)]
1
(
)
(
0
[



t
v
t
v
0
)]
(
)1
(
)
(
[




t
v
t
v
t
r
Image Source: Dayan & Abbott textbook 

9 
More Evidence for Prediction Error Signals 
Dopaminergic cells in VTA after Training 
Negative error 
)
(
)]
(
)1
(
 
)
(
[
0
)1
(
 ,0
)
(
t
v
t
v
t
v
t
r
t
v
t
r








Reward predicted 
but not delivered 
Image Source: Dayan & Abbott textbook 
10 
Next:  
Reinforcement Learning: Acting to Maximize Rewards 
Agent 
Environment 
State 
ut 
Reward 
rt 
Action 
at 


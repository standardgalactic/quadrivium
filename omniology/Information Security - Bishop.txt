Matt Bishop
Anderson C.A. Nascimento (Eds.)
 123
LNCS 9866
19th International Conference, ISC 2016
Honolulu, HI, USA, September 3–6, 2016
Proceedings
Information Security

Lecture Notes in Computer Science
9866
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, Lancaster, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Friedemann Mattern
ETH Zurich, Zurich, Switzerland
John C. Mitchell
Stanford University, Stanford, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
TU Dortmund University, Dortmund, Germany
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Gerhard Weikum
Max Planck Institute for Informatics, Saarbrücken, Germany

More information about this series at http://www.springer.com/series/7410

Matt Bishop
• Anderson C.A. Nascimento (Eds.)
Information Security
19th International Conference, ISC 2016
Honolulu, HI, USA, September 3–6, 2016
Proceedings
123

Editors
Matt Bishop
University of California
Davies, CA
USA
Anderson C.A. Nascimento
University of Washington
Tacoma, WA
USA
ISSN 0302-9743
ISSN 1611-3349
(electronic)
Lecture Notes in Computer Science
ISBN 978-3-319-45870-0
ISBN 978-3-319-45871-7
(eBook)
DOI 10.1007/978-3-319-45871-7
Library of Congress Control Number: 2016949632
LNCS Sublibrary: SL4 – Security and Cryptology
© Springer International Publishing Switzerland 2016
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the
material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now
known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors
give a warranty, express or implied, with respect to the material contained herein or for any errors or
omissions that may have been made.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG Switzerland

Preface
This volume contains the papers presented at the 19th Annual International Conference
on Information Security (ISC 2016) held on September 3–6, 2016, in Honolulu,
Hawaii, USA.
Held annually, the Information Security Conference is an international conference
covering research in the theory and applications of information security. It started as a
workshop in 1997 and became a conference in 2001. It has been held on ﬁve different
continents.
Rui Zhang from the University of Hawaii, USA and Masahiro Mambo, from Kana-
zawa University, Japan organized ISC 2016, in cooperation with the International
Association for Cryptologic Research.
Submissions to ISC 2016 were required to be anonymous. The 76 submitted papers
were reviewed using a double-blind process. Selecting the ﬁnal accepted papers was a
difﬁcult task that demanded many rounds of interaction. As a result, the Program
Committee accepted 19 full papers (resulting in an acceptance rate of 25 %) and 7 short
papers. We would like to thank the Program Committee for their valuable work.
We also thank Rui Zhang and Masahiro Mambo for the superb work they did
organizing the conference. We are grateful to the ISC Steering Committee for their
support, to Depeng Li for handling the local arrangements, and the creators and
maintainers of Easychair, the superb platform we used to manage the review process.
Finally, we would like to thank all the authors who submitted papers to ISC 2016.
They are the main reason for the success of this conference.
September 2016
Matt Bishop
Anderson C.A. Nascimento

Organization
Information Security Conference 2016, Honolulu, Hawaii, USA, September 7–9, 2016
General Chairs
Masahiro Mambo
Kanazawa University, Japan
Rui Zhang
University of Hawaii, USA
Program Chairs
Matt Bishop
University of California, Davis, USA
Anderson C.A. Nascimento
University of Washington, Tacoma, USA
Local Arrangements
Depeng Li
University of Hawaii, USA
Steering Committee
Colin Boyd
Norwegian University of Science and Technology,
Norway
Ed Dawson
Queensland University of Technology, Australia
Javier Lopez
University of Malaga, Spain
Masahiro Mambo
Kanazawa University, Japan
Eiji Okamoto
University of Tsukuba, Japan
Susanne Wetzel
Stevens Institute of Technology, USA
Rui Zhang
University of Hawaii, USA
Yuliang Zheng
University of Alabama at Birmingham, USA
Program Committee
Gail-Joon Ahn
Arizona State University, USA
Luis Antunes
University of Porto, Portugal
Diego Aranha
State University of Campinas, Brazil
Claudio Ardagna
Università degli Studi di Milano, Italy
Elias Athanasopoulos
Vrije Universiteit Amsterdam, The Netherlands
Nuttapong Attrapadung
AIST, Japan
Tuomas Aura
Aalto University, Finland
Elisa Bertino
Purdue University, USA
Alex Biryukov
University of Luxembourg, Luxembourg
Matt Bishop
UC Davis, USA
Liqun Chen
Hewlett Packard Labs, USA
Sherman S.M. Chow
Chinese University of Hong Kong, Hong Kong, China

Bruno Crispo
University of Trento, Italy
Paolo D’Arco
University of Salerno, Italy
Naccache David
Ecole Normale Supérieure, France
Yvo Desmedt
The University of Texas at Dallas, USA
Roberto Di Pietro
Bell Labs, USA
Josep Domingo-Ferrer
Universitat Rovira i Virgili, Spain
Christian Gehrmann
Swedish Institute of Computer Science, Sweden
Dieter Gollmann
Hamburg University of Technology, Germany
Stefanos Gritzalis
University of the Aegean, Greece
Jong Guo
Shandong University, China
Xinyi Huang
Fujian Normal University, China
Kangkook Jee
Columbia University, USA
Sokratis Katsikas
Center for Cyber and Information Security, NTNU,
Norway
Stefan Katzenbeisser
TU Darmstadt, Germany
Vasileios Kermelis
Brown University, USA
Helger Lipmaa
University of Tartu, Estonia
Yao Liu
University of South Florida, USA
Patrick Longa
Microsoft Research, USA
Di Ma
University of Michigan-Dearborn, USA
Masahiro Mambo
Kanazawa University, Japan
Mark Manulis
University of Surrey, UK
Fabio Martinelli
IIT-CNR, Italy
Catherine Meadows
NRL, USA
Jean-Francois Misarsky
Orange Labs, France
Reﬁk Molva
EURECOM, France
Yi Mu
University of Wollongong, Australia
Anderson Nascimento
University of Washington, USA
Miyako Ohkubo
NICT, Japan
Eiji Okamoto
University of Tsukuba, Japan
Charalampos Papamanthou
University of Maryland, College Park, USA
Josef Pieprzyk
Queensland University of Technology, Australia
Michael Polychronakis
Stony Brook University, USA
Joachim Posegga
University of Passau, Germany
Bart Preneel
Katholieke Universiteit Leuven, Belgium
Indrajit Ray
Colorado State University, USA
Kui Ren
State University of New York at Buffalo, USA
Rei Safavi-Naini
University of Calgary, Canada
Pierangela Samarati
Università degli Studi di Milano, Italy
Miguel Soriano
Universitat Politècnica de Catalunya, Spain
Claudio Soriente
Telefonica Research and Development, Spain
Martijn Stam
University of Bristol, UK
Vijay Varadharajan
Macquarie University, Australia
Guilin Wang
Huawei International Pte Ltd, Singapore
Susanne Wetzel
Stevens Institute of Technology, USA
Stephen Wolthusen
Royal Holloway, University of London, UK
VIII
Organization

Junjie Zhang
Wright State University, USA
Rui Zhang
University of Hawaii, USA
Ziming Zhao
Arizona State University, USA
Jianying Zhou
Institute for Infocomm Research, Singapore
Organization
IX

Contents
Cryptanalysis
Truncated and Multiple Differential Cryptanalysis of Reduced
Round Midori128 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Mohamed Tolba, Ahmed Abdelkhalek, and Amr M. Youssef
Improved Linear Cryptanalysis of Round-Reduced ARIA. . . . . . . . . . . . . . .
18
Ahmed Abdelkhalek, Mohamed Tolba, and Amr M. Youssef
Partial Key Exposure Attacks on CRT-RSA: General Improvement
for the Exposed Least Significant Bits . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
Atsushi Takayasu and Noboru Kunihiro
Cryptanalysis and Improved Construction of a Group Key Agreement
for Secure Group Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
Jun Xu, Lei Hu, Xiaona Zhang, Liqiang Peng, and Zhangjie Huang
Enhanced Correlation Power Analysis by Biasing Power Traces . . . . . . . . . .
59
Changhai Ou, Zhu Wang, Degang Sun, Xinping Zhou, Juan Ai,
and Na Pang
Damaging, Simplifying, and Salvaging p-OMD . . . . . . . . . . . . . . . . . . . . .
73
Tomer Ashur and Bart Mennink
Cryptographic Protocols
Blind Password Registration for Two-Server Password Authenticated
Key Exchange and Secret Sharing Protocols. . . . . . . . . . . . . . . . . . . . . . . .
95
Franziskus Kiefer and Mark Manulis
Chip Authentication for E-Passports: PACE with Chip Authentication
Mapping v2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
Lucjan Hanzlik and Mirosław Kutyłowski
AEP-M: Practical Anonymous E-Payment for Mobile Devices
Using ARM TrustZone and Divisible E-Cash . . . . . . . . . . . . . . . . . . . . . . .
130
Bo Yang, Kang Yang, Zhenfeng Zhang, Yu Qin, and Dengguo Feng
Universally Composable Two-Server PAKE . . . . . . . . . . . . . . . . . . . . . . . .
147
Franziskus Kiefer and Mark Manulis
Yet Another Note on Block Withholding Attack on Bitcoin Mining Pools . . .
167
Samiran Bag and Kouichi Sakurai

Network and Systems Security and Access Control
Cyber Security Risk Assessment of a DDoS Attack. . . . . . . . . . . . . . . . . . .
183
Gaute Wangen, Andrii Shalaginov, and Christoffer Hallstensen
Moving Target Defense Against Network Reconnaissance with Software
Defined Networking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
203
Li Wang and Dinghao Wu
Uni-ARBAC: A Unified Administrative Model for Role-Based
Access Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
218
Prosunjit Biswas, Ravi Sandhu, and Ram Krishnan
SKALD: A Scalable Architecture for Feature Extraction, Multi-user
Analysis, and Real-Time Information Sharing. . . . . . . . . . . . . . . . . . . . . . .
231
George D. Webster, Zachary D. Hanif, Andre L.P. Ludwig,
Tamas K. Lengyel, Apostolis Zarras, and Claudia Eckert
Privacy and Watermarking
Leveraging Internet Services to Evade Censorship. . . . . . . . . . . . . . . . . . . .
253
Apostolis Zarras
Analyzing Randomized Response Mechanisms Under Differential Privacy . . .
271
Atsushi Waseda and Ryo Nojima
Models and Algorithms for Graph Watermarking . . . . . . . . . . . . . . . . . . . .
283
David Eppstein, Michael T. Goodrich, Jenny Lam, Nil Mamano,
Michael Mitzenmacher, and Manuel Torres
Software Security
Policy-Based Implicit Attestation for Microkernel-Based
Virtualized Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
305
Steffen Wagner and Claudia Eckert
Generalized Dynamic Opaque Predicates: A New Control Flow
Obfuscation Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
323
Dongpeng Xu, Jiang Ming, and Dinghao Wu
A Bayesian Cogntive Approach to Quantifying Software Exploitability
Based on Reachability Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
343
Guanhua Yan, Yunus Kucuk, Max Slocum, and David C. Last
Control Flow Integrity Enforcement with Dynamic Code Optimization . . . . .
366
Yan Lin, Xiaoxiao Tang, Debin Gao, and Jianming Fu
XII
Contents

Encryption, Signatures and Fundamentals
Impossibility on the Provable Security of the Fiat-Shamir-Type Signatures
in the Non-programmable Random Oracle Model . . . . . . . . . . . . . . . . . . . .
389
Masayuki Fukumitsu and Shingo Hasegawa
Efficient Functional Encryption for Inner-Product Values
with Full-Hiding Security. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
408
Junichi Tomida, Masayuki Abe, and Tatsuaki Okamoto
MQSAS - A Multivariate Sequential Aggregate Signature Scheme . . . . . . . .
426
Rachid El Bansarkhani, Mohamed Saied Emam Mohamed,
and Albrecht Petzoldt
Cryptanalysis of Multi-Prime U-Hiding Assumption . . . . . . . . . . . . . . . . . .
440
Jun Xu, Lei Hu, Santanu Sarkar, Xiaona Zhang, Zhangjie Huang,
and Liqiang Peng
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
455
Contents
XIII

Cryptanalysis

Truncated and Multiple Diﬀerential
Cryptanalysis of Reduced Round Midori128
Mohamed Tolba, Ahmed Abdelkhalek, and Amr M. Youssef(B)
Concordia Institute for Information Systems Engineering,
Concordia University, Montr´eal, Queb´ec, Canada
youssef@ciise.concordia.ca
Abstract. Midori is a family of SPN-based lightweight block ciphers
designed to optimize the hardware energy consumption per bit during
the encryption and decryption operations. At ASIACRYPT 2015, two
variants of the cipher, namely Midori128 and Midori64, which support
a 128-bit secret key and a 64/128-bit block, respectively, were proposed.
Recently, a meet-in-the-middle attack and an invariant subspace attack
were presented against Midori64 but both attacks cannot be applied to
Midori128. In this paper, we present truncated and multiple diﬀerential
cryptanalysis of round reduced Midori128. Our analysis utilizes the spe-
cial structure of the S-boxes and binary linear transformation layer in
order to minimize the number of active S-boxes. In particular, we con-
sider diﬀerentials that contain only single bit diﬀerences in the input and
output of the active S-boxes. To keep this single bit per S-box patterns
after the MixColumn operation, we restrict the bit diﬀerences of the
output of the active S-boxes, which lie in the same column after the
shuﬄe operation, to be in the same position. Using these restrictions,
we were able to ﬁnd 10-round diﬀerential which holds with probability
2−118. By adding two rounds above and one round below this diﬀerential,
we obtain a 13 round truncated diﬀerential and use it to perform a key
recovery attack on the 13-round reduced Midori128. The time and data
complexities of the 13-round attack are 2119 encryptions and 2119 cho-
sen plaintext, respectively. We also present a multiple diﬀerential attack
on the 13-round Midori128, with time and data complexities of 2125.7
encryptions and 2115.7 chosen plaintext, respectively.
Keywords: Truncated diﬀerential cryptanalysis · Midori128 · Multiple
diﬀerential cryptanalysis
1
Introduction
Over the past few years, many lightweight block ciphers such as HIGHT [6],
mCrypton [10], DESL/DESXL [9], PRESENT [2], KATAN/KTANTAN [3], Pic-
colo [12] and PRINTcipher [7] were proposed. On the other hand, there has been
little work that focuses on determining the design choices that lead to the most
energy eﬃcient architecture. While power and energy are clearly correlated, opti-
mizing the power consumption for block ciphers does not necessarily lead to the
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 3–17, 2016.
DOI: 10.1007/978-3-319-45871-7 1

4
M. Tolba et al.
most energy eﬃcient designs since a low power optimized cipher may have high
latency, i.e., it takes longer to perform the encryption and decryption operations
and hence the required energy increases. In other words, there is no guarantee
that low power block cipher designs would lead to low energy designs and vice
versa.
By identifying some design choices that are energy eﬃcient and by choosing
components speciﬁcally tailored to meet the requirements of low energy design,
at ASIACRYPT 2015, Banik et al. [1] proposed an SPN-based lightweight block
cipher, Midori. In particular, Midori is designed to optimize the hardware energy
consumption per bit during the encryption and decryption operations. Two vari-
ants of the cipher, namely, Midori128 and Midori64 which support a 128-bit
secret key and a 64/128-bit block, respectively, were proposed. The linear and
non-linear operations of both versions were selected to optimize this objective.
The state in Midori is represented as 4 × 4 matrix, where the size of each
cell depends on the version of cipher, e.g., the cell size in Midori128 is 8 bits.
Midori uses 4 × 4 almost MDS binary matrix because, compared to other MDS
matrices, this almost MDS matrix is more eﬃcient in terms of area and signal-
delay. To compensate for the low branch number of the almost MDS matrix
(4 as compared to 5 in the case of MDS), the designers utilized an optimal
cell-permutation layer in order to improve the diﬀusion speed and increase the
number of active S-boxes.
Recently, Midori64 has been analyzed by two diﬀerent techniques. The ﬁrst
is a meet-in-the-middle with diﬀerential enumeration and key dependent sieving
[11] which attacks 11 rounds (resp. 12 rounds) with time, memory, and data
complexities of 2122 (resp. 2125.5) encryptions, 289.2 (resp. 2106) 64-bit blocks,
and 253 (resp. 255.5) chosen plaintext. The second is an invariant subspace attack
[5] against the full cipher. The latter attack proves that the security margin of
Midori64 is 96 bits instead of 128 bits. Both of these attacks are not applicable
to Midori128.
In this paper, we present truncated diﬀerential cryptanalysis of round reduced
Midori128. Our attack utilizes the following two observations. First, Midori128
uses four diﬀerent 8-bit S-boxes, namely SSb0, SSb1, SSb2 and SSb3, where each
one is composed of two 4-bit S-boxes Sb1 in addition to input and output bit
permutation. Consequently, in order to minimize the number of active S-boxes,
we consider only single bit diﬀerences (i.e., 1, 2, 4, 8, 16, 32, 64, 128) in the input
and output of the 8-bit S-boxes. Second, given the binary nature of the almost
MDS transformation, and the fact that the Hamming weight of each row is 3, it
follows that the active bytes in a column after the MixColumn operation have
a single bit diﬀerence if and only if the active bytes in the input column are all
equal and each one has a single active bit. Hence, to maintain the pattern of single
bit diﬀerences after the MixColumn operation, we restrict the bit diﬀerences of
the output of the active S-boxes, which lie in the same column after the shuﬄe
operation, to be in the same position.

Truncated and Multiple Diﬀerential Cryptanalysis
5
Based on these observations, we are able to ﬁnd a 10-round diﬀerential
that holds with probability 2−118. Then, we added two rounds above and one
round below this 10-round diﬀerential to obtain a 13-round truncated diﬀerential
[8] that holds with probability 2−230. Using this truncated diﬀerential, we can
recover the master key of the 13-round reduced cipher with time and data com-
plexities of 2119 encryptions, and 2119 chosen plaintext, respectively. We also
present a multiple diﬀerential attack [4] on the 13-round reduced cipher with
time and data complexities of 2125.7 encryptions and 2115.7 chosen plaintext,
respectively.
The rest of the paper is organized as follows. In Sect. 2, we provide the
notations used throughout the paper and a brief description of Midori128. In
Sect. 3, we describe and analyze the algorithm we use to eﬃciently search for
long diﬀerentials with small number of active S-boxes. Details of our truncated
diﬀerential attack on Midori128 reduced to 13 rounds are presented in Sect. 4.
Section 5 presents our multiple diﬀerential attack. Finally, the paper is concluded
in Sect. 6.
2
Speciﬁcations of Midori128
2.1
Notations
The following notations are used throughout the rest of the paper:
– at: Transposition of the vector or the matrix a.
– K: The master key.
– RKi: The 128-bit round key used in round i.
– WK: The 128-bit whitening key.
– xi: The 128-bit input to the SubCell operation at round i.
– yi: The 128-bit input to the ShuﬄeCell operation at round i.
– zi: The 128-bit input to the MixColumn operation at round i.
– wi: The 128-bit input to the KeyAdd operation at round i.
– xi[j]: The jth byte of xi, where 0 ≤j < 16.
– xi[j · · · l]: The bytes from j to l of xi, where j < l.
– xi[j, l]: The bytes j and l of xi.
– Δxi, Δxi[j]: The diﬀerence at state xi and byte xi[j], respectively.
2.2
Speciﬁcations
Midori128 can be considered as a variant of Substitution Permutation Networks
(SPNs). The state in Midori128 is represented as a 4×4 array of bytes as follow:
S =
⎛
⎜
⎜
⎝
s0 s4 s8 s12
s1 s5 s9 s13
s2 s6 s10 s14
s3 s7 s11 s15
⎞
⎟
⎟
⎠

6
M. Tolba et al.
Midori128 iterates over 20 rounds. Each round, except the last one, has 3
layers: S-layer (SubCell) which maps {0, 1}128 →{0, 1}128, P-layer (ShuﬄeCell
and MixColumn) which maps {0, 1}128 →{0, 1}128 and a key-addition layer
(KeyAdd) which maps {0, 1}128 × {0, 1}128 →{0, 1}128. The last round contains
only the S-layer. Moreover, before the ﬁrst and after the last rounds, prewhiten-
ing and postwhitening are performed using WK. In what follows, we show how
these operations update the 128-bit state S:
– SubCell: A nonlinear layer applies 4 8-bit S-boxes, namely SSb0, SSb1, SSb2,
and SSb3, on each byte of the state S in parallel, where si ←SSb(i mod 4) [si]
and 0 ≤i ≤15. As shown in Fig. 1, each 8-bit S-box SSbi is composed of input
and output bit permutations and 2 4-bit S-box Sb1, where Sb1 is 4-bit S-box
(see Table 1).
– ShuﬄeCell: The bytes of the state S is permuted as follow: (s0, s1, · · · , s15) ←
(s0, s10, s5, s15, s14, s4, s11, s1, s9, s3, s12, s6, s7, s13, s2, s8)
– MixColumn: Each column in the internal state is multiplied by a binary matrix
M, where
M =
⎛
⎜
⎜
⎝
0 1 1 1
1 0 1 1
1 1 0 1
1 1 1 0
⎞
⎟
⎟
⎠
Hence, the internal state is updated as follows:
(si, si+1, si+2, si+3)t ←M(si, si+1, si+2, si+3)t, i = 0, 4, 8, 12.
– KeyAdd: Where 128-bit round key RKi is XORed with the state S.
Table 1. 4-bit bijective S-box Sb1 in hexadecimal form [1]
x
0 1 2 3 4 5 6 7 8 9 a b c d e f
Sb1[x] 1 0 5 3 e 2 f
7 d a 9 b c 8 4 6
Fig. 1. SSb0, SSb1, SSb2, and SSb3 [1]

Truncated and Multiple Diﬀerential Cryptanalysis
7
The data encryption procedure of Midori128 is illustrated in Algorithm 1
where R = 20 denotes the number of rounds.
Algorithm 1. Data Encryption Algorithm 1
Data: X,WK, RK0, ..., RKR−2
Result: Y
S ←KeyAdd(X,WK);
for i ←0 to R −2 do
S ←SubCell(S);
S ←ShuﬄeCell(S);
S ←MixColumn(S);
S ←KeyAdd(S, RKi);
S ←SubCell(S);
Y ←KeyAdd(S, WK);
The prewhitening and postwhitening key WK in Midori128 are equal to the
master key K, the rounds keys RKi = K ⊕βi, 0 ≤i ≤18, where βi is a constant,
X is the plaintext, and Y is the ciphertext. Throughout our analysis, we measure
the time complexity of our attack in terms of the equivalent number of reduced-
round Midori128 encryptions. For further details about the design rational of
the cipher, the reader is referred to [1].
3
A 10-round Diﬀerential of Midori128
As mentioned above, in order to minimize the number of active S-boxes, we con-
sider diﬀerentials that have only single bit diﬀerences in the input and output of
the active 8-bit S-boxes. In this section, we describe and analyze the algorithm
we use to eﬃciently ﬁnd such diﬀerentials whose probabilities are greater than
2−128. In each round, we have 4 operations. The operations that can disturb the
single bit diﬀerence propagation patterns are the SubCell and MixColumn opera-
tions. From the structure of the S-boxes, it follows that for any active S-box and
for a given 1-bit input diﬀerence, there are at most 4 possible output diﬀerences
of 1-bit diﬀerence because only 1 4-bit S-box will be active. Furthermore, from
the properties of the binary almost MDS matrix, preserving single bit diﬀerences
propagation patterns requires that we restrict the bit diﬀerences of the output
of the active S-boxes, which lie in the same column after the shuﬄe operation,
to be in the same position. As a result, the active bytes in each column after the
MixColumn operation have the same value. Therefore, at each round we have
at most (8 × 15)4 ≈228 possible input diﬀerences. The term 8 in the previous
formula denotes the number of possible values of the diﬀerence in each column
(1, 2, 4, · · · , 128). The term 15 denotes the total number of combinations for
active bytes within each column and the exponent 4 denotes the total number
of columns. As noted above, for a given input diﬀerence ΔSi at the beginning
of each round, after the S-box layer the values of the active bytes, which lie in

8
M. Tolba et al.
the same column after the shuﬄe operation, should be equal. Therefore, for each
input diﬀerence i, we have a set Ωi which contains at most 44 possible output
diﬀerences (in each column we have at most four 1-bit diﬀerences, and we can
have at most four active columns.)
Algorithm 2 describes the procedure used to ﬁnd the maximum number of
rounds r, such that a diﬀerential with the above S-box propagation patterns
exists and its diﬀerential probability is greater than 2−128. The algorithm run
time is upper bounded by 228 × 44 × r. It utilizes four tables where each table
has 228 entries corresponding to the possible input diﬀerences at each round. In
what follows we describe the use of each table:
1. Each entry i in the table InputDiﬀProb indicates the probability to reach the
diﬀerence i at the beginning of the considered round.
2. Each entry i in the table OutputDiﬀProb indicates the probability to reach
the diﬀerence i at the end of the considered round.
3. Each entry i in the table InputParent indicates the input diﬀerence used at the
beginning of round 0 to reach diﬀerence i at the beginning of the considered
round. The value −1 is used to indicate that the diﬀerence i cannot be reached
from any input diﬀerence.
4. Each entry i in the table OutputParent indicates the input diﬀerence used at
the beginning of round 0 to reach diﬀerence i at the end of the considered
round. The value −1 indicates that the diﬀerence i cannot be reached from
any input diﬀerence.
As explained in Algorithm 2, we iterate over the input diﬀerences that have
diﬀerential probability > 0 round by round, i.e., at each round we propagate
all the input diﬀerences and begin with the obtained diﬀerences as input to
the next round. In our implementation, the 28-bit index i encodes the state
diﬀerence ΔS as follows: we use 7 bits for each one of the four columns where
these 7 bits are divided into two parts. The ﬁrst 3 bits represent the value
of the diﬀerence of the active bytes in the column and the remaining 4 bits
represent the diﬀerent 15 combinations of the active bytes within the column.
After applying the 3 operations: SubCell, ShuﬄeCell, and MixColumn, we have
3 cases: (i) this diﬀerence did not appear before (Outparent[j] = −1). In this
case we set its probability and parent, (ii) this diﬀerence appeared previously
and its previous parent is the same as the new parent. Therefore, we add the
probabilities, and (iii) this diﬀerence appears previously but with a diﬀerent
parent. In this case, we choose the parent with the higher probability. At the
end of each round, we copy the OutputDiﬀProb and OutParent into InputDiﬀProb
and InputParent, respectively, and initialize OutputDiﬀProb and OutParent to
begin another round.
Using a PC with Intel(R) Xeon(R) CPU E3-1280 V2 @ 3.6 GHz and 32
GB RAM, a non-optimized implementation of Algorithm 2 terminates in about
3 hours and outputs r = 10 rounds. A 10-round diﬀerential which holds with

Truncated and Multiple Diﬀerential Cryptanalysis
9
probability 2−118 is illustrated in Fig. 2. This 10-round diﬀerential has 2554
characteristics that are distributed as shown in Table 2. The characteristic that
holds with probability 2−123 is detailed in Table 3.
Fig. 2. A 10 rounds diﬀerential of Midori128
Table 2. The characteristics distribution of the 10-round diﬀerential
# of characteristics Probability of each characteristic
1
2−123
7
2−124
23
2−125
50
2−126
83
2−127
2390
≤2−128
4
13-round Truncated Diﬀerential Cryptanalysis of
Midori128
In this section, we show how we can extend the diﬀerential obtained in the pre-
vious section by two rounds above, and one round below, to obtain a truncated
diﬀerential over 13 rounds (see Fig. 3). Then, we present a key recovery attack
on Midori128 reduced to 13 rounds using this truncated diﬀerential.
The total probability of the 13-round diﬀerential can be calculated from its
three parts. First, the diﬀerential probability of the top two rounds which is
2−112 and can be computed as follows: (i) 4 →2, 3 →1, 4 →2, and 3 →1
transitions over MixColumn (z0 →w0) which happens with probability 2−16 ×
2−16 × 2−16 × 2−16 = 2−64, (ii) 3 →1 and 3 →1 transitions over MixColumn
(z1 →w1) which happens with probability 2−16 × 2−16 = 2−32 and (iii) w1[5]

10
M. Tolba et al.
and w1[10] equal diﬀerence 16 which happens with probability 2−8×2−8 = 2−16.
Second, the diﬀerential probability of 10-round is 2−118, calculated by Algorithm
2. Third, the bottom 1 round has a diﬀerential probability equals to 1. Therefore,
the diﬀerential probability of the 13-round is 2−230.
Table 3. The 2−123 10-round characteristic of Midori128
i
Input diﬀerence at round i (in hexadecimal)
0
00000000 00100000 00001000 00000000
1
00000000 00000000 00000000 00080800
2
40004040 00000000 00101010 00000000
3
40400040 40400000 08080008 08080000
4
00400040 00000040 08000000 08000800
5
40000040 00404000 08000008 00000000
6
40404000 00400040 08080008 08000800
7
00400000 40004000 00080000 08000800
8
00000000 00400000 00000800 00000000
9
00000000 00000000 00000000 00080800
10 40004040 00000000 00101010 00000000
In what follows, we show how we can perform our key recovery attack on
Midori128 reduced to 13 rounds using the above diﬀerential. The attack is decom-
posed of two steps. The ﬁrst step is the data collection in which we collect many
pairs of messages to guarantee that at least one of them conﬁrms to the 13-round
truncated diﬀerential in Fig. 3. The second step is the key recovery in which the
collected data pairs are used to identify the key candidates.
Proposition 1. (Diﬀerential Property of bijective S-boxes) Given two non-zero
diﬀerences, Δi and Δo, in F256, the equation: S(x) + S(x + Δi) = Δo has one
solution on average. This property also applies to S−1.
Data Collection. To reduce the number of required chosen plaintext and get
enough pairs to launch the attack, we use the structure technique. Here, our
structure takes all the possible values in all the bytes except bytes 2 and 11. These
bytes take ﬁxed value. Therefore, one structure generates 214×8×(214×8−1)/2 ≈
2223 possible pairs. We need to collect 2230 message pairs because the total prob-
ability of the 13-round diﬀerential is 2−230. Since each structure contains 2223
message pairs, we need to collect 27 structures to ﬁnd the right pair. Therefore,
we ask the encryption oracle for the encryption of 2119 messages.

Truncated and Multiple Diﬀerential Cryptanalysis
11
Fig. 3. 13-round truncated diﬀerential of Midori128

12
M. Tolba et al.
Algorithm 2. Find the maximum number of rounds, r, that has a diﬀer-
ential which holds with probability > 2−128 considering only the single bit
diﬀerence for the active S-boxes
Result: X: Input diﬀerence, Y : Output diﬀerence, Probability: The
probability of the diﬀerential, r: The number of rounds covered
by the diﬀerential
for i ←0 to 228 −1 do
InputDiﬀProb[i] ←1 ,OutputDiﬀProb[i] ←0;
InputParent[i] ←i ,OutputParent[i] ←−1;
r ←0, valid←true;
while valid do
for i ←0 to 228 −1 do
if InputDiﬀProb[i] = 0 then
continue;
map i to state ΔS (see Sect. 3);
forall the entries in Ωi do
ΔStemp ←nextvalue(Ωi);
prob ←Probability (ΔS →ΔStemp);
if prob = 0 then
continue;
ΔStemp ←ShuﬄeCell(ΔStemp);
ΔStemp ←MixColumn(ΔStemp);
map ΔStemp into index j;
if OutParent[j] = −1 then
OutputDiﬀProb[j] = InputDiﬀProb[i]× prob;
OutputParent[j] = InputParent[i];
else if OutParent[j] = InputParent[i] then
OutputDiﬀProb[j] = InputDiﬀProb[i]× prob +
OutputDiﬀProb[j];
else
if InputDiﬀProb[i]× prob > OutputDiﬀProb[j] then
OutputDiﬀProb[j] = InputDiﬀProb[i]× prob;
OutputParent[j] = InputParent[i];
InputDiﬀProb ←OutputDiﬀProb, InputParent ←OutParent;
initialize OutputDiﬀProb to 0 , initialize OutputParent to -1;
get the index l of the maximum entry in InputDiﬀProb;
if InputDiﬀProb[l] <= 2−128 then
valid ←false;
r ←r + 1;
Get the index l of the maximum entry in InputDiﬀProb;
X ←InputParent[l] , Y ←l ,Probability ←InputDiﬀProb[l];

Truncated and Multiple Diﬀerential Cryptanalysis
13
Key Recovery. In this step, we try to identify the key candidates that conﬁrm
to the 10-round diﬀerential. First, we try to identify the number of key sugges-
tions of 26 bytes WK[0, 1, 3 · · · 10, 12 · · · 15], RK0[1, 3, 6, 9, 11, 14], and WK[4, 5, 6
, 12, 13, 15] that correspond to each pair of messages. This can be achieved as
follows: to deduce the values of the 6 bytes WK[4, 5, 6, 12, 13, 15], we know that
the 6 bytes Δx12[4, 5, 6, 12, 13, 15] only take one value, the same diﬀerence of the
end of the 10-round diﬀerential since the key add layer does not change the diﬀer-
ence. The knowledge of the ciphertext allows to compute Δy12[4, 5, 6, 12, 13, 15].
Using the diﬀerential property of the S-box, we can evaluate y12[4, 5, 6, 12, 13, 15].
The knowledge of the ciphertext with y12[4, 5, 6, 12, 13, 15] allows us to deduce
the values of the 6 bytes WK[4, 5, 6, 12, 13, 15]. From the other side, we know
the diﬀerence ΔW1 since it is the same diﬀerence at the beginning of the 10-
round diﬀerential. Then we propagate this diﬀerence linearly trough MixColumn
and InvShuﬄeCell to get the diﬀerence Δy1. Δy1 has only 6 active bytes and
each, after the SubCell operation, has only 6 possible diﬀerences. Therefore, we
have 66 ≈215.6 possible diﬀerences at Δx1. Then after the MixColumn and
InvShuﬄeCell, we have only 215.6 possible diﬀerences at Δy0. The knowledge of
the plaintext allows us to compute the diﬀerence at Δx0. Then, guessing the 215.6
possible diﬀerences of Δy0 and using the S-box proposition, we get the value of
x0[0, 1, 3 · · · 10, 12 · · · 15]. From the knowledge of the plaintext we can drive the
value of WK[0, 1, 3 · · · 10, 12 · · · 15]. As a result we have 215.6 key candidates for
WK[0, 1, 3 · · · 10, 12 · · · 15] and WK[4, 5, 6, 12, 13, 15] but we have 6 bytes com-
mon; therefore, we have only 215.6−48 = 2−32.4 key candidates. To derive the 6
bytes value RK0[1, 3, 6, 9, 11, 14], we know that we have only one diﬀerence at
Δy1. Then we get one key candidate for RK0[1, 3, 6, 9, 11, 14] but also we have 5
bytes ﬁlter between WK[0, 1, 3 · · · 10 , 12 · · · 15] and RK0[1, 3, 6, 9, 11, 14], from
the key schedule as the whitening key is the master key K and the round keys
RKi = K ⊕βi and βi is constant. Therefore, we have only 2−32.4 ×2−40 = 2−72.4
key candidates for each message pair. To identify the remaining key candidates
for all the message pairs, we should identify the remaining message pairs after
the ciphertext ﬁlter. The ciphertext has a ﬁlter probability of 2−112.3 and is
computed as follows: we have 10 bytes of zero diﬀerence which have probability
of 2−80 and the remaining 6 bytes have probability of 215.7−48 = 2−32.3 since
we know that each byte of the 6 active bytes in Δx12 has only one diﬀerence
and after the S-box layer 5 bytes out of these 6 active bytes, each, has 6 pos-
sible diﬀerence and the remaining byte has 7 possible diﬀerences. Hence, we
have 65 × 7 = 215.7 possible diﬀerences at Δy12 which also the same diﬀerences
in the 6 bytes in the ciphertext. Therefore, after the ciphertext ﬁlter, we have
2230 × 2−112.3 = 2117.7 remaining message pairs to identify the key candidates.
As a result, we have 2117.7 ×2−72.4 = 245.3 remaining key candidates for 15 bytes
of the master key K.
In order to determine eﬃciently the remaining key candidates for all the
remaining message pairs after the ciphertext ﬁlter, we perform the following
steps:

14
M. Tolba et al.
1. From the ciphertext side, we have only one value for the active bytes of Δx12.
Then, using the S-box proposition, we can derive the 6 bytes WK[4, 5, 6, 12,
13, 15]. Therefore, we have 2117.7 key candidates for WK[4, 5, 6, 12, 13, 15] ≡
K[4, 5, 6, 12, 13, 15].
2. From the plaintext side, by guessing the 6 possible diﬀerences of Δw0[14]
and propagating them backward through the linear operations MixColumn
and InvShuﬄeCell we can know the values of Δy0[7, 8, 13]. Hence, we can use
the S-box proposition to derive the values of x0[7, 8, 13]. Then, knowing the
plaintext and x0[7, 8, 13] allows to derive WK[7, 8, 13] ≡K[7, 8, 13]. Using the
one value of the diﬀerence Δy1[14], we can derive the value of x1[14] using
the S-box proposition. Then, we can derive RK0[14] ≡K[14]. At this stage,
we have 2117.7 key candidates and we guess 6 values to derive K[7, 8, 13, 14]
but we have one ﬁlter of K[13] between this step and the previous step.
Therefore, in total, we have 2117.7×6×2−8 = 2112.3 remaining key candidates
for K[4 · · · 8, 12 · · · 15].
3. By guessing the 6 possible diﬀerences of Δw0[6] and propagating them back-
ward through the linear operations MixColumn and InvShuﬄeCell, we can
determine the values of Δy0[1, 4, 14]. Hence, we can use the S-box proposition
to derive the values of x0[1, 4, 14]. Then, knowing the plaintext and x0[1, 4, 14]
allows us to derive WK[1, 4, 14] ≡K[1, 4, 14]. Using the one value of the dif-
ference Δy1[6], we can derive the value of x1[6] using the S-box proposition.
Then, we can derive RK0[6] ≡K[6]. At this stage we have 2112.3 key can-
didates and we guess 6 values to derive K[1, 4, 6, 14] but we have 3 ﬁlters of
K[4, 6, 14] between this step and the previous step. Therefore, in total we have
2112.3 × 6 × 2−24 = 290.9 remaining key candidates for K[1, 4 · · · 8, 12 · · · 15].
4. By guessing the 62 possible diﬀerences of Δw0[1, 3] and propagating them
backward through the linear operations MixColumn and InvShuﬄeCell we
can determine the values of Δy0[0, 5, 10, 15]. Hence, we can use the S-box
proposition to derive the values of x0[0, 5, 10, 15]. Then, knowing the plaintext
and x0[0, 5, 10, 15] allows us to derive WK[0, 5, 10, 15] ≡K[0, 5, 10, 15]. Using
the one value of the diﬀerence Δy1[1, 3], we can derive the value of x1[1, 3]
using the S-box proposition. Then, we can derive RK0[1, 3] ≡K[1, 3]. At
this stage we have 290.9 key candidates and we guess 62 values to derive
K[0, 1, 3, 5, 10, 15] but we have 3 ﬁlters of K[1, 5, 15] between this step and the
previous step. Therefore, in total we have 290.9 × 62 × 2−24 = 272.1 remaining
key candidates for K[0, 1, 3 · · · 8, 10, 12 · · · 15].
5. By guessing the 62 possible diﬀerences of Δw0[9, 11] and propagating them
backward through the linear operations MixColumn and InvShuﬄeCell we
can determine the values of Δy0[3, 6, 9, 12]. Hence, we can use the S-box
proposition to derive the values of x0[3, 6, 9, 12]. Then, knowing the plaintext
and x0[3, 6, 9, 12] allows us to derive WK[3, 6, 9, 12] ≡K[3, 6, 9, 12]. Using the
one value of the diﬀerence Δy1[9, 11], we can derive the value of x1[9, 11]
using the S-box proposition. Then, we can derive RK0[9, 11] ≡K[9, 11]. At
this stage we have 272.1 key candidates and we guess 62 values to derive
K[3, 6, 9, 11, 12] but we have one ﬁlter in this step of K[9] and 3 of K[3, 6, 12]
between this step and the previous step. Therefore, in total we have 272.1 ×
62 × 2−32 = 245.3 remaining key candidates for K[0, 1, 3 · · · 15].

Truncated and Multiple Diﬀerential Cryptanalysis
15
Attack Complexity. The time complexity of the key recovery phase can be
derived from the previous steps as follows: step 1 needs 2 × 2 × 2117.7/(4 × 13) =
2114 encryptions, step 2 needs 2×2117.7 ×6/(4×13) = 2115.6 encryptions, step 3
needs 2×2112.3 ×6/(4×13) = 2110.2 encryptions, step 4 needs 2×290.9 ×62/(4×
13) = 291.4 encryptions, step 5 needs 2 × 272.1 × 62/(4 × 13) = 272.6 encryptions.
Therefore, the time complexity to ﬁnd 245.3 key candidates for K[0, 1, 3 · · · 15] is
2114 + 2115.6 + 2110.2 + 291.4 + 272.6 ≈2115.6. To retrieve the master key we make
an exhaustive search for the remaining key candidates with K[2] which needs
28 × 245.3 = 253.3 encryptions. Therefore, the time complexity of the attack is
dominated by the time needed to build the required structures which is 2119
encryptions. The data complexity of the attack is 2119 chosen plaintext.
5
Multiple Diﬀerential Cryptanalysis of Midori128
In this section, we describe a multiple diﬀerential attack that oﬀers some time-
data trade-oﬀcompared to the previous attack. Using Algorithm 2 and by enu-
merating all the 10-round diﬀerentials that hold with probability > 2−124, we
found 700 such diﬀerentials. In here, we show how to exploit 16 of them to
launch a multiple diﬀerential attack on Midori128. These 16 diﬀerentials are
shown in Table 4. As shown in the table, all these diﬀerentials have the same
input diﬀerence, but have diﬀerent output diﬀerences that are all active in the
same bytes. The ﬁrst diﬀerential in Table 4 is the diﬀerential that is used in the
attack described in the previous section. Therefore, the previous attack can be
applied with multiple diﬀerentials with small modiﬁcations.
In this attack, we retrieve the same key bytes as in the previous attack.
The total diﬀerential probability of the 16 diﬀerential is 2−114.7. Therefore, the
total diﬀerential probability of the 13-round diﬀerential is 2−112 × 2−114.7 =
2−226.7. Consequently, we need 2226.7−223 = 23.7 structures with 23.7 × 2112 =
2115.7 chosen plaintext. As shown in Table 4, Δx12[4, 5, 6] have the following
possible diﬀerences {4, 8, 16, 32} and Δx12[12, 13, 15] have the following possible
diﬀerences {8, 16, 32, 64}. Therefore, after the S-box layer, we have the following:
each one of Δy12[4, 6, 15] has 19 possible diﬀerences, Δy12[5] has 15 possible
diﬀerences, Δy12[12] has 17 possible diﬀerences, and Δy12[13] has 20 possible
diﬀerences. As a result, we have 193 ×15×17×20 = 225.1 possible diﬀerences at
the ciphertext. Consequently, we have 2226.7 ×2−80 ×225.1−48 = 2123.8 remaining
message pairs after the ciphertext ﬁlter. The remaining key candidates for each
pair are 16 × 215.6 × 2−88 = 2−68.4. Therefore, the number of remaining key
candidates for all the remaining message pairs, after the ciphertext ﬁlter, is given
by 2123.8 ×2−68.4 = 255.4 which can be exhaustively searched with the remaining
key byte. We can use the same steps that are used in the previous attack to
determine theses 255.4 remaining key candidates. The time complexity of the
attack is dominated by step 2. At the beginning of step 2, we have 2123.8 × 16 =
2127.8 key candidates for K[4, 5, 6, 12, 13, 15]. Therefore, the time complexity of
step 2 is 2 × 2127.8 × 6/(4 × 13) = 2125.7 encryptions. The data complexity is
2115.7 chosen plaintext.

16
M. Tolba et al.
Table 4. 10-round diﬀerentials of Midori128
Input diﬀerence (in hexadecimal)
Output diﬀerence (in hexadecimal)
Probability
00000000 00100000 00001000 00000000
40004040 00000000 00101010 00000000
2−118.09
00000000 00100000 00001000 00000000
08000808 00000000 00080808 00000000
2−118.12
00000000 00100000 00001000 00000000
08000808 00000000 00202020 00000000
2−118.12
00000000 00100000 00001000 00000000
40004040 00000000 00080808 00000000
2−118.18
00000000 00100000 00001000 00000000
40004040 00000000 00202020 00000000
2−118.18
00000000 00100000 00001000 00000000
10001010 00000000 00080808 00000000
2−118.36
00000000 00100000 00001000 00000000
10001010 00000000 00202020 00000000
2−118.36
00000000 00100000 00001000 00000000
10001010 00000000 00101010 00000000
2−118.43
00000000 00100000 00001000 00000000
10001010 00000000 00040404 00000000
2−118.8
00000000 00100000 00001000 00000000
40004040 00000000 00040404 00000000
2−118.8
00000000 00100000 00001000 00000000
20002020 00000000 00101010 00000000
2−118.81
00000000 00100000 00001000 00000000
08000808 00000000 00040404 00000000
2−119.29
00000000 00100000 00001000 00000000
20002020 00000000 00080808 00000000
2−119.81
00000000 00100000 00001000 00000000
20002020 00000000 00202020 00000000
2−119.81
00000000 00100000 00001000 00000000
08000808 00000000 00101010 00000000
2−120.32
00000000 00100000 00001000 00000000
20002020 00000000 00040404 00000000
2−120.43
6
Conclusion
We showed how to exploit the structure of the S-boxes and the MixColumn
operations of Midori128 in order to obtain long diﬀerentials that use single bit
diﬀerence for the inputs and outputs of the active S-boxes. Then, we developed
an algorithm that can be used to eﬃciently enumerate all such diﬀerentials for
a given number of rounds. Using this algorithm, we obtained a 10-round diﬀer-
ential that holds with probability 2−118. By appending 2 rounds above and one
round below this 10-round diﬀerential, we obtained a 13 round truncated diﬀer-
ential and used it to launch a key recovery attack attack on 13-round reduced
Midori128. The time and data complexities of the attack are 2119 encryptions
and 2119 chosen plaintext. Moreover, we presented a multiple diﬀerential attack
on the 13-round reduced cipher with time and data complexities of 2125.7 encryp-
tions and 2115.7 chosen plaintext, respectively.
References
1. Banik, S., Bogdanov, A., Isobe, T., Shibutani, K., Hiwatari, H., Akishita, T.,
Regazzoni, F.: Midori: a block cipher for low energy. In: Iwata, T., Cheon, J.H.
(eds.) ASIACRYPT 2015. LNCS, vol. 9453, pp. 411–436. Springer, Heidelberg
(2015). doi:10.1007/978-3-662-48800-3 17
2. Bogdanov,
A.A.,
Knudsen,
L.R.,
Leander,
G.,
Paar,
C.,
Poschmann,
A.,
Robshaw, M., Seurin, Y., Vikkelsoe, C.: PRESENT: an ultra-lightweight block
cipher. In: Paillier, P., Verbauwhede, I. (eds.) CHES 2007. LNCS, vol. 4727, pp.
450–466. Springer, Heidelberg (2007)

Truncated and Multiple Diﬀerential Cryptanalysis
17
3. De Canni`ere, C., Dunkelman, O., Kneˇzevi´c, M.: KATAN and KTANTAN — a
family of small and eﬃcient hardware-oriented block ciphers. In: Clavier, C., Gaj,
K. (eds.) CHES 2009. LNCS, vol. 5747, pp. 272–288. Springer, Heidelberg (2009)
4. Canteaut, A., Fuhr, T., Gilbert, H., Naya-Plasencia, M., Reinhard, J.-R.: Multiple
diﬀerential cryptanalysis of round-reduced PRINCE. In: Cid, C., Rechberger, C.
(eds.) FSE 2014. LNCS, vol. 8540, pp. 591–610. Springer, Heidelberg (2015)
5. Guo, J., Jean, J., Nikoli´c, I., Qiao, K., Sasaki, Y., Sim, S.M.: Invariant Subspace
Attack Against Full Midori64. IACR Cryptology ePrint Archive, 2015/1189 (2015).
https://eprint.iacr.org/2015/1189.pdf
6. Hong, D., Sung, J., Hong, S.H., Lim, J.-I., Lee, S.-J., Koo, B.-S., Lee, C.-H.,
Chang, D., Lee, J., Jeong, K., Kim, H., Kim, J.-S., Chee, S.: HIGHT: a new block
cipher suitable for low-resource device. In: Goubin, L., Matsui, M. (eds.) CHES
2006. LNCS, vol. 4249, pp. 46–59. Springer, Heidelberg (2006)
7. Knudsen, L., Leander, G., Poschmann, A., Robshaw, M.J.B.: PRINTcipher: a
block cipher for IC-printing. In: Mangard, S., Standaert, F.-X. (eds.) CHES 2010.
LNCS, vol. 6225, pp. 16–32. Springer, Heidelberg (2010)
8. Knudsen, L.R.: Truncated and higher order diﬀerentials. In: Preneel, B. (ed.) FSE
1994. LNCS, vol. 1008, pp. 196–211. Springer, Heidelberg (1995)
9. Leander, G., Paar, C., Poschmann, A., Schramm, K.: New lightweight DES vari-
ants. In: Biryukov, A. (ed.) FSE 2007. LNCS, vol. 4593, pp. 196–210. Springer,
Heidelberg (2007)
10. Lim, C.H., Korkishko, T.: mCrypton – a lightweight block cipher for security of
low-cost RFID tags and sensors. In: Song, J.-S., Kwon, T., Yung, M. (eds.) WISA
2005. LNCS, vol. 3786, pp. 243–258. Springer, Heidelberg (2006)
11. Lin, L., Wu, W.: Meet-in-the-Middle Attacks on Reduced-Round Midori-64. IACR
Cryptology ePrint Archive, 2015/1165 (2015). https://eprint.iacr.org/2015/1165.
pdf
12. Shibutani, K., Isobe, T., Hiwatari, H., Mitsuda, A., Akishita, T., Shirai, T.: Piccolo:
an ultra-lightweight blockcipher. In: Preneel, B., Takagi, T. (eds.) CHES 2011.
LNCS, vol. 6917, pp. 342–357. Springer, Heidelberg (2011)

Improved Linear Cryptanalysis
of Round-Reduced ARIA
Ahmed Abdelkhalek, Mohamed Tolba, and Amr M. Youssef(B)
Concordia Institute for Information Systems Engineering,
Concordia University, Montr´eal, Qu´ebec, Canada
youssef@ciise.concordia.ca
Abstract. ARIA is an iterated SPN block cipher developed by a group
of Korean cryptographers in 2003, established as a Korean standard in
2004 and added to the Transport Layer Security (TLS) supported cipher
suites in 2011. It encrypts 128-bit blocks with either 128, 192, or 256-
bit key. In this paper, we revisit the security of round-reduced ARIA
against linear cryptanalysis and present a 5-round linear hull using the
correlation matrix approach to launch the ﬁrst 8-round key recovery
attack on ARIA-128 and improve the 9 and 11-round attacks on ARIA-
192/256, respectively, by including the post whitening key. Furthermore,
sin all our attacks, we manage to recover the secret master key. The
(data in known plaintexts, time in round-reduced encryption operations,
memory in 128-bit blocks) complexities of our attacks are (2122.61, 2123.48,
2119.94), (2122.99, 2154.83, 2159.94), and (2123.53, 2238.13, 2239.95) for ARIA-
128, ARIA-192, and ARIA-256, respectively.
Keywords: Block cipher · Cryptanalysis · Linear cryptanalysis ·
ARIA · Key recovery · Linear hull · Correlation matrix
1
Introduction
ARIA is an iterated Substitution Permutation Network (SPN) block cipher that
operates on 128-bit blocks with 128, 192 or 256-bit key. It was designed by a
group of Korean cryptographers and published in ICISC 2003 [11]. When ARIA
was published in ICISC, it had 10/12/14 rounds for key sizes of 128/192/256
bits, respectively, and used 4 distinct S-boxes. In 2004, it was adopted by the
Korean Agency for Technology and Standards (KATS) as the Korean 128-bit
block encryption algorithm standard after increasing the number of rounds to
12/14/16 and introducing some modiﬁcations in the key scheduling algorithm.
The life span of ARIA has been extended since then and the latest extension was
in December 2014 where its life span was extended for another 5 years (KS X
1213-1:2014) [9]. Since 2011, ARIA is also one of the ciphers that are supported
in the Transport Layer Security (TLS) protocol [10].
Since its introduction, the security of ARIA was scrutinized by several cryp-
tographers. After the initial analysis of ARIA by its designers, Biryukov et al. [4]
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 18–34, 2016.
DOI: 10.1007/978-3-319-45871-7 2

Improved Linear Cryptanalysis of Round-Reduced ARIA
19
evaluated the security of ARIA against many cryptanalytic techniques. The best
attack they developed was based on a 7-round truncated diﬀerential. They have
also put forward dedicated linear attacks on 7-round ARIA-128 and 10-round
ARIA-192/256 in the weak-key setting, i.e., these attacks succeed for a limited
number of weak keys. Apart from the cipher designers, Wu et al. [24] were the
ﬁrst to evaluate the security of ARIA against impossible diﬀerential cryptanaly-
sis. They have proved, in contrast to the designers’ expectations, that 4-round
impossible diﬀerentials do exist and they can be used to mount a 6-round attack
on ARIA. The impossible diﬀerential attack proposed by Wu et al. was indepen-
dently enhanced by Li and Song [15] and Li et al. [21], and then it was extended
to 7-round ARIA-256 by Du and Chen [7]. Li et al. [14] presented 3-round inte-
gral distinguishers that can be used to attack 4/5-round ARIA and 6-round
ARIA-192/256. Afterwards, these 3-round integral distinguishers were modiﬁed
by Li et al. [16] to 4-round integral distinguishers which improved the complexity
of the 6-round integral attack and extended it to 7-round attack on ARIA-256.
Boomerang attacks on 5/6-round ARIA and 7-round ARIA-256 were presented
by Fleischmann [8]. Meet-in-the-Middle (MitM) attacks were applied to ARIA
for the ﬁrst time by Tang et al. [23], where they presented 5 & 6/7/8 MitM
attacks on ARIA-128/192/256, respectively. The complexities of these MitM
attacks were further improved by Bai and Yu [3] which enabled them to extend
the MitM attacks to 7-round ARIA-128 and 9-round ARIA-256. The complex-
ities of the 7/8-round MitM attacks on ARIA-192/256 were also enhanced by
Akshima et al. [2] and they presented the ﬁrst master key recovery attacks on
ARIA. Although the designers of ARIA did not expect the existence of eﬀective
attacks on 8 or more rounds of ARIA with any key size using linear crypt-
analysis, Liu et al. [17] managed to attack 7/9/11-round ARIA-128/192/256,
respectively, by presenting a special kind of linear characteristics exploiting the
diﬀusion layer employed in ARIA. However, the attacked rounds by Liu et al. [17]
did not include the post whitening key. This means that if the post whitening
key is considered, then the number of the reported rounds in their attacks will
be reduced by one for all versions of ARIA. Finally, after the introduction of the
Biclique cryptanalysis, it was applied on the full-round ARIA-256 [25], however,
this class of attacks is considered as an optimized exhaustive search.
Linear cryptanalysis is one of the major cryptanalysis techniques used against
symmetric-key ciphers. It was applied for the ﬁrst time to FEAL and then to DES
by Matsui [18,19]. In linear cryptanalysis, which is a known plaintext attack, the
adversary tries to ﬁnd a linear approximation between some bits from the plain-
text, ciphertext and the secret key which can be used as a statistical distinguisher
over several rounds of the cipher. Such linear distinguishers are then extended to
key-recovery attacks on a few additional rounds using partial decryption and/or
encryption. Subkeys of the appended rounds are guessed and the ciphertext is
decrypted and/or plaintext is encrypted using these subkeys to calculate inter-
mediate state value at the ends of the distinguisher. If the subkeys are correctly
guessed then the distinguisher should hold and it fails, otherwise. After the intro-
duction of linear cryptanalysis, many extensions and improvements have been

20
A. Abdelkhalek et al.
proposed. One particular improvement that we use in this paper is the intro-
duction of the notion of linear hull by Nyberg [20]. A linear hull is a set of
linear approximations that involve the same bits in the plaintext and ciphertext
and each one involves diﬀerent intermediate state bits. An equally important
framework for the description and understanding of the mechanisms of linear
cryptanalysis is the concept of correlation matrices of boolean functions which
was introduced by Daemen et al. [5]. The elements of the correlation matri-
ces of a boolean function F are all the correlation coeﬃcients between linear
combinations of input bits and that of output bits of F.
In this paper, we revisit the security of ARIA against linear cryptanalysis.
Inspired by the work of Liu et al. [17], we ﬁrst explore all the iterative patterns
across ARIA’s diﬀusion layer which have 8 active S-boxes in 2 rounds such as
3-5-3 and 4-4-4. Then, in order to have a good balance between the complexity
of the analysis rounds and the number of S-boxes involved in the distinguisher,
we focus our attention on the patterns that involve 4 S-boxes in each round, i.e.,
4-4-4. Among these patterns, we found 2 patterns that involve only 2 distinct
S-boxes (out of the 4 possible distinct S-boxes used in ARIA) in both the even
and odd rounds. Then, to simplify our analysis, we focus on these 2 patterns
and build their correlation potential matrices to estimate their linear hull eﬀect.
In a correlation potential matrix, every element of the correlation matrix is
squared. One of these patterns provide a new 5-round linear hull distinguisher
with correlation 2−114.93 which gives us one more round as compared to [17].
Based on this 5-round linear hull, we append 3/4/6 analysis rounds which enables
us to mount the ﬁrst attack on 8-round ARIA-128 and improve the 9 and 11-
round attacks on ARIA-192/256, respectively, to include the post whitening key.
Further, we use the recovered bytes of information from the round keys to recover
the master key. Our results and all previous attacks are summarized in Table 1.
The rest of the paper is organized as follows. Section 2 provides a description
of ARIA and the notations adopted in the paper. In Sect. 3, we brieﬂy give the
concepts required for the linear cryptanalysis of ARIA. In Sect. 4, we use the
correlation potential matrix to establish a linear hull of ARIA and present our
8, 9 and 11-round attacks on ARIA-128/192/256. We also show how the master
key can be recovered. Finally, we conclude the paper in Sect. 5.
2
Speciﬁcation of ARIA
ARIA [12] is an iterative 128-bit block cipher that follows the SPN structure. It
can be used with 3 diﬀerent key lengths, i.e., 128, 192 and 256 bits. The number
of rounds in ARIA diﬀers by the key length, i.e., 12 rounds for ARIA-128, 14
rounds for ARIA-192 and 16 rounds for ARIA-256. Similar to AES, the internal
state of ARIA can be represented as a 4 × 4 matrix, where each byte of the
matrix is an element in GF(28). An ARIA round applies the following three
transformations to the state matrix:
– Add Key (AK): XORing a 128-bit round key with the internal state. The
round keys are deduced from the master key via the key scheduling algorithm
which is described later in this section.

Improved Linear Cryptanalysis of Round-Reduced ARIA
21
Table 1. Summary of attacks on ARIA
Key size
Rounds
Attack type
Data
Time
Memory
Reference
128/192/256
4
IC
225 CP
225
∗
[14]
5
IDC
271.3 CP
271.6
272†
[21]
5
IC
227.2 CP
276.7
227.5†
[14]
5
MitM
25 CP
265.4
2121
[23]
5
BA
2109 ACPC
2110
257
[8]
6
IDC
2121 CP
2112
2121†
[24]
6
IDC
2120 CP
296
∗
[15]
6
IDC
2120.5 CP
2104.5
2121†
[21]
6
IDC
2113 CP
2121.6
2113†
[21]
6
MitM
256 CP
2121.5
2121
[23]
6
IC
299.2 CP
271.4
∗
[16]
6
BA
2128 KP
2108
256
[8]
7
TDC
281 CP
281
280
[4]
7
TDC
2100 CP
2100
251
[4]
7‡
LC
2105.8 KP
2100.99
279.73
[17]
7
MitM
2121 CP
2125.7
2122
[3]
192/256
6
IC
2124.4 CP
2172.4
2124.4†
[14]
7
MitM
2113 CP
2132
2130
[23]
7
MitM
296 CP
2161.3
2185
[23]
9‡
LC
2108.3 KP
2154.83
2159.77
[17]
10 wk
LC
2119 KP
2119
263
[4]
128
7 wk
LC
277 KP
288
261
[4]
8mk
LC
2122.61 KP
2123.48
2119.94
This paper
192
7
MitM
2113 CP
2135.1
2130
[2]
9mk
LC
2122.99 KP
2154.83
2159.94
This paper
256
7
IC
2100.6 CP
2225.8
∗
[16]
7
IDC
2125 CP
2238
∗
[7]
7
BA
2128 KP
2236
2184
[8]
7mk
MitM
2115 CP
2136.1
2130
[2]
8
MitM
256 CP
2251.6
2250
[23]
8
MitM
2113 CP
2244.61
2130
[3]
8mk
MitM
256 CP
2251.6
2252
[2]
8mk
MitM
2113 CP
2245.9
2138
[2]
9
MitM
2121 CP
2253.37
2250
[3]
11‡
LC
2110.3 KP
2218.54
2239.8
[17]
11mk
LC
2123.53 KP
2238.13
2239.95
This paper
16mk
BC
280 CP
2255.2
∗
[25]
Time in round-reduced ARIA encryptions and memory in 128-bit blocks
BA: Boomerang Attack
BC: Biclique Cryptanalysis
IC: Integral Cryptanalysis
IDC: Impossible Diﬀerential Cryptanalysis
LC: Linear Cryptanalysis
MitM: Meet-in-the-Middle
TDC: Truncated Diﬀerential Cryptanalysis
ACPC: Adaptive Chosen Plaintexts and Ciphertext
CP: Chosen Plaintext
KP: Known Plaintext
mk: Recovers the master key
wk: Weak-key setting
∗: Not given in the related paper
†: Estimated in [8]
‡: Without post whitening key

22
A. Abdelkhalek et al.
– SubBytes (SB): Applying non-linear invertible 8-bit to 8-bit S-box to each
byte of the state. ARIA employs 4 distinct S-boxes, namely, S1, S2 and their
inverses S−1
1 , S−1
2 . Moreover, the order in which the S-boxes are applied to
the internal state diﬀers between odd and even rounds. In the odd rounds,
the S-boxes are applied, column-wise, in the order: (S1, S2, S−1
1 , S−1
2 ) while in
the even rounds, the order, for each column, is: (S−1
1 , S−1
2 , S1, S2). Figure 1
depicts the order in which the S-boxes are applied in both odd (X1) and even
(X2) rounds.
– MixState (MS): Multiplication of the internal state by an involutional binary
matrix that has a branch number of 8. Given an input state Y , the output
state Z of the MS operation is computed as:
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
Z[0]
Z[1]
Z[2]
Z[3]
Z[4]
Z[5]
Z[6]
Z[7]
Z[8]
Z[9]
Z[10]
Z[11]
Z[12]
Z[13]
Z[14]
Z[15]
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0
0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1
0 1 0 0 1 0 1 0 0 0 1 1 1 0 0 1
1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0
1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1
0 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1
1 0 1 0 0 0 0 1 0 1 1 0 1 1 0 0
0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0
1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 1
1 1 0 0 0 1 1 0 0 0 0 1 1 0 1 0
0 0 1 1 0 1 1 0 1 0 0 0 0 1 0 1
0 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0
0 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0
1 0 0 1 0 0 1 1 1 0 1 0 0 1 0 0
1 0 0 1 1 1 0 0 0 1 0 1 0 0 1 0
0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
Y [0]
Y [1]
Y [2]
Y [3]
Y [4]
Y [5]
Y [6]
Y [7]
Y [8]
Y [9]
Y [10]
Y [11]
Y [12]
Y [13]
Y [14]
Y [15]
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
In the last round of ARIA, the MS linear transformation is replaced by an AK
operation, which is referred to as the post whitening key. The full encryption
function of an r-round ARIA is given in Fig. 1, where the ciphertext C is com-
puted from the plaintext P via r rounds using r + 1 round keys.
Key Schedule. The key schedule algorithm of ARIA takes the master key and
outputs 13, 15, or 17 128-bit round keys for ARIA-128/192/256, respectively.
First, the master key is divided into 2 128-bit values KL and KR, where KL
is the leftmost 128-bits of the master key and KR is the remaining bits, if any,
of the master key, right-padded with zeros to a 128-bit value. Then, a 3-round,
256-bit Feistel structure, as shown in Fig. 2, is used to compute 4 128-bits words
(W0, W1, W2, and W3), where Fo and Fe denote ARIA odd and even round
functions replacing the AK operation with pre-deﬁned constants addition. The

Improved Linear Cryptanalysis of Round-Reduced ARIA
23
S
1
1
−
S
1
2
−
S1
S 2
S
1
1
−
S
1
2
−
S1
S 2
S
1
1
−
S
1
2
−
S1
S 2
S
1
1
−
S
1
2
−
S1
S 2
S
1
1
−
S
1
2
−
S1
S 2
S
1
1
−
S
1
2
−
S1
S 2
S
1
1
−
S
1
2
−
S1
S 2
S
1
1
−
S
1
2
−
S1
S 2
Fig. 1. r-round ARIA
round keys are deduced from W0, W1, W2, and W3 as follows:
K1 = W0 ⊕(W1 ≫19),
K2 = W1 ⊕(W2 ≫19),
K3 = W2 ⊕(W3 ≫19),
K4 = (W0 ≫19) ⊕W3,
K5 = W0 ⊕(W1 ≫31),
K6 = W1 ⊕(W2 ≫31),
K7 = W2 ⊕(W3 ≫31),
K8 = (W0 ≫31) ⊕W3,
K9 = W0 ⊕(W1 ≪61),
K10 = W1 ⊕(W2 ≪61),
K11 = W2 ⊕(W3 ≪61),
K12 = (W0 ≪61) ⊕W3,
K13 = W0 ⊕(W1 ≪31),
K14 = W1 ⊕(W2 ≪31),
K15 = W2 ⊕(W3 ≪31),
K16 = (W0 ≪31) ⊕W3,
K17 = W0 ⊕(W1 ≪19),

24
A. Abdelkhalek et al.
where a ≪b and a ≫b denote that a is circularly rotated by b bit to the left
and right, respectively.
For more detailed information regarding the S-boxes and the key schedule algo-
rithm, the reader is referred to [12].
Fig. 2. ARIA key schedule - Initialization phase
2.1
Notations
The following notations are used throughout the rest of this paper:
– Ii: State value at the input of round i, where I1 is the plaintext P.
– Xi: State value after the AK operation of round i, where XR+1 is the cipher-
text C and R is 12 for ARIA-128, 14 for ARIA-192, and 16 for ARIA-256.
– Yi: State value after the SB operation of round i.
– Zi: State value after the MS operation of round i.
– Oi: State value at the output of round i, i.e., Oi = Ii+1.
– Si[j]: The (j + 1)th byte of state S at round i, where 0 ≤j ≤15, as numbered
in P in Fig. 1.
– Sk
i [j]: The (j + 1)th byte of state Sk at round i which corresponds to the
plaintext/ciphertext pair (P k, Ck).
– K{a,b,c,d}
i
: The XOR of 4 bytes of Ki, i.e., Ki[a] ⊕Ki[b] ⊕Ki[c] ⊕Ki[d].

Improved Linear Cryptanalysis of Round-Reduced ARIA
25
3
Linear Cryptanalysis
As mentioned above, linear cryptanalysis [18,19] is a known plaintext crypt-
analysis technique, in which the adversary attempts to construct linear approx-
imations for each round of a block cipher E, such that the output mask of a
round equals the input mask of the next round. The concatenation of these lin-
ear approximations creates a linear trail (Ω) whose correlation is computed by
multiplying the correlations of each round linear approximation. This results in a
linear distinguisher covering several rounds of E that can be used to distinguish
it from a random permutation. A linear approximation of a block cipher E is
typically given by a plaintext mask α and a ciphertext mask β, such that the
corresponding correlation COE(α, β) is non-negligible:
COE(α, β) = |2 × Pr[α • P ⊕β • C = γ • K] −1| ≫2−n/2,
where α, β, and γ denote the masks of the plaintext, ciphertext, and key,
respectively, n denotes the block length of the cipher and a • b denotes the
bitwise inner product of a and b. To distinguish E, the adversary gathers
N = O(1/CO2
E(α, β)) plaintexts and their corresponding ciphertexts and com-
putes the empirical correlation
ˆ
COE(α, β):
ˆ
COE(α, β) = |2 × #{i : α • P i ⊕β • Ci = 0}/N −1|.
The computed empirical correlation is close to COE(α, β) for the attacked block
cipher E, and smaller than 1/
√
N, with high probability, for a random permu-
tation [13]. By adding more rounds, the so-called analysis rounds, at the bottom
and/or the top of such linear distinguisher, it can be used to perform a key recov-
ery attack using partial decryption and/or encryption. The attack proceeds by
guessing the round keys used in the appended rounds, and computing an inter-
mediate state value(s) from the guessed round keys, ciphertext and/or plaintext.
The distinguisher is then applied to the deduced intermediate state value(s): if
the round keys guess is correct, the distinguisher is expected to hold, and fail
for wrong key guesses.
Linear Hulls. The notion of linear hulls was introduced by Nyberg [20], where
an r-round linear hull of a block cipher E is a set of all linear trails having the
same input mask α, output mask β and can diﬀer in the intermediate masks. If
we denote the square of a correlation by correlation potential, then the average
correlation potential of a linear hull over r rounds of a key-alternating block
cipher, averaged over all values of the expanded key (i.e. the concatenation of
all round keys), is the sum of the correlation potentials of all individual trails
that compose that linear hull, assuming independent round keys (Theorem 7.9.1
in [6]).
Correlation Matrices. High-probable linear hulls can be found by creating a
correlation matrix, or rather a correlation potential matrix, a notion that was
introduced by Daemen et al. [5]. For a key-alternating cipher of n-bit block

26
A. Abdelkhalek et al.
length, a correlation potential matrix M is an 2n ×2n matrix where the element
Mij in row i and column j of the matrix corresponds to the correlation potential
of an input mask αi and an output mask βi. Computing M r gives the correlation
potential after r rounds [1]. Constructing the correlation potential matrix for
modern block ciphers is infeasible as n is quite large. An alternative approach,
then, is to construct a submatrix of the correlation potential matrix that enables
us to obtain a lower bound on the average correlation potential of a linear hull.
4
Linear Cryptanalysis of ARIA
Liu et al. [17] have proposed a special kind of linear characteristics for byte-
oriented SPN block ciphers and applied it on ARIA. Their proposal exploited
the MS linear transformation in ARIA by ﬁnding a linear relation between 4
bytes of its input and 4 bytes of its output. Then, the linear approximation over
one round is formed by applying an input mask α and an output mask β to the
XOR of these input/output bytes, i.e.,
α • ⊕i∈V Ir[i] = β • ⊕i∈V Or[i],
where V is the set of the input/output bytes positions. For example, in their
attack V = {0, 3, 12, 15}.
Inspired by their work, we have ﬁrst explored the space of all iterative pat-
terns that have 8 active S-boxes in 2 rounds such as 3-5-3 and 4-4-4. We have
found that, for 5-round distinguisher and 3 analysis rounds, there is a trade-oﬀ
between the number of S-boxes involved in the linear characteristic, or rather
the linear hull, and the number of key bytes to be guessed in the analysis rounds.
On the one hand, the more S-boxes involved in the linear hull, the smaller the
correlation potential of the linear hull will be and thus the higher data complex-
ity of the attack will be. On the other hand, the more key bytes to be guessed
in the analysis rounds, the higher time complexity will be. Therefore, such a
trade-oﬀcan be thought of as a trade-oﬀbetween the data complexity and the
time complexity. As an example, in a 3-5-3-5-3-5-3-5 pattern, its ﬁrst 5-round
linear hull involves a total of 19 S-boxes and in its last three analysis rounds,
there are 13 key bytes to be guessed as will be illustrated in our attacks later. If
the same pattern is shifted by one round to be 5-3-5-3-5-3-5-3, then the number
of S-boxes involved in the 5-round distinguisher increases to 21 while the number
of key bytes to be guessed in the analysis rounds drops to 11 bytes. The pattern
that achieves the balance between the number of S-boxes in the distinguisher
and the number of guessed key bytes in the analysis rounds is the pattern 4-4-4.
We have automated the search for all the 4-4-4 patterns across ARIA’s MS
linear transformation and found that there are 204 such patterns. Among all
these patterns, there are only 2 patterns that have 2 active distinct S-boxes even
though the order of the application of the S-boxes alternates between the odd
and even rounds. The ﬁrst set of these two patterns is V 1 = {8, 10, 12, 14} which
has S1 and S−1
1
as the active S-boxes in both the odd and even rounds (the gray
cells in Fig. 3). The other set is V 2 = {9, 11, 13, 15} which has S2 and S−1
2
as the

Improved Linear Cryptanalysis of Round-Reduced ARIA
27
active S-boxes, once again in both the odd and even rounds (the black-hatched
cells in Fig. 3). Based on these two patterns, a 1-round linear trail of round i
with input mask α and output mask β can be written as:
V 1 :α • (Ii[8] ⊕Ii[10] ⊕Ii[12] ⊕Ii[14]) =
β • (Oi[8] ⊕Oi[10] ⊕Oi[12] ⊕Oi[14]),
V 2 :α • (Ii[9] ⊕Ii[11] ⊕Ii[13] ⊕Ii[15]) =
β • (Oi[9] ⊕Oi[11] ⊕Oi[13] ⊕Oi[15]).
S
1
1
−
S
1
2
−
S1
S 2
S
1
1
−
S
1
2
−
S1
S 2
S
1
1
−
S
1
2
−
S1
S 2
S
1
1
−
S
1
2
−
S1
S 2
S
1
1
−
S
1
2
−
S1
S 2
S
1
1
−
S
1
2
−
S1
S 2
S
1
1
−
S
1
2
−
S1
S 2
S
1
1
−
S
1
2
−
S1
S 2
Fig. 3. ARIA 4-4-4 iterative patterns involving 2 distinct S-boxes, each. The gray cells
represent pattern V 1 while the black-hatched cells represent pattern V 2.
Since both α and β ∈GF(28), the 1-round correlation potential matrix M for
each pattern has a size of a 28 ×28 and as the S-boxes involved in these patterns
do not change over the odd and even rounds, an M r correlation potential matrix
to get the average correlation potential after r rounds can be constructed by
simply raising M to the power r. Such a correlation potential matrix can be
regarded as a correlation potential submatrix of ARIA, restricting the inputs
and outputs of the matrix to the values that follow our speciﬁc patterns. We
have automatically constructed the 1-round correlation potential matrix for both
patterns. We were not able to go for more than 5 rounds as the highest correlation
potential starting M 6 exceeds 2−128. So, for M 5 of pattern V 1, the highest
average correlation potential was found to be 2−114.93 when the input mask α is
0x09 and the output mask β is 0x0E while for M 5 of pattern V 2, the highest
average correlation potential was found to be 2−115.63 when the input mask α is
0x24 and the output mask β is 0xD3.

28
A. Abdelkhalek et al.
4.1
Key Recovery Attacks on ARIA
As the highest correlation potential in V 1 is greater than the highest one in V 2,
we have opted for using pattern V 1. In our attacks, we have placed the 5-round
linear hull to cover rounds 1–5, hence it is represented as:
0x09 • (I1[8] ⊕I1[10] ⊕I1[12] ⊕I1[14]) =
0x0E • (O5[8] ⊕O5[10] ⊕O5[12] ⊕O5[14])
and since:
O5[8] ⊕O5[10] ⊕O5[12] ⊕O5[14] =
X6[8] ⊕X6[10] ⊕X6[12] ⊕X6[14]⊕
K6[8] ⊕K6[10] ⊕k6[12] ⊕K6[14]
the 5-round linear hull can be re-written as:
0x09 • (P[8] ⊕P[10] ⊕P[12] ⊕P[14]) =
0x0E • (X6[8] ⊕X6[10] ⊕X6[12] ⊕X6[14])
Fig. 4. Attack on 8-round ARIA
ARIA-128. The attack on 8-round ARIA-128 is based on the above 5-round
linear hull and adding 3 more rounds at its end, as illustrated in Fig. 4. The attack
proceeds as follows:

Improved Linear Cryptanalysis of Round-Reduced ARIA
29
1. First, we gather N plaintexts and their corresponding ciphertexts (P i, Ci),
where 1 ≤i ≤N.
2. Next, we initialize 232 counters Um, where the size of each counter is ⌈log2N⌉
bits and 0 ≤m ≤232 −1. Then, for each plaintext/ciphertext pair (P i, Ci),
we increment (resp. decrement) the counter Um by 1 if the parity of
0x09 • (P i[8] ⊕P i[10] ⊕P i[12] ⊕P i[14])
is 0 (resp. 1) and m equals the value of Ci[8]∥Ci[10]∥Ci[12]∥Ci[14].
3. We initialize 2120 counters Ul, where the size of each counter is ⌈log2N⌉bits
as well and 0 ≤l ≤2120 −1 and l represents the possible value of the 15
bytes of K{8,10,12,14}
9
∥K{8,10,12,14}
8
∥K{8,10,12,14}
7
∥Y8[8] ∥Y8[10]∥Y8[12]∥Y8[14]∥
Y7[8]∥Y7[10]∥Y7[12]∥Y7[14]∥Y6[8]∥Y6[10]∥Y6[12]∥Y6[14].
4. Then, for each possible value of K{8,10,12,14}
9
, K{8,10,12,14}
8
and
K{8,10,12,14}
7
, we do the following:
(a) For each possible value of the 232 values of m, compute Y m
8 [8]⊕Y m
8 [10]⊕
Y m
8 [12] ⊕Y m
8 [14] = m[0] ⊕m[1] ⊕m[2] ⊕m[3] ⊕K{8,10,12,14}
9
and denote
this value as tm
8 .
(b) For any value of the 224 values of Y8[8]∥Y8[10]∥Y8[12]∥Y8[14] satisfying
tm
8 , we deduce Xm
8 [8], Xm
8 [10], Xm
8 [12], Xm
8 [14] from the corresponding S-
boxes. Then, using the guessed value of K{8,10,12,14}
8
, compute Y m
7 [8] ⊕
Y m
7 [10]⊕Y m
7 [12]⊕Y m
7 [14] = Zm
7 [8]⊕Zm
7 [10]⊕Zm
7 [12]⊕Zm
7 [14] = Xm
8 [8]⊕
Xm
8 [10] ⊕Xm
8 [12] ⊕Xm
8 [14] ⊕K{8,10,12,14}
8
and denote this value as tm
7 .
(c) Then, for any value of the 224 values of Y7[8]∥Y7[10]∥Y7[12]∥Y7[14] sat-
isfying tm
7 , we deduce Xm
7 [8], Xm
7 [10], Xm
7 [12], Xm
7 [14] from the corre-
sponding S-boxes. Then, using the guessed value of K{8,10,12,14}
7
, compute
Y m
6 [8]⊕Y m
6 [10]⊕Y m
6 [12]⊕Y m
6 [14] = Zm
6 [8]⊕Zm
6 [10]⊕Zm
6 [12]⊕Zm
6 [14] =
Xm
7 [8] ⊕Xm
7 [10] ⊕Xm
7 [12] ⊕Xm
7 [14] ⊕K{8,10,12,14}
7
and denote this value
as tm
6 .
(d) For any value of the 224 values of Y6[8]∥Y6[10]∥Y6[12]∥Y6[14] satisfying
tm
6 , we deduce Xm
6 [8], Xm
6 [10], Xm
6 [12], Xm
6 [14] from the corresponding S-
boxes. Then, calculate the parity of:
0x0E • (Xm
6 [8] ⊕Xm
6 [10] ⊕Xm
6 [12] ⊕Xm
6 [14])
If the parity is 0 (resp. 1), increment (resp. decrement) the corresponding
counter Ul by the value of Um.
5. For l such that the value of Ul is maximal, output the value of the correspond-
ing K{8,10,12,14}
9
∥K{8,10,12,14}
8
∥K{8,10,12,14}
7
as the correct key information.
Attack complexity. The number of known plaintext/ciphertext pairs N
required to perform the attack is estimated by the following formula, which
is adopted from Corollary 1 in [22]:
N =

Φ−1(Ps) + Φ−1(1 −2−a−1)
2
	2
×
4
CO2 ,
(1)

30
A. Abdelkhalek et al.
where Ps is the probability of success, CO is the correlation of the linear hull,
Φ−1 is the inverse cumulative function of the standard normal distribution, and a
is the advantage of the adversary over the exhaustive search and equals k−log2 d
if the correct key was ranked among the top d candidates out of the 2k possible
candidates of an k-bit key.
In our attack, we guess 120 bits, set the advantage a to 120, i.e., the cor-
rect key information is the ﬁrst one of the list of candidates and set the prob-
ability of success to 0.95. Then, the number of plaintext/ciphertext pairs N
equals 25.68 ×
4
2−114.93 = 2122.61. The time complexity of the attack is domi-
nated by steps 2 and 4.(d). Therefore, the time complexity of the attack equals
2122.61 ×
4
16×8 + 224 × 232 × 224 × 224 × 224 ×
4
16×8 ≈2123.03 8-round ARIA
encryptions. The memory complexity of the attack is attributed to storing the
counters Ul, where the size of each counter is set to 123 bits. Hence, the memory
complexity of the attack is 2120 × 123
128 ≈2119.94 128-bit blocks.
ARIA-192/256. The attack on 8-round ARIA-128 can be extended to 9-round
ARIA-192 (resp. 11-round ARIA-256) with the post whitening key by utilizing
the same 5-round linear hull and having 4 (resp. 6) analysis rounds. The attack
procedure is similar to the attack on ARIA-128, except that in step 3, we initialize
2160 (resp. 2240) counters Ul, where in this case, 0 ≤l ≤2160 −1 (resp. 0 ≤
l ≤2240 −1) and represents the possible value of the 20 (resp. 30) bytes of
K{8,10,12,14}
r+1
∥Yr[8]∥Yr[10]∥Yr[12]∥Yr[14], where 6 ≤r ≤9 (resp. 6 ≤r ≤11) and
we add 1 (resp. 3) more sub-step(s) in step 4 to accommodate the additional
round(s).
In this case, for an advantage a of 160 (resp. 240) and Ps of 0.95, the number
of known plaintext/ciphertext pairs N is 26.06 ×
4
2−114.93 = 2122.99 for ARIA-192
and is 26.6 ×
4
2−114.93 = 2123.53 for ARIA-256. The time complexity of the attack
is 2122.99 ×
4
16×9 +232 ×232 ×224 ×224 ×224 ×224 ×
4
16×9 ≈2154.83 9-round ARIA
encryptions for ARIA-192 and is 2123.53 ×
4
16×11 + 248 × 232 × 224 × 224 × 224 ×
224 ×224 ×224 ×
4
16×11 ≈2218.54 11-round ARIA encryptions for ARIA-256. The
size of each counter of Ul is set to 123 (resp. 124) bits, therefore, the memory
complexity of the attack is 2160 × 123
128 ≈2159.94 128-bit blocks for ARIA-192 and
2240 × 124
128 ≈2239.95 128-bit blocks for ARIA-256.
4.2
Recovering the Master Key
In this subsection, we show how the recovered bytes of information from the
round keys can be used to recover the master key in all versions of ARIA.
ARIA-128. In the attack on 8-round ARIA-128, we recover 3 bytes of informa-
tion from K9, K8, and K7. Recall that in ARIA-128, KR is all zeros and KL is
the 128-bit master key and at the same time it is W0. In order to recover the
master key, we do the following:

Improved Linear Cryptanalysis of Round-Reduced ARIA
31
– First, we guess 15 bytes of W0, i.e., all the bytes except W0[7]. These bytes
enable us to compute W1[0], W1[2], W1[4], with 6 other bytes, which gives us
the ﬁrst 5 bits of bytes 8, 10, and 12 of (W1 ≪61).
– From the key schedule, we know that K9 = W0 ⊕(W1 ≪61). As we recover
K{8,10,12,14}
9
, i.e., K9[8]⊕K9[10]⊕K9[12]⊕K9[14], this means that we recover
W0[8]⊕W0[10]⊕W0[12]⊕W0[14]⊕(W1 ≪61)[8]⊕(W1 ≪61)[10]⊕(W1 ≪
61)[12] ⊕(W1 ≪61)[14].
– As we guessed W0[8], W0[10], W0[12] and W0[14], recovered
K{8,10,12,14}
9
and computed the ﬁrst 5 bits of bytes 8, 10, and 12 of (W1 ≪61),
we can deduce the ﬁrst 5 bits of byte 14 of (W1 ≪61) which in turn enables
us to deduce the last 5 bits of SB(W0[7]).
– Afterwards, we guess the 3 ﬁrst bits of SB(W0[7]) which means that we have
2123 candidates for W0 or rather the master key.
– Then, we run the key schedule and use the remaining 3 bits of K{8,10,12,14}
9
and
the two bytes of K{8,10,12,14}
8
and K{8,10,12,14}
7
to discard the wrong guesses
and so we end up with 2104 candidates for the master key which we can test
using 2 plaintext/ciphertext pairs.
The time complexity of the master key recovery phase is dominated by the last
step and equals 2123× 3
8 +2×2104 ≈2121.59 8-round ARIA encryptions as we need
to compute 3 rounds of ARIA for the 2123 candidates to deduce W2 and W3
and then test the remaining 2104 candidates using 2 plaintext/ciphertext pairs.
Therefore the total time complexity of the attack is 2123.03 + 2121.59 ≈2123.48.
ARIA-192. In the attack on 9-round ARIA-192, we recover 4 bytes of informa-
tion from K10, K9, K8, and K7. In order to recover the master key, we do the
following:
– First, we guess the 16 bytes of W0 and calculate Fo(W0, CK1). Then, to be
able to compute bytes 8, 10, 12 and 14 of (W1 ≪61), we guess 29 bits of KR
as the 8 right bytes of KR are zeros.
– We use the recovered K{8,10,12,14}
9
to discard the wrong guesses of W0 and
the 29 bits guessed from KR and so we have 2149 for W0 along with the 29
bits of KR.
– Next, we guess the remaining 35 bits of the master key, i.e., the remaining 35
bits of KR so we have 2184 candidates for the master key.
– Then, we run the key schedule to compute W1, W2, and W3 and use the
3 bytes of K{8,10,12,14}
10
, K{8,10,12,14}
8
and K{8,10,12,14}
7
to discard the wrong
guesses and we end up with 2160 candidates which we test using 2 plain-
text/ciphertext pairs.
The time complexity of the master key recovery phase equals 2184× 3
9 +2×2160 ≈
2182.42 9-round ARIA encryptions, hence the total time complexity of the attack
is 2154.83 + 2182.42 ≈2182.42.
ARIA-256. In the attack on 11-round ARIA-256, we recover 6 bytes of infor-
mation from K12, K11, K10, K9, K8, and K7. In order to recover the master key,
we do the following:

32
A. Abdelkhalek et al.
– First, we guess the 16 bytes of W2 and 14 bytes of W3 and use the recovered
K{8,10,12,14}
7
and K{8,10,12,14}
11
, both of them are deduced from W2 and W3,
to calculate the remaining two bytes of W3 which means that we have 2240
candidates for both W2 and W3.
– Next, starting from W2 and W3, we run the key schedule to compute W0 and
W1 and use the other 4 bytes of K{8,10,12,14}
12
, K{8,10,12,14}
10
, K{8,10,12,14}
9
and
K{8,10,12,14}
8
to discard the wrong guesses and we end up with 2208 candidates
for the master key which we test using 2 plaintext/ciphertext pairs.
The time complexity of the master key recovery phase is 2240 × 3
11 + 2 × 2208 ≈
2238.13 11-round ARIA encryptions, therefore the total time complexity of the
attack is 2218.54 + 2238.13 ≈2238.13.
5
Conclusion
In this paper, we have revisited the security of round-reduced ARIA against
linear cryptanalysis and presented the ﬁrst 8-round attack on ARIA-128 and
improved the previous 9 and 11-round attacks on ARIA-192/256 by including
the post whitening key. We have achieved these results by constructing a 5-round
linear hull on ARIA using the correlation matrix approach and exploiting the
binary linear transformation layer in the analysis rounds. For all our attacks,
we showed how the recovered bytes of information from the round keys can be
used to recover the master key. This paper shows some weaknesses of reduced
versions of ARIA, but the full round ARIA remains still secure.
References
1. Abdelraheem, M.A., Alizadeh, J., Alkhzaimi, H.A., Aref, M.R., Bagheri, N.,
Gauravaram, P.: Improved linear cryptanalysis of reduced-round SIMON-32
and SIMON-48. In: Biryukov, A., Goyal, V. (eds.) Progress in Cryptology -
INDOCRYPT 2015. LNCS, vol. 9462, pp. 153–179. Springer, Cham (2015).
http://dx.doi.org/10.1007/978-3-319-26617-6 9
2. Biryukov,
A.,
Goyal,
V.
(eds.):
Progress
in
Cryptology
–
INDOCRYPT
2015.
LNCS,
vol.
9462.
Springer,
Cham
(2015).
http://dx.doi.org/10.1007/978-3-319-26617-6 11
3. Bai, D., Yu, H.: Improved meet-in-the-middle attacks on round-reduced ARIA. In:
Desmedt, Y. (ed.) ISC 2013. LNCS, vol. 7807, pp. 155–168. Springer, Heidelberg
(2015). http://dx.doi.org/10.1007/978-3-319-27659-5 11
4. Biryukov, A., De Canniere, C., Lano, J., Ors, S.B., Preneel, B.: Security and perfor-
mance analysis of ARIA, version 1.2. Technical report, Katholieke Universiteit Leu-
ven, Belgium (2004).http://www.cosic.esat.kuleuven.be/publications/article-500.
pdf
5. Daemen, J., Govaerts, R., Vandewalle, J.: Fast Software Encryption. LNCS, vol.
1008. Springer, Heidelberg (1995). http://dx.doi.org/10.1007/3-540-60590-8 21
6. Daemen, J., Rijmen, V.: The Design of Rijndael. Springer, New York (2002)

Improved Linear Cryptanalysis of Round-Reduced ARIA
33
7. Du,
C.,
Chen,
J.:
Impossible
diﬀerential
cryptanalysis
of
ARIA
reduced
to
7
rounds.
In:
Heng,
S.-H.,
Wright,
R.N.,
Goi,
B.-M.
(eds.)
CANS
2010.
LNCS,
vol.
6467,
pp.
20–30.
Springer,
Heidelberg
(2010).
http://dx.doi.org/10.1007/978-3-642-17619-7 2
8. Fleischmann,
E.,
Forler,
C.,
Gorski,
M.,
Lucks,
S.:
New
boomerang
attacks
on
ARIA.
In:
Gong,
G.,
Gupta,
K.C.
(eds.)
INDOCRYPT
2010.
LNCS,
vol.
6498,
pp.
163–175.
Springer,
Heidelberg
(2010).
http://dx.doi.org/10.1007/978-3-642-17401-8 13
9. Korean Agency for Technology and Standards (KATS): 128-bit Block Encryption
Algorithm ARIA KS X 1213–1: December 2014 (in Korean)
10. Kim, W., Lee, J., Park, J., Kwon, D.: Addition of the ARIA cipher suites to
Transport Layer Security (TLS). RFC 6209, RFC Editor, April 2011. http://www.
rfc-editor.org/rfc/rfc6209.txt, http://www.rfc-editor.org/rfc/rfc6209.txt
11. Daesung,
K.,
et
al.:
Information
Security
and
Cryptology
-
ICISC
2003.
LNCS,
vol.
2971.
Springer,
Heidelberg
(2004).
http://dx.doi.org/10.1007/978-3-540-24691-6 32
12. Lee, J., Lee, J., Kim, J., Kwon, D., Kim, C.: A Description of the ARIA Encryption
Algorithm. RFC 5794, RFC Editor, March 2010
13. Leurent, G.: Improved diﬀerential-linear cryptanalysis of 7-round chaskey with
partitioning. Cryptology ePrint Archive, Report 2015/968 (2015). http://eprint.
iacr.org/
14. Li, P., Sun, B., Li, C.: Integral cryptanalysis of ARIA. In: Bao, F., Yung, M., Lin,
D., Jing, J. (eds.) Inscrypt 2009. LNCS, vol. 6151, pp. 1–14. Springer, Heidelberg
(2010). http://dx.doi.org/10.1007/978-3-642-16342-5 1
15. Li, S., Song, C.: Improved impossible diﬀerential cryptanalysis of ARIA. In: A
Description of the ARIA Encryption Algorithm. RFC 5794, RFC Editor Interna-
tional Conference on Information Security and Assurance, ISA 2008, pp. 129–132,
April 2008
16. Li,
Y.,
Wu,
W.,
Zhang,
L.:
Integral
attacks
on
reduced-round
ARIA
block
cipher.
In:
Kwak,
J.,
Deng,
R.H.,
Won,
Y.,
Wang,
G.
(eds.)
ISPEC
2010.
LNCS,
vol.
6047,
pp.
19–29.
Springer,
Heidelberg
(2010).
http://dx.doi.org/10.1007/978-3-642-12827-1 2
17. Liu,
Z.,
Gu,
D.,
Liu,
Y.,
Li,
J.,
Li,
W.:
Linear
cryptanalysis
of
ARIA
block
cipher.
In:
Qing,
S.,
Susilo,
W.,
Wang,
G.,
Liu,
D.
(eds.)
ICICS
2011.
LNCS,
vol.
7043,
pp.
242–254.
Springer,
Heidelberg
(2011).
http://dx.doi.org/10.1007/978-3-642-25243-3 20
18. Matsui, M.: Linear cryptanalysis method for DES cipher. In: Helleseth, T. (ed.)
EUROCRYPT 1993. LNCS, vol. 765, pp. 386–397. Springer, Heidelberg (1994).
http://dx.doi.org/10.1007/3-540-48285-7 33
19. Matsui, M., Yamagishi, A.: A new method for known plaintext attack of FEAL
cipher. In: Rueppel, R.A. (ed.) EUROCRYPT 1992. LNCS, vol. 658, pp. 81–91.
Springer, Heidelberg (1993). http://dx.doi.org/10.1007/3-540-47555-9 7
20. Nyberg, K.: Linear approximation of block ciphers. In: De Santis, A. (ed.)
EUROCRYPT 1994. LNCS, vol. 950, pp. 439–444. Springer, Heidelberg (1995).
http://dx.doi.org/10.1007/BFb0053460
21. Li, R., Bing Sun, P.Z., Li, C.: New Impossible Diﬀerential Cryptanalysis of ARIA.
Cryptology ePrint Archive, Report 2008/227 (2008). http://eprint.iacr.org/2008/
227.pdf
22. Sel¸cuk, A.A.: On probability of success in linear and diﬀerential cryptanalysis. J.
Cryptology 21(1), 131–147 (2007). http://dx.doi.org/10.1007/s00145-007-9013-7

34
A. Abdelkhalek et al.
23. Tang, X., Sun, B., Li, R., Li, C., Yin, J.: A meet-in-the-middle attack
on
reduced-round
ARIA.
J.
Syst.
Softw.
84(10),
1685–1692
(2011).
http://www.sciencedirect.com/science/article/pii/S016412121100104X
24. Wu, W.L., Zhang, W.T., Feng, D.G.: Impossible diﬀerential cryptanalysis of
reduced-round ARIA and Camellia. J. Comput. Sci. Technol. 22(3), 449–456
(2007). http://dx.doi.org/10.1007/s11390-007-9056-0
25. zhen Chen Tian-min Xu, S.: Biclique Attack of the Full ARIA-256. Cryptology
ePrint Archive, Report 2012/011 (2012). http://eprint.iacr.org/2012/011.pdf

Partial Key Exposure Attacks on CRT-RSA:
General Improvement for the Exposed Least
Signiﬁcant Bits
Atsushi Takayasu(B) and Noboru Kunihiro
The University of Tokyo, Tokyo, Japan
a-takayasu@it.k.u-tokyo.ac.jp
Abstract. Bl¨omer and May (Crypto 2003) used Coppersmith’s lattice
based method to study partial key exposure attacks on CRT-RSA, i.e.,
an attack on RSA with the least signiﬁcant bits of a CRT exponent.
The attack works for an extremely small public exponent e, however,
improved attacks were proposed by Lu, Zhang, and Lin (ACNS 2014),
Takayasu and Kunihiro (ACNS 2015). These attack works for e < N 0.375.
For a smaller (resp. larger) e, an attack of Lu et al. (resp. Takayasu-
Kunihiro’s attack) requires less partial information to attack RSA.
In this paper, we propose a further improved attack. Indeed, our
attack completely improves previous attacks in the sense that our attack
requires less partial information than previous attacks for all e < N 0.375.
We solve the same modular equation as Takayasu-Kunihiro, however, our
attack can ﬁnd larger roots. From the technical point of view, although
the Takayasu-Kunihiro lattice follows the Jochemsz-May strategy (Asi-
acrypt 2006), we carefully analyze the algebraic structure of the under-
lying polynomial and propose better lattice constructions.
Keywords: CRT-RSA · Cryptanalysis · Partial key exposure ·
Coppersmith’s method · Lattices
1
Introduction
1.1
Background
Let N = pq be a public RSA modulus where prime factors p and q are the
same bit-size [31]. A public exponent e and a secret exponent d satisfy ed = 1
mod (p−1)(q−1). For encryption/verifying (resp. decryption/signing), the heavy
modular exponentiation should be computed. To achieve a faster computation,
a simple solution is to use a smaller public (resp. secret) exponent. However,
Wiener [40] showed that a public RSA modulus can be factorized in polynomial
time when a secret exponent is too small. Boneh and Durfee [3] used Copper-
smith’s lattice based method [6] and showed that the attack works for d < N 0.284.
Moreover, in the same work, they proposed a better attack which works when
d < N 0.292.
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 35–47, 2016.
DOI: 10.1007/978-3-319-45871-7 3

36
A. Takayasu and N. Kunihiro
To thwart the small secret exponent attack and achieve a faster decryp-
tion/signing simultaneously, the Chinese Remainder Theorem (CRT) is often
used as described by Quisquater and Couvreur [29]. Instead of the original secret
exponent d, CRT exponents dp and dq are used where
edp = 1
mod (p −1)
and
edq = 1
mod (q −1).
Analogous to a small secret exponent RSA [3,40], too small CRT exponents
disclose the factorization of a public modulus [1,14,16,19,24]. The state-of-the-
art Jochemsz-May attack [19] factorizes a public RSA modulus N in polynomial
time for dp, dq < N 0.073 with a full size public exponent e. The attack showed
the CRT-RSA becomes more vulnerable for a small public exponent. To thwart
the attack for a practical setting such that e is small, e.g., e = 216 + 1, CRT
exponents should be ≈N 0.5.
Since RSA is one of the most famous cryptosystems and widely used, the
hardness of the RSA/factorization problem with some hints have been stud-
ied in numerous papers, e.g., partial knowledge of prime factors [5,30,36,39],
implicit hints of prime factors [13,22,27,33], and partial knowledge of a secret
exponent [2,4,12,20,34,37]. In this paper, we focus on the problem with par-
tial knowledge of CRT exponents and consider an attack scenario when the
least signiﬁcant bits of dp are exposed to attackers although there exist several
papers [2,23,32,38] whose attack scenarios are slightly diﬀerent. More concretely,
we study the following problem: given ˜dp > N 0.5−δ, which is the least signiﬁcant
bits of dp, then the goal of the problem is to factorize the public RSA modu-
lus N for e ≈N α, dp ≈N 0.5. Bl¨omer and May (Crypto 2003) [2] studied the
problem, however, the proposed attack works only for an extremely small public
exponent, i.e., e = poly(log N). Lu, Zhang, and Lin (ACNS 2014) [23], Takayasu
and Kunihiro (ACNS 2015) proposed improved attacks for larger α. The former
attack works for α < 0.25 and δ < 0.25 for α ≈0 whereas the latter attack
works for α < 0.375 and δ < 0.207 for α ≈0. Hence, although the latter attack
works for larger e, the former attack works with less partial information for a
small e1.
1.2
Our Contributions
Our Results. In this paper, we propose an improved attack as follows:
Theorem 1. Let N = pq be a public RSA modulus where the prime factors p
and q are the same bit-size. Let e ≈N α denote a public exponent and dp ≈
N 0.5 denote a CRT exponent such that edp = 1 mod (p −1). Given the public
elements (N, e) as well as ˜dp > N 0.5−δ which is the least signiﬁcant bits of a
CRT exponent. If
– δ < 5−2√1+14α
14
for
1
18 < α ≤3
8, or
1 Lu et al. [23] also proposed an attack for α < 0.375, however, the attack is always
weaker than Takayasu-Kunihiro’s attack for all α.

Partial Key Exposure Attacks on CRT-RSA
37
– η

α(1 −2(δ −α)) −δ (1 −4(δ −α))2
+ α(δ −α)(1 + 2α −4δ) < 0 where
η =
2δ(1−4(δ−α))+2√
δ(δ−α)(1+2α−8δ(1−2δ+2α))
1−2(δ−α)
for 0 < α ≤
1
18,
then the public modulus N can be factorized in polynomial time.
The attack works for α < 0.375 as the previous attacks, however, our attack com-
pletely improves previous two attacks proposed by Lu et al. [23] and Takayasu-
Kunihiro [38]. That means our attack requires less partial information than the
previous attacks for all α < 0.375.
Technical Overview. As the previous attacks2 [23,38], our attack is based on
Coppersmith’s method to solve modular equations which have small solutions [6].
To solve a modular equation, the method constructs a lattice whose short vectors
disclose the solutions. Appropriate lattice constructions are usually an important
point of the method. The most famous lattice construction strategy was proposed
by Jochemsz and May (Asiacrypt 2006) [18]. The strategy is simple, however,
does not always oﬀer the best bound. For example, in the context of small secret
exponent attacks on RSA, the Boneh-Durfee weaker bound d < N 0.284 can be
obtained by the Jochemsz-May strategy, however, the stronger bound d < N 0.292
cannot. To obtain better bounds, which cannot be obtained by the Jochemsz-
May strategy, is technical and a challenging problem. In this paper, we also focus
on the formulation of the equation. More concretely, how to formulate an attack
scenario is crucial for a resulting attack condition since sizes of recoverable root
bounds depend on the algebraic structure of the formulation.
Let d′
p and ˜dp be the most/least signiﬁcant bits of dp, respectively. As we
deﬁned above, ˜dp > N 0.5−δ and d′
p < N δ. Then the CRT exponent can be
rewritten as dp = d′
pM + ˜dp where M ≈N 0.5−δ. To thwart the Jochemsz-May
attack [19], we only consider the case dp ≈N 0.5 in this paper and omit the
analysis of the other case since the generalization is trivial. The CRT-RSA key
generation can be rewritten as
e

d′
pM + ˜dp

= 1 + ℓ(p −1)
with some integer ℓ. Lu et al. [23] formulated the following equation:
1 −e ˜dp −eMx −y = 0
mod p
whose solution is (x, y) = (d′
p, ℓ). There are two algorithms known to solve the
equation proposed by Herrmann and May [15], Takayasu and Kunihiro [35],
respectively. Herrmann and May’s algorithm is based on the Jochemsz-May
strategy whereas Takayasu and Kunihiro’s algorithm is not. Takayasu-Kunihiro’s
algorithm works for larger δ than Herrmann-May’s algorithm for small α; when
2 In [38], the attack is based on the other method; Coppersmith’s method to solve
integer equations [5,9,10]. However, we will show that their attack with the least
signiﬁcant bits of dp or dq can also be obtained from the modular method.

38
A. Takayasu and N. Kunihiro
α ≈0, the latter algorithm works for δ < 1/4 = 0.25 and the former algorithm
works for δ < (
√
2 −1)/2 = 0.20710 · · · . The fact shows that when we can con-
struct a better attack which cannot be obtained by the Jochemsz-May strategy,
it works with less partial information for the same α.
As opposed to Lu et al., Takayasu and Kunihiro [38] formulated the following
equation3:
1 −e ˜dp + x(y −1) = 0
mod eM
whose solution is (x, y) = (ℓ, p). They solved the equation where the lattice con-
struction is based on the Jochemsz-May strategy as the Herrmann-May. How-
ever, the formulation aﬀects the resulting attack condition. Takayasu-Kunihiro’s
attack works for large α than an attack of Lu et al. with Herrmann-May’s algo-
rithm; when δ ≈0, the latter algorithm works for α < 3/8 = 0.375 and the
former attack works for α < (
√
2 −1)/2 = 0.20710 · · · . The fact shows that
the latter formulation, i.e., mod eM equation, yields the attacks which work for
larger α than the former equation, i.e., mod p equation.
Our improved attack in this paper is constructed by solving the mod
eM equation and the lattice does not follow the Jochemsz-May strategy. The
improvement is reasonable from the above discussion. Although the same mod
eM equation is solved, Takayasu-Kunihiro’s attack [38] is based on the Jochemsz-
May strategy. Our proposed attack works for larger δ than Takayasu-Kunihiro’s
attack. Our proposed attack works for larger α than the attack of Lu et al.
Therefore, our attack is better than the previous best attacks for all α.
1.3
Organization
In Sect. 2, we introduce the overview of Coppersmith’s method; lattices, the
LLL algorithm, and Howgrave-Graham’s lemma. In Sect. 3, we brieﬂy review
a lattice construction of the Takayasu-Kunihiro attack [38]. Since we solve the
same modular equation, i.e., the mod eM equation, and improve the attack,
understanding of the previous lattice construction enables readers to understand
the point of our improvement. To obtain possibly better attacks, our lattice
constructions divide in two cases, i.e., the ﬁrst and the second conditions of
Theorem 1. In Sect. 4, due to the page limitation, we propose improved attacks
for only large α, i.e., 1/18 < α ≤3/8.
2
Preliminaries
In this section, we introduce Coppersmith’s lattice based method to solve mod-
ular equations [6]. For simplicity, we explain the simpler reformulation due to
Howgrave-Graham [17]. The method has been revealed numerous vulnerabilities
of RSA. See [7,8,25,26] for more information.
3 To be precise, the equation was formulated by Lu et al. in [23] and they solved the
equation, however, Takayasu and Kunihiro corrected some mistakes of the paper and
proposed an improved algorithm.

Partial Key Exposure Attacks on CRT-RSA
39
Let b1, . . . , bn ∈Zd be linearly independent d-dimensional vectors. All vectors
are row representations. The lattice L(b1, . . . , bn) spanned by the basis vectors
b1, . . . , bn is deﬁned as
L(b1, . . . , bn) =
⎧
⎨
⎩
n

j=1
cjbj : cj ∈Z
⎫
⎬
⎭.
We also use the matrix representation for lattice bases. A basis matrix B is
deﬁned as the n × d matrix which has basis vectors b1, . . . , bn in each row. In
this representation, a lattice spanned by the basis matrix B is deﬁned as L(B) =
{cB : c ∈Zn}. We call n a rank of the lattice and d a dimension of the lattice.
We call the lattice full-rank when n = d. In this paper, we only use full-rank
lattices. A parallelepiped of a lattice is deﬁned as P(B) = {cB : 0 ≤cj < 1}.
We deﬁne a determinant of a lattice det(L(B)) as an n-dimensional volume of
the parallelepiped. In general, a determinant of a lattice can be computed as
det(L(B)) =

det(BBT) where BT is a transpose of B. A determinant of a
full-rank lattice can be computed as det(L(B)) = | det(B)|.
In the context of cryptographic research, especially cryptanalysis, to ﬁnd
short lattice vectors is an important operation. See [28] for more information.
Although to ﬁnd the exact shortest vector is a computationally hard problem,
LLL algorithm [21] proposed by Lenstra, Lenstra, and Lov´asz ﬁnds the approx-
imate shortest lattice vectors in polynomial time.
Proposition 1 (LLL algorithm [25]). Given a lattice basis B ∈Zn×n, the
LLL algorithm ﬁnds short vectors v1 and v2 in L(B) which satisfy
∥v1∥≤2(n−1)/4(det(L(B)))1/n and ∥v2∥≤2n/4(det(L(B)))1/(n−1).
These norms are Euclidean norms. The running time of the LLL algorithm is
polynomial time in n and input length.
Although output vectors by the LLL algorithm are not the exact shortest lattice
vectors in the worst case, the quality is suﬃcient for Coppersmith’s method [6,17].
In the rest of this section, we explain the overview of Coppersmith’s method
and how to use the LLL algorithm. For a k-variate polynomial h(x1, . . . , xk) =
 hi1,...,ikxi1
1 · · · xik
k , let ∥h(x1, . . . , xk)∥=
 h2
i1,...,ik and ∥h(x1, . . . , xk)∥∞=
maxi1,...,ik |hi1,...,ik| denote the norms of polynomials. To solve a modular equa-
tion h(x1, . . . , xk) = 0 mod R, it suﬃces to ﬁnd k polynomials which have the
same roots over the integers as the original equation. To derive such polynomi-
als from the modular equation, we introduce the following Howgrave-Graham’s
lemma [17].
Lemma 1 (Howgrave-Graham’s
Lemma [17]). Let h(x1, . . . , xk)
∈
Z
[x1, . . . , xk] be a polynomial over the integers which consists of at most n
monomials. Let X1, . . . , Xk, R, and m be positive integers. If the polynomial
h(x1, . . . , xk) satisﬁes

40
A. Takayasu and N. Kunihiro
– h(˜x1, . . . , ˜xk) = 0 mod Rm, where |˜x1| < X1, . . . , |˜xk| < Xk,
– ∥h(x1X1, . . . , xkXk)∥< Rm/√n.
Then h(˜x1, . . . , ˜xk) = 0 holds over the integers.
Hence, to ﬁnd k polynomials which have the same roots as the original modular
polynomial, it is suﬃcient to ﬁnd algebraically independent k polynomials which
have the same roots modulo Rm and whose norms are small enough to satisfy
Howgrave-Graham’s lemma. Then Gr¨obner bases or the resultant of the polyno-
mials reveal the solution of the modular equation. For the purpose, we generate
n polynomials h1(x1, . . . , xk), . . . , hn(x1, . . . , xk) and construct a lattice whose
basis consists of coeﬃcients of h1(x1X1, . . . , xkXk), . . . , hn(x1X1, . . . , xkXk).
Then the LLL algorithm ﬁnds lattice vectors and the corresponding polyno-
mials whose norms are small. Therefore, the original modular equation can be
solved and all operations can be terminated in polynomial time.
To ﬁnd larger roots, what we should do is to construct lattices which contain
shorter lattice vectors. It is a challenging problem in this research area and the
main focus of this paper. For the purpose, we introduce the lattice construction
strategy which makes use of the notion of helpful polynomials. To solve a mod R
equation, helpful polynomials, which was introduced by May [26], have smaller
diagonals than the modulus Rm in a triangular basis matrix. Since the norms
of short vectors output by the LLL algorithm are roughly det(L(B))1/n and
the determinant is computed by a product of all diagonals, helpful polynomials
can reduce the norms. To summarize, Takayasu and Kunihiro [35] suggested
that as many helpful polynomials as possible and as few unhelpful polynomials
as possible should be selected in lattice bases as long as the basis matrix is
triangular. Indeed, they used the strategy and improved the Herrmann-May
algorithm [15].
We should note that the method requires heuristic argument. There are no
assurance if new polynomials obtained by vectors output by the LLL algorithm
are algebraically independent. In this paper, we assume that these polynomials
are always algebraically independent and resultants of polynomials will not van-
ish as previous works [2,23,38]. Moreover, there have been few negative reports
which contradict the assumption.
3
Lattice Construction of the Takayasu-Kunihiro
In this section, we summarize the Takayasu-Kunihiro lattice construction to solve
a mod eM equation. To understand the spirit of the lattice construction helps
readers to understand the point of our improvement easily in Sect. 4.
As discussed in Sect. 1, Takayasu and Kunihiro [38] found the small roots of
the modular polynomial
f(x, y) := 1 −e ˜dp + x(y −1)
mod eM
whose root is (x, y) = (ℓ, p). To obtain a better result, they also used an addi-
tional variable z = q to make use of the algebraic relation yz = N as [1,11].

Partial Key Exposure Attacks on CRT-RSA
41
The absolute values of the solutions are bounded above by X := N α, Y :=
N 1/2, Z := N 1/2 within constant factors. If two polynomials which have the
roots (x, y) = (ℓ, p) over the integers are obtained, then the roots can be recov-
ered. Notice that although there are three variables x, y, and z, two polynomials
suﬃce to disclose the root since we already know the relation yz = N.
To solve the above modular equation f(x, y) = 0, they constructed lattices
where the basis consists of coeﬃcients of the following shift-polynomials:
g[i,j](x, y, z) := xjzsf(x, y)i(eM)m−i,
and
g′
[i,j](x, y, z) := yjzsf(x, y)i(eM)m−i,
with some positive integers m and s = ηm where 0 ≤η ≤1. These polynomials
modulo (eM)m have the same root as the original modular polynomial, i.e.,
g[i,j](ℓ, p, q) = 0 mod (eM)m and g′
[i,j](ℓ, p, q) = 0 mod (eM)m. They collected
shift-polynomials
g[i,j](x, y, z)
for i = 0, 1, . . . , m; j = 0, 1, . . . , m −i,
and
g′
[i,j](x, y, z)
for i = 0, 1, . . . , m; j = 1, 2, . . . , t,
in a lattice basis with some positive integer t = τm where 0 ≤τ ≤η ≤1.
To reduce a determinant of the lattice, they multiply the inverse of N mod-
ulo (eM)m. This operation eliminates the powers of N in diagonals. Properly
ordered, the collection of polynomials generates a triangular basis matrix with
diagonals
– Xi+jY i−s(eM)m−i for g[i,j](x, y, z) and i ≥s,
– Xi+jZs−i(eM)m−i for g[i,j](x, y, z) and i < s,
– XiY i+j−s(eM)m−i for g′
[i,j](x, y, z) and i + j ≥s,
– XiZs−i−j(eM)m−i for g′
[i,j](x, y, z) and i + j < s.
Then a dimension n and a determinant of the lattice det(L(B)) = XsXY sY ZsZ ·
(eM)seM are computed by
n =
m

i=0
m−i

j=0
1 +
m

i=0
t

j=1
1 =
1
2 + τ

m2 + o(m2),
sX =
m

i=0
m−i

j=0
(i + j) +
m

i=0
t

j=1
i =
1
3 + τ
2

m3 + o(m3),
sY =
m

i=s
m−i

j=0
(i −s) +
m

i=s−t
t

j=s−t−i
(i + j −s) = (1 + τ −η)3
6
m3 + o(m3),
sZ =
s−1

i=0
m−i

j=0
(s −i) +
s−1

i=0
min{t,s−i}

j=1
(s −i −j)
=
η2
2 −(η −τ)2
6

m3 + o(m3),
seM =
m

i=0
m−i

j=0
(m −i) +
m

i=0
t

j=1
(m −i) =
1
3 + τ
2

m3 + o(m3).

42
A. Takayasu and N. Kunihiro
LLL outputs short lattice vectors and the corresponding polynomials satisfy
Howgrave-Graham’s lemma when XsXY sY ZsZ(eM)seM < (eM)mn. Ignoring
low order terms of m, the condition becomes
α
1
3 + τ
2

+ 1
2
(1 + τ −η)3
6
+ η2
2 −(η −τ)2
6

<

α + 1
2 −δ
 1
2 + τ −1
3 −τ
2

.
To maximize the right hand side of the inequality, optimizing the parameters
η = 1 −2δ
2
and τ =
√
1 −4δ −2δ
2
,
then the above condition results in
−1 + 8α + 8δ −12δ2 −2(1 −4δ)
√
1 −4δ < 0.
4
Our Proposed Attack
In this section, we propose an improved attack for 1/18 < δ ≤3/8, i.e., the
ﬁrst condition of Theorem 1. We revisit the previous lattice construction in
Sect. 4.2 and ﬁnd the drawback. Then we solve the same modular equation as
the Takayasu-Kunihiro [38] and ﬁnd larger roots.
4.1
An Observation of the Previous Lattice
As we explained in Sect. 3, Takayasu and Kunihiro [38] constructed a lattice
whose basis consists of polynomials which have the same roots as the original
polynomial modulo (eM)m. We want to analyze the validity of the lattice con-
struction by using the helpful polynomials strategy [26,35]. Based on the strat-
egy, as many helpful polynomials (which have diagonals whose sizes are smaller
than (eM)m) as possible should be selected and as few unhelpful polynomials
(which have diagonals whose sizes are larger than (eM)m) as possible should be
eliminated as long as a basis matrix to be triangular.
Then we observe the Takayasu-Kunihiro lattice after the parameter optimiza-
tion. There are polynomials with diagonals
– Xi+jY i−s(eM)m−i for i = s, s + 1, . . . , m; j = 0, 1, . . . , m −i,
– Xi+jZs−i(eM)m−i for i = 0, 1, . . . , s −1; j = 0, 1, . . . , m −i,
– XiY i+j−s(eM)m−i for i = s−t, s−t+1, . . . , m; j = s−t−i, s−t−i+1, . . . , t,
– XiZs−i−j(eM)m−i for i = 0, 1, . . . , s −1; j = 1, 2, . . . , min{t, s −i}.
We focus on the bottom two families of polynomials, i.e., g′
[i,j](x, y, z). The lat-
tice basis does not contain as many helpful polynomials as possible since when
polynomials
g′
[i,j](x, y, z)
for i =1, 2, . . . , s −1; j = s −i
are added in the basis, the corresponding diagonals become

Partial Key Exposure Attacks on CRT-RSA
43
– Xi(eM)m−i for i = 1, 2, . . . , s −1
and
Xi(eM)m−i = N(α+ 1
2 −δ)m−( 1
2 −δ)i < N(α+ 1
2 −δ)m = (eM)m.
Similarly, the lattice basis contains some unhelpful polynomials which do not
contribute for the basis matrix to be triangular. Indeed, the basis matrix is still
triangular without polynomials
g′
[i,j](x, y, z)
for i =
1 −
√
1 −4δ
4δ
m

,
1 −
√
1 −4δ
4δ
m

+ 1 . . . , m; j = t
whose corresponding diagonals are
– XiY i+t−s(eM)m−i for i =

1−√1−4δ
4δ
m

,

1−√1−4δ
4δ
m

+ 1, . . . , m
and the following inequality holds:
XiY i+t−s(eM)m−i = N(α+ 1
2 −δ)m+δi−1
2 (s−t) > N(α+ 1
2 −δ)m = (eM)m.
Notice that
δi −1
2(s −t) = δi −1
2
1 −2δ
2
−
√
1 −4δ −2δ
2

m
= δ

i −1 −
√
1 −4δ
4δ
m

> 0
for all i =

1−√1−4δ
4δ
m

,

1−√1−4δ
4δ
m

+ 1, . . . , m.
The above examples are not all the helpful polynomials which are not selected
and all the unhelpful polynomials which are selected. Hence, if we can construct
more appropriate lattices, the resulting attack condition should be improved.
4.2
Our Lattice Construction
Based on the observation in Sect. 4.1, we construct more appropriate lattices than
the Takayasu-Kunihiro [38]. More concretely, we select all helpful g′
[i,j](x, y, z)
for i + j ≥s and do not select any unhelpful g′
[i,j](x, y, z) for i + j ≥s.
At ﬁrst, we analyze which g′
[i,j](x, y, z) for i + j ≥s are helpful or not. As we
explained in Sect. 3, the corresponding diagonals are XiY i+j−s(eM)m−i. Then
the polynomials are helpful when
XiY i+j−s(eM)m−i < (eM)m ⇔αi + 1
2 (i + j −s) <

α + 1
2 −δ

i
⇔j < s −2δi.

44
A. Takayasu and N. Kunihiro
Therefore, we collect the following shift-polynomials:
g[i,j](x, y, z)
for i = 0, 1, . . . , m; j = 0, 1, . . . , m −i and
g′
[i,j](x, y, z)
for i = 0, 1, . . . , m; j = 1, 2, . . . , ⌊s −2δi⌋
in a lattice basis. Here, we do not take into account if polynomials g[i,j](x, y, z)
and g′
[i,j](x, y, z) for i + j < s are helpful or not, however, these polynomials
contribute the basis matrix to be triangular. Hence, we use the above collection
of shift-polynomials only when η > 2δ. Otherwise, polynomials g[i,j](x, y, z) for
i+j >
η
2δm do not contribute the basis matrix to be triangular. We will analyze
the other case in Sect. 5.
We compute the resulting attack condition. A dimension n and a determinant
of the lattice det(L(B)) = XsXY sY ZsZ(eM)seM are computed by
n =
m

i=0
m−i

j=0
1 +
m

i=0
⌊s−2δi⌋

j=1
1 =
1
2 −δ + η

m2 + o(m2),
sX =
m

i=0
m−i

j=0
(i + j) +
m

i=0
⌊s−2δi⌋

j=1
i =
1 −2δ
3
+ η
2

m3 + o(m3),
sY =
m

i=s
m−i

j=0
(i −s) +
m

i=0
⌊s−2δi⌋

j=max{s−i+1,0}
(i + j −s) = (1 −2δ)2
6
m3 + o(m3),
sZ =
s−1

i=0
m−i

j=0
(s −i) +
s−1

i=0
s−i

j=0
(s −i −j) = η2
2 m3 + o(m3),
seM =
m

i=0
m−i

j=0
(m −i) +
m

i=0
⌊s−2δi⌋

j=1
(m −i) =
1 −δ
3
+ η
2

m3 + o(m3).
LLL outputs short lattice vectors and the corresponding polynomials satisﬁes
Howgrave-Graham’s lemma when XsXY sY ZsZ(eM)seM < (eM)mn. Ignoring
low order terms of m, the condition becomes
α
1 −2δ
3
+ η
2

+ 1
2
(1 −2δ)2
6
+ η2
2

<

α + 1
2 −δ
 1
6 −2δ
3 + η
2

.
To maximize the right hand side of the inequality, we set the parameter η to be
a solution of
α1
2 + 1
2η =

α + 1
2 −δ
 1
2,
that is,
η = 1 −2δ
2
.
By substituting the parameter, the above attack condition becomes
7(1 −2δ)2 −4(1 −2δ) −8(α + 1/2) + 4 > 0.

Partial Key Exposure Attacks on CRT-RSA
45
Therefore, the attack works when
δ < 5 −2√1 + 14α
14
as required.
Notice that the attack works only when 1−2δ
2
> 2δ, that leads to δ < 1
6 and
equivalent to
α > 1
18.
5
Concluding Remarks
In this paper, we study the partial key exposure attacks on CRT-RSA when
the least signiﬁcant bits of CRT exponent are exposed to attackers. We solve
the same modular equation as Takayasu and Kunihiro [38], however, make use
of the property of helpful polynomials and provide better lattice constructions.
Our attack is better than previous attacks for all 0 < e ≤N 0.375; the attack of
Lu et al. [23] as well as the Takayasu-Kunihiro [38].
There are similar works for partial key exposure attacks on CRT-RSA which
are not the scope of this paper; attacks when the most signiﬁcant bits of CRT
exponent are exposed to attackers. There are attacks with the exposed most
signiﬁcant bits by Bl¨omer and May [2] and the Takayasu-Kunihiro [38] which are
analogous to the attack with the exposed least signiﬁcant bits by Lu et al. [23]
and the Takayasu-Kunihiro [38], respectively. An interesting open problem is
whether there exists a partial key exposure attack on CRT-RSA with the exposed
most signiﬁcant bits which is analogous to our attack with the exposed least
signiﬁcant bits.
Acknowledgement. The ﬁrst author is supported by a JSPS Fellowship for Young
Scientists. This research was supported by JSPS Grant-in-Aid for JSPS Fellows
14J08237, CREST, JST, and KAKENHI Grant Number 25280001.
References
1. Bleichenbacher, D., May, A.: New attacks on RSA with small secret CRT-
exponents. In: Yung, M., Dodis, Y., Kiayias, A., Malkin, T. (eds.) PKC 2006.
LNCS, vol. 3958, pp. 1–13. Springer, Heidelberg (2006)
2. Bl¨omer, J., May, A.: New partial key exposure attacks on RSA. In: Boneh, D. (ed.)
CRYPTO 2003. LNCS, vol. 2729, pp. 27–43. Springer, Heidelberg (2003)
3. Boneh, D., Durfee, G.: Cryptanalysis of RSA with private key d less than N 0.292.
IEEE Trans. Inf. Theor. 46(4), 1339–1349 (2000)
4. Boneh, D., Durfee, G., Frankel, Y.: An attack on RSA given a small fraction of the
private key bits. In: Ohta, K., Pei, D. (eds.) ASIACRYPT 1998. LNCS, vol. 1514,
pp. 25–34. Springer, Heidelberg (1998)
5. Coppersmith, D.: Finding a small root of a bivariate integer equation; factoring
with high bits known. In: Maurer, U.M. (ed.) EUROCRYPT 1996. LNCS, vol.
1070, pp. 178–189. Springer, Heidelberg (1996)

46
A. Takayasu and N. Kunihiro
6. Coppersmith, D.: Finding a small root of a univariate modular equation. In: Mau-
rer, U.M. (ed.) EUROCRYPT 1996. LNCS, vol. 1070, pp. 155–165. Springer,
Heidelberg (1996)
7. Coppersmith, D.: Small solutions to polynomial equations, and low exponent RSA
vulnerabilities. J. Cryptol. 10(4), 233–260 (1997)
8. Coppersmith, D.: Finding small solutions to small degree polynomials. In:
Silverman, J.H. (ed.) CaLC 2001. LNCS, vol. 2146, pp. 20–31. Springer,
Heidelberg (2001)
9. Coron, J.-S.: Finding small roots of bivariate integer polynomial equations revis-
ited. In: Cachin, C., Camenisch, J.L. (eds.) EUROCRYPT 2004. LNCS, vol. 3027,
pp. 492–505. Springer, Heidelberg (2004)
10. Coron, J.-S.: Finding small roots of bivariate integer polynomial equations: a direct
approach. In: Menezes, A. (ed.) CRYPTO 2007. LNCS, vol. 4622, pp. 379–394.
Springer, Heidelberg (2007)
11. Durfee, G., Nguyˆen, P.Q.: Cryptanalysis of the RSA schemes with short secret
exponent from Asiacrypt 1999. In: Okamoto, T. (ed.) ASIACRYPT 2000. LNCS,
vol. 1976, pp. 14–29. Springer, Heidelberg (2000)
12. Ernst, M., Jochemsz, E., May, A., de Weger, B.: Partial key exposure attacks on
RSA up to full size exponents. In: Cramer, R. (ed.) EUROCRYPT 2005. LNCS,
vol. 3494, pp. 371–386. Springer, Heidelberg (2005)
13. Faug`ere, J.-C., Marinier, R., Renault, G.: Implicit factoring with shared most sig-
niﬁcant and middle bits. In: Nguyen, P.Q., Pointcheval, D. (eds.) PKC 2010. LNCS,
vol. 6056, pp. 70–87. Springer, Heidelberg (2010)
14. Galbraith, S.D., Heneghan, C., McKee, J.F.: Tunable balancing of RSA. In: Boyd,
C., Gonz´alez Nieto, J.M. (eds.) ACISP 2005. LNCS, vol. 3574, pp. 280–292.
Springer, Heidelberg (2005)
15. Herrmann, M., May, A.: Solving linear equations modulo divisors: on factoring
given any bits. In: Pieprzyk, J. (ed.) ASIACRYPT 2008. LNCS, vol. 5350, pp.
406–424. Springer, Heidelberg (2008)
16. Herrmann, M., May, A.: Maximizing small root bounds by linearization and appli-
cations to small secret exponent RSA. In: Nguyen, P.Q., Pointcheval, D. (eds.)
PKC 2010. LNCS, vol. 6056, pp. 53–69. Springer, Heidelberg (2010)
17. Howgrave-Graham, N.: Finding small roots of univariate modular equations revis-
ited. In: Darnell, M.J. (ed.) Cryptography and Coding 1997. LNCS, vol. 1355.
Springer, Heidelberg (1997)
18. Jochemsz, E., May, A.: A strategy for ﬁnding roots of multivariate polynomials
with new applications in attacking RSA variants. In: Lai, X., Chen, K. (eds.)
ASIACRYPT 2006. LNCS, vol. 4284, pp. 267–282. Springer, Heidelberg (2006)
19. Jochemsz, E., May, A.: A polynomial time attack on RSA with private CRT-
exponents smaller than N 0.073. In: Menezes, A. (ed.) CRYPTO 2007. LNCS, vol.
4622, pp. 395–411. Springer, Heidelberg (2007)
20. Joye, M., Lepoint, T.: Partial key exposure on RSA with private exponents larger
than N. In: Ryan, M.D., Smyth, B., Wang, G. (eds.) ISPEC 2012. LNCS, vol. 7232,
pp. 369–380. Springer, Heidelberg (2012)
21. Lenstra, A., Lenstra, H., Lov´asz, L.: Factoring polynomials with rational coeﬃ-
cients. Math. Ann. 261, 515–534 (1982)
22. Lu, Y., Peng, L., Zhang, R., Hu, L., Lin, D.: Towards optimal bounds for implicit
factorization problem. In: Dunkelman, O., Keliher, L. (eds.) SAC 2015. LNCS, vol.
9566, pp. 462–476. Springer, Heidelberg (2016). doi:10.1007/978-3-319-31301-6 26

Partial Key Exposure Attacks on CRT-RSA
47
23. Lu, Y., Zhang, R., Lin, D.: New partial key exposure attacks on CRT-RSA with
large public exponents. In: Boureanu, I., Owesarski, P., Vaudenay, S. (eds.) ACNS
2014. LNCS, vol. 8479, pp. 151–162. Springer, Heidelberg (2014)
24. May, A.: Cryptanalysis of unbalanced RSA with small CRT-exponent. In: Yung, M.
(ed.) CRYPTO 2002. LNCS, vol. 2442, pp. 242–256. Springer, Heidelberg (2002)
25. May, A.: New RSA vulnerabilities using lattice reduction methods. Ph.D. thesis,
University of Paderborn (2003)
26. May, A.: Using LLL-reduction for solving RSA and factorization problems. In:
Nguyen, P.Q., Vall´ee, B. (eds.) The LLL Algorithm - Survey and Applications.
Information Security and Cryptography, pp. 315–348. Springer, Heidelberg (2010)
27. May, A., Ritzenhofen, M.: Implicit factoring: on polynomial time factoring given
only an implicit hint. In: Jarecki, S., Tsudik, G. (eds.) PKC 2009. LNCS, vol. 5443,
pp. 1–14. Springer, Heidelberg (2009)
28. Nguyˆen, P.Q., Stern, J.: The two faces of lattices in cryptology. In: Silverman, J.H.
(ed.) CaLC 2001. LNCS, vol. 2146, pp. 146–180. Springer, Heidelberg (2001)
29. Quisquater, J.J.: Fast decipherment algorithm for RSA public-key cryptosystem.
Electron. Lett. 18(2), 905–907 (1982)
30. Rivest, R.L., Shamir, A.: Eﬃcient factoring based on partial information. In:
Pichler, F. (ed.) EUROCRYPT 1985. LNCS, vol. 219, pp. 31–34. Springer,
Heidelberg (1986)
31. Rivest, R.L., Shamir, A., Adleman, L.M.: A method for obtaining digital signatures
and public-key cryptosystems. Commun. ACM 21(2), 120–126 (1978)
32. Sarkar, S., Maitra, S.: Partial key exposure attack on CRT-RSA. In: Abdalla, M.,
Pointcheval, D., Fouque, P.-A., Vergnaud, D. (eds.) ACNS 2009. LNCS, vol. 5536,
pp. 473–484. Springer, Heidelberg (2009)
33. Sarkar, S., Maitra, S.: Approximate integer common divisor problem relates to
implicit factorization. IEEE Trans. Inf. Theor. 57(6), 4002–4013 (2011)
34. Sarkar, S., Sen Gupta, S., Maitra, S.: Partial key exposure attack on RSA –
improvements for limited lattice dimensions. In: Gong, G., Gupta, K.C. (eds.)
INDOCRYPT 2010. LNCS, vol. 6498, pp. 2–16. Springer, Heidelberg (2010)
35. Takayasu, A., Kunihiro, N.: Better lattice constructions for solving multivariate lin-
ear equations modulo unknown divisors. IEICE Trans. 97(A(6)), 1259–1272 (2014)
36. Takayasu, A., Kunihiro, N.: General bounds for small inverse problems and its
applications to multi-prime RSA. In: Lee, J., Kim, J. (eds.) Information Security
and Cryptology - ICISC 2014. LNCS, vol. 8949, pp. 3–17. Springer, Heidelberg
(2014)
37. Takayasu, A., Kunihiro, N.: Partial key exposure attacks on RSA: achieving the
boneh-durfee bound. In: Joux, A., Youssef, A. (eds.) SAC 2014. LNCS, vol. 8781,
pp. 345–362. Springer, Heidelberg (2014)
38. Takayasu, A., Kunihiro, N.: Partial key exposure attacks on CRT-RSA: better
cryptanalysis to full size encryption exponents. In: Malkin, T., Kolesnikov, V.,
Lewko, A., Polychronakis, M. (eds.) ACNS 2015. LNCS, vol. 9092, pp. 518–537.
Springer, Heidelberg (2015). doi:10.1007/978-3-319-28166-7 25
39. de Weger, B.: Cryptanalysis of RSA with small prime diﬀerence. Appl. Algebra
Eng. Commun. Comput. 13(1), 17–28 (2002)
40. Wiener, M.J.: Cryptanalysis of short RSA secret exponents. IEEE Trans. Inf.
Theor. 36(3), 553–558 (1990)

Cryptanalysis and Improved Construction
of a Group Key Agreement for Secure
Group Communication
Jun Xu1,2, Lei Hu1,2(B), Xiaona Zhang1,2, Liqiang Peng1,2,
and Zhangjie Huang1,2
1 State Key Laboratory of Information Security,
Institute of Information Engineering,
Chinese Academy of Sciences, Beijing 100093, China
{xujun,hulei,zhangxiaona,pengliqiang,huangzhangjie}@iie.ac.cn
2 Data Assurance and Communications Security Research Center,
Chinese Academy of Sciences, Beijing 100093, China
Abstract. In this paper, we give a ciphertext-only attack on a NTRU-
based group key agreement. Our attack can recover the plaintext with-
out having access to the secret decryption key of any group member
even when there are only two group members. In order to overcome this
drawback, we propose an improved group key agreement and make the
corresponding cryptanalysis, which shows that it is secure and resilient
to this ciphertext-only attack as well as other attacks under some con-
straints.
Keywords: Group key agreement · Secure group communication ·
NTRU cryptosystem · Ciphertext-only attack
1
Introduction
An symmetric group key agreement enables two or more members to create a
common secret key, which is used by group members for encryption and decryp-
tion. Diﬃe and Hellman [5] constructed the ﬁrst symmetric group key agreement
in 1976, which is a one-round two-party protocol. Joux [11] proposed a tripartite
version of the Diﬃe-Hellman protocol in 2004. Upon to now, a mass of schemes
have been put forward in order to generalize the Diﬃe-Hellman protocol to the
multi-party situations, such as [2,4,10,12,13,15].
In Eurocrypt 2009, Wu et al. [20] proposed the concept of asymmetric group
agreement, which is a new class of group key agreement. In this sense, all group
members negotiate and publish a common group encryption key but withhold
respective secret decryption keys. Each member can correctly decrypt any cipher-
text encrypted with the common encryption key by using his own secret key. Wu
et al. also gave the generic construction and a concrete one-round asymmetric
group agreement based on bilinear pairings. However, their protocol is unau-
thenticated and only ﬁts for static groups.
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 48–58, 2016.
DOI: 10.1007/978-3-319-45871-7 4

Cryptanalysis and Improved Construction of a Group Key Agreement
49
A diﬀerent generic construction about the asymmetric group key agreement
was designed in [16,17]. The common group encryption key is generated by
combing public keys of all members using the idea of Chinese remainder theorem.
The designers in [16] presented an instantiation based on the ElGamal scheme
with both conﬁdentiality and nonrepudiation for mobile ad-hoc networks. The
authors in [17] proposed another concrete NTRU-based asymmetric group key
agreement for dynamic peer systems, which is an one-round, distributed and self-
organizing protocol. The designers pointed out that the security of this protocol
depends on NTRU cryptosystem.
NTRU is a lattice-based post-quantum encryption algorithm with its security
depends on the intractability of solving certain lattice problems. The ﬁrst ver-
sion of NTRU was proposed by Hoﬀstein, Pipher and Silverman in 1998 [7], that
is denoted by NTRU-1998. Subsequently, two main variants with diﬀerent para-
meter sets were presented in 2001 and 2005, which are respectively NTRU-2001
[8] and NTRU-2005 [9]. NTRU has been adopted as the IEEE 1363.1 Standard
[19] and the X9.98 Standard [1] for use in the ﬁnancial services industry.
In this paper, we demonstrate that the NTRU-based group key agreement
proposed in [17] is not secure. Any attacker can recover the plaintext when only
getting access to the corresponding ciphertext. Furthermore, we put forward an
improved design to resist this ciphertext-only attack, and reduce its security to
NTRU cryptosystem.
The rest of this paper is organized as follows. In Sect. 2, we introduce some
terminology and NTRU cryptosystem. In Sect. 3, we recall a NTRU-based group
key agreement. The ciphertext-only attack and experimental results are pre-
sented in Sect. 4. In Sect. 5, we give an improved construction and security analy-
sis. Section 6 is the conclusion.
2
Preliminary
For the rest of the paper, Z, Zm and Q are respectively denoted as integer ring,
the residue class ring and the rational ﬁeld, where m is a positive integer. Here,
Zm of m elements is regarded as the set of integers from (−m
2 , m
2 ]. For positive
integer N, R, Rm and R′ are respectively presented Z[x]/(xN −1), Zm[x]/
(xN−1) and Q[x]/(xN−1), which are the set of polynomials with degree less than
N and coeﬃcients in Z, Zm and Q respectively. Let ∗be cyclic multiplication.
For a(x) =
N−1

i=0
aixi, b(x) =
N−1

j=0
bjxj ∈R′, the product c(x) =
N−1

k=0
ckxk of a(x)
and b(x) is given by
c(x) = a(x) ∗b(x) with ck =

i+j=k (mod N)
aibj.

50
J. Xu et al.
For any polynomial a(x) =
N−1

i=0
aixi ∈R′, its vector and matrix forms are
respectively written as a = (a0, a1 · · · , aN−1)T and
Ma =
⎡
⎢⎢⎣
a0
aN−1 · · · a1
a1
a0
· · · a2
· · ·
aN−1 aN−2 · · · a0
⎤
⎥⎥⎦.
Thus, the corresponding vector and matrix forms of a(x) ∗b(x) are Ma · b and
Ma · Mb.
2.1
NTRU Public Key Cryptosystem
We give a description of NTRU cryptosystem. Please refer to [7–9] for detailed
information.
Parameters: The tuple (N, p, q, Lf, Lg, Lr, Lm) is a set of parameters. N is
an odd prime number. p is an integer or a polynomial. q is a positive integer.
Lf, Lg, Lr, Lm are subsets of R. Lf, Lg are related to private key space. Lr, Lm
are respectively random polynomial space and plaintext space.
Key generation: Randomly choose a polynomial f(x) ∈Lf such that there
exist polynomials fp(x), fq(x) ∈R satisfying
f(x) ∗fp(x) = 1 (mod p) and f(x) ∗fq(x) = 1 (mod q).
Randomly choose a polynomial g(x) ∈Lg and compute polynomial h(x) ∈Rq
by h(x) = p ∗fq(x) ∗g(x)mod q.
Public and private key: Polynomial h(x) is public key and tuple (f(x), fp(x))
is private key.
Encryption: For a plaintext m(x) ∈Lm, randomly choose a polynomial r(x) ∈
Lr and compute ciphertext c(x) ∈Rq as
c(x) = h(x) ∗r(x) + m(x) mod q.
(1)
Decryption: The receiver ﬁrst computes polynomial a(x) ∈Rq by a(x) =
f(x)∗c(x) mod q, which is equal to a(x) = p∗g(x)∗r(x)+f(x)∗m(x) (mod q).
Note that almost all coeﬃcients of p∗g(x)∗r(x)+f(x)∗m(x) are in (−q/2, q/2],
one has that a(x) = p ∗g(x) ∗r(x) + f(x) ∗m(x) with a very high possibility.
Finally, the receiver recovers plaintext m(x) due to m(x) = a(x) ∗fp(x) mod p.
Mol and Yung summarized the parameters of three main instantiations of
NTRU in [18] and we list their comparison in Table 1.
Here, df, dg, dr are known positive integers. B, B(d), T are subsets of R. The
coeﬃcients of all polynomials in B belong to {0, 1}. B(d) is a subset of B. For
each polynomial in B(d), there are exactly d 1’s in its coeﬃcients. The coeﬃcients
of all polynomials in T are in {−1, 0, 1}. For integers d1 and d2, L(d1, d2) is a
subset of T. For each polynomial in L(d1, d2), there are exactly d1 1’s and d2
−1’s in its coeﬃcients.

Cryptanalysis and Improved Construction of a Group Key Agreement
51
Table 1. Parameters of NTRU variants
Variants
q
p
Lf
Lg
Lr
Lm
NTRU-1998 2k ∈[ N
2 , N] 3
L(df, df −1)
L(dg, dg) L(dr, dr) T
NTRU-2001 2k ∈[ N
2 , N] x + 2 1 + p ∗B(df) B(dg)
B(dr)
B
NTRU-2005 prime
2
1 + p ∗B(df) B(dg)
B(dr)
B
3
Description of an NTRU-Based Group Key Agreement
In this section, we review an NTRU-based group key agreement proposed in
[17]. Please refer to [17] for further information. Here we denote the set of group
members as {u1, u2, . . ., un}.
3.1
An NTRU-Based Group Key Agreement
1: Every member’s public and private key. Every member ui (i = 1, . . . , n)
randomly generates a public key hi(x) and the corresponding private key

fi(x), fip(x)

of the NTRU cryptosystem. Every member broadcasts the
corresponding public key to other group members.
2: Public parameters. A sponsor member picks n considerably larger positive
integers m1, m2, . . . , mn compared with q in the NTRU cryptosystem. These
integers are relatively prime in pairs. The sponsor member broadcasts his
public key along with m1, m2, . . . , mn.
3: Group
encryption
key. Let M
=
n
i=1
mi, Mi = M/mi and yi
=
M −1
i
mod mi. Every member ui (i = 1, . . . , n) picks the corresponding mi
and locally computes the common group encryption key h(x) ∈RM by
h(x) =
n

i=1
Miyihi(x) mod M.
(2)
4: Encryption. Given a plaintext m(x) ∈Lm, any group member can randomly
choose a polynomial r(x) in Lr and compute ciphertext c(x) ∈RM as
c(x) =

h(x) ∗r(x) +
 n

i=1
Miyi

∗m(x)

mod M.
(3)
5: Decryption. For a given ciphertext c(x), every group member ui (i =
1, . . . , n) compute ci(x) = c(x) mod mi, where the coeﬃcients of ci(x) lie
in (−mi/2, mi/2]. This equation can be reduced to
ci(x) = hi(x) ∗r(x) + m(x) (mod mi).
According to the mi are suﬃciently larger than q, there is
ci(x) = hi(x) ∗r(x) + m(x).
(4)

52
J. Xu et al.
Then, he can recover the plaintext m(x) by using the corresponding private
key

fi(x), fip(x)

like the decryption procedure in the NTRU cryptosystem.
Remark 1. In practice, the designers of [17] pointed out that each member ui
should broadcast h′
i(x) = hi(x) ∗ϕi(x) instead of hi(x) for i = 1, . . . , n, where
the ϕi(x) are unknown random polynomials in Lr. In fact, this ϕi(x) can be
also regarded as the ephemeral secret keys to prevent possible attacks due to the
exposure of public key hi(x). In this situation, the common group encryption
key h(x) is generated by
h(x) =
n

i=1
Miyih′
i(x) mod M.
(5)
It is obvious that (5) degrades into (2) if ϕi(x) = 1 for i = 1, . . . , n.
4
Attack on NTRU-Based Group Key Agreement
In this section, we give an eﬃcient ciphertext-only attack on the NTRU-based
group key agreement.
4.1
Ciphertext-Only Attack
Noting that public keys h1(x), . . . , hn(x) and positive integers m1, . . . , mn are
publicly broadcasted, they can easily be obtained by any attacker, which means
that the attacker can get Eq. (4), namely,
ci(x) = hi(x) ∗r(x) + m(x) for i = 1, . . . , n.
Under the usual sense of polynomial multiplication, the above equation can be
transformed into
ci(x) = hi(x)r(x) + m(x) (mod xN −1) for i = 1, . . . , n.
(6)
It is clear that the attacker can recover the plaintext m(x) once the random
polynomial r(x) is revealed. According to this idea, the attacker ﬁrst eliminates
m(x) in (6) and gets the following equations
ci(x) −cj(x) = (hi(x) −hj(x)) r(x) (mod xN −1), where 1 ≤i < j ≤n.
Since N is a prime and xN −1 = (xN−1 + · · · + x + 1)(x −1), we know
that xN−1 + · · · + x + 1 and x −1 are coprime and irreducible over Q according
to linear algebra. Hence, there exist diﬀerent i and j such that polynomials
hi(x) −hj(x) and xN −1 are coprime with the overwhelming possibility. In
other words, hi(x)−hj(x) is likely to be invertible in R′. Therefore, the attacker
can compute out
r(x) = (hi(x) −hj(x))−1 (ci(x) −cj(x)) (mod xN −1).

Cryptanalysis and Improved Construction of a Group Key Agreement
53
Furthermore, the attacker recovers plaintext m(x) by computing
m(x) =

ci(x) −hi(x) (hi(x) −hj(x))−1 (ci(x) −cj(x))

(mod xN −1).
(7)
In fact, let ci = (ci,0, . . . , ci,N−1)T , r = (r0, . . . , rN−1)T and m = (m0, . . . ,
mN−1)T be the corresponding vectors of polynomials ci(x) =
N−1

k=0
ci,kxk, r(x) =
N−1

k=0
rkxk and m(x) =
N−1

k=0
mkxk. Let the corresponding matrices of polynomials
hi(x) =
N−1

k=0
hi,kxk be
Mhi =
⎡
⎢⎢⎣
hi,0
hi,N−1 · · · hi,1
hi,1
hi,0
· · · hi,2
· · ·
hi,N−1 hi,N−2 · · · hi,0
⎤
⎥⎥⎦with i = 1, . . . , n.
The polynomial equation (7) can also be rewritten as the following linear form
m = ci −Mhi · (Mhi −Mhj)−1 · (ci −cj),
(8)
where 1 ≤i < j ≤n.
4.2
Further Analysis
For the group encryption key h(x) in Remark 1, the above ciphertext-only attack
still works well. Similarly, the attacker can obtain the following polynomial equa-
tions
ci(x) = h′
i(x) ∗r(x) + m(x), i = 1, . . . , n.
The form of these equations is the same as (4) except that the public keys hi(x)
of the member ui is replaced with known h′
i(x). By performing the method in
Sect. 4.1, plaintext m(x) can be recovered by computing
m(x) =

ci(x) −h′
i(x)

h′
i(x) −h′
j(x)
−1 (ci(x) −cj(x))

(mod xN −1),
or
m = ci −Mh′
i · (Mh′
i −Mh′
j)−1 · (ci −cj),
where 1 ≤i < j ≤n.
4.3
Experiment Results
Our experiments were done by using the Magma computer algebra system [3]
on a PC with Intel(R) Core(TM) Quad CPU (2.83 GHz, 3.25 GB RAM, Win-
dows XP). Each experiment for a random instance were ﬁnished within 1 s. We
performed the above ciphertext-only attack against group key agreement based

54
J. Xu et al.
on three main instantiations of NTRU. We found that polynomial h1(x)−h2(x)
and matrix Mh1 −Mh2 are always invertible even when n = 2. For the case
that h(x) is generated by (5), we can also always obtain invertible polynomial
h′
1(x) −h′
2(x) and invertible matrix Mh′
1 −Mh′
2 even when n = 2. Some results
are listed in Table 2:
Table 2. Experiment results of attacking schemes based on diﬀerent variants of NTRU
Variants
N
q
p
df
dg
dr
Mh1 −Mh2
Mh′
1 −Mh′
2
Results
NTRU-1998 167 128 3
60 20 18 Invertible
Invertible
Success
NTRU-2001 167 128 x + 2 60 19 18 Invertible
Invertible
Success
NTRU-2005 107 97
2
25 24 25 Invertible
Invertible
Success
5
An Improved NTRU-Based Group Key Agreement
In this section, we propose an improved NTRU-based group key agreement to
prevent the above ciphertext-only attack and give the corresponding security
analysis.
5.1
The Improved Group Key Agreement
1: Every member’s public and private key. Every member ui (i = 1, . . . , n)
randomly choose a polynomial fi(x) ∈Lf such that there exist polynomials
fip(x), fiq(x) ∈R satisfying
fi(x) ∗fip(x) = 1 (mod p) and fi(x) ∗fiq(x) = 1 (mod q).
Randomly choose a polynomial gi(x) ∈Lg and compute invertible polynomial
hi(x) ∈Rq by hi(x) = p ∗fiq(x) ∗gi(x)mod q. Every member broadcasts the
corresponding public key to other group members.
2: Public parameters. A sponsor member picks n diﬀerent primes m1, m2,
. . . , mn satisfying min > (N + 2)q + 2, where min = min{m1, · · · , mn}. The
sponsor member broadcasts his public key along with m1, m2, . . . , mn. Every
member ui picks up the corresponding mi for i = 1, . . . , n.
3: Group
encryption
key. Let M
=
n
i=1
mi, Mi = M/mi and yi
=
M −1
i
mod mi. All members locally compute the common group encryption
key h(x) ∈RM according to
h(x) =
n

i=1
Miyihi(x) mod M.

Cryptanalysis and Improved Construction of a Group Key Agreement
55
4: Encryption.
Given
a
plaintext
m(x)
∈
Lm,
any
group
member
chooses n random polynomials r1(x), . . . , rn(x) ∈Lr
and n random
polynomials s1(x), . . . , sn(x) with all coeﬃcients are from the interval
(−⌊min−Nq−2
2q
⌋, ⌊min−Nq−2
2q
⌋]. Then, the member computes ciphertext c(x) ∈
RM as
c(x) =

h(x) ∗m(x) +
n

i=1
Miyi (ri(x) + q ∗si(x))

mod M.
(9)
Remark 2. Our encryption procedure is diﬀerent from [17]. This new design can
prevent the above ciphertext-only attack. Detailed analysis is presented in next
subsection.
5: Decryption. For a given ciphertext c(x), every group member ui (i =
1, . . . , n) can recover plaintext m(x) with the corresponding private key

fi(x), fip(x)

.
Next, we present the detailed decryption process and explain why it works.
Step 1. Every group member ui ﬁrstly computes the polynomial ci(x) ∈RM
as ci(x) = c(x) mod mi. According to (9), there is
ci(x) = hi(x) ∗m(x) + ri(x) + q ∗si(x)(mod mi).
Notice that all coeﬃcients of hi(x) and si(x) are respectively from (−q/2, q/2]
and (−⌊min−Nq−2
2q
⌋, ⌊min−Nq−2
2q
⌋], m(x) ∈Lm and ri(x) ∈Lr, according to the
triangular inequality, all coeﬃcients of polynomial hi(x)∗m(x)+ri(x)+q ∗si(x)
are bounded by
q
2N + 1 + q min −Nq −2
2q
= min
2
≤mi
2 .
Since all coeﬃcients of ci(x) are from (−mi/2, mi/2], there is
ci(x) = hi(x) ∗m(x) + ri(x) + q ∗si(x).
Furthermore, the member ui can get the following equation
ci(x) = hi(x) ∗m(x) + ri(x) (mod q).
(10)
Step 2. The member ui computes polynomial a(x) = fi(x) ∗ci(x) (mod q),
where a(x) ∈Rq. From (10), the member ui knows that
a(x) = p ∗gi(x) ∗m(x) + f(x) ∗ri(x) mod q.
Due to almost all coeﬃcient of polynomial p ∗gi(x) ∗m(x) + f(x) ∗ri(x) lie in
(−q/2, q/2] with a very high possibility. Therefore, a(x) = p∗gi(x)∗m(x)+f(x)∗
ri(x). Furthermore, ui can acquire ri(x) according to ri(x) = a(x)∗fip(x) mod p.
Step 3. The member ui recovers plaintext m(x) by computing
m(x) = h−1
i (x) ∗(ci(x) −ri(x)) mod q.

56
J. Xu et al.
5.2
Security Analysis
In this subsection, we give an analysis on the security of this improved group
key agreement based on NTRU cryptosystem.
First, we explain why this improved group key agreement is secure against
the ciphertext-only attack in Sect. 4. Notice that the key equations which can be
acquired by the attacker are (4) and (10) respectively. As for the former, there
are n polynomial equations with two unknown polynomials, thus, the plaintext
can be obtained only by using the eliminating technique. But for the latter,
there are n polynomial equations with n + 1 unknown polynomials. Hence, the
attacker can not ﬁnd the plaintext with the same technique.
Next, we give the following theorem to show that the security of our improved
group key agreement relies on the NTRU public key cryptosystem.
Theorem 1. The improved group key agreement in Sect. 5.1 is secure if the
underlying NTRU public key vcryptosystem is secure.
Proof. On one hand, public key hi(x) and private key

fi(x), fip(x)

of every
group member ui (1 ≤i ≤n) for the improved group key agreement are ran-
domly generated from the underlying NTRU public key cryptosystem. Thus, if
NTRU public key cryptosystem can prevent the attacks of recovering private
keys, the improved group key agreement in Sect. 5.1 is also secure under the
same situation.
On the other hand, according to the encryption equation (9) in the improved
group key agreement, the attacker can directly get the system of Eq. (10), i.e.,
ci(x) = hi(x) ∗m(x) + ri(x) (mod q) for i = 1, . . . , n.
Since all public keys hi(x) are invertible in Rq, the attacker can get the following
equations
h−1
i (x) ∗ci(x) = h−1
i (x) ∗ri(x) + m(x) (mod q) with i = 1, . . . , n.
Note that the form of the above equations is identical with the encryption equa-
tion (1) in NTRU public key cryptosystem. Hence, the improved group key
agreement in Sect. 5.1 is as secure as the NTRU public key cryptosystem on the
attacks about recovering the plaintext.
⊓⊔
Actually, there is constraints on the number n of the group members in
our improved group key agreement. When n is suﬃciently large, the broadcast
attacks on the NTRU cryptosystem in [6,14] may aﬀect the security of this
scheme. From the key equation (10), each ci(x) can be regarded as the ciphertext
of the same plaintext m(x) by diﬀerent public keys hi(x). Once the ciphertext
c(x) is exposed, the ci(x) can be easily obtained. In this circumstance, the attack
wants to recover the plaintext m(x). This is the so called broadcast attacks.
The authors in [6,14] gave lower bounds on n in order to make the broadcast
attacks work well, we present their results in Table 3.

Cryptanalysis and Improved Construction of a Group Key Agreement
57
Table 3. The least number of ciphertexts for success of broadcast attacks
Number of ciphertexts NTRU-1998 NTRU-2001 NTRU-2005
n [6]
N2+11
6
N+1
2
N+1
2
n [14]
3N−3
2
3N−5
2
3N−5
2
In order to prevent this attack, we give the constraints on the number n of
group members about this improved group key agreement as follows:
n <
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
3N−3
2
,
NTRU-1998,
N+1
2 ,
NTRU-2001,
N+1
2 ,
NTRU-2005,
where prime N is a parameter of NTRU public key cryptosystem.
6
Conclusion
In this paper, we proposed an eﬃcient ciphertext-only attack on a NTRU-based
group agreement. Our attack can recover the plaintext even when there are only
two group members. In order to overcome this disadvantage, we put forward
an improved construction. And ﬁnally, we presented detailed analyses on the
security of this improved scheme.
Acknowledgements. The authors would like to thank anonymous reviewers for their
helpful comments and suggestions. The work of this paper was supported by the
National Key Basic Research Program of China (Grants 2013CB834203), the National
Natural Science Foundation of China (Grants 61472417, 61472415 and 61502488), the
Strategic Priority Research Program of Chinese Academy of Sciences under Grant
XDA06010702, and the State Key Laboratory of Information Security, Chinese Acad-
emy of Sciences.
References
1. Security innovations NTRUEncrypt adopted as X9 standard for data protection.
Businesswire.com. Accessed 7 Dec 2014
2. Askoxylakis, I., Sauveron, D., Markantonakis, K., Tryfonas, T., Traganitis, A.:
A body-centered cubic method for key agreement in dynamic mobile ad hoc net-
works. In: Second International Conference on Emerging Security Information,
Systems and Technologies, SECURWARE 2008, pp. 193–202, August 2008
3. Bosma, W., Cannon, J., Playoust, C.: The magma algebra system I: the user
language. J. Symbolic Comput. 24(3–4), 235–265 (1997)
4. Burmester, M., Desmedt, Y.G.: A secure and eﬃcient conference key distribution
system. In: De Santis, A. (ed.) EUROCRYPT 1994. LNCS, vol. 950, pp. 275–286.
Springer, Heidelberg (1995)

58
J. Xu et al.
5. Diﬃe, W., Hellman, M.: New directions in cryptography. IEEE Trans. Inf. Theory
22(6), 644–654 (1976)
6. Ding, J., Pan, Y., Deng, Y.: An algebraic broadcast attack against NTRU. In:
Susilo, W., Mu, Y., Seberry, J. (eds.) ACISP 2012. LNCS, vol. 7372, pp. 124–137.
Springer, Heidelberg (2012)
7. Hoﬀstein, J., Pipher, J., Silverman, J.H.: NTRU: a ring-based public key cryptosys-
tem. In: Buhler, J.P. (ed.) ANTS 1998. LNCS, vol. 1423, pp. 267–288. Springer,
Heidelberg (1998)
8. Hoﬀstein, J., Silverman, J.: Optimizations for NTRU. Technical report, NTRU
Cryptosystems (2001)
9. Howgrave-Graham, N., Silverman, J.H., Whyte, W.: Choosing parameter sets for
NTRUEncrypt with NAEP and SVES-3. In: Menezes, A. (ed.) CT-RSA 2005.
LNCS, vol. 3376, pp. 118–135. Springer, Heidelberg (2005)
10. Ingemarsson, I., Tang, D., Wong, C.: A conference key distribution system. IEEE
Trans. Inf. Theory 28(5), 714–720 (1982)
11. Joux, A.: A one round protocol for tripartite Diﬃe-Hellman. J. Cryptology 17(4),
263–276 (2004)
12. Kim, Y., Perrig, A., Tsudik, G.: Communication-eﬃcient group key agreement. In:
Proceedings of the 16th International Conference on Information Security: Trusted
Information: The New Decade Challenge, SEC 2001, Norwell, MA, USA, pp. 229–
244. Kluwer Academic Publishers (2001)
13. Kim, Y., Perrig, A., Tsudik, G.: Tree-based group key agreement. ACM Trans. Inf.
Syst. Secur. 7(1), 60–96 (2004)
14. Li, J., Pan, Y., Liu, M., Zhu, G.: An eﬃcient broadcast attack against NTRU. In:
Proceedings of the 7th ACM Symposium on Information, Computer and Commu-
nications Security, ASIACCS 2012, pp. 22–23. ACM, New York (2012)
15. Li-ping, Z., Guo-Hua, C., Zhi-Gang, Y.: An eﬃcient group key agreement protocol
for ad hoc networks. In: 4th International Conference on Wireless Communications,
Networking and Mobile Computing, WiCOM 2008, pp. 1–5, October 2008
16. Lv, X., Li, H.: Secure group communication with both conﬁdentiality and non-
repudiation for mobile ad-hoc networks. IET Inf. Secur. 7(2), 61–66 (2013)
17. Lv, X., Li, H., Wang, B.: Group key agreement for secure group communication in
dynamic peer systems. J. Parallel Distrib. Comput. 72(10), 1195–1200 (2012)
18. Mol, P., Yung, M.: Recovering NTRU secret key from inversion oracles. In: Cramer,
R. (ed.) PKC 2008. LNCS, vol. 4939, pp. 18–36. Springer, Heidelberg (2008)
19. Whyte, W., Howgrave-Graham, N., Hoﬀstein, J., Pipher, J., Silverman, J.H.,
Hirschhorn, P.S.: IEEE p. 1363.1 draft 10: Draft standard for public key crypto-
graphic techniques based on hard problems over lattices. IACR Cryptology ePrint
Archive 2008/361 (2008)
20. Wu, Q., Mu, Y., Susilo, W., Qin, B., Domingo-Ferrer, J.: Asymmetric group key
agreement. In: Joux, A. (ed.) EUROCRYPT 2009. LNCS, vol. 5479, pp. 153–170.
Springer, Heidelberg (2009)

Enhanced Correlation Power Analysis
by Biasing Power Traces
Changhai Ou1,2(B), Zhu Wang1(B), Degang Sun1, Xinping Zhou1,2, Juan Ai1,2,
and Na Pang1,2
1 Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China
{ouchanghai,wangzhu,sundegang,zhouxinping,aijuan,pangna}@iie.ac.cn
2 University of Chinese Academy of Sciences, Beijing, China
Abstract. Biasing power traces with high Signal to Noise Ratio (SNR)
proposed by K. Yongdae et al. can signiﬁcantly improve the eﬃciency
of the CPA. But it is still a problem to be solved that how to eﬃ-
ciently select power traces with high SNR. Through the analysis of the
statistical characteristics of power traces, we propose three methods to
better solve this problem in this paper. We bias power traces by using
the Minkowski distance (i.e. Euclidean distance or Manhattan distance)
between each power trace and mean power trace. Biasing power traces
can also be carried out by using probability density function values of
power consumption of interesting points, or even directly using power
consumption of interesting points. Our schemes can blindly select power
traces with high SNR in a high probability. The eﬃciency of the CPA
by using the three of our methods is signiﬁcantly improved. Thus, our
schemes are more eﬀective compared to the one proposed by K. Yongdae
et al.
Keywords: Side channel · CPA · Power trace · Biasing power trace ·
Success rate
1
Introduction
There exist side channel leakages when cryptographic devices are in operation,
such as power consumption [12], electromagnetic [1], acoustic [8], et al. The
attacker can successfully recover the key used in the encryption algorithm by
using these information.
In 1996, Kocher et al. made full use of timing information leaked from cryp-
tographic devices and successfully recovered the key of RSA [13]. They proposed
Simple Power Analysis (SPA) and Diﬀerential Power Analysis (DPA) in 1999
[12]. Side channel attacks, such as Template Attack [7], Correlation Power Analy-
sis (CPA) [6], Mutual Information Analysis (MIA) [9], Collision Attack [16] and
side channel based watermarks [3], developed quickly then. Side channel attacks,
pose serious threats to the security of cryptographic devices.
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 59–72, 2016.
DOI: 10.1007/978-3-319-45871-7 5

60
C. Ou et al.
1.1
Related Works
The idea of biasing power traces was ﬁrstly proposed by K. Tiri et al. [19].
The power consumption of the interesting point most relevant to the outputs
of S-box can be approximated by a normal distribution [11]. K. Yongdae et al.
made full use of this characteristic [11]. By calculating the probability density
function values of power consumption of this interesting point, they obtained the
distribution. The smaller the probability density function value is, the higher the
SNR of the power trace will be. However, the result of the calculation is the only
reference to bias power traces. The power consumption of an interesting point
on a power trace may not have a signiﬁcant linear correlation with its Hamming
weight of S-box output since the presence of noise. Suppose that an encryption
algorithm has k bit-length S-box output. Hamming weights close to 0 or k may
correspond to power traces with greater probability density function values of
power consumption of interesting points. Accuracy of biasing power traces is
greatly reduced if noise on the power traces is large.
B. Noura et al. used a new power model to bias power traces and improved
the eﬃciency of the CPA [4]. They did not make improvements to the methods of
choosing power traces, but instead of improving the power consumption model.
The scheme is not perfect.
H. Wenjing et al. proposed an adaptive chosen plaintext correlation power
analysis [10]. They solved the problem of discarding too many power traces in
the scheme proposed by K. Yongade et al. [11]. However, this scheme was very
complex.
1.2
Our Contributions
In this paper, we analyze the statistical characteristics of power traces and pro-
pose three methods to bias power traces which we will introduce in Sect. 3:
1. Minkowski distance. We analyze the correlation between Hamming
weights of the outputs of S-box and power consumption of interesting points, and
we get a conclusion that Hamming weights 0 and 8 correspond to power traces
with highest or lowest power consumption. We can calculate the Minkowski
distance between each power trace and mean power trace to estimate the corre-
sponding Hamming weight of a power trace.
2. The sum of probability density function values of power con-
sumption of interesting points. The power consumption of an interesting
point can be approximated by a normal distribution. Power traces with high
SNR are located in two ends of the distribution. We use probability density
function values of power consumption of interesting points to bias power traces.
3. Power consumption of interesting points. The method of biasing
power traces by directly adding up power consumption of interesting points on
each power trace and then sorting the sums also works very well according to
the approximated normal distribution.
Not only the feasibility but also the high eﬃciency of our schemes are proved
and veriﬁed in this paper.

Enhanced Correlation Power Analysis by Biasing Power Traces
61
1.3
Organization
This paper is organized as follows. The statistical characteristics of power traces
are given in Sect. 2. In Sect. 3, we introduce the three of our methods to bias
power traces. Then, in Sect. 4, experiments are carried out to compare our
schemes with the scheme proposed by K. Yongdae et al. Finally, we conclude
this paper in Sect. 5.
2
The Statistical Characteristics of Power Traces
2.1
Composition of Power Consumption
The power consumption of each interesting point of a power trace can be modeled
as the sum of exploitable power consumption Pexp, the noise component Pnoise,
and the constant component Pconst [14].
Ptotal = Pexp + Pnoise + Pconst
(1)
The component Pexp corresponds to the sum of the operation-dependent com-
ponent Pop and the data-dependent component Pdata:
Pexp = Pop + Pdata.
(2)
The component Pnoise corresponds to the sum of the switching noise Psw.noise
and the electronic noise Pel.noise:
Pnoise = Pel.noise + Psw.noise.
(3)
The switching noise Psw.noise is not exploitable noise and it is not electronic
noise. If we consider all bits of the outputs of a S-box, the switching noise will
not exist. Referring to [10,14], the SNR of an interesting point of a power trace is
SNR = var(Pexp)
var(Pnoise).
(4)
S. Mangard et al. proposed that the var(Pexp) and var(Pnoise) quantiﬁed how
much an interesting point of power traces varied because of the exploitable signal
and noise respectively [14]. To continue the work that S. Mangard et al. have not
ﬁnished in their paper, we derive the formula (4). Because Pop, Pdata, Psw.noise
and Pel.noise are independent of each other, the variance
var(Pexp) = var(Pop + Pdata) = var(Pdata) + var(Pop),
(5)
and the variance
var(Pnoise) = var(Psw.noise + Pel.noise) = var(Psw.noise) + var(Pel.noise). (6)
Suppose that we use all bits of the output of a S-box of an encryption algorithm,
then the variance of switching noise var(Psw.noise) = 0. If we consider the same

62
C. Ou et al.
interesting point on diﬀerent power traces, the same operation (i.e. looking-up
table S-box) is executed. So, the variance var(Pop) = 0. Then,
SNR =
var(Pop + Pdata)
var(Psw.noise + Pel.noise),
(7)
then
SNR =
var(Pdata)
var(Pel.noise).
(8)
The electronic noise follows normal distribution N(0, σel.noise). The greater (or
smaller) the Hamming weight of the output of S-box is, the larger the variance
var(Pdata) will be, and so is the SNR.
2.2
Correlation Between SNR and Power Consumption of an
Interesting Point
Suppose that the attacker encrypts m random known plaintexts and acquires m
power traces using an oscilloscope when a cryptographic device is in operation.
We also assume that there are n interesting points on each power trace. Then,
the attacker obtains an m × n power consumption matrix. Each point consists
of the sum of exploitable power consumption Pexp, the noise component Pnoise,
and the constant component Pconst.
We only consider serial processor in this paper. Let P j
total donate the power
consumption of the jth interesting point of power traces. Then, the correlation
coeﬃcient between P j
total and P j
data is
ρ(P j
data, P j
total) =
cov(P j
data, P j
total)

var(P j
data)var(P j
total)
.
(9)
Let var(P j
data) denote the variance of P j
data and var(P j
total) denote the variance
of P j
total. Then,
cov(P j
data, P j
total) = cov(P j
data, P j
data + P j
op + P j
el.noise + P j
const)
= E

P j
data + P j
op + P j
el.noise + P j
const −E

P j
data + P j
op
+ P j
el.noise + P j
const
 
P j
data −E

P j
data
 
= E

P j
op −E

P j
op

+ P j
data −E

P j
data

+ P j
el.noise
−E

P j
el.noise

+ P j
const −E

P j
const
  
P j
data −E

P j
data
 
.
(10)
For each interesting point, the operation is exactly the same. So, P j
op −
E

P j
op

= 0. The constant component of this column is equal to E

P j
const

.
So, P j
const −E

P j
const

= 0. Suppose that the noise component follows normal

Enhanced Correlation Power Analysis by Biasing Power Traces
63
distribution N(0, σel.noise), and data-dependent component P j
data follows bino-
mial distribution which can be approximated by a normal distribution. Then,
cov

P j
data, P j
total

= E
 
P j
data −E

P j
data

+ P j
el.noise −E

P j
el.noise


P j
data −E

P j
data
 
= E
 
P j
data −E

P j
data
 
P j
data −E

P j
data

+

P j
el.noise −E

P j
el.noise
 
P j
data −E

P j
data
 
= σ2 + E

P j
el.noise −0
 
P j
data −E

P j
data

= σ2 + E

P j
el.noise −0

E

P j
data −E

P j
data

= σ2.
(11)
The variance
var

P j
total

= var

P j
op + P j
data + P j
el.noise + P j
const

.
(12)
Because P j
data, P j
op, P j
el.noise and P j
const are independent of each other, the
variance
var

P j
total

= var

P j
op

+ var

P j
data

+ var

P j
el.noise

+ var

P j
const

. (13)
Because of the variance var

P j
op

= 0 and var

P j
const

= 0. Then,
var

P j
total

= var

P j
data

+ var

P j
el.noise

= σ2 + σ2
el.noise.
(14)
Then,
ρ(P j
data, P j
total) =
σ
	
σ2 + σ2
el.noise
.
(15)
Then, the correlation coeﬃcient between P j
data and P j
total will be
ρ(P j
data, P j
total) =
1

1 + σ2
el.noise
σ2
=
1

1 +
1
SNR
.
(16)
From the above formula (16), we know that, the higher the SNR, the higher
the correlation coeﬃcient between Pdata and Ptotal. If we consider a column of
power consumption matrix, the correlation coeﬃcient ρ(Pdata, Ptotal) between
Pdata and Ptotal will be higher after biasing power traces. So, selecting power
traces with high SNR is conducive to improve the eﬃciency of the CPA.

64
C. Ou et al.
3
Our Methods to Bias Power Traces
If a cryptographic algorithm has k bit-length S-box output, then the number
of output Hamming weights follow binomial distribution (as shown in Table 1).
Let T = CN
k
denote the number of S-box outputs with a Hamming weight of
N. The power component Pdata of an interesting point of S-box outputs follows
binomial distribution.
Table 1. The binomial distribution of Hamming weights of S-box outputs
HW 0
1
2
... k-2
k-1
k
T
CN=0
k
CN=1
k
CN=2
k
... CN=k−2
k
CN=k−1
k
CN=k
k
In the next three sub-sections, we will introduce 4 metric methods to bias
power traces.
3.1
Biasing Power Traces by Using Minkowski Distance
Suppose that the power consumption of a cryptographic device can be modeled
using Hamming weight model. The Hamming weights of S-box outputs being
close to k or 0 will correspond to power traces with higher SNR than those
being close to k/2.
Suppose that the Hamming weights of S-box outputs corresponding to power
traces Ta and Tb are HWa and HWb respectively. Suppose that there are k
interesting points on the power traces. Let P (a,j) and P (b,j) donate the data-
dependent power consumption component of the jth interesting points of power
trace a and b respectively. The absolute value of HWa(or HWb) minus the mean
of Hamming weights HWmean is |HWa −HWmean| (or |HWb −HWmean|).
Condition 1: If HWa,HWb and HWmean meet HWa ≥HWb ≥HWmean
or HWa ≤HWb ≤HWmean, then |HWa −HWmean| ≥|HWb −HWmean|,
then


P (a,j) −P j
mean


≥


P (b,j) −P j
mean


, then k
j=1


P (a,j) −P j
mean


≥
k
j=1


P (b,j) −P j
mean


.
Condition 2: HWa ≤HWb and HWb ≥HWmean (or HWa ≥HWb
and HWb
≤
HWmean). If |HWa −HWmean|
≥
|HWb −HWmean|, then
we get


P (a,j) −P j
mean


 ≥


P (b,j) −P j
mean


. Then k
j=1


P (a,j) −P j
mean


 ≥
k
j=1


P (b,j) −P j
mean


 still be supported.
Biasing power traces can be done in the above two conditions. Power traces
corresponding to Hamming weights being close to 0 or k will be preferentially
selected.
Suppose that we use t interesting points to proﬁle templates. Let P (m,j)
denote data-dependent power consumption component of the jth interesting
point on the mth power trace, and μ(j) denote the mean power consumption

Enhanced Correlation Power Analysis by Biasing Power Traces
65
of the jth interesting point. Then, we can measure the distance between the mth
power trace and the mean power trace by using Manhattan distance
Distance(Manh,m) =
n

j=1



P (m,j) −μ(j)



 .
(17)
Since


P (a,j) −P j
mean


≥


P (b,j) −P j
mean


.
So,

P (a,j) −P j
mean
2
≥

P (b,j) −P j
mean
2, then we can get a conclusion that t
j=1

P (a,j) −P j
mean
2 ≥
t
j=1

P (b,j) −P j
mean
2. Absolutely, we can also measure the distance between
the mth power trace and the mean power trace by using Euclidean distances
Distance(Euc,m) =




t

j=1

P (m,j) −μ(j)
2.
(18)
Actually, Manhattan distance and Euclidean distance are the ﬁrst and second
order of Minkowski distance respectively. Higher order Minkowski distance
Distance(Mink,m) =
q




t

j=1

P (m,j) −μ(j)
q
(19)
can also be used to measure the distance between the mth power trace and the
mean power trace.
3.2
Biasing Power Traces by Using the Sum of Probability Density
Function Values of Power Consumption of Interesting Points
Let σ2
(j) denote the variance of the jth interesting point. If HWa, HWb
and HWmean satisfy Condition 1 or Condition 2 in the Sect. 3.1, then

P (a,j) −P j
mean
2 ≥

P (b,j) −P j
mean
2. Let
f (a, j) =
1

2πσ2
(j)

−

P (a,j) −P j
mean
2
2σ2
(j)

(20)
and
f (b, j) =
1

2πσ2
(j)

−

P (b,j) −P j
mean
2
2σ2
(j)

(21)
denote the probability density function value of data-dependent power consump-
tion component of jth interesting points of power trace a and power trace b
respectively. Then,
f (a, j) ≤f (b, j)
(22)

66
C. Ou et al.
If there are n interesting points on the power traces, then
n

j=1
f (a, j) ≤
n

j=1
f (b, j)
(23)
still be supported. So, power traces with high SNR can still be ﬁltered out.
We can measure the distance between the mth power trace and the mean
power consumption trace by using the sum of probability density function (PDF)
values
Distance(P DF,m) =
n

j=1
1

2πσ2
(j)
exp

−

P (m,j) −μ(j)
2
2σ2
(j)

.
(24)
It is worth noting that, the greater the sum of probability density function (PDF)
values is, the closer to μ(j) the power consumption P (m,j) is. In order to bias
power traces with high SNR, we calculate a Distance(P DF,m) for each power
trace, then sort the sums in ascending order. Smaller sums correspond to power
traces with higher SNR.
3.3
Biasing Power Traces by Directly Using Power Consumption of
Interesting Points
Referring to [11], the power consumption of an interesting point can be approx-
imated by a normal distribution. That is to say, the Hamming weight 0 and k
correspond to power traces with the highest or lowest power consumption. We
can directly sort the power consumption of an interesting point on the power
traces, then extract a small number of power traces from two ends of the dis-
tribution. SNR of power traces on both these two ends is generally high. We
add up all data-dependent power consumption component P(m,j) (1 ≤j ≤n) of
interesting points for each power trace and then sort the sums. Thus,
Distance(P ower,m) =
n

j=1
P(m,j).
(25)
Compared to the previous three methods, this scheme is very simple.
3.4
How to Bias Power Traces Using the Above 4 Schemes
We only consider the data-dependent power consumption component of power
traces when we introduce the four of our schemes to bias power traces. If we
repeatedly encrypt a plaintext and acquire a number of power traces. We aver-
age these power traces to remove the noise. Through the above two kinds of
measurements using Euclidean distance and Manhattan distance respectively,
we can measure the distance between the mth power trace and the mean power
trace. Greater distances correspond to power traces with higher SNR. Biasing

Enhanced Correlation Power Analysis by Biasing Power Traces
67
power traces can also be done by using the sum of probability density function
values, greater sums correspond to power traces with higher SNR, too.
However, if we want to bias power traces using the sum of power consumption
of interesting points, we should select power traces from two ends of the approxi-
mated normal distribution. Hamming weights 0 and 8 correspond to power traces
with the greatest or smallest power consumption on their interesting points. Bias-
ing power traces by using the above 4 metric methods can improve correlation
coeﬃcients, so as to improve the eﬃciency of the CPA.
If we encrypt each random known plaintext only once and acquire only one
power trace, this power trace may contain a lot of noise. The power consumption
of interesting points is no longer in linear relationship with their corresponding S-
box output Hamming weights. This will result in relatively low attack eﬃciency.
However, our schemes can still be utilized.
4
Experimental Results
We carry out our experiments on AT89S52 micro-controller. The clock frequency
of this chip is 12 MHz. The minimum instructions takes 12 clock cycles for exe-
cution. We utilize Tektronix DPO 7254 oscilloscope, and the sampling rate is set
to 1 GS/s. The outputs of the ﬁrst S-box in the ﬁrst round of AES encryption
is chosen as the attack point. We test the instruction ‘MOV CA, @A + DPTR’,
which treats the value of register A as the oﬀset value of S-box and treats the S-
box of AES algorithm as the base address, then looks up table S-box and writes
the result back to register A.
4.1
Interesting Points Extraction
So far, there exist various kinds of methods to extract interesting points. For
example, C. Rechberger et al. extracted interesting points by adding up all devi-
ations of every two power traces [15]. D. Agraval et al. used DPA to extract
interesting points [2]. L. Bohy et al. used PCA to extract interesting points
[5]. F. X. Standaert et al. extracted interesting points by using FLDA [17].
We randomly encrypt 100 known plaintexts and acquire 1000 power traces. We
calculate the Pearson correlation coeﬃcients between these power traces and
their corresponding S-box outputs’ Hamming weights, which is equivalent to
ρ(Pdata, Ptotal) (as shown in Fig. 1. Then we sort the correlation coeﬃcients by
descending order and choose the maximum 100 interesting points in the front of
the sorted sequence.
In Sects. 4.2 and 4.3, we will compare our schemes with the scheme proposed
by K. Yongdae et al. from two aspects: CRS which we will introduce in Sect. 4.2
and success rates (SR) proposed by F. X. Standaert et al. [18] which we will
introduce in Sect. 4.3. Speciﬁc comparison will be given in Sects. 4.2 and 4.3.

68
C. Ou et al.
0
500
1000
1500
2000
2500
3000
3500
4000
4500
5000
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Sampling Pionts
Correlation Coefficients
Fig. 1. Correlation coeﬃcients between S-box output Hamming weights and each col-
umn of power consumption matrix.
4.2
Comparison of Screening Accuracy
Suppose that we encrypt n random known plaintexts. The total number of S-
box outputs’ Hamming weights 0, 1, 7 and 8 of the ﬁrst S-box is k. We sort
the sums by using a scheme (i.e. Euclidean distance) and select k power traces
which satisfy the selection conditions of this scheme. Suppose that we ﬁnd out
t power traces corresponding to S-box outputs’ Hamming weights 0, 1, 7, or 8
from these k power traces. Then, we deﬁne correct rates of selection as
CRS = t
k
(26)
If we consider 30 interesting points and choose the 30 points most relevant
to S-box outputs. We encrypt 700 random known plaintexts. The correct rates
of selection of all 16 S-boxes are shown in Fig. 2. Both the scheme proposed by
K. Yongdae et al. [11] and our schemes are used.
2
4
6
8
10
12
14
16
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
the i−th S−box
correct rates of selection
Euclidean distance
Manhattan distance
the sum of PDF values
PDF values
the sum of power consumption
Fig. 2. The CRS of all 16 S-boxes outputs’ Hamming weights 0, 1, 7 and 8.

Enhanced Correlation Power Analysis by Biasing Power Traces
69
As shown in Fig. 2, by using 30 interesting points, the four schemes proposed
by us have higher correct rates of selection of all 16 S-boxes than the scheme
proposed by Kongdae et al. [11]. We also compare our schemes with the scheme
proposed by K. Yongdae et al. by using diﬀerent number of power traces. The
experimental results by using the same 30 interesting points are shown in Fig. 3.
The correct rates of selection by using Euclidean distance, Manhattan Distance,
the sum of probability density function (PDF) values and the sum of power
consumption under 30 interesting points are about 15 % higher than that of
considering the probability density function value of only one interesting point
corresponding to the scheme of K. Yongdae et al.
0
200
400
600
800
1000
1200
1400
0.2
0.25
0.3
0.35
0.4
0.45
0.5
Number of power traces
CRS
Euclidean distance
Manhattan distance
the sum of PDF values
v
the sum of power consumption
Fig. 3. CRS under diﬀerent number of power traces by using the four schemes of ours
and the scheme proposed by K. Yongdae et al.
We also compare the CRS in our schemes with the scheme proposed by K.
Yongdae et al. [11] by using diﬀerent number of interesting points. We choose the
100 points most relevant to S-box outputs and sort the correlation coeﬃcients
of these points in descending order. We then extract 1, 11, 21,..., 91 interesting
points respectively. By calculating the average CRS of 16 S-boxes under 700
power traces, we get the experimental results as shown in Fig. 4.
As shown in Fig. 4, if we extract more interesting points, we will get higher
CRS when the number of interesting points is less than or equal to 21. When
the number of interesting points is greater than 21, the correct rates will be very
close to each other. Compared with the scheme proposed by K. Yongdae et al.
[11] corresponding to the sum of probability density function values where the
number of interesting points is equal to 1 in Fig. 4, the CRS in the four schemes
proposed by us have been signiﬁcantly improved.
4.3
Comparison of Success Rates
We also utilize success rate (SR) proposed by F. X. Standaert et al. [18] to
evaluate the attack eﬃciency in our experiments. Bying using the traditional

70
C. Ou et al.
0
10
20
30
40
50
60
70
80
90
100
0.18
0.2
0.22
0.24
0.26
0.28
0.3
0.32
0.34
0.36
Number of interesting points
CRS
Euclidean distance
Manhattan distance
the sum of PDF values
the sum of power consumption
Fig. 4. The average CRS of 16 S-boxes by using diﬀerent number of interesting points
CPA, we can recover the key used in AT89S52 with a probability of about 1.00
when the number of power traces reaches 136. We encrypt 20000 random known
plaintexts and acquire 20000 power traces. We randomly choose 136 power traces
to perform CPA attacks by using the four of our schemes and scheme proposed
by K. Yongdae et al. [11]. The output of the ﬁrst S-box is chosen as the attack
point. By repeating 100 times using the 30 interesting points most relevant to
S-box outputs, the experimental results are shown in Fig. 5.
10
15
20
25
30
35
40
45
50
55
60
0
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
Number of power traces
Success rates
Euclidean distance
Manhattan distance
the sum of PDF values
PDF values
the sum of power consumption
Fig. 5. Success rates by using the four schemes proposed by us and the scheme proposed
by Kongdae et al.

Enhanced Correlation Power Analysis by Biasing Power Traces
71
As shown in Fig. 5, we get higher success rates by using Euclidean distance,
Manhattan distance and the sum of probability density function values proposed
by us than the scheme proposed by K. Yongdae et al. [11]. Our method of directly
using the sum of power consumption of interesting points also gets very good
attack eﬃciency.
5
Conclusion
In this paper, we analyze the statistical characteristics of power traces, and
the relationship between SNR and S-box outputs’ Hamming weight in serial
processors. We use Minkowski distance, the sum of probability density func-
tion values of power consumption of interesting points, power consumption of
interesting points to bias power traces. Compared with the scheme proposed by
K. Yongdae et al., the three of our schemes can blindly screen power traces with
high SNR with a higher CRS of 15 %. Thus, signiﬁcantly improve the eﬃciency
of CPA. Signal preprocessing can improve the accuracy of screening, which is
the content of our future research. Better and more eﬀective methods to bias
power traces are still our research goal in the future.
Acknowledgment. This research is supported by the Nation Natural Science Founda-
tion of China (No. 61372062). The authors would like to thank the anonymous referees
of ISC 2016 for the suggestions to improve this paper.
References
1. Agrawal, D., Archambeault, B., Rao, J.R., Rohatgi, P.: The EM side—channel(s).
In: Kaliski Jr., B.S., Ko¸c, C¸.K., Paar, C. (eds.) CHES 2002. LNCS, vol. 2523.
Springer, Heidelberg (2003)
2. Agrawal, D., Rao, J.R., Rohatgi, P., Schramm, K.: Templates as master keys. In:
Rao, J.R., Sunar, B. (eds.) CHES 2005. LNCS, vol. 3659, pp. 15–29. Springer,
Heidelberg (2005)
3. Becker, G.T., Kasper, M., Moradi, A., Paar, C.: Side-channel based watermarks
for integrated circuits. In: IEEE International Symposium on Hardware-Oriented
Security and Trust, pp. 30–35 (2010)
4. Benhadjyoussef, N., Machhout, M., Tourki, R.: Optimized power trace numbers in
CPA attacks. In: 2011 8th International Multi-Conference on Systems, Signals and
Devices (SSD), pp. 1–5 (2011)
5. Bohy, L., Neve, M., Samyde, D., Quisquater, J.J.: Principal and independent com-
ponent analysis for crypto-systems with hardware unmasked units. In: Proceedings
of e-Smart 2003 (2003)
6. Brier, E., Clavier, C., Olivier, F.: Correlation power analysis with a leakage model.
In: Joye, M., Quisquater, J.-J. (eds.) CHES 2004. LNCS, vol. 3156, pp. 16–29.
Springer, Heidelberg (2004)
7. Chari, S., Rao, J.R., Rohatgi, P.: Template attacks. In: Kaliski Jr., B.S., Ko¸c, C¸.K.,
Paar, C. (eds.) CHES 2002. LNCS, vol. 2523. Springer, Heidelberg (2003)
8. Genkin, D., Shamir, A., Tromer, E.: Acoustic cryptanalysis. J. Cryptol. 1–52
(2016). doi:10.1007/s00145-015-9224-2

72
C. Ou et al.
9. Gierlichs, B., Batina, L., Tuyls, P., Preneel, B.: Mutual information analysis. In:
Oswald, E., Rohatgi, P. (eds.) CHES 2008. LNCS, vol. 5154, pp. 426–442. Springer,
Heidelberg (2008)
10. Hu, W., Wu, L., Wang, A., Xie, X., Zhu, Z., Luo, S.: Adaptive chosen-plaintext
correlation power analysis. In: Tenth International Conference on Computational
Intelligence and Security, pp. 494–498 (2014)
11. Kim, Y., Sugawara, T., Homma, N., Aoki, T., Satoh, A.: Biasing power traces to
improve correlation in power analysis attacks. ESRC Centre Population Change
2(3), 10–16 (2014)
12. Kocher, P., Jaﬀe, J., Jun, B.: Diﬀerential power analysis. Int. Cryptol. Conf. Adv.
Cryptol. 1666, 388–397 (1999)
13. Kocher, P.C.: Timing attacks on implementations of Diﬃe-Hellman, RSA, DSS,
and other systems. Int. Cryptol. Conf. Adv. Cryptol. 1109, 104–113 (2010)
14. Mangard, S., Oswald, E., Popp, T.: Power Analysis Attacks: Revealing the Secrets
of Smart Cards. Springer, New York (2007)
15. Rechberger, C., Oswald, E.: Practical template attacks. In: Lim, C.H., Yung, M.
(eds.) WISA 2004. LNCS, vol. 3325, pp. 440–456. Springer, Heidelberg (2005)
16. Schramm, K., Wollinger, T., Paar, C.: A new class of collision attacks and its
application to DES. Fast Softw. Encryp. FSE 2887(6), 206–222 (2003)
17. Standaert, F.-X., Archambeau, C.: Using subspace-based template attacks to com-
pare and combine power and electromagnetic information leakages. In: Oswald, E.,
Rohatgi, P. (eds.) CHES 2008. LNCS, vol. 5154, pp. 411–425. Springer, Heidelberg
(2008)
18. Standaert, F.-X., Malkin, T.G., Yung, M.: A uniﬁed framework for the analysis of
side-channel key recovery attacks. In: Joux, A. (ed.) EUROCRYPT 2009. LNCS,
vol. 5479, pp. 443–461. Springer, Heidelberg (2009)
19. Tiri, K., Schaumont, P.: Changing the odds against masked logic. In: Biham, E.,
Youssef, A.M. (eds.) SAC 2006. LNCS, vol. 4356, pp. 134–146. Springer, Heidelberg
(2007)

Damaging, Simplifying, and Salvaging p-OMD
Tomer Ashur1,2(B) and Bart Mennink1,2
1 Department of Electrical Engineering, ESAT/COSIC, KU Leuven, Leuven, Belgium
{tomer.ashur,bart.mennink}@esat.kuleuven.be
2 iMinds, Ghent, Belgium
Abstract. One of the submissions to the CAESAR competition for
the design of a new authenticated encryption scheme is Oﬀset Merkle-
Damg˚ard (OMD). At FSE 2015, Reyhanitabar et al. introduced p-OMD,
an improvement of OMD that processes the associated data almost for
free. As an extra beneﬁt, p-OMD was claimed to oﬀer integrity against
nonce-misusing adversaries, a property that OMD does not have. In this
work we show how a nonce-misusing adversary can forge a message for
the original p-OMD using only 3 queries (including the forgery). As a
second contribution, we generalize and simplify p-OMD. This is done
via the introduction of the authenticated encryption scheme Spoed. The
most important diﬀerence is the usage of a generalized padding func-
tion GPAD, which neatly eliminates the need for a case distinction in
the design speciﬁcation and therewith allows for a signiﬁcantly shorter
description of the scheme and a better security bound. Finally, we intro-
duce the authenticated encryption scheme Spoednic, a variant of Spoed
providing authenticity against a nonce-misusing adversary at a mod-
est price.
Keywords: Authenticated encryption · CAESAR · p-OMD · Nonce-
misuse · Forgery · Simpliﬁcation
1
Introduction
The principle of authenticated encryption, where both the conﬁdentiality as well
as the integrity of data is guaranteed has gained renewed attention in the last
couple of years. Emerged from this is the CAESAR competition for the design of
new authenticated encryption schemes [6]. CAESAR has received 57 submissions,
30 of which have recently advanced to the second round. Many of these designs
have already received further attention via attacks, supporting security proofs,
or generalizations.
One of the second round candidates of the CAESAR competition is Oﬀset
Merkle-Damg˚ard (OMD) by Cogliani et al. [9,10]. It is characterized by the
usage of a full-ﬂedged compression function, and in fact the CAESAR submission
takes the SHA256 compression function. OMD is proven to achieve birthday-
bound security on the state against adversaries that are not allowed to re-use the
nonce. At ProvSec 2014, Reyhanitabar et al. [18] showed how to generalize the
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 73–92, 2016.
DOI: 10.1007/978-3-319-45871-7 6

74
T. Ashur and B. Mennink
scheme to achieve security against nonce-misusing adversaries. On the downside,
these schemes are not online and are less eﬃcient than OMD. At FSE 2015
Reyhanitabar et al. [19] presented p-OMD (pure OMD). p-OMD improves over
classical OMD in that the associated data is processed almost for free. This
is achieved by processing the message blocks as normal message inputs to the
compression function and by XORing the associated data into the state. The
authors prove that p-OMD inherits all security features of OMD, particularly
birthday-bound security against nonce-respecting adversaries. In [20], an early
version of [19], it was suggested that p-OMD also oﬀers integrity against nonce-
misusing adversaries.
1.1
Nonce-Misuse Forgery on p-OMD (Damaging)
As ﬁrst contribution of this work, we point out that this claim is incorrect. In
more detail, we present a nonce-misusing adversary that can forge a message
for p-OMD in only 3 queries, including the forgery itself. At a high level, the
attack relies on the observation that if an evaluation for p-OMD is made for a
certain nonce, the adversary learns (most of) the corresponding state values. If
the adversary is allowed to misuse the nonces, this means that it can eﬀectively
inﬂuence the state values, and henceforth generate a forgery. We also point
out where the mistake occurs in the proof. We stress that this attack does not
invalidate the security of p-OMD (nor OMD) in the nonce-respecting setting:
that proof seems sound and the scheme achieves conﬁdentiality and integrity.
1.2
Spoed (Simplifying)
One may argue that the ﬂaw slipped into [20] in part due to the complex char-
acter of p-OMD. Indeed, the speciﬁcation of p-OMD consists of 6 cases (or in
fact 13, if you consider the scheme in full detail), depending on the number of
associated data and message blocks. The forking of one scheme into a plural-
ity of cases entails diﬃculties both on the theory side, leading to longer and
more cumbersome proofs (which are incidentally harder to verify, as in the case
above), and on the practical side, forcing less eﬃcient and error-prone implemen-
tations. Additionally, it does not particularly facilitate an easy understanding
and adoption of the scheme.
Driven by these conclusions and the potential that p-OMD oﬀers for certain
scenarios, we next explore the possibilities to generalize and simplify p-OMD.
In more detail, we introduce Spoed,1 a variant of p-OMD that aims to provide
a higher level of simplicity at the same eﬃciency as p-OMD. In more detail,
Spoed is an authenticated encryption mode that can use any keyed compression
function FK : {0, 1}2n →{0, 1}n for n ≥1 as its underlying primitive. Spoed
diﬀers from p-OMD in the following aspects:
– Most importantly, Spoed uses a generalized padding scheme GPAD. It takes
as input the associated data A and the message M, and injectively maps those
1 The name is an acronym for “Simpliﬁed Pure OMD Encryption and Decryption”.

Damaging, Simplifying, and Salvaging p-OMD
75
to generalized message blocks of size 2n bits. As GPAD includes the length
encodings of A and M as one of the generalized message blocks, it allows to
give a uniﬁed description of the scheme: one scheme for all variants;
– p-OMD relies on Gray codes for case separation. Due to the usage of the
generalized padding scheme, we can resort to the simpler-to-grasp powering-
up approach [21] or word-based LFSR approach by Granger et al. [12]. Note
that the usage of these approaches for state sizes larger than 128 bits has only
been validated recently [12].
We prove that, assuming FK is a suﬃciently secure keyed compression function,
Spoed redeems the security results of p-OMD in the nonce-respecting setting. To
instantiate FK, one can use the SHA256 or SHA512 compression functions. These
functions have been the target of extensive cryptanalysis, and their security is
well understood. They are also in wide use and eﬃcient implementations of them
can be found for practically any platform.
Spoed makes exactly the same number of compression function calls as
p-OMD, except in the rare case where a > m and a + m is odd (in which
case Spoed makes one extra compression function call due to the length encod-
ing of A and M). We see this as a modest price to pay for achieving a scheme
that (i) has a shorter and simpler description, making it easier to implement,
(ii) requires less precomputational overhead, and (iii) has a proof of about 1/4th
the size of the proof of p-OMD, making it easier to verify. The fact that Spoed
has a slightly improved security bound can be seen as a bonus.
1.3
Spoednic (Salvaging)
For the cases where nonce-misuse resistance is needed, we introduce Spoednic.2
Spoednic is a variant of Spoed preserving integrity, up to the birthday bound, in
the nonce-misuse scenario at the cost of one additional ﬁnite ﬁeld multiplication
per primitive call. Intuitively, the ﬁnite ﬁeld multiplication is used to obfuscate
the value XORed into the state, thus preventing an adversary from choosing a
“convenient” value.
We prove that Spoednic inherits all security traits of Spoed, and has the
added beneﬁt of preserving the integrity against a nonce-reusing adversary. Sur-
prisingly, the proof for this case leads to a better security bound than the ﬂawed
one claimed for p-OMD [20].
We stress that the reader should not take Spoednic as a recommendation for
allowing the nonce to repeat. The question about who is responsible for dealing
with the uniqueness of the nonce is debated in the cryptographic community.
One side to this discussion believes that making sure the nonce is unique is
an implementation matter while the other side believes that it should be dealt
with by the algorithms designers. Both sides agree that a repeating nonce is an
unwanted scenario, and the contribution of Spoednic is to allow for a graceful
fail rather than a disastrous one in this unwanted event.
2 The name is an acronym for “Simpliﬁed Pure OMD Encryption and Decryption with
Nonce-misuse Integrity Conserved”.

76
T. Ashur and B. Mennink
2
Security Model
Throughout, n ≥1 denotes the state size. By ⊕we denote the exclusive-or
(XOR) operation, and by ⊗or · ﬁnite ﬁeld multiplication over 2n. Concatenation
is denoted using ∥. Denote by {0, 1}∗the set of binary strings of arbitrary length
and by {0, 1}n the set of blocks of size n. Denote by ({0, 1}n)+ the set of strings of
length a positive multiple of n. For an arbitrary string X, |X| denotes its length,
and ⟨|X|⟩n denotes its encoding in n ≥1 bits. By leftn(X) (resp. rightn(X)) we
denote its n leftmost (resp. rightmost) bits. We use little-endian notation, which
means the three notations “bit position 0”, “the rightmost bit”, and “the least
signiﬁcant bit” all refer to the same bit.
In Sect. 2.1, we describe our model for the security of authenticated encryp-
tion. Then, we present some theoretical background on keyed compression func-
tions in Sect. 2.2.
2.1
Authenticated Encryption
Let Π = (E, D) be an authenticated encryption scheme, where
E : (K, N, A, M) 	→(C, T) and
D : (K, N, A, C, T) 	→M/⊥
are the encryption and decryption functions of Π. Let $ be a random function
that returns (C, T)
$←−{0, 1}|M| ×{0, 1}τ on every new tuple (N, A, M). In other
words, E and $ have the same interface, but the latter outputs a uniformly
randomly drawn ciphertext and tag for every new input.
An adversary A is a probabilistic algorithm that has access to one or more
oracles O, denoted AO. By AO = 1 we denote the event that A, after interacting
with O, outputs 1. In below games, the adversaries have oracle access to EK or
its counterpart $, and possibly DK. The key K is randomly drawn from {0, 1}k
at the beginning of the security experiment. We say that A is nonce-respecting
(nr) if it never queries its encryption oracle under the same nonce twice, and
nonce-misusing (nm) if it is allowed to make multiple encryption queries with
the same nonce. The security deﬁnitions below follow [1,5,11,13,14].
We deﬁne the advantage of A in breaking the conﬁdentiality of Π as follows:
Advconf
Π
(A) =
Pr

K
$←−{0, 1}k , AEK = 1

−Pr

A$ = 1
 .
For n ∈{nr, nm}, we denote by Advconf
Π
(n, q, ℓ, σ, t) the maximum advantage over
all n-adversaries that make at most q queries, each of length at most ℓgeneralized
message blocks and together of length at most σ generalized message blocks, and
that run in time t.
For integrity, we consider an adversary that tries to forge a ciphertext, which
means that DK ever returns a valid message (other than ⊥) on input (N, A, C, T)

Damaging, Simplifying, and Salvaging p-OMD
77
and no previous encryption query EK(N, A, M) returned (C, T) for any M. For-
mally:
Advint
Π (A) = Pr

K
$←−{0, 1}k , AEK,DK forges

.
For n ∈{nr, nm}, we denote by Advint
Π (n, qE, qD, ℓ, σ, t) the maximum advantage
over all n-adversaries that make at most qE encryption and qD decryption queries,
each of length at most ℓgeneralized message blocks and together of length at
most σ generalized message blocks, and that run in time t. We remark that the
nonce-respecting condition only applies to encryption queries: the adversary is
always allowed to make decryption queries for “old” nonces, and to make an
encryption query using a nonce which is already used in a decryption query
before.
2.2
(Tweakable) Keyed Compression Function
Let F : {0, 1}k × {0, 1}n+m →{0, 1}n be a keyed compression function. Denote
by Func({0, 1}n+m, {0, 1}n) the set of all compression functions from n + m to
n bits. We deﬁne the PRF security of F as
Advprf
F (A) =

Pr

K
$←−{0, 1}k , AFK = 1

−Pr

R
$←−Func({0, 1}n+m, {0, 1}n) , AR = 1


.
We denote by Advprf
F (q, t) the maximum advantage over all adversaries that
make at most q queries and that run in time t.
A tweakable keyed compression function F : {0, 1}k×T ×{0, 1}n+m →{0, 1}n
takes as additional input a tweak t ∈T . Denote by 
Func(T , {0, 1}n+m, {0, 1}n)
the set of all tweakable compression functions from n + m to n bits, where the
tweak inputs come from T . Formally, a tweakable keyed compression function is
equivalent to a keyed compression function with a larger input, but for our analy-
sis it is more convenient to adopt dedicated notation. We deﬁne the tweakable
PRF (
PRF) security of F as
Adv

prf

F (A) =

Pr

K
$←−{0, 1}k , A 
FK = 1

−Pr

R
$←−
Func(T , {0, 1}n+m, {0, 1}n) , A 
R = 1


.
We denote by Adv

prf
F (q, t) the maximum advantage over all adversaries that
make at most q queries and that run in time t.
3
p-OMD
Let k, m, n, τ ∈N such that m ≤n. Let F : {0, 1}k × {0, 1}n+m →{0, 1}n
be a keyed compression function. p-OMD is a mapping that takes as input a

78
T. Ashur and B. Mennink
n
τ
m
0n
⟨τ⟩m
A1
A2
M1
M1
C1
T
ΔN,1,0
ΔN,2,4
FK
FK
trunc
Fig. 1. p-OMD for the speciﬁc case of |A| = 2n and |M| = m.
key K ∈{0, 1}k, a nonce N ∈{0, 1}≤n−1, an arbitrarily sized associated data
A ∈{0, 1}∗, and an arbitrarily sized message M ∈{0, 1}∗, and it returns a
ciphertext C ∈{0, 1}|M| and tag T ∈{0, 1}τ.
For our attack it suﬃces to describe p-OMD for the speciﬁc case where
|A| = 2n and |M| = m (or in other words, the associated data consists of two
integral blocks and the message of one integral block). It is depicted in Fig. 1
(and corresponds to Case A of [20]). Here,
ΔN,1,0 = FK(N∥10n−1−|N|, 0m) ⊕16FK(0n, 0m),
ΔN,2,4 = FK(N∥10n−1−|N|, 0m) ⊕(32 ⊕16 ⊕4)FK(0n, 0m),
but our attack will not eﬀectively use these masking values.
3.1
Preliminary Security Claims of p-OMD
In [20], Reyhanitabar et al. proved the following security levels for p-OMD:
Theorem 1. We have
Advconf
p-OMD(nr, q, ℓ, σ, t) ≤3σ2
2n + Advprf
F (2σ, t′),
Advint
p-OMD(nr, qE, qD, ℓ, σ, t) ≤3σ2
2n + ℓqD
2n + qD
2τ + Advprf
F (2σ, t′),
Advint
p-OMD(nm, qE, qD, ℓ, σ, t) ≤3σ2
2n + ℓ(q2
E + qE)qD
2n
+ qD
2τ + Advprf
F (2σ, t′),
where t′ ≈t.
In the updated version [19], the authors removed the last claim of the three as
a result of this attack also presented in [2]. In the remainder of the section, we
demonstrate why the bound does not hold.
3.2
Nonce-Misusing Attack on p-OMD
We consider a nonce-misusing adversary that operates as follows:
(i) Fix N = ε and choose arbitrary M ∈{0, 1}m and A1, A2, A′
1 ∈{0, 1}n such
that A1 ̸= A′
1;

Damaging, Simplifying, and Salvaging p-OMD
79
(ii) Query p-OMDK(N, A1A2, M) →(C, T);
(iii) Query p-OMDK(N, A′
1A2, M) →(C′, T ′);
(iv) Set A′
2 = C ⊕C′ ⊕A2;
(v) Query forgery p-OMD−1
K (N, A′
1A′
2, C′, T).
For the ﬁrst and second evaluation of p-OMD, it holds that the state diﬀerence
right before the second F-evaluation equals C ⊕C′. The forgery is formed simply
by adding this value to A2. Consequently, it holds that the ﬁrst call to p-OMD
and the forgery attempt have the exact same input to the second F-evaluation,
and thus the same tag. Therefore, the forgery attempt succeeds as
p-OMD−1
K (N, A′
1A′
2, C′, T) = M
by construction. In other words, for some negligibly small t,
Advint
p-OMD(nm, 2, 1, 2, 6, t) = 1.
The issue appears in the proof of [20] in Lemma 4 case 4, and more speciﬁcally
the analysis of probability Pr(intcol | E4). The authors claim that an adversary
can, indeed, ﬁnd an internal collision, but that any such collision happens with a
birthday bound only. This reasoning, however, assumes that the input to every
F-call is random, which is not the case given that the adversary can re-use the
nonce and thus observe and modify the state using encryption queries.
4
Spoed
We introduce the authenticated encryption scheme Spoed with the motivation
of generalizing and simplifying p-OMD. As a bonus, the simpliﬁcation allows
for a better bound and a signiﬁcantly shorter proof, making the scheme less
susceptible to mistakes hiding in one of the lemmas.
4.1
Syntax
Let k, n, τ ∈N such that τ ≤n. Here and throughout, we assume Spoed to
process blocks of m = n bits. However, the results easily generalize to arbitrary
(but ﬁxed) block sizes. Let F : {0, 1}k × {0, 1}2n →{0, 1}n be a keyed com-
pression function. Spoed consists of an encryption function E and a decryption
function D.
– The encryption function E takes as input a key K ∈{0, 1}k, a nonce N ∈
{0, 1}n, an arbitrarily sized associated data A ∈{0, 1}∗, and an arbitrarily
sized message M ∈{0, 1}∗. It returns a ciphertext C ∈{0, 1}|M| and a tag
T ∈{0, 1}τ;
– The decryption function D takes as input a key K ∈{0, 1}k, a nonce N ∈
{0, 1}n, an arbitrarily sized associated data A ∈{0, 1}∗, an arbitrarily sized
ciphertext C ∈{0, 1}∗, and a tag T ∈{0, 1}τ. It returns either a message
M ∈{0, 1}|C| such that M satisﬁes E(K, N, A, M) = (C, T) or a dedicated
failure sign ⊥.

80
T. Ashur and B. Mennink
The encryption and decryption function are required to satisfy
D(K, N, A, E(K, N, A, M)) = M
for any K, N, A, M.
4.2
Generalized Padding
Spoed uses a generalized padding function
GPADn,τ : {0, 1}∗× {0, 1}∗→

{0, 1}2n+ .
It is indexed by state sizes n, τ, and it maps the associated data and message to
generalized message blocks. Formally, it is deﬁned as follows: First, A (associ-
ated data) and X (message or ciphertext) are padded into n-bit message blocks
A1∥· · · ∥Aa = A∥0n−|A| mod n and X1∥· · · ∥Xm = X∥0n−|X| mod n, respectively.
Denote ℓ= max

m, ⌈a+m
2 ⌉
	
+ 1, and deﬁne len(A, X) = ⟨|A|⟩n/2∥⟨|X|⟩n/2.3
The function GPADn,τ(A, X) outputs Z1, . . . , Zℓas follows:
if a ≤m:
Z1
= ⟨τ⟩n ∥A1
Z2
= X1 ∥A2
· · ·
Za
= Xa−1 ∥Aa
Za+1 = Xa ∥0n
· · ·
Zℓ−1 = Xm−1 ∥0n
Zℓ
= Xm ∥len(A,X)
if a > m, a + m even:
Z1
= ⟨τ⟩n ∥A1
Z2
= X1 ∥A2
· · ·
Zm+1 = Xm ∥Am+1
Zm+2 = Am+2 ∥Am+3
· · ·
Zℓ−1 = Aa−2 ∥Aa−1
Zℓ
= Aa ∥len(A,X)
if a > m, a + m odd:
Z1
= ⟨τ⟩n ∥A1
Z2
= X1 ∥A2
· · ·
Zm+1 = Xm ∥Am+1
Zm+2 = Am+2 ∥Am+3
· · ·
Zℓ−1 = Aa−1 ∥Aa
Zℓ
= 0n ∥len(A,X)
The encoding of the message length is included in order to circumvent the need
for a case distinction in the description of Spoed. Note that, in fact, almost any
injective padding rule would do the job; however, for our purposes the described
GPADn,τ is the most suitable. We generically write Zi = Z0
i ∥Z1
i , and denote
Zβ = Zβ
1 ∥· · · ∥Zβ
ℓfor β ∈{0, 1}.
4.3
Data Processing
Spoed is designed with the SHA256 and SHA512 compression functions in mind
as its underlying primitive. SHA256 is a compression function
SHA256 : {0, 1}256 × {0, 1}512 →{0, 1}256.
3 As we show in Sect. 6, Spoed achieves birthday bound security, and the limitation
of the length of X and A to 2n/2 −1 does not pose any issues.

Damaging, Simplifying, and Salvaging p-OMD
81
. . .
b
b
τ
0b
Z1
1
Z0
1 =⟨τ⟩b
Z1
2
Z1
ℓ–1
Z1
ℓ
Z0
2
Z0
ℓ–1
Z0
ℓ
C1
Cℓ–1
T
2L
22L
2ℓ–1L
2ℓ3L
FK
FK
FK
FK
trunc
Fig. 2. Spoed encryption, which outputs C = left|M|(C1∥· · · ∥Cℓ−1) and T. Here, L =
FK(N∥0)
Similarly, SHA512 is a compression function SHA512 : {0, 1}512 × {0, 1}1024 →
{0, 1}512. In the sequel, we will deﬁne Spoed using SHA256, or in other words
used keyed compression function
FK(Z) = SHA256(K, Z),
where K is injected through the chaining value interface, and the block is injected
through the message interface. Note that this implicitly means that we take
k = n = 256. We nevertheless continue to use k and n for clarity. Note that
Spoed can be equivalently designed using the SHA512 compression function,
but a proper change in the sizes of Z0
i and Z1
i should be introduced.
We now informally describe how to use Spoed, and refer the reader to Algo-
rithms 1 and 2 for a formal speciﬁcation. Deﬁne L = FK(N∥0). First, the asso-
ciated data and message are padded into
(Z1, . . . , Zℓ) = GPADn,τ(A, M),
where each Zi is a (2n = 512)-bit block consisting of two blocks Zi = Z0
i ∥Z1
i
of size 256 bits each. Spoed reads all blocks but the last one sequentially and
processes them by
ti = FK(ti−1 ⊕2iL ⊕Z0
i ∥Z1
i ),
where t0 = 0n. The ciphertext block Ci is generated as Ci = ti ⊕Mi, chopped to
the appropriate length if Mi does not contain a full amount of n message bits.
The last block Zℓcontains the lengths of the message and the associated data
and is processed through
tℓ= FK(tℓ−1 ⊕2ℓ3L ⊕Z0
ℓ∥Z1
ℓ).
The tag T is generated by removing the leftmost 256-τ bits of tℓ. Spoed is
depicted in Fig. 2.
Decryption goes fairly the same way: a ti and a Ci value are used to recover
Mi, and the state is set to Ci. This eventually leads to a veriﬁcation tag T ′, and
the message M is released if T = T ′.

82
T. Ashur and B. Mennink
Algorithm 1. Spoed encryption E
Input: (K, N, A, M)
Output: (C, T)
1: (Z1, . . . , Zℓ) = GPADn,τ(A, M)
2: m = ⌈|M|/n⌉
3: L = FK(N∥0)
4: t0 = 0n
5: for i = 1, . . . , ℓ−1 do
6:
ti = FK(ti−1 ⊕2iL ⊕Z0
i ∥Z1
i )
7:
Ci = ti ⊕Z0
i+1
8: tℓ= FK(tℓ−1 ⊕2ℓ3L ⊕Z0
ℓ∥Z1
ℓ)
9: C = left|M|(C1∥· · · ∥Cℓ−1)
10: T = leftτ(tℓ)
11: return (C, T)
Algorithm 2. Spoed decryption D
Input: (K, N, A, C, T)
Output: M or ⊥
1: (Z1, . . . , Zℓ) = GPADn,τ(A, C)
2: m = ⌈|C|/n⌉, ρ = |C| mod n
3: L = FK(N∥0)
4: t0 = 0n , M0 = Z0
1
5: for i = 1, . . . , ℓ−1 do
6:
ti = FK(ti−1 ⊕2iL ⊕Mi−1 ∥Z1
i )
7:
if i < m then Mi = ti ⊕Z0
i+1
8:
if i = m then Mi = leftρ(ti)⊕Z0
i+1
9:
if i > m then Mi = Z0
i+1
10: tℓ= FK(tℓ−1 ⊕2ℓ3L ⊕Z0
ℓ∥Z1
ℓ)
11: M = left|C|(M1∥· · · ∥Mℓ−1)
12: T ′ = leftτ(tℓ)
13: return T = T ′ ? M : ⊥
4.4
Security of Spoed
Spoed achieves conﬁdentiality and integrity against nonce-respecting adversaries.
Note that we do not claim security against nonce-misusing adversaries.
Theorem 2. We have
Advconf
Spoed(nr, q, ℓ, σ, t) ≤1.5σ2
2n
+ Advprf
F (2σ, t′),
Advint
Spoed(nr, qE, qD, ℓ, σ, t) ≤1.5σ2
2n
+ ℓqD
2n + qD
2τ + Advprf
F (2σ, t′),
where t′ ≈t.
These bounds, surprisingly, improve over the ones of p-OMD (see Theorem 1),
but with a much shorter proof. The proof is given in Sect. 6.
5
Spoednic
Spoed is simpler and more eﬃcient than p-OMD, but it also falls victim to nonce-
misuse attacks. In this section, we introduce Spoednic, a strengthened version of
Spoed that retains some level of security if the nonce gets reused. As a matter
of fact, Spoednic diﬀers from Spoed in (and only in) the fact that it uses an
additional subkey L′ = FK(N∥1) and that the input values Z0
i are blinded by L′.
More formally, Spoednic inherits the syntax and generalized padding from Spoed
(see Sects. 4.1 and 4.2). The data processing is fairly similar to that of Spoed
(Sect. 4.3); we only present the depiction in Fig. 3 and the formal description in
Algorithms 3 and 4. Both algorithms diﬀer from Algorithms 1 and 2 only in lines
3 and 6. Spoednic boils down to Spoed (Fig. 2) if one would use L′ = 1 instead
of L′ = FK(N∥1).

Damaging, Simplifying, and Salvaging p-OMD
83
. . .
b
b
τ
0b
Z1
1
Z0
1 =⟨τ⟩b
Z1
2
Z1
ℓ–1
Z1
ℓ
Z0
2
Z0
ℓ–1
Z0
ℓ
C1
Cℓ–1
T
2L
22L
2ℓ–1L
2ℓ3L
L′
L′
L′
L′
FK
FK
FK
FK
trunc
Fig. 3. Spoednic encryption, which outputs C = left|M|(C1∥· · · ∥Cℓ−1) and T. Here,
L = FK(N∥0) and L′ = FK(N∥1)
Algorithm 3. Spoednic encryption E
Input: (K, N, A, M)
Output: (C, T)
1: (Z1, . . . , Zℓ) = GPADn,τ(A, M)
2: m = ⌈|M|/n⌉
3: L = FK(N∥0), L′ = FK(N∥1)
4: t0 = 0n
5: for i = 1, . . . , ℓ−1 do
6:
ti = FK(ti−1⊕2iL⊕(Z0
i ·L′) ∥Z1
i )
7:
Ci = ti ⊕Z0
i+1
8: tℓ= FK(tℓ−1 ⊕2ℓ3L ⊕Z0
ℓ∥Z1
ℓ)
9: C = left|M|(C1∥· · · ∥Cℓ−1)
10: T = leftτ(tℓ)
11: return (C, T)
Algorithm 4. Spoednic decryption D
Input: (K, N, A, C, T)
Output: M or ⊥
1: (Z1, . . . , Zℓ) = GPADn,τ(A, C)
2: m = ⌈|C|/n⌉, ρ = |C| mod n
3: L = FK(N∥0), L′ = FK(N∥1)
4: t0 = 0n , M0 = Z0
1
5: for i = 1, . . . , ℓ−1 do
6:
ti = FK(ti−1⊕2iL⊕(Mi−1·L′) ∥Z1
i )
7:
if i < m then Mi = ti ⊕Z0
i+1
8:
if i = m then Mi = leftρ(ti) ⊕Z0
i+1
9:
if i > m then Mi = Z0
i+1
10: tℓ= FK(tℓ−1 ⊕2ℓ3L ⊕Z0
ℓ∥Z1
ℓ)
11: M = left|C|(M1∥· · · ∥Mℓ−1)
12: T ′ = leftτ(tℓ)
13: return T = T ′ ? M : ⊥
5.1
Security of Spoednic
We prove that Spoednic achieves conﬁdentiality against nonce-respecting adver-
saries and integrity against both nonce-respecting and nonce-misusing adver-
saries. Note that we do not claim conﬁdentiality against nonce-misusing adver-
saries.
Theorem 3. We have
Advconf
Spoednic(nr, q, ℓ, σ, t) ≤1.5σ2
2n
+ Advprf
F (3σ, t′),
Advint
Spoednic(nr, qE, qD, ℓ, σ, t) ≤1.5σ2
2n
+ ℓqD
2n + qD
2τ + Advprf
F (3σ, t′),
Advint
Spoednic(nm, qE, qD, ℓ, σ, t) ≤1.5σ2
2n
+ ℓq2
E/2
2n
+ ℓqEqD
2n
+ qD
2τ + Advprf
F (3σ, t′),
where t′ ≈t.
The proof is given in Sect. 7.

84
T. Ashur and B. Mennink
6
Security of Spoed (Theorem 2)
The proof of Theorem 2 is given in Sect. 6.2. It relies on a preliminary result on
a tweakable keyed compression function, which will be given in Sect. 6.1.
6.1
Security of Tweakable Keyed Compression Function
In the proof of Spoed we will use the following result. It is in fact an abstraction
of the XE tweakable blockcipher [21] to compression functions, and it has also
been used for OMD [10] and p-OMD [19], albeit with a worse bound.
Lemma 1. Let F : {0, 1}k×{0, 1}2n →{0, 1}n be a keyed compression function.
Let T = [1, 2n/2] × [0, 1] × {0, 1}n, and deﬁne F : {0, 1}k × T × {0, 1}2n →
{0, 1}n as
F(K, (α, β, N), S) = F(K, (2α3β · FK(N∥0) ∥0n) ⊕S).
(1)
Then, we have
Adv

prf

F (q, t) ≤1.5q2
2n
+ Advprf
F (2q, t′),
where t′ ≈t.
Proof. The proof is performed using the H-coeﬃcient technique [8,17]. It closely
follows the proof of [12, Theorem 2]; the only signiﬁcant diﬀerences appear in
the fact that the underlying primitive is a one-way function instead of a per-
mutation, and hence various bad events have become redundant. To wit, in the
terminology of [12, Theorem 2], the events bad1,2 and bad2,K are inapplicable
(as the adversary has no access to the underlying primitive), and for the events
bad1,1, bad1,K, and badK,K, we only have to consider input collisions to the
primitives. Checking the corresponding bounds reveals a term 1.5q2/2n.
As a ﬁrst step, we replace the evaluations of FK for K
$←−{0, 1}k by a
random function R : {0, 1}2n →{0, 1}n. As every evaluation of F renders at
most 2 evaluations of FK, this step costs us Advprf
F (2q, t′), where t′ ≈t, and
allows us to consider
F : ((α, β, N), S) 	→R((2α3β · R(N∥0) ∥0n) ⊕S),
(2)
based on R
$←−Func({0, 1}2n, {0, 1}n). As we have replaced the underlying func-
tion F by a secret random primitive, we can focus on adversaries with unbounded
computational power, and consider them to be information theoretic. Without
loss of generality, any such adversary is deterministic. For the remainder of the
analysis, consider any ﬁxed deterministic adversary A. Without loss of general-
ity, we assume that A does not repeat any queries.
Let R
$←−Func({0, 1}2n, {0, 1}n) and R
$←−
Func(T , {0, 1}2n, {0, 1}n). Con-
sider any ﬁxed deterministic adversary A. In the real world, it has access to F

Damaging, Simplifying, and Salvaging p-OMD
85
of (2), while in the ideal world it has access to R, and its goal is to distinguish
both worlds. It makes q queries to the oracle, which are summarized in a view
νF = {(α1, β1, N1, S1, T1), . . . , (αq, βq, Nq, Sq, Tq)}.
Note that, as A is deterministic, this view νF properly summarizes the inter-
action with the oracle. To suit the analysis, we will provide A with addi-
tional information after its interaction with its oracle. In more detail, it is
given a subkey transcript νL that includes the computations of R(N∥0) for all
N ∈{N1, . . . , Nq}. As the latter set may include duplicates, i.e., it may be that
Ni = Nj, the formalism of νL requires some notation. Let {M1, . . . , Mr} be a
minimal set that includes N1, . . . , Nq. Then, after the interaction of A with its
oracle, we reveal
νL = {(M1, L1), . . . , (Mr, Lr)},
In the real world the values L1, . . . , Lr are deﬁned as Li = R(Mi∥0), while in the
ideal world, these values are randomly generated dummy subkeys Li
$←−{0, 1}n.
Clearly, the disclosure of νL is without loss of generality as it only increases
the adversary’s chances. The complete view is deﬁned as ν = (νF , νL). It is
important to note that, as A never repeats queries, νF does not contain any
duplicate elements. Neither does νL, by minimality of the set {M1, . . . , Mr}.
H-Coeﬃcient Technique. For brevity, denote A’s distinguishing advantage
by ΔA( F; R). Denote by X 
F the probability distribution of views when A is
interacting with F and by X 
R the probability distribution of views when A is
interacting with R. Let V be the set of all attainable views, being the views that
can be generated from R with non-zero probability. Let V = Vgood ∪Vbad be a
partition of the set of attainable views. The H-coeﬃcient technique states the
following. Let 0 ≤ε ≤1 be such that for all ν ∈Vgood we have
Pr

X 
F = ν

Pr

X 
R = ν
 ≥1 −ε.
Then, the distinguishing advantage of A satisﬁes
ΔA( F; R) ≤ε + Pr

X 
R ∈Vbad

.
(3)
We refer to [7] for a proof.
Bad Transcripts. Note that every tuple in νF uniquely ﬁxes a subkey in νL and
therewith uniquely ﬁxes one evaluation R(s) = t. On the other hand, the evalu-
ations in νL represent evaluations of R themselves. Informally, we will consider
a transcript as bad if there exist two diﬀerent tuples that have the same input
to R. Formally, we say that a view ν is bad if it satisﬁes one of the following
conditions:
Bad1. There exist (α, β, N, S, T) ∈νF and (N, L), (M ∗, L∗) ∈νL such that:
(2α3β · L ∥0n) ⊕S = M ∗∥0n;

86
T. Ashur and B. Mennink
Bad2. There exist distinct (α, β, N, S, T), (α∗, β∗, N ∗, S∗, T ∗) ∈νF and (not
necessarily distinct) (N, L), (N ∗, L∗) ∈νL such that:
(2α3β · L ∥0n) ⊕S = (2α∗3β∗· L∗∥0n) ⊕S∗.
Probability of Bad Transcripts. Consider a view ν in the ideal world R. We
will consider both bad events separately.
Bad1. Consider any query (α, β, N, S, T) ∈νF with corresponding subkey
(N, L) ∈νL, and let (M ∗, L∗) ∈νL (q2 choices in total). The queries ren-
der a bad view if
2α3β · L = S0 ⊕M ∗.
As in the ideal world L
$←−{0, 1}n, this equation is satisﬁed with probability
1/2n. Summing over all possible choices of queries, Bad1 is satisﬁed with
probability at most q2/2n;
Bad2. Consider any distinct (α, β, N, S, T), (α∗, β∗, N ∗, S∗, T ∗) ∈νF with cor-
responding (N, L), (N ∗, L∗) ∈νL (
q
2

choices in total). The queries render a
bad view if
2α3β · L ⊕S0 = 2α∗3β∗· L∗⊕S∗0 ∧S1 = S∗1.
Clearly, if N ̸= N ∗, then L
$←−{0, 1}n is generated independently of the
remaining values, and the ﬁrst part of the condition holds with proba-
bility 1/2n. Similar for the case where N
= N ∗but 2α3β ̸= 2α∗3β∗.
On the other hand, if N = N ∗and 2α3β =α∗3β∗, we necessarily have
(N, α, β) = (N ∗, α∗, β∗) (due to the non-colliding property of 2α3β). As the
two queries in νF are distinct, we have S ̸= S∗, making above condition false.
Concluding, Bad2 is satisﬁed with probability at most
q
2

/2n.
We thus obtained that Pr

X 
R ∈Vbad

≤1.5q2/2n.
Good Transcripts. Consider a good view ν. Denote by Ω 
F the set of all possible
oracles in the real world and by comp 
F (ν) ⊆Ω 
F the set of oracles compatible
with view ν. Deﬁne Ω 
R and comp 
R(ν) similarly. The probabilities Pr

X 
F = ν

and Pr

X 
R = ν

can be computed as follows:
Pr

X 
F = ν

= |comp 
F (ν)|
|Ω 
F |
and Pr

X 
R = ν

= |comp 
R(ν)|
|Ω 
R|
.
Note that |Ω 
F | = (2n)22n and |Ω 
R| = (2n)|T |+22n · (2n)r (taking into account
that in the ideal world ν contains r dummy subkeys). The computation of the
number of compatible oracles is a bit more technical. Starting with comp 
F (ν),
as ν is a good view, every tuple in ν represents exactly one evaluation of R, q +r
in total, and hence the number of functions R compatible with ν is |comp 
F (ν)| =
(2n)22n−(q+r). Next, for comp 
R(ν), the tuples in νF all deﬁne exactly one eval-
uation of R, q in total, and νL ﬁxes all dummy keys. Therefore, the number of

Damaging, Simplifying, and Salvaging p-OMD
87
compatible oracles in the ideal world is |comp 
R(ν)| = (2n)|T |+22n−q. We conse-
quently obtain
Pr

X 
F = ν

Pr

X 
R = ν
 = |comp 
F (ν)| · |Ω 
R|
|Ω 
F | · |comp 
R(ν)| = (2n)22n−(q+r) · (2n)|T |+22n · (2n)r
(2n)22n · (2n)|T |+22n−q
= 1,
putting ε = 0.
Conclusion. The proof is concluded via (3) and above computations.
⊓⊔
Note that p-OMD uses tweaks of the form 2α, while we use 2α3β. This is not
a problem as long as the oﬀsets are unique [21] (i.e., there is no (α, β) ̸= (α′, β′)
such that 2α3β = 2α′3β′). For the case of n = 128, Rogaway [21] proved—
via the computation of discrete logarithms—that the tweak domain [1, 2n/2] ×
[0, 1] works properly, but this result is inadequate for our purposes as we use a
compression function with n ∈{256, 512}. Granger et al. [12] recently computed
discrete logarithms for n ≤1024, therewith conﬁrming properness of the tweak
set domain. Note that the tweak sets computed in [12,21] commonly exclude
the all-zero tweak (α, β) = (0, 0) because it is a representative of 1 and hence
problematic for XEX: see also [21, Sect. 6] and [16, Sect. 4]. Because F is a one-
way function, its security analysis follows the one of XE, and this issue does not
apply.
Also from an eﬃciency point of view, there is a diﬀerence between the mask-
ing of F in p-OMD and in Spoed. In more detail, p-OMD uses the Gray code
masking (also used in OCB1 and OCB3) while for Spoed we have opted to
describe it with powering-up (used in OCB2 and in various CAESAR candi-
dates). Krovetz and Rogaway demonstrated that Gray codes are more eﬃcient
than powering-up [15], but on the downside they require more precomputation.
Granger et al. [12] revisited the principle of masking of tweakable blockciphers,
and presented a masking technique based on word-based linear feedback shift
registers that improves over both Gray codes and powering-up in terms of eﬃ-
ciency and simplicity. The new masking technique can be implemented with
Spoed with no sacriﬁce in security (and the result of Lemma 1 still applies).
6.2
Proof of Theorem 2
Let K ∈{0, 1}k. Note that all evaluations of FK are done in a tweakable manner,
namely via (1). We replace these tweakable evaluations of FK by a random tweak-
able compression function R
$←−
Func([1, 2n/2]×[0, 1]×{0, 1}n, {0, 1}2n, {0, 1}n).
Note that for both conﬁdentiality and integrity, the underlying FK is invoked at
most σ times. In other words, this step costs (cf. Lemma 1)
Adv

prf

F (σ, t) ≤1.5σ2
2n
+ Advprf
F (2σ, t′),
where t′ ≈t. This step has led us to an idealized version of Spoed, called IdSpoed.
IdSpoed is depicted in Fig. 4. Concretely, we have obtained that

88
T. Ashur and B. Mennink
Advconf
Spoed(nr, q, ℓ, σ, t) ≤Advconf
IdSpoed(nr, q, ℓ, σ) + 1.5σ2
2n
+ Advprf
F (2σ, t′),
Advint
Spoed(nr, qE, qD, ℓ, σ, t) ≤Advint
IdSpoed(nr, qE, qD, ℓ, σ) + 1.5σ2
2n
+ Advprf
F (2σ, t′),
where t dropped out of the advantage function for IdSpoed because it has
become irrelevant (formally, we proceed by considering an adversary that is
unbounded in time). We prove in Lemma 2 that its conﬁdentiality security satis-
ﬁes Advconf
IdSpoed(nr, q, ℓ, σ) = 0, and in Lemma 3 that it provides integrity up to
bound Advint
IdSpoed(nr, qE, qD, ℓ, σ) ≤ℓqD
2n + qD
2τ .
. . .
b
b
τ
0b
Z1
1
Z0
1 =⟨τ⟩b
Z1
2
Z1
ℓ–1
Z1
ℓ
Z0
2
Z0
ℓ–1
Z0
ℓ
C1
Cℓ–1
T
trunc
RN
1,0
RN
2,0
RN
ℓ-1,0
RN
ℓ,1
Fig. 4. IdSpoed encryption, which outputs C = left|M|(C1∥· · · ∥Cℓ−1) and T
Lemma 2. The advantage of any nonce-respecting adversary trying to break the
conﬁdentiality of IdSpoed is bounded as:
Advconf
IdSpoed(nr, q, ℓ, σ) = 0.
Proof. The functions RN
i,j for i = 1, . . . , ℓ−1, j = 0, 1, and N ∈{0, 1}n are inde-
pendently and randomly distributed compression functions. As the adversary is
assumed to be nonce-respecting, every nonce is used at most once. Every nonce
is used in at most ℓcalls to R, but these calls are by design all for diﬀerent
tweaks (i, j) ∈[1, 2n/2] × [0, 1]. Therefore, all responses are randomly generated
from {0, 1}n, and all ciphertext blocks and tag values are perfectly random.
⊓⊔
Lemma 3. The advantage of any nonce-respecting adversary trying to break the
integrity of IdSpoed is bounded as:
Advint
IdSpoed(nr, qE, qD, ℓ, σ) ≤ℓqD
2n + qD
2τ .
Proof. Assume that A has made encryption queries (N j, Aj, M j) for j =
1, . . . , qE, and denote the ciphertexts and tags by (Cj, T j). Write (Zj
1, . . . , Zj
ℓj) =
GPADn,τ(Aj, M j) and denote the in- and outputs of the random functions by
(sj
i, tj
i) for i = 1, . . . , ℓj.
Consider any forgery attempt (N, A, C, T), and denote its length by ℓ. Denote
the message computed upon decryption by M. Refer to the state values as (si, ti)

Damaging, Simplifying, and Salvaging p-OMD
89
for i = 1, . . . , ℓ, and write (Z1, . . . , Zℓ) = GPADn,τ(A, M). The forgery is suc-
cessful if T = leftτ(tℓ).
Denote by col the event that there exists an encryption query j with N j = N,
ℓj = ℓ, and an index i ∈{1, . . . , ℓ}, such that
tj
i−1 ⊕Z0j
i
∥Z1j
i
̸= ti−1 ⊕Z0
i ∥Z1
i ∧tj
i = ti.
Note that, as the adversary is nonce-respecting, there is at most one query j
with N j = N. We have, using shorthand notation [i = ℓ] for 0 if i ̸= ℓand 1 if
i = ℓ,
Pr (col) ≤
ℓ

i=1
Pr

sj
i ̸= si ∧RN
i,[i=ℓ](sj
i) = RN
i,[i=ℓ](si)

≤ℓ
2n .
(4)
We make the following, fairly simple, case distinction:
(i) N /∈{N 1, . . . , N qE}. This particularly means that R has never been queried
for tweak (ℓ, 1, N), and thus that RN
ℓ,1 responds with tℓ
$←−{0, 1}n. The
forgery is successful with probability 1/2τ;
(ii) N = N j for some (unique) j. As the diﬀerent evaluations of IdSpoed for
diﬀerent tweaks are independent, it suﬃces to focus on these two construc-
tion queries (the jth encryption query and the forgery). We proceed with a
further case distinction:
– ℓ̸= ℓj. This, again, means that R has never been queried for tweak
(ℓ, 1, N). The forgery is successful with probability 1/2τ;
– ℓ= ℓj. We proceed with a further case distinction:
• sℓ̸= sj
ℓj. In this case, R has been queried before for tweak (ℓ, 1, N),
but only once (as the adversary must be nonce-respecting) and never
on input sℓ. Consequently, the response tℓis uniformly randomly
drawn from {0, 1}n and the forgery is successful with probability 1/2τ;
• sℓ= sj
ℓj. As the forgery must be diﬀerent from the encryption queries,
and as GPADn,τ is an injective mapping, this case implies the exis-
tence of a non-trivial state collision. Hence, the forgery is successful
with probability at most Pr (col).
Concluding, the forgery is successful with probability at most Pr (col) + 1/2τ,
where Pr (col) is bounded in (4). A summation over all qD forgery attempts
(cf. [4]) gives our ﬁnal bound.
⊓⊔
7
Security of Spoednic (Theorem 3)
The proof of Theorem 3 is given in Sect. 7.2. It relies on a preliminary result on
a tweakable keyed compression function, which will be given in Sect. 7.1.

90
T. Ashur and B. Mennink
7.1
Security of Tweakable Keyed Compression Function
We will use a slightly more complex version of the tweakable keyed compression
function of Sect. 6.1, where the masking using Z0
i · L′ is included within the
function. The proof is a fairly straightforward extension of the one of Lemma 1.
Lemma 4. Let F : {0, 1}k×{0, 1}2n →{0, 1}n be a keyed compression function.
Let T = [1, 2n/2]×[0, 1]×{0, 1}n×{0, 1}n, and deﬁne F : {0, 1}k×T ×{0, 1}2n →
{0, 1}n as
F(K, (α, β, A, N), S) = F(K, (2α3β · FK(N∥0) ⊕A · FK(N∥1) ∥0n) ⊕S). (5)
Then, we have
Adv

prf

F (q, t) ≤1.5q2
2n
+ Advprf
F (3q, t′),
where t′ ≈t.
Proof. The proof is a slight extension of the one of Lemma 1, where now we have
twice as many subkeys. Consequently, this means that the transcript contains
twice as many subkeys, but because the diﬀerent subkey generations never collide
(due to domain separation 0/1), and for collisions between construction queries
and subkey evaluations of F we focus on the leftmost n bits (i.e., N) in the ﬁrst
place, this does not aﬀect the security bound. The proof is included in the full
version of the paper [3].
⊓⊔
7.2
Proof of Theorem 3
Let K ∈{0, 1}k. Note that all evaluations of FK are done in a tweakable man-
ner, namely via (5). We replace these tweakable evaluations of FK by a ran-
dom tweakable compression function R
$←−
Func([1, 2n/2] × [0, 1] × {0, 1}n ×
{0, 1}n, {0, 1}2n, {0, 1}n). Note that for both conﬁdentiality and integrity, the
underlying FK is invoked at most σ times. In other words, this step costs
(cf. Lemma 4)
Adv

prf

F (σ, t) ≤1.5σ2
2n
+ Advprf
F (3σ, t′),
where t′ ≈t. This step has led us to an idealized version of Spoednic, called
IdSpoednic. IdSpoednic is depicted in Fig. 5. Concretely, we have obtained that
Advconf
Spoednic(nr, q, ℓ, σ, t) ≤Advconf
IdSpoednic(nr, q, ℓ, σ) + 1.5σ2
2n
+ Advprf
F (3σ, t′),
Advint
Spoednic(n, qE, qD, ℓ, σ, t) ≤Advint
IdSpoednic(n, qE, qD, ℓ, σ) + 1.5σ2
2n
+ Advprf
F (3σ, t′),

Damaging, Simplifying, and Salvaging p-OMD
91
. . .
b
b
τ
0b
Z1
1
Z0
1 =⟨τ⟩b
Z1
2
Z1
ℓ–1
Z1
ℓ
Z0
2
Z0
ℓ–1
Z0
ℓ
C1
Cℓ–1
T
trunc
RN
1,0
RN
2,0
RN
ℓ-1,0
RN
ℓ,1
Fig. 5. IdSpoednic encryption, which outputs C = left|M|(C1∥· · · ∥Cℓ−1) and T. The
boxes in R indicate that Z0
i also functions as a tweak.
where n ∈{nr, nm}, and where t dropped out of the advantage function
for IdSpoednic because it has become irrelevant. The remainder of the proof
centers around this scheme. For the nonce-respecting setting, the bounds of
Lemmas 2 and 3 carry over almost verbatim, with the same security bound.
We consider integrity in the nonce-misuse setting in Lemma 5 and prove that
Advint
IdSpoednic(nm, qE, qD, ℓ, σ) ≤ℓq2
E/2
2n
+ ℓqEqD
2n
+ qD
2τ .
Lemma 5. The advantage of any nonce-misusing adversary trying to break the
integrity of IdSpoednic is bounded as:
Advint
IdSpoed(nm, qE, qD, ℓ, σ) ≤ℓq2
E/2
2n
+ ℓqEqD
2n
+ qD
2τ .
Proof. At a high level, the proof follows the one of Lemma 3, with the diﬀer-
ence that now, potentially, nonces may be the same leading to a slightly dif-
ferent bound. A formal proof of Lemma 5 is included in the full version of the
paper [3].
⊓⊔
Acknowledgments. This work was supported in part by the Research Council KU
Leuven: GOA TENSE (GOA/11/007). In addition, this work was partially supported
by the Research Fund KU Leuven, OT/13/071, and by European Unions Horizon
2020 research and innovation programme under No. H2020-MSCA-ITN-2014-643161
ECRYPT-NET. Bart Mennink is a Postdoctoral Fellow of the Research Foundation –
Flanders (FWO).
References
1. Andreeva, E., Bogdanov, A., Luykx, A., Mennink, B., Tischhauser, E., Yasuda,
K.: Parallelizable and authenticated online ciphers. In: Sako, K., Sarkar, P. (eds.)
ASIACRYPT 2013, Part I. LNCS, vol. 8269, pp. 424–443. Springer, Heidelberg
(2013)
2. Ashur, T., Mennink, B.: Trivial nonce-misusing attack on pure OMD. Cryptology
ePrint Archive, Report 2015/175 (2015)
3. Ashur, T., Mennink, B.: Damaging, simplifying, and salvaging p-OMD. Cryptology
ePrint Archive, Report 2016/534 (2016). http://eprint.iacr.org/2016/534
4. Bellare, M., Goldreich, O., Mityagin, A.: The power of veriﬁcation queries in
message authentication and authenticated encryption. Cryptology ePrint Archive,
Report 2004/309 (2004)

92
T. Ashur and B. Mennink
5. Bellare, M., Namprempre, C.: Authenticated encryption: relations among notions
and analysis of the generic composition paradigm. J. Cryptology 21(4), 469–491
(2008)
6. CAESAR: Competition for Authenticated Encryption: Security, Applicability, and
Robustness, May 2014. http://competitions.cr.yp.to/caesar.html
7. Chen, S., Lampe, R., Lee, J., Seurin, Y., Steinberger, J.: Minimizing the two-round
even-mansour cipher. In: Garay, J.A., Gennaro, R. (eds.) CRYPTO 2014, Part I.
LNCS, vol. 8616, pp. 39–56. Springer, Heidelberg (2014)
8. Chen, S., Steinberger, J.: Tight security bounds for key-alternating ciphers. In:
Nguyen, P.Q., Oswald, E. (eds.) EUROCRYPT 2014. LNCS, vol. 8441, pp. 327–
350. Springer, Heidelberg (2014)
9. Cogliani, S., Maimut, D., Naccache, D., do Canto, R.P., Reyhanitabar, R.,
Vaudenay, S., Viz´ar, D.: Oﬀset Merkle-Damgrd (OMD) version 1.0, submission
to CAESAR competition (2014)
10. Cogliani, S., Maimut¸, D.-S., Naccache, D., do Canto, R.P., Reyhanitabar, R.,
Vaudenay, S., Viz´ar, D.: OMD: a compression function mode of operation for
authenticated encryption. In: Joux, A., Youssef, A. (eds.) SAC 2014. LNCS, vol.
8781, pp. 112–128. Springer, Heidelberg (2014)
11. Fleischmann, E., Forler, C., Lucks, S.: McOE: a family of almost foolproof on-line
authenticated encryption schemes. In: Canteaut, A. (ed.) FSE 2012. LNCS, vol.
7549, pp. 196–215. Springer, Heidelberg (2012)
12. Granger, R., Jovanovic, P., Mennink, B., Neves, S.: Improved masking for tweak-
able blockciphers with applications to authenticated encryption. In: Fischlin, M.,
Coron, J.-S. (eds.) EUROCRYPT 2016. LNCS, vol. 9665, pp. 263–293. Springer,
Heidelberg (2016). doi:10.1007/978-3-662-49890-3 11
13. Iwata, T., Ohashi, K., Minematsu, K.: Breaking and repairing GCM security
proofs. In: Safavi-Naini, R., Canetti, R. (eds.) CRYPTO 2012. LNCS, vol. 7417,
pp. 31–49. Springer, Heidelberg (2012)
14. Jovanovic, P., Luykx, A., Mennink, B.: Beyond 2c/2 security in sponge-based
authenticated encryption modes. In: Sarkar, P., Iwata, T. (eds.) ASIACRYPT 2014.
LNCS, vol. 8873, pp. 85–104. Springer, Heidelberg (2014)
15. Krovetz, T., Rogaway, P.: The software performance of authenticated-encryption
modes. In: Joux, A. (ed.) FSE 2011. LNCS, vol. 6733, pp. 306–327. Springer,
Heidelberg (2011)
16. Minematsu, K.: Improved security analysis of XEX and LRW modes. In: Biham, E.,
Youssef, A.M. (eds.) SAC 2006. LNCS, vol. 4356, pp. 96–113. Springer, Heidelberg
(2007)
17. Patarin, J.: The “coeﬃcients H” technique. In: Avanzi, R.M., Keliher, L., Sica, F.
(eds.) SAC 2008. LNCS, vol. 5381, pp. 328–345. Springer, Heidelberg (2009)
18. Reyhanitabar, R., Vaudenay, S., Viz´ar, D.: Misuse-resistant variants of the OMD
authenticated encryption mode. In: Chow, S.S.M., Liu, J.K., Hui, L.C.K., Yiu,
S.M. (eds.) ProvSec 2014. LNCS, vol. 8782, pp. 55–70. Springer, Heidelberg (2014)
19. Reyhanitabar, R., Vaudenay, S., Viz´ar, D.: Boosting OMD for almost free authen-
tication of associated data. In: Leander, G. (ed.) FSE 2015. LNCS, vol. 9054, pp.
411–427. Springer, Heidelberg (2015)
20. Reyhanitabar, R., Vaudenay, S., Viz´ar, D.: Boosting OMD for almost free authen-
tication of associated data. In: FSE 2015 preprint version (2015)
21. Rogaway, P.: Eﬃcient instantiations of tweakable blockciphers and reﬁnements to
modes OCB and PMAC. In: Lee, P.J. (ed.) ASIACRYPT 2004. LNCS, vol. 3329,
pp. 16–31. Springer, Heidelberg (2004)

Cryptographic Protocols

Blind Password Registration for Two-Server
Password Authenticated Key Exchange
and Secret Sharing Protocols
Franziskus Kiefer1(B) and Mark Manulis2
1 Mozilla, Berlin, Germany
mail@franziskuskiefer.de
2 Department of Computer Science, Surrey Center for Cyber Security,
University of Surrey, Guildford, UK
mark@manulis.eu
Abstract. Many organisations enforce policies on the length and for-
mation of passwords to encourage selection of strong passwords and pro-
tect their multi-user systems. For Two-Server Password Authenticated
Key Exchange (2PAKE) and Two-Server Password Authenticated Secret
Sharing (2PASS) protocols, where the password chosen by the client is
secretly shared between the two servers, the initial remote registration
of policy-compliant passwords represents a major problem because none
of the servers is supposed to know the password in clear.
We solve this problem by introducing Two-Server Blind Password
Registration (2BPR) protocols that can be executed between a client
and the two servers as part of the remote registration procedure.
2BPR protocols guarantee that secret shares sent to the servers belong
to a password that matches their combined password policy and that the
plain password remains hidden from any attacker that is in control of at
most one server. We propose a security model for 2BPR protocols cap-
turing the requirements of policy compliance for client passwords and
their blindness against the servers. Our model extends the adversarial
setting of 2PAKE/2PASS protocols to the registration phase and hence
closes the gap in the formal treatment of such protocols. We construct
an eﬃcient 2BPR protocol for ASCII-based password policies, prove its
security in the standard model, give a proof of concept implementation,
and discuss its performance.
1
Introduction
Password policies set by organisations aim to rule out potentially “weak” pass-
words and by this contribute to the protection of multi-user systems. In tradi-
tional web-based password authentication mechanisms a password policy chosen
by the server is typically enforced during the password registration phase — the
corresponding compliance check is performed either by the client or on the server
side, depending on the available trust assumptions. If the client is not trusted
with the selection of a policy-compliant password, then the compliance check
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 95–114, 2016.
DOI: 10.1007/978-3-319-45871-7 7

96
F. Kiefer and M. Manulis
must be performed by the server. The most common approach in this case is
to transmit chosen passwords over a secure channel, e.g., TLS channel, to the
server that performs the check on the received (plain) password. The drawback
of this approach, however, is that the client’s password is disclosed to the server.
Although this approach represents a common practice nowadays, its main draw-
back is the necessity to trust the server to process and store the received pass-
word in a protected way, e.g., by hashing it. This trust assumption often does not
hold in practice as evident from the frequent server-compromise attacks based
on which plain password databases have been disclosed [8,26,31,33].
Considering that “password-cracking tools” such as Hashcat [15] and John
the Ripper [28] are very eﬃcient, it is safe to assume that leaked password hashes
are not safer than un-hashed ones when compromised by an attacker [3,9,11,24].
The notion of threshold and two-server password authenticated key-exchange [12,
25] has been proposed where the password is not stored on a single server but split
between a number of servers such that leakage of a password database on a non-
qualiﬁed subset does not reveal the password. The two-server setting is regarded
as more practical (in comparison to a more general threshold setting) given that
if one server is compromised a notiﬁcation to change the password can be sent
out to the clients. Two-server password authenticated key-exchange protocols
(2PAKE) [4,20,32] split the client’s password pw into two shares s1 and s2 such
that each share is stored on a distinct server. During the authentication phase
both servers collaborate in order to authenticate the client. Yet, no server alone
is supposed to learn the plain password. A second, more recent development in
two-server (and threshold) password protocols is password authenticated secret
sharing (PASS) [2,6,17] where a client stores shares of a (high-entropy) secret
key on a number of servers and uses a (low-entropy) password to authenticate
the retrieval process.
Registering password shares for 2PAKE/2PASS protocols however makes it
impossible for the servers to verify their password policies upon registration
unless the password is transferred to each of them in plain. This however, would
imply that the client trusts both servers to securely handle its password, which
contradicts the purpose and trust relationships of multi-server protocols. The use
of two-server password protocols in a remote authentication setting, therefore,
requires a suitable password registration procedure in which none of the servers
would receive information enabling it (or an attacker in control of the server)
to deliberately or inadvertently recover the client’s password. This registration
procedure must further allow for policy compliance checks to be performed by
the servers since secret sharing per se does not protect against “weak” pass-
words. A trivial approach of sending s1 and s2 to the corresponding servers over
secure channels is not helpful here since it is not clear how the two servers can
perform the required compliance check. To alleviate a similar problem in the
veriﬁer-based PAKE setting, Kiefer and Manulis [22] introduced the concept of
zero-knowledge password policy checks (ZKPPC), where upon registration the
client can prove to the server the compliance of its chosen password with respect
to the server’s policy without disclosing the actual password. In this work, we

Blind Password Registration for 2PAKE and Secret Sharing Protocols
97
propose the concept of blind password registration for two-server password pro-
tocols and thus show how to realise secure registration of password shares in
a way that protects against at most one malicious server (if both servers are
malicious, the attacker obviously gets the password), yet allows both servers
to check password compliance against their mutual password policy. It bases
on techniques introduced in the framework for ZKPPC from [22] but uses a
security model for the entire blind setup process and is based in the two-server
setting which brings additional challenges. Two-server Blind Password Registra-
tion (2BPR) is not vulnerable to oﬄine dictionary attacks as long as one server
remains honest. This is in contrast to the single-server setting where an attacker
is always able to perform oﬄine dictionary attacks on password veriﬁers after
compromising a server. Our main contribution is the 2BPR security model and
the corresponding protocol for secure registration of 2PAKE/2PASS passwords.
We show how secure distribution of password shares can be combined with an
appropriate policy-compliance proof for the chosen password in a way that does
not reveal the password and can still be veriﬁed by both servers. Our 2BPR pro-
tocol can be used to enforce policies over the alphabet of all 94 printable ASCII
characters1, including typical requirements on password length and character
types.
2
Preliminaries
In this section we recall the underlying primitives and concepts that are used in
the construction of our two-server blind password registration protocol.
2.1
Commitments
Let C = (CSetup, Com) denote a commitment scheme and C ←Com(x; r) a com-
mitment on x using randomness r, with CSetup generating parameters for C. A
commitment scheme C = (CSetup, Com) is eﬃcient if CSetup(λ) and (C, d) ←
Com(x; r) are computable in polynomial time, complete if Com(d) = (C, d) for
(C, d) ←Com(x; r), and secure if it is
– Binding: For all PPT adversaries A there exists a negligible function εbi(·)
such that for all (x, x′, r, r′, C) ←A: Pr[x ̸= x′ ∧(C, d) = Com(x; r) ∧(C, d′) =
Com(x′; r′)] ≤εbi(λ),
– Hiding: For all PPT adversaries A there exists a negligible function εhi(·) such
that for all x0, x1 with |x0| = |x1| and b ∈R {0, 1}, (C, d) ←Com(xb; r) and
b′ ←A(C, x1, x2): Pr[b = b′] ≤1/2 + εhi(λ).
Pedersen Commitments [29]. We use perfectly hiding, computationally bind-
ing, homomorphic Pedersen commitments [29] deﬁned as follows. Let CP =
(CSetup, Com) with (g, h, q, λ) ←CSetup(λ) and C ←Com = (x; r) = gxhr
1 Note that using other encodings such as UTF-8 is possible but might inﬂuence
performance due to a diﬀerent size of possible characters.

98
F. Kiefer and M. Manulis
denote the Pedersen commitment scheme where g and h are generators of a
cyclic group G of prime order q with bit-length in the security parameter λ
and the discrete logarithm of h with respect to base g is not known. Pedersen
commitments are additively homomorph, i.e. for all (Ci, di) ←Com(xi; ri) for
i ∈0, . . . , m it holds that m
i=0 Ci = Com(m
i=0 xi; m
i=0 ri).
Trapdoor Commitments. In order to build zero-knowledge proofs of knowl-
edge with malicious veriﬁers we require a trapdoor commitment scheme, which
allows a party knowing the correct trapdoor to open a commitment to any value.
Fortunately, Pedersen commitments are trapdoor commitments as they can be
opened to any element using the discrete logarithm logg(h) as trapdoor.
2.2
Zero Knowledge Proofs
A zero-knowledge proof is executed between a prover and a veriﬁer, proving that
a word x is in a language L, using a witness w proving so. An interactive protocol
ZKP for a language L between prover P and veriﬁer V is a zero knowledge proof
if the following holds:
– Completeness: If x ∈L, V accepts if P holds a witness proving so.
– Soundness: For every malicious prover P ∗(x) with x ∈L that the probability
of making V accept is negligible.
– Zero-Knowledge: If x ∈L, then there exists an eﬃcient simulator Sim that
on input of x is able to generate a view, indistinguishable from the view of a
malicious veriﬁer V ∗.
A zero-knowledge proof of knowledge ZKPoK is a zero-knowledge proof with the
following special soundness deﬁnition:
– Soundness: There exists an eﬃcient knowledge extractor Ext that can extract
a witness from any malicious prover P ∗(x) with x ∈L that has non-negligible
probability of making V accept.
We use the following committed Σ-protocol to ensure extractability (ZKPoK)
and simulatability when interacting with a malicious veriﬁer [7,18]. Let
P1(x, w, r) and P2(x, w, r, c) denote the two prover steps of a Σ-protocol and
H : {0, 1}∗→Zq a collision-resistant hash function. A committed Σ-protocol
based on Pedersen commitments is then given by the following steps:
– The prover computes m1
←
P1(x, w, r), Co
←
Com(H(x, m1); r1)
=
gH(x,m1)hr1, and sends Co to the veriﬁer.
– The veriﬁer picks random challenge Ch = c and returns it to the prover.
– The prover computes m2
←
P2(x, w, r, c), Rs1
←
Com(H(m2); r2)
=
gH(m2)hr2, and sends Rs to the veriﬁer.
– Further, the prover opens the commitments Co and Rs1 by sending Rs2 =
(x, m1, m2, r1, r2) to the veriﬁer.

Blind Password Registration for 2PAKE and Secret Sharing Protocols
99
– The veriﬁer accepts if both commitments are valid and if the veriﬁcation of
the Σ-protocol (x, m1, c, m2) is successful.
We note that in the malicious veriﬁer setting, this type of protocol is a concurrent
zero-knowledge proof since its security proof does not require rewinding [7,18].
We observe that all zero-knowledge protocols used in this work are committed Σ-
protocols those security relies on the hardness of the discrete logarithm problem
in G and the collision resistance property of H.
Passwords. We adopt the reversible, structure-preserving encoding scheme
from [22] that (uniquely) maps strings of printable ASCII characters to inte-
gers. We use pw for the ASCII password string, ci = pw[i] for the i-th ASCII
character in pw, and integer π for the encoded password string. The encoding
proceeds as follows: π ←PWDtoINT(pw) = n−1
i=0 bi(ASCII(ci)−32) for the pass-
word string pw and πi ←CHRtoINT(ci) = ASCII(ci) −32 for the i-th unshifted
ASCII character in pw. Note that n denotes the length of pw and b ∈N is used
as shift base. (We refer to [22] for a discussion on the shift base b. Note, however,
that shift base related attacks on the password veriﬁer from [22] are not possible
in our two-server setting.) The ASCII function returns the decimal ASCII code
of a character.
Remark 1. While password distribution is important for the security of pass-
word registration protocols for Veriﬁer-based PAKE [22], the role of password
distribution in the two-server setting is diﬀerent. Since each server stores only a
random-looking password share, oﬄine dictionary attacks from an attacker who
compromises at most one of the two servers become infeasible. Security of 2BPR
protocols deﬁned in this work is therefore independent of client passwords. Note
however that the password strength still continues to play an important role for
the security of 2PAKE/2PASS protocols, where it inﬂuences the probability of
successful online dictionary attacks.
Password Sharing. We focus on the additive password sharing of client pass-
words, i.e. π = s0 +s1 mod q over a prime-order group Gq. Such sharing has been
used in various 2PAKE protocols, including [19–21,34]. To be used in combina-
tion with 2PASS protocols such as [6] one can deﬁne the password as gπ and
thus adopts the multiplicative sharing gπ = gs0gs1. Password shares are created
as s0 ∈R Zq and s1 = π −s0 mod q. We remark that other sharing options such
as XOR have been used in literature [4,32] but are not supported by our 2BPR
protocol.
Password Policies. We represent password policies as in [22], i.e. a password
policy f = (R, nmin) consists of a simpliﬁed regular expression R that deﬁnes
ASCII subsets that must be present in the chosen password string and the min-
imum length nmin of the password string. The expression R is deﬁned over the
four ASCII subsets Σ = {d, u, l, s} with digits d, upper case letters u, lower

100
F. Kiefer and M. Manulis
case letters l and symbols s, and gives the minimum frequency of a character
from the subset that is necessary to fulﬁl the policy; for instance, R = ulld
means that policy-conform password strings must contain at least one upper
case letter, two lower case letters and one digit. In the two-server setting, if
each of the servers has its own password policy, i.e. f0 and f1, then registered
passwords would need to comply with the mutual password policy deﬁned as
f = f0 ∩f1 = (max(R0, R1), max(nmin0, nmin1)), where max(R0, R1) is the regu-
lar expression with the maximum number of characters from each of the subsets
u, l, d, s from R0 and R1. A mutual policy is fulﬁlled, i.e. f(pw) = true, iﬀ
f0(pw) = true and f1(pw) = true, and not fulﬁlled, i.e. f(pw) = false, iﬀ
f0(pw) = false or f1(pw) = false. We mainly operate on the integer repre-
sentation π of a password string pw throughout this paper and sometimes write
f(π), which means f(pw) for π ←PWDtoINT(pw). Further note that a charac-
ter ci ∈pw is called signiﬁcant if this character is necessary to fulﬁl R and we
denote the corresponding set Rj ∈R as a signiﬁcant set for the policy.
Password Dictionaries. A password dictionary Df, if not speciﬁed otherwise,
is a set of password strings adhering to a given policy f = (R, nmin), i.e. their
length is limited by nmin ≤|pw| and the required types of characters are identiﬁed
by R. We denote the size of a dictionary D by |D|. We omit index f if the policy is
clear from the context. We further deﬁne dictionary Df,n holding policy-conform
passwords according to f of length n and will use it throughout the paper. In
order to be able to use the optimal dictionary Df, the client would either have
to prove correctness of password characters that are not necessary for R without
revealing their number (which seems impossible with the approach used in this
paper), or use a ﬁxed password length to hide n in it (which is ineﬃcient). (Note
that we only consider reasonable dictionaries sizes, i.e. |Df,n| > 1).
3
Two-Server Blind Password Registration
Two-server Blind Password Registration (2BPR) allows a client to register pass-
word shares with two servers for later use in 2PAKE/2PASS protocols and prove
that the shares can be combined to a password that complies with the mutual
password policy of both servers, without disclosing the password. A 2BPR pro-
tocol is executed between client C and two servers S0 with password policy f0
and S1 with password policy f1. C interacts with S0 and S1 in order to distrib-
ute shares of a freshly chosen password string pw and prove its compliance with
the mutual policy, i.e. f0(pw) = true and f1(pw) = true. A 2BPR protocol
between an honest client C and two honest servers S0 and S1 is correct if S0
and S1 accept their password shares if and only if the client is able to prove the
following statement for f = f0 ∩f1:
(pw, s0, s1) : PWDtoINT(pw) = s0 + s1 ∧f(pw) = true.
(1)
Note that the 2BPR protocol can be used to register new clients or to register
new passwords for existing clients. The following deﬁnition formally captures the
functionality of 2BPR protocols.

Blind Password Registration for 2PAKE and Secret Sharing Protocols
101
Deﬁnition 1 (Two-Server Blind Password Registration). A 2BPR proto-
col is executed between a client C and two servers S0 and S1, holding a password
policy fb each, such that the servers, when honest, eventually accept password
shares sb of a policy compliant, client chosen password pw iﬀf(pw) = true for
f = f0 ∩f1, PWDtoINT(pw) = sb + s1−b and b ∈{0, 1}.
Deﬁnition 1 requires that password shares s0 and s1 can be combined to the
policy-compliant integer password π. The corresponding veriﬁcation must there-
fore be part of the 2BPR protocol. Otherwise, the client could register password
shares s0 and s1 that can both be combined to a policy compliant password in
the respective proofs with the servers, but combining s0 and s1 might result in a
password that is not policy compliant, i.e. f(s0+s′) = true and f(s1+s′′) = true
but f(π) ̸= true. This further ensures that servers hold valid password shares,
which is crucial for the security of 2PAKE/2PASS protocols that should be exe-
cuted later with these password shares. We assume that the protocol is initiated
by servers (possibly after the client expresses his interest to register). This allows
each server to send its password policy to the client. We further assume that both
servers can communicate with each other over an authenticated and conﬁdential
channel. This communication can either be done directly between the servers or
indirectly using the client to transmit messages.
3.1
Security Model for 2BPR Protocols
2BPR protocols must guarantee that the client knows the sum PWDtoINT(pw)
of the password shares s0 and s1, and that pw fulﬁls both password poli-
cies f0 and f1 if both servers accept the registration procedure. We translate
Eq. (1) into a game-based security model that captures 2BPR security in form
of two security requirements. The ﬁrst requirement is called Policy Compliance
(PC) of the registered password. In particular, if both servers are honest while
accepting their password shares in the 2BPR protocol, the combination π of the
shares represents a password compliant with their mutual policy f = f0 ∩f1,
i.e. f(sb + s1−b) = true. The second requirement relates to the fact that servers
should not learn anything about the registered password and is therefore called
Password Blindness (PB), i.e. a malicious server Sb may only learn whether a
registered password is compliant with the mutual policy and nothing else. We
observe that the blindness property must hold for all possible password policies
and all compliant passwords. PB also implies impossibility of mounting an oﬄine
dictionary attack after observing 2BPR executions or through gaining access to
and controlling at most one of the servers.
Setup and Participants. Protocol participants C, S0, S1 with C from the uni-
verse of clients and S0, S1 from the universe of servers have common inputs,
necessary for the execution of the protocol. Instances of protocol participants
C or S are denoted Ci, S0,i or S1,i. Protocol participants without speciﬁed role
are denoted by P, and Sb and S1−b for unspeciﬁed servers. A client can register

102
F. Kiefer and M. Manulis
one password with any pair of servers from the universe. We use C and Sb as
unique identiﬁers for the client and servers (e.g. C can be seen as a username
that will be stored by servers alongside with password shares). We say a client
C registers a password share for (C, S1−b) at server Sb and a password share for
(C, Sb) at server S1−b. There can be only at most one (most recent) password
share registered at Sb resp. S1−b for (C, S1−b) resp. (C, Sb) at any given time. A
tuple (C, S1−b, sb) is stored on server Sb and tuple (C, Sb, s1−b) on server S1−b
only if the 2BPR protocol is viewed as successful by the servers.
Oracles. A PPT adversary A has access to Setup, Send, Execute and Corrupt
oracles for interaction with the protocol participants.
– Setup(C, S0, S1, pw′) creates new instances of all participants and stores iden-
tiﬁers of the other parties to each participant. To this end the client receives
the server policies f0 ∩f1 = f and either chooses a new policy compliant
password pw ∈Df if pw′ = ⊥or uses pw = pw′.
– Execute(C, S0, S1) models a passive attack and executes a 2BPR protocol
between new instances of C, S0 and S1. It returns the protocol transcript
and the internal state of all corrupted parties.
– SendC(Ci, Sb,j, m) sends message m, allegedly from client instance Ci, to server
instance Sb,j for b ∈{0, 1}. If Ci or Sb,j does not exist, the oracle aborts. Note
that any instance Ci and Sb,j was thus set up with Setup and therefore has an
according partner instance S1−b,j. If all participants exist, the oracle returns
the server’s answer m′ if there exists any. Necessary inter server communi-
cation is performed in SendC queries. If m = ⊥, server Sb,j returns its ﬁrst
protocol message if it starts the protocol.
– SendS(Sb,i, Cj, m) sends message m, allegedly from server instance Sb,i for
b ∈{0, 1}, to client instance Cj. If Sb,i or Cj does not exist, the oracle aborts.
Note that any instance Sb,i and Cj was thus set up with Setup and therefore
has an according partner instance S1−b,i. If all participants exist, the oracle
returns the client’s answer m′ if there exists any. If m = ⊥, server Sb,i returns
its ﬁrst message if he starts the protocol.
– SendSS(Sb,i, S1−b,j, m) sends message m, from server instance Sb,i for b ∈
{0, 1}, to server instance S1−b,j. If Sb,i or S1−b,j does not exist, the oracle
aborts. Note that any instance Sb,i and S1−b,j was thus set up with Setup. If
all participants exist, the oracle returns the server’s answer m′ if there exists
any.
– Corrupt(Sb) allows the adversary to corrupt a server Sb and retrieve its internal
state, i.e. stored messages and randomness, and the list of stored password
shares (C, S1−b, sb). Sb is marked corrupted.
Note that we allow the adversary to register passwords with servers without
requiring existence of a client instance Ci in a successful registration session.
This is because we do not assume authenticated clients, i.e. client identiﬁers C
are unique but not secret and can therefore be used by the adversary.

Blind Password Registration for 2PAKE and Secret Sharing Protocols
103
Policy Compliance. This is a natural security property of 2BPR proto-
cols, requiring that registered client passwords comply with the mutual policy
f(pw) = true. The attacker here plays the role of the client trying to register a
password pw that is not policy compliant at two honest servers.
Deﬁnition 2 (Policy Compliance). Policy compliance of a 2BPR protocol
holds if for every PPT adversary A with access to Setup and SendC oracles the
probability that two server instances Sb,i and S1−b,j exist after A stopped that
accepted (C, S1−b, sb), (C, Sb, s1−b) respectively, with f(sb + s1−b) = false is
negligible.
Password Blindness. This property requires that every password, chosen and
set-up by an honest client must remain hidden from an adversary who may cor-
rupt at most one of the two servers, thus obtaining the internal state and taking
full control over the corrupted server. We model password blindness through a
distinguishing experiment where the attacker, after interacting with the oracles,
outputs a challenge comprising of two passwords (pw0 and pw1), two clients
(C0 and C1), and a pair of servers (S0 and S1). After a random assignment of
passwords to the two clients, the adversary interacts with the oracles again and
has to decide which client used which password in the 2BPR protocol execution.
This is formalised in the following deﬁnition.
Deﬁnition 3 (Password Blindness). The password blindness property of a
2BPR protocol Π holds if for every PPT adversary A there exists a negligible
function ε(·) such that
AdvPB
Π,A =
Pr[ExpPB
Π,A = 1] −1
2
 ≤ε(λ).
ExpPB
Π,A :
(C0, C1, S0, S1, pw0, pw1) ←ASetup,SendS,SendSS,Execute,Corrupt
1
check pw0, pw1 ∈Df0∩f1, |pw0| = |pw1|, C0, C1 ∈{C} and S0, S1 ∈{S}
b′ ←ASetup′,SendS,SendSS,Execute,Corrupt(λ, D, {C}, {S})
if S0 or S1 is uncorrupted, return b = b′; otherwise return 0
where the modiﬁed oracle Setup′ (in contrast to Setup) picks a random bit b ∈R
{0, 1} and uses pwb as a password for client C0 and pw1−b for C1.
4
An Eﬃcient Two-Server BPR Protocol
Before diving into technical details, we give a high-level description of our 2BPR
protocol. We assume that client C selected two servers S0 and S1 to register with.
We also assume the existence of server-authenticated and conﬁdential channels
(e.g. TLS channels [10,16,23]) between C and each Sb, b ∈{0, 1} as well as
between S0 and S1. These channels prevent active impersonation of any server Sb,

104
F. Kiefer and M. Manulis
b ∈{0, 1} and hide the contents of exchanged messages unless the corresponding
server is corrupted.
Our 2BPR protocol further assumes a common reference string crs = (g, h, q)
containing two generators g and h of a cyclic group of prime order q where logg(h)
is not known.
At the beginning of the registration phase the client C commits to the integer
representation π of the chosen password string pw and sends this commitment
together with a password share sb to the corresponding server Sb, b ∈{0, 1},
along with auxiliary information that is needed to perform the policy compliance
proof. For the latter, the client needs to prove the knowledge of π in the com-
mitment such that π = s0 + s1 and that it fulﬁlls both policies f1 and f2. Thus,
servers S0 and S1 eventually register the new client, accept and store the client’s
password share, iﬀeach Sb holds sb such that s0 +s1 = π for π ←PWDtoINT(pw)
and f(pw) = true for f = f0 ∩f1.
4.1
Protocol Overview
In Fig. 1 we give an overview of the 2BPR protocol involving a client C and
two servers Sb, b ∈{0, 1}. The protocol proceeds in three phases. In the ﬁrst
phase (client preparation) the client chooses pw ∈R Df, encodes it to π, com-
putes shares s0 and s1, and computes commitments C0, C1, D0, D1 to the shares
and the password. In the second phase (password registration) C interacts with
each server Sb, b ∈{0, 1} over a server-authenticated and conﬁdential channel.
C computes a commitment Ci for each encoded character πi ←CHRtoINT(ci),
ci ∈pw, and a second commitment C′
i as a re-randomised version of Ci. The
set C′ containing the re-randomised commitments C′
i, is then shuﬄed and used
to prove through the Proof of Membership (PoM) protocol that each character
committed to in C′
i ∈C′ is a member of some character set ωφ(i), chosen accord-
ing to policy f. Note that PoM must be performed over the shuﬄed set C′ of
commitments as the server would otherwise learn the type (i.e. lower/upper case,
digit, or symbol) of each password character. To further prove that transmitted
commitments C, Cb, and Db are correct, namely that the product of commit-
ments in C commits to the password pw, Cb contains the correct share sb, and
Db contains pw, client and server execute the Proof of Correctness (PoC) pro-
tocol. Finally, the client proves to each server that set C′ is a shuﬄe of set C
by executing the Proof of Shuﬄe (PoS) protocol. This proof is necessary to
ﬁnally convince both servers that (1) the characters committed to in C′ are the
same as the characters in the commitments in C, which can be combined to
password pw (as follows from the PoC protocol) and (2) each commitment Ci
is for a character ci ∈pw from some set ωi, chosen according to policy f (as
follows from the PoM protocol). For all three committed Σ-protocols (PoM,
PoC, PoS) we use variables as deﬁned in Sect. 2. If each server Sb, b ∈{0, 1}
successfully veriﬁes all three committed Σ-protocols and the length of the com-
mitted password pw is policy-conform, then both servers proceed with the last
phase. In the third phase (share veriﬁcation) the two servers S0 and S1 interact
with each other over a mutually-authenticated and conﬁdential channel. Each

Blind Password Registration for 2PAKE and Secret Sharing Protocols
105
Fig. 1. Two-Server BPR Protocol — A high-level overview ω contains character sets of
cφ(i) ordered according to permutation φ used in PoM
Sb computes its veriﬁcation value D′
1−b and sends it to S1−b. Upon receiving D′
b,
Sb checks it against Db to verify that the client used the same password with
both servers in the second phase, i.e. that sb + s1−b = π. If this veriﬁcation is
successful, Sb stores the client’s password share (C, S1−b, sb) and considers C as
being registered.
4.2
Two-Server BPR Speciﬁcation
In the following we give a detailed description of the 2BPR protocol. To this end
we describe the three proofs PoC, PoM and PoS detailing on their computa-
tions. We describe the interaction between client C and server Sb and therefore
only consider one policy fb. Note that C and each server Sb perform the same
protocol. If both servers accept, the password fulﬁls the policy f = fb ∩f1−b.
We ﬁrst describe the client’s pre-computations such as password encoding
and sharing before giving a detailed description of the proofs. The protocol

106
F. Kiefer and M. Manulis
operates on a group G of prime-order q with generator g. Further, let h, fi ∈R G
for i ∈[−4, m] denote random group elements such that their discrete logarithm
with respect to g is unknown. Public parameters of the protocol are deﬁned as
(q, g, h, f) with f = {fi} where m is at least n = |pw|. In practice m can be
chosen big enough, e.g., 100, in order to process all reasonable passwords. Note
that we use the range i ∈[0, n −1] for characters pw[i], but [1, x] for most other
ranges.
Phase I – Client Preparation. We assume that password policies f0 and f1
are known by the client. This can be achieved by distributing them beforehand
with other set-up parameters. The client chooses a password pw ∈R Df from
the dictionary and encodes it π ←PWDtoINT(pw). The password is shared by
choosing a random sb ∈R Zq and computing s1−b = π −sb. The client then
commits to both password shares Cb = gsbhrb and C1−b = gs1−bhr1−b with
rb, r1−b ∈R Zq and computes commitments to the entire password π with the
same randomness, i.e. Db = Cbgs1−b and D1−b = C1−bgsb. For the following
proofs the client further encodes every character ci ∈pw as πi ←CHRtoINT(ci).
Phase II – Password Registration. The client iterates over all encoded
characters πi to perform the following operations: commit to πi by computing
Ci = gπihri, C′
i = Cihr′
i for ri, r′
i ∈R Z∗
q; choose a random permutation φ(i) over
[1, n] to shuﬄe C′
i; if πi is signiﬁcant for any Rj ∈R, set ωφ(i) ←Rj, otherwise
ωφ(i) ←Σ (all ASCII characters). Let li ∈N denote the index in ωφ(i) such that
ci = ωφ(i)[li]. Values (Ci, C′
i, ωφ(i), φ(i), li, πi, ri, r′
i) are used in the following zero-
knowledge proofs. The client combines previously computed values C = {Ci}.
Shuﬄed commitments C′
φ(i) and sets ωφ(i) are combined according to the shuﬄed
index φ(i), i.e. C′ = {C′
φ(i)} and ω = {ωφ(i)}. Once these computations are
ﬁnished C and Sb proceed with the protocol. In the following we describe the
three proofs PoM, PoC and PoS and deﬁne their messages.
Proof of Correctness (PoC). This proof links the password shares, sent to
each server, to the proof of policy compliance and shows knowledge of the other
password share. We deﬁne the proof of correctness for an encoded password π,
which proves that share sb can be combined with a second share s1−b such that
π = sb + s1−b and that the received commitments to password characters ci can
be combined to a commitment to that same password π. PoC is deﬁned as a
committed zero-knowledge proof between C and Sb for the statement
ZKP{(π, r1−b, rb, rCb) : C1−bgsb = gπhr1−b ∧
n−1

i=0
Cbi
i
= gπhrCb ∧Db = gπhrb}.
Ci = gπihri are character commitments from the set-up stage and rCb =
n−1
i=0 bi· ri is the combined randomness from the character commitments Ci.
C1−b = gs1−b hr1−b, Db = Cbgs1−b, and Cb = gsbhrb are the share and password
commitments from the client preparation phase. This incorporates the link of

Blind Password Registration for 2PAKE and Secret Sharing Protocols
107
the password commitment to the product of the commitments to the single char-
acters with the proof of knowledge of the combined password π = sb +s1−b. The
messages for PoC are computed as follows:
1. The client chooses random kπ, kρb, kρ(1−b), kρC ∈R Zq, computes tC(1−b) =
gkπhkρ(1−b), tC = gkπhkρC and tDb = gkπhkρb. The ﬁrst message with
rComPoC ∈R Zq is then given by commitment
ComPoC = gH(C1−bgsb,{Ci},Db,tC(1−b),tC,tDb)hrComPoC.
2. After receiving ComPoC from the client the server chooses a random challenge
ChPoC,b ∈R Zq and sends it back to the client.
3. After receiving the challenge ChPoC,b, the client computes sπ = kπ+ChPoC,bπ,
sρ(1−b) = kρ(1−b) + ChPoC,br1−b, sρC = kρC + ChPoC,b
n−1
i=0 biri and sρb =
kρb + ChPoC,brb before computing the next message with rRsPoC ∈R Zq
RsPoC1 = gH(sπ,sρ(1−b),sρC,sρb)hrRsPoC.
4. Eventually the client sets the decommitment message
RsPoC2 = (sb, C1−b, {Ci}, Db, tC(1−b), tC, tDb, sπ, sρ(1−b), sρC, sρb, rComPoC, rRsPoC).
RsPoC1 and RsPoC2 form together client’s response message RsPoC. The server
veriﬁes the proof by checking the following:
−ComPoC
?= gH(C1−bgsb,{Ci},Db,tC(1−b),tC,tDb)hrComPoC
−RsPoC1
?= gH(sπ,sρ(1−b),sρC,sρb)hrRsPoC;
gsπhsρ(1−b)
?= tC(1−b)(C1−bgsb)ChPoC,b
−gsπhsρC
?= tC(
n−1

i=0
Cbi
i )ChPoC,b;
gsπhsρb
?= tDbDChPoC,b
b
Proof of Membership (PoM). The proof of membership PoM proves for
every password character cφ(i) ∈pw that its integer value πφ(i) ∈ωφ(i) using the
shuﬄed commitments C′
φ(i), i.e.
ZKP{{πi, ri}i∈[0,n−1] : C′
φ(i) = gπihri ∧πφ(i) ∈ωφ(i)}.
This proof consists of the following steps:
1. To prove that every C′
φ(i) commits to a value in the according set ωφ(i) the
client computes the following values for the ﬁrst move of the proof:
−∀πj ∈ωφ(i) ∧πj ̸= πφ(i) :
sj ∈R Z∗
q, cj ∈R Z∗
q and tj = gπjhsj(C′
φ(i)/gπj)cj
−kρi ∈R Z∗
q;
tlφ(i) = gπihkρi
Values (tφ(i), sφ(i), cφ(i), kρi), with tφ(i) = {tj, tlφ(i)}, sφ(i) = {sj}, and cφ(i) =
{cj} are stored for future use. Note that tlφ(i) has to be added at the correct
position lφ(i) in tφ(i). A commitment CoPoM = gH(ω,C′,tφ(i))hrCoPoM with
rCoPoM ∈R Zq is computed as output with ω = {ωφ(i)}.

108
F. Kiefer and M. Manulis
2. The server stores received values, checks them for group membership, and
chooses a random challenge ChPoM = c ∈R Z∗
q.
3. After receiving the challenge c from the server, the client computes the fol-
lowing veriﬁcation values for all commitments C′
φ(i) (note that sj and cj for
all j ̸= lφ(i) are chosen already):
clφ(i) = c ⊕
|ωφ(i)|

j=1,j̸=lφ(i)
cj;
slφ(i) = kρφ(i) −clφ(i)(ri + r′
φ(i)),
where i is the index of C′
φ(i) before shuﬄing. The client then combines s =
{sφ(i) ∪{slφ(i)}} and c = {cφ(i) ∪{clφ(i)}}. Note again that the set union
has to consider the position of lφ(i) to add the values at the correct position.
A commitment RsPoM1 = gH(s,c)hrRsPoM with rRsPoM ∈R Zq is computed as
output.
4. Eventually the client sets the decommitment message with t = {tφ(i)}, ω =
{ωφ(i)}, rCoPoM = {rCoPoMi}, rRsPoM = {rRsPoMi}, and C′ = {C′
φ(i)} to
RsPoM2 = (ω, C′, t, s, c, rCoPoM, rRsPoM).
RsPoM1 and RsPoM2 form together RsPoM. To verify the proof, i.e. to verify that
every commitment C′
φ(i) in C′ commits to a character ci from either a subset of
Σ if signiﬁcant or Σ if not, the server veriﬁes the following for every set ωφ(i) ∈ω
with i ∈[1, n] and x = φ(i):
– Let cj ∈ci for ci ∈c and verify c
?= |ωi|
j=1 cj
– Let πj
∈
ωφ(i), si
∈
s, ti
∈
t, and ci
∈
c, and verify ti[j]
?=
gπjhsi[j](C′
i/gπj)ci[j] for all j ∈[1, |ωφ(i)|]
The server further veriﬁes commitments CoPoM
?= gH(ω,C′,t) hrCoPoM and
RsPoM
?= gH(s,c)hrRsPoM. The veriﬁcation of the proof is successful iﬀall equa-
tions above are true and ω contains all signiﬁcant characters for fb.
Proof of Shuﬄe (PoS). The proof of correct shuﬄing PoS is based on the
proofs from [13,14]. In the following we specify the proof to work with Pedersen
commitments instead of ElGamal ciphertexts. Note that indices for commitments
C and C′ run from 1 to n and index ranges in the following change frequently.
1. In the ﬁrst move, the client (prover) builds a permutation matrix and commits
to it. First he chooses random A′
j ∈R Z∗
q for j ∈[−4, n]. Let Aij denote a
matrix with i ∈[−4, n] and j ∈[0, n], i.e. of size (n + 5) × (n + 1), such that
a n × n sub-matrix of Aij is the permutation matrix (built from permutation
φ). Further, let φ−1 be the inverse shuﬄing function. This allows us to write
the shuﬄe as C′
i = n
j=0 CAji
j
= Cκihr′
κi with C0 = h and κi = φ−1(i)

Blind Password Registration for 2PAKE and Secret Sharing Protocols
109
for i ∈[1, n]. The matrix Aij is deﬁned with Aw0 ∈R Z∗
q, A−1v ∈R Z∗
q and
A0v = r′
φ(v) for w ∈[−4, n] and v ∈[1, n]. The remaining values in Aij are
computed as follows for v ∈[1, n]:
−A−2v =
n

j=1
3A2
j0Ajv;
A−3v =
n

j=1
3Aj0Ajv;
A−4v =
n

j=1
2Aj0Ajv
After generating Aij the client commits to it in (C′
0, ˜f, f ′, w, ˜w) for f ′ = {f ′
v}
with v ∈[0, n]:
−f ′
v =
n

j=−4
f Ajv
j
;
˜f =
n

j=−4
f
A′
j
j
;
˜w =
n

j=1
A2
j0 −A−40
−C′
0 = g
n
j=1 πjAj0hA00+n
j=1 rjAj0;
w =
n

j=1
A3
j0 −A−20 −A′
−3
(2)
Note
that
C′
0
=
n
j=0 CAj0
j
=
hA00 n
j=1 CAj0
j
,
but
Eq. 2
saves
n −1
exponentiations.
The
output
is
then
created
as
CoPoS
=
gH({Ci},{C′
φ(i)},C′
0, ˜
f,f ′,w, ˜
w)hrCoPoS with rCoPoS ∈R Zq.
2. When receiving CoPoS the server chooses c = {cv} with cv ∈R Z∗
q for v ∈[1, n]
and sets ChPoS = c.
3. After receiving challenges c from the server, the client computes the following
veriﬁcation values (s, s′) for s = {sv} and s′ = {s′
v} with v ∈[−4, n] and
c0 = 1:
sv =
n

j=0
Avjcj;
s′
v = A′
v +
n

j=1
Avjc2
j
The client sets RsPoS1 = gH(s,s′)hrRsPoS with rRsPoS ∈R Zq.
4. Eventually, the client sends the decommitment message to the server
RsPoS2 = (C′
0, ˜f, f ′, w, ˜w, s, s′, rCoPoS, rRsPoS).
Note that {Ci} and {C′
φ(i)} are omitted here as they are part of RsPoC2,
RsPoM2 respectively, already. If this proof is used stand-alone, those values
have to be added to RsPoS2.
RsPoS1 and RsPoS2 form together RsPoS. The server veriﬁes now that the
correctness of the commitments CoPoS
?= gH({Ci},{C′
φ(i)},C′
0, ˜
f,f ′,w, ˜
w)hrCoPoS and
RsPoS1
?= gH(s,s′)hrRsPoS, and that the following equations hold for a randomly
chosen α ∈R Z∗
q and C0 = h:
−
n

v=−4
f sv+αs′
v
v
?= f ′
0 ˜f α
n

j=1
f ′
j
cj+αc2
j;
n

v=0
Csv
v
?=
n

j=0
C′
j
cj;
−
n

j=1
(s3
j −c3
j)
?= s−2 + s′
−3 + w;
n

j=1
(s2
j −c2
j)
?= s−4 + ˜w

110
F. Kiefer and M. Manulis
The server accepts the proof if and only if all those veriﬁcations succeed. This
concludes the proof of correct shuﬄing.
Phase III – Share Veriﬁcation. To verify that the client used the same
password pw and shares s0, s1 with both servers S0 and S1, the servers compute
the commitment D′
b from the share commitment Cb and their share s1−b, and
exchange it. Comparing D′
b with the value Db received from the client, the server
veriﬁes share correctness. This concludes the 2BPR protocol and each server Sb
stores (C, S1−b, sb) if all checks were successful.
4.3
Security Analysis
We show that our 2BPR protocol is secure in the model from Sect. 3.1 and
thus oﬀers policy compliance and password blindness. For space limitations we
include only the proofs of Theorems 1 and 2 note that PoM and PoC protocols
are standard concurrent ZK proofs and PoS is a slightly modiﬁed concurrent
ZK proof from [13,14].
Lemma 1. The PoC protocol from Sect. 4.2 is a concurrent zero-knowledge
proof if the discrete logarithm problem in the used group G is hard and H :
{0, 1}∗→Zq is a collision resistant hash function.
Lemma 2. The PoM protocol from Sect. 4.2 is a concurrent zero-knowledge
proof if the discrete logarithm problem in the used group G is hard and H :
{0, 1}∗→Zq is a collision resistant hash function.
Lemma 3 ([13,14]). The PoS protocol from Sect. 4.2 is a concurrent zero-
knowledge proof of knowledge of shuﬄing φ if the discrete logarithm problem in
the used group G is hard and H : {0, 1}∗→Zq is a collision resistant hash
function.
Theorem 1. If G is a DL-hard group of prime-order q with generators g and
h, and H a collision resistant hash function, the construction in Fig. 1 provides
policy compliance according to Deﬁnition 2.
Proof. We show how to build a successful attacker on the soundness of PoC,
PoM and PoS using a successful attacker against policy compliance who has
access to Setup and SendC oracles.
Game0 : This game corresponds to the correct execution of the protocol.
Game1 : In this game we change how SendC(Ci, Sb,j, m) queries are answered.
If m is parsed as (CoPoM, CoPoC, CoPoS) the CoPoM is used by the challenger as
output to the PoM veriﬁer who returns challenge ChPoM which is then returned
in response to the SendC query (other challenges are generated at random). If m
is parsed as (RsPoM1, RsPoC1, RsPoS1) or (RsPoM2, RsPoC2, RsPoS2) and the ﬁrst
SendC query from that session was forwarded to the veriﬁer then RsPoM1, RsPoM2
respectively, is used as output to the PoM veriﬁer. It is easy to see that the

Blind Password Registration for 2PAKE and Secret Sharing Protocols
111
challenger breaks soundness of PoM if the adversary uses a password pw ̸∈Df
and PoM veriﬁes successfully. We can therefore assume for the remaining games
that pw ∈Df.
Game2 : In this game we introduce another change to the processing of SendC(Ci,
Sb,j, m) queries. If m is parsed as (CoPoM, CoPoC, CoPoS) then CoPoC is used by
the challenger as output to the PoC veriﬁer who returns challenge ChPoC that is
then used as response to the SendC query (other challenges are generated at ran-
dom). If m is parsed as (RsPoM1, RsPoC1, RsPoS1) or (RsPoM2, RsPoC2, RsPoS2)
and the ﬁrst SendC query from that session was forwarded to the veriﬁer then
RsPoC1, RsPoC2 respectively, is used as output to the PoC veriﬁer. It is easy to
see that the challenger breaks soundness of PoC if s0 +s1 ̸= π, i.e. the password
share sb can not be used with a second share s1−b to rebuild the password π
committed to in C, i.e. 
i biπi ̸= π. Observe further that the second share s1−b
has to be stored on server S1−b, i.e. the attacker has not performed the set-up
with Sb and S1−b with shares that do not combine to the same encoded password
π. Otherwise we can break the binding property of Pedersen commitments. In
particular, the attacker has to generate commitments C0, C1, D0 and D1 such
that C0gs1 = D0 or C1gs0 = D1. We can therefore for the remaining games that
the password share sb received by server Sb can be combined with the second
share s1−b of server S1−b to an encoded password π with according character
commitments Ci.
Game3 : In this game we change once more how SendC(Ci, Sb,j, m) queries are
answered. If m is parsed as (CoPoM, CoPoC, CoPoS) then CoPoS is used by the
challenger as output to the PoS veriﬁer who returns challenge ChPoS that is
then out in response to the SendC query (other challenges are generated at
random). If m from the adversary is parsed as (RsPoM1, RsPoC1, RsPoS1) or
(RsPoM2, RsPoC2, RsPoS2) and the ﬁrst SendC query from that session was for-
warded to the veriﬁer then RsPoS1, RsPoS2 respectively, is used as the output to
the PoS veriﬁer. In this case if the attacker is rewindable, the challenger can act
as a knowledge extractor for PoS. In particular, we can extract shuﬄing func-
tion φ and re-randomiser {r′
i} to break soundness of PoS. This implies that C′
is a correct shuﬄe of C. We conclude the proof by observing that the password
shares stored on both servers can be combined to a policy compliant password.
Theorem 2. If G is a DL-hard group of prime-order q with generators g and
h, and H a collision resistant hash function, the construction in Fig. 1 provides
password blindness according to Deﬁnition 3.
Proof. We prove this theorem through a sequence of games. In the last game
simulated interactions between servers and clients are simulated and password
independent, thus requiring the attacker to perform a random guess of the bit b.
Game0 : This is the correct execution of the protocol.
Game1 : The challenger computes crs with the knowledge of the trapdoor
τ = logg(h).
Game2 : The challenger simulates the proofs PoC, PoM and PoS and messages
exchanged between the servers as part of the Execute oracle but stores two correct

112
F. Kiefer and M. Manulis
shares on the servers to allow consistency if servers become corrupted. Since at
least one server must remain uncorrupted the probability diﬀerence between
both games is negligible due to the zero-knowledge property of the proofs.
Game3 : This game modiﬁes SendS and SendSS responses if the second partici-
pating server is uncorrupted by simulating zero-knowledge proofs and answering
SendSS queries using D′
b = Db. To guarantee consistency in case of corruptions
the challenger still stores appropriate shares. The probability diﬀerence between
both games is negligible due to the zero-knowledge property of the proofs. Since
all proofs are password-independent and Pedersen commitments oﬀer uncondi-
tional hiding the attacker can only win by guessing b.
5
Performance and Use with 2PAKE/2PASS Protocols
An unoptimised prototype of the 2BPR protocol from Sect. 4 was implemented
over the NIST P-192 elliptic curve [27] in Python using the Charm framework
[1] to estimate the performance. The tests (completed on a laptop with an Intel
Core Duo P8600 at 2.40 GHz for both client and server) underline the claim
that the protocol is practical. For instance, for a password of length 10 and
policies (dl, 5) and (ds, 7) computations take 1.4 s on the client and 0.68 s on
each server. The overall computing time for a password of length 10 was 2.76 s
and increased to 6.34 s for a password of length 20. Also note that the execution
can be parallelised if the client performs the proofs with S0 and S1 at the same
time. The source code is available from https://goo.gl/XfIZtn.
Application to Existing 2PAKE/2PASS Protocols. Our 2BPR protocol
can be used to register passwords for 2PAKE and 2PASS protocols that adopt
additive password sharing in Zq or multiplicative sharing in G. This includes
2PAKE protocols from [20,21] for which no password registration procedures
were addressed. Integration of 2BPR into 2PASS protocols is more involved since
password registration is considered to be part of the 2PASS protocol during the
secret sharing phase. 2PASS protocols in general can be divided in two stages:
password and secret registration/sharing and secret reconstruction. While the
approach from [2] and subsequent works [5,17,30] do not actually share the pass-
word and could therefore use other means to verify policy compliance of a pass-
words used, the UC-secure 2PASS protocol from [6] uses multiplicative password
sharing in G. In order to use our 2BPR protocol withing the setup procedure of
[6] we can redeﬁne the encoded password to gπ with π ←PWDtoINT(pw) such
that shares are computed as gπ = gs0gs1. The ﬁrst message (step 1) from the
setup protocol in [6] can piggyback the ﬁrst 2BPR protocol message. The subse-
quent three messages between the client and each server are performed between
step 1 and step 2, while the inter-server communication can be piggybacked on
step 2 and step 3. In addition to checking correctness of shares in the setup of
[6] the servers can now verify the 2BPR proofs to check policy compliance. This
would adds three ﬂows to the setup protocol of [6].

Blind Password Registration for 2PAKE and Secret Sharing Protocols
113
6
Conclusion
In this work we introduced the notion of two-server blind password registration
(2BPR), which is a solution for secure registration of policy-compliant, user-
selected passwords for 2PAKE/2PASS protocols where each server is supposed
to learn only its own share of the password and whether the combined password
is conform with his password policy. Our eﬃcient 2BPR protocol can be used
to register 2PAKE/2PASS passwords satisfying server-chosen policies over the
alphabet of all 94 printable ASCII characters.
References
1. Akinyele, J.A., Garman, C., Miers, I., Pagano, M.W., Rushanan, M., Green, M.,
Rubin, A.D.: Charm: a framework for rapidly prototyping cryptosystems. J. Crypt.
Eng. 3(2), 111–128 (2013)
2. Bagherzandi, A., Jarecki, S., Saxena, N., Lu, Y.: Password-protected secret sharing.
In: CCS 2011, pp. 433–444. ACM (2011)
3. Bonneau, J.: The science of guessing: analyzing an anonymized corpus of 70 million
passwords. In: IEEE S&P, pp. 538–552. IEEE Computer Society (2012)
4. Brainard, J.G., Juels, A., Kaliski, B., Szydlo, M.: A new two-server approach
for authentication with short secrets. In: USENIX Security Symposium, USENIX
Association (2003)
5. Camenisch, J., Lehmann, A., Lysyanskaya, A., Neven, G.: Memento: how to recon-
struct your secrets from a single password in a hostile environment. In: Garay, J.A.,
Gennaro, R. (eds.) CRYPTO 2014, Part II. LNCS, vol. 8617, pp. 256–275. Springer,
Heidelberg (2014)
6. Camenisch, J., Lysyanskaya, A., Neven,G.: Practical yet universally composable
two-server password-authenticated secret sharing. In: CCS 2012, pp. 525–536. ACM
(2012)
7. Damg˚ard, I.B.: Eﬃcient concurrent zero-knowledge in the auxiliary string model.
In: Preneel, B. (ed.) EUROCRYPT 2000. LNCS, vol. 1807, pp. 418–430. Springer,
Heidelberg (2000)
8. Goodin, D., Hack of cupid media dating website exposes 42 million plaintext pass-
words. http://goo.gl/ImLE1C. Accessed 01 Apr 2015
9. Dell’Amico, M., Michiardi, P., Roudier, Y.: Password strength: an empirical analy-
sis. In: INFOCOM, pp. 983–991. IEEE (2010)
10. Dierks, T., Rescorla, E.: The Transport Layer Security (TLS) protocol version 1.2.
RFC 5246 (proposed standard), updated by RFCs 5746, 5878, 6176, 7465, August
2008
11. D¨urmuth, M., Kranz, T.: On password guessing with GPUs and FPGAs. In: PASS-
WORDS 2014, pp. 19–38 (2014)
12. Ford, W., Kaliski, Jr. B.S.: Server-assisted generation of a strong secret from a
password. In: WETICE, pp. 176–180. IEEE (2000)
13. Furukawa, J.: Eﬃcient and veriﬁable shuﬄing and shuﬄe-decryption. IEICE Trans.
88–A(1), 172–188 (2005)
14. Furukawa, J., Sako, K.: An eﬃcient scheme for proving a shuﬄe. In: Kilian, J. (ed.)
CRYPTO 2001. LNCS, vol. 2139, pp. 368–387. Springer, Heidelberg (2001)
15. hashcat: hashcat - advanced password recovery. http://hashcat.net/. Accessed 01
Apr 2015

114
F. Kiefer and M. Manulis
16. Jager, T., Kohlar, F., Sch¨age, S., Schwenk, J.: On the security of TLS-DHE in the
standard model. In: Safavi-Naini, R., Canetti, R. (eds.) CRYPTO 2012. LNCS,
vol. 7417, pp. 273–293. Springer, Heidelberg (2012)
17. Jarecki, S., Kiayias, A., Krawczyk, H.: Round-optimal password-protected secret
sharing and T-PAKE in the password-only model. In: Sarkar, P., Iwata, T. (eds.)
ASIACRYPT 2014, Part II. LNCS, vol. 8874, pp. 233–253. Springer, Heidelberg
(2014)
18. Jarecki, S., Lysyanskaya, A.: Adaptively secure threshold cryptography: intro-
ducing concurrency, removing erasures (extended abstract). In: Preneel, B. (ed.)
EUROCRYPT 2000. LNCS, vol. 1807, p. 221. Springer, Heidelberg (2000)
19. Jin, H., Wong, D.S., Xu, Y.: An eﬃcient password-only two-server authenticated
key exchange system. In: Qing, S., Imai, H., Wang, G. (eds.) ICICS 2007. LNCS,
vol. 4861, pp. 44–56. Springer, Heidelberg (2007)
20. Katz, J., MacKenzie, P.D., Taban, G., Gligor, V.D.: Two-server password-only
authenticated key exchange. In: Ioannidis, J., Keromytis, A.D., Yung, M. (eds.)
ACNS 2005. LNCS, vol. 3531, pp. 1–16. Springer, Heidelberg (2005)
21. Kiefer, F., Manulis, M.: Distributed smooth projective hashing and its applica-
tion to two-server password authenticated key exchange. In: Boureanu, I., Owe-
sarski, P., Vaudenay, S. (eds.) ACNS 2014. LNCS, vol. 8479, pp. 199–216. Springer,
Heidelberg (2014)
22. Kiefer, F., Manulis, M.: Zero-knowledge password policy checks and veriﬁer-based
PAKE. In: Kutylowski, M., Vaidya, J. (eds.) ICAIS 2014, Part II. LNCS, vol. 8713,
pp. 295–312. Springer, Heidelberg (2014)
23. Krawczyk, H., Paterson, K.G., Wee, H.: On the security of the TLS protocol: a
systematic analysis. In: Canetti, R., Garay, J.A. (eds.) CRYPTO 2013, Part I.
LNCS, vol. 8042, pp. 429–448. Springer, Heidelberg (2013)
24. Ma, J., Yang, W., Luo, M., Li, N.: A study of probabilistic password models. In:
IEEE S&P, pp. 689–704 (2014)
25. MacKenzie,
P.D.,
Shrimpton,
T.,
Jakobsson,
M.:
Threshold
password-
authenticated key exchange. In: Yung, M. (ed.) CRYPTO 2002. LNCS, vol. 2442,
pp. 385–400. Springer, Heidelberg (2002)
26. Cubrilovic, N., Hack, R.: From bad to worse (2014). http://goo.gl/AF5ZDM.
Accessed 01 Apr 2015
27. NIST: National Institute of Standards and Technology. Recommended elliptic
curves for federal government use (1999). http://goo.gl/M1q10h
28. Openwall: John the Ripper password cracker. http://www.openwall.com/john/.
Accessed 01 Apr 2015
29. Pedersen, T.P.: Non-interactive and information-theoretic secure veriﬁable secret
sharing. In: Feigenbaum, J. (ed.) CRYPTO 1991. LNCS, vol. 576, pp. 129–140.
Springer, Heidelberg (1992)
30. Pryvalov, I., Kate, A.: Introducing fault tolerance into threshold password-
authenticated key exchange. Cryptology ePrint Archive, report 2014/247 (2014)
31. Reuters: Trove of Adobe user data found on web after breach: security ﬁrm (2014).
http://goo.gl/IC4lu8. Accessed 01 Apr 2015
32. Szydlo, M., Kaliski, B.: Proofs for two-server password authentication. In: Menezes,
A. (ed.) CT-RSA 2005. LNCS, vol. 3376, pp. 227–244. Springer, Heidelberg (2005)
33. Reuters, T.: Microsoft India store down after hackers take user data. http://goo.
gl/T7puD1. Accessed 01 Apr 2015
34. Yang, Y., Deng, R.H., Bao, F.: A practical password-based two-server authen-
tication and key exchange system. IEEE Trans. Dependable Sec. Comput. 3(2),
105–114 (2006)

Chip Authentication for E-Passports:
PACE with Chip Authentication Mapping v2
Lucjan Hanzlik(B) and Miroslaw Kutylowski
Wroclaw University of Technology,
Wybrze˙ze Wyspia´nskiego 27, 50-370 Wroclaw, Poland
{lucjan.hanzlik,miroslaw.kutylowski}@pwr.edu.pl
Abstract. According to the European Commission Decision C(2006)
2909, EU Member States must implement Supplemental Access Control
(SAC) on biometric passports. The SAC standard describes two versions
of a password based authenticated key exchange protocol called PACE-
GM and PACE-IM. Moreover, it deﬁnes an extension called PACE-CAM.
Apart from password authentication and establishing a session key, the
PACE-CAM protocol executes an active authentication of the ePassport
with just one extra modular multiplication. However, it uses PACE-GM
as a building block and does not work with the more eﬃcient proto-
col PACE-IM. In this paper we propose an active authentication exten-
sion, which can be used with both PACE-GM and PACE-IM. Moreover,
the protocol’s overhead on the side of the ePassport, remains the same
despite more universality.
Keywords: ePassport · Supplemental Access Control · PACE · Active
Authentication · Chip Authentication Mapping · ICAO
1
Introduction
Electronic passport (ePassport) is a combination of a traditional paper passport
and an electronic layer. This layer includes a microprocessor (also called chip or
tag) and an RFID antenna used for power supply and communication. A chip is
not merely a data storage, but also a secure suite for performing cryptographic
algorithms.
Since 2004, standards regarding machine-readable travel documents (MRTD)
are issued by the International Civil Aviation Organization (ICAO). These guide-
lines deﬁne cryptographic algorithms used during the inspection procedure as
well as the format of the document. In particular, it deﬁnes a special machine-
readable zone (MRZ), which can be read by an optical reader and is usually
placed on the bottom of the ﬁrst page of the ePassport. The ICAO standard has
been implemented by most countries and is the de-facto standard for ePassports.
In 2008 the German Federal Oﬃce for Information Security proposed changes
to the ePassport standard [BSI15]. The new standard deﬁnes a password-based
authentication key exchange protocol called Password Authenticated Connec-
tion Establishment (PACE). Unlike the still used Basic Access Control protocol
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 115–129, 2016.
DOI: 10.1007/978-3-319-45871-7 8

116
L. Hanzlik and M. Kutylowski
(BAC) it is secure against oﬄine dictionary attacks. The new standard was suc-
cessfully implemented on new German personal identity documents (nPA - neuer
Personalausweis). On the downside, due to the interoperability requirement, all
ePassports must implement the old ICAO standard. Fortunately, in 2011 ICAO
proposed a new standard called Supplemental Access Control (SAC) [ISO11],
which proposes PACE as a recommended replacement for BAC. The standard
deﬁnes two protocol versions: PACE with Generic Mapping (PACE-GM) and
PACE with Integrated Mapping (PACE-IM). It also deﬁnes additional means to
authenticate the chip. Active Authentication and Chip Authentication (from the
standard [BSI15]) are recommended. However, those protocol require at least one
exponentiation. In 2014 ICAO updated the SAC standard [ISO14]. This update
introduced PACE with Chip Authentication Mapping (PACE-CAM). This pro-
tocol veriﬁes the authenticity of the ePassport chip at a cost of one extra modular
multiplication and is integrated with PACE-GM. Note that this is more eﬃcient
than using Active Authentication or Chip Authentication as a separate proto-
col. However, this version is limited to PACE-GM and cannot be used with the
more eﬃcient PACE-IM. Moreover, BSI patented PACE-CAM [BK12]. In this
paper we will focus on the PACE protocol and in particular on the PACE-CAM
version.
Related Work.
Password-authenticated key exchange protocols were ﬁrst
introduced by Bellovin and Merritt [BM92]. Their solution, called Encrypted
Key Exchange (EKE), uses a combination of symmetric and public-key cryptog-
raphy and provides resistance against oﬄine-dictionary attacks. In 1996, David
Jablon proposed SPEKE (Simple Password Encrypted Key Exchange) [Jab96]
as an extension of EKE. Apart from security against dictionary attacks, the
SPEKE protocol achieves perfect forward secrecy, i.e. disclosure of a password
does not compromise the remaining session keys.
The PACE protocol introduced by BSI is somewhat similar to the SPEKE
protocol. Both parties use a shared password and a mapping function to derive
a secret group generator. This generator is then used in a DH key exchange
protocol with some additional steps based on message authentication codes.
PACE-GM was proven to be a secure authenticated key exchange protocol
under a new assumption called Password-Based Chosen-Element DH [BFK09].
The authors also proposed a general version of this problem and used it to prove
security of PACE with an arbitrary mapping function. Coron et al. proved secu-
rity of the PACE-IM protocol [CGIP11] using a newly introduced cryptographic
assumption called Gap Chosen-Base Diﬀe-Hellman. This problem implies that
the protocol is secure even for groups with a DDH oracle (in real life it can be
instantiated with groups with admissible bilinear maps).
The ﬁrst extension of the PACE protocol that additionally authenticates the
chip was proposed by Bender et al. in [BDFK12] and called PACE|AA protocol.
Their solution was based on a combination of the PACE-GM protocol with
ECDSA signature scheme. The chip reuses some random coins from the PACE-
GM protocol to produce the signature, minimizing the cost of its generation.

Chip Authentication for E-Passports
117
The authors showed that PACE|AA inherits the AKE security from PACE-GM
and that it is secure against impersonation attacks.
Independently, Bender et al. [BFK13] and Hanzlik et al. [HKK13] published
an extension that instead of using signatures, uses the random coins from PACE-
GM to bind them to the public key. In other words, the user gives a “proof”
that, if it knows the random coins used (which is implicitly veriﬁed by PACE-
GM), it also knows the secret key. The authors of [BFK13] propose two roughly
equivalent versions: with a multiplicative binding (same as the one proposed
by [HKK13]) and an additive binding. The multiplicative version was later on
added to the ICAO SAC standard and renamed as Chip Authentication Map-
ping version (PACE-CAM). The security against impersonation of PACE-CAM,
described in [BFK13] is based on non-standard assumptions. On the other hand,
the authors of [HKK13] propose only security arguments without a reduction
based proof and security model. However, they discuss privacy of the solution
and show in particular that a reader cannot convince a third party that it has
interacted with an ePassport. Moreover, they propose a leakage-resilient ver-
sion of the protocol, i.e. the secret key is secure even in the case of leakage of
all ephemeral value (this is not the case for the standard version and the one
proposed in [BDFK12]).
Motivation. Today, the scale of usage of travel documents is rapidly growing.
At the same time, the share of ID documents equipped with electronic layer is
growing as well. Speed-up and accuracy of document inspection comes with a
price: we have to ensure that the personal and biometric data are secure and
that their privacy is well protected. For this reason, there have been signiﬁcant
eﬀorts of the ICAO organization to improve security and interoperability of travel
documents. This work has a big impact, since most state authorities issuing
ePassports are using the ICAO standard as a guidance.
Unfortunately, as shown by many authors, this work is not always perfect.
Formal security proofs of the introduced cryptographic protocols are frequently
published after appearance of the standard and after deployment in the ePass-
ports. Since a passport is usually issued with validity period of 10 years, lack of
a formal security proof or its incompleteness is a major risk. Indeed, in case of
a security ﬂaw, there is no way to install a security patch in the ePassports and
there is no way to exchange in a reasonable time the ﬂawed passports, unless
their number is small. In this situation any mistake or unnoticed weakness might
have profound consequences in the future.
Last not least, due to the application scale, there is also a strong motivation
for deep eﬃciency optimization of the protocols. Indeed, the saving for the pro-
duction cost of a single chip on ePassport might be a few cents, but we have to
take into account billions of travel documents issued. Similar arguments apply
for the time necessary to inspect a travel document in an automatic border con-
trol booth – ideally the travellers should simply go through these booths and
the inspection should take a small fraction of a second.

118
L. Hanzlik and M. Kutylowski
Paper Contribution. In this paper we consider the Password Authenticated
Connection Establishment (PACE) protocol. In particular, we focus on the Chip
Authentication Mapping introduced in the latest revision of the Supplemental
Access Control document [ISO14].
Our contribution is protocol called PACE-CAM v2. Thanks to bilinear maps
we can apply the same trick as in PACE-CAM, but independent of the map-
ping function used in PACE. Moreover, the eﬃciency on side of the ePassport
remains the same. Note that the standard PACE-CAM is limited to PACE with
the generic mapping function (based on Diﬃe-Hellman key exchange protocol).
Moreover, our results indicate that currently PACE-CAM v2 has better security
guarantees than PACE-CAM. In particular, we prove that under the 2-Strong
Diﬃe-Hellman assumption PACE-CAM v2 (with Integrated Mapping or Generic
Mapping) is secure against impersonation.
2
Preliminaries
In this chapter we recall basic deﬁnitions, number-theoretic assumptions and
cryptographic primitives necessary to formally deﬁne and analyze the crypto-
graphic protocol presented in this paper.
2.1
Bilinear Maps
Let us consider cyclic groups (G1, +), (G2, +), (GT , ·) of a prime order q. Let
P, Q be generators of respectively G1 and G2. We say that (G1, G2, GT ) are
bilinear map groups if there exists a bilinear map e : G1 × G2 →GT satisfying
the following properties:
Bilinearity: for all (S, T) ∈G1 × G2 and a, b ∈Zq, we have
e(aS, bT) = e(S, T)a·b,
Non-degeneracy: e(P, Q) ̸= 1 is a generator of group GT ,
Computability: e(S, T) is eﬃciently computable for all (S, T) ∈G1 × G2.
Moreover, we say that the map e is an admissible bilinear map or admissible
pairing function. Depending on the choice of groups we say that map e is of:
Type 1: if G1 = G2,
Type 2: if G1 and G2 are distinct groups and there exists an eﬃciently com-
putable isomorphism ψ : G2 →G1,
Type 3: if G1 and G2 are distinct groups and no eﬃciently computable isomor-
phism ψ : G2 →G1 is known.
Bilinear map groups are known to be instantiable with ordinary elliptic
curves introduced by Barreto and Naehrig [BN05]. Thus, Hereafter, by the group
description G we also mean all the parameters that are required to perform
computations in G. Moreover, in the course of this paper we will only use the
multiplicative notation. This means that, while using elliptic curves, we denote
scalar multiplication as exponentiation and point addition as multiplication.

Chip Authentication for E-Passports
119
2.2
Assumptions and Cryptographic Primitives
First, we recall the Strong Diﬃe-Hellman assumption formulated by Boneh and
Boyen in [BB08]:
Deﬁnition 1 (ℓ-Strong Diﬃe-Hellman (ℓ−SDH)). Given (ℓ+ 3) elements
(g1, gx
1, gx2
1 . . . , gxℓ
1 , g2, gx
2) ∈Gℓ+1
1
× ∈G2
2, output (c, g1/(x+c)
1
) ∈Zq × G1. We
say that an algorithm A has advantage ϵ in solving the ℓ−SDH1 in G1, G2 of
prime order q if:
Pr[(c, g1/(x+c)
1
) ←A(g1, gx
1, gx2
1 . . . , gxℓ
1 , g2, gx
2)] ≥ϵ,
where the probability is taken over the random choice of the generators g1 ∈
G1, g2 ∈G2, the random choice of x ∈Zq, and the random bits of A. By
Advℓ−SDH(t) we denote the maximal advantage for any adversary running in
time t in solving the ℓ-Strong Diﬃe-Hellman problem.
Deﬁnition 2 (Certiﬁcation Scheme). A certiﬁcation scheme consists of
three PPT algorithms (KeyGencert, Certify, CVer). KeyGencert takes as input secu-
rity parameter 1λ and outputs a key pair (skCA, pkCA) of the CA. Algorithm
Certify takes as input secret key skCA, user’s public key pkU and other informa-
tion infoU and outputs a certiﬁcate certU. Deterministic algorithm CVer is used
to verify certiﬁcate certU. We denote by Advforge
CA (t, qCA) a bound on the value
ϵ for which no attacker in time t can forge a certiﬁcate (while making at most
qCA certifying queries).
Deﬁnition 3 (Message Authentication Codes). A message authentication
codes consists of three algorithms (KeyGenMac, Mac, MVer) deﬁned as follows:
KeyGenMac(1λ) : on input of the security parameter 1λ, the probabilistic algorithm
KeyGenMac outputs a secret key sk.
Mac(sk, m) : on input of the secret key sk and message m, algorithm Mac outputs
a tag T.
MVer(sk, m, T) : on input of the secret key sk, message m and tag T, the deter-
ministic algorithm MVer outputs either 1 (when T is valid) or 0 (when T is
invalid).
We say that a message authentication code is (t, qm, qv, ϵ)-unforgeable against
adaptively-chosen-message attacks (UNF-CMA) if no algorithm A, running in
time t, making at most qm queries to a tagging oracle and qv queries to a verifying
oracle, has advantage (denoted by Advforge
Mac (t, qm, qv)) at most ϵ in outputting
a valid tag for a not queried message.
Deﬁnition 4 (Symmetric
Encryption
Scheme). A symmetric encryp-
tion scheme consists of three polynomial time algorithms (KeyGenEnc, Enc, Dec)
deﬁned as follows:
KeyGenEnc(1λ) : on input of the security parameter 1λ, the probabilistic algorithm
KeyGenEnc outputs a secret key sk.

120
L. Hanzlik and M. Kutylowski
Enc(sk, m) : on input of the secret key sk and a message m, algorithm Enc
outputs a ciphertext c of m.
Dec(sk, c) : on input of the secret key sk and ciphertext c of m the deterministic
algorithm Dec outputs the plaintext m.
We say the advantage Advind−cpa
E
(A) of the adversary A in attacking this
encryption scheme is the probability that he wins the IND-CPA game. We
assume that encryption scheme can be used to encrypt blocks of messages from
{0, 1}ℓ.
Deﬁnition 5 (Key Derivation Function). Key derivation functions can be
used to derive keys in a way that is indistinguishable from probing with the
uniform distribution in the key space. In practice, such functions are imple-
mented using hash functions or message authentication codes. The ICAO stan-
dard [ISO14] deﬁnes the following key derivation function: KDF(K, c) = H(K||c),
where K is the shared secret, c is a counter and H is a hash function. We
will use the notations KDFEnc(K) = KDF(K, 1), KDFMac(K) = KDF(K, 2) and
KDFπ(π) = KDF(f(π), 3), where f is some encoding of password π deﬁned in
[ISO14].
2.3
Security Model
As already mentioned, PACE is a password-based authenticated key exchange
protocol. We follow the authors of [BDFK12,BFK13], who provide a security
model built on top of the BPR model [BPR00], which at the same time can be
used to capture session key conﬁdentiality and impersonation resistance.
To prove security in this model, we have to show that there exists no adver-
sary A that wins the real-or-random game with a non-negligible probability. The
game is played in a system with a set of users U ∈U. The set U is divided into
the set of clients C and the set of servers S. Each user C ∈C is given a key pair
(skC, pkC). Moreover, each pair of a client and a server shares a secret password
π1. We assume that this password is randomly chosen from a dictionary with
N elements. The adversary is given all public keys of honest users C ∈C. Note
that the adversary can register new users which we call adversarially controlled.
During the game the adversary may create several instances of a user. The
i-th instance of a user U is denoted by U i. After successful termination, the
instance U i outputs a session key K, a session ID sid, and a user ID pid identify-
ing the intended partner. The session identiﬁer sid contains the entire transcript
of the communication.
The goal of the adversary A is to distinguish between real session keys,
derived by honest parties, from random keys. This is modeled using a test oracle,
which veriﬁes the capability of the adversary to distinguish the session keys
from the random keys. To achieve his goals, the adversary controls the entire
communication in the system. Formally, this is modeled by the following oracles:
1 For the protocols concerned, in fact we may assume there is one server with many
instances.

Chip Authentication for E-Passports
121
Execute(Ci, Sj) - this query allows to model passive adversaries. The output
consists of all messages exchanged during a protocol execution between client
Ci and server Sj.
Send(U i, m) - this query allows to model an active adversary. The output of this
query is the message that U i would generate upon receipt of the message m.
The adversary is also given the power to gain control over honest users and
reveal session keys. This is modeled by the following oracles:
Reveal(U, i) - reveals the session key computed by U i in an accepting state.
Corrupt.pw(U) - returns the secret password π of the user U.
Corrupt.key(U) - returns the secret key sk∗
C of user U ∈C.
Register(C, pk) - allows to register a public key pk in the name of a new, adver-
sarially controlled, client (identity) C ∈C,
Test(U, i) - at the beginning of the real-or-random game, this oracle is initialized
with a bit b. Assume that at some point of the game, the adversary makes
a test query about (U, i). In addition, let U i terminate in accepting state,
holding a session key K (otherwise this oracle returns ⊥). Then this oracle
returns K if b = 0 or a random key K′ if b = 1. Without loss of generality
we assume that the adversary never queries twice for the same user instance.
Two instances Ai and Bj are called partnered if they both have terminated
in an accepting state with the same output. An instance Ai is called fresh if: (a)
there has been no Reveal(A, i) query, (b) there has been no Reveal(B, j) query,
for the partner Bj of Ai, (c) Ai and the partner of Ai are not adversarially
controlled (i.e. there were no Corrupt.pw queries for A or B). Otherwise, it is
called unfresh. Informally, an instance is called fresh if the session key (computed
by this instance) has not been leaked and both Ai and its partner Bj are not
controlled by the adversary.
AKE Security. Finally, the adversary outputs a bit b′. We say that the adver-
sary wins the real-or-random game if b = b′ (b is the bit chosen internally by the
Test oracle) and instance (U, i) queried to the Test oracle is fresh. We measure
the resources Q of the adversary by the maximum number initiated executions
qe, hash oracle queries qh and cipher oracle queries qc.
The advantage of an AKE adversary A (i.e. advantage in winning the real-
or-random game by A) for a protocol Π is deﬁned by:
Advake
Π (A) = 2 · Pr[A wins the ROR Game] −1
Advake
Π (t, Q) = max{Advake
Π (A) | A is (t, Q) −bounded}.
Impersonation Resistance (IKE Security). Informally, an adversary suc-
cessfully impersonates, if he succeeds to impersonate an honest client, make the
server accept a fake certiﬁcate (without knowing the corresponding secret key)
or perform any kind of man-in-the-middle attacks.

122
L. Hanzlik and M. Kutylowski
Formally, the adversary impersonates if an honest reader accepts with partner
id pid and session id sid such that: (a) the intended partner C in pid is not
adversarially controlled or the public key pkC has not been registered, (b) there
have been no Corrupt.key query for C, before the reader accepted, (c) the session
id sid has not appeared in any other accepting session.
The advantage of IKE adversaries against the protocol Π is deﬁned as follows:
Advike
Π (A) = Pr[A successfully impersonates]
Advike
Π (t, Q) = max{Advike
Π (A) | A is (t, Q) −bounded}.
3
Generic Version of the Chip Authentication Mapping
The major disadvantage of PACE-CAM is that it heavily relies on the com-
putation performed during the PACE-GM instantiation of the function Map.
This means that we cannot use the more eﬃcient instantiation based on hash
functions used in PACE-IM. Moreover, the same trick cannot be simply applied
to the value y′
C as the public key pkC and the value Y ′
C are computed using
diﬀerent group generators (g and ˆg, respectively). However, it can be done using
bilinear maps as we will now show.
Protocol Description. In the table below we present the consequtive steps
and communicates exchanged between the ePassport and the Reader during
the PACE-CAM v2 protocol. Both parties share a password π and the pairing
friendly elliptic curve parameters (G1, G2, GT , e). It is worth noting that the
ePassport must only store the deﬁnition of group G1. Moreover, in case of inte-
grated mapping both parties share a hash function HEC that maps arbitrary
strings to elements of group G1. The ePassport also receives a private/public
key pair which is certiﬁed by the Document Issuer. Similar to Active Authen-
tication and Chip Authentication, the public key is stored in data groups of
the ePassport and signed by the Document Issuer. We follow the approach of
[BDFK12,BFK13,HKK13] and use a certiﬁcate on the public key to model this
in an easy-to-follow way. We will denote the public key of the Document Issuer
as pkCA.
Our protocol makes minimal changes in the PACE protocol – which itself
is one of important design objectives from the practical point of view (e.g. it
allows to reuse a major part of the work done for formal certiﬁcation of PACE
products).

Chip Authentication for E-Passports
123
Remark 1. It is worth noting that the ePassport only uses computations in group
G1, which is a ordinary elliptic curve if pairing friendly BN-curves are used
[BN05]. Thus, the eﬃciency on side of the ePassport is similar to the original
scheme PACE-CAM.

124
L. Hanzlik and M. Kutylowski
Security Analysis. Due to space reasons, we just argue that the AKE security
relies on the security of the PACE protocol and the same reasoning as in [BFK13]
can be applied. Note that PACE-CAM v2 extends PACE only by one encryption
c. We now focus on the main issue, namely IKE security of PACE-CAM v2. We
present a combined proof for both cases, i.e. when the function Map is instanti-
ated as in Integrated Mapping (MapIM) or as in Generic Mapping (MapGM)
and use markings to identify the instantiation for which the part of the proof
applies.
Theorem 1. In the random oracle model, the PACE-CAM v2 (with MapIM or
MapGM) satisﬁes:
Advike
PACE−CAMv2(t, Q) ≤q2
e · qh · Adv2−SDH(t∗) + qh
2λ + q2
e
q
+ 2qe · Advforge
Mac (t∗, 2qe, 2qe) + Advforge
CA
(t∗, qe)
where t∗∼t and Q = (qe, qh, qc).
Proof. First we use the game based techniques [Sho04] to evade simple attacks.
The idea is that at the end the adversary cannot perform any replay attacks, the
adversary can only impersonate honest ePassports and any malicious reader or
ePassport must query the random oracle for the key K′
Mac in order to compute
a valid message authentication code. We sketch a proof that these constraints
only lower the adversary’s advantage by a negligible fraction. Finally, for the
last game we will show a reduction to the 2 −SDH problem.
Game 0: The original impersonation game as deﬁned in Subsect. 2.3.
Game 1: Similar to the above but we abort the game in case any honest reader
accepts an unregistered public key.
Note that the public key of the ePassport is contained in the certiﬁcate,
which are issued by the Document Issuer. Thus, we lower the adversary’s
advantage by the advantage of a successful forgery of a certiﬁcate.
Game 2: Abort if an honest ePassport computes the same key K that any
honest ePassport has computed before.
The key K depends on the values Y ′
C and Y ′
R. Note that the honest ePassport
sends the random element Y ′
C after it receives Y ′
R from an honest or malicious
reader. Hence, the key K is a uniformly random key and the probability
that it matches any of the previous i keys, is at most i/q. It follows that
the adversary’s advantage can drop by a fraction of 1
2q2
e/q, where qe is the
maximal number of executions.
Game 3: Abort if there are collisions among the values Y ′
R computed by the
honest readers.
By the birthday bound this again lowers the adversary’s advantage by a
fraction of 1
2q2
e/q.

Chip Authentication for E-Passports
125
Game 4: Abort if a malicious reader submits a valid TR to the honest ePassport,
and such that the adversary has not made a hash query to the key K derived
by the honest ePassport in the execution before and TR was not computed
by an honest reader.
We will sketch a proof that an adversary submitting such TR can be used
to break the UNF-CMA security of the message authentication code. Note
that by Game 2 honest ePassports use unique Y ′
C and the adversary has
to compute a tag for a new message (Y ′
C, G). Thus, we can simulate the
whole protocol and with probability 1/qe choose the execution in which the
adversary submits this TR. In this particular execution we use the UNF-
CMA oracles to compute TC. Thus, we conclude that the adversary’s loss
can be at most qe times the advantage of forging a Mac.
Game 5: Abort if a malicious ePassport submits a valid TC to the honest reader,
and such that the adversary has not made a hash query to the key K derived
by the honest reader in the execution before and TC was not computed by
an honest ePassport.
Similar reasoning as above can be applied. Thus, we conclude that the adver-
sary’s loss can be again at most qe times the advantage of forging a Mac.
Let us assume that there exists an algorithm A = Aike
PACE−CAMv2 running in
time t and making at most Q = (qe, qh, qc) queries has advantage ϵ in breaking
the IKE security in Game 5. We show how to use A to create reduction R,
running in time t∗, that has advantage ϵ in solving the 2 −SDH problem on
input (h1, hx
1, hx2
1 , h2, hx
2). First, R prepares the following impersonation game:
– it sets the system parameters g1 = hx
1 and g2 = hx
2 using the problem’s
instance,
– it takes the public parameters of CA,
– for a given number of ePassports n, R creates and certiﬁes their key pairs,
– for each pair of users, it chooses passwords from a, possibly small, set N,
– R chooses the ePassport to be impersonated and replaces his public key with
(h2)r where r ←R Zq. Note that this implies that r · x−1 is the private key
of the impersonated user. We will use (sk∗
C, pk∗
C) to denote this particular
private/public keypair.
Random Oracle queries. The hash function used for key derivation is similar
to the one from the previous proof.
MapIM Speciﬁc Steps:
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
The hash function HEC used by Map is programmed as follows. For the i-th
unique oracle query of A, the reduction chooses a random number ri ←R Z∗
q and
it outputs gri
1 . For repeated queries, the reduction uses the query table based
approach.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

126
L. Hanzlik and M. Kutylowski
Standard executions. All oracle queries for regular ePassports (all but the imper-
sonated one) can be simulated by R. Note that it knows all the necessary data,
i.e. the private keys and the certiﬁcates.
Executions for the impersonated ePassport. Since R does not know the expo-
nent x−1, the way R handles oracle queries of A must be shown. Again, two
cases can be distinguished, i.e. A uses the Execute oracle to create transcripts
of communication and A uses the Send oracle to interact with the ePassport.
In both cases we will use the following idea. R does not know the private key
α, it follows that it cannot compute the value w = y′
C/sk∗
C. However, if R uses
Y ′
C = ˆg1
sk∗
C·y′
C, then it can compute w as y′
C. Note that this only works if we are
able to compute ˆg1
sk∗
C.
The transcripts received from the Execute oracle for the ePassport and
a reader can be easily computed by R. First, it computes ˆg1 according to the
protocol. Let us denote by γ the discrete logarithm of ˆg1 to the base g1, i.e.
γ = logg1 ˆg1. Note that in case of MapIM γ is one of exponents ri used in deﬁn-
ition of HEC. On the other hand, in case of MapGM γ = yC · yR + s and yC, yR
and s are chosen by R. It follows that in both cases R knows γ.
R then uses Y ′
C = (h1)r·γ·y′
C, for a random y′
C ←R Z∗
q. Then, it can com-
pute the key K using the formula of the reader’s side (Y ′
C)y′
R. Finally, R sets
w = y′
C. Note that we have Y ′
C = (g1)sk∗
C·γ·y′
C = ˆg1
sk∗
C·y′
C and e(Y ′
C, g2) =
e( ˆg1
sk∗
C·y′
C, g2) = e( ˆg1, gsk∗
C
2
)y′
C = e( ˆg1, pk∗
C)w.
The second case, i.e. A uses the Send oracle to interact with the ePassport,
is a bit more tricky. This time γ is only known when using MapIM. Therefore
depending on the instantiation of Map R will perform diﬀerently.
MapGM Speciﬁc Steps:
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
First, R computes the random value yC according to protocol but intead of
computing YC = gyC
1 , it computes YC = (hx2
1 )r−1·yC = gx·r−1·yC
1
. It follows
that ˆg1 = g(sk∗
C)−1·yC·yR+s
1
, where only yC and s are known to R. In order
to compute ˆg1
sk∗
C R has to compute gyC·yR+sk∗
C·s
1
. Note that R can com-
pute (YR)yC = gyC·yR
1
. It remains to show that knowing s, R can use it and
hr
1 = gx−1·r
1
= gsk∗
C
1
to compute the remaining part.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
MapIM Speciﬁc Steps:
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
For MapIM this part is easier. R computes ˆg = grj
1 according to the protocol.
Note that R knows the inputs to the mapping function and can ﬁnd the right
exponent rj. Moreover, R can compute ˆg1
sk∗
C by computing hr
1 = gx−1·r
1
= gsk∗
C
1
and (gsk∗
C
1
)rj = ˆg1
sk∗
C.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Chip Authentication for E-Passports
127
Knowing ˆg1
sk∗
C, R sets Y ′
C = ( ˆg1
sk∗
C)y′
C. It follows that R cannot compute
K, since in this case the exponent y′
R is unknown. The key computed by A is
K = (( ˆg1
sk∗
C)y′
C)y′
R and R cannot compute it. However, the ePassport only uses
this key, if the tag TR sent by the reader is valid. It follows that A must query the
random oracle for the key K (this is ensured by Game 4). Thus, R may search
the query table with entries of the form (query, output) for an output such that
the tag is valid. Then R can extract the key K from the query corresponding to
the found output and derive all keys. Finally, R sets w = y′
C. Note that in both
cases the pairing equation (checked by the reader) holds.
Impersonation execution. Since A wins the impersonation game, there must
be a protocol execution i (among at most qe executions) in which A tries to
impersonate the ePassport and R plays the role of the reader. R guesses this
execution and succeeds with probability at least 1/qe.
Unlike in the regular executions, R chooses y′
R ←R Z∗
q and sets Y ′
R = hy′
R
1 . It
follows that A computes K = (Y ′
R)y′
C = (hy′
R
1 )y′
C. Note that R cannot compute
this K and derive key K′
Mac used to create the tag TR. However, with probability
at least 1/qh, R may guess the step at which A queries the random oracle for
K, 4 and returns a random and known value. In this way R is able to compute
TR that will be accepted by A. The only way A may notice the diﬀerence in the
random oracle is when he queries with the key K in another query. However,
since the key K is always distinct (for the honest ePassports), it follows that the
probability that A queries this key is at most qh/2λ. Then, since the adversary
must compute a valid ciphertext c, R is able to ﬁnd the key K in the oracle table,
derive the key K′
Enc and decrypt the last message of A receiving w = y′
C/sk∗
C.
Finally, R has the following data: K = (hy′
R
1 )y′
C, w = y′
C/sk∗
C, y′
R, so R can
compute Kw−1·y′−1
R
= hsk∗
C
1
. Since sk∗
C = x−1 · r, R can compute (hsk∗
C
1
)r−1 =
hx−1
1
. Thereby R solves the 2 −SDH problem with probability
ϵ
qh·qe·min{n,qe} (R
must choose the correct execution, the correct ePassport and the right oracle
query of A) by returning (0, h1/x
1
).
4
Conclusion
In this paper we propose a new version of the PACE protocol with Chip Authenti-
cation Mapping based on bilinear maps. It is not only as eﬃcient as the standard
version but it can also be used with an arbitrary Map function, e.g. based on
hash functions like in PACE with Integrated Mapping.
Although, the Chip Authentication Mapping v2 seems to be interesting from
industrial (patent free) and academic (provable security under the 2-Strong
Diﬃe-Hellman assumption) perspective, it might require some time and acknowl-
edgment from involved parties in order for the protocol to become a part of the
ICAO standard. Moreover, bilinear maps are not commonly used by the indus-
try which may additionally slow down and even prevent the protocol from being
implemented in a near future. On the bright side, bilinear maps are not required

128
L. Hanzlik and M. Kutylowski
on the ePassport but only on the readers, which have the computational power
to compute such maps in reasonable time.
Acknowledgment. The research was supported by the Polish National Science Cen-
tre based on the decision DEC-2013/08/M/ST6/00928. Initial work of the ﬁrst author
has been supported by Foundation for Polish Science project VENTURES/2012-9/4.
References
[BB08] Boneh, D., Boyen, X.: Short signatures without random oracles and the
SDH assumption in bilinear groups. J. Cryptology 21(2), 149–177 (2008)
[BDFK12] Bender, J., Dagdelen, ¨O., Fischlin, M., K¨ugler, D.: The PACE—AA proto-
col for machine readable travel document, and its security. In: Proceedings
of the 16th International Conference on Financial Cryptography and Data
Security (2012)
[BFK09] Bender, J., Fischlin, M., K¨ugler, D.: Security analysis of the PACE key-
agreement protocol. In: Samarati, P., Yung, M., Martinelli, F., Ardagna,
C.A. (eds.) ISC 2009. LNCS, vol. 5735, pp. 33–48. Springer, Heidelberg
(2009)
[BFK13] Bender, J., Fischlin, M., K¨ugler, D.: The PACE|CA protocol for machine
readable travel documents. In: Bloem, R., Lipp, P. (eds.) INTRUST 2013.
LNCS, vol. 8292, pp. 17–35. Springer, Heidelberg (2013)
[BK12] Bender, J., K¨ugler, D.: Verfahren zur Authentisierung, RF-chip-Dokument,
RF-Chip-Leseger¨at und Computerprogrammprodukte, 13 September 2012.
WO Patent App. PCT/EP2012/001,076 (2012)
[BM92] Bellovin, S.M., Merritt, M.: Encrypted key exchange: password-based pro-
tocols secure against dictionary attacks. In: IEEE Symposium on Research
in Security and Privacy, pp. 72–84 (1992)
[BN05] Barreto, P.S.L.M., Naehrig, M.: Pairing-friendly elliptic curves of prime
order. In: Preneel, B., Tavares, S. (eds.) SAC 2005. LNCS, vol. 3897, pp.
319–331. Springer, Heidelberg (2006)
[BPR00] Bellare, M., Pointcheval, D., Rogaway, P.: Authenticated key exchange
secure against dictionary attacks. In: Preneel, B. (ed.) EUROCRYPT 2000.
LNCS, vol. 1807, pp. 139–155. Springer, Heidelberg (2000)
[BSI15] BSI. Advanced Security Mechanisms for Machine Readable Travel Docu-
ments and eIDAS Token 2.20. Technical Guideline TR-03110-2 (2015)
[CGIP11] Coron, J.-S., Gouget, A., Icart, T., Paillier, P.: Supplemental Access Control
(PACE v2): Security Analysis of PACE Integrated Mapping. Cryptology
ePrint Archive, Report 2011/058 (2011)
[HKK13] Hanzlik, L., Krzywiecki, L., Kutylowski, M.: Simpliﬁed PACE|AA protocol.
In: Deng, R.H., Feng, T. (eds.) ISPEC 2013. LNCS, vol. 7863, pp. 218–232.
Springer, Heidelberg (2013)
[ISO11] ISO/IEC JTC1 SC17 WG3/TF5 for the International Civil Aviation Orga-
nization. Supplemental access control for machine readable travel docu-
ments v1.01. Technical report, 08 March 2011
[ISO14] ISO/IEC JTC1 SC17 WG3/TF5 for the International Civil Aviation Orga-
nization. Supplemental access control for machine readable travel docu-
ments v1.1. Technical report, 15 April 2014

Chip Authentication for E-Passports
129
[Jab96] David, P.: Jablon: strong password-only authenticated key exchange. SIG-
COMM Comput. Commun. Rev. 26(5), 5–26 (1996)
[Sho04] Shoup, V.: Sequences of games: a tool for taming complexity in security
proofs. Cryptology ePrint Archive, Report 2004/332 (2004). http://eprint.
iacr.org/

AEP-M: Practical Anonymous E-Payment
for Mobile Devices Using ARM TrustZone
and Divisible E-Cash
Bo Yang1, Kang Yang1(B), Zhenfeng Zhang1, Yu Qin1, and Dengguo Feng1,2
1 Trusted Computing and Information Assurance Laboratory, Institute of Software,
Chinese Academy of Sciences, Beijing, China
{yangbo,yangkang,zfzhang,qin yu,feng}@tca.iscas.ac.cn
2 State Key Laboratory of Computer Science,
Institute of Software, Chinese Academy of Sciences, Beijing, China
Abstract. Electronic payment (e-payment) has been widely applied
to electronic commerce and has especially attracted a large number of
mobile users. However, current solutions often focus on protecting users’
money security without concerning the issue of users’ privacy leakage. In
this paper, we propose AEP-M, a practical anonymous e-payment scheme
speciﬁcally designed for mobile devices using TrustZone. On account of
the limited resources on mobile devices and time constraints of elec-
tronic transactions, we construct our scheme based on eﬃcient divisible
e-cash system. Precisely, AEP-M allows users to withdraw a large coin
of value 2n at once, and then spend it in several times by dividing it
without revealing users’ identities to others, including banks and mer-
chants. Users’ payments cannot be linked either. AEP-M utilizes bit-
decomposition technique and pre-computation to further increase the
ﬂexibility and eﬃciency of spending phase for mobile users. As a conse-
quence, the frequent online spending process just needs at most n expo-
nentiations on elliptic curve on mobile devices. Moreover, we elaborately
adapt AEP-M to TrustZone architecture for the sake of protecting users’
money and critical data. The methods about key derivation and sensi-
tive data management relying on a root of trust from SRAM Physical
Unclonable Function (PUF) are presented. We implement a prototype
system and evaluate AEP-M using Barreto-Naehrig (BN) curve with
128-bit security level. The security analysis and experimental results
indicate that our scheme could meet the practical requirement of mobile
users in respects of security and eﬃciency.
Keywords: E-Payment · Privacy · TrustZone · Divisible e-cash · PUF
1
Introduction
Depending on the development and achievements of wireless network as well as
modern mobile devices, electronic commerce (e-commerce) is beneﬁting more
and more people’s daily lives. As e-commerce becomes a major component of
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 130–146, 2016.
DOI: 10.1007/978-3-319-45871-7 9

AEP-M: Practical Anonymous E-Payment for Mobile Devices
131
business operations, e-payment, which builds up e-commerce, has become one of
the most critical issues for successful business and ﬁnancial services. Deﬁned as
the transfer of an electronic value of payment from a payer to a payee through
the Internet, e-payment has been already realized in diﬀerent ways and applied
to mobile devices by intermediaries such as PayPal, Google Wallet, Apple Pay
and Alipay [8]. Unfortunately, with the widespread use of e-payment, users are
faced with the risk of privacy disclosure.
Although the intermediaries and online banks try the best to enhance the
security of their e-payment solutions, the privacy-preserving scheme is often
neglected or weakened in the implementation [9]. Generally, the spending pro-
cedure is associated with the authenticated identity to indicate who withdraws
digital coins from banks, so that all the user’s relevant consuming behaviors are
identiﬁed and linked. In reality, the most of current deployed e-payment solu-
tions unintentionally reveal user personal information, perhaps involving user
real identity, billing and shopping records etc., to banks, intermediaries or payees
[11]. Such sensitive information implies one’s political view, location, religion or
health condition. Statistically, mobile users account for a high proportion among
all the e-payment users [10]. Thus, the issue of information leakage is threatening
mobile e-payment users’ personal privacy.
In theory, constructing anonymous e-payment scheme is able to eﬀectively
solve the above problem. Some anonymous protocols are the candidates here
including direct anonymous attestation (DAA) and U-Prove. Based on DAA,
Yang et al. [19] put forward LAMS for anonymous mobile shopping. However,
these protocols hardly fulﬁll the anonymous e-payment from the perspectives
of both anonymity and ﬂexibility for payment. Acting as a targeted compo-
nent for e-payment, electronic cash (e-cash), introduced by Chaum [5], allows
users to withdraw digital coins from a bank and to spend them to merchants
in an anonymous way, thus perfectly emulating conventional cash transactions.
Derived from e-cash, divisible e-cash systems are proposed to address the issue
of splitting coins of large values. Depending on it, users could withdraw a large
coin of value 2n at once and spend it in several times by dividing it. In practice,
divisible e-cash makes the cash transactions more eﬃcient and ﬂexible. In regard
to mobile devices, the limited resources along with the strong time constraints
of electronic transactions indeed require the practical withdrawal and spending
procedures. Therefore, it is advisable to build anonymous e-payment scheme
upon eﬃcient divisible e-cash for mobile devices.
It is commonly believed that good security and trust will ultimately
increase the use of e-payment. Nevertheless, the direct application of anonymous
e-payment scheme on mobile devices would bring potential security risks. With-
out the dedicated protection, the scheme’s executing codes and sensitive data
are easily either compromised or stolen by the malwares. In some cases, the
attacks on mobile e-payment could cause user’s great loss of property. The tech-
nique of Trusted Execution Environment (TEE) on mobile devices is able to lend
us a helping hand. Isolated from a Rich Execution Environment (REE) where
the Guest OS runs, TEE aims to protect sensitive codes execution and assets.

132
B. Yang et al.
As a prevalent example of providing TEE for embedded devices, ARM TrustZone
[1] has been used to execute security-critical services [17]. Actually, TrustZone
enables a single physical processor to execute codes in one of two possible isolated
operating worlds: the normal world (NW) for REE and the secure world (SW)
for TEE. The two worlds have independent memory address spaces and diﬀerent
privileges. As a hardware-based security extension of ARM architecture, Trust-
Zone is widely supported and applied by mobile devices. But there is a ﬂy in
the ointment that TrustZone does not deﬁnitely provide the root of trust with
inside root key for sensitive data management. To the best of our knowledge,
there is no anonymous e-payment scheme specially designed for mobile devices
using TrustZone.
1.1
Our Contribution
Based on ARM TrustZone and the divisible e-cash scheme with the best eﬃciency
by Canard et al. [4], we propose AEP-M, a practical anonymous e-payment
scheme for mobile devices, which enables a user to spend his digital coins securely
and eﬃciently while preserving his privacy. This is the ﬁrst complete work to
design an eﬃcient anonymous e-payment scheme integrated with TrustZone. We
substantially modify the original e-cash scheme for adapting it to the executing
mode of TrustZone and guaranteeing its security on mobile devices.
For device-centered design, we make following steps towards practical and
secure usage:
– the sensitive codes on the user side of AEP-M are isolated and executed in TEE
provided by TrustZone for the possibility that the guest OS is compromised;
– AEP-M utilizes some secret keys, which are derived from a root key seed
reproduced via an on-chip SRAM PUF [6], to protect users’ coins and data;
– in AEP-M, online banks could authenticate a user who holds a mobile device
with available TrustZone and a valid account-password pair.
AEP-M elaborately protects the security of the user’s passwords and coins even
if the NW of his mobile device is corrupted while the SW still keeps honest.
The pre-computation stage is carefully added into our scheme such that the
computation amounts of the frequent online spending phase for mobile users are
decreased. Furthermore, our scheme supports that one spends a coin of value v
for any 1 ≤v ≤2n at once by using the bit-decomposition technique, while the
original scheme [4] cannot, where the maximum denomination of a coin is 2n.
We implement a prototype of AEP-M and evaluate its eﬃciency using BN
curve at the security level of 128-bit. The experimental results show that our
scheme is eﬃcient enough for practical usage, even from the perspective of mobile
devices.
1.2
Related Work
E-Payment Scheme. Diﬀerent from pre-paid cards, credit cards and electronic
checks, e-cash system does a better job to construct anonymous e-payment. After

AEP-M: Practical Anonymous E-Payment for Mobile Devices
133
Chaum ﬁrst introduced e-cash [5], Camenisch et al. [2] presented the compact
e-cash system allowing users to withdraw wallets with 2n coins at once. Unfor-
tunately, its spending procedure should be done coin by coin. Afterwards, some
truly anonymous divisible e-cash systems [3] were described, but quite ineﬃ-
ciency. Recently, Canard et al. [4] proposed the ﬁrst really eﬃcient divisible
e-cash system by deﬁning one global binary tree. Our scheme takes it as a refer-
ence and further increases its eﬃciency and security according to our architecture
of trusted mobile device.
TrustZone Technology. ARM TrustZone technology for the mobile devices can
guarantee codes integrity and data security. Relying on TrustZone, many prac-
tical mobile schemes are proposed. For instance, AdAttester [7] was presented
specially for secure mobile advertisement on a TrustZone-enabled device. To
date, TrustZone has been popularized and applied by many mainstream mobile
manufacturers, such as Apple, Samsung and Huawei, to achieve secure applica-
tions [7].
2
Preliminaries
2.1
Notation
Throughout the paper, we use the notation shown in Table 1.
Let Λ = (p, G1, G2, GT , e, g, g) be a description of bilinear groups which con-
sist of three (multiplicatively written) groups G1, G2 and GT of prime order p
equipped with a bilinear map e : G1 × G2 →GT , where g and g is the gen-
erator of G1 and G2 respectively. In this paper, we only consider the Type-3
pairings [16].
2.2
ARM TrustZone
ARM TrustZone [1] is a hardware-based security extension technology incorpo-
rated into ARM processors. The whole system is separated into two worlds and
each world has banked registers and memory to run the domain-dedicated OS
and software. As a result, access permissions are strictly under the control of the
secure world that the normal world components cannot access the secure world
resources. As the processor only runs in one world at a time, to run in the other
world requires context switch. A secure monitor mode exists in the secure world
to control the switch and migration between the two worlds.
2.3
Physical Unclonable Functions
Physical Unclonable Functions (PUFs) [12] are functions where the relationship
between input (or challenge) and output (or response) is decided by a physical
system. Randomness and unclonability are two signiﬁcant properties of PUFs.
PUFs are able to implicitly “store” a piece of secret data. PUFs provide much
higher physical security by extracting the secret data from complex physical
systems rather than directly reading them from non-volatile memory.

134
B. Yang et al.
Table 1. Notation used in this paper
Notation
Descriptions
λ
Security parameter
x
$
←S
x chosen uniformly at random from a set S
y := x
y assigned as x
x||y
Concatenation of x and y
(y1, ..., yj) ←A(x1, ..., xi)
A (randomized) algorithm with input (x1, ..., xi) and
output (y1, ..., yj)
1G
The identity element of a group G
G∗
G \ {1G} for a group G
Σ1 = (KeyGen, Sign, Verify) Digital signature algorithm
Σ2 = (MAC)
Message authentication code
Σ3 = (Encasym, Decasym)
Asymmetric (public key) encryption and decryption
algorithm
Σ4 = (Encsym, Decsym)
Symmetric encryption and decryption algorithm
Strictly speaking, only equipped with a root of trust, TrustZone becomes
a real “trusted” execution environment (TEE) [22]. Because TrustZone almost
does not internally install an available root key, it loses the capability to oﬀer a
root of trust. Employing a PUF can cover this shortage. In this paper, AEP-M
takes the secret data extracted from the PUF as a root key seed to generate
other keys. We adopt SRAM PUF [6] that leverages the relationship between an
SRAM cell’s address for the challenge and its power up value for the response.
3
System Model and Assumptions
3.1
System Model
The system model of AEP-M is composed of ﬁve kinds of entities: mobile device
D, merchant M, trusted authority T , central bank B and traditional commer-
cial bank. In practice, there could be a number of mobile devices and merchants
participating in our system. For the sake of brevity and clarity, we use D and M
to represent an individual instantiation respectively. D is equipped with ARM
processor having TrustZone extension technology. B is responsible for issuing
digital coins to legitimate (or trusted) D through Withdraw phase. B could be
a bank card organization supporting e-payment or an intermediary serving elec-
tronic transactions. In the background, several commercial banks, where users
actually deposit money, are in cooperation with B for dealing with money trans-
fers in the real world. Service or product providers play the role of M in this
interactive model. They collect digital coins from D via Spend phase and redeem
them from B via Deposit phase. Note that M veriﬁes the digital coins of some

AEP-M: Practical Anonymous E-Payment for Mobile Devices
135
Mobile Device
Central Bank 
TrustZone
Merchant 
Bank A
Bank B
Bank C
Trusted Authority
Fig. 1. System model of AEP-M.
user without revealing user’s identity to any entities including M itself. Man-
aged by the government or the industry administration, in Identify phase T
performs revealing identity of the users who attempt to double-spend digital
coins. Figure 1 illustrates the system model for our scheme.
3.2
Assumptions and Threat Model
To simplify our design in the system model, we assume that data communi-
cations between B and traditional bank, and between B and T build on secure
transport protocols, such as TLS, which can provide conﬁdentiality, authenticity
and integrity protection for data transmission. Also, M, D and B are able to
acquire public parameters from T in the correct way. Public Key Infrastructure
(PKI) is supposed to be already realized for authenticating B and M. As a con-
sequence, (1) D and M can accurately obtain the public key of B by verifying
its certiﬁcate; (2) D and B can accurately obtain the public key of M similarly.
Based on the assumptions, AEP-M protects against the following adversary:
– The adversary can attack the scheme itself by attempting to pretend entities,
manipulate data transmission between entities and forge data.
– The adversary can perform software-based attacks which compromise the
mobile Rich OS or existing applications running in REE. AEP-M interfaces
in REE are also available for the adversary.
– The adversary can physically access the mobile device. He can reboot the
device and gain access to data residing on persistent storage.
However, we ignore the malicious behaviors of tampering with the TrustZone
hardware or mounting side-channel attacks on PUF.
4
AEP-M Scheme for Mobile Devices
In this section, we provide the speciﬁc architecture of trusted mobile device, and
then present the key derivation and sensitive data management. Depending on

136
B. Yang et al.
Secure World (SW)
TrustZone Isolation Boundary
User Mode
Kernel Mode
Hardwares with ARM TrustZone Extension
SW-Driver
Monitor
NW-Driver
AEP-M Service
Crypto Library
API Functions
Data Handler
Key Manager
SRAM PUF
AEP-M Proxy
Software Stack
Crypto Library
Command Caller
Preprocessing Engine
Mobile OS Kernel
TEE OS Kernel
App1
App2
Appn
Normal World (NW)
. . .
App1 Trustlet
Appn Trustlet
. . .
Secure Memory
Logic Engine
Framebuffer
Fig. 2. Architecture of trusted mobile device for AEP-M.
these, the construction of AEP-M scheme is detailed next. Finally, the security
properties of AEP-M is analyzed.
4.1
The Architecture of Trusted Mobile Device
Leveraging TrustZone and PUF technology, we design the architecture of trusted
mobile device speciﬁcally for AEP-M based on our previous work [20]. The
software-based implementation of AEP-M functionality on existing hardwares
targets at economy, ﬂexibility and extensibility. Meanwhile, our architecture is
designed to be compatible with the conventional running model of secure appli-
cations using TrustZone. Figure 2 shows the detailed architecture with the way
components interact with each other.
AEP-M functionality in the architecture contains two components: untrusted
AEP-M Proxy in normal world (NW) and security-sensitive AEP-M Service in
secure world (SW). In reality, SW instantiates TEE, while NW implements REE.
Depending on the whitelist and integrity protection mechanism, only the trusted
codes of programs in SW could be loaded and executed. Thus, AEP-M Service
resides in a relatively secure environment isolated from other codes running in
NW. The diﬀerent components from [20] are formally described as follows.
AEP-M Proxy. This is the component visible for mobile (e-payment) appli-
cations in NW. Waiting for their AEP-M service requests, the proxy handles
the parameters and preprocesses them. Preprocessing Engine executes pre-
computation for AEP-M after digital coins are withdrawn from central bank to
the mobile device.
AEP-M Service. This is the core component to perform AEP-M critical com-
putations and operations. The execution of the component codes is under the well
protection of TrustZone isolation mechanism. Framebuﬀer stores the image of
conﬁrmation message (e.g., the identity of merchant to be paid) to be securely

AEP-M: Practical Anonymous E-Payment for Mobile Devices
137
displayed for the user. Diﬀerent from the general frame buﬀer in NW, Frame-
buﬀer is devoted to the reliable graphical user interface (GUI) for SW.
Application and Application Trustlet. The corresponding application
should be launched if the user wants to enjoy e-payment service. For upper-
level interaction, the application released by B consists of two parts: App for
NW and App Trustlet for SW. App provides the general GUI and basic func-
tions, while App Trustlet is securely loaded by SW and trusted for processing
security-sensitive user inputs and data operations.
Components in Hardwares. Protected by TrustZone mechanism, SRAM PUF
component and Secure Memory component are only accessible for SW. Secure
Memory contributes to temporally saving sensitive data.
4.2
Key Derivation and Sensitive Data Management
Prior to describing the concrete construction of our AEP-M scheme, we show how
to derive various keys for diﬀerent purposes using the root key seed extracted
from SRAM PUF and how to utilize the derived keys to protect sensitive data.
Root Key Seed Extraction. We use the technique of SRAM PUF in [22] to
extract the secret root key seed, which is a unique bit string picked randomly
by the OEM who “stores” it in D through the physical features of one SRAM
inside D. From SRAM PUF component, seed is only reproduced and securely
cached by Key Manager when D starts up every time in normal use.
Key Derivation. Key Manager has the deterministic key derivation function
KDF: S × {0, 1}∗→K, where S is the key seed space, and K is the derived key
space. Using the KDF, the device key pair and the storage root key is derived
as (dsk, dpk) ←KDFseed("identity") and srk ←KDFseed("storage root")
respectively. The unique device key pair is analogous to the endorsement key
deﬁned in trusted computing but supports encryption and decryption. The stor-
age root key srk is used for generating speciﬁc storage keys to preserve sensitive
data. The hierarchical structure of storage keys enhances the security for key
usage. Note that all the derived keys are never stored permanently. Instead,
they are regained via KDF with seed at the same way when needed.
Sensitive Data Management. We can utilize the storage keys derived from
the storage root key srk to seal the AEP-M’s public parameters params, D’s
digital coin σ, the secret key m, and other related variables CT and δ. What
these variables represent will be explained in Sect. 4.3. The sealed results of these
data are stored in the insecure positions of D.
– Protect integrity for params: mkparams ←KDFsrk("storage key"||"MAC"||
params), and blobparams ←Data Seal("MAC", mkparams, params), where
blobparams := params||MAC(mkparams, params).

138
B. Yang et al.
– Protect integrity for σ: mkσ
←
KDFsrk("storage key"||"MAC"||σ), and
blobσ ←Data Seal("MAC", mkσ, σ), where
blobσ := σ||MAC(mkσ, σ).
– Protect both conﬁdentiality and integrity for m, CT and δ with the aid
of U: (skm, mkm) ←KDFsrk("storage key"||"Enc+MAC"||U), and blobm ←
Data Seal("Enc+MAC", skm, mkm, m||CT||δ, U), where
blobm := Encsym(skm, m||CT||δ)||U||MAC(mkm, Encsym(skm, m||CT||δ)||U).
Data Handler can use Data Unseal() to recover and verify the sensitive data from
blobs with the related keys regained by Key Manager.
4.3
The Details of AEP-M Scheme
0
φ
1
00
...
...
01
010
011
...
...
Fig. 3. Public global tree for all
coins.
Following the divisible e-cash scheme [4], a
unique and public global tree of depth n is used
for all coins of value V = 2n as illustrated in
Fig. 3. So each leaf denotes the smallest unit of
value to spend. We deﬁne Sn as the set of bit
strings of size smaller than or equal to n and Fn
as the set of bit strings of size exactly n. Thus,
each node of the tree refers to an element s ∈Sn,
the root to the empty string φ, and each leaf to
an element f ∈Fn. For any node s ∈Sn, Fn(s)
= {f ∈Fn|s is a preﬁx of f} contains all the
leaves in the subtree below s.
Assume, before leaving the factory, D is initialized by the OEM in SW to
generate the unique device key (dsk, dpk) which could uniquely identify D. Then,
the OEM issues a certiﬁcate certD w.r.t. the public key dpk to indicate the OEM’s
recognition for D. The certiﬁcate certD also contains some D’s conﬁguration
information (e.g., whether TrustZone is available).
AEP-M scheme consists of six phases: Setup, KeyGen, Withdraw, Spend,
Deposit and Identify. First of all, Setup is executed to create the public parame-
ters by T . After that, B and M can execute KeyGen to generate their public-
private key pairs according to the public parameters. Then, other phases are
enabled to be executed according to requirements. The phases of the scheme are
presented in detail as follows.
Setup. In this phase, the trusted authority T creates the public parameters.
Given a security parameter λ, T picks the suitable bilinear groups parameters
Λ := (p, G1, G2, GT , e, g, ˜g) described in Sect. 2.1 such that |p| ≥2λ. And
then, according to the global tree, T generates (1) rs
$←Zp and gs := grs for
each s ∈Sn, and (2) lf
$←Zp and ˜gs→f := ˜glf /rs for each s ∈Sn and each
f ∈Fn(s). T keeps sck = {rs|s ∈Sn} as its secret keys to be used in Identify

AEP-M: Practical Anonymous E-Payment for Mobile Devices
139
phase. Also, T determines a series of algorithms Ψ including the algorithms
covering from Σ1 to Σ4 in Table 1, and four independent collision-resistant
hash functions:
H1 : {0, 1}∗→Zp, H2 : {0, 1}∗→Zp, H3 : {0, 1}∗→{0, 1}2λ, H4 : {0, 1}∗→{0, 1}2λ.
Finally, T sets (Λ, n, Ψ, {rs|s ∈Sn}, {˜gs→f|s ∈Sn ∧f ∈Fn(s)}) as the pub-
lic
parameters,
where
D
and
M only need to know params := (Λ, n, Ψ, {rs|s ∈Sn}), while B requires
params′ := (Λ, n, Ψ, {rs|s ∈Sn}, {˜gs→f|s ∈Sn ∧f ∈Fn(s)}). After obtain-
ing params, D calls Data Seal() to seal it and stores the output blobparams.
KeyGen. This phase initializes the public-private key pair for the central bank
B and a merchant M.
– Key Generation for Central Bank. First, given params′ as input, B picks
x, y
$←Z∗
p, and computes X := ˜gx and Y := ˜gy. B sets (x, y) as the
private key skB and publishes (X, Y ) as the public key pkB. Second, B
uses KeyGen() in Σ1 to generate key pair for establishing sessions with D:
(skB, pkB) ←KeyGen(1λ), where skB is the private key.
– Key Generation for Merchant. Similarly, M uses KeyGen() to generate key
pair for establishing sessions with D: (skM, pkM) ←KeyGen(1λ).
Accordingly, D could get the correct pkB and pkM from B and M via veri-
fying their certiﬁcates. And likewise, M and B could acquire the correct pkB
and pkM respectively as well as T obtains pkB.
Withdraw. In this phase, a user with mobile device D could withdraw some
digital coins from the central bank B as follows.
1. The user operates App in NW of D to prepare for withdrawing some
digital coins. D switches into SW and chooses a nonce nD
$←{0, 1}λ. nD
is saved in Secure Memory and delivered to AEP-M Proxy that sends nD,
D’s dpk with its certiﬁcate certD to B.
2. B checks whether dpk is valid with certD and checks the conﬁguration
information on certD. If the check is passed, B chooses a nonce nB
$←
{0, 1}λ, a key kmac
$←{0, 1}λ for MAC and a key kenc
$←{0, 1}λ for Encsym
and Decsym. Then, B encrypts nB, kmac and kenc using dpk to get a cipher
text CB ←Encasym(dpk, nB||kmac||kenc) and signs dpk, nD and CB using
skB to output a signature α ←Sign(skB, dpk||nD||CB). Finally, B sends a
commitment request commreq := (CB, α) to D.
3. AEP-M Proxy invokes AEP-M Service with input commreq. In SW, App
Trustlet waits for the user to input his bank account accountD, the cor-
responding password pwd and the amount of digital coins to withdraw.
For simplicity, we only describe how to withdraw one coin. The With-
draw phase could be easily extended to support withdrawing multiple
coins at once. After the user ﬁnishes inputting, Logic Engine calls the
API AEPM SW Withdraw() to generate a commitment response:
commres ←AEPM SW Withdraw(blobparams, nD, pkB, commreq, accountD, pwd),
where the API is executed as follows:

140
B. Yang et al.
(1) Unseal the blob blobparams to get params by calling Data Unseal().
(2) Verify α using pkB: res ←Verify(pkB, dpk||nD||CB, α). If res = false,
commres := ⊥and return.
(3) Decrypt CB using dsk: (n′
B, kmac, kenc) ←Decasym(dsk, CB).
(4) Choose m
$←Z∗
p as the secret key for a coin, and compute the com-
mitment U := gm.
(5) Set δ := V where δ denotes the current balance of the coin.
(6) Set CT as a string of 2n+1 −1 bits where each bit is 1. CT denotes
the current tree structure of the unspent coin.
(7) Call Data Seal() to seal m, CT and δ, and generate blobm(see
Sect. 4.2).
(8) Choose a random number rD
$←Z∗
p and compute RD := grD.
(9) Compute cD := H1(g||U||RD||CB||α||n′
B).
(10) Compute sD := rD + cD · m (mod p).
(11) Generate a cipher context CD ←Encsym(kenc, accountD||pwd).
(12) Generate τD ←MAC(kmac, U||n′
B||cD||sD||CD), and output
commres := (τD, U, n′
B, cD, sD, CD).
AEP-M Service saves n′
B and kmac in Secure Memory as well as stores
blobm in non-volatile storage. Then D switches back to NW and sends
commres to B.
4. On input commres, B runs the following algorithm to generate a digital
coin σ on m for D:
(σ, τB) ←Gen DC(commres, params′, kmac, kenc, nB, skB).
The algorithm has seven steps:
(1) Verify τD = MAC(kmac, U||n′
B||cD||sD||CD), and check whether n′
B =
nB.
(2) Check whether U has not been used before by querying the database.
(3) Compute R′
D := gsD · U −cD and c′
D := H1(g||U||R′
D||CB||α||nB).
(4) Check whether c′
D = cD.
(5) Decrypt CD using Decsym and kenc: accountD||pwd
←
Decsym
(kenc, CD), then check the plaintext’s validness via communicating
with the related commercial bank. If the account balance is enough,
deduct money from the account and temporarily save it in B.
(6) Choose a random number a
$←Z∗
p, compute A := ga, B := Ay,
C := gax · U axy and D := U ay, and generate σ := (A, B, C, D).
(7) Generate τB ←MAC(kmac, σ||nD||nB).
In the above algorithm, if any check is failed, B aborts the process. If not,
B sends (σ, τB) to D, and sends (U, dpk, IDbank, IDuser) to T to backup for
detecting possible double-spender. IDbank is the identity of the commercial
bank which the user account belongs to, and IDuser is the identity of the
user.
5. Upon receiving (σ, τB), D switches into SW and veriﬁes τB using MAC,
kmac and n′
B. Then, Data Handler calls Data Seal() to seal σ and generates
blobσ. Finally, Logic Engine deletes nD, n′
B and kmac from Secure Memory.

AEP-M: Practical Anonymous E-Payment for Mobile Devices
141
Pre-Compute. After the above step, D returns back to NW. AEP-M Proxy
executes pre-computation in the background (oﬀ-line) to prepare for the
following Spend phase. Preprocessing Engine calls AEPM NW PreCmpt() to
generate a blinded coin:
(l, R, S, T, W) ←AEPM NW PreCmpt(blobparams, blobσ),
where the algorithm consists of the following steps.
(1) Get params and digital coin σ by directly reading the plaintext part of
blobparams and blobσ respectively.
(2) Parse σ as (A, B, C, D).
(3) Choose l
$←Z∗
p and compute (R, S, T, W) := (Al, Bl, Cl, Dl).
(4) Output (l, R, S, T, W).
Preprocessing Engine stores (l, R, S, T, W) together with blobσ.
Spend. This is an interactive phase executed between a user with his mobile
device D and a merchant M, which enables D to anonymously pay some
digital coins to M.
1. App of D sends a nonce ¯nD
$←{0, 1}λ to the merchant M for initiating
a transaction.
2. Receiving
¯n′
D,
M
chooses
a
nonce
nM
$←
{0, 1}λ
and
gen-
erates a signature
β
←
Sign(skM, "Spend"||info) where info
:=
(v, date, trans, pkM, ¯n′
D, nM). info is the string collection containing the
amount value v of coins to pay, transaction date, other necessary transac-
tion information and the related nonce values. M sends (info, certM, β) to
D. In fact, issued by CA, certM is M’s certiﬁcate, containing IDM, pkM
and the signature SignCA(IDM||pkM), where IDM indicates the identitiy
of M.
3. When D receives the above data, AEP-M Proxy assembles the command
to request AEP-M Service for payment. Without loss of generality, we
assume that the user has a coin of value δ such that δ ≥v. For the case
that δ < v, the user could spend another several coins in the same way in
order that the sum amounts value of all coins equals v. On account of the
request, D’s environment is switched into SW. First, Logic Engine veri-
ﬁes β using Verify and pkM with certM. Then, D enters the secure GUI
after authenticating the user’s inputted PIN (or ﬁngerprint). Relying on
Framebuﬀer, the secure GUI displays IDM and the content of v, date and
trans. It is important for the user to conﬁrm the exact IDM and transac-
tion information in case an adversary falsiﬁes the transaction. When the
user presses the button of “OK”, Logic Engine calls AEPM SW Spend()
to create a master serial number Z of value v of coins together with a
proof π of its validity, using the related pre-computation result as:
(Z, π) ←AEPM SW Spend(blobparams, blobm, blobσ||(l, R, S, T, W), info),
where the detailed process is presented as follows:
(1) Unseal the blobs to get params, (m, CT, δ) and (A, B, C, D) by
calling Data Unseal().

142
B. Yang et al.
(2) Check whether ¯n′
D = ¯nD.
(3) Represent v by bits: v = bnbn−1...b0 and set Φ := {i| 0 ≤i ≤n∧
bi = 1}.
(4) For each i ∈Φ from n to 0, based on CT, select uniformly at random
an unspent node si ∈Sn of level n −i in the tree, and then mark it
as the spent one.
(5) For each chosen node si, compute tsi := gm
si , and form three sets: s :=
{si|i ∈Φ}, g s := {gsi|i ∈Φ} and ts := {tsi|i ∈Φ}. Set Z := (s, ts).
(6) Choose a random number ¯r
$←Z∗
p, compute Li := g¯r
si for each i ∈Φ,
form a set L := {Li|i ∈Φ} and compute L := Bl·r.
(7) Compute ¯c := H2(g s||ts||R||S||T||W||L||L||info).
(8) Compute ¯z := ¯r + ¯c · m (mod p).
(9) Set π := (R, S, T, W, ¯c, ¯z).
(10) Delete (l, R, S, T, W) from the non-volatile storage.
(11) Update CT and δ := δ −v. If δ > 0, call Data Seal() again to regen-
erate blobm using the updated CT and δ, else delete blobm and blobσ.
After the API ﬁnally returns, D switches back into NW and sends (Z, π)
to M.
4. M sets Tr := (info, Z, π) and veriﬁes Tr by the means of calling the
specialized veriﬁcation algorithm Tr Verify() as:
res ←Tr Verify(params, pkB, Tr),
where the algorithm runs in detail as follows:
(1) Parse Tr as (info, Z = (s, ts), π = (R, S, T, W, ¯c, ¯z)).
(2) For any two nodes in s, check that the one does not belong to the
subtree rooted at the other one (i.e., each node is not a preﬁx of any
other one).
(3) Compute L
′ := S ¯z · W −¯c, L′
i := g¯z
si · t−¯c
si
for each si ∈s, and set
L′ := {L′
i|i ∈Φ}.
(4) Compute ¯c′ := H2(g s||ts||R||S||T||W||L′||L
′||info).
(5) Check whether the relations R ̸= 1, W ̸= 1, e(R, Y ) = e(S, ˜g),
e(T, ˜g) = e(R · W, X) and ¯c′ = ¯c hold.
(6) If all the above checks are passed, then res := true, else res := false.
According to the veriﬁcation result res, M decides whether to accept the
payment from D and provide services or goods for the user. If M accepts
the transaction, he sends D a receipt θM ←Sign(skM, "receipt"||Tr) as
the proof of accepting digital coins.
Pre-Compute. After Step 3 above, AEP-M Proxy of D
in NW exe-
cutes pre-computation again in the background to generate a new tuple
(l′, R′, S′, T ′, W ′) w.r.t. some blobσ, if exists, for the next Spend use.
Deposit. In this phase, M could deposit money from Tr to his preferable bank
accountM through the central bank B.
1. M generates a signature γ ←Sign(skM, "Deposit"||Tr||accountM). Then
he sends Tr, accountM and γ together with certM to B.

AEP-M: Practical Anonymous E-Payment for Mobile Devices
143
2. B ﬁrst veriﬁes γ using Verify and certM. Secondly, B retrieves pkM from
info and checks whether it is the same one inside certM. Thirdly, B com-
putes H3(Tr) and queries database DBTr to check whether Tr has been
used before. If not, B runs the veriﬁcation algorithm Tr Verify() to verify
the validity of Tr. If it is valid, B immediately transfers the exact amount
v of real money to accountM with the help of some commercial bank.
3. B detects double-spending oﬀ-line after the above step. The detection
process is presented as follows:
(1) Retrieve s and ts from Tr, and load params′.
(2) For each tsi ∈ts and each f ∈Fn(si), compute dsi→f := e(tsi, ˜gsi→f)
and dsi,f := H4(dsi→f).
(3) Set d := {dsi,f|si ∈s ∧f ∈Fn(si)}.
(4) Insert the item (H3(Tr), Tr, d) into DBTr.
(5) For each dsi,f, query DBTr to check whether there exists a transaction
Tr′ that has the same dsi,f. If exists, send both Tr and Tr′ to T through
the secure channel for revealing the identity of the double-spender.
Identify. This phase endows T with the ability to reveal the identity of some
double-spender.
1. When T receives the double-spending report (Tr, Tr′) from B, it executes
the veriﬁcation algorithm Tr Verify() to verify the validity of Tr and Tr′.
If both valid, T chooses one node si ∈s from data of Tr and ﬁnds out
the related rsi from its secret coin keys sck to recover U by computing
U := t
1/rsi
si
(i.e. gm). Likewise, B recovers U ′ from Tr′.
2. If U = U ′, it indicates that Tr and Tr′ lead to a double-spending.
T would publish the spender’s information (Tr, Tr′, U, dpk, IDbank, IDuser).
Then, some possible penalties on the user IDuser, for example deducting
money from the user’s account or temporally prohibiting the user from
using e-payment system, would be triggered.
4.4
Optional Defense Mechanisms and Security Analysis
AEP-M satisﬁes the desired security properties such as unlinkability, traceability,
exculpability, conﬁdentiality and authenticity. Additional defense mechanisms
could further enhance our scheme’s security. The detailed description of these
properties, mechanisms and the analysis can be found in the full paper [21].
5
Implementation and Evaluation
In this section, we ﬁrst present the prototype of AEP-M from both aspects
of hardware and software. Afterwards, we show the eﬃciency of the proposed
scheme. Finally, we give the performance evaluation and analysis based on our
prototype system.

144
B. Yang et al.
5.1
Implementation
Hardware Platform. To simulate real environment, we implement the role of
merchant on one PC platform, and central bank as well as trusted authority
on another one. For simulating mobile device, we leverage a development board
Zynq-7000 AP Soc Evaluation Kit [18] to implement functions of AEP-M. It is
TrustZone-enabled and equipped with ARM Cortex-A9 MPCore, 1 GB DDR3
memory and On-Chip Memory (OCM) module. We utilize an SRAM chip that
is the type IS61LV6416-10TL [15] to act as our SRAM PUF. The processor can
fetch the SRAM data in the RAM cache via the bus. In addition, the methods
given in [13] are applied to fulﬁll Framebuﬀer for secure display.
Software Implementation. The software implementation on the development
board for mobile device is divided into two parts. In secure world, we use Open
Virtualization SierraTEE as the basic TEE OS which is compliant with GP’s
TEE Speciﬁcations [14]. For Crypto Library, we use OpenSSL-1.0.2 g for general
cryptographic algorithms, and Pairing-Based Cryptography (PBC) 0.5.14 library
for computations of elliptic curves and bilinear maps. The security parameter λ
is set to 128 (bits), so we choose SHA256 for H3 and H4, HMAC-SHA256 for
MAC, 3072-bit RSA for Encasym −Decasym, 256-bit ECDSA for Sign-Verify and
128-bit AES-CBC for Encsym −Decsym. 5268 lines of code (LOC) in C language
totally make up our components and auxiliary functions in secure world. In
normal world, we run a Linux as REE OS with kernel 3.8.6. AEP-M Proxy
totally comprises 2879 LOC. Besides we program one test application that could
execute upon AEP-M scheme. It contains 1268 LOC for App running in NW
and 661 LOC for App Trustlet in SW. Furthermore, there are several tens of
thousands of LOC for other entities.
5.2
Eﬃciency and Performance Evaluation
The speciﬁc analysis of AEP-M’s eﬃciency also appears in the full paper [21].
Since the resource-constrained mobile device is the performance bottleneck as
well as the focus of our attention, we measure the performance of AEP-M on
the prototype system revolving around mobile device. We select BN curve with
embedding degree 12. For testing the security level of 128-bit, we conduct the
experiments using BN256. Each average experimental result is taken over 50
test-runs.
For coins of value 210 and 220 respectively, and spending 287 of them, Fig. 4
illustrates the average time overheads of critical processes including the compu-
tations of Withdraw, Pre-Compute and Spend on mobile device for user side
and Spend on PC for merchant side. The results show that the frequent compu-
tations about either Pre-Compute or Spend only take less than 450 milliseconds
(ms), while infrequent and time-consuming Withdraw spends less than 660 ms.
Moreover, the time overhead is indeed low on PC platform.
Figure 5 shows the average time overheads of Spend phase on mobile device
for user side using n = 10 and diﬀerent v. |Φ| takes corresponding values from
v’s representations by bits. We can see that as the value of |Φ| increases, the time

AEP-M: Practical Anonymous E-Payment for Mobile Devices
145
0
100
200
300
400
500
600
700
Withdraw
Pre-Compute
Spend
Spend (     )
Time Overhead (ms)
n = 10
n = 20
Fig. 4. Time overheads of the critical
processes for coins of 2n and v = 287.
0
100
200
300
400
500
Time Overhead (ms)
v=122
|Φ|=5
v=287
|Φ|=6
v=512
|Φ|=1
v=683
|Φ|=6
v=1023
|Φ|=10
v=736
|Φ|=4
Fig. 5. Time overheads of Spend phase
for n = 10 with diﬀerent values of v.
overheads of Spend have evident growth, which nearly has nothing to do with v
itself, big or small. Encouragingly, under the worst-case scenario where |Φ| = 10,
the resulting overhead spends less than 500 ms, which is completely acceptable
for a mobile user. According to our eﬃciency analysis and experimental results,
AEP-M can be considered as a reasonably eﬃcient scheme for mobile device.
6
Conclusion
In this paper, we propose AEP-M, a complete and practical anonymous
e-payment scheme using TrustZone and divisible e-cash. AEP-M tackles both
security and privacy issues specially for mobile electronic payers. The scheme
allows users to withdraw a coin of value 2n and spend it in several times by
dividing it. Pre-computation and the bit-decomposition technique for coin’s rep-
resentation are carefully taken into our consideration to raise scheme’s eﬃciency
and ﬂexibility. What is more, TrustZone provides data and execution protection
for AEP-M. Our implementation and evaluation convince that AEP-M is quite
practical for payers using resource-constrained mobile devices.
Acknowledgment. This work was supported in part by grants from the National
Natural Science Foundation of China (No. 91118006 and No. 61402455).
References
1. Limited ARM: ARM security technology-building a secure system using TrustZone
technology, April 2009
2. Camenisch, J.L., Hohenberger, S., Lysyanskaya, A.: Compact e-cash. In: Cramer,
R. (ed.) EUROCRYPT 2005. LNCS, vol. 3494, pp. 302–321. Springer, Heidelberg
(2005)
3. Canard, S., Gouget, A.: Divisible e-cash systems can be truly anonymous. In: Naor,
M. (ed.) EUROCRYPT 2007. LNCS, vol. 4515, pp. 482–497. Springer, Heidelberg
(2007)

146
B. Yang et al.
4. Canard, S., Pointcheval, D., Sanders, O., Traor´e, J.: Divisible e-cash made practi-
cal. In: Katz, J. (ed.) PKC 2015. LNCS, vol. 9020, pp. 77–100. Springer, Heidelberg
(2015)
5. Chaum, D.: Blind signatures for untraceable payments. In: Chaum, D., Rivest,
R.L., Sherman, A.T. (eds.) Advances in Cryptology, pp. 199–203. Springer,
New York (1983)
6. Guajardo, J., Kumar, S.S., Schrijen, G.J., Tuyls, P.: FPGA intrinsic PUFs and
their use for IP protection. In: Paillier, P., Verbauwhede, I. (eds.) Cryptographic
Hardware and Embedded Systems - CHES 2007. LNCS, vol. 4727, pp. 63–80.
Springer, Heidelberg (2007)
7. Li, W., Li, H., Chen, H., Xia, Y.: AdAttester: secure online mobile advertisement
attestation using TrustZone. In: Proceedings of MobiSys 2015, pp. 75–88. ACM
(2015)
8. Lim, A.S.: Inter-consortia battles in mobile payments standardisation. Electron.
Commer. Res. Appl. 7(2), 202–213 (2008)
9. Preibusch, S., Peetz, T., Acar, G., Berendt, B.: Purchase details leaked to PayPal
(short paper). In: B¨ohme, R., Okamoto, T. (eds.) FC 2015. LNCS, vol. 8975, pp.
217–226. Springer, Heidelberg (2015)
10. Reaves, B., Scaife, N., Bates, A., Traynor, P., Butler, K.R.B.: Mo(bile) money,
mo(bile) problems: analysis of branchless banking applications in the developing
world. In: Proceedings of the 24th USENIX Conference on Security Symposium
(2015)
11. Rial, A.: Privacy-preserving e-commerce protocols. Ph.D. thesis, Faculty of Engi-
neering Science, KU Leuven, March 2013
12. Suh, G.E., Devadas, S.: Physical unclonable functions for device authentication
and secret key generation. In: 44th ACM/IEEE DAC 2007, pp. 9–14 (2007)
13. Sun, H., Sun, K., Wang, Y., Jing, J.: Trust OTP: transforming smartphones into
secure one-time password tokens. In: Proceedings of CCS 2015, pp. 976–988. ACM
(2015)
14. GlobalPlatform: Tee client API speciﬁcation version 1.0 (2010)
15. Integrated Silicon Solution Inc, IS61LV6416-10TL. http://www.alldatasheet.com/
datasheet-pdf/pdf/505020/ISSI/IS61LV6416-10TL.html
16. ISO/IEC: 15946–5: 2009 Information Technology-Security Techniques: Crypto-
graphic Techniques based on Elliptic Curves: Part 5: Elliptic Curve Generation
(2009)
17. Proxama (2015). http://www.proxama.com/platform/. Accessed 15 Oct 2015
18. Xilinx: Zynq-7000 all programmable soc zc702 evaluation kit. http://www.xilinx.
com/products/boards-and-kits/EK-Z7-ZC702-G.htm
19. Yang, B., Feng, D., Qin, Y.: A lightweight anonymous mobile shopping scheme
based on DAA for trusted mobile platform. In: IEEE TrustCom 2014, pp. 9–17.
IEEE (2014)
20. Yang, B., Yang, K., Qin, Y., Zhang, Z., Feng, D.: DAA-TZ: an eﬃcient DAA scheme
for mobile devices using ARM TrustZone. In: Conti, M., Schunter, M., Askoxylakis,
I. (eds.) TRUST 2015. LNCS, vol. 9229, pp. 209–227. Springer, Heidelberg (2015)
21. Yang, B., Yang, K., Zhang, Z., Qin, Y., Feng, D.: AEP-M: practical anonymous
e-payment for mobile devices using ARM Trust Zone and divisible e-cash (full
version). ePrint (2016)
22. Zhao, S., Zhang, Q., Hu, G., Qin, Y., Feng, D.: Providing root of trust for ARM
trust zone using on-chip SRAM. In: Proceedings of TrustED 2014, pp. 25–36. ACM
(2014)

Universally Composable Two-Server PAKE
Franziskus Kiefer1(B) and Mark Manulis2
1 Mozilla, Berlin, Germany
mail@franziskuskiefer.de
2 Department of Computer Science, Surrey Center for Cyber Security,
University of Surrey, Guildford, UK
mark@manulis.eu
Abstract. Two-Server
Password
Authenticated
Key
Exchange
(2PAKE) protocols apply secret sharing techniques to achieve protec-
tion against server-compromise attacks. 2PAKE protocols eliminate the
need for password hashing and remain secure as long as one of the servers
remains honest. This concept has also been explored in connection with
two-server password authenticated secret sharing (2PASS) protocols for
which game-based and universally composable versions have been pro-
posed. In contrast, universally composable PAKE protocols exist cur-
rently only in the single-server scenario and all proposed 2PAKE proto-
cols use game-based security deﬁnitions.
In this paper we propose the ﬁrst construction of an universally
composable 2PAKE protocol, alongside with its ideal functionality. The
protocol is proven UC-secure in the standard model, assuming a com-
mon reference string which is a common assumption to many UC-secure
PAKE and PASS protocols. The proposed protocol remains secure for
arbitrary password distributions. As one of the building blocks we deﬁne
and construct a new cryptographic primitive, called Trapdoor Distrib-
uted Smooth Projective Hash Function (TD-SPHF), which could be of
independent interest.
1
Introduction
Password Authenticated Key Exchange (PAKE) protocols have been extensively
researched over the last twenty years. They allow two protocol participants shar-
ing a low-entropy secret (password) to negotiate an authenticated secret key.
Several PAKE security models are widely used such as the game-based PAKE
model, called BPR, by Bellare, Pointcheval and Rogaway [4,8] and the PAKE
model in the Universal Composability (UC) framework by Canetti [18]. PAKE
protocols are often considered in a client-server scenario where the client pass-
word is registered and stored in a protected way on the server side such that it
can be used later to authenticate the client. This approach however leads to an
intrinsic weakness of single-server PAKE protocols against server-compromise
attacks. An attacker who breaks into the server can eﬃciently recover client’s
password and impersonate the client to the server as well as to other servers
if this password is used across many client accounts which is often the case. A
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 147–166, 2016.
DOI: 10.1007/978-3-319-45871-7 10

148
F. Kiefer and M. Manulis
number of approaches have been proposed to alleviate this threat. For instance,
veriﬁer-based PAKE [11,22,34], also known as augmented PAKE [9], considers
an asymmetric setting in which the server uses a randomized password hash
to verify a client holding the corresponding password. The crucial weakness of
VPAKE protocols is that they do not protect against oﬄine dictionary attacks
on compromised password hashes, i.e. an attacker can still recover the password,
which can often be done eﬃciently with current tools like [23,31].
Two-server PAKE (2PAKE) protocols solve this problem through secret shar-
ing techniques. The client password is split into two shares and each server
receives its own share upon registration. In order to authenticate the client
both servers take part in the protocol execution. 2PAKE security typically holds
against an active attacker who can compromise at most one server and thus
learn the corresponding password share. 2PAKE protocols can be symmetric
(e.g. [12,27,29,33]) where both servers compute the same session key and asym-
metric (e.g. [27]) where each server can compute an independent session key with
the client or assist another server in the authentication process [26,35] without
computing the key. A potential drawback of symmetric protocols is that by
corrupting one server the attacker may use learned key material to read com-
munications between the client and the other server. Existing 2PAKE protocols
were analysed using variants of the BPR model and do not oﬀer compositional
security guarantees. While 2PAKE can be seen as a special case of Thresh-
old PAKE (TPAKE), e.g. [30,32], that adopt t-out-of-n secret sharing, exist-
ing TPAKE protocols do not necessarily provide solutions for 2PAKE, e.g. [32]
requires t < n/3. Finally, we note that UC-security was considered for a class of
Two-Server/Threshold Password Authenticated Secret Sharing (2/TPASS) pro-
tocols, e.g. [13,14,24], that address a diﬀerent problem of sharing a chosen key
across multiple servers and its subsequent reconstruction from the password.
In this paper we propose the ﬁrst UC-secure (asymmetric) 2PAKE proto-
col where one of the two servers computes an independent session key with the
client. We rely on a common reference string, which is a standard assumption
for UC-secure PAKE protocols. As a consequence of UC modeling our proto-
col oﬀers security for all password distributions, which is notoriously diﬃcult
to achieve in BPR-like models. One challenge in achieving UC security is that
the protocol must remain simulatable against active attackers that play with
a correctly guessed password (unlike in game-based models where simulation
can be aborted). In order to achieve simulatability we introduce a new build-
ing block, called Trapdoor Distributed Smooth Projective Hash Functions (TD-
SPHF), oﬀering distributed SPHF properties from [29] and the SPHF trapdoor
property from [10]. While traditional SPHF were used in the design of single-
server PAKE protocols, the 2PAKE protocol framework from [29], a generalisa-
tion of [27] that was proven secure in the BPR-like model, required an extension
of SPHF to a distributed setting. Such distributed SPHF alone are not suﬃcient
for achieving the UC security. Our TD-SPHF helps to achieve simulatability
for 2PAKE protocols and could be of independent interest for other UC-secure
constructions.

Universally Composable Two-Server PAKE
149
2
Preliminaries and Building Blocks
Our 2PAKE protocol is deﬁned over bilinear groups G1 and G2 of prime order q
with an eﬃciently computable map e : G1 × G2 →GT . The following properties
have to hold: (i) If g1 is a generator of G1 and g2 is a generator of G2, then
e(g1, g2) is a generator of GT . (ii) For generators g1, g2 and scalar x ∈R Zq
it holds that e(gx
1, g2) = e(g1, gx
2) = e(g1, g2)x. We require further that the
Symmetric External Diﬃe-Hellman assumption (SXDH) ([5,6] amongst others)
holds in those groups. SXDH states that the DDH problem is hard in G1 and
G2. All computations deﬁned on a q-order group in the following are performed
in G1. Let λ denote the security parameter throughout this work.
Commitments. By C = (CSetup, Com) we denote an eﬃcient commitment
scheme and use Pedersen commitments in our constructions where (g, h, q, λ) ←
CSetup(λ) and C ←Com = (x; r) = gxhr with g and h being generators of a
cyclic group G of prime-order q with bit-length in the security parameter λ and
where the discrete logarithm of h with respect to base g is not known. Peder-
sen commitments are additively homomorph, i.e. for all (Ci, di) ←Com(xi; ri),
i ∈0, . . . , m we have m
i=0 Ci = Com(m
i=0 xi; m
i=0 ri).
Committed Zero-Knowledge Proofs. We use committed Σ-protocols for
security against malicious veriﬁers [21,25]. Note that we do not require
extractability (proof of knowledge) here, which allows us to avoid the necessity
of rewinding. A zero-knowledge proof ZKP is executed between a prover and a
veriﬁer, proving that a word x is in a language L, using a witness w proving so.1
Let P1(x, w, r) and P2(x, w, r, c) denote the two prover steps of a Σ-protocol and
H : {0, 1}∗→Zq a collision-resistant hash function. A committed Σ-protocol is
then given by the following four steps:
– The prover computes the ﬁrst message Co ←P1(x, w, r) and m1 ←Com(H(x,
Co); r1) = gH(x,Co)hr1, and sends m1 to the veriﬁer.
– The veriﬁer chooses challenge Ch = c ∈R Zq and returns it to the prover.
– The prover computes the second message Rs ←P2(x, w, r, c) and m2 ←
Com(H(Rs); r2) = gH(Rs)hr2, and sends m2 to the veriﬁer.
– Further, the prover opens the commitments m1 and m2 sending (x, Co, Rs, r1,
r2) to the veriﬁer.
– The veriﬁer accepts iﬀboth commitments are valid and if the veriﬁcation of
the Σ-protocol (x, Com, Ch, Rs) is successful.
Cramer-Shoup
Encryption
with
Labels.
Let C
=
(ℓ, u, e, v)
←
EncCS
pk (ℓ, m; r) (on label ℓ, message m, and randomness r) with u = (u1, u2) =
(gr
1, gr
2), e = hrgm
1 and v = (cdξ)r with ξ = Hk(ℓ, u, e) denote a labelled Cramer-
Shoup ciphertext. We assume m ∈Zq and G is a cyclic group of prime order q
1 Zero-knowledge languages L are independent from the smooth projective hashing
languages introduced in Sect. 2.1.

150
F. Kiefer and M. Manulis
with generators g1 and g2 such that gm
1 ∈G. The CS public key is deﬁned as
pk = (p, G, g1, g2, c, d, Hk) with c = gx1
1 gx2
2 , d = gy1
1 gy2
2 , h = gz
1 and hash function
Hk such that τ = (x1, x2, y1, y2, z) denotes the decryption key. Decryption is
deﬁned as gm
1 = DecCS
dk (C) = e/uz
1 if ux1+y1·ξ′
1
ux2+y2·ξ′
2
= v with ξ′ = Hk(ℓ, u, e).
2.1
Smooth Projective Hashing (SPHF)
First, we recall deﬁnitions for classical SPHF tailored to the PAKE use-case
and cyclic groups G of prime-order q. We use languages of ciphertexts with the
password as message and the randomness as witness. An SPHF language L for
a given password pw from dictionary D is given by Lpw. The public parameter
of the language is the common reference string crs containing the public key
pk of the encryption scheme. By τ we denote the crs trapdoor, the secret key
to pk. Let L be the encryption scheme used to generate words in Lpw. Unless
stated otherwise we assume that L is a labelled CCA-secure encryption scheme,
e.g. labelled Cramer-Shoup scheme.
Deﬁnition 1 (Languages of Ciphertexts). Let Lpw ⊆{(ℓ, C, pw∗)} = C
denote the language of labelled ciphertexts under consideration with ciphertext
(ℓ, C) under pk and password pw∗∈D. A ciphertext C is in language Lpw iﬀ
there exists randomness r such that C ←EncL
pk(ℓ, pw; r).
Smooth projective hashing for languages of ciphertexts where the projection
key does not depend on the ciphertext is deﬁned as follows (see also [10,28]).
Deﬁnition 2 (KV-SPHF). Let Lpw denote a language of ciphertexts such that
C ∈Lpw if there exists randomness r proving so. A smooth projective hash
function for ciphertext language Lpw consists of the following four algorithms:
– KGenH(Lpw) generates a random hashing key kh for language Lpw.
– KGenP(kh, Lpw) derives the projection key kp from hashing key kh.
– Hash(kh, Lpw, C) computes hash value h from hashing key kh and ciphertext
C.
– PHash(kp, Lpw, C, r) computes hash value h from projection key kp, ciphertext
C and randomness r.
A SPHF has to fulﬁl the following three properties:
– Correctness: If C ∈L, with r proving so, then Hash(kh, Lpw, C) = PHash(kp,
Lpw, C, r).
– Smoothness: If {(ℓ, C, pw∗)} ∋C ̸∈Lpw, the hash value h is (statistically)
indistinguishable from a random element.
– Pseudorandomness: If C ∈Lpw, the hash value h is (computationally) indis-
tinguishable from a random element.

Universally Composable Two-Server PAKE
151
2.2
Trapdoor Smooth Projective Hashing
For eﬃcient one-round UC-secure PAKE a new SPHF ﬂavor, called Trapdoor
SPHF (T-SPHF), was introduced in [10]. T-SPHF adds three additional func-
tions to the classical SPHF deﬁnition allowing computation of the hash value
from the projection key, ciphertext and trapdoor τ ′.2
Deﬁnition 3 (Trapdoor SPHF). Let Lpw denote a language of ciphertexts
such that C ∈Lpw if there exists randomness r proving so. A trapdoor smooth
projective hash function for a ciphertext language Lpw consists of the following
seven algorithms:
– KGenH, KGenP, Hash and PHash are as given in Deﬁnition 2
– TSetup(crs) generates a second crs′ with trapdoor τ ′ on input of a crs
– VerKp(kp, Lpw) returns 1 iﬀkp is a valid projection key, 0 otherwise
– THash(kp, Lpw, C, τ ′) computes the hash value h of C using the projection key
kp and trapdoor τ ′
We assume crs′ is, like crs, made available to all parties.
2.3
Distributed Smooth Projective Hashing
Another ﬂavor, called Distributed SPHF (D-SPHF), was introduced in [29] for
use in (non-composable) 2PAKE protocols such as [27] where servers hold pass-
word shares pw1 and pw2 respectively, and the client holds pw = pw1 + pw2.
Due to the nature of the words considered in D-SPHF they produce two diﬀerent
hash values. One can think of the two hash values as h0 for C0 (from the client)
and hx for C1, C2 (from the two servers). The hash value h0 can be either com-
puted with knowledge of the client’s hash key kh0 or with the server’s witnesses
r1, r2 that C1, C2 are in Lpwi, i ∈{1, 2} respectively. The hash value hx can be
computed with knowledge of the server hash keys kh1, kh2 or with the client’s
witness r0 that C0 is in Lpw. The combined language is denoted by L
pw.
Deﬁnition 4 (Distributed SPHF). Let L
pw denote a language such that C =
(C0, C1, C2) ∈L
pw if there exists a witness r = (r0, r1, r2) proving so, pw =
pw1 + pw2 and there exists a function Dec′ such that Dec′(C1C2) = Dec′(C0).
A distributed smooth projective hash function for language L
pw consists of the
following six algorithms:
– KGenH(L
pw) generates a hashing key khi for i ∈{0, 1, 2} and language L
pw.
– KGenP(khi, L
pw) derives projection key kpi from hashing key khi for i ∈
{0, 1, 2}.
– Hashx(kh0, L
pw, C1, C2) computes hash value hx from hashing key kh0 and two
server ciphertexts C1 and C2.
– PHashx(kp0, L
pw, C1, C2, r1, r2) computes hash value hx from projection key
kp0, two ciphertexts C1 and C2, and witnesses r1 and r2.
2 Note that τ ′ is a diﬀerent trapdoor than the CRS trapdoor τ.

152
F. Kiefer and M. Manulis
– Hash0(kh1, kh2, L
pw, C0) computes hash value h0 from hashing keys kh1 and
kh2 and ciphertext C0.
– PHash0(kp1, kp2, L
pw, C0, r0) computes hash value h0 from projection keys kp1
and kp2, the ciphertext C0, and witness r0.
A distributed SPHF protocol between three participants C, S1, S2 computing
hx and h0 is described by three interactive protocols Setup, PHashD
x and HashD
0 .
Let Π denote D-SPHF as described above.
– Setup(pw, pw1, pw2, C, S1, S2) initialises a new instance for each participant
with (pw, C, S1, S2) for C, (pw1, S1, C, S2) for S1 and (pw2, S2, C, S1) for
S2. Eventually, all participants compute and broadcast projection keys kpi
and encryptions Ci ←EncL
pk(ℓi, pwi; ri) of their password (share) pwi using
Π.KGenH, Π.KGenP and the associated encryption scheme L. Participants store
incoming kpi, Ci for later use. After receiving (kp1, C1, kp2, C2), the client
computes h0 ←Π.PHash0(kp1, kp2, L
pw, C0, r0) and hx ←Π.Hashx(kh0, L
pw,
C1, C2).
– PHashD
x is executed between S1 and S2. Each server Si performs PHashD
x on
input (kp0, pwi, C1, C2, ri) such that S1 eventually holds hx while S2 learns
nothing about hx.
– HashD
0 is executed between S1 and S2. Each server Si performs HashD
0 on
input (pwi, khi, C0, C1, C2) such that S1 eventually holds h0 while S2 learns
nothing about h0.
2.4
Ideal Functionalities
For our 2PAKE realisation we rely on some commonly used ideal functionalities
within the UC framework. These are: Fcrs for the common reference string from
[17], FCA for the CA from [16] to establish veriﬁed public keys for the servers,
Finit from [7] to establish unique query identiﬁers between the parties in a
protocol. We refer for their descriptions to the original sources.
3
Trapdoor Distributed Smooth Projective Hashing
T-SPHF enabled constructions of one-round UC-secure PAKE [10] because of
simulatability even in presence of attackers who guess correct passwords. In order
to use the trapdoor property for simulatability in 2PAKE protocols T-SPHF
must ﬁrst be extended to the distributed setting of D-SPHF (cf. Sect. 2.3). We
denote this new ﬂavor by TD-SPHF and describe it speciﬁcally for usage in our
2PAKE, i.e. using languages based on Cramer-Shoup ciphertexts. A more general
description of TD-SPHF accounting for more servers and/or other languages can
be obtained similarly to the general description of D-SPHF in [29].
Deﬁnition 5 (TD-SPHF). Let L
pw denote a language such that C
=
(C0, C1, C2) ∈L
pw if there exists a witness r = (r0, r1, r2) proving so, pw =
pw1 + pw2 and there exists a function Dec′ such that Dec′(C1C2) = Dec′(C0). A
trapdoor distributed smooth projective hash function for language L
pw consists
of the following ten algorithms:

Universally Composable Two-Server PAKE
153
– (crs′, τ ′)
R←TSetup(crs) generates crs′ with trapdoor τ ′ from crs
– KGenH, KGenP, Hashx, PHashx, Hash0, PHash0 behave as for D-SPHF
– b ←VerKp(kp, L
pw) returns b = 1 iﬀkp is a valid projection key and b = 0
otherwise
– hx ←THashx(kp0, L
pw, C1, C2, τ ′) computes hash value hx of ciphertexts C1
and C2 using projection key kp0 and trapdoor τ ′
– h0 ←THash0(kp1, kp2, L
pw, C0, τ ′) computes hash value h0 of C0 using projec-
tion keys kp1 and kp2, and trapdoor τ ′
Security of TD-SPHF can be derived from D-SPHF security and the exten-
sions made on SPHF for T-SPHF. However, we do not consider security of
TD-SPHF on its own but rather incorporate it in the security proof of the
2PAKE protocol in the following section. This is due to the fact that description
of TD-SPHF is done only for this speciﬁc application such that a separate secu-
rity deﬁnition is more distracting than giving any beneﬁt. However, we deﬁne
correctness and soundness of TD-SPHF since they diﬀer from that of D-SPHF.
In particular, correctness of TD-SPHF extends correctness of D-SPHF by the
statement that for every valid ciphertext triple (C0, C1, C2), generated by L, and
honestly generated keys (kh0, kh1, kh2) and (kp0, kp1, kp2), it holds not only that
Hash0(kh1, kh2, L
pw, C0) = PHash0(kp1, kp2, Lpw,pw1,pw2, C0, r0), and
Hashx(kh0, L
pw, C1, C2) = PHashx(kp0, Lpw,pw1,pw2, C1, C2, r1, r2)
but also that VerKp(kpi, L
pw) = 1 for i ∈{0, 1, 2} and
Hash0(kh1, kh2, L
pw, C0) = THash0(kp1, kp2, Lpw,pw1,pw2, C0, τ ′) and
Hashx(kh0, L
pw, C1, C2) = THashx(kp0, Lpw,pw1,pw2, C1, C2, τ ′).
To capture soundness of TD-SPHFs we deﬁne (t, ε)-soundness, complementing
the previous correctness extension, as follows.
Deﬁnition 6 (TD-SPHF(t, ε)-soundness). Given crs, crs′ and τ, no adver-
sary running in time at most t can produce a projection key kp, a password pw
with shares pw1 and pw2, a word (C0, C1, C2), and valid witness (r0, r1, r2), such
that (kp0, kp1, kp2) are valid, i.e. VerKp(kpi, L
pw) = 1 for i ∈{0, 1, 2}, but
THashx(kp0, L
pw, C1, C2, τ ′) ̸= PHashx(kp0, L
pw, C1, C2, r1, r2) or
THash0(kp1, kp2, L
pw, C0, τ ′) ̸= PHash0(kp1, kp2, L
pw, C0, r0)
with probability at least ε(λ). The perfect soundness states that the property holds
for any t and any ε(λ) > 0.
3.1
Cramer-Shoup TD-SPHF
In the following we present TD-SPHF for labelled Cramer-Shoup ciphertexts
by extending the corresponding D-SPHF from [29] with the trapdoor property
from [10] in the setting of bilinear groups. Let C = (ℓ, u1, u2, e, v) denote a
Cramer-Shoup ciphertext as deﬁned in Sect. 2.

154
F. Kiefer and M. Manulis
– TSetup(crs) draws a random τ ′ ∈R Zq and computes crs′ = ζ = gτ ′
2
– KGenH(L
pw) returns khi = (η1,i, η2,i, θi, μi, νi) ∈R Z1×5
p
for i ∈{0, 1, 2}
– KGenP(khi, L
pw) generates
kpi = (kp1,i = gη1,i
1,1 gθi
1,2hμicνi, kp2,i = gη2,i
1,1 dνi, kp3,i)
with kp3,i = (χ1,1,i, χ1,2,i, χ2,i, χ3,i, χ4,i) and
χ1,1,i = ζη1,i, χ1,2,i = ζη2,i, χ2,i = ζθi, χ3,i = ζμi, χ4,i = ζνi for i ∈{0, 1, 2}
– Hashx(kh0, L
pw, C1, C2) computes
h′
x = (u1,1 · u1,2)η1,0+(ξ1+ξ2)η2,0(u2,1 · u2,2)θ0((e1 · e2)/gpw
1,1)μ0(v1 · v2)ν0
and returns hx = e(h′
x, g2)
– PHashx(kp0, L
pw, C1, C2, r1, r2) computes h′
x = kpr1+r2
1,0
kp
ξ1r1+ξ2r2
2,0
and outputs
hx = e(h′
x, g2)
– Hash0(kh1, kh2, L
pw, C0) computes
h′
0 = uη1,1+η1,2+ξ0(η2,1+η2,2)
1,0
uθ1+θ2
2,0
(e0/gpw
1,1)μ1+μ2vν1+ν2
0
and outputs h0 = e(h′
0, g2)
– PHash0(kp1, kp2, L
pw, C0, r0) computes
h′
0 = (kp1,1kp1,2)r0(kp2,1kp2,2)r0ξ0
and outputs h0 = e(h′
0, g2)
– VerKp(kpi, L
pw) veriﬁes that
e(kp1,i, crs′)
?= e(g1,1, χ1,1,i) · e(g1,2, χ2,i) · e(h, χ3,i) · e(c, χ4,i)
and
e(kp2,i, crs′)
?= e(g1,1, χ1,2,i) · e(d, χ4,i) for i ∈{0, 1, 2}
– THash0(kp1, kp2, L
pw, C0, τ ′) computes
h0 =

e(u1,0, χ1,1,1χ1,1,2(χ1,2,1χ1,2,2)ξ0) · e(u2,0, χ2,1χ2,2)
·e(e0/gpw
1,1, χ3,1χ3,2) · e(v0, χ4,1χ4,2)
1/τ ′
– THashx(kp0, L
pw, C1, C2, τ ′) computes
hx =

e(u1,1u1,2, χ1,1,0χξ1+ξ2
1,2,0 ) · e(u2,1u2,2, χ2,0) · e((e1e2)/gpw
1,1, χ3,0)
·e(v1v2, χ4,0)
1/τ ′

Universally Composable Two-Server PAKE
155
Distributed computation of PHashx and Hash0 is done as in D-SPHF with
additional proofs for correctness and adding the pairing computation at the
end to lift the hash value into GT . We formalise execution of the Cramer-
Shoup TD-SPHF in the following paragraph. Necessary zero-knowledge proofs
are described in the subsequent two paragraphs and only referenced in the
description of the TD-SPHF. We describe the Σ protocol here, which we can
use after transforming it to a committed Σ protocol (cf. Sect. 2). Note that we
merge crs and crs′ here for readability. Protocol participants are denoted C, S1
and S2 if their role is speciﬁed, or P, Q and R otherwise. Let further 0 denote
the client’s index and 1, 2 the indices of servers S1, S2, respectively. The session
ID is given by sid = C||S1||S2 and the unique query identiﬁer qid is agreed
upon start using Finit.
All TD-SPHF participants have crs = (q, g1,1, g1,2, h, c, d, G1, g2, ζ, G2, GT , e,
Hk) as common input where τ = (x1, x2, y1, y2, z) is the crs trapdoor, i.e. the
according Cramer-Shoup secret key, and τ ′ the trapdoor, i.e. discrete logarithm
to base g2, of crs′ = ζ. Each server holds an ElGamal key pair (pk1, dk1) and
(pk2, dk2) respectively such that pk1 is registered with the CA for S1 and pk2
for S2 and thus available to all parties (using FCA). An, otherwise unspeciﬁed,
protocol participant P is initiated with (NS, sid, qid, P, x). We further deﬁne
pw0 = pw.
CS TD-SPHF Computation
(a) Generate TD-SPHF keys khi ∈R Z5
q and kpi = (kp1,i = gη1,i
1,1 gθi
1,2hμicνi,
kp2,i = gη2,i
1,1 dνi, χ1,1,i = ζη1,i, χ1,2,i = ζη2,i, χ2,i = ζθi, χ3,i = ζμi, χ4,i = ζνi).
Encrypt pwi to C = (ℓi, u1,i, u2,i, ei, vi) ←(ℓ, gri
1,1, gri
1,2, hrrgpwi
1,1 , (cdξi)ri)
with ξi = Hk(ℓi, u1,i, u2,i, ei) for ℓi = sid||qid||kpi and ri ∈R Zq. If P = S1,
set h0 = hx = null. Output (sid, qid, 0, P, Ci, kpi) to Q and R.
(b) When P, waiting for the initial messages, is receiving a message (sid, qid, 0,
Q, C1, kp1) and (sid, qid, 0, R, C2, kp2) it proceeds as follows. P proceeds
only if the projection keys kp1 and kp2 are correct, i.e. VerKp(kp1, L
pw) = 1
and VerKp(kp2, L
pw) = 1. If the veriﬁcation fails, P outputs (sid, qid, ⊥, ⊥)
and aborts the protocol.
(i) If P = C, compute
hx = e((u1,1 · u1,2)η1,0+(ξ1+ξ2)η2,0(u2,1 · u2,2)θ0
((e1 · e2)/gpw
1,1)μ0(v1 · v2)ν0, g2) and
h0 = e

(kp1,1kp1,2)r0(kp2,1kp2,2)r0ξ0, g2

, and outputs (sid, qid, h0, hx).
(ii) If P = S2, compute hx,2 = (kp1,0 · kp
ξ2
2,0)r2 and Chx,2 = gH(hx,2,Co1)
1,1
hrc1
with rc1 ∈R Zq and send (sid, qid, PHashx, 0, S2, Chx,2) to S1.
(iii) If P = S1, compute m0 = EncEG
pk1(g−μ1
1,1 ; r) and c0 = EncEG
pk1(gpw1
1,1 ; r′) with
r, r′ ∈R Zq, and send (sid, qid, Hash0, 0, S1, m0, c0) to S2.
(c) On input (sid, qid, PHashx, 0, S2, Chx,2) S1 in the correct state draws chal-
lenge c ∈R Zq and returns (sid, qid, PHashx, 1, S1, c) to S2.

156
F. Kiefer and M. Manulis
(d) On input (sid, qid, PHashx, 1, S1, c) S2 in the correct state computes
Cshx,2 = gH(Rs1)
1,1
hrc2 with rc2 ∈R Zq and sends (sid, qid, PHashx, 2, S2,
Cshx,2 ) to S1. Subsequently, it sends (sid, qid, PHashx, 3, S2, hx,2, Co1,
Rs1, rc1, rc2) to S1.
(e) On input (sid, qid, PHashx, 2, S2, Cshx,2 ) S1 in the correct state stores it and
waits for the ﬁnal PHashx message.
(f) On input (sid, qid, PHashx, 3, S2, hx,2, Co1, Rs1, rc1, rc2) S1 in the correct
state parses Co1 as (t1, t2) and Rs2 as shx,2 and veriﬁes correctness of com-
mitments and the ZKP and computes hx = e

hx,2 · (kp0,1 · kp
ξ1
0,2)r1, g2

if
the veriﬁcations are successful, hx ̸= ⊥and h0 ̸= ⊥, or sets h0 = ⊥and
hx = ⊥otherwise.
(g) On input (sid, qid, Hash0, 0, S1, m0, c0) S2 in the correct state retrieves pk1
from FCA and computes CHash0,1 = gH(m1,m2,Co2)
1,1
hrc3 with rc3 ∈R Zq, m1 ←
mpw2
0
× c−μ2
0
× EncEG
pk1(g−μ2·pw2
1,1
· uη1,2+ξ0η2,2
1,0
· uθ2
2,0 · eμ2
0 · vν2
0 ; r′′), and m2 ←
EncEG
pk1(g−μ2
1,1 ; r′′′) with r′′, r′′′ ∈Zq, and sends (sid, qid, Hash0,1, S2, CHash0,1)
back to S1.
(h) On input (sid, qid, Hash0,1, S2, CHash0,1) S1 in the correct state draws chal-
lenge c ∈R Zq and returns (sid, qid, Hash0,2, S1, c) to S2.
(i) On input (sid, qid, Hash0,2, S1, c) S2 in the correct state computes CRs2 =
gH(Rs2)
1,1
hrc4 with rc4 ∈R Zq and sends (sid, qid, Hash0,3, S2, CRs2) to S1.
Subsequently, it sends (sid, qid, Hash0,4, S2, m1, m2, Co2, Rs2, rc3, rc4) to S1.
(j) On input (sid, qid, Hash0,4, S2, m1, m2, Co2, Rs2, rc3, rc4) S1 in the correct
state parses Co2 as (tm1, tm2, te2, tv2, tkp12, tkp22) and Rs2 as (spw2, sμ2, sη12,
sη22, sθ2, sν2, sr2), veriﬁes correctness of commitments and ZKP, and com-
putes h0 = e

g−μ1·pw1
1,1
· DecEG
dk1(m1) · uη1,1+ξ0η2,1
1,0
· uθ1
2,0 · eμ1
0 · vν1
0 , g2

if the
veriﬁcations are successful, hx ̸= ⊥and h0 ̸= ⊥, or sets h0 = ⊥and hx = ⊥.
(k) Eventually S1 outputs (sid, qid, h0, hx) if h0 ̸= null and hx ̸= null.
ZK Proof for PHashx Correctness In order to ensure correct computation of
hx on S1 server S2 has to prove correctness of his computations. To this end
S2 sends, in addition to the PHashx message hx,2 the following zero-knowledge
proof.
ZKP

(r2) :
hx,2 = (kp1,0kp
ξ2
2,0)r2 ∧v2 = (cdξ2)r2	
(1)
where r2 is the randomness used to create C2, ξ2 and v2 are part of C2, kp1,0, kp2,0
are part of C’s projection key, and c, d are from the crs. The construction of
the according zero-knowledge proof is straight-forward. The prover computes
commitments
thx2 = (kp1,0kp
ξ2
2,0)khx2;
tv2 = (cdξ2)khx2
with fresh randomness khx2 ∈R Zq, and response sr2 = khx2 −cr2 for veriﬁer
provided challenge c. This allows the veriﬁer to check
thx2
?= hc
x,2(kp1,0kp
ξ2
2,0)shx2;
tv2
?= vc
2(cdξ2)shx2.

Universally Composable Two-Server PAKE
157
It is easy to see that this zero-knowledge proof is correct, sound and (honest-
veriﬁer) simulatable. We refer to the messages as Co1 = (thx2, tv2), Rs1 = sr2,
and Ch1 = c.
ZK Proof for Hash0 Correctness Let m1 and m2 denote the messages
encrypted in m1 and m2 respectively and m0,1 and c0,1 the second part (e)
of the ElGamal ciphertext m0, c1 respectively. In order to ensure correct com-
putation of h0 on S1 server S2 has to prove correctness of his computations. To
this end S2 sends, additionally to the Hash0 messages m1 and m2 the following
zero-knowledge proof
ZKP

(x, η1,2, η2,2, θ2, μ2, ν2, r2) : m1 = mpw2
0,1 c−μ2
0,1 g−μ2x
1,1
uη1,2+ξ0η2,2
1,0
uθ2
2,0eμ2
0 vν2
0
∧m2 = g−μ2
1,1
∧e2 = hr2gpw2
1,1
∧v2 = (cdξ2)r2
∧kp1,2 = gη1,2
1,1 gθ2
1,2hμ2cν2 ∧kp2,2 = gη2,2
1,1 dν2	
,
(2)
where r2 is the randomness used to create C2, ξ2 and v2 are part of C2, ξ0 is part
of C0, (μ2, η1,2, η2,2, θ2, ν2) is S2’s hashing key, pw2 S2’s password share, and c, d
are from the crs. The construction of the according Σ proof is straight-forward.
The prover computes commitments
tm1 = mpw2
0,1 ckμ2
0,1 mkx
2 ukη12+ξ0kη22
1,0
ukθ2
2,0 e−kμ2
0
vkν2
0
;
tm2 = gkμ2
1,1 ;
te2 = hkr2gpw2
1,1 ;
tv2 = (cdξ2)kr2;
tkp12 = gkη12
1,1 gkθ2
1,2 hkμ2ckν2;
tkp22 = gkη22
1,1 dkν2
for kpw2, kμ2, kη12, kη22, kθ2, kν2 ∈R Zq
and responses
spw2 = kpw2 −cpw2;
sμ2 = kμ2 + cμ2;
sη12 = kη12 −cη1,2;
sη22 = kη22 −cη2,2;
sθ2 = kθ2 −cθ2;
sν2 = kν2 −cν2;
sr2 = kr2 −cr2
for veriﬁer provided challenge c. This allows the veriﬁer to check
tm1
?= mc
1m
spw2
0,1
c
sμ2
0,1 m
spw2
2
u
sη12+ξ0sη22
1,0
usθ2
2,0 e
sμ2
0
vsν2
0
;
tm2
?= mc
2g
sμ2
1,1 ;
te2
?= ec
2hsr2g
spw2
1,1
;
tv2
?= vc
2(cdξ2)sr2;
tkp12
?= kpc
1,2g
sη12
1,1 gsθ2
1,2 hsμ2csν2;
tkp22
?= kpc
2,2g
sη22
1,1 dsν2.
While this is mainly a standard zero-knowledge proof tm1 uses m2 instead of
g1,1 as base for the third factor and kpw2 as exponent (spw2 in the veriﬁca-
tion). This is necessary due to the fact that the exponent −μ2pw2 of the third
factor in m1 is a product of two values that have to be proven correct. The
ZK proof uses the auxiliary message m2 to prove that logg1,1(m2) = −μ2 such
that it is suﬃcient to prove logm2(mpw2
2
) = pw2. We refer to the messages
as Co2 = (tm1, tm2, te2, tv2, tkp12, tkp22), Rs2 = (spw2, sμ2, sη12, sη22, sθ2, sν2, sr2),
and Ch2 = c.

158
F. Kiefer and M. Manulis
4
Universally Composable Two-Server PAKE
With TD-SPHF it is straight forward to build a 2PAKE protocol. We follow the
general framework described in [29] to build 2PAKE protocols from distributed
smooth projective hash functions. However, instead of aiming for key generation,
where the client establishes a key with each of the two servers, we focus on a
protocol that establishes a single key with one server, w.l.o.g. the ﬁrst server. By
running the protocol twice, keys can be exchanged between the client and the
second sever. Note that UC security allows concurrent execution of the protocol
such that round complexity is not increased by establishing two keys.
4.1
The Protocol
We obtain our 2PAKE protocol using the general 2PAKE framework from [29]
yet using our TD-SPHF instead of original D-SPHF. Client C and both servers
S1 and S2 execute a TD-SPHF protocol from Sect. 3 which provides C and S1
with two hash values h0 and hx each. The session key is then computed by both
as a product sk = h0 · hx.
4.2
Ideal Functionality for 2PAKE
Our ideal functionality for 2PAKE with implicit client authentication, F2PAKE,
is given in Fig. 1. Observe that implicit client authentication is suﬃcient for
building UC-secure channels [19]. The ideal adversary can take control of any
server from the outset of the protocol and learn the corresponding password
share. The actual password remains hidden unless the adversary corrupts both
servers. The use of static corruptions is motivated in the following. First, as
explained in [18], PAKE security against static corruptions in the UC model
implies security against adaptive corruptions in the BPR model. Second, existing
single-server PAKE protocols that are UC-secure against adaptive corruptions,
e.g. [1–3], rely on more complex SPHF constructions that are not translatable
to the distributed setting of D-SPHF.
2PAKE Functionality. Our F2PAKE is very similar to single-server PAKE func-
tionality but assumes two servers from which one generates a session key. The
main diﬀerence is in the modelling of participants. We specify two initialisa-
tion interfaces KEX Init, one for the client and one for the servers. A client
is initialised with a password pw while a server gets a password share αb. The
TestPwd interface allows the ideal world adversary to test client passwords. A
tested session is marked interrupted if the guess is wrong, i.e. client and server
in this session receive randomly chosen, independent session keys, or marked as
compromised if the password guess is correct, i.e. the attacker is now allowed to
set the session key. The attacker can only test client passwords but not password
shares of the servers. Without knowledge of the password or any password share,
a share is a uniformly at random chosen element and therefore not eﬃciently
guessable. If the adversary corrupted server S2, retrieving the second password

Universally Composable Two-Server PAKE
159
Functionality F2PAKE
The functionality F2PAKE is parameterised by a security parameter λ. It
interacts with an adversary, a client C and two servers S1 and S2 via the
following interfaces. Without loss of generality the key is exchanged between
C and S1.
KEX InitC: Upon input (KEXinit, sid, qid, pw) from client C, check that
sid is (C, S1, S2) and that qid is unique (entries (KEX, sid, qid, S1, α1)
or (KEX, sid, qid, S2, α2) may exist) and send (KEX, sid, qid, C) to SIM.
If this is a valid request, create a fresh record (KEX, sid, qid, C, pw).
KEX InitS: Upon input (KEXinit, sid, qid, αb) from server Sb, b
∈
{1, 2}, check that sid is (C, S1, S2) and that qid is unique (entries
(KEX, sid, qid, C, pw) or (KEX, sid, qid, S3−b, α3−b) may exist) and send
(KEX, sid, qid, Sb) to SIM. If this is a valid request, create a fresh record
(KEX, sid, qid, Sb, αb).
TestPwd: Upon
input
(TP, sid, qid, pw )
from
SIM
check
that
a
fresh record (KEX, sid, qid, C, pw) exists. If this is the case, mark
(KEX, sid, qid, S1, α1) as compromised and reply with “correct guess” if
pw = pw , and mark it as interrupted and reply with “wrong guess”
if pw = pw .
Failed: Upon
input
(FA, sid, qid)
from
SIM
check
that
records
(KEX, sid, qid, C, pw) and (KEX, sid, qid, S1, α1) exist that are not
marked completed. If this is the case, mark both as failed.
NewKey: Upon input (NK, sid, qid, P, sk ) from SIM with P ∈{C, S1},
check that a respective (KEX, sid, qid, C, pw) or (KEX, sid, qid, S, α1)
record exists, sid = (C, S1, S2), |sk | = λ, then:
– If the session is compromised, or either C or S1 and S2 are cor-
rupted, then output (NK, sid, qid, sk ) to P; else
– if the session is fresh and a key sk was sent to P
with sid =
(P, P , S2) or sid = (P , P, S2) while (KEX, sid, qid, P , ·) was fresh,
then output (NK, sid, qid, sk) to P.
– In any other case, pick a new random key sk of length λ, and send
(NK, sid, qid, sk) to P.
In any case, mark qid as completed for P.
Fig. 1. Ideal functionality F2PAKE
share α1 from S1 is equivalent to guessing the password. Complementing the
TestPwd interface is a Failed interface that allows the adversary to let sessions
fail. This allows the attacker to prevent protocol participants from computing
any session, i.e. failed parties do not compute a session key. Eventually the
NewKey interface generates session keys for client C and server S1. NewKey
calls for S2 are ignored. If client C or server S1 and S2 are corrupted, or the
attacker guessed the correct password, the adversary chooses the session key. If

160
F. Kiefer and M. Manulis
a session key was chosen for the partnered party and the session was fresh at
that time, i.e. not compromised or interrupted, the same session key is used
again. In any other case a new random session key is drawn.
Instead of using a single session identiﬁer sid we use sid and qid. The session
identiﬁer sid is composed of the three participants (C, S1, S2) (note that we use
the client C also as “username” that identiﬁes its account on the servers) and
therefore human memorable and unique. To handle multiple, concurrent 2PAKE
executions of one sid, we use a query identiﬁer qid that is unique within sid
and can be established with Finit. In the multi-session extension 
F2PAKE the
sid becomes ssid and sid is a globally unique identiﬁer for the used universe,
i.e. server public keys (CA) and crs.
4.3
Security
The following theorem formalises the security of the proposed 2PAKE protocol.
Note that we do not rely on any security of the TD-SPHF. Instead we reduce
the security of our 2PAKE protocol directly to the underlying problem (SXDH).
Thereby, we give an indirect security proof of the proposed TD-SPHF.
Theorem 1. The 2PAKE protocol from Sect. 4.1 securely realises 
F2PAKE with
static corruptions in the Fcrs-FCA-hybrid model if the DDH assumption holds in
both groups G1 and G2 and if Hk is a universal one-way hash function.
Proof (Sketch). In the following we highlight changes in the sequence of games
from the real-world execution in G1 to the ideal-world execution via F2PAKE in
G17 and describe the ideal-world adversary SIM. Due to space limitations the
analysis of game hops is available in the full version.
G1 : Game 1 is the real-world experiment in which Z interacts with real partic-
ipants that follow, if honest, the protocol description, and the real-world adver-
sary A controlling the corrupted parties.
G2 : In this game all honest participants are replaced by a challenger C that
generates crs together with its trapdoor τ and interacts with A on behalf of
honest parties.
G3 : When C, on behalf of S1, receives ﬁrst messages (C0, kp0) and (C2, kp2), it
decrypts C0 to pw′ and checks if this is the correct password, i.e. pw′ = pw. If
this is not the case, pw′ ̸= pw, C chooses a random h′
0 ∈R GT if the subsequent
Hash0 computation with S2 is successful, i.e. all zero-knowledge proofs can be
veriﬁed, and aborts S1 otherwise.
G4 : In this game C chooses sk ∈R GT at random if h0 was chosen at random
(as in G0) and computation of sk on S1 is successful.
G5 : Upon receiving an adversarially generated C1 or C2 on behalf of client C,
challenger C chooses hx ∈R GT uniformly at random instead of computing it
with Hashx if C1 or C2 do not encrypt the correct password share pw1 or pw2
respectively.

Universally Composable Two-Server PAKE
161
G6 : In this game C chooses sk ∈R GT at random if hx was chosen at random
(as in G0) and computation of sk on C is successful, i.e., projection keys kp1 and
kp2 are correct.
G7 : C replaces computation of hash values h0 and hx with a lookup table with
index (kh1, kh2, Lpw,pw2,pw2, C0) for h0 and (kh0, Lpw,pw2,pw2, C1, C2) for hx. If no
such value exists, it is computed with the appropriate Hash or PHash function
and stored in the lookup table.
G8 : Instead of computing Hash0 for S1 in case pw′ decrypted from C0 is the
same as pw, C draws a random h0 ∈R GT .
G9 : In this game C chooses sk ∈R GT at random in case h0 was chosen at
random (as in G0) and computation of sk on S1 is successful.
G10 : Upon receiving correct C1 or C2, i.e. encrypting pw1 and pw2 respectively,
on behalf of client C, challenger C chooses hx ∈R GT uniformly at random
instead of computing it with Hashx.
G11 : In this game C chooses sk ∈R GT at random in case h0 was chosen random
(as in G0) and computation of sk on C is successful (projection keys kp1 and kp2
are correct).
G12 : The entire crs including ζ is chosen now by challenger C.
G13 : Upon receiving C1 and C2, encrypting correct password shares, C uses
THash0 to compute h0 on client C instead of PHash0. This is possible because C
now knows trapdoor τ ′.
G14 : Upon receiving C0, encrypting correct password, C uses THashx to com-
pute hx on server S1 instead of PHashx. This is possible because C now knows
trapdoor τ ′.
G15 : Instead of encrypting the correct password pw in C0 on behalf of client C,
C encrypts 0 (which is not a valid password).
G16 : Instead of encrypting the correct password share pwi in Ci on behalf of
server Si with i ∈[1, 2], C encrypts a random element pw′
i ∈R Zq.
G17 : This is the ﬁnal game where instead of the challenger C the simulation
is done by the ideal-world adversary (simulator) SIM that further interacts with
the ideal functionality F2PAKE. While this game is structurally diﬀerent from G0
the interaction with A is indistinguishable from the latter. This combined with
the following description of the simulator concludes the proof.
Simulator. We describe SIM for a single session sid = (C, S1, S2). The security
then follows from the composition theorem [15] covering multiple sessions and
from the joint-state composition theorem [20], covering creation of a joint state
by FCA and Fcrs for all sessions and participants. As before, we assume that 0 is
not a valid password.
First, SIM generates crs = (q, g1,1, g1,2, h, c, d, G1, g2, ζ, G2, GT , e, Hk) with
Cramer-Shoup secret key as trapdoor τ = (x1, x2, y1, y2, z) and second trapdoor

162
F. Kiefer and M. Manulis
τ ′ for ζ = gτ ′
2 to answer all Fcrs queries with crs. Further, SIM generates ElGa-
mal key pairs (gz1, z1) and (gz2, z2), and responds to Retrieve(Si) queries to FCA
from Si with (Retrieve, Si, (gzi, zi)) for i ∈{1, 2} and with (Retrieve, Si, gzi)
to all other request.
When receiving (KEX, sid, qid, P) with sid = (C, S1, S2) and P ∈{C, S1, S2}
from F2PAKE, SIM starts simulation of the protocol for party P by computing
Mi = (Ci, kpi) for i ∈{0, 1, 2} and encrypting a dummy value (0 for P = C and
a random value α′
i ∈R Zq for P = Si, i ∈{1, 2}). SIM outputs (Ci, kpi) to A.
The ﬁrst round of messages is handled as follows.
(i) When a party receives an adversarially generated but well formed ﬁrst
message Mi, i ∈{1, 2} from uncorrupted Si, i.e. VerKp on the projection
key kpi is 1, SIM queries (FA, sid, qid), which marks the session failed for
the receiving party and thus ensures that the party receives an independent,
random session key (if any) on a NewKey query.
(ii) When a party receives an adversarially generated but well formed ﬁrst
message M2 from a corrupted S2 while S1 is not corrupted, SIM decrypts
C2 to α′
2. If this value is not correct, α′
2 ̸= α2 (the party is corrupted such
that SIM knows the correct value), SIM queries (FA, sid, qid) to ensure
independent session keys on NewKey queries.
(iii) When client C receives an adversarially generated but well formed ﬁrst
message M1 from a corrupted S1 while S2 is not corrupted, SIM decrypts
C1 to α′
1. If this value is not correct, α′
1 ̸= α1, SIM queries (FA, sid, qid) to
ensure independent session keys on NewKey queries.
(iv) When a party receives adversarially generated but well formed ﬁrst mes-
sages M1, M2 from corrupted S1, S2, SIM decrypts C1 and C2 to α′
1, α′
2
respectively, and veriﬁes their correctness against α1 and α2. If they are
correct, SIM computes h0 ←THash0(kp1, kp2, Lpw,pw1,pw2, C0, τ ′), hx ←
Hashx(kp0, L
pw, C1, C2), and skC = h0 · hx. Otherwise choose a random
skC ∈GT .
(v) When an honest S1 or S2 receives an adversarially generated but well
formed ﬁrst message M0, i.e. VerKp on kp0 is true, SIM extracts pw′ from C0
and sends (TP, sid, qid, C, pw′) to F2PAKE. If F2PAKE replies with “correct
guess”, SIM uses pw′, crs and τ ′ to compute hx ←THashx(kp0, L
pw, C1,
C2, τ ′), h0 ←Hash0(kh1, kh2, Lpw,pw1,pw2, C0), and skS = h0 · hx.
(vi) If veriﬁcation of any kpi fails at a recipient, SIM aborts the session for the
receiving participant.
If a party does not abort, SIM proceeds as follows. After C received all cipher-
texts and projection keys and the previously described checks were performed
SIM sends (NK, sid, qid, C, skC) to F2PAKE if skC for this session exists, or
(NK, sid, qid, C, ⊥) otherwise. After S1 and S2 received all ciphertexts and pro-
jection keys and the previously described checks were performed, SIM simulates
PHashx and Hash0 computations between S1 and S2 with random elements and
simulated zero-knowledge proofs. If all messages received by S1 are oracle gener-
ated, SIM sends (NK, sid, qid, S1, skS) to F2PAKE if this session is compromised

Universally Composable Two-Server PAKE
163
and (NK, sid, qid, S1, ⊥) if not. If any PHashx or Hash0 message received by S1
can not be veriﬁed, SIM does nothing and aborts the session for S1.
5
F2PAKE Discussion
F2PAKE and the BPR 2PAKE Model. While other security models for
2PAKE protocols where proposed [33], the BPR-like security model from [27] is
the most comprehensible and (in its two-party version) established model. To
compare security of a 2PAKE protocol Π in a game-based and UC setting we
have to ensure that it supports session ids (necessary in the UC framework). We
therefore assume that Π already uses UC compliant session ids. Before looking
into the relation between the game-based model for 2PAKE and F2PAKE we
want to point out that Π, securely realising F2PAKE, oﬀers “forward secrecy”,
i.e. even an adversary that knows the correct password is not able to attack an
execution of Π without actively taking part in the execution. With this in mind
it is easy to see that Π, securely realising F2PAKE, is secure in the BPR-like
model from [27]. This is because the attacker is either passive, which is covered
by the previous observation, or is active and is therefore able tests one password.
Those password tests (TestPwd in F2PAKE and Send in the game based model)
give the attacker a success probability of q/|D|, with q the number of active
sessions and |D| the dictionary size, when considering a uniform distribution of
passwords inside the dictionary D.
F2PAKE and FPAKE. While FPAKE and F2PAKE are very similar they contain
some signiﬁcant diﬀerence we want to point out here. First, the key-exchange is
performed between all three participants, but only C and, w.l.o.g., S1 agree on
a common session key. The role is a technical necessity in FPAKE for correct
execution. Since we have explicit roles in F2PAKE this is not necessary here.
Due to the asymmetry in F2PAKE (a client negotiates with two servers) we
assume that the client is always the invoking party. The asymmetric setting in
F2PAKE further restricts TestPwd queries to the client since the servers hold
high entropy password shares. While it is enough for the attacker to corrupt one
party in FPAKE to control the session key, in F2PAKE he has to either corrupt
or compromise the client, or corrupt both servers. As long as only one server
is corrupted, the adversary has no control over the session keys and the parties
receive uniformly at random chosen session keys In F2PAKE session ids are human
memorisable, consisting of all three involved parties (C, S1, S2), and unique query
identiﬁer is used to distinguish between diﬀerent (possibly concurrent) protocol
runs of one account (sid). This is a rather technical diﬀerence to FPAKE that
uses only session identiﬁers.
Corruptions. The two-server extension of the BPR 2PAKE model used in
[27] does not consider corruptions at all. While parties can be malicious in the
model (static corruption), the attacker is not allowed to query a corrupt ora-
cle to retrieve passwords or internal state of participants. In our model the

164
F. Kiefer and M. Manulis
attacker is allowed to corrupt parties before execution. This however implies
security in the model from [27] even if the attacker is allowed to corrupt clients
to retrieve their passwords. This is because the environment can provide the
BPR attacker with the password. However, this does not increase his success
probability. Dynamic corruptions in F2PAKE on the other hand are much more
intricate. While UC-secure two party PAKE protocols with dynamic corrup-
tions exist, their approaches are not translatable to the 2PAKE setting. The
challenge of dynamic corruptions is that the simulation has to be correct even
if the attacker corrupts one party after the protocol execution has started. This
is left open for future work.
6
Conclusion
This paper proposed the ﬁrst UC-secure 2PAKE and introduced Trapdoor Dis-
tributed Smooth Projective Hashing (TD-SPHF) as its building block. The pro-
posed 2PAKE protocol uses a common reference string and the SXDH assump-
tion on bilinear groups and is eﬃcient thanks to the simulatability of TD-SPHF.
References
1. Abdalla, M., Benhamouda, F., Blazy, O., Chevalier, C., Pointcheval, D.: SPHF-
friendly non-interactive commitments. In: Sako, K., Sarkar, P. (eds.) ASIACRYPT
2013, Part I. LNCS, vol. 8269, pp. 214–234. Springer, Heidelberg (2013)
2. Abdalla, M., Benhamouda, F., Pointcheval, D.: Removing Erasures with Explain-
able Hash Proof Systems. Cryptology ePrint Archive, Report 2014/125 (2014)
3. Abdalla, M., Chevalier, C., Pointcheval, D.: Smooth projective hashing for condi-
tionally extractable commitments. In: Halevi, S. (ed.) CRYPTO 2009. LNCS, vol.
5677, pp. 671–689. Springer, Heidelberg (2009)
4. Abdalla, M., Fouque, P.-A., Pointcheval, D.: Password-based authenticated key
exchange in the three-party setting. In: Vaudenay, S. (ed.) PKC 2005. LNCS, vol.
3386, pp. 65–84. Springer, Heidelberg (2005)
5. Ateniese, G., Camenisch, J., Hohenberger, S., de Medeiros, B.: Practical group
signatures without random oracles. Cryptology ePrint Archive, 2005:385 (2005)
6. Ballard, L., Green, M., de Medeiros, B., Monrose, F.: Correlation-resistant storage
via keyword-searchable encryption. Cryptology ePrint Archive, 2005:417 (2005)
7. Barak, B., Lindell, Y., Rabin, T.: Protocol Initialization for the Framework of
Universal Composability. Cryptology ePrint Archive, 2004:6 (2004)
8. Bellare, M., Pointcheval, D., Rogaway, P.: Authenticated key exchange secure
against dictionary attacks. In: Preneel, B. (ed.) EUROCRYPT 2000. LNCS, vol.
1807, pp. 139–155. Springer, Heidelberg (2000)
9. Bellovin, S.M., Merritt, M.: Augmented encrypted key exchange: a password-based
protocol secure against dictionary attacks and password ﬁle compromise. In: ACM
CCS 1993, pp. 244–250. ACM (1993)
10. Benhamouda, F., Blazy, O., Chevalier, C., Pointcheval, D., Vergnaud, D.: New
techniques for SPHFs and eﬃcient one-round PAKE protocols. In: Canetti, R.,
Garay, J.A. (eds.) CRYPTO 2013, Part I. LNCS, vol. 8042, pp. 449–475. Springer,
Heidelberg (2013)

Universally Composable Two-Server PAKE
165
11. Benhamouda, F., Pointcheval, D.: Veriﬁer-based password-authenticated key
exchange: New models and constructions. Cryptology ePrint Archive, 2013:833
(2013)
12. Brainard, J., Juels, A.: A new two-server approach for authentication with short
secrets. In: USENIX03 (2003)
13. Camenisch, J., Enderlein, R.R., Neven, G.: Two-Server Password-Authenticated
Secret Sharing UC-Secure Against Transient Corruptions. Cryptology ePrint
Archive, 2015:006 (2015)
14. Camenisch, J., Lysyanskaya, A., Neven, G.: Practical yet universally composable
two-server password-authenticated secret sharing, pp. 525–536. ACM (2012)
15. Canetti, R., Security, U.C.: A new paradigm for cryptographic protocols. In: FOCS
2001, p. 136. IEEE CS, Washington, DC, USA (2001)
16. Canetti, R.: Universally composable signature, certiﬁcation, and authentication.
In: CSFW 2004, p. 219. IEEE CS (2004)
17. Canetti, R., Fischlin, M.: Universally composable commitments. In: Kilian, J. (ed.)
CRYPTO 2001. LNCS, vol. 2139, pp. 19–40. Springer, Heidelberg (2001)
18. Canetti, R., Halevi, S., Katz, J., Lindell, Y., MacKenzie, P.: Universally composable
password-based key exchange. In: Cramer, R. (ed.) EUROCRYPT 2005. LNCS,
vol. 3494, pp. 404–421. Springer, Heidelberg (2005)
19. Canetti, R., Krawczyk, H.: Analysis of key-exchange protocols and their use for
building secure channels. In: Pﬁtzmann, B. (ed.) EUROCRYPT 2001. LNCS, vol.
2045, pp. 453–474. Springer, Heidelberg (2001)
20. Canetti, R., Rabin, T.: Universal composition with joint state. In: Boneh, D. (ed.)
CRYPTO 2003. LNCS, vol. 2729, pp. 265–281. Springer, Heidelberg (2003)
21. Damg˚ard, I.B.: Eﬃcient concurrent zero-knowledge in the auxiliary string model.
In: Preneel, B. (ed.) EUROCRYPT 2000. LNCS, vol. 1807, pp. 418–430. Springer,
Heidelberg (2000)
22. Gentry, C., MacKenzie, P.D., Ramzan, Z.: A method for making password-based
key exchange resilient to server compromise. In: Dwork, C. (ed.) CRYPTO 2006.
LNCS, vol. 4117, pp. 142–159. Springer, Heidelberg (2006)
23. hashcat. hashcat - advanced password recovery (2014). http://hashcat.net/.
Accessed 1 Dec 2014
24. Jarecki, S., Kiayias, A., Krawczyk, H.: Round-optimal password-protected secret
sharing and T-PAKE in the password-only model. In: Sarkar, P., Iwata, T. (eds.)
ASIACRYPT 2014, Part II. LNCS, vol. 8874, pp. 233–253. Springer, Heidelberg
(2014)
25. Jarecki, S., Lysyanskaya, A.: Adaptively secure threshold cryptography: introduc-
ing concurrency, removing erasures (Extended Abstract). In: Preneel, B. (ed.)
EUROCRYPT 2000. LNCS, vol. 1807, pp. 221–242. Springer, Heidelberg (2000)
26. Jin, H., Wong, D.S., Xu, Y.: An eﬃcient password-only two-server authenticated
key exchange system. In: Qing, S., Imai, H., Wang, G. (eds.) ICICS 2007. LNCS,
vol. 4861, pp. 44–56. Springer, Heidelberg (2007)
27. Katz, J., MacKenzie, P.D., Taban, G., Gligor, V.D.: Two-server password-only
authenticated key exchange. In: Ioannidis, J., Keromytis, A.D., Yung, M. (eds.)
ACNS 2005. LNCS, vol. 3531, pp. 1–16. Springer, Heidelberg (2005)
28. Katz, J., Vaikuntanathan, V.: Round-optimal password-based authenticated key
exchange. In: Ishai, Y. (ed.) TCC 2011. LNCS, vol. 6597, pp. 293–310. Springer,
Heidelberg (2011)

166
F. Kiefer and M. Manulis
29. Kiefer, F., Manulis, M.: Distributed smooth projective hashing and its appli-
cation to two-server password authenticated key exchange. In: Boureanu, I.,
Owesarski, P., Vaudenay, S. (eds.) ACNS 2014. LNCS, vol. 8479, pp. 199–216.
Springer,
Heidelberg (2014)
30. MacKenzie, P., Shrimpton, T., Jakobsson, M.: Threshold password-authenticated
key exchange. In: CRYPTO 2002, p. 141 (2002)
31. Openwall. John the Ripper password cracker (2014). http://www.openwall.com/
john/. Accessed 1 Dec 2014
32. Raimondo, M.D., Gennaro, R.: Provably secure threshold password-authenticated
key exchange. In: EUROCRYPT 2003, p. 507523 (2003)
33. Szydlo, M., Kaliski, B.: Proofs for two-server password authentication. In: Menezes,
A. (ed.) CT-RSA 2005. LNCS, vol. 3376, pp. 227–244. Springer, Heidelberg (2005)
34. Wu, T.: RFC 2945 - The SRP Authentication and Key Exchange System,
September 2000
35. Yang, Y., Deng, R., Bao, F.: A practical password-based two-server authentication
and key exchange system. IEEE TDSC 3(2), 105–114 (2006)

Yet Another Note on Block Withholding
Attack on Bitcoin Mining Pools
Samiran Bag1(B) and Kouichi Sakurai2
1 Newcastle University, Newcastle upon Tyne, UK
samiran.bag@newcastle.ac.uk
2 Kyushu University, Fukuoka, Japan
sakurai@csce.kyushu-u.ac.jp
Abstract. In this paper we provide a short quantitative analysis of Bit-
coin Block Withholding (BWH) Attack. In this study, we investigate the
incentive earned by a miner who either independently or at the diktat of
a separate mining pool launches Block Withholding attack on a target
mining pool. The victim pool shares its earned revenue with the rogue
attacker. We investigate the property revenue function of the attacker
and ﬁnd parameters that could maximize the gain of the attacker. We
then propose a new concept that we call “special reward”. This spe-
cial rewarding scheme is aimed at discouraging the attackers by grant-
ing additional incentive to a miner who actually ﬁnds a block. A BWH
attacker who never submits a valid block to the pool will be deprived
from this special reward and her gain will be less than her expectation.
Depending upon the actual monetary value of the special reward a pool
can signiﬁcantly reduce the revenue of a BWH attacker and thus can
even ward oﬀthe threat of an attack.
1
Introduction
Bitcoin [1] has been the most successful cryptocurrency ever. It was proposed
by an anonymous person who identiﬁed himself as ‘Satoshi Nakamoto’ but did
not ever disclose his real identity. In this cryptocurrency system users make
online payments by creating digital transactions signed by the payer which is
then broadcast across the entire Bitcoin network. Since, there is no centralized
authority to validate a transaction, the Bitcoin network cleverly bestows this
task on the most resourceful parties of the Bitcoin network who are called min-
ers. The miners demonstrate their computing resources through solving a hard
computational puzzle, which cannot be solved beforehand and thus present a
proof of work thereof. This proof of work scheme is based on Back’s Hashcash
[2] technique. The miners create a block of transactions created by Bitcoin users
and ﬁnd an appropriate proof of work that can be associated with the block. This
block of transactions along with the proof of work is then broadcast over the
S. Bag—A member of the faculty of Kyushu University, Fukuoka, Japan at the time
of submission of this paper.
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 167–180, 2016.
DOI: 10.1007/978-3-319-45871-7 11

168
S. Bag and K. Sakurai
entire Bitcoin network. Every node in the Bitcoin network receives it and veri-
ﬁes the PoW associated with the block. If the veriﬁcation is successful and the
block is constructed ﬂawlessly, the Bitcoin nodes append it to the ‘blockchain’
which is a data structure containing chain of blocks of transactions which are
linked to each other by means of cryptographic hash function. Since, construc-
tion of a Bitcoin block requires solving a proof of work puzzle, which in turn
requires a huge amount of computation, it is very unlikely for a small miner
having limited amount of computing resources to be able to mine a single block
until a long time. Hence, such a small miner will need to mine patiently for a
long time (which may be as long as few years) before she is able to mine her
ﬁrst block and earn her ﬁrst Bitcoin. This is the reason why small miners join
hands to form large pools owning sizable amount of computing resources. This
computing powerhouse has higher probability of winning the mining race than a
small miner. So, the mining pool earns Bitcoins more often and distributes the
earned revenue among its members following a fair policy keeping in view the
contribution of each miner towards solving the proof of work puzzle. Thus, the
purpose of mining pools is to allow every small miners to earn small incentives
frequently rather than winning 25BTC once after patiently mining for few years.
But the pools do not blindly trust their members. Since, many pools are open
and allow untrusted miners to join and mine on behalf of the pool, there should
be a strategy in place for assessment of the performance of the miners. For this
purpose, the pool administrator requires all the pool members to submit ‘pool
shares’. These pool shares are partial proofs of work with a limited diﬃculty,
usually lower than the diﬃculty level of the full proof of work associated with
a valid Bitcoin block. Hence, pool members tend to ﬁnd them more frequently
than they ﬁnd a full proof of work. The pool administrator sets a diﬃculty level
for the partial PoWs so that it does not cause a high overhead on the adminis-
trator to check every pool share submitted by the miners and at the same time
the frequency of a small miner to ﬁnd a pool share remains as high as possible
making it easy for the pool administrator to judge the performance of a miner on
the basis of the number of pool shares the latter submitted. In block withholding
attack [3], a rogue miner secretly discards all full proofs of work computed by
her and only submits partial proofs of work that cannot be used by the pool
administrator to construct a valid Bitcoin block worth 25BTC. Thus, a block
withholding attacker reduces the revenue of a mining pool, breaching the pool’s
trust on her. This paper deals with analyzing block withholding attack in Bitcoin
mining pools and also suggests a simple trick that could be used to repel BWH
attackers to some extent. We discuss this attack in details in Sect. 3.1.
1.1
Contribution
The contribution of this paper is two fold. Firstly, we analyze Block Withholding
Attack with respect to a simple though realistic model of Bitcoin mining pool.
We consider two pools and an attacker. The attacker uses a part of her computing
power for mining honestly in one pool and uses the rest of her computing power to
attack the second pool. We show that the attacker can expend upto p
2 fraction of

Yet Another Note on Block Withholding Attack on Bitcoin Mining Pools
169
her entire computing power to attack the pool beyond which her earned incentive
will tend to decrease, p being the computing power of the victim pool. We show
this in Lemma 1. Then, we consider a k-BWH attacker who does not withhold
all her valid blocks found while mining for the victim pool but instead, selects a
fraction of blocks for withholding. This attacker withholds a block with a ﬁxed
probability pB and submits a block to the pool with probability 1−pB. We show
that for such an attacker her gain will be maximized when she uses a certain
fraction of computing power for attacking the victim pool.
Then we discuss one simple technique for countering block withholding
attack. In this technique a Bitcoin mining pool gives extra reward to a miner
who actually ﬁnd the winning block on behalf of the pool. We study the long
term viability of this technique for a Bitcoin mining pool. Our main results from
Sect. 5 is given in Lemmas 4 and 5. In Lemmas 4 and 5 we compute the min-
imum amount of special reward that would make a Bitcoin pool resistant to
BWH attack and will repulse unwanted attackers.
2
Related Work
In Block Withholding attack (BWH) [3], a malicious miner submits to the pool
administrator, as pool shares, all those PoWs that do not constitute a full
PoW which can be used to generate a revenue of 25BTC, and withholds all
pool shares that represent a valid full PoW for the Bitcoin network. That is
the miner submits all partial proofs of work that allows her to convince the
pool administrator that she is indeed trying to mine for the pool. However,
when by chance she ends up ﬁnding a full proof of work, she withholds it. The
pool protocol does not allow her to submit the full proof of work to the Bit-
coin network and claim the entire reward [3,4]. So, she chooses to conceal her
ﬁnd causing a loss of at least 25BTC to the pool for which she has been min-
ing. The pool, however remains oblivious of this act and she, satisﬁed with the
pool shares submitted by the attacker, considers her to be a valuable asset and
shares her earned revenue with the attacker. One can argue that by launching
BWH attack on a pool, a miner decreases its own revenue. But, the eﬃcacy
of BWH attack lies in the fact that by reducing one pool’s revenue the miner
actually increases the revenue of other pools in the Bitcoin network as the rate
at which Bitcoins are generated in Bitcoin network remains the same for ever.
Saha et al. [5] discussed a scenario where a miner can use BWH attack for increas-
ing her revenue by splitting her computing power and attacking one pool using a
fraction of her computing power and mining independently with the rest of her
computing power. Saha et al. showed that in their model the BWH attack always
increases the incentive of a rational attacker. Eyal et al. [6] analyzed a scenario
where mining pools send their miners to attack other pools and reduce their
revenue and thereby increasing the selﬁsh pool’s incentive. They have deﬁned
and analyzed a game where identical mining pools attack each other. They have
showed that in a scenario where the mining pools attack each other, there exists
a Nash equilibrium where all of them earn less than what they should have

170
S. Bag and K. Sakurai
if none had attacked. Laszka et al. [7] deﬁned and analyzed a game theoretic
model of Bitcoin block withholding attack and showed interesting results about
the long term viability of a Nash equilibrium between attacking pools in the
Bitcoin network.
3
Preliminaries
3.1
Bitcoin Mining and Block Withholding Attack
Bitcoin block withholding attack [3,5–7] has long been discussed on Bitcoin
forums. In this attack, a rogue miner joins an open Bitcoin mining pool pre-
tending to be an honest miner. She then demonstrates her work to the pool
administrator by regularly submitting ‘pool shares’ which are partial proofs of
work having an associated diﬃculty level which is generally lower than the diﬃ-
culty level of the proof of work acceptable for the Bitcoin network. This partial
proofs are such that the set of all partial proofs of work for any pool contain
within itself, the set of full Bitcoin proofs. Hence, these partial proofs are in
reality potential candidates for Bitcoin proofs of work. The mining pool scru-
tinizes all partial proofs submitted by its miners. This serves as a check for
the performance of the miners as the number of partial proofs submitted by an
individual miner faithfully reﬂect the amount of computing power that partic-
ular miner has indeed invested for mining. Again, since, the partial proofs are
potential candidates for Bitcoin proof of work, the pool administrator may ﬁnd
a full proof from the set of partial proofs computed by the members of the pool.
So, the pool administrator tasks itself with checking all partial proofs until it
ﬁnds a proof of work that matches the diﬃculty level of the Bitcoin system.
If it ﬁnds such a proof of work, it uses the same to claim the mining reward
from the Bitcoin system. So, with this strategy, the pool administrator’s job
reduces to examining all partial proofs of work submitted by the miners until
it ﬁnds a full proof of work that will allow it to earn 25BTC by submitting the
same to the Bitcoin network. If someone outside the pool ﬁnds the correct proof
of work, the particular pool refreshes its mining parameters and starts mining
with a fresh set of parameters. In BWH attack, a malicious member submits
to the administrator only those partial proofs that cannot be associated with a
valid Bitcoin block and withholds all partial proofs that represent a full proof
of work and which can be used to earn 25BTC from the system. However, the
pool administrator remains oblivious to the fact and considers the miner to be
an asset to the pool. Hence, the pool administrator shares the earned revenue of
the pool with the malicious miner even though she does not contribute even a
single block. Thus, a BWH attacker incurs loss to an unsuspecting mining pool.
One may argue that a malicious miner thus causes loss to herself whenever she
withholds a block as the pool would have shared with her, the revenue gener-
ated with the block if she had submitted a valid block to the pool. But, the
key to success of BWH attack is that whenever a miner launches BWH attack
on a pool, its revenue decreases. The decreased revenue gets distributed to all
other miners of the Bitcoin network depending upon there computing power.

Yet Another Note on Block Withholding Attack on Bitcoin Mining Pools
171
So, the malicious miner who attacks one pool causes the revenue of other miners
to increase as the total amount of Bitcoins generated in a ﬁxed amount of time
is nearly constant. Thus, BWH attack is launched generally by other pools who
send their miners to inﬁltrate victim mining pools [6]. When the mining pool
comes under BWH attack, its revenue goes down which in turn results in the
increase of the revenues of other pools including the one that sent its agents to
attack the victim pool. BWH attack can also be carried out proﬁtably by solo
miners who expend part of their hashpower to attack some other pool [5].
3.2
Notations and Terminologies
We, in our analysis assume that the total computing power of the Bitcoin network
is 1, so that the computing power of each entity can be expressed as a fraction.
Thus, when we write α to be the computing power of any miner, we mean that
the computing power of that entity is α fraction of that of the entire Bitcoin
network. Similarly, we express the earned incentive of miners as a fraction of the
entire revenue generated by Bitcoin network(which is 25BTC plus transaction
fees per block). We use the term ‘gain’ to imply the earned revenue of any Bitcoin
miner. If gm be the incentive earned by a miner and gB be the incentive earned
by the entire Bitcoin network, we say the gain of the miner is gm
gB . This is the
same deﬁnition of gain of a miner as was used in [5].
4
Analysis of BWH Attack
We start our analysis with a network model comprising only two pools and an
attacker. It is obvious that this model is too simplistic as there are multiple
mining pools in today’s Bitcoin network. However this study can build the basis
for substantial studies of BWH attack taking multiple pools into account. Let, A
be an attacker with computing power α. We consider two pools P and P ′ with
computing power p and p′ respectively. The attacker A splits her computing
power into two parts. She uses β fraction of her computing power to launch
block withholding attack against pool P. She joins the open pool P pretending
to be an honest miner having computing power α and upon joining, she regularly
submits partial PoW to the pool to demonstrate that she, like every other miner
of the pool, is striving to ﬁnd an appropriate PoW that could allow the pool
to earn some revenue. However, she never submits a complete PoW. Whenever,
by chance, she computes one full PoW, she secretly discards it and continues to
compute partial PoWs. The pool P ′ has no way to detect this treacherous act
of A and remains oblivious about the deception. The partial PoWs submitted
by the attacker A allow her to mislead the pool P ′ into trusting her. The miner
uses the rest of her computing power to mine honestly in the second pool P ′. We
assume the Bitcoin network comprises of these two pools only. Hence, p+p′ = 1.
Lemma 1. When the attacker launches BWH attack on P and mines honestly
in pool P ′, her gain will be an increasing function as long as she expends upto
p
2 fraction of her computing power for attacking P.

172
S. Bag and K. Sakurai
Proof. When A launches BWH attack on the pool P, the computing power
of the pool reduces from p to p −αβ and the computing power of the entire
Bitcoin network reduces to 1 −αβ. The incentive earned by A from the pool
P is GP = p−αβ
1−αβ
αβ
p = αβ(p−αβ)
p(1−αβ) . Similarly, the incentive earned from pool P ′ is
GP ′ =
p′
1−αβ
α(1−β)
p′
= α(1−β)
1−αβ . So, the total amount of incentive earned by A is
G = αβ(p−αβ)
p(1−αβ) + α(1−β)
1−αβ = pα−α2β2
p(1−αβ) . Taking partial derivative with respect to β,
∂G
∂β = (pα −α2β2)pα −2p(1 −αβ)α2β
p2(1 −αβ)2
That is ∂G
∂β =
α
p(1−αβ)2 (pα + α2β2 −2αβ).
At β = 0, ∂G
∂β |β=0 = α2 > 0. Hence, the gain of the attacker A is increasing at
β = 0. Now, the value of G will reach a maxima/minima at some β = β0 such
that ∂G
∂β = 0. This will happen if pα + α2β2 −2αβ = 0. Now, this quadratic
equation will have a solution at β = 1−√1−pα
α
. Now, if α ≪1, then pα ≪1.
Hence, β ≈1−(1−0.5pα)
α
= p
2. Hence, the gain of the attacker is an increasing
function in the range (0, p
2).
□
The attacker A may not use all the β fraction of her computing power for
attacking the pool P as the pool administrator may grow skeptical about the
intention of A if she does not submit even a single valid PoW in the long run.
So, the attacker can spare some of her computing power to mone honestly in
the pool P, while investing a major fraction to launch BWH attack. So, we
consider a scenario where the attacker honestly mines in P with δ fraction of her
computing power, attacks the same pool P using β fraction of her computing
power and mines honestly in pool P ′ with the rest of her computing power and
prove the following lemma.
Lemma 2. When the attacker uses some k fraction of her computing power
invested in the pool P to mine honestly, and mines honestly in P ′ with the rest
of her computing power, her gain will be an increasing function as long as she
expends
p
2(k+1) fraction of her computing power for attacking P.
Proof. The incentive she gains for mining pool P is GP =
p−αβ
1−αβ
αβ+αδ
p
. Sim-
ilarly, the incentive obtained from P ′ will be GP ′ =
1−α(β+δ)
1−αβ
. The total
gain of the attacker will be G =
α(p−αβ)(β+δ)+p−pα(β+δ)
p(1−αβ)
. Now, in order to
deceive the pool P, δ should be proportional to β. Let, δ = kβ. Now, the
value of the earned incentive will be G =
α(p−αβ)β(1+k)+p−pα(1+k)β
p(1−αβ)
. Now,
∂G
∂β =
α2(α(1+k)β2−2(1+k)β+p)
p(1−αβ)2
. Thus
∂G
∂β

β=0 =
α2
(1−αβ)2 > 0. Hence, G is an
increasing function of β at β = 0. Also, G will have a local maxima at β = β0
if α(1 + k)β2
0 −2α(1 + k)β0 + p = 0. Solving this quadratic equation we get,
β0 =
2(1+k)−√
4(1+k)2−4α(1+k)p
2α(1+k)
=
1−√
1−αp
1+k
α
. Now, if α ≪1, then
αp
1+k ≪1.
Then, β0 ≈
p
2(1+k).
□

Yet Another Note on Block Withholding Attack on Bitcoin Mining Pools
173
Next, we consider a scenario where the attacker acts as a liaison of the smaller
pool P ′. That is, she attacks pool P but transfers all the incentive received
from P to P ′. P ′ then distributes the total incentive to all the members. So,
the attacker does not gain anything directly from P. Instead, she receives all
her incentives from P ′. The amount of incentive the attacker gets from P(that
she transfers to P ′) is g1 = p−αβ
1−αβ
αβ
p
= αβ(p−αβ)
p(1−αβ) . Now, the total incentive P ′
receives through mining honestly is g2 =
p′
1−αβ . As the attacker transfer her
ill-gotten incentive to P ′, the total incentive earned by P ′ reaches g = g1 + g2 =
αβ(p−αβ)
p(1−αβ) +
p′
1−αβ . Now, the incentive P ′ gives to the attacker is G = g ∗α
p′ =
α
1−αβ + α2β(p−αβ)
pp′(1−αβ) . Now, replacing p′ by 1−p, we get, G =
α
1−αβ +
α2β(p−αβ)
p(1−p)(1−αβ) =
α
p(1−p)
p−p2+αβp−α2β2
1−αβ
. Diﬀerentiating partially with respect to β,
∂G
∂β =
α2
p(1 −p)
(p −αβ)(2 −αβ −p)
(1 −αβ)2
.
Since, p ≫αβ, ∂G
∂β is always positive. Hence, the attacker always gains more
by attacking a pool on behalf on another pool.
If the attacker was honest, she could have earned only α by honestly mining
in either pool. The ratio of increase of her earned incentive with respect to the
incentive for honest mining is G−α
α
=
1
1−αβ +
αβ(p−αβ)
p(1−p)(1−αβ) −1.
We now consider another scenario, where there are three pools, namely P,
P ′ and P ′′ with computing power p + ap, p′ and 1 −p −p′. P ′′ is a closed pool.
Out of the p computing workforce, ap computing power is already invested for
attacking P, reducing the active computing power of P to p. Now, the pool P ′
sends the attacker A to attack P using β fraction of her computing power α.
As such, the total expected incentive of P will be g1 =
p−αβ
1−αβ . So, the
incentive given to the attacker is
p−αβ
1−αβ
αβ
p+aP . The incentive obtained from
the pool P ′ is
α(1−β)
1−αβ . So, the total incentive earned by the attacker will
be G
=
p−αβ
1−αβ
αβ
p+aP +
α(1−β)
1−αβ . Partially diﬀerentiating with respect to β
we get,
∂G
∂β
=
α
p+aP

1 −
1−p
(1−αβ)2

−
α(1−α)
(1−αβ)2 . Simplifying we get,
∂G
∂β
=
α(α2β2−2αβ+αp+αaP −aP )
(p+aP )(1−αβ)2
. Now, if aP >
αp
1−α, α2β2 −2αβ + (αp + αaP −aP ) will
always be negative and hence, ∂G
∂β will be negative. So, the gain of the attacker
will decrease as she invests more computing resources for attacking the pool
which is already under attack by an attacker having suﬃciently high computing
power.
5
Proposed Remedy to BWH Attack
Here, we propose a new strategy to defeat BWH attack. The key idea here is
to reduce the incentive of an attacker to such an extent that she does not ﬁnd
it proﬁtable to launch block withholding attack on a pool. In the existing pool

174
S. Bag and K. Sakurai
protocol, a miner gets entitled to the share of the revenue generated by the entire
pool by submitting partial proofs to the pool. The number of partial proofs a
miner submits to the pool is directly proportional to her computing power. So,
a mining pool judges the computing power of the miner by counting the partial
proofs submitted by a miner. It can be noted that a BWH attacker does not
submit any full proof but that does not decrease her count of partial proofs as
she ﬁnds a full PoW quite often. Thus, even though she withholds all full proofs,
the number of partial proofs computed by her does not decrease signiﬁcantly
and the dividend she gets from the pool thus becomes directly proportional to
her computing power thanks to the high number of partial proofs computed by
her. Now, we propose to introduce a new notion of rewarding a miner. We call
this a “special reward” that is to be disbursed to a miner who actually solves
the PoW puzzle and constructs a full proof of work that will eventually be used
by the pool to earn a revenue from the Bitcoin system. This special reward will
be paid from the pool’s revenue. Hence, every time the pool wins the mining
game a ﬁxed amount of special reward will be given to the miner who actually
computed a full PoW and constructed a valid Bitcoin block. The rest of the
revenue will be shared among all the miners of the pool (including the winner of
the special reward) depending upon their contribution to the pool which can be
calculated on the basis of the number of partial proofs submitted by them. Any
miner who launches BWH attack will never receive a special reward and will
only be entitled to the normal share of revenue for submitting the partial proofs.
As such the incentive earned by the attacker will be less as she will never receive
a special reward. This should discourage the attacker from launching this attack.
The Lemma 3 shows that this strategy does not decrease the gain of an honest
miner of the pool in the long run. Thus, this strategy only reduces the long term
revenue of an attacker while the revenue of all the honest miners remains the
same as before in the long run.
Lemma 3. The proposed strategy does not alter the earned revenue of an honest
miner of a pool in the long run.
Proof. Let, there be a pool P with computing power p. Also, let A be a miner
of the pool with computing power a. Assume that the computing power of the
entire Bitcoin network is 1. If the pool does not adopt the special rewarding
strategy, the earned incentive of A will always be a, which is the fraction of
computing power of A with respect to the computing power of the entire Bitcoin
network, which we take as 1 in this study. Let WP be the event that pool P
wins the mining game on a certain epoch. Also, let WA be the event that A
has computed the winning block. Let I(A) be the incentive function of A that
corresponds to the expected amount of incentive earned by miner A by mining
within the pool P. Hence, the expected incentive of A will be E(I) = P[WP ] ∗
I(A|WP )
=
P[WP ] ∗

I(A|WA)P[WA|WP ] + I(A| ¯WA ∩WP )P[ ¯WA|WP ]

=
I(A|WA)∗P[WP ]∗P[WA|WP ]+I(A|WP \WA)∗P[WP ]∗P[ ¯WA|WP ] = I(A|WA)∗
P[WA∩WP ]+I(A|WP \WA)∗P[WP \WA]. Now, I(A|WA) is the incentive earned
by A when A constructs the winning block. Hence, I(A|WA) = γ + (1−γ)a
p
.

Yet Another Note on Block Withholding Attack on Bitcoin Mining Pools
175
Similarly, I(A|WP \WA) = (1−γ)a
p
. Now, P[WA ∩WP ] = a. Also, P[WP \WA] =
p −a. Hence, E(I) = a(γ + (1−γ)a
p
) + (p −a) (1−γ)a
p
= a.
□
We consider a situation where the pool P gives extra reward to a miner that
solves a PoW. Let this ﬁxed amount of incentive be denoted as γ. Whenever some
miner of the pool P solves a proof of work and successfully constructs a valid
Bitcoin block worth 25BTC, the pool P oﬀers this special reward to her. The rest
of the revenue is then distributed fairly among the miners of the pool depending
upon their invested computing power which is measured on the basis of the num-
ber of pool shares submitted by them. Therefore, the revenue of the miner who
constructs a valid block is two fold, she gets the special reward from the pool
and also her regular share from the pool’s generated revenue. Like before, here
too we consider an attacker with computing power α who uses β fraction of her
computing power for attacking the pool P. So, the computing power dedicated
to attacking the pool P is αβ. The attacker uses the rest of her computing power
to mine privately. While the attack is being carried out, the revenue of the pool
P gets reduced from p to p−αβ
1−αβ . The pool P will spend an expected γ fraction of
this amount for paying the special reward to the miners who really constructed
Bitcoin blocks. The rest of the incentive amounting
p−αβ
1−αβ (1 −γ) will be dis-
tributed among all the miners depending upon the exact quantity of computing
power they have contributed to the pool. The miner who gets the special reward
will also be entitled to a share of this incentive. The attacker does not contribute
any block and hence will not ever get the special reward. What she will get is αβ
p
fraction of the remaining incentive after the special reward is paid. Thus, she will
receive an amount given by GBW H
A
= αβ(p−αβ)(1−γ)
p(1−αβ)
. She also mines privately
utilizing α(1 −β) computing power and the expected incentive gained from this
private mining will be GP RIV
A
= α(1−β)
1−αβ . Thus, her total generated revenue will
be GT otal
A
=
αβ(p−αβ)(1−γ)
p(1−αβ)
+ α(1−β)
1−αβ
=
pα−α2β2−γαβ(p−αβ)
p(1−αβ)
. The attacker will
be discouraged against launching attack on the pool P if GT otal
A
is less than the
incentive that she could make through mining honestly. Since, the attacker’s com-
puting power is α, she could earn an incentive of GHons
A
= (p−γp) α
p +(γp) α
p = α
if she had mined honestly. So, the attacker will not be interested in the execution
of the attack if pα−α2β2−γαβ(p−αβ)
p(1−αβ)
< α. This inequality will hold if γ > α(p−β).
From this, we can state the following lemma;
Lemma 4. In the above scenario, the attacker can be prevented from launching
BWH attack if the pool chooses a special reward γ higher than αp −αβ.
Lemma 5. Let there be two pools P and P ′ having computing power p and p′
respectively such that p + p′ = 1 i.e. all the computing power of the Bitcoin
network is held by both the pools. The pool P ′ tasks some of its members with
inﬁltrating the pool P and carrying out BWH attack on the pool P. Whatever
incentive the rogue inﬁltrators make by joining the pool P is transferred to the
pool P ′. The pool P ′ then distributes them to all of its members. Let δ be the
total amount of computing power of all the miners of pool P ′ who are sent to

176
S. Bag and K. Sakurai
inﬁltrate and attack the pool P. As such the attack could be defeated by choosing
a special reward γ > 1 −p −δ.
Proof. As such, the computing power of the entire Bitcoin network will get
reduced to 1 −δ. Thus, when the inﬁltrators start attacking P, the gain of pool
P will become GBW H
P
=
p
1−δ. The miner who ﬁnds a block by solving the PoW
will get an extra amount γ. The rest of the incentive amounting
p
1−δ(1 −γ) will
be given away to all the miners of the pool P including the selﬁsh inﬁltrators.
So, the inﬁltrators of the pool will get GBW H
INF
= (
p
1−δ(1 −γ))
δ
p+δ amount of
incentive totally. This amount will be transferred to the pool P ′. The gain of the
pool P ′ from its own mining will be Gpriv
P ′
= p′−δ
1−δ . So, the total gain of P ′ will
be GBW H
P ′
= Gpriv
P ′
+ GBW H
INF
= p′−δ
1−δ + p(1−γ)
1−δ
δ
p+δ. Substituting p′ by 1 −p, we
get GBW H
P ′
= Gpriv
P ′
+ GBW H
INF
= 1−p−δ
1−δ
+
pδ(1−γ)
(1−δ)(p+δ). If P ′ had mined honestly,
its total gain would be GHONS
P ′
= p′ = 1 −p. In order to discourage P ′ from
attacking the pool P, GHONS
P ′
must be higher than GBW H
P ′
, that is the gain of the
miner from honest mining should be higher than the gain from selﬁsh mining.
This can hold only if
1
1−δ(1 −p −δ + pδ(1−γ)
p+δ
) < 1 −p. Hence, the attacker will
stop attacking P if γ > 1 −p −δ.
□
0.55
0.6
0.65
0.7
0.75
0.8
0
0.05
0.1
0.15
0.2
0.25
-0.1
0
0.1
0.2
0.3
0.4
0.5
Min Value of γ
Value of special reward(γ)
+
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Compuing power of pool P1
Compuing of attackers(δ)
Fig. 1. Graphical presentation of the minimum value of γ to counter BWH attack for
diﬀerent value of p and δ.
Lemma 5 shows that in order to discourage the second pool from launching
BWH attack on the pool P, the pool P should make provision for a special reward
amounting (1 −fraction of gross hashpower of P) × 25BTC, to be awarded to
the miner who constructs a block by solving the PoW puzzle. As such, the other
pool P ′ won’t be interested to attack pool P and will stick to honest mining in
its own pool or in the other pool P. So, the pool P’s computing power will drop

Yet Another Note on Block Withholding Attack on Bitcoin Mining Pools
177
down to p. Every time the pool P wins the mining race, the pool administrator
spends γ = 1 −p fraction of the reward (25BTC) for giving the special reward
to the miner who constructed the block with the appropriate PoW. Now, every
time the pool P wins 25BTC, a miner who did not ﬁnd the block will get ζ
pp = ζ
fraction of 25BTC, ζ being the computing power of that miner. Without the
provision of the special reward, the miner would have got ζ
p fraction of 25BTC.
So, this special reward system can cause a reduction of ζ 1−p
p
of incentive to the
miner every time the pool wins but the miner does not construct the block for
that epoch. However, as we have shown in Lemma 3, the long term incentive of
an honest miner will be same as what she would have earned without having the
special reward scheme in place. So, this special reward scheme will be viable in
long term as it does not deprive honest miners in long term. Figure 1 shows the
minimum values of the special reward γ for diﬀerent values of p and δ. Figure 2
shows the diﬀerence between the earned incentives of honest miners and attacker
in the pool P per unit hashpower. The value of the earned incentives of both
honest miners and the attacker are normalized by dividing the gain by the actual
computing power invested by the miners/attacker. Figure 2 shows that when
the computing power of the attacker is low, the gain of honest miners is much
higher than the attackers. However, as more attackers pour in and the attackers
hashpower in pool P increase, the gain of attackers per unit hashpower tends
to be close to that of the honest miners. So, our technique can signiﬁcantly
reduce the earned incentive of attackers when the computing power of all the
attackers is low. In other words, this special rewarding scheme is eﬀective when
the combined hashpower of all the attackers expended in this attack is less than
a certain limit.
Consider two mining pools, P and P ′, each with computing power p and p′
respectively. The computing power of the rest of the Bitcoin network is 1−p−p′.
The pool P ′ sends miners having δ computing power to inﬁltrate and attack pool
P. As such, the computing power of P goes up to p + δ. The computing power
of P ′ gets reduced to p′ −δ. The incentive earned by the attackers sent by pool
P ′ is transferred back P ′. This amount is given by GBW H
INF
=
p
1−δ(1 −γ)
δ
p+δ.
The incentive earned through P ′’s own mining is given by Gpriv
P ′
= p′−δ
1−δ . Thus,
the total earned incentive of pool P ′ is given by GBW H
P ′
= Gpriv
P ′
+ GBW H
INF
=
p′−δ
1−δ + p(1−γ)
1−δ
δ
p+δ. If P ′ had mined honestly, it would have generated an incentive
given by GHons
P ′
= p′. Now, if P ′ has to be discouraged against attacking the
pool P, GHons
P ′
should be higher than GBW H
P ′
. So, P ′ will be discouraged if
p′−δ
1−δ +
pδ(1−γ)
(p+δ)(1−δ) < p′ that is γ > p′ −δ(1−p′)
p
.
Corollary 1. Let P1, P2, . . . , Pn be n mining pools all having trusted min-
ers as members. The hashpower of pool i is pi, ∀i ∈{1, 2, . . . , n}. If a pool
Pi, i ∈{1, 2, . . . , n} wants to accept an external miner having hashpower δ from
any other pool, she should choose a special reward γ, given by maxn
j=1, j̸=i(pj −
δ(1−pj)
pi
) in order to repel all attackers.

178
S. Bag and K. Sakurai
 0.6
 0.7
 0.8
 0.9
 1
 1.1
 0
 0.05
 0.1
 0.15
 0.2
 0.25
Gain per unit Hashpower
Computing power of attacker
Honest Miner
Attacker
Fig. 2. Graphical presentation of the gain of an honest miner and an attacker per unit
computing power. Here the computing power of pool P is p = 0.6 of the entire Bitcoin
network. We choose a γ equal to the numeric value of 1 −p −δ, where δ is the total
hashpower of all the attackers in the pool P.
Let there be n pools in the Bitcoin network represented as P1, P2, . . . , Pn. The
pools have computing powers p1, p2, . . . pn respectively, satisfying n
j=1 pj = 1.
The pool Pj spends αj computing power to attack other pools and is itself
attacked by miners from other pools having a total of βj computing power. So,
n
j=1 αi = n
j=1 βj. Now, we assume that a particular pool Pi, i ∈{1, 2, . . . , n},
uses a special reward γ to discourage all traitors against block withholding.
Now, since Pi uses αi amount of computing power to attack other pools, its
computing power reduces to pi −αi. We assume that Pi is honest and hence,
αi = 0. The expected amount of revenue generated by Pi is
pi
1−n
j=1 βj . The
pool Pi uses γ fraction of this to give special reward to the miners who found
the winning block(s). The rest of the revenue amounting to (pi−αi)(1−γ)
1−n
j=1 βj
is dis-
tributed among all the miners. So, the block withholding attackers will earn
a total revenue of
(pi−αi)(1−γ)
1−n
j=1 βj
βi
pi+βi . We have assumed Pi to be honest and
hence αi = 0. Therefore, the earned incentive of all honest miners of Pi will
be
pi
1−n
i=1 βi γ +
(pi)(1−γ)
1−n
j=1 βj
pi
pi+βi . The miners of pool Pi won’t be deprived if
pi
1−n
i=1 βi γ +
(pi)(1−γ)
1−n
j=1 βj
pi
pi+βi ≥pi. Simplifying this relation we get,
γ ≥1 −
n

j=1
βj −pi
βi
n

j=1
βj.
(1)

Yet Another Note on Block Withholding Attack on Bitcoin Mining Pools
179
0
0.05
0.1
0.15
0.2
0.25
0.3 0
0.1
0.2
0.3
0.4
0.5
0
0.1
0.2
0.3
0.4
0.5
0.6
++++++++++++++++++++
++++++++++++++++++++++
+++++++++++++++++++++++
+++++++++++++++++++++++++
++++++++++++++++++++++++++
++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++
++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++
++++++++++++++++++++++++++++
++++++++++++++++++++++++++++
+++++++++++++++++++++++++++
++++++++++++++++++++++++++
+++++++++++++++++++++++++
+++++++++++++++++++++++++
++++++++++++++++++++++++
+++++++++++++++++++++++
++++++++++++++++++++++
++++++++++++++++++++++
+++++++++++++++++++++
+++++++++++++++++++++
++++++++++++++++++++
+++++++++++++++++++
++++++++++++++++++
+++++++++++++++++
++++++++++++++++
+++++++++++++++
++++++++++++++
+++++++++++++
++++++++++++
+++++++++++
++++++++++
+++++++++
++++++++
+++++++
+++++++++++++++++++++
βi
n
j=1 βj
γmin
Fig. 3. The value of γmin = 1 −n
j=1 βj −pi
βi
n
j=1 βj for diﬀerent values of the
parameters. We choose pi = 0.25.
It can be seen from Eq. 1 that the minimum value of γ decreases as the
computing power of Pi increases. So, the special reward can be low for a big
mining pool. In other words, a big pool can ward oﬀthe threat of BWH attack
by aﬀording a small special reward. Figure 3 gives a pictorial depiction of how
the minimum value of the special reward varies with other parameters. We ﬁxed
the computing power of Pi to be 0.25 in Fig. 3.
Now, let us consider another scenario where there are n pools represented
as P1, P2, . . . , Pn. These pools have computing power denoted by p1, p2, . . . , pn.
Each pool Pi, i ∈{1, 2, . . . , n} uses αi computing power to attack other pool.
Also, each pool Pi is itself attacked by other pools’ members having an aggre-
gate of βi computing power. Since, there are only n pools, n
i=1 αi = n
i=1 βi.
Now, a pool Pi’s eﬀective computing power will come down to pi −αi after
it has sent few miners to attack other pools. The revenue earned by the
pool Pi from mining within the pool is given by Gself
Pi
=
pi−αi
1−n
j=1 βj . We
assume that βi = 0, i.e. the pool Pi is not attacked. Let, Pi attacks a pool
Pj, , 1 ≤j ≤n, j ̸= i with a computing power αij. So, if a pool Pj follows the
special rewarding policy, the incentive the attackers from pool Pi could earn can
be written as GBW H
ij
=
(1−γ)(pj−αj)αij
(1−n
i=1 βj)(pj−αj+βj). Hence, the total earned incen-
tive of pool Pi will be GPi =
pi−αi
1−n
j=1 βj + n
j=1,j̸=i
(1−γ)(pj−αj)αij
(1−n
i=1 βj)(pj−αj+βj) =
pi−αi
1−n
j=1 βj +
(1−γ)
(1−n
i=1 βj)
n
j=1,j̸=i
(pj−αj)αij
(pj−αj+βj). Now Pi will be beneﬁted from
the attack only if
pi−αi
1−n
j=1 βj +
(1−γ)
(1−n
i=1 βj)
n
j=1,j̸=i
(pj−αj)αij
(pj−αj+βj) ≥
pi
1−n
j=1,j̸=i βj .
Now, while the attack is put on execution, the gain of Pi will be maximized if
(pj−αj)αij
pj−αj+βj = c, ∀j ∈{1, 2, . . . , n}\{i}. This will hold if αij = c(pj−αj+βj)
pj−αj
that is
αij = c(1 +
βj
pj−αj ) for all c.

180
S. Bag and K. Sakurai
6
Conclusion
In this paper, we have incorporated few results that deal with the optimal strat-
egy of a block withholding attacker who, with an intention to maximize her
revenue uses her computing power rationally to attack a Bitcoin mining pool.
We also showed that this attack will not yield the desired outcome if the victim
pool is already under attack by one or more attackers having computing power
beyond a speciﬁc threshold. In other words, an overwhelmingly invaded mining
pool may not be good destination for a BWH attacker who wants to utilize her
computing power most eﬃciently in order to earn incentives at the cost of unsus-
pecting miners of an open mining pool. We also propose and analyze a strategy
that honest mining pools can use to repulse all block withholding attackers. This
paper proposes to pay extra incentive to a miner who submits a valid Bitcoin
block to the mining pool which the mining pool can use to earn incentives from
the Bitcoin network. We have discussed the exact amount of the special reward
that can eliminate the risk of BWH attack in a pool for some chosen models of
Bitcoin mining pools.
Acknowledgement. The authors were partially supported by the Japan Society for
the Promotion of Science (Japan) and the Department of Science and Technology
(India) under the Japan-India Science Cooperative Program of research project named:
“Computational Aspects of Mathematical Design and Analysis of Secure Communica-
tion Systems Based on Cryptographic Primitives”.
The second author was partially supported by JSPS Grants-in-Aid for Scientiﬁc
Research named: “KAKEN-15H02711”.
References
1. Nakamoto, S.: Bitcoin: a peer-to-peer electronic cash system. Consulted 1(2012),
28 (2008)
2. Back, A.: Hashcash - a denial of service counter-measure. Technical report, August
2002
3. Rosenfeld, M.: Analysis of bitcoin pooled mining reward systems. CoRR,
abs/1112.4980 (2011)
4. Courtois, N.T., Bahack, L., On subversive miner strategies, block withholding
attack in bitcoin digital currency. arXiv preprint arXiv:1402.1718 (2014)
5. Luu, L., Saha, R., Parameshwaran, I., Saxena, P., Hobor, A., On power splitting
games in distributed computation: the case of bitcoin pooled mining. In: proceed-
ings of IEEE Computer Security Foundations Symposium, CSF 2015, Verona, Italy
(2015)
6. Eyal, I.: The miner’s dilemma. In: IEEE Symposium on Security and Privacy, San
Jose (2015)
7. Laszka, A., Johnson, B., Grossklags, J.: When bitcoin mining pools run dry. In:
Brenner, M., Christin, N., Johnson, B., Rohloﬀ, K. (eds.) FC 2015 Workshops.
LNCS, vol. 8976, pp. 63–77. Springer, Heidelberg (2015)

Network and Systems Security and
Access Control

Cyber Security Risk Assessment
of a DDoS Attack
Gaute Wangen1(B), Andrii Shalaginov1, and Christoﬀer Hallstensen2
1 NISlab, Norwegian Security Laboratory,
Center for Cyber and Information Security, Gjøvik, Norway
{gaute.wangen2,andrii.shalaginov}@NTNU.no
2 IT Services, NTNU, Trondheim, Norway
christoffer.hallstensen@NTNU.no
Abstract. This paper proposes a risk assessment process based on dis-
tinct classes and estimators, which we apply to a case study of a com-
mon communications security risk; a distributed denial of service attack
(DDoS) attack. The risk assessment’s novelty lies in the combination
both the quantitative (statistics) and qualitative (subjective knowledge-
based) aspects to model the attack and estimate the risk. The approach
centers on estimations of assets, vulnerabilities, threats, controls, and
associated outcomes in the event of a DDoS, together with a statistical
analysis of the risk. Our main contribution is the process to combine
the qualitative and quantitative estimation methods for cyber security
risks, together with an insight into which technical details and variables
to consider when risk assessing the DDoS ampliﬁcation attack.
1
Introduction to InfoSec Risk Assessment
To conduct an information security (InfoSec) risk analysis (ISRA) is to compre-
hend the nature of risk and to determine the level of risk [2]. InfoSec risk comes
from applying technology to information [6], where the risks revolve around
securing the conﬁdentiality, integrity, and availability of information. InfoSec
risk management (ISRM) is the process of managing these risk while maximiz-
ing long-term proﬁt in the presence of faults, conﬂicting incentives, and active
adversaries [19]. Risks for information systems are mainly analyzed using a prob-
abilistic risk analysis [3,17], where risk is deﬁned by estimations of consequence
for the organization (e.g. ﬁnancial loss if an incident occurred) and the probabil-
ity of the risk occurring within a time interval. ISRA is mostly conducted using
previous cases and historical data. Depending on statistical data (quantitative)
alone for risk assessments will be too naive as the data quickly become obsolete
[18] and is limited to only previously observed events [16]. While the subjective
(qualitative) risk assessment is prone to several biases [11] (Part II) [16]. ISRM
methods claim to be mainly quantitative [6,8] or qualitative [7], but the quanti-
tative versus qualitative risk situation is not strictly either-or. There are degrees
of subjectivity and human-made assumptions in any risk assessment, and the
intersection of these two approaches remains largely unexplored. The goal of
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 183–202, 2016.
DOI: 10.1007/978-3-319-45871-7 12

184
G. Wangen et al.
this paper is to explore this intersection and discuss the beneﬁts and drawbacks
from each approach, and how they can complement each other. Moreover, we
will discuss alternative ways of expressing uncertainty in risk assessment.
The remainder of the paper is structured as follows: The two following subsec-
tions introduces the reader to Distributed Denial of Service attacks and discusses
the related work in ISRA. The Sect. 2 provides a brief description of the DDoS
attack and development trend. Also, we present the method applied for ISRA
and statistical analysis of the DDoS attack. Later in the Sect. 3 we give an insight
into the qualitative ISRM approach together with results and the quantitative
risk assessment in the Sect. 4 based on statistical methods. Lastly, we discuss
and conclude the results, the relationship between this work and previous ISRA
work, limitations and propose future work in the Sect. 5.
1.1
Distributed Denial of Service Attacks
A denial of service (DoS) occurs when an ICT (Information and Communica-
tion Technology) resource becomes unavailable to its intended users. The attack
scenario is to generate enough traﬃc to consume either all of the available band-
width or to produce enough traﬃc on the server itself to prevent it from handling
legitimate requests (resource exhaustion). The attacker needs to either exploit a
vulnerable service protocol or to exploit network device(s) to generate traﬃc, or
to amplify his requests via a server to consume all of the bandwidth. The DoS
attack is distributed (DDoS) when the attacker manages to send traﬃc from
multiple vulnerable devices. The attacker can achieve ampliﬁcation through the
exploitation of vulnerable protocols or through using botnets.
The increase of Internet throughput capacity has also facilitated the growth
in traﬃc volume for DDoS-attacks. According to Arbor Networks, the largest
observed attack in 2002 was less 1 Gbps (Gigabit per second). While the biggest
observed attack until now targeted a British television channel and reportedly
generated ≈600 Gbps of traﬃc. That is an approximate 60x development in
capacity for DDoS attacks over the course of about 14 years, see Fig. 1.
1.2
Related Work in ISRA
The ISRA approach presented in this paper primarily builds on two previous
studies; ﬁrstly, Wangen et al.’s [17] Core Uniﬁed Risk Framework (CURF), which
is a bottom-up classiﬁcation of nine ISRA methods. The motivation behind
CURF, was that there are several ISRA methods which conduct similar tasks,
but there is no common way to conduct an ISRA. The approach ranked as
most complete in CURF was ISO27005 [3] (from this moment referred to as
ISO27005), while ISO27005 has many strengths, such as the process descrip-
tions and taxonomies, one of the primary deﬁcits of the ISO27005 is the lack
of variables to consider and risk estimation techniques. The proposed approach
in this paper builds on ISO27005 and addresses the outlined issues by deﬁn-
ing classes and estimations for each step. Second, the probabilistic model pre-
sented in this paper builds on the feasibility study conducted by Wangen and

Risk Assessment of a DDoS Attack
185
Fig. 1. The development of bandwidth consumption (Gbps) of DDoS-attacks during
the last 15 years. Data source: Arbor Networks and media reports
Shalaginov [18], which discusses statistics and Black Swan (see Taleb [16]) issues
in ISRA. The Authors [18] found that there are Black Swan related aspects of
the ICT domain that may render past observations (Statistics) inappropriate for
probability, such as for novel and unique attacks, and the fast development of
ICT, for example, Fig. 1. However, the authors also found that quantifying and
modeling InfoSec risks have utility as long as the risk assessor is aware of the
properties of the risk and the domain we are modeling. The Single and Annual
Loss Expectancy (SLE/ALE) represent the most developed area of statistics in
ISRA, where risk is described as the probability of a loss occurring [6]. Yet,
risk must be considered as more than an expected loss [5]. Knowledge-based
probabilities represent the main approach in ISRA [17], as previously discussed,
there is utility in statistical data. The combination of these two approaches to
probability has remained relatively unexplored in ISRA. So, this study proposes
to combine a statistical and a qualitative ISRA to address the research gap.
Thus, this paper proposes a step-by-step process model for an ISRA of a dis-
tributed denial of service (DDoS) attack, and we apply the model to a real-world
case as a proof of concept and feasibility study. The proposed ISRA approach is
compliant with ISO27005.
2
Choice of Methods
This section outlines the core risk assessment concepts applied in this paper.
First, we present the fundamentals of our risk analysis approach, then the qual-
itative ISRA method, and, lastly, discuss the statistical methods employed for
quantitative analysis. Our overarching approach to validation is case study.

186
G. Wangen et al.
The proposed approach is based on the two ISO27005 steps (i) Risk Iden-
tiﬁcation -process of ﬁnding, recognizing and describing risks [2], and (ii) Risk
Estimation - process of comparing the results of risk analysis with risk criteria
to determine whether the risk and/or its magnitude is acceptable or tolerable
[2]. We go further proposing classes and estimations for qualitative asset eval-
uation, and vulnerability, threat, and control assessment, together with both
quantitative and qualitative risk estimations.
2.1
Fundamentals of Risk Analysis
Our proposed ISRA approach builds on the set of triplets as deﬁned by Kaplan
and Garrick [12], Scenario, Likelihood, and Consequences. In which we deﬁne the
scenario as a combination of assets, vulnerability, threat, controls, and outcome.
Each step in the approach generates useful knowledge in on its own, for example,
a thorough threat assessment will provide information regarding opponents that
are also useful in other risk-related activities and decision-making.
We combine the two approaches to risk and probability proposed by Aven [5]:
(i) the frequentist (“the fraction of times the event A occurs when considering an
inﬁnite population of similar situations or scenarios to the one analyzed”), and
(ii) the subjective knowledge-based probability (“assessor’s uncertainty (degree
of belief) of the occurrence of an event”). In terms of risk analysis, the key
components of a risk (R) related to an activity for discussion and calculation are
as follows [4] (p. 229): R is described as a function of events (A), consequences
(C), associated uncertainties (U), and probabilities (P). U and P calculations
rely on background knowledge (K) which captures the qualitative aspect of the
risk, for example, low K about a risk equals more U. Model sensitivities (S)
display the underlying dependencies on the variation of the assumptions and
conditions. Thus, R = f(A, C, U, P, S, K) allows for a comprehensive output
and incorporates the most common components of risk.
In the following section, we deﬁne the classes and estimators for each of the
key elements of InfoSec risk as subjective knowledge, where the classes describe
and categorize the risk components, and the estimators represent qualitative
estimations based on expert knowledge and collected data. We do not deﬁne the
scales for each estimator in this paper as this is individual for each organization.
2.2
Proposed Methodology for Qualitative Risk Analysis
The proposed qualitative methodology is based on descriptions, classes, and esti-
mators. Based on ISO27005 we deﬁned these for Assets evaluation, Vulnerability
assessment, Threat assessment, and Control Assessment.
Asset Identiﬁcation and Evaluation. To start, the Institution needs to iden-
tify and know its assets. We deﬁne Asset Identiﬁcation as the process of identify-
ing assets, while asset Evaluation assess their value, importance, and criticality.
According to ISO27005 [3] Annex B, there are two primary assets, (i) Business

Risk Assessment of a DDoS Attack
187
Processes & activities and (ii) Information. While Asset Container identiﬁes
where assets are stored, transported, and processed [7].
As a part of the process, we map the organizational goals and objectives for
risk assessment, as these are important in deriving security goals for the InfoSec
program. Also, we consider these when determining the risk event outcome.
– Assets - Something of value to the organization, person, or entity in question.
– Asset type - Description of the asset class, E.g. sensitive information.
– Asset Container - refers to where and how the asset is stored [7].
– Asset value - Estimated, either monetary or some intangible measurement of
value
– Importance in Business Process is an estimation of the criticality of the asset
in daily operations
– Asset criticality is the comprehensive assessment of the asset value and role
in business process estimations.
Vulnerability Assessment. Vulnerability Identiﬁcation is the process of iden-
tifying vulnerabilities of an asset or a control that can be exploited by a threat
[2]. Vulnerability Assessment is the process of identifying, quantifying, and pri-
oritizing (or ranking) the vulnerabilities in a system. Vulnerabilities can be dis-
covered through many activities, such as automated vulnerability scanning tools,
security tests, security baselining, code reviews, and penetration testing. In the
case of network penetration from a resourceful attacker, the analyst should also
consider the attacker graph: how compromising one node in the network and
establishing a foothold in the network can be exploited to move laterally inside
the network and compromising additional nodes.
– Vulnerability type - A classiﬁcation and description of vulnerability, weakness
of an asset or control that can be exploited by one or more threats [2].
– Attack description - description of the attack for single attacks such as DDoS,
or attacker graph where the adversary obtains access to an asset or asset group.
The attacker graph is a visual representation of how the attacker traverses the
network and gains access to an asset or a group of assets.
– Attack diﬃculty - Estimation, how diﬃcult is it to launch the attack?
– Vulnerability severity - Estimation of the seriousness of the vulnerability
– System Resilience - How well will the system function under and after an
assault, especially important for availability related risk
– Robustness - is the measure of how strong an attack will the system absorb.
– Exposure assessment - Determines exposure of entity’s assets through the
vulnerability and attack
Threat Identiﬁcation and Assessment. Threat identiﬁcation is the process
of identifying relevant threats for the organization. A Threat is a potential cause
of an unwanted incident, which may result in harm to a system or organization
[2]. Besides mother nature, the threat is always considered as a human. For
example, the threat is not the computer worm, but the worm’s author. While

188
G. Wangen et al.
the threat Assessment comprises of methods and approaches to determine the
credibility and seriousness of a potential threat. The assessment process relies on
the quality of threat intelligence and understanding of the adversary. For each
threat, we propose to consider the following classes and estimators:
– Threat actor - Describes the human origin of the threat. There are several
classes of threat agents in InfoSec, for example, malware authors, Cyberspies,
and hackers.
– Intention - Deﬁnes what the threat actor’s objectives with the attack, for
example, unauthorized access, misuse, modify, deny access, sabotage, or dis-
closure.
– Motivation - Deﬁnes the primary motivation for launching the attack, pre-
vious work on malicious motivations [13] suggests Military or Intelligence,
Political, Financial, Business, Grudge, Amusement, Self-assertion, Fun, and
Carelessness.
– Breach type - which type of security breach is the threat actor looking to make;
either conﬁdentiality, integrity, availability, non-repudiation, or accountability.
– Capacity - Estimation of the resources he/she has at their disposal to launch
the attack. For example, if an attack requires a lengthy campaign against
your systems to succeed, the threat actor must have the resources available to
launch such an attack.
– Capability - Estimation the threat’s know how and ability for launching the
attack.
– Willingness to attack - Estimation of how strong the motivation is to attack.
For example, historical observations of the threat actor’s frequency attacking
the system is a good indicator.
– Threat severity is the comprehensive assessment of the above variables and
the main output of the process.
Control Eﬃciency Estimation. Existing controls are measures already in
place in the organization to modify risk [2]. Control identiﬁcation is the activity
of identifying existing controls for asset protection. Control (eﬃciency) Assess-
ment are methods and approaches to determine how eﬀectively the existing
controls are at mitigating an identiﬁed risk.
The important issue to consider here is if the control suﬃciently mitigates the
risk in question. If the control is considered adequate, the risk can be documented
for later review.
– Control Objectives - a written description or classiﬁcation of what the control
is in place to achieve.
– Control domain - Addresses in what domain the identiﬁed control is, either in
the physical, technical, or administrative [9] (pp. 166–167).
– Control class - Addresses what the control is supposed to achieve; either pre-
vent, detect, deter, correct, compensate or recovery [9] (pp. 166–167).
– Risk Event components - Consists of the Asset Criticality, Exposure Assess-
ment, and Threat Severity for the identiﬁed risk event.
– Control eﬃciency - Estimation, addresses how eﬃcient the control is at modi-
fying the identiﬁed threat event and how well it achieves the control objectives.

Risk Assessment of a DDoS Attack
189
2.3
Methodology for Statistical Risk Analysis
The main statistical approaches considered in this paper are for theoretical
analysis of the supplied historical data to run calculations. The motivation is
to use conventional statistical methods to extract particular characteristics that
are suitable for Quantitative ISRA. Additionally, we make hypotheses about an
applicability of each particular method concerning available data. The calcula-
tions in this article are based on DDoS attacks data from the Akamai Tech-
nology’s State of the Internet Reports (duration and magnitude) [1] and data
gathered from the assessed case study institution on occurrence. These data are
considered as quantitative observation of metrics of selected events, for example,
some DDoS attacks over time. We utilize several community-accepted methods
to deal with the historical data when it is necessary to make predictions in num-
bers. In particular, these are Conditional Probability and Bayes Theorem. First,
the probabilistic model p(x) is suggested and the corresponding set of parame-
ters are estimated from the data to ﬁt suggested distribution. In sequence, we
apply statistical testing, which is an important part of our work since further for
the DDoS case study we will justify the usage of a speciﬁc statistical method and
make a hypothesis about their applicability. By testing, we can make a quanti-
tative analysis of diﬀerent statistical models quality. However, this is based only
on pure analysis of the case’s data and deducing the most applicable model that
can describe the data and ﬁt the purposes. The testing is suitable for determin-
ing whether the data follow a particular distribution model with some degree
of deﬁned beforehand conﬁdence interval measured in %. The tests evaluate
the actual observed data O with the expected data E from the hypothesized
distribution. This is done with a help of QQ-plot or Quantile-Quantile plot
representing a probability plot by depicting expected theoretical quantiles E
and observed practical quantiles O against each other. The quality of hypoth-
esized data distribution can be evaluated using linearity in this plot. It means
that if the expectations match observations, even with some minor outliers, then
the null hypothesis can be rejected, and data ﬁt selected distribution. Second,
the probabilistic model can be used to estimate the probability of similar events
in this very period or later on. We observe the following well-known shortcom-
ings of the probabilistic modeling. First, very few data points from history may
cause a wrong decision. Second, very rare events have negligibly small proba-
bilities which might cause trouble in predicting corresponding outcomes. The
authors have applied the statistical analysis software IBM SPSS, GNU PSPP
and RapidMiner. Later on, we also discuss the application of this methodology
and possible ways of its improvement.
3
Case Study: Qualitative Risk Assessment of a DDoS
Attack
The case data together with relevant available statistics was collected from an
institution whose IT-operations delivers services to about 3,000 users. The Case

190
G. Wangen et al.
study Institution (hence referred to as “The Institution”) is a high-availability
organization delivering a range of services to the employees and users, mainly
within research and development. The objectives of the IT-operations is to
deliver reliable services with minimal downtime. The target of this study has
a 10 Gbps main ﬁber optics connection link, which is the threshold of a success-
ful DDoS attack. Figure 2 displays the institution’s network capacity and average
traﬃc during regular weekdays, this case study considers attacks on the main
link. During the ﬁve previous years, the Institution has had an average annual
occurrence of two DDoS attempts, whereas none has been successful thus far.
The goal of this assessment is to derive the qualitative risk of the Institution
experiencing a successful attack by applying the proposed method.
The case study starts with asset identiﬁcation and evaluation, further, con-
sidering vulnerabilities, threat assessment, control eﬃciency, and outcomes. Our
contribution in this section is the application of the classes and qualitative esti-
mators for each step of the risk assessment process.
Case Asset Evaluation. A DDoS attack is primarily an attack on the availabil-
ity of the organization’s Internet connection. We compare the Internet connec-
tion capacity with a pipeline; it’s capacity limits the pipe’s throughput. Once the
capacity is ﬁlled, no additional traﬃc can travel through the pipe. The attacker’s
goal is to ﬁll the pipeline with traﬃc and eﬀectively block all legitimate traﬃc
from traveling through the pipe.
In the considered case, a successful DDoS attack will lock the users out of the
network and prevent them from conducting their connectivity-dependent tasks.
Most of the organization’s value chain is dependent on some level of connectivity,
which makes the availability of services and assets the top priority when consid-
ering DDoS attacks. For simplicity, we consider “Service” as the main asset. As
the institution is high availability and has up-time as one of the top priorities,
service delivery is seen as crucial for production. Table 1 shows the classiﬁcation
and estimation considered for protection in the case study.
Fig. 2. Illustration of network robustness with an absorbed ampliﬁcation attack. Net-
work capacity at 10 Gbps, everything above constitutes a DoS.

Risk Assessment of a DDoS Attack
191
Table 1. Asset considerations for the DDoS attack
Asset
Container
Protection
attribute
Importance
in
business process
Asset value
Asset criticality
Service delivery
Infrastructure
- Internet
Pipeline
Availability
Essential (70–100)
Very high (50–85)
Essential
(70–100)
Case Vulnerability Assessment Results. The Institution is exposed to sev-
eral attack vectors for achieving DoS; for example resource starvation, applica-
tion layer-based, and volumetric/ﬂood. We provide a technical description of one
attack, together with a vulnerability assessment. These estimations assume a 10
Gbps connection and the current security level in the Institution.
We measure the robustness in the DDoS-case in the gap between maximum
network capacity and average traﬃc, illustrated in Fig. 2. A narrow gap between
average load and maximum capacity is an indicator of fragility towards traﬃc
generating attacks. To describe the network robustness we look at the maximum
load versus the average load and measure the gap. The average load on the
network is ≈1 Gbps; the system can absorb DDoS attacks up to ≈9 Gbps
before the users experience denial of service, Fig. 2.
On resilience, the network will continue to function within acceptable service
delivery up to traﬃc of about approximately 6–9 Gbps, depending on several
variables such as weekday and hours, before users start to experience a degra-
dation in service. Although attacks in this vicinity do not entirely cause a DoS,
they reduce the latency in the network and eﬃciency of the workforce.
Based on our assessment of the network, we deﬁne four events (A) for further
assessment:
1. Attacks less than 6 Gbps which will be absorbed by the network robustness
and will go by unnoticed by the users (A1).
2. Attacks ranging 6–9 Gbps can cause reduction of service in the network (A2).
3. Attacks ranging above 9 Gbps will cause DoS together with day-to-day use
(A3).
4. Attacks ranging from approximately 50 Gbps carry the potential for caus-
ing damage at the Internet Service Provider (ISP) level but carry the same
consequences for the institution (A4).
Attacks need to be able to generate a traﬃc within the ranges of scenarios A2–
A4 to be considered a threat potential threat in the case study, for illustration
purposes, we only considered volumetric and ﬂood-based attacks. The Institu-
tion’s vulnerability is then the generic network capacity; we assume that no
vulnerable services are running on the Institution’s internal network. Volumet-
ric and ﬂood based attacks aims to saturate the amount of connections to the
Link, through UDP (User Datagram Protocol) ampliﬁcation generating a small
amount of data from the attacker resulting in a lot of data traﬃc to the victim.
UDP DDoS attacks exploit the fact that the UDP does not require a hand-
shake to transmit data, and requires the service to return more bytes than the

192
G. Wangen et al.
attacker sent with spoofed source IP. Hilden [10] provides the following exam-
ple, services running a vulnerable CharGen (Character generator protocol) can
be exploited to generate traﬃc: the attacker sends a 1-byte sized packet with a
spoofed IP (the target’s IP) to the vulnerable servers. Due to no handshake, the
servers immediately responds with a 1024 byte large packet to the target IP. The
attacker can amplify his traﬃc (bytes sent) with 1024x (bytes received by the
target) by exploiting one vulnerable server. The Table 2 represents the attacker’s
bandwidth limits the attack.
The UDP ampliﬁcation attack requires access to either a botnet or vulnerable
service, both of which are readily available on the Internet, the former for hire
and the latter for exploitation. The technical expertise required to launch an
attack is low, where the trick is to locate vulnerable services through scans.
The attacker can create traﬃc volumes in the ranges A2–A4, whereas attacks
within ranges A2 and A3 are easily achieved with a low number of vulnerable
services, Table 2. The A4 scenario requires more resources regarding bandwidth
and services, but is still easily achieved for the technically skilled.
With a 10 Gbps connection, the Institution is inherently vulnerable to DDoS
attacks, and since this is an attack on availability, the duration of the attack is
also important to consider. We have deﬁned the following downtime scenarios
according to the Institution’s risk tolerance:
1. Attack ranging between 0–10 min are considered negligible (B1).
2. 11–30 min will produce a slight loss in production (B2).
3. 31–120 min will produce a moderate loss in production, it is also likely that
employees will seek out the helpdesk and cause extra overhead (B3).
4. 2–24 h will produce a critical loss in production, at this point everyone will
have exhausted their tasks that can be solved without connectivity (B4).
5. >24 h will qualify as a catastrophe (B5).
The Institution is exposed to volumetric and ﬂood-based attacks due to ease of
exploitation and eﬀective ampliﬁcation. Attacks ranging within A2–A3 are easily
achievable with an initial technical insight, while ability to maintain the attack
up to scenarios B3–B4 depend on a number of externalities that have a high level
of uncertainty related to them, such as internal reaction time, threat capacity,
and ISP capabilities. We address uncertainty related to the threat actor in the
next section.
Case Threat Assessment Results. Based on the exposure assessment, we
identify and assess one threat actor in the position to trigger the attacks. For
the threat actor, we consider the motivation, intention, willingness, capacity, and
capability, to determine threat severity. The ampliﬁcation attacks in question
are easy to implement as long as vulnerable services are running, so, the analyst
should consider less able attackers. However, for the case study we consider only
one threat actor based on the estimated properties regarding the speciﬁcally
analyzed DDoS attack:

Risk Assessment of a DDoS Attack
193
Actor 1 is the politically motivated hacktivist whose weapon of choice is com-
monly the DDoS attack. Due to some of the research conducted in the Institution
being controversial, they are the a potential target of Actor 1. We estimate the
capacity for maintaining a lengthy attack (B3–B4) as Moderate and the capa-
bility for launching the attacks A2–A5 as Very high. It is uncertain whether this
actor has been observed attacking their networks in the past, Table 3.
Table 2. Examples of approximate ampliﬁcations by exploiting vulnerable UDP,
including possible ampliﬁcation of the 100 Mbps connection. Data source: Hilden [10],
Norwegian Security Authority (NSM)
Protocol
Ampliﬁcation Ratio 100 Mbit/s ⇒
NTP
1:556
55.6 Gbit/s
CharGen
1:358
35.8 Gbit/s
QOTD
1:140
14 Gbit/s
Quake (servers)
1:63
6.3 Gbit/s
DNS (open resolver) 1:28–54
2.8–5.4 Gbit/s
SSDP
1:30
3 Gbit/s
SNMP
1:6
600 Mbit/s
Steam (Servers)
1:6
600 Mbit/s
Table 3. Threat assessment for DDoS attack, K represents conﬁdence in the estimates
Threat actor
Motivation
Intention
Capacity
Capability
Willingness
K
Threat severity
Actor 1
Political
Disruption
Moderate
Very high
Moderate
Low
High
Actor 2
Military or
Intelli-
gence
Access
Very high
Very high
Very low
Medium
Medium
Actor 3
Self-
assertion
Deny access
Low
Medium
Very high
High
Medium
Control Assessment Case Results. We provide a description of countermea-
sures for the considered attack, together with an estimation of eﬃciency which,
for reactive controls, can be measured in time until the attack is mitigated.
In the case organization, the ﬁrst and primary control strategy is to ﬁlter vul-
nerable UDP protocols on ingress network traﬃc. This control limits the attack
surface of the organization’s network and limits the eﬀectiveness of exploiting
vulnerable UDP based protocols. This control does not completely mitigate the
possibility of attack because there is still network nodes that need to respond
to UDP like Network Time Protocol and Domain Name System, but these are

194
G. Wangen et al.
Table 4. Control eﬃciency estimation. K represents conﬁdence in the estimates
Control objectives Control domain Control class K
Control eﬃciency
1. Filter UDP
traﬃc
Logical
Preventive
Medium Medium
2. Agreement
with upstream
ISP
Organizational
Reactive
High
High
conﬁgured to provide low possibility for ampliﬁcation values so that threat actors
cannot eﬀectively use them for attacking other systems on the Internet.
The second available mitigation strategy is to have a close cooperation with
the Internet service provider’s CSIRT. This control is vital because of the ISP’s
capabilities to blackhole (null-routing), rate-limit or even block network traﬃc
that originates outside of their own network, or the country itself. For large DDoS
attacks, the ISP is the only one capable of ﬁltering away this traﬃc eﬃciently.
On a day-to-day basis and within normal work hours, to involve the ISP CSIRT
to start shaping or blocking traﬃc is highly eﬀective and possible to implement
within 1 to 2 h. After working hours, 2 to 5 h is estimated.
3.1
Events and Results
The Event outcomes describes the range of outcomes of the event, consisting
of asset, vulnerability, threat, and control, and how it aﬀects the stakeholders
and the organization. The process consists of identifying and describing the likely
outcome(s) of the event regarding breaches of conﬁdentiality, integrity, and avail-
ability, which does not entail calculations of consequence, as this is performed
in the risk analysis. For example, an event outcome can have a ﬁnancial impact
or an impact on reputation.
The qualitative risk assessment shows that the most severe risk facing the
organization is a DDoS campaign in the ranges A3–A4 (>9 Gbps) and lasting
longer than 2 h (B4–B5). The Institution is currently vulnerable to such attacks
due to the dependency on connectivity for running business processes. There is
currently one politically motivated threat actor with a high capability of launch-
ing such an attack, but a moderate capacity for maintaining a lengthy campaign.
We estimate the existing controls to be quite eﬃcient to mitigate UDP ampli-
ﬁcation attacks, although the upstream ISP option includes third party depen-
dencies which the institution does not control and introduces another layer of
uncertainty. We continue the ISRA with the quantitative assessment of available
real-case data from Akamai in the next section.
4
Quantitative Risk Analysis
The
Risk
analysis
phase
consists
of
estimating
risk
concerning
R
=
f(A, C, U, P, S, K).
We
assign
the
identiﬁed
adverse
outcomes,
Sect. 3.1,

Risk Assessment of a DDoS Attack
195
probability according to previous observations and subjective knowledge.
A (event) is the result of the risk identiﬁcation process and in the analysis
described as a range of adverse outcomes based on the consequence calcula-
tions. There are primarily two approaches to probability, frequentist or subjective
knowledge-based assessments (quantitative and qualitative). This section starts
with the quantitative risk approach, before combining it with the qualitative
results to obtain the risk.
4.1
Risk Calculations
The goal of the risk estimation is to reduce U related to risk occurring. For P&C
calculations, we suggest merging the objective data gathered through observa-
tions and statistics with the subjective knowledge-based probabilities. We deﬁne
the following:
– Quantitative Assessment (Objective data) - prior frequencies of occurrence,
including past observations of the risk and generic risk data used to derive
objective measurements of probability. Together with the gathering of relevant
metadata through observations made by others.
– Qualitative Assessment (Knowledge-based data) - a combination of knowledge
that is speciﬁc to the organization and the threat it is facing. Primarily derived
from the risk event components, Sect. 3.
– Risk Estimate - The ﬁnal estimate of the probability for the risk, derived from
quantitative and qualitative data.
The consequence estimation is derived primarily from two factors, monetary
loss and intangible losses such as loss of reputation. Besides, the consequence
estimation should consider the organizational objectives and goals [3]. The loss
calculation is challenging as complex systems may fail in unpredictable ways.
Possible data sources and input for consequence/impact considerations: prior
loss data, monetary losses, consequences for organizational goals and objectives,
and risk speciﬁc factors such as response time and attack duration.
Observed Frequencies of DDoS Attacks. By monitoring activity, we can
obtain reliable numbers on how large the average DDoS attack and generate cor-
responding reports. The data applied in this article was provided by Akamai [1],
and is based on 4,768 valid observations from 2014–2015, shown in the Table 5.
There was no observed attack magnitudes over 255 Gbps in the data set. The
observed frequencies of attacks towards the case study institution averaged two
annual attacks during the last ﬁve years, Pocc = 1
6 ≈17 % of monthly occur-
rence, none of which have succeeded in attaining the necessary magnitude to
achieve DoS. One of which managed to cause instability in the wireless network,
thus, classifying as an A2 scenario.
Further, to test our hypothesis about the distribution of the data we used Q-Q
plot, depicted in the Fig. 3. The plot shows the dependency between the observed
data and expected data according to Gamma distribution prediction. Also,

196
G. Wangen et al.
Table 5. Frequencies of DDoS magnitude observations from Akamai dataset [1].
Characteristic
Valid
Missing
Mean
Median
Std. Dev.
Minimum
Maximum
Duration
4768
0
154,931.00
48,180.00
622,073.00
600
29,965,740.00
Gbps
4768
0
6.09
1.50
15.63
10−5
249.00
Fig. 3. Fitting DDoS Magnitude and duration data set by means of Q-Q Plot using
γ-distribution. Two outliers are evident at the high end of the range for both distrib-
utions.
one can see two outliers at the high bandwidth interval indicating either unusual
events or possible error in logging the characteristics of the events.
Observed Values for Impact Estimation. By monitoring activity, we can
also obtain reliable numbers on the duration of DDoS attacks and generate
distributions. Our data provides us with Table 5, the data shows that the doc-
umented DDoS durations observed in this period were in the range from 600
up to 29 · 106 s, the longest lasting attack lasting approximately 347 days with
magnitudes reaching about 4 Gbps. Removing two outliers from the data set
gives a new mean value equal to 1.4 · 105 s. The Fig. 4 displays the data clus-
tering in the area around the mode and median. The majority of the data are
distributed in this particular interval. In the case of probabilistic estimation, it
means that the data located far from this region are going to have a negligible
level of occurrence.
Our tests showed that there is no correlation between the variables “attack
duration” and “attack magnitude”. There is a small diﬀerence between the mean
attack durations in the considered outcomes, but it is not statistically signiﬁcant,
Table 6. The A3 attacks seem to have shorter durations than the other; the
one-way ANOVA (Analysis of variance model) shows that these two groups of
observations are similar only to signiﬁcance P = 85 %. Yet, if we combine the A3
and A4 attacks this mean duration rises, and there is no signiﬁcance.
Figure 5 depicts the correlation between duration and magnitude, where the
attacks from the A1 and A2 scenarios are distributed nearly uniformly across the

Risk Assessment of a DDoS Attack
197
Fig. 4. Histogram of DDoS magnitudes and durations with normal curve, without two
largest outliers. Data Source: Akamai [1]
Table 6. Frequencies for the deﬁned events, A. Data Source: Akamai [1]
Scenario
Magnitude
Gbps
Mean
Median
N
Std. Dev
% of attacks
P(Pocc ∧A)
A1
<6
159,956.64
48,900
3,713
682,039.967
77.9
13.2 %
A2
6–8.9
162,124.35
44,700
331
450,382.579
6.9
2.6 %
A3
9–49.4
117,437.50
46,080
624
259,646.272
13.1
1.8 %
A4
>49.5
178,485.20
52,380
100
284,012.424
2.1
0.4 %
duration scale. It means that the nature of such attacks is more random and non-
deterministic, which was also conﬁrmed by our correlation tests. Going further,
one can see that the majority of the attacks from the range of A3 are located in
the duration range around 103 · · · 106 s. Finally, same stands for the scenario A4,
where the dispersion of possible magnitudes is large in comparison to A3. How-
ever, much higher frequency in case of probabilist model suppresses less frequent
cases, while fuzzy logic describes data independently from the frequency of its
appearance, only taking into consideration its possibility as described before by
Shalaginov et al. [14].
4.2
Probabilistic Modeling for Risk Estimation
Unplanned downtime is an adverse event for which most ICT-dependent organi-
zations need to have contingencies. The Institution considered in this paper have
deﬁned the severity metrics in Table 7, ranging from “Negligible” to “Catastro-
phe”, together with the distribution of duration within the deﬁned intervals.
Losses are considered to be moderate up to two hours downtime, as most employ-
ees will be able to conduct tasks that do not require connectivity for a short
period. Losses are estimated to start to accumulate after 2 h of downtime. The
analysis shows that the deﬁned events B3–B5 are over 99 % likely to last more
than 2 h, which falls well outside of the Institutions risk tolerance. The condi-
tional probability that the institution will suﬀer DDoS events in a given month

198
G. Wangen et al.
Fig. 5. Bubble plot of the attack bandwidth depending on the duration for each sce-
nario. Size of the bubble also denote magnitude of the attack. Scenarios are depicted
with diﬀerent colours (Color ﬁgure online).
Table 7. Overview attack severity for the case study and duration frequencies. Data
Source: Akamai [1]
Outcome Interval (min) Seconds
Severity
Frequency % of Attacks
B1
0–10 min
0–600
Negligible
1
0.0
B2
11–30 min
601–1,800
Slight
1
0.0
B3
31–120 min
1,800–7,200
Moderate
28
0.6
B4
2–24 h
7,201–86,400 Critical
3,346
70.2
B5
> 24 h
> 86400
Catastrophe 1,392
29.2
is described in Table 6, right column. The risk estimation is modeled as an Event
tree, Fig. 6, based on conditional probabilities P(Pocc ∧A ∧B).
Sensitivity. The most sensitive numbers for the risk calculation is the Pocc,
which is based on approximately ten observations from the last ﬁve years. The
low amount of observations makes the mean sensitive to changes and one can
capture this aspect in the analysis by assigning ranges to Pocc instead of concrete
numbers. A probability range will help to make the assessment more robust, by
for example adjusting for a range of 1–6 (or more) occurrences of DDoS attacks
every year.
5
Discussion and Conclusion
In this section, we discuss the possibility of adjusting the risk model with addi-
tional qualitative input and propose an expanded model. We then discuss the
limitations of the work and the potential future directions for the work.

Risk Assessment of a DDoS Attack
199
Fig. 6. Event tree displaying probability of monthly DDoS occurrence for the case
study.
5.1
Adjusting for Knowledge-Based Probability Estimations
The primary objective of the ISRA process is to provide the decision-maker with
as good a decision basis as possible. The beneﬁt of the quantitative analysis is
that the results are grounded in reality and defensible in a risk communication
process. From the other side, the advantage of the qualitative risk assessment is
that it allows more dynamic risk assessments. The main fragility of quantitative
approaches is the dependence on the data quality and quantity of observations.
We know about the fast-paced developments in ICT, for example, Fig. 1, showed
the progress in capacity for DDoS attacks, and that attack trends may vary which
have implications for the annual occurrence (discussed in [18]). The duration
and magnitude of γ distributions should be more stable although the observed
values are likely to increase according to the trend. However, the limitation of
quantitative risk assessments is that attacks may not be present in the dataset,
which makes the probabilistic approach less ﬂexible as conducted in Sect. 4.2. It
means that there is a need to have a control or introduce an additional factor
that may indicate the possibility of the attacks.
One speciﬁc ﬁnding is the Control eﬃciency, Table 4, in which we have iden-
tiﬁed one proactive and one reactive control in place to mitigate an attack. For
this discussion, we disregard the proactive control Filter UDP traﬃc as attacks
have been occurring at a regular rate even with this control in place. We consider
the reactive control, Agreement with upstream ISP, as a part of the risk assess-
ment, where, during the workday we can expect an attack to be mitigated within
1–2 h, and after working hours the handling time is between 2–5 h. Although our
quantitative analysis, Fig. 6, shows the combined risk of a monthly DDoS attack
ranging from critical to a catastrophic loss at ≈2, 3 %. Further, if we include
the control eﬃciency assessment we can adjust down the risk estimate for DDoS

200
G. Wangen et al.
attacks lasting longer than two hours. A caveat here is that we must consider
the event of control failure, in this case, we have a high degree of knowledge
about the control eﬃciency and can put more trust in its functionality. How-
ever, third party dependency always comes with uncertainties due to information
asymmetry problems between the service provider and the institution.
We also have the opportunity to adjust Pocc estimates based on the threat
assessment, which applies to cases where the attacker attributes changes, for
example, willingness to attack in the case of controversial political events. A
thorough threat assessment is likely the best data source for more technical and
rarer attacks than the DDoS. An understanding of the threats intention and
motivation will also provide a better understanding of possible consequences.
The qualitative risk assessment shows that the Institution is facing one seri-
ous threat actor who both has the capacity, capability, and moderately willing
to launch an attack. At the current time, the UDP-based ampliﬁcation attack
vector is easily exploitable and can generate traﬃc far beyond system limits to
achieve all adverse scenarios between A2-A4. Which means that threat actors
with less capacity and capability will be able to produce more powerful attacks.
For a more technical and resource intensive attack, it would make sense to con-
sider the threat assessment where the more resourceful threats are linked to the
more advanced attacks, for example, Threat Actor 2 (Table 3) is more likely
to be behind attacks in the critical to catastrophic loss events. Actor 3 will be
responsible for most attacks, but due to his limitations in capacity and capa-
bility; attacks will primarily be limited to short lasting and small magnitude
attacks. While Actor 2 is rarely observed, but can launch the catastrophic range
attacks.
Taking into account both the threat and control assessments, we modify
the Event tree to accommodate the qualitative assessment. For the combined
assessment, we consider control eﬃciency concerning subjective ranges for P of a
successful attack with Control 2 in place. To operationalize the threat assessment
in the model, we have visualized our estimated attack ranges assigned to the
identiﬁed threat actors in the left column, Fig. 7.
5.2
Limitations and Future Work
Our work has proposed an approach on how to combine quantitative and qual-
itative risk estimates. However, there is a limitation in our model due to the
combination of the subjective and statistical assessments. We believe that appli-
cation of possibilistic models such that Fuzzy Logic may help to understand the
reasoning of statistical models better when the probabilities of two events are
nearly equal and are very small. It means that the diﬀerence between two similar
events can be below the limit of computing error because the event falls under the
category of what Taleb deﬁnes as Extremistan (see [15,18]). Therefore, applying
a combination of subjective and objective estimators, we will be able to achieve
better generalization of the model. Another way to improve the methodology
is to use hierarchical models that ensemble inference of human-understandable
Fuzzy Rules (also used for decision support) into a comprehensive framework.

Risk Assessment of a DDoS Attack
201
Fig. 7. Expanded event tree also including subjective estimates of threat actors and
control eﬃciency.
We propose to apply our approach to model other cyber risks for further
validation. The risk considered in this paper is a very technical communications
risk, and the risk model would beneﬁt from testing in areas where historical
data is less available. Another limitation is the limited generalization of our case
study; the ISRA approach should also be applied to other types of organizations.
5.3
Conclusion
In this paper, we have proposed and applied classes and estimators for qualita-
tive ISRA, which should contribute towards making the overall risk assessment
process easier and more comprehensive. Our work shows that applying statistical
methods for a cyber risk is feasible as long as there is data available. Moreover,
with more accurate data there are possibilities for even more accurate and better
quality models. Also, we adjusted the quantitative risk estimates with qualitative
ﬁndings, for example, the deﬁnitions of scenario events (A and B) were based on
qualitative measures of vulnerability and applied to categorize objective data.
This paper also took the merging further by implementing the ﬁndings from the
qualitative threat and control eﬃciency assessments into the probabilistic model.
The control estimation is crucial to the risk estimation as it directly aﬀects the
estimation result, which in our case study made the most severe outcomes very
unlikely. Thus, the conclusion is that combination of both the qualitative and
quantitative aspects of ISRA is both feasible and beneﬁcial. Deﬁning an ISRM
method as either-or in this manner may cause the risk analyst to miss out on
valuable information for the assessment.
Acknowledgements. The authors acknowledge Professors Einar Snekkenes, Katrin
Franke, and Dr. Roberto Ferreira Lopes from NTNU, Anders Einar Hilden from the
Norwegian Security Authority (NSM), Karine Gourdon-Keller, David Fernandez, and

202
G. Wangen et al.
Martin McKeay from Akamai. Also, the support from the COINS Research School for
InfoSec is highly appreciated. Lastly, we acknowledge the contributions made by the
anonymous reviewers.
References
1. 2014–2015 DDoS attack duration and magnitude dataset. Technical report, Akamai
Technologies (2015)
2. Information technology, security techniques, ISMS, overview and vocabulary,
ISO/IEC 27000:2014 (2014)
3. Information technology, security techniques, information security risk management,
ISO/IEC 27005:2011 (2011)
4. Aven, T.: Misconceptions of Risk. Wiley, New York (2011)
5. Aven, T.: The risk concept - historical and recent development trends. Reliab. Eng.
Syst. Saf. 99, 33–44 (2012)
6. Blakley, B., McDermott, E., Geer, D.: Information security is information risk
management. In: Proceedings of the 2001 Workshop on New Security Paradigms,
pp. 97–104. ACM (2001)
7. Caralli, R.A., Stevens, J.F., Young, L.R., Wilson, W.R.: Introducing octave allegro:
Improving the information security riskassessment process. Technical report, DTIC
Document (2007)
8. Freund, J., Jones, J.: Measuring and Managing Information Risk: A FAIR App-
roach. Butterworth-Heinemann, Newton (2014)
9. Gregory, P.H.: All in One - CISA - Certiﬁed Information Systems Auditor - Exam
Guide. McGraw-Hill Companies, New York (2012)
10. Hilden, A.E.: UDP-Based DDoS Ampliﬁcation Attacks. Norwegian Security
Authority (NSM). Lecture held at NTNU (Gjøvik), 7 October 2015
11. Kahneman, D.: Thinking, Fast and Slow. Macmillan, New York (2011)
12. Kaplan, S., Garrick, B.J.: On the quantitative deﬁnition of risk. Risk Anal. 1(1),
11–27 (1981)
13. Pipkin, D.L.: Halting the Hacker: A Practical Guide to Computer Security, 2nd
edn. Pearson Education, New York (2003)
14. Shalaginov, A., Franke, K.: A new method of fuzzy patches construction in neuro-
fuzzy for malware detection. In: IFSA-EUSFLAT. Atlantis Press (2015)
15. Taleb, N.N.: Errors, robustness, and the fourth quadrant. Int. J. Forecast. 25(4),
744–759 (2009)
16. Taleb, N.N., Swan, T.B.: The Impact of the Highly Improbable, 2nd edn. Random
House LLC, New York (2010)
17. Wangen, G., Hallstensen, C., Snekkenes, E.: A framework for estimating informa-
tion security risk assessment method completeness - core uniﬁed risk framework.
Submitted for Review (2016)
18. Wangen, G., Shalaginov, A.: Quantitative risk, statistical methods and the four
quadrants for information security. In: Lambrinoudakis, C., Gabillon, A. (eds.)
CRiSIS 2015. LNCS, vol. 9572, pp. 127–143. Springer, Heidelberg (2016). doi:10.
1007/978-3-319-31811-0 8
19. Wangen, G., Snekkenes, E.A.: A comparison between business process management
and information security management. In: Paprzycki, M., Ganzha, M., Maciaszek,
L. (ed.) Proceedings of the 2014 Federated Conference on Computer Science and
Information Systems, vol. 2, pp. 901–910. IEEE (2014). Annals of Computer Sci-
ence and Information Systems

Moving Target Defense Against Network
Reconnaissance with Software
Deﬁned Networking
Li Wang and Dinghao Wu(B)
College of Information Sciences and Technology,
The Pennsylvania State University, University Park, PA 16802, USA
{lzw158,dwu}@ist.psu.edu
Abstract. Online hosts and networks are easy targets of network
attacks due to their static nature, which creates an information asym-
metry and makes them easy to attack and hard to defend. To break the
asymmetry, Moving Target Defense was proposed to bring uncertainties
to computer systems. It can be applied to all levels of protections, cov-
ering applications, system software, operating systems, and networks.
In this paper, we present, Sniﬀer Reﬂector, a new method to practice
Moving Target Defense against network reconnaissance, which is usually
considered as the very ﬁrst step of most attacks. Sniﬀer Reﬂector employs
Software-Deﬁned Networking to disturb network reconnaissance. We use
virtualization to provide an obfuscated reconnaissance result for attack-
ers. Our method can be easily combined with existing security tools for
network forensics as well. We have developed a prototype in a virtual
local area network. Our experiment results show that Sniﬀer Reﬂector is
eﬀective and eﬃcient in blurring various network reconnaissance.
Keywords: Network reconnaissance · Network reﬂector · Software-
Deﬁned Networking · Moving Target Defense · Shadow Networks
1
Introduction
Online hosts and networks are easy targets of various attacks due to their static
nature. Under the current Internet architecture, it is not easy for networked com-
puter systems to change their network parameters once being established. Most
networked services are deployed with a set of well-devised computing infrastruc-
ture and serve in a stable network environment. For example, typically, a web
server open to public visit will be deployed with a ﬁxed domain name and con-
nected to a physical network device, router or switch, and assigned a public IP
address locatable on the Internet. Once deployed, the web server’s network para-
meters will not be changed frequently. This is a good practice because users can
easily get online service through the server’s domain name or IP address. How-
ever, this also exposes valuable network information to attackers for malicious
use. Theoretically, attackers have unlimited time to study the server’s network
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 203–217, 2016.
DOI: 10.1007/978-3-319-45871-7 13

204
L. Wang and D. Wu
environment and ﬁnd out a method to ﬁnally take over it. Although existing
security tools, like ﬁrewalls and intrusion detection systems, can prevent most
common attacks, these protections are essentially static and cannot change the
static nature of the online servers. The static nature of online hosts and networks
leads to an asymmetry between the attackers and defenders, and the attack and
defense game is always unfair.
To change the attack and defense game, Moving Target Defense (MTD) was
proposed to break the asymmetry between the attacker and defender. By intro-
ducing uncertainties and diversiﬁcations, MTD provides a dynamic defense envi-
ronment for adversaries. Adversaries are forced to reprobe, reassess and restudy
the protected environment. For example, an MTD strategy can periodically
change a part of an operating system that makes it harder for attackers to ﬁnd
a known vulnerability for a speciﬁc OS. With MTD, it is hard for adversaries
to decide and verify the authenticity of the obtained information. Therefore,
adversaries can hardly start eﬀective attacks in an MTD protected environment.
Inspired by the promising defense philosophy of MTD, a lot of research has
been proposed [8] to seek adaptive, dynamic, and practical security solutions for
modern computer systems.
Software-Deﬁned Networking (SDN) is a rising network technology which
oﬀers suﬃcient control ﬂexibilities for users to modify network behaviors on the
ﬂy. Consider the fact that more and more critical services are moved from oﬄine
to online, network naturally becomes the top attack vector in most attack scenar-
ios. Attackers have to make use of network infrastructures to achieve their attack
goals. Typically, there are four attack phases for an attack over network [13]:
network reconnaissance1, targeting vulnerable machines, ﬁnding exploits, and
conducting eﬀective attacks. Each phase represents an attack step. For example,
network reconnaissance is used to collect eﬀective information in a target net-
work, like how many nodes are alive, what are the versions, and so on. It provides
valuable information for an attacker to conduct the rest attack steps. Existing
research results show that, port scan should be regarded as the initial step of
the cyber attack routine [2,6], and more than 70 % of the network scans [16] are
connected with attack activities. We manage to use MTD to mitigate network
reconnaissance with the SDN technologies.
Scan is a special network activity, which can be used both for protectors and
attackers. Protectors, like security engineers and network administrators, use
network scans to assess the security status of a target network. In most cases,
scan activity is regarded highly dangerous in real networks. In our work, unless
with a special mention, we recognize network scan as an attack activity.
In this paper, we present Sniﬀer Reﬂector, a new MTD method against net-
work scans with SDN. Diﬀerent from traditional protection mechanisms, Sniﬀer
Reﬂector does not block or drop network scan traﬃc. Instead, it reﬂects scan
traﬃc to a shadow network where scan replies are generated and obfuscated.
Shadow network can simulate arbitrary network structures and services with an
1 Technically, the terms network reconnaissance and network scan are exchangeable
when describing network probe activities. We use them equally in this paper.

Moving Target Defense Against Network Reconnaissance with SDN
205
acceptable overhead. Therefore, attackers can only obtain obfuscated network
views from a shadow network instead of a target network. With our method,
attackers can no longer collect eﬀective network information through network
reconnaissance. Consequently, it is hard for attackers to continue the rest three
attack phases and ﬁnish desired attacks.
In summary, the main contributions of Sniﬀer Reﬂector are as follows:
– Provide a new MTD method against network reconnaissance. As far as we
know, it is the ﬁrst work that employs MTD on network scan. We try to pre-
vent attackers from collecting eﬀective network information through network
scan. Consider attackers rely on scan responses to collect vulnerability infor-
mation, our method fail attackers by obfuscating the returned scan responses.
– Obfuscate attackers’ view with shadow network. We use shadow network to
provide forged responses to scan traﬃc. Shadow network can establish any
desired network environment to obfuscate attackers’ view, which is an invisible
and isolated environment, and implemented with small overhead through the
virtualization technologies.
– Achieve stealthy protection. Sniﬀer Reﬂector provides “stealthy” protection
against network scan. Here, “stealthy” means an attacker cannot detect the
fact that the responses he obtained are from a shadow network instead of the
target network. Our method ﬁnishes at link layer and does not give any hints
about its existence. As a result, Sniﬀer Reﬂector is invisible to the network
layer and above.
Sniﬀer Reﬂector can be easily combined with other security tools for network
forensic purposes. We have developed a Sniﬀer Reﬂector prototype by using vir-
tualization technologies. It mainly has three components, Scan Sensor, Reﬂector
and Shadow Network, and the three components cooperate together to protect
a target network. Our implementation can be easily deployed in real productive
network environments. The experimental results show that Sniﬀer Reﬂector is
eﬀective and eﬃcient in defending various network scans. We tested our proto-
type in a local area network. The experiment results show attackers can only
receive obfuscated scan responses from the shadow network.
2
Background
2.1
Moving Target Defense
For a long time, the cyber defenses are mainly static. Security analysts follow
the conventional process to deploy protections in a productive network, which
includes accessing information properties, planning defense strategies, deploying
defense technologies and conducting penetration tests. Once the defense is estab-
lished, it will keep running statically as deployed for a long time. Consequently,
adversaries can take time to systematically study the network environment, plan
malicious activities, ﬁnd out a break point and conquer the protected system
ﬁnally.

206
L. Wang and D. Wu
Moving Target Defense tries to increase attack bar for adversaries by intro-
ducing uncertainties and diversiﬁcations to computer systems. It forces adver-
saries to reprobe, reassess and restudy the target systems. Providing dynamic
defense makes MTD a promising research topic in academia. The existing MTD
researches can be categorized at two levels: system level and network level. At
system level [5], researchers tried MTD on operating system, processor architec-
ture, program runtime environment, application source code and binary code,
and so on. At network level [1,21], MTD was practiced with frequent IP address
reshuﬄing, network port remapping, network conﬁgurations adaptation, network
topology mutation, dynamic changes on routing information and IP hopping.
More MTD research details will be discussed in Sect. 6.
2.2
Network Scan
Network scan is composed of a set of network activities systematically collecting
network information from a target network. Based on diﬀerent purposes, network
scan can be divided into three phases, which are illustrated as follows:
Host Detection. The ﬁrst phase of network scan is host detection. Host detec-
tion tries to determine the accessible hosts and their IP addresses in an unknown
network. The most used method for host detection is sending an ICMP echo
request. If the remote host is alive and the request arrives unblocked, an ICMP
echo reply will be answered. When the ICMP reply is received, we know the
remote host is on.
Port Discovery. Port discovery is the second phase of a network scan, which
puts eﬀorts on searching all open network ports of a live host. In general, there
are two scans to discover open ports of a host, UDP scan and TCP scan. The
most dominant one is TCP scan because most valuable services are implemented
in TCP protocol. In this paper, we mainly introduce TCP scan. There are sev-
eral ways to perform TCP scan: (1) Full TCP Scan. scanning host tries to ﬁnish
the classic TCP three-way handshake and establish a full TCP connection with
the target host; (2) Half-Open TCP Scan. Scanning host sends a SYN segment
to a selected port on the target host. If a SYN/ACK reply is received, scan-
ning host knows the selected port is open and the target host is on; if a RST
reply is received, that means the port is closed. It does not establish a complete
TCP connection; (3) Stealth Scan. The word “illegal”means the standard TCP
three-way handshake does not consist any of these segments. Attackers forge
illegal TCP segments (mainly on control bits) to start a Stealth Scan [20], which
includes FIN scan, Xmas scan, NULL scan and so on.
Vulnerability Assessment. Vulnerability assessment is the last phase of net-
work scan. After identifying the live hosts and open ports, an attacker needs to
know further details about a target system, such as OS version, service version,

Moving Target Defense Against Network Reconnaissance with SDN
207
and conﬁguration, to make further attack strategy. Due to most security vulnera-
bilities are OS dependent, it is necessary to ﬁngerprint the OS information. Also,
the protocol and service versions are valuable to attackers as well, for choosing
attack exploits. By ﬁguring out the speciﬁc OS and protocol information, the
attacker can ﬁnd out corresponding vulnerabilities and start eﬀective attacks.
2.3
Software-Deﬁned Networking
Software-Deﬁned Networking (SDN) is a hot topic both in academia and indus-
try. The key innovation of SDN is changing the static nature of network devices
and making them programmable [11]. SDN separates the control function and
forwarding function of a network device, which provides network users abilities
to change network behaviors dynamically. Prior to SDN, network devices are
designed like “dead” boxes. Each network device will be installed with the same
chips and ﬁrmware in the factory. Once delivered, network devices can only work
in a predeﬁned way. If users want to make some changes to their networks, they
have to start over and rebuild the physical network environment, which proves
a time-consuming task. With SDN, users can easily modify network packets,
change traﬃc ﬂow directions, form a new network topology, and so on.
Inspired by the defense philosophy of MTD, we consider putting MTD prac-
tice into network reconnaissance protection. Since SDN provides the power to
modify network behaviors during running, we try to make use of SDN’s ﬂexibil-
ities to modify scan traﬃc ﬂow and provide obfuscated scan replies. If network
scan is obfuscated, the following attack steps will not succeed. We believe our
method can greatly raise the attack bar for attackers.
3
System Architecture
In this section, we present Sniﬀer Reﬂector architecture, which seeks to employ
SDN technologies and shadow networks to provide forged scan responses to
attackers. Figure 1 shows the Sniﬀer Reﬂector architecture.
As can be seen in Fig. 1, Sniﬀer Reﬂector has three main components, Scan
Sensor, Reﬂector, Shadow Network. The wholly protected network environment
is called a protection domain. Each protection domain will have one or more pro-
tected objects. Each object is connected at least one Reﬂector, and each Reﬂector
has one or more protected objects connected. In the ﬁgure, we have two pro-
tected objects in a protection domain, one target host and one target network.
The green node stands for a Scan Sensor, which is responsible for monitoring
network traﬃc. The red node represents an attacker node and sends out scan
traﬃc, which is represented by a red curved arrow. The dash line in red shows
the scan traﬃc is visible to the Scan Sensor when going through the Reﬂector. If
scan traﬃc is detected, the Scan Sensor will send out a reﬂection message to the
Reﬂector. After receiving the reﬂection message, the Reﬂector will manipulate
the scan traﬃc and reﬂect it to a Shadow Network, where scan responses will
be generated. The working process for the Sniﬀer Reﬂector architecture could

208
L. Wang and D. Wu
Shadow Network
…
Attacker
ķ
ĸ
Ĺ
Fig. 1. The Sniﬀer Reﬂector architecture (Color ﬁgure online)
be illustrated as following steps: (1) Scan Sensor will be monitoring the coming
traﬃc for target hosts and target networks. (2) Once scan traﬃc is detected,
Scan Sensor will send a reﬂection message to the corresponding Reﬂector, and
the Reﬂector will follow the message and redirect the scan traﬃc to Shadow Net-
work. (3) Shadow Network receives redirected traﬃc from Reﬂector and disturbs
attackers’ view by making obfuscated scan responses.
3.1
Scan Sensor
Scan Sensor is a scan detection engine in our architecture, which is responsi-
ble for detecting scan activities and generating reﬂection messages. To perceive
scan activities, it is required to observe all the traﬃc happening in a protection
domain. Once scan activities are detected, Scan Sensor will notify Reﬂector,
and Reﬂector and Shadow Network will cooperate together to reﬂect scan traf-
ﬁc. Scan Sensor ﬁnishes two functions: (1) Detecting scan traﬃc. Scan Sensor
monitor the network traﬃc and detect the scan activities. It uses scan detection
algorithms to ﬁnd out possible scan traﬃc. (2) Generating reﬂection message.
Once scan activities are conﬁrmed, Scan Sensor needs to generate a reﬂection
message to notify Reﬂector. The message contains scan type, scan source, scan
target, port info, sensitive level, and action. An example message could be like
this: (TCP Xmas scan, source IP 192.168.2.12, target IP 192.168.1.105, tar-
get port 80, high density, reﬂection ﬂag). Reﬂector will receive the message and
execute the reﬂection action.
3.2
Reﬂector
Reﬂector is in charging of reﬂecting scan traﬃc. It communicates with the Scan
Sensor and the Shadow Network, and executes reﬂection actions to redirect the

Moving Target Defense Against Network Reconnaissance with SDN
209
scan traﬃc to the Shadow Network. In a protection domain, each Reﬂector
will have at lease one protected host or protected network connected. For these
connected objects, Reﬂector should be invisible to them. When there is no scan
activity, Reﬂector acts as a regular network device and provides traﬃc ﬂow
functions to the connected nodes. It maintains the packets switching and traﬃc
management like most network devices do. Consider the scan types may cover
ICMP, TCP, UDP and other protocols, when reﬂecting, Reﬂector should be able
to conduct ﬁne-grained traﬃc control to identify scan traﬃc by protocol. Besides,
to reﬂect scan traﬃc, the Reﬂector needs to provide at least two routes to change
the ﬂow of scan traﬃc. One route is for normal traﬃc, which leads to the target
host; the other route is for scan traﬃc, which goes to the Shadow Network.
3.3
Shadow Network
All the scan traﬃc will be redirected to a Shadow Network. The Shadow Network
is an isolated and invisible network, which is composed of shadow nodes. Except
for the Reﬂector, no node in the protection domain is aware of the existence
of the Shadow Network. Unless receiving scan traﬃc from the Reﬂector, the
Shadow Network does not create any network traﬃc to the protection domain.
A protection domain may have multiple Shadow Networks. And, these Shadow
Networks can cooperate together to simulate a complex network structure. A
typical strategy could be to simulate a replica of the protection domain. The
forged network environment has the same look of the protection domain, such as
the same number of nodes, the same network topology, and conﬁgurations. It can
be used to cheat attackers to believe the reached network is the target network.
Shadow Network can provide further responses if attackers keep attacking.
4
Design and Implementation
4.1
Design Principles
We have three design principles. First, all three components of Sniﬀer Reﬂector
should be trusted and not disturbed by attackers. Moreover, the communications
within Sniﬀer Reﬂector should be invisible to attackers and target nodes. Second,
in the protection domain, no network traﬃc can escape from being monitored,
and Scan Sensor can observe any communication happened in the protection
domain. Last, once being detected and reﬂected, attackers cannot bypass the
reﬂection mechanisms of Sniﬀer Reﬂector.
4.2
Prototype Implementation
Based on our three design principles, we implemented a prototype of Sniﬀer
Reﬂector to provide scan protections in a virtual network. The prototype runs
a protection domain which consists of two parts, a target network and a Sniﬀer
Reﬂector framework. As shown in Fig. 2, the target network has two virtual

210
L. Wang and D. Wu
Reflector
Scan Sensor
Attacker Node
Node n
Node 1
…...
Shadow 
Network 
Scan Traffic:
Normal Traffic:
Target Network
Node 1'
Node n’
Fig. 2. A prototype of Sniﬀer Reﬂector
machine nodes and both of them are connected to Reﬂector. Accordingly, the
Sniﬀer Reﬂector framework has three components, just as we discussed in Sect. 3,
Scan Sensor, Reﬂector and Shadow Network.
We choose to implement the prototype on a virtualization platform, for more
and more organizations are transplanting their services to cloud and data cen-
ter. We employ the Kernel Virtual Machine (KVM) [10] virtualization platform.
On KVM platform, 80 % of instructions on guest machine can be directly exe-
cuted on physical CPU, which provides a high eﬃciency of resource utilization.
We customized a virtual switch, VDE Switch, to run as Reﬂector. Both KVM
guest machines and VDE Switch are running as user processes at host OS. We
illustrate the three components’ implementation details as follows.
We modify Snort to implement a Scan Sensor. There are three working modes
in Snort, sniﬀer mode, packet logger mode and NIDS mode. To ﬁt our purpose,
we take advantage of NIDS mode. Under NIDS mode, Snort allows security
engineers install a set of security rules to detect suspicious events over network.
When a suspicious event is detected, an alert information will be generated and
written to the log ﬁle. We located the source code where the alert information
is generated and inserted an additional module. The module is responsible for
looking scan alert patterns and generating reﬂection messages. The message
includes scan type, scan source, scan target, port info, sensitivity level and action.
Moreover, we developed a communication module sending reﬂection messages to
Reﬂector. More details about message content and scan detection policies will
be given in Sect. 5.
The implementation of Reﬂector is based on a virtual switch, VDE Switch [3].
The basic functionalities of VDE Switch are receiving, processing and forwarding
network packets for the connected nodes. Each connected node will be assigned a
dedicated port number for packets switching purpose. We modiﬁed VDE Switch

Moving Target Defense Against Network Reconnaissance with SDN
211
and inserted it an extra layer for packet checking function. When receiving reﬂec-
tion messages, VDE Switch will translate the messages to reﬂection rules. Reﬂec-
tion rules contain the characteristics of scan traﬃc, such as scan source IP and
protocol information. All the reﬂection rules will be stored in a ReﬂRules List.
All the packets going through VDE Switch will be checked by the list. If any traf-
ﬁc got a match, it will be reﬂected. The modiﬁed VDE Switch is capable of verify-
ing packet headers from link layer to transport layer, which enables VDE Switch
to identify traﬃc ﬂows by protocol. That helps us achieve a ﬁne-grained traf-
ﬁc control. To implement secret channels for communications happened within
Sniﬀer Reﬂector, we designed two reserved port numbers in VDE Switch. These
two reserved port numbers are dedicated to Scan Sensor and Decoy Network. For
each reserved port, we set a switch ﬂag. When there is no scan traﬃc, the switch
ﬂag will be set oﬀand the reserved port is blocked; when there is scan traﬃc
detected, the switch ﬂag will turn on and scan traﬃc will be reﬂected through
reserved ports. The detailed reﬂection decision is demonstrated in Algorithm 1.
Algorithm 1. Scan traﬃc reﬂection decision
Require: The current packet p; The switch ﬂag sf; The set of reﬂection rules:
Refl Rules List; The port used by Shadow Network: SN port;
1: for every packet p do
2:
if ((p.source IP, p.protocol) ∈[Refl Rules List]) &&(sf is on) then send p to
SN port;
3:
end if
4: end for
Shadow Network is implemented with virtualization technologies as well. We
use KVM virtual machines to simulate nodes of Shadow Network. These virtual
machines are connected with virtual network devices to construct a network
structure. On each virtual machine, we deploy the corresponding OS and network
services, which are intended to provide obfuscated scan responses to attackers.
The virtual machines are in full control and can be conﬁgured with arbitrary
network parameters to simulate desired network behaviors. The reason why we
use real virtual machines instead of virtual honeypots/honeynets in Shadow
Network is virtual machine can provide a fully responsible OS and TCP/IP
stack for attackers. Some light-weight virtual honeynets, like honeyd [17], can
be used to simulate a network, but their responses can be easily detected by
attackers.
5
Evaluation
We evaluate our prototype in a virtual LAN 192.168.1.0/24. The entire virtual
network facility is deployed on a physical machine, and all other network nodes
and devices are running as virtual machines. Our testbed was deployed on Intel
Core i7-3370 3.4 Ghz processor with 16 GB RAM. The host machine is running

212
L. Wang and D. Wu
alert tcp any any -> $HOME NET any (msg:”TCP SYN”; ﬂow:stateless; ﬂags:S;
detection ﬁlter:track by dst, count 100, seconds 5; sid:1000001;rev:1)
alert tcp $EXTERNAL NET any -> $HOME NET any (msg:”SCAN NULL”; ﬂow:stateless;
ack:0; ﬂags:0; seq:0; reference:arachnids,4; classtype:attempted-recon; sid:623; rev:6;)
alert tcp $EXTERNAL NET any -> $HOME NET any (msg:”SCAN SYN FIN”;
ﬂow:stateless; ﬂags:SF,12; reference:arachnids,198; classtype:attempted-recon; sid:624; rev:7;)
alert tcp $EXTERNAL NET any -> $HOME NET any (msg:”SCAN XMAS”; ﬂow:stateless;
ﬂags:SRAFPU,12; reference:arachnids,144; classtype:attempted-recon; sid:625; rev:7;)
alert tcp $EXTERNAL NET any -> $HOME NET any (msg:”SCAN nmap XMAS”;
ﬂow:stateless; ﬂags:FPU,12; reference:arachnids,30; classtype:attempted-recon; sid:1228;
rev:7;)
Fig. 3. Scan detection rules in Snort
CentOS 6.0 with kernel 2.6.32 x86 64 and qemu-kvm-0.15.1. Similarly, all other
guest OSes are running CentOS Linux as well. The Scan Sensor virtual machine
is running Snort 2.9.7.5 for scan detection. The Shadow Network is deployed
with two virtual machines as shadow nodes to provide forged scan responses.
The Reﬂector employs VDE Switch 2.3.2.
As can be seen in Fig. 2, we have ﬁve nodes in prototype. The host is running
with 192.168.1.107. Two other nodes, 192.168.1.153 and 198.168.1.154, run Linux
OS as normal nodes of the protection domain. Node 192.168.1.153 is running
with open ports 22, 23, 80, 111 and 443; node 192.168.1.154 is running with open
ports 22, 80 and 111. Shadow Network has two shadow nodes, which both run
Windows XP systems with the same IP addresses as normal nodes, 192.168.1.153
and 192.168.1.154. Both two shadow nodes are conﬁgured to open ports, 53,
135, 139, 445 and 3389. An attacker node runs with IP 192.168.2.1 in subnet
192.168.2.0/24. The attacker node is used to send scan traﬃc and collect scan
results. Besides, Snort and VDE Switch are running as user-space processes on
host OS.
Scan detection rules were conﬁgured to detect TCP SYN ﬂood and TCP
stealth scans on Snort. The content of rules is shown in Fig. 3. The ﬁrst rule
detects TCP SYN ﬂood scan, which is deﬁned as X TCP SYN requests in Y
time period. In our evaluation, we deﬁne TCP SYN ﬂood as any TCP connection
requests sent more than 100 times in 5 s (this assumption can be modiﬁed to
accommodate diﬀerent detection scenarios). Then, the following detection rules
give the details of TCP stealth scans, including FIN scan, NULL scan, and
XMAS scan.
We employ nmap to generate scan traﬃc. To demonstrate the eﬀectiveness
of our protection, we design three types of scans, SYN scan, Xmas scan and
version detection scan. See the nmap commands as follows:

Moving Target Defense Against Network Reconnaissance with SDN
213
1. nmap -sS 192.168.1.0/24
2. nmap -sX 192.168.1.153/154
3. nmap -A -T4 -F 192.168.1.153/154
The ﬁrst nmap command executes a TCP SYN scan for LAN 192.168.1.0/24.
TCP SYN scan probes remote ports through a full TCP three-way handshake.
Since TCP SYN scan is the most typical scan over the internet, most system
logs will default capture TCP SYN scan. Then, the second scan command starts
a TCP Xmas scan on two hosts 192.168.1.153/154. TCP Xmas scan is one of the
TCP stealth scans, which is usually blocked by most ﬁrewalls. The third scan
in our evaluation is a composite scan, which contains a series of scan activities.
The -A option means the scan command will run OS detection, version detec-
tion, script scanning, and traceroute. nmap will match the scan results with
its ﬁngerprint database and estimate the OSes and version information of the
scanned hosts. We execute the scan commands in two rounds. The ﬁrst scan
Table 1. Scan results comparisons on 192.168.1.0/24
Without
Sniﬀer
Reﬂector
Protection
With Sniﬀer Reﬂector Protection
TCP SYN scan nmap
-sS 192,168.1.0/24
192.168.1.107 is up
192.168.1.107 is up
0.00015 s latency
0.00032 s latency
Not shown: 998 closed ports
Not shown: 998 closed ports
Port State Service
Port State Service
22/tcp open ssh
22/tcp open ssh
111/tcp open rpcbind
111/tcp open rpcbind
192.168.1.153 is up
192.168.1.153 is up
0.00029 s latency
0.015 s latency
Not shown: 995 closed ports
Not shown: 995 closed ports
Port State Service
Port State Service
22/tcp open ssh
53/tcp open domain
23/tcp open telnet
135/tcp open msrpc
80/tcp open http
139/tcp open netbios-ssn
111/tcp open rpcbind
445/tcp open microsoft-ds
443/tcp open https
3389/tcp ﬁltered ms-term-serv
192.168.1.154 is up
192.168.1.154 is up
0.00023s latency
0.015s latency
Not shown: 997 closed ports
Not shown: 995 closed ports
Port State Service
Port State Service
22/tcp open ssh
53/tcp open domain
80/tcp open http
135/tcp open msrpc
111/tcp open rpcbind
139/tcp open netbios-ssn
445/tcp open microsoft-ds
3389/tcp ﬁltered ms-term-serv

214
L. Wang and D. Wu
round is executed without the protection of Sniﬀer Reﬂector; the second round
is scanned with the protection of Sniﬀer Reﬂector. Then, we compared the scan
results, which show our prototype is eﬀective and eﬃcient in defending various
network scans.
Table 1 shows the scan results on subnetwork 192.168.1.0/24. The ﬁrst col-
umn is the nmap scan command. The second and third columns show the scan
results without and with the protection of Sniﬀer Reﬂector. As can be seen from
the second column, scan results return all the live nodes in 192.168.1.0/24. For
each live node, the results show scan latency, how many ports are closed, and a
list of open port/state/service information. When there is no Sniﬀer Reﬂector,
the scanner is able to receive network information from the protection domain.
From the returned IP and open ports information, we can see the attacker can
easily collect network information from the target network. The third column
lists the scan results under the protection of Sniﬀer Reﬂector. The listed open
ports match with the virtual machines we conﬁgured as shadow nodes, which
shows the scanner actually obtains the network information from shadow net-
work. Comparing the scan results of two columns, we can tell there is a diﬀerence
in scan latency. With Sniﬀer Reﬂector, the average latency of simulated decoy
service is 0.015 s, which is higher than the latency of real nodes (0.00015 s).
The delay is probably because the reﬂection is implemented in software, and
VDE Switch needs to reﬂect scan traﬃc packet one by one.
Table 2
illustrates
the
TCP
Xmas
scan
results
on
two
hosts
192.168.1.153/154. The nmap scan command is in column one, and the rest
columns demonstrate the scan results received with/without the protection of
Sniﬀer Reﬂector. Diﬀerent from SYN scan, the port state information in Xmas
scan results shows as “o/f”, representing “open or ﬁled”. The scan results show,
when Sniﬀer Reﬂector is on, both two scans are returning simulated results from
shadow nodes. That presents, the scan traﬃc is reﬂected and the scan results
come from shadow network. And, we also got the scan results from the nmap
composite scan. The results show that, with the protection of Sniﬀer Reﬂector,
the composite scan can also be detected and reﬂected in our framework.
Table 2. Scan result comparisons on a single host
Without Sniﬀer Reﬂector Protection
With Sniﬀer Reﬂector Protection
for 153
for 154
for 153
for 154
192.168.1.153 is up
192.168.1.154 is up
192.168.1.153 is up
192.168.1.154 is up
TCP
0.00029s latency
0.00028s latency
0.015s latency
0.0020 latency
Xmas
995 closed ports
997 closed ports
999 closed ports
999 closed ports
scan
Port State Service
Port State Service
Port State Service
Port State Service
22/tcp o/f ssh
22/tcp o/f ssh
3389/tcp o/f m-t-s
3389/tcp o/f m-t-s
nmap -sX
23/tcp o/f telnet
80/tcp o/f http
scanned in 14.28 sec scanned in 14.25 sec
192.168.1.X
80/tcp o/f http
111/tcp o/f rpcbin
111/tcp o/f rpcbin scanned in 14.25 sec
443/tcp o/f https
scanned in 14.26 sec

Moving Target Defense Against Network Reconnaissance with SDN
215
6
Related Work
MTD researches are quite popular in recent years. Many MTD methods have
been proposed to mitigate security threats. The existing researches on MTD
can be divided into two levels, system level and network level. The network
level MTD solutions [1,4] include IP address reshuﬄing, network conﬁguration
randomization, and so on. These methods all tried to obfuscate attackers at net-
work level. The system level MTD methods cover platform [15], runtime environ-
ment [19], and software applications [18]. Kewley et al. [9] proposed to reduce
network attacks using dynamic network reconﬁgurations. They tried to force
attackers to use the outdated network conﬁgurations to prepare an attack. A
live IPv6 version MTD was implemented by Groat et al. [7]. By using DHCPv6
protocol, a hidden connection is built between IPv6 address and DHCP identity,
which could be used to protect sensitive communications in government or con-
ﬁdential organizations. Unlike our method, these methods are still in the phase
of prototype and far from mature for deployment.
Our work is also motivated by SDN researches. OpenFlow [14] is the most
widely accepted SDN protocol. It is designed to dynamically change network
behaviors by using controllers and switches. Controllers are responsible for man-
aging the attached switches and deciding the ﬂow tables of switches, and switches
are using ﬂow tables to forward network traﬃc. Lara et al. [12] provided a sur-
vey on innovations of OpenFlow, which has been used in network management,
traﬃc analysis, fault tolerance, security, and many other areas. SnortFlow [22]
is proposed to integrate intrusion prevention systems (IPS) with OpenFlow on a
cloud. The basic idea is using the IPS alerts to change cloud behaviors. However,
due to the heavy structure of cloud, It is impractical to provide timely reactions
on security events. In comparison, Sniﬀer Reﬂector provides instant changes on
reﬂection actions and can be deployed with no diﬃculties in a real network.
7
Conclusion
The relatively static nature of today’s computing and network systems has led
to an information asymmetry between the attackers and defenders. To break
this asymmetry, in this paper, we present Sniﬀer Reﬂector, a new Moving Tar-
get Defense method against network reconnaissance with Software-Deﬁned Net-
working, to obfuscate the attackers’ view of the target network information. We
have designed and implemented a prototype in a virtual local area network. Our
experimental results with various scan activities show that Sniﬀer Reﬂector is
eﬀective and eﬃcient.
Acknowledgements. This work was supported in part by the NSF Grants CCF-
1320605 and CNS-1223710, and ONR Grants N00014-13-1-0175 and N00014-16-1-2265.

216
L. Wang and D. Wu
References
1. Al-Shaer, E.: Toward network conﬁguration randomization for moving target
defense. In: Jajodia, S., Ghosh, A.K., Swarup, V., Wang, C., Wang, X.S. (eds.)
Moving Target Defense, vol. 54, pp. 153–159. Springer, New York (2011)
2. Allman, M., Paxson, V., Terrell, J.: A brief history of scanning. In: Proceedings of
the 7th ACM SIGCOMM Conference on Internet Measurement, pp. 77–82 (2007)
3. Davoli, R.: VDE: virtual distributed ethernet. In: 1st International Conference
on Testbeds and Research Infrastructures for the Development of Networks and
Communities, pp. 213–220. IEEE (2005)
4. Dunlop, M., Groat, S., Urbanski, W., Marchany, R., Tront, J.: MT6D: A moving
target IPv6 defense. In: Military Communications Conference. IEEE (2011)
5. Evans, D., Nguyen-Tuong, A., Knight, J.: Eﬀectiveness of moving target defenses.
In: Jajodia, S., Ghosh, A.K., Swarup, V., Wang, C., Wang, X.S. (eds.) Moving
Target Defense, vol. 54, pp. 29–48. Springer, New York (2011)
6. Gadge, J., Patil, A.A.: Port scan detection. In: 16th IEEE International Conference
on Networks, ICON 2008, pp. 1–6. IEEE (2008)
7. Groat, S., Dunlop, M., Urbanksi, W., Marchany, R., Tront, J.: Using an IPv6
moving target defense to protect the smart grid. In: 2012 IEEE PES Innovative
Smart Grid Technologies (ISGT), pp. 1–7, January 2012
8. Jajodia, S., Ghosh, A.K., Swarup, V., Wang, C., Wang, X.S. (eds.): Moving Tar-
get Defense - Creating Asymmetric Uncertainty for Cyber Threats. Advances in
Information Security, vol. 54. Springer, New York (2011)
9. Kewley, D., Fink, R., Lowry, J., Dean, M.: Dynamic approaches to thwart adver-
sary intelligence gathering. In: Proceedings of DARPA Information Survivability
Conference and Exposition II, pp. 176–185 (2001)
10. Kivity, A., Kamay, Y., Laor, D., Lublin, U., Liguori, A.: KVM: the Linux virtual
machine monitor. Proc. Linux Symp. 1, 225–230 (2007)
11. Lantz, B., Heller, B., McKeown, N.: A network in a laptop: rapid prototyping for
software-deﬁned networks. In: Proceedings of the 9th ACM SIGCOMM Workshop
on Hot Topics in Networks, p. 19 (2010)
12. Lara, A., Kolasani, A., Ramamurthy, B.: Network innovation using openﬂow: a
survey. IEEE Commun. Surv. Tutorials 16, 493–512 (2014)
13. Liao, H.J., Lin, C.H.R., Lin, Y.C.: Intrusion detection system: a comprehensive
review. J. Netw. Comput. Appl. 36, 16–24 (2013)
14. McKeown, N., Anderson, T., Balakrishnan, H., Parulkar, G., Peterson, L., Rexford,
J., Shenker, S., Turner, J.: Openﬂow: enabling innovation in campus networks.
ACM SIGCOMM Comput. Commun. Rev. 38(2), 69–74 (2008)
15. Okhravi, H., Comella, A., Robinson, E., Haines, J.: Creating a cyber moving tar-
get for critical infrastructure applications using platform diversity. Int. J. Crit.
Infrastruct. Prot. 5(1), 30–39 (2012)
16. Panjwani, S., Tan, S., Jarrin, K.M., Cukier, M.: An experimental evaluation to
determine if port scans are precursors to an attack. In: Proceedings on International
Conference on Dependable Systems and Networks, DSN 2005, pp. 602–611. IEEE
(2005)
17. Provos, N.: Honeyd-a virtual honeypot Daemon. In: 10th DFN-CERT Workshop,
Hamburg, Germany, vol. 2, p. 4 (2003)
18. Rinard, M.: Manipulating program functionality to eliminate security vulnerabili-
ties. In: Jajodia, S., Ghosh, A.K., Swarup, V., Wang, C., Wang, X.S. (eds.) Moving
Target Defense, vol. 54, pp. 105–115. Springer, New York (2011)

Moving Target Defense Against Network Reconnaissance with SDN
217
19. Shacham, H., Page, M., Pfaﬀ, B., Goh, E.J., Modadugu, N., Boneh, D.: On the
eﬀectiveness of address-space randomization. In: Proceedings of the 11th ACM
Conference on Computer and Communications Security, pp. 298–307 (2004)
20. Staniford, S., Hoagland, J.A., McAlerney, J.M.: Practical automated detection of
stealthy portscans. J. Comput. Secur. 10(1/2), 105–136 (2002)
21. Wang, H., Jia, Q., Fleck, D., Powell, W., Li, F., Stavrou, A.: A moving target
DDoS defense mechanism. Comput. Commun. 46, 10–21 (2014)
22. Xing, T., Huang, D., Xu, L., Chung, C.J., Khatkar, P.: Snortﬂow: a openﬂow-based
intrusion prevention system in cloud environment. In: Research and Educational
Experiment Workshop (GREE), Second GENI, pp. 89–92. IEEE (2013)

Uni-ARBAC: A Uniﬁed Administrative Model
for Role-Based Access Control
Prosunjit Biswas, Ravi Sandhu(B), and Ram Krishnan
Institute for Cyber Security,
University of Texas at San Antonio, San Antonio, USA
prosun.csedu@gmail.com, {ravi.sandhu,ram.krishnan}@utsa.edu
Abstract. Many of the advantages of Role Based Access Control
(RBAC) accrue from the ﬂexibility of its administrative models. Over
the past two decades, several administrative models have been proposed
to manage user-role, permission-role and in some cases role-role rela-
tions. These models are based on diﬀerent administrative principles and
bring inherent advantages and disadvantages. In this paper, we present
a uniﬁed model, named Uni-ARBAC, for administering user-role and
permission-role relations by combining many of the administrative prin-
ciples and novel concepts from prior models. For example, instead of
administering individual permissions Uni-ARBAC combines permissions
into tasks which are assigned to roles as a unit. Slightly diﬀerently, users
are assigned to user-pools from where individual users are assigned to
roles. The central concept of Uni-ARBAC is to integrate user-role and
task-role administration into a more manageable unit called an Adminis-
trative Unit (AU). AUs partition roles, tasks and user-pools and they are
organized in a rooted tree hierarchy. Administrative users are assigned to
AUs with possibility of restricting their authority to user-role assignment
or task-role assignment. While most existing models assume existence of
administrative roles for managing regular roles, we present an approach
for engineering AUs based on structured partitioning of roles and tasks.
1
Introduction
Role Based Access Control (RBAC) [6,17] is one of the most widely deployed
and studied access control models. Instead of directly assigning permissions to
users, RBAC assigns permissions to roles and users to roles. While the number of
roles in a large organization might vary from dozens to thousands, the number of
users or permissions could vary from tens of thousands to hundreds of thousands
and even millions. Thus, maintaining the user-role and permission-role relations
are the most commonly carried out administrative actions in RBAC. While some
models also speak to administering the role-role hierarchy [5,15], it is evident
that modiﬁcations to the role-role relationship can have signiﬁcant impact, so
it might be advisable to keep this authority relatively centralized. Hence, we
limit our scope in this paper to decentralized administration of the user-role and
permission-role relations.
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 218–230, 2016.
DOI: 10.1007/978-3-319-45871-7 14

Uni-ARBAC: A Uniﬁed Administrative Model for Role-Based Access Control
219
To a large degree the advantages of RBAC accrue from the ﬂexibility of
administering the permission-role and user-role relations. In this regard, several
administrative models have been proposed in the literature (see Sect. 2). These
models are based on diﬀerent administrative principles and oﬀer inherent advan-
tages and disadvantages. Each one incorporates some novel and putatively useful
concepts relative to the others. To our knowledge, there has been no eﬀort so far
to comprehensively consolidate the various novel concepts introduced in diﬀer-
ent administrative models into a coherent uniﬁed model that potentially brings
together the inherent advantages of the individual models.
In this paper, we present a novel uniﬁed model, named Uni-ARBAC, for
administering user-role and permission-role relations by combining many of the
existing administrative principles and novel concepts. For example, instead of
administering individual permissions, Uni-ARBAC combines permissions into
tasks and assigns tasks to roles. For administrative purposes, Uni-ARBAC decou-
ples users and tasks from roles following the decoupling principle of ARBAC02
[13]. Uni-ARBAC utilizes user-pools as sets of candidate users who can be
assigned to a role, while tasks act as permission-pools. One advantage of using
tasks as permission-pools is that tasks can be designed during role engineering
according to some top-down approaches (e.g. [11]). User-pools on the other hand
can be designed via the organization structure.
Uni-ARBAC integrates user-role and task-role administration into a more
manageable unit, we call Administrative Unit (AU). AUs partition roles, tasks
and user-pools, and are organized in a rooted tree hierarchy. Administrative users
are assigned to AUs with possibility of restricting their authority to user-role
assignment or task-role assignment. The partitioning of roles and tasks across
AUs leads us to propose an engineering process for AUs for given role hierarchy
and/or task hierarchy. The potential for engineering AUs in this manner is a
signiﬁcant advantage of Uni-ARBAC.
This paper makes the following contributions. We have presented a uniﬁed
model (Uni-ARBAC) for administering user-role and permission-role relation
for RBAC. Uni-ARBAC combines several novel concepts and administrative
principles from prior models into a more powerful and manageable unit called
administrative unit (AU). We proposed an engineering approach for developing
AUs. While most other administrative models assume the existence of separate
administrative roles, we relax this assumption and our approach for engineering
administrative units can also be used for engineering administrative roles.
The remainder of this paper is organized as follows. We discuss related work
in Sect. 2 highlighting the concepts we have adopted from prior administrative
models. We present our model in Sect. 3 and some variations of the model in
Sect. 4. Section 5 discusses our approach for engineering AUs. We conclude the
paper in Sect. 6.
2
Background and Related Work
In this section, we review prior models for administering RBAC, emphasizing
those of their driving principles which have been incorporated in Uni-ARBAC.
These concepts and principles are summarized in Table 1.

220
P. Biswas et al.
The value of grouping permissions into a higher level abstraction has often
been recognized in the literature. Task-role based access control (TRBAC) [12]
proposes the notion of a task as a group of permissions which constitute a fun-
damental unit of business work in an enterprise. Similar to TRBAC, two-sorted
RBAC [9] and scenario-based role engineering [11] organize tasks into another
higher level abstract.
One of the central notions of RBAC administration is to separate user-role
and permission-role assignments. Introduced in ARBAC97 [15], this notion is
adopted by many other models including [13,14,16]. Uni-ARBAC accepts this
separation to be at the core of the model.
Another essential concept of ARBAC97 is to keep administration of roles
separate from regular roles. To this end, ARBAC97 introduced the concept of
administrative roles. Uni-ARBAC adopts the former separation principle, but
eschews the use of administrative roles for this purpose. Instead, Uni-ARBAC
introduces a more sophisticated construct of Administrative Units to achieve the
desired separation.
Table 1. Concepts motivating Uni-ARBAC (* denotes source of the concept)
Concepts and principles
ARBAC97
[15]
ARBAC02
[13]
SARBAC
[4,5]
UARBAC
[10]
Role
graph
model [18]
Uni-ARBAC
Task and task hierarchy
✓
Separation of user and
permission administration
✓*
✓
✓
✓
✓
Separation of regular roles from
administration
✓*
✓
✓
✓
User pools and user pool
hierarchy
✓*
✓
Administrative structure design
✓*
✓
✓
Reversibility and administrative
structure flexibility
✓*
✓
Senior most administrators
✓*
✓
ARBAC02 [13] is another inﬂuential model for administrative RBAC. It doc-
uments a number of problems with ARBAC97 and introduces the notions of
user-pools and permission-pools. Uni-ARBAC adopts the user-pool and user-
pool hierarchy from ARBAC02, while on the permission side it adopts the task
and task hierarchy from [12] as discussed above.
Crampton et al. developed a model called SARBAC [4,5] based on the con-
cept of administrative scope, which conﬁnes the side eﬀects of role hierarchy mod-
iﬁcation in a highly disciplined manner relative to ARBAC97. Notably, adminis-
trative scope becomes a means to deﬁne administrative roles which are otherwise
assumed to be given in ARBAC97 and ARBAC02. Administrative scope is math-
ematically deﬁned based on the given role hierarchy. Uni-ARBAC incorporates
the general notion that the role hierarchy should inﬂuence the administrative
structure. However, it departs from the strict mathematical deﬁnition of SAR-
BAC to accommodate a heuristic top-down approach in designing administrative
units, based on the role-hierarchy and task-role allocation.

Uni-ARBAC: A Uniﬁed Administrative Model for Role-Based Access Control
221
The UARBAC [10] model proposes a number of principles for RBAC admin-
istration, such as scalability and ﬂexibility, psychological acceptability and econ-
omy of mechanism. As noted earlier Uni-ARBAC departs from UARBAC on the
question of whether or not administrative permissions should be assigned to reg-
ular roles. However, all the other principles of UARBAC are considered similarly
desirable in Uni-ARBAC. As it stands some of the UARBAC principles, such as
psychological acceptability and scalability, are qualitative and diﬃcult to con-
vincingly claim for a given model. Here we conﬁne our attention to the two prin-
ciples of reversibility and administrative structure ﬂexibility, which have been
explicitly adopted from UARBAC into Uni-ARBAC. The reversibility principle
requires that administrative operations should be reversible. This is incorpo-
rated in Uni-ARBAC by coupling grant and revoke operations for user-role or
task-role assignment in a single administrative unit. The principle of adminis-
trative structure ﬂexibility (called policy neutrality in [10]) argues against the
tight coupling of administrative structure to role hierarchy, such as in SARBAC.
It remains to consider the Role-Graph Administration Model [18]. It par-
titions roles into units called administrative domains. The model explicitly
includes a single highest administrative domain which includes the MaxRole
and MinRole from the underlying Role-Graph model. Uni-ARBAC adopts this
concept embodying it in the highest administrative unit at the root of the admin-
istrative unit tree hierarchy.
In addition to the administrative models discussed above, there are other
notable models developed in various applied contexts, especially in temporal/lo-
cation aware RBAC [1,2], Enterprise RBAC [7,8]), event driven RBAC [3],
administration of cryptographic RBAC [19] etc.
3
The Uni-ARBAC Model
In this section, we describe the Uni-ARBAC model, along with formal deﬁnitions.
The overall structure of Uni-ARBAC is illustrated in Fig. 1. We consider Uni-
ARBAC in two parts: the operational model for RBAC with respect to regular
Fig. 1. The Uni-ARBAC model for user-role and task-role administration

222
P. Biswas et al.
roles and permissions, and the administrative model for administering the user-
role and task-role relations of the former. These are respectively discussed in the
following two subsections.
3.1
Uni-ARBAC Operational Model
The sets and relations in the top part of Fig. 1 represent the Uni-ARBAC opera-
tional model, which is slightly diﬀerent from the standard RBAC model [6]. The
most salient diﬀerence is that there is a level of indirection in role-permission
assignment, so permissions are assigned to tasks and tasks are assigned as a
unit to roles. As discussed in Sect. 2, this additional indirection has emerged in
several diﬀerent administrative models in the literature. Additionally tasks are
organized in a partial order ⪰t, whereby a senior task inherits all permissions
from its juniors. For example, in Fig. 2(b), task t1 is senior to tasks t2 and t3,
so it inherits permissions from both of them, and so on. User-role assignment
remains unchanged from standard RBAC, so individual users are assigned to
and deassigned from roles. For simplicity, we have not considered the standard
RBAC concepts of sessions and role activation.
The Uni-ARBAC operational model is formalized in Table 2. Item I speciﬁes
the familiar components carried over from traditional RBAC. Item II speciﬁes
the additional components which eﬀect the additional indirection between per-
missions and roles via tasks. Item III formalizes the interaction between the
role hierarchy, task hierarchy, and permission-task and task-role assignments.
The interaction is schematically depicted in Fig. 3 and formally expressed in the
authorized perms function. The authorization function in item IV speciﬁes the
authorization required for a user to exercise a permission, which is that the per-
mission must be authorized to at least one role assigned to the user. A familiar
role hierarchy from the literature and an example task hierarchy are shown in
Figs. 2(a) and (b), respectively.
Table 2. Uni-ARBAC operational model

Uni-ARBAC: A Uniﬁed Administrative Model for Role-Based Access Control
223
Fig. 2. Examples of Uni-ARBAC hierarchies

224
P. Biswas et al.
Fig. 3. Interaction of role and task
hierarchies in operational model
Fig. 4. Scope of control of an AU
3.2
Uni-ARBAC Administrative Model
We now turn to the Uni-ARBAC administrative model illustrated in the lower
part of Fig. 1, and formalized in Table 3. The administrative model introduces
a number of additional components. First we have the notion of user-pools and
user-pool hierarchy adopted from ARBAC02 [13]. An example user-pool hierar-
chy is shown in Fig. 2(c). This example has three independent user-pools DP,
DevP and EP, with DevP being senior to a number of other user-pools, i.e.,
CTP, CPLP, MTP and MPLP. Motivation for the user-pool hierarchy in this
instance is by virtue of qualiﬁcations, so every user in the CPLP pool is also eli-
gible to be a developer in the DevP pool. The hierarchy obviates the need to do
multiple assignments of a user to both pools in such cases. Users are assigned to
user-pools via the UUPA user to user-pool assignment relation. The user-pool
notion is formally speciﬁed in item I of Table 3.
The central mechanism in Uni-ARBAC is the administrative unit. The set
of administrative units is denoted as AU, while individual administrative units
are indicated as au, aui, auj, etc. Uni-ARBAC requires that each au manages
an exclusive set of roles which is not under the purview of another au. The roles
function in item II of Table 3 is a partitioning assignment in that it must satisfy
the requirements that roles(aui) ∩roles(auj) = ∅for aui ̸= auj, and 
au∈AU
roles(au) = R. The eﬀect of partitioning is that each role is allocated to exactly
one au for administration. Each au only manages the roles it is directly assigned.
The eﬀect of the role hierarchy is limited to the operational model.
The partitioning concept is further applied in Uni-ARBAC to tasks and user-
pools via the tasks and user pools functions in item II of Table 3. These func-
tions must satisfy the requirements tasks(aui) ∩tasks(auj) = ∅for aui ̸= auj,

au∈AU tasks(au) = T, user pools(aui) ∩user pools(auj) = ∅for aui ̸= auj,
and 
au∈AU user pools(au) = UP.
In this manner an administrative unit manages a explicitly assigned partition
of roles, to which it can assign users from an assigned partition of user-pools and
tasks from an assigned partition of tasks. Unlike for roles, Uni-ARBAC extends
the authority of an au to junior tasks and user-pools, for which purpose we
deﬁne the tasks∗and user pools∗functions in item III of Table 3. The net eﬀect

Uni-ARBAC: A Uniﬁed Administrative Model for Role-Based Access Control
225
Table 3. Uni-ARBAC administrative model
is illustrated in Fig. 4, and further discussed in context of item V of Table 3. An
example partitioning of roles, tasks and user-pools across four administrative
units is shown in Fig. 2(d). We also note that for a given au it is permissible to
assign an empty partition of roles, tasks or user-pools. While, such situations
may be unusual, the model does not prohibit them.
Next we consider assignment of users to administrative units (item IV of
Table 3). Users can be assigned via the TA admin or the UA admin relation.
The former authorizes the task-role assigment power of an au, while the latter
authorizes the user-role assignment power. In this way, these two capabilities
can be separately assigned to users, even though they are coupled in the au.
This embodies the separation of user and permission assignment principle of
Sect. 2. A user assigned to any au via TA admin or UA admin is said to be an
administrative user.
For convenience in maintaining the TA admin and UA
admin relations,
Uni-ARBAC also deﬁnes a hierarchy ⪰au on administrative units. Assignment
of user u to a senior aui for task-role administration, i.e., (u, aui) ∈TA admin,
eﬀectively also assigns u for task-role administration to all auj for aui ⪰au auj.
Likewise for (u, aui) ∈UA admin. For simplicity, Uni-ARBAC requires ⪰au to

226
P. Biswas et al.
Fig. 5. Task-role authorization
Fig. 6. User-role authorization
be a rooted tree hierarchy. One eﬀect of this is that there is a seniormost au. An
example administrative units tree is shown in Fig. 2(d).
The authorization functions of Uni-ARBAC are speciﬁed in item V
of Table 3 as boolean functions that return true or false. The function
can manage task role(u : U, t : T, r : R) speciﬁes the conditions for user u to
assign/revoke task t to/from role r. The requirement is schematically depicted
in Fig. 5. User u must be assigned as a TA Admin to the unique administra-
tive unit auj which has jurisdiction over role r, or alternately so assigned to an
administrative unit aui ⪰au auj. In either case task t must be assigned to auj
or be junior to a task assigned to auj.
The can manage user role(u1 : U, u2 : U, r : R) similarly speciﬁes the con-
ditions for user u1 to assign/revoke user u2 to/from role r, and is schematically
depicted in Fig. 6. User u1 must be assigned as UA Admin to the unique admin-
istrative unit auj which has jurisdiction over role r, or alternately to an admin-
istrative unit aui ⪰au auj. In either case user u2 must be assigned via UUPA
to a user-pool which is directly assigned to auj or to some user-pool junior to a
user-pool directly assigned to auj.
Item VI of Table 3 formalizes the four administrative actions of Uni-ARBAC.
Assigning and revoking have the same authorization and the eﬀect is self-
explanatory. The alignment of authorization for assign and revoke embodies
the principle of reversibility of administrative actions discussed in Sect. 2.
3.3
Uni-ARBAC Invariants
Invariants are properties that hold for the lifetime. For the moment assume we
start with an initial state in which both TA and UA are empty, i.e., the roles have
no tasks or users assigned. Let us denote TA where all possible TA assignments
have been made as TAmax. It is evident that,
TAmax =

au∈AU
{(t, r)|t ∈tasks∗(au) ∧r ∈roles(au)}.
Because of reversibility of assign and revoke, and the independence of each
assignment from another, in any state TA must satisfy
∅⊆TA ⊆TAmax.
(1)

Uni-ARBAC: A Uniﬁed Administrative Model for Role-Based Access Control
227
It is further evident that any value of TA bounded as in Eq. 1 is realizable. We
can either build up from an empty TA or build down from TAmax. In fact we
can take the system from any value of TA compliant with Eq. 1 to any other
compliant value. Further, we can relax our assumption of an empty TA in the
initial state. Any initial state with TA compliant with Eq. 1 will ensure that TA is
maintained within these bounds. Finally, let TA0 denote TA in the initial state.
Any (t, r) ∈TA0/TAmax cannot be revoked and will persist in all subsequent
states. These observations can be proved formally but are quite evident. As an
example, the upper bound of TA for the administrative unit hierarchy of Fig. 2(d)
does not contain the pair of (t1, CPL). Thus the task t1 cannot be assigned to
the role CPL using the instance of the AUs in Fig. 2(d).
We can make similar observations with respect to the maximal possible values
of UA as follows,
UAmax =

au∈AU
{(u, r)|(∃up ∈UP )[(u, up) ∈UUP A ∧r ∈roles(au) ∧up ∈user pools∗(au)]}
so UA is bounded as follows.
∅⊆UA ⊆UAmax
(2)
4
Variations of Uni-ARBAC
In this section we discuss some variations of Uni-ARBAC which materially alter
the characteristics of the model. Uni-ARBAC has a rich structure so it is not
surprising that many variations are possible. Some are relatively incremental,
such as allowing the administrative hierarchy to be a general partial order rather
than a rooted tree. Here we discuss a few variations that raise some substantial
policy issues.
4.1
Aggressive Inheritance Model
In this variation we allow a senior au to do more than simply the sum of what
the au itself is authorized to do plus what each of the junior au’s are allowed. To
be concrete consider the AU hierarchy of Fig. 2(d), and an administrative user u
assigned as TA admin and UA admin to the Management Unit administrative
unit. This user also inherits membership in the Cloud Unit and Mobile Unit
administrative units. User u is thereby authorized, for example, to assign task
t2 to role CPL and users from user-pool CTP to role CPL. However, u cannot
make assignments across the junior administrative units, such as task t2 to
role MPL and users from user-pool MTP to role CPL. We denote this form of
inheritance in the AU hierarchy as membership inheritance.
With the alternate aggressive inheritance we allow cross assignments across
junior administrative units by a senior administrator. There is clearly a major
policy diﬀerence between the two forms of inheritance. The senior au eﬀectively
serves as a single consolidated au with freedom to assign any task to any role,
and users from any user-pool to any role. With aggressive inheritance the user

228
P. Biswas et al.
Table 4. Uni-ARBAC with aggressive inheritance
u discussed above will be able to assign t2 to role MPL and users from user-
pool MTP to role CPL. Perhaps more dangerously, user u will be able to assign
task t1 to MPL. The eﬀect of aggressive inheritance is formally stated in the
modiﬁed authorization functions of Table 4. Everything else from Table 3 applies
unchanged to Uni-ARBAC with aggressive inheritance. The senior most au at
the root of the AU hierarchy can assign any task to any role and any user
(assuming every user is in at least one user-pool) to any role, so TAmax = T ×R
and UAmax = U × R. Equations 1 and 2 continue to hold. Other less aggressive
variations can also be considered.
4.2
No Self-administration Model
Consider the administrative actions assign user to role(u1 : U, u2 : U, r : R) and
revoke user from role(u1 : U, u2 : U, r : R) of Table 3 item VI. In general, u1
can equal u2, so it is permissible for u1 to assign and revoke himself to and from
roles. In some contexts, this may be considered as a conﬂict of interest. To avoid
this, an additional check that u1 ̸= u2 can be added to the can manage user role
authorization function of Table 3.
5
Engineering Administrative Units
There can be diﬀerent meaningful AU hierarchies for a given set of roles. For
example, for the roles in Fig. 2(a), two diﬀerent instances of AUs are given in
Figs. 2(d) and 7, based on diﬀerent partitioning of the roles. Crampton and
Loizou partition roles in deﬁning ‘administrative scopes’ in [5] to conﬁne the
side eﬀects of role hierarchy modiﬁcation in a highly disciplined manner. The
AU structure in Fig. 2(d) is based on the partitioning of roles deﬁned in admin-
istrative scope.
As we have argued, one particular partition is not suitable for all require-
ments. UARBAC [10] argues that role partitioning according to administrative
scope does not work well for all diﬀerent types of role hierarchies and there
should be ﬂexibility in the administration structure.
We develop the following simple and ﬂexible partitioning heuristics, which
are applicable to many diﬀerent types of hierarchies and can be conﬁgured to
produce diﬀerent partitions for a given set of roles or tasks.

Uni-ARBAC: A Uniﬁed Administrative Model for Role-Based Access Control
229
Fig. 7. Alternate AUs for roles, tasks and user-pools as given in Fig. 2
1. Most senior roles in the role hierarchy contain critical tasks. They should be
administered separately. Similarly, most junior roles contain tasks that most
other roles inherit. They should also be administered separately.
2. For rest of the roles, iteratively select the most senior and most junior roles
until all roles are partitioned.
After partitioning and specifying role set for each AU, we can populate tasks
in the AUs using given task-role allocation. Scenario based top-down approach
for engineering roles [11] derives tasks in an intermediate process and assign
them to roles. Thus, we believe, the process of engineering roles can be utilized
to derive task-role allocation. On the other hand, the process of engineering
user-pools and allocating them in AUs, assigning administrative users into AUs
should also be carried out to develop working AUs. We do not further elaborate
these issues here.
6
Conclusion
In this paper, we present Uni-ARBAC, a uniﬁed model for administering user-
role and task-role relations in Role Based Access Control. It combines various
novel concepts and administrative principles from prior works. It integrates user-
role and permission-role administration into a manageable unit, we call Adminis-
trative Unit. While most of the previous models assume existence of administra-
tive roles for managing regular roles, we relax this assumption and our approach
for engineering administrative units can be used for engineering administrative
roles.
Nonetheless, Uni-ARBAC has limitations. It uses several sets and relations
of which it administers only task-role and user-role relations. Further research
in this area is needed to realize a complete model.
Acknowledgement. This research is partially supported by NSF Grants CNS-
1111925 and CNS-1423481.

230
P. Biswas et al.
References
1. Bertino, E., Bonatti, P.A., Ferrari, E.: TRBAC: a temporal role-based access con-
trol model. TISSEC 4(3), 191–233 (2001)
2. Bertino, E., Catania, B., Damiani, M.L., Perlasca, P.: GEO-RBAC: a spatially
aware RBAC. In: Proceedings of 10th SACMAT, pp. 29–37. ACM (2005)
3. Bonatti, P., Galdi, C., Torres, D.: ERBAC: event-driven RBAC. In: Proceedings
of 18th SACMAT, pp. 125–136. ACM (2013)
4. Crampton, J.: Understanding and developing role-based administrative models. In:
Proceedings of 12th ACM CCS, pp. 158–167 (2005)
5. Crampton, J., Loizou, G.: Administrative scope: a foundation for role-based admin-
istrative models. ACM TISSEC 6(2), 201–231 (2003)
6. Ferraiolo, D.F., Sandhu, R., Gavrila, S., Kuhn, D.R., Chandramouli, R.: Proposed
NIST standard for role-based access control. ACM TISSEC 4(3), 224–274 (2001)
7. Kern, A.: Advanced features for enterprise-wide role-based access control. In: Pro-
ceedings of 18th ACSAC, pp. 333–342. IEEE (2002)
8. Kern, A., Schaad, A., Moﬀett, J.: An administration concept for the enterprise
role-based access control model. In: Proceedings of 8th ACM SACMAT, pp. 3–11
(2003)
9. Kuijper, W., Ermolaev, V.: Sorting out role based access control. In: Proceedings
of 19th ACM SACMAT, pp. 63–74 (2014)
10. Li, N., Mao, Z.: Administration in role-based access control. In: Proceedings of 2nd
ACM ASIACCS, pp. 127–138 (2007)
11. Neumann, G., Strembeck, M.: A scenario-driven role engineering process for func-
tional RBAC roles. In: Proceedings of 7th ACM SACMAT, pp. 33–42 (2002)
12. Oh, S., Park, S.: Task-role-based access control model. Inf. Syst. 28(6), 533–562
(2003)
13. Oh, S., Sandhu, R.: A model for role administration using organization structure.
In: Proceedings of 7th ACM SACMAT, pp. 155–162 (2002)
14. Sandhu, R.: The ASCAA principles for next-generation role-based access control.
In: Proceedings of 3rd ARES (2008)
15. Sandhu, R., Bhamidipati, V., Munawer, Q.: The ARBAC97 model for role-based
administration of roles. ACM TISSEC 2(1), 105–135 (1999)
16. Sandhu, R., Munawer, Q.: The ARBAC99 model for administration of roles. In:
Proceedings of 15th Annual ACSAC, pp. 229–238. IEEE (1999)
17. Sandhu, R.S., Coyne, E.J., Feinstein, H.L., Youman, C.E.: Role-based access con-
trol models. Computer 29(2), 38–47 (1996)
18. Wang, H., Osborn, S.L.: An administrative model for role graphs. In: De Capitani
di Vimercati, S., Ray, I., Ray, I. (eds.) Data and Applications Security XVII. IFIP,
vol. 142, pp. 302–315. Springer, New York (2004)
19. Zhou, L., Varadharajan, V., Hitchens, M.: Secure administration of cryptographic
role-based access control for large-scale cloud storage systems. JCSS 80(8), 1518–
1533 (2014)

SKALD: A Scalable Architecture
for Feature Extraction, Multi-user Analysis,
and Real-Time Information Sharing
George D. Webster(B), Zachary D. Hanif, Andre L.P. Ludwig,
Tamas K. Lengyel, Apostolis Zarras, and Claudia Eckert
Technical University of Munich, Garching, Germany
webstergd@sec.in.tum.de
Abstract. The inability of existing architectures to allow corporations
to quickly process information at scale and share knowledge with peers
makes it diﬃcult for malware analysis researchers to present a clear pic-
ture of criminal activity. Hence, analysis is limited in eﬀectively and
accurately identify the full scale of adversaries’ activities and develop
eﬀective mitigation strategies. In this paper, we present Skald: a novel
architecture which guides the creation of analysis systems to support the
research of malicious activities plaguing computer systems. Our design
provides the scalability, ﬂexibility, and robustness needed to process cur-
rent and future volumes of data. We show that our prototype is able
to process millions of samples in only few milliseconds per sample with
zero critical errors. Additionally, Skald enables the development of new
methodologies for information sharing, enabling analysis across collective
knowledge. Consequently, defenders can perform accurate investigations
and real-time discovery, while reducing mitigation time and infrastruc-
ture cost.
1
Introduction
Cyber crime has evolved to the point where teams of criminals command sophis-
ticated tools and infrastructures, while possessing the resources required to stay
ahead of the defender [4,18]. For instance, in 2012, McAfee received over 100
thousand samples per day [3], yet on one day in 2015, VirusTotal received over
a million unique samples [25]. Disproportionately, malware analysis systems are
struggling to scale to meet this challenge and present a clear picture of criminal
activity [22,23,26]. A solution to this problem is twofold: (i) how can corpora-
tions extract features and retain a central repository at scale and (ii) how can
industry peers collaborate in a timely manner without exposing sensitive data
and retain essential context.
Simply put, the rise in malware, has strained the ability of the security teams
to run analytics and maintain a central repository of generated information. The
reason why is because current analytic tools fall short when being used together
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 231–249, 2016.
DOI: 10.1007/978-3-319-45871-7 15

232
G.D. Webster et al.
in an automated fashion and enabling a collaborative environment [22,23,26].
As stated by MITRE, this causes a situation where analysts often regenerate
information and duplicate the work of their peers—a huge waste of time and
resources [23]. With respect to collaboration with industry peers, the second
problem is that once information is received, it is diﬃcult to be shared with
security partners in a manner that is timely, retains context, and protects the
collection methods. Although, rapid information sharing is an essential element
of eﬀective cybersecurity, and will lessen the volume companies need to process,
companies are weary of sharing data for fear of tarnishing their business rep-
utation, loosing market share, impairing proﬁts, privacy violations, and reveal-
ing internal sources and methods [6]. As a result, it is now common practice
to share only with trusted groups large sets of data with minimal context or
select post-processed information. Recognizing the criticality of this issue, the
US Government recently issued an Executive Order to promoting the sharing of
cybersecurity information in private sector [2].
In an attempt to alleviate the burden on analysts, a number of solutions
has been developed to help triage data through feature extraction and create a
central repository of the collected information [9,23,28]. Unfortunately, many of
these tools struggle to scale and provide the fault-tolerance required to support
the sheer volume of data needed to be processed in part due to the linear,
monolithic, and tightly coupled processing pipeline. For instance, analysis tools,
like CRITs [23] and MANTIS [9], are not separated from the core Django/Apache
system and are executed on the same physical host, while VIPER [28] has been
developed for a single user with the intention of being deployed on a workstation.
Consequently, when these system becomes overloaded, a bottleneck occurs that
prevents the feature extraction tools from executing properly. To make things
even worse, when one of the aforementioned tools fails, it is diﬃcult to perform
a graceful exit or cleanup, and the system becomes overwhelmed with a load of
only a few thousand malware samples. This results in a situation in which feature
extraction cannot be performed quickly and at scale using current technologies
and architectures. Furthermore, none of these tools address the issue of how to
make assessments on the extracted features.
In this paper we present Skald, a novel architecture to create systems that
can perform feature extraction at scale and provide a robust platform for ana-
lytic collaboration and data-sharing. In essence, Skald provides the required
infrastructure to extract features at a scale that can: (i) cope with the growing
volume of information, (ii) be resilient to system failures, and (iii) be ﬂexible
enough to incorporate the latest technology trends. In addition, Skald takes a
new approach in terms of how data is shared by providing a platform that grants
analysts’ tools access to the entire extracted feature set without requiring the
analysts, and their tools, to have direct access to the raw malware samples or
other primary analytic object, such as a domain or an IP address. This enables
correlations, clustering, and data discovery over the entire set of collected knowl-
edge, while still protecting the raw objects, the sources, and the methods used to
obtain the data. To this end, we develop an open-source prototype and conduct

SKALD: A Scalable Architecture for Feature Extraction
233
extensive experiments that demonstrate that our architecture has a near linear
growth rate and is able to eliminate critical failures when extracting features
across millions of entries. Furthermore, we show major performance gains with
the ability to conduct feature extraction at a rate of 3.1 ms per sample, compared
to 2.6 s when using existing systems. Finally, we discuss how our methodology
provides a platform for analysis on a collective set of raw data and extracted
features, which enables more accurate clustering of malicious information and
real-time data discovery while minimizing the need for redundant feature extrac-
tion and thereby reducing analysis time and infrastructure cost.
In summary, we make the following main contributions:
– We develop a framework for feature extraction that is scalable, ﬂexible, and
resilient, while it demonstrates near linear growth with zero critical errors over
millions of samples.
– We display major speed improvements over traditional techniques using only
3.1 ms per sample when utilizing 100 workers.
– We exhibit the ability of our approach to allow partner organizations to sub-
mit new raw analysis objects in real-time leveraging the infrastructure from
multiple organizations on diﬀerent continents.
– We show Skald’s capacity to share resultant extracted features and analysis
with geographically and organizationally diverse partners who are not suﬃ-
ciently trusted to have unrestricted access to the raw analysis objects.
2
System Overview
Skald is an architecture for developing analytic platforms for teams working to
thwart cyber crime. At its core, Skald dictates the required structure to per-
form asynchronous feature extraction against submitted objects. It scales these
actions horizontally, at a near linear rate, across millions of objects while remain-
ing resilient to failures and providing the necessary ﬂexibility to change ana-
lytic methods and core components. Skald additionally provides the necessary
infrastructure to perform advanced analytics over the extracted features while
empowering analysts to retrieve and share information using their preferred tools
and scripting interface. Furthermore, Skald’s design allows the sharing of data
with partners without requiring the release of raw data. This is because Skald’s
Access Control Layer (ACL) and intelligent core components allow analytics to
be executed across a combination of central instance and in-house replicas. Ergo,
the information can be easily shared, enabling analysis over a more complete and
collective set of data, which overcomes biases caused by informational gaps.
Skald’s achievements are primarily due to the approach of logically abstract-
ing the system into “loose coupled” themes and core components, as depicted in
Fig. 1; allowing the creation of systems that are scalable, ﬂexible, and resilient.
This level of abstraction between system elements is critically missing in widely-
used systems such as CRITs, VIPER, and MANTIS. This has lead these systems
to become monolithic and tightly coupled in design; creating a major hindrance

234
G.D. Webster et al.
in allowing them to evolve and scale by leveraging distributed computing tech-
niques. However, Skald apart from the scalability it oﬀers, it enables systems to
evolve so they can meet the challenges of future cyber criminals by allowing com-
ponents to be easily exchanged, added, or subtracted. Thus, if a newer, better, or
simply diﬀerent method is discovered, this can be easily incorporated alongside
existing methods or simply replace the old ones. This also allows system com-
ponents to be outsourced to a company, institution, or organization specializing
in that work. Finally, this design allows Skald to orchestrate tasking which
create eﬃcient system by substantially reducing the infrastructure and network
overhead for transmitting data to and from multiple Services.
Fig. 1. Organization of Skald’s components and core themes.
The structure of Skald is composed of three main components: Transport,
Planners, and Services. As Fig. 1 illustrates, Transport is the main orchestrator
and moves data and tasking to the Planners. Then, Planners allocate infrastruc-
ture, enforce security, and oversees the execution of Services. Services in turn
perform the requested work and provide the resultant response along with perti-
nent meta information, such as error messages, back to Planners. This is further
described in the following sections.
2.1
Planner
The Planner’s primary purpose is to serve as an intelligent orchestrator for Ser-
vices. At its core, it manages tasking, allocates resources, enforces the ACL,
and provides an abstraction between the Services and other aspects of Skald.
The Planner also informs the Transport what Services are available for tasking
and provides status information back to the system core. As previously men-
tioned, Planners are loosely coupled with other parts of Skald. In this way,
they provide ﬂexibility by ensuring that changes to the core aspects of a Planner
will not aﬀect other parts of the system. This helps to improve resiliency by
allowing the Transport to delegate tasking to redundant Planners during system

SKALD: A Scalable Architecture for Feature Extraction
235
failures [5,19]. Furthermore, this allows the system to horizontally scale by per-
mitting the Transport element to instantiate additional Planners under heavy
load while also allowing the Planners to schedule the tasking of Services and
allocated additional resources on internal servers and cloud infrastructure.
Communication with Services. The Planner communicates with Services
through the HTTP over TLS protocol. We select this method for three main
reasons. First, HTTP is widely understood and is capable of transferring a variety
of data-types. Second, the wide adoption of the HTTP protocol allows analysts to
be able to add new analytics in the language they feel most comfortable. Third,
HTTP communication allows Services to be deployed either locally or across
network partitions. However, Skald does not dictate the format of messages
transmitted over HTTP over TLS. That said, our prototype provides developers
with interfaces for typed JSON message parsing for stronger message safety and
a loosely-typed Map data-structure when message safety is not a concern. We
choose this because when evaluating other formats in the past, such as Google’s
Protocol Buﬀers [8], we found the overall speed increase did not justify the
greater level of diﬃculty.
Skald provides two methods for the Planner to communicate objects to Ser-
vices. For local analytics, the object is delivered via a RAM disk, with fail-over
to local disks based on size. This creates a fast and easily-available data-store
for analytic ﬁle reads. For external Services, the Planner will deliver the object
via HTTP. When interacting with Services that do not require a local ﬁle, the
Planner will attach the pertinent meta-data to the tasking message. We select
this method for transmitting objects as many existing analytics require a local
ﬁle to be read. As such, this allows existing tools to maintain relevance through
leveraging Skald’s ability to distribute and scale workloads. Furthermore, this
improves performance since each Service does not require a costly network trans-
mission to access an object.
Service Orchestration and Management. The Planner leverages conﬁgura-
tion management and containers to package Services together in order to manage
the tasking of Services and optimize their execution. This provides three core
beneﬁts with respect to speed, ﬂexibility, and resiliency [7]. Regarding speed,
the packaging of multiple isolated Services on one worker reduces the volume
and frequency of data passing through the network. This in turn is a major
advantage with distributed and cloud based systems because it eliminates the
need for multiple large ﬁle transfers. Furthermore, packaging Services increases
the ﬂexibility of Skald-based systems by allowing the rapid deployment of new
Services without concern for complex dependency management while ensuring
discrete versions of Services and conﬁgurations. Finally, containers easily allow
Quality of Service (QoS) operations to automatically re-instantiate critically
failed Services.
Access Control Layer Enforcement. The Planner is the primary element
responsible for managing the ACL by ensuring that tasking is authorized. The
Planner also limits potential exposure caused by analyzing an object by enforcing

236
G.D. Webster et al.
Service execution restrictions through the use of ACL meta-tags. This is done
by allowing tasking to state that the execution should only run, for example,
on internal hardware, without Internet access, and restrict DNS lookups. This
is critically missing in current system designs.
2.2
Planner Themes
Skald breaks down Planners into ﬁve discrete themes as Fig. 1 illustrates. This
prevents systems from becoming monolithic through the separation of core parts
of the system into categories based on their area of inﬂuence. Planners are cat-
egorized as members of one of the following: Gateway, Investigation, Storage,
Interrogation, and Presentation.
Gateway. The Gateway’s primary purpose is to receive taskings and push it
to the Transport. When tasking is received, the Gateway ﬁrst performs an ACL
check to guarantee that the tasking is authorized. If authorized, Gateway then
ensures taskings are valid, scheme compliant, and that the pipeline can handle
the requested work. The Gateway can also automatically assign tasking based
on an object type. Together this ensures that pipeline resources are not wasted
and provides the ﬁrst level of system security.
Investigation. This theme is responsible for performing feature extraction
against objects. When tasking is received, it schedules the execution of its Ser-
vices which are capable of performing static and dynamic analysis as well as
gather data from third parties. During scheduling it optimizes the execution of
Services by packaging them together and directly provides them the data. As
taskings are executed, it performs the two-fold QoS strategy by monitoring the
health of Services and validating received results. Additionally, it enforces the
ACL and ensures Services adhere to the meta-tags conﬁgured restrictions.
To help illustrate the goals of the Investigation Planner, we describe an ideal
execution ﬂow. The Transport layer T2 (see Sect. 2.4) submits an object for
Investigation along with a set of taskings and ACL tags. The Planner ﬁrst identi-
ﬁes which Services are available for tasking and conﬁgures them according to the
tasking request and ACL meta-tags. Next, it either packages an object together
with a set of Services for execution on one node, or sends the object to a preex-
isting node dedicated to a Service. The Planner will then monitor the health of
the Service and perform any remediation action as needed. For instance, if a Ser-
vice is unable to gather data from a source due to a query cap, the Planner will
reschedule the Service’s execution once the cap has expired. When the results
of a Service are received, the Planner passes them to the Transport layer which
then commands the Storage Planner to archive them. Additionally, if a Service
returns new objects, the Planner will submit the object back to the Transport
layer with the pertinent ACL tags for storage and tasking to the Gateway.
Storage. This Planner controls how data is stored and retrieved in Skald.
At its core, it is an abstraction layer for the database Services. It enforces a
standard storage scheme and passes the requests to the appropriate database

SKALD: A Scalable Architecture for Feature Extraction
237
elements, identiﬁed by UUID4. This enables Skald to be storage system agnos-
tic and utilize a single or hybrid data storage scheme for resultant data and
objects of analysis. The beneﬁt of this approach is that data can be stored in
databases optimized for the data type while also easing the inclusion of legacy
archives. For example, objects can use Amazon’s Simple Storage Service (S3)
while the features can be stored in a system optimized for textural data such
as Cassandra [16]. This approach additionally has a major beneﬁt of allowing
industry partners to perform in-house replication of selected sets of data while
enforcing restrictions on more restricted datasets. For instance, in our proto-
type, raw objects are stored in restricted datasets hosted by the originator while
the extracted features are replicated across all partners. When restricted data
is required, the Planner provides contact information to the requester and once
approved the Planner will automatically conﬁgure access.
The design also enabled the Storage Planner to perform storage-based opti-
mizations, for example, performing deduplication, compression, and deconﬂict
updates and deletions of temporally sensitive datasets. Finally, the Planners
overarching view of the system allows it to automatically perform QoS-based
operations such as automatically instantiating additional storage shards and
coordinating multi-datacenter replication.
Interrogation. The Interrogation Planner focuses on how to turn the extracted
features into intelligence in two distinct forms. Aside these forms, the Planner
is responsible for scaling the number of Services to balance system load. In the
ﬁrst form, Interrogation Services process system data for retrieval through mech-
anisms such as an API, plugin, or website. This deviates from previous systems
by separating the feature extraction process from the rendering of data. Thus,
displaying information is not bound to the extraction method and can change
based on what a user’s desire. For example, a Service can display the VirusTo-
tal score along side any Yara rule matches. In the second form, an Interrogation
Service orchestrates the execution of advanced analytics, such as Machine Learn-
ing. To do this, the Planner distributes the work load across available Services to
complete the task at hand. This load balancing allows the integration with mini-
batch training techniques to achieve large scale model training and generation
and serve as a distribution layer for pre-trained models.
Presentation. This Planner provides a standard mechanism for interacting with
stored data and the data the Interrogation layer generates. When requests are
received, it ﬁrst ensures that the request accessing information is authenticated
and scheme compliant. As this Planner theme is the most likely to vary based
on individual use cases, we opt to keep its deﬁnition as minimal as possible.
That being said, the Presentation layer can be imagined as a microservice fog
surrounding the datastores queried by the Interrogation layer.
2.3
Service
Services perform the work being orchestrated by Planners. In Skald, Services
are “loosely coupled” and only interact with their parent Planner. As discussed

238
G.D. Webster et al.
by Papazoglou et al. [19], this highly decentralized model allows services to be
platform independent and scale as needed. Additionally, this improves fault-
tolerance as no Service is reliant on another Services. Furthermore, the atomic
nature of Services provides a great level of ﬂexibility by allowing them to be
exchanged as new technology emerges and requirements change. In essence, a
Service is attached to a Planner and performs work associated with that Plan-
ner’s theme. For example, under the Investigation Planner, Services can gather
data from VirusTotal, generate a PEHash [27], and perform dynamic analysis
through Cuckoo [10] and DRAKVUF [17]. An Interrogation Service, on the other
hand, will perform an action across a set or subset of the data through data gath-
ering, machine learning, or other data mining and exploration techniques and
provide a mechanism to display the resulting information [14].
2.4
Transport
Transport’s main task is to move data among the Planners. In addition, the
Transport layer monitors the health of the Planners and performs remediation
actions to support QoS. This enables a robust level of resilience by ensuring
that requested work is always stored in a queue and that results and taskings
are never lost. Additionally, the Transport layer improves Skald’s ability to
scale by reducing adverse eﬀects of system overloads by allowing the distribution
of work across multiple Planners. This is a huge beneﬁt over current available
cybersecurity systems that are only design to scale vertically.
The Transport consists of four main sections (T1, T2, T3, and T4) as Fig. 2
illustrates. This permits the selection of optimized technology to handle the
interaction among Planners. In the following we introduce these sections.
Fig. 2. Interaction between the Transports and Planners.
Transport - T1. T1 is focused on moving data to the Investigation Planner
for analysis. To do this, T1 is required to perform three primary actions. The
ﬁrst action is to receive tasking from the Gateway Planner and schedule their

SKALD: A Scalable Architecture for Feature Extraction
239
transmission to the Investigation Planner. The second is to receive tasking from
T2 and submit them to the Gateway Planner for validation. The ﬁnal action is to
monitor the health of the Interrogation Planner and perform QoS management.
When implementing T1, we recommend the utilization of a distributed message
broker such as Kafka or RabbitMQ.
Transport - T2. T2 is focused on receiving objects and results from the Inves-
tigation and Interrogation Planners. To do this, T2 has two primary actions. The
ﬁrst action is to receive data from the Investigation and Interrogation Planners
and schedule their submission to the T3 for Storage. This is separated from T3
to allow a message queue service to be implemented to help throttle the storage
of data during peak loads as storage operations can be costly but are often not
time critical. The secondary action is to receive objects from the Investigation
and Interrogation Planners and pass them to T1 for further analysis. Like T1,
we recommend the use of a distributed message broker.
Transport - T3. T3 is focused on submitting and retrieving data from the
Storage Planner. As such, T3 is responsible for three primary actions: (i) provide
data from the Storage Planner directly to the Investigation and Interrogation
Planner, (ii) receive information from T2 and pass the data on to the Storage
Planner, and (iii) manage the QoS of the Storage Planner. We recommend the
ﬁrst two actions to be implemented with no message queues between the Storage
Planner and the databases. This permits database Services to rely on their own
optimization frameworks during the retrieval of data; databases are often heavily
optimized for retrieval of data and implement their own form of message queues.
Transport - T4. T4 handles the exchange of information between the Inter-
rogation and Presentation Planners. The ﬁrst action is to provide a conduit
for communication between the Presentation and Interrogation Planners. The
second is to monitor the health of the Investigation Planner and perform QoS
management. As the Interrogation Planner will typically provide data through
HTTP calls, we recommend implementing T4 as HTTP load balancers.
3
System Wide Aspects
In this section we present system wide aspects of Skald. We ﬁrst introduce the
QoS strategy we follow and then discuss the ACL requirements.
3.1
Quality of Service
Malware authors are incentivized to thwart analysis and as a result the failure
of Services should be expected. To counter this, Skald automatically recovers
from issues arising from the execution of Services by leveraging a robust QoS
pattern. It does this through a two-fold QoS philosophy of monitoring the actual
Service and the resultant response from a Service. Thus, Skald accounts for the
scenario of when the returned work has failed or even if the actual Service has
failed and accounts for them diﬀerently.

240
G.D. Webster et al.
To implement the QoS strategy, the Planner monitors the health of the Ser-
vice worker using container status messages and HTTP response codes. The
Planner then evaluates the availability, response time, and throughput of each
attached Service. If the evaluation responds by stating the Service is operating
within normal bounds, the Planner will then send the results to the second stage
to evaluate the returned work. This allows Service authors to specify deep level
checks and perform automated remediation actions that are Service speciﬁc. If a
failure occurs at either step, the Planner will determine if the Service has entered
a failed state and perform remediation action, such as restarting the Service con-
tainer. If the Service appears to be healthy, the Planner will re-queue the work
that has demonstrated failure while saving otherwise successful Service results.
The unique aspect of this strategy is that Skald views the tasking of each
Service as single elements and process them individually. When the Service or
returned work fails, Skald will only discard failed work as opposed to abandon-
ing combined tasking and queued work. This is a key diﬀerence between Skald
and previously systems. For example, the methodology used by systems which
rely on the HDFS and MapReduce model for their data and task distribution,
such as BinaryPig [11] and BitShred [13], will cease processing or discard suc-
cessful results when percentages of work fail. However in Skald, even if there is
a high number of failed jobs, any successful result will be saved and failed work
will be reattempted. Furthermore, in the event of pathological failure, Skald
will store the tasking in a separate queue for human intervention. Our evalua-
tion showed this new approach provides signiﬁcant performance beneﬁts and was
outright required when executing large, historical, analytic tasks across cyber-
security data as the data is often designed to confound investigations and cause
failures.
Skald’s QoS system also accounts for congestion during times of peak load
as well as Service shutdown and instantiation. To do this, the Planner enforces
throttling of Services using a pull-based pattern with an “at least once” message
delivery scheme [12]. In this scheme, the Planner is aware of the system’s current
message load and pulls a conﬁgurable number of messages, in which it is capable
of completing, from the Transport layer. Each requested message is then tracked
within the Planner as a discrete entity. Upon completion of work, the Planner
notiﬁes the Transport of the work status, pushes the results to the queue, and
pulls additional tasking. This allows Skald to prevent the overburdening of
Planners while also ensuring that no work is lost due to component failure. This
approach also reduces the chance that a failed Service state will replicate to
other parts of the system and cause a work stoppage due to being overburdened.
3.2
Access Control Layer
The nature of handling unique and sensitive data within Skald requires the
incorporation of a complex ACL system. Unfortunately, the standard ACL model
used in cybersecurity only allows for isolation based on the source of the raw
data or a user’s role. However, the problem is that this does not address access
to sensitive capabilities, diﬀerentiate Services from users, or separate raw data

SKALD: A Scalable Architecture for Feature Extraction
241
from features and analytics; creating an “all or nothing” approach to access.
Therefore, the systems cannot be designed to easily allow analysts or Services
to derive intelligence across a collective set of information without granting the
analysts access to all sources and sensitive capabilities.
To overcome this, Skald creates a new ACL model by granting access based
on User, Capability, Source, and Meta-tags. The User deﬁnes the users and com-
ponents of the system. While Capabilities map to Services and their derived
information, Source maps the origin of the raw data. Finally, Meta-tags pro-
vide Planners with Service execution restrictions. For example, these tags can
specify that dynamic analysis can only execute without Internet access. While
implemented in our prototype, the full deﬁnition of the ACL is left to future
work.
4
Evaluation
To evaluate Skald, we created an open-source prototype and performed a series
of experiments. We acknowledge that the Skald focuses on the structure of a
system and does not prescribe implementation methods. While this can create
varying performance metrics, we feel it is prudent to present a baseline imple-
mentation to demonstrate the signiﬁcant improvements aﬀorded by the Skald
architecture. Throughout this section, we use our prototype to evaluate the archi-
tecture’s (i) scalability, (ii) resiliency, and (iii) ﬂexibility.
4.1
Experimental Environment
We leveraged three hardware proﬁles for our evaluation. The ﬁrst proﬁle was
used as a control and deployed CRITs using their recommended setup with
the following nodes: (i) Ingest VM : 2 cores 4 GB RAM, (ii) MongoDB VM :
10 cores 32 GB RAM, and (iii) CRITs VM : 6 cores 32 GB RAM. CRITs was
selected for our control as it is an industry standard for performing multi-user
analytics. Additionally, we made the assumption that CRITs performs similarly
to other systems such as MANTIS [9], as the architectures are remarkably similar
and Django-based. The second proﬁle deploys our prototype using a similar
hardware proﬁle as the control. In this proﬁle we deployed the prototype in a
cloud-based environment using the following nodes: (i) Workers: three AWS
EC2 M3 large instances, (ii) Transport: One AWS M3 xlarge instance, and
(iii) Storage: One AWS M3 medium instance. Finally, the third proﬁle was used
to evaluate the horizontal scalability of the Skald architecture. In this proﬁle
we used the following nodes: (i) Workers: 100 AWS EC2 M3 large instances,
(ii) Transport: One AWS M3 xlarge instance, and (iii) Storage: One AWS M3
medium instance.
In all experiments we used a diverse set of malicious PE32 samples from Virus
Share, Maltrieve, Shadowserver, and private donations. These binaries encom-
passes traditional criminal malware, highly advanced state-sponsored malware,
and programs which are not conﬁrmed as malicious but are suspicious.

242
G.D. Webster et al.
4.2
Scalability
In order to provide a meaningful evaluation of Skald’s scalability, we studied
the ability to ingest PE32 samples and then execute a series of three Investiga-
tion Services on our proﬁles. We selected PE32s as this provides a direct mapping
to CRITs and our prototype performs at a near identical level when processing
Domain, PCAP, IP, and other objects. To this end, we did not perform any
experiments related to the storage of Service results because these experiments
would vary depending on the data-store used. Furthermore, we omitted exper-
iments on the Interrogation Services as this architectural model is relatively
similar to Investigation and it is diﬃcult to perform clear correlations among
existing systems.
During our experiments, we used four sets of data: 1000, 5000, 10,000 and
50,000 randomly selected samples. As Skald is intended to scale to support
large datasets, we additionally ran Skald with a set of one million samples. In
both cases we pushed the samples into each system through a linear sequence of
RESTful calls with no delays. We did this to evaluate the ability for the systems
to queue work and simulate batch queues that we regularly encounter in our
work. Additionally, to ensure that the evaluation was as fair as possible, we
levered existing CRITs Services and added a RESTful wrapper around Services
to make them compatible with our prototype. These Services gather PEInfo
data, check the ﬁle against the VirusTotal private API, and run each sample
against 12,431 Yara [1] signatures provided by Yara Exchange [20].
Table 1. Average time to process samples in seconds.
Framework
1K
5K
10K
50K
CRITs
2.8000 3.1774 3.3781 1.1929
3 Workers
0.0502 0.0558 0.0616 0.1303
100 Workers 0.0032 0.0032 0.0032 0.0025
Our ﬁrst scalability experiment revealed that Skald outperforms existing
systems, as shown in Table 1. Our prototype, which was deployed with 100
workers, was able to process each sample at an approximate rate of 3.1 ms.
Additionally, this hardware setup began to show speed improvements, due to
the scheduler caching, at about 50,000 samples.
An interesting ﬁnding is that even with similar hardware, the design still
greatly outperformed existing systems and was able to process each sample at an
average rate of 74.5 ms. In terms of speed, this is a signiﬁcant improvement over
CRITs’ average rate of 2.64 s per sample. Furthermore, these results highlight
one of the critical issues plaguing existing systems. The perceived rate increase
with CRITs at 50,000 samples was caused by an overload of the system. When
this occurred, the operating system began to randomly kill processes before
completion, producing a false appearance of speed improvements.

SKALD: A Scalable Architecture for Feature Extraction
243
0
10,000
20,000
30,000
40,000
50,000
200,000
400,000
600,000
800,000
1,000,000
Time (sec)
Number of PE32 Samples
10 Workers
100 Workers
Fig. 3. Skald’s speed in processing one million samples.
Having established Skald’s performance against existing systems, we then
studied the prototypes ability to scale to meet the demand of very large sam-
ple sets, as Fig. 3 depicts. Unfortunately, a direct comparison with CRITs was
not possible because CRITs could not maintain stability beyond 50,000 sam-
ples. Nevertheless, this experiment clearly showed that our prototype was able
to scale to meet the demand of processing a million samples and performed at
an approximate rate of 7.5 ms per sample using 100 workers. When analyzing
the results, we identiﬁed that the slowdown, with respect to the previous experi-
ment, was caused by the Transport requiring disk reads due to tasking size. This
however, was a constant rate and we are conﬁdent that Skald performs at a
similar rate irrespective of the volume of samples.
4.3
Resilience
Next, we executed two experiments to study the resiliency of Skald. For the ﬁrst
experiment, we wrote 26 KB worth of random bytes across 20 % of the samples.
This was done to generate ﬁles that represent work that will potentially confuse
and fail analysis tasks. This allowed us to evaluate Skald’s ability to cope
with failed, long-running, and troublesome work. For the second experiment,
we studied how Skald-based systems perform while core components of the
infrastructure are unavailable or entered a failed state. We did this by running a
script that randomly killed and restarted the machines, in isolation and in unison,
hosting the Planner, Transport, and Service components of the system. During
both experiments, we reran newly generated ﬁle sets through each hardware
proﬁle using the same method as used in the scalability experiments.
It was immediately apparent that Skald’s design and QoS paradigm greatly
outperformed current systems. In the ﬁrst experiment, as Table 2 shows, our
prototype encountered zero critical errors, deﬁned as a Service failing to complete
tasking. While the Services did fail, the Planners successfully recovered from
all errors encountered by Services and continued processing other unaﬀected

244
G.D. Webster et al.
Services. This is in direct contrast with CRITs reporting 17,012 critical errors
using a set of 50,000 samples. In an investigation of the results, the high critical
error rate was because the system was unable to handle the load caused by
failed Services and, as a result, these Services entered a locked state, consuming
valuable resources, or were killed by the operating system before results could
be submitted for storage.
Table 2. Critical failures in sample processing.
Framework
1K 5K 10K 50K
CRITs
0
0
151
17,012
3 Workers
0
0
0
0
100 Workers 0
0
0
0
During the second experiment, Skald remained tolerant of faults. While
the overall processing speed was decreased, our prototype’s QoS paradigm was
able to identify failed states and re-queue the tasking without any operator
interaction. The ﬁnal outcomes revealed that no work was lost and 100 % of the
tasking were completed with no critical errors.
These results showed the beneﬁts of Skald. During these experiments, our
prototype was not only resilient to handling poorly performing Services, but
also easily handled failures to critical components of the system. Furthermore,
these results revealed the beneﬁts of Skald’s QoS structure in that work was
never lost and the system never entered a failed state. By contrast, CRITs’ failed
Services remained in a failed state and human intervention would be required to
clean faulty results from the database, reset service, and re-task the system.
4.4
Flexibility
In order to examine Skald’s ﬂexibility, we ﬁrst evaluated its ability to incor-
porate existing feature extraction methods. To do so, we created an Interro-
gation Service by modifying the existing CRITs PEInfo Service. This required
the modiﬁcation of less than 50 lines of code in order to remove CRITs speciﬁc
commands and provide a RESTful HTTP wrapper. During evaluation, we noted
that the wrapped Service performed with no discernible diﬀerence when com-
pared to natively written Skald Services. The prototype was able to seamlessly
incorporate the Service to include performing QoS operations. In summary, this
demonstrated Skald’s ability to provide ﬂexibility by showing a minimal level
or required work to incorporate a none native Service.
Next, we wanted to study Skald’s ﬂexibility in changing a core component.
We did this by testing the ability of our prototype to directly work with the
CRITs database. This experiment was selected because the original implemen-
tation of Skald’s Storage Planner used Cassandra and Amazon S3 as the data-
base back-end. In contrast, the CRITs framework uses MongoDB Documents

SKALD: A Scalable Architecture for Feature Extraction
245
and GridFS. Thus, we were not only required to change the underlying scheme
for storing data but also the core database technologies. To do this, we made
three modiﬁcations to the original prototype: (i) we created a Storage Service
that parsed the results into the CRITs database scheme, (ii) we introduced an
additional Storage Service that queried the existing CRITs database system to
retrieve raw objects, and (iii) we included logic in the Storage Planner to iden-
tify which Storage Services to select when handling tasking. In total, we were
able to make these changes using less than 100 lines of code. Furthermore, no
modiﬁcations to the other parts of Skald were required.
In summary, we are conﬁdent that Skald provides the ﬂexibility required to
relatively easily change Services as well as core components of a system. We are
conﬁdent that these results are applicable to other parts of the system.
5
Use Cases
In this section we present use cases to demonstrate how Skald can overcome
many of the current limitations in sharing data among industry partners. These
use cases are based on our experience in using our framework to manage a
collective set of millions of objects, including their associated analysis, across
three globally distributed organizations.
5.1
Sharing Resources with Geographically Distributed Partners
Security partners rarely share infrastructure for fear that this will lead to infor-
mational exposure. This fear is well founded as it has in the past tarnished
business reputations, reduced market share, impaired proﬁts, caused privacy
violations, and revealed internal sources and methods [6]. This poses a problem
where processing resources become duplicated and large capital is required for
their maintenance. While we acknowledge that Skald cannot overcome the rea-
sons behind why data is restricted, our framework can overcome the problem of
how to develop a shared infrastructure.
This is because Skald allows each partner to leverage their own Storage
Planner to restrict the transfer of raw and restricted data and conﬁgure the
Transport to acknowledge internal and external Planners. This in turn allows
partners to leverage a collective set of hardware resources while allowing the cre-
ation of Investigation Services capable of performing data discovery, clustering,
and colorations over the entire set of collective knowledge.
5.2
Sharing Derived Information with Partners
Cybersecurity analysis is mostly retrospective in nature and based on identify-
ing previously observed attack patterns [6]. Sharing information is vital in this
process as it allows analysts to make better correlations and see a more accu-
rate picture of what is happening. Unfortunately, current methods for sharing
information among partners require too much time to be truly eﬀective. This is

246
G.D. Webster et al.
because information ﬁrst needs to be identiﬁed as important, processed, validated
as sharable, and then transmitted to partners often in the form of Indicators of
Compromise. This is an inherently slow process but an even more critical issue is
that feature extraction methods are not uniform and vary among industry part-
ners [21]. This causes the receiving party to rerun extraction methods before the
information can be leveraged by their analysts.
Skald overcomes the above described issue by providing a platform that
supports breaking the traditional paradigm of how information is shared. This
is accomplished by providing the infrastructure to allow partners to directly
share a central repository of extracted features and jointly perform analysis
against the information. This approach allows partners: (i) to have a common
set of extraction methods that is understood by all parties, (ii) to have real-time
access to current information, and (iii) to have a wider view of the malicious
activity during the investigation.
Skald makes the aforementioned setup an easy task. In our deployment, we
created this setup with globally distributed partners by adding a Storage Service
that contained the logic required to replicate features between industry peers
while restricting sensitive raw data. Thus, this created a uniformed platform that
allowed all partners to leverage a collective pool of information. Furthermore,
when working with Investigation tasking, the taskings can leverage the ACL
meta-tags, discussed in Sect. 2.1, to inform the system if it should only use in-
house resources or use collective resources.
6
Related Work
The need for a large scale malware analysis framework has been well discussed
in prior work. Bitshred is a prime example and can perform malware correlation
based on hashes of features, thus greatly increasing the throughput of the analy-
sis system [13]. However, it only addresses the problem of dealing with already
extracted data through clustering correlations on these hashed features. It does
not address the full analysis lifecycle and is reliant on other systems for fea-
ture extraction. Hanif et al. [11] attempted to solve the problem of performing
scalable feature extraction in their work on BinaryPig, by performing feature
extraction in a distributed fashion through the use of a Hadoop based back-end.
Unfortunately, BinaryPig only performs static analysis of malware binaries, it
does not support deriving intelligence over the features, and the QoS schedule
cannot appropriately handle failed work.
Signiﬁcant research has been put forward in distributed setups addressing
the issues of scaling, ﬂexibility, and resilience. Service Oriented Architectures
(SOA) breaks systems down into business processes, i.e., Services, and integrates
them in an asynchronous event-driven manner through an Enterprise Service Bus
(ESB) [15]. By establishing disjoint components, SOA enables distributed sys-
tems by deﬁning how Services communicate versus their implementation. The
xSOA architecture expands upon traditional SOA by allowing multiple Services
to be combined under a single composite Service [19]. This composite server then

SKALD: A Scalable Architecture for Feature Extraction
247
provides a management layer which can perform Service orchestration, routing,
provisioning, as well as integrity checking. Verma et al. [24] has taken xSOA a
step further in their work on large-scale cluster management with Borg. They
developed a xSOA like system and optimized it by introducing a BorgMaster.
The BorgMaster serves as the master for Services and schedules their execution
across Borglets. Together, the BorgMaster and Borglets intelligently manage the
execution of Services and perform necessary actions to improve resilience, ﬂexi-
bility, and scalability. Additionally, Borg packages Services together for execution
to improve eﬃciency by cutting down on transmission time, network bandwidth,
and resource utilization. However, these architectures are not meant to deal with
cybersecurity work as the data is often designed to cause component failures and
Skald’s additional abstraction is required.
7
Conclusions
In this paper, we designed Skald: an architecture to support the entire lifecy-
cle of analysis against the ever growing volume of malicious activities plaguing
computer systems. Skald enables the design of a large-scale, distributed system
that is applicable in the security domain. To this end, it supports multiple users
and scales horizontally to support analysis of millions of objects. Skald pro-
vides mechanisms for implementors to incorporate static and dynamic feature
extraction techniques and apply advanced analytic across these features to derive
intelligence. Furthermore, it breaks the paradigm that the automated extraction
is the ultimate goal, opting to provide a ﬂexible architecture to empower human
analysts. In a similar manner, Skald’s design overcomes the limitation of cur-
rent sharing models. It does this by providing the infrastructure needed to allow
industry peers to perform analysis across collective knowledge while protecting
sensitive data. Empirical results conﬁrm that our solution scales horizontally,
at near linear growth, and is able to process objects at a rate of 3.1 ms with
zero critical error in contrast to existing system’s rate of 2.6 s and thousands
of critical errors. Additionally, Skald allows analysis across a collaborative set
of raw data and extracted features. Thus, it enables more accurate clustering
of malicious information and real-time data discovery while minimizes the need
for redundant feature extraction systems and thereby reduces analysis time and
infrastructure cost.
Availability
The prototype used to evaluate the Skald framework is open-source under the
Apache2 license. It can be accessed at: http://holmesprocessing.github.io/.
Acknowledgments. We would like to thank the Technical University of Munich for
providing ample infrastructure to support our prototype development. We would also
like to thank the United States Air Force for sponsoring George Webster in his academic
pursuit. In addition, we thank the German Federal Ministry of Education and Research

248
G.D. Webster et al.
for providing funding for hardware under grant 16KIS0328 (IUNO). Lastly, we would
like to thank the members of VirusTotal, Yara Exchange, and DARPA for their valuable
discussions and support.
References
1. Alvarez, V.M.: Yara 3.3.0. VirusTotal (Google Inc.) (2015). http://plusvic.github.
io/yara/
2. Barack, O.: Executive Order No. 13691. Promoting Private Sector Cybersecurity
Information Sharing (2015)
3. Bu, Z., Dirro, T., Greve, P., Lin, Y., Marcus, D., Paget, F., Pogulievsky, V., Schmu-
gar, C., Shah, J., Sommer, D., et al.: McAfee Threats Report: Second Quarter 2012
(2012)
4. Choo, K.-K.R.: The cyber threat landscape: challenges and future research direc-
tions. Comput. Secur. 30(8), 719–731 (2011)
5. Cristian, F.: Understanding fault-tolerant distributed systems. Commun. ACM
34(2), 56–78 (1991)
6. DARPA: Cyber Information Sharing - DARPA Cyber Forum, October 2015
7. Estublier, J.: Software conﬁguration management: a roadmap. In: Conference on
the Future of Software Engineering (2000)
8. Google:
Protocol
Buﬀers,
November
2015.
https://developers.google.com/
protocol-buﬀers/
9. Grobauer, B., Berger, S., G¨obel, J., Schreck, T., Wallinger, J.: The MANTIS
Framework: Cyber Threat Intelligence Management for CERTs, Boston, USA,
June 2014
10. Guarnieri, C., Tanasi, A., Bremer, J., Schloesser, M.: The Cuckoo Sandbox (2012).
http://cuckoosandbox.org
11. Hanif, Z., Calhoun, T., Trost, J.: BinaryPig: scalable static binary analysis over
Hadoop. In: Black Hat USA (2013)
12. HiveMQ: MQTT Essentials Part 6: Quality of Service 0, 1 & 2 (2015). http://
www.hivemq.com/blog/mqtt-essentials-part-6-mqtt-quality-of-service-levels
13. Jang, J., Brumley, D., Venkataraman, S.: BitShred: feature hashing malware for
scalable triage and semantic analysis. In: Conference on Computer and Communi-
cations Security, CCS (2011)
14. Kolosnjaji, B., Zarras, A., Lengyel, T., Webster, G., Eckert, C.: Adaptive
semantics-aware malware classiﬁcation. In: Caballero, J., Zurutuza, U., Rodr´ıguez,
R.J. (eds.) DIMVA 2016. LNCS, vol. 9721, pp. 419–439. Springer, Heidelberg
(2016). doi:10.1007/978-3-319-40667-1 21
15. Krafzig, D., Banke, K., Slama, D.: Enterprise SOA: Service-Oriented Architecture
Best Practices. Prentice Hall Professional, Indianapolis (2005)
16. Lakshman, A., Malik, P.: Cassandra: a decentralized structured storage system.
ACM SIGOPS Oper. Syst. Rev. 44(2), 35–40 (2010)
17. Lengyel, T.K., Maresca, S., Payne, B.D., Webster, G.D., Vogl, S., Kiayias, A.: Scal-
ability, ﬁdelity and stealth in the DRAKVUF dynamic malware analysis system.
In: Annual Computer Security Applications Conference, ACSAC (2014)
18. Ollmann, G.: Behind todays crimeware installation lifecycle: how advanced mal-
ware morphs to remain stealthy and persistent. Technical report, Damballa (2011)
19. Papazoglou, M.P., Van Den Heuvel, W.-J.: Service oriented architectures:
approaches, technologies and research issues. VLDB J. 16(3), 389–415 (2007)

SKALD: A Scalable Architecture for Feature Extraction
249
20. Parkour, M., DiMino, A.: Deepend Research - Yara Exchange, May 2015. http://
www.deependresearch.org/2012/08/yara-signature-exchange-google-group.htm
21. Shields, W.: Problems with PEHash Implementations, September 2014. https://
gist.github.com/wxsBSD/07a5709fdcb59d346e9e
22. Stamos, A.: The Failure of the Security Industry, April 2015. http://www.
scmagazine.com/the-failure-of-the-security-industry/article/403261/
23. The
MITRE
Corporation:
Collaborative
Research
Into
Threats
(CRITs),
June
2014.
http://www.mitre.org/capabilities/cybersecurity/overview/
cybersecurity-blog/collaborative-research-into-threats-crits
24. Verma, A., Pedrosa, L., Korupolu, M.R., Oppenheimer, D., Tune, E., Wilkes, J.:
Large-scale cluster management at Google with Borg. In: European Conference on
Computer Systems, EuroSys (2015)
25. VirusTotal: File Statistics, May 2015. https://www.virustotal.com/en/statistics/
26. Vixie, P.: Internet Security Marketing: Buyer Beware, April 2015. http://www.
circleid.com/posts/20150420 internet security marketing buyer beware/
27. Wicherski, G.: PEHash: a novel approach to fast malware clustering. In: USENIX
Workshop on Large-Scale Exploits and Emergent Threats, LEET (2009)
28. Zeltser,
L.:
SANS
-
Managing
and
Exploring
Malware
Samples
with
Viper,
June
2014.
https://digital-forensics.sans.org/blog/2014/06/04/
managing-and-exploring-malware-samples-with-viper

Privacy and Watermarking

Leveraging Internet Services
to Evade Censorship
Apostolis Zarras(B)
Technical University of Munich, Garching, Germany
zarras@sec.in.tum.de
Abstract. Free and uncensored access to the Internet is an important
right nowadays. Repressive regimes, however, prevent their citizens from
freely using the Internet and utilize censorship to suppress unwanted
content. To overcome such ﬁltering, researchers introduced circumven-
tion systems that avoid censorship by cloaking and redirecting the cen-
sored traﬃc through a legitimate channel. Sadly, this solution can raise
alerts to censors, especially when it is mistakenly used. In this paper, we
argue that relying on a single channel is not suﬃcient to evade censor-
ship since the usage pattern of a circumvention system diﬀers compared
to a legitimate use of a service. To address this limitation of state-of-the-
art systems, we introduce Camouflage, an approach to combine multi-
ple non-blocked communication protocols and dynamically switch among
these tunnels. Each protocol is only used for a limited amount of time and
the Internet connection is transparently routed through instances of dif-
ferent censorship circumvention systems. We prototype Camouflage by
using applications which are based on these protocols and also oﬀer end-
to-end encryption to prevent censors from distinguishing circumvention
systems from regular services. We evaluate Camouflage in countries
that impose censorship and demonstrate that our approach can success-
fully bypass existing censorship systems while remaining undetected.
1
Introduction
The Internet has become the medium of choice for people to search for infor-
mation, conduct business, and enjoy entertainment. It oﬀers an abundance of
information accessible to anyone with an Internet connection. Additionally, the
explosion of social media plays a central role in shaping political debates, as
for example the coordination of movements in “Arab Spring”, where citizens
used Facebook, Twitter, and YouTube to put pressure on their governments.
However, free communication threatens repressive regimes as it, in many cases,
exposes concealed truths and government corruption. Personal communication
is then becoming subject to pervasive monitoring and surveillance, and various
state and corporate actors are trying to block access to controversial informa-
tion. In detail, regimes can trace, monitor, ﬁlter, and block data ﬂows using
sophisticated technologies such as IP address blocking, DNS hijacking, and deep
packet inspection [12,27].
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 253–270, 2016.
DOI: 10.1007/978-3-319-45871-7 16

254
A. Zarras
With the use of censorship technologies to be more timely than ever,
researchers developed a number of diﬀerent systems to retain the freedom of
the Internet [5,16,19], which are widely-known as censorship circumvention sys-
tems and most of the time try to deploy a redirection proxy that provides access
to blocked websites. Nevertheless, censors can locate such proxies and instantly
block them [20,33]. The root cause of most of the systems’ identiﬁcation is
the diﬀerentiation they exhibit from regular Internet traﬃc. To overcome this
limitation, unobservable circumvention systems were introduced. These systems
try to impersonate popular applications to blend with the allowed Internet traf-
ﬁc [35,44,45]. Although these systems sound promising, they fail to raise the bar
against censorship mostly because they only implement the imitating protocol
partially and thus fall into discrepancies that censors can locate [25].
Consequently, unobservability by imitation is a fundamentally ﬂawed app-
roach [25]. For this reason, researchers introduced new systems that operate
higher in the protocol stack. These systems avoid censorship by executing the
actual protocol instead of trying to impersonate it [1,2,26,50] and thus can pro-
tect the users from various Internet restrictions. However, they suﬀer from being
shut down if they got recognized, mostly due to their users’ inexperience. In
general, the average end-user is not familiar with the proper conﬁguration and
utilization of a circumvention system. Hence, the erroneous usage of such a sys-
tem can create traﬃc that may appear suspicious to censors. Additionally, such
systems support only one protocol and therefore are susceptible to loosing their
functionality in case a government decides to completely block this protocol.
In this paper, we present Camouflage, a novel approach that protects users
from Internet censorship, while requiring limited expertise from the users’ side
compared to existing systems. More speciﬁcally, Camouflage is a framework
to which existing or future censorship-resistant systems can be plugged in, coop-
erate, and provide increased resistance against censorship. The main idea of our
approach is to tunnel Internet traﬃc inside multiple non-blocked communication
protocols and dynamically switch among them. Many of the existing systems can
protect users from being subjects of censorship by tunneling the traﬃc through
various protocols [1,2,26,50], yet they all work independently from each other.
Thus, users who want to avoid censors’ surveillance should install and conﬁgure
as many of these systems on their computers, and manually rotate the forwarded
traﬃc among them, which is likely an error-prone process. In our approach, the
censorship-resistant tools can be attached to Camouflage as plugins and the
framework itself decides when and for how long they will be used.
To demonstrate the functionality of our framework, we built a prototype
implementation, which supports four diﬀerent protocols used by four widely-
used applications. To evaluate Camouflage in real-world, we chose countries
that impose censorship and browsed the web with censored terms from inside
these countries. The experimental results exhibit that our prototype can be
successfully used for web browsing, while resisting censors’ blocking eﬀorts. To
the best of our knowledge, our approach is the ﬁrst attempt to combine a variety
of diﬀerent censorship circumvention systems under one solid framework.

Leveraging Internet Services to Evade Censorship
255
In summary, we make the following main contributions:
– We propose Camouflage, a novel approach for censorship circumvention that
combines the advantages of existing systems while makes it easier for users to
employ them. Its plug-and-play architecture can be used by existing or future
censorship-resilient systems.
– We build a prototype based on widely-used applications and evaluate its per-
formance and security. We show that the produced overhead is related to the
implementation of each circumvention system.
– We evaluate our prototype in existing censored networks and show its ability
to bypass censorship in real-world, concealing at the same time its presence
from the censors.
2
Threat Model
Throughout this paper we use the following threat model. We assume that a user
connects to the Internet through an Internet Service Provider (ISP) that utilizes
some kind of censorship system. In fact, governments can control and regulate
ISPs, which can be forced to monitor and block users’ access to certain Internet
destinations. For example, China can ﬁlter IP packets [9], or even can censor
services such as blog platforms [52], chat programs [31], and search engines [51].
In our scenario, we assume that the user operates in an inhospitable network
where ISPs can trace, monitor, ﬁlter, and block data ﬂows using sophisticated
technologies [12]. Additionally, we assume that the users are restricted from
using proxies or other circumvention systems to evade censorship. Therefore, we
take for granted that ISPs can identify and block the traﬃc that is forwarded
through a censorship circumvention system using a variety of diﬀerent features
and strategies [22,25] and notify the authorities for any incident. Then, the
violators might face severe punishments varying from payment of ﬁnes to even
imprisonment [46].
We also assume that regimes do not want to jeopardize the usability of the
Internet due to political and economical implications. For instance, services such
as email and VoIP constitute important parts of today’s communications among
businesses, which beneﬁt from these services to reduce their operational costs.
Furthermore, VoIP and chat communications are also widespread among individ-
uals due to the level of convenience and ﬂexibility they oﬀer. Finally, another type
of Internet services that is popular, mostly among young people, is online games,
which are used for entertainment purposes. Thus, we speculate that censorship
regulations do not interfere with fundamental Internet services such as email
communications, VoIP, ﬁle sharing, and even entertainment services including
online games. In the extreme case in which a censor might decide to block some
of these services, we believe that this decision will not aﬀect all of them, but
only a fraction of the services.

256
A. Zarras
Fig. 1. Abstract architecture of Camouflage.
3
System Design
Camouflage acts as a pluggable framework for diﬀerent circumvention systems
that evade censorship by leveraging existing protocols and services. Thus, in
order to gain access to the framework, users need to install the Camouflage
client on their computers along with the appropriate plugins (i.e., circumvention
systems). On the other hand, administrators should install the Camouflage
server as well as the supported plugins. Note that a server can support multiple
circumvention systems that operate in parallel and are synchronized. Both clients
and servers consist of key components necessary for converting and forwarding
the traﬃc. In the remainder of this section, we introduce the main components of
Camouflage and describe how do they contribute to the design and operation
of a secure and easy-to-use framework.
3.1
Abstract Architecture
Camouflage is composed of two key components: a client and a server. The
client usually runs in a censored environment in which all the communications
are monitored by censors, while the server operates in an uncensored and secure
environment where the network traﬃc ﬂows unrestricted. In addition, our frame-
work transfers all the data transparently and thus any software which can be
conﬁgured to use a proxy, such as a web browser, is able to use the client. On
the server side, we set up a proxy (HTTP or SOCKS), which is responsible to
communicate with the outside world and access the censored content. Figure 1
illustrates the framework’s abstract architecture.
The client consists of the following components: a connector, a dispatcher,
and diﬀerent plugins. The connector receives the traﬃc from the user’s applica-
tion (e.g., the web browser), transforms it from multiple connections’ traﬃc to
a serialized data stream by adding suitable headers, and then encrypts it. Next,
it forwards the traﬃc to the dispatcher, which decides through which plugin
the data stream will be transferred. Finally, the plugin running in the client’s
side is responsible for transmitting the data stream. Accordingly, when a plugin
receives an encrypted data stream transfers it to the connector through the dis-
patcher. In this component, the data stream is decrypted, is split into multiple
connections, and the traﬃc is forwarded to the appropriate application.

Leveraging Internet Services to Evade Censorship
257
The server, on the other hand, consists of the proxy, the connector and the
supported plugin. When encrypted data is received, the plugin transfers it to
the connector. The server’s connector operates in a similar way to the one pre-
sented for the client. More speciﬁcally, when a data stream is received, it checks
the connection identiﬁer and forwards the data through the corresponding con-
nection with the proxy, or opens a new connection if the identiﬁer is currently
unused (i.e., the connection was newly established on the client’s side). Similar,
when the server transmits data back to the client, the proxy sends the data
sequences to the connector in order to transform them into an encrypted data
stream and then the plugin transfers it through the suitable channel. As we pre-
viously mentioned, a server can support more than one circumvention system.
In this case, the existence of a dispatcher is necessary in order to combine the
diﬀerent plugins and forward the traﬃc to the same proxy.
3.2
Connector
Modern applications, such as web browsers, use HTTP/1.1 protocol that allows
multiple network connections to run in parallel. In detail, when a user visits a
web page that has many diﬀerent objects on it (e.g., images, JavaScript ﬁles,
frames, data feeds, etc.), the browser tries to download several of them at once
in diﬀerent parallel sessions to obtain better performance. Most HTTP servers
and browsers use an HTTP protocol feature called keep-alive that does not
close the TCP connection when the client is done with it; the connection closes
either after an idle connection timeout or after a maximum number of allowed
requests. This makes sense since opening a remote connection is expensive due
to the three-way TCP handshake, so it is faster to open one connection and then
download n items compared to open and close a connection n times. However,
the plugins provide only one data channel for forwarding the data. To overcome
this obstacle, we need a middleware that combines the multiple connections
generated from applications with the single data channel provided by the plugins.
In Camouflage, the connector plays the role of this middleware.
In detail, the connector bundles the traﬃc from all connections to one data
stream by adding a small header in front of the payload. This header contains
the identiﬁer of the connection and the payload’s length. Using the identiﬁers,
the connector can forward the data to the correct connection within the user’s
application, while the length of the payload ensures that the appropriate amount
of data will be read. The transformed data stream is then encrypted and for-
warded to plugins. The connector also receives a similar data stream back from
the plugins with the contents of the censored web pages. Additionally, it ensures
that the state of each connection is consistent at both client and server side.
Especially if one side closes a connection, this information is transmitted to the
other side which in turn closes the connection as well.

258
A. Zarras
3.3
Dispatcher
The dispatcher acts as an intermediate component between the connector and
the actual plugins. More precisely, the use of the dispatcher is twofold: (i) for-
ward the traﬃc from a client to a server and vice-versa, and (ii) schedule the
orchestration of activities by planning and monitoring the plugins. For the ﬁrst
part, the dispatcher forwards the outgoing traﬃc from the connector to a server
over the currently active plugins. Accordingly, it receives the incoming traﬃc
from a server and delivers it to the connector. To this end, the dispatcher should
know which plugins are supported by the current instance of Camouflage. If
a connection to a plugin is not feasible anymore, the dispatcher will temporarily
disable this plugin and will try to connect with an alternative one. Nevertheless,
the plugin will be enabled again after a timeout period.
For the second part, the dispatcher administers the plugins by implementing
the schedule in which a plugin is selected. The implementation of the schedule
could be created either manually by the user or automatically by integrating
the manifest ﬁles of plugins. The former solution is recommended for advanced
users, while inexperienced ones can always use the latter. The dispatcher can
decide at any time if it wants to execute the plugins on parallel, or one at a
time. This way, makes it diﬃcult for the censors to create detection patterns.
3.4
Plugins
The data channels (circumvention systems) that used for tunneling the traﬃc
are the lifeblood of Camouflage. Each circumvention system is implemented
in our framework as a standalone plugin. Camouflage supports an abstract
architecture, in which new circumvention systems can be easily plugged in. It
is mandatory that the implementation of each system to leverage an actual
protocol, or service, and not try to imitate it. Additionally, each plugin must
accompanied by a manifest ﬁle. This ﬁle contains information such as the plu-
gin’s unique identiﬁer, the recommended operation time used by the dispatcher,
the suggested timeout period after which the dispatcher will try to reconnect to
the plugin if a previous connection attempt failed, and the list of the required
software. Both the usage time duration and the timeout period are only recom-
mended values by developers and can be voluntarily modiﬁed by users.
Each plugin is responsible for concealing its tunneled data stream. There-
fore, it is highly recommended the utilization of a two-layer encryption strategy.
First, each plugin should leverage applications that use encrypted communica-
tions by default. This will prevent regimes that monitor the network traﬃc to
have a direct access to unencrypted data. However, this may not be always the
case. For instance, a company that wants to operate in a country that imposes
censorship must accept the country’s requirements in order to be allowed to
enter the country’s market. These requirements could permit, among the others,
the government’s censors to have access to unencrypted network traﬃc. Thus,
no matter how strong is the encryption that the application uses, the plugin
should implement a second layer of encryption as well. More speciﬁcally, it must

Leveraging Internet Services to Evade Censorship
259
encrypt the data stream before being encrypted by the application itself. This
way, the censors that monitor the network traﬃc will be prevented from accessing
the plaintext data if an agreement with the application provider is established.
Overall, the two layers of encryption ensure that even if the application gets
compromised, the content of the data stream will remain hidden.
4
Circumvention Systems
As we mentioned, the lifeblood of our system is the diﬀerent plugins it supports
and each of them implements a circumvention system. To prove the feasibility of
our approach, we implemented a prototype of Camouflage using four diﬀerent
circumvention systems that are attached to our framework as plugins. These plu-
gins leverage four diﬀerent services and protocols to evade censorship and do not
rely on emulation or any other imitation technique, but each of them utilizes the
actual implementation of a protocol. To test each service, we selected an applica-
tion that supports a utilized protocol and implemented the censorship-resistant
system on top of this application. In brief, we chose the following applications:
1. Public email providers: to implement a circumvention system based on SMTP
protocol.
2. Skype: to leverage VoIP and instant messaging communication protocols.
3. Runes of Magic: to beneﬁt from communication protocols in multiplayer
online game platforms.
4. Dropbox: to conceal traﬃc within a ﬁle sharing service.
Note that these applications are selected only as proof-of-concept scenar-
ios and therefore the circumvention systems can operate in the same way with
diﬀerently selected applications (e.g., utilization of Google Voice as VoIP appli-
cation). Same principle applies in the case that an application is forbidden in a
country. It is worth to mention here that some of these applications were pro-
posed in previous works [26,50], however, it was not possible to ﬁnd their actual
implementations and in order to evaluate our prototype we had to design and
implement them from scratch. In addition, for the purpose of this paper, the com-
munication between the client and the server is already initialized, which means
that there is no need for any kind of registration. Nevertheless, sophisticated
registration strategies could be applied in real-world circumvention systems to
prevent attacks, such as denial-of-service against the system, but these strate-
gies are outside the scope of this paper. In general, the framework itself allows
developers to design their own registration strategies without any restrictions.
4.1
Email
Electronic mail (email) is a widely-used method of exchanging digital messages
from an author to one or more recipients. Modern email systems are based
on a store-and-forward architecture, which permits servers to accept, forward,

260
A. Zarras
deliver, and store messages. The wide acceptance of email allows us to create a
circumvention system that utilizes the email delivery system to evade censorship.
More precisely, the system uses publicly available email providers to hide the
data stream inside email messages. Private email providers could be used as well
to perform this task, but we believe they are more prone to manipulation by
repressive regimes compared to public email services provided by international
corporations. We assume that censors do not have access to the users’ mailboxes
hosted outside of the regime’s geographical boundaries. In contrast, it is easier
for a censor to access mailboxes of email providers that are hosted inside the
regime’s borders. Moreover, we consider as suitable email providers those which
by default oﬀer email encryption. This way, an encrypted email will not be
considered suspicious by censorship authorities, if all the emails sent by this
email provider are encrypted as well.
With regards to the design of the email circumvention system, both client
and server plugins share the same mailbox that is hosted at a public service. The
email servers of this service must reside outside the censors’ jurisdiction (e.g.,
Gmail, Yahoo Mail, Outlook.com). The client communicates with the server, and
vice-versa, by sending emails to the same mailbox. The data stream is divided
into blocks, which are encoded in Base64 format. Additionally, we use encryp-
tion to protect users from revealing the content of the visited web pages to a
compromised email provider. The body of the email contains the data stream,
while the header contains a message counter to retain the correct sequence of
data blocks and a ﬂag that indicates the sender of this message. Both client
and server implementations use the IMAP IDLE feature to get notiﬁed on new
emails and recreate the data stream from the received messages. This feature
allows IMAP email users to immediately receive any mailbox changes without
having to automatically and repeatedly ask the server for new messages [28].
4.2
VoIP and Instant Messaging
Voice over Internet Protocol (VoIP) is a technology used for the delivery of voice
communications and multimedia sessions over Internet Protocol (IP) networks.
Instant messaging, on the other hand, is a type of online chat that oﬀers real-time
text transmission over the Internet. Short messages are typically transmitted bi-
directionally between two parties, where each party composes a sentence and
sends it to the other. Modern applications that support VoIP often support
instant messaging as well. One such application is Skype, in which users com-
municate with each other both with voice and text. For the prototype of this
circumvention system we use Skype and implement it to transmit data either
as audio or as encrypted text messages. However, any other VoIP software will
work in a similar way. In the following paragraphs we analyze the operation of
each service in more detail.
Nowadays, many companies switch from traditional telephony to VoIP to
reduce their telephone costs [10,17]. Similarly, individuals communicate with
each other using publicly available and usually free VoIP services. Hence, VoIP is
diﬃcult to be manipulated or even blocked by censorship authorities. We exploit

Leveraging Internet Services to Evade Censorship
261
this censorship weakness and create a system that transfers uncensored data over
VoIP. More speciﬁcally, our system modulates binary data into audio signals,
which are transmitted over the voice channel of a VoIP software. This concept
is similar to the operation of modems (i.e., a device that modulates an analog
carrier signal to encode digital information and also demodulates such a carrier
signal to decode the transmitted information). The goal is to produce a signal,
which can be transmitted and decoded, to reproduce the original digital data.
For our prototype we used a 1200 baud Audio Frequency Shift Keying modem
without sophisticated error correction. However, in a real-world application a
more advanced approach should be applied.
We mentioned that for the prototype implementation of the system we
selected Skype. Therefore, the data stream is converted to audio signals and
transmitted from one edge to the other using Skype call. For sending the gen-
erated audio signal over Skype we use a virtual audio device. This device is
installed as a driver and behaves like a real sound card. In Skype it can be con-
ﬁgured as the microphone device. Additionally, a second virtual audio device is
set as Skype’s output and all signals pointing to this device are recorded by the
system, and eventually demodulated back to the initial data stream.
Alongside with VoIP services, some companies leverage instant messaging as
a complementary method to the customer support services they oﬀer. In addi-
tion, instant messaging is one of the most popular Internet activities among
individuals. According to studies, a vast majority of the Internet’s population
uses instant messaging as its main communication tool [29]. Consequently, if
a repressive regime blocks a major application such as Skype, which provides
instant messaging communication services to its users, this will cause a severe
impact to a signiﬁcant portion of the regime’s inhabitants and companies. Hence,
in some cases is preferable for a repressive regime to monitor this means of com-
munication than completely forbid it. With this in mind, we created a circum-
vention system that utilizes the text channels of instant messaging services. To
our beneﬁt, Skype automatically encrypts the transmitted messages, fact that
oﬀers us a two-layer encryption similar to email circumvention system. In gen-
eral, we used a similar approach to the one presented for the email circumvention
system.
4.3
Online Gaming
Online games are video games which are played over the Internet. These games
are divided in single-player where input from only one player is expected through-
out the course of the gaming session and multi-player that allow more than one
person to play in the same game environment at the same time. The latter
provide their users (players) with a form of social communication channel in
which the players can interact with each other. These communication services in
multi-player games can be used as covert channels for the secret transfer of data
streams. Modern multi-player games oﬀer various communication channels such
as public chat rooms, private instant messaging, and voice chat. We leverage the
services oﬀered by multi-player games to create a circumvention system.

262
A. Zarras
We prototype a circumvention system, by using the voice chat communication
channel of Runes of Magic, a massively multiplayer online role-playing game.
We create two characters (avatars) which are connected to the same server. The
client owns the ﬁrst avatar, while the server possesses the second. The avatars
create a voice chat room where they communicate. We utilize a similar approach
to the one presented for the VoIP circumvention system. More accurately, we
convert the digital data stream to audio signals and transmit them from one
side to the other through the voice chat room. The other side demodulates the
signals back to their original form and processes the requests.
This approach is only a proof-of-concept. We want to show that a strategy
that leverages online games to evade censorship is feasible. Thus, we only use
the voice chat rooms to transmit the data, which imposes limitations in the
amount of data being transferred. To increase the amount of transmitted data,
we can use in parallel approaches that beneﬁt, for example, from the movements
of avatars to transmit information over the online game engine [47,48].
4.4
File Sharing
File sharing is a private or public distribution of data or resources, such as
documents, multimedia, graphics, computer programs, images, or e-books, in a
network with diﬀerent levels of sharing privileges. More speciﬁcally, it allows to a
number of people to simultaneously modify the same ﬁles. File sharing is known
for quite a long period. It is massively used, from companies and universities,
among people who work on the same projects and need to share and modify the
same documents and data. Recently, as social media has become increasingly
popular with hundreds of millions of users, ﬁle sharing became attractive among
individuals as well. Nowadays, people use it to upload and share pictures and
videos among friends. File sharing is so massively used in today’s world, which
oﬀers a unique opportunity for a circumvention system based on this service.
We create a circumvention system on top of Dropbox. Dropbox is a personal
cloud storage service frequently used for ﬁle sharing. Our censorship-resistant
system uses the oﬃcial Dropbox client to create a private communication channel
between the client and the server. After conﬁguring and starting Dropbox on
both sides, the tool uses a shared folder for data exchange. The sent data is
split into blocks which are then stored as encrypted ﬁles in the shared folder.
The folder is automatically synchronized with the other party. Both plugins can
monitor the folder for new ﬁles, read the data from them, and recreate the data
stream. For additional security we can apply solutions that revoke the access to
ﬁles after a certain period of time [6,21,37,49].
5
Evaluation
We implemented a prototype of Camouflage and evaluated the eﬀectiveness
of our approach. We ﬁrst measured its performance and then studied the users’
behavior to properly conﬁgure the plugins. Finally, we evaluated Camouflage’s
ability to evade censorship on countries that impose censorship to their citizens.

Leveraging Internet Services to Evade Censorship
263
100
101
102
103
104
Instant Messaging Email
File Sharing
VoIP
Overhead (%)
Circumvention System
Overhead
Latency
Fig. 2. Overhead and latency in load-
ing a page with diﬀerent plugins.
100
101
102
103
104
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
Time (s)
File size (kB)
Email
VoIP
Instant Messaging
File Sharing
Fig. 3. Downloading time for ﬁles size
from 10 to 100 kB.
5.1
Performance
Using a circumvention system is the only way to access a censored web page,
especially if services such as Virtual Private Networks (VPNs) are forbidden.
Thus, to show that a circumvention system must only be used for accessing
censored web pages, and not for web surﬁng, we measured its performance.
Therefore, all the performance experiments were conducted by using broadband
connections for accessing the Internet on both client and server side. We believe
that individuals can contribute to the Internet’s freedom by hosting at least
one circumvention system (Tor [16] uses a similar infrastructure). Hence, we
chose to use DSL instead of a university’s connection to make our experiments
more realistic. For instance, individuals who want to contribute in the ﬁght
against censorship can run a Camouflage server during the night when their
bandwidth is usually idle. During our experiments, the utilized DSL connections
remained idle and the only traﬃc sent over the network was the traﬃc which was
created by the circumvention systems. Obviously, using broadband connections
had a huge impact in the performance of Camouflage, compared to previous
works [26], but at the same time it provided us with realistic results instead of
the ideal results we would have gotten in a sterile laboratory environment.
In our ﬁrst experiment, we measured the overhead and the latency added by
each circumvention system when a web page is downloaded. First, we visited the
main page of popular websites with a browser without using any intermediate
channel and captured all the incoming traﬃc which constitutes the bottom line
of the experiment. Then, we visited the same web pages with diﬀerent circum-
vention systems. Between the measurements we cleared the browser’s cache so
all the contents of the web pages were loaded directly from the network. Figure 2
shows the average values for the overhead and the latency of each system. As we
can see, instant messaging increased the incoming data only by 39 % compared
to VoIP, which increased the traﬃc by a factor of 84. This is caused because VoIP
can conceal a smaller amount of information in its audio signals than the trans-
ferred information through raw text. Regarding email, it increased the overhead
by 107 %, while ﬁle sharing increased it by 272 %.

264
A. Zarras
The latency on the other hand is caused by diﬀerent factors for the various
systems. For instant messaging, the latency is low because this service is designed
to be real-time and transferring data this way does not create much overhead. In
the case of the email, the latency is mainly caused by the time email servers spend
for processing the mails; the traﬃc overhead is considered negligible here. Same
principle applies for the ﬁle sharing. The data is ﬁrst sent to a ﬁle sharing server,
processed there, and then synchronized with the ﬁle sharing client. Finally, the
latency of the VoIP system is clearly caused by its limited bandwidth.
Expanding the previous experiment, we conducted downloads of ﬁles of var-
ious sizes and measured the time required for a complete download. The exper-
iment used the same conﬁgurations described earlier. Figure 3 illustrates the
results. The outcomes of this experiment help us to create a version of our frame-
work in which all the supported circumvention systems can operate in parallel.
In detail, we notice that a circumvention system that uses instant messaging is
the most eﬀective approach no matter what the size of the downloaded ﬁle is.
Therefore, this approach could be used for web elements that require more band-
width such as videos and high-resolution images. Email and ﬁle sharing systems
behave in similar way and the oﬀered bandwidth by each system is rather close
to the other. Thus, these systems could be used to download medium and low
quality images and medium size Flash applications. Finally, VoIP systems oﬀer
a rather small bandwidth for transferring data. Hence, VoIP censorship-resistant
tools can be used to transmit a limited amount of data, such as a text entry in
a microblogging service like Twitter, or to download the mobile versions of the
websites. Although, in theory all circumvention systems can be used for all the
tasks, in practice is not always feasible.
5.2
Traﬃc Patterns
Camouflage oﬀers unobservable connections. By unobservability we mean the
ability of a circumvention system to hide its existence from censorship author-
ities. In other words, the censors should not be able to identify whether a user
is using a circumvention system. This is essential for the user’s safety because
authorities can prohibit the use of technologies that evade censorship. To pre-
vent a censor from detecting Camouflage, the behavior of the protocols used
as tunnels should be as close as possible to a user’s normal use of those protocols.
Table 1. Suggested time values for dif-
ferent applications.
Application
Duration (min)
Email
1–3
VoIP
20–30
Instant messaging 15–20
File sharing
5–10
For this, we monitored the users’
behavior when they utilize the proposed
applications. Additionally, we considered
reports that studied traﬃc characteris-
tics [8,40,41]. The outcomes allowed us
to create generic traﬃc patterns that
match the behavior of the majority of
users. Table 1 depicts an overview of our
exported results. Keep in mind that these
results are aﬀected by many factors, such
as the age and cultural inﬂuences, and may look diﬀerent in separate regions.

Leveraging Internet Services to Evade Censorship
265
Unfortunately, there exist individuals whose behaviors do not match with our
proposed patterns. This allows to a well-trained censor to detect these discrep-
ancies. With that in mind, we designed our prototype to liberally allow its users
to conﬁgure it based on their demands. More precisely, during the installation
process of a circumvention system, users get informed about the typically-used
traﬃc pattern and asked for any modiﬁcations. Note that users are not bind to
their initial decisions and can modify these traﬃc patterns any time they want.
However, we recommend only the advanced users to manually modify these val-
ues because a misconﬁgured system can cause exactly the opposite results and
the existence of Camouflage to be revealed to censors.
Nevertheless, our experiments showed that is not always feasible for users to
know their unique traﬃc patterns as they may do not have neither the experience
nor the suitable tools to measure it. Simultaneously, users that use the very same
services leveraged by our plugins in their everyday lives can raise suspicions when
these services used with and without Camouflage. This results in two diﬀerent
traﬃc patterns which can be observed by anomaly-based detection systems. To
overcome this limitation we enhanced Camouflage with the ability to monitor
and decipher the network traﬃc patterns of a user, when the plugins are not
used. This way, the generated network patterns will be unique and based on
user’s typical behavior. Consequently, a censor will not be able to distinguish
between the real traﬃc and the artiﬁcial one, and thus will not be able to detect
the existence of our framework, as the results of our experiments revealed. More
precisely, the traﬃc patterns with and without the use of Camouflage looked
almost identical which makes it impossible for censors to spot the diﬀerences.
5.3
Real-World Deployment
To explore how Camouflage performs in a real-world scenario, we evaluated
it in countries that impose censorship to their citizens. Therefore, we acquired
access to servers hosted in these countries and imposed to the same censor-
ship as these countries’ inhabitants. Then, we tried to access forbidden websites,
which are usual websites that contain known forbidden keywords. We repeated
our experiments in diﬀerent time periods to capture any possible changes in
the detection capabilities of the censors. Our early results demonstrate that our
framework can successfully evade censorship, while it remains undetectable for
a large period. In detail, without Camouflage it was not possible to render
a plethora of web pages that contained one or more forbidden keywords, which
became possible once we utilized our framework. To be certain that our results
are accurate, we repeated these experiments for a period of one month. Dur-
ing this period we were able to evade censorship that was imposed in diﬀerent
countries.
A perfect example of such a country is China. It is well known that China has
the world’s most complex Internet censorship system [23]. However, its censors
are very prudent to perform DNS hijacking nowadays due to the risk of aﬀecting
the network in other countries [34]. Chinese censors impose strict restrictions
on international Internet traﬃc and the most eﬀective ﬁltering mechanism is

266
A. Zarras
the keyword ﬁltering. These factors make China an ideal candidate to evaluate
Camouflage. For this purpose, we used a list of known forbidden keywords.
We ran our experiments in daily basis for a period of one month and moni-
tored if during this period (i) we were able to access censored web pages and
(ii) Camouflage got detected and its services were banned. The outcomes of
this experiment revealed that with the utilization of Camouflage we could
access web pages that otherwise would not be possible. Additionally, during the
period of our experiment we were able to continuously access these forbidden
web pages which shows that our framework was not detected by censors. There-
fore, we believe that Camouflage can assure unobservability as it blends with
the real network traﬃc. It is worth to mention that not all the applications were
available in China, for instance, we could not use Dropbox. Therefore, we per-
formed our experiments only with the allowed plugins. Nevertheless, even if an
application is not available to a country, it can easily be replaced by another.
Therefore, by using more plugins we increase our chances to access the data we
want in a heavy censored environment.
Overall, Camouflage was able to evade censorship when applied. However,
the main goal of the framework is to provide access to censored web pages. Thus,
it should explicitly be used only for this speciﬁc scenario and not for everyday
tasks such as web surﬁng, e-radio listening, or any other activities that require
a high bandwidth connection and could raise suspicions due to highly-produced
network traﬃc over services that are not designed to produce so.
6
Related Work
Circumvention systems try to ensure anonymity to their users. Anonymity is
an old idea. Chaum proposed a technique based on public key cryptography
that allows an electronic mail system to hide both the participants and the
content of the communication [7]. There exist systems that provide anonymity by
following a high-latency network design [13,24]. These systems can resist against
strong adversaries, but introduce too much lag for interactive tasks such as web
browsing. On the opposite side, systems with low-latency network design [3,4,11]
can anonymize interactive network traﬃc, but it is diﬃcult to prevent an attacker
that eavesdrops both ends of the communication from correlating the timing and
volume of traﬃc [38]. Finally, there is the peer-to-peer network design in which
the participants both generate and relay traﬃc for others [30,39].
On the other hand, the oldest technique to evade censorship and surveillance
is the use of open proxy servers [18,42]. Toward this direction, Infranet [19]
improves the proxies infrastructure by leveraging a tunnel protocol that pro-
vides a covert communication channel between the clients and the servers. Simi-
larly, Collage [5] uses user-generated content on social-networking and image-
sharing websites such as Facebook and Flickr to embed hidden messages into
cover traﬃc. To this end, researchers presented an obfuscation-based approach
that enables users to follow privacy-sensitive channels, while makes it diﬃcult
for the censors to discover the users’ actual interests [36]. However, these designs
are susceptible to inside attacks where censors pretend to be ordinary users to

Leveraging Internet Services to Evade Censorship
267
locate and block the infrastructure of censorship resilient systems [20]. Over the
years, researchers have designed better proxy distribution strategies that pro-
tect proxies from Sybil attacks [20,33]. Additionally, reputation systems might
be used to detect censors who have inﬁltrated in the proxies’ network [43]. These
strategies are adequate against individual users, but are insuﬃcient in thwarting
censors that rule a signiﬁcant amount of untrustworthy users.
One of the most eﬀective circumvention tools is Tor [16], which is a circuit-
based anonymous communication system that uses encryption to conceal the
network packets. More speciﬁcally, it interposes at least three relays between each
user and the website the user visits. The transferred packets through these relays
are encrypted, and each relay can decrypt only the necessary information that
leads the packet to the next relay. Although the relationship between the user and
the visited website through Tor is secure, repressive governments can block the
Tor itself [14]. To make Tor resilient to such attacks, developers have proposed
a centralized discovery service to disseminate a restricted set of relay identities
to requesting users [15]. Obfsproxy [32] is the ﬁrst Tor pluggable transport,
which adds an additional layer of encryption to Tor’s traﬃc to obfuscate its
identiﬁers. However, albeit the developers’ modiﬁcations, the problem is that
the traﬃc generated by Tor remains recognizable by its characteristic patterns
and content signatures.
Unobservable circumvention systems, on the other hand, instead of encrypt-
ing the web content and access it through proxies, imitate applications and blend
with the authorized Internet traﬃc. SkypeMorph [35] is a system designed to
encapsulate the Tor’s traﬃc into a connection that resembles Skype video traﬃc.
CensorSpoofer [44] is a framework for censorship-resistant web browsing that
exploits the asymmetric nature of web browsing traﬃc. StegoTorus [45] is a
tool that comprehensively disguises Tor from protocol analysis. Although these
approaches sound promising, Houmansadr et al. [25] demonstrate that these sys-
tems fail to achieve unobservability because they implement only partially the
imitating protocol and fall into discrepancies.
An alternative to unobservable circumvention systems by imitation is to run
the actual protocols and tunnel the hidden content inside their traﬃc. FOE [2]
and MailMyWeb [1] are two systems that can download a requested web-
site and send it as an email attachment to the requesting user. These systems
can evade censorship, however, the users cannot interact with the actual web-
site and they can only leverage these systems for accessing static websites.
SWEET [50] encapsulates a censored user’s traﬃc inside email messages. In
detail, the client tunnels its network traﬃc inside a series of email messages
that are changed between the client and an email server operated by SWEET’s
server. The server acts as an Internet proxy by forwarding the encapsulated
traﬃc to the requested blocked destinations. FreeWave [26] operates by tun-
neling Internet traﬃc inside non-blocked VoIP communications by modulating
them into acoustic signals that are carried over VoIP connections. These sys-
tems appear to work ﬂawless. Nevertheless, their main limitation is that they
only support one protocol and thus their overuse by users can trigger alerts on
censors.

268
A. Zarras
7
Conclusions
In this paper, we presented Camouflage, a novel approach that protects users
from Internet censorship. The key idea of Camouflage is a framework where
diﬀerent circumvention systems can be plugged in and help users to access an
uncensored Internet. The framework operates one layer below the circumvention
system and thus the design and the implementation of each system lies in the
hands of each developer. To demonstrate the feasibility of our approach, we built
a proof-of-concept prototype on widely-used applications and evaluate its per-
formance and security. We showed that diﬀerent systems can co-exist with each
other, while a central framework can synchronize them. Finally, we evaluated
Camouflage in countries that impose censorship. The outcomes of our experi-
ments revealed that by using Camouflage is possible to access an uncensored
Internet while at the same time the existence of our framework remained hidden
from the deployed censors.
Acknowledgments. The research was supported by the German Federal Ministry of
Education and Research under grant 16KIS0328 (IUNO).
References
1. MailMyWeb, June 2013. http://www.mailmyweb.com
2. The Foe Project, November 2013. https://code.google.com/p/foe-project
3. Berthold, O., Federrath, H., K¨opsell, S.: Web MIXes: a system for anonymous and
unobservable internet access. In: Federrath, H. (ed.) Designing Privacy Enhancing
Technologies. LNCS, vol. 2009, pp. 115–129. Springer, Heidelberg (2001)
4. Boyan, J.: The anonymizer - protecting user privacy on the web. Comput. Mediated
Commun. Mag. 4(9) (1997)
5. Burnett, S., Feamster, N., Vempala, S.: Chipping away at censorship ﬁrewalls with
user-generated content. In: USENIX Security Symposium (2010)
6. Castelluccia, C., De Cristofaro, E., Francillon, A., Kaafar, M.-A.: EphPub: toward
robust Ephemeral Publishing. In: IEEE International Conference on Network Pro-
tocols (2011)
7. Chaum,
D.L.:
Untraceable
electronic
mail,
return
addresses,
and
digital
pseudonyms. Commun. ACM 24(2), 84–90 (1981)
8. Chui, M., Manyika, J., Bughin, J., Dobbs, R., Roxburgh, C., Sarrazin, H., Wester-
gren, M.: The Social Economy: Unlocking Value and Productivity Through Social
Technologies. McKinsey, New York (2012)
9. Crandall, J.R., Zinn, D., Byrd, M., Barr, E.T., East, R.: ConceptDoppler: a weather
tracker for internet censorship. In: ACM Conference on Computer and Communi-
cations Security, CCS (2007)
10. Crispin, J.: The importance of VoIP and a business continuity plan for business
survival, June 2011. http://www.tech2date.com/the-importance-of-voip-and-a-
business-continuity-plan-for-business-survival.html
11. Dai, W.: Pipenet 1.1. Usenet post (1996)
12. Dainotti, A., Squarcella, C., Aben, E., Claﬀy, K.C., Chiesa, M., Russo, M., Pescap´e,
A.: Analysis of country-wide internet outages caused by censorship. In: ACM SIG-
COMM Conference on Internet Measurement, IMC (2011)

Leveraging Internet Services to Evade Censorship
269
13. Danezis, G., Dingledine, R., Mathewson, N.: Mixminion: design of a type III anony-
mous remailer protocol. In: IEEE Symposium on Security and Privacy (2003)
14. Dingledine, R.: Tor and circumvention: lessons learned. In: Rogaway, P. (ed.)
CRYPTO 2011. LNCS, vol. 6841, pp. 485–486. Springer, Heidelberg (2011)
15. Dingledine, R., Mathewson, N.: Design of a blocking-resistant anonymity system.
The Tor Project, Technical report, 11:15–16 (2006)
16. Dingledine, R., Mathewson, N., Syverson, P.: Tor: the second-generation onion
router. In: USENIX Security Symposium (2004)
17. Douglas, T.L.: The importance of VoIP, May 2010. http://ezinearticles.com/?
The-Importance-of-VoIP&id=4278231
18. Dynamic Internet Technology: Dynaweb, November 2013. http://www.dit-inc.us/
dynaweb
19. Feamster, N., Balazinska, M., Harfst, G., Balakrishnan, H., Karger, D.R.: Infranet:
circumventing web censorship and surveillance. In: USENIX Security Symposium
(2002)
20. Feamster, N., Balazinska, M., Wang, W., Balakrishnan, H., Karger, D.R.: Thwart-
ing web censorship with untrusted messenger discovery. In: Dingledine, R. (ed.)
PET 2003. LNCS, vol. 2760, pp. 125–140. Springer, Heidelberg (2003)
21. Geambasu, R., Kohno, T., Levy, A.A., Levy, H.M.: Vanish: increasing data privacy
with self-destructing data. In: USENIX Security Symposium (2009)
22. Geddes, J., Schuchard, M., Hopper, N.: Cover your ACKs: pitfalls of covert channel
censorship circumvention. In: ACM Conference on Computer and Communications
Security, CCS (2013)
23. Global Internet Freedom Consortium (GIFC): The great ﬁrewall revealed,
December
2002.
http://www.internetfreedom.org/ﬁles/WhitePaper/
ChinaGreatFirewallRevealed.pdf
24. Gulcu, C., Tsudik, G.: Mixing e-mail with Babel. In: ISOC Network and Distrib-
uted System Security Symposium, NDSS (1996)
25. Houmansadr, A., Brubaker, C., Shmatikov, V.: The parrot is dead: observing unob-
servable network communications. In: IEEE Symposium on Security and Privacy
(2013)
26. Houmansadr, A., Riedl, T., Borisov, N., Singer, A.: I want my voice to be heard: IP
over Voice-over-IP for unobservable censorship circumvention. In: ISOC Network
and Distributed System Security Symposium, NDSS (2013)
27. Leberknight, C.S., Chiang, M., Poor, H.V., Wong, F.: A taxonomy of internet cen-
sorship and anti-censorship, December 2012. http://www.princeton.edu/chiangm/
anticensorship.pdf
28. Leiba, B.: RFC 2177: IMAp. 4 IDLE Command, June 1997
29. Leskovec, J., Horvitz, E.: Planetary-scale views on a large instant-messaging net-
work. In: International Conference on World Wide Web, WWW (2008)
30. Levine, B.N., Shields, C.: Hordes: a multicast based protocol for anonymity. J.
Comput. Secur. 10(3), 213–240 (2002)
31. MacKinnon, R.: Race to the bottom-corporate complicity in Chinese internet cen-
sorship. In: Human Rights Watch, HRW (2009)
32. Mathewson, N.: The Tor project - a simple obfuscating proxy, November 2013.
https://gitweb.torproject.org/obfsproxy.git
33. McCoy, D., Morales, J.A., Levchenko, K.: Proximax: a measurement based system
for proxies dissemination. In: International Conference on Financial Cryptography
and Data Security, FC (2011)
34. McMillan, R.: China’s great ﬁrewall spreads overseas, March 2010. http://www.
networkworld.com/news/2010/032510-chinas-great-ﬁrewall-spreads.html

270
A. Zarras
35. Moghaddam, H.M., Li, B., Derakhshani, M., Goldberg, I.: SkypeMorph: protocol
obfuscation for Tor bridges. In: ACM Conference on Computer and Communica-
tions Security, CCS (2012)
36. Papadopoulos, P., Papadogiannakis, A., Polychronakis, M., Zarras, A., Holz,
T., Markatos, E.P.: K-subscription: privacy-preserving microblogging browsing
through obfuscation. In: Annual Computer Security Applications Conference,
ACSAC (2013)
37. Perlman, R.: The ephemerizer: making data disappear. J. Inf. Syst. Secur. (JISSec)
1, 51–68 (2005)
38. Serjantov, A., Sewell, P.: Passive attack analysis for connection-based anonymity
systems. In: Snekkenes, E., Gollmann, D. (eds.) ESORICS 2003. LNCS, vol. 2808,
pp. 116–131. Springer, Heidelberg (2003)
39. Sherwood, R., Bhattacharjee, B., Srinivasan, A.: P 5: a protocol for scalable anony-
mous communication. In: IEEE Symposium on Security and Privacy (2002)
40. Statista: Daily time spent playing video games per capita in the United States,
September
2014.
http://www.statista.com/statistics/186960/time-spent-with-
videogames-in-the-us-since-2002
41. Statistic Brain: Skype statistics, September 2012. http://www.statisticbrain.com/
skype-statistics
42. Ultrareach Internet Corporation: Ultrasurf, November 2013. http://www.ultrasurf.
us
43. Walsh, K., Sirer, E.G.: Experience with an object reputation system for peer-
to-peer ﬁlesharing. In: USENIX Symposium on Networked Systems Design and
Implementation, NSDI (2006)
44. Wang, Q., Gong, X., Nguyen, G.T., Houmansadr, A., Borisov, N.: CensorSpoofer:
asymmetric communication using IP spooﬁng for censorship-resistant web brows-
ing. In: ACM Conference on Computer and Communications Security, CCS (2012)
45. Weinberg, Z., Wang, J., Yegneswaran, V., Briesemeister, L., Cheung, S., Wang,
F., Boneh, D.: StegoTorus: a camouﬂage proxy for the Tor anonymity system. In:
ACM Conference on Computer and Communications Security, CCS (2012)
46. Wilkins, B.: 25 shocking facts about Chinese censorship, July 2009. http://www.
onlinecollege.org/2009/07/05/25-shocking-facts-about-chinese-censorship
47. Zander, S., Armitage, G., Branch, P.: Covert channels in multiplayer ﬁrst person
shooter online games. In: IEEE Conference on Local Computer Networks, LCN
(2008)
48. Zander, S., Armitage, G., Branch, P.: Reliable transmission over covert channels in
ﬁrst person shooter multiplayer games. In: IEEE Conference on Local Computer
Networks, LCN (2009)
49. Zarras, A., Kohls, K., D¨urmuth, M., P¨opper, C.: Neuralyzer: ﬂexible expiration
times for the revocation of online data. In: ACM Conference on Data and Appli-
cation Security and Privacy, CODASPY (2016)
50. Zhou, W., Houmansadr, A., Caesar, M., Borisov, N.: SWEET: serving the web by
exploiting email tunnels. In: Privacy Enhancing Technologies Symposium, PETS
(2013)
51. Zhu, T., Bronk, C., Wallach, D.S.: An analysis of Chinese search engine ﬁltering.
arXiv preprint arXiv:1107.3794 (2011)
52. Zhu, T., Phipps, D., Pridgen, A., Crandall, J.R., Wallach, D.S.: The velocity of
censorship: high-ﬁdelity detection of microblog post deletions. In: USENIX Security
Symposium (2013)

Analyzing Randomized Response Mechanisms
Under Diﬀerential Privacy
Atsushi Waseda(B) and Ryo Nojima
National Institute of Information and Communications Technology, Cybersecurity
Research Institute, 4-2-1, Nukui-Kitamachi, Koganei, Tokyo, Japan
{a-waseda,ryo-no}@nict.go.jp
Abstract. The randomized response technique was ﬁrst introduced by
Warner in 1965 [27] as a technique to survey sensitive questions. Since
it is considered to protect the respondent’s privacy, many variants and
applications have been proposed in the literature. Unfortunately, the
randomized response and its variants have not been well evaluated from
the privacy viewpoint historically. In this paper, we evaluate them by
using diﬀerential privacy. Speciﬁcally, we show that some variants have
a tradeoﬀbetween the privacy and utility, and that the “negative” survey
technique obtains negative results.
1
Introduction
The randomized response technique was ﬁrst introduced by Warner in 1965 [27]
as a technique to solve the following survey problem: estimating the ratio of peo-
ple in a population that has attribute A for example he/she is aged between 20
and 29. Intuitively, the randomized response technique works as follows: the ques-
tioner asks the responder Pi whether he has an attribute A, then Pi answers hon-
estly with some probability or otherwise answers randomly, and ﬁnally the ques-
tioner estimates the ratio by collecting all the answers from {P1, . . . , Pn}. Since
Warner developed it, many variants [1,3,4,12,15,21,23,24] and their applica-
tions [6,10,13,22] have been proposed. Due to the “randomized” (or the “noisy”)
response, the questioner cannot conﬁdently determine whether the responder
has A, which means the responder has some kind of deniability. With this prop-
erty, the randomized response technique is employed in many privacy preserving
applications, for instance location privacy [17,26], sensor network [16,18], and
data mining [7,19]. The weakness among the research related to the randomized
response is that how much privacy is leaked is not measured with the same mea-
surement, and hence the application designers cannot decide which is the best
from the privacy perspective. In this paper, we consider employing diﬀerential
privacy as the common measurement.
Diﬀerential privacy [8] is a quantitative notion of privacy that bounds how
much a single individual’s private data can contribute to a public output and is
employed in many privacy preserving applications [2,5,11,14,20]. In this notion,
intuitively, two neighboring databases D1, D2 are applied to some randomized
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 271–282, 2016.
DOI: 10.1007/978-3-319-45871-7 17

272
A. Waseda and R. Nojima
algorithm M, which is called a mechanism, and the diﬀerence between their
outputs M(D1) and M(D2) is measured. However, this setting is somewhat dif-
ferent from the randomized response. Let tA be a true answer of the respondent
and M(tA) be an actual noisy response. Then what we want to measure is pri-
vacy loss of tAi rather than that of the database Di. Hence, we regard tAi as
a singleton set, i.e., Di = {tAi}, and analyze the privacy with the diﬀerential
privacy. This is not the ﬁrst time for kind of analysis [9]. However, Dwork and
Roth only evaluated very simpliﬁed and tiny randomized response.
In this paper, we evaluate the randomized response techniques and those
applications with the diﬀerential privacy. First, we evaluate the simpliﬁed ran-
domized response technique and show a useful lemma (Lemma 1). By using
this lemma, we evaluate several well studied randomized response techniques,
Warner’s mechanism [27], Kuk’s mechanism [21], and negative survey [12] and
its variants [3]. We show that the randomized response technique has a trade-
oﬀbetween the privacy and utility. Finally, the applications of the randomized
response are evaluated. Speciﬁcally, we evaluate the location privacy proposed by
Quercia et al. [26] and private collaborative recommendation algorithm proposed
by Polat and Du [25].
This paper is organized as follows. In Sect. 2, we introduce the overview of
diﬀerential privacy and the randomized response techniques. In Sect. 3, we eval-
uate the randomized response techniques under diﬀerential privacy. In Sect. 4,
the variants of several randomized response and those applications are evaluated.
Finally, we conclude this paper in Sect. 5.
2
Background
2.1
Diﬀerential Privacy
Diﬀerential privacy [8] is a quantitative notion of privacy that bounds how much
a single individual’s private data discloses. The standard setting involves a data-
base of private information and a mechanism that computes an output given in
the database. Formally, a database D is a multiset of records belonging to some
data universe X, where a record corresponds to one individual’s private data.
We say that the two databases are neighbors if they are identical except for a
single record. A mechanism M is a randomized function that takes the database
as input and outputs an element of the range R.
Deﬁnition 1 ([8]). Given ϵ ≥0, a mechanism M is ϵ-diﬀerentially private
if, for any two neighboring databases D and D′ and for any subset S ⊆R of
outputs,
Pr[M(D) ∈S] ≤exp(ϵ) · Pr[M(D′) ∈S].
(1)
If S is a countable set, then we can modify the inequation (1) as
Pr[M(D) = s] ≤exp(ϵ) · Pr[M(D′) = s],
(2)
where we consider S as a singleton set. We use the inequation (2) in this paper
and assume that R = X.

Analyzing Randomized Response Mechanisms Under Diﬀerential Privacy
273
2.2
Randomized Response Mechanisms
The randomized response mechanism was ﬁrst introduced by Warner [27] in
1965 to solve the following survey problem: to estimate the ratio of people in
a population that has attribute A. That is, the randomized response mech-
anism has two types of participants, the questioner Q and the respondents
P = {P1, P2, · · · , Pn}, and Q wants to estimate
|{Pi ∈A | Pi ∈P}|.
To solve this, the mechanism works as follows:
– Q sends (the description of) A0 := A, and A1 := P \ A0.
– Then, Pi ﬂips a coin b such that Pr[b = 0] = p and Pr[b = 1] = 1 −p, and
returns 1 if Pi ∈Ab or 0 otherwise.
Since Q does not know b, he/she cannot decide whether Pi ∈A or not. After
the proposal of the above mechanism, many extensions have been studied. For
example, Abul-Ela et al. [1] considered the estimation of
|{Pi ∈A0 | Pi ∈P}|, . . . , |{Pi ∈At | Pi ∈P}|.
To analyze the diﬀerential privacy of the various randomized mechanisms,
we provide a simple deﬁnition. The randomized response mechanism consists of
the following algorithms (S, Res, Eval), where
– S is a randomized algorithm that generates a question q.
– Res is a randomized algorithm that takes q and true answer tA as input and
output the noisy answer nA. We often omit q and denote Res(tA) instead of
Res(q, tA).
– Eval(nA1, . . . , nAn) is a randomized algorithm that takes noisy answers
nA1, . . . , nAn as input and outputs the estimation, for example |{Pi ∈A |
Pi ∈P}|.
We formalize the privacy requirement of the randomized response mechanism
as follows:
Deﬁnition 2. Given ϵ
≥
0, a randomized response mechanism M
=
(S, Res, Eval) is ϵ-diﬀerentially private if, for any two answers tA0 ∈X and
tA1 ∈X, and for any element s ∈X,
Pr[Res(tA0) = s] ≤exp(ϵ) · Pr[Res(tA1) = s].
(3)
If there exists a distance function d for every two answers tA0 ∈X and tAi ∈X,
then we can modify Eq. (3) as
Pr[Res(tA0) = s] ≤exp(d(tA0, tA1) · ϵ) · Pr[Res(tA1) = s],
which is similar to Andr´es et al. [2]. We can consider d as Euclid distance,
Manhattan distance, etc.

274
A. Waseda and R. Nojima
3
Analyzing the Randomized Response Mechanisms
We begin with the simpliﬁed variance that Warner proposed [27]. Let us consider
two randomized functions Rand0 and Rand1 such that
– Rand0 outputs 0 with probability p0 and 1 with p1 = 1 −p0; and
– Rand1(x) is a randomized algorithm whose domain is X such that for every
x ∈X, there exists i ∈X satisfying
Pr[Rand1(x) = i] ≥0,
and 
i∈X px,i = 1, where Pr[Rand1(x) = i] = px,i.
The algorithm Res(tA) works as follows:
1. Run Rand0 to obtain b0 ∈{0, 1}
2. If b0 = 0 then set nA = tA, or otherwise run Rand1 to obtain b1 and set
nA = b1.
3. Output nA.
The case
p0 = p1 = 1/2, X = {0, 1}, p0,0 = p0,1 = p1,0 = p1,1 = 1/2,
(4)
was evaluated by Dwork, which results in ln 3-diﬀerential privacy. The more
general case is proven as follows:
Lemma 1 (Key Lemma). If for any tA, s ∈X, Pr[Res(tA) = s] > 0, then the
simpliﬁed randomized response mechanism has ϵ-diﬀerential privacy, where
ϵ = ln max
1 + p1(ptA0,tA0 −1)
p1ptA1,tA0
, p1ptA0,s
p1ptA1,s
,
p1ptA0,tA1
1 + p1(ptA1,tA1 −1)

,
(5)
and the maximum is taken over tA0 ∈X, tA1 ∈X \ {tA0}, s ∈X \ {tA0, tA1}.
Proof. Let us denote the respondent’s noisy output as nA ∈X and true answer
as tA ∈X. Modifying inequality (3), we have
maxtA0,tA1∈X,tA0̸=tA1,s∈X
Pr[Res(tA0) = s]
Pr[Res(tA1) = s]

≤exp(ϵ),
(6)
where the randomness is taken over the choice of Res, and what we want to
estimate is ϵ. To do so, we consider three cases:
Case 1: s = tA0,
Case 2: s = tA1,
Case 3: s ̸= tA0, s ̸= tA1.

Analyzing Randomized Response Mechanisms Under Diﬀerential Privacy
275
Here, the probability of Res(tA) producing nA = tA is
Pr[nA = tA] = p0 + p1ptA,tA = 1 + p1(ptA,tA −1),
and for every nA ∈X such that tA ̸= nA,
Pr[nA ̸= tA] = p1ptA,nA.
Hence, for Case 1,
Pr[Res(tA0) = tA0)]
Pr[Res(tA1) = tA0)] = 1 + p1(ptA0,tA0 −1)
p1ptA1,tA0
.
Case 2 is similar to Case 1. Finally, for Case 3,
Pr[Res(tA0) = s]
Pr[Res(tA1) = s] = p1ptA0,s
p1ptA1,s
.
Putting these together with (6) and taking the logarithm we have proved the
lemma.
⊓⊔
Applying the case of (4), where Dwork analyzed, in the above lemma, we
obtain
ϵ = ln max
1 + 1/2(1/2 −1)
1/2 · 1/2
, 1/2
1/2,
1/2 · 1/2
1 + 1/2(1/2 −1)

= ln max {3, 1, 1/3}
= ln 3.
Hence, the result contains Dwork’s analysis.
4
Privacy Analysis
4.1
Analyzing the Variants
Warner’s Original Mechanism [27]: The aim is to estimate the proportion
πA of people who have some attribute A. More formally, each person has some
attribute A under Bernoulli distribution with πA. The questioner estimates
πA. However, if A represent some sensitive attribute, the responder has a high
possibility of responding untruthfully. Thus, Warner proposed the randomized
response technique below.
The questioner prepares red cards, non-red cards and the box, where the ratio
of red cards among all the cards is q with 0 < q < 1 and q ̸= 1
2. The questioner
puts those cards in the box and sends it to respondent P. Respondent P draws
one card from the box. If the card is “red”, then P truthfully replies “True” or
“False” to the question “I am a member of A,” or otherwise truthfully replies to

276
A. Waseda and R. Nojima
the question “I am not a member of A”. Let ˆT be the proportion to which the
respondents reply “True.” It is easy to see that the expectation of ˆT is
E( ˆT) = qπA + (1 −q)(1 −πA).
Then, the estimation of πA is calculated as
π′
A =
ˆT −(1 −q)
2q −1
,
with sampling variance
V (π′
A) = E( ˆT)(1 −E( ˆT))
n(2q −1)2
= πA(1 −πA)
n
+
q(1 −q)
n(2q −1)2 ,
where n is the number of respondents. This variance will be small when q
approaches 0 or 1. We can regard this mechanism as a special case of the sim-
pliﬁed randomized response by setting
– |X| = 2,
– (the deﬁnition of Rand0) p0 = q,
– (the deﬁnition of Rand1)
ptA,x =

0 if tA = x,
1 otherwise.
By applying Lemma 1, we obtain the following result.
Lemma 2. The randomized response proposed by Warner [27] satisﬁes the ϵW -
diﬀerential privacy, where
ϵW = max

ln 1 −q
q
, ln
q
1 −q

.
This ϵW is small when q approaches 1/2. It is desirable that ϵW and sampling
variance are small. Hence, q must be chosen carefully since it controls V (π′
A)
and ϵW .
Kuk’s Mechanism [21]: Kuk’s proposed another kind of randomized response
mechanism to estimate the proportion πA, the same as Warner’s mechanism. The
questioner prepares two boxes, Box1 and Box2. These boxes contain red cards
and non-red cards. The ratio of the red cards among all the cards in Boxi is qi,
where 0 < q1, q2 < 1, q1 ̸= q2. The questioner sends these boxes to respondent
P. Then, respondent P takes one card from each box.
– If P is member of A, then P replies “red card” or “non-red card” in accordance
with the card taken from Box1
– Otherwise, he does the same as above except that he takes a card from Box2.

Analyzing Randomized Response Mechanisms Under Diﬀerential Privacy
277
Let ˆR be the proportion to which the respondents say “red card.” It is easy
to see that the expectation of ˆR is
E( ˆR) = q1πA + q2(1 −πA).
Then, the estimation of πA is calculated as
π′
A =
ˆR −q2
q1 −q2
,
with sampling variance,
V (π′
A) = E( ˆR)(1 −E( ˆR))
n(q1 −q2)2
= ((−q1 + q2)πA + (1 −q1))((q1 −q2)πA + q1)
n(q1 −q2)2
.
We can also regard Kuk’s mechanism as a special case of the simpliﬁed random-
ized response by setting
– f : {“True”, “False”} 	→{1, 2},
– |X| = 2,
– (the deﬁnition of Rand0) p0 = 0,
– (the deﬁnition of Rand1)
ptA,x =

qf(tA) if x = red card,
1 −qf(tA) otherwise.
By applying Lemma 1, we obtain the following result.
Lemma 3. The randomized response proposed by Kuk [21] satisﬁes the ϵK-
diﬀerential privacy, where
ϵK = ln max
q1
q2
, q2
q1
, 1 −q1
1 −q2
, 1 −q2
1 −q1

.
V (π′
A) is small when |q1 −q2| is large, but ϵK becomes large.
Thus, Warner’s mechanism and Kuk’s mechanism have a tradeoﬀbetween
privacy ϵ and utility V (π′
A).
Negative Survey Mechanism [12]: A negative survey mechanism is a special
case of the randomized response mechanism. In this mechanism, the respondent
always gives a non-true answer, i.e., always answering nA ̸= tA. For example,
if X = {a0, a1, a2, a3} and tA = a0, then nA = ai such that 1 ≤i ≤3 and
Pr[nA = ai] = 1/3 for all i. Since the respondent does not answer truthfully, the
questioner never receives the true answer. By obtaining the noisy answers in this
way, the number of people X(i) whose true answer is ai can be estimated as
X(i) = n −(α −1)Y (i),

278
A. Waseda and R. Nojima
where n is the number of respondents and α is the number of choices, i.e. α = |X|.
Y (i) is the number of people whose response nA is ai. We consider the following
setting
– |X| = α,
– (the deﬁnition of Rand0) p0 = 0, p1 = 1,
– (the deﬁnition of Rand1)
ptA,x =

0 if tA = x,
1
α−1 otherwise.
Considering the case s = tA1 in inequation (3), to have the ϵ-diﬀerential privacy,
Pr[Res(tA0) = tA1] ≤exp(ϵ) Pr[Res(tA1) = tA1].
must be satisﬁed. However,
Pr[Res(tA0) = tA1] =
1
α −1 > exp(ϵ) Pr[Res(tA1) = tA1] = 0,
for any ﬁnite ϵ. Hence we can conclude as follows:
Lemma 4. The negative survey mechanism does not satisfy ϵ-diﬀerential pri-
vacy for any ﬁnite ϵ.
Intuitively, if nA is given to the questioner, then he/she can know that the respon-
dent’s true answer is not nA. This makes the mechanism very weak.
Limited Negative Survey Mechanism [3]: Aoki et al., proposed the limited
negative survey mechanism as an extension of the negative survey [3]. In limited
negative survey mechanism, the privacy information is protected as follows. Let
tA = (x1, x2, . . . , xD) be original data, where each xi is chosen from Xi, i.e.,
tA ∈X = X1 × · · · × XD. Then the noisy data nA = (x′
1, x′
2, . . . , x′
D) satisﬁes the
following:
∀i : x′
i < xi −βi or xi + βi < x′
i.
for some positive βi. For the same reason as the negative survey mechanism, we
can conclude the following:
Lemma 5. The limited negative survey mechanism does not satisfy ϵ-diﬀerential
privacy with ﬁnite ϵ.
t-times Negative Survey: Aoki and Sezaki [4] considered using the negative
survey twice to make the time complexity of the estimation small. The privacy
analysis is shown in the following theorem. Note that the analysis is given in a
more general form:

Analyzing Randomized Response Mechanisms Under Diﬀerential Privacy
279
Theorem 1. Let |X| = α and the respondent makes the response by run-
ning the negative survey mechanism t-times. Then, the mechanism satisﬁes the
ϵ-diﬀerential privacy, where
ϵ =

ln (1−α)((1−α)t−1)−1
(1−α)t−1)
if t is even,
ln
(1−α)t−1
(1−α)((1−α)t−1−1) if t is odd.
Proof. Let st be the response of t times negative survey mechanism. We construct
the following recurrence relation:
⎧
⎪
⎪
⎨
⎪
⎪
⎩
Pr[s0 = tA0] = 1,
Pr[s0 ̸= tA0] = 0,
Pr[st = tA0] = Pr[st−1 = tA0] × 0 + (α −1) × Pr[st−1 ̸= tA0] ×
1
α−1,
Pr[st ̸= tA0] = Pr[st−1 = tA0] ×
1
α−1 + (α −2) × Pr[st−1 ̸= tA0] ×
1
α−1.
Solving this recurrence relation, we obtain the following
Pr[st = tA0] = 1
α

1 −
1
(1 −α)t−1

,
Pr[st ̸= tA0] = 1
α

1 −
1
(1 −α)t

.
By applying Lemma 1, we obtain this theorem.
⊓⊔
Note that ϵ becomes closer to 0 as t tends to ∞.
4.2
Analyzing Applications
We analyze the application to which the randomized response mechanism was
applied.
Location Obfuscation Algorithm SpotMe [26]: SpotMe, proposed by Quer-
cia et al., is a mechanism for aggregating user locations in real-time. They real-
ized this mechanism by a randomized response. Furthermore, they implemented
SpotMe on mobile phones and conducted experiments in Zurich and London.
As a result, they reported that SpotMe works with reasonable communication,
computational time and storage overheads.
Let k be the number of locations on a map. Then, the main part of SpotMe
works as follows:
– with probability p, the mobile phone chooses the location k uniformly at ran-
dom, and
– with probability 1 −p, it chooses the true location.
Thus, this is
– |X| = k,
– The deﬁnition of Rand0 is p0 = 1 −p.

280
A. Waseda and R. Nojima
– The deﬁnition of Rand1 is
ptA,l = 1
k ,
in the simpliﬁed randomized response mechanism.
By applying Lemma 1, we conclude that SpotMe [26] satisﬁes the ϵ-diﬀerential
privacy, where
ϵ = ln k −(k −1)p
p
.
For example, if p = 0.5 and k = 100 [26], SpotMe satisﬁes the ln 101-diﬀerential
privacy.
Private Collaborative Recommendation [25]: Polat and Du proposed a
private collaborative recommendation mechanism. In this mechanism, a user
poses a rating tA ∈{0, 1} of some item and submits it to the system privately.
To do so,
– with probability 1 −q, the user submits a randomly chosen rating in {0, 1},
and
– with probability q, it chooses the true rating.
Since this is exactly the same as Warner’s mechanism, we can conclude that
private collaborative recommendation proposed by Polat and Du [25] satisﬁes
the ϵ-diﬀerential privacy, where
ϵ = max

ln 1 −q
q
, ln
q
1 −q

.
5
Concluding Remarks
In this paper, we evaluate the randomized response techniques by using dif-
ferential privacy. First, we presented a useful lemma (Lemma 1) evaluating the
simpliﬁed randomized response technique. By using this lemma, we evaluated
several well studied randomized response techniques and their applications. We
obtained results in which neither ϵ nor sampling variance could be improved. We
hope that our results help application designers to choose suitable parameters
in the randomized response.
References
1. Abul-Ela, A.L.A., Greenberg, G.G., Horvitz, D.G.: A multi-proportions random-
ized response model. J. Am. Stat. Assoc. 62(319), 990–1008 (1967)
2. Andr´es, M.E., Bordenabe, N.E., Chatzikokolakis, K., Palamidessi, C.: Geo-
indistinguishability: diﬀerential privacy for location-based systems. In: Proceedings
of the 2013 ACM SIGSAC Conference on Computer and Communications Security,
pp. 901–914. ACM (2013)

Analyzing Randomized Response Mechanisms Under Diﬀerential Privacy
281
3. Aoki, S., Iwai, M., Sezaki, K.: Limited negative surveys: privacy-preserving partic-
ipatory sensing. In: 2012 IEEE 1st International Conference on Cloud Networking
(CLOUDNET), pp. 158–160. IEEE (2012)
4. Aoki, S., Sezaki, K.: Negative surveys with randomized response techniques for
privacy-aware participatory sensing. IEICE Trans. Commun. 97(4), 721–729 (2014)
5. Dankar, F.K., El Emam, K.: The application of diﬀerential privacy to health data.
In: Proceedings of the 2012 Joint EDBT/ICDT Workshops, pp. 158–166. ACM
(2012)
6. Dietz, P., Striegel, H., Franke, A.G., Lieb, K., Simon, P., Ulrich, R.: Randomized
response estimates for the 12-month prevalence of cognitive-enhancing drug use in
university students. Pharmacother. J. Hum. Pharmacol. Drug Ther. 33(1), 44–50
(2013)
7. Du, W., Zhan, Z.: Using randomized response techniques for privacy-preserving
data mining. In: Proceedings of the ninth ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pp. 505–510. ACM (2003)
8. Dwork, C.: Diﬀerential privacy. In: Bugliesi, M., Preneel, B., Sassone, V., Wegener,
I. (eds.) ICALP 2006. LNCS, vol. 4052, pp. 1–12. Springer, Heidelberg (2006)
9. Dwork, C., Roth, A.: The algorithmic foundations of diﬀerential privacy. Found.
Trends Theor. Comput. Sci. 9(3–4), 211–407 (2014)
10. Eichhorn, B.H., Hayre, L.S.: Scrambled randomized response methods for obtaining
sensitive quantitative data. J. Stat. Plan. Infer. 7(4), 307–316 (1983)
11. Erlingsson, ´U., Pihur, V., Korolova, A.: RAPPOR: randomized aggregatable
privacy-preserving ordinal response. In: Proceedings of the 2014 ACM SIGSAC
Conference on Computer and Communications Security, pp. 1054–1067. ACM
(2014)
12. Esponda, F.: Negative surveys. arXiv preprint math/0608176 (2006)
13. Fidler, D.S., Kleinknecht, R.E.: Randomized response versus direct questioning:
two data-collection methods for sensitive information. Psychol. Bull. 84(5), 1045
(1977)
14. Friedman, A., Schuster, A.: Data mining with diﬀerential privacy. In: Proceedings
of the 16th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining, pp. 493–502. ACM (2010)
15. Greenberg, B.G., Abul-Ela, A.L.A., Simmons, W.R., Horvitz, D.G.: The unrelated
question randomized response model: theoretical framework. J. Am. Stat. Assoc.
64(326), 520–539 (1969)
16. Groat, M.M., Edwards, B., Horey, J., He, W., Forrest, S.: Enhancing privacy in par-
ticipatory sensing applications with multidimensional data. In: 2012 IEEE Inter-
national Conference on Pervasive Computing and Communications (PerCom), pp.
144–152. IEEE (2012)
17. Horey, J., Forrest, S., Groat, M.: Reconstructing spatial distributions from
anonymized locations. In: 2012 IEEE 28th International Conference on Data Engi-
neering Workshops (ICDEW), pp. 243–250. IEEE (2012)
18. Horey, J., Groat, M.M., Forrest, S., Esponda, F.: Anonymous data collection in
sensor networks. In: Fourth Annual International Conference on Mobile and Ubiq-
uitous Systems: Networking and Services, MobiQuitous 2007, pp. 1–8. IEEE (2007)
19. Huang, Z., Du, W.: OptRR: optimizing randomized response schemes for privacy-
preserving data mining. In: IEEE 24th International Conference on Data Engi-
neering, ICDE 2008, pp. 705–714. IEEE (2008)
20. Inan, A., Kantarcioglu, M., Ghinita, G., Bertino, E.: Private record matching
using diﬀerential privacy. In: Proceedings of the 13th International Conference
on Extending Database Technology, pp. 123–134. ACM (2010)

282
A. Waseda and R. Nojima
21. Kuk, A.Y.: Asking sensitive questions indirectly. Biometrika 77(2), 436–438 (1990)
22. Lara, D., Garc´ıa, S.G., Ellertson, C., Camlin, C., Su´arez, J.: The measure of
induced abortion levels in Mexico using random response technique. Sociol. Meth.
Res. 35(2), 279–301 (2006)
23. Mangat, N.S.: An improved randomized response strategy. J. R. Stat. Soc. Ser. B
(Methodol.) 56(1), 93–95 (1994)
24. Mangat, N., Singh, R.: An alternative randomized response procedure. Biometrika
77(2), 439–442 (1990)
25. Polat, H., Du, W.: Achieving private recommendations using randomized response
techniques. In: Ng, W.-K., Kitsuregawa, M., Li, J., Chang, K. (eds.) PAKDD 2006.
LNCS (LNAI), vol. 3918, pp. 637–646. Springer, Heidelberg (2006)
26. Quercia, D., Leontiadis, I., McNamara, L., Mascolo, C., Crowcroft, J.: SpotME if
you can: randomized responses for location obfuscation on mobile phones. In: 2011
31st International Conference on Distributed Computing Systems (ICDCS), pp.
363–372. IEEE (2011)
27. Warner, S.L.: Randomized response: a survey technique for eliminating evasive
answer bias. J. Am. Stat. Assoc. 60(309), 63–69 (1965)

Models and Algorithms for Graph Watermarking
David Eppstein1, Michael T. Goodrich1, Jenny Lam2(B), Nil Mamano1,
Michael Mitzenmacher3, and Manuel Torres1
1 Department of Computer Science, University of California, Irvine, CA, USA
2 Department of Computer Science, San Jos´e State University, San Jos´e, CA, USA
jenny.lam@sjsu.edu
3 Department of Computer Science, Harvard University, Cambridge, MA, USA
Abstract. We introduce models and algorithmic foundations for graph
watermarking. Our approach is based on characterizing the feasibility
of graph watermarking in terms of keygen, marking, and identiﬁca-
tion functions deﬁned over graph families with known distributions. We
demonstrate the strength of this approach with exemplary watermark-
ing schemes for two random graph models, the classic Erd˝os-R´enyi model
and a random power-law graph model, both of which are used to model
real-world networks.
1
Introduction
In the classic media watermarking problem, we are given a digital representa-
tion, R, for some media object, O, such as a piece of music, a video, or an image,
such that there is a rich space, R, of possible representations for O besides R
that are all more-or-less equivalent. Informally, a digital watermarking scheme
for O is a function that maps R and a reasonably short random message, m, to
an alternative representation, R′, for O in R. The veriﬁcation of such a marking
scheme takes R and a presumably-marked representation, R′′ (which was possi-
bly altered by an adversary), along with the set of messages previously used for
marking, and it either identiﬁes the message from this set that was assigned to
R′′ or it indicates a failure. Ideally, it should diﬃcult for an adversary to trans-
form a representation, R′ (which he was given), into another representation R′′
in R, that causes the identiﬁcation function to fail. Some example applications of
such digital watermarking schemes include steganographic communication and
marking digital works for copyright protection.
With respect to digital representations of media objects that are intended to
be rendered for human performances, such as music, videos, and images, there
is a well-established literature on digital watermarking schemes and even well-
developed models for such schemes (e.g., see Hopper et al. [8]). Typically, such
watermarking schemes take advantage of the fact that rendered works have many
possible representations with almost imperceptibly diﬀerent renderings from the
perspective of a human viewer or listener.
In this paper, we are inspired by recent systems work on graph watermarking
by Zhao et al. [18], who propose a digital watermarking scheme for graphs, such
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 283–301, 2016.
DOI: 10.1007/978-3-319-45871-7 18

284
D. Eppstein et al.
as social networks, protein-interaction graphs, etc., which are to be used for com-
mercial, entertainment, or scientiﬁc purposes. This work by Zhao et al. presents
a system and experimental results for their particular method for performing
graph watermarking, but it is lacking in formal security and algorithmic founda-
tions. For example, Zhao et al. do not provide formal proofs for circumstances
under which graph watermarking is undetectable or when it is computationally
feasible. Thus, as complementary work to the systems results of Zhao et al.,
we are interested in the present paper in providing models and algorithms for
graph watermarking, in the spirit of the watermarking model provided by Hopper
et al. [8] for media ﬁles. In particular, we are interested in providing a framework
for identifying when graph watermarking is secure and computationally feasible.
Additional Related Work. Under the term “graph watermarking,” there is
some additional work, although it is not actually for the problem of graph water-
marking as we are deﬁning it. For instance, there is a line of research involving
software watermarking using graph-theoretic concepts and encodings. In this
case, the object being marked is a piece of software and the goal of a “graph
watermarking” scheme is to create a graph, G, from a message, m, and then
embed G into the control ﬂow of a piece of software, S, to mark S. Examples of
such work include pioneering work by Collberg and Thomborson [6], as well as
subsequent work by Venkatesan, Vazirani, and Sinha [16] and Collberg et al. [5].
This work on software watermarking diﬀers from the graph watermarking prob-
lem we study in the present paper, however, because in the graph watermarking
problem we study an input graph is provided and we want to alter it to add
a mark. In the graph-based software watermarking problem, a graph is instead
created from a message to have a speciﬁc, known structure, such as being a per-
mutation graph, and then that graph is embedded into the control ﬂow of the
piece of software.
A line of research that is more related to the graph watermarking problem
we study is anonymization and de-anonymization for social networks. One of the
closest examples of such prior work is by Backstrom, Dwork, and Kleinberg [1],
who show how to introduce a small set of “rogue” vertices into a social network
and connect them to each other and to other vertices so that if that same network
is approximately replicated in another setting it is easy to match the two copies.
Such work diﬀers from graph watermarking, however, because the set of rogue
vertices are designed to “stand out” from the rest of the graph rather than
“blend in,” and it may in some cases be relatively easy for an adversary to
identify and remove such rogue vertices. In addition to this work, also of note is
work by Narayanan and Shmatikov [13], who study the problem of approximately
matching two social networks without marking, as well as the work on Khanna
and Zane [9] for watermarking road networks by perterbing vertex positions
(which is a marking method outside the scope of our approach).
Our Results. In this paper, we introduce a general graph watermarking frame-
work that is based on the use of key generation, marking, and identiﬁcation func-

Models and Algorithms for Graph Watermarking
285
tions, as well as a hypothetical watermarking security experiment (which would
be performed by an adversary). We deﬁne these functions in terms of graphs
taken over random families of graphs, which allows us to quantify situations in
which graph watermarking is provably feasible.
We also provide some graph watermarking schemes as examples of our frame-
work, deﬁned in terms of the classic Erd˝os-R´enyi random-graph model and a
random power-law graph model. Our schemes extend and build upon previ-
ous results on graph isomorphism for these graph families, which may be of
independent interest. In particular, we design simple marking schemes for these
random graph families based on simple edge-ﬂipping strategies involving high-
and medium-degree vertices. Analyzing the correctness of our schemes is quite
nontrivial, however, and our analysis and proofs involve intricate probabilistic
arguments, some of which we include in the ePrint version of this paper [7]. We
provide an analysis of our scheme against adversaries that can themselves ﬂip
edges in order to defeat our mark identiﬁcation algorithms. In addition, we pro-
vide experimental validation of our algorithms, showing that our edge-ﬂipping
scheme can succeed for a graph without speciﬁc knowledge of the parameters of
its deriving graph family.
2
Our Watermarking Framework
Suppose we are given an undirected graph, G = (V, E), that we wish to mark. To
deﬁne the security of a watermarking scheme for G, G must come from a family
of graphs with some degree of entropy [19]. We formalize this by assuming a
probability distribution D over the family G of graphs from which G is taken.
Deﬁnition 1. A graph watermarking scheme is a tuple (keygen, mark, identify)
over a set, G, of graphs where
– keygen : N × N →Aux is a private key generation function, such that
keygen(ℓ, n) is a list of ℓ(pseudo-)random graph elements, such as vertices
and/or vertex pairs, deﬁned over a graph of n vertices. These candidate loca-
tions for marking are deﬁned independent of a speciﬁc graph; that is, ver-
tices in Aux are identiﬁed simply by the numbering from 1 to n. For example,
keygen(ℓ, n) could be a small random graph, H, and some random edges to
connect H to a larger input graph [19], or keygen(ℓ, n) could be a set of vertex
pairs in an input graph that form candidate locations for marking.
– mark : Aux × G →N × G takes a private key z generated by keygen, and
a speciﬁc graph G from G, and returns a pair, S = (id, H), such that id is
a unique identiﬁer for H and H is the graph obtained by adding the mark
determined by id to G in the location determined by the private key z. mark is
called every time a diﬀerent marked copy needs to be produced, with the i-th
copy being denoted by Si = (idi, Hi). Therefore, the unique identiﬁers should
be thought of as being generated randomly. To associate a marked graph Hi
with the user who receives it, the watermarking scheme can be augmented with
a table storing user name and unique identiﬁers. Alternatively, the identiﬁers

286
D. Eppstein et al.
can be generated pseudo-randomly as a hash of a private key provided by the
user.
– identify : Aux × G × Nk × G →N ∪{⊥} takes a private key from Aux, the
original graph, G, k identiﬁers of previously-marked copies of G, and a test
graph, G′, and it returns the identiﬁer, idi, of the watermarked graph that it
is identifying as a match for G′. It may also return ⊥, as an indication of
failure, if it does not identify any of the graphs Hi as a match for G′.
In addition, in order for a watermarking scheme to be eﬀective, we require that
with high probability1 over the graphs from G and k output pairs, S1, . . . , Sk of
mark(z, G), for any (id, G′) = Si, we have identify(z, G, id1, . . . , idk, G′) = id.
Algorithm 1 shows a hypothetical security experiment for a watermarking
scheme with respect to an adversary, A : G →G, who is trying to defeat the
scheme. Intuitively, in the hypothetical experiment, we generate a key z, choose
a graph G, from family G according to distribution D (as discussed above),
and then generate k marked graphs according to our scheme (for some set of k
messages). Next, we randomly choose one of the marked graphs, G′, and commu-
nicate it to an adversary. The adversary then outputs a graph GA that is similar
to G′ where his goal is to cause our identiﬁcation algorithm to fail on GA.
Algorithm 1. Hypothetical Watermarking Security Experiment
experiment(A, k, ℓ, n):
1. z ←keygen(ℓ, n)
2. G ←D G
3. Si ←mark(z, G), for i = 1, . . . , k
4. randomly choose Si = (id, G′) from {S1, . . . , Sk}
5. GA ←A(G′)
In order to characterize diﬀerences between graphs, we assume a similarity
measure dist : G × G →R, deﬁning the distance between graphs in family G.
We also include a similarity threshold θ, that deﬁnes the advantage of an adver-
sary performing the experiment in Algorithm 1. Speciﬁcally, the advantage of an
adversary, A : G →G who is trying to defeat our watermarking scheme is
P [dist(G, GA) < θ and identify(z, G, id1, . . . , idk, GA) ̸= id] .
The watermarking scheme is (D, dist, θ, k, ℓ)-secure against adversary A if the
similarity threshold is θ and A’s advantage is polynomially negligible (i.e., is
O(n−a) for some a > 0).
Examples of adversaries could include the following:
– Arbitrary edge-ﬂipping adversary: a malicious adversary who can arbitrarily
ﬂip edges in the graph. That is, the adversary adds an edge if it is not already
there, and removes it otherwise.
1 Or “whp,” that is, with probability at least 1 −O(n−a), for some a > 0.

Models and Algorithms for Graph Watermarking
287
– Random edge-ﬂipping adversary: an adversary who independently ﬂips each
edge with a given probability.
– Arbitrary adversary: a malicious adversary who can arbitrarily add and/or
remove vertices and ﬂip edges in the graph.
– Random adversary: an adversary who independently adds and/or removes
vertices with a given probability and independently ﬂips each edge with a
given probability.
Random Graph Models. As deﬁned above, a graph watermarking scheme
requires that graphs to be marked come from some distribution. In this paper,
we consider two families of random graphs—the classic Erd˝os-R´enyi model and
a random power-law graph model—which should capture large classes of appli-
cations where graph watermarking would be of interest.
Deﬁnition 2 (The Erd˝os-R´enyi model). A random graph G(n, p) is a graph
with n vertices, where each of the
n
2

possible edges appears in the graph inde-
pendently with probability p.
Deﬁnition 3 (The random power-law graph model, Sect. 5.3 of [4]).
Given a sequence w = (w1, w2, . . . , wn), such that maxi w2
i < 
k wk, the general
random graph G(w) is deﬁned by labeling the vertices 1 through n and choosing
each edge (i, j) independently from the others with probability p[i, j] = ρwiwj,
where ρ = 1/ 
j wj.
We deﬁne a random power-law graph G(wγ) parameterized by the maximum
degree m and average degree w. Let wi = ci−1/(γ−1) for values of i in the range
between i0 and i0 + n, where
c = γ −2
γ −1wn
1
γ−1 ,
i0 = n
 w(γ −2)
m(γ −1)
γ−1
.
(1)
This deﬁnition implies that each edge (i, j) appears with probability
P[i, j] = K0

nγ−3ij
−
1
γ−1 ,
where K0
def
=
γ −2
γ −1
2
w.
(2)
Graph Watermarking Algorithms. We discuss some instantiations of the
graph watermarking framework deﬁned above. Unlike previous watermarking
or de-anonymization schemes that add vertices [1,19], we describe an eﬀective
and eﬃcient scheme based solely on edge ﬂipping. Such an approach would be
especially useful for applications where it could be infeasible to add vertices as
part of a watermark.
Our scheme does not require adding labels to the vertices or additional
objects stored in the graph for identiﬁcation purposes. Instead, we simply rely
on the structural properties of graphs for the purposes of marking. In particu-
lar, we focus on the use of vertex degrees, that is, the number of edges incident
on each vertex. We identify high and medium degree vertices as candidates for

288
D. Eppstein et al.
ﬁnding edges that can be ﬂipped in the course of marking. The speciﬁc degree
thresholds for what we mean by “high-degree” and “medium-degree” depend on
the graph family, however, so we postpone deﬁning these notions precisely until
our analysis sections.
Algorithms providing an example implementation of our graph watermarking
scheme are shown in Algorithm 2. The keygen algorithm randomly selects a set
of candidate vertex pairs for ﬂipping, from among the high- and medium-degree
vertices, with no vertex being incident to more than a parameter t of candidate
pairs. We introduce a procedure, label(G), which labels high-degree vertices by
their degree ranks and each medium-degree vertex, w, by a bit vector identifying
its high-degree adjacencies. This bit vector has a bit for each high-degree vertex,
which is 1 for neighbors of w and 0 for non-neighbors. The algorithm mark(z, G),
takes a random set of candidate edges and a graph, G, and it ﬂips the correspond-
ing edges in G according to a resampling of the edges using the distribution D.
The algorithm, approximate-isomorphism(G, H), returns a mapping of the high-
and medium-degree vertices in G to matching high- and medium-degree vertices
in H, if possible. The algorithm, identify(z, G, id1, . . . , idk, H), uses the approxi-
mate isomorphism algorithm to match up high- and medium-degree vertices in
G and H, and then it extracts the bit-vector from this matching using z.
As mentioned above, we also need a notion of distance for graphs. We use two
diﬀerent such notions. The ﬁrst is the graph edit distance, which is the minimum
number of edges needed to ﬂip to go from one graph to another. The second is
vertex distance, which intuitively is an edge-ﬂipping metric localized to vertices.
Deﬁnition 4 (Graph distances). Let G be the set of graphs on n vertices. If
G, H ∈G, deﬁne Π as the set of bijections between the vertex sets V (G) and
V (H). Deﬁne the graph edit distance diste : G × G →N as
diste(G, H) = min
π∈Π |E(G) ⊕π E(H)| ,
where ⊕π is the symmetric diﬀerence of the two edge sets under correspondence
π. Deﬁne the vertex distance distv : G × G →N as
distv(G, H) = min
π∈Π max
v∈V (G) |E(v) ⊕π E(π(v))| ,
where E(v) is the set of edges incident to v.
3
Identifying High- and Medium-Degree Vertices
We begin analyzing our proposed graph watermarking scheme by showing how
high- and medium-degree vertices can be identiﬁed under our two random graph
distributions. We ignore low-degree vertices: their information content and dis-
tinguishability are low, and they are not used by our example scheme.
We ﬁrst ﬁnd a threshold number k such that the k vertices with highest
degree are likely to have distinct and well-separated degree values. We call these k

Models and Algorithms for Graph Watermarking
289
Algorithm 2. Watermarking scheme for random graphs.
t: the maximum number of ﬂipped edges that can be adjacent to the same
vertex. keygen(ℓ, n):
1. Let x denote the total number of high- and medium-degree vertices
2. X = {(u, v) | 1 ≤u < v ≤x}
3. Let z be a list of ℓpairs randomly sampled (without replacement) from X such
that no end vertex appears more than t times
4. return z
label(G):
1. sort the vertices in decreasing order by degree and identify the high- and
medium-degree vertices
2. if the degrees of high-degree vertices are not unique, return failure
3. label each high-degree vertex with its position in the vertex sequence
4. label each medium-degree vertex with a bit vector encoding its high-degree
adjacencies
5. if the bit vectors are not unique, return failure
6. otherwise, return the labelings
mark(z, G):
1. S = ∅
2. V is the set of high- and medium-degree vertices of G, sorted lexicographically by
their labels given by L = label(G)
3. generate an ℓ-bit string id where each bit i is independently set to 1 with
probability pz[i], where pz[i] is the probability of the edge z[i] in D
4. let H be a copy of G
5. for j from 1 to ℓ:
6.
(u, v) = z[j]
7.
if id[j] is 1:
8.
insert edge (V [u], V [v]) in H
9.
else:
10.
remove edge (V [u], V [v]) from H
11. return (id, H)
approximate-isomorphism(G, H):
1. call label(G) and label(H), returning failure if either of these fail.
2. match each of G’s high-degree vertices with the vertex in H with the same label.
3. match each of G’s medium-degree vertices with the vertex in H whose label is
closest in Hamming distance.
4. if H has a vertex that is matched more than once, return failure.
5. otherwise, return the (partial) vertex assignments between G and H.
identify(z, G, id1, . . . , idk, H):
1. ﬁnd an approximate-isomorphism(G, H), returning ⊥if failure occurred at any step.
2. V is the set of high- and medium-degree vertices of G, sorted lexicographically by
their labels given by L = label(G)
3. V ′ is the set of vertices of H identiﬁed as corresponding to those in V , in that
same order.
4. id is an empty bit string
5. for (u, v) in z (from left to right):
6.
b = 1 iﬀthere is an edge between V ′[u] and V ′[v] in H.
7.
append b to id
8. return among the idi’s the one closest to id

290
D. Eppstein et al.
vertices the high-degree vertices. Next, we look among the remaining vertices for
those that are well-separated in terms of their high-degree neighbors. Speciﬁcally,
the (high-degree) neighborhood distance between two vertices is the number of
high-degree vertices which are connected to exactly one of the two vertices. Note
that we will omit the term “high-degree” in “high-degree neighborhood distance”
from now on, as it will always be implied.
In the Erd˝os-R´enyi model, we show that all vertices that are not high-degree
nevertheless have well-separated high-degree neighborhoods whp. In the random
power-law graph model, however, there will be many lower-degree vertices whose
high-degree neighborhoods cannot be separated. Those that have well-separated
high-degree neighborhoods with high probability form the medium-degree ver-
tices, and the rest are the low-degree vertices.
For completeness, we include the following well-known Chernoﬀconcentra-
tion bound, which we will refer to time and again.
Lemma 5 (Chernoﬀinequality [4]). Let X1, . . . , Xn be independent random
variables with
P [Xi = 1] = pi,
P [Xi = 0] = 1 −pi.
We consider the sum X = n
i=1 Xi, with expectation E [X] = n
i=1 pi. Then
P [X ≤E [X] −λ] ≤e−
λ2
2E[X] ,
P [X ≥E [X] + λ] ≤e−
λ2
2E[X]+λ/3 .
Vertex Separation in the Erd˝os-R´enyi Model. Let us next consider vertex
separation results for the classic Erd˝os-R´enyi random-graph model. Recall that
in this model, each edge is chosen independently with probability p.
Deﬁnition 6. Index vertices in non-increasing order by degree. Let di represent
the i-th highest degree in the graph. Given h = O(n), we say that a vertex is
high-degree with respect to dh if it has degree at least dh. Otherwise, we say that
the vertex is medium-degree.
Note that in this random-graph model, there are no low-degree vertices.
Deﬁnition 7. A graph is (d, d′)-separated if all high-degree vertices diﬀer in
their degree by at least d and all medium-degree vertices are neighborhood distance
d′ apart.
Note: this deﬁnition depends on how high-degree or medium-degree vertices
are deﬁned and will therefore be diﬀerent for the random power-law graph model.
Lemma 8 (Extension
of
Theorem
3.15
in
[2]).
Suppose
m
=
o(pqn/ log n)1/4, m →∞, and α(n) →0. Then with probability
1 −mα(n) −1/

m (log(n/m))2
,

Models and Algorithms for Graph Watermarking
291
G(n, p) is such that
di −di+1 ≥α(n)
m2
 pqn
log n
1/2
for every i < m,
where q = 1 −p.
Proof.
See the ePrint version [7].
⊓⊔
Lemma 9 (Vertex separation in the Erd˝os-R´enyi model).
Let 0 <
ε < 1/9, d ≥3, C ≥3, h = n(1−ε)/8. Suppose 0 < p = p(n) ≤
1
2 is such
that p = ω(n−ε log n). Then G(n, p) is (d, C log n)-separated with probability
1 −O(n−(1−ε)/8).
Proof.
See the ePrint version [7].
⊓⊔
Thus, high-degree vertices are well-separated with high probability in the
Erd˝os-R´enyi model, and the medium-degree vertices are distinguished with high
probability by their high-degree neighborhoods.
Vertex Separation in the Random Power-Law Graph Model. We next
study vertex separation for a random power-law graph model, which can match
the degree distributions of many graphs that naturally occur in social network-
ing and science. For more information about power-law graphs and their appli-
cations, see e.g. [3,12,14].
In the random power-law graph model, vertex indices are used to deﬁne
edge weights and therefore do not necessarily start at 1. The lowest index that
corresponds to an actual vertex is denoted i0. So vertex indices range from i0
to i0 + n. Additionally, there are two other special indices iH and iM, which we
deﬁne in this section, that separate the three classes of vertices.
Deﬁnition 10. The vertices ranging from i0 to iH are the high-degree vertices,
those that range from iH + 1 to iM are the medium-degree vertices, and those
beyond iM are the low-degree vertices.
In this model, the value of i0 is constrained by the requirement that
P[i0, i0] < 1. When γ ≥3, this constraint is not actually restrictive. However,
when γ < 3, i0 must be asymptotically greater than n−(γ−3)/2. The constraints
on i0 also constrain the value of the maximal and average degree of the graph.
We deﬁne iH and iM to be independent of i0, but dependent on parame-
ters that control the amount and probability of separation at each level. The
constraints that i0 < iH and iH < iM translate into corresponding restrictions
on the valid values of γ, namely that γ > 5/2 and γ < 3. We deﬁne iH in the
following lemma.
Lemma 11 (Separation of high-degree vertices). In the G(wγ) model, let
δi = |wi+1 −wi| /2. Then,
c
2(γ −1)(i + 1)−
γ
γ−1 ≤δi ≤
c
2(γ −1)i−
γ
γ−1 .
(3)

292
D. Eppstein et al.
Moreover, for all ε1 satisfying 0 < ε1 ≤1 and C1 > 0, the probability that
|deg(i) −wi| < ε1δi
for all i ≤iH
def
=

cε2
1
16(γ −1)2C1 log n
 γ−1
2γ−1
is at least 1 −n−C1.
Proof. The ﬁrst statement follows from the fact that wi is a convex function of
i and from taking its derivative at i and i + 1.
For the second statement, let C > 0 and let i′
H
def
=
	
cε2
1
8(γ−1)2C log n

 γ−1
2γ−1 . We
will show that if i ≤i′
H, then
P [|deg(i) −wi| ≥ε1δi] < n−C.
(4)
Now we choose C such that C1 + log iH/ log n < C ≤2C1. The inequality
C ≤2C1 implies that iH ≤i′
H and (4) holds for all i ≤iH. By the union bound
applied to (4)
P [∃i ≤iH, |deg(i) −wi| ≥ε1δi] ≤iHn−C.
Since C1 + log iH/ log n < C, the right hand side is bounded above by n−C1.
This proves the result.
Now, we prove (4). Clearly, since δi = (wi −wi+1)/2, we have that wi ≥δi.
So if ε1 ≤1 and λi = ε1δi, then wi ≥λi/3. This implies that
λ2
i
wi + λi/3 ≥λ2
i
2wi
≥
cε2
1
8(γ −1)2 i−2γ−1
γ−1 ,
where the second inequality follows from (3) and the deﬁnition of wi given in
Deﬁnition 3. If i ≤i′
H, the right hand side is lower-bounded by C log n. The
result follows by applying a Chernoﬀbound (Lemma 5).
⊓⊔
For simplicity, we often use the following observation.
Observation 12. Rewriting iH to show its dependence on n, we have
iH(ε1, C1) = K1(ε1, C1) n
1
2γ−1 (log n)−γ−1
2γ−1 ,
K1(ε1, C1)
def
=

γ −2
(γ −1)3
wε2
1
16C1
 γ−1
2γ−1
. (5)
For the graph model to make sense, the high-degree threshold must be asymptoti-
cally greater than the lowest index. In other words, we must have that i0 = o(iH).
Since i0 = Ω(n−(γ−3)/2), this implies that γ > 5/2.
We next deﬁne iM, the degree threshold for medium-degree vertices, in the
following lemma.
Lemma 13 (Separation of medium-degree vertices).
Let K0 be deﬁned
as in Deﬁnition 3, K1(ε1, C1) be deﬁned as in (5), and
K2(ε1, C1, ε2, C2)
def
=
Kγ−1
0
Kγ−2
1
(ε1, C1)
(C2 + 2Γ + 2 log(Kγ−1
0
Kγ−2
1
(ε1, C1)) + 2ε2)γ−1 .
(6)

Models and Algorithms for Graph Watermarking
293
Let Xij denote the neighborhood distance between two vertices i and j in G(wγ).
If 5/2 < γ < 3, for every ε2 > 0 and C2 > 0, the probability that
Xij > ε2 log n,
for all iH ≤i, j ≤iM
where
iM(ε1, C1, ε2, C2)
def
= K2(ε1, C1, ε2, C2) nΓ (log n)−3(γ−1)2
2γ−1
,
Γ
def
= −2γ2 −8γ + 5
2γ −1
,
(7)
is at least 1 −n−C2 for suﬃciently large n.
Proof. Let C > 0 and let
i′
M
def
=

C2 + 2Γ + 2 log(Kγ−1
0
Kγ−2
1
) + 2ε2
C + 2ε2
γ−1
iM.
We claim that if iH ≤i, j ≤i′
M, then
P [Xij ≤ε2 log n] ≤n−C.
(8)
If we choose C = C2 + 2Γ + 2 log Kγ−1
0
Kγ−2
1
, we have that iM = i′
M, so that (8)
applies to all i, j such that i, j ≤iM. Moreover, since
iM ≤Kγ−1
0
Kγ−2
1
nΓ ≤nlog(Kγ−1
0
Kγ−2
1
)nΓ,
our choice of C implies that i2
M n−C ≤n−C2. By applying the union bound to
(8), we have
P [∃i, j s.t. iH ≤i, j ≤iM, Xij ≤ε2 log n] ≤i2
Mn−C ≤n−C2,
which establishes the lemma.
Let us now prove the claim. Observe that Xij is the sum over the high-degree
vertices k, of indicator variables Xk
ij for the event that vertex k is connected to
exactly one of the vertices i and j. It i For ﬁxed i and j, these are independent
random variables. Therefore, we can apply a Chernoﬀbound. The probability
that Xk
ij = 1 is
P[i, k](1 −P[j, k]) + P[j, k](1 −P[i, k]) ≥2P[iM, iH](1 −P[i0, iH]).
Since P[i0, iH] →0, for suﬃciently large n, this expression is bounded below by
P[iM, iH], and
E [Xij] ≥iHP[iM, iH] ≥(C + 2ε2) log n,
by (2), (5) and (7), as can be shown by a straightforward but lengthy computa-
tion. Let d = ε2 log n. This implies that
(E [Xij] −d)2
E [Xij]
≥E [Xij] −2d ≥C log n.
Therefore, applying the Chernoﬀbound (Lemma 5) to the Xk
ij for ﬁxed i and j
and all high-degree vertices k proves the claim.
⊓⊔

294
D. Eppstein et al.
Observation 14. We would have the undesirable situation that iM = o(1)
whenever
2γ2−8γ+5
2γ−1
> 0, or equivalently when γ > 2 +

3/2 > 3. In fact,
in order for iH = o(iM), we must have γ < 3.
We illustrate the breakpoints for high-, medium-, and low-degree vertices in
Fig. 1.
iM
0
i0
iH
vertex index
i0+n
high-degree
medium-degree
low-degree
Θ
	
n
1
2γ−1 (log n)−γ−1
2γ−1

Θ

n−2γ2−8γ+5
2γ−1
(log n)−3(γ−1)2
2γ−1

Ω(n
3−γ
2 )
Fig. 1. Degree breakpoints for the random power-law graph model.
The next lemma summarizes the above discussion and provides the forms of
iH and iM that we use in our analysis.
Lemma 15 (Vertex separation in the power-law model). Let 5/2 < γ <
3. Fix ε > 0, C1 > 0, C2 > 0. Let iH = iH(ε1, C1) and iM = iM(ε1, C1, ε2, C2)
where ε1 = 1 and ε2 = ε. Let
d = n
1
2γ−1
and
d′ = log n.
For suﬃciently large n, the probability that a graph G(wγ) is not (εd, εd′)-
separated is at most n−C1 + n−C2.
Proof. Let δi be deﬁned as in Lemma 11. A straightforward computation using
(1), (3), and (5) shows that
δiH ≥constant · n
1
2γ−1 (log n)
γ
2γ−1 .
So for suﬃciently large n, we have δiH ≥3εd/2. For all i ≤iH, the average
degrees wi of consecutive vertices are at least 3εd/2 apart. So for two high-
degree vertices to be within εd of each other, at least one of the two must have
degree at least (3ε/2 −ε/2)d away from its expected degree. By Lemma 11, the
probability that some high-degree vertex i satisﬁes |deg(i) −wi| > δiH is at most
n−C1.
By Lemma 13, the probability that there are two medium-degree vertices
with neighborhood distance less than εd′ is at most n−C2.
⊓⊔
Thus, our marking scheme for the random power-law graph model is eﬀective.

Models and Algorithms for Graph Watermarking
295
4
Adversary Tolerance
In this section, we study the degree to which our exemplary graph watermarking
scheme can tolerate an arbitrary edge-ﬂipping adversary.
Theorem 16 (Security against an arbitrary edge-ﬂipping adversary in
the Erd˝os-R´enyi model). Let 0 < ε < 1/9, d ≥3, h = n(1−ε)/8 and p ≤1/2
such that p = ω(n−ε log n). Let d be suﬃciently large so that
ε d + 1
d −1 < 1.
(9)
Suppose the similarity measure is the vertex distance distv, the similarity thresh-
old is θ = d, we have a number k = nC of watermarked copies, and their identi-
ﬁers are generated using ℓ= 8(2C + C′)nε bits. Suppose also that the identiﬁers
map to sets of edges of a graph constrained by the fact that no more than t = d
edges can be incident to any vertex. The watermarking scheme deﬁned in Algo-
rithm 2 is (G(n, p), distv, θ, k, ℓ)-secure against any deterministic adversary.
The proof of this theorem relies on two lemmas. Lemma 17 identiﬁes con-
ditions under which a set of bit vectors with bits independently set to 1 is
unlikely to have two close bit vectors. Lemma 18 states that a deterministic
adversary’s ability to guess the location of the watermark is limited. Informally,
this is because the watermarked graph was obtained through a random process,
so that there are many likely original graphs that could have produced it.
Lemma 17 (Separation of IDs).
Consider k = nC random bit strings of
length ℓ, where each bit is independently set to 1, and the i-th bit is 1 with
probability qi satisfying p ≤qi ≤1/2 for a ﬁxed value p. The probability that at
least two of these strings are within Hamming distance D = 4(2C + C′) log n of
each other is at most n−C′ if ℓp ≥2D.
Proof.
See the ePrint version [7].
⊓⊔
Lemma 18 (Guessing power of adversary). Consider a complete graph on
N vertices, and let r of its edges be red. Let s be a sample of ℓedges chosen
uniformly at random among those that satisfy the constraint that no more than
t edges of the sample can be incident to any one vertex. Suppose also that ℓ, N
and t are non-decreasing functions of n such that
ℓt+1
N t−1 →0 as n →∞.
(10)
For suﬃciently large N, the probability that s contains at least R = 8ℓr/N 2
red edges is bounded by 4 exp

−12ℓr/(7N 2)

. Moreover, if ℓr/N 2
→
0,
then the probability that s contains at least R = 1 red edge is bounded by
4 exp

−cN 2/(ℓr)

, for some c > 0 and for suﬃciently large N.
Proof.
See the ePrint version [7].
⊓⊔

296
D. Eppstein et al.
Proof (Theorem 16). An upper bound on the advantage of any deterministic
adversary A : G →G on graphs on n vertices is given by the conditional proba-
bility
P [identify(z, G, id1, . . . , idk, GA) ̸= id|distv(G, GA) < θ] ,
where the parameters passed to identify are deﬁned according to the experiment
in Algorithm 1. We show that this quantity is polynomially negligible.
For GA to be successfully identiﬁed, it is suﬃcient for the following three
conditions to hold:
1. the original graph G = G(n, p) is (4d, 4d)-separated;
2. the Hamming distance between any two id and id′ involved in a pair in S is
at least D = 4(2C + C′) log n;
3. A changes no edges of the watermark.
These are suﬃcient conditions because we only test graphs whose vertices had
at most d incident edges modiﬁed by the adversary, and another d incident edges
modiﬁed by the watermarking. So for original graphs that are (4d, 4d)-separated,
the labeling of the vertices can be successfully recovered. Finally, if the adversary
does not modify any potential edge that is part of the watermark, the id of the
graph is intact and can be recovered from the labeling.
Now, by Lemma 9, the probability that G(n, p) is not (4d, 4d)-separated is
less than O(n−(1−ε)/8). Moreover, since ℓp ≥2D, by Lemma 17, the probability
that there are two identiﬁers in S that are within D of each other is at most n−C′.
Finally, for graphs in which an adversary makes fewer than d modiﬁcations
per vertex, the total number of edges the adversary can modify is r ≤dn/2.
Since all vertices are high- and medium-degree vertices in this model, N = n.
Therefore, ℓr/N 2 = O(1/n(1−ε)) →0. Eq. (9) guarantees that the hypothesis
given by (10) of Lemma 18 is satisﬁed. Consequently, the probability that A
changes one or more adversary edges is O(exp[cn1−ε]) for some constant c.
This proves that each of the three conditions listed above fails with polyno-
mially negligible probability, which implies that the conditional probability is
also polynomially negligible.
⊓⊔
Theorem 19 (Security against an arbitrary edge-ﬂipping adversary in
the random power-law graph model).
Let 5/2 < γ < 3, C > 0, iH =
iH(ε1, C1) and iM = iM(ε1, C1, ε2, C2) where ε1 = 1, ε2 = 8(C + 1) and C1 =
C2 = C.
Let p = P [iM, iM]. Suppose the similarity measure is a vector of distances
dist = (diste, distv), that the corresponding similarity threshold is the vector θ =
(r, log n) where r = p(iM)2/32 is the maximum number of edges the adversary
can ﬂip in total, and log n the maximum number of edges it can ﬂip per vertex.
Suppose that we have k = nC′′ watermarked copies of the graph, that we use
ℓ= 8(2C′′ + C′)(log n)/p to watermark a graph.
Suppose also that the identiﬁers map to sets of edges of a graph constrained by
the fact that no more than t = log n edges can be incident to any vertex. Then the
watermarking scheme deﬁned in Algorithm 2 is (G(wγ), dist = (diste, distv), θ =
(r, log n), k, ℓ)-secure against any deterministic adversary.

Models and Algorithms for Graph Watermarking
297
Proof.
See the ePrint version [7].
⊓⊔
Discussion. It is interesting to note how the diﬀerences in the two random
graph models translate into diﬀerences in their watermarking schemes. The
Erd˝os-R´enyi model, with its uniform edge probability, allows for constant separa-
tion of high-degree vertices, at best. But all the vertices tend to be well-separated.
On the other hand, the skewed edge distribution that is characteristic of the ran-
dom power-law model allows high-degree vertices to be very well-separated, but
a signiﬁcant number of vertices—the low-degree ones, will not be easily distin-
guished.
These diﬀerences lead to the intuition that virtually all edges in the Erd˝os-
R´enyi model are candidates for use in a watermark, as long as only a constant
number of selected edges are incident to any single vertex. Therefore, both our
watermarking function and the adversary are allowed an approximately linear
number of changes to the graph. Theorem 16 conﬁrms this intuition with a
scheme that proposes O(nε) bits for the watermark, and a nearly linear number
O(n) bits that the adversary may modify.
In contrast, the number of edges that can be used as part of a watermark in
the random power-law graph model is limited by the number of distinguishable
vertices, which is on the order of iM or O(nε), where ε = −2γ2−8γ+5
2γ−1
.
5
Experiments
Although our paper is a foundational complement to the systems work of Zhao
et al. [18], we nevertheless provide in this section the results of a small set of
empirical tests of our methods, so as to experimentally reproduce the hypothet-
ical watermarking security experiment from Algorithm 1. Our experiments are
performed on two large social network graphs, Youtube [17] from the SNAP
library [10], and Flickr [11], as well as a randomly generated graph drawn from
the random power-law graph model distribution. Table 1 illustrates the basic
properties of the networks. To generate the random power-law graph, we set the
number of nodes to n = 10000, the maximum degree to m = 1000, the average
degree to w = 20, and γ = 2.75.
To adapt our theoretical framework to the rough-and-tumble world of empir-
ical realities, we made three modiﬁcations to our framework for the sake of our
empirical tests.
Table 1. Network statistics
Network
# nodes
# edges
Max. degree
Avg. degree
Unique degree
Estimated γ
Power-law
10, 000
94, 431
960
18.89
14
—
Youtube
1, 134, 890
2, 987, 624
28, 754
5.27
29
1.48
Flickr
1, 715, 256
15, 554, 181
27, 203
18.14
130
1.62

298
D. Eppstein et al.
Table 2. Experiment parameters
Network
# high-degree # medium-degree Key size Marking dK-2 deviation
Power law
64
374
219
0.065
Youtube
256
113
184
0.033
Flickr
300
5901
3250
0.002
First, instead of using the high-degree and medium-degree thresholds derived
from Lemmas 11 and 13, for the power-law distribution, to deﬁne the cutoﬀs for
high-degree and medium-degree vertices, we used these and the other lemmas
given above as justiﬁcations for the existence of such distinguishing sets of ver-
tices and we then optimized the number of high- and medium-degree vertices to
be values that work best in practice. The column, “Unique degree,” from Table 1
shows, for each network, the number of consecutive nodes with unique degree
when considering the nodes in descending order of degree. Since this value is
too small in most cases, we applied the principles of Lemmas 9 and 11 again, in
a second-order fashion, to distinguish and order the high-degree nodes. In par-
ticular, in addition to the degree of each high-degree vertex, we also label each
vertex with the list of degrees of its neighbors, sorted in decreasing order. With
this change, we are not restricted in our choice of number of high-degree nodes
as required by applying these lemmas only in a ﬁrst-order fashion. Table 2 shows
the values used in our experiments based on this second-order application. As
medium-degree vertices, we picked the maximum number such that there are no
collisions among their bit vectors of high-degree node adjacencies.
Second, instead of returning failure if (a) two high-degree nodes have the
same degree and list of degrees of their neighbors, (b) two medium-degree nodes
have the same bit vector, or (c) the approximate isomorphism is not injective,
we instead proceed with the algorithm. Despite the existence of collisions, the
remaining nodes often provide enough information to conclude successfully.
Finally, we simpliﬁed how we resampled (and ﬂipped) edges in order to create
a graph watermark, using our approach for the Erd˝os-R´enyi model even for
power-law graphs, since resampling uniformly among our small set of marked
edges is likely not to cause major deviations in the graph’s distribution and,
in any case, it is empirically diﬃcult to determine the value of γ for real-world
social networks. Therefore, we set the resampling probability to 0.5 so that it is
consistent with the Erd˝os-R´enyi model and so that each bit in the message is
represented uniformly and independently.
Experiment Parameters. For the experiment parameters other than the orig-
inal network and the number of high- and medium-degree nodes, we set the
following values.
Maximum Flips Adjacent to Any Given Node During Marking: 1.
Key size: We set this to the maximum possible value (i.e., the number of high-
and medium-degree vertices divided by two, as shown in Table 2), because the

Models and Algorithms for Graph Watermarking
299
numbers of high- and medium- degree nodes are not large. This eﬀectively
means that every high- and medium-degree node has exactly one edge added
or removed.
Number of Marked Graphs: 10.
Adversary: We used a time-eﬃcient variation of the arbitrary edge-ﬂipping
adversary. This adversary selects a set of pairs of nodes randomly, and ﬂips
the potential edge among each pair.
Results. We evaluated how much distortion the adversary can introduce before
our method fails to identify the leaked network correctly. For this purpose, we
compared the identiﬁcation success rate to the amount of distortion under dif-
ferent fractions of modiﬁed edges by the adversary. To estimate the success rate,
we ran the experiment 10 times and reported the fraction of times that the
leaked network was identiﬁed correctly. As a measure of distortion, we used the
dK-2 deviation [18] between the original network and the version modiﬁed by
Flickr
Youtube
Power-law
Success rate
0
0.5
1
Fraction of modified edges
10−7
10−6
10−5
10−4
10−3
0.01
dK-2 deviation
0
0.25
0.5
0.75
Fraction of modified edges
10−7
10−6
10−5
10−4
10−3
0.01
Success rate
0
0.5
1
Fraction of modified edges
10−7
10−6
10−5
10−4
dK-2 deviation
0
2.5
5
7.5
Fraction of modified edges
10−7
10−6
10−5
10−4
Success rate
0
0.5
1
Fraction of modified edges
10−7
10−6
10−5
10−4
dK-2 deviation
0
0.5
1
Fraction of modified edges
10−7
10−6
10−5
10−4
Fig. 2. Success rate and dK-2 deviation under diﬀerent fractions of modiﬁed potential
edges by the adversary, for the Power law, Youtube, and Flickr networks.

300
D. Eppstein et al.
the adversary. The dK-2 deviation is the euclidean distance between the dK-2
series [15] of the two graphs, normalized by the number of tuples in the dK-
2 series. The dK-2 deviation captures the diﬀerences between the joint degree
distributions of the networks, that is, the probability that a randomly selected
edge has as endpoints nodes with certain degrees. We average the dK-2 deviation
among the 10 runs. Figure 2 shows the outcome of our experiments. Moreover,
Table 2 shows the dK-2 deviation introduced by the marking alone.
Based on our experiments, the success rate of our scheme is high but it
drops after a certain threshold. This demonstrates that there is a distinct range
of adversarial edge ﬂips that can be tolerated by our scheme. Speciﬁcally, our
scheme worked well when the fraction of potential edges ﬂipped by the adversary
is up to 10−3 and 10−5 for the random power-law and Youtube networks, respec-
tively. For these graphs, this number of ﬂipped potential edges corresponds to
52.9 % and 215.6 % of the number of edges in the original graphs, respectively.
For the Flickr network, the runtime of the adversary modiﬁcation became exces-
sive before the success rate could decrease, at a fraction of 10−4 of potential
edges ﬂipped.
The distortion introduced by the watermark is negligible compared to the
distortion caused by the number of ﬂips that the scheme can tolerate. On aver-
age, the marking modiﬁes half of the edges on the key, which corresponds to
1.1 · 10−3, 3 · 10−5, and 10−4 of the number of edges in the original random
power-law, Youtube, and Flickr networks, respectively.
For the same number of ﬂips, the dK-2 deviation in the Youtube network
was much larger than in the Flickr network, which in turn was larger than that
of the random power-law network. A possible explanation for this is that any
set of uniform edge ﬂips has a bigger eﬀect on the dk2-deviation of a skewed
graph than on the dK-2 deviation of a less skewed graph. Note that the Youtube
network has the largest skew, as the maximum degree is on the same order as
the Flickr network, but the average degree is less.
References
1. Backstrom, L., Dwork, C., Kleinberg, J.: Wherefore art thou r3579x?: Anonymized
social networks, hidden patterns, and structural steganography. Commun. ACM
54(12), 133–141 (2011)
2. Bollob´as, B.: Random Graphs, Cambridge Studies in Advanced Mathematics, vol.
73, 2nd edn. Cambridge University Press, Cambridge (2001)
3. Caldarelli, G.: Scale-Free Networks: Complex Webs in Nature and Technology.
Oxford University Press, Oxford (2013)
4. Chung, F., Lu, L.: Complex graphs and networks. In: CBMS Regional Conference
Series in Mathematics, vol. 107. American Mathematical Society (2006)
5. Collberg, C.S., Kobourov, S.G., Carter, E., Thomborson, C.: Graph-based
approaches to software watermarking. In: Bodlaender, H.L. (ed.) WG 2003. LNCS,
vol. 2880, pp. 156–167. Springer, Heidelberg (2003)
6. Collberg, C., Thomborson, C.: Software watermarking: models and dynamic
embeddings. In: ACM Symposium on Principles of Programming Language
(POPL), pp. 311–324 (1999)

Models and Algorithms for Graph Watermarking
301
7. Eppstein, D., Goodrich, M.T., Lam, J., Mamano, N., Mitzenmacher, M., Torres,
M.: Models and algorithms for graph watermarking. ArXiv ePrint abs/1605.09425
(2016). http://arxiv.org/abs/1605.09425
8. Hopper, N.J., Molnar, D., Wagner, D.: From weak to strong watermarking. In:
Vadhan, S.P. (ed.) TCC 2007. LNCS, vol. 4392, pp. 362–382. Springer, Heidelberg
(2007)
9. Khanna, S., Zane, F.: Watermarking maps: hiding information in structured data.
In: 11th ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 596–605
(2000). http://dl.acm.org/citation.cfm?id=338219.338612
10. Leskovec, J., Sosiˇc, R.: SNAP: a general purpose network analysis and graph mining
library in C++. http://snap.stanford.edu/snap
11. Mislove, A., Marcon, M., Gummadi, K.P., Druschel, P., Bhattacharjee, B.: Mea-
surement and analysis of online social networks. In: 5th ACM/Usenix Internet
Measurement Conference (IMC) (2007)
12. Mitzenmacher, M.: A brief history of generative models for power law and lognor-
mal distributions. Internet Math. 1(2), 226–251 (2004)
13. Narayanan, A., Shmatikov, V.: De-anonymizing social networks. In: IEEE Sympo-
sium on Security and Privacy (SP), pp. 173–187 (2009)
14. Newman, M., Barabasi, A.L., Watts, D.J.: The Structure and Dynamics of Net-
works. Princeton Studies in Complexity. Princeton University Press, Princeton
(2006)
15. Sala, A., Cao, L., Wilson, C., Zablit, R., Zheng, H., Zhao, B.Y.: Measurement-
calibrated graph models for social network experiments. In: 19th International
Conference on the World Wide Web (WWW), pp. 861–870 (2010)
16. Venkatesan, R., Vazirani, V.V., Sinha, S.: A graph theoretic approach to software
watermarking. In: Moskowitz, I.S. (ed.) IH 2001. LNCS, vol. 2137, pp. 157–168.
Springer, Heidelberg (2001)
17. Yang, J., Leskovec, J.: Deﬁning and evaluating network communities based on
ground-truth. CoRR abs/1205.6233 (2012). http://arxiv.org/abs/1205.6233
18. Zhao, X., Liu, Q., Zheng, H., Zhao, B.Y.: Towards graph watermarks. In: 2015
ACM Conference on Online Social Networks (COSN), pp. 101–112 (2015)
19. Zhao, X., Liu, Q., Zhou, L., Zheng, H., Zhao, B.Y.: Graph watermarks. ArXiv
ePrint abs/1506.00022 (2015). http://arxiv.org/abs/1506.00022

Software Security

Policy-Based Implicit Attestation
for Microkernel-Based Virtualized Systems
Steﬀen Wagner1(B) and Claudia Eckert2
1 Fraunhofer Institute AISEC, Munich, Germany
steffen.wagner@aisec.fraunhofer.de
2 Technische Universit¨at M¨unchen, Munich, Germany
eckert@sec.in.tum.de
Abstract. We present an attestation mechanism that enables a remote
veriﬁer to implicitly evaluate the trustworthiness of the prover’s system
through policies. Those policies are veriﬁed and enforced by a TPM 2.0,
when the attestor interacts with a virtualized hardware component of
the prover’s system. For instance, when the veriﬁer reads a virtualized
sensor device and requests integrity-protected sensor data, such as the
average temperature, a heartbeat value, or an anomaly detection score,
the prover’s TPM, which acts as a trust anchor, checks and enforces
the policies speciﬁed by the veriﬁer. The prover, in turn, is also able to
deﬁne policies, which can limit access to certain hardware components
and are also enforced by the TPM. As a result, both parties have to
cooperate for a successful attestation, which implicitly creates veriﬁable
proof of the prover’s trustworthiness using mainly symmetric instead of
expensive asymmetric cryptographic operations like digital signatures.
Keywords: Remote attestation · Trusted platform module · Policy ·
Data integrity · Microkernel
1
Introduction
With hardware-based virtualization technologies, such as Intel VT [12] or ARM’s
Virtualization Extensions [3,5], isolating rich operating systems like Linux from
each other and the rest of the system is a very eﬀective way to ensure overall
system security. This level of security can be even further increased if a micro-
kernel, such as L4/Fiasco.OC [11,18], serves as the basis for an unprivileged
hypervisor. Since microkernels implement all non-essential system components
as user-space tasks, strictly separate those tasks, and have a very small code
size, a microkernel-based hypervisor in user space reduces the system’s attack
surface signiﬁcantly. As a result, such microkernel-based virtualized systems are
suited even for the most security critical applications, e.g., in mobile, industrial,
automotive, or avionic systems. However, since the virtualized rich operating sys-
tems usually still require some degree of access to physical hardware, they can-
not be completely isolated and a virtual machine monitor (VMM) must provide
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 305–322, 2016.
DOI: 10.1007/978-3-319-45871-7 19

306
S. Wagner and C. Eckert
mechanisms to make selected hardware components available to the virtualized
systems.
One mechanism to give a virtualized system access to a physical hardware
component, such as a display or camera, directly maps the component’s physical
memory address to the address space of a virtualized system, which is then able
to exclusively use this component. However, not all components can be directly
assigned to a speciﬁc virtualized system, because some hardware components,
such as the physical network interface, a mobile broadband modem, or sensors,
are shared. In addition, this simple and naive mechanism does not allow for
inspecting, dynamically restricting, and multiplexing the access to a component.
That is why hardware components are usually virtualized, which means that
access requests, i.e., read and write operations, have to go through the virtual
machine monitor and to device drivers, which support virtualization.
On the other hand, virtualizing a hardware component, such as a sensor,
also presents a number of challenges. For example, a virtualized system cannot
be sure that the access to a component was handled as requested, because it is
not able to directly access that component and make the request itself. Using
device emulation techniques, a hypervisor could simulate a hardware component,
particularly a sensor, and modify, for example, the result of a read operation
before it is returned to the virtualized system. That is why a (remote) user
or system, which interacts with the virtualized rich operating system, needs to
be able to verify the integrity of the underlying system to be able to trust the
data from a hardware component. Unfortunately, most existing mechanisms to
(remotely) verify the integrity of a system, such as IBM’s Integrity Measurement
Architecture (IMA) [14] in combination with a Trusted Platform Module (TPM),
are not able to attest a microkernel-based system acting as hypervisor.
To overcome these challenges, we ﬁrst present a microkernel-based system
architecture, which uses ARM’s Virtualization Extensions to run multiple rich
operating systems and ARM TrustZone together with a TPM 2.0 [17] to securely
handle critical operations and store sensitive information, like keys. Our main
contribution is a security protocol, which leverages our system design to protect
the integrity of data while implicitly verifying the trustworthiness of the system.
The proposed protocol uses eﬃcient symmetric cryptographic operations, such
as hashing and hash-based message authentication codes (HMACs), and only
optionally utilizes asymmetric cryptographic operations during the setup phase.
Our third contribution is a prototype implementation of our microkernel-based
system architecture and our proposed attestation protocol. Due to the lack of a
dedicated hardware TPM 2.0, however, it features a fully functional simulator,
which we have extracted from the public PDF version of the TPM Library
Speciﬁcation [17] using a Python script [19] that we have made open source.
The rest of the paper is structured as follows. In Sect. 2, we discuss related
work with a focus on existing remote attestation mechanisms. Section 3 outlines
our scenario and attacker model, which is the basis for the design of our overall
system architecture described in Sect. 4. We then present our main contribution,
the integrity protection and policy-based attestation protocol, in Sect. 5, while

Policy-Based Implicit Attestation for Microkernel-Based Virtualized Systems
307
reserving details about the prototype implementation for Sect. 6. Finally, we
discuss the security of our protocol in Sect. 7 and conclude with Sect. 8.
2
Related Work
A remote attestation is a cryptographic process for creating veriﬁable proof that
enables a remote veriﬁer to detect modiﬁcations to the prover’s system, thus
allowing the attestor to determine the prover’s trustworthiness.
In a hash-based remote attestation as speciﬁed by the Trusted Computing
Group (TCG) [16,17], the prover’s system calculates static load-time integrity
measurements for all relevant software components, which are securely stored
inside so-called platform conﬁguration registers (PCRs) of the TPM and can be
used to prove the system’s integrity to a remote veriﬁer. More precisely, each
boot component hashes the next software component during authenticated boot
starting from an immutable Core Root of Trust for Measurement (CRTM). After
the boot process has been completed, the operating system continues to measure
software binaries through integrity veriﬁcation mechanisms such as IMA. For a
hash-based remote attestation, the integrity measurements inside the PCRs are
signed by the TPM and sent to the remote veriﬁer. With the corresponding pub-
lic key and a so-called stored measurement log (SML), the remote party is able
to verify the signature and check the entries of the SML against expected mea-
surements provided the prior signature veriﬁcation was successful. To address
privacy concerns related to the public key, the TCG alternatively also adopted
a remote attestation primitive called Direct Anonymous Attestation (DAA) [6],
which aims to preserve the prover’s privacy using zero-knowledge proofs.
However, since both primitives speciﬁed by the TCG, traditional remote
attestation and DAA, primarily focuses on hash-based load-time integrity mea-
surements for software binaries only, other schemes, such as property-based [13],
semantic [8], group-based [1], or logical attestation [15], have tried to extend and
generalize the attestation mechanism. For example, the idea behind property-
based attestation is to prove certain security characteristics and qualities rather
than to verify the hash-based integrity of certain software components. Similarly,
logical attestation is based on veriﬁable statements about software properties,
which are expressed in a logic. Group-based attestation, in turn, uses Chameleon
signatures [9] to enhance privacy and the ability to manage software integrity.
Unfortunately, those characteristics, qualities, and logical properties are not
enforced by the TPM, but the operating system. Our attestation mechanism,
on the other hand, is based on hash-based cryptographic policies, which are
enforced by a TPM 2.0. In addition, our approach does not rely on expensive
cryptographic operations, such as digital signatures, to create veriﬁable proof for
the integrity of the prover’s system. Furthermore, our remote attestation mech-
anism, which is designed for, but not limited to microkernel-based virtualized
systems, enables a veriﬁer to protect the integrity of data, e.g., from a virtualized
device, while implicitly verifying the trustworthiness of the prover’s system.

308
S. Wagner and C. Eckert
3
Scenario and Attacker Model
In this section, we describe the scenario, which outlines the settings for our
integrity protection and attestation protocol. We also specify the attacker model.
3.1
Data Integrity Protection and Attestation Scenario
For our data integrity protection, which enables the detection of unauthorized
modiﬁcations and also implicitly attests the integrity of the underlying system,
we deﬁne a prover (P) and a veriﬁer (V). The prover is a microkernel-based
system, such as an industrial control system, a smartphone, or a vehicle. P is
equipped with a TPM 2.0 and able to virtualize rich operating systems, such as
Linux or Android, though hardware-based virtualization technologies like ARM’s
Virtualization Extensions. Since we assume the prover also executes safety- or
security-critical applications as native microkernel tasks, which must be strictly
isolated from the rich operating systems (and sometimes even the VMM), the
prover’s system additionally provides hardware-enforced separation mechanism
like ARM’s Security Extension, also known as TrustZone. V, on the other hand,
is a remote veriﬁer, which is considered honest and trustworthy in the context
of our security protocol. Like the prover, the veriﬁer V is also equipped with a
TPM 2.0 to store sensitive information, such as cryptographic keys.
Without loss of generality, we assume that the prover is an industrial control
system with at least one rich operating system and a set of sensors monitoring
its state, environmental conditions, and a ﬁxed number of attached components.
In our scenario, the veriﬁer is allowed to log into the rich operating system with
credentials provided by the prover’s administrator and, hence, is able to interact
with certain sensors. However, since the rich operating system is virtualized and
device access has to go through the hypervisor, which is controlled by the prover,
the veriﬁer can only access said sensors in a very controlled and restricted way.
As a result, the veriﬁer cannot be sure that the data from a virtualized device has
not been modiﬁed. For example, the data of a heartbeat sensor might have been
modiﬁed by the prover to reﬂect a system working without any interruptions or
anomalies, while the system was, in fact, not available for some time. Obviously,
we also have to assume that an attacker might try to modify the data when sent
to the veriﬁer’s system for further evaluation if that is part of the scenario.
3.2
Attacker Model
In our data integrity and remote attestation scenario, an adversary (A) can read
messages sent between the prover P and the veriﬁer V as long as those messages
are not encrypted with a scheme that is still considered secure. An adversary,
which can include P if the prover acts dishonestly, is also able to manipulate data
if its integrity is not protected, e.g., by a message authentication code (MAC).
As a result, A can only decrypt an encrypted message or forge a correct MAC
for a modiﬁed message if the attacker has access to the correct key. Furthermore,
an attacker is not able to invert cryptographic hash functions.

Policy-Based Implicit Attestation for Microkernel-Based Virtualized Systems
309
In addition, we assume that hardware attacks are not feasible, as speciﬁed
for most remote attestation protocols. In particular, security mechanisms, which
are integrated into the chip, like ARM TrustZone, or provided by a TPM cannot
be compromised by an attacker. That means we assume that the implementa-
tion of hardware-based security features, e.g., cryptographic engines or security
extensions, and any ﬁrmware components implemented in software is correct.
4
Microkernel-Based System Architecture with TPM 2.0
In this section, we describe the design of our proposed microkernel-based sys-
tem architecture, which includes a TPM 2.0 and makes use of hardware-based
virtualization and security mechanisms of modern ARM system-on-chips (SoCs).
As the name suggests, microkernels only have a fraction of the code size of
regular monolithic kernels like Linux. In addition, microkernel-based systems
implement all non-essential system components, such as drivers, as user-space
tasks, strictly separate those tasks, and provide only a small number of sys-
tem calls. That is why they are ideally suited even for the most safety- and
security-critical systems. Hence, we use a microkernel as the basis for our sys-
tem architecture.
As shown in Fig. 1, the design of our system architecture separates a secure
execution environment (right side) from a non-secure environment (left side).
Both execution environments accommodate a microkernel-based system, which
consists of at least a kernel component core in privileged levels PL1+ and a user-
space component init in the unprivileged level PL0. The separation into two
isolated execution environments can be realized, for example, through the ARM
TrustZone mechanism, which basically assigns system components, devices, and
memory to either Secure World or Non-secure World. In the Secure World,
the microkernel-based system or, more precisely, its TrustZone VMM (tzvmm),
handles all request to security-critical tasks and devices, such as the TPM 2.0.
In the Non-secure World, a second virtual machine monitor (vmm) similarly
handles calls (and traps) to the hypervisor, in this case however, with the goal
to virtualize a rich operating system like Linux.
The virtualized rich operating system, which can be an conventional Linux or
Android, provides the usual services for a user to interact with the system. That
means a user can log into the rich operating system and, for example, read data
from a hardware device, such as a sensor. If the device is virtualized, access is
trapped to the virtual machine monitor. The VMM, in turn, either forwards the
request to a device driver, which also resides in the Non-secure World, or it uses
a hardware-based interface, a so-called Secure Monitor Call (SMC), to access a
device driver implemented in Secure World as depicted in Fig. 1. If the request
is handled by the virtual machine monitor in the TrustZone, tzvmm forwards
it to the corresponding device driver. That way, a security-critical device like a
TPM 2.0 can be accessed and shared between multiple rich operating systems,
while the VMM is able to monitor, restrict, and deny access if necessary.
At this point it is very important to note that the minimal hardware-based
TrustZone interface, the SMC, which is even less complex than the small set of

310
S. Wagner and C. Eckert
Secure World
Non-secure W
orld
PL1 modes
(supervisor mode)
PL0 modes
(user space)
PL2 Modes
(HYP mode)
PL0 modes
(user space)
PL1 modes
(kernel space)
init
tzvmm
core
core
init
rich OS kernel
(e.g., Lin ux)
Dom0
Dom1…n
device client
monitor mode
tpm0
dev0
/dev
vmm
device driver
TPM 2.0
tpm2
device hardware
Fig. 1. Microkernel-based system architecture with TPM 2.0 on an ARM SoC
system calls used by microkernels, is also the reason for having two microker-
nels in our system architecture. That way, there is a strong possibility that the
system can still function and actively recover, even if the Non-secure World was
compromised. Assuming the system architecture was implemented correctly, an
adversary needs to successfully attack the core component in the Secure World
(while using mainly SMC calls or traps) in order to fully compromise the system.
However, even with a separation of resources, such as memory or devices,
though hardware-based virtualization and security mechanisms like TrustZone,
the design of our system architecture also includes a TPM 2.0, which is used to
securely create and store sensitive information, particularly cryptographic keys.
In contrast to the ARM’s TrustZone, a TPM is a dedicated non-programmable
hardware security module, which not only implements cryptographic engines in
hardware, but also establishes trust, precisely because it provides assurances
that its ﬁrmware cannot be easily modiﬁed by any user or remote attacker.
In addition to acting as a trust anchor, a TPM 2.0 provides mechanisms
to store cryptographic integrity measurements. Those measurements are usually
collected during authenticated boot, where the current component measures the
next component in the boot chain and extends the hash into one of the PCRs.
In a remote attestation as speciﬁed by the TCG, those PCRs are digitally signed
with an asymmetric key to create proof about the trustworthiness of the system.
Since the PCRs are only reset when the system is reset and can only be updated
or, more precisely, extended with new measurements, an attacker is not able to
modify a boot component without detection. On top of that, PCRs can also be
used to cryptographically bind a key to speciﬁc values, which means that the

Policy-Based Implicit Attestation for Microkernel-Based Virtualized Systems
311
key can only be used if the current values in the PCRs match the speciﬁed ones.
With the TPM 2.0, the TCG generalized this idea and developed a concept
called Extended Authorization (EA). With EA, the TPM allows the use of a
cryptographic key if the user can satisfy a policy. A TPM 2.0 policy is represented
by a cryptographic hash, which needs to match a speciﬁc value, and can include,
for example, a hashed secret, the value of NV memory inside the TPM, or PCR
values. That is why we use TPM 2.0 policies as the basis for our attestation.
5
Data Integrity Protection with Implicit Attestation
In this section, we present our data integrity protection and attestation pro-
tocol. In contrast to a traditional remote attestation as speciﬁed by the TCG,
our protocol uses eﬃcient symmetric operation instead of relying on expensive
asymmetric cryptographic operations to create veriﬁable proof of the system’s
integrity. As the main contribution, however, our implicit attestation protocol
makes use of the Extended Authorization mechanism provided by the TPM 2.0,
which allows for a ﬂexible deﬁnition of authorization and attestation policies.
The main idea of our proposed protocol is based on the fact that both parties,
the prover P and the veriﬁer V, each control a cryptographic key with a policy.
For a successful policy-based attestation, P has to satisfy the veriﬁer’s policy
in order to be able to load V’s key, when V requests integrity-protected data.
V’s policy, thus, may include trusted PCR values for the prover’s system, which
means the key can only be loaded if P’s system is still in a trustworthy state.
In turn, V must act according to the prover’s policy, which convinces P to load
the integrity key on behalf of V and create integrity-protected data. Hence, P’s
policy could, for example, specify that V may only access a virtualized hardware
resource if the device is enabled and the access pattern meets certain criteria. As
a result, each policy must be satisﬁed by the other party for a successful implicit
attestation when requesting integrity-protected data from a virtualized device.
In the following sections, we ﬁrst deﬁne the notations and the cryptographic
keys in Sects. 5.1 and 5.2. After that, we specify the setup phase in Sect. 5.3 and
then focus on the integrity protection and attestation mechanism in Sect. 5.4.
5.1
Notation
In general, a hash function H can compress arbitrary-length input to an output
with length l, which depends on a speciﬁc algorithm, that is H : {0, 1}∗→{0, 1}l.
A cryptographic hash function is such a one-way hash function with collision and
pre-image resistance, which both describe additional security properties.
A message authentication code (MAC) is a cryptographic value, which can
be calculated based on a cryptographic hash function and a shared symmetric
key, and used to verify the authenticity and integrity of a message. Formally,
a MAC algorithm is a function that generates a message digest d with ﬁxed
length l for a secret key K and a given input m with virtually arbitrary size
as MAC(K, m) = d = {0, 1}l. As an example for a hash-based MAC, a HMAC

312
S. Wagner and C. Eckert
calculates a message authentication digest for data m based on a key symmetric
key K as HMAC

K, m

= H((K ⊕opad) || H((K ⊕ipad) || m)), where || denotes
a concatenation, ⊕the exclusive or, opad the outer and ipad the inner padding.
Cryptographic hash functions are also used to measure the integrity of soft-
ware components before they are loaded. We assume that the load-time integrity
of a microkernel-based system can be adequately described by a set of measure-
ment values, which are securely stored in the PCRs of a TPM. Such a set of PCR
values are referred to as platform conﬁguration PC := (PCR[i1], . . . , PCR[ik]),
where i ∈{0 . . . r−1}, k ≤r, and r is the number of available PCRs.
To store an integrity measurement value µ in a PCR with index i, the cur-
rent value inside the TPM is combined with the new measurement value using
PCR Extend(PCR[i], µ), which is speciﬁed as PCR[i] ←H(PCR[i] || µ). For the
sake of simplicity, we assume that the PCR values of our microkernel-based
system are public and, hence, known to a veriﬁer in advance.
Similar to integrity measurements, a policy P is represented as a crypto-
graphic hash, which can be used to authorize TPM operations. For example, to
use a key K for signing, the initial empty hash P0 is extended with a command
code for signing, i.e., Psign ←H(P0 || TPM CC PolicyCommandCode || TPM2 Sign).
If the TPM can verify that the resulting policy hash Psign matches the policy
hash P ′ assigned to a key K, this key can be used according to that speciﬁc
policy. Since multiple policies can also be combined, e.g., with TPM2 PolicyOR,
it is possible to create larger, more complex, yet ﬂexible policies.
5.2
Cryptographic Keys
For our data integrity protection and attestation protocol, we assume that a
primary storage key KPSK, which is created from a TPM primary seed PS, and
a storage key KSK already exist in the prover’s TPM. As shown in Fig. 2, we
only require for the sake of simplicity that KSK can be loaded with a policy, i.e.,
PKSK ←H(P0 || TPM CC PolicyCommandCode || TPM2 Load).
In addition to the storage key KSK, the prover needs a second key Kp, which
acts as a intermediate parent key for the integrity key Kint. The key allows for a
policy-based import of child keys, which is encoded by specifying the command
code TPM2 Import as shown in Fig. 2. Like KSK, this key is bound to the prover’s
TPM as well as its dedicated parent, which is enforced by the TPM though the
key attributes fixedTPM and fixedParent. The second policy attached to Kp is
used in our protocol to control access to hardware devices, such as sensors, by
binding the content of NV memory areas to the load command. More precisely,
if a bit in NV1 is set, the hardware component assigned to this bit is enabled,
which is a requirement to access it. In NV2, a minimum threshold, e.g., for the
number of sensor values used in a average function or a anomaly detection score,
can be stored. That way, the administrator of the prover’s system can restrict
access to a device if the number of access requests stored in NV2 do not match
the speciﬁed threshold of the policy or the pattern indicates malicious behavior.
Finally, we specify a keyed hash key Kint, which is used to symmetrically
sign data with a HMAC to protect the integrity on behalf of the veriﬁer.

Policy-Based Implicit Attestation for Microkernel-Based Virtualized Systems
313
Fig. 2. Cryptographic key hierarchy for our policy-based implicit attestation protocol
This key is initially created by the veriﬁer’s TPM and migrated to prover’s
system during the setup phase of our protocol. Consequently, the veriﬁer alone
is able to deﬁne the policies that have to be satisﬁed in order to be able to use
the key as indicated in Fig. 2. For our protocol, we at least require that the key
allows for the calculation of a HMAC if the current PCRs values, which must
include the microkernel-based system, i.e., core, init, tzvmm, etc., match the
speciﬁed ones of a trusted platform conﬁguration PCt. For the sake of simplic-
ity, we also specify a policy-based authorization for a duplication, which is the
TPM 2.0 term for migration, from the veriﬁer’s to the prover’s TPM. Since we
encrypt Kint during the duplication, we optionally deﬁne a decryption key Kdec,
which can be used to decrypt the AES encryption key Kaes. However, please
note that this encryption and decryption step is only required once during the
setup phase.
5.3
Phase 1: Setup
In the setup phase of our protocol, the administrator of the prover’s system ﬁrst
creates two non-volatile memory areas, NV1 and NV2, inside the TPM using
TPM2 NV DefineSpace. The ﬁrst NV location is used to enable access to a device,
whereas the second one allows for a more ﬁne grained access control. Both NV
areas can be read with a policy PNV Read and used with TPM CC PolicyNV.

314
S. Wagner and C. Eckert
In NV1, the administrator can set certain bits to enable the corresponding
hardware components. For instance, if bit 0 of NV1 is assigned to a device with
an index 0, the administrator can set this bit to 1 in order to enable access to
the device. Additionally, the administrator can use NV2 in a policy to specify
a minimum granularity or threshold. For example, if we assume the device is a
sensor and the veriﬁer should only be able to read the average value of at least
n sensor values, the current value in NV2 could be compared to the reference
value n speciﬁed in the policy. NV2 could also be used to store the result of
an anomaly detection algorithm, which needs to be below a certain threshold t
deﬁned in a policy to be able to load and use the key Kint.
After setting up the non-volatile memory locations, which are used as part
of the policies for Kp, the storage key KSK is loaded using the policy of KPSK.
Then, Kp is created with the policy described in the previous section, which is
PKp = H(Pbase || TPM CC PolicyOR || PImport || PLoad NVs),
(1)
where
PImport = H(P0 || TPM CC PolicyCommandCode || TPM CC Import),
PLoad NVs = H(PNVs || TPM CC PolicyCommandCode || TPM CC Load),
(2)
and Pbase is either PImport or PLoad NVs.
The value PNVs, in turn, is calculated based on the following equations, which
use a cryptographic hash value generated during the initialization of the NV
locations as the respective name of NV1 and NV2:
PNV1 = H(P0 || TPM CC PolicyNV || args || nvIndex →Name)
(3)
PNVs = H(PNV1 || TPM CC PolicyNV || args || nvIndex →Name)
(4)
with
args = H(operandB.buﬀer || oﬀset || operation)
where operandB is the value used for the comparison, oﬀset is the start value of
the NV data, and operation is the type of comparison. For NV1, the operation
is (A&B) = B, which checks that all bits in B are set in A, while the operation
for NV2 is A ≥B, which enables the prover to specify a minimum value.
Once the policy PKp is successfully generated, the key Kp is created with
TPM2 Create, which calculates a new (ordinary) key. For this command, a public
template speciﬁes the properties of the key to be generated by the TPM, e.g.,
the type of key and the associated policy. The command returns the public and
encrypted private key as well as data about the creation, which can be certiﬁed.
When the intermediate parent key Kp was created, an asymmetric decryption
key pair Kdec can be optionally generated using TPM2 Create while the storage
key KSK is still loaded. Like Kp, this key is also attached with a combined policy,
which allows for loading and decryption:
PKdec = H(Pbase || TPM CC PolicyOR || PLoad || PDecrypt),
(5)

Policy-Based Implicit Attestation for Microkernel-Based Virtualized Systems
315
where
PLoad = H(P0 || TPM CC PolicyCommandCode || TPM CC Load),
PDecrypt = H(P0 || TPM CC PolicyCommandCode || TPM CC Decrypt),
and Pbase is either PLoad or PDecrypt.
As described in the previous section, this asymmetric key is only used to
securely transfer an AES key, which is used to encrypt V’s integrity key Kint,
from the veriﬁer to the prover. This optional step only executed once and, hence,
has no signiﬁcant impact on our protocol.
On the veriﬁer’s system, V generates the keyed hash key Kint as part of the
setup process. This key is a symmetric signing key, which can be migrated to a
new TPM (fixedTPM is CLEAR) and is cryptographically bound to the integrity
measurements of the prover’s microkernel-based system. As a result, it can only
be loaded if the current values of the PCRs match the ones speciﬁed by the
veriﬁer. The policy for this key, which—for the sake of simplicity—allows for a
duplication without a strong authentication, is calculated as
PKint = H(Pbase || TPM CC PolicyOR || PDup || PPCR HMAC),
(6)
with
PDup = H(P0 || TPM CC PolicyCommandCode || TPM CC Duplicate)
and Pbase is either PDup or PPCR HMAC. The value PPCR HMAC, in turn, is
calculated by the TPM as
PPCR = H(P0 || TPM CC PolicyPCR || pcrs || digestTPM)
PPCR HMAC = H(PPCR || TPM CC PolicyCC || TPM CC HMAC Start)
where pcrs is a structure specifying the bits corresponding to the PCRs and
digestTPM is the digest of the selected PCRs provided by the veriﬁer using a
so-called trial session. This type of session allows for specifying the expected
PCR values, whereas in a non-trial session the TPM would use the internal
PCR values to calculate the digest.
Once the policy PKint is successfully generated, the key Kint can be created
using the command TPM2 Create. Again, the policy and key type (keyed hash
key) can be speciﬁed in the public template, which is used by the TPM to
create a key accordingly. To duplicate or migrate the keyed hash key Kint to
the prover, the TPM cryptographically binds the key to its new parent key Kp,
whose integrity and authenticity can be veriﬁed using the certiﬁed creation data
produced by the prover’s TPM, when Kp was created. For the actual duplication
of Kint to the prover’s system, the veriﬁer runs TPM2 Duplicate with an optional
AES encryption key and the public portion of the storage key Kp as input. Note
that this implicitly also restricts the duplication to the prover’s TPM, since the
attribute fixedTPM is SET for Kp. The result of TPM2 Duplicate is an AES-
encrypted key structure, which includes all necessary information to import the

316
S. Wagner and C. Eckert
key into the target TPM. To complete the migration of Kint, the AES key Kaes
is encrypted with the public portion of Kdec and sent to the prover together with
the encrypted Kint.
On the prover’s system the AES encryption key is decrypted using the private
portion of Kdec and, in turn, used to decrypt Kint while it is imported to its
new parent Kp. Please note that since all of those commands, TPM2 Duplicate,
TPM2 Decrypt, and TPM2 Import, are part of the policy of their respective keys,
this process does not require any interactive authorization by an administrator.
5.4
Phase 2: Data Integrity Protection with Implicit Attestation
In this section, we describe our data integrity protection and attestation protocol,
which implicitly creates veriﬁable proof that P’s system is still in a trustworthy
state while protecting the integrity of data from a virtualized device for V.
To read data from a device, such as a sensor, which is virtualized by the
microkernel-based system, the veriﬁer V ﬁrst conﬁgures the device through a
mechanism provided by the rich operating system, e.g., ioctl. This conﬁguration
includes the setting of the granularity or threshold n and a nonceV. However,
since the prover P does not allow the kernel or device drivers of the rich operating
system to conﬁgure the device directly, the operation is trapped to the hypervisor
in the Non-secure World as shown in the top half of Fig. 3 above the dashed line.
The hypervisor, in turn, evaluates the conﬁguration—in particular, the value of
the granularity n—which is used, for example, in an average function or simply
to limit access to the hardware device. If the conﬁguration is valid and matches
the criteria set by the prover’s administrator, the hypervisor forwards the request
to the secure device driver implemented in the Secure World, which is able to
conﬁgure the physical hardware device. In parallel, the VMM in TrustZone stores
the granularity n in NV2, which is a critical step for using the key Kp and part
of a correct behavior of tzvmm that is assumed to be reﬂected in the PCRs.
After the conﬁguration, the client application in the rich operating system
can read data from the device, which is, again, trapped and forwarded to the
secure device driver in the Secure World. For a sensor, the device driver in the
Secure World then reads the necessary sensor values based on the granularity n
and, for example, calculates average. To protect the integrity of the result, the
TrustZone VMM then requests the TPM to calculate a HMAC mac as
mac = HMAC

Kint, (nonceV || data)

.
(7)
This is only possible if the keyed hash key Kint is loaded and the PCRs of the
microkernel-based systems, i.e., core, init, tzvmm, vmm, etc., match the speciﬁed
values. Kint, however, can only be loaded under the parent key Kp, if the device
is enabled in NV1 and the granularity n stored in NV2 is above the threshold
speciﬁed by the prover’s administrator in the policy PNV1, PNVs, and PKp.
Consequently, the prover needs to re-create the policy PKp (cf. Eq. 1) in a
policy session inside the TPM to be able to load the key Kint on behalf of the
veriﬁer. More precisely, P has to calculate PLoad NVs (cf. Eq. 2), which is only

Policy-Based Implicit Attestation for Microkernel-Based Virtualized Systems
317
Fig. 3. Data access with policy-based implicit attestation
possible if the values in the NV indices satisfy the respective policies that are
calculated as follows:
P ′
NV1 = H(P0 || TPM CC PolicyNV || args || nvIndex →Name)
with:
args = H(operandB.buﬀer || oﬀset || operation),
where operandB is the value in NV1, oﬀset is the start value (0), and operation
is the type of comparison, i.e., (A&B) = B for NV1 in our example. This policy
veriﬁes that the device is enabled. If the comparison returns true, P ′
NV1 equals
PNV1, which means the device is enabled. The policy P ′
NVs can then be calculated
as
P ′
NVs = H(P ′
NV1 || TPM CC PolicyNV || args || nvIndex →Name)
with:
args = H(operandB.buﬀer || oﬀset || operation),
where operandB is the value n in NV2, oﬀset is the start value (0), and oper-
ation is a comparison of A ≥B, that is n ≥nPNVs. This policy checks that the

318
S. Wagner and C. Eckert
granularity or threshold n is above the value speciﬁed in the policy PNVs. Again,
if the comparison returns true, P ′
NVs equals PNVs and the device access pattern
is accepted. Based on P ′
NVs, the policy P ′
Load NVs can be calculated as
P ′
Load NVs = H(P ′
NVs || TPM CC PolicyCommandCode || TPM CC Load).
If P ′
Load NVs equals PLoad NVs, this policy can satisfy the OR-policy PKp as
speciﬁed in (1), which enables the prover to load Kint. The prover only has to
combine P ′
Load NVs with the pre-calculated value of PImport using TPM2 PolicyOR
to generate P ′
Kp. By specifying the session with the freshly generated policy P ′
Kp,
which should be equal to PKp, the prover is able to load the key Kint on behalf
of the veriﬁer.
To use Kint to protect the integrity of the device data and implicitly verify
the integrity of the system, the prover simply has to re-create the policy PKint.
By creating a new policy session inside the TPM and using TPM2 PolicyPCR,
the prover ﬁrst creates PPCR as
P ′
PCR = H(P0 || TPM CC PolicyPCR || pcrs || digestTPM).
For this policy, the PCRs of the microkernel-based system, which are usually
stored in one of the lower PCRs, must be speciﬁed. We assume that the prover
and the veriﬁer agree on the selection of PCRs, since both aim for a success-
ful attestation. The policy P ′
PCR is then used in TPM2 PolicyCommandCode to
calculate P ′
PCR HMAC, which is combined with the pre-calculated value PDup to
generate P ′
Kint:
P ′
PCR HMAC = H(P ′
PCR || TPM CC PolicyCC || TPM CC HMAC Start)
P ′
Kint = H(P ′
PCR HMAC || TPM CC PolicyOR || PDup || P ′
PCR HMAC).
If P ′
Kint equals PKint, the policy can ﬁnally be used to create the HMAC mac
over data and nonceV, which is used to prove freshness, as described in (7).
The data and the HMAC mac are then returned to the rich operating system.
The HMAC-protected data can then be transferred to the veriﬁer’s system, where
a fresh HMAC mac′ can be generated with the veriﬁer’s Kint as
mac′ = HMAC

Kint, (nonceV || data)

.
(8)
If the freshly generated HMAC mac′ matches the HMAC mac from the prover
and nonceV is the expected nonce, the veriﬁer does not only know that the data
has not been modiﬁed, but also that the prover’s system is still trustworthy.
6
Implementation
In this section, we present details about our proof-of-concept implementation,
which we realized on an Arndale board. This development board, which is a so-
called single-board computer, features an Exynos 5250 SoC with a Cortex-A15
MPCore [4] that includes both ARM’s Virtualization and Security Extensions.

Policy-Based Implicit Attestation for Microkernel-Based Virtualized Systems
319
The main components of our prototype comprise a microkernel-based system
in TrustZone, which includes a TPM 2.0 simulator (tpm2sim) as native micro-
kernel tasks, and a similar system in the Non-secure World, which acts as a
hypervisor and virtualizes a rich operating system.
As our microkernel-based system, we employ the bare-metal kernel of the
Genode [10] base-hw project, which combines Genode’s core component with a
small kernel library, only has a size of about 17 thousand of lines of code (KLOC)
and supports TrustZone as well as virtualization. On top of the microkernel, we
use regular Genode user-space components, such as init, which is about 3 KLOC,
and the virtual machine monitors, vmm in the Non-secure World and tzvmm in
the Secure World. For our protocol, we mainly adapted the trapping and the
World Switch mechanism to be able to transfer data to and from the virtual
machine monitors in Non-secure World and in TrustZone.
In the Secure World, we have extended the virtual machine monitor tzvmm
to handle requests from the Non-secure World and also be able to execute the
appropriate TPM commands. Since we did not have a hardware TPM 2.0 when
we started the implementation, we created a Python script to extract a working
simulator from the public PDF version of the TPM 2.0 Library Speciﬁcation,
which we have made open source. However, since the code of the speciﬁcation is
primarily written for Windows, we had to port the code to Genode to be able
to run the simulator as a native microkernel task. This included modiﬁcations
to the random number generation, the NV memory subsystem, and the commu-
nication, which was socket-based and uses inter-process communication (IPC)
with a shared memory area for the commands and responses in our Genode port.
For our rich operating system, which is a conventional unmodiﬁed Linux 4.0
kernel with a BusyBox [2], we created a device client application and a kernel
module to implement the device driver for a hardware component. In our proto-
type, this device driver, which would normally conﬁgure and access the hardware
component directly, is trapped to the hypervisor of the Non-secure World. To be
able to transfer data from the rich operating system to the hypervisor, we added
Table 1. Code size of relevant native components (calculated with cloc [7])

320
S. Wagner and C. Eckert
a memory trap to the hypervisor conﬁguration and additionally implemented an
smc-based World Switch, which uses shared memory locations to transfer data
to and from the TrustZone.
To put our prototype implementation in perspective, the modiﬁcations to the
existing Genode components, such as core, tzvmm, or vmm, only amount to a few
hundred lines of code per component as shown in Table 1. The reason for that is
the fact that most of the protocol is handled by the TPM 2.0 simulator, which
we extracted from the speciﬁcation document and has about 40 KLOC, and the
virtual machine monitors in the Secure and Non-secure World. The rest of the
system uses mechanisms, such as IPC and shared memory, which are already
part of the microkernel-based system provided by Genode.
7
Security Discussion
In this security discussion, we analyze the key security aspects of our protocol.
Since our proposed protocol combines data access and integrity protection with
an implicit attestation, we ﬁrst focus on the integrity of the data, which is
transmitted from the prover to the veriﬁer. After that, we discuss the security
of our policy-based implicit attestation mechanism in detail.
To protect the integrity, the prover’s TPM calculates a message authenti-
cation code over the data, e.g., from a sensor, using the shared HMAC key
Kint. This key, which is created and controlled by the veriﬁer, is encrypted and
migrated from the veriﬁer’s TPM to the prover’s TPM during the setup phase
and can only be used inside those respective TPMs. That way, an attacker is
not able to easily forge a correct HMAC for data with unauthorized modiﬁca-
tion, because it is not able to intercept the HMAC key, decrypt it, and use it
in an arbitrary TPM. In addition, please note that the identity of the prover is
implicitly included in the HMAC if the veriﬁer creates a distinct key for each
prover. More precisely, since the HMAC key, which has been created for a par-
ticular prover, is duplicated specifying the public key of that prover’s Kp, the
HMAC key is cryptographically bound to the identity of that prover and its
TPM. Similarly, the AES encryption key, which is used during duplication, is
also cryptographically bound to the prover’s Kdec, which is ﬁxed to the prover’s
TPM and, thus, cannot be migrated to an arbitrary TPM.
Furthermore, our protocol includes a nonce for freshness, which has to be
checked by the veriﬁer to make sure that the HMAC has been generated for
most recent request. For an adversary, this eliminates the possibility to replay
old data, which has been protected with a correct HMAC, but for data that
is potentially no longer valid. This is particularly relevant for devices, such as
a heartbeat sensor, where the veriﬁer must be able to detect a replay attack,
where an attacker (or even the prover) might try to convince the veriﬁer that
the system still functions without any downtime or anomalies.
In order to create an correct HMAC on behalf of the veriﬁer, the prover
needs to load the HMAC key Kint, which is implicitly used for an attestation.
In our attestation mechanism, the veriﬁer creates and controls the key Kint,

Policy-Based Implicit Attestation for Microkernel-Based Virtualized Systems
321
which means the veriﬁer is able to deﬁne the policies, which have to be satisﬁed
by the prover. That allows the veriﬁer to specify, for example, the exact PCR
values, which we assume reﬂect a known and trusted platform conﬁguration,
particularly of the microkernel-based systems in Secure and Non-secure World.
Since the authentication value (authValue) for the key is only known to the
veriﬁer, the prover cannot change this policy later. However, the prover is able
to deﬁne policies for the parent key Kp, which enables the prover to restrict
access to certain data sources.
During the attestation, the prover’s policies are evaluated ﬁrst. If the policies
cannot be satisﬁed, the veriﬁer does not get access to a device, which eﬀectively
allows the prover to limit access to devices. However, if the prover’s policies can
be met, the policies deﬁned by the veriﬁer, which include at least a trusted set of
PCR values, are evaluated before the HMAC for the device data is calculated. At
this point, it is important to note that the policies are not veriﬁed by the operat-
ing system, as it is usually the case in policy-based authorization schemes. In our
protocol, the policies are instead veriﬁed by the TPM, which also moves the point
of enforcement inside the TPM. Consequently, a successful attestation is only pos-
sible if the TPM ensure that policies are satisﬁed, which means, for example, that
the prover’s system is in a trustworthy state as reﬂected by the PCRs. If the prover
or any attacker has modiﬁed the system, the ﬁnal policy of the veriﬁer cannot be
met and the prover is not able to load the HMAC key Kint on behalf of the veriﬁer.
As a result, the prover cannot protect the integrity of the data and the attestation
eventually fails, because the veriﬁer does not receive a fresh and valid HMAC.
8
Conclusion
In this paper, we presented a policy-based implicit attestation protocol, which
does not rely on expensive asymmetric cryptographic operations traditionally
required in a remote attestation. Instead, our attestation mechanism mainly
uses hash-based message authentication codes and policies that are enforced by
a TPM. In particular, our policy-based approach enables the veriﬁer to create a
key, which is used for integrity protection, and cryptographically bind a policy,
which speciﬁes the characteristics of a trustworthy system, to that key. For a
successful attestation, the prover is expected to use that key to protect the
integrity of the requested data from a virtualized hardware component, such as
a sensor. Consequently, the veriﬁer can implicitly evaluate the trustworthiness of
the prover’s system, whenever it accesses a virtualized device and the requested
data is protected with the key, which has been bound to an attestation policy.
As a result, our approach enables the veriﬁer to not only ensure that the data
requested from a virtualized device has not been modiﬁed, but also to implicitly
verify the integrity and trustworthiness of the prover’s system.
Acknowledgments. Parts of this work were funded by the Industrial Data Space
project (GN: 01IS15054) of the German Federal Ministry of Education and Research.
We also like to thank Sergej Proskurin and Tamas Bakos for contributing to our pro-
totype and to the TPM 2.0 Simulator Extraction Script.

322
S. Wagner and C. Eckert
References
1. Alsouri, S., Dagdelen, ¨O., Katzenbeisser, S.: Group-based attestation: enhanc-
ing privacy and management in remote attestation. In: Acquisti, A., Smith,
S.W., Sadeghi, A.-R. (eds.) TRUST 2010. LNCS, vol. 6101, pp. 63–77. Springer,
Heidelberg (2010)
2. Andersen, E., Landley, R., Vlasenko, D., et al.: Busybox. https://busybox.net
3. ARM Ltd.: Virtualization extensions architecture speciﬁcation (2010). http://
infocenter.arm.com
4. ARM Ltd.: ARM Cortex-A15 technical reference manual. ARM DDI 0438C, Sep-
tember 2011
5. ARM Ltd.: ARM architecture reference manual. ARMv7-A and ARMv7-R edition.
ARM DDI 0406C.b, July 2012
6. Brickell, E., Camenisch, J., Chen, L.: Direct anonymous attestation. In: Proceed-
ings of the 11th ACM Conference on Computer and Communications Security, CCS
2004, pp. 132–145. ACM, New York (2004). http://doi.acm.org/10.1145/1030083.
1030103
7. Danial, A.: CLOC - Count Lines of Code. Version 1.67. https://github.com/
AlDanial/cloc
8. Haldar, V., Chandra, D., Franz, M.: Semantic remote attestation: a virtual machine
directed approach to trusted computing. In: Proceedings of the 3rd Conference on
Virtual Machine Research and Technology Symposium, Berkeley, CA, USA (2004)
9. Krawczyk, H., Rabin, T.: Chameleon hashing and signatures. IACR Cryptology
ePrint Archive (1998)
10. Genode Labs. http://www.genode.org
11. Liedtke, J.: Microkernels must and can be small. In: Proceedings of the 5th IEEE
International Workshop on Object-Orientation in Operating Systems (IWOOOS).
Seattle, WA, October 1996. http://l4ka.org/publications/
12. Neiger, G., Santoni, A., Leung, F., Rodgers, D., Uhlig, R.: Intel virtualization
technology: hardware support for eﬃcient processor virtualization. Intel Technol.
J. 10(3), 167–177 (2006)
13. Sadeghi, A.R., St¨uble, C.: Property-based attestation for computing platforms:
caring about properties, not mechanisms. In: Proceedings of the 2004 Workshop
on New Security Paradigms, NSPW 2004, pp. 67–77. ACM, New York (2004)
14. Sailer, R., Zhang, X., Jaeger, T., van Doorn, L.: Design and implementation of a
TCG-based integrity measurement architecture. In: Proceedings of the 13th Con-
ference on USENIX Security Symposium, vol. 13, Berkeley, CA, USA (2004)
15. Sirer, E.G., de Bruijn, W., Reynolds, P., Shieh, A., Walsh, K., Williams, D., Schnei-
der, F.B.: Logical attestation: an authorization architecture for trustworthy com-
puting. In: Proceedings of the Twenty-Third ACM Symposium on Operating Sys-
tems Principles, SOSP 2011, pp. 249–264. ACM, New York (2011)
16. Trusted Computing Group (TCG): TPM Main Speciﬁcation Version 1.2 rev. 116.
http://www.trustedcomputinggroup.org/resources/tpm main speciﬁcation
17. Trusted Computing Group (TCG): Trusted Platform Module Library Speciﬁcation.
Family “2.0”. Level 00, Revision 01.16. http://www.trustedcomputinggroup.org/
resources/tpm library speciﬁcation
18. TU Dresden OS Group: L4/Fiasco.OC. http://os.inf.tu-dresden.de/ﬁasco/
19. Wagner, S., Proskurin, S., Bakos, T.: TPM 2.0 Simulator Extraction Script (2016).
https://github.com/stwagnr/tpm2simulator

Generalized Dynamic Opaque Predicates:
A New Control Flow Obfuscation Method
Dongpeng Xu, Jiang Ming, and Dinghao Wu(B)
College of Information Sciences and Technology,
The Pennsylvania State University, University Park, PA 16802, USA
{dux103,jum310,dwu}@ist.psu.edu
Abstract. Opaque predicate obfuscation, a low-cost and stealthy con-
trol ﬂow obfuscation method to introduce superﬂuous branches, has
been demonstrated to be eﬀective to impede reverse engineering eﬀorts
and broadly used in various areas of software security. Conventional
opaque predicates typically rely on the invariant property of well-known
number theoretic theorems, making them easy to be detected by the
dynamic testing and formal semantics techniques. To address this limi-
tation, previous work has introduced the idea of dynamic opaque predi-
cates, whose values may vary in diﬀerent runs. However, the systematical
design and evaluation of dynamic opaque predicates are far from mature.
In this paper, we generalize the concept and systematically develop a
new control ﬂow obfuscation scheme called generalized dynamic opaque
predicates. Compared to the previous work, our approach has two dis-
tinct advantages: (1) We extend the application scope by automatically
transforming more common program structures (e.g., straight-line code,
branch, and loop) into dynamic opaque predicates; (2) Our system design
does not require that dynamic opaque predicates to be strictly adjacent,
which is more resilient to the deobfuscation techniques. We have devel-
oped a prototype tool based on LLVM IR and evaluated it by obfuscat-
ing the GNU core utilities. Our experimental results show the eﬃcacy
and generality of our method. In addition, the comparative evaluation
demonstrates that our method is resilient to the latest formal program
semantics-based opaque predicate detection method.
Keywords: Software protection · Obfuscation · Opaque predicate ·
Control ﬂow obfuscation
1
Introduction
Predicates are conditional expressions that evaluate to true or false. An opaque
predicate means its value are known to the obfuscator at obfuscation time, but
it is diﬃcult for an attacker to ﬁgure it out afterwards. Used together with
junk code, the eﬀect of opaque predicates results in a heavily cluttered con-
trol ﬂow graph with redundant infeasible paths. Therefore, any further analysis
based on the control ﬂow graph will turn into arduous work. Compared with
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 323–342, 2016.
DOI: 10.1007/978-3-319-45871-7 20

324
D. Xu et al.
other control ﬂow graph obfuscation methods such as control ﬂow ﬂattening [26]
and call stack tampering [24], opaque predicates are more stealthy because it
is diﬃcult to diﬀerentiate opaque predicates from original path conditions in
binary code [4,5]. Also, another beneﬁt of opaque predicates is they have a small
impact on the runtime performance and code size. First proposed by Collberg
et al. [6], opaque predicates have been applied widely in various ways, such as
software diversiﬁcation [10,15], metamorphic malware mutation [2,3], software
watermarking [1,21], and Android Apps obfuscation [14]. Due to the low-cost and
stealthy properties, most real-world obfuscation toolkits have supported insert-
ing opaque predicates into a program, through link-time program rewriting or
binary rewriting [8,13,18].
On the other hand, opaque predicate detection has attracted many secu-
rity researchers’ attention. Plenty of approaches have been proposed to identify
opaque predicates inside programs. For instance, Preda et al. [23], Madou [17]
and Udupa et al. [25] did research on opaque predicate detection based on the fact
that the value of opaque predicate doesn’t change during multiple executions.
The invariant property of those “static” opaque predicates leads to the fact that
they are likely to be detected by program analysis tools. Furthermore, recent
research work [19] shows that even dynamic opaque predicate, which is more
complicated and advanced than traditional static opaque predicates, can also
be detected by their deobfuscation tool. Dynamic opaque predicates overcome
the invariant weakness of static opaque predicates by using a set of correlated
predicates. The authors claims that they can detect static and dynamic opaque
predicates inside an execution binary trace.
Essentially, existing opaque predicates detection techniques utilize several
weaknesses of opaque predicates. First, as mentioned above, the invariant prop-
erty of traditional algebraic based opaque predicates reveals their existence.
Second, the design of dynamic opaque predicate is far from mature. Existing
technique can only insert dynamic opaque predicates into a piece of straight-line
code. It cannot spread dynamic opaque predicates across branch conditions. This
limitation leads to the consequence that all predicates constituting a dynamic
opaque predicate are adjacent, which is utilized by advanced opaque predicates
detection tools such as LOOP [19].
In order to overcome the limitations of current opaque predicates, we present
a systematic design of a novel control ﬂow obfuscation method, Generalized
Dynamic Opaque Predicates, which is able to inject diversiﬁed dynamic opaque
predicates into complicated program structures such as branch and loop. Being
compared with the previous technique which can only insert dynamic opaque
predicates into straight-line program, our new method is more resilient to pro-
gram analysis tools. We have implemented a prototype tool based on the LLVM
compiler infrastructure [16]. The tool ﬁrst performs ﬁne-grained data ﬂow analy-
sis to search possible insertion locations. After that it automatically transforms
common program structures to construct dynamic opaque predicates. We have
tested and evaluated the tool by obfuscating several hot functions of GNU core
utilities with diﬀerent obfuscation levels. The experimental results show that our

Generalized Dynamic Opaque Predicates
325
method is eﬀective and general in control ﬂow obfuscation. Besides, we demon-
strate that our obfuscation can defeat the commercial binary diﬀerence analy-
sis tools and the state-of-the-art formal program semantics-based deobfuscation
methods. The performance data indicate that our proposed obfuscation only
introduces negligible overhead.
In summary, we make the following contributions.
– First, we propose an eﬀective and generalized opaque predicate obfuscation
method. Our method outperforms existing work by automatically inserting
opaque predicates into more general program structures like branches and
loops, whereas previous work can only work on straight-line code.
– Second, we demonstrate our obfuscation is very resilient to the state-of-art
opaque predicate detection tool.
– Third, we have implemented our method on top of LLVM and the source code
is available.
The rest of the paper is organized as follows. Section 2 introduces the related
work on opaque predicates and state-of-the-art opaque predicate detection meth-
ods. Section 3 presents our new obfuscation method, generalized dynamic opaque
predicates in detail. Section 4 presents our implementation details. We evaluate
our method in Sect. 5 and conclude the paper in Sect. 6.
2
Related Work
In this section, we ﬁrst introduce the related work on static and dynamic opaque
predicates. Then we discuss the drawbacks of current opaque predicate detec-
tion methods, which also inspires us to propose the generalized dynamic opaque
predicates.
2.1
Static Opaque Predicates
Static opaque predicates indicates the opaque predicates whose value is ﬁxed
during runtime. Basically, there are two categories of static opaque predicates:
invariant opaque predicates and contextual opaque predicates. According to pre-
vious research work [19], invariant opaque predicates refer to those predicates
whose value always evaluates to true or false for all possible inputs. The predicate
is opaque since it is diﬃcult to know the value in advance except the obfuscator.
Usually invariant opaque predicates are constructed by utilizing some algebraic
theorems [21] or quadratic residues [1] as follows.
∀x ∈Z. (4x2 + 4) mod 19 ̸≡0
As a result of its simplicity, there are large numbers of invariant opaque
predicates candidates. On the other hand, the invariant feature also leads to
the shortage of this category of opaque predicates. One possible way to detect

326
D. Xu et al.
invariant opaque predicates is to observe the branches that never change at run
time with fuzzing testing [17].
The other kind of static opaque predicates is contextual opaque predicate. It
is proposed by Drape [11] to avoid an opaque predicate always produces the ﬁxed
value for all inputs. Contextual opaque predicate only evaluates to always true
or false under a given precondition. Typically this kind of opaque predicate is
an implication relation between two predicates, which is elaborately constructed
in a particular program context. An example of contextual opaque predicates is
presented as follows.
∀x ∈Z. (7x −5) mod 3 ≡0 ⇒(28x2 −13x −5) mod 9 ≡0
In this example, the predicate (28x2 −13x −5) mod 9 ≡0 is always true
given (7x −5) mod 3 ≡0 and x is an integer. In addition, the constant value
in contextual opaque predicates can be further obfuscated so as to hide the
implication relation [20].
2.2
Conventional Dynamic Opaque Predicates
Palsberg et al. [22] ﬁrst introduce the concept of dynamic opaque predicates,
which consist of a family of correlated predicates that all present the same value
in one given execution, but the value may be changed in another execution. Thus
the values of the dynamic opaque predicates switch dynamically at run time.
Here we use the term “conventional dynamic opaque predicates” to distinguish
it from the generalized dynamic opaque predicates we proposed in this paper.
Particularly, since its design is still immature although the concept is noval,
conventional dynamic opaque predicate can only be injected into straight-line
programs, which results in that all predicates are set adjacently. We provide a
conventional dynamic opaque predicate example as follows.
1
2
3
1
2
1
3
2
3
(a)
(b)
p
q
T
F
T
F
Fig. 1. An example of conventional dynamic opaque predicates.

Generalized Dynamic Opaque Predicates
327
Figure 1(a) shows the original straight-line code and Fig. 1(b) shows the
obfuscated version using conventional dynamic opaque predicates. In this paper,
we use a rectangle to represent a basic block and the numbers inside to indicate
instructions. The small circles represent predicates. Section 3.2 provides more
detailed description of those symbols. In Fig. 1(b), p and q are two correlated
predicates. They are evaluated to both true or false in any given run. In the orig-
inal program as shown in Fig. 1(a), three instructions are executed one by one:
[1 2 3]. In the obfuscated version, each execution either follows all left branches
(p∧q holds) or all right branches (¬p∧¬q holds). The same instruction sequence
is executed in both cases: [1 2]->[3] when taking the left branches and
[1]->[2 3] vice versa. Since the predicate q split the two paths into diﬀerent
segments, p and q have to be adjacent to maintain the semantic equivalence.
2.3
Opaque Predicate Detection
Collberg et al. [6] ﬁrst propose the idea of opaque predicates to prevent malicious
reverse engineering attempts. In addition, the authors also provide some ad-
hoc detection methods, such as “statistical analysis”. This approach utilize the
assumption that, if a predicate that always produces the same result over a
larger number of test cases, it is likely to be an opaque predicate. Due to the low
coverage of inputs, statistical analysis could lead to high false positive rates.
Preda et al. [23] propose to detect opaque predicates by another method
called abstract interpretation. However, their approach can only handle a speciﬁc
type of known invariant opaque predicates. Madou [17] ﬁrst identiﬁes candidate
branches that never changes at run time, and then veriﬁes such predicates by
fuzz testing with a considerably high error rate. Furthermore, Udupa et al. [25]
utilize static path feasibility analysis to determine whether an execution path
is feasible. Note that their approaches are still based on detection of invariant
features such as infeasible branches, so they cannot detect the dynamic opaque
predicates.
Currently, the state-of-the-art work on opaque predicate detection is LOOP
[19], a logic oriented opaque predicate detection tool for obfuscated binary code.
The authors propose an approach based on symbolic execution and theorem
proving techniques to automatically detect static and dynamic opaque pred-
icates. When detecting invariant opaque predicates, LOOP perform symbolic
execution on an execution trace and check whether one branch condition is
always true or false. Furthermore, it runs an logic implication check to decide
whether one predicate is a contextual opaque predicate.
Particularly, LOOP is also able to detect the conventional dynamic opaque
predicate. The detection is based on the fact that each predicate in a dynamic
opaque predicates is semantically equivalent, or in another word, they implies
each other logically, such as p and q in Fig. 1(b). Therefore, LOOP performs
two implication check on two adjacent predicates. One is on the execution trace
and the other is on the execution with the inverted path condition. Taking
the example in Fig. 1, LOOP decides it is a dynamic opaque predicate when

328
D. Xu et al.
p ⇒q and ¬p ⇒¬q both hold. By this approach, LOOP is able to check the
conventional dynamic opaque predicate.
However, to our knowledge, LOOP still utilizes the limitation of conventional
dynamic opaque predicates that all predicates should be adjacently injected into
a straight-line code. When testing ¬p ⇒¬q, LOOP ﬁrst generates a new trace
by negating the path condition p. Then it tests whether the next predicate in
the new trace is equivalent to ¬q. If so, LOOP further checks whether the new
trace is semantically equivalent to the original trace. This procedure is very
time consuming, which limits LOOP’s searching capacity. Therefore, LOOP’s
heuristic is only checking two adjacent predicates such as p and q in Fig. 1.
In the following sections, we present that our method overcome the limitation
of existing conventional dynamic opaque predicate and lead to LOOP’s poor
detection ratio on our generalized dynamic opaque predicates.
3
Generalized Dynamic Opaque Predicates
In this section, we present the details of the generalized dynamic opaque predi-
cates method. First, we introduce the concept of correlated predicate. After that,
we explain how to insert generalized dynamic opaque predicates into straight-line
programs, branches and loops.
3.1
Correlated Predicates
Correlated predicate, as brieﬂy discussed in Sect. 2.2, is a basic concept in
dynamic opaque predicate. In this section, we present the formal deﬁnition of
correlated predicate. First we need to deﬁne correlated variables. Correlated vari-
ables is a set of variables that are always evaluated to the same value in any
program execution. One common example of correlated variables is the aliases
of the same variable, like the pointers in C or the references in C++ or Java.
Correlated predicates are a set of predicates that are composed of correlated
variables and have a ﬁxed relation of their true value. The ﬁxed relation means
that, given a set of correlated predicates, if the true value of one of them is given,
all other predicates’ true value are known. Usually, it is intuitive to construct
correlated predicates using correlated variables. Table 1 shows some examples of
correlated predicates. The integer variables x, y and z in the ﬁrst column is the
correlated variables (CV). The CP1, CP2 and CP3 columns show three sets of
diﬀerent correlated predicates.
Here we take the CP2 column as an example to show how correlated pred-
icates work. First, since x, y and z are correlated integer variables, they are
always equivalent. There are three predicates in CP2, x%2 == 1, y%2 == 0 and
z%2 == 1. Note that x, y and z are integer variables, so they are either even
or odd. Therefore, given the true value of any one of these predicates, we can
immediately get the others’ true values. Furthermore, it is not necessary that
correlated predicates have similar syntax form. We can use semantically equiv-
alent operations to create correlated predicates. CP3 shows such an example.

Generalized Dynamic Opaque Predicates
329
Table 1. Examples of correlates predicates.
CV CP1
CP2
CP3
x
x > 0
x % 2 == 1 x + x > 0
y
y > 0
y % 2 == 0 2 * y <= 0
z
z <= 0 z % 2 == 1 z << 1 > 0
Although the syntax of each predicate is diﬀerent from others, they still meet
the deﬁnition of correlated predicates.
One problem we need to pay attention to is that the value of the correlated
variables should not be changed during the dynamic opaque predicates, which
ensures that every correlated variable are evaluated to the same value in all
dynamic opaque predicates in one execution. Therefore, we compute the def-
use chain inside a function and choose the section between two deﬁnitions of a
variable as the candidate to be obfuscated. Note that pointer access operations
could still cause the variable’s value changes. Our solution is performing a simple
alias analysis to decide whether the pointer is an alias of the variable. If not, we
can include the pointer access instructions inside the dynamic opaque predicates;
otherwise not. Since alias analysis is complicated and diﬃcult, we only run a
light-weighted address-taken algorithm [12] in our implementation. It is ﬂow-
insensitive and context-insensitive. If the analysis cannot tell whether the pointer
is an alias of the correlated variable, we will conservatively consider that it
could point to the variable and exclude it from the dynamic opaque predicates
candidates.
3.2
Straight-Line Code
In this section, we present how to insert dynamic opaque predicate into a
straight-line code. Before digging into the details, we ﬁrst explain the symbols
in the ﬁgures as follows.
1. A rectangle is a basic block.
2. A number in a rectangle represents one instruction.
3. A circle indicates a correlated predicate.
4. An arrow between two basic blocks indicates the control ﬂow transfer. Typ-
ically, it is a conditional or unconditional jump. If there is only one arrow
between two blocks, it is an unconditional jump; otherwise, it is a conditional
jump.
Given the deﬁnition above, Fig. 2(a) shows a straight-line code which contains
only one basic block, in which there are ﬁve sequential instructions. If a straight-
line code comprises multiple basic blocks which are connected by unconditional
jumps, it can be merged into one basic block. So for the ease of understanding,
we use the single basic block example to present straight-line code.

330
D. Xu et al.
1
2
3
4
5
1
2
1
3
4
2
3
5
4
5
(a)
(b)
(c)
1
2
3
4
5
1
2
3
4
5
1
1
2
3
1
2
2
3
4
5
4
5
3
4
5
(d)
(e)
1
2
3
4
5
1
2
3
4
5
1
2
3
4
5
Fig. 2. Dynamic opaque predicate insertion in straight-line code.
When inserting dynamic opaque predicates into straight-line code, we have
two strategies, depth-ﬁrst and breadth-ﬁrst, whose obfuscation result is shown in
Fig. 2(c) and (e). Here we introduce the depth-ﬁrst style ﬁrst and brieﬂy discuss
the breadth-ﬁrst later since they are similar. When inserting dynamic opaque
predicates in depth-ﬁrst style, we select the ﬁrst correlated predicate and then
make a copy of the original basic block, as shown in Fig. 2(b). After that, the
two basic blocks are split at diﬀerent locations so as to create two chains of basic
blocks in which each basic block are diﬀerent with each other. At last, we insert
other correlated predicates to ensure that the control ﬂow takes either all left
branches or all right branches.
Furthermore, when inserting depth-ﬁrst dynamic opaque predicates, we
could insert as many correlated predicates as we can by splitting the basic
blocks at diﬀerent locations. As shown in Fig. 2(c), those basic blocks consti-
tute two chains, in which the execution ﬂow will either take every left branch
or right branch. We call the basic block sequence that consists of all left or
right branches an opaque trace. In this paper, the multiple execution traces
caused by the eﬀect of opaque predicates are called opaque trace. As shown
in Fig. 2(c), if the execution ﬂow takes all the left branches, the opaque trace is
[1 2]->[3 4]->[5]. Similarly, when taking all right branches, the opaque trace
is [1]->[2 3]->[4 5]. Therefore, Fig. 2(c) contains two opaque traces.
Generally speaking, the steps to insert depth-ﬁrst dynamic opaque predicates
to a single basic block BB are described as follows.
1. Select a correlated variable and creating the ﬁrst correlated predicate accord-
ingly.
2. Clone a new basic block BB′ from BB.

Generalized Dynamic Opaque Predicates
331
3. Split BB and BB′ at diﬀerent locations to create two sequences of basic
blocks, or say, two opaque traces T1 and T2:
T1 = BB1 →BB2 →· · · →BBn
T2 = BB′
1 →BB′
2 →· · · →BB′
n
4. Create and insert the remaining n −1 correlated predicates.
5. Insert conditional or unconditional jumps into the end of each basic block to
create the correct control ﬂow.
The other strategy is breadth-ﬁrst inserting dynamic opaque predicates. It
create more opaque traces via correlated predicates that have multiple branches.
The inserting process is similar as depth-ﬁrst. Assuming each predicate has three
branches, the ﬁrst step is to select and insert the ﬁrst correlated predicate and
create two copies of the original basic block as shown in Fig. 2(d). Then split the
three basic blocks at diﬀerent oﬀsets so as to create three opaque traces. At last,
insert the other correlated predicates and other jump instructions to adjust the
CFG. The result is shown in Fig. 2(e).
Furthermore, we can easily create more complicated generalized dynamic
opaque predicates by iteratively applying depth ﬁrst and breadth ﬁrst injection.
For example, the basic block [1,2,3] can also be split to create a depth ﬁrst
generalized dynamic opaque predicate. Note that it naturally breaks the adja-
cency of the two predicates in Fig. 2(e). Being compared with the conventional
dynamic opaque predicate shown in Sect. 2.2 which only has two adjacent predi-
cates p and q, our method can insert more generalized and non-adjacent dynamic
opaque predicates in straight-line code.
3.3
Branches
In the previous section, we present the approach to inserting dynamic opaque
predicates into straight-line code. However, real world programs also consist of
other structures such as branches and loops. When considering inserting dynamic
opaque predicates into branches or loops, one straight forward idea is only insert-
ing dynamic opaque predicates into basic blocks independently by treating them
as straight-line code. However, this idea has one obvious problem: it doesn’t
spread the dynamic opaque predicates across the branch or loop condition, so
essentially it is still the same as what we have done in Sect. 3.2.
In this section, we describe the process to insert dynamic opaque predicates
into a branch program, which improves the program obfuscation level. For the
ease of presenting our approach, we consider the branch program which contains
three basic blocks as shown in Fig. 3(a). Our solution can also be applied to
more complicated cases such as each branch contains multiple basic blocks. As
shown in Fig. 3, Cond is the branch condition. BB1 is located before the branch
condition. BB2 is the true branch and BB3 is the false branch.
As the ﬁrst step of inserting branch dynamic opaque predicate, we back-
wards search for an instruction that is independent from all instructions until

332
D. Xu et al.
1
2
3
4
1
2
1
3
4
2
3
5
6
7
8
9
10
Cond
Cond
5
6
4
5
7
6
7
8
9
4
8
10
9
10
(a)
(b)
1
2
3
4
1
2
3
Cond
5
6
7
4
5
6
7
8
9
10
4
8
9
10
(c)
BB1
BB2
BB3
BB1
BB2
BB3
BB1'
BB2'
BB3'
T
F
T
F
T
F
Fig. 3. Dynamic opaque predicate insertion in a branch program.
the branch condition, and also independent from the branch instruction. In this
paper, this instruction is called a branch independent instruction. Essentially, it
can be moved across the branch condition so as to create the oﬀset in diﬀerent
opaque traces. In Fig. 3(a), the underlined instruction 4 is a branch independent
instruction. Based on our observation, there are plenty of branch independent
instructions. For example, the Coreutils program ls contains 289 branch con-
ditions, in each of which we ﬁnd at least one branch independent instruction.
Typically, these instructions prepare data which are used both in the true and
false branch.
After identifying the branch independent instruction, we select and insert the
correlated variables, then make a copy of each basic blocks. Moreover, we move
the instruction 4 along the right opaque trace across the branch condition and
Fig. 3(b) shows the result. Note that due to instruction 4 is branch independent,
so moving it to the head of basic blocks in the branches will not change the orig-
inal program’s semantics. At last, we create straight-line code dynamic opaque
predicates for BB1, BB2 and BB3. The ﬁnal result of the obfuscated CFG is
shown in Fig. 3(c). We brieﬂy summarize the steps of inserting dynamic opaque
predicates into a branch program as follows.
1. Find the branch independent instruction in BB1.
2. Select and insert the correlated predicates.

Generalized Dynamic Opaque Predicates
333
3. Clone BB1, BB2 and BB3 as BB′
1, BB′
2 and BB′
3.
4. Move the branch independent instruction from BB′
1 to BB′
2 and BB′
3.
5. Split basic blocks and create dynamic opaque predicates as in straight-line
code.
3.4
Loops
Previous sections present the details about how to insert dynamic opaque pred-
icates into straight-line code and branch programs. In this section, we consider
inserting dynamic opaque predicates into a loop. In this paper, a loop refers to
a program which contains a backward control ﬂow, such as Fig. 4(a). BB1 is
the ﬁrst basic block of the loop body and BB2 is the last one. The dashed line
indicates other instructions in the basic block. The dashed arrow means other
instructions in the loop body, which could be a basic block, branch or even
another loop. Particularly, if there is only one basic block in the loop body, BB1
and BB2 refer to the same basic block.
1
2
2
1
1
3
3
2
Cond
Cond
(a)
(c)
3
(b)
BB1
BB2
2
1
2
1
3
3
Cond
BB1
BB2
BB1
BB2
BB1'
BB2'
BB1'
BB2'
Fig. 4. Dynamic opaque predicate insertion in a loop.
The key idea in inserting dynamic opaque predicates to a loop program is
ﬁnding a loop independent instruction and moving it across the loop condition in
the same opaque trace. We deﬁne loop independent instruction as an instruction
in a loop whose operands are all loop invariants. Loop invariant is a classical
concept in compiler optimization. A variable is called loop invariant if its value
never changes no matter how many times the loop is executed. For instance,
Fig. 5 shows a loop invariant. The variable m is deﬁned outside the loop and
is never changed inside the loop. Each iteration of the loop accesses the same

334
D. Xu et al.
array element A[m] and assigns it to the variable x. Therefore, m, A[m] and
x are loop invariant. Note that here we use the C source code to present the
idea. Actually we are working on the compiler IR level, where every instruction
is close to a machine instruction. As a result, in the IR level, all instructions
that only operate the loop invariants are loop independent instructions. For
instance, the instruction that load the value of A[m] from memory to x is an
loop independent instruction. Based on our observation, there are plenty of loop
independent instructions inside a loop body, such as the instructions to compute
a variable’s oﬀset address. In the experiment, we ﬁnd at least loop independent
instruction for each of the 61 loops in the Coreutils program ls.
Fig. 5. An example of loop invariants.
In traditional compiler optimization, the loop independent instructions are
extracted out of the loop body so as to reduce the loop body size and fur-
ther improve the runtime performance. All compiler frameworks implement a
data ﬂow analysis to analyze and identify the loop invariants. In this paper, we
take advantage of the loop independent instructions to create the oﬀset between
opaque traces. Consider the example shown in Fig. 4(a). First, we search and
identify that instruction 2 is a loop independent instruction. Second, we lift the
instruction 2 to the beginning of the loop body, since other instructions might
need the output of instruction 2. Then we make copies of BB1 and BB2 as BB′
1
and BB′
2. After that we select the correlated predicates and initialize the ﬁrst
one to ensure that it takes the left branch. The bold arrow in Fig. 4(b) indicates
the initialized predicates. We will soon discuss the reason. At last, the loop inde-
pendent instruction 2 is moved from BB′
1 to BB′
2 and the ﬁnal result is shown
in Fig. 4(c). We summarize the steps of creating loop dynamic opaque predicates
as follows.
1. Find the loop independent instruction Ii.
2. Lift Ii to the beginning of the loop body in BB1.
3. Select the correlated predicates and initialize the ﬁrst one correctly.
4. Clone BB1 and BB2 as BB′
1 and BB′
2.
5. Remove I ′
i from BB′
1 and add it to the end of BB′
2.
6. Add dynamic opaque predicates as separate basic blocks and according jumps
to build correct control ﬂow.
Note that at the third step, we initialize the correlated variables so as to
ensure the control ﬂow goes to the left branch at the ﬁrst iteration. The reason

Generalized Dynamic Opaque Predicates
335
is that we have to make the loop invariant instructions executed at least once
at the ﬁrst iteration of the loop in order to assure all loop invariants loaded,
computed and stored correctly. The value of correlated variables may change
during the dashed part of the loop body so as to divert the execution ﬂow to
each opaque trace. Particularly, when the execution reaching the last iteration
of the loop, there is a redundant instruction 2 if the execution follows the right
branch. Since instruction 2 is loop independent, it doesn’t aﬀect the semantic of
the program execution.
4
Implementation
Our implementation is based on Obfuscator-LLVM [13], an open source fork of
the LLVM compilation suite that aims to improve the software security via code
obfuscation and tamper-prooﬁng. The architecture of our system is shown in
Fig. 6. The generalized dynamic opaque predicate obfuscator (GDOP obfuscator)
is surrounded with dashed lines. Basically, our automatic GDOP obfuscator
works as a pass in LLVM framework. The workﬂow contains three steps. First,
the LLVM frontend Clang read the source code and translate it into LLVM IR.
Second, GDOP obfuscator reads the IR and inserts generalized dynamic opaque
predicates to the appropriate location. At last, the LLVM backend outputs the
executable program based on the obfuscated IR ﬁles.
Clang
GDOP Obfuscator
Sequence
Branch
Loop
IR
Obfuscated 
IR
LLVM
Backend
Source
Code
Executable
Code
Fig. 6. The architecture of dynamic opaque predicate obfuscator.
Particularly, we implement the procedure of inserting generalized dynamic
opaque predicates to a straight-line, branch and loop program as three separate
passes, which includes 1251 lines of C++ code in total. We also write a driver
program to invoke the three passes so as to insert all kinds of generalized dynamic
opaque predicates. In addition, we implement a junk code generator to insert
useless code into functions, such as redundancy branches and extra dependencies.
Moreover, we provide a compiler option for users to conﬁgure the probability
for inserting generalized dynamic opaque predicates. For each basic block, our
obfuscator generates a random number between zero and one. If the number is
smaller than the given probability, it tries to insert generalized dynamic opaque
predicates into the basic block; otherwise it skips the basic block.

336
D. Xu et al.
5
Evaluation
We conduct our experiments with several objectives. First, we want to evaluate
whether our approach is eﬀective to obfuscate control ﬂow graph. To this end,
we measure control ﬂow complexity of GNU Coreutils with three metrics. We
also test our tool with a commercial binary diﬃng tool which is based on control
ﬂow graph comparison. Last but not least, we want to prove our approach can
defeat the state-of-the-art deobfuscation tool. Our testbed consists of an Intel
Core i7-3770 processor (Quad Core with 3.40 GHz) and 8 GB memory, running
Ubuntu Linux 12.04 LTS. We turn oﬀother compiler optimization options by
using -g option.
5.1
Obfuscation Metrics with Coreutils
This section shows our evaluation result of inserting generalized dynamic opaque
predicates into the GNU Coreutils 8.23. Since the generalized dynamic opaque
predicate is an intra-procedural obfuscation [22], we evaluate it by comparing the
control ﬂow complexity of the modiﬁed function before and after the generalized
dynamic opaque predicate obfuscation. In this experiments, we choose ﬁve hot
functions in the Coreutils program set by proﬁling. At the same time, we make
sure all the functions containing at least ten basic blocks1. After proﬁling, the
ﬁve hot functions we select are as follows.
1. get next: This function is deﬁned in tr.c. It returns the next single character
of the expansion of a list.
2. make format: This function is deﬁned in stat.c. It removes unportable ﬂags
as needed for particular speciﬁers.
3. length of file name and frills: This function is deﬁned in ls.c for count-
ing the length of ﬁle names.
4. print file name and frills: This function is also deﬁned in ls.c. It prints
the ﬁle name with appropriate quoting with ﬁle size and some other informa-
tion as requested by switches.
5. eval6: This function is deﬁned in eval6.c to handle sub-string, index, quot-
ing and so on.
The metrics that we choose to show the CFG complexity are the number
of CFG edges, the number of basic blocks and the cyclomatic number. The
cyclomatic number is calculated as e −n + 2 where e is the number of CFG
edges and n is the number of basic blocks. The cyclomatic number is considered
as the amount of decision points in a program [9] and has been used as the metrics
for evaluating obfuscation eﬀects [7]. We ﬁrst insert generalized dynamic opaque
predicates into the hot functions with two diﬀerent probability level: 50 % and
100 %. After that, we perform functionality testing to make sure our obfuscation
is semantics-preserving. Table 2 shows the obfuscation metrics of the original
clean version and the obfuscated version. The data shows that our dynamic
opaque predicate obfuscation can signiﬁcantly increase the program complexity.
1 We do not consider dynamic link library functions because our approach takes the
target program source code as input.

Generalized Dynamic Opaque Predicates
337
Table 2. Obfuscation metrics and BinDiﬀscores of hot functions in Coreutils.
Function # of Basic blocks
# of CFG edges
Cyclomatic number Bindiﬀscore
Orig 50 % 100 % Orig 50 % 100 % Orig 50 % 100 %
50 % 100 %
1
43
171
229
62
258
338
21
89
111
0.05
0.02
2
20
75
105
30
114
158
12
41
55
0.02
0.01
3
30
94
120
49
141
177
21
49
59
0.02
0.02
4
46
138
208
80
220
320
36
84
114
0.04
0.01
5
76
272
376
117
425
573
43
155
199
0.05
0.02
To test the control ﬂow graph after our obfuscation is heavily cluttered, we
also evaluate our approach with BinDiﬀ2, which is a commercial binary diﬃng
tool by measuring the similarity of two control ﬂow graphs. We run BinDiﬀto
compare the 50 % and 100 % obfuscated versions with the original ﬁve programs
and the similarity score is presented in the ﬁfth column in Table 2. The low
scores indicate that the obfuscated program is very diﬀerent from the original
version.
5.2
Resilience
In this experiment, we evaluate the resilience to deobfuscation by applying
LOOP [19], the latest formal program semantics-based opaque predicate detec-
tion tool. The authors present a program logic-based and obfuscation resilient
approach to the opaque predicate detection in binary code. Their approach rep-
resents the characteristics of various opaque predicates with logical formulas and
veriﬁes them with a constraint solver. According to the authors, LOOP is able
to detect various opaque predicates, including not only simple invariant opaque
predicates, but also advanced contextual and dynamic opaque predicates.
In our evaluation, we run two round of 100 % obfuscation on the ﬁve Coreutils
functions and use LOOP to check them. The results are presented in Table 3.
Table 3. The result of LOOP detection.
Function
Straight line DOP
Branch DOP
Loop DOP
Total
Detected
Ratio
Total
Detected
Ratio
Total
Detected
Ratio
1
52
3
5.77 %
21
0
0.00 %
8
0
0.00 %
2
28
2
7.14 %
15
0
0.00 %
6
0
0.00 %
3
27
2
7.41 %
23
0
0.00 %
6
0
0.00 %
4
54
5
9.26 %
26
0
0.00 %
8
0
0.00 %
5
82
8
9.76 %
52
0
0.00 %
14
0
0.00 %
2 http://www.zynamics.com/bindiﬀ.html.

338
D. Xu et al.
As shown in Table 3, LOOP can detect very few number of the generalized
dynamic opaque predicates inserted in straight-line code but fails to detect all
those in branches and loops. We look into every generalized dynamic opaque
predicate that is detected by LOOP and ﬁnd that they are all conventional
adjacent dynamic opaque predicates. We also check verify that LOOP fails to
detect the remaining generalized dynamic opaque predicates.
We carefully analyze LOOP’s report and ﬁnd several reasons that lead to
LOOP’s poor detection ratio on generalized dynamic opaque predicates. First,
iterative injection causes LOOP fails to detect majority of the generalized
dynamic opaque predicates in straight-line code. Our obfuscation method can be
iteratively executed on a candidate function, which means we are able to insert
generalized dynamic opaque predicates into the same function several times.
Note that each time we choose diﬀerent correlated variables and diﬀerent cor-
related predicates. Therefore, the generalized dynamic opaque predicates that
are inserted by the later pass will break the adjacency of those inserted by the
previous pass. In addition, junk code injection is another reason that prevents
LOOP’s detection.
Second, generalized dynamic opaque predicates spread across the branch or
loop structure so they naturally break the adjacency property, which causes
LOOP detects none of the generalized dynamic opaque predicates in branches
and loops. For example, when we execute the loop shown in Fig. 4, there are two
correlated but not adjacent predicates. They are separated by the instructions
in the dashed line and the loop condition. Therefore, the detection method in
the LOOP paper fails to detect the generalized dynamic opaque predicates.
5.3
Cost
This section presents the cost evaluation of our generalized dynamic opaque
predicate obfuscation. We evaluate the cost from two aspects: binary code size
and execution time. For binary code size, we measure and compare the number of
bytes of the compiled programs that contain the ﬁve hot functions. For instance,
we compare the size of tr’s binary code when inserting generalized dynamic
opaque predicates to function get next with diﬀerent probabilities such as 50 %
and 100 %. For the evaluation of execution time, we record and compare the
Table 4. Cost evaluation of the dynamic opaque predicate obfuscation.
Function Program Binary size (bytes)
Execution time (ms)
Orig
50 %
100 %
Ratio
Orig 50 % 100 %
1
tr
132,084 132,826 133,491 0.53 %
2.2
2.2
2.4
2
stat
210,864 211,355 211,710 0.20 %
4.0
4.0
4.1
3
ls
350,076 350,916 351,527 0.21 % 23.2
23.4
23.7
4
ls
350,076 351,083 351,742 0.24 % 23.2
23.3
23.8
5
expr
129,696 130,836 131,409 0.66 %
0.6
0.6
0.6

Generalized Dynamic Opaque Predicates
339
Table 5. Obfuscation metrics of sort files.
Function
# of basic blocks
# of CFG edges
Cyclomatic number
Orig
Round 1
Round 2
Orig
Round 1
Round 2
Orig
Round 1
Round 2
sort files
19
160
405
25
255
539
8
97
136
execution time of clean version and the obfuscated program. We conﬁgure the
switches and input ﬁles so as to ensure the control ﬂow touches the obfuscated
function.
Table 4 shows the evaluation result. We can observe that our approach slightly
increases the binary code size, which is less than 0.7 %. Moreover, according to
our experiments, the generalized dynamic opaque predicates have a small impact
Fig. 7. Comparison between CFGs after diﬀerent rounds of dynamic opaque predicate
obfuscation.

340
D. Xu et al.
on program performance. The execution time of most programs stays the same
when inserting generalized dynamic opaque predicates with 50 % probability and
increases a little when inserting with 100 % probability.
5.4
Case Study
As mentioned in Sect. 5.2, our generalized dynamic opaque predicates can be
iteratively apply to a candidate program so as to create more obfuscated result.
In this section, we provide a case study to show the result of generalized dynamic
opaque predicate obfuscation iteration.
The target function is the sort files function in the ls program. We choose
this function since its CFG size is appropriate and it contains straight-line
codes, branches and loops, which are suited for inserting all three categories of
generalized dynamic opaque predicates. We perform two rounds of generalized
dynamic opaque predicate obfuscation with 100 % probability. Table 5 presents
the same measures as shown in the last section and Fig. 7 shows the result CFG.
Figure 7(a) shows the original CFG of sort files Fig. 7(b) presents the CFG
after the ﬁrst round of dynamic opaque predicate obfuscation. Next, we perform
another round of dynamic opaque predicate obfuscation on (b) and the result is
shown in Fig. 7(c). The comparison of the three CFGs clearly indicates that our
generalized dynamic opaque predicate obfuscation can signiﬁcantly modify the
intra-procedural control ﬂow graph.
6
Conclusion
Opaque predicate obfuscation is a prevalent control ﬂow obfuscation method
and has been widely applied both in malware and benign software protection.
Dynamic opaque predicate obfuscation is regarded as a promising method since
the predicate values may vary in diﬀerent executions and thus make them more
resilient to detection. However, little work discusses the systematical design of
dynamic opaque predicates in detail. Also, some recent advanced deobfuscation
tools utilize certain speciﬁc properties as ad hoc heuristics to detect dynamic
opaque predicates. In this paper, we present generalized dynamic opaque pred-
icates to address these limitations. Our method automatically inserts dynamic
opaque predicates into common program structures and is hard to be detected
by the state-of-the-art formal program semantics-based deobfuscation tools. The
experimental results show the eﬃcacy and resilience of our method with negli-
gible performance overhead.
Availability
To better facilitate future research, we have released the source code of our
dynamic opaque predicate obfuscation tool at https://github.com/s3team/gdop.

Generalized Dynamic Opaque Predicates
341
Acknowledgements. We thank the anonymous reviewers for their valuable feed-
back. This research was supported in part by the National Science Foundation (NSF)
grants CNS-1223710 and CCF-1320605, and the Oﬃce of Naval Research (ONR) grants
N00014-13-1-0175 and N00014-16-1-2265.
References
1. Arboit, G.: A method for watermarking Java programs via opaque predicates. In:
Proceedings of 5th International Conference on Electronic Commerce Research
(ICECR-5) (2002)
2. Bruschi, D., Martignoni, L., Monga, M.: Detecting self-mutating malware
usingcontrol-ﬂow graph matching. In: Proceedings of Detection of Intrusions and
Malware and Vulnerability Assessment (DIMVA 2006) (2006)
3. Bruschi, D., Martignoni, L., Monga, M.: Code normalization for self-mutating mal-
ware. IEEE Secur. Priv. 5(2), 46–54 (2007)
4. Cappaert, J., Preneel, B.: A general model for hiding control ﬂow. In: Proceedings
of the 10th Annual ACM Workshop on Digital Rights Management (DRM 2010)
(2010)
5. Chen, H., Yuan, L., Wu, X., Zang, B., Huang, B., Yew, P.C.: Control ﬂow
obfuscation with information ﬂow tracking. In: Proceedings of the 42nd Annual
IEEE/ACM International Symposium on Microarchitecture (MICRO 42) (2009)
6. Collberg, C., Thomborson, C., Low, D.: A taxonomy of obfuscating transforma-
tions. The University of Auckland, Technical report (1997)
7. Collberg, C., Thomborson, C., Low, D.: Manufacturing cheap, resilient, and
stealthy opaque constructs. In: Proceedings of the 25th ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages (POPL 1998) (1998)
8. Collberg, C., Myles, G., Huntwork, A.: Sandmark-a tool for software protection
research. IEEE Secur. Priv. 1(4), 40–49 (2003)
9. Conte, S.D., Dunsmore, H.E., Shen, V.Y.: Software Engineering Metrics and Mod-
els. Benjamin-Cummings Publishing Co. Inc., REdwood City (1986)
10. Coppens, B., De Sutter, B., Maebe, J.: Feedback-driven binary code diversiﬁcation.
ACM Trans. Architect. Code Optim. (TACO) 9(4), 24:1–24:26 (2013)
11. Drape, S.: Intellectual property protection using obfuscation. Technical report,
RR-10-02, Oxford University Computing Laboratory (2010)
12. Hind, M., Pioli, A.: Which pointer analysis should i use?. In: Proceedings of
the ACM SIGSOFT International Symposium on Software Testing and Analysis
(ISSTA 2000), pp. 113–123. ACM (2000)
13. Junod, P., Rinaldini, J., Wehrli, J., Michielin, J.: Obfuscator-LLVM - software
protection for the masses. In: Proceedings of the 1st International Workshop on
Software Protection (SPRO 2015) (2015)
14. Kovacheva, A.: Eﬃcient code obfuscation for Android. Master’s thesis, University
of Luxembourg (2013)
15. Larsen, P., Homescu, A., Brunthaler, S., Franz, M.: SoK: automated software diver-
sity. In: Proceedings of the 2014 IEEE Symposium on Security and Privacy (SP
2014) (2014)
16. Lattner, C., Adve, V.: LLVM: a compilation framework for lifelong program analy-
sis and transformation. In: Proceedings of the International Symposium on Code
Generation and Optimization (CGO 2004) (2004)
17. Madou, M.: Application security through program obfuscation. Ph.D. thesis, Ghent
University (2007)

342
D. Xu et al.
18. Madou, M., Van Put, L., De Bosschere, K.: LOCO: an interactive code
(de)obfuscation tool. In: Proceedings of the 2006 ACM SIGPLAN Symposium
on Partial Evaluation and Semantics-Based Program Manipulation (PEPM 2006)
(2006)
19. Ming, J., Xu, D., Wang, L., Wu, D.: LOOP: logic-oriented opaque predicate detec-
tion in obfuscated binary code. In: Proceedings of the 22nd ACM SIGSAC Con-
ference on Computer and Communications Security (CCS 2015) (2015)
20. Moser, A., Kruegel, C., Kirda, E.: Limits of static analysis for malware detection.
In: Proceedings of the 23th Annual Computer Security Applications Conference
(ACSAC 2007), December 2007
21. Myles, G., Collberg, C.: Software watermarking via opaque predicates: implemen-
tation, analysis, and attacks. Electron. Commer. Res. 6(2), 155–171 (2006)
22. Palsberg, J., Krishnaswamy, S., Kwon, M., Ma, D., Shao, Q., Zhang, Y.: Experience
with software watermarking. In: Proceedings of the 16th Annual Computer Security
Applications Conference (ACSAC 2000) (2000)
23. Preda, M.D., Madou, M., Bosschere, K.D., Giacobazzi, R.: Opaque predicate detec-
tion by abstract interpretation. In: Proceedings of 11th International Conference
on Algebriac Methodology and Software Technology (AMAST 2006) (2006)
24. Roundy, K.A., Miller, B.P.: Binary-code obfuscations in prevalent packer tools.
ACM J. Name 1, 21 (2012)
25. Udupa, S.K., Debray, S.K., Madou, M.: Deobfuscation: Reverse engineering obfus-
cated code. In: Proceedings of the 12th Working Conference on Reverse Engineering
(WCRE 2005) (2005)
26. Wang, C., Hill, J., Knight, J.C., Davidson, J.W.: Protection of software-based
survivability mechanisms. In: Proceedings of the 2001 International Conference on
Dependable Systems and Networks (DSN 2001) (2001)

A Bayesian Cogntive Approach to Quantifying
Software Exploitability Based
on Reachability Testing
Guanhua Yan1(B), Yunus Kucuk1,2, Max Slocum1, and David C. Last3
1 Department of Computer Science,
Binghamton University, State University of New York, Binghamton, USA
{ghyan,ykucuk1,mslocum1}@binghamton.edu
2 Defense Sciences Institute, Turkish Military Academy, Ankara, Turkey
ykucuk@kho.edu.tr
3 Resilient Synchronized Systems Branch,
Air Force Research Laboratory, Rome, USA
david.last.1@us.af.mil
Abstract. Computer hackers or their malware surrogates constantly
look for software vulnerabilities in the cyberspace to perform various
online crimes, such as identity theft, cyber espionage, and denial of ser-
vice attacks. It is thus crucial to assess accurately the likelihood that
a software can be exploited before it is put into practical use. In this
work, we propose a cognitive framework that uses Bayesian reasoning
as its ﬁrst principle to quantify software exploitability. Using the Bayes’
rule, our framework combines in an organic manner the evaluator’s prior
beliefs with her empirical observations from software tests that check
if the security-critical components of a software are reachable from its
attack surface. We rigorously analyze this framework as a system of non-
linear equations, and henceforth perform extensive numerical simulations
to gain insights into issues such as convergence of parameter estimation
and the eﬀects of the evaluator’s cognitive characteristics.
1
Introduction
Software ﬂaws are diﬃcult, if not impossible, to avoid, either due to the lim-
ited cognitive capacities of the programmers to test all corner cases, or the
fundamental weaknesses of the programming languages used. Software defects
enable cybercriminals or their malware surrogates to perform a wide spectrum
of malicious online activities, such as identity theft, cyber espionage, and denial
of service attacks. As evidenced by numerous hacks that have occurred in the
past, vulnerable software can result in signiﬁcant economic losses and reputa-
tion damages. For instance, it was estimated that the revelation of the Shellshock
vulnerability had led to one billion attacks [2], and an announced software vul-
nerability costs a ﬁrm an average loss of 0.5 % value in stock price [33].
When a software system is put into practical use, its operator is concerned
with the likelihood that it can be exploited maliciously. For security-critical
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 343–365, 2016.
DOI: 10.1007/978-3-319-45871-7 21

344
G. Yan et al.
applications, a software system can only be trusted for operational use if its
operator’s conﬁdence level in its unexploitability exceeds a certain threshold,
say, 99 %. The challenge, then, is: how can we derive such conﬁdence levels to
assist human operators with decision-making? This problem is largely unex-
plored in the literature. There are some publicly available sources to ﬁnd known
software vulnerabilities, such as National Vulnerability Database [4], Exploit
Database [5], and OSVDB (Open Sourced Vulnerability Database) [6]. However,
these sources contain only known vulnerabilities in typically popular software,
and thus cannot be solely relied upon to evaluate the security of a software
system. Moreover, containing vulnerabilities does not necessarily mean that the
software is exploitable in a certain running environment, as a successful software
exploitation requires the existence of a realizable execution path from the attack
surface of the program to its vulnerable software components [24,27,36].
Quantiﬁable measures of software exploitability can guide human operators
in deciding whether it is suﬃciently secure to put a software into operation.
The Common Vulnerability Scoring System (CVSS) [28] is widely used in the
industry, but its design is more of an art rather than a science. For example, it
assesses the security of a vulnerable software with an overly simplistic equation:
BaseScore = 1.176×(3I/5+2E/5−3/2), where impact factor I and exploitabil-
ity factor E take circumstance-speciﬁc values. Although this equation has surely
been thoroughly meditated, there lack rigorous scientiﬁc arguments on why its
parameters are so chosen.
In this work, we model the evaluation of software exploitability as a dynamic
process done by an evaluator, who has her prior belief in software exploitability
based upon some of its static features (e.g., its size, type, or some other metrics).
Henceforth, she uses reachability testing tools to check whether there exists an
injection vector from its attack surface that enables reachability of its security-
critical components, such as a system call capable of privilege escalation or a
potential buﬀer overﬂow vulnerability. The exploitability of the software is then
characterized as the evaluator’s subjective belief dynamically adjusted with the
reachability testing results presented to her. During this process, the evaluator
also continuously updates her perceptions about the performances of the tools
used.
To model human cognition, we adopt a ﬁrst-principled approach that inte-
grates an evaluator’s prior belief in software exploitability with her empirical
observations from the reachability tests in a Bayesian manner. Bayesian reason-
ing is performed in a probabilistic paradigm, where given a hypothesis H and the
evidence E, the posterior probability, or the probability of hypothesis H after see-
ing evidence E is calculated based upon the Bayes’ rule: P{H|E} = P{E|H}·P{H}
P{E}
.
Although there lacks evidence that humans reason in a Bayesian way at the
neural level, psychological experiments show that humans behave consistently
with the model at a functional level in a number of scenarios [17,20,29].
In a nutshell, our contributions can be summarized as follows:
– We
propose
a
Bayesian
cognitive
framework
that
quantiﬁes
software
exploitability as the evaluator’s belief in whether an injection vector can be

A Bayesian Cogntive Approach to Quantifying Software Exploitability
345
found from the attack surface of a software to enable the execution of a sen-
sitive code block (e.g., one invoking a system call that leads to privilege esca-
lation). The evaluator’s belief is dynamically updated with the Bayes’ rule,
which uses the past performances of the reachability testing tools to calculate
the likelihood functions for each hypothesis.
– We represent the Bayesian cognitive framework for quantifying software
exploitability with a system of nonlinear equations, and rigorously analyze
its time and space complexity, its sensitivity to the order of reachability tests,
and the conditions under which the evaluator’s belief in software exploitability
improves or deteriorates.
– We use numerical simulations to analyze the Bayesian cognitive framework,
including the convergence of the evaluator’s beliefs, convergence of estimated
parameters, eﬀects of the evaluator’s prior beliefs, eﬀects of the ordering of
software reachability tests, eﬀects of dependency among diﬀerent reachability
testing tools, eﬀects of short memory in parameter estimation, and eﬀects of
lazy evaluation. Our analysis shows that the nature of nonlinear equations
leads to interesting observations that are not so intuitive.
From a high level, our work suggests a continuous and adaptive methodology
for quantiﬁable cybersecurity, which is hard for an environment like the Inter-
net that is open, dynamic and adversarial [34]. Although put in the context of
software exploitability evaluation, the proposed Bayesian cognitive framework
can be applied to various cybersecurity problems, such as malware detection
and anomaly detection. Moreover, such cognitive frameworks allow us to further
design autonomous systems that mimic the decision-making process of human
defenders, thus preventing human errors.
2
Related Work
A large body of research has been dedicated to identifying security-sensitive
software bugs in an eﬃcient manner. One of the most widely used methods for
ﬁnding software bugs in practice is black-box fuzzing, which generates malformed
inputs in a brute-force manner to force crashes. The key challenge facing black-
box fuzzing is lack of eﬃciency when dealing with large software systems, and
there have been some recent works aimed at improving its performance [16,30].
In contrast to black-box fuzzing, white-box fuzzing takes advantage of knowledge
of the internal structures of the program to ﬁnd software bugs. The key enabling
technology behind eﬀective white-box fuzzing is the so-called concolic execution
or dynamic symbolic execution [13], which allows systematic exploration of pro-
gram branches for whole-program security testing. Notable white-box fuzzing
tools include EXE [12], KLEE [11] and SAGE [18,19]. One step further, a few
tools have been developed to automate the process of ﬁnding software exploits,
such as APEG [10], AEG [8] and MAYHEM [15]. Many aforementioned tools
can be used, directly or indirectly, for software reachability testing. Black-box
fuzzing tools, for instance, can be used to test software reachability in an oppor-
tunistic manner. Symbolic or concolic execution tools can be adapted to ﬁnd
satisﬁable paths reaching security-critical code blocks of interest.

346
G. Yan et al.
Our work on quantifying software exploitability intersects with existing
eﬀorts on security metrics, which are valuable to strategic support, quality assur-
ance, and tactical oversight in cyber security operations [22]. Although security
metrics are important for cyber security to progress as a scientiﬁc ﬁeld [25],
it is hard to develop practically useful security metrics due to the dynamic and
adversarial nature of the cyberspace [9,22,34]. As desirable properties of security
metrics include objectivity and repeatability, software exploitability quantiﬁed
by our proposed scheme does not qualify as a security metric. However, useful
metrics indicative of software exploitability can be incorporated into our cog-
nitive framework as the evaluator’s prior belief. As the landscape of software
exploitation is changing over time [26], these metrics may gradually lose their
predictive power. Our cognitive framework allows the evaluator to adjust her
beliefs with observations from new exploitation tests.
Our work ﬁnds inspirations from recent advances in modeling human cog-
nition. A number of psychological experiments have shown that humans tend
to behave consistently with the Bayesian cognitive model at the functional
level [17,20,29]. Cognition-inspired methods have found a few applications in
cyber security, such as malware family identiﬁcation [23] and cyber-attack analy-
sis [37]. Such cognition-based methods can be used in autonomous cyber defense
systems to mimic the decision-making process of human operators and prevent
human mistakes or their intrinsic cognitive biases [32].
3
Software Exploitation Based on Reachability Testing
An experienced hacker would narrow down the attack target to a few security-
sensitive code blocks, a technique called red pointing [21]. Successful software
exploitation requires both the existence of a software defect and the ability of
the attacker to exploit it to achieve his attack goal [8]. With a software bug as
the target, if there exists an execution path from the attack surface (which is
controllable by the attacker) to invoke the software bug, the bug is deemed as
exploitable. Note that our deﬁnition of software exploitability is diﬀerent from
that in [8], where a software bug is considered to be exploitable only if it is
reachable from the attack surface of the program and the runtime environment
satisﬁes the user-deﬁned exploitation predicate after the control ﬂow is hijacked
(e.g., the shellcode is well-formed in memory and will be eventually executed).
Consider the following C program with a buﬀer overﬂow bug:
#include <stdio.h>
#include <fcntl.h>
void innocent() { return; }
void vulnerable() { char buf[8]; gets(buf); }
int main(int argc, char** argv) {
if (argc != 2) { return -1; }
int fn = open(argv[1], O_RDONLY);
char c, d = 0;
int i;

A Bayesian Cogntive Approach to Quantifying Software Exploitability
347
for (i = 0; i < 10; i++) {
if(read(fn, &c, 1) == 1) d = d^c; else break;
}
if (d == 0) vulnerable(); else innocent();
close(fn);
}
To reach vulnerable() with a buﬀer overﬂow bug, we need to ﬁnd an input
ﬁle the XOR of whose ﬁrst 10 bytes is 0. We tried the following on a commod-
ity PC:
Black-box Fuzzing: A black-box fuzzer randomly generates input ﬁles to
force program crashes. We add assert(0); at the beginning of function
vulnerable() to cause a crash when it is called, and then use BFF [14] to
fuzz against the program. Using a single seed ﬁle of size 1,805 bytes, BFF can
ﬁnd the ﬁrst crash within a second.
Symbolic Execution: Symbolic execution does not need to execute the pro-
gram concretely. Rather, it relies upon symbolic evaluation to ﬁnd an input that
causes a part of the program to be executed. We use the Z3 tool developed by
Microsoft Research [31] to ﬁnd a satisﬁable condition that enables the execution
of function vulnerable(). As Z3 does not support the char type explicitly, we
use bit-vectors of size 8 (in Z3 parlance, they are deﬁned with: Z3 sort bv sort
= Z3 mk bv sort(ctx, 8) where ctx is a Z3 context) to perform bit-wise XOR
operations. With 10 symbolic variables of type bv sort deﬁned, Z3 can ﬁnd within
a few milliseconds their assignments such that the condition for entering function
vulnerable() is satisﬁed.
Concolic Execution: Concolic execution combines symbolic execution with
concrete execution to speed up code exploration. We ﬁrst try the CREST tool [1]
to ﬁnd solutions to the 10 symbolic variables of type CREST char, each corre-
sponding to a byte read from a ﬁle. However, as CREST uses Yices 1 as its
SMT solver for satisﬁability of formulas [7], which does not support bit-vector
operations, it does not ﬁnd a condition that leads to the execution of function
vulnerable(). Another popular concolic execution tool is KLEE [11], which
works on object ﬁles in the LLVM bitcode format and uses the STP solver sup-
porting bit-vectors and arrays [3]. Similarly, by deﬁning 10 symbolic variables
using klee make symbolic, each corresponding to a byte read from the input
ﬁle, we are able to use KLEE to ﬁnd quickly their proper assignments that enable
the execution of function vulnerable().
For a large and complex software, some of the tools may not ﬁnd exploits
enabling reachability of its security-critical components. A security evaluator
may need multiple tools for a software exploitation task, and intuitively, her
memory of the past performances of these tools aﬀects her evaluation of software
exploitability.

348
G. Yan et al.
4
A Bayesian Cognitive Framework
Motivated by the example in the previous section, we model software exploitation
as a process of ﬁnding a proper injection vector in the attack surface of a soft-
ware that enables its execution to reach one of its security-critical code blocks,
using some reachability testing tools. Our goal is to quantify software exploitabil-
ity as the likelihood that, given a security-critical target in the software, there
exists such an injection vector that successfully leads to its execution. We assume
that the evaluation of software exploitability is performed by an evaluator. Intu-
itively, if she has already found such an injection vector, her perception of the
exploitability of this software is certain. Otherwise, she is uncertain about the
exploitability of the software: there may exist an execution path that reaches the
target from the attack surface but she just cannot ﬁnd it at the moment. The
evaluator may proceed to use some other tools to check the existence of such
an injection vector, and with more failed attempts, she should be increasingly
conﬁdent in the notion that the security-critical target of the software system is
not exploitable.
Some notations are needed to describe the probabilistic model characterizing
this cognitive process. We deﬁne the software-target pair (s, x) as an exploitation
task, whose goal is to ﬁnd whether target x is reachable in software s from
its attack surface. We consider a null hypothesis H0(s, x), which simply states
that target x is unreachable in software s from its attack surface. Hence, the
unexploitability of target x in software s is quantiﬁed by the probability with
which the null hypothesis is true, i.e., P{H0(s, x) is true}, or simply P{H0(s, x)}.
For ease of presentation, we let the null hypothesis H0(s, x) be the evaluator’s
belief in the unreachability of target x in software s and P{H0(s, x)} her belief
level.
Available to the evaluator is also a list of reachability testing tools, as dis-
cussed in Sect. 3, for ﬁnding an injection vector from a software’s attack surface
to reach a security-critical target of interest. Let Z denote such a list of tools,
each of which works as follows: given target x in software s, tool z ∈Z either
outputs that x is not reachable in s from its attack surface, or an injection vector
that it detects to be able to reach target x. Given an injection vector v by a tool,
the evaluator can execute the software with the injection vector v to validate
whether target x can be reached. Like any other security detector, a reachability
testing tool may wrongly report that target x is unreachable in software s, or
misdetect a wrong injection vector as being able to reach target x.
Table 1. Tool parameters (IV: injection vector)
Truth/Result Unreachable Reachable, correct IV Reachable, wrong IV
Unreachable
α
0
1 −α
Reachable
β
γ
1 −β −γ

A Bayesian Cogntive Approach to Quantifying Software Exploitability
349
We thus model the performance of a reachability testing tool as probabilities
in Table 1. Each tool has three performance parameters, α, β, and γ: (1) The
truth is that the target is unreachable. A binomial process is used to characterize
the output of the tool, which returns a result of being unreachable with probabil-
ity α, and a result of being reachable with probability 1−α. (2) The truth is that
the target is reachable. The tool behaves as a multinomial process: it classiﬁes
the target as being unreachable with probability β, as being reachable with a
correct injection vector with probability γ, and as being reachable with a wrong
injection vector with probability 1 −β −γ.
The rationale behind choosing the binomial and multinomial processes in
our model is two-fold: they not only lead to a parsimonious model of human
recognition of tool performances (by simple counting), but also provide algebraic
convenience as their conjugate priors are well known. In a more ﬁne-grained
model, for the same tool z, the evaluator may associate diﬀerent parameter
values with some properties of the software (e.g., its size, type, or some other
metrics). To deal with such subtleties, for each tool z, the evaluator can associate
diﬀerent values of parameters α(z,k), β(z,k), and γ(z,k) when it is applied on
software of type k. Moreover, to reﬂect the dynamics of these parameters, we
use subscript t to indicate their values at time t. For example, α(z,k[s])
t
gives the
value of parameter α at time t when tool z is used on the type of software k[s].
Next we discuss how the evaluator, after using tool z for a new reachability
test, updates her posterior belief in the reachability of target x in software s. Let
the new observation made at time t be Ot, which falls into one of the following
types:
– Type E0: The tool detects target x to be unreachable in software s.
– Type E1: The tool detects that target x is reachable in software s, and also
returns an injection vector v, which is veriﬁed to be true by the evaluator.
– Type E2: The tool detects that the target x is reachable in software s, and
also returns an injection vector v, which is veriﬁed to be false by the evaluator.
After performing a reachability test with tool z and observing Ot from the
test at time t, her belief level in the unreachability of target x in software s is
updated to be the posterior probability P{H0(s, x)|Ot} according to Eqs. (1–3)
in Fig. 1.
P{H0(s, x)|Ot = E0} =
P{H0(s, x)} · α(z,k[s])
t
P{H0(s, x)} · α(z,k[s])
t
+ (1 −P{H0(s, x)}) · β(z,k[s])
t
(1)
P{H0(s, x)|Ot = E1} = 0
(2)
P{H0(s, x)|Ot = E2} =
P{H0(s,x)}·(1−α(z,k[s])
t
)
P{H0(s,x)}·(1−α(z,k[s])
t
)+(1−P{H0(s,x)})·(1−β(z,k[s])
t
−γ(z,k[s])
t
)
(3)
Fig. 1. Calculation of posterior probability after seeing the result from a reachability
test

350
G. Yan et al.
The calculation of Eqs. (1–3) is based upon the Bayes’ rule and the perfor-
mance of the reachability testing tool in Table 1. In Eq. (1), the observation is
that the tool detects the target to be unreachable. As the hypothesis H0(s, x)
states that the target is unreachable, the probability that the observation results
from the hypothesis being true is α(z,k[s])
t
. If the opposite hypothesis holds (the
target is reachable), the observation occurs with probability β(z,k[s])
t
. Hence,
Eq. (1) naturally follows based on the Bayes’ rule. Moreover, when it is observed
that the tool classiﬁes the target to be reachable with a correct injection vector,
it is certain that hypothesis H0(s, x) must not hold any more. This can be con-
ﬁrmed by Eq. (2) as P{E1|H0(s, x)} equals 0. Similarly, we can reason about the
case when the tool classiﬁes the target as being reachable but provides a wrong
injection vector, and derive Eq. (3).
5
Parameter Updating
In this section, we discuss how the evaluator dynamically updates the values of
the performance parameters (i.e., α, β, and γ) associated with each reachability
testing tool based on the Bayes’ rule. To evaluate the performance of a reacha-
bility testing tool, it would help if the ground truth is known to the evaluator.
For example, if it is known that target x is surely reachable from the attack
surface of software s, any tool that reports it being unreachable has a false neg-
ative error. One important observation, however, is that if it is true that target
x is unreachable in software s, it may never be veriﬁable by the evaluator for
a large software, although the opposite is not true: as long as a single injection
vector is found to reach target x, it is certain that the target must be reachable.
Hence, when no veriﬁable injection vector has been found yet to reach target
x from the attack surface of software s, a “relative fact” reﬂecting whether a
target has been found reachable is used to replace the truth in Table 1. There-
fore, for each reachability testing tool z ∈Z used on software of type k, the
evaluator keeps a performance counting table, or PCT (z,k), which contains ﬁve
performance counters c(z,k)
0
, ..., c(z,k)
4
as in Table 2. When the context is clear,
we drop the superscript (z, k).
Table 2. The performance counting table for tool z used on software of type k, i.e.,
PCT (z,k).
“Relative fact”/Result Unreachable Reachable, correct IV Reachable, wrong IV
Unreachable
c(z,k)
0
N/A
c(z,k)
1
Reachable
c(z,k)
2
c(z,k)
3
c(z,k)
4
The evaluator performs a sequence of software reachability tests, Q =
{q0, q1, ..., qt, ...}, where in qt = (st, xt, zt, ot), tool zt is used to test the reacha-
bility of xt in software st at time step t with observed test result ot. For ease of

A Bayesian Cogntive Approach to Quantifying Software Exploitability
351
explanation, we further deﬁne subsequences of software exploitation tests, each
corresponding to a speciﬁc software exploitation task (s, x):
Qs,x = {qt | st = s ∧xt = x},
(4)
and the ﬁrst element in Qs,x is given as Qs,x[0].
For exploitation task (s, x), parameters are updated based upon its mode
m(s, x): pre-exploitation and post-exploitation. In the pre-exploitation mode, the
evaluator has not found any injection vector that enables reachability of target x
in software s, and by contrast, in the post-exploitation mode, such an injection
vector has already been found. Initially, for every software exploitation task (s, x)
its mode m(s, x) is set to be pre-exploitation.
Consider the software reachability tests in Q sequentially. Given a new test
(s, x, z, o) in Q, which corresponds to the i-th one in Qs,x (i.e., Qs,x[i] =
(s, x, z, o)), the evaluator uses the following rules to update the performance
counters in table PCT (z,k[s]), where k[s] is the type of software s:
– Rule I applies to the case when o = E0. If m(s, x) is pre-exploitation, c(z,k[s])
0
increases by 1; otherwise, c(z,k[s])
2
increases by 1.
– Rule II applies to the case when o = E1. If m(s, x) is post-exploitation,
c(z,k[s])
3
increases by 1. Otherwise, if m(s, x) is pre-exploitation, the evalua-
tor has just found an injection vector to reach target x in software s. After
increasing c(z,k[s])
3
by 1, mode m(s, x) is changed from pre-exploitation to post-
exploitation. During this change of mode, the evaluator also needs to update
the performance counters for those tools that have been previously used to test
the software, as the “relative fact” that has been used to update these counters
previously turns out to be false. Hence, for every j with 0 ≤j < i, suppos-
ing that Qs,x[j] = (s, x, z′, o′), the following revision steps are applied: (1) if
o′ = E0, then decrease c(z′,k[s])
0
by 1 and increase c(z′,k[s])
2
by 1; (2) if o′ = E2,
then decrease c(z′,k[s])
1
by 1 and increase c(z′,k[s])
4
by 1. Note that it is impos-
sible to have o′ = E1 (otherwise, the mode must have already been changed
to post-exploitation after o′ is seen). Hence, the evaluator needs to revise the
performance counts based on the newly found truth that target x is reachable
from the attack surface of software s.
– Rule III applies to the case when o = E2. If m(s, x) is pre-exploitation,
c(z,k[s])
1
increases by 1; otherwise, c(z,k[s])
4
increases by 1.
The performance counters in table PCT (z,k) can be used to estimate the
parameters α(z,k)
t
, β(z,k)
t
, and γ(z,k)
t
at the current time t. We let the values
of the performance counters in table PCT (z,k) at time t be c(z,k)
i
(t), for i =
0, ..., 4. Using a frequentist’s view, parameters α(z,k)
t
, β(z,k)
t
, and γ(z,k)
t
could be
estimated as their relative frequencies. When few tests have been done, however,
the estimated values of α(z,k)
t
, β(z,k)
t
, and γ(z,k)
t
as derived may not be suﬃciently
reliable to characterize the performance of the reachability testing tool. This
resembles the scenario that a person, whose prior belief is that any coin is fair,

352
G. Yan et al.
would not believe that the coin will always produce head even after seeing three
heads in a row.
Our model, again, takes the evaluator’s prior belief into account when esti-
mating these parameters. After tool z is used to test whether target x is reachable
in software s, which is of type k, if m(s, x) is still pre-exploitation, the truth may
not be known to the evaluator. Without knowing the truth, the evaluator relies
on the “relative fact” that target x is not reachable from the attack surface of
software s. Therefore, depending on the current mode of exploitation task (s, x),
she updates the parameters as follows:
– If m(s, x) is pre-exploitation, tool z works as a Binomial process where it
returns a result of being unreachable with probability α(z,k). As the conjugate
prior for a Binomial process is a Beta distribution, we assume that the prior
for parameter α(z,k) takes a Beta(d(z,k)
0
+ 1, d(z,k)
1
+ 1) distribution. We use
the MAP (Maximum A Posteriori) estimate to update α(z,k):
α(z,k)
t
=
d(z,k)
0
+c(z,k)
0
(t)
d(z,k)
0
+c(z,k)
0
(t)+d(z,k)
1
+c(z,k)
1
(t)
(5)
– If m(s, x) is post-exploitation, tool z behaves as a multinomial process where
it returns being unreachable with probability β(z,k), being reachable with a
correct injection vector γ(z,k), and being reachable with a wrong injection
vector 1 −β(z,k) −γ(z,k). Similarly, as the conjugate prior for a multinomial
process is the Dirichlet distribution, we assume that the prior for parameter
(β(z,k), γ(z,k)) follows a Dirichlet distribution Dir(d(z,k)
2
+ 1, d(z,k)
3
+ 1, d(z,k)
4
+
1). We again use the MAP estimate to update β(z,k) and γ(z,k):
β(z,k)
t
=
d(z,k)
2
+c(z,k)
2
(t)
4
i=2 d(z,k)
i
+4
i=2 c(z,k)
i
(t)
(6)
γ(z,k)
t
=
d(z,k)
3
+c(z,k)
3
(t)
4
i=2 d(z,k)
i
+4
i=2 c(z,k)
i
(t)
(7)
The evaluator assumes target x to be unreachable from the attack surface
of software s if mode m(s, x) is pre-exploitation, and this assumption is used as
the relative fact to update the performance counters in related PCTs. However,
when a later test ﬁnds an exploitation for the task (s, x), which invalidates the
assumption, the parameters of those tools whose values have been previously
estimated based upon this relative fact should be updated to reﬂect this change
of mode. Mechanically, however, the evaluator can simply maintain PCTs like
Table 2, and whenever it is necessary to use parameters α, β, and γ in Eq. (1–3),
the tables are used to calculate their latest values based on Eq. (5–7).
6
Model Analysis
Space Complexity. The space used in the cognitive model includes those PCTs
that the evaluator uses to keep the aggregate results from previous software
reachability tests. It is noted that the prior information for parameter updating

A Bayesian Cogntive Approach to Quantifying Software Exploitability
353
(i.e., d0-d4) can be put in the tables as initial values; hence, each entry in the
table represents c(z,k)
i
(t) + d(z,k)
i
where 0 ≤i ≤4. Supposing that there are
|Z| reachability testing tools and |K| software types, as each PCT contains 5
entries (see Table 2), it requires 5|Z||K| to store the tables. Clearly, as the space
is linear with |K|, more ﬁne-grained categorization of software would bring more
cognitive burden to the evaluator unless auxiliary methods are used to help
remember these tables.
For every exploitation task (s, x), it is necessary to remember the evaluator’s
belief level P{H0(s, x)} and its current mode m(s, x). When an exploitation
task is in the pre-exploitation mode, the evaluator also needs to remember the
tools that have been previously used for the task, so if later an exploit is found,
the evaluator can take the revision steps to correct the performance counters
associated with these tools (Rule II of parameter updating). Therefore, if no
speciﬁc ordering scheme on the exploitation tools is used, the amount of tests
that the evaluator has to remember may be large, and in the worse case, it is |Q|.
To alleviate her cognitive burden, the evaluator may use auxiliary devices
(e.g., papers) for remembering the information needed in the model, or simplify
the model. For example, all the tools are numbered, and for every exploitation
task, these tools are always used in an increasing order. Rules can be used to
check if a tool is applicable for an exploitation task. Hence, when the mode
of an exploitation task changes from pre-exploitation to post-exploitation, the
evaluator can simply revise the PCTs of those applicable tools that are numbered
lower than the one ﬁnding the exploitation.
Time Complexity. Given the input Q, it is assumed that executing each of
Eqs. (1–3) takes a constant amount of time. For an exploitation task (s, x), chang-
ing its mode from pre-exploitation to post-exploitation requires updating the
performance counters of those tools that have previously been used on them.
However, for each reachability test in Q, revision of its result occurs at most
once. Therefore, the time complexity of the model is O(|Q|).
We can thus establish the following theorem regarding the complexity of the
model:
Theorem 1. The space and time complexity of the cognitive model is O(|Z||K|+
n + |Q|) and O(|Q|), respectively, where |Z| is the number of reachability testing
tools, |K| is the number of software types, n is the number of exploitation tasks,
and |Q| is the total number of reachability tests done by the evaluator.
6.1
Order Sensitivity
Equations (1–3) and (5–7) form a complex nonlinear system, whose input is
comprised of sequence Q, the initial states of the PCTs for all tools in Z, and
the prior values of P{s, x} for every exploitation task (s, x). We say that the
cognitive model is order insensitive if no matter how we change the order of
tests in Q, the following conditions are satisﬁed after all tests: (1) the evaluator’s
ﬁnal belief level for every exploitation task is the same, and (2) the states of all

354
G. Yan et al.
the PCTs are the same. It is noted that the mode of each exploitation task must
not change with the order of tests in Q: For any exploitation task (s, x), if its
mode is post-exploitation before tests in Q, its mode remains the same after
all tests in Q; otherwise, if there exists any test in Q for this task that leads
to observation E1, regardless of its order in Q, the mode of the task must be
changed to post-exploitation, or otherwise if no such test exists, its mode should
be pre-exploitation.
To understand under what circumstances the cognitive model is order insen-
sitive, we ﬁrst start with a simple case where there are only two reachability
tests in Q. We can establish the following lemma (proof in [35]):
Lemma 1. For
any
Q
=
[(s0, x0, z0, o0), (s1, x1, z1, o1)]
and
Q′
=
[(s1, x1, z1, o1), (s0, x0, z0, o0)], if (s0, x0) = (s1, x1) or z0 ̸= z1, the cognitive
model is order insensitive.
Now we consider the general case of array Q which may have more than
two tests. According to Lemma 1, for any two consecutive reachability tests in
a sequence, as long as they do not use the same reachability testing tool on two
diﬀerent exploitation tasks, we can swap their order. We call such a swapping of
consecutive reachability tests a safe swapping. Given a sequence of reachability
tests in Q, we can freely perform safe swappings on two consecutive tests without
aﬀecting the evaluator’s ﬁnal beliefs. We can thus establish the following theorem
(proof in [35]):
Theorem 2. For any sequence Q of software exploitation tests and Q′ one of its
permutations, assume that for every reachability testing tool, the relative order
of reachability tests using this tool is the same in Q and Q′. Then the evaluator’s
ﬁnal belief in every exploitation task must be the same after ﬁnishing Q and Q′.
6.2
Exploitability Analysis
We now consider under what conditions a new reachability test, (s, x, z, o),
improves the posterior probability P{H0(s, x)}. We consider the following cases.
Without loss of generality, we drop the subscripts of the parameters.
Observation o = E0: Given Eq. (1), in order to have P{H0(s, x) | Ot =
E0} > P{H0(s, x)}, we must have both α > β and 0 < P{H0(s, x)} < 1. If
P{H0(s, x)} = 1, the evaluator is certain that the target is not reachable a priori
and thus any new evidence does not improve the posterior probability. On the
other hand, if P{H0(s, x)} = 0, the Bayes’ rule tells us that the posterior proba-
bility is also 0. With α > β, it means that an unreachable target is detected to be
unreachable with a higher probability than a reachable target being mistakenly
classiﬁed as unreachable. Therefore, when a new test shows that the target is
unreachable, it is better to use the former as the explanation than the latter,
which suggests that the posterior probability P{H0(s, x)|E0} becomes higher
after the test.
Observation o = E1: Given Eq. (2), if the mode is still pre-exploitation,
then seeing the test result lowers the evaluator’s belief; otherwise, her belief
level remains to be 0.

A Bayesian Cogntive Approach to Quantifying Software Exploitability
355
Observation o = E2: Given Eq. (1), in order to have P{H0(s, x) | Ot =
E2} > P{H0(s, x)}, we must have: α < β + γ and 0 < P{H0(s, x)} < 1. The
same argument holds when P{H0(s, x)} = 0 or 1 as in the case when o = E0.
With α < β + γ or equivalently 1 −α > 1 −(β + γ), it is more likely that
an unreachable target is detected by the tool to be reachable with a wrong
injection vector than a reachable target being detected as reachable but with a
wrong input vector; hence, given the same observation E2, it is better to use the
former than the latter to explain the observation.
The above analysis leads to the following theorem:
Theorem 3. For an exploitation task in a pre-exploitation mode, with a reach-
ability testing tool of parameters α, β, and γ for the type of software in the task,
the test result by this tool boosts the evaluator’s belief level if and only if the eval-
uator’s prior belief is in (0, 1) and we have α > β if E0 is observed or α < β + γ
if E2 is observed.
7
Numerical Results
We perform experiments that simulate the Bayesian cognitive model, a system
of non-linear equations. The baseline conﬁguration of an experiment is shown in
Table 3. The reachability testing tools are those discussed in Sect. 3. As a reach-
ability testing software may behave diﬀerently under diﬀerent conﬁgurations,
they are treated as diﬀerent tools in our experiments. Parameter φ denotes the
true probability that an exploitation task is achievable. For the test ordering, the
tests are ﬁrst ordered by the software to be exploited and then for each software,
it is tested with the tools in the same order. The experiments mentioned in this
section use parameter settings in Table 3 unless stated otherwise. We assume
that the tests performed by all the tools are independent. For each tool, as the
initial counts in its PCTs are all 1’s, the evaluator’s prior estimations of α, β,
and γ are 1/2, 1/3, and 1/3, respectively.
Convergence of Estimated Parameters α, β, γ. In this set of experiments,
we study how the estimated parameters converge over time. We consider 10
reachability testing tools, which are used to test 10,000 software. For each tool,
its parameters α, β, and γ have true values, 0.75, 0.1, and 0.5, respectively. The
others are the same as in Table 3.
Table 3. Parameter settings in baseline cases
Parameter
Value Parameter
Value
Number of tools
100
Initial counts in PCTs All 1’s
Number of software 100
Parameter α
[0.2, 0.4, 0.6, 0.8]
Prior belief level
0.5
Parameters β, γ
[0.1, 0.2, 0.3, 0.4, 0.5]
Parameter φ
0.3
Test ordering
Order by software then tools

356
G. Yan et al.
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
 80000
 90000
 100000
Parameter α
Time step
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
 80000
 90000
 100000
Parameter β
Time step
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0
 10000
 20000
 30000
 40000
 50000
 60000
 70000
 80000
 90000
 100000
Parameter α
Time step
(1) Parameter α
(2) Parameter β
(3) Parameter γ
Fig. 2. Convergence of parameters α, β, and γ. The true values of these parameters
are 0.75, 0.1, and 0.5, respectively. In each time step, a reachability test is performed.
The ranges of these estimated parameters among the 10 tools in the last time step
are 0.0152, 0.0248, and 0.0292, which are 2.0 %, 24.8 %, and 5.8 % of their true values,
respectively.
Figure 2 shows the convergence of the parameters estimated by the evaluator.
We observe that the estimation of each parameter eventually converges towards
its true value, but the convergence occurs slowly. For instance, even after per-
forming reachability tests for 1000 software (i.e., after time step 10000 as each
software uses 10 time steps, one by each tool), the estimated value of each para-
meter is still not very stable. Also, although the 10 tools have the same true
values for their parameters, there is signiﬁcant variation among these tools after
10000 reachability tests.
Convergence of Belief Levels. In this set of experiments, we study the con-
vergence of the evaluator’s belief levels. Figure 3 presents, for each combination
of parameter settings, the average number of tests the evaluator needs to reach a
belief level of 99 % for a truly unexploitable software (left), along with the aver-
age number of tests to ﬁnd an exploit for a truly exploitable software (right).
We ﬁrst examine the results for truly unexploitable software. From Fig. 3(1),
we observe that for a truly unexploitable software, the average number of tests
required to reach a belief level of 99 % ranges from 3.6 to 27.2, showing a wide
variation across diﬀerent combinations of parameter settings. We also observe
that given the same parameters α and β, increasing γ reduces the number of
tests needed. This is because that the evaluator’s belief level is aﬀected by γ
only through Eq. (3), where a higher γ boosts her belief level. The observation
also agrees well with Theorem 3,
The eﬀects of parameter β, however, are not as straightforward with the same
α and γ. We observe that when α is small, a higher β reduces the number of
tests required, but when α is large, increasing β would also increase the number
of tests. This can be explained as follows. Note that both observations E0 and
E2 allow β to aﬀect the evaluator’s belief. When α is higher, the number of
observations of type E0 increases, and the importance of Eq. (1) becomes higher,
where a higher β decreases the evaluator’s belief level; by contrast, when α is
smaller, the number of observations of type E2 increases, which increases the
importance of Eq. (3), where a higher β increases the evaluator’s belief level.

A Bayesian Cogntive Approach to Quantifying Software Exploitability
357
α = 0.2
 0.1  0.2  0.3  0.4  0.5
β
 0.1
 0.2
 0.3
 0.4
 0.5
γ
 0
 5
 10
 15
 20
 25
 30
α = 0.4
 0.1  0.2  0.3  0.4  0.5
β
 0.1
 0.2
 0.3
 0.4
 0.5
γ
 0
 5
 10
 15
 20
 25
 30
α = 0.6
 0.1  0.2  0.3  0.4  0.5
β
 0.1
 0.2
 0.3
 0.4
 0.5
γ
 0
 5
 10
 15
 20
 25
 30
α = 0.8
 0.1  0.2  0.3  0.4  0.5
β
 0.1
 0.2
 0.3
 0.4
 0.5
γ
 0
 5
 10
 15
 20
 25
 30
α = 0.2
 0.1  0.2  0.3  0.4  0.5
β
 0.1
 0.2
 0.3
 0.4
 0.5
γ
 0
 2
 4
 6
 8
 10
 12
α = 0.4
 0.1  0.2  0.3  0.4  0.5
β
 0.1
 0.2
 0.3
 0.4
 0.5
γ
 0
 2
 4
 6
 8
 10
 12
α = 0.6
 0.1  0.2  0.3  0.4  0.5
β
 0.1
 0.2
 0.3
 0.4
 0.5
γ
 0
 2
 4
 6
 8
 10
 12
α = 0.8
 0.1  0.2  0.3  0.4  0.5
β
 0.1
 0.2
 0.3
 0.4
 0.5
γ
 0
 2
 4
 6
 8
 10
 12
(1) Left case: unexploitable software
(2) Right case: exploitable software
Fig. 3. Convergence of belief levels. The left case gives the average number of tests
before the evaluator’s belief level reaches 99 % for a truly unexplotable software, and
the right one the average number of tests before the evaluator’s belief level reaches 0 %
for a truly exploitable software.
We next study the results for truly exploitable software. From Fig. 3(2), we
observe that the range of tests required for the subject to ﬁnd a successful exploit
is from 1.9 to 11.0. The dominating factor is γ, where a higher γ reduces the
number of tests needed. This agrees well with our intuition that with tools that
are more capable of ﬁnding exploits, the evaluator needs fewer tests to ﬁnd
exploits.
Eﬀects of Prior Beliefs. In this set of experiments, we vary the evaluator’s
prior belief levels to study their eﬀects. Figure 4 presents the average number of
tests for the evaluator to reach a belief level of 99 % for a truly unexploitable
software and the average number of tests to ﬁnd an exploit for a truly exploitable
software. For the former, it is observed that a higher prior belief reduces the
number of tests to reach a certain belief level. This is because regardless of the
observation types (E0 or E2), the posterior belief increases monotonically with
the prior belief as seen in both Eqs. (1) and (3). At one extreme, if the evaluator
holds her prior belief ﬁrmly that the target must be reachable, any observation
that no exploitation has been found against the software does not change that
belief at all. That is to say, the number of tests for her to reach a belief of
99 % would be inﬁnity. At the other extreme, if the evaluator is certain that the
software is not exploitable, obviously it does not need any test for her to reach
a belief level of at least 99 %.
Furthermore, as reachability tests are performed independently, the average
number of tests to ﬁnd an exploit for a truly exploitable software is always 1/γ,
irrespective of the evalutor’s prior belief level. This is conﬁrmed by Fig. 4(2),
where the evaluator’s belief level does not change with the average number of
tests needed to ﬁnd an exploit.
Eﬀects of Test Ordering on Belief Convergence. We now study how
changing the order of software reachability tests aﬀects the evaluator’s belief
convergence. We perform three groups of experiments: In the ﬁrst group

358
G. Yan et al.
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Tests to reach belief of 99% 
 for unexploitable software
Prior belief
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
 11
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Tests to find exploitation 
 for exploitable software
Prior belief
(1) Unexploitable software
(2) Exploitable software
Fig. 4. Eﬀects of prior beliefs
(order-by-software-then-tools), the tests are ﬁrst ordered by the software to be
exploited and then for each software, we test it using 100 tools in the same order.
In the second group (order-by-tools-then-software), the tools are ﬁrst ordered,
and then for each tool, it is used to exploit the 100 software consecutively in the
same order. In the third group order-randomly, the reachability tests are ordered
randomly. Figure 5 again shows the average number of tests needed to reach a
belief level of 99 % for a truly unexploitable software (left) and the average num-
ber of tests to ﬁnd an exploit for a truly exploitable software (right).
Interestingly, we observe that given a truly unexploitable software, on aver-
age it takes more tests to reach a certain belief level in the group of order-by-
software-then-tools than those in the group of order-by-tools-then-software. The
key diﬀerence is illustrated by a simple example shown in Fig. 5(3), where three
tools, 1, 2, and 3, are used to test software A, B, and C. The test results of
applying tools 1, 2, and 3 on software A are E0, E2, and E1, respectively. For
ease of explanation, we assume that before the tests, the performance counters
c0, c1, c2, c3, and c4 of all the tools are all initialized to be 1. If the tests are ﬁrst
ordered by software and then tools (the upper row), then the ﬁrst three tests are
performed with the three tools on software A. After these three tests, because
tool 3 ﬁnds an exploitable path, the performance counters of the three tools are:
(1,1,2,1,1), (1,1,1,1,2), and (1,1,1,2,1). These counts will be used to update the
posterior belief levels of software B and C later. By contrast, if the tests are ﬁrst
 0
 5
 10
 15
 20
 25
 30
 35
 40
 = 0.2
 = 0.4
 = 0.6
 = 0.8
Number of tests
Parameter setting
Software-then-tools
Tools-then-software
Randomly
 0
 3
 6
 9
 12
 15
 = 0.2
 = 0.4
 = 0.6
 = 0.8
Number of tests
Parameter setting
Software-then-tools
Tools-then-software
Randomly
(1) Unexploitable software
(2) Exploitable software
(3) Illustration of effects of ordering
Fig. 5. Eﬀects of test ordering on belief convergence. The left case gives the average
number of tests needed to reach a belief level of 99 % for a truly unexploitable software,
and the right one the average number of tests to ﬁnd an exploit for a truly exploitable
software. For each α setting, the tests are sorted by the increasing order of tuple (β, γ).

A Bayesian Cogntive Approach to Quantifying Software Exploitability
359
ordered by tools and then software (the bottom row), after the ﬁrst test (tool
1 used on software A), the performance counters of tool 1 becomes (2,1,1,1,1)
and these counts are used to update the posterior belief levels on software B and
C in the second and third tests. Similarly, after the fourth test (tool 2 used on
software A), the performance counters of tool 2 becomes (1,2,1,1,1), which will
be used to update the posterior beliefs on software B and C next.
Hence, when the tests are ﬁrst ordered by software and then tools, if any
tool can ﬁnd an exploitable path of software, this fact can change the mode
of the software from pre-exploitation to post-exploitation and the performance
counters of the tools previously used to test this software are updated to reﬂect
this fact before they are used for testing other software. In contrast, if the tests
are ﬁrst ordered by the tools and then software, when the mode of the software
is changed from pre-exploitation to post-exploitation, the performance counters
of the tools previously used to test this software were updated assuming that
the software is unexploitable, and then used to update the posterior beliefs of
those software that were tested with these tools before the mode change.
How does such a diﬀerence aﬀect the evaluator’s posterior belief levels? For
the same observation E0, the performance counter c0 increases by 1 if the mode
is pre-exploitation, or c2 increases 1 if the mode is post-exploitation. As we have:
β
α = c2/(c2+c3+c4)
c0/(c0+c1)
=
1+c1/c0
1+(c3+c4)/c2 ,
(8)
post-exploitation updating increases β
α compared to pre-exploitation updating,
which further decreases the evaluator’s belief level after she sees E0 according
to Eq. (1).
Similarly, for the same observation E2, the performance counter c1 increases
by 1 in the mode of pre-exploitation, or c4 increases 1 in the mode of post-
exploitation. Since
1−β−γ
1−α
= c4/(c2+c3+c4)
c1/(c0+c1)
=
1+c0/c1
1+(c2+c3)/c4 ,
(9)
post-exploitation updating increases 1−β−γ
1−α
compared to pre-exploitation updat-
ing, which further decreases the evaluator’s belief level after E2 is observed
according to Eq. (3).
In summary, post-exploitation updating always reduces the evaluator’s belief
level for the software exploitation task at hand. This explains why more tests are
needed for the evaluator to reach a certain belief level when tests are ﬁrst ordered
by software and then tools than when they are ﬁrst ordered by the tools and then
software, because the former case has more post-exploitation updatings than the
latter, as seen in Fig. 5(1). To conﬁrm this, we did the experiments without any
observations of type E1 and then the diﬀerences in Fig. 5(1) between order-by-
software-then-tools and order-by-tools-then-software disappeared. Hence, there
seems to be an irony: postponing knowing that some software are exploitable helps
improve the evaluator’s belief level in the unexploitability of the others!
In Fig. 5(2), we present the average number of tests for the evaluator to ﬁnd a
successful exploit for a truly exploitable software. It is seen that the eﬀect of the

360
G. Yan et al.
order of the reachability tests is little. This is because the test results by diﬀerent
reachability testing tools are assumed to be independent. With a probability of
γ for any tool to ﬁnd the proper injection vector for an exploitable software, the
average number of tests needed is thus 1/γ.
Eﬀects of Short Memory. Recall that in the basic cognitive model, the evalu-
ator has to remember the test results for each software exploitation task that is
still in the pre-exploitation mode. According to Theorem 1, this may cause high
cognitive burden to the evaluator. Hence, in a new set of experiments, we study
the eﬀects of short memory, with which the evaluator omits the revision steps
in Rule II of parameter updating.
 0
 5
 10
 15
 20
 25
 30
 35
 = 0.2
 = 0.4
 = 0.6
 = 0.8
Tests to reach belief level of 99% 
 for unexploitable software
Parameter setting
Full memory
Short memory
 0
 2
 4
 6
 8
 10
 12
 14
 = 0.2
 = 0.4
 = 0.6
 = 0.8
Tests to find an exploitation 
 for exploitable software
Parameter setting
Full memory
Short memory
(1) Left case
(2) Right case
Fig. 6. Eﬀects of short memory. The left case shows the average number of tests needed
to reach a belief level of 99 % for an unexploitable software, and the right case the
average number of tests needed to ﬁnd an exploit for an exploitable software.
Figure 6 shows the eﬀects of having a short memory in parameter updating
on the evaluator’s belief convergence. We observe that due to a shorter mem-
ory, the evaluator needs fewer tests for her to reach a belief level of 99 % for
a truly unexploitable software, but the average number of tests for her to ﬁnd
a proper injection vector for a truly exploitable software changes little. Equa-
tions (8) and (9) can be used again to explain the smaller number of tests needed
to reach a certain belief level for a truly unexploitable software. When the mode
of an exploitation task changes from pre-exploitation to post-exploitation, hav-
ing a short memory has the following eﬀect for any tool that is previously used
for this task:
– If the observation in that test was E0, having a short memory omits moving 1
from c0 to c2. This makes β/α smaller based on Eq. (8), which increases the
evaluator’s belief level with a new observation E0 according to Eq. (1), but
makes (1−β−γ)/(1−α) larger due to Eq. (9), which decreases the evaluator’s
belief level with a new observation E2 according to Eq. (3).
– If E2 was observed in that test, having a short memory omits moving 1 from c1
to c4. This makes β/α larger based on Eq. (8), which decreases the evaluator’s
belief level with a new observation E0 due to Eq. (1), but makes (1 −β −
γ)/(1 −α) smaller due to Eq. (9), which improves the evaluator’s belief level
with observation E2 due to Eq. (3).

A Bayesian Cogntive Approach to Quantifying Software Exploitability
361
At ﬁrst glance, having a short memory has mixed eﬀects on a latter obser-
vation, be it E0 or E2. However, the key observation here is that the impact
of having a short memory on improving the evaluator’s belief level is positive if
the same type of observation is made later, and is negative otherwise. Hence,
if the distribution of observations is stationary over time as assumed in the
experiments, the positive impact outweighs the negative one. This resembles
the positive externality in economics. Therefore, having a short memory helps
improve the convergence of the evaluator’s belief level when the software is truly
unexploitable. On the other hand, as having a short memory does not aﬀect the
estimation of parameter γ, the average number of tests to ﬁnd a proper injection
vector for a truly exploitable software, which is 1/γ, is not aﬀected by a short
memory in parameter updating.
Eﬀects of Dependency. In another set of experiments, we evaluate eﬀects
of dependency on the evaluator’s belief convergence. To model the dependency
among the test results, we use the ﬁrst tool to test a software independently.
For any other tool, with probability p the test result is exactly the same as that
done by the ﬁrst one, and with probability 1 −p the result is independent of
those from the other tests. We vary dependence parameter p among 0.0, 0.2,
and 0.4. Figure 7 gives how the average number of tests needed to reach a belief
level of 99 % for a truly unexploitable software (left) and the average number
of tests needed to ﬁnd an exploit for a truly exploitable software (right) change
with parameter p.
 0
 5
 10
 15
 20
 25
 30
 35
 0
 0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
Tests to reach belief of 99% 
 for unexploitable software
Dependency
 0
 2
 4
 6
 8
 10
 12
 14
 16
 18
 0
 0.05  0.1  0.15  0.2  0.25  0.3  0.35  0.4
Tests to find exploitation 
 for exploitable software
Dependency
(1) Left case
(2) Right case
Fig. 7. Eﬀects of dependency. The left case shows the average number of tests needed
to reach a belief level of 99 % for a truly unexploitable software, and the right case the
average number of tests needed to ﬁnd an exploit for a truly exploitable software.
Clearly, when the test results by the tools become more similar, the evaluator
needs to perform more tests to reach the same belief level for a truly unexploitable
software, and also more tests to ﬁnd an exploit for a truly exploitable software.
To explain this phenomenon, we examine the distribution of observations per
software when α = 0.4, β = 0.2, and γ = 0.2. As the parameter setting is the
same for all the tools, we ﬁnd that the total number of observations of each
type (E0, E1, or E2) over all software is similar. However, when p = 0.4, the
distribution of these observations per software is more bursty than that when

362
G. Yan et al.
p = 0.0. That is to say, when p = 0.4, the variation of the numbers of the same
type of observations is higher across diﬀerent software than that when p = 0.0.
Diﬀerent types of observations increases (or decreases) the evaluator’s poste-
rior belief to diﬀerent degrees. For example, when β/α > (1 −β −γ)/(1 −α) or
equivalently, β > α(1−γ), the evaluator’s posterior belief after seeing E0 is lower
than that after seeing E2. As the rule of updating posterior beliefs is nonlinear,
the average number of tests required to reach a certain belief level on a truly
unexploitable software, or to ﬁnd an exploit for a truly exploitable software, is
not the same if we skew the distribution of diﬀerent types of observations among
diﬀerent software even though the total numbers of observations for the same
types of observations remain the same among all software.
Eﬀects of Lazy Evaluation. In this set of experiments, the reachability tests
are ﬁrst ordered by software and then by tools. There are 100 tools and 100
software to be exploited. We model a “lazy” evaluator who, after observing the
software is exploitable (i.e., seeing E1), stops using the remaining tools to test it.
Figure 8(1,2) shows the average number of tests needed for the evaluator to
reach a belief level of 99 % for a truly unexploitable software and the average
number of tests to ﬁnd an exploit for a truly exploitable software. The parameters
in the plots are ordered ﬁrst by α, then β, and lastly γ. According to Fig. 8(2),
lazy evaluation does not aﬀect the number of tests to ﬁnd an exploitation, which
is obvious as reachability tests are omitted only after the ﬁrst exploit has been
found for each software.
 0
 5
 10
 15
 20
 25
 30
 35
 = 0.2
 = 0.4
 = 0.6
 = 0.8
Tests to reach belief of 99% 
 for unexploitable software
Parameter setting
Full evaluation
Lazy evaluation
 0
 2
 4
 6
 8
 10
 12
 14
 = 0.2
 = 0.4
 = 0.6
 = 0.8
Tests to find an exploitation 
 for exploitable software
Parameter setting
Full evaluation
Lazy evaluation
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 10  20  30  40  50  60  70  80  90  100
Estimated 
Tool
Full evaluation
Lazy evaluation
 0
 0.2
 0.4
 0.6
 0.8
 1
 0
 10  20  30  40  50  60  70  80  90  100
Estimated 
Tool
Full evaluation
Lazy evaluation
(1) Left Case
(2) Right Case
(3) β
(4) γ
Fig. 8. Comparison of lazy evaluation with full evaluation (1,2) and estimation of
parameters β and γ (3,4). In lazy evaluation, the evaluator stops testing a software
after an exploit has been found. In contrast, full evaluation tests a software with all
the tools.
The eﬀects of lazy evaluation on the number of tests for the evaluator to
reach a belief level of 99 % for a truly unexploitable software are mixed: in some
cases, more tests are needed, and in others fewer are necessary. We examine
the estimated values of parameters α, β and γ when their true values are 0.2,
0.1, and 0.5, respectively. Lazy evaluation does not aﬀect much the estimation
of parameter α, but it only estimates the values of parameters β and γ for a
few tools, as seen in Fig. 8(3,4)! That is to say, for the majority of the tools,
parameters β and γ remain to be their initial values, which are 1/3 and 1/3,
respectively.

A Bayesian Cogntive Approach to Quantifying Software Exploitability
363
The diﬀerences between lazy evaluation and full evaluation as seen in Fig. 8
boil down to the diﬀerences in the estimated values of parameters β and γ. If
an observation of type E0 is seen, a larger β reduces the evaluator’s posterior
belief level (see Eq. (1)). On the other hand, if the new observation is of type
E2, then a larger β or γ helps improve the evaluator’s posterior belief level (see
Eq. (3)). With these observations, we can explain some cases where lazy evalu-
ation requires more tests for belief convergence than full evaluation in Fig. 8(1).
First, when α is small, there are more observations of type E2; as the majority of
the tools in lazy evaluation have parameters β and γ set to be both 1/3, if their
true values are higher than 1/3, lazy evaluation tends to underestimate their
true values and thus reduces the evaluator’s posterior belief level, which leads
to more tests needed compared to full evaluation. The eﬀect of parameter γ is
more prominent than that of β as the latter is mixed in Eqs. (1) and (2). On the
other hand, when α is large, there are more observations of type E0. If the true
value of β is smaller than 1/3, lazy evaluation always overestimates it and thus
reduces the evaluator’s posterior belief level according to Eq. (1), which leads to
more tests needed for belief convergence than full evaluation.
8
Concluding Remarks
In this work, we propose a new cognitive framework using Bayesian reasoning
as its ﬁrst principle to quantify software exploitability. We rigorously analyze
this framework, and also use intensive numerical simulations to study the con-
vergence of parameter estimation and the eﬀects of the evaluator’s cognitive
characteristics. In our future work, we plan to extend this work by integrating
into this framework some real-world tools (e.g., software fuzzers and concolic
execution tools) that can be used to exploit vulnerable software. We also plan
to enrich the cognitive model used in this work.
Acknowledgment. We acknowledge the support of the Air Force Research Labora-
tory Visiting Faculty Research Program for this work.
References
1. Crest: Concolic test generation tool for c. https://jburnim.github.io/crest/
2. http://www.securityweek.com/shellshock-attacks-could-already-top-1-billion-report
3. Stp constraint solver. http://stp.github.io/
4. https://nvd.nist.gov/
5. https://www.exploit-db.com/
6. http://www.osvdb.org/
7. The Yices SMT Solver. http://yices.csl.sri.com
8. Avgerinos, T., Cha, S.K., Hao, B.L.T., Brumley, D.: AEG: automatic exploit gen-
eration. NDSS 11, 59–66 (2011)
9. Bellovin, S.M.: On the brittleness of software and the infeasibility of security met-
rics. IEEE Secur. Priv. 4(4), 96 (2006)

364
G. Yan et al.
10. Brumley, D., Poosankam, P., Song, D., Zheng, J.: Automatic patch-based exploit
generation is possible: techniques and implications. In: IEEE Symposium on Secu-
rity and Privacy (2008)
11. Cadar, C., Dunbar, D., Engler, D.R.: KLEE: unassisted and automatic generation
of high-coverage tests for complex systems programs. OSDI 8, 209–224 (2008)
12. Cadar, C., Ganesh, V., Pawlowski, P.M., Dill, D.L., Engler, D.R.: EXE: automat-
ically generating inputs of death. ACM Trans. Inf. Syst. Secur. (TISSEC) 12(2),
10 (2008)
13. Cadar, C., Sen, K.: Symbolic execution for software testing: three decades later.
Commun. ACM 56(2), 82–90 (2013)
14. CERT.
Basic
fuzzing
framework
(bﬀ).
https://www.cert.org/
vulnerability-analysis/tools/bﬀ.cfm?
15. Cha, S.K., Avgerinos, T., Rebert, A., Brumley, D.: Unleashing mayhem on binary
code. In: IEEE Symposium on Security and Privacy (SP), pp. 380–394. IEEE
(2012)
16. Cha, S.K., Woo, M., Brumley, D.: Program-adaptive mutational fuzzing. In: Pro-
ceedings of the IEEE Symposium on Security and Privacy (2015)
17. Cooper, G.F.: The computational complexity of probabilistic inference using
Bayesian belief networks. Artif. Intell. 42(2), 393–405 (1990)
18. Godefroid, P., Levin, M.Y., Molnar, D.: SAGE: whitebox fuzzing for security test-
ing. Queue 10(1), 20 (2012)
19. Godefroid, P., Levin, M.Y., Molnar, D.A.: Automated whitebox fuzz testing. In:
Proceedings of Network and Distributed System Security Symposium (NDSS)
(2008)
20. Griﬃths, T.L., Kemp, C., Tenenbaum, J.B.: Bayesian models of cognition (2008)
21. Hoglund, G., McGraw, G.: Exploiting Software: How to Break Code. Addison-
Wesley, Boston (2004)
22. Jansen, W.: Directions in Security Metrics Research. Diane Publishing, Collingdale
(2010)
23. Lebiere, C., Bennati, S., Thomson, R., Shakarian, P., Nunes, E.: Functional cogni-
tive models of malware identiﬁcation. In: Proceedings of International Conference
on Cognitive Modeling (2015)
24. Manadhata, P.K., Wing, J.M.: An attack surface metric. IEEE Trans. Soft. Eng.
37(3), 371–386 (2011)
25. McMorrow, D.: Science of cyber-security. Technical report, JASON Program Oﬃce
(2010)
26. Nagaraju, S., Craioveanu, C., Florio, E., Miller, M.: Software vulnerability exploita-
tion trends (2013)
27. Nayak, K., Marino, D., Efstathopoulos, P., Dumitra¸s, T.: Some vulnerabilities are
diﬀerent than others. In: Stavrou, A., Bos, H., Portokalidis, G. (eds.) RAID 2014.
LNCS, vol. 8688, pp. 426–446. Springer, Heidelberg (2014)
28. Forum of Incident Response and Security Teams (FIRST). Common vulnerabilities
scoring system (cvss). http://www.ﬁrst.org/cvss/
29. Perfors, A., Tenenbaum, J.B., Griﬃths, T.L., Xu, F.: A tutorial introduction to
bayesian models of cognitive development. Cognition 120(3), 302–321 (2011)
30. Rebert, A., Cha, S.K., Avgerinos, T., Foote, J., Warren, D., Grieco, G., Brumley,
D.: Optimizing seed selection for fuzzing. In: Proceedings of the USENIX Security
Symposium (2014)
31. Microsoft Research. Z3. https://github.com/Z3Prover/z3
32. Smith, S.W.: Security and cognitive bias: exploring the role of the mind. IEEE
Secur. Priv. 5, 75–78 (2012)

A Bayesian Cogntive Approach to Quantifying Software Exploitability
365
33. Telang, R., Wattal, S.: An empirical analysis of the impact of software vulnerability
announcements on ﬁrm stock price. IEEE Trans. Soft. Eng. 33(8), 544–557 (2007)
34. Verendel, V.: Quantiﬁed security is a weak hypothesis: a critical survey of results
and assumptions. In: Proceedings of the 2009 Workshop on New Security Para-
digms Workshop. ACM (2009)
35. Yan, G., Kucuk, Y., Slocum, M., Last, D.C.: A Bayesian cogntive approach to
quantifying software exploitability based on reachability testing (extended version).
http://www.cs.binghamton.edu/∼ghyan/papers/extended-isc16.pdf
36. Younis, A., Malaiya, Y.K., Ray, I.: Assessing vulnerability exploitability risk using
software properties. Soft. Qual. J 24(1), 1–44 (2016)
37. Zhong, C., Yen, J., Liu, P., Erbacher, R., Etoty, R., Garneau, C.: An integrated
computer-aided cognitive task analysis method for tracing cyber-attack analysis
processes. In: Proceedings of the 2015 Symposium and Bootcamp on the Science
of Security. ACM (2015)

Control Flow Integrity Enforcement
with Dynamic Code Optimization
Yan Lin1(B), Xiaoxiao Tang1, Debin Gao1, and Jianming Fu2
1 School of Information Systems, Singapore Management University,
Singapore, Singapore
yanlin0816@gmail.com
2 Computer School, Wuhan University, Wuhan, China
Abstract. Control Flow Integrity (CFI) is an attractive security prop-
erty with which most injected and code reuse attacks can be defeated,
including advanced attacking techniques like Return-Oriented Program-
ming (ROP). However, comprehensive enforcement of CFI is expensive
due to additional supports needed (e.g., compiler support and presence
of relocation or debug information) and performance overhead. Recent
research has been trying to strike the balance among reasonable approx-
imation of the CFI properties, minimal additional supports needed, and
acceptable performance. We investigate existing dynamic code optimiza-
tion techniques and ﬁnd that they provide an architecture on which
CFI can be enforced eﬀectively and eﬃciently. In this paper, we propose
and implement DynCFI that enforces security policies on a well estab-
lished dynamic optimizer and show that it provides comparable CFI
properties with existing CFI implementations while lowering the overall
performance overhead from 28.6 % to 14.8 %. We further perform com-
prehensive evaluations and shed light on the exact amount of savings
contributed by the various components of the dynamic optimizer includ-
ing basic block cache, trace cache, branch prediction, and indirect branch
lookup.
Keywords: Control Flow Integrity · Return-oriented programming ·
Dynamic code optimization
1
Introduction
Control Flow Integrity (CFI) introduced by Abadi et al. [2] provides attractive
security features because of its eﬀectiveness in defending against most injected
and code reuse attacks, including the recent and advanced attacking techniques
like Return-Oriented Programming (ROP) [22]. Its basic idea is to enforce a
control-ﬂow graph (usually built from static analysis) so that the program only
makes control transfers to intended target locations.
However, having an accurate and practical enforcement of CFI is known
to be hard [2,13,18,23,27]. First, it is generally diﬃcult to accurately identify
the target locations for all control transfers. Existing solutions typically apply
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 366–385, 2016.
DOI: 10.1007/978-3-319-45871-7 22

Control Flow Integrity Enforcement with Dynamic Code Optimization
367
a coarse-grained policy (e.g., to allow indirect calls to any functions [25]) or
require compiler support or presence of relocation or debug information [4,19,27],
which may not be applicable to Commercial Oﬀ-The-Shelf (COTS) software.
Second, intercepting control transfers and doing the necessary checking typically
result in large performance overhead [7,10]. Many have proposed ways of striking
the balance among reasonable approximation of the CFI properties, minimizing
additional supports needed, and acceptable performance [20,28]. Therefore, any
noticeable reduction in the performance overhead would likely lead to more
practical implementation and potentially better security properties.
An interesting observation is that prior to the introduction of CFI in
2005, there have already been a lot of research on dynamic code optimiza-
tion to improve performance of dynamic program interpreters, e.g., Wiggins/
Redstone [11], Dynamo [3], Mojo [8], and DynamoRIO [5]. Dynamo and
DynamoRIO are among the more popular and mature ones. Dynamo targets
a PA-RISC machine and uses a speculative scheme MRET (Most Recently
Executed Tail) to pick hot traces without doing any path or branch proﬁling.
DynamoRIO uses the same scheme to pick hot traces, except that it targets
the x86-64 system. Although most of these were not proposed by the security
community, there is at least one noticeable work called program shepherding [15]
which makes use of a general purpose dynamic optimizer RIO [5] to enforce
security policies. DynamoRIO and program shepherding provide nice interfaces
for enforcing security policies on control transfers, which makes us believe that
they can be good candidate architectures for CFI enforcement. Since these well
established and mature dynamic code optimizers are proven to introduce min-
imal overhead, we believe that they could result in a system that outperforms
existing CFI implementations.
In this paper, we propose DynCFI that enforces a set of security policies on
top of DynamoRIO for CFI properties. We show that DynCFI achieves similar
security properties when compared to a number of existing CFI implementations
while experiencing a much lower performance overhead of 14.8 % as opposed to
28.6 % of BinCFI. We stress that DynCFI is not necessarily a CFI enforcement
implementation that has the lowest performance overhead. Instead, our contri-
bution lies on the utilization of the dynamic code optimization system which is a
matured system proposed and well studied before CFI was even introduced. To
the best of our knowledge, DynCFI is the ﬁrst implementation of CFI enforce-
ment on top of a dynamic code optimizer.
In the second half of this paper, we further investigate the exact contribution
to this performance improvement. We propose a three-dimensional design space
and perform comprehensive experiments to evaluate the contribution of each axis
in the design space in terms of performance overhead. Among many interesting
ﬁndings, we show that traces in the dynamic optimizer, which consist of cached
basic blocks stitched together, had contributed the most performance improve-
ment. Results show that traces have decreased the performance overhead from
22.7 % to 14.8 %. We also evaluate how branch prediction and indirect branch
lookup have changed the performance. To the best of our knowledge, this is the

368
Y. Lin et al.
ﬁrst comprehensive evaluation on the performance overhead contributed by var-
ious components of the system, and we believe that this detailed understanding
would aid future research and development of eﬃcient CFI enforcement systems.
The remainder of this paper is structured as follows. Section 2 summarizes
related work and outlines our motivation of using a dynamic optimizer. Section 3
introduces the security policies of DynCFI we enforce on top of DynamoRIO and
compares them with a number of existing CFI enforcement implementations. In
Sect. 4, we propose a three-dimensional design space for DynCFI and present a
set of experiments to evaluate the contributing factors of various components of
the dynamic optimizer. We present our security evaluation and some discussion
in Sect. 5. In the end, we conclude in Sect. 6.
2
Related Work and Motivation
In this section, we ﬁrst cover some important related work on CFI and dynamic
code optimization, and then motivate our idea of enforcing CFI on top of one of
the most well-established dynamic optimizers.
2.1
Control Flow Integrity
Control-ﬂow Integrity (CFI) was ﬁrst introduced by Abadi et al. [2]. The basic
idea of CFI is to mark the valid targets of indirect branches with unique iden-
tiﬁers and then insert ID-checks into the program before each indirect branch
transfer. Since its introduction in 2005, there have been a large body of CFI
variants introduced [4,10,12,20,25,27,28].
Some of these proposals focus on extracting accurate targets of indirect trans-
fers. For example, CFL [4] requires recompilation of the target application to
obtain such target information, and performs a “lock” operation before each
indirect control ﬂow transfer with a corresponding “unlock” operation at valid
destinations only. ROPdefender [10] makes use of the dynamic binary instru-
mentation tool Pin [16] to implement a shadow stack where the return addresses
are recorded and later compared with the return target address executed. It
suﬀers from performance issues due to its checking for every return instruction
executed. CFIMon [25] makes use of BTS [14] supported by hardware to col-
lect in-ﬂight branch transfers. Once the BTS buﬀer is full, a monitor process
will start to detect whether these branch transfers are valid. However, BTS is a
debugging mechanism that records all branches in a user-deﬁned memory area,
and there will be high overhead because of the large number of memory accesses.
In BinCFI [28], potential candidates of indirect branch targets are recorded and
all indirect branches are instrumented to be a jump to a CFI validation routine.
BinCFI will cause high performance overhead as it has to translate all indirect
branch targets executed, especially for programs which have a large percentage
of indirect branches. CFIGuard [26] uses Last Branch Record and Performance
Monitor Unit supported by hardware to record source and target addresses for
indirect branches, and then compare them with valid targets obtained ahead

Control Flow Integrity Enforcement with Dynamic Code Optimization
369
of time, but it relies on source code to obtain valid targets for indirect calls.
Lockdown [17] is implemented in a dynamic binary translation platform called
libdetox, which also uses shadow stack similar to ROPdefender to restrict the
targets of return branches. However, its security policy for indirect jumps is rela-
tively weak in allowing the target of a jump instruction to be any function entry
points or any addresses inside the current function. This gives a lot of ﬂexibility
to attackers in using various gadgets.
Others focus on eﬃcient ways of enforcing the CFI property for lower per-
formance overhead. For example, in CCFIR [27], all control ﬂow targets for
indirect branches are allocated on a so-called springboard section, and indirect
branches are only allowed to use control ﬂow targets contained in the spring-
board section.The main restriction is that it requires relocation information to
be included in the binaries. kBouncer [20] uses LBR [14] on Intel to record branch
transfers. It checks whether the target of a return instruction is call-proceeded
when a system call is invoked. It can be bypassed because the LBR mechanism
only records limited number of branch transfers. ROPGuard [12] also performs
CFI validation on Windows API calls. Like kBouncer, it requires that return
addresses are call-preceded and the memory word before each return address is
the start address of the API function.
In general, all existing proposals of CFI implementation enforce an approx-
imation of the original and strict security policies due to the lack of accurate
indirect transfer target information and performance considerations. Many have
to trade security for better performance of the resulting system. Research has
shown that some of these approximated CFI implementation are vulnerable to
various attacks [6,13,21]. Therefore, any noticeable reduction in the performance
overhead not only would lead to better user acceptance, but might translate into
a better approximation of the CFI security policy.
2.2
Dynamic Code Optimization
We notice that another body of work called dynamic code optimization, mostly
done by the software engineering community, could potentially be useful for
improving the performance overhead. Most of them build hot traces for blocks
frequently executed to boost execution. Dynamo [3], a dynamic optimizer for
a PA-RISC machine, acts as a native interpreter which allows it to observe
runtime behavior without instrumentation. Wiggins/Redstone [11] uses perfor-
mance counters on the Alpha to build traces. Mojo [8] uses the same mecha-
nism in Dynamo to pick hot traces and targets Windows NT running on IA-32.
DynamoRIO [5] is an x86 system based on Dynamo. Some of these platforms
provide nice interfaces of intercepting control ﬂow transfers of the target program
with very low overhead, e.g., DynamoRIO [5], to the extent that the overhead
could be negative (performance improvement) for some situations. Such plat-
forms could be perfect candidates on top of which CFI properties are enforced.
We are not the ﬁrst to make use of such systems for security purposes. Pro-
gram Shepherding [15] successfully makes use of DynamoRIO to restrict code

370
Y. Lin et al.
origins and control transfers. DynamoRIO provides a suitable platform for secu-
rity enforcement because the sandboxing checks added cannot be bypassed [15].
Due to this reason and the fact that it provides eﬃcient interfaces of intercepting
control ﬂow transfers, we choose it for our CFI enforcement, too.
2.3
DynamoRIO
Figure 1 shows an overview of DynamoRIO [5], with darker shading indicating
the application code to be monitored.
Fig. 1. Overview of DynamoRIO
DynamoRIO ﬁrst copies basic blocks into the basic block cache. If a target
basic block is present in the code cache and is targeted via a direct branch,
DynamoRIO links the two blocks together with a direct jump. If the basic block
is targeted via an indirect branch, DynamoRIO goes to the indirect branch
lookup routine to translate its target address to the code cache address. Basic
blocks that are frequently executed in a sequence are stitched together into the
trace cache. When connecting beyond a basic block that ends in an indirect
branch, a check is inserted to ensure that the actual target of the branch will
stay on the trace. If the check fails, it will go to the indirect branch lookup
routine to ﬁnd the translated address.
To make itself a secure platform on which programs are executed,
DynamoRIO splits the user-space address into two modes: the untrusted appli-
cation mode and the trusted and protected RIO mode. This design protects
DynamoRIO against memory corruption attacks. Meanwhile, the beauty of
DynamoRIO (and the corresponding good performance) come mainly from the
indirect branch lookup which is very eﬃcient in determining control transfer
targets with a hashtable. This hashtable maps the original target addresses with
addresses in the basic block cache and trace cache so that most control transfers
require minimal processing. We delay further details of DynamoRIO to Sects. 3
and 4 when we explain policies to be enforced on top of it and when we evaluate
the improved performance achieved by individual components of DynamoRIO.

Control Flow Integrity Enforcement with Dynamic Code Optimization
371
3
Design, Implementation, and Security Comparison
As discussed in Sect. 1, our motivation is to use DynamoRIO to enforce CFI
properties in anticipation for improved performance. Our objective is to design
a practical and eﬃcient CFI enforcement without the extra requirement of re-
compilation or dependency on debug information. In this section, we ﬁrst present
the design of DynCFI that can be eﬀectively enforced on DynamoRIO and the
implementation of it, and then compare the security property it achieves with
some existing CFI (and related defense) approaches.
3.1
Returns
The most frequently executed indirect control transfer instructions are returns.
DynCFI maintains a shadow call stack for each thread to remember caller infor-
mation and the corresponding return address. The whole process is shown in
Fig. 2. For a call instruction, we store the return address on our shadow stack.
For a return instruction, we check whether the address on the shadow stack
equals to the address stored at the stack memory speciﬁed by %esp. Such a
shadow stack enables DynCFI to apply a strict policy that only returning to the
caller is allowed, although a relaxed version could also be applied to reduce over-
head (see Sect. 3.4 for more discussion). DynCFI also takes care of the following
exceptions in special cases.
Fig. 2. Shadow stack operations
– Signals: A signal comes with a return address but not a call instruction.
Fortunately, DynamoRIO records all necessary signal information for us to
maintain a correct shadow stack.
– Lazy binding: The procedure dl runtime resolve() in lazy binding uses
ret (without a corresponding call) to perform a jmp operation. The pattern
of the code is fairly easy to identify though.
– setjmp and longjmp: setjmp and longjmp allow bypassing of multiple stack
frames. We pop out return addresses continuously until a match is found or
when the shadow stack is empty.
– C++ exception handling: We use the second argument of Unwind SetIP
as the return address for proper enforcement of our policy.

372
Y. Lin et al.
3.2
Indirect Jumps and Indirect Calls
We further classify indirect jumps into normal indirect jumps and PLT jumps,
such as jmp offset (base register), which are used to call functions in other
modules, target of which can only be exported from other modules. To obtain
target information for every indirect branch, we use the static analysis engine
provided by another well-known CFI enforcement BinCFI [28], which combines
linear and recursive disassembling techniques and uses static analysis results to
ensure correct disassembling. Targets of indirect calls are function entry points
and targets of indirect jumps are function entry points and targets of returns.
Meanwhile, targets of PLT jumps are exported symbol addresses. These valid
jump and call targets are organized into three diﬀerent hashtables to improve
performance—one for indirect jumps, one for indirect calls, and one for PLT
jumps. Most importantly, the shadow stack and hashtables readable only in the
untrusted application mode, so attackers cannot modify them.
3.3
Implementation
As discussed in Sect. 2, DynamoRIO maintains a hashtable that maps original
control transfer target addresses with addresses of code caches. The hashtable has
to be built when the control transfer occurs the ﬁrst time though. This process,
together with the dispatcher which is invoked when matches are not found in the
hashtable (see Fig. 1), become the natural place of our CFI enforcement, since
CFI mainly concerns control transfer targets.
We obtained the source code of DynamoRIO version 5.0.0 from the devel-
oper’s website [1], and added more than 700 lines of code (in C) to implement
DynCFI. Most of the additional code is added to the dispatcher where checks
of control ﬂow transfers are performed. Some code is also added to basic block
cache building to implement our shadow call stack and to initialize DynamoRIO
to load the valid jump/call target addresses into our own hashtables.
DynCFI does not implement the full sets of CFI properties originally pro-
posed by Abadi et al. [2]. We only perform checks on indirect control transfers
at the ﬁrst time when the target of an indirect branch occurs. However, it does
not really impact security, and it is exactly the reason why DynamoRIO is
widely accepted as an eﬃcient dynamic optimizer—original code is cached in
short sequences and security policies, if any, need only be checked the ﬁrst time
the code cache is executed [5]. Subsequent executions of the same code cache
will be allowed (without checking) as long as the control transfer targets remain
unchanged. Any violations to our policy will miss the (very eﬃcient) indirect
branch hashtable lookup and go back to the dynamic interpreter which will con-
sider the control transfer a ﬁrst timer and perform all the checks (ineﬃcient).
3.4
Security Comparison
Table 1 shows the security policy of DynCFI when compared with some existing
CFI implementations and ROP defense solutions. A caveat here is that we make

Control Flow Integrity Enforcement with Dynamic Code Optimization
373
Table 1. Security comparison with other CFI and ROP defenses
Approach
Policy
Return
Indirect jump
Indirect call
PLT jump
BinCFI [28]
Call-preceded
Function entry, return
address
Function entry
Exported symbol
address
CCFIR [27]
Corresponding springboard section
Nil
CFIMon [25]
Call-preceded
Any address in the
training set
Any function
entry
Nil
ROPdefender [10]
Caller
Nil
Nil
Nil
kBouncer [20]
Call-preceded
Nil
Nil
Nil
LockDown [17]
Caller
Function entry,
instruction in the
current function
Function entry
Nil
DynCFI
First execution:
Caller, Others:
Call-preceded
Function entry, return
address
Function entry
Exported symbol
address
use of the shadow call stack information only when a new target is added to the
hashtable. This will make the policy eﬀectively call-proceeded only. Since call-
proceeded policy is widely considered as adequate by many other approaches, we
apply this performance improvement in our subsequent evaluation. This relaxed
policy also enables a fair comparison between DynCFI and other CFI enforce-
ment schemes since many others also use a call-proceeded policy.
DynCFI achieves similar security when compared with these existing
approaches. In particular, DynCFI is mostly comparable to BinCFI in that
both maintain a list of valid target addresses to be checked at runtime, with
one noticeable diﬀerence in the enforcement mechanism: BinCFI enforces the
policies with static instrumentation to translate indirect target address while
DynCFI uses DynamoRIO as the interpreter platform. This makes BinCFI the
perfect candidate for performance overhead comparison with DynCFI, which is
the topic of our next Section.
4
Detailed Performance Proﬁling
In this section, we conduct a comprehensive set of experiments on the perfor-
mance overhead of DynCFI. Besides the overall performance overhead, we run
some detailed performance proﬁling to ﬁnd out the contribution to such overhead
by various components of the dynamic optimizer. We wish that such a detailed
proﬁling could shed light on the part that contributes most to the performance
overhead, and give guidance to future research in further improvement.
To better understand our evaluation strategy, we present our ﬁrst attempt
in the proﬁling, show the results, and explain the limitation of this attempt.
We then choose an existing CFI implementation for the detailed comparison
with DynCFI. We analyze the design space of CFI enforcement implementation
and organize it along three axes on which the two systems under comparison
could be clearly identiﬁed. Lastly, we perform a sequence of experiments by

374
Y. Lin et al.
modifying individual components of DynCFI so that the contribution of each to
performance overhead can be evaluated.
4.1
Target Applications
To evaluate the performance overhead, we need to subject DynCFI (and another
CFI implementation for comparison purposes) to some applications. To enable
fair comparison with existing work, we used twelve pure C/C++ programs we
can ﬁnd in SPEC CPU2006, which are also used in the evaluation of the original
work of BinCFI [28], as our benchmarking suite.
Experiments were executed on a desktop computer with an i7 4510u CPU
and 8 GB of memory running x86 version of Ubuntu 12.04. Each individual
experiment was conducted 10 times, average of which is reported in this paper.
4.2
First Attempt in Performance Proﬁling
As an initial attempt to understanding the performance overhead contributed
by various components of DynCFI, we use program counter sampling to record
the amount of time spent in various components of DynCFI. We use the
ITIMER VIRTUAL timer which counts down only when the process is executing
and delivers a signal when it expires. The handler used for this signal records the
program counter of the process at the time the signal is delivered. We sample
the program counter every ten milliseconds.
Table 2 shows the percentage of time each application spends in various steps
in DynCFI. It suggests that more than 90 % of the time is spent on the applica-
tion’s code on average. Other non-negligible processes include Indirect Branch
Lookup (IBL) inlined with the application’s code and that not inlined, basic
block and trace cache building, as well as the dispatcher.
In an attempt to explain why some applications, e.g., gcc, omnetpp, soplex,
and povray, incur larger overhead, we count the number of diﬀerent control
transfers in each application (runtime) and present statistics in Table 3. The
correlation between the two tables suggests that larger number of control trans-
fers could lead to the higher overhead.
Although it sounds like we have obtained detailed understanding of the per-
formance overhead, there is one important factor that we have overlooked so
far—the overhead contribution of the dynamic optimizer on executing the appli-
cation’s code (second column of Table 2). In other words, Table 2 does not tell
us if the dynamic optimizer had sped up or slowed down the execution of the
application’s code, and what had contributed to that speedup or slowdown. Our
further comparison veriﬁes this suspicion, see Table 4, as there is noticeable dif-
ference in the amount of time spent.
Therefore, we want to further investigate the contribution of various com-
ponents of the dynamic optimizer in speeding up or slowing down the applica-
tion’s code. We present our second attempt in the rest of this section. With the
objective of ﬁnding out contributions to the performance overhead by individual
components of the dynamic optimizer, our strategy is to

Control Flow Integrity Enforcement with Dynamic Code Optimization
375
Table 2. Percentage of time spent on various components
Application
Application
IBL
IBL
Basic block
Trace
Dispatch
Others
code
inlined
not inlined
building
building
bzip2
97.99
0.60
0.00
0.20
1.20
0.00
0.00
gcc
86.78
7.46
0.26
0.91
3.42
1.10
0.07
mcf
97.48
0.42
1.26
0.14
0.07
0.14
0.49
gobmk
80.00
1.08
0.00
2.70
11.35
4.86
0.00
sjeng
94.10
5.67
0.11
0.02
0.09
0.02
0.00
libquantum
99.51
0.49
0.00
0.00
0.00
0.00
0.00
omnetpp
84.88
14.50
0.38
0.06
0.15
0.03
0.01
astar
94.36
4.79
0.78
0.00
0.01
0.04
0.01
namd
99.89
0.69
0.00
0.00
0.02
0.00
0.00
soplex
74.21
25.42
0.03
0.10
0.10
0.10
0.02
povray
89.71
6.88
0.82
0.76
1.01
0.76
0.06
lbm
99.99
0.00
0.00
0.00
0.01
0.00
0.00
Average
91.57
5.62
0.30
0.41
1.45
0.59
0.06
Table 3. Statistics of diﬀerent types of control transfers
Application
%Indirect call
%Indirect jump
%Return
%Direct branch
Total
bzip2
0.002
0.002
0.774
99.222
2813437750
gcc
0.434
1.958
7.767
89.841
40789466606
mcf
0.001
0.029
5.402
94.568
5000155956
gobmk
0.001
0.027
4.811
95.161
687830197
sjeng
1.072
2.289
4.718
91.921
122978889385
libquantum
0.000
0.000
0.242
99.758
706839248554
omnetpp
1.609
1.763
33.998
62.630
87535408451
astar
1.698
0.049
19.738
78.515
30621019276
namd
0.000
0.008
3.292
96.700
115933566091
soplex
0.002
0.018
23.239
76.741
73160950993
povray
2.776
0.154
26.279
70.791
8195937460
lbm
0.000
0.017
0.035
99.948
15270883768
1. Find an existing CFI implementation X for comparison.
2. Continuously disable or modify individual components of DynCFI so that
the modiﬁed system eventually becomes similar to the implementation of X.
3. In every step of disabling or modifying the components, perform experiments
to ﬁnd the corresponding (diﬀerence in) performance overhead.
4.3
Picking BinCFI for Detailed Comparison
With this strategy, it is important that we choose an X that

376
Y. Lin et al.
Table 4. Time spent in application code
Application in DynCFI (sec) Natively (sec) Overhead (%)
bzip2
4.88
4.86
0.41
gcc
60.73
56.25
7.96
mcf
13.91
14.19
−1.97
gobmk
1.48
1.35
9.62
sjeng
158.93
150.01
5.95
libquantum 813.12
821.63
−1.04
omnetpp
138.54
122.23
13.34
astar
76.16
75.44
0.95
namd
735.51
733.73
0.24
soplex
64.81
61.15
5.98
povray
14.21
14.12
0.64
lbm
375.45
388.14
−3.27
– Is an independent, state-of-the-art implementation of CFI enforcement;
– Shares the same high-level idea with DynCFI while validating control transfers
with a diﬀerent approach (e.g., by binary instrumentation) from that of the
dynamic optimizer/interpreter as in DynCFI.
so that our evaluation could attribute the diﬀerence in performance overhead to
the dynamic optimizer.
As discussed at the end of Sect. 3, BinCFI and DynCFI are similar in that
both maintain a set of valid control transfer targets and use a centralized valida-
tion routine for CFI enforcement. In both cases, the validation routine maintains
a hashtable for the valid control transfer targets. Figure 3 shows the work-ﬂow
of BinCFI.
Fig. 3. Overview of BinCFI
The diﬀerence between BinCFI and DynCFI is that BinCFI obtains the valid
target addresses of indirect branches statically and records their corresponding
instrumented target addresses into the hashtable, and then replaces the indirect

Control Flow Integrity Enforcement with Dynamic Code Optimization
377
instructions with a direct jump to the CFI validation routine. BinCFI satisﬁes
our requirements for the performance comparison, and is therefore chosen for
our subsequent detailed evaluation.
4.4
Overall Comparison and the Design Space
The overall performance overhead of executing the benchmarking applications
under (original, unmodiﬁed) DynamoRIO, DynCFI, and (original, unmodiﬁed)
BinCFI is shown in Fig. 4. Results are shown in terms of percentage overhead
beyond natively executing the applications on an unmodiﬁed Linux Ubuntu
system. We obtained the source code implementation of BinCFI [28] from its
authors.
Fig. 4. Overall performance overhead
An interesting observation is that the original DynamoRIO and DynCFI do
not diﬀer much in terms of overhead (a relatively small 1.3 % diﬀerence). This
shows that the interfaces provided by DynamoRIO are convenient and eﬀective
for CFI enforcement, which conﬁrms our intuition since DynamoRIO intercepts
all control transfers and no additional intercepting is needed in our modiﬁcation
to DynamoRIO.
DynCFI experiences a signiﬁcantly smaller overhead of 14.8 % compared to
BinCFI at 28.6 %. This suggests that the dynamic optimizer provides a more eﬃ-
cient platform for CFI enforcement compared to existing approaches like binary
instrumentation as in BinCFI. That said, the two systems diﬀer in other aspects
and therefore this overall evaluation result is insuﬃcient in attributing the major-
ity of the performance gain to mechanisms of the dynamic optimizer.
As discussed in Sect. 4.3, our strategy to this diﬃculty is to continuously dis-
able or modify individual components of DynCFI so that eventually it becomes
similar to BinCFI, in terms of their operating mechanism as well as the per-
formance overhead. By doing so, we would likely observe degradation of perfor-
mance (increase in overhead) of the modiﬁed system which is deﬁnitely due to the
corresponding feature disabled or modiﬁed. The question is – which individual
component or feature to disable or modiﬁed?

378
Y. Lin et al.
To answer this question, we analyze the internal validation mechanisms of the
two approaches and identify three main factors that could signiﬁcantly contribute
to the diﬀerent performance overhead.
1. Trace. Trace is the most important mechanism in DynamoRIO to speed
up indirect transfers. Traces are formed by stitching together basic blocks
that are frequently executed in a sequence. Beneﬁts include avoiding indirect
branch lookups by inlining a popular target of an indirect branch into a trace
(with a check to ensure that the target stays on the trace and otherwise fall
back to the full security check), eliminating inter-block branches, and helping
branch prediction. Trace is unique in DynamoRIO and is not in BinCFI.
2. Branch prediction. Modern processors maintain buﬀers for branch predic-
tion, e.g., Branch Target Buﬀer (BTB) and Return Stack Buﬀer (RSB). The
eﬀectiveness of these predictors could get seriously aﬀected due to the modiﬁ-
cations to the control transfers. For example, turning a return instruction into
a indirect jump would make RSB useless in the branch prediction, potentially
leading to an increase in the performance overhead.
3. Indirect branch lookup routine. Besides implementation details that
are not necessarily due to the architectural design (to be discussed more
in Sect. 4.5), a dynamic optimizer could use a single lookup routine for the
entire application including the dynamically loaded libraries, while systems
that apply static analysis and binary instrumentation would likely have to use
a dedicated lookup routine for each module because some dynamically loaded
libraries might not have been statically analyzed or instrumented. This could
contribute to noticeable diﬀerences in performance overhead.
We want to explore details into these three axes to see how each of them
aﬀects the performance overhead. Other factors that might contribute to the
overhead in DynCFI which we do not further investigate include
– Building basic block caches;
– Building trace caches;
– Inserting new entries into hashtables;
– Context switches between DynamoRIO and code caches.
4.5
Proﬁling Along the Three Axes
With identiﬁcation of the three axes, we make our second attempt in detailed
understanding of the performance overhead of the two systems. Since executing
on DynCFI and executing on the original unmodiﬁed DynamoRIO experience
about the same overhead (see Fig. 4), our subsequent experiments will only focus
on comparing DynCFI and BinCFI. Also recall that our strategy is to disable
or modify one component of DynCFI at a time and observe the corresponding
change in performance overhead.

Control Flow Integrity Enforcement with Dynamic Code Optimization
379
4.5.1
Traces. Traces are unique in dynamic optimizers like DynamoRIO and
DynCFI. There are potentially two ways in which traces impact the performance
overhead. First, the stitching of basic blocks together eliminates some inter-block
branches. Second, each trace has inlined code to check if the control transfer
target is still on the trace (we call this InT). If the target is still on the trace,
execution will just carry on without further checking; otherwise, a second inlined
code (we call this InH) is executed to perform hashtable lookup without collisions.
If collision happens, execution will go to the full indirect branch lookup routine
(denoted as R). We examine contribution of InT and InH by disabling them
individually. We also examine the eﬀect of traces overall and present the results
in Fig. 5.
Fig. 5. Impact of trace on overhead
Figure 5 shows that the contribution due to InT is big, averaging to 5.5 %.
Exceptions go to bzip2 and soplex which do not gain much with InT mainly
because the fall-back of InH is very eﬀective on them (which can be veriﬁed from
the next-to-zero time spent in IBL not inlined in Table 2).
Although performance overhead increases when disabling InT (see Fig. 5),
DynCFI is still better than BinCFI. When disabling traces altogether, the over-
head of DynCFI increases from 14.8 % to 22.7 % on average, with some going over
the overhead in BinCFI. This shows that traces are contributing signiﬁcantly in
the low overhead of DynCFI. For applications with a large percentage of indirect
branches (see Table 3), DynCFI with traces disabled still outperforms BinCFI.
This suggests that there are other contributing factors in DynCFI which we have
not evaluated.
4.5.2
Branch Prediction. The way in which DynCFI and BinCFI inter-
cept and deliver control ﬂow transfers has an implicit eﬀect on branch predic-
tion. Branch prediction is typically achieved by remembering a history of con-
trol transfer targets by the same instruction. Both DynCFI and BinCFI could
weaken branch prediction due to R using the same instruction (an indirect jump)
to execute control transfers originally executed by diﬀerent instructions in the

380
Y. Lin et al.
application [5,28]. Table 5 summaries how indirect control transfers in an appli-
cation are executed in DynCFI and BinCFI.
Table 5. Execution of indirect control transfers
Original transfer
Return Indirect call/jump
DynCFI
Basic block cache Jump to R, indirect jump to target
Trace cache
InT or InH or jump to R, indirect jump to target
BinCFI
Return jump to R, indirect jump to target
In summary, DynCFI leads BinCFI in retaining branch prediction for indi-
rect calls and jumps when trace caches are used due to InT and InH; however,
BinCFI would perform better than DynCFI for returns. That said, note that
there are typically far more return instructions than indirect calls and jumps
executed for all the applications in our benchmarking suite, see Table 3.
To better understand the eﬀect of various components of DynCFI and
BinCFI on branch prediction, we count the number of mispredictions when
executing the benchmarking applications on a number of diﬀerent settings –
DynCFI, DynCFI with InT disabled, DynCFI with InH disabled, DynCFI with
traces disabled, BinCFI, BinCFI with returns being replaced by jumps to R,
and present the results in Fig. 6.
Fig. 6. Impact of traces on the number of branch mispredictions
We observe that disabling InH has a larger impact on branch prediction than
disabling InT in general. This shows that the inlined hashtable lookup has its
fair share of its contribution on lower overhead. It also indirectly shows that the
hashtable implementation in DynCFI is good in that collisions do not happen
often (since R not inlined is not executed often as shown in Table 2). Another
interesting ﬁnding is that replacing returns with indirect jumps on BinCFI adds
a large number of mispredictions for some programs. In terms of overhead, this
translates to about 2 % more in the overhead as shown in Fig. 7.

Control Flow Integrity Enforcement with Dynamic Code Optimization
381
Fig. 7. Impact of branch prediction on overhead
4.5.3
Indirect Branch Lookup Routine R. The indirect branch lookup
routine in DynCFI and BinCFI very much shares the same strategy. Both use
an eﬃcient implementation of a hashtable to record valid control transfer targets.
One noticeable diﬀerence, though, is that BinCFI requires an extra step to check
if the target resides within the same software module before directing control
to the corresponding R. Each software module has to implement its own copy
of R because some dynamically loaded libraries might not have been statically
analyzed or instrumented and BinCFI cannot use a centralized R for all modules.
On the other hand, DynCFI executes the application on top of a dynamic
interpreter without static analysis or binary instrumentation, and therefore has
three centralized R (one for returns, one for indirect jumps, and one for indirect
calls) for all software modules. This architectural diﬀerence contributes to some
additional performance overhead to BinCFI.
Besides the diﬀerence due to the architectural design, there are also lower
level diﬀerences in implementing R between DynCFI (inheriting the same R
from DynamoRIO) and BinCFI. In particular, they diﬀer in the indirect jump
instructions used (DynCFI uses a register to specify the target while BinCFI
uses a memory), the number of registers used throughout the algorithm (and as
a result the number of registers to be saved and restored), and eﬃciency of the
hashtable lookup algorithm.
To evaluate the contribution of R in the overall performance overhead, we
replace R in both DynCFI (with traces disabled) and BinCFI (with returned
replaced with indirect jumps) with R′, our (supposedly more eﬃcient) implemen-
tation of the algorithm, and show the resulting performance overhead in Fig. 8.
Comparing these results with those shown in Fig. 7, we ﬁnd that such low-
level details in the implementation of R translates to signiﬁcant diﬀerences in
the overhead. In particular, the diﬀerence between DynCFI and BinCFI shrinks
with R′ replacing R, indicating that the original R used in DynCFI is more
eﬃcient than that in BinCFI.
4.5.4
Summary. Recall that our strategy in the second attempt of detailed
proﬁling of DynCFI is to continuously disable or modify various components to

382
Y. Lin et al.
Fig. 8. Performance overhead with uniﬁed R′
ﬁnd the contribution of them in terms of performance overhead. Figure 8 shows
the comparison between DynCFI with traces disabled (bringing both systems to
the same conﬁguration on the ﬁrst axis) and BinCFI with returns replaced by
indirect jumps (bringing both systems to the same conﬁguration on the second
axis) while they use the same R′ (bringing both systems to the same conﬁguration
on the third axis). They are fairly close to each other in their performance,
conﬁrming that we manage to attribute their originally large diﬀerence being
successfully attributed to the three axes.
We therefore believe that this second attempt provides a successful and accu-
rate detailed proﬁling for DynCFI and BinCFI. With the detailed understanding
of the contribution of each components on the three axes, we hope that future
research could improve the performance further by, e.g., designing an indirect
branch lookup routine that results in better branch prediction.
5
Security Evaluation and Discussions
5.1
Real World Exploits
We use a publicly available intrusion prevention evaluator RIPE [24] to verify
that DynCFI oﬀers comparable security properties with existing CFI propos-
als (as analysis presented in Sect. 3.4). In particular, we check if DynCFI can
detect exploits that employ the advanced Return-Oriented Programming (ROP)
techniques.
RIPE contains 140 return-to-libc exploits out of which 60 exploit return
instructions and 80 exploit indirect call instructions. For the 60 exploits on
return instructions, our experiments conﬁrm that DynCFI manages to detect
all of them because they violate the call-preceded policy we enforced on return
instructions. RIPE also contains 10 ROP attacks using return instructions, which
are all successfully detected by DynCFI as the targets of these gadgets are not
call-preceded.
DynCFI and BinCFI share the weakness in detecting exploits that change
the value of a function pointer to a valid entry point of a function. Such attacks
cannot be detected by most other CFI implementations either [25].

Control Flow Integrity Enforcement with Dynamic Code Optimization
383
5.2
Average Indirect Target Reduction
Zhang and Sekar [28] propose a metric for measuring the strength of CFI called
Average Indirect target Reduction (AIR). As DynCFI uses diﬀerent policy on
return branches, we apply the same metric to test DynCFI when applied to
the SPEC benchmarking suite. We compares the AIR metrics for DynCFI and
BinCFI. We can ﬁnd that average AIR for DynCFI is 98.80 %, which is compa-
rable to 98.86 % for the case of BinCFI.
5.3
Shadow Stack in Full Enforcement
As described in Sect. 3, in order to improve the performance, we do not check
the shadow call stack if the target address is found in our hashtable (in which
all addresses have already been fully checked when they were ﬁrst added to the
hashtable).
We understand that a full enforcement of the shadow call stack is more secure
as it ensures that every return jumps to its caller; however, its high performance
overhead is also well documented in previous research [9,10]. To verify such high
performance overhead, we modify DynCFI to check the shadow call stack for
every return instruction, and show the results in Fig. 9.
Figure 9 shows that DynCFI with full enforcement of the shadow stack runs
with an average performance overhead of 29.8 %, a big jump from our optimized
implementation at 14.8 %. Although such a full enforcement of the shadow stack
takes away the performance advantage of DynCFI compared to BinCFI, DynCFI
now oﬀers much better security. We check the AIR metric and ﬁnd that AIR
for DynCFI with full enforcement of the shadow stack increases from 98.80 %
to 99.66 % for SPEC CPU2006, which is better than that of BinCFI at 98.86 %.
Our experiments also show that DynCFI can now detect some more advanced
ROP attacks, e.g., the ROP attack constructed by Goktas et al. [13] using call-
preceded gadgets. A call-proceeded-only policy, e.g., that used in BinCFI, would
miss such advanced attacks.
Fig. 9. Performance overhead with shadow stack

384
Y. Lin et al.
6
Conclusion
In this paper, we propose DynCFI, a new implementation of CFI properties on
top of a well-studied dynamic code optimization platform. We show that DynCFI
achieves comparable CFI security properties with many existing CFI proposals
while enjoying much lower performance overhead of 14.8 % on average compared
to that of a state-of-the-art CFI implementation BinCFI at 28.6 %. Our detailed
proﬁling of DynCFI shows that traces, a mechanism in the dynamic code opti-
mization platform, contribute the most to such performance improvement.
Acknowledgment. This work was supported by No. 61373168 and No. 2012014
1110002.
References
1. DynamoRIO. http://www.dynamorio.org/
2. Abadi, M., Budiu, M., Erlingsson, U., Ligatti, J.: Control-ﬂow integrity. In: Pro-
ceedings of the 12th ACM Conference on Computer and Communications Security,
pp. 340–353. ACM (2005)
3. Bala, V., Duesterwald, E., Banerjia, S.: Dynamo: a transparent dynamic optimiza-
tion system. In: ACM SIGPLAN Notices, vol. 35, pp. 1–12. ACM (2000)
4. Bletsch, T., Jiang, X., Freeh, V.: Mitigating code-reuse attacks with control-ﬂow
locking. In: Proceedings of the 27th Annual Computer Security Applications Con-
ference, pp. 353–362. ACM (2011)
5. Bruening, D.: Eﬃcient, transparent, and comprehensive runtime code manipula-
tion. Ph.D. thesis. Massachusetts Institute of Technology (2004)
6. Carlini, N., Wagner, D.: Rop is still dangerous: breaking modern defenses. In:
USENIX Security Symposium (2014)
7. Chen, P., Xing, X., Han, H., Mao, B., Xie, L.: Eﬃcient detection of the return-
oriented programming malicious code. In: Jha, S., Mathuria, A. (eds.) ICISS 2010.
LNCS, vol. 6503, pp. 140–155. Springer, Heidelberg (2010)
8. Chen, W.-K., Lerner, S., Chaiken, R., Gillies, D.M.: Mojo: a dynamic optimization
system. In: 3rd ACM Workshop on Feedback-Directed and Dynamic Optimization
(FDDO-3), pp. 81–90 (2000)
9. Dang, T.H., Maniatis, P., Wagner, D.: The performance cost of shadow stacks and
stack canaries. In: ACM Symposium on Information, Computer and Communica-
tions Security, ASIACCS, vol. 15 (2015)
10. Davi, L., Sadeghi, A.-R., Winandy, M.: ROPdefender: a detection tool to defend
against return-oriented programming attacks. In: Proceedings of the 6th ACM
Symposium on Information, Computer and Communications Security, pp. 40–51.
ACM (2011)
11. Deaver, D., Gorton, R., Rubin, N., Wiggins, R.: An on-line program specializer.
In: Proceedings of the IEEE Hot Chips XI Conference (1999)
12. Fratric, I.: Runtime Prevention of Return-Oriented Programming Attacks. Univer-
sity of Zagreb (2012)
13. Goktas, E., Athanasopoulos, E., Bos, H., Portokalidis, G.: Out of control: over-
coming control-ﬂow integrity. In: 2014 IEEE Symposium on Security and Privacy
(SP), pp. 575–589. IEEE (2014)

Control Flow Integrity Enforcement with Dynamic Code Optimization
385
14. Intel Corporation. Intell R
⃝64 and IA-32 Architectures Software Developer’s Manual
(2015)
15. Kiriansky, V., Bruening, D., Amarasinghe, S.P.: Secure execution via program
shepherding. In: USENIX Security Symposium, vol. 92 (2002)
16. Luk, C.-K., Cohn, R., Muth, R., Patil, H., Klauser, A., Lowney, G., Wallace, S.,
Reddi, V.J., Hazelwood, K.: Pin: building customized program analysis tools with
dynamic instrumentation. In: ACM Sigplan Notices, vol. 40, pp. 190–200. ACM
(2005)
17. Mathias, P., Antonio, B., Thomas, R.: Fine-grained control-ﬂow integrity through
binary hardening. In: Almgren, M., Gulisano, V., Maggi, F. (eds.) Detection of
Intrusions and Malware, and Vulnerability Assessment. LNCS, vol. 9148, pp. 144–
164. Springer, Cham (2015)
18. Mohan, V., Larsen, P., Brunthaler, S., Hamlen, K., Franz, M.: Opaque control-ﬂow
integrity. In: Symposium on Network and Distributed System Security (NDSS)
(2015)
19. Niu, B., Tan, G.: Modular control-ﬂow integrity. In: Proceedings of the 35th ACM
SIGPLAN Conference on Programming Language Design and Implementation,
p. 58. ACM (2014)
20. Pappas, V., Polychronakis, M., Keromytis, A.D.: Transparent ROP exploit miti-
gation using indirect branch tracing. In: USENIX Security, pp. 447–462 (2013)
21. Schuster, F., Tendyck, T., Pewny, J., Maaß, A., Steegmanns, M., Contag, M., Holz,
T.: Evaluating the eﬀectiveness of current anti-ROP defenses. In: Stavrou, A., Bos,
H., Portokalidis, G. (eds.) RAID 2014. LNCS, vol. 8688, pp. 88–108. Springer,
Heidelberg (2014)
22. Shacham, H.: The geometry of innocent ﬂesh on the bone: return-into-libc with-
out function calls (on the x86). In: Proceedings of the 14th ACM Conference on
Computer and Communications Security, pp. 552–561. ACM (2007)
23. van der Veen, V., G¨oktas, E., Contag, M., Pawlowski, A., Chen, X., Rawat, S., Bos,
H., Holz, T., Athanasopoulos, E., Giuﬀrida, C.: A tough call: mitigating advanced
code-reuse attacks at the binary level. In: IEEE Symposium on Security and Pri-
vacy (S&P) (2016)
24. Wilander, J., Nikiforakis, N., Younan, Y., Kamkar, M., Joosen, W.: RIPE: Runtime
Intrusion Prevention Evaluator. In: Proceedings of the 27th Annual Computer
Security Applications Conference, pp. 41–50. ACM (2011)
25. Xia, Y., Liu, Y., Chen, H., Zang, B.: CFIMon: detecting violation of control ﬂow
integrity using performance counters. In: 2012 42nd Annual IEEE/IFIP Interna-
tional Conference on Dependable Systems and Networks (DSN), pp. 1–12. IEEE
(2012)
26. Yuan, P., Zeng, Q., Ding, X.: Hardware-assisted ﬁne-grained code-reuse attack
detection. In: Bos, H., et al. (eds.) Raid 2015. LNCS, vol. 9404, pp. 66–85. Springer,
Heidelberg (2015). doi:10.1007/978-3-319-26362-5 4
27. Zhang, C., Wei, T., Chen, Z., Duan, L., Szekeres, L., McCamant, S., Song, D., Zou,
W.: Practical control ﬂow integrity and randomization for binary executables. In:
2013 IEEE Symposium on Security and Privacy (SP), pp. 559–573 (2013)
28. Zhang, M., Sekar, L.: Control ﬂow integrity for COTS binaries. In: Proceedings of
the 22th USENIX Security Symposium, pp. 337–352 (2013)

Encryption, Signatures and
Fundamentals

Impossibility on the Provable Security
of the Fiat-Shamir-Type Signatures
in the Non-programmable Random
Oracle Model
Masayuki Fukumitsu1(B) and Shingo Hasegawa2
1 Faculty of Information Media, Hokkaido Information University,
Nishi-Nopporo 59-2, Ebetsu, Hokkaido 069-8585, Japan
fukumitsu@do-johodai.ac.jp
2 Graduate School of Information Sciences, Tohoku University,
41 Kawauchi, Aoba-ku, Sendai, Miyagi 980-8576, Japan
hasegawa@cite.tohoku.ac.jp
Abstract. On the security of Fiat-Shamir (FS) type signatures, some
negative circumstantial evidences were given in the non-programmable
random oracle model (NPROM). Fischlin and Fleischhacker ﬁrst showed
an impossibility for speciﬁc FS-type signatures via a single-instance
reduction. In ISC 2015, Fukumitsu and Hasegawa found another con-
ditions to prove such an impossibility, however their result requires a
strong condition on a reduction, i.e. a key-preserving reduction. In this
paper, we focus on a non-key-preserving reduction, and then we show that
an FS-type signature cannot be proven to be secure in the NPROM via a
sequentially multi-instance reduction from the security of the underlying
ID scheme. Our result can be interpreted as a generalization of the two
impossibility results introduced above.
By applying our impossibility result, the security incompatibility
between the DL assumption and the security of the Schnorr signature in
the NPROM via a sequentially multi-instance reduction can be shown.
Our incompatibility result means that the security of the Schnorr signa-
ture is not likely to be proven in the NPROM.
Keywords: Fiat-Shamir transformation · Schnorr signature · Non-
programmable random oracle model · Meta-reduction · Static message
attack
1
Introduction
The Fiat-Shamir (FS) transformation is known as a general way to yield an
eﬃcient signature from a canonical identiﬁcation (ID) scheme. By using this
method, several famous signature schemes can be constructed such as the
Schnorr signature [28] and the Guillou-Quisquater (GQ) signature [20].
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 389–407, 2016.
DOI: 10.1007/978-3-319-45871-7 23

390
M. Fukumitsu and S. Hasegawa
The security of FS-type signatures, the signatures yielded via the FS transfor-
mation, is discussed in several literatures. Pointcheval and Stern [27] showed that
an FS-type signature is strongly existentially unforgeable against the chosen-
message attack (sEUF-CMA) in the random oracle model (ROM) if the under-
lying ID scheme is a honest-veriﬁer zero-knowledge proof of knowledge. This
implies that the Schnorr signature can be proven to be sEUF-CMA in the
ROM from the discrete logarithm (DL) assumption. Subsequently, Abdalla, An,
Bellare and Namprempre [1] relaxed the condition on the underlying ID scheme.
They showed the equivalence between the sEUF-CMA security of an FS-type
signature in the ROM and the security of the underlying ID scheme against an
impersonation under the passive attack (imp-pa security).
On the other hand, Paillier and Vergnaud [25] gave a negative circumstan-
tial evidence on the provable security of FS-type signatures in the standard
model. More precisely, they showed the impossibility of proving the security of
the Schnorr signature in the standard model via an algebraic reduction from
the DL assumption as long as the one-more (OM) DL assumption holds. In a
similar manner, such an impossibility was also proven for the GQ signature in
the standard model.
One can observe that the security of the FS-type signatures can be proven
in the ROM, whereas it may not be proven in the standard model. The main
reason of this diﬀerence is whether or not the programming technique of a hash
function can be utilized. The programming of a hash function means that a
reduction R, which aims to prove the security of a cryptographic scheme, is
allowed to program a value of a hash function for an input in the security proof.
By using this technique, the security of several cryptographic schemes including
FS-type signatures was proven in the ROM. Although the programming property
is valuable to prove the security of cryptographic schemes, this is known to be
strong [32]. On the diﬀerence between the ROM and the standard model, one
of the interests of the theoretical cryptography is how one can constrain the
programmability on the security proof of cryptographic schemes [13,14,32]. In
order to discuss this topic, several variants of the ROM were proposed. One
of these is the non-programmable random oracle model (NPROM) [23]. In the
NPROM, a hash value is obtained from the random oracle as well as in the
ROM, but the random oracle is dealt with the independent party in the security
proof. Namely, the reduction in the security proof cannot program hash values
in the NPROM. Fischlin, Lehmann, Ristenpart, Shrimpton, Stam and Tessaro
[14] discussed the provable security of several cryptographic schemes such as the
FDH signature and the Shoup’s trapdoor-permutation-based key encapsulation
scheme [30] in the NPROM.
Fischlin and Fleischhacker [13] ﬁrst gave the impossibility of proving the
security of some speciﬁc FS-type signatures in the NPROM. More speciﬁcally,
they showed that the Schnorr signature cannot be proven to be EUF-CMA in
the NPROM via a single-instance reduction from the DL assumption as long
as the OM-DL assumption holds. The single-instance reduction is a reduction
which can invoke a forger F of the designated signature in the security proof

Impossibility on the Provable Security of the Fiat-Shamir-Type Signatures
391
only once, but it is allowed to rewind F many times. They also mentioned
that such an impossibility result is applicable to other FS-type signatures which
satisfy the following two conditions: (1) the “one-more” assumption related to
the assumption from which the security of the signature schemes is proven in
the ROM holds, (2) for any public key pk of the signature schemes, there exists
the unique secret key corresponding to pk. On the other hand, it is not known
whether or not a similar impossibility holds for FS-type signatures which do not
satisfy above conditions such as [21,24]. In ISC 2015, Fukumitsu and Hasegawa
[16] found another abstract conditions by which the impossibility of proving
the security of many FS-type signatures in the NPROM holds. Their conditions
are to restrict the type of reduction to being key-preserving and to require the
underlying ID scheme to being secure against an impersonation under the active
attack (imp-aa security). The key-preserving reduction means that a reduction
invokes a forger F with the public key which is the same one given to the
reduction. Namely, they showed that an FS-type signature cannot be proven
to be secure in the NPROM via a key-preserving reduction from the imp-pa
security of the underlying ID scheme as long as the ID scheme is imp-aa secure.
Although their result likely covers many FS-type signatures than the result in
[13], the condition of the key-preserving reduction is a bit strong as noted in
[32]. In fact, the public key with which the key-preserving reduction invokes F
is wholly preserved to that given to R, whereas the public key of the single-
instance reduction R is just partially limited in a sense that the public key is
required to contain the same group description as that given to R. Therefore,
the result by [16] may not be regarded as a generalization of the result by [13]. It
eventually remains open whether or not the impossibility of the provable security
of FS-type signatures in the NPROM via a non-key-preserving reduction can be
proven.
1.1
Our Result
In this paper, we give an impossibility of FS-type signatures in the NPROM
via a non-key-preserving reduction by employing the technique introduced by
[4]. Namely, we show that FS-type signatures cannot be proven to have the new
security, i.e. security against the static message attack (SMA security) which is
weaker than EUF-CMA, by the following theorem.
Theorem 6 (Informal). If the underlying ID scheme is imp-pa secure, then an
FS-type signature cannot be proven to be SMA secure in the NPROM via a
sequentially multi-instance reduction from the imp-pa security of the ID scheme.
We prove this theorem by the meta-reduction technique [8]. This technique is
frequently used to show the impossibility of proving the security of cryptographic
schemes such as [2–5,10,11,13,15,16,18,22,25,26,29,32], and the relationships
among cryptographic assumptions [8,9,17,31].
We now explain our conditions. The ﬁrst is to restrict an underlying ID
scheme to being imp-pa secure. We consider that this condition is natural. This is
because an ID scheme is in general proven to be secure against an impersonation

392
M. Fukumitsu and S. Hasegawa
Table 1. Comparison of restriction of impossibility results
Type of a reduction
Assumptions on the theorem
[13]
Single-instance
The signature is unique key,
The related OM assumption holds
[16]
Key-preserving
The underlying ID scheme is imp-aa secure
[Ours] Sequentially multi-instance The underlying ID scheme is imp-pa secure
Table 2. Type of reductions concerned in the impossibility results
Type of a reduction
# of invocation of F
Public key given to F
[13]
Single-instance
Once
Any
[16]
Key-preserving
Many times
Preserved to pk given to R
[Ours]
Sequentially multi-instance
Many times
Any
under the concurrent attack which is stronger than the imp-pa security. More-
over, our condition is weaker than that of [16], namely [16] applied the imp-aa
security as a condition. Table 1 summarizes conditions where the impossibility
results on FS-type signatures requires.
The second is to focus only on a sequentially multi-instance reduction. The
sequentially multi-instance reduction R is a reduction which can invoke a forger
F polynomially many times, but it is prohibited to invoke the clones of F dur-
ing an invocation of F. Table 2 shows the comparison of the three types of
reductions. Namely, the restriction of the sequentially multi-instance reduction is
weaker than that of the single-instance reduction and that of the key-preserving
reduction. Note that reductions concerned in the general security proofs e.g.
[1,6,7,10,13,25,30] belong to this type.
In Theorem 6, we give the impossibility of proving the SMA security of FS-
type signatures. A signature scheme is said to be SMA secure if there exists
no probabilistic polynomial-time (PPT) forger F such that on given any poly-
nomially many messages m1, m2, . . . , mq, F can forge one mj∗of the massages
by utilizing a tuple of signatures σi of the messages mi other than mj∗. For
the relationship among other security notions, Bader, Jager and Li, Sch¨age [4]
showed that the SMA security is weaker than the EUF-CMA security. Therefore,
the impossibility of proving that a signature scheme is SMA secure means the
impossibility of proving that the signature is EUF-CMA.
One can interpret Theorem 6 as a generalized result of [13,16]. Our result
indicates that the security of FS-type signatures may not be proven in the
NPROM by employing general proof techniques only. However, this does not
exclude the possibility that the security of FS-type signatures is proven in the
NPROM via a reduction which is allowed to invoke multiple clones of the forger
F concurrently.
By applying Theorem 6 to the Schnorr signature, one can prove the security
incompatibility between the DL assumption and the EUF-CMA security of the

Impossibility on the Provable Security of the Fiat-Shamir-Type Signatures
393
Table 3. Type of reductions concerned in the incompatibility results
Type of a reduction
# of invocation of F
Public key given to F
[16]
Single-instance and key-preserving
Once
Preserved to pk given to R
[Ours]
Sequentially multi-instance
Many times
Any
Schnorr signature in the NPROM via a sequentially multi-instance reduction.
This security incompatibility means that the EUF-CMA security of the Schnorr
signature in the NPROM is not compatible with the DL assumption. By employ-
ing Theorem 6, we speciﬁcally show that the Schnorr signature cannot be proven
to be EUF-CMA in the NPROM via a sequentially multi-instance reduction from
the DL assumption as long as the DL assumption holds. Such an incompatibility
was ﬁrst proven in the previous work [16]. They showed the incompatibility via
a strongly restricted reduction, namely single-instance key-preserving reduction.
Table 3 shows the comparison of the type of reduction concerned in our incom-
patibility result and theirs. The advantage of our incompatibility is that the
restriction on a reduction is weaker than that of theirs. Since it is believed that
the DL assumption holds, our incompatibility result means that the EUF-CMA
security of the Schnorr signature is not likely to be proven in the NPROM.
It should be noted that Fischlin and Fleischhacker [13] showed that such an
incompatibility cannot be proven from the DL assumption. We now explain that
our incompatibility result does not contradict to theirs by comparing the meta-
reduction concerned in ours with that in theirs. By employing the meta-reduction
technique, one shows an impossibility as follows: assume that there exists a PPT
reduction R which proves the security of a designated cryptographic scheme.
Then, one aims to construct a PPT meta-reduction algorithm M which breaks
some cryptographic assumption. In [13], they formally showed that if there exists
a PPT meta-reduction M which proves the impossibility of the provable security
of the Schnorr signature in the NPROM from the DL assumption as long as the
DL assumption holds, then the Schnorr signature is not sEUF-CMA. In their
result, they only consider the meta-reduction which does not execute another
clone of an assumed reduction R during the execution of R, nevertheless it can
execute R polynomially many times. On the other hand, we circumvent their
result by constructing the meta-reduction which executes the polynomially many
clones of R concurrently. Thus our impossibility result does not contradict to
theirs.
2
Preliminaries
For any natural number n, let Zn be the residue ring Z/nZ. The notation [n]
denotes the set of all natural numbers 1 ≤i ≤n. We write x ∈U D to denote
that an element x is chosen uniformly at random from a ﬁnite set D. We say
that the family {Dλ}λ of ﬁnite sets is polynomial-time samplable if there exists
an probabilistic polynomial-time (PPT) algorithm that outputs an element y
which is uniformly distributed over Dλ on input λ. By x := y, we mean that an

394
M. Fukumitsu and S. Hasegawa
element x is deﬁned or substituted by y. For any algorithm A, y ←A(x) means
that A outputs y on input x. When A is a probabilistic algorithm, y ←A(x; r)
denotes that A outputs y on input x with random coins r, and A(x) is the
random variable for the output of A on input x where the randomness is taken
over the internal coin ﬂips r of A. For an algorithms A and O, AO means that A
has O as an oracle. A function ν is said to be negligible in λ if for any polynomial
f, there exists a natural number λ0 such that ν(λ) < 1/f(λ) for any λ ≥λ0.
2.1
Digital Signature Scheme
A signature scheme Sig consists of three algorithms (KGen, Sign, Ver). KGen is a
PPT key generation algorithm which on input 1λ, outputs a key pair (sk, pk) of a
secret key sk and a corresponding public key pk. Sign is a PPT signing algorithm
which on input (sk, pk, m) of a key pair (sk, pk) and a message m, outputs a
signature σ on the message m under the public key pk. Ver is a deterministic
polynomial-time verifying algorithm which on input (pk, m, σ) of a public key
pk, a message m and a signature σ, outputs 1 if σ is a signature on the message
σ under the public key pk.
Let Sig = (KGen, Sign, Ver) be a signature scheme. We deﬁne the existential
unforgeability against the chosen-message attack (EUF-CMA) [19]. The EUF-
CMA is deﬁned by the existentially forging game of Sig against the chosen-
message attack (the EF-CMA game). This game is played by two parties (algo-
rithms), a challenger C and a forger F, in the following way: C ﬁrst generates
a key pair (sk, pk) ←KGen(1λ), and then invokes F with the public key pk.
Then F aims to output a pair (m∗, σ∗) of a message m∗and its signature σ∗
under pk. Here, F can make polynomially many queries mi to C adaptively in
order to obtain its signature σi. When F eventually outputs a pair (m∗, σ∗), C
outputs 1 if m∗has not been queried to C and Ver(pk, m∗, σ∗) = 1. F is said
to win the EF-CMA game of Sig if C outputs 1 in the EUF-CMA game of Sig
between C and F. Then, Sig is EUF-CMA if there exists no PPT forger F that
wins EF-CMA game of Sig with non-negligible probability.
We also deﬁne the security notion, the security against the static message
attack (SMA security) given by [4]. This security is deﬁned by the SMA game
between a challenger C and a forger F in a similar manner to the EF-CMA game.
In this game, the behavior of F is divided into two sub-algorithms F1 and F2.
This game is proceeded as depicted in Fig. 1, where q := q(λ) is a polynomial
in λ. F is said to win the SMA game of Sig if C outputs 1 in the SMA game
of Sig between C and F = (F1, F2). Then, Sig is SMA secure if there exists no
PPT forger F = (F1, F2) that wins the SMA game of Sig with non-negligible
probability.
For these two security notions, the following relation is known.
Proposition 1 ([4]). Let Sig be a signature scheme. If Sig is EUF-CMA, then
Sig is SMA secure.

Impossibility on the Provable Security of the Fiat-Shamir-Type Signatures
395
SMA Game of Sig between a challenger C and a forger F = (F1, F2)
On input 1λ, C proceeds as follows:
Init. C invokes F1 pk, (mi)i∈[q]; rF , where it chooses pk, (mi)i∈[q] and rF in the
following way:
1. Generate a key pair (sk, pk) ←KGen 1λ
and random coins rF.
2. Choose q messages m1, m2, . . . , mq randomly and disjointly.
Select. When F1 outputs a pair (j∗, stF) of a challenge index j∗and a state stF of F,
C invokes F2 (σi)i∈[q]\{j∗}, stF , where for each index i ∈[q] \ {j∗}, the signature
σi is generated by σi ←Sign(sk, pk, mi).
Challenge When F2 outputs σ∗, C outputs 1 if Ver(pk, mj∗, σ∗) = 1.
Fig. 1. SMA game of Sig
Prover P
Veriﬁer V
(st, cmt) ←P1(sk, pk)
cmt
−−−−−−−−−−−−−→
cha
←−−−−−−−−−−−−−cha ∈U CHpk
res ←P2(sk, pk, st, cmt, cha)
res
−−−−−−−−−−−−→
output V (pk, cmt, cha, res)
Fig. 2. Canonical ID scheme
2.2
Canonical ID Scheme
A canonical identiﬁcation (ID) scheme consists of a tuple (K, P1, P2, CH, V ). K
is a PPT key generation algorithm which on input 1λ, outputs a key pair (sk, pk).
CH := {CHpk}pk∈PKλ is a family of polynomial-time samplable sets CHpk of all
challenges cha indexed by a public pk ∈PKλ, where PKλ is a set of all public
keys which can be generated by K(1λ). Both P1 and P2 are algorithms for a
prover P. Namely, P1 outputs a pair (st, cmt) of a state st and a commitment
cmt on input (sk, pk). P2 outputs a response res on input (sk, pk, st, cmt, cha). V
is a verifying algorithm for a veriﬁer V. On input (pk, cmt, cha, res), V outputs
1 if it accepts P. The communication between the prover P and the veriﬁer V
is depicted as in Fig. 2.
Let ID = (K, P1, P2, CH, V ) be an ID scheme. We deﬁne the security of ID
against an impersonation under the passive attack (imp-pa) [1]. In a similar
manner to [16], this security is deﬁned by an imp-pa game of ID between a
challenger C and an impersonator I depicted in Fig. 3. An impersonator I is said
to win the imp-pa game of ID if C ﬁnally outputs 1 in the imp-pa game of ID
between C and I. Then, ID is imp-pa secure if there exists no PPT impersonator
I that wins the imp-pa game of ID with non-negligible probability.

396
M. Fukumitsu and S. Hasegawa
imp-pa game of ID between a challenger C and an im-
personator I
Given 1λ, C proceeds in the following way:
Init. C executes I(pk), where the public key pk is gen-
erated by (sk, pk) ←K 1λ .
Transcript. When I makes a t-th query, C replies a
transcript (cmtt, chat, rest) ←TrID
sk,pk to I.
Impersonate. When I ﬁnally outputs a commitment
ˆ
cmt, C proceeds as follows:
1. send a challenge
ˆ
cha ∈U CHpk to I;
m
o
rf
s
e
rˆ
e
s
n
o
p
s
e
r
a
g
n
i
v
ie
c
e
r
r
e
tf
a
.2
I,
output V (pk, ˆ
cmt, ˆ
cha,
.)
s
e
rˆ
Transcript Oracle TrID
sk,pk
1. (stt, cmtt) ←P1(sk, pk);
2. chat ∈U CHpk;
3. rest ←
P2(sk, pk, stt, cmtt, chat);
4. return (cmtt, chat, rest).
Fig. 3. imp-pa game of ID
Signature FS-Sig = (KGen, Sign, Ver) Yielded by Applying Fiat-Shamir Transformation
to ID Scheme ID
KGen coincides with K.
Sign on input (sk, pk, m), issues a signature σ := (cmt, res) in the following way:
1. (st, cmt) ←P1(sk, pk);
2. cha := Hpk(cmt, m);
3. res ←P2(sk, pk, st, cmt, cha).
Ver on input (pk, m, σ), sets c := Hpk(cmt, m), and then outputs V (pk, cmt, c, res).
Fig. 4. Fiat-Shamir transformation
2.3
Fiat-Shamir Transformation
Let
ID
=
(K, P1, P2, CH, V )
be
an
ID
scheme,
and
let

Hpk : {0, 1}∗→CHpk

λ,pk∈PKλ be a family of hash functions indexed by
security parameters λ and public keys pk ∈PKλ generated by K(1λ). Then,
the signature FS-Sig is yielded by the Fiat-Shamir (FS) transformation [12] as
depicted in Fig. 4. The signature FS-Sig is referred to as the FS-type signature.
3
Impossibility of Proving the SMA Security of FS-Type
Signature
In this section, we prove the impossibility of proving the SMA security of an
FS-type signature in the NPROM from the imp-pa security of the underlying ID
scheme as long as the ID scheme is imp-pa secure. Let ID = (K, P1, P2, CH, V )
be an ID scheme, and let

Hpk : {0, 1}∗→CHpk

λ,pk∈PKλ be a family of hash

Impossibility on the Provable Security of the Fiat-Shamir-Type Signatures
397
functions. Then FS-Sig = (KGen, Sign, Ver) denotes the FS-type signature yielded
by applying the FS transformation to ID. Before explaining the statement of the
impossibility, we formally describe the situation where FS-Sig is proven to be
SMA secure in the NPROM from the imp-pa security of the underlying ID
scheme ID. The deﬁnition of this situation is given by black-box reduction such
as [4,13]. Namely, this situation holds if there exist a black-box reduction R and
a non-negligible function ϵR such that R wins the imp-pa game of ID with the
probability ϵR by black-box accessing to a forger F = (F1, F2) which wins the
SMA game of FS-Sig with the non-negligible probability. Through the black-box
access, R would play the SMA game of FS-Sig with F in which R is placed at
the challenger’s position. Here R is supposed to invoke F at most I times.
3.1
Case: Simple Reduction
For ease of the explanation of the main theorem, we ﬁrst consider the simple
case, namely I = 1. We say that such a reduction R is simple. In this case, the
behavior of R is separated into a tuple (R1, R2, R3, R4) of four sub-algorithms
as follows:
R1 on a public key pk given from the imp-pa challenger C with random coins
rR, outputs a pair

pk, (mi)i∈[q], rF

, stR2

of an input

pk, (mi)i∈[q], rF

to F1 and a state stR2. Then, F1(pk, (mi)i∈[q]; rF) is invoked.
R2 on a challenge index j∗∈[q] output by F1(pk, (mi)i∈[q]; rF) and the state
stR2, outputs a pair

(σi)i∈[q]\{j∗}, stR3

of a sequence (σi)i∈[q]\{j∗} of signa-
tures σi = (cmti, resi) replied to F2 and a state stR3. Then, by using a state
stF output by F1, F2((σi)i∈[q]\{j∗}, stF) is invoked.
R3 on a challenge signature σ∗= (cmt∗, res∗) output by F2((σi)i∈[q]\{j∗}, stF)
and the state stR3, moves onto Challenge phase of the imp-pa game with
sending a commitment
ˆ
cmt to C.
R4 on a challenge ˆ
cha received from C and the state stR4, replies a response ˆ
res
to C.
The conﬁguration of R is described in Fig. 5. Here R plays a role of the chal-
lenger of the SMA game. Therefore, it needs to reply for F1’s queries. However,
R, namely R2, may fail to reply since R is a PPT algorithm. Namely, R2 may
output a sequence (σi)i∈[q]\{j∗} such that there exists an index i0 ∈[q]\{j∗}
such that Ver(pk, mi0, σi0) ̸= 1. In this case, F2 is allowed to output any symbol.
Moreover, we suppose that the states stR2, stR3 and stR4 contains the ran-
dom coins rR which are given to R1, and R2, R3 and R4 are invoked with
rR.
An imp-pa impersonator R can query to the transcript oracle TrID
sk,pk in
Transcript phase before moving onto Challenge phase. In other words, R1,
R2 and R3 are allowed to query to TrID
sk,pk adaptively. In the non-programmable
random oracle model (NPROM), all of the parties R1, R2, R3, R4, F1 and
F2 obtain hash values from the random oracle. Here R = (R1, R2, R3, R4) can

398
M. Fukumitsu and S. Hasegawa
Fig. 5. Conﬁguration of R
observe all of random oracle queries by F = (F1, F2). However, R is prohibited
to program hash values.
We have described the behavior of the reduction R: it proceeds to (I) execute
F1, (II) execute F2, and then (III) move on to Challenge phase of the imp-pa
game. It should be noted that it has the possibility that the processes of R are
ordered by (I), (III) and (II), and by (III), (I) and (II). One can discuss the
impossibility in a similar manner even in such cases. Therefore, we only consider
the case where the processes of R are ordered by (I), (II) and (III) in this paper.
Theorem 2. Assume that FS-Sig is proven to be SMA secure in the NPROM
via a simple reduction from the imp-pa security of the underlying ID scheme ID.
Then ID is not imp-pa secure.
Proof (Sketch). Assume that FS-Sig is proven to be SMA secure in the NPROM
via a simple reduction from the imp-pa security of ID. Then there exist a PPT
simple reduction R = (R1, R2, R3, R4) and a non-negligible function ϵR such
that R wins the imp-pa game of ID with the probability ϵR by accessing an SMA
forger F = (F1, F2) which wins the SMA game of FS-Sig with non-negligible
probability. Here, the simple reduction R can invoke F only once without any
rewind. In this proof, we aim to construct a PPT meta-reduction M which wins
the imp-pa game of ID with non-negligible probability by utilizing R. Note that
R can win the imp-pa game with non-negligible probability if a winning SMA
forger F is provided. We ﬁrst deﬁne a hypothetical computationally unbounded
forger ˜F =

˜
F1, ˜
F2

. It is noted that R should win the imp-pa game with non-
negligible probability even if such a forger ˜F is provided. We next construct
the meta-reduction M so that M executes R with the simulation of the forger
˜F =

˜
F1, ˜
F2

in polynomial time.

Impossibility on the Provable Security of the Fiat-Shamir-Type Signatures
399
Hypothetical Forger ˜F =
˜
F1, ˜
F2
˜
F1(pk, (mi)i∈[q]; rF) chooses a challenge index j∗
∈U
[q], sets a state stF
:=
j∗, pk, (mi)i∈[q], rF , and then outputs a pair (j∗, stF).
˜
F2((σi)i∈[q]\{j∗}, stF) outputs σ∗as chosen in the following way:
if for any index i ∈[q] \ {j∗}, Ver pk, mi, σi
= 1; then ﬁnds σ∗= (cmt∗, res∗)
satisfying Ver pk, mj∗, σ∗= 1. Here, it makes a sequence Q of queries, where
Q is a set of all of queries (cmti, mi) for each i ∈[q] \ {j∗} and (cmt∗, mj∗),
to obtain the corresponding hash values.
otherwise; sets σ∗:= ⊥.
Fig. 6. Hypothetical forger ˜F
Hypothetical Forger
˜F. We depict the hypothetical forger ˜F =

˜
F1, ˜
F2

in
Fig. 6. Let (mi)i∈[q] be a sequence of q messages mi given to ˜
F1. For a challenge
index j∗output by ˜
F1, ˜
F2 correctly outputs a signature σ∗= (cmt∗, res∗) on
the message mj∗, if ˜
F2 is given all of correct signatures σi = (cmti, resi) on the
messages mi other than the challenge message mj∗. Otherwise, ˜
F2 is allowed to
output any symbol as mentioned in the situation of R. Therefore, ˜F =

˜
F1, ˜
F2

is a winning SMA forger with non-negligible probability. In the NPROM, ˜
F2
needs to obtain any hash value from the random oracle. Here, ˜
F2 obtains the
hash values including the sequence Q of all of the pairs (cmti, mi) and the pair
(cmt∗, mj∗) from the random oracle.
Note that the running time of
˜
F2 would not be bounded in polynomial.
We will show that M can be constructed in a way that it can simulate ˜
F2 in
polynomial time sooner.
Meta-reduction M. Recall that R can win the imp-pa game of ID with the
probability ϵR if the hypothetical forger ˜F is provided. Here, we construct a
meta-reduction M depicted in Fig. 7 which wins the imp-pa game of ID by
executing the reduction R with the simulation of ˜F.
Correctness and Success Probability of M. We show that M wins the imp-pa
game of ID with non-negligible probability. As in Fig. 7, M attempts to win its
game by executing the reduction R with the simulation of the forger ˜F. In the
execution of M, when R1, R2 and R3 make a query, M forwards a transcript
(cmt, cha, res) by querying to TrID
sk,pk provided by the imp-pa challenger C. In
Challenge phase of the imp-pa game between C and M, M just intermediates
between C and R as described in (M-5) and (M-6). It follows that M can win
the imp-pa game of ID if R wins its game. Recall that R can win the imp-pa
game of ID with the non-negligible probability ϵR if a winning SMA forger ˜F
of FS-Sig is provided. In a nutshell, M can win the imp-pa game of ID with

400
M. Fukumitsu and S. Hasegawa
Meta-Reduction M
On a public key pk given from the imp-pa challenger C, M proceeds as follows:
(M-1) choose random coins rR, and then execute
pk, (mi)i∈[q], rF, stR2
←
R
TrID
sk,pk
1
(pk; rR).
(M-2) simulate ˜
F1 by choosing a challenge index j∗∈U [q], and then setting stF :=
j∗, pk, (mi)i∈[q], rF .
(M-3) for each j ∈[q], execute
σ(j)
i
i∈[q]\{j}, st(j)
R3
←R
TrID
sk,pk
2
(j, stR2).
(M-4) simulate ˜
F2 in the following way:
if for any i ∈[q] \ {j∗}, Ver pk, mi, σ(j∗)
i
= 1; then
if there exists an index j0 ∈[q] \ {j∗} such that Ver pk, mj∗, σ(j0)
j∗
= 1;
then set σ∗:= σ(j0)
j∗. Here, it makes a sequence Q of queries, where Q is
a set of all of queries (cmti, mi) for each i ∈[q] \ {j∗} and (cmt∗, mj∗),
to obtain the corresponding hash values.
otherwise; abort.
otherwise; set σ∗:= ⊥.
(M-5) execute
ˆ
cmt, stR4
←R
TrID
sk,pk
3
σ∗, st(j∗)
R3
, and move onto Challenge phase of
the imp-pa game with sending
ˆ
cmt to C.
(M-6) given
ˆ
cha from C
s
e
rˆ
e
t
u
c
e
x
e
,
←R4
ˆ
cha, stR4
.s
e
rˆ
t
u
p
t
u
o
n
e
h
t
d
n
a
,
Fig. 7. Meta-reduction M
non-negligible probability if it succeeds in the simulation of ˜F. We show that M
indeed simulates ˜
F1 and ˜
F2 correctly.
Lemma 3. M simulates ˜
F1 perfectly in the viewpoint of R.

Impossibility on the Provable Security of the Fiat-Shamir-Type Signatures
401
Proof. Let

pk, (mi)i∈[q]

and rF be the input and the random coins of
˜
F1
output by R1 in (M-1), respectively. In (M-2), M chooses a challenge index
j∗∈U [q], and then sets stF :=

j∗, pk, (mi)i∈[q], rF

in the same way as ˜
F1.
Therefore, M perfectly simulates ˜
F1 in the viewpoint of R.
⊓⊔
Lemma 4. M simulates ˜
F2 perfectly in the viewpoint of R if it does not abort
in (M-4).
Proof. Let

σ(j∗)
i

i∈[q]\{j∗}, stF

be the input to ˜
F2, where

σ(j∗)
i

i∈[q]\{j∗}
is a sequence of signatures output by the j∗-th execution of R2 in (M-3) and
stF is the state set in (M-2). In (M-4), M ﬁrst checks if for any i ∈[q]\{j∗},
Ver(pk, mi, σ(j∗)
i
) = 1.
If its check passes, M is required to ﬁnd a signature σ∗= (cmt∗, res∗) sat-
isfying Ver(pk, mj∗, σ∗) = 1 as the behavior of ˜
F2. M does it by utilizing the
sub-algorithm R2 as in (M-3). Note that on each index j ∈[q], R2 would out-
put a sequence

σ(j)
i

i∈[q]\{j} of correct signatures σi on the messages mi other
than the j-th message mj. In particular, there is the possibility that σ(j0)
j∗
which
is output by R on some index j0 ̸= j∗is a correct signature on the challenge
message mj∗. Now, we assume that M does not abort in (M-4). Namely, it is
guaranteed that there exists a signature on mj∗in the set

σ(j)
j∗
	
j∈[q]\{j∗} of
the signatures obtained in (M-3). Therefore, M ﬁnds a signature σ∗such that
Ver(pk, mj∗, σ∗) = 1 from

σ(j)
j∗
	
j∈[q]\{j∗}. As the behavior of ˜
F2, M also makes
the sequence Q of the queries to the random oracle. Therefore, M indeed sim-
ulates ˜
F2 in such a case. Otherwise, M sets σ∗:= ⊥in the same way as ˜
F2.
Thus, M simulates ˜
F2 perfectly in the viewpoint of R if it does not abort.
⊓⊔
We now evaluate the probability that M wins the imp-pa game of ID. Recall
that M can win the imp-pa game of ID if it succeeds in the simulation of ˜F. It
follows from Lemmas 3 and 4 that M succeeds in the simulation if it does not
abort in (M-4). Therefore, we show that M does not abort with non-negligible
probability. M does not abort if there exists an index j0 ∈[q]\{j∗} such that
Ver(pk, mj∗, σ(j0)
j∗) = 1. It suﬃces that there exists an index j1 ∈[q]\{j∗} such
that for any index i ∈[q]\{j1}, Ver(pk, mi, σ(j1)
i
) = 1 for the j1-th sequence

σ(j1)
i

i∈[q]\{j1} issued in (M-3). In other words, M does not abort if on some
index j1 ̸= j∗, R2 correctly replies the sequence

σ(j1)
i

i∈[q]\{j1} in (M-3). On
the other hand, one can show the following lemma on the success probability
of R2.
Lemma 5. If the probability that R2 correctly replies a sequence (σi)i∈[q]\{j} on
input j ∈[q] is negligible, then ID is no longer imp-pa secure.

402
M. Fukumitsu and S. Hasegawa
Proof. Assume that the probability that R2 correctly replies (σi)i∈[q]\{j} on
input j ∈[q] is negligible. This means that the probability that for any i ∈
[q]\{j}, Ver(pk, mi, σi) = 1 for signatures σi output by R2 is negligible. Then,
we now show that ID is not imp-pa secure by constructing a PPT reduction R′
that wins the imp-pa game of ID without any black-box access. On a public key
pk given by the imp-pa challenger C, R′ proceeds as follows:
(R’-1) choose random coins rR, and then execute

pk, (mi)i∈[q], rF, stR2

←
R
TrID
sk,pk
1
(pk; rR).
(R’-2) simulate F1 by choosing a challenge index j∗∈U [q], and then setting
stF :=

j∗, pk, (mi)i∈[q], rF

.
(R’-3) execute

(σi)i∈[q]\{j∗}, stR3

←R
TrID
sk,pk
2
(j∗, stR2).
(R’-4) simulate F2 by aborting if for any i ∈[q]\{j∗}, Ver(pk, mi, σi) = 1, or
setting σ∗:= ⊥otherwise.
(R’-5) execute

 ˆ
cmt, stR4

←R
TrID
sk,pk
3
(σ∗, stR3), and move onto Challenge
phase of the imp-pa game with sending
ˆ
cmt to C.
(R’-6) given
ˆ
cha from C, execute ˆ
res ←R4

ˆ
cha, stR4

, and then output ˆ
res.
Since F2 is allowed to reply any symbol if R2 fails to reply, namely there exists
an index i0 ∈[q]\{j∗} such that Ver(pk, mi0, σi0) ̸= 1, the reduction R, hence R′
would win the imp-pa game of ID with non-negligible probability ϵR if it does not
abort. On the other hand, R′ aborts if for any i ∈[q]\{j∗}, Ver(pk, mi, σj∗,i) = 1.
Namely, R′ aborts if R2 correctly replies the sequence (σi)i∈[q]\{j∗} in (R’-3).
By the assumption, such a probability is negligible. Therefore, R′ can win the
imp-pa game of ID with non-negligible probability without any black-box access.
Thus ID is not imp-pa secure.
⊓⊔
It follows from Lemma 5 that Theorem 2 holds if the sub-algorithm R2 of the
assumed reduction R correctly replies with negligible probability. Otherwise, it
holds that the probability that R2 correctly replies the sequence

σ(j1)
i

i∈[q]\{j1}
on input j1 ∈[q] is non-negligible. Namely, M does not abort with non-negligible
probability. Let ϵSim be the probability of the non-abortion of M. Thus the
success probability of M is evaluated as follows:
Pr[SuccM] ≥Pr[SuccM ∧Sim]
= Pr[Sim] Pr[SuccM | Sim]
≥Pr[Sim] Pr[SuccR | Sim]
≥ϵSimϵR,
where SuccM and SuccR denote the events of winning the imp-pa game of M
and R, respectively. Sim stands for the event of the correct simulation of ˜F.

Impossibility on the Provable Security of the Fiat-Shamir-Type Signatures
403
Running Time of M. In (M-1), (M-5) and (M-6), M executes the PPT sub-
algorithms R1, R3 and R4 of R once, respectively. Moreover, it also executes
the PPT sub-algorithm R2 q = q(λ) times in (M-3). Observe that the processes
of (M-2) and (M-4) runs in polynomial time as in (M-2) and (M-4). Therefore,
the meta-reduction M runs in polynomial time.
Thus, ID is not imp-pa secure.
⊓⊔
3.2
Case: Sequentially Multi-Instance Reduction
We also consider the case where the reduction R is sequentially multi-instance.
The sequentially multi-instance reduction means that the reduction invokes a
forger F polynomially many times, namely I is polynomial. In this case, the
reduction R is separated into a tuple

R0, (Rk,1, Rk,2, Rk,3)k∈[I], R4, R5

of
sub-algorithms:
R0 on a public key pk given from the imp-pa challenger C with random coins
rR, outputs a state stR1,1.
Rk,1, Rk,2, Rk,3 for each k ∈[I], the k-th invocation of the SMA forger F =
(F1, F2) is done in the following way:
Rk,1 on the state stRk,1 output by the previous sub-algorithm, outputs a pair

pkk, (mk,i)i∈[q], rk,F

, stRk,2

of a k-th input

pkk, (mk,i)i∈[q], rk,F

to
F1 and a state stRk,2. Then, F∞(pkk, (mk,i)i∈[q]; rk,F) is invoked.
Rk,2
on
a
challenge
index
j∗
k
∈
[q]
which is queried by F1(pkk, (mk,i)i∈[q]; rk,F) and the state stRk,2, out-
puts a pair

(σk,i)i∈[q]\{j∗
k}, stRk,3

of a sequence (σk,i)i∈[q]\{j∗
k} of sig-
natures replied to F2 and a state stRk,3. Then, F2((σk,i)i∈[q]\{j∗
k}, stk,F)
is invoked, where stk,F is a state which is output by the k-th invocation
of F1.
Rk,3 on a challenge signature σ∗
k output by F2, ((σk,i)i∈[q]\{j∗
k} and the state
stRk,3, outputs the state stRk+1,1 which will be used to execute the next
sub-algorithm.
R4 on the state stRI+1,1 output by RI,3, moves onto Challenge phase of the
imp-pa game with sending a commitment ˆ
cmt to C. Simultaneously, it outputs
a state stR5.
R5 on a challenge
ˆ
cha received from C and the state stR5, replies a response ˆ
res
to C.
As
the
simple
reduction,
we
suppose
that
the
states

stRk,1, stRk,2, stRk,3

k∈[I], stRI+1,1 and stR5 contain the random coins rR which
are given to R0, and the algorithms

(Rk,1, Rk,2, Rk,3)k∈[I], R4, R5

use the ran-
dom coins rR in these executions.

R0, (Rk,1, Rk,2, Rk,3)k∈[I], R4

can query to
the transcript oracle TrID
sk,pk and the random oracle. R5, F1 and F2 are allowed
to query to the random oracle.

404
M. Fukumitsu and S. Hasegawa
Note that the reduction which is allowed to rewind F can be converted into
a sequentially multi-instance reduction by regarding the rewind of F as the new
invocation of F as mentioned in the proof of Theorem 1 on [16]. In a similar
manner to Theorem 2, the following theorem can be proven.
Theorem 6. Assume that FS-Sig is proven to be SMA secure in the NPROM via
a sequentially multi-instance reduction from the imp-pa security of the underlying
ID scheme ID. Then ID is not imp-pa secure.
By combining Theorem 6 with Proposition 1, the following corollary follows.
Corollary 7. Assume that FS-Sig is proven to be EUF-CMA in the NPROM via
a sequentially multi-instance reduction from the imp-pa security of the underlying
ID scheme ID. Then ID is not imp-pa secure.
4
Impossibility of Proving the Security of the Schnorr
Signature
In this section, we prove that the DL assumption is a necessary and suﬃcient
condition for the impossibility of proving the EUF-CMA security of the Schnorr
signature in the NPROM via a sequentially multi-instance reduction.
Let G be a group of prime order p with a generator g. GGen is a group
generation algorithm which on a security parameter 1λ, outputs a tuple (G, p, g)
of a group description G, the order p of G and a generator g of G such that p is
of polynomial length in λ. An algorithm R is said to solve the discrete logarithm
(DL) problem if on a pair ((G, p, g), y) of a tuple (G, p, g) and an element y ∈G,
R outputs a solution x ∈Zp such that y = gx. Then the DL assumption holds
if there exists no PPT algorithm R which solves the DL problem for a tuple
(G, p, g) ←GGen(1λ) and y = gx with non-negligible probability, where the
probability is taken over the choice of x ∈U Zp and the internal coin ﬂips of
GGen and R.
The Schnorr signature is derived from the Schnorr ID scheme which is
depicted in Fig. 8 via the FS transformation. It is known that the imp-pa secu-
rity of the Schnorr ID is polynomially equivalent to the DL assumption [28]. By
employing this fact and by applying Corollary 7 to the Schnorr signature, we
have the following theorem.
Theorem 8. Assume that the Schnorr signature is proven to be EUF-CMA in
the NPROM via a sequentially multi-instance reduction from the DL assumption.
Then, the DL assumption does not hold.
Proof. Assume that the Schnorr signature is proven to be EUF-CMA in the
NPROM via a sequentially multi-instance reduction from the DL assumption.
Since the imp-pa security of the Schnorr ID is polynomially equivalent to the
DL assumption [28], the Schnorr signature is proven to be EUF-CMA in the
NPROM via a sequentially multi-instance reduction from the imp-pa security of
the Schnorr ID. It follows from Corollary 7 that the Schnorr ID is not imp-pa

Impossibility on the Provable Security of the Fiat-Shamir-Type Signatures
405
Schnorr ID [28]
K on a security parameter 1λ, outputs a key pair (sk, pk) := (x, ((G, p, g), y)) by
running (G, p, g) ←GGen(1λ), choosing x ∈U Zp and then setting y := gx.
CH for each pk ∈PKλ, let CHpk := Zp.
P1 on a key pair (sk, pk), outputs a pair (st, cmt) of a state st ∈U Zp and a commitment
cmt := gst.
P2 on a key pair (sk, pk), a pair (st, cmt) of an output by P1, and a challenge cha ∈Zp,
outputs a response res := st + cha · sk mod p.
V on a tuple (pk, (cmt, cha, res)) of a public key pk and a transcript (cmt, cha, res),
outputs 1 if it holds that cmt = gresy−cha.
Fig. 8. Schnorr ID
secure. This implies that the DL assumption does not hold by the equivalence
between the imp-pa security of the Schnorr ID and the DL assumption.
⊓⊔
On the other hand, if the DL assumption does not hold, the Schnorr signature
is trivially proven to be EUF-CMA in the NPROM from the DL assumption.
Therefore, this result shows that the DL assumption is not compatible with the
EUF-CMA security of the Schnorr signature in the NPROM.
References
1. Abdalla, M., An, J.H., Bellare, M., Namprempre, C.: From identiﬁcation to signa-
tures via the Fiat-Shamir transform: necessary and suﬃcient conditions for security
and forward-security. IEEE Trans. Inf. Theor. 54(8), 3631–3646 (2008)
2. Abe, M., Groth, J., Ohkubo, M.: Separating short structure-preserving signatures
from non-interactive assumptions. In: Lee, D.H., Wang, X. (eds.) ASIACRYPT
2011. LNCS, vol. 7073, pp. 628–646. Springer, Heidelberg (2011)
3. Abe, M., Haralambiev, K., Ohkubo, M.: Group to group commitments do not
shrink. In: Pointcheval, D., Johansson, T. (eds.) EUROCRYPT 2012. LNCS, vol.
7237, pp. 301–317. Springer, Heidelberg (2012)
4. Bader, C., Jager, T., Li, Y., Sch¨age, S.: On the impossibility of tight cryptographic
reductions. In: Fischlin, M., Coron, J.-S. (eds.) EUROCRYPT 2016. LNCS, vol.
9666, pp. 273–304. Springer, Heidelberg (2016). doi:10.1007/978-3-662-49896-5 10
5. Baldimtsi, F., Lysyanskaya, A.: On the security of one-witness blind signature
schemes. In: Sako, K., Sarkar, P. (eds.) ASIACRYPT 2013, Part II. LNCS, vol.
8270, pp. 82–99. Springer, Heidelberg (2013)
6. Bellare, M., Rogaway, P.: Random oracles are practical: a paradigm for designing
eﬃcient protocols. In: ACM CCS 1993, Fairfax, Virginia, USA, pp. 62–73. ACM
Press, New York (1993)
7. Boneh, D., Franklin, M.: Identity-based encryption from the Weil pairing. In:
Kilian, J. (ed.) CRYPTO 2001. LNCS, vol. 2139, pp. 213–229. Springer, Heidelberg
(2001)

406
M. Fukumitsu and S. Hasegawa
8. Boneh, D., Venkatesan, R.: Breaking RSA may not be equivalent to factoring.
In: Nyberg, K. (ed.) EUROCRYPT 1998. LNCS, vol. 1403, pp. 59–71. Springer,
Heidelberg (1998)
9. Bresson, E., Monnerat, J., Vergnaud, D.: Separation results on the “One-More”
computational problems. In: Malkin, T. (ed.) CT-RSA 2008. LNCS, vol. 4964, pp.
71–87. Springer, Heidelberg (2008)
10. Chen, Y., Huang, Q., Zhang, Z.: Sakai-Ohgishi-Kasahara identity-based non-
interactive key exchange scheme, revisited. In: Susilo, W., Mu, Y. (eds.) ACISP
2014. LNCS, vol. 8544, pp. 274–289. Springer, Heidelberg (2014)
11. Coron, J.-S.: Optimal security proofs for PSS and other signature schemes. In:
Knudsen, L.R. (ed.) EUROCRYPT 2002. LNCS, vol. 2332, pp. 272–287. Springer,
Heidelberg (2002)
12. Fiat, A., Shamir, A.: How to prove yourself: practical solutions to identiﬁcation
and signature problems. In: Odlyzko, A.M. (ed.) CRYPTO 1986. LNCS, vol. 263,
pp. 186–194. Springer, Heidelberg (1987)
13. Fischlin, M., Fleischhacker, N.: Limitations of the meta-reduction technique: the
case of Schnorr signatures. In: Johansson, T., Nguyen, P.Q. (eds.) EUROCRYPT
2013. LNCS, vol. 7881, pp. 444–460. Springer, Heidelberg (2013)
14. Fischlin, M., Lehmann, A., Ristenpart, T., Shrimpton, T., Stam, M., Tessaro, S.:
Random oracles with(out) programmability. In: Abe, M. (ed.) ASIACRYPT 2010.
LNCS, vol. 6477, pp. 303–320. Springer, Heidelberg (2010)
15. Fleischhacker, N., Jager, T., Schr¨oder, D.: On tight security proofs for Schnorr
signatures. In: Sarkar, P., Iwata, T. (eds.) ASIACRYPT 2014. LNCS, vol. 8873,
pp. 512–531. Springer, Heidelberg (2014)
16. Fukumitsu, M., Hasegawa, S.: Black-box separations on Fiat-Shamir-type signa-
tures in the non-programmable random oracle model. In: L´opez, J., Mitchell, C.J.
(eds.) ISC 2015. LNCS, vol. 9290, pp. 3–20. Springer, Heidelberg (2015)
17. Fukumitsu, M., Hasegawa, S., Isobe, S., Koizumi, E., Shizuya, H.: Toward separat-
ing the strong adaptive pseudo-freeness from the strong RSA assumption. In: Boyd,
C., Simpson, L. (eds.) ACISP. LNCS, vol. 7959, pp. 72–87. Springer, Heidelberg
(2013)
18. Fukumitsu, M., Hasegawa, S., Isobe, S., Shizuya, H.: On the impossibility of prov-
ing security of strong-RSA signatures via the RSA assumption. In: Susilo, W., Mu,
Y. (eds.) ACISP 2014. LNCS, vol. 8544, pp. 290–305. Springer, Heidelberg (2014)
19. Goldwasser, S., Micali, S., Rivest, R.L.: A digital signature scheme secure against
adaptive chosen-message attacks. SIAM J. Comput. 17(2), 281–308 (1988)
20. Guillou, L.C., Quisquater, J.-J.: A practical zero-knowledge protocol ﬁtted to
security microprocessor minimizing both transmission and memory. In: G¨unther,
C.G. (ed.) EUROCRYPT 1988. LNCS, vol. 330, pp. 123–128. Springer, Heidelberg
(1988)
21. Katz, J., Wang, N.: Eﬃciency improvements for signature schemes with tight secu-
rity reductions. In: ACM CCS 2003. pp. 155–164. ACM, New York (2003)
22. Kawai, Y., Sakai, Y., Kunihiro, N.: On the (im)possibility results for strong attack
models for public key cryptsystems. JISIS 1(2/3), 125–139 (2011)
23. Nielsen, J.B.: Separating random oracle proofs from complexity theoretic proofs:
the non-committing encryption case. In: Yung, M. (ed.) CRYPTO 2002. LNCS,
vol. 2442, pp. 111–126. Springer, Heidelberg (2002)
24. Okamoto, T.: Provably secure and practical identiﬁcation schemes and correspond-
ing signature schemes. In: Brickell, E.F. (ed.) CRYPTO 1992. LNCS, vol. 740, pp.
31–53. Springer, Heidelberg (1993)

Impossibility on the Provable Security of the Fiat-Shamir-Type Signatures
407
25. Paillier, P., Vergnaud, D.: Discrete-log-based signatures may not be equiva-
lent to discrete log. In: Roy, B. (ed.) ASIACRYPT 2005. LNCS, vol. 3788, pp.
1–20. Springer, Heidelberg (2005)
26. Paillier, P., Villar, J.L.: Trading one-wayness against chosen-ciphertext security in
factoring-based encryption. In: Lai, X., Chen, K. (eds.) ASIACRYPT 2006. LNCS,
vol. 4284, pp. 252–266. Springer, Heidelberg (2006)
27. Pointcheval, D., Stern, J.: Security arguments for digital signatures and blind sig-
natures. J. Cryptology 13(3), 361–396 (2000)
28. Schnorr, C.: Eﬃcient signature generation by smart cards. J. Cryptology 4(3),
161–174 (1991)
29. Seurin, Y.: On the exact security of Schnorr-type signatures in the random oracle
model. In: Pointcheval, D., Johansson, T. (eds.) EUROCRYPT 2012. LNCS, vol.
7237, pp. 554–571. Springer, Heidelberg (2012)
30. Shoup, V.: A proposal for an iso standard for public key encryption. Cryptology
ePrint Archive, Report 2001/112 (2001). http://eprint.iacr.org/
31. Zhang, J., Zhang, Z., Chen, Y., Guo, Y., Zhang, Z.: Black-box separations for
one-more (static) CDH and its generalization. In: Sarkar, P., Iwata, T. (eds.) ASI-
ACRYPT 2014, Part II. LNCS, vol. 8874, pp. 366–385. Springer, Heidelberg (2014)
32. Zhang, Z., Chen, Y., Chow, S.S.M., Hanaoka, G., Cao, Z., Zhao, Y.: Black-box
separations of hash-and-sign signatures in the non-programmable random oracle
model. In: Au, M.-H., et al. (eds.) ProvSec 2015. LNCS, vol. 9451, pp. 435–454.
Springer, Heidelberg (2015). doi:10.1007/978-3-319-26059-4 24

Eﬃcient Functional Encryption
for Inner-Product Values
with Full-Hiding Security
Junichi Tomida1(B), Masayuki Abe2, and Tatsuaki Okamoto2
1 Kyoto University, Kyoto, Japan
tomida@ai.soc.i.kyoto-u.ac.jp
2 NTT Secure Platform Laboratories, Tokyo, Japan
{abe.masayuki,okamoto.tatsuaki}@lab.ntt.co.jp
Abstract. We construct an eﬃcient non-generic private-key functional
encryption (FE) for inner-product values with full-hiding security, where
conﬁdentiality is assured not only for encrypted data but also for func-
tions associated with secret keys. Recently, Datta et al. presented such
a scheme in PKC 2016 and this is the only scheme that achieved full-
hiding security. Our scheme has an advantage over their scheme for the
following points.
1. More eﬃcient: our scheme is two times faster in encryption and
decryption, and a master secret key, secret keys and ciphertexts are
the half size, compared with their scheme.
2. Weaker assumption: our scheme is secure under the decisional linear
(DLIN) assumption or its variant, while their scheme is under a
stronger assumption, the symmetric external Diﬃe-Hellman (SXDH)
assumption.
3. More ﬂexible: we can apply our scheme to any type of bilinear pairing
groups, while their scheme is suitable only for type 3 groups.
Keywords: Functional encryption · Inner product · Function privacy
1
Introduction
Functional encryption (FE) is a very useful tool for non-interactive computation
on encrypted data [8]. In a FE scheme, an owner of a master secret key msk
can create a secret key skf for a function f, and it enables users to compute the
value of f(x) by decrypting a ciphertext for x without revealing anything else
about x. As cloud services are increasing rapidly, users’ demand for computation
on encrypted data is also increasing because cloud servers are by no means
trustful. FE is one solution for this problem, providing a paradigm where users
can compute a function f on encrypted data using a secret key skf without
revealing anything else about the encrypted data to the cloud server.
One of principal interests in FE is what class of of functions F can be sup-
ported and what kind of security can be achieved. It has started from identity-based
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 408–425, 2016.
DOI: 10.1007/978-3-319-45871-7 24

Eﬃcient Functional Encryption for Inner-Product Values
409
encryption (IBE) [7], followed by attribute-based encryption (ABE) [22], inner-
product encryption (IPE) [16,17,19,20] and predicate encryption (PE) [16]. Amaz-
ingly, recent works realize computation of general polynomial-size circuit [13,14],
although they require expensive assumptions like indistinguishability obfuscation
and they are far from being practical. Motivated by this unreality, Abdalla et al.
[1] introduced a new non-generic FE scheme specialized for computation of inner-
product values, which is eﬃcient and constructed from standard assumptions. As
Abdalla et al. mentioned in their work, evaluation of inner-product is a very use-
ful tool for statistics because it can provide the weighted mean. The scheme of [1]
only has a selective security, and following works present adaptively secure schemes
[2,5]. Note that FE for inner-product values is diﬀerent from inner-product encryp-
tion (IPE) in the context of predicate encryption, where a secret key for a predicate
⃗y and a ciphertext for a message m and an attribute ⃗x yield m iﬀ⃗x · ⃗y = 0. On the
other hand, FE for inner-product values enables users to compute inner-product
values themselves. To avoid confusion, we refer to FE for inner-product values as
inner-product value encryption (IPVE) and this is the main topic of our paper.
Function Privacy of Functional Encryption.
In terms of security, most of
research on FE has concentrated on conﬁdentiality of ciphertexts [8,21]. It is
very inconvenient for real applications, however, because one may need to hide a
function as well as a plaintext from others. Consider the case where Alice holds
encrypted data on an untrusted cloud server and wants to compute a secret
algorithm f on the data. Alice may accomplish her purpose by a FE scheme, to
create a secret key for the function f, send it to the server and get the result.
However, if the FE scheme does not support the privacy of functions associated
with secret keys, the secret key for f may reveal sensitive information about
f to the server. Consequently, it is important to consider the function privacy
of FE, which assure that secret keys do not reveal any information on associ-
ated functions. In the public-key setting, we need to put some restrictions on
the distribution of the functions to obtain a meaningful security of function pri-
vacy [10,11]. It is because users can encrypt any message and examine what
the function associated with their secret key is, by decrypting them. On the con-
trary, we can obtain the strongest notion of indistinguishability-based security in
the private-key setting, known as full-hiding security, which guarantees function
privacy as well as message privacy [4,9].
Full-Hiding Security. For private-key FE, the notion of full-hiding security con-
siders an adversary that interacts with an encryption oracle Encb and a secret key
oracle KeyGenb where b is randomly chosen from {0, 1}. The adversary queries a
pair of messages (x(ℓ)
0 , x(ℓ)
1 ) to the encryption oracle and gets a ciphertext for x(ℓ)
b .
It also gets a secret key for f (j)
b
from the key generation oracle similarly. In the
game for guessing b, the minimum necessary restriction is f (j)
0 (x(ℓ)
0 ) = f (j)
1 (x(ℓ)
1 )
for all ℓand j, as otherwise the adversary can trivially determine b. After the
query phase under the above restriction, if any adversary can guess b only with
negligible advantage, then we say that the private-key FE scheme has full-hiding

410
J. Tomida et al.
security. This notion implies that any eﬃcient adversary that has secret keys for
f1, · · · , fm and encryption of x1, · · · , xn, cannot obtain any information other
than the values of {fi(xj)}1≤i≤m,1≤j≤n [9].
Inner-Product Value Encryption with Full-Hiding Security. For these reasons,
we have great interest to construct direct and eﬃcient FE schemes for prac-
tical functionalities with full-hiding security, and it is a quite important task
for research on FE. Bishop et al. [6] have taken a ﬁrst step for a private-key
IPVE scheme that has function privacy, although their scheme achieved rather
weak security. Considering natural application of full-hiding security to private-
key IPVE, the restriction of queries that the adversary can make would be
⃗x(ℓ)
0 ·⃗y(j)
0
= ⃗x(ℓ)
1 ·⃗y(j)
1 . However they assumes the following restriction in the game,
that is ⃗x(ℓ)
0
· ⃗y(j)
0
= ⃗x(ℓ)
1
· ⃗y(j)
0
= ⃗x(ℓ)
0
· ⃗y(j)
1
= ⃗x(ℓ)
1
· ⃗y(j)
1 . This unnatural restric-
tion weakens its security guarantee. Very recently, Datta et al. [12] presented a
private-key IPVE scheme with natural full-hiding security. Unfortunately, their
scheme is wasteful because it needs a 4n dimensional vector space to encrypt n
dimensional vectors.
1.1
Our Contribution
We construct a more eﬃcient and ﬂexible private-key IPVE scheme with full-
hiding security than that of [12], which is the only scheme that achieved full-
hiding security. In addition, our scheme is secure under weaker assumptions than
their scheme. To ensure correctness, like [6,12], our scheme requires that inner-
products are within a polynomial range, where discrete logarithm of g⃗x·⃗y can be
found in polynomial time. This is a reasonable requirement because results of
statistical computation, like average, on a polynomial-size database will naturally
be in a polynomial range.
Eﬃciency. Before discussing the eﬃciency, we recall techniques of private-key
IPVE schemes. The schemes of [6,12] and our scheme are all constructed based
on dual paring vector spaces (DPVS) introduced by Okamoto et al. [18,19],
and they have the same structure. Namely, a master secret key is orthonormal
bases of DPVS, secret keys and ciphertexts are vectors of DPVS, both key gen-
eration algorithm and encryption algorithms involve scalar multiplications on
cyclic groups, and a decryption algorithm involves paring operations on bilinear
paring groups. Our scheme is superior to that of [12] with a constant factor 2,
in terms of both necessary storage and computational eﬃciency (Table 1).
Assumption and Flexibility. The schemes of [6,12] are secure under the sym-
metric external Diﬃe-Hellman (SXDH) assumption, while our scheme is the
decisional linear (DLIN) assumption or its variant (XDLIN). SXDH holds in
only type 3 bilinear pairing groups because if there is a isomorphism from one
group to the other group, the SXDH problem is trivial. On the other hand, DLIN
and XDLIN hold even if there is any isomorphism between two groups. In other

Eﬃcient Functional Encryption for Inner-Product Values
411
Table 1. Comparison of private-key IPVE schemes, where n is a dimension of a vector
to encrypt, msk, sk and ct size are numbers of group elements.
BJK15 [6]
DDM15 [12]
Our scheme
Security
Not full-hiding Full-hiding
Full-hiding
msk size
8n2 + 8
8n2 + 12n + 28 4n2 + 18n + 20
sk,ct size
2n + 2
4n + 8
2n + 5
#Scalar multiplications 2n + 2
4n + 8
2n + 5
#Paring operations
2n + 2
4n + 8
2n + 5
Assumptions
SXDH
SXDH
XDLIN or DLIN
Pairing groups
type 3
type 3
type 1,2,3
words, they hold in any type of bilinear pairing groups. In that sense, DLIN
and XDLIN are weaker assumptions than SXDH. For the reason, the schemes
of [6,12] work in only type 3 groups while we can use our scheme in any type
of groups. We often construct cryptographic schemes on type 3 groups in cur-
rent situation but the future condition is unpredictable, then we believe that the
ﬂexibility of our scheme is advantage. See Sect. 2.2 about the types of bilinear
paring groups.
Our Technique. From a technical view point, it is generally diﬃcult to achieve
more eﬃcient cryptographic schemes from weaker assumptions. In our case, the
DLIN assumption indicates that it is hard to guess whether an DLIN instance
spans 2 or 3 dimensions, while the SXDH assumption does 1 or 2. Then it
seems that we need more dimensions or group elements to construct schemes
from DLIN than from SXDH. However, we have overcome this diﬃculty by
developing new two techniques for the security proof. These techniques enable
us to change the form of secret keys and ciphertexts in compact DPVS over
security game transition. Let we give an intuitive explanation. The ﬁrst technique
is, between two hybrid games, to reduce a diﬀerence of one coeﬃcient in a secret
key or a ciphertext, to a DLIN or XDLIN instance. In contrast, Datta et al.
have made reductions from a diﬀerence of n coeﬃcients to an SXDH instance
in their security proof. Our technique can save about n dimensions of vector
spaces compared with their technique. The second technique is, in the series of
security game, to change directly a secret key including ⃗y(j)
0
into one including
⃗y(j)
1 . Datta et al. employed a more complicated but unnecessary technique for
this transformation, which requires another n dimensions.
2
Preliminary
2.1
Notation
For a set S, x
U
←−S denotes that x is uniformly chosen from S. For a proba-
bility distribution X, x
R
←−X denotes that x is chosen from X according to its

412
J. Tomida et al.
distribution. For a prime q, Zq denotes a set of integers {0, · · · , q −1}, and Z×
q
denotes a set of integers {1, · · · , q −1}. ⃗0 denotes a zero vector. For a n dimen-
sional vector ⃗x, xi(1 ≤i ≤n) denotes the i-th component of ⃗x. For vectors
⃗x, ⃗y ∈Zn
q , ⃗x · ⃗y denotes inner-product of ⃗x and ⃗y over Zq. For vector compo-
nents, 0n denotes a line of n zeros, e.g., ⃗a := (0, 0, 0, 1) = (03, 1). For a security
game and an adversary A, ExpGame
A
(λ) →b denotes an event where A outputs b
in the game. For a function f : N →R, f(λ) < ϵ(λ) denotes that f is negligible
in λ, and means that ∀c > 0, ∃n ∈N, ∀λ > n, f(λ) < λ−c.
2.2
Bilinear Pairing Groups
Bilinear pairing groups are deﬁned by the tuple (q, G1, G2, GT , e), where q is a
prime, G1, G2 and GT are cyclic groups of order q, and e : G1 × G2 →GT is a
map that has the following properties:
1. Bilinear: ∀G1 ∈G1, ∀G2 ∈G2, ∀a, b ∈Zq, e(aG1, bG2) = e(G1, G2)ab
2. Non-degenerate: if ∀G1 ∈G1, e(G1, G2) = 1, then G2 = 0.
There are three types of bilinear groups according to whether eﬃcient isomor-
phisms exist or not between G1 and G2 [15]. In the type 1, both the isomorphism
φ : G2 →G1 and its inverse φ−1 : G1 →G2 can be computed eﬃciently, i.e.,
G1 = G2. In the type 2, the isomorphism φ : G2 →G1 is computed eﬃciently but
its inverse is not. Type 3 groups have no eﬃcient isomorphisms between G1 and
G2. Type1 groups are called symmetric bilinear pairing groups, and type 2 and 3
are called asymmetric bilinear pairing groups. We use type 3 groups for our scheme
in this paper, but we can easily apply it to type 1 or 2 groups. Let Gabpg be an asym-
metric bilinear pairing group generator that takes 1λ and outputs a description of
groups (q, G1, G2, GT , e) and generators of groups G1 ̸= 0 ∈G1, G2 ̸= 0 ∈G2. We
denote the tuple (q, G1, G2, GT , G1, G2, e) by paramG.
2.3
Dual Pairing Vector Spaces [18,19]
We will construct our scheme based on dual pairing vector spaces (DPVS). There
are two types of DPVS, one is using symmetric bilinear pairing groups and
the other is asymmetric bilinear pairing groups [19]. In this paper, we use the
asymmetric version of DPVS but we can also use the symmetric version similarly.
We brieﬂy explain the notion of DPVS here, and see [19] for more details.
Deﬁnition 1 (Dual Pairing Vector Spaces: DPVS). DPVS are deﬁned by
the tuple (q, V, V∗, GT , A, A∗, ˜e), which is directly constructed from paramG
R
←−
Gabpg(1λ). V := Gn
1 and V∗:= Gn
2 are n dimensional vector spaces, A :=
(a1, · · · , an) and A∗
:=
(a∗
1, · · · , a∗
n) are canonical bases, where ai
:=
(0i−1, G1, 0n−i), a∗
i := (0i−1, G2, 0n−i), and ˜e : V × V∗→GT is pairing deﬁned
below. A prime q and a group GT are the same entities as the instance of pair-
ing groups. The pairing is deﬁned by ˜e(x, y) := n
i=1 e(Xi, Yi) ∈GT , where
x := (X1, · · · , Xn) ∈V, y := (Y1, · · · , Yn) ∈V∗.

Eﬃcient Functional Encryption for Inner-Product Values
413
Here we consider random dual orthonormal bases:
ψ
U
←−Z×
q , B := (bi,j)1≤i,j≤n
U
←−GL(n, Zq), (b∗
i,j)1≤i,j≤n := ψ(BT)−1
bi :=
n

j=1
bi,jaj, b∗
i :=
n

j=1
b∗
i,ja∗
j for i = 1, · · · , n,
B := (b1, · · · , bn), B∗:= (b∗
1, · · · , b∗
n), gT := e(G1, G2)ψ.
Let Gob be random dual orthonormal basis generator that takes 1λ and
a dimension of bases n and outputs (paramG, B, B∗, gT ), where B, B∗, gT are
computed as demonstrated above. We denote the combination (paramG, gT ) by
paramV. For a vector ⃗x := (x1, · · · , xn)T ∈Zn
q and a basis B := (b1, · · · , bn), we
denote n
i=1 xibi by (⃗x)B. Then it can be seen that
˜e((⃗x)A, (⃗y)A∗) =
n

i=1
e(xiG1, yiG2) = e(G1, G2)
n
i=1 xiyi = e(G1, G2)⃗x·⃗y,
∴
˜e((⃗x)B, (⃗y)B∗) = ˜e

(B⃗x)A, (ψ(BT)−1⃗y)A∗
= e(G1, G2)ψB⃗x·(BT)−1⃗y = g⃗x·⃗y
T .
2.4
External Decisional Linear Assumption
When we construct our scheme on type 3 groups, we assume the following
property.
Deﬁnition 2 (External Decisional Linear Assumption: XDLIN [3]). We
choose an arbitrary number x ∈{1, 2}. The XDLIN problem is to guess a bit b,
given Pb, where
paramG
R
←−Gabpg(1λ), ξ, κ, δ, σ, ρ
U
←−Zq,
Y0 := (δ + σ)Gx, Y1 := (δ + σ + ρ)Gx,
Pb := (paramG, ξG1, κG1, δξG1, σκG1, ξG2, κG2, δξG2, σκG2, Yb).
For any probabilistic polynomial time (PPT) adversary A, if the advantage of A
for the XDLIN problem is negligible in λ, then we say that the XDLIN assump-
tion holds. Namely,
AdvXDLIN
A
(λ) :=
Pr[A(1λ, P0) →1] −Pr[A(1λ, P1) →1]
 < ϵ(λ).
Remark 1. We can also construct our scheme on type 1 or 2 groups, and in
that case we use the standard decisional linear (DLIN) assumption in G2. See
Deﬁnition 11 in [19] about the DLIN assumption. Roughly speaking, the DLIN
assumption in G2 is suﬃcient for the security proof of our scheme because we
can obtain G1 elements that have the same coeﬃcients as G2 elements of a DLIN
instance, using a eﬃcient isomorphism φ : G2 →G1.

414
J. Tomida et al.
2.5
The Notion of Private-Key Inner-Product Value Encryption
Weadoptthegeneralnotionoffunction-privateFEintheprivate-keysetting,intro-
duced in [4], to the particular functionality of computing inner-product values over
Zq. We denote private-key inner-product value encryption by Priv-IPVE.
Deﬁnition 3 (Private-Key Inner-Product Value Encryption). A Priv-
IPVE scheme Π consists of four PPT algorithms Setup, KeyGen, Enc and Dec:
– Setup(1λ, n): The setup algorithm takes as input a security parameter 1λ and
a vector length parameter n (a positive integer that is polynomial in λ). Then
it outputs a public parameter pp and a master secret key msk.
– KeyGen(pp, msk, ⃗y): The key generation algorithm takes as input a public para-
meter pp, a master secret key msk and a key vector ⃗y ∈Zn
q . Then it outputs
a corresponding secret key sk⃗y.
– Enc(pp, msk, ⃗x): The encryption algorithm takes as input a public parameter
pp, a master secret key msk and a message vector ⃗x ∈Zn
q . Then it outputs a
ciphertext ct⃗x.
– Dec(pp, sk⃗y, ct⃗x): The decryption algorithm takes as input a public parameter
pp, a secret key sk⃗y and a ciphertext ct⃗x. Then it outputs either a value m ∈Zq
or a symbol ⊥.
We assume the following property for correctness: for all (pp, msk)
R
←−Setup
(1λ, n), all sk⃗y
R
←−KeyGen(pp, msk, ⃗y) and all ct⃗x
R
←−Enc(pp, msk, ⃗x), Dec
(pp, sk⃗y, ct⃗x) must output m = ⃗x · ⃗y without negligible probability.
Deﬁnition 4 (Full-Hiding Security). An Priv-IPVE scheme Π has full-
hiding security if, for any PPT adversaries A, the advantage in the following
game is negligible in λ:
1. The challenger runs Setup(1λ, n) to generate pp and msk, and gives pp to A.
It also chooses a random bit b.
2. A may adaptively make a polynomial number of queries of the following two
types:
– Secret key query: For the j-th query, A submits a pair of vectors (⃗y(j)
0 , ⃗y(j)
1 )
and the challenger replies to A with sk⃗y(j)
b
R
←−KeyGen(pp, msk, ⃗y(j)
b ).
– Ciphertext query: For the ℓ-th query, A submits a pair of vectors
(⃗x(ℓ)
0 , ⃗x(ℓ)
1 ) and the challenger replies to A with ct⃗x(ℓ)
b
R
←−Enc(pp, msk, ⃗x(ℓ)
b ).
There is a restriction for queries that A can make such that all queried vectors
must suﬃce ⃗x(ℓ)
0
· ⃗y(j)
0
= ⃗x(ℓ)
1
· ⃗y(j)
1
for all j and ℓ.
3. A outputs a bit b′ as a conjecture of b.

Eﬃcient Functional Encryption for Inner-Product Values
415
The advantage of A in this game is deﬁned as
AdvFHS,Π
A
(λ) :=
Pr[ExpGame 0
A
(λ) →1] −Pr[ExpGame 1
A
(λ) →1]

where this game is deﬁned as Game 0 if b = 0 and as Game 1 if b = 1.
3
Construction
In this section, we present our private-key IPVE scheme with full-hiding security.
Setup(1λ, n): The setup algorithm selects (B, B∗, paramV)
R
←−Gob(1λ, 2n + 5) and
outputs (pp, msk), where
	B := (b1, · · · , bn, b2n+1, b2n+2), 	B∗:= (b∗
1, · · · , b∗
n, b∗
2n+3, b∗
2n+4),
pp := (1λ, paramV), msk := 	B, 	B∗.
KeyGen(pp, msk, ⃗y): The key generation algorithm computes and outputs a secret
key sk⃗y as
β, θ
U
←−Zq, sk⃗y := (⃗y, 0n, 0, 0, β, θ, 0)B∗.
Enc(pp, msk, ⃗x): The encryption algorithm computes and outputs a ciphertext
ct⃗x as
α, φ
U
←−Zq, ct⃗x := (⃗x, 0n, α, φ, 0, 0, 0)B.
Dec(pp, sk⃗y, ct⃗x): The decryption algorithm computes the pairing of a secret key
and a ciphertext as d := ˜e(ct⃗x, sk⃗y). Then it computes m such that gm
T = d
in the polynomial range ﬁxed in advance. If it ﬁnds m that satisﬁes gm
T = d,
outputs m. Otherwise, outputs ⊥.
Correctness: Observe that d := ˜e(ct⃗x, sk⃗y) = g⃗x·⃗y
T . Therefore, m that the decryp-
tion algorithm outputs is ⃗x · ⃗y.
Remark 2. In the schemes of [6,12], we employ two pairs of dual bases, one is
used for encrypting vectors and the other is used to encode the same scalars that
randomize the corresponding vectors for secret keys and ciphertexts. Because of
this construction, it looks like diﬃcult to create another ciphertext from some
ciphertexts, while it is easy to do that in our scheme. In other words, our scheme
is malleable. However the schemes of [6,12] have not been proven that they are
non-malleable. Consequently, our scheme can achieve the same security as [12].
4
Security
4.1
Lemmas for the Security Proof
We consider the following problems and use them to prove the security of our
scheme. As mentioned in Sect. 2, we consider type 3 groups.

416
J. Tomida et al.
Deﬁnition 5 (Problem 0). Problem 0 is to guess a bit b, given (paramP0,
B, 	B∗, yb, κG1, ξG2), where
paramG
R
←−Gabpg(1λ),
B := (bi,j)1≤i,j≤3
U
←−GL(3, Zq), (b∗
i,j)1≤i,j≤3 := (BT)−1,
κ, ξ
U
←−Z×
q , bi := κ
3

j=1
bi,jaj, b∗
i := ξ
3

j=1
b∗
i,ja∗
j
for i = 1, 2, 3,
B := (b1, b2, b3), 	B∗:= (b∗
1, b∗
3),
gT := e(G1, G2)κξ, paramP0 := (paramG, gT ),
δ, σ
U
←−Zq, ρ
U
←−Z×
q , y0 := (δ, 0, σ)B, y1 := (δ, ρ, σ)B.
Deﬁnition 6 (Problem 1). Problem 1 is to guess a bit b, given (paramV,
	B, 	B∗, gb), where
(B, B∗, paramV)
R
←−Gob(1λ, 2n + 5),
	B := (b1, · · · , b2n, b2n+1, b2n+2), 	B∗:= (b∗
1, · · · , b∗
2n, b∗
2n+3, b∗
2n+4),
α, φ
U
←−Zq, τ
U
←−Z×
q ,
g0 := (02n, α, φ, 0, 0, 0)B, g1 := (02n, α, φ, 0, 0, τ)B.
Deﬁnition 7 (Problem 2). Problem 2 is to guess a bit b, given (paramV,
	B, 	B∗, g∗
b), where
(B, B∗, paramV)
R
←−Gob(1λ, 2n + 5),
	B := (b1, · · · , b2n, b2n+1, b2n+2), 	B∗:= (b∗
1, · · · , b∗
2n, b∗
2n+3, b∗
2n+4),
β, θ
U
←−Zq, η
U
←−Z×
q ,
g∗
0 := (02n, 0, 0, β, θ, 0)B∗, g∗
1 := (02n, 0, 0, β, θ, η)B∗.
For a PPT algorithm A, the advantage for Problem n (n = 0, 1, 2) is deﬁned as
AdvPn
A (λ) :=
Pr[A(1λ, P0) →1] −Pr[A(1λ, P1) →1]

where Pb is an instance of the Problem n deﬁned above. Then following three
lemmas hold.
Lemma 1. For any PPT adversary B for Problem 0, there exists a PPT adver-
sary A for the XDLIN problem such that AdvP0
B (λ) ≤AdvXDLIN
A
(λ) + 5/q.
Lemma 2. ∀B, ∃A, AdvP1
B (λ) ≤AdvP0
A (λ).
Lemma 3. ∀B, ∃A, AdvP2
B (λ) ≤AdvP0
A (λ).
We perform a random linear transformation on V in the proofs of lemmas.
The deﬁnition of this operation is the same as Lemma 14 in [19].

Eﬃcient Functional Encryption for Inner-Product Values
417
Proof (Lemma 1). We show that we can construct a PPT adversary A for the
XDLIN problem from any PPT adversary B for Problem 0. A sets x = 1 and is
given an instance of XDLIN problem. A sets
gT := e(κG1, ξG2), paramP0 := (paramG, gT )
u1 := (ξ, 0, 1)A, u2 := (0, 0, 1)A, u1 := (0, κ, 1)A,
u∗
1 := (κ, 0, 0)A∗, u∗
2 := (−κ, −ξ, κξ)A∗, u∗
1 := (0, ξ, 0)A∗,
wb := (δξG1, σκG1, Yb).
A can compute u1, u2, u3, u∗
1, u∗
3. Then A generates a random linear transfor-
mation W on G3 and sets
bi := W(ui)
for i = 1, 2, 3, b∗
i := (W −1)T(u∗
i )
for i = 1, 3,
B := (b1, b2, b3), 	B∗:= (b∗
1, b∗
3), yb := W(wb).
Then A gives (paramP0, B, 	B∗, yb, κG1, ξG2) to B, and outputs b′ if B outputs b′.
If b = 0, observe that yb = (δ, 0, σ)B when κ, ξ ̸= 0, i.e., except with probability
2/q. If b = 1, yb = (δ, ρ, σ)B when κ, ξ, ρ ̸= 0, i.e., except with probability 3/q.
It is the same as an instance of Problem 0.
⊓⊔
Proof (Lemma 2). We show that we can construct a PPT adversary A for Prob-
lem 0 from any PPT adversary B for Problem 1. A is given an instance of Problem
0 (paramP0, B, 	B∗, yb, κG1, ξG2). Then A generates a random linear transforma-
tion W on G2n+5, and sets
paramV := paramP0,
di := W(0i+2, κG1, 02n+2−i)
for i = 1, · · · , 2n,
di := W(0i, κG1, 02n+4−i)
for i = 2n + 3, 2n + 4,
d2n+1 := W(b1, 02n+2), d2n+2 := W(b3, 02n+2), d2n+5 := W(b2, 02n+2),
d∗
i := (W −1)T(0i+2, ξG2, 02n+2−i)
for i = 1, · · · , 2n,
d∗
i := (W −1)T(0i, ξG2, 02n+4−i)
for i = 2n + 3, 2n + 4,
d∗
2n+1 := (W −1)T(b∗
1, 02n+2), d∗
2n+2 := (W −1)T(b∗
3, 02n+2),
d∗
2n+5 := (W −1)T(b∗
2, 02n+2), hb := W(yb, 02n+2),
D := (d1, · · · , d2n+5), D∗:= (d∗
1, · · · , d∗
2n+5).
We can see that (D, D∗) are dual orthonormal bases. A does not have b∗
2 but it
can compute
	D := (d1, · · · , dn, d2n+1, d2n+2),
	D∗:= (d∗
1, · · · , d∗
n, d∗
2n+3, d∗
2n+4).
Then A gives (paramV, 	D, 	D∗, hb) to B, and outputs b′ if B outputs b′. We can
see that h0 := (02n, α′, φ′, 0, 0, 0)D, h1 := (02n, α′, φ′, 0, 0, τ ′)D, where α′ := δ,
φ′ := σ and τ ′ := ρ. It is the same as an instance of Problem 1.
⊓⊔
Proof (Lemma 3). The proof of Lemma 3 is similar to that of Lemma 2.
⊓⊔

418
J. Tomida et al.
4.2
Security Proof of Our Scheme
The proposed scheme has full-hiding security, then the following theorem holds.
Theorem 1. The proposed Priv-IPVE scheme Π has full-hiding security under
the XDLIN assumption. For any PPT adversary B, there exists a PPT adversary
A for the XDLIN problem such that
AdvFHS,Π
B
(λ) ≤(2q1 + 4q2)(AdvXDLIN
A
(λ) + 5/q),
where q1 is a number of B’s secret key queries and q2 is a number of B’s ciphertext
queries.
Proof (Theorem 1). For the proof of Theorem 1, we use a hybrid argument over
a series of games that diﬀer in the construction of the challenge ciphertexts and
secret keys. The game sequence proceeds as Table 2. It also shows the structure
of ciphertexts and secret keys in the end of each game. In the Game 1 sequence,
we make ⃗x1 appear in the domain of n+1 to 2n-th dimensions of each ciphertext
one by one. In the Game 2 sequence, for each secret key, we gradually change ⃗y0
in the domain of 1 to n-th dimensions, into ⃗y1 in the domain of n + 1 to 2n-th
dimensions. Now we explain the sequence of the games. We frame a coeﬃcient
by a box that was changed from a previous game.
Table 2. Game sequence with structure of ciphertexts and secret keys.
Game
Ciphertexts
Secret keys
Game 0
(⃗x0, 0n, α, φ, 0, 0, 0)B (⃗y0, 0n, 0, 0, β, θ, 0)B∗
Game 1-1-1
...
...
...
Game 1-q2-3 (⃗x0, ⃗x1, α, φ, 0, 0, 0)B
(⃗y0, 0n, 0, 0, β, θ, 0)B∗
Game 2-1-1
...
...
...
Game 2-q1-3 (⃗x0, ⃗x1, α, φ, 0, 0, 0)B
(0n, ⃗y1, 0, 0, β, θ, 0)B∗
Game 3
(⃗x1, ⃗x0, α, φ, 0, 0, 0)B
(⃗y1, 0n, 0, 0, β, θ, 0)B∗
Game 4
(⃗x1, 0n, α, φ, 0, 0, 0)B (⃗y1, 0n, 0, 0, β, θ, 0)B∗
Game 0: This game is a original one where the challenger selects 0 as a random
bit. Namely, for all j = 1, · · · , q1 and all ℓ= 1, · · · , q2, the replies to the j-th
secret key query for (⃗y(j)
0 , ⃗y(j)
1 ) and the ℓ-th ciphertext query for (⃗x(ℓ)
0 , ⃗x(ℓ)
1 )
are
sk(j)
⃗y
:= (⃗y(j)
0 , 0n, 0, 0, β(j), θ(j), 0)B∗,
ct(ℓ)
⃗x
:= (⃗x(ℓ)
0 , 0n, α(ℓ), φ(ℓ), 0, 0, 0)B,

Eﬃcient Functional Encryption for Inner-Product Values
419
where β(j), θ(j), α(ℓ), φ(ℓ)
U
←−Zq.
Game 1-μ-1(μ = 1, · · · , q2): Game 1-0-3 is identical with Game 0. This game
is the same as Game 1-(μ-1)-3 except that the reply to the μ-th ciphertext
query for (⃗x(μ)
0 , ⃗x(μ)
1 ) is
ct(μ)
⃗x
:= (⃗x(μ)
0 , 0n, α(μ), φ(μ), 0, 0, τ )B,
where α(μ), φ(μ)
U
←−Zq and τ
U
←−Z×
q .
Game 1-μ-2(μ = 1, · · · , q2): This game is the same as Game 1-μ-1 except that
the reply to the μ-th ciphertext query for (⃗x(μ)
0 , ⃗x(μ)
1 ) is
ct(μ)
⃗x
:= (⃗x(μ)
0 , ⃗x(μ)
1
, α(μ), φ(μ), 0, 0, τ)B,
where α(μ), φ(μ)
U
←−Zq and τ
U
←−Z×
q .
Game 1-μ-3(μ = 1, · · · , q2): This game is the same as Game 1-μ-2 except that
the reply to the μ-th ciphertext query for (⃗x(μ)
0 , ⃗x(μ)
1 ) is
ct(μ)
⃗x
:= (⃗x(μ)
0 , ⃗x(μ)
1 , α(μ), φ(μ), 0, 0, 0 )B,
where α(μ), φ(μ)
U
←−Zq.
Game 2-ν-1(ν = 1, · · · , q1): Game 2-0-3 is identical with Game 1-q2-3. This
game is the same as Game 2-(ν-1)-3 except that the reply to the ν-th secret
key query for (⃗y(ν)
0 , ⃗y(ν)
1 ) is
sk(ν)
⃗y
:= (⃗y(ν)
0 , 0n, 0, 0, β(ν), θ(ν), η )B∗,
where β(ν), θ(ν)
U
←−Zq and η
U
←−Z×
q .
Game 2-ν-2(ν = 1, · · · , q1): This game is the same as Game 2-ν-1 except that
the reply to the ν-th secret key query for (⃗y(ν)
0 , ⃗y(ν)
1 ) is
sk(ν)
⃗y
:= ( 0n, ⃗y(ν)
1
, 0, 0, β(ν), θ(ν), η)B∗,
where β(ν), θ(ν)
U
←−Zq and η
U
←−Z×
q .
Game 2-ν-3(ν = 1, · · · , q1): This game is the same as Game 2-ν-2 except that
the reply to the ν-th secret key query for (⃗y(ν)
0 , ⃗y(ν)
1 ) is
sk(ν)
⃗y
:= (0n, ⃗y(ν)
1 , 0, 0, β(ν), θ(ν), 0 )B∗,
where β(ν), θ(ν)
U
←−Zq.

420
J. Tomida et al.
Game 3: This game is the same as Game 2-q2-3 except that, for all j = 1, · · · , q1
and all ℓ= 1, · · · , q2, the replies to the j-th secret key query for (⃗y(j)
0 , ⃗y(j)
1 )
and the ℓ-th ciphertext query for (⃗x(ℓ)
0 , ⃗x(ℓ)
1 ) are
sk(j)
⃗y
:= ( ⃗y(j)
1 , 0n , 0, 0, β(j), θ(j), 0)B∗,
ct(ℓ)
⃗x
:= ( ⃗x(ℓ)
1 , ⃗x(ℓ)
0
, α(ℓ), φ(ℓ), 0, 0, 0)B,
where β(j), θ(j), α(ℓ), φ(ℓ)
U
←−Zq.
Game 4: This game is the same as Game 3 except that, for all ℓ= 1, · · · , q2,
the replies to the ℓ-th ciphertext query for (⃗x(ℓ)
0 , ⃗x(ℓ)
1 ) are
ct(ℓ)
⃗x
:= (⃗x(ℓ)
1 , 0n , α(ℓ), φ(ℓ), 0, 0, 0)B,
where α(ℓ), φ(ℓ)
U
←−Zq. Note that this game is a original one where the
challenger selects 1 as a random bit.
Claim 1. For any PPT distinguisher B between Game 1-(μ-1)-3 and Game 1-
μ-1, there exists a PPT algorithm A for Problem 1, such that for any security
parameter λ,
Pr[ExpGame1−(μ−1)−3
B
(λ) →1] −Pr[ExpGame1−μ−1
B
(λ) →1]
 ≤AdvP1
A (λ).
Proof. We demonstrate that it is possible to construct a PPT algorithm A for
Problem 1 using any PPT distinguisher B between Game 1-(μ-1)-3 and Game
1-μ-1 as a blackbox. A takes a role to B as a challenger of the security game.
1. A is given a Problem 1 instance (paramV, 	B, 	B∗, gb).
2. A gives (1λ, paramV) to B as pp.
3. A computes sk(j)
⃗y
using 	B∗when B queries (⃗y(j)
0 , ⃗y(j)
1 ), and give it to B.
4. A computes ct(ℓ)
⃗x
when B queries (⃗x(ℓ)
0 , ⃗x(ℓ)
1 ) as
ct(ℓ)
⃗x
:=
n

i=1
x(ℓ)
0,ibi +
2n

i=n+1
x(ℓ)
1,i−nbi + α(ℓ)b2n+1 + φ(ℓ)b2n+2
if ℓ< μ,
ct(ℓ)
⃗x
:=
n

i=1
x(ℓ)
0,ibi + gb if ℓ= μ,
ct(ℓ)
⃗x
:=
n

i=1
x(ℓ)
0,ibi + α(ℓ)b2n+1 + φ(ℓ)b2n+2 if ℓ> μ,
where α(ℓ), φ(ℓ)
U
←−Zq, and give it to B.
5. If B outputs b′, A outputs b′ as it is.

Eﬃcient Functional Encryption for Inner-Product Values
421
It can be seen that if b = 0, B’s view is the same as that in Game 1-(μ-1)-3, and
if b = 1, B’s view is the same as that in Game 1-μ-1.
⊓⊔
Claim 2. For any PPT distinguisher B between Game 1-μ-1 and Game 1-μ-2,
Pr[ExpGame1−μ−1
B
(λ) →1] = Pr[ExpGame1−μ−2
B
(λ) →1].
Proof. We demonstrate that B’s view in Game 1-μ-1 is the same as that in Game
1-μ-2. For that purpose, we deﬁne new bases (F, F∗) on G2n+5 such that
f2n+5 := b2n+5 −
2n

i=n+1
x(μ)
1,i−n
τ
bi,
f ∗
i := b∗
i + x(μ)
1,i−n
τ
b∗
2n+5
for i = n + 1, · · · , 2n,
F := (b1, · · · , b2n+4, f2n+5), F∗:= (b∗
1, · · · , b∗
n, f ∗
n+1, · · · , f ∗
2n, b∗
2n+1, · · · , b∗
2n+5).
Observe that (F, F∗) are dual orthonormal bases and they are distributed com-
pletely at random. Then, the secret keys are
sk(j)
⃗y
:= (⃗y(j)
0 , 0n, 0, 0, β(j), θ(j), 0)B∗= (⃗y(j)
0 , 0n, 0, 0, β(j), θ(j), 0)F∗.
On the other hand, the ciphertexts are
ct(μ)
⃗x
= (⃗x(μ)
0 , 0n, α(μ), φ(μ), 0, 0, τ)B,
=
n

i=1
x(μ)
0,i bi + α(μ)b2n+1 + φ(μ)b2n+2 + τb2n+5
=
n

i=1
x(μ)
0,i bi + α(μ)b2n+1 + φ(μ)b2n+2 + τ

f2n+5 +
2n

i=n+1
x(μ)
1,i−n
τ
bi

=
n

i=1
x(μ)
0,i bi +
2n

i=n+1
x(μ)
1,i−nbi + α(μ)b2n+1 + φ(μ)b2n+2 + τf2n+5
= (⃗x(μ)
0 , ⃗x(μ)
1 , α(μ), φ(μ), 0, 0, τ)F,
ct(ℓ)
⃗x
= (⃗x(ℓ)
0 , ⃗x(ℓ)
1 , α(ℓ), φ(ℓ), 0, 0, 0)B = (⃗x(ℓ)
0 , ⃗x(ℓ)
1 , α(ℓ), φ(ℓ), 0, 0, 0)F if ℓ< μ,
ct(ℓ)
⃗x
= (⃗x(ℓ)
0 , 0n, α(ℓ), φ(ℓ), 0, 0, 0)B = (⃗x(ℓ)
0 , 0n, α(ℓ), φ(ℓ), 0, 0, 0)F if ℓ> μ.
Therefore, B’s view in both game is information-theoretically identical.
⊓⊔
Claim 3. For any PPT distinguisher B between Game 1-μ-2 and Game
1-μ-3, there exists a PPT algorithm A for Problem 1, such that for any security
parameter λ,
Pr[ExpGame1−μ−2
B
(λ) →1] −Pr[ExpGame1−μ−3
B
(λ) →1]
 ≤AdvP1
A (λ).
Proof. The proof of Claim 3 is the same as that of Claim 1 except that the second
equation in Step 4 is ct(ℓ)
⃗x
:= n
i=1 x(ℓ)
0,ibi + 2n
i=n+1 x(ℓ)
1,i−nbi + gb if ℓ= μ.
⊓⊔

422
J. Tomida et al.
Claim 4. For any PPT distinguisher B between Game 2-(ν-1)-3 and Game
2-ν-1, there exists a PPT algorithm A for Problem 2, such that for any security
parameter λ,
Pr[ExpGame2−(ν−1)−3
B
(λ) →1] −Pr[ExpGame2−ν−1
B
(λ) →1]
 ≤AdvP2
A (λ).
Proof. We demonstrate that it is possible to construct a PPT algorithm A for
Problem 2 using any PPT distinguisher B between Game 2-(ν-1)-3 and Game
2-ν-1 as a blackbox. A takes a role to B as a challenger of the security game.
1. A is given a Problem 2 instance (paramV, 	B, 	B∗, g∗
b).
2. A gives (1λ, paramV) to B as pp.
3. A computes sk(j)
⃗y
when B queries (⃗y(j)
0 , ⃗y(j)
1 ) as
sk(j)
⃗y
:=
2n

i=n+1
y(j)
1,i−nb∗
i + β(j)b∗
2n+3 + θ(j)b∗
2n+4 if j < ν,
sk(j)
⃗y
:=
n

i=1
y(j)
0,i b∗
i + g∗
b if j = ν,
sk(j)
⃗y
:=
n

i=1
y(j)
0,i b∗
i + β(j)b∗
2n+3 + θ(j)b∗
2n+4 if j > ν,
where β(j), θ(j)
U
←−Zq, and give it to B.
4. A computes ct(ℓ)
⃗x
of the Game 2 form using 	B when B queries (⃗x(ℓ)
0 , ⃗x(ℓ)
1 ), and
give it to B.
5. If B outputs b′, A outputs b′ as it is.
It can be seen that if b = 0, B’s view is the same as that in Game 2-(ν-1)-3, and
if b = 1, B’s view is the same as that in Game 2-ν-1.
⊓⊔
Claim 5. For any PPT distinguisher B between Game 2-ν-1 and Game 2-ν-2,
Pr[ExpGame2−ν−1
B
(λ) →1] = Pr[ExpGame2−ν−2
B
(λ) →1].
Proof. We demonstrate that B’s view in Game 2-ν-1 is the same as that in Game
2-ν-2. For that purpose, we deﬁne new bases (F, F∗) on G2n+5 such that
fi := bi −
y(ν)
0,i
η b2n+5
for i = 1, · · · , n,
fi := bi +
y(ν)
1,i−n
η
b2n+5
for i = n + 1, · · · , 2n,
f ∗
2n+5 := b∗
2n+5 +
n

i=1
y(ν)
0,i
η b∗
i −
2n

i=n+1
y(ν)
1,i−n
η
b∗
i ,
F := (f1, · · · , f2n, b2n+1, · · · , b2n+5), F∗:= (b∗
1, · · · , b∗
2n+4, f ∗
2n+5).

Eﬃcient Functional Encryption for Inner-Product Values
423
Observe that (F, F∗) are dual orthonormal bases and they are distributed com-
pletely at random. Then, the secret keys are
sk(ν)
⃗y
= (⃗y(ν)
0
, 0n, 0, 0, β(ν), θ(ν), η)B∗,
=
n

i=1
y(ν)
0,i b∗
i + β(ν)b∗
2n+3 + θ(ν)b∗
2n+4 + ηb∗
2n+5
=
n

i=1
y(ν)
0,i b∗
i + β(ν)b∗
2n+3 + θ(ν)b∗
2n+4 + η
⎛
⎝f∗
2n+5−
n

i=1
y(ν)
0,i
η
b∗
i +
2n

i=n+1
y(ν)
1,i−n
η
b∗
i
⎞
⎠
=
2n

i=n+1
y(ν)
1,i−nb∗
i + β(ν)b∗
2n+3 + θ(ν)b∗
2n+4 + ηf∗
2n+5
= (0n, ⃗y(ν)
1
, 0, 0, β(ν), θ(ν), η)F∗,
sk(j)
⃗y
= (0n, ⃗y(j)
1 , 0, 0, β(j), θ(j), 0)B∗= (0n, ⃗y(j)
1 , 0, 0, β(j), θ(j), 0)F∗if j < ν,
sk(j)
⃗y
= (⃗y(j)
0 , 0n, 0, 0, β(j), θ(j), 0)B∗= (⃗y(j)
0 , 0n, 0, 0, β(j), θ(j), 0)F∗if j > ν.
On the other hand, the ciphertexts are
ct(ℓ)
⃗x
= (⃗x(ℓ)
0 , ⃗x(ℓ)
1 , α(ℓ), φ(ℓ), 0, 0, 0)B
=
n

i=1
x(ℓ)
0,ibi +
2n

i=n+1
x(ℓ)
1,i−nbi + α(ℓ)b2n+1 + φ(ℓ)b2n+2
=
n

i=1
x(ℓ)
0,i
⎛
⎝fi+
y(ν)
0,i
η
b2n+5
⎞
⎠+
2n

i=n+1
x(ℓ)
1,i−n
⎛
⎝fi−
y(ν)
1,i−n
η
b2n+5
⎞
⎠+α(ℓ)b2n+1+φ(ℓ)b2n+2
=
n

i=1
x(ℓ)
0,ifi+
2n

i=n+1
x(ℓ)
1,i−nfi+⃗x(ℓ)
0
· ⃗y(ν)
0
−⃗x(ℓ)
1
· ⃗y(ν)
1
η
b2n+5+α(ℓ)b2n+1+φ(ℓ)b2n+2
=
n

i=1
x(ℓ)
0,ifi +
2n

i=n+1
x(ℓ)
1,i−nfi + α(ℓ)b2n+1 + φ(ℓ)b2n+2
=(⃗x(ℓ)
0 , ⃗x(ℓ)
1 , α(ℓ), φ(ℓ), 0, 0, 0)F,
because ⃗x(ℓ)
0
· ⃗y(j)
0
= ⃗x(ℓ)
1
· ⃗y(j)
1
for all j and ℓ. Therefore, B’s view in both game
is information-theoretically identical.
⊓⊔
Claim 6. For any PPT distinguisher B between Game 2-ν-2 and Game 2-ν-
3, there exists a PPT algorithm A for Problem 2, such that for any security
parameter λ,
Pr[ExpGame2−ν−2
B
(λ) →1] −Pr[ExpGame2−ν−3
B
(λ) →1]
 ≤AdvP2
A (λ).
Proof. The proof of Claim 6 is the same as that of Claim 4 except that the second
equation in Step 3 is sk(j)
⃗y
:= 2n
i=n+1 y(j)
1,i−1b∗
i + g∗
b if j = ν.
⊓⊔
Claim 7. For any PPT distinguisher B between Game 2-q1-3 and Game 3,
Pr[ExpGame2−q1−1
B
(λ) →1] = Pr[ExpGame3
B
(λ) →1].

424
J. Tomida et al.
Proof. We deﬁne new bases (F, F∗) on G2n+5 as
fi := bn+i,
f ∗
i := b∗
n+i,
fn+i := bi,
f ∗
n+i := b∗
i
for i = 1, · · · , n,
F := (f1, · · · , f2n, b2n+1, · · · , b2n+5), F∗:= (f ∗
1 , · · · , f ∗
2n, b∗
2n+1, · · · , b∗
2n+5).
These bases are dual orthonormal. Then we can easily see that
sk(j)
⃗y
= (0n, ⃗y(j)
1 , 0, 0, β(j), θ(j), 0)B∗= (⃗y(j)
1 , 0n, 0, 0, β(j), θ(j), 0)F∗,
ct(ℓ)
⃗x
= (⃗x(ℓ)
0 , ⃗x(ℓ)
1 , α(ℓ), φ(ℓ), 0, 0, 0)B = (⃗x(ℓ)
1 , ⃗x(ℓ)
0 , α(ℓ), φ(ℓ), 0, 0, 0)F.
This is just a conceptual change and B’s view in the both game is the same. ⊓⊔
Claim 8. For any PPT distinguisher B between Game 3 and Game 4,
Pr[ExpGame3
B
(λ) →1] −Pr[ExpGame4
B
(λ) →1]

=
Pr[ExpGame0
B
(λ) →1] −Pr[ExpGame1−q2−3
B
(λ) →1]

Proof. The diﬀerence between Game 3 and Game 4 is the same as that between
Game 0 and Game 1-q2-3, just switching ⃗x(ℓ)
0
with ⃗x(ℓ)
1 , and ⃗y(j)
0
with ⃗y(j)
1 .
⊓⊔
From Claims 1, · · · , 8 and Lemmas 1, 2 and 3, Theorem 1 holds.
⊓⊔
References
1. Abdalla, M., Bourse, F., De Caro, A., Pointcheval, D.: Simple functional encryption
schemes for inner products. In: Katz, J. (ed.) PKC 2015. LNCS, vol. 9020, pp. 733–
751. Springer, Heidelberg (2015)
2. Abdalla, M., Bourse, F., Caro, A.D., Pointcheval, D.: Better security for func-
tional encryption for inner product evaluations. Cryptology ePrint Archive, Report
2016/11 (2016)
3. Abe, M., Chase, M., David, B., Kohlweiss, M., Nishimaki, R., Ohkubo, M.:
Constant-size structure-preserving signatures: generic constructions and simple
assumptions. In: Wang, X., Sako, K. (eds.) ASIACRYPT 2012. LNCS, vol. 7658,
pp. 4–24. Springer, Heidelberg (2012)
4. Agrawal, S., Agrawal, S., Badrinarayanan, S., Kumarasubramanian, A., Prab-
hakaran, M., Sahai, A.: Function private functional encryption and property pre-
serving encryption: new deﬁnitions and positive results. Cryptology ePrint Archive,
Report 2013/744 (2013)
5. Agrawal, S., Libert, B., Stehle, D.: Fully secure functional encryption for inner
products, from standard assumptions. Cryptology ePrint Archive, Report 2015/608
(2015)
6. Bishop, A., Jain, A., Kowalczyk, L.: Function-hiding inner product encryption.
Cryptology ePrint Archive, Report 2015/672 (2015)
7. Boneh, D., Franklin, M.: Identity-based encryption from the weil pairing. In: Kilian,
J. (ed.) CRYPTO 2001. LNCS, vol. 2139, pp. 213–229. Springer, Heidelberg (2001)
8. Boneh, D., Sahai, A., Waters, B.: Functional encryption: deﬁnitions and challenges.
In: Ishai, Y. (ed.) TCC 2011. LNCS, vol. 6597, pp. 253–273. Springer, Heidelberg
(2011)

Eﬃcient Functional Encryption for Inner-Product Values
425
9. Brakerski, Z., Segev, G.: Function-private functional encryption in the private-key
setting. In: Dodis, Y., Nielsen, J.B. (eds.) TCC 2015, Part II. LNCS, vol. 9015, pp.
306–324. Springer, Heidelberg (2015)
10. Boneh, D., Raghunathan, A., Segev, G.: Function-private identity-based encryp-
tion: hiding the function in functional encryption. In: Canetti, R., Garay, J.A.
(eds.) CRYPTO 2013, Part II. LNCS, vol. 8043, pp. 461–478. Springer, Heidelberg
(2013)
11. Boneh, D., Raghunathan, A., Segev, G.: Function-private subspace-membership
encryption and its applications. In: Sako, K., Sarkar, P. (eds.) ASIACRYPT 2013,
Part I. LNCS, vol. 8269, pp. 255–275. Springer, Heidelberg (2013)
12. Datta, P., Dutta, R., Mukhopadhyay, S.: Functional encryption for inner product
with full function privacy. Cryptology ePrint Archive, Report 2015/1255 (2015)
13. Garg, S., Gentry, C., Halevi, S., Raykova, M., Sahai, A., Waters, B.: Candi-
date indistinguishability obfuscation and functional encryption for all circuits. In:
FOCS, pp. 40–49. IEEE (2013)
14. Garg, S., Gentry, C., Halevi, S., Zhandry, M.: Fully secure functional encryption
without obfuscation. Cryptology ePrint Archive, Report 2014/666 (2014)
15. Galbraith, S.D., Paterson, K.G., Smart, N.P.: Pairings for cryptographers. Discrete
Appl. Math. 156(16), 3113–3121 (2008)
16. Katz, J., Sahai, A., Waters, B.: Predicate encryption supporting disjunctions, poly-
nomial equations, and inner products. In: Smart, N.P. (ed.) EUROCRYPT 2008.
LNCS, vol. 4965, pp. 146–162. Springer, Heidelberg (2008)
17. Lewko, A., Okamoto, T., Sahai, A., Takashima, K., Waters, B.: Fully secure
functional encryption: attribute-based encryption and (hierarchical) inner prod-
uct encryption. In: Gilbert, H. (ed.) EUROCRYPT 2010. LNCS, vol. 6110, pp.
62–91. Springer, Heidelberg (2010)
18. Okamoto, T., Takashima, K.: Hierarchical predicate encryption for inner-products.
In: Matsui, M. (ed.) ASIACRYPT 2009. LNCS, vol. 5912, pp. 214–231. Springer,
Heidelberg (2009)
19. Okamoto, T., Takashima, K.: Fully secure functional encryption with general rela-
tions from the decisional linear assumption. In: Rabin, T. (ed.) CRYPTO 2010.
LNCS, vol. 6223, pp. 191–208. Springer, Heidelberg (2010)
20. Okamoto, T., Takashima, K.: Adaptively attribute-hiding (hierarchical) inner
product encryption. In: Pointcheval, D., Johansson, T. (eds.) EUROCRYPT 2012.
LNCS, vol. 7237, pp. 591–608. Springer, Heidelberg (2012)
21. ONeill, A.: Deﬁnitional issues in functional encryption. Cryptology ePrint Archive,
Report 2010/556 (2010)
22. Ostrovsky, R., Sahai, A., Waters, B.: Attribute-based encryption with non-
monotonic access structures. In: ACM Conference on Computer and Communi-
cations Security, pp. 195–203. ACM (2007)

MQSAS - A Multivariate Sequential Aggregate
Signature Scheme
Rachid El Bansarkhani1, Mohamed Saied Emam Mohamed1(B),
and Albrecht Petzoldt2
1 Technische Universit¨at Darmstadt, Darmstadt, Germany
{elbansarkhani,mohamed}@cdc.informatik.tu-darmstadt.de
2 Kyushu University, Fukuoka, Japan
petzoldt@imi.kyushu-u.ac.jp
Abstract. (Sequential) Aggregate signature schemes enable a group of
users u1, . . . , uk with messages m1, . . . , mk to produce a single signa-
ture Σ which states the integrity and authenticity of all the messages
m1, . . . , mk. The length of the signature Σ is thereby signiﬁcantly shorter
than a concatenation of individual signatures. Therefore, aggregate sig-
natures can improve the eﬃciency of numerous applications, e.g. the
BGPsec protocol of Internet routing and the development of new eﬃ-
cient aggregate signature schemes is an important task for cryptographic
research. On the other hand, most of the existing schemes for aggregate
signatures are based on number theoretic problems and therefore become
insecure as soon as large enough quantum computers come into existence.
In this paper, we propose a technique to extend multivariate signature
schemes such as HFEv- to sequential aggregate signature schemes. By
doing so, we create the ﬁrst multivariate signature scheme of this kind,
which is, at the same time, also one of the ﬁrst post-quantum aggregate
signature schemes. Our scheme is very eﬃcient and oﬀers compression
rates that outperform current lattice-based constructions for practical
parameters.
Keywords: Sequential aggregate signatures · Multivariate cryptogra-
phy · HFEv-
1
Introduction
(Sequential) aggregate signature schemes enable a group of users U
=
{u1, . . . , uk}, each of them having a message mi to be signed, to generate a sin-
gle signature Σ which proofs the integrity and authenticity of all the messages
m1, . . . , mk. The key point hereby is that the size of the aggregate signature
Σ is much smaller than a concatenation of the individual signatures. There-
fore, (sequential) aggregate signature schemes have a great deal of applications
such as the BGPsec protocol [7], which plays an important role in securing the
global Internet routing system. In this protocol, each node on a certain path of
n hops receives n certiﬁcates and the same amount of signatures. It then veriﬁes
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 426–439, 2016.
DOI: 10.1007/978-3-319-45871-7 25

MQSAS - A Multivariate Sequential Aggregate Signature Scheme
427
the signatures, creates its own signature attesting for this path and sends this
result together with the previous signatures to the next hop. As a consequence,
the number of certiﬁcates and signatures increases linearly with the number of
nodes on this path. This amount of bandwidth cost can be reduced drastically
by the use of a sequential aggregate signature scheme. Similar ideas can in gen-
eral be applied to public key infrastructures of any depth requiring chains of
certiﬁcates and signatures in order to authenticate public keys at the leafs. Such
schemes come always into use, when chains and paths need to be authenticated
as a condition for the protocol to work.
The currently available solutions for (sequential) aggregate signature schemes
are mainly based on classical cryptosystems such as RSA and ECC. However,
these schemes will become insecure as soon as large quantum computers arrive.
The reason for this is Shor’s algorithm [11], which solves number theoretic prob-
lems like integer factorization and discrete logarithm in polynomial time on
a quantum computer. It is therefore an important task to develop (sequential)
aggregate signature schemes whose security is based on hard mathematical prob-
lems not aﬀected by quantum computer attacks (so called post-quantum cryp-
tosystems). Besides lattice, code and hash based cryptosystems, multivariate
cryptography is one of the main candidates for this [1].
In this paper we show how to extend multivariate signature schemes to
sequential aggregate signature schemes. While our technique can be applied to
arbitrary multivariate schemes, we mainly concentrate on the HFEv- signature
scheme, which is one of the best known and most studied multivariate schemes.
Our scheme is the ﬁrst multivariate and one of the ﬁrst post-quantum (sequen-
tial) aggregate signature schemes and enables high compression rates and there-
fore very short sizes of the aggregate signature. Furthermore, with regard to its
performance, our scheme outperforms current lattice-based constructions [3].
The rest of this paper is organized as follows. In Sect. 2 we repeat the basic
concepts of (sequential) aggregate signatures. Section 3 gives an overview of the
area of multivariate cryptography and introduces the HFEv- signature scheme,
which is the basis of our construction. In Sect. 4 we then present our technique to
extend HFEv- to a multivariate sequential aggregate signature scheme. whereas
Sect. 5 deals with the security of our construction. Section 6 gives concrete para-
meter sets for our scheme and compares it with other existing (sequential) aggre-
gate signature schemes. Finally, Sect. 7 concludes the paper.
2
Sequential Aggregate Signatures
In this section we describe the concept of (sequential) aggregate signature
schemes. Let U = {u1, . . . , uk} be a set of users participating in the protocol,
each of them having a message mi to be signed as well as a key pair (ski, pki) of
a digital signature scheme. Each user ui applies his private key ski to generate a
signature σi for a message mi (the messages mi are not necessarily distinct). Let
us assume that the k users ui (i = 1, . . . , k) desire to prove to a single veriﬁer V
that each user ui indeed signed his message mi. This could be accomplished by

428
R. El Bansarkhani et al.
sending all the messages mi (i = 1, . . . , k) and a signature ˜σ = (σ1, . . . , σk) to
the veriﬁer V . However, by doing so, the length of ˜σ to be transmitted becomes
very large if the number k of users increases.
An alternative way of proving the integrity and authenticity of the messages
m1, . . . , mk is to combine all the signatures σi to a single signature Σ with
|Σ| ≪|˜σ|. To achieve this, one can use a (sequential) aggregate signature scheme.
The signature generation process of a sequential aggregate signature scheme
is an iterative process between the users u1, . . . , uk (see Fig. 1).
u1
m1, sk1 →Σ1 = σ1
Σ1
u2
m2, sk2 →σ2
Σ1, σ2 →Σ2
Σ2
u3
m3, sk3 →σ3
Σ2, σ3 →Σ3
Σ3
. . .
Σk−1 uk
mk, skk →σk
Σk−1, σk →Σ = Σk
Fig. 1. Generation of a sequential aggregate signature
The ﬁrst signer generates a standard signature σ1 for his message m1, while
the second signer generates a signature on the combination of his message m2
and σ1 to obtain an aggregate signature Σ2 for both the messages m1 and m2.
Hereby, the goal is to hide a big portion of σ1 in Σ2 in such a way that σ1 can be
recovered during the veriﬁcation. By doing so, we ensure that it is still possible
to validate the message m1. This step is repeated for the signers u3, . . . , uk. The
last signer produces the ﬁnal signature Σ = Σk, which is now an aggregate
signature for all the messages m1, . . . , mk.
Compression Rate. Let |σi| be the size of an individual signature σi (i =
1, . . . , k) and |Σ| be the size of the (sequential) aggregate signature Σ. Following
[3], we deﬁne the compression rate of the aggregate signature scheme by
τ(k) = 1 −
|Σ|
k
i=1 |σi|
.
(1)
The size ratio τ expresses therefore the amount of memory that has been saved
due to the use of the aggregate signature scheme. A value of τ = 0 corresponds
to an aggregate signature Σ which is as long as the concatenation of all the
individual signatures (i.e. no compression at all). A value of τ = 1 −1
k expresses
that the aggregate signature Σ has the size of an individual signature, which
corresponds to an optimal aggregate signature scheme.
3
The HFEv- Signature Scheme
In this section we review the HFEv- signature scheme, which is the basis of our
construction. Before we give a detailed description of the scheme itself, we start
with a short overview of the basic concepts of multivariate cryptography.

MQSAS - A Multivariate Sequential Aggregate Signature Scheme
429
3.1
Multivariate Cryptography
The basic objects of multivariate cryptography are systems of multivariate
quadratic polynomials (see Eq. (2)).
p(1)(x1, . . . , xn) =
n

i=1
n

j=i
p(1)
ij · xixj +
n

i=1
p(1)
i
· xi + p(1)
0
p(2)(x1, . . . , xn) =
n

i=1
n

j=i
p(2)
ij · xixj +
n

i=1
p(2)
i
· xi + p(2)
0
...
p(m)(x1, . . . , xn) =
n

i=1
n

j=i
p(m)
ij
· xixj +
n

i=1
p(m)
i
· xi + p(m)
0
(2)
The security of multivariate schemes is based on the MQ problem.
Deﬁnition 1 (MQ Problem). Given m multivariate quadratic polynomials
p(1)(x), . . . , p(m)(x) in n variables x1, . . . , xn as shown in Eq. (2), ﬁnd a vector
¯x = (¯x1, . . . , ¯xn) such that p(1)(¯x) = . . . = p(m)(¯x) = 0.
The MQ problem (for m ≈n) is proven to be NP-hard even for quadratic
polynomials over the ﬁeld GF(2) [6].
To build a public key cryptosystem based on the MQ problem, one starts
with an easily invertible quadratic map F : Fn →Fm (central map). To hide
the structure of F in the public key, one composes it with two invertible aﬃne
(or linear) maps S : Fm →Fm and T : Fn →Fn. The public key is therefore
given by P = S ◦F ◦T . The private key consists of S, F and T and therefore
allows to invert the public key. We note that, due to the above construction, the
security of multivariate schemes is not only based on the MQ-Problem but also
on the EIP-Problem (“Extended Isomorphism of Polynomials”) of ﬁnding the
composition of P.
In this paper we focus on multivariate signature schemes of the BigField
family. For this type of multivariate schemes, the map F is a specially chosen
and easily invertible map over a degree n extension ﬁeld E of F. One uses an
isomorphism Φ : Fn →E to transform F into a quadratic map
¯F = Φ−1 ◦F ◦Φ
(3)
from Fn to itself. The public key of the scheme is therefore given by
P = S ◦¯F ◦T = S ◦Φ−1 ◦F ◦Φ ◦T : Fn →Fn.
(4)
with two invertible aﬃne maps S, T : Fn →Fn.
The standard signature generation and veriﬁcation process of a multivariate
BigField scheme works as shown in Fig. 2.
Signature Generation: To generate a signature for a message h ∈Fn, one com-
putes recursively x = S−1(h) ∈Fn, X = Φ(x) ∈E, Y = F−1(X) ∈E,

430
R. El Bansarkhani et al.
Signature Generation
h ∈Fn
x ∈Fn
y ∈Fn
z ∈Fn
P
S−1
¯F −1
T −1
Signature Veriﬁcation
X ∈E
Y ∈E
F −1
Φ
Φ−1
Fig. 2. General workﬂow of multivariate BigField signature schemes
y = Φ−1(Y ) ∈Fn and z = T −1(y). The signature of the message h is given
by z ∈Fn.
Veriﬁcation: To check the authenticity of a signature z ∈Fn, one simply com-
putes h′ = P(z) ∈Fn. If h′ = h holds, the signature is accepted, otherwise
rejected.
3.2
HFEv-
A famous example for a multivariate signature scheme from the BigField fam-
ily is the HFEv- signature scheme [8] which uses, additionally to the BigField
structure, the Minus and the Vinegar modiﬁcation for multivariate schemes. The
scheme can be described as follows. Let F = Fq be a ﬁnite ﬁeld with q elements
and E be a degree n extension ﬁeld of F. Furthermore, we choose integers D, a
and v. Let Φ be the canonical isomorphism between Fn and E, i.e.
Φ(x1, . . . , xn) =
n

i=1
xi · Xi−1.
(5)
The central map F of the HFEv- scheme is a map from E × Fv to E of the form
F(X) =
qi+qj≤D

0≤i≤j
αij · Xqi+qj +
qi≤D

i=0
βi(v1, . . . , vv) · Xqi + γ(v1, . . . , vv),
(6)
with αij ∈E, βi : Fv →E being linear and γ : Fv →E being a quadratic
function.
Due to the special form of F, the map ¯F = Φ−1 ◦F ◦(Φ × idv) is a quadratic
polynomial map from Fn+v to Fn. To hide the structure of ¯F in the public
key, one composes it with two aﬃne (or linear) maps S : Fn →Fn−a and
T : Fn+v →Fn+v of maximal rank.
The public key of the scheme is the composed map P = S ◦¯F ◦T : Fn+v →
Fn−a, the private key consists of S, F and T .

MQSAS - A Multivariate Sequential Aggregate Signature Scheme
431
Signature Generation: To generate a signature for a message h ∈Fn−a, the
signer performs the following three steps.
1. Compute a pre-image x ∈Fn of h under the aﬃne map S.
2. Lift x to the extension ﬁeld E (using the isomorphism Φ). Denote the result
by X.
Choose random values for the vinegar variables v1, . . . , vv ∈F and compute
FV = F(v1, . . . , vv).
Solve the univariate polynomial equation FV (Y ) = X by Berlekamp’s algo-
rithm and compute y′ = Φ−1(Y ) ∈Fn.
Set y = (y′||v1|| . . . ||vv).
3. Compute the signature z ∈Fn+v by z = T −1(y).
Signature Veriﬁcation: To check, if z ∈Fn+v is indeed a valid signature for a
message h ∈Fn−a, one simply computes h′ = P(z) ∈Fn−a. If h′ = h holds, the
signature is accepted, otherwise rejected.
3.3
Gui
Recently, Petzoldt et al. proposed the multivariate signature scheme Gui [9],
which is based on the concept of HFEv-. In fact, the private and public keys of
Gui are just HFEv- keys over the ﬁeld GF(2) with specially chosen parameters
n, D, a and v. Since the number of equations in the public key and therefore
the input size of Gui is only 90 bits, it would be possible for an attacker to
ﬁnd two messages m1 and m2 whose hash values collide in these ﬁrst 90 bits.
To overcome this problem, the authors of [9] developed a specially designed
signature generation process. For this, they used a special parameter l (denoted
as repetition factor). The signature generation process of Gui works as shown
in Algorithm 1. Roughly spoken, one computes HFEv- signatures for l diﬀerent
hash values of the message d and combines them to a single signature of size
(n−a)+l·(a+v). Similarly, the veriﬁcation algorithm (see Algorithm 2) evaluates
the public key l times.
4
Our Sequential Aggregate Signature Scheme
In the design of our multivariate sequential aggregate signature scheme we apply
the HFEv-/Gui [9] trapdoor functions, which results in a scheme resembling
these basic components. However, while in Gui all the partial signatures are
computed using the same private key, we use here for every l-th partial signature
another key.

432
R. El Bansarkhani et al.
Algorithm 1. Signature Generation
Process of Gui
Input: HFEv- private key (S, F, T ) mes-
sage d, repetition factor l
Output: signature σ ∈F(n−a)+l·(a+v)
2
h ←SHA-256(d)
S0 ←0 ∈Fn−a
2
for i = 1 to l do
Di ←ﬁrst n −a bits of h
(Si, Xi) ←HFEv−−1(Di ⊕Si−1)
h ←SHA-256(h)
end for
σ ←(Sl||Xl|| . . . ||X1)
return σ
Algorithm 2. Signature Veriﬁcation
Process of Gui
Input: HFEv- public key P, message
d, repetition factor l, signature σ ∈
F(n−a)+l·(a+v)
2
Output: TRUE or FALSE
h ←SHA-256(d)
(Sl, Xl, . . . , X1) ←σ
for i = 1 to l do
Di ←ﬁrst n −a bits of h
h ←SHA-256(h)
end for
for i = l −1 to 0 do
Si ←P(Si+1||Xi+1) ⊕Di+1
end for
if S0 = 0 then
return TRUE
else
return FALSE
end if
4.1
Key Generation
Let F = Fq be a ﬁnite ﬁeld with q elements, n, D, a, v, l ∈N be public parameters
and U = {u1, . . . , uk} be a set of users. Every user ui ∈U generates an HFEv-
key pair ((Si, Fi, Ti), Pi) according to the given parameter set. Additionally, he
computes a public key identity idi = H(Pi) using a hash function H modeled as
a random oracle. He publishes his public key Pi and his public key identity idi
while keeping Si, Fi and Ti secret.
The reason for introducing public key identities in our scheme is the fact,
that the public keys serve as input to the hash function multiple times. For large
public keys it is therefore more eﬃcient to utilize public identities instead of the
public keys itself, by which we can reduce the input size of the hash functions.
This results in faster signature generation engines.
4.2
Signature Generation
Assume that each user ui has a message mi to be signed. To generate an aggre-
gate signature Σ for the messages m1, . . . , mk, every signer ui (i = 1, . . . , k)
performs successively Algorithm 3 (using the output of signer ui−1 as his input).
The ﬁnal aggregate signature Σ is given by the output of the signer uk.
The ﬁrst signer u1 just computes a standard HFEv- (Gui) signature for the
message m1 (using Algorithm 1) and returns it as the ﬁrst aggregate signature
Σ1.
In addition to his own private key and message, each signer ui (i ∈{2, . . . , k})
requires as input lists of the public keys {pk1, . . . , pki−1}, the public key iden-
tities {id1, . . . , idi−1}, and the messages m1, . . . , mi−1 of the previous signers

MQSAS - A Multivariate Sequential Aggregate Signature Scheme
433
u1, . . . , ui−1 and the (i −1)-th sequential aggregate signature Σi−1. Before com-
puting his own signature σi and combining it with Σi−1, ui checks the correctness
of Σi−1 via the veriﬁcation algorithm (see Algorithm 4).
In order to generate the aggregate signature Σi, the signer ui (i = 2, . . . , k)
splits up the input aggregate signature Σi−1 into two blocks ˜S, ˜X, where ˜S has
a length of n−a bits. He uses his own private key ski to compute a standard Gui
signature σi = (Sl, Xl, . . . , X1) for the hash value h = H(m1, . . . , mi, id1, . . . , idi)
by running Algorithm 1. After that, he combines his signature (Sl, Xl, . . . , X1)
with the previous aggregate signature (Σi−1) to generate the new aggregate
signature Σi. Algorithm 3 shows this process in algorithmic form.
Algorithm 3. Signature Generation Process of MQSAS for each user i ∈
1, · · · , k
Input: private key ski, message mi , public keys pk1, . . . , pki−1, public key identities
id1, . . . , idi−1, messages m1, · · · , mi−1, aggregate signature Σi−1, where Σ0 = ∅,
repetition factor l
Output: aggregate signature Σi
1: if i = 1 then
2:
˜S = 0n−a
3:
˜
X ←∅
4: else if AggVerify(i −1, Σi−1, pk1, · · · , pki−1, m1, · · · , mi−1) =TRUE then
5:
( ˜S, ˜
X) ←split(Σi−1)
6: else
7:
print(′′IncorrectSignature′′)
8:
return
9: end if
10: h ←H(m1, . . . , mi, id1, · · · , idi)
11: for j = 1 to l do
12:
Dj ←ﬁrst n −a bits of h
13:
(Sj, Xj) ←HFEV−−1(Dj ⊕Sj−1)
14:
h ←H(h)
15: end for
16: ˜S ←˜S ⊕Sl
17:
˜
X ←(Xl|| . . . ||X1|| ˜
X)
18: Σi ←( ˜S, ˜
X)
19: return Σi
4.3
Signature Veriﬁcation
To check the authenticity of an aggregate signature Σi (Algorithm 4), we parse
Σi into the sequence of blocks ˜S, Xi·l, . . . , X(i−1)·l+1, X(i−1)·l, . . . , X1. Here, the
length of the block ˜S is n −a, while all the other blocks are of length a + v.
After this, the veriﬁcation of the aggregate signature Σi works very similar to
the veriﬁcation of a Gui signature.

434
R. El Bansarkhani et al.
In the j-th iteration, the algorithm ﬁrst reconstructs the hash values
D1, . . . , Dl used during the generation of the j-th partial signature. Just as in
Algorithm 2 it then evaluates the public key pkj l times to compute the new
value of ˜S. At termination, the aggregate signature Σi is accepted, if and only
if ˜S = 0 holds.
Algorithm 4. Veriﬁcation Process of MQSAS
Input: public keys pk1, . . . , pki, public key identities id1, . . . , idi, messages m1, . . . , mi,
repetition factor l and aggregate signature Σi
Output: boolean value TRUE or FALSE
1: ( ˜S, Xi·l, . . . , X(i−1)·l+1, X(i−1)·l, . . . , X1) ←split(Σi)
2: for j = i to 1 do
3:
h ←H(m1, . . . , mj, id1, · · · , idj)
4:
for k = 1 to l do
5:
Dk ←ﬁrst n −a bits of h
6:
h ←H(h)
7:
end for
8:
for k = l −1 to 0 do
9:
˜S ←pkj( ˜S||X(j−1)·l+k+1) ⊕Dk+1
10:
end for
11: end for
12: if ˜S = 0 then
13:
return TRUE
14: else
15:
return FALSE
16: end if
The Algorithms 3 and 4 show how to instantiate the MQSAS signature
scheme with HFEv-/Gui. However we note that our multivariate sequential
aggregate signature scheme can also be instantiated on the basis of every other
multivariate signature scheme such as Rainbow. Nevertheless, since HFEv-/Gui
leads to optimal compression rates, we restrict here to initializing our scheme
with HFEv-/Gui.
5
Security
The security analysis of our scheme is done in two steps. First we show that
an attack against our sequential aggregate signature scheme penetrates the
(trapdoor-) onewayness of the underlying HFEv- scheme. In the second step
we then analyze the practical security of this scheme. However, due to lack of
space, we can not present all the details of this analysis here and therefore refer
for this to the extended version of our paper [2].

MQSAS - A Multivariate Sequential Aggregate Signature Scheme
435
Theorem 1. Assuming the (t′, ϵ′)-one-wayness of the public key of HFEv-,
the sequential aggregate signature scheme presented in Sect. 4 is (t, qH, qS, n, ϵ)-
secure against existential forgery under adaptive sequential aggregate chosen-
message attack such that
(qS + qH + 1) · ϵ′ ≥ϵ and t ≤t′ −(4kqH + 4kqS + 7k −1)
for all t and ϵ.
Proof. see [2].
The practical security of HFEv- has been studied in several papers [5,9].
Additionally to this, we performed a number of experiments with the direct
attack against HFEv- instances which showed that the complexity of this attack
against the MQSAS instances proposed in the next section is beyond the claimed
levels of security. The details of these experiments can be found in the extended
version of this paper [2].
6
Parameters and Comparison
In this section we propose concrete parameter sets for the MQSAS scheme and
compare its compression capabilities and performance to that of other (sequen-
tial) aggregate signature schemes. In particular, we propose 5 parameter sets
for 80-bit security and 2 parameter sets for 120-bit security, allowing a trade
oﬀbetween compression rate and performance (see Table 1). The parameters
shown in the table are chosen in such a way that the complexity of a direct
attack against the scheme is beyond the claimed level of security.
Table 1. Proposed parameters and resulting key sizes for the MQSAS scheme
Security level
(bit)
MQSAS (F, n, D, a, v, l)
Public key
size (kB)
Private key
size (kB)
|σ| (bit) (20
signers)
Compression
factor τ(20)
80
(GF(2),96,5,6,6,2)
57.7
2.4
570
0.75
(GF(2),95,9,5,5,2)
55.5
2.3
490
0.78
(GF(2),94,17,4,4,2)
53.3
2.3
410
0.81
(GF(2),96,65,2,2,2)
55.7
2.3
254
0.88
(GF(7),62,8,2,2,1)
47.1
2.9
420
0.89
120
(GF(2),127,9,4,6,2)
133.8
4.1
523
0.81
(GF(7),93,8,3,3,1)
156.7
6.4
630
0.90
When instantiating our scheme over GF(2), we need, due to the threat of
birthday attacks, to choose the repetition factor l of the Gui scheme to be ≥2.
However, this has negative consequences for the compression capabilities of our
scheme. To avoid this, we also propose parameters for the MQSAS scheme over
GF(7). We can eﬃciently store 14 bits in 5 GF(7)-elements, while an element

436
R. El Bansarkhani et al.
of GF(7) is stored in 3 bits. By doing so, we need 60 GF(7)-elements to store a
hash value of 160 bits (80-bit security), while 90 GF(7) elements are needed to
store a hash value of 240 bits (120-bit security).
Note that the key sizes listed in Table 1 are those of a single signer ui. The
verifyer of the aggregate signature Σ = Σk is faced with a public key of size k
times the value listed in the table.
The size of a sequential aggregate signature of our scheme is given by
|Σ| = (n −a) + k · l · (a + v),
(7)
the compression rate τ is given by
τ = 1 −
|Σ|
k · |σ| = 1 −1
k ·

1 + (k −1) · l · (a + v)
(n −a) + l · (a + v)

.
(8)
Table 2 and Fig. 3 show the signature sizes and the compression rates τ for
the parameter sets listed in Table 1 and diﬀerent numbers of users.
6.1
Implementation
To estimate the performance of MQSAS, we created an implementation of our
scheme in C. For the implementation of the underlying HFEv- scheme we thereby
adapted the implementation of Gui [9] to our setting. Table 3 shows the com-
putation times needed to generate and verify an aggregate signature for diﬀer-
ent number of users and parameter sets. The experiments were run on a PC
with a Core-i5 3750k processor (Ivy Bridge) at 2.4 GHz and with 16 GB of
RAM. The timings in the table are the average values of 500 signature genera-
tion/veriﬁcation processes.
6.2
Discussion
Table 1 indicates that we achieve very short aggregate signatures and high com-
pression rates at security levels of both 80-bit and 120-bit. For example, for 80-bit
security, our scheme allows to generate an aggregate signature for 20 signers of
Table 2. Signature sizes and compression rates of the MQ-SAS scheme
MQ-SAS
5 signers
10 signers
20 signers
50 signers
100 signers
(F, n, D, a, v, l)
|Σ| (bit)
τ
|Σ| (bit)
τ
|Σ| (bit)
τ
|Σ| (bit)
τ
|Σ| (bit)
τ
(GF(2),96,5,6,6,2)
210
0.63
330
0.71
570
0.75
1,290
0.77
2,490
0.78
(GF(2),95,9,5,5,2)
190
0.65
290
0.74
490
0.78
1,090
0.80
2,090
0.81
(GF(2),94,17,4,4,2)
170
0.68
250
0.76
410
0.81
890
0.83
1,690
0.84
(GF(2),96,65,2,2,2)
134
0.73
174
0.83
254
0.88
494
0.90
894
0.91
(GF(7),62,8,2,2,1)
240
0.75
300
0.84
420
0.89
780
0.92
1,380
0.93
(GF(2),127,9,4,6,2)
223
0.69
323
0.77
523
0.82
1,123
0.84
2,123
0.85
(GF(7),93,8,3,3,1)
360
0.75
450
0.84
630
0.89
1,170
0.92
2,070
0.93

MQSAS - A Multivariate Sequential Aggregate Signature Scheme
437
Fig. 3. Compression rate τ of the MQ-SAS scheme
Table 3. Signature generation/veriﬁcation time of MQSAS
MQ-SAS
Signature generation/veriﬁcation time (ms)
(F, n, D, a, v, l)
5 signers
10 signers
20 signers
50 signers
100 signers
(GF(2),96,5,6,6,2)
1.24/0.127
3.13/0.206
6.11/0.339
27.86/0.644
82.91/1.292
(GF(2),95,9,5,5,2)
2.94/0.119
6.75/0.182
8.52/0.228
36.31/0.672
94.25/1.328
(GF(2),94,17,4,4,2)
9.30/0.086
17.19/0.137
32.32/0.242
68.18/0.638
158.01/1.267
(GF(2),127,9,4,6,2)
3.42/0.169
5.23/0.214
11.61/0.439
45.3/0.983
141.4/1.969
(GF(2),96,65,2,2,2)
150.86/0.051
302.23/0.117
599.24/0.231
1,509.69/0.590
3,053.25/1.185
length only 254 bits, which is less than one quarter of a single RSA signature at
the same security level. Remarkably, the compression rates for 120-bit security
are even higher than in the 80-bit case. On the other hand, the key sizes are
considerably larger for the corresponding parameter sets.
Table 2 shows the aggregate signature sizes and associated compression rates
for diﬀerent parameter sets and number of users. The highest compression rates
can hereby be achieved by the two schemes MQSAS(GF(7), 62, 8, 2, 2, 1) and
MQSAS(GF(7), 93, 8, 3, 3, 1). The reason for this is that, for these parameter
sets, we can choose the repetition factor of Gui to be 1. As the table shows,
these parameter sets allow compression rates of up to 93 %, which means that
we need only the size of 7 individual signatures to prove the validity of an
aggregate signature for 100 signers (see also Fig. 3).
In Table 3 we provide the timings for the signing and veriﬁcation engine of
our scheme. In fact, we note, that each signing step by construction invokes
the veriﬁcation engine in order to check the validity of the previous aggregate
signature, before the signer is able to proceed. The timings indicate that the

438
R. El Bansarkhani et al.
parameter set (GF(2), 96, 65, 2, 2, 2), due to the high degree of the HFE poly-
nomial in use, is much slower than the remaining ones. We therefore observe a
trade oﬀbetween the compression rate and the performance of the scheme.
6.3
Comparison to Other Aggregate Signature Schemes
In this subsection we compare our sequential aggregate signature scheme with
other constructions. In fact, we observe that multivariate-based sequential aggre-
gate signature schemes are more suitable for practice than their counterparts
from classical and lattice-based cryptography. In terms of signature size and per-
formance, HFEv- has been shown to be far more eﬃcient than the other schemes.
The size of an individual signature is only slightly more than one hundred bits,
whereas the underlying signature schemes of the other sequential aggregate sig-
nature schemes produce signatures of size more than 1,000 bits.
With regard to the performance, the timings of our scheme for signing and
veriﬁcation are also signiﬁcantly better than those of the other schemes. We have
shown that the overhead, that our sequential aggregate signature scheme entails,
can be at least as low as 7 bits per signature for a reasonable level of security.
Hence, almost the whole signature of a signer is concealed within the signature of
its successor. Furthermore, the arithmetic operations of our scheme are mainly
performed over the ﬁeld GF(2), which is well studied and thus allows to carry
out fast operations which furthermore can be accelerated by the use of special
processor instructions such as Pclmulqdq [9]. On the other hand, lattice-based
systems work over Zn
q , which implies to carry out more complex arithmetic
operations such as reductions modulo q for at least n ≥256 components.
Compared to RSA-based sequential aggregate signature schemes, our scheme
is much easier to instantiate. The reason for this is that, in the case of RSA,
the domains of the participating signers diﬀer (Since every signer has a diﬀerent
public modulus N, the possible range of the hash values can be quite diﬀerent).
Therefore, it is important to agree on how to choose the hash functions before-
hand, which leads to a signiﬁcant communication overhead. Furthermore, the
operations of RSA are more complex and hence lead to a less eﬃcient scheme.
This has also been observed in [3].
Due to the short signatures and simple operations, our scheme is also a
much better candidate than RSA for an aggregate signature scheme on restricted
devices (e.g. sensor networks).
7
Conclusion
In this paper we proposed a multivariate sequential aggregate signature scheme
on the basis of the HFEv- signature scheme, which is the ﬁrst multivariate and
one of the ﬁrst post-quantum schemes of this kind. Due to the use of HFEv-/
Gui as the underlying signature scheme, the resulting signatures are very short
(less than 1 kbit for 100 signers at 80 bits of security) and we achieve high

MQSAS - A Multivariate Sequential Aggregate Signature Scheme
439
compression rates (up to 93 % for k = 100 signers). Furthermore, due to the eﬃ-
ciency of arithmetic operations over GF(2), our scheme outperforms all current
(sequential) aggregate signature schemes in terms of performance.
Acknowledgments. We thank the anonymous reviewers of ISC for their comments
which helped to improve this paper. The third author is supported by JSPS KAKENHI
15F15350.
References
1. Bernstein, D.J., Buchmann, J., Dahmen, E. (eds.): Post Quantum Cryptography.
Springer, Heidelberg (2009)
2. El Bansarkhani, R., Mohamed, M.S.E., Petzoldt, A.: MQSAS - a multivariate
sequential aggregate signature scheme - Extended Versions. IACR eprint 2016/503
(2016)
3. El Bansarkhani, R., Buchmann, J.: Towards lattice based aggregate signatures.
In: Pointcheval, D., Vergnaud, D. (eds.) AFRICACRYPT. LNCS, vol. 8469, pp.
336–355. Springer, Heidelberg (2014)
4. Ding, J., Gower, J.E., Schmidt, D.S.: Multivariate Public Key Cryptosystems.
Springer, Heidelberg (2006)
5. Ding, J., Yang, B.-Y.: Degree of regularity for HFEv and HFEv-. In: Gaborit, P.
(ed.) PQCrypto 2013. LNCS, vol. 7932, pp. 52–66. Springer, Heidelberg (2013)
6. Garey, M.R., Johnson, D.S.: Computers and Intractability: A Guide to the Theory
of NP-Completeness. W.H. Freeman and Company, Paris (1979)
7. Network Working Group: A Border Gateway Protocol (BGP-4). RFC 4271.
https://tools.ietf.org/html/rfc4271
8. Patarin, J., Courtois, N.T., Goubin, L.: QUARTZ, 128-bit long digital signatures.
In: Naccache, D. (ed.) CT-RSA 2001. LNCS, vol. 2020, p. 282. Springer, Heidelberg
(2001)
9. Petzoldt, A., Chen, M.-S., Yang, B.-Y., Tao, C., Ding, J.: Design principles for
HFEv- based multivariate signature schemes. In: Iwata, T., Cheon, J.H. (eds.)
ASIACRYPT 2015. LNCS, vol. 9452, pp. 311–334. Springer, Heidelberg (2015).
doi:10.1007/978-3-662-48797-6 14
10. Rivest, R.L., Shamir, A., Adleman, L.: A method for obtaining digital signatures
and public-key cryptosystems. Commun. ACM 21(2), 120–126 (1978)
11. Shor, P.: Polynomial-time algorithms for prime factorization and discrete loga-
rithms on a quantum computer. SIAM J. Comput. 26(5), 1484–1509 (1997)

Cryptanalysis of Multi-Prime Φ-Hiding
Assumption
Jun Xu1,2, Lei Hu1,2(B), Santanu Sarkar3, Xiaona Zhang1,2,
Zhangjie Huang1,2, and Liqiang Peng1,2
1 State Key Laboratory of Information Security, Institute of Information
Engineering, Chinese Academy of Sciences, Beijing 100093, China
{xujun,hulei,zhangxiaona,huangzhangjie,pengliqiang}@iie.ac.cn
2 Data Assurance and Communications Security Research Center,
Chinese Academy of Sciences, Beijing 100093, China
3 Indian Institute of Technology, Sardar Patel Road, Chennai 600 036, India
sarkar.santanu.bir@gmail.com
Abstract. In Crypto 2010, Kiltz, O’Neill and Smith used m-prime RSA
modulus N with m ≥3 for constructing lossy RSA. The security of
the proposal is based on the Multi-Prime Φ-Hiding Assumption. In this
paper, we propose a heuristic algorithm based on the Herrmann-May lat-
tice method (Asiacrypt 2008) to solve the Multi-Prime Φ-Hiding Problem
when prime e > N
2
3m . Further, by combining with mixed lattice tech-
niques, we give an improved heuristic algorithm to solve this problem
when prime e > N
2
3m −
1
4m2 . These two results are veriﬁed by our exper-
iments. Our bounds are better than the existing works.
Keywords: Multi-Prime Φ-Hiding Assumption · Multi-Prime Φ-Hiding
Problem · Lattice · LLL algorithm · Coppersmith’s technique · Gauss
algorithm
1
Introduction
1.1
Background
The Φ-Hiding Assumption [1] ﬁrstly introduced by Cachin, Micali and Stadler
in Eurocrypt 1999 was used for building a practical private information retrieval
scheme. Based on this assumption, many cryptographic schemes have been
designed, such as [3,4,6,12]. This assumption is roughly stated as follows:
“For a given integer N with unknown factorization, it is hard to decide
whether a given prime e divides Φ(N), where Φ is the Euler function.”
Obviously, the Φ-Hiding Assumption holds with some requirements on the
size of e since it is not true for e ≥N. The Φ-Hiding Assumption with RSA
modulus N = pq2k has been analyzed in Asiacrypt 2008 [16]. The corresponding
result is that a case of this variant fails with a good probability for any prime e.
c
⃝Springer International Publishing Switzerland 2016
M. Bishop and A.C.A. Nascimento (Eds.): ISC 2016, LNCS 9866, pp. 440–453, 2016.
DOI: 10.1007/978-3-319-45871-7 26

Cryptanalysis of Multi-Prime Φ-Hiding Assumption
441
For cryptographic applications, one would like e to be as large as possible, but
from a security point of view, if e divides Φ(N) and is suﬃciently large, then one
can recover the factorization of N by using the idea of Coppersmith [2,9,14].
Thus, it is interesting to know the minimal size of e that allows for eﬃcient
factoring attacks.
It is well known that one can utilize Coppersmith’s method to factorize the
standard RSA modulus N = pq when prime e > N
1
4 divides Φ(N) = (p−1)(q−1).
In Asiacrypt 2012, Kakvi, Kiltz and May proposed a lattice algorithm for obtaining
a non-trivial factor of general N under the above condition [10].
In Crypto 2010, Kiltz et al. [12] showed that the RSA function f : x →xe
mod N is a log e lossy trapdoor permutation (LTDP) under the Φ-Hiding
Assumption with N = pq. They also showed that the RSA-OAEP is indis-
tinguishable against chosen plaintext attack (IND-CPA) in the standard model
under this assumption, which is a long time open problem. Furthermore, they
generalized this assumption to the multi-prime situation in order to obtain a
more eﬃcient LTDP such that RSA-OAEP can securely encrypt longer plain-
text. To be speciﬁc, this multi-prime situation is described as follows:
“For a given RSA modulus N = p1 · · · pm where bit-length of the pi are
equal for all 1 ≤i ≤m, it is hard to decide whether a given prime e divides
pi −1 for all pi except one prime factor of N.”
The condition that e divides pi −1 for all pi except one prime factor of N
implies that em−1 divides Φ(N) = (p1 −1) · · · (pm −1). So, this is a special case
of e divides Φ(N). Therefore, it is a variant of the Φ-Hiding Assumption. For the
sake of terminology, it is called as the Multi-Prime Φ-Hiding Assumption.
Now when e|(pi −1) for i ∈[1, m −1], there are integers xi such that exi =
pi −1. So if one obtains the integer root xi of equation exi = pi −1 for any
i ∈[1, m−1], factorization of N is easily possible as gcd(exi+1, N) = pi. Lattice
method like Coppersmith’s technique can be used to ﬁnd xi in polynomial time.
So the research goal is to maximize the bound up to which xi can be computed
eﬃciently. Since the prime pi are of the same bit-length, one fully breaks the
Multi-Prime Φ-Hiding Assumption when the bound of xi reaches N
1
m .
Originally, the bound N
1
m2 of xi was received by the Howgrave-Graham
method in [9]. Later, this bound was improved up to N O(
1
mc ) for some 1 <
c ≤2 in [7,12]. Eventually, the bounds N O(
1
m log m ) were acquired in [15,18,19].
However, it is open whether a bound N O( 1
m ), i.e., the exponent being linear in
1
m, could be achieved.
1.2
Previous Works
In this subsection, we recall some known attacks on the Multi-Prime Φ-Hiding
Problem. Note that if e divides pi −1 for all 1 ≤i ≤m, then N ≡1 mod e. It
gives a polynomial time distinguisher. To decide if e is Multi-Prime Φ-Hidden in
N, consider the system of equations
ex1 + 1 ≡0 mod p1,
ex2 + 1 ≡0 mod p2,
. . . ,
exm−1 + 1 ≡0 mod pm−1.

442
J. Xu et al.
Let x1 = N δ. Here all pi are of sizes of the same magnitude for 1 ≤i ≤m −1.
Usually we have
x2 ≈· · · ≈xm−1 ≈N δ.
The Howgrave-Graham method [9] can be used to ﬁnd the desired small
solutions of a modular linear equation
exi + 1 = 0 mod pi for some i ∈{1, · · · , m −1}.
Using Howgrave-Graham’s method, one can solve the Multi-Prime Φ-Hiding
Problem in polynomial time if
δ <
1
m2 .
In Crypto 2010, Kiltz et al. [12] constructed a polynomial equation
em−1
m−1

i=1
xi

+ · · · + e
m−1

i=1
xi

+ 1 ≡0 mod
m−1

i=1
pi
by multiplying all given equations. Then they linearized the polynomial and
solved it by the Herrmann-May theorem [8]. They showed that one can solve the
Multi-Prime Φ-Hiding Problem in polynomial time if1
δ < 2
m
 1
m

m
m−1
.
Later in Africacrypt 2011, Herrmann [7] improved the work of Kiltz et al. He
used the Herrmann-May theorem to ﬁnd the desired root (x, y) in equation
e2x + ey + 1 = 0 mod
m−1

i=1
pi,
where x = em−3
m−1

i=1
xi + · · · +

j>i
xixj, y =
m−1

i=1
xi. He solved the Multi-Prime
Φ-Hiding Problem in polynomial time if
δ < 2
3
 1
m
 3
2
.
In ACISP 2012, Tosu and Kunihiro [19] generalized the method of Herrmann.
Instead of taking two variables, they considered linear polynomials of k variables
for k ∈[1, m −1]. They proved that one can solve the Multi-Prime Φ-Hiding
Problem in polynomial time if
δ <
max
1≤k≤m−1

2
k + 1
 1
m
 k+1
k 	
.
1 There is a minor mistake in proceedings version of Crypto 2010 as reported in
[7, Page 97].

Cryptanalysis of Multi-Prime Φ-Hiding Assumption
443
For large m, Tosu and Kunihiro further optimized k and got
δ <
2
em(ln m + 1)
where e is the base of the natural logarithm. Thus, asymptotically bound of δ is
2
em ln m = O(
1
m log m).
In SPACE 2012, Sarkar [15] observed that the sizes of two components of the
desired root (x, y) in the analysis of Herrmann are not balanced. Based on this
observation, he obtained better bound on δ than the work of Herrmann.
Takayasu and Kunihiro generalized the work of Herrmann and May [8] in [17,
18]. Their bounds are better when components of the desired root are of diﬀerent
size. Since there is a big diﬀerence between the sizes of x and y in Φ-Hiding Poly-
nomial of [7], one can get a better bound on δ than the work of Herrmann. The
bound of δ in the work of [17] is very close to [15], however, the work of [17] is more
ﬂexible and it can deal with modular equations with more variables than [15].
1.3
Our Contribution
In this paper, we show that the Multi-Prime Φ-Hiding Assumption does not hold
when δ <
1
3m. For the ﬁrst time, we obtain such a bound of δ which is linear in 1
m.
Thus we can solve the Multi-Prime Φ-Hiding Problem in polynomial time if
e > N
1
m −
1
3m = N
2
3m .
Further, we improve the bound of δ up to
1
3m +
1
4m2 . This improvement is
enormous for small values of m. Hence Multi-Prime Φ-Hiding Problem can be
solved in polynomial time if
e > N
1
m −(
1
3m +
1
4m2 ) = N
2
3m −
1
4m2 .
1.4
Organization of the Paper
We organize our paper as follow. In Sect. 2, we recall some preliminaries. In
Sect. 3, we propose an algorithm using lattice technique. We give an improved
algorithm using mixed lattice methods in Sect. 4. In Sect. 5, we give the compar-
ison of our work with the existing results. We present our experiment results in
Sect. 6. Section 7 concludes the paper.
2
Preliminaries
2.1
Lattice
A lattice L is a discrete subgroup of Rn. An alternative equivalent deﬁnition of
an integer lattice can be given using a basis.

444
J. Xu et al.
Let b1, · · · , bm be linear independent row vectors in Rn, a lattice L spanned
by them is
L =
 m

i=1
kibi


 ki ∈Z
	
.
The set {b1, · · · , bm} is called a basis of L and B = [b1
T , · · · , bm
T ]T is the
corresponding basis matrix. The dimension and determinant of L are respectively
dim(L) = m, det(L) =

det(BBT ).
When m = n, lattice is called full rank. In case of a full rank lattice, det(L) =
| det(B)|. From Hadamard’s inequality, it is known that det(B) ≤
n

i=1
∥bi∥, where
∥b∥denotes Euclidean ℓ2 norm of a vector b.
For any two-dimensional lattice L, the Gauss algorithm can ﬁnd out the
reduced basis vectors v1 and v2 satisfying
∥v1∥≤∥v2∥≤∥v1 ± v2∥
in time O

log2(max{∥v1∥, ∥v2∥})

. Here v1 is the shortest nonzero vector in L
and v2 is the shortest vector in L \ {kv1 | k ∈Z}. A shortest vector v of an
n dimensional lattice satisﬁes the Minkowski bound ∥v∥≤√n(det(L))
1
n . The
following result will be used in Sect. 4.
Lemma 1. (See, e.g., [5]).
Let v1 and v2 be the reduced basis vectors of L
by the Gauss algorithm and x ∈L. For the unique pair of integers (α, β) that
satisﬁes x = αv1 + βv2, we have
∥αv1∥≤
2
√
3∥x∥, ∥βv2∥≤
2
√
3∥x∥.
2.2
Finding Small Roots
Coppersmith gave rigorous methods for extracting small roots of modular
univariate polynomials and bivariate integer polynomials. These methods can
extend to multivariate cases under the following assumption.
Assumption 1. Let h1, · · · , hn ∈Z[x1, · · · , xn] be the polynomials that are
found by Coppersmith’s algorithm. Then the ideal generated by the polynomial
equations h1(x1, · · · , xn) = 0, · · · , hn(x1, · · · , xn) = 0 has dimension zero.
Herrmann and May used the idea of Coppersmith’s technique to analyze
modular linear polynomials and got the following result for bivariate linear poly-
nomials.

Cryptanalysis of Multi-Prime Φ-Hiding Assumption
445
Theorem 1 ([8]). Let ϵ > 0 and N be a large integer with a divisor p ≥N β. Let
f(x1, x2) ∈Z[x1, x2] be a linear polynomial. Under Assumption 1, one can ﬁnd
all solutions (x1, x2) of the equation f(x1, x2) = 0 mod p with |x1| ≤N γ1, |x2| ≤
N γ2 in polynomial time if
γ1 + γ2 ≤3β −2 + 2(1 −β)
3
2 −ϵ.
In our analyses, we consider the asymptotic case and ignore the low order
term.
2.3
Multi-Prime Φ-Hiding Assumption
We brieﬂy introduce the Multi-Prime Φ-Hiding Assumption and the correspond-
ing problem. Please refer to [7,12,15,19] for more details.
Deﬁnition 1 (Multi-Prime Φ-Hiding Problem).
Let N = p1 · · · pm be a
Multi-Prime RSA modulus where the pi are of the same bit length for 1 ≤i ≤m.
Let e be a given prime of the size N
1
m −δ. The problem is to decide whether
e | (p1 −1), · · · , e | (pm−1 −1), e ∤(pm −1).
Deﬁnition 2 (Multi-Prime Φ-Hiding Assumption). There is no polyno-
mial time algorithm that solves the Multi-Prime Φ-Hiding Problem with a non-
negligible probability of success.
3
Algorithm Using Lattice Technique
In this section we give an algorithm for solving the Multi-Prime Φ-Hiding Prob-
lem. Our algorithm can be derived from the following theorem.
Theorem 2. Let N = p1 · · · pm be a Multi-Prime RSA modulus where the pi
are of same bit length for 1 ≤i ≤m. Let e be a prime of the size N
1
m −δ. Under
Assumption 1, we can solve the Multi-Prime Φ-Hiding Problem in polynomial
time when
δ <
1
3m.
Proof. Let r = N mod e and s = ( N−r
e ) mod e. If e | (p1 −1), · · · , e | (pm−1 −1)
and e ∤(pm −1), there exist unknown integers x1, · · · , xm−1 such that
ex1 + 1 = p1, · · · , exm−1 + 1 = pm−1.
Since N = p1 · · · pm, we have (ex1 + 1) · · · (exm−1 + 1) · pm = N. Then taking
modulo e on both sides we get
pm mod e = N mod e = r.

446
J. Xu et al.
Thus, there is an equation exm + r = pm with unknown xm. We multiply all
equations together to get (ex1 + 1) · · · (exm−1 + 1)(exm + r) = N. So we have
(em−1
m−1

i=1
xi + · · · + e2

1≤i<j≤m−1
xixj + e

1≤i≤m−1
xi + 1)(exm + r) = N
⇒(em−1
m−1

i=1
xi + · · · + e2

1≤i<j≤m−1
xixj + e

1≤i≤m−1
xi + 1)(exm + r) = N
⇒(e

1≤i≤m−1
xi + 1)(exm + r) ≡N mod e2
⇒exm + er

1≤i≤m−1
xi + r ≡N mod e2
⇒exm + er

1≤i≤m−1
xi ≡es mod e2
⇒xm + r

1≤i≤m−1
xi −s ≡0 mod e.
Let y1 = xm, y2 = x1 + · · · + xm−1. Consider the bivariate modular linear
equation
f(y1, y2) = y1 + ry2 −s.
(1)
The Eq. (1) has root y := (xm, x1 + · · · + xm−1) in Ze as f(y1, y2) ≡0 mod e.
First, let us bound the size of y. Since 0 < xi = pi−1
e
<
N
1
m
N
1
m −δ = N δ for
i = 1, . . . , m, we have
0 < x1 + · · · + xm−1 < (m −1)N δ = e
loge(m−1)+
δ
1
m −δ .
Next, we use Theorem 1 for solving Eq. (1). Since modulus e is known, we
take β = 1. Here γ1 =
δ
1
m −δ and γ2 = loge(m −1) +
δ
1
m −δ. Under Assumption 1,
we can ﬁnd all solution (y1, y2) in polynomial time when
γ1 + γ2 =
2δ
1
m −δ + loge (m −1) ≤1 −ϵ.
Considering the asymptotic case and ignoring the lower order terms, the above
condition is simpliﬁed to
δ <
1
3m.
Further, we check whether gcd(ey1 + r, N) gives a nontrivial factor of N
for every candidate. Thus, we can ﬁnd out the desired root y and recover pm.
Conversely, if we cannot get a non-trivial factor of N under Assumption 1, then
relation e | (p1 −1), · · · , e | (pm−1 −1), e ∤(pm −1) in the Multi-Prime Φ-Hiding
Problem does not hold.
Based on the Theorem 2, we have the Algorithm 1 to solve the Multi-Prime
Φ-Hiding Problem.

Cryptanalysis of Multi-Prime Φ-Hiding Assumption
447
Algorithm 1. Solving Multi-Prime Φ-Hiding Problem
Input: Public key (N, e) and m is the number of prime factors of N.
Output: Decide whether e | (p1 −1), · · · , e | (pm−1 −1), e ∤(pm −1).
1: Compute r = N mod e and s = ( N−r
e
) mod e.
2: Solve equation y1 + ry2 −s ≡0 mod e using Theorem 1.
3: If gcd(ey1 + r, N) for all solutions (y1, y2) are trivial factors of N, output no. Else,
output yes.
4
Improved Algorithm Using Mixed Lattice Methods
In this section we present an improved algorithm in order to improve the bound
δ <
1
3m. This algorithm is obtained by dealing with Eq. (1) with mixed lattice
methods in the following theorem.
Theorem 3. Let N = p1 · · · pm be a Multi-Prime RSA modulus where the pi
are of same bit length for 1 ≤i ≤m. Let e be a prime of the size N
1
m −δ. Under
Assumption 1, we can solve the Multi-Prime Φ-Hiding Problem in polynomial
time when
δ <
4
3m −2
3 + 2
3

1 −1
m
3/2
.
Proof. If e | (p1 −1), · · · , e | (pm−1 −1) and e ∤(pm −1), we know
y1 + ry2 ≡s mod e
(2)
has integer root y := (xm, x1 + · · · + xm−1), where
∥y∥=

(x1 + · · · + xm−1)2 + x2m < m · N δ.
The set of solutions
L =

(y1, y2) ∈Z2 | y1 + ry2 ≡0 mod e

forms an additive discrete subgroup of Z2. Thus, L is a 2-dimensional integer
lattice. Lattice L is spanned by the row vectors of the basis matrix
B =

−r 1
e 0

.
Let us brieﬂy check integer span of B, denoted by span(B) is indeed equal
to L. First both (−r, 1) and (e, 0) are solutions of y1 + ry2 ≡0 mod e. Thus
span(B) ⊆L. Conversely, let (y1, y2) ∈L. So we have y1 + ry2 = ke for some
k ∈Z. Then (y2, k)B = (y1, y2) ∈span(B). Thus L ⊆span(B).
Consider the set
L′ =

(s + y1, y2) | (y1, y2) ∈L

.
It is clear that for any (x, y) ∈L′, (x, y) will satisfy the Eq. (2).

448
J. Xu et al.
Let u := (u1, u2) be the smallest length vector in L′, which can be obtained by
the closest vector algorithm on the lattice L from the point (−s, 0) in polynomial
time (see, e.g., [11]). Obviously, ∥u∥≤∥y∥< m · N δ.
Let v1 := (v11, v12), v2 := (v21, v22) be Gauss-reduced basis vectors of L.
Since y −u belongs to L, there exist integer coeﬃcients α1, α2 such that
y −u = α1v1 + α2v2.
(3)
Observing that the ﬁrst component of y is equal to
pm−r
e
and rearranging
Eq. (3), we get ev11α1 + ev21α2 + eu1 + r = pm. In Appendix A, we prove
that |v11| ≤
√
2e and v11 ̸= 0. Thus, the bivariate modular linear equation
(ev11)x1 + (ev21)x2 + (eu1 + r) ≡0 mod pm
(4)
has an integer root (α1, α2).
First, let us bound the sizes of unknown α1 and α2. From (3), according to
Lemma 1, we obtain
|α1| ≤2∥y −u∥
√
3∥v1∥≤2(∥y∥+ |u∥)
√
3∥v1∥
< 4mN δ
√
3∥v1∥,
|α2| ≤2∥y −u∥
√
3∥v2∥≤2(∥y∥+ |u∥)
√
3∥v2∥
< 4mN δ
√
3∥v2∥.
So |α1α2| <
16m2N2δ
3∥v1∥∥v2∥. Notice that e = det(L) ≤∥v1∥∥v2∥. Thus we have
|α1α2| < 16m2N 2δ
3∥v1∥∥v2∥= N 3δ−1
m +logN
16m2
3
, as e = N
1
m −δ.
Next, we use Theorem 1 to solve the Eq. (4), where the size of unknown
modulus pm is N
1
m . So we take β =
1
m. Under Assumption 1, we can ﬁnd all
roots (x1, x2) of the Eq. (4) in polynomial time when
3δ −1
m + logN
16m2
3
≤3
m −2 + 2

1 −1
m
 3
2
−ϵ.
Ignoring the term logN
16m2
3
as m ≪N, we get
δ <
4
3m −2
3 + 2
3

1 −1
m
3/2
.
Furthermore, we check whether gcd(ev11x1 + ev21x2 + eu1 + r, N) gives a
nontrivial factor of N for every candidate. Thus, we can obtain the desired root
(α1, α2) and recover the factor pm of N.
⊓⊔
Since (1 −1
m)
3
2 = 1 −
3
2m +
3
8m2 + o( 1
m2 ), we have
4
3m −2
3 + 2
3

1 −1
m
3/2
≈
1
3m +
1
4m2 . Thus the simpliﬁed condition is
δ <
1
3m +
1
4m2 .

Cryptanalysis of Multi-Prime Φ-Hiding Assumption
449
So when e > N
1
m −
1
3m −
1
4m2 = N
2m
3 −
1
4m2 , one can solve Multi-Prime Φ-Hiding
Problem in polynomial time.
Since
4
3m −2
3 + 2
3

1 −1
m
3/2
>
1
3m, bound of δ in Theorem 3 is better than
that of Theorem 2. In Fig. 1, we present the two bounds pictorially.
Fig. 1. Comparison between the bounds δ for Theorem 2 and Theorem 3 when 3 ≤
m ≤20.
Based on the Theorem 3, we have the Algorithm 2 to solve the Multi-Prime
Φ-Hiding Problem.
Algorithm 2. Further Solving Multi-Prime Φ-Hiding Problem
Input: Public key (N, e) and m is the number of prime factors of N.
Output: Decide whether e | (p1 −1), · · · , e | (pm−1 −1), e ∤(pm −1).
1: Compute r = N mod e and s = ( N−r
e
) mod e.
2: Find the smallest Euclidean length root (u1, u2) of equation y1 + ry2 ≡s mod e
using the closest vector algorithm.
3: Generate lattice L spanned by the row vectors of the matrix
−r 1
e 0

.
4: Compute Gauss-reduced basis vectors (v11, v12) and (v21, v22) of lattice L.
5: Solve equation (ev11)x1 + (ev21)x2 + (eu1 + r) ≡0 mod pm using Theorem 1.
6: If gcd(ev11x1 + ev21x2 + eu1 + r, N) for all solutions (x1, x2) are trivial factors of
N, output no. Else, output yes.

450
J. Xu et al.
5
Comparison with the Existing Works
In this section, we compare our results with previous works.
In Fig. 2, we compare our results with the existing works pictorially. We observe
that the curve of [18] is almost identical with that of [15]. So we do not plot it
explicitly. It is clear from the ﬁgure that our bound is much better than the existing
bounds. Thus our new attack solves the Multi-Prime Φ-Hiding Problem for more
values of e than the existing works. One can see that existing curves [7,15,19] are
very close to each other. On the other hand, we are achieving much improved curve.
More importantly, for small values of m, these diﬀerences are more prominent.
For example when m = 4, new bound of δ becomes 0.09968 whereas existing was
Fig. 2. Comparison of our bound δ <
4
3m −2
3 + 2
3

1 −1
m
3/2
with the existing works
for 3 ≤m ≤20.
Table 1. Comparison of bit lengths of the minimum e with 2048-bit N
Results
m
3
4
5
6
7
8
9
10
Kiltz et al. [12]
420
351
301
262
233
209
190
174
Herrmann [7]
420
342
288
249
219
196
177
162
Tosu et al. [19]
420
342
288
248
217
192
173
158
Sarkar [15]
420
341
286
245
214
190
170
155
Takayasu et al. [18]
421
341
286
245
214
190
170
154
( 1
m −
1
3m) · 2048
456
342
274
228
196
171
152
137
1
m −( 4
3m −2
3 + 2
3(1 −1
m)3/2)

· 2048 395
308
252
213
185
163
146
132

Cryptanalysis of Multi-Prime Φ-Hiding Assumption
451
0.08358 in [18]. Thus the improvement is signiﬁcant for small values of m. Also m
cannot be large as in that case Elliptic Curve Factorization [13] will be eﬃcient.
In Table 1, we present the minimum bit lengths of e for which Φ-Hidding
Problem is polynomial time solvable for diﬀerent values of m. Here we take
2048-bit N. From the table, it is clear that all existing bounds are almost same
for m = 3. Though early works improve the work of [12] for values m > 3, this
work improves the bound on δ for m > 3 as well as m = 3. So one can solve
Φ-Hidding Problem in polynomial time for much smaller values of e.
6
Experiment Results
We implement the above attacks with LLL algorithm in Magma on a PC with
Intel(R) Core(TM) Quad CPU (2.83 GHz, 3.25 GB RAM, Windows XP). In
our experiments, Assumption 1 is always veriﬁed. We present our experimental
results in Table 2. As we can see that the experimental results and theoretical
upper bounds on δ are perfectly match.
For Theorem 2, we take 3 ≤m ≤10. We use Theorem 1 to solve equation
y1 + ry2 −s = 0 mod e. For a positive integer t, we generate polynomials
gk,i(y1, y2) := yi
2(y1 + ry2 −s)ket−k
which share the common root y modulo et, where k = 0, · · · , t; i = 0, . . . , t −k.
In our experiments, we choose t = 8. The dimensions of the involved lattices are
1
2(t2 + 3t + 2) = 45. Then the desired root can be obtained by lattice reduction.
Hence the factor pm of the modulus N can be recovered when e is Multi-Prime
Φ-Hidden in N and the corresponding δ satisﬁes the experimental value.
For Theorem 3, we present the situations of 3 ≤m ≤5. We neglect running
times of the closest vector algorithm and the Gauss algorithm as they are negli-
gible since the corresponding lattices are only two-dimensional. In order to use
Table 2. Experiment results for diﬀerent values of m with 2048 bit N
Analyses
m
δ (theoretical) δ (experimental) LLL (seconds) Gr¨obner (seconds)
Theorem 2
3
0.1111
0.1100
60.497
0.842
4
0.0833
0.0820
32.854
0.484
5
0.0667
0.0657
19.859
0.421
6
0.0556
0.0548
15.241
0.296
7
0.0476
0.0469
11.778
0.287
8
0.0417
0.0409
8.299
0.187
9
0.0370
0.0355
6.349
0.125
10
0.0333
0.0315
5.444
0.078
Theorem 3
3
0.1407
0.1320
3975.826
1120.540
4
0.0997
0.0891
2059.156
121.734
5
0.0770
0.0683
1866.188
109.938

452
J. Xu et al.
Theorem 1, we ﬁrst multiply the equation (ev11)x1 + (ev21)x2 + (eu1 + r) ≡
0 mod pm by (ev11)−1 modulo N and get a monic equation f(x1, x2) ≡0 mod pm.
Then, we collect the polynomials which share a common root (α1, α2) modulo N l
hk,i(x1, x2) := xi
2f k(x1, x2)N max{l−k,0}
for k = 0, · · · , t; i = 0, . . . , t−k and l =
 
1 −

m−1
m

t

. In our experiments, we
take t = 12. The dimensions of the corresponding lattices are 1
2(t2 + 3t + 2) = 91.
Finally, we obtain the desired (α1, α2).
7
Conclusion
In this paper, we have reduced the Multi-Prime Φ-Hiding Problem to the problem
of ﬁnding small root of a bivariate modular linear equation. Based on this, we
have proposed two algorithms using lattice techniques to solve the problem. We
have obtained better bounds than the existing works.
Acknowledgements. The authors would like to thank anonymous reviewers for their
helpful comments and suggestions. The work of this paper was supported by the National
Key Basic Research Program of China (Grants 2013CB834203), the National Natural
Science Foundation of China (Grants 61472417, 61472415 and 61502488), the Strategic
Priority Research Program of Chinese Academy of Sciences under Grant XDA06010702,
and the State Key Laboratory of Information Security, Chinese Academy of Sciences.
A
Proof on |v11| ≤
√
2e and v11 ̸= 0
Proof. Note that v1 = (v11, v12) is the shortest nonzero vector in lattice L.
According to Minkowski bound, we know that
∥v1∥≤

2 det(L) =
√
2e.
Since v11 is a component of v1, we have |v11| ≤
√
2e. Now, we prove that v11 ̸= 0.
Since v1 ∈L, there exists some integer c1 such that
v11 + rv12 = c1e.
If v11 = 0, we get rv12 = c1e. Since e is a prime and 0 < r < e, e divides v12.
Thus e divides ∥v1∥. So ∥v1∥≥e. However, it is impossible since ∥v1∥≤
√
2e.
Therefore, v11 ̸= 0.
⊓⊔
References
1. Cachin, C., Micali, S., Stadler, M.A.: Computationally private information retrieval
with polylogarithmic communication. In: Stern, J. (ed.) EUROCRYPT 1999.
LNCS, vol. 1592, p. 402. Springer, Heidelberg (1999)

Cryptanalysis of Multi-Prime Φ-Hiding Assumption
453
2. Coppersmith, D.: Small solutions to polynomial equations, and low exponent RSA
vulnerabilities. J. Cryptol. 10(4), 233–260 (1997)
3. Gentry, C., Mackenzie, P., Ramzan, Z.: Password authenticated key exchange using
hidden smooth subgroups. In: Proceedings of the 12th ACM Conference on Com-
puter and Communications Security CCS 2005, pp. 299–309. ACM, New York
(2005)
4. Gentry, C., Ramzan, Z.: Single-database private information retrieval with constant
communication rate. In: Caires, L., Italiano, G.F., Monteiro, L., Palamidessi, C.,
Yung, M. (eds.) ICALP 2005. LNCS, vol. 3580, pp. 803–815. Springer, Heidelberg
(2005)
5. Gomez, D., Gutierrez, J., Ibeas, A.: Attacking the pollard generator. IEEE Trans.
Inf. Theor. 52(12), 5518–5523 (2006)
6. Hemenway, B., Ostrovsky, R.: Public-key locally-decodable codes. In: Wagner, D.
(ed.) CRYPTO 2008. LNCS, vol. 5157, pp. 126–143. Springer, Heidelberg (2008)
7. Herrmann, M.: Improved cryptanalysis of the Multi-Prime φ - Hiding Assumption.
In: Nitaj, A., Pointcheval, D. (eds.) AFRICACRYPT 2011. LNCS, vol. 6737, pp.
92–99. Springer, Heidelberg (2011)
8. Herrmann, M., May, A.: Solving linear equations modulo divisors: on factoring
given any bits. In: Pieprzyk, J. (ed.) ASIACRYPT 2008. LNCS, vol. 5350, pp.
406–424. Springer, Heidelberg (2008)
9. Howgrave-Graham, N.: Approximate integer common divisors. In: Silverman, J.H.
(ed.) CaLC 2001. LNCS, vol. 2146, p. 51. Springer, Heidelberg (2001)
10. Kakvi, S.A., Kiltz, E., May, A.: Certifying RSA. In: Wang, X., Sako, K. (eds.)
ASIACRYPT 2012. LNCS, vol. 7658, pp. 404–414. Springer, Heidelberg (2012)
11. Kannan, R.: Minkowski’s convex body theorem and integer programming. Math.
Oper. Res. 12(3), 415–440 (1987)
12. Kiltz, E., O’Neill, A., Smith, A.: Instantiability of RSA-OAEP under chosen-
plaintext attack. In: Rabin, T. (ed.) CRYPTO 2010. LNCS, vol. 6223, pp. 295–313.
Springer, Heidelberg (2010)
13. Lenstra Jr., H.W.: Factoring integers with elliptic curves. Ann. Math. 126, 649–673
(1987)
14. May, A.: Using LLL-reduction for solving RSA and factorization problems. In:
Nguyen, P.Q., Valle, B. (eds.) The LLL Algorithm. Information Security and Cryp-
tography, pp. 315–348. Springer, Heidelberg (2010)
15. Sarkar, S.: Reduction in lossiness of RSA trapdoor permutation. In: Bogdanov,
A., Sanadhya, S. (eds.) SPACE 2012. LNCS, vol. 7644, pp. 144–152. Springer,
Heidelberg (2012)
16. Schridde, C., Freisleben, B.: On the validity of the Φ-hiding assumption in crypto-
graphic protocols. In: Pieprzyk, J. (ed.) ASIACRYPT 2008. LNCS, vol. 5350, pp.
344–354. Springer, Heidelberg (2008)
17. Takayasu, A., Kunihiro, N.: Better lattice constructions for solving multivariate
linear equations modulo unknown divisors. In: Boyd, C., Simpson, L. (eds.) ACISP.
LNCS, vol. 7959, pp. 118–135. Springer, Heidelberg (2013)
18. Takayasu, A., Kunihiro, N.: Better lattice constructions for solving multivariate lin-
ear equations modulo unknown divisors. IEICE Trans. 97–A(6), 1259–1272 (2014)
19. Tosu, K., Kunihiro, N.: Optimal bounds for multi-prime Φ-hiding assumption. In:
Susilo, W., Mu, Y., Seberry, J. (eds.) ACISP 2012. LNCS, vol. 7372, pp. 1–14.
Springer, Heidelberg (2012)

Author Index
Abdelkhalek, Ahmed
3, 18
Abe, Masayuki
408
Ai, Juan
59
Ashur, Tomer
73
Bag, Samiran
167
Biswas, Prosunjit
218
Eckert, Claudia
231, 305
El Bansarkhani, Rachid
426
Eppstein, David
283
Feng, Dengguo
130
Fu, Jianming
366
Fukumitsu, Masayuki
389
Gao, Debin
366
Goodrich, Michael T.
283
Hallstensen, Christoffer
183
Hanif, Zachary D.
231
Hanzlik, Lucjan
115
Hasegawa, Shingo
389
Hu, Lei
48, 440
Huang, Zhangjie
48, 440
Kiefer, Franziskus
95, 147
Krishnan, Ram
218
Kucuk, Yunus
343
Kunihiro, Noboru
35
Kutyłowski, Mirosław
115
Lam, Jenny
283
Last, David C.
343
Lengyel, Tamas K.
231
Lin, Yan
366
Ludwig, Andre L.P.
231
Mamano, Nil
283
Manulis, Mark
95, 147
Mennink, Bart
73
Ming, Jiang
323
Mitzenmacher, Michael
283
Mohamed, Mohamed Saied Emam
426
Nojima, Ryo
271
Okamoto, Tatsuaki
408
Ou, Changhai
59
Pang, Na
59
Peng, Liqiang
48, 440
Petzoldt, Albrecht
426
Qin, Yu
130
Sakurai, Kouichi
167
Sandhu, Ravi
218
Sarkar, Santanu
440
Shalaginov, Andrii
183
Slocum, Max
343
Sun, Degang
59
Takayasu, Atsushi
35
Tang, Xiaoxiao
366
Tolba, Mohamed
3, 18
Tomida, Junichi
408
Torres, Manuel
283
Wagner, Steffen
305
Wang, Li
203
Wang, Zhu
59
Wangen, Gaute
183
Waseda, Atsushi
271
Webster, George D.
231
Wu, Dinghao
203, 323
Xu, Dongpeng
323
Xu, Jun
48, 440
Yan, Guanhua
343
Yang, Bo
130
Yang, Kang
130
Youssef, Amr M.
3, 18
Zarras, Apostolis
231, 253
Zhang, Xiaona
48, 440
Zhang, Zhenfeng
130
Zhou, Xinping
59


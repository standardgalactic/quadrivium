Springer Tracts in Advanced Robotics
Volume 26
Editors: Bruno Siciliano · Oussama Khatib · Frans Groen

Springer Tracts in Advanced Robotics
Edited by B. Siciliano, O. Khatib, and F. Groen
Vol. 25: Corke, P.; Sukkarieh, S. (Eds.)
Field and Service Robotics
580 p. 2006 [3-540-33452-1]
Vol. 24: Yuta, S.; Asama, H.; Thrun, S.; Prassler, E.;
Tsubouchi, T. (Eds.)
Field and Service Robotics
550 p. 2006 [3-540-32801-7]
Vol. 23: Andrade-Cetto, J,; Sanfeliu, A.
Environment Learning for Indoor Mobile Robots
130 p. 2006 [3-540-32795-9]
Vol. 22: Christensen, H.I. (Ed.)
European Robotics Symposium 2006
209 p. 2006 [3-540-32688-X]
Vol. 21: Ang Jr., H.; Khatib, O. (Eds.)
Experimental Robotics IX
618 p. 2006 [3-540-28816-3]
Vol. 20: Xu, Y.; Ou, Y.
Control of Single Wheel Robots
188 p. 2005 [3-540-28184-3]
Vol. 19: Lefebvre, T.; Bruyninckx, H.; De Schutter, J.
Nonlinear Kalman Filtering for Force-Controlled
Robot Tasks
280 p. 2005 [3-540-28023-5]
Vol. 18: Barbagli, F.; Prattichizzo, D.; Salisbury, K. (Eds.)
Multi-point Interaction with Real and Virtual Objects
281 p. 2005 [3-540-26036-6]
Vol. 17: Erdmann, M.; Hsu, D.; Overmars, M.;
van der Stappen, F.A (Eds.)
Algorithmic Foundations of Robotics VI
472 p. 2005 [3-540-25728-4]
Vol. 16: Cuesta, F.; Ollero, A.
Intelligent Mobile Robot Navigation
224 p. 2005 [3-540-23956-1]
Vol. 15: Dario, P.; Chatila R. (Eds.)
Robotics Research { The Eleventh International
Symposium
595 p. 2005 [3-540-23214-1]
Vol. 14: Prassler, E.; Lawitzky, G.; Stopp, A.;
Grunwald, G.; Hagele, M.; Dillmann, R.;
Iossiˇdis. I. (Eds.)
Advances in Human-Robot Interaction
414 p. 2005 [3-540-23211-7]
Vol. 13: Chung, W.
Nonholonomic Manipulators
115 p. 2004 [3-540-22108-5]
Vol. 12: Iagnemma K.; Dubowsky, S.
Mobile Robots in Rough Terrain {
Estimation, Motion Planning, and Control
with Application to Planetary Rovers
123 p. 2004 [3-540-21968-4]
Vol. 11: Kim, J.-H.; Kim, D.-H.; Kim, Y.-J.; Seow, K.-T.
Soccer Robotics
353 p. 2004 [3-540-21859-9]
Vol. 10: Siciliano, B.; De Luca, A.; Melchiorri, C.;
Casalino, G. (Eds.)
Advances in Control of Articulated and Mobile Robots
259 p. 2004 [3-540-20783-X]
Vol. 9: Yamane, K.
Simulating and Generating Motions of Human Figures
176 p. 2004 [3-540-20317-6]
Vol. 8: Baeten, J.; De Schutter, J.
Integrated Visual Servoing and Force Control
198 p. 2004 [3-540-40475-9]
Vol. 7: Boissonnat, J.-D.; Burdick, J.; Goldberg, K.;
Hutchinson, S. (Eds.)
Algorithmic Foundations of Robotics V
577 p. 2004 [3-540-40476-7]
Vol. 6: Jarvis, R.A.; Zelinsky, A. (Eds.)
Robotics Research { The Tenth International Symposium
580 p. 2003 [3-540-00550-1]
Vol. 5: Siciliano, B.; Dario, P. (Eds.)
Experimental Robotics VIII
685 p. 2003 [3-540-00305-3]
Vol. 4: Bicchi, A.; Christensen, H.I.;
Prattichizzo, D. (Eds.)
Control Problems in Robotics
296 p. 2003 [3-540-00251-0]
Vol. 3: Natale, C.
Interaction Control of Robot Manipulators {
Six-degrees-of-freedom Tasks
120 p. 2003 [3-540-00159-X]
Vol. 2: Antonelli, G.
Underwater Robots { Motion and Force Control of
Vehicle-Manipulator Systems
209 p. 2003 [3-540-00054-2]
Vol. 1: Caccavale, F.; Villani, L. (Eds.)
Fault Diagnosis and Fault Tolerance for Mechatronic
Systems { Recent Advances
191 p. 2002 [3-540-44159-X]

Geoffrey Taylor  Lindsay Kleeman
Visual Perception
and Robotic Manipulation
3D Object Recognition, Tracking
and Hand-Eye Coordination
With 96 Figures

Professor Bruno Siciliano, Dipartimento di Informatica e Sistemistica, Universit`a degli Studi di Napoli Fede-
rico II, Via Claudio 21, 80125 Napoli, Italy, email: siciliano@unina.it
Professor Oussama Khatib, Robotics Laboratory, Department of Computer Science, Stanford University,
Stanford, CA 94305-9010, USA, email: khatib@cs.stanford.edu
Professor Frans Groen, Department of Computer Science, Universiteit vanAmsterdam, Kruislaan 403, 1098 SJ
Amsterdam, The Netherlands, email: groen@science.uva.nl
Authors
Geoffrey Taylor
Lindsay Kleeman
Monash University
Intelligent Robotics Research Centre
Department of Electrical & Computer Systems Engineering
Monash University, VIC 3800
Australia
grtay1@yahoo.com.au
lindsay.kleeman@eng.monash.edu.au
ISSN print edition: 1610-7438
ISSN electronic edition: 1610-742X
ISBN-10 3-540-33454-8
Springer Berlin Heidelberg New York
ISBN-13 978-3-540-33454-5
Springer Berlin Heidelberg New York
Library of Congress Control Number: 2006923557
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is concerned,
speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on
microﬁlm or in other ways, and storage in data banks. Duplication of this publication or parts thereof is permitted
only under the provisions of the German Copyright Law of September 9, 1965, in its current version, and
permission for use must always be obtained from Springer. Violations are liable to prosecution under German
Copyright Law.
Springer is a part of Springer Science+Business Media
springer.com
© Springer-Verlag Berlin Heidelberg 2006
Printed in Germany
The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply,
even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws and
regulations and therefore free for general use.
Typesetting: Digital data supplied by authors.
Data-conversion and production: PTP-Berlin Protago-TEX-Production GmbH, Germany (www.ptp-berlin.com)
Cover-Design: design & production GmbH, Heidelberg
Printed on acid-free paper
89/3141/Yu - 5 4 3 2 1 0

Editorial Advisory Board
EUROPE
Herman Bruyninckx, KU Leuven, Belgium
Raja Chatila, LAAS, France
Henrik Christensen, KTH, Sweden
Paolo Dario, Scuola Superiore Sant’Anna Pisa, Italy
R¨udiger Dillmann, Universit¨at Karlsruhe, Germany
AMERICA
Ken Goldberg, UC Berkeley, USA
John Hollerbach, University of Utah, USA
Lydia Kavraki, Rice University, USA
Tim Salcudean, University of British Columbia, Canada
Sebastian Thrun, Stanford University, USA
ASIA/OCEANIA
Peter Corke, CSIRO, Australia
Makoto Kaneko, Hiroshima University, Japan
Sukhan Lee, Sungkyunkwan University, Korea
Yangsheng Xu, Chinese University of Hong Kong, PRC
Shin’ichi Yuta, Tsukuba University, Japan
STAR (Springer Tracts in Advanced Robotics) has been promoted under the auspices
of EURON (European Robotics Research Network)
ROBOTICS
Research
Network
European
EURON
*
*
*
*
*
*
*
*
*
*
*
*

/ Ì> >` ÕÃi

ÀiÜÀ`
Ì Ìi `>Ü v Ìi iÜ iÕ] ÀLÌVÃ Ã Õ`iÀ}} > >À ÌÀ>ÃvÀ>Ì
 ÃV«i >` `iÃ° À > >À}iÞ `>Ì `ÕÃÌÀ> vVÕÃ] ÀLÌVÃ Ã À>«`Þ
iÝ«>`} Ì Ìi V>i}iÃ v ÕÃÌÀÕVÌÕÀi` iÛÀiÌÃ° ÌiÀ>VÌ} ÜÌ] >ÃÃÃÌ
}] ÃiÀÛ}] >` iÝ«À} ÜÌ Õ>Ã] Ìi iiÀ}} ÀLÌÃ Ü VÀi>Ã}Þ ÌÕV
«i«i >` ÌiÀ ÛiÃ°
/i }> v Ìi -«À}iÀ /À>VÌÃ  `Û>Vi` ,LÌVÃ ­-/,® ÃiÀiÃ Ã Ì LÀ}]
 > ÌiÞ v>Ã] Ìi >ÌiÃÌ >`Û>ViÃ >` `iÛi«iÌÃ  ÀLÌVÃ  Ìi L>ÃÃ v
ÌiÀ Ã}wV>Vi >` µÕ>ÌÞ° Ì Ã ÕÀ «i Ì>Ì Ìi Ü`iÀ `ÃÃi>Ì v ÀiÃi>ÀV
`iÛi«iÌÃ Ü ÃÌÕ>Ìi Ài iÝV>}iÃ >` V>LÀ>ÌÃ >} Ìi ÀiÃi>ÀV
VÕÌÞ >` VÌÀLÕÌi Ì vÕÀÌiÀ >`Û>ViiÌ v ÌÃ À>«`Þ }ÀÜ} wi`°
/i }À>« ÜÀÌÌi LÞ ivvÀiÞ />ÞÀ >` `Ã>Þ ii> Ã > iÛÕÌ
v Ìi wÀÃÌ ÕÌÀ½Ã *°° `ÃÃiÀÌ>Ì° /Ã L «ÀÛ`iÃ > Ìi}À>Ìi` ÃÞÃÌiÃ
ÛiÜ v V«ÕÌiÀ ÛÃ >` ÀLÌVÃ] VÛiÀ} > À>}i v vÕ`>iÌ> Ì«VÃ 
VÕ`} ÀLÕÃÌ >` «Ì> ÃiÃÀ `iÃ}] ÛÃÕ> ÃiÀÛ}] Î LiVÌ `i} >`
ÀiV}Ì] >` ÕÌVÕi ÌÀ>V}° /i ÌÀi>ÌiÌ Ã `i«Ì] ÜÌ `iÌ>Ã v ÌiÀÞ]
Ài>Ìi «iiÌ>Ì] >` iÝÌiÃÛi iÝ«iÀiÌ> ÀiÃÕÌÃ° Ã ÃÕV] Ìi L >Ã
Ü`i >««i> Ì LÌ ÌiÀiÌV> >` «À>VÌV> ÀLÌVÃÌÃ° ÕÀÌiÀÀi] Ìi VÕ>
Ì v Ìi ÜÀ ÜÌ Ìi `iÃÌÀ>Ì v ÌÜ Ài>ÜÀ` `iÃÌV Ì>ÃÃ Ài«ÀiÃiÌÃ
> wÀÃÌ ÃÌi« ÌÜ>À`Ã Ìi Ài>â>Ì v >ÕÌÕÃ ÀLÌÃ vÀ ÃiÀÛVi >««V>ÌÃ°
/i wÀÃÌ L  Ìi ÃiÀiÃ Ì Li >VV«>i` LÞ > V«ÀiiÃÛi ÕÌi`>
ÃÕ««iiÌ] ÌÃ ÌÌi VÃÌÌÕÌiÃ > ÛiÀÞ wi >``Ì Ì -/,t
 >«iÃ] Ì>Þ]
	ÀÕ -V>
iLÀÕ>ÀÞ ÓääÈ
-/, `ÌÀ

*Àiv>Vi
ÕÌÕÃ ÀLÌÃ >Ûi Ìi «ÌiÌ> Ì ÀiÛÕÌâi Ìi ÃiÀÛVi `ÕÃÌÀÞ° -«i
V>âi` >««V>ÌÃ ÃÕV >Ã «>iÌâ}] Û>VÕÕ Vi>} >` ÌÕÀ }Õ`} V> Li
ÃÛi` ÜÌ Ì`>Þ½Ã ÌiV}Þ] LÕÌ iÝÌ}iiÀ>Ì ÕÛiÀÃ> ÀLÌÃ Ü ÀiµÕÀi iÜ
ÃÕÌÃ Ì Ìi V>i}i v «iÀ>Ì}  > ÕVÃÌÀ>i` Õ> iÛÀiÌ° À
iÝ>«i] >}i ÞÕ ÃÕvviÀ vÀ >ÀÌÀÌÃ Ã ÃiÛiÀi Ì>Ì ÞÕÀ `>Þ ii`Ã } Õ>Ì
Ìi`i`°  ÀLÌV >` VÕ` ÀiÃÌÀi ÞÕÀ `i«i`iVi LÞ i«} Ì «ÕÌ  ÞÕÀ
ÃiÃ] «ÕÀ > `À] ÃÌ>V `ÃiÃ] «ÕÌ >Ü>Þ }ÀViÀiÃ À > ÃÌ v ÌiÀ ÕÃi`
Ì>ÃÃ ÛÛ} vÌ}] Li`} >` V>ÀÀÞ}° -ÕV > ÀLÌ ÕÃÌ Li V>«>Li v ÌiÀ
«ÀiÌ} >` «>} >VÌÃ L>Ãi`  Ã«i ÃÕ«iÀÛÃÀÞ V>`Ã] ÀiV}â} >
Û>ÀiÌÞ v LiVÌÃ `iÃ«Ìi > VÌÕÕÃ Ã«iVÌÀÕ v Û>À>Ì  >««i>À>Vi] >` >
«Õ>Ì} > `Þ>V iÛÀiÌ  VÌÕÕÃÞ iÜ Ü>ÞÃ° 1i ÌiÀ `ÕÃÌÀ>
VÕÌiÀ«>ÀÌÃ] Ü`iÃ«Ài>` >`«Ì v `iÃÌV ÀLÌÃ `i>`Ã > Ài>Vi 
>««V>Ì Ã«iVwV Üi`}i >` } ÀLÕÃÌiÃÃ Ì iÛÀiÌ> V>}iÃ >` 
iÛÌ>Li «iÀ>Ì> Üi>À° ,V ÃiÃ} `>ÌiÃ ÃÕV >Ã ÛÃ >Ài ÌiÀivÀi iÞ
Ì «>Þ > ViÌÀ> Ài  ÌiÀ ÃÕVViÃÃ° /Ã L Ì>iÃ ÃÌi«Ã ÌÜ>À`Ã Ìi Ài>â>Ì
v `iÃÌV ÀLÌÃ LÞ «ÀiÃiÌ} Ìi vÕ`>iÌ> V«iÌÃ v > Î `iL>Ãi`
ÀLÌ ÛÃ vÀ>iÜÀ° Ì > ÃÌ>}iÃ vÀ «iÀVi«Ì ÌÀÕ} Ì VÌÀ] i«>ÃÃ
Ã «>Vi`  ÀLÕÃÌiÃÃ Ì ÕÜ iÛÀiÌ> V`ÌÃ >` V>LÀ>Ì iÀÀÀÃ°
Ì Ìi ÜiÃÌ iÛi v «iÀVi«Ì] ÃÌiÀiÃV«V }Ì ÃÌÀ«i ÃV>} V>«ÌÕÀiÃ
> `iÃi À>}i >« v Ìi iÛÀiÌ° 1i VÛiÌ> ÃÌÀÕVÌÕÀi` }Ì ÌiV
µÕiÃ] ÃÌiÀiÃV«V ÃV>} iÝ«ÌÃ Ài`Õ`>VÞ Ì ÀLÕÃÌÞ `iÌvÞ Ìi «À>ÀÞ
}Ì ÃÌÀ«i ÀiyiVÌ `iÃ«Ìi ÃiV`>ÀÞ ÀiyiVÌÃ] VÀÃÃÌ> >` ÌiÀ ÃÕÀViÃ v
ÌiÀviÀiVi°  >ÕÌ>ÌV «ÀVi`ÕÀi Ì V>LÀ>Ìi Ìi ÃÞÃÌi vÀ Ìi ÃV> v >
>ÀLÌÀ>ÀÞ «>>À LiVÌ Ã >Ã `iÃVÀLi`°
Ì Ìi iÝÌ iÛi v «iÀVi«ÌÕ> >LÃÌÀ>VÌ] À>}i `>Ì> Ã Ãi}iÌi` >` wÌÌi`
ÜÌ }iiÌÀV «ÀÌÛiÃ VÕ`} «>iÃ] VÞ`iÀÃ] ViÃ >` Ã«iÀiÃ° 
iÌÀ>
Ì ÌÃ «ÀViÃÃ Ã > ÃÕÀv>Vi ÌÞ«i V>ÃÃwV>Ì >}ÀÌ Ì>Ì V>À>VÌiÀâiÃ Ìi V>
Ì«}Þ v À>}i `>Ì>° /i V>ÃÃwiÀ «ÀiÃiÌi`  ÌÃ L Ã ÃÜ Ì >ViÛi
Ã}wV>ÌÞ }Ài>ÌiÀ Ãi ÀLÕÃÌiÃÃ Ì> VÛiÌ> ÌiVµÕiÃ° >Þ V>ÃÃiÃ
v `iÃÌV LiVÌÃ V> Li `iÌwi` >Ã V«ÃÌiÃ v iÝÌÀ>VÌi` «ÀÌÛiÃ ÕÃ} >
}À>« >ÌV} ÌiVµÕi] `iÃ«Ìi Ã}wV>Ì Û>À>ÌÃ  >««i>À>Vi°

8
*Àiv>Vi
ÌÊÌiÊ}iÃÌÊiÛiÊvÊ«iÀVi«Ì]Ê`iL>Ãi`ÊÌÀ>V}ÊV«iÃ>ÌiÃÊvÀÊÃVii
`Þ>VÃÊ>`ÊV>LÀ>ÌÊiÀÀÀÃÊ`ÕÀ}Ê>«Õ>Ì°Ê-iiVÌÊvÊÃ>iÌÊÌÀ>V}
vi>ÌÕÀiÃÊÃÊ>ÊV>i}}Ê«ÀLiÊÜiÊLiVÌÃÊ>`ÊÛÃÕ>ÊV`ÌÃÊ>ÀiÊÕÜ°
/ÃÊLÊiÝ«ÀiÃÊÌiÊÕÃiÊvÊÕÌVÕiÊvÕÃÊL>Ãi`ÊÊi`}i]ÊÌiÝÌÕÀiÊ>`ÊVÕÀ
VÕiÃ]ÊÜVÊ>ViÛiÃÊ}ÌiÀÊÀLÕÃÌiÃÃÊ`iÃ«ÌiÊÃ}Ê`Û`Õ>ÊVÕiÃÊ>ÃÊLiVÌÃ
>`ÊÛÃÕ>ÊV`ÌÃÊÛ>ÀÞ°ÊÕÌVÕiÊÌÀ>V}ÊÃÊÃÜÊÌÊÃÕVVii`ÊÊV>i}}
ÃÌÕ>ÌÃÊÜiÀiÊÃ}iVÕiÊÌÀ>ViÀÃÊv>°
>Þ]ÊÀLÕÃÌÊ>`iÞiÊVÀ`>ÌÊÌÊ«iÀvÀÊÕÃivÕÊ>VÌÃÊÀiµÕÀiÃÊÛÃÕ>
vii`L>VÊVÌÀÊvÊ>ÊÀLÌÊ>«Õ>ÌÀ°Ê/ÃÊLÊÌÀ`ÕViÃÊÞLÀ`Ê«ÃÌL>Ãi`
ÛÃÕ>ÊÃiÀÛ}]ÊÜVÊvÕÃiÃÊi>ÌVÊ>`ÊÛÃÕ>Êi>ÃÕÀiiÌÃÊÌÊÀLÕÃÌÞÊ>`i
VVÕÃÃÊ>`Ê«ÀÛ`iÊ>ÊiV>ÃÊvÀÊiÊV«iÃ>ÌÊvÊV>LÀ>ÌÊiÀÀÀÃ]
LÌÊV>ÃÃV>Ê«ÀLiÃÊÊ«ÃÌL>Ãi`ÊÛÃÕ>ÊÃiÀÛ}°
/iÊÀiÃÕÌÃÊvÊiÝÌiÃÛiÊÌiÃÌ}ÊÊ>ÊÕ««iÀÌÀÃÊÕ>`ÊÀLÌÊ>ÀiÊ«ÀiÃiÌi`
ÌÊÃÕ««ÀÌÊÌiÊvÀ>iÜÀÊ`iÃVÀLi`ÊÊÌÃÊL°Ê/iÊVÕ>ÌÊvÊÌiÊiÝ«iÀiÌ>
ÜÀÊÃÊÌiÊ`iÃÌÀ>ÌÊvÊÌÜÊÀi>ÜÀ`Ê`iÃÌVÊÌ>ÃÃ\ÊV>Ì}Ê>`Ê}À>Ã«}Ê>
ÕÜÊLiVÌÊ>`Ê«ÕÀ}ÊÌiÊVÌiÌÃÊvÊ>ÊÌiÀ>VÌÛiÞÊÃiiVÌi`ÊVÕ«ÊÌÊ>ÊLÜ°
>Þ]ÊÌiÊÀiÊvÊÛÃÊÊ>Ê>À}iÀÊÕÌÃiÃÀÊvÀ>iÜÀÊÃÊiÝ«Ài`ÊÌÀÕ}ÊÌi
Ì>ÃÊvÊ`iÌvÞ}Ê>ÊVÕ«ÊvÊiÌ>ÊvÀÊ>}ÊÃiÛiÀ>ÊV>``>ÌiÃÊL>Ãi`ÊÊÛÃÕ>]
`ÕÀÊ>`Ê>ÀyÜÊÃiÃ}°
VV«>Þ}ÊÌÃÊLÊÃÊ>Ê
,"ÊVÌ>}ÊÛ`iÊV«Ã]Ê6,Ê`>Ì>]Ê
³³
V`iÊÜÌÊ`>Ì>ÊÃiÌÃÊ>`ÊiVÌÕÀiÊÃ`iÃÊÌÊÃÕ««iiÌÊÌiÊ`ÃVÕÃÃÃÊ>`ÊiÝ«iÀiÌ>
ÀiÃÕÌÃ°Ê/ÃÊ>ÌiÀ>ÊÃÊÀiviÀÀi`ÊÌÊ>ÃÊÌiÊÕÌi`>ÊÝÌiÃÃÊ>ÌÊÀiiÛ>ÌÊ«ÌÃÊ
ÌiÊÌiÝÌ°ÊÊ>``Ì]ÊÛ`iÊV«ÃÊvÊ«Ài>ÀÞÊ>`ÊÀi>Ìi`ÊiÝ«iÀiÌÃÊÌÊ`ÃVÕÃÃi`
Ê>ÊÌiÝÌÊ>ÀiÊ>ÃÊ«ÀiÃiÌi`]Ê«ÀÛ`}Ê>ÊL>V}ÀÕ`ÊÌÊÌiÊ`iÛi«iÌÊvÊÌÃ
ÜÀ°Ê/ÊÛiÜÊÌiÊÕÌi`>ÊÝÌiÃÃ]ÊÃiÀÌÊÌiÊ
,"ÊÊ>Ê*
Ê`ÃÊ`ÀÛi
>`Ê>ÊÜiLÊLÀÜÃiÀÊÃÕ`Ê>ÕVÊÜÌÊÌiÊ/ÊÌiÀv>Vi°ÊvÊÌiÊLÀÜÃiÀÊ`iÃÊÌ
>ÕVÊ>ÕÌ>ÌV>Þ]ÊÌiÊÌiÀv>ViÊV>ÊLiÊ>VViÃÃi`ÊLÞÊ«i}Ê
	 ÊÌi
ÀÌÊ`ÀiVÌÀÞÊvÊÌiÊ
,"°
/iÊ>ÕÌÀÃÊÜÕ`ÊiÊÌÊÌ>Ê,°Ê`ÞÊ,ÕÃÃi]ÊÜÊ«ii`ÊÕ«ÊÌiÀiÃÌ}ÊiÜ
ÀiÃi>ÀVÊ`ÀiVÌÃÊLÞÊV>LÀ>Ì}ÊÊÌiÊ`ÕÀÊ>`Ê>ÀyÜÊÃiÃ}ÊiÝ«iÀiÌÃÊ

>«ÌiÀÊÇ]Ê>`Ê-ÛiÊÊ>`Ê Âi 7iÀiÀÃÃ vÀ >} ÌiÀ ÃÌ>Þ >Ì ÕiÂ> 1
ÛiÀÃÌÞ v /iV}Þ] -Üi`i] > Û>Õ>Li iÝ«iÀiVi >` vÀ ÌÛ>Ì} Ìi }Ì
ÃÌÀ«i ÀiÃi>ÀV  
>«ÌiÀ Î° /Ã ÀiÃi>ÀV Ü>Ã ÃÕ««ÀÌi` LÞ > ÕÃÌÀ>> *ÃÌ}À>`
Õ>Ìi Ü>À` >` Ìi -ÌÀ>Ìi}V >Ã 1ÛiÀÃÌÞ ,iÃi>ÀV Õ` ­-1,® vÀ Ìi
Õ>` ,LÌVÃ\ *iÀVi«Ì] Ìi}iVi >` 
ÌÀ «ÀiVÌ >Ì Ìi Ìi}iÌ
,LÌVÃ ,iÃi>ÀV 
iÌÀi ­,,
®] >` Ìi ÕÃÌÀ>> ,iÃi>ÀV 
ÕV 
iÌÀi vÀ
*iÀVi«ÌÛi >` Ìi}iÌ >ViÃ  
«iÝ ÛÀiÌÃ ­*
®° /i vÕ`
} Ã }À>ÌivÕÞ >VÜi`}i`°
>Ã 1ÛiÀÃÌÞ]
ivvÀiÞ />ÞÀ
iLÀÕ>ÀÞ ÓääÈ
`Ã>Þ ii>

*Àiv>Vi
8
/ÃÊLÊÃÊÌiÊÀiÃÕÌÊvÊvÕÀÊÞi>ÀÃÊvÊ«ÃÌ}À>`Õ>ÌiÊÀiÃi>ÀVÊ>ÌÊÌiÊi«>ÀÌiÌ
vÊiVÌÀV>Ê>`Ê
«ÕÌiÀÊ-ÞÃÌiÃÊ}iiÀ}]Ê>ÃÊ1ÛiÀÃÌÞ°Ê/>ÊÞÕÊÌ
ÌÃiÊÃÌ>vvÊ>`ÊÃÌÕ`iÌÃÊÜÊi«i`ÊLÌÊ`ÀiVÌÞÊ>`Ê`ÀiVÌÞÊÌÊ>iÊÞÊÕÀiÞÊ>
VÕÀvÕ]Ê«À`ÕVÌÛiÊ>`ÊiÞ>LiÊi°ÊÊ>Ê«>ÀÌVÕ>ÀÞÊ}À>ÌivÕÊÌÊÞÊ«ÃÌ}À>`Õ
>ÌiÊÃÕ«iÀÛÃÀÊ>`ÊV>ÕÌÀÊ`Ã>ÞÊii>]ÊÜÃiÊiÝÌÀiiÊ}iiÀÃÌÞÊÜÌÊÌi]
iVÕÀ>}iiÌÊ>`ÊiÝ«iÀÌÃiÊÜiÀiÊLÌÊ>ÊLiiwÌÊÌÊÌÃÊLÊ>`ÊÞÊ«iÀÃ>Ê`iÛi
«iÌÊ>ÃÊ>ÊÀiÃi>ÀVÊÃViÌÃÌ°Ê/ÃÊLÊÜÕ`ÊÌÊ>ÛiÊLiiÊ«ÃÃLiÊÜÌÕÌÊÌi
«>ÌiViÊ>`ÊÃÕ««ÀÌÊvÊÞÊ«>ÀiÌÃ]Ê,Ê>`ÊiÊ/>ÞÀ]Ê>`ÊÞÊÜviÊÌ>]ÊÜ
«ÀÛ`i`ÊÌiÊÃÌ>LiÊiÛÀiÌÊÌ>ÌÊ>Üi`ÊiÊÌÊ«ÕÀÃÕiÊÞÊ`Ài>Ã°
iLÀÕ>ÀÞ ÓääÈ
ivvÀiÞ />ÞÀ


ÌiÌÃ
£
ÌÀ`ÕVÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°
£
£°£ÊÊ ÌÛ>ÌÊ>`Ê
>i}iÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°
{
£°ÓÊÊ 
>«ÌiÀÊ"ÕÌiÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°
È
Ó
Õ`>ÌÃÊvÊ6ÃÕ>Ê*iÀVi«ÌÊ>`Ê
ÌÀÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ ££
Ó°£ÊÊ >Ìi>ÌV>Ê*Ài>ÀiÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ ££
Ó°£°£ÊÊ }iiÕÃÊ>`Ê}iiÕÃÊ6iVÌÀÃÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ ££
Ó°£°ÓÊÊ 
À`>ÌiÊÀ>iÃÊ>`Ê}iiÕÃÊ/À>ÃvÀ>ÌÃÊÊ°Ê°Ê°Ê°ÊÊ £Ó
Ó°£°ÎÊÊ ,i«ÀiÃiÌ>ÌÊvÊÎÊ"ÀiÌ>ÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ £Î
Ó°£°{ÊÊ *ÃiÊ>`Ê6iVÌÞÊ-VÀiÜÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ £x
Ó°ÓÊÊ -iÃÀÊ`iÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ £È
Ó°Ó°£ÊÊ *iÊ
>iÀ>Ê`iÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°
£È
Ó°Ó°ÓÊÊ ,>`>ÊÃÌÀÌÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ £Ç
Ó°Ó°ÎÊÊ VÌÛiÊ-ÌiÀiÊ
>iÀ>Êi>`ÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ £n
Ó°Ó°{ÊÊ ,iVÌi>ÀÊ-ÌiÀiÊ>`Ê*ÀiVÌÛiÊ,iVÌwV>ÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ Ó£
Ó°ÎÊÊ ÀÊ>}iÃÊÌÊ*iÀVi«Ì°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ ÓÓ
Ó°Î°£ÊÊ }Ì>Ê>}iÊ*ÀViÃÃ}Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ ÓÓ
Ó°Î°ÓÊÊ /iÊ,iÊvÊLÃÌÀ>VÌ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ ÓÎ
Ó°{ÊÊ ÀÊ*iÀVi«ÌÊÌÊ
ÌÀÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°
Óx
Ó°{°£ÊÊ i>À}	>Ãi`ÊiÌ`ÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ ÓÈ
Ó°{°ÓÊÊ 6ÃÕ>Ê-iÀÛ}°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ ÓÈ
Ó°{°ÎÊÊ -Õ>ÀÞÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ Îä
Î
->«iÊ,iVÛiÀÞÊ1Ã}Ê,LÕÃÌÊ}ÌÊ-ÌÀ«iÊ-V>}Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ Î£
Î°£ÊÊ 
ÛiÌ>Ê}ÌÊ-ÌÀ«iÊ,>}}Ê>`Ê,i>Ìi`Ê7ÀÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ ÎÓ
Î°ÓÊÊ ,LÕÃÌÊ-ÌiÀiÃV«VÊ}ÌÊ-ÌÀ«iÊ-iÃ}ÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ Î{
Î°Ó°£ÊÊ *ÀLiÊ-Ì>ÌiiÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ Î{
Î°Ó°ÓÊÊ -ÞÃÌiÊ`iÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ ÎÇ
Î°Ó°ÎÊÊ iiÀ>âi`Ê>}iÊ*>iÊÀÀÀÊÕVÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ În
Î°Ó°{ÊÊ -«iV>Ê
>Ãi\Ê,iVÌi>ÀÊ-ÌiÀiÊ>`Ê*iÊ
>iÀ>ÃÊÊ°Ê°Ê°Ê°ÊÊ {ä
Î°Ó°xÊÊ >ÃiÀÊ*>iÊÀÀÀÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ {Ó
Î°Ó°ÈÊÊ ``Ì>Ê
ÃÌÀ>ÌÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ {Î
Î°ÎÊÊ VÌÛiÊ
>LÀ>ÌÊvÊ-ÞÃÌiÊ*>À>iÌiÀÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ {{

86

ÌiÌÃ
Î°{ÊÊ «iiÌ>ÌÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ {È
Î°{°£ÊÊ }ÌÊ-ÌÀ«iÊi>ÃÕÀiiÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ {Ç
Î°{°ÓÊÊ ,>}iÊ>Ì>Ê*ÃÌ*ÀViÃÃ}ÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ {n
Î°xÊÊ Ý«iÀiÌ>Ê,iÃÕÌÃÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ {
Î°x°£ÊÊ ,LÕÃÌiÃÃÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ {
Î°x°ÓÊÊ ÀÀÀÊ>ÞÃÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ xÓ
Î°ÈÊÊ ÃVÕÃÃÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ x{
Î°ÇÊÊ -Õ>ÀÞÊ>`Ê
VÕÃÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ xÈ
{
ÎÊ"LiVÌÊ`i}Ê>`Ê
>ÃÃwV>ÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ xÇ
{°£ÊÊ ÌÛ>ÌÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ xn
{°ÓÊÊ /iÊ>ÕÃÃ>Ê>}iÊÊ
«ÕÌiÀÊ6ÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ x
{°ÎÊÊ -i}iÌ>ÌÊ}ÀÌ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ È£
{°{ÊÊ  «>À>iÌÀVÊ-ÕÀv>ViÊ/Þ«iÊ
>ÃÃwV>Ì °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ ÈÓ
{°{°£ÊÊ *ÀV«>Ê
ÕÀÛ>ÌÕÀiÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ ÈÎ
{°{°ÓÊÊ 
ÛiÝÌÞÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ ÈÈ
{°xÊÊ ÌÌ}ÊiiÌÀVÊ*ÀÌÛiÃÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ ÈÇ
{°x°£ÊÊ *>iÃ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ Èn
{°x°ÓÊÊ -«iÀiÃÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ Èn
{°x°ÎÊÊ 
Þ`iÀÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ È
{°x°{ÊÊ 
iÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ Çä
{°ÈÊÊ "LiVÌÊ`i}Ê>`Ê
>ÃÃwV>ÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ Ç£
{°È°£ÊÊ `i}Ê>Ê	ÝÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ ÇÎ
{°È°ÓÊÊ `i}Ê>Ê
Õ«É	ÜÉ
>Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ Ç{
{°È°ÎÊÊ `i}Ê>Ê	>Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ Çx
{°ÇÊÊ Ý«iÀiÌ>Ê,iÃÕÌÃÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ Çx
{°Ç°£ÊÊ -i}iÌ>ÌÊ>`Ê"LiVÌÊ`i}ÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ Çx
{°Ç°ÓÊÊ 
ÕÀÛ>ÌÕÀi	>Ãi`Ê6iÀÃÕÃÊ «>À>iÌÀVÊ-ÕÀv>ViÊ/Þ«i

>ÃÃwiÀÃÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ nä
{°nÊÊ ÃVÕÃÃÊ>`Ê
VÕÃÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ nÓ
x
ÕÌVÕiÊÎÊ`i	>Ãi`Ê"LiVÌÊ/À>V}Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ nx
x°£ÊÊ ÌÀ`ÕVÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ nÈ
x°ÓÊÊ -ÞÃÌiÊ"ÛiÀÛiÜÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ n
x°ÎÊÊ >>ÊÌiÀÊÀ>iÜÀÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ £
x°{ÊÊ i>ÌÕÀiÊi>ÃÕÀiiÌÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ Ó
x°{°£ÊÊ 
ÕÀÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ Ó
x°{°ÓÊÊ `}iÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ {
x°{°ÎÊÊ /iÝÌÕÀiÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ Ç
x°xÊÊ «iiÌ>ÌÊ>`ÊÝ«iÀiÌ>Ê,iÃÕÌÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£ä£
x°x°£ÊÊ -iµÕiViÊ£\Ê*ÀÊ6ÃÕ>Ê
`ÌÃÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£äÓ
x°x°ÓÊÊ -iµÕiViÊÓ\Ê"VVÕÃÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£äx
x°x°ÎÊÊ -iµÕiViÊÎ\Ê,Ì>ÌÊLÕÌÊÝÃÊvÊ-ÞiÌÀÞÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£ä
x°x°{ÊÊ -iµÕiViÊ{\Ê}ÌÊ
«iÃ>ÌÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£ä
x°ÈÊÊ ÃVÕÃÃÊ>`Ê
VÕÃÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£££


ÌiÌÃ
86
È
ÞLÀ`Ê*ÃÌ	>Ãi`Ê6ÃÕ>Ê-iÀÛ}ÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ££x
È°£ÊÊ ÌÀ`ÕVÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ££È
È°ÓÊÊ 6ÃÕ>Ê-iÀÛÊ
ÌÀiÀÊ"ÛiÀÛiÜÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ££n
È°ÎÊÊ 6ÃÕ>Êii`L>VÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£Óä
È°Î°£ÊÊ À««iÀÊ`iÊÜÌÊVÌÛiÊ6ÃÕ>Ê
ÕiÃÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£Óä
È°Î°ÓÊÊ *ÃiÊÃÌ>ÌÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£Ó£
È°Î°ÎÊÊ 
«iÃ>ÌÊvÊ
>LÀ>ÌÊÀÀÀÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£ÓÎ
È°{ÊÊ i>ÌVÊii`L>VÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£ÓÈ
È°xÊÊ ÕÃÊvÊ6ÃÕ>Ê>`Êi>ÌVÊii`L>VÊÜÌÊ-iv
>LÀ>ÌÊÊ°Ê°Ê°ÊÊ£ÓÇ
È°ÈÊÊ «iiÌ>ÌÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£Îä
È°È°£ÊÊ >}iÊ*ÀViÃÃ}Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£Î£
È°È°ÓÊÊ Ì>â>ÌÊ>`Ê
>LÀ>ÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£Î{
È°ÇÊÊ Ý«iÀiÌ>Ê,iÃÕÌÃÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£Îx
È°Ç°£ÊÊ *ÃÌ}ÊVVÕÀ>VÞÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£ÎÈ
È°Ç°ÓÊÊ /À>V}Ê,LÕÃÌiÃÃÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£În
È°Ç°ÎÊÊ vviVÌÊvÊ
>iÀ>Ê
>LÀ>ÌÊÀÀÀÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£Î
È°nÊÊ ÃVÕÃÃÊ>`Ê
VÕÃÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£{Ó
Ç
-ÞÃÌiÊÌi}À>ÌÊ>`ÊÝ«iÀiÌ>Ê,iÃÕÌÃÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£{x
Ç°£ÊÊ Ý«iÀiÌ>Ê>`ÞiÊ,LÌÊ*>ÌvÀÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£{È
Ç°£°£ÊÊ >À`Ü>ÀiÊ
w}ÕÀ>ÌÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£{È
Ç°£°ÓÊÊ -vÌÜ>ÀiÊÀVÌiVÌÕÀiÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£{
Ç°ÓÊÊ />ÃÊ-«iVwV>ÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£xä
Ç°ÎÊÊ />ÃÊ*>}ÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£x£
Ç°{ÊÊ Ý«iÀiÌÊ£\ÊÀ>Ã«}Ê>Ê1ÜÊ"LiVÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£xÓ
Ç°{°£ÊÊ «iiÌ>ÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£xÎ
Ç°{°ÓÊÊ ,iÃÕÌÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£x{
Ç°xÊÊ Ý«iÀiÌÊÓ\Ê*ÕÀ}Ê/>ÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£xx
Ç°x°£ÊÊ «iiÌ>ÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£xÇ
Ç°x°ÓÊÊ ,iÃÕÌÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£xn
Ç°ÈÊÊ Ý«iÀiÌÊÎ\ÊÕÌ-iÃÀÊ-ÞiÀ}ÞÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£È£
Ç°È°£ÊÊ «iiÌ>ÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£ÈÓ
Ç°È°ÓÊÊ ,iÃÕÌÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£Èx
Ç°ÇÊÊ ÃVÕÃÃÊ>`Ê
VÕÃÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£È
n
-Õ>ÀÞÊ>`ÊÕÌÕÀiÊ7ÀÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£Ç£
n°£ÊÊ ÃÜiÀ}ÊÌiÊ
>i}iÊvÊ-iÀÛViÊ,LÌVÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£Ç£
n°ÓÊÊ ÊÕ`>ÌÊvÀÊÌiÊÕÌÕÀiÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£Ç{

VÌÛiÊ-ÌiÀiÊi>`Ê
>LÀ>ÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£ÇÇ
	
}ÌÊ-ÌÀ«iÊ6>`>ÌÊ>`Ê,iVÃÌÀÕVÌÊÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£Ç
	°£ÊÊ "«Ìâ>ÌÊvÊ}ÌÊ*>iÊÀÀÀÊÕVÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°ÊÊ£Ç
	°ÓÊÊ "«Ì>Ê,iVÃÌÀÕVÌÊvÀÊ,iVÌi>ÀÊ-ÌiÀiÊ>`Ê*iÊ
>iÀ>Ã£nä

86

ÌiÌÃ

ÌiÀ>Ìi`ÊÝÌi`i`Ê>>ÊÌiÀÊ °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £nx

-ÌiÀiÊ,iVÃÌÀÕVÌÊÀÀÀÊ`iÃÊ °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £n
°£Ê "«Ì>Ê,iVÃÌÀÕVÌÊvÊ>Ê-}iÊ*ÌÊ °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £n
°ÓÊ ÀÀÀÊ`iÊvÀÊ,iVÃÌÀÕVÌÊvÊ>Ê-}iÊ*ÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê ££
°ÎÊ ÀÀÀÊ`iÊvÀÊ*ÃiÊÃÌ>ÌÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £Î


>LÀ>ÌÊvÊ-ÞÃÌiÊ>ÌiViÃÊ °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £x

/>ÃÊ*>}Ê °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £
°£Ê À>Ã«}Ê>Ê	ÝÊ °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £
°ÓÊ À>Ã«}Ê>Ê
Õ«Ê °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê Óä£
°ÎÊ /À>iVÌÀÞÊ*>}Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê ÓäÎ
°{Ê *Õ>Êi>ÌVÊ`iÊ °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê Óä{
,iviÀiViÃÊ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê ÓäÇ
`iÝÊ °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê Ó£Ç

ÃÌ v }ÕÀiÃ
£°£
-iÀÛVi ÀLÌÃ v Ìi «>ÃÌ] «ÀiÃiÌ >` vÕÌÕÀi° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
Ó
£°Ó
	V `>}À> v «À«Ãi` vÀ>iÜÀ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
Ç
Ó°£
/À>ÃvÀ>Ì v > «Ì LiÌÜii VÀ`>Ìi vÀ>iÃ° ° ° ° ° ° ° ° ° ° ° ° ° °
£Î
Ó°Ó
*i V>iÀ> `i° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
£È
Ó°Î
,>`> `ÃÌÀÌ >` VÀÀiVÌ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
£n
Ó°{
,iVÃÌÀÕVÌ v > «Ì vÀ ÃÌiÀi i>ÃÕÀiiÌÃ ° ° ° ° ° ° ° ° ° ° ° ° ° °
£
Ó°x

À`>Ìi vÀ>iÃ >ÃÃV>Ìi` ÜÌ Ìi >VÌÛi ÛÃ «>ÌvÀ°° ° ° ° ° ° °
Óä
Ó°È

«>ÀÃ v «ÃÌL>Ãi` >` >}iL>Ãi` ÛÃÕ> ÃiÀÛ}° ° ° ° °
Ón
Î°£

ÛiÌ> Ã}iV>iÀ> }Ì ÃÌÀ«i ÃiÃÀ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
ÎÎ
Î°Ó
6>`>ÌÉÀiVÃÌÀÕVÌ «ÀLi° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
Îx
Î°Î
6>À>Ì v ÀiVÃÌÀÕVÌ iÀÀÀ ÜÌ `i«Ì° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
ÎÈ
Î°{
}Ì ÃÌÀ«i V>iÀ> ÃÞÃÌi `i° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
ÎÇ
Î°x
*ÃÃLi «ÃÌÃ v Ìi }Ì «>i vÀ Ìi ÃV> v > «>>À
V>LÀ>Ì Ì>À}iÌ°° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
{x
Î°È
Ý«iÀiÌ> ÃÌiÀiÃV«V }Ì ÃÌÀ«i ÃV>iÀ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
{È
Î°Ç
/ÀiÃ`Ã vÀ ÀLÕÃÌ iÝÌÀ>VÌ v ÕÌ`> «ÕÃiÃ° ° ° ° ° ° ° ° ° ° ° ° °
{Ç
Î°n
,iÛ> v «ÕÃi Ãi >` iÃ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
{n
Î°
,LÕÃÌ ÃV>} iÝ«iÀiÌ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
xä
Î°£ä -}iV>iÀ> ÀiÃÕÌÃ  Ìi «ÀiÃiVi v ÃiV`>ÀÞ ÀiyiVÌÃ° ° ° ° ° ° °
xä
Î°££ ÕLiV>iÀ> ÀiÃÕÌÃ  Ìi «ÀiÃiVi v ÃiV`>ÀÞ ÀiyiVÌÃ° ° ° ° ° °
x£
Î°£Ó ,LÕÃÌ ÃV>iÀ ÀiÃÕÌÃ  Ìi «ÀiÃiVi v ÃiV`>ÀÞ ÀiyiVÌÃ° ° ° ° ° °
x£
Î°£Î ÃÌÀLÕÌ v ÀiÃ`Õ> ÀiVÃÌÀÕVÌ iÀÀÀÃ  Ìi >}i «>i°° ° ° °
xÎ
Î°£{ }Ì ÃÌÀ«i ÃV> v > }Þ «Ãi` LiVÌ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
xx
{°£
/i >ÕÃÃ> >}i v > VÞ`iÀ°° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
Èä
{°Ó
-ÕÀv>Vi ÌÞ«iÃ V>ÃÃwi` LÞ V> Ã>«i° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
ÈÎ
{°Î
>ÕÃÃ> >}iÃ vÀ V>À>VÌiÀÃÌV ÃÕÀv>Vi Ã>«iÃ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
È{
{°{
i>ÃÕÀiiÌ v À> ÛiVÌÀ `ÃÌÀLÕÌ vÀ > VÞ`iÀ° ° ° ° ° ° ° ° ° ° °
Èx
{°x
i>ÃÕÀ} Ìi VÛiÝÌÞ LiÌÜii ÌÜ ÃÕÀv>Vi «ÌÃ ° ° ° ° ° ° ° ° ° ° ° ° ° °
ÈÈ

88Ê
ÃÌÊvÊ}ÕÀiÃ
{°ÈÊ *>À>iÌiÀâ>ÌÊvÊ>ÊVÞ`iÀ°°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê
È
{°ÇÊ *>À>iÌiÀâ>ÌÊvÊ>ÊVi°Ê °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê
Çä
{°nÊ ->«iÊLiVÌÃÊvÀÊÃViiÊ>>ÞÃÃÊ>`ÊLiVÌÊ`i}°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê
ÇÓ
{°Ê 
>VÕ>Ì}ÊÌiÊ«ÃÌÊvÊ>Ê``iÊv>ViÊvÀÊi`}iÃÊvÊ>`>ViÌÊv>ViÃ°Ê Ç{
{°£äÊ ,iÃÕÌÃÊvÀÊLÝ]ÊL>Ê>`ÊVÕ«°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê
ÇÈ
{°££Ê ,iÃÕÌÃÊvÀÊLÜ]ÊvÕiÊ>`Ê}LiÌ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê
Çn
{°£ÓÊ ,iÃÕÌÃÊvÀÊLÌÌiÊ>`ÊLÜ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê
Ç
{°£ÎÊ ,iÃÕÌÃÊvÀÊ«ÞÀ>`°Ê °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê
n£
{°£{Ê 
ÕÀÛ>ÌÕÀiL>Ãi`ÊÃÕÀv>ViÊÌÞ«iÊÀiÃÕÌ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê
nÓ
x°£Ê 
>«ÌÕÀi`Ê>}i]Ê«Ài`VÌi`Ê«ÃiÊ>`ÊÀi}ÊvÊÌiÀiÃÌ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê
ä
x°ÓÊ *Ài`VÌi`Ê>««i>À>ViÊvÊLiVÌ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê
ä
x°ÎÊ 
ÕÀÊVÕiÊÌ>â>ÌÊ>`Êi>ÃÕÀiiÌÊvÊVÕÀÊViÌÀ`°Ê °Ê°Ê°Ê°Ê°Ê°Ê
Î
x°{Ê `}iÊ`iÌiVÌÊ>`Ê>ÌV}°Ê °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê
È
x°xÊ /iÝÌÕÀiÊvi>ÌÕÀiÊÃiiVÌÊ>`Ê>ÌV}°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê
n
x°ÈÊ 
ÃÌÀÕVÌÊvÊv>ViÌÊVVÕ«>VÞÊV`Ì°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £ää
x°ÇÊ -iiVÌi`ÊvÀ>iÃÊvÀÊLÝÊÌÀ>V}ÊÃiµÕiVi°Ê °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £äÎ
x°nÊ *iÀvÀ>ViÊvÊÕÌVÕiÊÌÀ>ViÀÊÊLÝÊÌÀ>V}ÊÃiµÕiVi°Ê °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £ä{
x°Ê >ÕÀiÊvÊÃ}iVÕiÊÌÀ>V}ÊwÌiÀÃ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £äx
x°£äÊ -iiVÌi`ÊvÀ>iÃÊvÀÊVVÕ`i`ÊLÝÊÌÀ>V}ÊÃiµÕiVi°Ê °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £äÈ
x°££Ê *iÀvÀ>ViÊvÊÕÌVÕiÊÌÀ>ViÀÊÊVVÕ`i`ÊLÝÊÌÀ>V}ÊÃiµÕiVi°Ê £äÇ
x°£ÓÊ *iÀvÀ>ViÊvÊÌiÝÌÕÀiÊÞÊÌÀ>ViÀÊÊÌiÊ«ÀiÃiViÊvÊVVÕÃÃ°Ê°Ê°Ê°Ê £än
x°£ÎÊ *iÀvÀ>ViÊvÊi`}iÊÞÊÌÀ>ViÀÊÊÌiÊ«ÀiÃiViÊvÊVVÕÃÃ°Ê°Ê°Ê°Ê°Ê°Ê £än
x°£{Ê -iiVÌi`ÊvÀ>iÃÊ­ivÌÊV>iÀ>ÊÞ®ÊvÀÊVvviiÊÕ}ÊÌÀ>V}ÊÃiµÕiVi°££ä
x°£xÊ "LÃiÀÛi`ÊÀiÌ>ÌÊvÊÕ}ÊvÀÊÃ}iÊ>`ÊÕÌVÕiÊÌÀ>ViÀÃ°°Ê°Ê°Ê°Ê°Ê°Ê £££
x°£ÈÊ -iiVÌi`ÊvÀ>iÃÊvÀÊi}ÌÊÌÀ>V}ÊÃiµÕiVi°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê ££Ó
È°£Ê iwÌÊvÊÛÃÕ>ÊÃiÀÛ}ÊÌ>ÃÊ °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê ££
È°ÓÊ VÌÛiÊÊvi>ÌÕÀiÃÊ>`Ê>ÀÌVÕ>Ìi`Ê`iÊvÀÊÛÃÕ>Ê}À««iÀÊÌÀ>V}°Ê£Ó£
È°ÎÊ 	VÊ`>}À>ÊvÊÞLÀ`Ê«ÃÌL>Ãi`ÊÛÃÕ>ÊÃiÀÛ}ÊVÌÀÊ«°Ê £Îä
È°{Ê iÌiVÌÊvÊÊV>``>ÌiÃÊÛ>ÊVÕÀÊwÌiÀ}°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £ÎÓ
È°xÊ -Û}ÊÌiÊ>ÃÃV>ÌÊLiÌÜiiÊ«Ài`VÌi`Ê>`Êi>ÃÕÀi`Êvi>ÌÕÀiÃ°Ê°Ê°Ê°Ê £ÎÎ
È°ÈÊ Ý«iÀiÌ>ÊÌ>ÃÊvÀÊiÛ>Õ>ÌÊvÊÞLÀ`ÊÛÃÕ>ÊÃiÀÛ}°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £Îx
È°ÇÊ *ÃÌ}ÊÌ>ÃÊÀiÃÕÌÊÜÌÊÞLÀ`Ê«ÃÌL>Ãi`ÊÃiÀÛ}°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £ÎÇ
È°nÊ *ÃiÊiÀÀÀÊvÀÊÞLÀ`Ê«ÃÌL>Ãi`ÊÛÃÕ>ÊÃiÀÛ}°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £În
È°Ê *ÃÌ}ÊÌ>ÃÊÀiÃÕÌÊÜÌÊ
ÊVÌÀiÀ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £În
È°£äÊ *ÃÌ}ÊÌ>ÃÊÀiÃÕÌÊÜÌÊ"ÊVÌÀiÀ°°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £În
È°££Ê -iiVÌi`ÊvÀ>iÃÊvÀÊ«ÃÌ}ÊÌ>ÃÊÜÌÊVVÕÃÃÊ>`ÊVÕÌÌiÀ°Ê°Ê°Ê°Ê £{ä
È°£ÓÊ 6ÃÕ>ÊÃiÀÛ}Ê«iÀvÀ>ViÊÜÌÊVVÕÃÃÊ>`ÊVÕÌÌiÀ°Ê °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £{£
È°£ÎÊ *iÀvÀ>ViÊvÊ«À«Ãi`ÊVÌÀiÀÊvÀÊL>ÃiiÊV>LÀ>ÌÊiÀÀÀ°Ê °Ê°Ê°Ê £{Ó
È°£{Ê *iÀvÀ>ViÊvÊ«À«Ãi`ÊVÌÀiÀÊvÀÊÛiÀ}iÊV>LÀ>ÌÊiÀÀÀ°Ê °Ê°Ê°Ê°Ê°Ê £{Ó
Ç°£Ê iÌ>>\Ê>ÊiÝ«iÀiÌ>ÊÕ««iÀÌÀÃÊÕ>`ÊÀLÌ°Ê °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £{Ç
Ç°ÓÊ 	VÊ`>}À>ÊvÊV«iÌÃÊÊiÝ«iÀiÌ>Ê«>ÌvÀ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £{n
Ç°ÎÊ *>ViiÌÊvÊ}À>Ã«ÊvÀ>iÊvÀÊ«ÜiÀÊ>`Ê«ÀiVÃÊ}À>Ã«Ã°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £xÓ

ÃÌÊvÊ}ÕÀiÃ
88
Ç°{Ê -«wi`Ê}À>Ã«Ê>`ÊÌÀ>iVÌÀÞÊ«>}ÊvÀÊ>ÊLÝ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £xÓ
Ç°xÊ ÀÀ>}iiÌÊvÊLiVÌÃÊvÀÊÝ«iÀiÌÊ£°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £xÎ
Ç°ÈÊ ,iyiVÌÃÊ`ÕÀ}Ê}ÌÊÃÌÀ«iÊÃV>}ÊÊ>ÊÃ«iÊ`iÃÌVÊÃVii°Ê °Ê £xx
Ç°ÇÊ -ViiÊ>>ÞÃÃÊvÀÊ}À>Ã«}ÊÌ>Ã°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £xÈ
Ç°nÊ 
ÕÀÊV>ÀÌÃÊvÀÊ`iÌvÞ}ÊÌiÊÞiÜÊLÝ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £xÈ
Ç°Ê -iiVÌi`ÊvÀ>iÃÊvÀÊ}À>Ã«}ÊÌ>Ã°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £xÇ
Ç°£äÊ ÀÀ>}iiÌÊvÊLiVÌÃÊvÀÊÝ«iÀiÌÊÓ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £xn
Ç°££Ê -ViiÊ>>ÞÃÃÊvÀÊ«ÕÀ}ÊÌ>Ã°Ê °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £x
Ç°£ÓÊ À>«V>ÊÌiÀv>ViÊvÀÊÌiÀ>VÌ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £x
Ç°£ÎÊ -iiVÌi`ÊvÀ>iÃÊvÀÊ«ÕÀ}ÊÌ>ÃÊ °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £Èä
Ç°£{Ê `ivviVÌÀÊÌÀ>V}ÊÃÃÊ>ÌÊV«iÌÊvÊ«ÕÀ}ÊÌ>Ã°°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £È£
Ç°£xÊ ``ÌÊvÊ>ÀyÜÊ>`Ê`ÕÀÊÃiÃÀÃÊÌÊiÌ>>ÊvÀÊiÝ«iÀiÌÊÎ°Ê £ÈÓ
Ç°£ÈÊ 7ÃiÀÊÃiÃÀÊiiiÌÊvÀÊi>ÃÕÀ}Ê>ÀyÜ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £ÈÎ
Ç°£ÇÊ 
iV>ÊÃiÃÀÊÀiÃ«ÃiÊÌÊÕ}ÊvÊiÌ>°Ê °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £È{
Ç°£nÊ iwÌÊvÊLi>À}Ê>}iÊ vÊÕ}ÊÜÌÊÀiÃ«iVÌÊÌÊÜ`Ê`ÀiVÌ°Ê°Ê £È{
Ç°£Ê Ý«iÀiÌ>Ê>ÀÀ>}iiÌÊvÀÊiÝ«iÀiÌÊÎ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £ÈÈ
Ç°ÓäÊ 
iV>ÊÃiÃÀÊÀi>`}ÃÊ>`Êi`Ê­iÌ>ÊÊivÌÊVÕ«®°Ê °Ê°Ê°Ê°Ê°Ê°Ê £ÈÇ
Ç°Ó£Ê -ÕVViÃÃvÕÊV«iÌÊvÊÌ>ÃÊvÀÊivÌ>`ÊVÕ«°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £ÈÇ
Ç°ÓÓÊ -ÕVViÃÃvÕÊV«iÌÊvÊÌ>ÃÊvÀÊÀ}ÌÃÌÊVÕ«°Ê °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £Èn
Ç°ÓÎÊ 
iV>ÊÃiÃÀÊÀi>`}ÃÊ>`Êi`Ê­iÌ>ÊÊÀ}ÌÊVÕ«®°Ê°Ê°Ê°Ê°Ê°Ê £Èn
°£Ê 6iÀ}iÊiV>ÃÊvÀÊ	V«ÃÊi>`Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £Çn
°ÓÊ 
>LÀ>ÌÊvÊÛiÀ}iÊ>}i°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £Çn
°£Ê ÞÃÌiÀiÃÃÊ`ÕiÊÌÊ>VµÕÃÌÊ`i>ÞÊLiÌÜiiÊVÀÀiÃ«`}Ê>}iÃ
>`ÊiV`iÀÊi>ÃÕÀiiÌÃÊvÊÌiÊ}ÌÊÃÌÀ«i°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £È
°ÓÊ ,iÃ`Õ>ÊiÀÀÀÊvÀÊiÃÌ>ÌÊvÊÌiÊ`i>ÞÊvÀÊi>ÀÊÀi}ÀiÃÃ°Ê°Ê°Ê°Ê £È
°ÎÊ ,i>ÌÃ«ÊLiÌÜiiÊV>«ÌÕÀi`Ê>}iÃÊ>`ÊiV`iÀÊi>ÃÕÀiiÌÃ
vÊÌiÊ}ÌÊÃÌÀ«iÊ>vÌiÀÊV«iÃ>Ì}ÊvÀÊ>ÌiVÞ°Ê °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê £È
°£Ê
À>Ã«Ê«>}ÊvÀÊ>ÊLÝ°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê Óää
°ÓÊ
À>Ã«Ê«>}ÊvÀÊ>ÊÕ«À}ÌÊVÕ«°Ê °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê ÓäÓ
°ÎÊ
/À>iVÌÀÞÊ«>}ÊvÀÊ>««À>V}ÊÌiÊ}À>Ã«ÊvÊ>ÊLÝÊ>`ÊVÕ«°Ê °Ê°Ê°Ê°Ê ÓäÎ
°{Ê
*Õ>Êi>ÌVÊ`i°Ê °Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê°Ê Óä{

ÃÌ v />LiÃ
Î°£
ÛiÀ>}i >` Û>À>Vi v i>ÃÕÀiiÌÃ >` ÃÞÃÌi «>À>iÌiÀÃ] >`
VÌÀLÕÌ Ì ÀiVÃÌÀÕVÌ iÀÀÀ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
xÎ
{°£
-ÕÀv>Vi ÌÞ«i V>ÃÃwV>Ì ÀÕiÃ ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° °
ÈÇ
{°Ó

«ÕÌ>Ì> iÝ«iÃi v > ÃÌi«Ã  Ãi}iÌ>Ì° ° ° ° ° ° ° ° ° ° ° ° °
ÇÇ
x°£
/Þ«V> «ÀViÃÃ} ÌiÃ vÀ V«iÌÃ v Ìi ÕÌVÕi ÌÀ>V}
>}ÀÌ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° £äÓ
È°£

«>ÀÃ v «ÃÌ} >VVÕÀ>VÞ vÀ iÝ«iÀiÌ> ÛÃÕ> ÃiÀÛ
VÌÀiÀÃ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° ° £Î

ÃÌ v VÀÞÃ



«ÕÌiÀ `i` iÃ}




>À}i
Õ«i` iÛVi
"
i}ÀiiÃ v Àii`


`«Ì 
Ãi`«

ÝÌi`i` >ÕÃÃ> >}i
"
`«Ì "«i«

ÌiÀ>Ìi` ÝÌi`i` >> ÌiÀ

}Ì ÌÌ} `i

iÛiLiÀ}>ÀµÕ>À`Ì
8
ÕÌi`> ÝÌiÃÃ
*

*ÀV«> 
«iÌÃ >ÞÃÃ
,	
,i` Àii 	Õi
,"
,i} "v ÌiÀiÃÌ
-
-}i ÃÌÀÕVÌ] ÕÌ«i >Ì>
--
-Õ v -µÕ>Ài` vviÀiVi
--
-ÌÀi>} - ÝÌiÃÃ
6,
6ÀÌÕ> ,i>ÌÞ >ÀÕ« >}Õ>}i

1
Introduction
If the early parallels between computers and robots hold up, we can
expect to see ... killer applications for robots in the next ten to ﬁfteen years,
and by the year 2020 robots will be pervasive in our lives.
– Rodney Brooks, Flesh and Machines
Since the middle of last century, technologists have conﬁdently predicted the im-
minent revolution in domestic and service robotics. We are told that intelligent robots
will begin to appear in guises such as cars, vacuum cleaners, and even humanoids.
These machines will lift the burden of work in our homes and ofﬁces by taking the
role of cleaner, courier, nurse, and security guard among others. Robots will recog-
nize their owners by appearance and voice, and understand natural modes of com-
munication including gestures and speech. We will give simple commands such as
“Set the table” or “Bring the coffee to my desk”, and our service robot will have the
necessary sensing and intelligence to fulﬁll the request. Robotic tele-presence will
allow owners to remotely feed pets, check security and perform other tasks while
away. Robotic aids will improve the quality of life for the elderly and disabled, while
society as a whole will beneﬁt from the reduced demand on health care services.
Eventually, service robots will roam freely around our homes and ofﬁces to help in
whatever way they see ﬁt, driven by an internal emotional model and learning from
their surroundings.
While the above scenario is still a fantasy, the steady decrease in the size and cost
of sensors and computing resources over the past decade has made the possibilities
of service robots seem even closer and more compelling than ever. Robots are sure
to ﬁnd widespread appeal as they continue the trend in home automation that has
been underway for over a century. Appliances such as sewing machines, washing
machines, central heating and kettles are now ubiquitous in our lives. All contain
what may be described in robotic terms as sensors, actuators and signal processors,
and can adapt their operation in a limited sense to environmental conditions. More
importantly, each new technology is readily accepted as a positive contribution to the
standard of living. The primitive precursors of domestic robots have already found a
place the home.
G. Taylor and L. Kleeman: Visual Perception and Robotic Manipulation, STAR 26, pp. 1–10, 2006.
© Springer-Verlag Berlin Heidelberg 2006

2
1 Introduction
Fig. 1.1. Service robots of the past, present and future. Elektro (left) was built by West-
inghouse in 1937 and exhibited as the ultimate appliance, although it was little more than
a complex automaton. Present-day service robots such as the Roomba (centre) from iRo-
bot can perform speciﬁc tasks such as vacuum cleaning autonomously, but have limited ap-
plication. The concept robot Wakamaru (right) from Mitsubishi is designed to develop its
own motivations to help in the daily life of its user, and represents the ﬁrst generation of
universal aids. Elektro image from http://davidszondy.com/future/robot/elektro1.htm. Roomba
image from http://www.frc.ri.cmu.edu/ hpm/talks/Extras/roomba.JPG. Wakamaru image from
http://www.expo2005.or.jp/jp/C0/C3/C3.7/C3.7.5/.
Figure 1.1 provides a perspective on the evolution and continuing trends in do-
mestic service robotics. Present-day commercial service robots generally take the
form of autonomous vacuum cleaners and lawn mowers. These robots are equipped
with a variety of sensors that are used to avoid obstacles, seek out re-charging sta-
tions, and even perform simultaneous localization and mapping in an unknown envi-
ronment. While these capabilities represent signiﬁcant strides in home automation,
modern cleaning robots, like refrigerators and washing machines, are still highly
specialized devices that do not deliver the promise of a universal aid.
In both research and popular culture, humanoid robots have long been regarded
as the ultimate universal aid. The reason is simple: our ofﬁces and homes are nat-
urally designed for humans, and robots in humanoid form can effectively exploit
existing infrastructure. The humanoid form also facilitates human-machine interac-
tion, allowing people to communicate and work cooperatively and intuitively with
their robotic companions. Our knowledge and intuition about how humans reason
and interact with the world will boot-strap the development of parallel humanoid ro-
bot capabilities. Any activity that can be performed by a person, such as climbing a
ladder and squeezing through a man hole, can (in principle at least!) be mimicked
by a humanoid robot. Unhindered by the constraints of biology, humanoid robots
will also develop “super-human” skills. Sensory information such as sonar, struc-

1 Introduction
3
tured light, night vision, or cameras on each ﬁngertip will usher as yet unimagined
possibilities and challenge our view of human-like intelligence.
In anticipation of this ultimate trend in home automation, humanoid robots have
recently started to progress from academia into the commercial world. HOAP-1 is a
small-scale (48 cm tall) humanoid robot developed by Fujitsu1 and marketed towards
researchers and developers. In contrast, Sony’s more recent Qrio2 is described as an
entertainment robot. The ambitious medium-scale wheeled humanoid robot known
as Wakamaru from Mitsubishi3 is intended to provide practical domestic services
such as security, health care, access to information and tele-presence. Toyota’s Part-
ner Robots4 are intended to provide similar functionality. Along with the more well
known humanoid robots such as Asimo from Honda Motor and HRP-2 from Kawada
Industries, these robots may well represent the ﬁrst practical universal aids.
Just as humanoid robots are inspired by familiar forms, the importance of vision
in human perception will likely be mirrored in our service robots. Physical coordina-
tion, navigating through complex environments and recognizing objects and people
are all enhanced by visual information. We take these skills for granted, yet much re-
search remains to be done before we grasp the underlying mechanisms. The science
of computer vision aspires to emulate our ability to extract meaningful information
from images of the world. Taking this goal a step further, robot vision explores how
visual information can be used to drive interactions with the world, by linking per-
ception to action. Robot vision has already been an active area of research for several
decades. Limitations on computing power resulted in early robotic vision systems
being either slow, like Shakey the Robot from Stanford Research Institute5, large,
such as the early Navlab vehicles from Carnegie Mellon University6, or tethered to
off-board computing, such as the keyboard-playing Wabot-2 from Waseda Univer-
sity7. However in more recent years, the exponential increase in available computing
power and decrease in the cost of both computers and cameras has stimulated an ac-
celeration in our understanding of robot vision. Compact, untethered robots that use
real-time visual sensing to navigate complex environments, locate and grasp objects
and interact with humans are now a reality.
Three things are almost certain about universal service robots of the future: many
will have manipulators (and probably legs!), most will have cameras, and almost all
will be called upon to grasp, lift, carry, stack or otherwise manipulate objects in our
environment. Visual perception and coordination in support of robotic grasping is
thus a vital area of research for the progress of universal service robots. With their
imminent arrival, the time is ripe to direct research effort towards the skills that will
facilitate widespread application.
1http://www.automation.fujitsu.com/en/products/products07.html
2http://www.sony.net/SonyInfo/QRIO/top nf.html
3http://www.mhi.co.jp/kobe/wakamaru/english/
4http://www.toyota.co.jp/en/special/robot/
5http://www.sri.com/about/timeline/shakey.html
6http://www.cs.cmu.edu/afs/cs/project/alv/www/index.html
7http://www.humanoid.rise.waseda.ac.jp/booklet/kato02.html

4
1 Introduction
1.1 Motivation and Challenges
The development of general-purpose service robots will require advances in diverse
ﬁelds including mechanics, control, sensing and artiﬁcial intelligence. Signiﬁcant
progress has already been made in many areas including stable bipedal locomo-
tion on uneven terrain [144, 145], the related problems of localization and naviga-
tion [22, 118], mechanical compliance for safe interaction [119], dextrous manip-
ulation [95, 103], social interaction [14, 78], various sensing and actuation modali-
ties and a range of problems in artiﬁcial intelligence. The focus of this book is on
identifying and manipulating unknown objects under visual guidance, which is a
fundamental skill for practical service applications such as assisting the elderly and
disabled. Object recognition and manipulation are classical problems in robotics, and
many techniques already exist for handling unknown objects. However, much work
remains to be done in extending these techniques to address the complexities and
uncertainties of an unprepared domestic environment. This book aims to address this
need by providing a framework for visual perception and manipulation of unknown
objects, with an emphasis on robustness for autonomous service robots in the real
world.
The functional building blocks that enable a robot to perform arbitrary tasks can
be broadly divided into the four areas of human-machine interaction, perception,
intelligence and control. Bidirectional human-machine interaction is necessary to
communicate the requirements of a task to the robot and relate the progress back
to the user. Interaction with service robots will likely to take the form of natural
speech and gestures, or a simple graphical interface in the case of remote operation.
Perception is an essential building block for autonomous behaviour and encompasses
both low-level sensing and high-level abstraction of useful information about the
world. For robotic vacuum cleaners and similar devices, perception may simply drive
reactive behaviours without intervening intelligence. Complex tasks may require a
more sophisticated approach involving the maintenance of a consistent world model,
and associated high-level interpretation to drive planning, prediction and monitoring
of actions. These latter activities are the function of intelligence, which provides the
link between interaction, perception and control.
Robots face many new and complicating challenges when promoted from indus-
trial to domestic applications. Interaction must be intuitive and robust to ambiguous
interpretation. Task speciﬁcations are likely to be ad hoc and therefore incomplete
as not all pertinent information is available. The desired behaviour must be either
inferred by the robot or actively pursued by prompting for further interaction. The
robot may be required to learn new behaviours and condition existing behaviour
based on feedback from the user. Sensing must provide useful measurements over
a wide variety of unpredictable operating conditions. The robot must be capable of
identifying and locating broad classes of objects in cluttered surroundings. Control
of the limbs must be robust to wear and other incidental variations in mechanical
parameters. All of these challenge must be met with real-time operation if the robot
is to interact naturally with humans.

1.1 Motivation and Challenges
5
Addressing all of these challenges is beyond the scope of this book. We therefore
focus on the core building blocks of visual perception and control, which neverthe-
less provide the foundation for domestic tasks involving more sophisticated intelli-
gence and interaction. Within this limited scope, four main areas of difﬁculty can be
identiﬁed: imprecise information about tasks and objects, operation in an unstruc-
tured, cluttered environment, robustness to uncertainty in calibration, and real-time
operation. The working hypothesis is that these speciﬁc challenges can be satisﬁed in
a framework based on visual sensing, in the same manner that vision is the primary
sense for humans. These main challenges are elaborated below:
Imprecise task speciﬁcations and lack of prior knowledge
Conventional industrial robots are characterized by repetitive tasks in a precisely
calibrated workspace. At the other extreme, almost every domestic task differs with
respect to the location and identity of objects, and the required manipulations. Do-
mestic tasks are usually only speciﬁed at a supervisory level with reference to general
actions and classes of objects, as in the simple example: “Please get the cup from the
table.” Lack of prior knowledge affects all stages of sensing, planning and actuation.
Firstly, the robot must be able to recognize and locate a cup without necessarily hav-
ing seen the object before. Visual sensing cannot assume the presence of suitable
colours and textures for accurate measurements. Segmentation, the process of divid-
ing the visual data into meaningful regions, is confounded by the distinction between
adjacent unknown objects and different parts of the same unknown object. The un-
known cup must be distinguished from a soft drink can, honey pot or other similarly
shaped objects that may be present in the scene. Furthermore, the existence of a cup
is not guaranteed if the command was not given in the vicinity of the table. If the cup
is found, the robot must plan and execute a stable grasp without having previously
handled the object. If the cup is not found or the results are ambiguous, the robot
may need to request additional information based on existing sensory data, such as:
“Do you mean the red or blue cup?” Dealing with imprecise knowledge about the
required task drives many design choices in the framework presented in this book.
Robust visual sensing in a cluttered environment
Data association is required in almost all sensing applications, and is the general
problem of determining the identity of primitives extracted from sensor data by relat-
ing features to each other and/or an internal world model. Association errors can lead
to false internal models and, ultimately, catastrophic failure. Any sensing application
can suffer from association errors, and the opportunity for error is particularly acute
in cluttered, dynamic, unpredictable environments such as encountered by domestic
service robots. Visual sensing can be hindered by background colours and textures,
lighting variation, reﬂections, image noise and a variety of other distractions. Robust
data association is thus a signiﬁcant challenge in service applications.
Another effect of operation in a cluttered, unpredictable environment is the pos-
sibility of occlusions and the resulting loss of visual information. This can impact

6
1 Introduction
the ability to recognize objects, update the world model and control limbs using
hand-eye coordination. Perception and control algorithms with high tolerance to oc-
clusions are therefore desirable.
Robustness to operational wear and calibration errors
In the classical solution to robotic reaching and grasping, visual sensing accurately
recovers the metric position of the object, and the end-effector is accurately driven
to the desired location using a kinematic model. Any errors in the calibration of the
sensors or mechanics, which becomes more likely as the robot increases in complex-
ity, can lead to failure of this simple approach. Two sources of variation are readily
identiﬁed: operational wear, and the complexity of calibration. Industrial robots also
suffer from these problems, but the effects are ampliﬁed in domestic applications.
Accidents are a real possibility in an unpredictable environment and accelerate the
process of wear. Constraints on the weight and safety of service robots require light,
compliant structures that are difﬁcult to model kinematically. To ﬁnd widespread ap-
plication, robots must be produced with cheaper sensors and relaxed manufacturing
tolerances. They will also need to operate autonomously for extended periods with-
out costly technical maintenance, while tolerating the effects of wear on sensors and
mechanical components. Overall, the development of perception and control meth-
ods that reduce the reliance on accurate calibration may ultimately lead to robots that
are cheaper, more reliable and require lower maintenance.
Real-time operation
To interact and work cooperatively with humans, service robots must be capable of
operating with comparable reﬂexes, that is, in real-time. Industrial robots are char-
acterized by super-human speed and accuracy. Conversely, visual sensing and plan-
ning algorithms often perform many times slower than a human in tasks such as
scene interpretation and grasp planning. This problem is usually solved using off-
board networks of parallel computers and specialized signal processing hardware.
Unfortunately, the general computing resources available to a practical service robot
are bounded by physical dimensions and power consumption and therefore likely to
be much lower. Some specialized hardware may be present, such as “smart” cam-
eras with simple image processing capabilities. In the long term, the constraints of
real-time operation will progressively weaken as general purpose computers con-
tinue to increase in speed. Real-time operation guides the development of many
algorithms presented in this book, but is usually secondary to the considerations
described above.
1.2 Chapter Outline
The framework presented in this book addresses all stages of the sense-plan-act cycle
for autonomously manipulating unknown objects in a domestic environment. Figure

1.2 Chapter Outline
7
active
vision
hybrid position−based
visual servoing
stereoscopic light
stripe scanner
grasp and
trajectory
planning
model−based
object tracking
3D object modelling
and classification
multi−cue 3D
Fig. 1.2. Block diagram of proposed framework for classifying and manipulating unknown
objects with robustness to uncertainties in a domestic environment.
1.2 illustrates the structure of this framework and serves as a road-map to the con-
cepts and techniques developed in later chapters.
In the proposed framework, perception of the world begins with the stereoscopic
light stripe scanner and object classiﬁcation and modelling blocks working together
to generate data-driven, textured, polygonal models of segmented objects. Object
modelling and classiﬁcation provides the link between low-level sensing and high-
level planning. Unknown objects are classiﬁed into families such as cups, bowls or
boxes based on geometry. This allows the robot to perform ad hoc tasks with pre-
viously unknown objects without a large database of learned models. Colour and
texture information enables objects in the same family to be distinguished from each
other. The perceived world model is maintained using multi-cue 3D model-based
tracking to continuously estimate the state of each object based on texture, colour
and geometric measurements. Tracking is necessary even for static objects to com-
pensate for camera motion and detect collisions or unstable grasps. Finally, the de-
sired manipulation is performed by controlling the end-effector using visual feedback
in a hybrid position-based visual servoing framework. During servoing, active vision
controls the gaze direction to maximize available visual information.
The following chapters ﬁll in the details of the framework sketched above, start-
ing from low-level sensing through to high-level perception and control, the ﬁnally
concluding with experimental results to illustrate the integration of the entire frame-
work. A brief outline of each chapter is presented in the following sections. In ad-
dition, the accompanying Multimedia Extensions CD-ROM provides videos, VRML
data, C++ code and data sets to supplement this monograph.

8
1 Introduction
Chapter 2: Foundations of Visual Perception and Control
Chapter 2 establishes a foundation of concepts, notational conventions and system
models that will form the basis of algorithms and analyses in later chapters. Topics
covered include homogeneous vectors and transformations, the projective camera
model, kinematics of an active stereo camera head, digital image processing, and a
survey of related work in visual perception and control for robotic manipulation.
Chapter 3: Shape Recovery Using Robust Stereoscopic Light Stripe Scanning
Chapter 3 describes the stereoscopic light stripe scanner block in ﬁgure 1.2. Acquir-
ing depth information is a fundamental ﬁrst step in building the 3D world model
used to locate and classify objects of interest. Light stripe scanning is an active
triangulation-based ranging technique that provides greater accuracy than related
passive methods and has been used in robotics and computer vision for several
decades. However, almost all conventional scanners are designed to operate under
controlled conditions and cannot cope with the reﬂections, clutter, cross-talk or ob-
jects with unknown geometries, textures and surface properties that characterize a
service robot’s environment. This chapter reviews conventional light stripe scanning
and then shows how a robust stereoscopic scanner can be designed to overcome
these problems. Robustness is achieved by exploiting redundancy and geometric
constraints to identify true measurements in the presence of noise. Validation, 3D re-
construction and auto-calibration techniques are derived, and implementation details
including image processing are presented. Experimental results validate the system
model and demonstrate the improvement over conventional techniques.
Chapter 4: 3D Object Modelling and Classiﬁcation
To minimize prior knowledge, a service robot would beneﬁt from the ability to clas-
sify unknown objects into known families such as cups, plates and boxes. The clas-
siﬁcation framework presented in Chapter 4 is based on modelling objects as collec-
tions of geometric primitives (planes, cylinders, cones and spheres). The primitives
are extracted automatically from range data using a segmentation algorithm based on
surface type classiﬁcation. The classiﬁer uses principal curvatures (from Gaussian
image analysis) and convexity to determine the local shape of range patches. Geo-
metric primitives are ﬁtted to connected segments of homogeneous surface type,
and an iterative reﬁnement attempts to merge as many segments as possible without
degrading the ﬁtted models. Finally, the algorithm constructs an attributed graph ex-
pressing the relationship between segmented primitives, and families of objects are
recognized using sub-graph matching. The segmentation, surface type classiﬁcation,
model-ﬁtting and graph matching algorithms are described in detail, and experimen-
tal results are presented for several scenes. In particular, the non-parametric surface
type classiﬁer described in this chapter is shown to achiever greater robustness to
noise than conventional classiﬁers without additional computational expense.

1.2 Chapter Outline
9
Chapter 5: Multi-cue 3D Model-Based Object Tracking
To close the visual feedback loop, a 3D model-based tracking algorithm continu-
ously reﬁnes the pose of objects extracted from range data. Model-based tracking
algorithms typically choose a particular cue such as edges or texture to locate the
tracked target in a captured video. Unfortunately, the failure mode of any particular
cue is almost certain to arise at some time in the unpredictable lighting and clutter
of a domestic environment, making single-cue trackers unsuitable for service robots.
Chapter 5 shows how multi-cue 3D model-based tracking overcomes this problem
based on the notion that different cues exhibit complementary failure modes and are
thus unlikely to fail simultaneously. The multi-cue tracker fuses intensity edges, tex-
ture and colour cues from stereo cameras in an extensible Kalman ﬁlter framework.
Image processing, feature association and measurement models for each cue are de-
scribed in detail, along with the Kalman ﬁlter implementation. Experimental results
demonstrate the robustness of multi-cue 3D model-based tracking in the presence
of visual conditions that otherwise cause single-cue trackers to fail, including low
contrast, lighting variations, motion blur and occlusions.
Chapter 6: Hybrid Position-Based Visual Servoing
Chapter 6 addresses the problem of robust visual control of robotic manipulators
for service applications. Traditional look then move control requires accurate kine-
matic and camera calibration, which is unrealistic for service robots. Position-based
visual servoing is a feedback control technique in which the end-effector and target
are continuously observed to minimize the effect of kinematic uncertainty. How-
ever, conventional position-based visual servoing is limited by a reliance on accurate
camera calibration for unbiased pose estimation, and fails completely when the end-
effector is obscured. This chapter presents an error analysis of the camera model in
visual servoing to show that the pose bias can be corrected by introducing a sim-
ple scale factor. Furthermore, the problem of visual obstructions can be overcome
by fusing visual and kinematic measurements in a new framework called hybrid
position-based visual servoing. Kinematic measurements allow servoing to continue
when the end-effector is obscured, while visual measurements minimize the uncer-
tainty in kinematic calibration. A Kalman ﬁlter framework for estimating the visual
scale factor and fusing visual and kinematic measurements is described, along with
image processing, feature association and other implementation details. Experimen-
tal results demonstrate the improved accuracy and robustness of the hybrid approach
compared to conventional servoing.
Chapter 7: System Integration and Experimental Results
In Chapter 7, the components developed in previous chapters are integrated into a
complete sense-plan-action framework for service robots. The complete framework
is implemented on an experimental upper-torso humanoid robot, and three simple
domestic tasks are designed to evaluate the performance of the system. The ﬁrst

10
1 Introduction
task requires the robot to fetch an object speciﬁed only by colour and family, the
second task requires the robot to grasp an interactively selected cup and pour the
contents into a bowl, and the ﬁnal task requires the robot to identify and grasp a cup
of ethanol based on airﬂow and chemical sensor readings. Implementation details
of the humanoid robot, including simple human-machine interaction, grasp planning
and task planning components are provided.
Appendices
Appendix A describes the mechanical properties and calibration procedure for the
Biclops active stereo head on the humanoid service robot used for experiments
in this book.
Appendix B details the analytical models used in the development of the stereo-
scopic light stripe scanner described in Chapter 3.
Appendix C provides details of the Iterated Extended Kalman Filter used by the
model-based tracking algorithms developed in Chapters 6 and 5.
Appendix D details the stereo reconstruction error models used in the development
of the hybrid position-based visual servo controller in Chapter 6.
Appendix E describes a simple calibration procedure to determine the various ac-
quisition and actuation delays between inputs and outputs of the system.
Appendix F details the simple task planning algorithms used in the implementation
of the experimental service robot in Chapter 7.

2
Foundations of Visual Perception and Control
The purpose of this chapter is to provide a brief overview of robotic and computer
vision concepts, algorithms and mathematical tools that form the basis of the frame-
work developed in the remainder of this book. Section 2.1 lays the groundwork with
a brief discussion of homogeneous vectors, transformations, and representations of
rotation and velocity typically used to model robotic and computer vision systems.
Section 2.2 builds on this foundation with descriptions of models for the common
components of a 3D robotic vision system, namely the imaging sensor and active
stereo camera head. Finally, Sections 2.3 and 2.4 paint a broad outline of issues and
methods in visual perception and control as they relate to robotic manipulation. In
particular, we discuss how the choice of representation in perceptual models affects
the capabilities of a visual system, and provide a survey of recent work in visual
servoing.
2.1 Mathematical Preliminaries
2.1.1 Homogeneous and Inhomogeneous Vectors
Most geometric models in robotic vision systems involve transformations within and
between three dimensional Euclidean space R3 and the two dimensional space R2 of
the image plane. To distinguish these spaces, we adopt the convention of labelling
vectors in R3 using upper-case bold symbols such as X and vectors in R2 using lower-
case bold, as in x. Analytical models can be cumbersome in Euclidean space since
rotations, translations and camera projections are described by different linear and
non-linear transformations. Under the alternative framework of projective geometry,
all coordinate transformations are uniﬁed as linear matrix multiplications, known as
homogeneous or projective transformations (see next section). Projective geometry
is therefore used extensively in place of Euclidean geometry to model robotic and
computer vision systems.
To apply homogeneous transformations, a point in Rn is replaced by an equiv-
alent homogeneous (n + 1)-vector in the projective space Pn. The projective space
G. Taylor and L. Kleeman: Visual Perception and Robotic Manipulation, STAR 26, pp. 11–30, 2006.
© Springer-Verlag Berlin Heidelberg 2006

12
2 Foundations of Visual Perception and Control
Pn is deﬁned as the space of linear subspaces of Rn+1. Consider a point in R3 with
inhomogeneous coordinates X, Y, and Z; by introducing a scalar λ, a corresponding
linear subspace of points X(λ) can be deﬁned in R4 as
X(λ) = λ(X,Y,Z,1)
(2.1)
which is in fact a ray emanating from the origin in the direction of (X,Y,Z,1).
Equation (2.1) speciﬁes the homogeneous coordinates of the point at X, Y, and Z in
the projective space P3. By convention and without loss of generality, the unknown
scale can be discarded and the homogeneous coordinates written as X = (X,Y,Z,1),
which will be the convention adopted unless otherwise speciﬁed. An alternative con-
vention sometimes encountered in the literature is to write X  (X,Y,Z,1), where
 denotes equivalence only up to the unknown scale λ. For the image plane vec-
tor x in R2, the corresponding homogeneous coordinates in P2 can be written as
x = (x,y,1). In Section 2.2.1, we will see how expressing image coordinates in this
way reduces a non-linear Euclidean camera model to a linear projective transforma-
tion.
Just as each inhomogeneous vector can be associated with a homogeneous repre-
sentation, we can also deﬁne the inverse mapping. For a general homogeneous vector
X = (X,Y,Z,λ) with λ = 0, the equivalent Euclidean point X is deﬁned as
X = (X,Y ,Z) = (X/λ,Y/λ,Z/λ)
(2.2)
When λ = 0, the homogeneous vector X = (X,Y,Z,0) can be interpreted loosely
as a point “at inﬁnity” in the direction of the inhomogeneous vector (X,Y,Z).
Since the homogeneous and inhomogeneous coordinates of a point are equiva-
lent, we do not distinguish these representations with different notation. The repre-
sentation used in a given analytical model can generally be inferred from context,
for example when a homogeneous transformation is applied to a point. An overview
of projective geometry for computer vision can be found in [36] and [52], and a de-
tailed discussion of homogeneous coordinates and transformations for robotics can
be found in [102].
2.1.2 Coordinate Frames and Homogeneous Transformations
Coordinate vectors, whether homogeneous or inhomogeneous, must be speciﬁed
with respect to a frame that deﬁnes the point of origin and orientation of the canoni-
cal axes. Coordinate frames are denoted using uppercase italics such as F. The frame
in which a particular point is measured is appended as a superscript, such as FX, but
may be omitted when the frame is clear from the context. Coordinates are trans-
formed from one frame to another by application of a homogeneous transformation
(or homography). Let AX and BX represent the location of a single point with respect
to frames A and B, as shown in ﬁgure 2.1. Let the inhomogeneous vector BTA specify
the position of the origin of frame A in frame B. The orientation of frame A with re-
spect to frame B can be deﬁned as a 3×3 rotation matrix BRA (see also the discussion
in the following section). Then, the transformation of the homogeneous coordinates

2.1 Mathematical Preliminaries
13
A
B
BX
X
AX
BRA, BTA
Fig. 2.1. Transformation of a point between coordinate frames.
of X from A to B can be written as BX = BHAAX, where the 4 × 4 homogeneous
transformation matrix BHA is given by:
BHA =
 BRA BTA
01×3
1

(2.3)
Matrix BHA may be interpreted as the coordinate transformation from frame A to B or
alternatively as the pose (orientation and position) of frame A with respect to frame
B; both interpretations will be called upon in later formulations.
2.1.3 Representation of 3D Orientation
Equation (2.3) introduced the 3 × 3 rotation matrix BRA as one possible representa-
tion of 3D orientation. However, the rotation matrix representation over-parameter-
izes 3D orientation with 9 parameters for 3 degrees of freedom. In practice, this can
make rotation matrices cumbersome to use in analytical models and also lead to nu-
merical instabilities. Two common alternative representations of 3D orientation are
Euler angles and quaternions. Euler angles specify a triplet of canonical rotations
about the axes of a coordinate frame. Numerous Euler angle conventions regarding
the order and axes of rotation are found in the literature; this book adopts the yaw-
pitch-roll convention which speciﬁes rotations by φ, θ and ψ about the X, Y and
Z-axes in order. The equivalent rotation matrix can be calculated as
R(φ,θ,ψ) = Rz(ψ)Ry(θ)Rx(φ)
=


cosψ −sinψ 0
sinψ
cosψ 0
0
0
1




cosθ 0 sinθ
0
1
0
−sinθ 0 cosθ




1
0
0
0 cosφ −sinφ
0 sinφ
cosφ


=


cθcψ sφsθcψ −cφsψ cφsθcψ +sφsψ
cθsψ sφsθsψ +cφcψ cφsθsψ −sφcψ
−sθ
sφcθ
cφcθ


(2.4)
where cx = cosx, sx = sinx, and Rx(φ), Ry(θ) and Rz(ψ) are the canonical rotations.
While intuitively and algebraically straightforward, the main drawback of this
representation is that the Euler angles are not unique for a given orientation. This
is due to both their modulo nature (2π and 0 represent the same angle) and the fact

14
2 Foundations of Visual Perception and Control
that some orientations can be represented by entirely different Euler angles (for ex-
ample, R(0,π,0) ≡R(π,0,π)). Unlike Euler angles, normalized quaternions offer a
unique representation for all orientations1 (see [36, 102] for a detailed discussion).
Quaternion q is written as the 4-vector
q = (S,U) = (S,Ux,Uy,Uz) = S+iUx +jUy +kUz
(2.5)
A normalized quaternion satisﬁes the constraint S2 +U2
x +U2
y +U2
z = 1. Quaternions
are an extension of complex numbers, where S is the real component and Ux, Uy and
Uz are coordinates along orthogonal imaginary axes represented by i, j and k, where
i2 = j2 = k2 = −1. The connection between a quaternion q and a rotation of angle α
about an axis represented by the inhomogeneous vector A is
q(α,A) = (cos(α/2),Asin(α/2))
(2.6)
It is instructive here to note the similarity between equation (2.6) and the relationship
between Cartesian and polar representations of complex numbers, that is c(r,θ) =
rcos(θ)+irsin(θ). We can also write the relationship between Euler angles φ, θ, ψ
and the equivalent quaternion as:
S = cos 1
2φ cos 1
2θ cos 1
2ψ +sin 1
2φ sin 1
2θ sin 1
2ψ
(2.7)
Ux = sin 1
2φ cos 1
2θ cos 1
2ψ −cos 1
2φ sin 1
2θ sin 1
2ψ
(2.8)
Uy = cos 1
2φ sin 1
2θ cos 1
2ψ +sin 1
2φ cos 1
2θ sin 1
2ψ
(2.9)
Uz = cos 1
2φ cos 1
2θ sin 1
2ψ −sin 1
2φ sin 1
2θ cos 1
2ψ
(2.10)
Quaternions possess a number of useful properties for manipulating rotations. A
rotation in the opposite direction of a normalized quaternion is its complex conjugate:
q(−α,A) = q(α,A) = (S,−U)
(2.11)
Furthermore, like rotation matrices, quaternions can be multiplied to concatenate
transformations; the ﬁnal orientation q3 after rotating a frame by q1 and then q2 is
given by the complex non-commutative2 product
q3 = q2 ∗q1 = (s2s1 −u2 ·u1,s2u1 +s1u2 +u2 ×u1)
(2.12)
were the ∗operator represents complex multiplication, · is the vector dot product
and × is the vector cross product3. Unfortunately, the redundant parameter and nor-
malization constraint required by this representation make quaternions difﬁcult to
1Strictly, there are two representations for a given orientation, one being obtained from the
other by inverting both the angle and axis of rotation. This ambiguity is avoided by requiring
the quaternion representation to have S ≥0.
2Order is important when concatenating rotations in general, so q1 ∗q2 = q2 ∗q1 and
R1R1 = R2R1
3Equation (2.12) can be obtained from equation (2.5) and the identities ij = −ji = k, jk =
−kj = i and ki = −ik = j.

2.1 Mathematical Preliminaries
15
use in tracking ﬁlters. In practice, a combination of Euler angles and quaternions are
employed to represent orientations (see Section 6.5).
For completion, the angle and axis of rotation corresponding to quaternion q =
(s,u) are θ = 2cos−1(s) and A = u/|u|, and the corresponding Euler angles are:
φ = tan−1(2(uyuz +sux)/(s2 −u2
x −u2
y +u2
z))
(2.13)
θ = sin−1(−2(uxuz −suy))
(2.14)
ψ = tan−1(2(uxuy +suz)/(s2 +u2
x −u2
y −u2
z))
(2.15)
All other transformations between Euler angle, axis/angle, quaternion and rotation
matrix representations follow from combinations of the above transformations.
2.1.4 Pose and Velocity Screw
As described in Section 2.1.2, the pose of frame A with respect to frame B can be
represented as the 4 × 4 homogeneous matrix BHA. However, following the above
discussion it is now clear that the pose (position and orientation) of a frame in R3
can be represented by only six degrees of freedom: three for position and three for
orientation. Thus, the pose of a frame (or object) is often written more compactly as
a pose vector p = (X,Y,Z,φ,θ,ψ), where X, Y and Z represent position and φ, θ
and ψ are the Euler angles describing orientation. Associated with this representa-
tion is the velocity screw ˙r = (V,Ω) = ( ˙X, ˙Y, ˙Z, ˙φ, ˙θ, ˙ψ), which encodes the time
derivative of the pose vector parameters.
The pose of an object in frame A can be transformed to frame B by constructing
the equivalent homogeneous matrix representation of the pose and multiplying by
the coordinate transformation BHA:
H(Bp) = BHAH(Ap)
(2.16)
where H(Ap) and H(Bp) are the homogeneous matrix representation of pose vectors
Ap and Bp, constructed from equations (2.3)-(2.4). Alternatively, representing the
position of an object in frame A as AT and the orientation as AR, the transformation
in equation (2.16) expands as
BR = BRAAR
(2.17)
BT = BRAAT+ BTA
(2.18)
where the relationship between BTA, BRA and BHA is deﬁned in equation (2.3). The
velocity screw A˙r = (AV, AΩ) of an object in frame A transforms to B˙r = (BV, BΩ)
in frame B as (see [68]):
BΩ = BRAAΩ
(2.19)
BV = BRA[AV−AΩ× BTA]
(2.20)

16
2 Foundations of Visual Perception and Control
optical
axis
principal
point
image
plane
optical
centre
X
x
X
x
y
Y
f
Z
Fig. 2.2. Pin-hole camera model.
2.2 Sensor Models
This section describes how to analytically model the major components of a robotic
vision system, namely the projective camera and active stereo head. Speciﬁc models
for the experimental platform described in Chapter 7 are developed, but the general
approach applies to a wide range of platforms. These models will be revisited in later
chapters as the basis of algorithms and error analyses.
2.2.1 Pin-Hole Camera Model
A camera can be described as a system that performs the non-invertible mapping
from real-space coordinates to image plane coordinates (R3 →R2). An analytical
model for this transformation is essential for solving stereo reconstructions and pre-
dicting image plane measurements from an internal world model. The most com-
monly adopted camera model in computer vision literature is the pin-hole (or central
projection) model. This model formally applies to a particular type of lensless cam-
era, but in practice offers a good approximation to most lensed cameras. For cases in
which the pin-hole approximation breaks down, such as zoom lens cameras, a more
general thick lens model is required [94].
Figure 2.2 illustrates the projection of point X in real space onto the image plane
at x for a simple pin-hole camera located at the origin of the world frame. This
projection can be expressed as the non-linear transformation:
x = fX/Z
(2.21)
y = fY/Z
(2.22)
In the framework of projective geometry (see [53]), the operation of a camera can be
described more compactly by a linear projective transformation:

2.2 Sensor Models
17
x = PX
(2.23)
where X and x are homogeneous coordinates in the world frame and image plane
respectively, and P is the 3×4 projection matrix of the camera.
The inhomogeneous projection in (2.21)-(2.22) assumes that the camera is lo-
cated at the origin of the world frame. When this is not the case, X must be trans-
formed from the world frame to the camera frame before applying the projection.
The position and orientation of the camera in the world frame are commonly called
the extrinsic camera parameters. Representing these by an inhomogeneous transla-
tion vector T and rotation matrix R, a new projection matrix can be composed to
perform both the coordinate transformation and projection onto the image plane:
P = K(R|−RT)
(2.24)
where (R| −RT) is a 3 × 4 matrix constructed from the extrinsic camera para-
meters (noting that R−1 ≡R for a rotation matrix), and K is the 3 × 3 camera
calibration matrix. The calibration matrix for a general pin-hole camera is
K =


af sf x0
0
f y0
0
0 1


(2.25)
where f is the focal length, x0 and y0 locate the intersection of the optical axis and
image plane (also known as the principal point), a is the pixel aspect ratio and s de-
scribes pixel skew. These quantities are collectively known as the intrinsic camera
parameters, since they are independent of the position and orientation of the camera.
Recent advances in camera self-calibration have resulted in several methods to de-
termine the calibration matrix automatically from sequences of images [124]. When
accurate calibration is not required, it is usually quite reasonable to assume that a
typical CCD or CMOS camera has square pixels (unity aspect ratio and zero skew)
and a principal point at the origin of the image plane. This leads to a calibration
matrix parameterized by a single intrinsic parameter, the focal length f:
K =


f 0 0
0 f 0
0 0 1


(2.26)
The models presented in later chapters will adopt this approximation, with the fo-
cal length taken from known speciﬁcations, and the extrinsic parameters calibrated
manually, as described in Appendix A.
2.2.2 Radial Distortion
Radial lens distortion is a deviation from the ideal pin-hole model described above
and affects most lensed cameras. The characteristic barrel-roll effect is evident in
the image of a grid shown in Figure 2.3(a), which was taken with a typical CCD

18
2 Foundations of Visual Perception and Control
(a) Before correction.
(b) After correction.
Fig. 2.3. Radial distortion and correction.
camera. This distortion would lead to signiﬁcant errors in depth measurement and
tracking if left uncorrected. To correct the image, the distortion is modelled as a
radial displacement of image plane coordinates as follows:
xr = cr +(x−cr)

1+
∞
∑
i=1
Ki|x−cr|i

(2.27)
where x is the position of an undistorted feature, xr is the distorted position and
cr is the radial distortion centre (not necessarily at the optical centre). In practice,
the distortion function is approximated by estimating only the ﬁrst four coefﬁcients
Ki,i = 1,...,4.
It is a simple matter to estimate the parameters in equation 2.27 based on knowl-
edge that the grid in Figure 2.3 should contain only straight lines when observed
through an ideal pin-hole camera. The image in Figure 2.3(a) has been corrected us-
ing the following iterative algorithm based on this idea. At each iteration, the current
estimate of the Ki’s are used to warp the original image by replacing each pixel x with
the pixel at the distorted position xr calculated from equation (2.27). The warped im-
age is then processed using edge extraction, centroid calculations and connectivity
analysis to automatically extract a set of points on each grid-line. Straight lines ﬁtted
to each set of points using linear regression and the residual errors are tallied into a
single cost associated with the current Ki. The simplex algorithm [112] is used to it-
eratively search over Ki to ﬁnd the distortion parameters that lead to the minimal cost
and result in grid-lines with minimum curvature, as shown in Figure 2.3(b). During
normal operation, the correction is applied to every captured frame before further
processing.
2.2.3 Active Stereo Camera Head
Many of the techniques in this book rely on both active vision and recovery of struc-
ture in three dimensions. Depth perception has been studied in computer vision in a

2.2 Sensor Models
19
left
camera
camera
right
Rx = RPX
Lx = LPX
X
Fig. 2.4. Reconstruction of a point X from corresponding measurements Lx and Rx from stereo
cameras with known projection matrices LP and RP.
variety of forms for several decades. Depth can be recovered from a single image by
exploiting cues such as focus, shading and texture [73] or utilizing a prior model of
the object under observation (as discussed in Chapter 5). With two or more views of
a scene, depth can be recovered by exploiting geometric constraints. Structure from
motion has gained recent popularity and allows a scene to be reconstructed using
many views from a single moving camera [77,123]. However, the minimal and most
commonly used multiple view method in robotics is stereo vision.
Figure 2.4 illustrates the well known principles of stereo reconstruction. Let Lx
and Rx represent the projection of X onto the image planes of two cameras with
different viewpoints. The stereo reconstruction of X is the intersection of the back-
projected rays (shown as dotted lines) from the camera centres through Lx and Rx,
which can be found via the known projection matrices LP and RP. In practice, stereo
reconstruction presents two main difﬁculties. Firstly, corresponding measurements
of each 3D point must be found on both image planes. This problem is simpliﬁed by
considering the epipolar geometry established by the relative pose of the cameras,
which constrains the possible locations of corresponding measurements. Secondly, if
the extrinsic parameters of the cameras are not precisely known, the back-projected
rays may not intersect anywhere. In this case, some optimal estimate of X is required
[54]. Both problems are exacerbated by measurement noise.
Active vision refers to the ability to dynamically adjust the viewpoint of the
cameras to aid in the selective acquisition and interpretation of visual cues4. Both
stereo and active vision are important components of human perception, and an ac-
tive stereo head is a mechanical device that aims to provide a robot with visuo-motor
characteristics similar to a human head. Figure 2.5 illustrates a prototypical conﬁgu-
4Ambiguously, sensors that project energy into the environment are also referred to as
active, and a light stripe sensor with this property is discussed in Chapter 3. However, the term
“active vision” is generally reserved for dynamic viewpoint adjustment in this book.

20
2 Foundations of Visual Perception and Control
right
camera
ν
ν
b
b
camera
left
Z
R
L
C,W
τ
ρ
X
Y
Fig. 2.5. Coordinate frames associated with the active vision platform.
ration of such a device, based on the “Biclops” head used experimentally in Chapter
7 (see Figure 7.1). Stereo cameras are separated by a baseline congruent with human
vision, and rotational joints aligned to the y-axis of each camera enable the nom-
inally parallel optical axes to converge or diverge. The Biclops verge mechanism
constrains the cameras to rotate symmetrically in opposite directions, so vergence
contributes only a single mechanical degree of freedom. A neck provides two addi-
tional rotational degrees of freedom to pan and tilt the cameras in unison. Beyond
the minimalist Biclops design, active stereo heads have been constructed with sev-
eral additional degrees of freedom (such as independent verge and roll), and multiple
cameras for human-like peripheral and foveal visual ﬁelds [136].
The following angles, coordinate frames and transformations are deﬁned to
model the Biclops camera conﬁguration shown in Figure 2.5:
•
L and R are the left and right camera frames, separated by baseline 2b and centred
at the focal points of the cameras. The direction of the X and Y-axes, parallel to
the image plane of the cameras, are deﬁned by the ordering of pixels on the image
plane in computer memory: left and down. The Z-axes are parallel to the optical
axes of the cameras, with the direction dictated by the right-handed coordinates.
Angle ν denotes vergence, which is positive for L and negative for R when the
cameras are converged.
•
Stereo camera frame C is attached to the top of the neck between the cameras,
and provides a reference for stereo reconstruction. Frames C, L and R move as
the Biclops pans and tilts. The axes of C are parallel to L and R, and the latter are
centred at
CL,R = (b,0,0)
(2.28)
with respect to C, taking the negative sign for L and the positive for R. Let LPC
and RPC represent the projection matrices of the left and right cameras in C. To
simplify later models, the cameras are assumed to have identical focal length.
Furthermore, Section 2.2.4 will show that projective rectiﬁcation allows us to

2.2 Sensor Models
21
treat the verge angle as zero, and set R = I in equation (2.24). Under these con-
ditions, the left and right projection matrices in C are given by
L,RPC =


f 0 0 ±fb
0 f 0
0
0 0 1
0


(2.29)
•
World frame W is deﬁned at the same position as C, but is attached to the base
of the neck and remains stationary with respect to the body of the robot. Trans-
forming reconstructed measurements from C to W compensates for ego-motion
as the Biclops head pans and tilts. Representing the pan angle as ρ and the tilt
angle as τ, the homogeneous transformation from C to W is
WHC(ρ,τ) =




cosρ
sinρ sinτ
0
0
0
cosτ
−sinτ
0
−sinρ cosρ sinτ cosρ cosτ 0
0
0
0
1




(2.30)
A simple active vision strategy commonly encountered in the literature is to
maintain a moving feature (say, a tracked target) near the centre of the image plane so
that it always remains visible. Let Lx = (Lx, Ly) and Rx = (Rx, Ry) represent stereo
measurements of the tracked feature, and ˙ωρ, ˙ωτ and ˙ων represent the angular pan,
tilt and verge velocities required to keep the target centred. A simple proportional
control law to calculate these velocities is
˙ωρ = kρ(Lx+ Rx)
(2.31)
˙ωτ = kτ(Ly+ Ry)
(2.32)
˙ων = kν(Lx−Rx)
(2.33)
with suitably chosen gains kρ, kτ and kν. This particular form of active vision is
known as smooth pursuit tracking. An alternative form of active tracking is saccad-
ing, which involves short, rapid motions to centre the target.
2.2.4 Rectilinear Stereo and Projective Rectiﬁcation
One particular conﬁguration of stereo cameras deserves special consideration: the
alignment of parallel camera axes and coplanar image planes, also known as rectilin-
ear stereo. The simple camera projection matrices that arise in this case were given
in equation (2.29). The usefulness of rectilinear stereo arises from a simple epipolar
geometry that constrains corresponding stereo measurements to lie on the same scan-
line in both images, facilitating the search for stereo correspondences. Furthermore,
the reconstruction equations have a simple form for rectilinear stereo. A particularly
useful property of rectilinear stereo is that, for a general non-rectilinear conﬁgura-
tion, we can always recover equivalent rectilinear stereo measurements by applying
a transformation known as projective rectiﬁcation. In general, the transformation to
rectify stereo images can be determined purely from a set of point correspondences

22
2 Foundations of Visual Perception and Control
(see [53]). However, since the Biclops conﬁguration is known we can calculate the
transformation directly from the measured verge angle.
The following formulation is based on the knowledge that when a camera un-
dergoes a ﬁxed rotation, the image plane coordinates undergo a known projective
transformation. Thus, the equivalent image prior to the rotation can be recovered
by applying the inverse transformation. Let the verge angle ν represent the rotation
about the y-axis between the rectilinear frame and the rotated camera frame, and let
P represent the projection matrix of a camera centred at the origin. Also, let xm rep-
resent the measured location of a feature in the unrectiﬁed image, and xr represent
the equivalent feature after rectiﬁcation. A real-space point Xr projecting onto xr can
be recovered (up to an unknown scale) as
Xr = P+xr
(2.34)
where P+ is the pseudo inverse of P, given by
P+ = P(PP)−1
(2.35)
such that PP+x = x [52]. For a measurement at xr = (xr,yr,1), equation (2.34)
gives Xr = (x/ f,y/ f,1,0), which is the point at inﬁnity in the direction of the
ray back-projected through xr. Now, let Ry(ν) represent the rotation that transforms
points from the rectilinear frame to the current camera frame. By rotating Xr and
re-projecting into the image plane, the measurement corresponding to the rectiﬁed
feature is calculated as
xm = PRy(ν)P+xr
(2.36)
For the pin-hole camera model given by equations (2.21)-(2.22), the transformation
above can be expressed in inhomogeneous coordinates as
xm = (xr cosν + f sinν)f
f cosν −xr sinν
(2.37)
ym =
yr f
f cosν −xr sinν
(2.38)
To apply projective rectiﬁcation, the position of each pixel in the rectiﬁed image is
transformed by equations (2.37)-(2.38) to locate the corresponding pixel in the mea-
sured image. Since the left and right cameras verge in opposite directions, the above
transformation is applied with angle −ν for L and +ν for R. The transformation can
be implemented efﬁciently as a lookup table of pixel coordinates since it remains
ﬁxed from frame to frame unless the verge angle changes. For the experimental work
presented in later chapters, rectiﬁcation is applied to all captured images to simplify
subsequent operations.
2.3 From Images to Perception
2.3.1 Digital Image Processing
A digital image is a quantized, discrete 2-dimensional signal formed by sampling the
intensity and frequency of incident light on the image plane. Typically, each sample

2.3 From Images to Perception
23
is formed by integrating light for a short period over a small area on a 2D sensor
array such as a charged coupled device (CCD) or CMOS transistor array. Analyti-
cally, we represent a digital image as a W ×H array I(x,y), with x = 0,1,..,W −1 and
y = 0,1,..H −1 spanning the width and height of the image. The term pixel describes
a single element of the array, while scan-line refers to an entire row. In practice, pix-
els in the sensor array are addressed with I(0,0) at the top left, x increasing to the
right and y increasing downward, which accounts for the choice of frame orientation
in Figures 2.2 and 2.5. To represent colour, pixels may be vector-valued with com-
ponents representing the intensity of incident light in red, green and blue frequency
bands (or channels). Image processing also frequently involves grayscale images in
which pixels represent overall intensity, and binary images that encode the presence
or absence of features (such as a particular colour) as boolean-valued pixels.
Along the path from images to perception, the progression through low-level,
mid-level and high-level vision provides an illustrative (though simplistic) road map
of the archetypal computer vision algorithm. Operations at successive levels are dis-
tinguished by their increasing abstraction away from the raw images, and decreasing
awareness of incidental factors such as lighting and viewpoint. The ﬁrst and lowest
level of processing, also known as early vision, typically involves detecting features
such as colours or spatial and temporal intensity gradients, and matching features
for depth recovery and optical ﬂow. Mid-level vision is concerned with forming low-
level features into coherent entities through segmentation, model-ﬁtting and tracking.
Finally, high-level algorithms distill usable semantics by classifying or recognizing
objects and behavioural patterns. Many excellent texts cover all levels of the com-
puter vision in greater detail, and the interested read is referred to [36,39,52].
2.3.2 The Role of Abstraction
One of the fundamental issues in perception for robotic systems is the choice of
representation, that is, how the environment and goals are abstracted. Traditional
robotic systems employ a hierarchy of sensing, planning and action modules, with
suitable abstractions of the world providing the “glue”. The role of perception is
then to reduce the real world to a set of relevant semantic tokens (such as recognized
objects), to form an abstract internal model for high-level processing.
An alternative approach proposed by Brooks [16] is intelligence without repre-
sentation in which the notion of a centralized world model is abandoned. Brooks
proposes that instead of the traditional functional components, control should be im-
plemented as a layered set of competing activities. Each activity independently cou-
ples perception to action with little or no intermediate abstraction, and careful design
of the interactions between layers leads to emergent intelligence. This approach fa-
cilitates simple and robust perception algorithms. Many robotic grasping methods
found in the literature can be used in this spirit. Rodrigues et al. [130] describe an
algorithm for ﬁnding three-ﬁngered grasp points on the boundary of an arbitrary
2D object directly from a binary image. Semantic-free grasp planning algorithms
based on moments of inertia and boundary tracing, using 2D [40, 55] or 3D visual
data [46], have also been demonstrated. Image-based visual servoing (discussed in

24
2 Foundations of Visual Perception and Control
detail below) controls the pose of a robot using low-level visual measurements such
as corners and lines, without further abstraction. Combining these algorithms would
enable a service robot to grasp arbitrary, visually sensed targets without imposing
any interpretations on the measurements.
While many low-level behaviours can be performed directly on measurements,
abstractions can be very useful at higher levels of planning. For example, classifying
an object as a telephone allows a robot to minimize the grasp planning problem by
exploiting prior knowledge about telephones. Furthermore, semantics extracted from
available measurements combined with prior knowledge (whether programmed or
learned) can be used to infer structure that may be hidden from view.
Ultimately, abstractions are needed at some level to enable a robot to both learn
and interact naturally with humans. Consider a task given as the high-level com-
mand: “Please fetch a bottle of water”. The robot must sufﬁciently abstract the world
to identify, at a minimum, objects satisfying the description of bottle. Thus, the robot
must be programmed with, or preferably learn autonomously, a representation for
bottles associated with this semantic label. A common approach to object recogni-
tion in robotics is to form a database of known objects from labelled training samples.
Object models may be geometrical, view-based or use a combination of representa-
tions, and are created with varying degrees of human interaction. Many industrial
service tasks involve only a small set of well deﬁned objects, in which case prede-
ﬁned CAD models (typically edge-based) can be used to locate and track objects
during manipulation. Examples of this approach include [41,142,152].
Other representations have been explored to deal with the wide variety of objects
encountered in domestic applications, and to facilitate autonomous learning. The hu-
manoid service robot developed by Kefalea et al. [80] represents objects using a set
of sampled edge-based templates, each with an associated orientation and grasp con-
ﬁguration. The robot generates templates semi-autonomously by placing the object
on a turntable. Given an unknown scene, all templates for every known object are
elastically matched to the captured image to recover the identity, approximate pose
and grasp information. Kragi´c [88] describes a similar scheme using a set of image
templates and associated poses in a principal components analysis framework. In
this case, each object is also represented by an edge-based geometric model used to
reﬁne the initial pose and track the object during grasping. In related work, Ekvall
et al. [33] replace the image templates with colour cooccurrence histograms, which
represent the spatial distribution of colour with scale and rotation independence. Ob-
jects are identiﬁed using a histogram similarity measure, and the pose is recovered as
a weighted mean of the orientation associated with different views. A variety of other
representations for object recognition and pose estimation for robotic manipulation
can be found in [85,113,146].
The approaches described above require an explicit model for each instance of
an object. However, a kitchen may contain dozens of cups with unique variations
in colour, texture and geometry that require each to be modelled separately. As the
number of objects in the database increases so does the burden on storage space and
the computational expense of matching. One solution to this problem is to represent
classes, such as “cups”, rather than speciﬁc objects. Much recent work has focussed

2.4 From Perception to Control
25
on the similar problem of detecting faces or people in images independently of the
individual (for example, see [160]). For classifying domestic objects, data-driven
geometric modelling is a popular solution. Geometric modelling has been demon-
strated using bivariate polynomials [12], Generalized Cylinders [23,126] and primi-
tives such as planes, spheres and cones [100,167]. Once a scene has been modelled,
objects such as cups can identiﬁed as speciﬁc collections of primitives without stor-
ing a model for each cup. Data-driven modelling and classiﬁcation will be explored
further in Chapter 4.
2.4 From Perception to Control
The principal control task in traditional industrial robotics is to accurately, rapidly
and repeatably drive the end-effector through a sequence of pre-programmed poses.
Perception plays little part since both the robot and environment are completely de-
ﬁned. In contrast, service robots may only need to match the speed and accuracy of
their human partners, while service tasks may be poorly deﬁned and require little
repetition. This leads to control algorithms with a distinctly different aim: to ro-
bustly close the loop between perception and action. The major approaches to visu-
ally guided robotic grasping are discussed in the following sections.
The classical framework of look-then-move or open-loop kinematic control solves
the important problem of uncertain target location. Typical implementations of this
approach are used to control the humanoid robots in [10,104]. The target is localized
using calibrated cameras and the required joint angles for grasping are calculated
using inverse kinematics. The obvious drawback of this end-effector open-loop ap-
proach is that kinematic and camera calibration errors can still defeat the robot.
Calibration errors can be minimized by directly observing the gripper in addition
to the target, and two distinct approaches to this problem have been demonstrated.
The ﬁrst is to construct a mapping between robot joint angles and locations in camera
space by observing the gripper at various locations. Given a target pose, this mapping
then provides the required joint angles. This form of open-loop control is exempliﬁed
by the learning-based schemes discussed in Section 2.4.1. The second approach,
known as visual servoing, is to continuously measure the pose of both the gripper
and target during grasping, and drive the joint angles as a function of the pose error.
Naturally, the gripper will be driven toward the target until the pose error reduces to
zero. The various visual servoing schemes are discussed further in Section 2.4.2.
It is important to recognize that end-effector open and closed-loop schemes offer
complementary advantages. Closed-loop control achieves low sensitivity to calibra-
tion errors but constrains the gripper to be observable, while open-loop control is
less accurate with fewer constraints. Gaskett and Cheng [43] attempt to exploit the
advantages of both approaches by combining learning-based control with conven-
tional visual servoing. The learned mapping between camera space and joint angles
provides a gross motion to bring the gripper within view, after which the closed-loop
controller takes over. Chapter 6 describes an alternative framework for combining
both open and closed-loop visual control.

26
2 Foundations of Visual Perception and Control
2.4.1 Learning-Based Methods
Learning algorithms are usually inspired by advances in our understanding of hu-
man learning from studies in neuroscience. One such idea that has found popularity
in robotics is the concept of the visuo-motor mapping, which describes the relation-
ship between visually perceived features and the motor signals necessary to act upon
them. One of the elementary problems in visuo-motor mapping is learning to sac-
cade; generating the eye motion required to center a visual stimulus at the fovea.
In [127], the saccadic visuo-motor map for an active camera head is represented as
a vector of motor signals at each image location, learned via an iterative process in-
volving random saccades. Learning-based control of an active head has also been
used for the related problem of smooth pursuit tracking [42].
Another visuo-motor mapping can be deﬁned between cameras and a robot arm.
Ritter et al. [128] apply this idea to controlling a three-link robotic arm using stereo
cameras. The mapping is implemented as a neural network that takes as input a target
location speciﬁed as corresponding points on the stereo image planes, and output the
three joint angles required to drive the arm to that location. The network is trained by
driving the arm to random locations and observing the gripper. The humanoid robot
described by Marjanovi´c et al. [99] autonomously learns a visuo-motor mapping
to point at visual targets through trial and error, with no knowledge of kinematics.
Rather than mapping joint angles directly, pointing motions are constructed from a
basis of predeﬁned arm positions. The visuo-motor mapping generates weights to
linearly interpolate these base poses and drive the arm in the desired direction.
Learning-based methods are also exploited at the task planning level, and are ca-
pable of addressing complex issues. For example, the system described by Wheeler
et al. [162] learns how to grasp objects to satisfy future constraints, such as how
the object will be placed (known as prospective planning). Programming by demon-
stration5 is a popular planning framework that has been demonstrated both for in-
dustrial [70] and service robots [32, 86] (see [137] for a comprehensive review). In
this approach, the robot observes the actions and subsequent environmental changes
made by a human teacher and stores the sequence for later repetition. Programming
by demonstration eases the problem of planning complex manipulations, but raises
a number of other issues: differing kinematic constraints of the teacher and robot
require adaption of observations [7]; the robot must be able to discriminate task-
oriented motions from incidental motions and measurement noise [21]; suitable ab-
stractions must be constructed so that learned motions can be applied in different
circumstances [1].
2.4.2 Visual Servoing
Visual servoing describes a class of closed-loop feedback control algorithms in ro-
botics for which the control error is deﬁned in terms of visual measurements. Like all
5also known in the literature as learning from demonstration, imitation learning and
assembly-plan-from-observation.

2.4 From Perception to Control
27
feedback systems, the advantage of this approach is to minimize the effect of distur-
bances and uncertainty in kinematic and sensor models. The effectiveness of the vi-
sual servoing paradigm is demonstrated by an extensive literature and diverse appli-
cations, including industrial automation [75], assistance in medical operations [91],
control of ground vehicles [158], airborne vehicles [159], and robotic grasping. A
comprehensive tutorial and review of visual servoing for robotic manipulators can be
found in [68], and a review of more recent developments can also be found in [89].
The visual servoing literature has established a well-deﬁned taxonomy of pop-
ular conﬁgurations and control algorithms. At the kinematic level, the relationship
between the robot and camera is often described as eye-in-hand or ﬁxed-camera. In
the former conﬁguration, the camera is rigidly attached to the end-effector and the
goal is to control the view of the camera (and thus the pose of the gripper) with
respect to a target. Conversely, a ﬁxed-camera conﬁguration places the camera and
end-effector at opposing ends of a kinematic chain, and pose of the gripper is con-
trolled independently of the camera view. This latter conﬁguration is more akin to
human hand-eye coordination.
A systematic categorization of visual servoing control architectures was ﬁrst in-
troduced by Sanderson and Weiss [135]. One of the principal conceptual divisions
is the distinction between position-based and image-based visual servoing, as illus-
trated in Figure 2.6 for a ﬁxed-camera conﬁguration. In position-based visual ser-
voing, the control error is calculated after reconstructing the 6D pose of the grip-
per from visual measurements, typically via 3D model-ﬁtting. Joint angles are then
driven by the error between the observed and desired pose. Conversely, the control
error in image-based visual servoing is formulated directly as the difference between
the observed and desired location of features (such as points and edges) on the im-
age plane. It is interesting to note that a recent comparison of position-based and
image-based servoing concluded that both exhibit similar stability, robustness and
sensitivity to calibration errors [31].
Visual servoing schemes can be further classiﬁed as direct visual servo6 or dy-
namic look-and-move, depending on whether the control law directly generates joint
motions, or Cartesian set points for lower-level joint controllers. This distinction is
usually not made in the literature as almost all practical systems are regarded as
dynamic look-and-move.
A ﬁnal signiﬁcant distinction can be made between endpoint open-loop (EOL)
and endpoint closed-loop (ECL) control [68]. An ECL controller observes both the
end-effector and target to determine the control error, while an EOL controller ob-
serves only the target. In the latter case, the relative position of the robot is controlled
using a known, ﬁxed camera to robot (hand-eye) transformation. The required hand-
eye calibration is often based on observation of a calibration rig [156] or the motion
of the end-effector [6], and has a signiﬁcant effect on the accuracy of EOL control.
Importantly, hand-eye calibration is not required for ECL control since the pose of
the end-effector is measured directly. Positioning accuracy is therefore independent
of kinematic uncertainty (although stability and other dynamic issues may arise).
6This terminology was introduced by Hutchinson et al. [68].

28
2 Foundations of Visual Perception and Control
estimate and track
pose, OH
vector, f
extract feature
OH
pose, DH
desired
OHD
pose error, OHD
control law
Cartesian
(a) Position-based visual servoing framework. The control error is the transformation
OHD between the desired pose DH and the observed pose OH. The observed pose is
estimated from the visual feature vector, f.
f
fd
control law
f
vector, fd
feature
desired
fe
Feature space
vector, f
extract feature
feature error, fe
(b) Image-based visual servoing framework. The control error fe is the difference
between the observed image feature vector f and the desired feature vector fd.
Fig. 2.6. Comparison of position-based and image-based visual servoing.
Image-based visual servoing, which generally receives greater attention in the
literature, has been studied for both eye-in-hand [34,81,115] and ﬁxed-camera [63,
65] conﬁgurations. Typically, the desired locations of target features are obtained
from a real image of the robot already in the desired pose. The robot can then be
servoed back to the desired pose with a high degree of robustness to calibration errors
and without knowledge of the 3D structure of the target. A central concept in image-
based visual servoing is the image Jacobian (or interaction matrix), which relates
differential changes in the 6D pose or joint angles of the end-effector to changes in
the image-space location of features. The image Jacobian thus allows an image-space
control error to be transformed into a pose or joint-space error. The image Jacobians
for a variety of features, including points and lines, are derived in [34].
A well known drawback of classical image-based control is the existence of sin-
gularities in the image Jacobian and local minima in the control error, leading to
instabilities and non-convergence [20]. Furthermore, calculation of the image Jaco-
bian requires knowledge of depth, which may be recovered using 3D model-based
tracking [87] or some approximation [97]. Alternatively, Hosada and Asada [65] dis-
pose of analytical Jacobians altogether and avoid these problems by maintaining an
on-line discrete-time estimate of the image Jacobian. A ﬁnal signiﬁcant drawback is
that classical image-based visual servoing provides no explicit control of pose, which

2.4 From Perception to Control
29
can lead to inefﬁcient and unpredictable trajectories in Cartesian space. Recent re-
sults show that this issue can be avoided by decoupling the control of orientation and
translation [83,97,129].
As noted earlier, position-based visual servoing requires explicit reconstruction
of the robot and target pose, which imposes a greater computational burden than
image-based control [68]. Furthermore, pose estimation typically relies on knowl-
edge of the camera parameters and a precise 3D model of the object. In contrast
to image-based control, the accuracy of the controller is therefore sensitive to both
camera calibration and the chosen pose estimation algorithm [31]. On the other hand,
controlling the pose of the end-effector in real space leads to predictable trajectories
and allows simple path planners to be directly incorporated into the controller. How-
ever, since position-based control imposes no constraints on image-space trajecto-
ries, the observability of the target within the camera frame is no longer guaranteed.
While this is considered one of the classical drawbacks of position-based control,
Cervera and Martinet [18] demonstrated that simply formulating the control error
to nullify the target pose leads to simple image plane trajectories that are likely to
maintain the visibility of the target.
Wilson et al. [163] present the design and implementation of a classical EOL
position-based visual servo controller. Using an end-effector mounted camera, the
robot is able to maintain a desired pose relative to a target work-piece using a Kalman
ﬁlter tracker based on point features. By achieving a sampling rate of 61 Hz in the
visual control loop, this work demonstrates that pose estimation is not necessarily
a computational burden. Wunsch and Hirzinger [165] present a similar real-time
position-based scheme that tracks edge features instead of points. Tracking ﬁlters
can simplify measurement association in model-based pose estimation, and there-
fore play a prominent role in position-based visual servoing. However, a drawback
is the requirement of an explicit dynamic model for target pose prediction, since
targets with poorly deﬁned dynamics can easily lead to tracking failures. Adaptive
Kalman ﬁltering promises to alleviate this problem via a dynamically estimated mo-
tion model [164].
Various techniques have been proposed in the literature to overcome the difﬁ-
culties of both image and position-based schemes. The hybrid approach of 2-1/2-D
visual servoing [97] employs partial pose reconstruction to control the orientation of
the end-effector in real space, while translation is controlled using extended image
coordinates. A rough estimate of depth is required for translation control, but does
not affect the positioning accuracy of the controller. Unlike conventional position-
based control, 2-1/2-D servoing does not require precise hand-eye and camera cal-
ibration or a 3D model of the target. Furthermore, the controller produces linear
trajectories in real space, while the singularities associated with the classical image
Jacobian are avoided.
Other schemes aim to eliminate the need for accurate camera and kinematic cali-
bration by employing linear approximations. For example, Cipolla and Hollinghurst
[24] present an approach based on afﬁne stereo. The hand-eye transformation and
camera model are reduced to 16 linear coefﬁcients that are robustly estimated from
the stereo measurements of four known points. Visual servoing based on afﬁne stereo

30
2 Foundations of Visual Perception and Control
is shown to be robust to linear kinematic errors, camera disturbances and perspective
effects. Namba and Maru [110] achieve an approximate linear stereo reconstruction
by parameterizing camera space using angular quantities, and also describe a linear
approximation to the inverse kinematics of a humanoid robot. In this scheme, joint
velocities are calculated directly as a linear function of the image plane error. In-
terestingly, it is straightforward to show that image and position-based control are
equivalent for point-to-point alignment in a linear servoing framework [61]. How-
ever, as with any approximate method, the performance of this class of controllers is
bounded by the validity of the linear approximation.
2.4.3 Summary
Sensor models play an important role in 3D model-based vision, which is a central
theme of this book. The analytical tools commonly used in modelling robotic and
vision systems were presented in Section 2.1, and Section 2.2 applied this framework
to modelling the major components of a robotic vision system. In particular, the pin-
hole approximation of a lensed camera and the kinematic model of an active stereo
head were described, which form the basis of several algorithms developed in later
chapters. For example, the validation and reconstruction algorithm for robust light
stripe sensing in Chapter 3, the 3D model-based tracker in Chapter 5 and the hybrid
position-based visual servoing method in Chapter 6 all draw heavily on both camera
and kinematic models.
The broad overview of perception and control techniques for robotic manipula-
tion presented in Sections 2.3 and 2.4 serve primarily to show that much research
remains to be done in this area. The adoption of 3D model-based vision in this book
is motivated by several of the issues raised here. In particular, the chosen world
representation deﬁnes the robot’s ability to classify, autonomously learn and attach
semantic meaning to unknown objects, which is important when working in an do-
mestic environment. Object modelling and recognition using compositions of simple
geometric primitives (autonomously extracted from range data) is a well known ap-
proach to addressing these issues, and is discussed further in Chapter 4.
Visual control algorithms are typically based on either biologically inspired
learning or some form of visual servoing. The latter approach is pursued in this book
as it provides greater robustness to the dynamic disturbances and modelling uncer-
tainties likely encountered by a service robot. Fitting in with the 3D model-based
vision paradigm, a position-based visual servoing algorithm based on the principles
outlined above is presented in Chapter 6.

3
Shape Recovery Using Robust Light Stripe Scanning
Recovering the three dimensional structure of a scene accurately and robustly is im-
portant for object modelling and robotic grasp planning, which in turn are essential
prerequisites for grasping unknown objects in a cluttered environment. Shape recov-
ery techniques are broadly described as either passive or active. Passive methods
include recovering shape from a single image using cues such as shading, texture
or focus, and shape from multiple views using stereopsis or structure-from-motion.
Passive shape recovery has relatively low power requirements, is non-destructive and
more akin to our biological sensing modalities. However, the accuracy and reliability
of passive techniques is critically dependent on the presence of sufﬁcient image fea-
tures and the absence of distractions such as reﬂections. Active sensing is necessary
to achieve the accuracy required for the object modelling and tracking techniques
described in the following chapters.
Active sensing relies on projecting energy into the environment and interpreting
the modiﬁed sensory view. Active techniques generally achieve high accuracy and
reliability, since the received signal is well-deﬁned but at the cost of high power
consumption. Active shape recovery is generally based on either some form of
structured light projection or time-of-ﬂight ranging. Recent developments in active
CMOS time-of-ﬂight cameras enable a complete 3D image to be captured in a single
measurement cycle [93], however this technology is still limited by timing resolu-
tion. Structured light ranging is based on the same principles as stereopsis, but with
a masked light pattern projected onto the scene to provide easily identiﬁable image
features. The main research in this area has focused on designing coded light patterns
to uniquely identify each surface point [106]. For robotic applications, the disadvan-
tage of this approach is the requirement of a large, high-power pattern projector.
Light stripe ranging is a simple structured light technique that uses a light plane
(typically generated using a laser diode) to reconstruct a single range proﬁle of the
target with each frame. This technique offers the advantages of compactness, low
power consumption and minimal computational expense. However, applying light
stripe sensing to service robots presents unique challenges. Conventional scanners
require the brightness of the stripe to exceed that of other features in the image to be
reliably detected, making the technique most effective in highly controlled environ-
G. Taylor and L. Kleeman: Visual Perception and Robotic Manipulation, STAR 26, pp. 31–56, 2006.
© Springer-Verlag Berlin Heidelberg 2006

32
3 Shape Recovery Using Robust Light Stripe Scanning
ments. Robust light stripe detection methods have been proposed in previous work
but suffer from issues including assumed scene structure, lack of error recovery and
acquisition delay. Thus, the goal in this chapter is to develop a robust light stripe
sensor for service robots that overcomes this major shortcoming.
The solution is an actively calibrated stereoscopic light stripe scanner, capable of
robustly detecting the stripe in the presence of secondary reﬂections, cross-talk and
other noise mechanisms. Measurement validation and noise rejection are achieved
by exploiting the redundancy in stereo measurements (conventional scanners use a
single camera). The validation and reconstruction algorithms are optimal with re-
spect to sensor noise, which results in higher precision ranging than conventional
methods. Furthermore, self-calibration from an arbitrary non-planar target allows ro-
bust validation to be achieved independently of ranging accuracy. Finally, operating
in normal light allows colour and range measurements to be captured and implicitly
registered in the same camera. Registered colour/range data is highly desirable for
the object modelling and tracking techniques developed in the following chapters.
The following section introduces the basic principles of conventional single-
camera light stripe range sensing and discusses existing robust techniques. The the-
oretical framework of the method proposed in this chapter is presented in Section
3.2. Section 3.3 details the active calibration process that allows the scanner to be
calibrated from measurements of an arbitrary non-planar target. The experimental
implementation of the scanner is described in Section 3.4, including low-level im-
age processing for stripe detection and range data post-processing. Finally, Section
3.5 experimentally validates the proposed technique, including a comparison of the
noise robustness achieved by the proposed scanner and other methods. VRML mod-
els of experimental colour/range scans are provided in the Multimedia Extensions,
along with source code and data sets to implement the calibration and scanning al-
gorithms.
3.1 Conventional Light Stripe Ranging and Related Work
Light stripe ranging is an active, triangulation-based technique for non-contact sur-
face measurement that has been studied for several decades [3, 139]. A review of
conventional light stripe scanning and related range sensing methods can be found
in [9,12,59]. Range sensing is an important component of many robotic applications,
and light stripe ranging has been applied to a variety of robotic tasks including nav-
igation [4,116], obstacle detection [57], object recognition for grasping [5,126] and
visual servoing [82].
Figure 3.1 illustrates the operation of a conventional single-camera light stripe
sensor. The principle is similar to binocular stereo (see Section 2.2.3), with one of
the cameras replaced by a light plane projector (typically a laser with a suitable
lens). The stripe reﬂected from the target is measured by the camera and each point
in the 3D surface proﬁle (for example X in Figure 3.1) is reconstructed by triangula-
tion, using the known transformation between the camera and projector. To capture

3.1 Conventional Light Stripe Ranging and Related Work
33
projector
camera
X
known
plane
transformation
target
Fig. 3.1. Conventional single-camera light stripe sensor.
a complete range image, the light plane is mechanically panned across the target and
the range slices are registered into a mesh1.
The drawback of conventional single-camera light stripe ranging is that favour-
able lighting conditions and surface reﬂectance properties are required so the stripe
can be identiﬁed as the brightest feature in the image. In practice, this is achieved
by coating the target with a matte ﬁnish, using high contrast cameras or reducing the
level of ambient light. When the range sensor is intended for use by a service robot
to recognize unknown objects in a domestic or ofﬁce environment [147], various
noise mechanisms may interfere to defeat stripe detection: smooth surfaces cause
secondary reﬂections, edges and textures may have a stripe-like appearance, and
cross-talk can arise when multiple robots scan the same environment.
A number of techniques for improving the robustness of light stripe scanners
have been proposed in other work, using both stereo and single-camera conﬁgu-
rations. Magee et al. [96] develop a scanner for industrial inspection using stereo
measurements of a single stripe. Spurious reﬂections are eliminated by combining
stereo ﬁelds via a minimum intensity operation. This technique depends heavily on
user intervention and a priori knowledge of the scanned target. Trucco et al. [155]
also use stereo cameras to measure a laser stripe, and treat the system as two in-
dependent single-camera sensors. Robustness is achieved by imposing a number of
consistency checks to validate the range data, the most signiﬁcant of which requires
independent single-camera reconstructions to agree within a threshold distance. An-
other constraint requires valid scan-lines to contain only a single stripe candidate, but
a method for error recovery in the case of multiple candidates is not proposed. Thus,
secondary reﬂections cause both the true and noisy measurements to be rejected.
1An alternative to rotating the light stripe is to rotate the target itself. For robotic applica-
tions, this could be achieved by grasping and rotating the object in the gripper. However, this
approach is difﬁcult in practice when 3D sensing is a prerequisite to grasp planning!

34
3 Shape Recovery Using Robust Light Stripe Scanning
Nakano et al. [109] develop a similar method to reject false data by requiring con-
sensus between independent scanners, but using two laser stripes and only a single
camera. In addition to robust ranging, this conﬁguration provides direct measure-
ment of the surface normal. The disadvantage of this approach is that each image
only recovers a single range point at the intersection of the two stripes, resulting in a
signiﬁcant acquisition delay for the complete image.
Other robust scanning techniques have been proposed using single-camera, sing-
le-stripe conﬁgurations. Nygards and Wernersson [117] identify specular reﬂections
by moving the scanner relative to the scene and analyzing the motion of recon-
structed range data. In [57], periodic intensity modulation distinguishes the stripe
from random noise. Both of these methods require data to be associated between
multiple images, which is prone to error. Furthermore, intensity modulation does not
disambiguate secondary reﬂections, which vary in unison with the true stripe. Alter-
natively, Clark et al. [25] use linearly polarized light to reject secondary reﬂections
from metallic surfaces, based on the observation that polarized light changes phase
with each specular reﬂection. However, the complicated acquisition process requires
multiple measurements through different polarizing ﬁlters.
Unlike the above robust techniques, the method described below uniformly re-
jects interference due to secondary reﬂections, cross-talk, background features and
other noise mechanisms. A signiﬁcant improvement over previous techniques for
error detection is a mechanism for the recovery of valid measurements from a set
of noisy candidates. The reconstructed depth data is optimal with respect to sensor
noise, unlike the stereo techniques in [109,155], and stereo measurements are fused
with the light plane parameters to provide greater precision than a single-camera con-
ﬁguration. Finally, conventional techniques using a special camera for stripe detec-
tion typically require a second camera to measure colour [58]. The ability to operate
in ambient indoor light allows our sensor to measure implicitly registered colour and
range in the same camera.
3.2 Robust Stereoscopic Light Stripe Sensing
As discussed above, the shortcomings of light stripe sensors arise from the difﬁculty
in disambiguating the primary reﬂection of the stripe from secondary reﬂections,
cross-talk and other sources of noise. The following sections detail the principles of
an optimal strategy to resolve this ambiguity and robustly identify the true stripe by
exploiting the redundancy in stereo measurements.
3.2.1 Problem Statement
Figure 3.2 shows a simpliﬁed plan view of a stereoscopic light stripe scanner to
demonstrate the issues involved in robust stripe detection. The light plane projector
is located between a pair of stereo cameras, which are arranged in a rectilinear stereo
conﬁguration with optical centres at CL and CR. The scene contains two planar sur-
faces, with the light stripe projected onto the right-hand surface. The surface causes

3.2 Robust Stereoscopic Light Stripe Sensing
35
secondary
reﬂection
primary
reﬂection
CR
CL
plane
right image
plane
left image
generator
light plane
X
R
X
X
XL
XR
Lx
Rx
Rx
Rˆx
Fig. 3.2. Validation/reconstruction problem. Reprinted from [151]. c2004, Sage Publica-
tions. Used with permission.
a primary reﬂection at X that is measured (using a noisy process) at Lx and Rx on the
stereo image planes. However, a secondary specular reﬂection causes another stripe
to appear at X, which is measured on the right image plane at Rx but obscured from
the left camera (in practice, such noisy measurements are produced by a variety of
mechanisms other than secondary reﬂections). The 3D reconstructions, labelled XL,
XR and X
R in Figure 3.2, are recovered as the intersection of the light plane and
the rays back-projected through the image plane measurements. These points will be
referred to as the single-camera reconstructions. As a result of noise on the CCD
(exaggerated in this example), the back-projected rays do not intersect the physical
reﬂections at X and X.
The robust scanning problem may now be stated as follows: given the laser plane
position and the measurements Lx, Rx and Rx, one of the left/right candidate pairs,
(Lx,Rx) or (Lx,Rx), must be chosen as representing stereo measurements of the pri-
mary reﬂection. Alternatively, all candidates may be rejected. This task is referred to
as the validation problem, and a successful solution in this example should identify
(Lx,Rx) as the valid measurements. The measurements should then be combined to
estimate the position of the ideal projection Rˆx (arbitrarily chosen to be on the right
image plane) of the actual point X on the surface of the target.
Formulation of optimal validation/reconstruction algorithms should take account
of measurement noise, which is not correctly modelled in previous related work. In
[155] and [109], laser stripe measurements are validated by applying a ﬁxed thresh-
old to the difference between corresponding single-camera reconstructions (XL, XR

36
3 Shape Recovery Using Robust Light Stripe Scanning
generator
light plane
image plane
C
δx
δx
x1
x2
ΔX1
ΔX2
Fig. 3.3. Variation of reconstruction error with depth. Reprinted from [151]. c2004, Sage
Publications. Used with permission.
and X
R in Figure 3.2). Such a comparison requires a uniform reconstruction error
over all depths, which Figure 3.3 illustrates is clearly not the case. Two independent
measurements at x1 and x2 generally exhibit a constant error variance on the image
plane, as indicated by the interval δx. However, projecting δx onto the laser plane re-
veals that the reconstruction error increases with depth, since ΔX1 < ΔX2 in Figure
3.3. Thus, the validation threshold on depth difference should increase with depth to
account for measurement noise, otherwise validation is more lenient for closer re-
constructions. Similarly, taking either XL, XR or the arithmetic average 1
2(XL + XR)
as the ﬁnal reconstruction in Figure 3.2 is generally sub-optimal for noisy measure-
ments.
The following sections present optimal solutions to the validation/reconstruction
problem, based on an error model with the following features (the assumptions of
the error model are corroborated with experimental results in Section 3.5.2):
1. Light stripe measurement errors are independent and Gaussian distributed with
uniform variance over the entire image plane.
2. The dominant error in the light plane position is the angular error about the axis
of rotation as the plane is scanned across the target, which couples all measure-
ments in a given image.
3. All other parameters of the sensor (described in the following section), are as-
sumed to be known with sufﬁcient accuracy that any uncertainty can be ignored
for the purpose of validation and reconstruction.

3.2 Robust Stereoscopic Light Stripe Sensing
37
light plane
axis of
rotation
light
plane
left camera
frame
L
viewing direction
right image
left image
plane
plane
b
f
b
R
right camera
frame
C
camera
frame
θz
θy
P
light plane
frame
Z0
X0
θx
CR
CL
y
y
Y
x
x
Z
X
Fig. 3.4. Light stripe camera system model. Reprinted from [151]. c2004, Sage Publications.
Used with permission.
3.2.2 System Model
Figure 3.4 details the parameters of the system model for the stereoscopic light stripe
sensor. L and R denote the left and right camera frames, P is the frame of the rotating
light plane and C is the camera frame (deﬁned in Section 2.2.3). As described in Sec-
tion 2.2.1, the cameras are modelled by the 3×4 projection matrices L,RP, given by
equation (2.29). The cameras are assumed to have identical focal length f, and are in
rectilinear stereo conﬁguration (optical axes aligned to the world z-axis) with optical
centres located at CL,R = (b,0,0) in the camera frame. As described in Section
2.2.4, the cameras are allowed to verge about the y-axis, and projective rectiﬁcation
is applied to every frame to recover the equivalent rectilinear stereo measurements.
Frame P is rigidly attached to the laser, and points X on the light plane are deﬁned
by the plane equation ΩX = 0, where Ω represents the parameters of the light plane.
Furthermore, frame P is deﬁned such that the light plane is approximately vertical
and parallel to the z-axis. This allows Ω to be expressed in P as
PΩ = (1, B0, 0, D0)
(3.1)
where B0 is related to the small angle between the plane and the y-axis (B0  1 for
an approximately vertical plane), and D0 is the distance of the plane from the origin.
Thus, the normal vector of the plane is already normalized, since |(1,B0,0)| ≈1.
During a scan, frame P rotates about its y-axis with angle θy, where θy ≡0 when the
light plane is parallel to the optical axes of the cameras. The rotation axis intersects
the xz-plane of the camera frame at (X0,0,Z0) (in the camera frame), and the orien-
tation of the rotation axis relative to the y-axis of the camera frame is deﬁned by the

38
3 Shape Recovery Using Robust Light Stripe Scanning
small ﬁxed angles θx and θz. The scan angle (θy in Figure 3.4) is assumed to have
a linear relationship with the measured optical encoder value e via two additional
parameters m and k:
θy = me+k
(3.2)
Now let CHP represent the homogeneous coordinate transformation from P to C.
Representing the homogeneous transformation for rotation about the x, y and z-axes
by angle θ by matrices Rx(θ), Ry(θ) and Rz(θ), and the translation of (X,Y,Z) by
the matrix T(X,Y,Z), PHC can be expressed as
CHP = T(X0,0,Z0)·Rz(θz)·Rx(θx)·Ry(θy)
(3.3)
It is straightforward to show that if PHC is the coordinate transformation from P to
C, the plane parameters transform from P to C as
CΩ = (CHP)− · PΩ
(3.4)
Combining equations (3.3) and (3.4), the laser plane parameters are expressed in the
camera frame as:
CΩ =




cycz −sxsysz −B0cxsz
cysz +sxsycz +B0cxcz
−cxsy +B0sx
(sxsysz −cycz +B0cxsz)X0 +(cxsy −B0sx)Z0 +D0




(3.5)
where cx = cosθx and sx = sinθx. By making the simplifying assumptions B0, θx,
θz  1, many insigniﬁcant terms in equation (3.5) can be neglected to give an ap-
proximate model:
CΩ =




cosθy
θx sinθy +θz cosθy +B0
−sinθy
−X0 cosθy +Z0 sinθy +D0




(3.6)
Finally, equations (3.2) and (3.6) allow points on the laser plane to be identiﬁed
in the world frame, using the plane equation CΩ(B0,D0,X0,Z0,θx,θy,θz)CX = 0.
3.2.3 Generalized Image Plane Error Function
This section now presents an analytical treatment of the validation/reconstruction
problem. Let Lx and Rx represent noisy measurements of the stripe on corresponding
epipolar lines, and let Ω = (A,B,C,D) represent the location of the light plane.
The likelihood that the measurements correspond to the primary reﬂection of the
stripe can be formulated as an error distance on the image plane. Let X represent the
maximum likelihood 3D reconstruction, which is constrained to coincide with the
laser plane, but not necessarily at the intersection of the back-projected rays from the
noisy image plane measurements. Errors in the light plane parameters are considered
in the next section, but for now the light plane parameters Ω are assumed to be known

3.2 Robust Stereoscopic Light Stripe Sensing
39
exactly. The optimal reconstruction X projects to the ideal corresponding points on
the left and right image planes at L,Rˆx = L,RPX, according to equation (2.23). Now,
in a similar manner to equation (6.12), the sum of squared errors between the ideal
and measured points can be used to determine whether the candidate measurement
pair (xL,xR) corresponds to a point X on the light plane:
E = d2(Lx, Lˆx)+d2(Rx, Rˆx)
= d2(Lx, LPX)+d2(Rx, RPX)
(3.7)
where d(x1,x2) is the Euclidean distance between x1 and x2. For a given candidate
pair, the optimal reconstruction X with respect to image plane error is found by a
constrained minimization of E with respect to the condition that X is on the laser
plane:
ΩX = 0
(3.8)
When multiple ambiguous correspondences exist, equation (3.7) is optimized with
respect to the constraint in (3.8) for all possible candidate pairs, and the pair with
minimum error is chosen as the most likely correspondence. Finally, the result is
validated by imposing a threshold on the maximum allowed squared image plane
error E.
Performing the constrained optimization of equations (3.7)-(3.8) is analytically
cumbersome. Fortunately, the problem may be reduced to an unconstrained opti-
mization by determining the direct relationship between projections Lˆx and Rˆx for
points on the light plane. Taking the intersection between the light plane and the
back-projected ray from Rˆx, the relationship between X and Rˆx for points on the
light plane Ω is given by (see Appendix B for a complete derivation of this result):
X = [CR(RP+Rˆx) −(RP+Rˆx)C
R ]Ω
(3.9)
where RP+ is the pseudo-inverse of RP (given by equation (2.35)). Now, projecting
X onto the left image plane, the relationship between the projections Lˆx and Rˆx for
points on the light plane Ω can be expressed as (see Appendix B):
Lˆx = LPRˆx
=

LP[CRΩ −(C
R Ω)I]RP+
Rˆx
(3.10)
Equation (3.10) is of the form Lˆx = HRˆx and simply states that points on the laser
plane induce a homography between coordinates on the left and right image planes,
which is consistent with known results [52]. Finally, the error function becomes
E = d2(Lx,HRˆx)+d2(Rx, Rˆx)
(3.11)
where H = LP[CRΩ −(C
R Ω)I]RP+. The reconstruction problem can now be for-
mulated as an unconstrained optimization of equation (3.11) with respect to Rˆx.
Then, the minimum squared error E over all candidates is used to resolve the val-
idation/correspondence problem, and the reconstruction X can be recovered from
(3.9).

40
3 Shape Recovery Using Robust Light Stripe Scanning
3.2.4 Special Case: Rectilinear Stereo and Pin-Hole Cameras
The results of the previous section apply to general camera models and stereo geom-
etry. However, the special case of rectilinear stereo and pin-hole cameras is important
as it reduces equation (3.11) to a single degree of freedom. Furthermore, rectilinear
stereo applies without loss of generality (after projective rectiﬁcation), and the pin-
hole model is a good approximation for CCD cameras (after correcting for radial lens
distortion). The stereo cameras used in this work are assumed to have unit aspect ra-
tio and no skew (see Section 2.2.1), and the pin-hole models are parameterized by
equal focal length f. Details of the analysis presented in this section can be found in
Appendix B.
The camera centres CL,R and projection matrices L,RP for rectilinear pin-hole
cameras are given by equations (2.28)-(2.29). Substituting these into equation (3.10),
the relationship between the projections of a point on the light plane can be written
as:
Lˆx =


Ab−D
2Bb
2Cb f
0
−(Ab+D)
0
0
0
−(Ab+D)

Rˆx
(3.12)
In inhomogeneous coordinates, the relationship between Lˆx = (L ˆx, L ˆy) and Rˆx =
(R ˆx, R ˆy) given by the homogeneous transformation in equation (3.12) can be ex-
pressed as
L ˆx = −(Ab−D)R ˆx+2BbR ˆy+2Cb f
Ab+D
(3.13)
L ˆy = R ˆy
(3.14)
Since the axes of L and R are parallel (rectilinear stereo), the notation ˆy ≡L ˆy = R ˆy
replaces equation (3.14). Rectilinear stereo gives rise to epipolar lines that are paral-
lel to the x-axis, so the validation algorithm need only consider possible correspon-
dences on matching scan-lines in the stereo images. Any measurement error in the
stripe detection process (see Section 3.4.1) is assumed to be in the x-direction only,
while the y-coordinate is ﬁxed by the height of the scan-line. Thus, the y-coordinate
of the optimal projections are also ﬁxed by the scan-line, ie. ˆy = y, where y is the
y-coordinate of the candidate measurements Lx and Rx.
Finally, substituting equations (3.13)-(3.14) with ˆy = y into (3.11), the image
plane error E can be expressed as a function of a single variable, R ˆx:
E = (Lx+αR ˆx+βy+γ f)2 +(Rx−R ˆx)2
(3.15)
where the following change of variables is introduced:
α = (Ab−D)/(Ab+D)
(3.16)
β = 2Bb/(Ab+D)
(3.17)
γ = 2Cb/(Ab+D)
(3.18)

3.2 Robust Stereoscopic Light Stripe Sensing
41
For the experimental scanner, with Ω given by equation (3.6), α, β and γ can be
written as:
α = −k1 cosθy +k2 sinθy +k3
cosθy +k2 sinθy +k3
(3.19)
β = (1−k1)(θx sinθy +θz cosθy +B0)
cosθy +k2 sinθy +k3
(3.20)
γ =
(k1 −1)sinθy
cosθy +k2 sinθy +k3
(3.21)
where θy = me + c and the following change of variables is made in the system
parameters:
k1 = −(b+X0)/(b−X0)
(3.22)
k2 = Z0/(b−X0)
(3.23)
k3 = D0/(b−X0)
(3.24)
Optimization of equation (3.15) now proceeds using standard techniques, setting
dE
dR ˆx = 0 and solving for R ˆx. Let R ˆx∗represent the optimal projection resulting in the
minimum squared error, E∗. It is straightforward to show (see Appendix B) that the
optimal projection is given by
R ˆx∗= [Rx−α(Lx+βy+γ f)]/(α2 +1)
(3.25)
and the minimum squared error E∗for the optimal solution is:
E∗= (Lx+αRx+βy+γ f)2/(α2 +1)
(3.26)
For completion, substituting equation (3.25) and R ˆy∗= y into (3.13) gives the corre-
sponding optimal projection on the left image plane as
L ˆx∗= [α2Lx−αRx−(βy+γ f)]/(α2 +1)
(3.27)
Finally, the optimal 3D reconstruction X∗is recovered by substituting (3.25) into
equation (3.9). In non-homogeneous coordinates, the optimal reconstruction leading
to the minimum image plane error for candidate measurements Lx and Rx is (see
Appendix B):
X∗= [(α −1)(αLx−Rx)−(α +1)(βy+γ f)]b
(α +1)(αLx−Rx)+(α −1)(βy+γ f)
(3.28)
Y ∗=
2by(α2 +1)
(α +1)(αLx−Rx)+(α −1)(βy+γ f)
(3.29)
Z∗=
2bf(α2 +1)
(α +1)(αLx−Rx)+(α −1)(βy+γ f)
(3.30)
The validation problem can now be solved by evaluating E∗in equation (3.26)
for all pairs of candidate measurements on matching scan-lines, and selecting the

42
3 Shape Recovery Using Robust Light Stripe Scanning
pair with the minimum error (less than some validation threshold). Once the valid
measurements have been identiﬁed, the position of the light plane is calculated from
the encoder count e using equations (3.2) and (3.19)-(3.21), and the optimal recon-
struction is recovered from the image plane measurements using (3.28)-(3.30).
3.2.5 Laser Plane Error
The above solution is optimal with respect to the error of image plane measurements,
and assumes that the parameters of the laser plane are known exactly. In practice, the
encoder measurements are likely to suffer from both random and systematic error
due to acquisition delay and quantization. Unlike the image plane error, the encoder
error is constant for all stripe measurements in a given frame and thus cannot be
minimized independently for candidate measurements on each scan-line.
Let Lxi and Rxi, i = 1...n represent valid corresponding measurements of the
laser stripe on the n scan-lines in a frame. The reconstruction error E∗
i (e) for each pair
can be treated as a function of the encoder count via the system model in equations
(3.19)-(3.21). The total error E∗
tot(e) over all scan-lines for a given encoder count e
is calculated as:
E∗
tot(e) =
n
∑
i=1
E∗
i (e)
(3.31)
Finally, an optimal estimate of the encoder count e∗is calculated from the minimiza-
tion
e∗= argmin
e [E∗
tot(e)]
(3.32)
Since E∗
tot(e) is a non-linear function, the optimization in equation (3.32) is im-
plemented numerically using Levenberg-Marquardt (LM) minimization from MIN-
PACK [105], with the measured value of e as the initial estimate.
As noted above, valid corresponding measurements must be identiﬁed before
calculating E∗
tot(e). However, since the correspondences are determined by minimiz-
ing E∗over all candidate pairs given the plane parameters, the correspondences are
also a function of the encoder count. Thus, the reﬁned estimate e∗may relate to a
different set of optimal correspondences than those from which it was calculated.
To resolve this issue, the optimal correspondences and encoder count are calculated
recursively. In the ﬁrst iteration, correspondences are calculated using the measured
encoder value e to yield the initial estimate e∗
0 via equations (3.31)-(3.32). A new set
of correspondences are then extracted from the raw measurements using the reﬁned
encoder value e∗
0. If the new correspondences differ from the previous iteration, an
updated estimate of the encoder value e∗
1 is calculated (using e∗
0 as the initial guess).
The process is repeated until a stable set of correspondences is found.
The above process is applied to each captured frame, and the optimal encoder
count e∗and valid correspondences, Lxi and Rxi, are substituted into equations (3.28)-
(3.30) to ﬁnally recover the optimal 3D proﬁle of the laser.

3.2 Robust Stereoscopic Light Stripe Sensing
43
3.2.6 Additional Constraints
As already described, robust stripe detection is based on minimization of the image
plane error in equation (3.26). However, the minimum image plane error is a nec-
essary but insufﬁcient condition for identifying valid stereo measurements. In the
practical implementation, two additional constraints are employed to improve the
robustness of stripe detection.
The ﬁrst constraint simply requires stripe candidates to be moving features; a
valid measurement must not appear at the same position in previous frames. This
is implemented by processing only those pixels with sufﬁciently large intensity dif-
ference between successive frames. While this constraint successfully rejects static
stripe-like edges or textures in most scenes, it has little effect on cross-talk or reﬂec-
tions, since these also appear as moving features.
The second constraint is based on the fact that valid measurements only occur
within a sub-region of the image plane, depending on the angle of the light plane.
It can be shown (see equation (B.29) in Appendix B), that the inhomogeneous z-
coordinate of a single-camera reconstruction X can be expressed as a function of the
image plane projections L,Rˆx as
Z = ±
2bf
(α +1)L,R ˆx+βy+γ f
(3.33)
where the positive sign is taken for L and negative sign for R. Rearranging the above,
the projected x-coordinate of a point on the light plane may be expressed as a function
of depth Z and the height y of the scan-line:
L,R ˆx = −βy+γ f
α +1 ±
2bf
Z(α +1)
(3.34)
The extreme boundaries for valid measurements can now be found by taking the limit
of equation (3.34) for points on the light plane near and far from the camera. Taking
the limit for distant reﬂections gives one boundary at:
lim
Z→∞
L,R ˆx = −βy+γ f
α +1
(3.35)
Taking the limit Z →0 for close reﬂections gives the other boundary at L,R ˆx →±∞.
Now, if w is the width of the captured image, valid measurements on a scan-line at
height y must be constrained to the x-coordinate ranges
Lx ∈

−βv+γ f
α +1 ,+w
2

(3.36)
Rx ∈

−w
2 ,−βv+γ f
α +1

(3.37)
Stripe extraction is only applied to pixels within the boundaries deﬁned in (3.36)-
(3.37); pixels outside these ranges are immediately classiﬁed as invalid. In addition
to improving robustness, sub-region processing also reduces computational expense
by halving the quantity of raw image data and decreasing the number of stripe can-
didates tested for correspondence.

44
3 Shape Recovery Using Robust Light Stripe Scanning
3.3 Active Calibration of System Parameters
In this section, determination of the unknown parameters in the model of the light
stripe scanner are considered. Let the unknown parameters be represented by the
vector
p = (k1,k2,k3,θx,θz,B0,m,k)
where k1, k2 and k3 were introduced in equations (3.22)-(3.24). Since most of the
parameters relate to mechanical properties, the straightforward approach to calibra-
tion is manual measurement. However, such an approach would be both difﬁcult and
increasingly inaccurate as parameters vary through mechanical wear. To overcome
this problem, a strategy is now proposed to optimally estimate p using only image-
based measurements of a non-planar but otherwise arbitrary surface with favourable
reﬂectance properties (the requirement of non-planarity is discussed below). This
allows calibration to be performed cheaply and during normal operation.
The calibration procedure begins by scanning the stripe across the target and
recording the encoder and image plane measurements for each captured frame. Since
the system parameters are initially unknown, the validation problem is approximated
by recording only the brightest pair of features per scan-line. Let Lxi j and Rxi j, i =
1...n j, j = 1...t represent the centroids of the brightest corresponding features on
nj scan-lines of t captured frames, and let ej represent the measured encoder value
for each frame. As described earlier, image plane measurements have independent
errors, while the encoder error couples all measurements in a given frame. Thus,
optimal system parameters are determined from iterative minimization of the stripe
measurement and encoder errors, based on the algorithm ﬁrst described in Section
3.2.5. First, the total image plane error E∗
tot is summed over all frames:
E∗
tot =
t
∑
j=1
nj
∑
i=1
E∗(Lxij, Rxij,ej,p)
(3.38)
where E∗is deﬁned in equation (3.26). The requirement of a non-planar calibration
target can now be justiﬁed. For a planar target, the stripe appears as a straight line and
the image plane measurements obey a linear relationship of the form xi j = ajyi j +bj.
Then, the total error E∗
tot reduces to the form
E∗
tot =
t
∑
j=1
n j
∑
i=1
(Ajyij +Bj)2
(3.39)
Clearly, the sign of Aj and Bj cannot be determined from equation (3.39), since the
total error remains unchanged after substituting −Aj and −Bj. The geometrical in-
terpretation of this result is illustrated in Figure 3.5, which shows the 2D analogue
of a planar target scan. For any set of encoder values ej and collinear points Xj
measured over t captured frames, there exist two symmetrically opposed laser plane
generators capable of producing identical results. This ambiguity can be overcome
by constraining the calibration target to be non-planar. It may also be possible for cer-
tain non-planar targets to produce ambiguous results, but the current implementation
assumes that such an object will rarely be encountered.

3.3 Active Calibration of System Parameters
45
scanning
direction
scanning
direction
X1
light plane
light plane
generator
generator
Xt
X j
scanned plane
e1
et
e1
et
e j
e j
Fig. 3.5. Possible positions of the light plane from the scan of a planar calibration target.
The position of the light plane generator is ambiguous. Reprinted from [151]. c2004, Sage
Publications. Used with permission.
An initial estimate p∗
0 of the parameter vector is given by the minimization
p∗
0 = argmin
p [E∗
tot(p)]
(3.40)
using the measured encoder values ej and stereo correspondences Lxi j and Rxi j.
Again, equation (3.40) is implemented numerically using LM minimization. The
stripe measurements Lxij and Rxij are likely to contain gross errors resulting from
the initial coarse validation constraint (in the absence of known system parameters).
Thus, the next calibration step reﬁnes the measurements by applying outlier rejec-
tion. Using ej and the initial estimate p∗
0, the image plane error E∗(Lxi j, Rxi j,ej,p∗
0)
in equation (3.26) is calculated for each stereo pair. The measurements are then
sorted in order of increasing error, and the top 20% are discarded.
The system parameters and encoder values are then sequentially reﬁned in an
iterative process. The initial estimate p∗
0 is only optimal with respect to image plane
error, assuming exact encoder values ej. To account for encoder error, the encoder
value is reﬁned for each frame using the method described in Section 3.2.5 with
the initial estimate p∗
0 of the system model. The resulting encoder estimates e∗
j,0 are
optimal with respect to p∗
0. A reﬁned system model p∗
1 is then obtained from equation
(3.40) using the latest encoder estimates e∗
j,0 and inlier image plane measurements.
At the kth iteration, the model is considered to have converged when the fractional
change in total error E∗
tot is less then a threshold δ:
E∗
tot,k−1 −E∗
tot,k
E∗
tot,k−1
< δ
(3.41)
The ﬁnal parameter vector p∗
k is stored as the near-optimal system model for process-
ing regular scans using the methods described in Section 3.2. A ﬁnal check for global

46
3 Shape Recovery Using Robust Light Stripe Scanning
camera
right
left
camera
position
encoder
laser diode
Fig. 3.6. Experimental stereoscopic light stripe scanner. Reprinted from [151]. c2004, Sage
Publications. Used with permission.
optimality is performed by comparing the minimum total error E∗
tot,k to a ﬁxed thresh-
old, based on an estimate of the image plane error. The rare case of non-convergence
(less than 10% of trials) is typically due to excessive outliers introduced by the sub-
optimal maximum intensity validation constraint applied to the initial measurements.
Non-convergence is resolved by repeating the calibration process with a new set of
data.
The calibration technique presented here is practical, fast and accurate, requir-
ing only a single scan of any suitable non-planar scene. Furthermore, the method
does not rely on measurement or estimation of any metric quantities, and so does
not require accurate knowledge of camera parameters b and f. Thus, image-based
calibration allows the validation and correspondence problems to be solved robustly
and independently of reconstruction accuracy.
3.4 Implementation
Figure 3.6 shows the components of the experimental stereoscopic light stripe scan-
ner, which is mounted on the Biclops head. A vertical light plane is generated by a
laser diode module with a cylindrical lens, and is scanned across a scene by rotating
the laser about a vertical axis. The angle of rotation is measured by an optical encoder
connected to the output shaft via a toothed belt. PAL colour cameras capture stereo
images of the stripe at 384×288 pixel (half-PAL) resolution. The shaft encoder and
stereo images are recorded at regular 40 ms intervals (25 Hz PAL frame-rate). The
laser is mechanically geared to displace the stripe by about one pixel of horizontal
motion per captured frame, so a complete scan requires approximately 384 processed
frames (15 seconds).

3.4 Implementation
47
intensity
profile
pixel position
δedge
δmax
x1
x2
δwidth
Fig. 3.7. Thresholds for robust extraction of multi-modal pulses. Reprinted from [151].
c2004, Sage Publications. Used with permission.
For each captured stereo pair, the vertical stripe is extracted from the image plane
and combined with the laser angle and system parameters to recover the 3D proﬁle
of the illuminated surface. As the stripe is scanned across the scene, the laser proﬁles
are assembled into an array of 3D points, which is referred to as the range map. Each
element of the range map records the 3D position of the target surface as viewed from
the right camera. A colour image is captured and registered with the range map at the
completion of a scan. Captured frames are processed at PAL frame-rate (25 Hz) on
the 2.2 GHz dual Xeon host PC. Motor control and optical encoder measurements
are implemented on a PIC microcontroller, which communicates with the host PC
via an RS-232 serial link (see Figure 7.2).
3.4.1 Light Stripe Measurement
Laser stripe extraction is performed using intensity data only, which is calculated by
taking the average of the colour channels. As noted in Section 3.2.6, the motion of the
stripe distinguishes it from the static background, which is eliminated by subtract-
ing the intensity values in consecutive frames and applying a minimum difference
threshold. The resulting difference image is morphologically eroded and dilated to
reduce noise and improve the likelihood of stripe detection. In Section 3.2.6 it was
also shown that valid measurements occur in a predictable sub-region of the image.
This is calculated from equations (3.36)-(3.37) and the measured encoder value, and
pixels outside this region are set to zero in the difference image. Further processing
is only applied to pixels with non-zero difference.
The intensity proﬁle on each scan-line is then examined to locate candidate stripe
measurements. If the stripe appeared as a simple unimodal pulse, the local max-
ima would be sufﬁcient to detect candidates. However, mechanisms including sensor
noise, surface texture and saturation of the CCD interfere and perturb the intensity

48
3 Shape Recovery Using Robust Light Stripe Scanning
(a) Before post-processing
(b) After post-processing
Fig. 3.8. Removal of impulse noise and holes (eliminated features are circled on the left).
Reprinted from [151]. c2004, Sage Publications. Used with permission.
proﬁle. These issues are overcome by extracting pulses using a more sophisticated
strategy of intensity edge extraction and matching. On each scan-line, left and right
edges are identiﬁed as an increase or decrease in the intensity proﬁle according to
the thresholds deﬁned in Figure 3.7. Processing pixels from left to right, the location
of a left edge xl is detected when the intensity difference between successive pixels
exceeds a threshold δedge, and the closest local intensity maxima to the right of xl
exceeds the intensity at xl by a larger threshold δmax. Right edges xr are extracted
by processing the scan-line in reverse. Finally, the edges are examined to identify
left/right pairs without intervening edges. When xl and xr are closer than a threshold
distance δwidth, the pair are assumed to delimit a candidate pulse. The pulse cen-
troid is calculated to sub-pixel accuracy as the mean pixel position weighted by the
intensity proﬁle within these bounds.
The result of the above process is a set of candidate stripe locations on each scan-
line of the stereo images. Along with the measured encoder value, these candidates
are analyzed using the techniques described in Section 3.2 to reﬁne the laser plane
angle, identify valid corresponding measurements and reconstruct an optimal 3D
proﬁle. The reconstruction on each scan-line is stored in the range map at the location
of the corresponding measurement in the right image.
3.4.2 Range Data Post-processing
Post-processing is applied after each complete scan to further reﬁne the measured
data. Despite robust scanning, the raw range map may still contain outliers as the
stripe validation conditions are occasionally satisﬁed by spurious noise. Fortunately,
the sparseness of the outliers make them easy to detect and remove using a simple
thresholding operation: the minimum distance between each 3D point and its eight
neighbours is calculated, and when this exceeds a threshold (10 mm in the current
implementation), the associated point is removed from the range map.
Holes (pixels for which range data could not be recovered) may occur in the range
map due to specular reﬂections, poor surface reﬂectivity, random noise and outlier

3.5 Experimental Results
49
removal. A further post-processing step ﬁlls these gaps with interpolated depth data.
Each empty pixel is checked to determine whether it is bracketed by valid data within
a vertical or horizontal distance of two pixels. To avoid interpolating across depth
discontinuities, the distance between the bracketing points must be less than a ﬁxed
threshold (30 mm in the current implementation). Empty pixels satisfying these con-
straints are assigned a depth linearly interpolated between the valid bracketing points.
The effect of both outlier rejection and interpolation on a raw scan is demonstrated
in Figure 3.8.
Finally, a colour image is registered with the range map. Since robust scanning
allows the sensor to operate in normal light, the cameras used for stripe detection
also capture colour information. However, depth and colour cannot be sampled si-
multaneously for any given pixel, since the laser masks the colour of the surface.
Instead, a complete range map is captured before registering a colour image from the
right camera (assuming the cameras have not moved). Each pixel in this ﬁnal image
yields the colour of the point measured in the corresponding pixel of the range map.
3.5 Experimental Results
3.5.1 Robustness
To evaluate the performance of the robust methods proposed in this chapter, the scan-
ner is implemented along with two other common techniques on the same experimen-
tal platform. The ﬁrst method is a simple single-camera scanner without any opti-
mal or robust properties. A single-camera reconstruction is calculated from equation
(3.9), using image plane measurements from the right camera only. Since no vali-
dation is possible in this conﬁguration, the stripe is simply detected as the brightest
feature on each scan-line. The second alternative implementation will be referred to
as a double-camera scanner. This approach is based on the robust techniques pro-
posed in [109, 155], which exploit the requirement of consensus between two inde-
pendent single-camera reconstructions. Again, the single-camera reconstructions XL
and XR are calculated from equation (3.9), from measurements Rx and Rx. Measure-
ments are discarded when |XL −XR| exceeds a ﬁxed distance threshold. For valid
measurements, the ﬁnal reconstruction is calculated as 1
2(XL + XR).
The performance of the three methods in the presence of a phantom stripe (sec-
ondary reﬂection) was assessed using the test scene shown in Figure 3.9. A mirror at
the rear of the scene creates a reﬂection of the objects and scanning laser, simulat-
ing the effect of cross-talk and specular reﬂections. To facilitate a fair comparison,
the three methods operated simultaneously on the same raw measurements captured
during a single scan of the scene.
Figure 3.10 shows the colour/range data captured by the single-camera scanner.
As a result of erroneous associations between the phantom stripe and laser plane, nu-
merous phantom surfaces appear in the scan without any physical counterpart. Figure
3.11 shows the output of the double-camera scanner, which successfully removes the
spurious surfaces. However, portions of real surfaces have also been rejected, since

50
3 Shape Recovery Using Robust Light Stripe Scanning
Fig. 3.9. Robust scanning experiment. A mirror behind the objects simulates the effect of
cross-talk and reﬂections. Reprinted from [151]. c2004, Sage Publications. Used with per-
mission.
Fig. 3.10. Single-camera results in the presence of secondary reﬂections (see Multimedia Ex-
tensions for VRML model). Reprinted from [151]. c2004, Sage Publications. Used with
permission.

3.5 Experimental Results
51
Fig. 3.11. Double-camera results in the presence of secondary reﬂections (see Multimedia
Extensions for VRML model). Reprinted from [151]. c2004, Sage Publications. Used with
permission.
Fig. 3.12. Robust scanner results in the presence of secondary reﬂections (see Multimedia
Extensions for VRML model). Reprinted from [151]. c2004, Sage Publications. Used with
permission.

52
3 Shape Recovery Using Robust Light Stripe Scanning
the algorithm is unable to disambiguate the phantom stripe from the primary reﬂec-
tion when both appear in the scene. Finally, Figure 3.12 shows the result using the
techniques presented in this chapter. The portions of the scene missing from Figure
3.11 are successfully detected using the proposed robust scanner, while the phantom
stripe has been completely rejected. Also noteworthy is the implicitly accurate reg-
istration of colour and range. VRML models of the scans in Figures 3.10-3.12 are
provided in the Multimedia Extensions.
The single-camera result highlights the need for robust methods when using
light stripe scanners on a domestic robot. While the double-camera scanner suc-
cessfully rejects reﬂections and cross-talk, the high rejection rate for genuine mea-
surements may cause problems for segmentation or other subsequent processing. In
contrast, segmentation and object classiﬁcation have been successfully applied to the
colour/range data from the proposed robust scanner to facilitate high-level domestic
tasks (see Chapter 4 and [147]).
3.5.2 Error Analysis
The results in this section experimentally validate of the system and noise models
used to derive the theoretical results. In particular, the encoder angle estimation and
calibration techniques described in Sections 3.2.5 and 3.3 are shown to be sufﬁciently
accurate that any uncertainty in the system parameters and encoder values can be
ignored for the purposes of optimal validation and reconstruction.
First, the calibration procedure described in Section 3.3 was performed using
the corner formed by two boxes as the calibration target, and the valid image plane
measurements and encoder values for each frame were recorded. Using the esti-
mated system parameters, the optimal projections L,R ˆx∗and residuals (L,Rx −L,R ˆx∗)
were calculated from equations (3.25) and (3.27) for all measurements. Figure 3.13
shows the histogram of residuals for measurements on the right image plane, and a
Gaussian distribution with the same parameters for comparison. The residuals are
approximately Gaussian distributed as expected, and assuming the light stripe mea-
surement errors are similarly distributed, the error model proposed in Section 3.2.1
is found to be valid. The variance of the image plane measurements are shown in the
ﬁrst two rows of Table 3.1.
The variance of the system parameters and encoder values were determined using
statistical bootstrapping. In this process, the residuals were randomly and uniformly
sampled (with replacement) from the initial data, and added to the optimal projec-
tions L,R ˆx∗to generate a new set of pseudo-measurements. The system parameters
were then estimated for each new set of pseudo-measurements using the calibration
process described in Section 3.3. A total of 5000 re-sampling experiments were per-
formed, and the resulting variance in the estimated system parameters are shown in
the third column of Table 3.1.
Finally, the optimality of the proposed reconstruction method is assessed by cal-
culating the contribution from each parameter to the variance of the optimal image
plane reconstruction R ˆx∗in equation (3.25). Representing the components of the pa-
rameter vector as p = {pi}, and assuming the parameters have independent noise,

3.5 Experimental Results
53
Table 3.1. Average and variance of measurements and system parameters, and contribution to
reconstruction error.
pi
¯pi
var(pi)
var(R ˆx∗)i
Lx 59.2
8×10−3 2×10−3
Rx -47.4
7×10−3 2×10−3
y
6.0
0.0
0.0
e
451.5
5×10−5 2×10−5
k1 -1.1033
2×10−8 1×10−5
k2 0.1696
4×10−8 2×10−8
k3 0.0717
1×10−8 2×10−5
θx -0.0287
3×10−10 2×10−12
θz 0.0094
5×10−8 2×10−6
B0 -0.0004
5×10−8 2×10−6
m 0.001105 7×10−16 3×10−5
c
-0.4849
1×10−10 4×10−5
0
2000
4000
6000
8000
10000
12000
14000
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
Histogram count
Residual error (pixels)
Fig. 3.13. Distribution of residual reconstruction errors on the image plane. Reprinted from
[151]. c2004, Sage Publications. Used with permission.
the contribution of parameter pi (with all other parameters ﬁxed) to the variance of
R ˆx∗, represented as var(R ˆx∗)i, is calculated as
var(R ˆx∗)i =

∂R ˆx∗
∂pi
####
p
2
·var(pi)
(3.42)
The independence of the parameters is readily veriﬁed from the covariance of p.

54
3 Shape Recovery Using Robust Light Stripe Scanning
Choosing a test point near the centre of the image plane, the contribution of each
parameter to the total variance was calculated from equation (3.42) and the results
are shown in the far right column of Table 3.1. Importantly, the errors due to Rx
and Lx are two orders of magnitude greater than the contribution from the system
parameters and encoder value. For comparison, the variance in R ˆx∗measured from
the bootstrapping process was 0.0035 pixels2, which agrees well with the sum of
contributions from Lx and Rx. Finally, it should be noted that the variance of R ˆx∗is
about half the variance of Rx, indicating that the optimal reconstruction has a higher
precision than a single-camera reconstruction.
These results demonstrate the reliability of the image-based techniques presented
in Sections 3.2.5 and 3.3 for estimating the encoder value and calibrating the system
parameters in the presence of noisy measurements. Furthermore, the main assump-
tions in deriving equations (3.25) and (3.26) are now justiﬁed: any uncertainty in the
system parameters and encoder value can be reasonably ignored for the purpose of
validation and reconstruction.
3.6 Discussion
In addition to providing a mechanism for validation, the error distance E∗in equation
(3.26) could be used to measure the random error of range points. As discussed in
Section 3.2.1, the error variance of a 3D reconstruction increases with depth as the
reconstruction problem becomes ill-conditioned. This systematic uncertainty can be
calculated directly from the reconstruction equations (3.28)-(3.30). In contrast, E∗
measures the random uncertainty due to sensor noise. A suitable function of these
systematic and random components could be formulated to provide a unique conﬁ-
dence interval for each 3D point, which would be useful in subsequent processing.
For example, parametric surface ﬁtting could be optimized with respect to measure-
ment error by weighting each point with the conﬁdence value.
One of the main limitations of light stripe scanning (compared to methods such
as passive stereo) is the acquisition rate. In the current implementation, the PAL
frame-rate of 25 Hz results in a 15 second measurement cycle to capture a complete
half-PAL resolution range map of 384 stripe proﬁles. Clearly, such a long acquisition
time renders the sensor unsuitable for dynamic scenes. However, a more subtle issue
is that the robot must remain stationary during a scan to ensure accurate registration
of the measured proﬁles. Obviously, the acquisition rate could be improved using
high-speed cameras and dedicated image processing hardware; high-speed CMOS
cameras are now capable of frame-rates exceeding 1000 Hz. Assuming the image
processing could be accelerated to match this speed, the sensor could be capable of
acquiring 2-3 range maps per second. An example of a high-speed monocular light
stripe sensor using a “smart” retina is described by [47].
To minimize complexity and cost, the experimental prototype uses a red laser
diode to generate the light plane. Consequently, the scanner only senses surfaces
which contain a high component of red. Black, blue and green surfaces reﬂect insuf-
ﬁcient red laser light and are effectively invisible to the sensor. Since the light plane

3.6 Discussion
55
(a) Specular reﬂections
(b) Raw colour/range map using robust scanner
Fig. 3.14. Light stripe scan of a highly polished object. Reprinted from [151]. c2004, Sage
Publications. Used with permission.
is not required to be coherent or monochromatic, the laser diode could be replaced
by a white light source such as a collimated incandescent bulb. However, laser diodes
have particular design advantages including physical compactness, low power con-
sumption and heat generation, and are thus more desirable than other light sources.
To solve the colour deﬁciency problem while retaining these advantages, the light
plane could be generated using a triplet of red, green and blue laser diodes. Cur-
rently, the main obstacle to this approach is the high cost of green and blue laser
diodes.
As with colour, surfaces with high specular and low Lambertian reﬂection may
appear invisible, since insufﬁcient light is reﬂected back to the sensor. This limitation
is common to all active light sensors and can also defeat passive stereopsis, since
the surface differs in appearance at each viewpoint. To illustrate this effect, Figure
3.14 shows the raw image and resulting scan of a highly polished object. The only
visible regions appearing in the range map are the high curvature edges that provide
a specular reﬂection directly back to the sensor. The best that can be achieved is
to ensure that secondary reﬂections do not interfere with range data acquisition, as
demonstrated in this result.
The stripe validation method developed in this chapter may provide an interest-
ing future research direction for multi-stripe scanners. Multi-stripe scanners have
the potential to solve a number of issues associated with single-stripe scanners: illu-
minating a target with two stripes could double the acquisition rate, and projecting
the stripes from different positions reveals points that would otherwise be hidden in
shadow. Current multi-stripe systems rely on different colours or sequences of illu-
mination to disambiguate the stripes (see for example [69]). However, extending the
theoretical developments in this chapter could allow multiple stripes from a single

56
3 Shape Recovery Using Robust Light Stripe Scanning
scanner to be uniquely identiﬁed using the same principles that provide validation
for a single stripe.
3.7 Summary and Conclusions
This chapter presented the theoretical framework and implementation of a robust
light stripe scanner for a domestic robot, capable of measuring natural scenes in am-
bient indoor light. The scanner uses the light plane orientation and stereo camera
measurements to robustly identify the primary reﬂection of the stripe in the presence
of secondary reﬂections, cross-talk and other sources of interference. The valida-
tion and reconstruction framework is based on minimization of an error distance
measured on the image planes. Unlike previous stereo scanners, this formulation is
optimal with respect to the measurement error. An image-based procedure for cali-
brating the parameters of the system from the scan of an arbitrary non-planar target
is also demonstrated.
Results from the experimental scanner demonstrate that the proposed method is
more effective at recovering range data in the presence of reﬂections and cross-talk
than comparable light stripe methods. Experimental results also conﬁrm the assump-
tions of the noise model, and show that image-based calibration produces reliable
results in the presence of noisy image plane measurements. Finally, the optimal re-
constructions from the proposed robust scanner are shown to be more precise than
the reconstructions from a single-camera scanner.
The light stripe scanner proposed in this chapter is suitable for operation in an un-
structured environment on a service robot. However, the acquisition of dense colour
and range data is only the ﬁrst step towards task planning for autonomous manip-
ulations. The range data must now be interpreted to locate and parameterize target
objects for a given task, which is the topic of the next chapter.

4
3D Object Modelling and Classiﬁcation
Locating and classifying unknown objects is a challenging problem in machine per-
ception, but a crucial skill for performing robotic manipulation tasks ﬂexibly and
autonomously. This is particularly evident in domestic applications, where unknown
objects are abundant and instances of the same class (cups, bottles and books, for
example) can vary signiﬁcantly in size, colour and shape. In a typical task, the ob-
ject of interest may vary from a speciﬁc instance (ie. your favourite mug) to a more
general class (ie. a cup). Thus, a suitable framework must be developed that includes
classiﬁcation of unknown objects in addition to learning and recognizing speciﬁc
instances.
Many object modelling and recognition techniques are used in practice, and some
of the major approaches were brieﬂy outlined in Section 2.3.2. Object representa-
tions varying from simple view-based templates to abstract colour histograms, each
with corresponding learning and recognition algorithms. The models employed in
this chapter are based on simple 3D geometric primitives such as planes, cylinders,
cones and spheres. The following sections demonstrate how to automatically and ro-
bustly extract these geometric primitives from captured range data (see Chapter 3).
Many domestic objects are readily modelled and recognized in terms of such prim-
itives, and recognition is enhanced by augmenting the geometry with colour and
texture. Furthermore, primitives automatically extracted from range data can be used
to classify unknown objects.
The algorithm developed in this chapter ﬁrst segments range data into regions
modelled by geometric primitives, then constructs an attributed adjacency graph
(nodes representing primitives and edges describing adjacency) to describe the re-
lationship between primitives. Predeﬁned object classes are also represented by
adjacency graphs, and unknown objects in the scene are recognized by apply-
ing sub-graph matching. Speciﬁc objects can be identiﬁed by examining nodal at-
tributes such as size and colour. A critical component of this algorithm is the ro-
bust, non-parametric surface type classiﬁer that provides an initial range data seg-
mentation. Experimental results demonstrate that this new classiﬁer produces less
over-segmentation in the presence of noise than conventional classiﬁers, without ad-
G. Taylor and L. Kleeman: Visual Perception and Robotic Manipulation, STAR 26, pp. 57–83, 2006.
© Springer-Verlag Berlin Heidelberg 2006

58
4 3D Object Modelling and Classiﬁcation
ditional computational expense. Reduced over-segmentation produces larger initial
segments that can be robustly ﬁtted with data-driven geometric primitives.
The following section outlines the motivation for the object modelling and clas-
siﬁcation framework. Section 4.2 introduces the Gaussian image, which is a cen-
tral concept in surface type classiﬁcation. Section 4.3 provides an overview of the
processing steps in the segmentation algorithm, followed by the technical details of
surface type classiﬁcation and ﬁtting geometric primitives in Sections 4.4 and 4.5.
Following segmentation, Section 4.6 describes the construction of adjacency graphs
to classify unknown objects. The basic principles are demonstrated with several sim-
ple classes including boxes and cups. Finally, Section 4.7 provides an experimental
demonstration of the proposed method for a variety of common objects, using real
range images. The surface type classiﬁcation algorithm is also experimentally com-
pared to conventional methods to verify the increased robustness. VRML models of
all results can be found in the Multimedia Extensions, along with source code and
data sets to implement the algorithms.
4.1 Motivation
To achieve maximum ﬂexibility, a domestic robot must deal with the countless ob-
jects it will encounter in an efﬁcient manner. This motivates the development of a
3D object recognition framework that can identify instances, which may not have
been previously seen, of classes of objects using minimal prior knowledge. The
framework must also allow new classes or speciﬁc instances of objects to be au-
tonomously learned. The selection of an appropriate object representation is criti-
cal to achieving these goals. Representations in previous work on 3D object recog-
nition include CAD models [87], data-driven geometric primitives [167], General-
ized Cylinders [23,126], and non-parametric approaches [108]. Scene interpretation
based on CAD models may provide good classiﬁcation, but does not satisfy the re-
quirement of minimal prior knowledge. At the other extreme, non-parametric inter-
pretations do not attempt to model any part of a scene. This approach limits task
planning, as objects cannot be classiﬁed or extrapolated into occluded regions.
Classiﬁcation based on data-driven parametric surfaces offers a suitable compro-
mise for handling unknown objects. Generalized Cylinders is one such approach,
in which an object is modelled as a planar cross-sectional function swept about an
axis. The cross-section and axis may be arbitrary, but simple functions are chosen
in practice to minimize computational expense. This approach has been used suc-
cessfully for scene interpretation in robotic applications, using both range data [126]
and image-based measurements [23]. However, Generalized Cylinders are unsuitable
for most tasks since the simpliﬁed shapes used in practice do not provide sufﬁcient
detail for classifying diverse objects. Based on the above considerations, geometric
primitives (planes, spheres, cylinders and cones) are chosen as the preferred frame-
work for data-driven modelling in this research. Geometric primitives are relatively
inexpensive to compute, provide distinct classes to aid classiﬁcation, and faithfully
describe the appearance of many simple domestic objects.

4.2 The Gaussian Image in Computer Vision
59
A variety of range data segmentation algorithms using parametric surface mod-
els have been presented in previous work, and are typically based on a few simple
principles. The most common region growing approach involves growing a set of
seed regions by iteratively adding range elements that satisfy a hypothesized surface
model. Typical surface models include geometric primitives [100] and variable-order
bivariate polynomials [12]. The complementary approach assumes range elements
are initially connected, and splits the scene into regions at depth discontinuities, lo-
cal extremes in curvature [35] and changes in local surface shape (also known as
surface type) [50,154]. Split-and-merge techniques allow regions to merge after the
initial segmentation or split further to improve global consistency [44]. A compari-
son of prior work in range data segmentation can be found in [62].
The segmentation algorithm presented in this chapter requires only a single split
and merge step, by exploiting the increased robustness achieved by ﬁtting primi-
tives to larger initial segments. The algorithm attempts to maintain large segments
by initially partitioning range data at jump boundaries and creases, and then further
at surface type boundaries if the initial segments are not adequately modelled. Sub-
sequent merging of over-segmented regions is based on an iterative boundary cost
minimization scheme [44]. While the steps in the segmentation algorithm are not
new, the main novelty of this work is the surface type classiﬁer that achieves greater
robustness to noise than existing methods, and signiﬁcantly reduces the degree of
initial over-segmentation.
Surface type classiﬁcation is the process of assigning a local shape descriptor to
each range element, and is a central component of the segmentation algorithm. Con-
ventional classiﬁcation is based on estimating the mean and Gaussian curvatures by
ﬁtting parametric surfaces to the range data [12, 50, 154]. However, this process is
sensitive to measurement noise, and the arbitrary selection of an approximating sur-
face can introduce systematic errors [90]. This book makes an important contribution
to surface type analysis by demonstrating that robust classiﬁcation can be performed
without the need for parametric surfaces. The proposed classiﬁer is based on the
analysis of principal curvatures (from the Gaussian image [64]) and surface convex-
ity of small patches of range data. Six surface types are distinguished with greater
robustness to noise than conventional classiﬁers, and without additional computa-
tional expense. Thus, the proposed classiﬁer simpliﬁes segmentation by producing
larger initial segments of homogeneous surface type.
4.2 The Gaussian Image in Computer Vision
Figure 4.1 describes the relationship between the surface representation and the cor-
responding Gaussian image of a cylinder. A normal vector can be calculated at each
point on the surface by applying a local planar approximation. The Gaussian image
is then formed by discarding the spatial information and plotting the normal vectors
on a unit sphere (also known as a Gaussian sphere). The extended Gaussian im-
age (EGI) is a useful representation for polyhedra, and is constructed by associating
with each normal vector a scalar value proportional to the area of the corresponding

60
4 3D Object Modelling and Classiﬁcation
cylinder
Gaussian image of cylinder
Fig. 4.1. The Gaussian image of a cylinder.
face. Kang and Ikeuchi [79] extend the concept even further with the introduction of
the complex extended Gaussian image, which encodes the area, position and normal
direction as a complex value on the Gaussian sphere.
In computer vision research, Gaussian images have found application in common
tasks including object recognition [120], localization [79] and segmentation [28].
The usefulness of the EGI arises from the property that it is more compact than the
associated surface model but nevertheless provides a unique representation for con-
vex polyhedra, ie. the mapping between surface model and Gaussian image is invert-
ible [64]. Often, computer vision algorithms do not use the EGI directly, but instead
compute a discrete approximation called an orientation histogram. The Gaussian
sphere is tessellated into a ﬁnite number of bins, and each records the number of
range elements with a normal vector falling within its bounds. Various tessellations
are employed, including both uniform [143] and non-uniform [111] schemes.
For object recognition and localization, the Gaussian image is used as a com-
plete template for matching entire objects [79, 120]. Other computer vision tasks
exploit direct analysis of features in the Gaussian image. For example, Okatani and
Deguchi [121] relate the sign of the Gaussian curvature to ‘folds’ in the Gaussian
image, while Sun and Sherrah [143] detect planes of symmetry by exploiting the
association between the symmetry of an object and its Gaussian image. Nayar et
al. [111] extract features such as moments of inertia, homogeneity and polygonality
from an orientation histogram to assess the quality of solder joints (a similar scheme
was revisited by Ryu and Cho [134]). In this case, the analysis was simpliﬁed by the
known relationship between the sensor and target surface.
The surface type classiﬁer proposed in this chapter exploits the observation that
the Gaussian image for different shapes exhibit compact and unique structures. Fig-
ure 4.1 demonstrates how planes reduce to local maxima on the Gaussian image,
while cylinders map to great circles. This property was exploited for range data seg-
mentation in the early work of Dane and Bajcsy [28]. Their analysis involved clus-

4.3 Segmentation Algorithm
61
tering cells in the orientation histogram, then examining the spread of points in a
selected cluster to determine if the associated surface was planar or quadric. How-
ever, the results were found to vary signiﬁcantly with the resolution of the orientation
histogram. Vanden Wyngaerd and Van Gool [166] presented a similar analysis of the
orientation histogram to ﬁnd planes, cylinders and cones in range data. The disad-
vantage of their clustering approach is that it cannot distinguish spheres, which do
not form peaks in the Gaussian image. An alternative framework was presented by
Chaperon and Goulette [19] to identify cylinders in range maps. The analysis is based
on ﬁtting a plane to random sets of points in the Gaussian image, and evaluating a
cost function to assess if the plane passes though a great circle of points. However,
random sampling is sub-optimal and occasionally leads to gross errors. The classi-
ﬁer proposed in this chapter searches for similar structures in the Gaussian image,
and can identify spheres in addition to planes and cylinders. Furthermore, orientation
histograms are not required.
Cohen and Rimey [26] developed a segmentation algorithm using surface type
classiﬁcation from Gaussian images, and is similar in philosophy to the method pro-
posed in this chapter. Local patches of normal vectors are classiﬁed as planar, cylin-
drical or spherical using an iterative maximum likelihood scheme, and pixels are
clustered into regions according to surface type. In contrast, this chapter presents a
closed-form solution to the classiﬁcation problem, and includes analysis of convexity
to distinguish six surface types instead of only three.
4.3 Segmentation Algorithm
We now outline the segmentation algorithm for extracting geometric primitives from
range data, which is based on a conventional split then merge approach. The input is a
range image of 3D points Mi. The orientation of each surface element is represented
by Ni, and the set of points in the M ×M surface patch centred at Mi are denoted by
Ri (typically M = 15). The steps in the segmentation algorithm are outlined below:
1. Approximate surface normals Ni are calculated for each element by taking the
cross product of average surface tangents over rows and columns of the surface
patch Ri. This method is faster than the usual plane ﬁtting approach and sufﬁ-
ciently accurate for the purpose of segmentation.
2. Surface patches are classiﬁed as spanning a depth discontinuity when the average
point ¯Mi is sufﬁciently far from the centre:
| ¯Mi −Mi| > dM,
¯Mi = 1
M2 ∑
j∈Ri
M j
(4.1)
for a ﬁxed threshold dM. Patches are classiﬁed as spanning a crease when the
maximum magnitude of the difference between the central normal and another
normal in the patch exceeds a threshold dN:
max
j∈Ri |Nj −Ni| > dN
(4.2)
Points near discontinuities and creases are removed from the range map.

62
4 3D Object Modelling and Classiﬁcation
3. Binary connectivity is applied to the remaining elements to identify smoothly
connected regions. Using the methods described later in Section 4.5, the algo-
rithm attempts to ﬁt a geometric primitive to each region, selecting the model
with the lowest residual error as the best description.
4. Surface normals are recalculated for each patch Ri using only data from within
the same smoothly connected region to reduce perturbations that arise at surface
discontinuities. Using the algorithm in Section 4.4, each element is then classi-
ﬁed into one of the six surface types illustrated in Figure 4.2, by analysing the
Gaussian image and convexity of normal vectors in the patch.
5. Each region is checked for consistency between the ﬁtted model and dominant
surface type. For example, regions ﬁtted as cylinders should have a high pro-
portion of surface elements classiﬁed as ridges or valleys. If the ﬁtted primitive
does not agree with a sufﬁcient proportion of the surface type for elements in the
same region (less than 70% in the current implementation), the region is split
into sub-regions of homogeneous surface type. The algorithm then attempts to
ﬁt a geometric primitive to sub-regions larger than a threshold size. If a region
cannot be modelled after splitting by surface type, the constituent pixels are la-
belled as unclassiﬁed and allowed to join other regions in the following step.
6. Iterative region growing assigns unlabelled range elements to existing regions.
Regions are iteratively grown by adding neighbouring pixels that satisfy the con-
straints of the surface model to within a threshold error. After region growing,
normal vectors are recalculated and geometric primitives are re-ﬁtted to each
region, providing an opportunity for models to change if required.
7. Finally, regions are merged using iterative boundary cost minimization [44]. For
each pair of adjacent regions, a geometric model is ﬁtted using the combined
range points. At each iteration, the pair with the lowest combined residue are
merged, and the combined models for all neighbouring regions are updated.
Merging continues until the minimum combined residue for all neighbouring
regions exceeds a threshold.
4.4 Non-parametric Surface Type Classiﬁcation
Surface type classiﬁcation is used to verify ﬁtted primitives and identify homoge-
neous regions when depth discontinuities and creases are insufﬁcient for segmenta-
tion. Six of the eight fundamental surface types deﬁned in [12] are considered here,
as illustrated in Figure 4.2. As noted by Trucco and Fisher [154], the omitted surface
types (saddle ridge and saddle valley) are not perceptually signiﬁcant. Classiﬁcation
is based on two properties of the local surface patch Ri: convexity, and the number
of non-zero principal curvatures (determined from the Gaussian image).
The principal curvatures of Ri are deﬁned as the maximum and minimum cur-
vatures of a line formed by the intersection of the surface patch and a plane parallel
to Ni and passing through the central point Mi. By inspection, both principle cur-
vatures of a planar patch are zero, a cylindrical patch (ridge/valley) has one zero

4.4 Non-parametric Surface Type Classiﬁcation
63
ridge
valley
pit
saddle
peak
plain
Fig. 4.2. Six surfaces types classiﬁed by local shape (the saddle ridge and saddle valley deﬁned
in [12] are not considered in this chapter). Reprinted from [148]. c2003, IASTED. Used with
permission.
and one non-zero principal curvature, and a saddle or spherical patch (pit/peak) has
two non-zero principal curvatures. Thus, patches can be classiﬁed as planar, cylindri-
cal or spherical/saddle by determining the number of non-zero principal curvatures.
Section 4.4.1 shows how the number of non-zero principal curvatures can be robustly
determined from the Gaussian image of Ri. The distinction between the three groups
deﬁned by principal curvatures and the six classes shown in Figure 4.2 depends only
on convexity: ridges/peaks are convex, valleys/pits are concave, and planes/saddles
are neither. Section 4.4.2 describes how to robustly measure convexity from the spa-
tial distribution of normal vectors.
For segmentation, the convexity and number of principal curvatures are com-
bined to assign a surface type label to each patch. Table 4.1 summarizes the classi-
ﬁcation rules and the conditions satisﬁed by each surface type. The main difference
between this method and conventional surface type analysis (based on the sign of the
mean and Gaussian curvature) is the manner in which planes and saddles are treated.
The mean curvature H of a patch is related to the principal curvatures, κ1 and κ2,
by H = 1
2(κ1 +κ2). Thus, the sign of the mean curvature is equivalent to convexity:
pits and valleys are concave (H < 0), peaks and ridges are convex (H > 0), while
planes and saddles are neither (H = 0). The Gaussian curvature K is related to the
principle curvatures by K = κ1κ2. Thus, conventional classiﬁcation places planes
in the same super-class as ridges and valleys (K = 0), while saddles form an inde-
pendent class (K < 0). Conversely, classiﬁcation based on the number of principal
curvatures groups saddles with pits and peaks (two non-zero principle curvatures)
while assigning planes to an independent class.
4.4.1 Principal Curvatures
The Gaussian image of a surface patch Ri is formed by plotting the normal vectors
on the Gaussian sphere. Figure 4.3 plots the Gaussian images for planar, cylindrical

64
4 3D Object Modelling and Classiﬁcation
-0.1
0
0.1
x
-0.1
0
0.1
y
0.97
0.98
0.99
1
z
(a) Plane
-0.1
0
0.1
x
-0.1
0
0.1
y
0.97
0.98
0.99
1
z
(b) Ridge/valley
-0.1
0
0.1
x
-0.1
0
0.1
y
0.97
0.98
0.99
1
z
(c) Pit/peak/saddle
Fig. 4.3. Gaussian images for characteristic surface shapes from experimental data. Reprinted
from [148]. c2003, IASTED. Used with permission.

4.4 Non-parametric Surface Type Classiﬁcation
65
N
j
Amin
Y
θmin
X
Fig. 4.4. Measurement of normal vector distribution for a cylinder (Amin is a vector in the
direction of minimum spread). Reprinted from [148]. c2003, IASTED. Used with permission.
and spherical surface patches obtained from experimental data. Clearly, the planar
normals are clustered in the same direction, the cylindrical normals are distributed
over a small arc and the spherical normals exhibit a uniform spread. These obser-
vations reveal that the number of non-zero principal curvatures can be estimated by
measuring the maximum and minimum spread of normal vectors in the Gaussian
image. We now show how this spread can be determined from the residue of a ﬁtted
plane constrained to pass through the centre of the range patch.
Let Np ∈[0,1,2] represent the number of non-zero principal curvatures and Nj,
j ∈Ri, represent the set of normal vectors in the surface patch Ri, with the central
normal denoted as Ni. To simplify the analysis, the normal vectors Nj are rotated to
align Ni with the Z-axis Z. This transformation can be written as
N
j = R[cos−1(N
i Z),Ni × Z,]·Nj
(4.3)
where R(φ,A) is the rotation matrix for a rotation of angle φ about axis A. Figure 4.4
illustrates the transformed normal vectors projected onto the XY-plane for the case of
a cylindrical patch. Let A = (cosθ,sinθ,0) represent a unit vector in the XY-plane
with angle θ to the X-axis. Now, the spread of normal vectors in direction A can be
measured as the residue of a plane with normal A, constrained to pass through the
origin (ie. coplanar with Ni), ﬁtted to N
j. The mean square error in direction A is:
e = 1
M2 ∑
j∈Ri
(N
j A)2 = 1
M2 ∑
j∈Ri
(Xj cosθ +Yj sinθ)2
(4.4)
where N
j = (Xj,Yj,0). The maximum and minimum spread of normal vectors are at
the extremum of equation (4.4); Figure 4.4 illustrates the ﬁtted plane and direction of
minimum spread (for a cylinder). The optimization is performed in the usual manner
by setting de
dθ = 0, which results a quadratic function of tanθ:
∑
j∈Ri
(XjYj tan2 θ +(X2
j −Y 2
j )tanθ −XjYj) = 0
(4.5)

66
4 3D Object Modelling and Classiﬁcation
Nj ×Ni
D j
Mj
(Nj ×Ni)×Dj
Mi
Ni
N j
(a) Convex
(Nj ×Ni)×D j
N j ×Ni
D j
M j
Ni
N j
Mi
(b) Concave
Fig. 4.5. Measuring the convexity between two surface points Mi and M j. Reprinted from
[148]. c2003, IASTED. Used with permission.
Equation (4.5) has the standard solutions:
θmax,θmin = tan−1

∑j(Y 2
j −X2
j )±
√
Δ
2∑j(XjYj)

(4.6)
where
Δ = [ ∑
j∈Ri
(X2
j −Y 2
j )]2 +4[ ∑
j∈Ri
(XjYj)]2
(4.7)
The corresponding maximum and minimum residues emax = e(θmax) and emin =
e(θmin) are calculated from equation (4.4). When emax > eth or emin > eth for a ﬁxed
threshold eth, the corresponding principal curvature is considered to be non-zero.
The total number of non-zero principal curvatures Np is calculated as the number
of emin,emax > eth, which classiﬁes the patch as a plane, cylinder or sphere/saddle.
Selection of the threshold eth depends on the quality of the range data, and can be
estimated by observing the spread of normal vectors for a planar surface patch.
4.4.2 Convexity
The convexity of Ri can be determined by observing that the normal vectors in a
concave patch tend to converge towards the centre, while the normals in a convex
patch tend to point away. Figure 4.5 illustrates a robust measure of convexity based
on this principle. For range points in Ri, let Dj = Mj −Mi represent a vector pointing
from the central surface element Mi to a non-central surface element M j. Then,
for each normal vector Nj, the angle between the central normal Ni and the vector
product (Nj ×Ni)×Dj is greater or less than π/2 depending on whether Nj points
towards or away from Ni. Each element j ∈Ri can therefore be classiﬁed as locally
convex or concave using the following test:
[(Nj ×Ni)×Dj]Ni

> 0 : convex
< 0 : concave
(4.8)

4.5 Fitting Geometric Primitives
67
Table 4.1. Surface type classiﬁcation rules
Num. Principal Curvatures
Np = 0 Np = 1
Np = 2
Convex
Plane
Ridge
Peak
S > Sth
Concave
Plane
Valley
Pit
S < 1/Sth
Neither
Plane Saddle
Saddle
1/Sth < S < Sth
After evaluating the convexity of each element j, the number of locally convex
elements Ncv and locally concave elements Ncc is counted over the whole patch Ri.
Now, let S = Ncv/Ncc represent the ratio of the number of locally convex to locally
concave elements. Global patch convexity is determined by the dominant local prop-
erty: convex when S > Sth, concave when S < 1/Sth, or neither when 1/Sth < S < Sth,
where Sth is a ﬁxed threshold ratio. The threshold is ideally unity, but in practice
Sth = 1.5 is used to ensure that the dominant property represents a signiﬁcant ma-
jority. Finally, the statistical measures Np and S are combined to characterize each
patch into one of the six classes shown in Figure 4.2, using the classiﬁcation rules
summarized in Table 4.1.
It should be noted that if the range elements in a patch closely follow one of the
classiﬁed shapes, the local convexity in equation (4.8) could have been measured
using only the difference in range points Dj and non-central normal Nj with the
simpliﬁed condition:
N
j Dj

> 0 : convex
< 0 : concave
(4.9)
However, the condition given by equation (4.8) was found to be more accurate in the
presence of range data noise without excessive computational expense.
4.5 Fitting Geometric Primitives
Geometric primitives are ﬁtted to range segments during three stages of the segmen-
tation algorithm: after the initial segmentation due to creases and depth discontinu-
ities, after growing regions with unlabelled range elements, and ﬁnally when deter-
mining whether a pair of segments should be merged. Each time a geometric model
is required, the algorithm ﬁts all possible primitives to the range data and selects the
description with the minimum mean square error. Model ﬁtting is thus an important
and computationally expensive component of the algorithm and requires an efﬁcient,
robust solution. The following sections describe techniques that can be used to ﬁt
planes, spheres, cylinders and cones, including novel estimates of the initial model
parameters for passing to a numerical least squares solver.

68
4 3D Object Modelling and Classiﬁcation
4.5.1 Planes
Planes are parameterized by a normal vector N and a perpendicular distance to the
origin d, determined using least squares regression. For a segmented region with N
position vectors Mi, i = 1...N, the normal vector N is the eigenvector associated
with the smallest absolute eigenvalue λ of the covariance matrix Λ for mi [36],
which is given by
Λ = 1
N
N
∑
i=0
(Mi −¯M)(Mi −¯M),
¯M =
N
∑
i=0
Mi
(4.10)
The perpendicular distance to the origin is calculated from the mean position ¯M as
d = −N· ¯M. The mean square regression error is the minimum eigenvalue λ.
4.5.2 Spheres
Spheres are parameterized by a radius r and centre C, which are estimated by mini-
mization of the mean square distance e2
sph of elements Mi from the estimated surface:
e2
sph = 1
N ∑
i
(|Mi −C|−r)2
(4.11)
The optimization is performed using Levenberg-Marquardt (LM) minimization, as
implemented in MINPACK [105]. Fast convergence of the algorithm relies on ac-
curate initial estimates of the model parameters, denoted as r0 and C0, and a novel
method for robustly estimating these values is now introduced. Ideally, the centre of
the sphere is located at a distance of r in the direction of the surface normal Ni from
the corresponding surface point Mi. Assuming Ni are uniformly directed towards or
away from C and taking the mean over all samples, the centre of the sphere C0 for a
given radius r0 is estimated as:
C0 = 1
N ∑
i
(Mi +r0Ni)
(4.12)
The error variance in the estimated C0 is:
σ2
C = 1
N ∑
i
(Mi +r0Ni −C0)(Mi +r0Ni −C0)
(4.13)
The initial radius r0 is calculated as the value that minimizes the variance in the
estimate of C0. Setting dσ2
C
dr0 = 0 gives:
r0 = −N ∑i M
i Ni −∑i M
i ∑i Ni
N ∑i N
i Ni −∑i N
i ∑i Ni
(4.14)
The initial estimate of C0 is obtained by substituting equation (4.14) into equation
(4.12). Finally, |r0| and C0 provide initial values for the numerical minimization of
equation (4.11).

4.5 Fitting Geometric Primitives
69
α
r
φ
θ
P
A
d
Nφ
Nθ
N
Z
X
Y
Fig. 4.6. Parameterization of a cylinder.
4.5.3 Cylinders
Cylinders are parameterized using the principles developed in [100], and illustrated
in Figure 4.6. Let P represent the point on the axis of the cylinder nearest to the
origin, which can be represented as a distance d and direction N in polar coordinates
(θ,φ):
P = dN,
N = (cosφ sinθ, sinφ sinθ, cosθ)
(4.15)
Let A represent a unit vector in the direction of the cylinder axis, noting that A and
N are orthogonal. Two basis vectors, Nθ and Nφ, which are orthogonal to each other
and N, are constructed by taking the partial derivatives of N and normalizing to unit
length:
Nθ = (cosφ cosθ, sinφ cosθ, −sinθ)
(4.16)
Nφ = (−sinφ, cosφ, 0)
(4.17)
The axis A can now be expressed as a function of Nθ, Nφ and a single parameter α:
A = Nθ cosα +Nφ sinα
(4.18)
The complete parameterization of the cylinder includes d, θ, φ, α and radius r. As
before, the LM algorithm is used to estimate the parameters by minimizing the mean
square distance e2
cyl of range points Mi from the surface of the cylinder. The error
function is:
e2
cyl = 1
N ∑
i
(|Di|−r)2,
Di = P+A(Mi −P)A−Mi
(4.19)
where P and A are given by equations (4.15) and (4.18).
As before, rapid convergence of the model requires accurate initial estimates
of the parameters. An initial estimate of the cylinder axis A0 is obtained using an
extension of the surface classiﬁcation algorithm in Section 4.4. First, the normal

70
4 3D Object Modelling and Classiﬁcation
α
φ
θ
r
δ
P
d
A
C
Nφ
Nθ
N
Y
X
Z
Fig. 4.7. Parameterization of a cone.
vectors are transformed as N
i = R[cos−1( ¯NZ), ¯N× Z]Ni to align the mean normal
with the Z-axis, where the mean normal is given by
¯N = N
|N|,
N =
N
∑
i=1
Ni
(4.20)
Using equations (4.6)-(4.7), the transformed cylinder axis A
0 is approximated as the
vector in the XY-plane that minimizes the spread of normals in the Gaussian image,
as shown by Amin in Figure 4.4. The cylinder axis A0 is recovered by taking the
inverse transformation A0 = R−1[cos−1( ¯NZ), ¯N× ˆZ]A
0.
The initial estimates r0 and P0 are obtained using the method introduced in the
Section 4.5.2. The samples Mi and normals Ni are rotated to align A0 with the Z-axis,
then projected onto the XY-plane to give the circular cross-section of the cylinder.
The centre C
0 and radius r0 of the circle are estimated from the projected data using
equations (4.12) and (4.14). Finally the centre is transformed back to the original
frame as C0, and the closest point on the axis is recovered as P0 = C0 −A0C
0 A0.
4.5.4 Cones
Cones are parameterized in a similar manner to cylinders, as illustrated in Figure
4.7. Let P represent the point on the axis closest to the origin, and A represent a unit
vector in the direction of the axis. Using equations (4.15)-(4.18), these vectors are
again parameterized in terms of d, θ, φ and α. Now, if the apex is located at C, and
the half-angle of the cone is δ, the radius r at P along the axis can be calculated as:
r = (CA)tanδ
(4.21)
Thus, the cone is completely parameterized in terms of d, θ, φ, α, r and δ. Again,
the LM algorithm is used to estimate the cone parameters by minimizing the mean
square distance e2
cone between the samples Mi and the surface of the cone. The error
function is given by:

4.6 Object Modelling and Classiﬁcation
71
e2
cone = 1
N ∑
i
(|Di|−ri)2,
(4.22)
where
Di = C+A(Mi −C)A−Mi
ri = [(Mi −C)A]tanδ
To obtain initial estimates for the parameters, it is ﬁrst noted that a normal vector
Ni on the surface of an ideal cone has angle ψ = π
2 −δ to the cone axis A. For a
given estimate of the axis A, this angle can be estimated as ψi(A) = cos−1(N
i A) for
measured normal Ni. Taking the average over all normals, an estimate for ψ0 and its
variance σ2
ψ are calculated as:
¯ψ0 = 1
N ∑
i
cos−1(N
i A)
(4.23)
σ2
ψ = 1
N ∑
i
[cos−1(N
i A)−¯ψ]2
(4.24)
The axis can now be estimated as the direction that minimizes the variance in the
above estimate of ψ0. LM minimization is used with A expressed in polar coordinates
and initially set to the direction of the Z-axis. It should be noted that the same method
could have been used to estimate the axis of a cylinder, although the closed form
solution presented in Section 4.5.3 is preferred. Once the initial estimate A0 is found,
ψ0 is calculated from equation (4.23) and an initial value for the cone half angle is
recovered as δ0 = π
2 −ψ0.
The initial estimates C0 and r0 are obtained in a similar manner to the parame-
ters of a cylinder. First, range elements Mi and normals Ni are rotated to align A0
with the Z-axis. Range points M
i are then projected away from the apex, tangen-
tially to the surface of the cone, onto the XY-plane. For point M
i, the direction of
projection is Si = R(π −δ, Z × N
i)Z, and the resulting point on the XY-plane is
M
i = M
i −(m
iz/siz)Si. The centre and radius of the projected arc are then estimated
using equations (4.12) and (4.14). Finally, the results are transformed back to the
original frame to recover C0 and r0.
4.6 Object Modelling and Classiﬁcation
The purpose of the scene analysis and object modelling algorithms presented in this
section are to associate the abstract mathematical descriptions extracted by segmen-
tation with meaningful semantics for high-level task planning. As mentioned earlier,
the implementation described in this book is limited to simple convex objects such as
boxes and cans to demonstrate the basic principles. To aid the following discussion,
the scene shown in Figure 4.8(a) is analyzed as a speciﬁc example. In this case, a
successful analysis should identify the presence of a box, ball and cup, and generate
suitable textured polygonal models of the objects.

72
4 3D Object Modelling and Classiﬁcation
P3
P2
S1
P5
P4
P1
C1
(a) Raw colour/range scan.
P2
P3
P4
P1
C1
S1
P5
box
cc
cc
cc
cc
cc
cc
cc
cc
cc
cc
cc
ball
cup
cc
cv
(b) Adjacency graph and convex objects.
Fig. 4.8. Sample objects for scene analysis and object modelling. The extracted models are
shown in Figure 4.11(d).
To recognize unknown objects, the system represents generic classes of objects
using attributed adjacency graphs. The nodes of a graph represent geometric primi-
tives, with attributes describing shape parameters and texture information, and edges
represent the type of connection (convex or concave) between primitives. Thus, a box
is recognized as two or three orthogonal, convexly connected planes (the remaining
planes are unobservable), while a can is a cylinder (or cone) convexly connected to a
plane. Further constraints (such as the allowed radius of a cylinder) can be placed on
the nodal attributes to distinguish similarly shaped objects such as cups and bowls.
The ﬁrst step in scene analysis is to generate an attributed graph describing the
relationship between geometric primitives in the observed scene. Segmentation pro-
vides a list of neighbouring regions, and the following tests are applied to determine
the type of connection (convex or concave) between adjacent primitives:
Plane/Plane: The convexity of the edge between two planes is determined using the
principles described in Section 4.4.2. Let n1 and n2 represent the normal vectors
of the adjacent planes, and m1 and m2 represent the average position of range
elements associated with each plane. Then, following the simpliﬁed convexity
condition in equation (4.9), the convexity of the edge joining the planes is clas-
siﬁed according to:
N
1 (M2 −M1)

< 0 : convex
> 0 : concave
(4.25)
Plane/Cylinder or Plane/Cone: The convexity of an edge between a cylinder (or
cone) and a plane is determined by noting that for a closed cylinder, the points
on the end-plane lie within the radius of the cylinder. Conversely, the points
on the concavely connected planar rim (such as the brim of a hat) lie outside
the radius of the cylinder. Thus, the convexity of the connection is classiﬁed by
measuring the average distance of the N points Mi in the plane from the axis A

4.6 Object Modelling and Classiﬁcation
73
of the cylinder:
1
N ∑
i
|Mi −[P+A(Mi −P)A]|

< r : convex
> r : concave
(4.26)
where P is a point on the axis of the cylinder and r is the radius (average radius in
the case of a cone). A similar analysis could be used to determine the convexity
of an edge between a plane and a sphere.
For the simpliﬁed analysis considered in this chapter, the edges joining all re-
maining adjacent primitives are classiﬁed as concave. Figure 4.8(b) shows the at-
tributed graph for the scene in Figure 4.8(a), where Pi, Ci and Si denote planes,
cylinders and spheres, and cc and cv describe concave and convex connections re-
spectively. Convex objects are identiﬁed by removing all concave connections and
matching the remaining (convexly connected) sub-graphs to the models for generic
classes of objects. Sub-graphs that do not match any class (such as P1, P2 and P5 in
Figure 4.8) are ignored. Four classes are identiﬁed in the current implementation:
boxes, cups/bowls, cans and balls. For the example scene in Figure 4.8(b), the classi-
ﬁed sub-graphs are shown by the dotted circles. Finally, a textured polygonal model
is generated for each classiﬁed object based on the primitives in the sub-graph, as
described in the following sections.
4.6.1 Modelling a Box
A rectangular box is modelled as three pairs of parallel planes, with adjacent sides ap-
proximately orthogonal. Let Ai and Bi, i = 1...3, represent the three pairs of parallel
planes, where Ai is closest to the observer for each pair. Plane pairs are parameter-
ized by a common normal Ni and two perpendicular distances ai and bi to the origin,
where ai < bi. At most, three sides of the box are visible (planes Ai), and determine
the normal vectors Ni and distances ai. If only two sides are visible, the normal vec-
tor for the third pair is calculated as N3 = N1 × N2, based on the assumption that
adjacent faces are orthogonal.
The distance parameters bi of the hidden faces (including a3 if only two faces
are visible) are then determined by ﬁtting planes to the edges of the visible surfaces
using the voting scheme illustrated in Figure 4.9. In this example, b3 is calculated by
extracting the set of range points Ej along the boundary of the segments associated
with adjacent faces A1 and A2. The voting scheme assumes that B3 will be coincident
with one of the points Ej along the adjacent edges. Each edge point Ej is assigned a
likelihood cj that measures the evidence that B3 is coincident with Ej. The likelihood
is calculated as:
cj = ∑
k
(|N
3 (Ek −Ej)|2 +1)−1
(4.27)
and increases as more edge points are approximately coincident with B3 passing
through Ej. Two local maxima exist, corresponding to the front and rear planes in the
pair, and the distance parameter for the hidden plane is calculated as b3 = −N
3 Emax,
where Emax is the edge point for the most distant maxima.

74
4 3D Object Modelling and Classiﬁcation
hidden
face
ﬁtted
plane
edge points of
visible faces
position
cj
c2
c1
B3
A1
A2
E2
N
E1
Fig. 4.9. Calculating the position of the hidden face B3 with known normal N3 from edge
points E j (solid black circles) of adjacent faces. The cost function c j in equation (4.27) gives
evidence for the hypothesis that the plane passes through E j, and has been evaluated for points
E1 and E2 (hollow black circles). The cost function indicates that E1 is supported by more
coplanar edge points than E2.
Finally, a polygonal model of the box is constructed. Eight corner vertices at the
intersections of the planar faces are found by solving linear systems such as


N
1
N
2
N
3

V1 =


a1
a2
a3


(4.28)
for the intersection V1 between A1, A2 and A3. The remaining vertices are calculated
similarly, and are used to deﬁne the six polygonal faces of the box. The colour/range
segment associated with each face is projected onto the surface of the box and ren-
dered into a texture map using conventional computer graphics techniques.
4.6.2 Modelling a Cup/Bowl/Can
Convex cylindrical objects are modelled as cones or cylinders, and zero or more par-
allel end-planes. Let A represent the axis of the cylinder or cone, N represent the
common normal of the end-planes, and d1, d2 represent the perpendicular distances
of the end-planes from the origin, where d1 corresponds to the upper plane. If either
end-plane is visible, the normal direction of the plane is assigned to N, and the dis-
tance parameter is assigned to d1 (or d2 if the lower plane is visible). Otherwise, the
end-plane normal N is assumed to be parallel to the axis A. The distance parameters
di for the unobserved end-planes are determined using the voting scheme introduced

4.7 Experimental Results
75
above. Boundary points are extracted from the range data for the cylindrical region,
and the support for a plane with normal N passing through each edge point is cal-
culated from (4.27). Again, the likelihood function exhibits two local maxima corre-
sponding to the two end-planes, and the distance di to the hidden plane is calculated
appropriately.
Finally, a textured polygonal model is constructed to approximate the cylindrical
object. Two sets of vertices are generated at ﬁxed angles around the top and bot-
tom rim where the cylinder intersects the end-planes. The vertices are tessellated
using triangular facets, and the range/colour data is projected onto the surface of
each primitive and rendered into a texture map using conventional computer graph-
ics techniques.
4.6.3 Modelling a Ball
Since a ball is represented by single sphere, constructing an approximate polygo-
nal model is more straightforward than the objects considered above. Intersections
between spheres and other geometric primitives are not considered in the current
implementation. A polygonal approximation is constructed by parameterizing the
surface of the sphere using spherical coordinates, and generating vertices at regular
intervals of longitude and latitude. While other tessellations could be used generate a
more uniform distribution of vertices, this scheme was found to be sufﬁcient for the
experiments considered in here. The range/colour data is ﬁnally projected onto the
surface of the sphere and rendered into a texture map (in spherical coordinates).
4.7 Experimental Results
The algorithms described in this chapter were tested using experimental range im-
ages at half-PAL resolution (384 × 288 pixels) from the robust stereoscopic light
stripe scanner described in Chapter 3. Section 4.7.1 presents the segmentation re-
sults for selected scenes containing a variety of domestic objects. Section 4.7.2 then
compares the noise robustness of the proposed surface type classiﬁer to the conven-
tional approach, by applying both algorithms to the same range data. VRML models
of the results can be found in the Multimedia Extensions.
4.7.1 Segmentation and Object Modelling
The ﬁrst scene in Figure 4.10(a) contains typical domestic objects described by
simple geometric primitives: a box, ball and cup. Figure 4.10(b) shows the result
after removing depth discontinuities and creases, and calculating the surface type.
Range elements with identical surface type are represented in homogeneous colour.
The surface labels generally agree with perceptual expectations, although some mis-
classiﬁcation is apparent near discontinuities and creases where normal vectors are
less reliable. The primitives ﬁtted to regions segmented by discontinuities and creases

76
4 3D Object Modelling and Classiﬁcation
(a) Colour/range scan of box, ball and cup.
plane
pit
peak
valley
ridge
saddle
(b) Discontinuity/crease removal and surface type classiﬁcation.
(c) Final segmentation.
(d) Classiﬁed object models.
Fig. 4.10. Results for box, ball and cup (see also Multimedia Extensions).

4.7 Experimental Results
77
Table 4.2. Computational expense of main steps in segmentation.
Computation
Time (ms)
normal vector calculation
1730
discontinuity/crease removal 800
surface type labeling
2860
region growing
232
region merging
13200
agree well with the dominant surface type, and further segmentation in this case was
not necessary. The ﬁnal segmentation after growing and merging regions is shown in
Figure 4.10(c), and is consistent with qualitative expectations. Finally, Figure 4.10(d)
shows a textured rendering of the objects identiﬁed as possible grasping targets for
the robot. The complete analysis required about 24 seconds on a 2.2 GHz dual Intel
Xeon PC, and Table 4.2 provides a breakdown the the computational expense for
the main components of the algorithm. Clearly, the ﬁnal merging stage (which relies
heavily on ﬁtting geometric primitives) is the most expensive computation, while
surface type classiﬁcation is comparatively cheap.
In the previous scene, depth discontinuities and creases were sufﬁcient to seg-
ment the geometrically simple objects. This is contrasted by the situation shown in
Figure 4.11(a), which involves objects of greater complexity: a bowl, funnel and
(upside-down) goblet. Each object is comprised of multiple smoothly-connected
primitives of different geometry. Figure 4.11(b) shows the result of discontinu-
ity/crease removal and surface type classiﬁcation. Clearly, discontinuities and creases
provide an incomplete segmentation, and surface type homogeneity must be con-
sulted to isolate simple primitives. As expected, the funnel and goblet are both
divided into approximately equal areas of peaks and ridges, but the bowl is over-
segmented into planes, ridges, valleys and pits due to the subtle curves in the wall
and lip. However, this over-segmentation is corrected in the subsequent region grow-
ing/merging stage as shown by the ﬁnal segmentation in Figure 4.11(c) (although
segmentation of the lip is still not ideal).
The geometric primitives corresponding to the objects are shown in Figure
4.11(d). In this case, the funnel and goblet have not been classiﬁed as distinct ob-
jects, since no generic class for these shapes were deﬁned. However, the component
primitives agree well with qualitative expectations: the funnel contains a spherical
body, conical spout and planar handle, and the cup is decomposed into a planar base,
cylindrical stem and a body comprised of a spherical base and cylindrical walls.
These results lend support to the possibility of learning new classes based on the ob-
served primitives. Note also that this result (and the result in Figure 4.10(d)) demon-
strates the ability to handle partial occlusion. In the ﬁrst scene, the box was partially
obscured by the ball, while both foreground objects masked the funnel in the second.
Object modelling was not affected in either case, since sufﬁcient data was available
to establish the correct surface model.

78
4 3D Object Modelling and Classiﬁcation
(a) Colour/range scan of bowl, funnel and goblet.
plane
pit
peak
valley
ridge
saddle
(b) Discontinuity/crease removal and surface type classiﬁcation.
(c) Final segmentation.
(d) Classiﬁed object models.
Fig. 4.11. Results for bowl, funnel and goblet (see also Multimedia Extensions).

4.7 Experimental Results
79
(a) Colour/range scan of bottle and bowl.
plane
pit
peak
valley
ridge
saddle
(b) Discontinuity/crease removal and surface type classiﬁcation.
(c) Final segmentation.
(d) Classiﬁed object models.
Fig. 4.12. Results for bottle and bowl (see also Multimedia Extensions).

80
4 3D Object Modelling and Classiﬁcation
Figure 4.12 shows an example of a sauce bottle with subtly curved body that can-
not be accurately modelled in terms of simple geometric primitives. Figure 4.12(b)
demonstrates the high sensitivity of the surface type classiﬁer to distinguish the con-
ical upper-half of the bottle from the pseudo-spherical lower-half. The ﬁnal segmen-
tation and model in Figures 4.12 and 4.12 show that a single cone was chosen as the
most accurate interpretation of the bottle. Obviously, the sensitivity of the algorithm
to subtle variations in surface shape can be adjusted by varying the thresholds for
region splitting and merging.
A ﬁnal example is shown in Figure 4.13(a), and the target in this case is a shal-
low pyramid approximately 5 cm wide and 3.5 cm high. As before, discontinuities
and creases are insufﬁcient for a complete segmentation, but in this case the scene
contains only planar surfaces. Figure 4.13(b) shows the outcome of surface type
classiﬁcation, with some interesting results. While the central portion of each face is
correctly classiﬁed as planar, the edges are perceived as ridges or valleys depending
on convexity. Similarly, the corners are labelled as saddle points. On closer inspec-
tion, these results accurately reﬂect the topology of the scene but nevertheless lead
to an over-segmentation. Figure 4.13(c) shows the ﬁnal segmentation after growing
and merging the initial regions. In this case, the merging step was unable to com-
pletely correct the over-segmentation, and the convex edges between each face are
still identiﬁed as cylindrical. If this result was used for object recognition, the error
might be corrected by introducing heuristics regarding the possible shape of planes
intersecting at a shallow angle. Regardless of the segmentation result, Figure 4.13(b)
demonstrates the good sensitivity of the proposed surface type classiﬁer to subtle
variations in shape.
4.7.2 Curvature-Based Versus Non-parametric Surface Type Classiﬁers
In this section, the performance of the proposed non-parametric surface type classi-
ﬁer is compared to conventional curvature-based techniques. The surface-type clas-
siﬁer described by Besl and Jain [12] was chosen as representative of this approach.
This method performs equally-weighted least squares estimation of ﬁrst and second
order directional derivatives by convolving the range image with window operators.
The sign of the mean and Gaussian curvatures are then computed from the deriv-
atives and used to classify each pixel. A method to reduce computational cost is
also described, by combining smaller derivative estimation windows with Gaussian
smoothing. However, this optimization was not implemented, since the size of the
smoothing window was noted to inﬂuence the estimated derivatives, for a compara-
tively small gain in performance.
The curvature-based surface type classiﬁer was applied to the scene shown in
Figure 4.10. The range image was computed by taking the magnitude of each 3D
point in the range map, and 15×15 pixel window operators were applied to calculate
the surface type. From examination of experimental trials, thresholds εH = 0.005 and
εK = 0.0001 were chosen for detection of non-zero mean and Gaussian curvatures.
The processing time required by the curvature-based classiﬁer was approximately
the same as the non-parametric algorithm, and Figure 4.14 shows the classiﬁcation

4.7 Experimental Results
81
(a) Colour/range scan of pyramid.
plane
pit
peak
valley
ridge
saddle
(b) Discontinuity/crease removal and surface type classiﬁcation.
(c) Final segmentation.
Fig. 4.13. Results for pyramid.

82
4 3D Object Modelling and Classiﬁcation
plane
pit
peak
valley
ridge
saddle
Fig. 4.14. Curvature-based surface type result (compare to Figure 4.10(b)).
result. The areas of particular interest are the ball and interior/exterior faces of the
cup, which should be classiﬁed as peak, valley and ridge respectively. Detection of
the correct shape in these areas is patchy, and modifying the detection thresholds to
improve the result tended to degrade other parts of the image. The equivalent analysis
using the non-parametric classiﬁer is shown in Figure 4.10(b). Clearly, the method
proposed in this chapter labels the ball and cup with signiﬁcantly greater consistency,
and much less noise is evident in the planar ﬂoor and wall.
As noted by Besl and Jain, the second order derivative estimates required to cal-
culate mean and Gaussian curvatures result in a high sensitivity to noise. Their pro-
posed segmentation process overcomes this problem by eroding large areas of homo-
geneous surface type into small (approximately 10 pixel) seed regions, then dilating
the seeds using a model-based region growing algorithm. Conversely, the robust la-
belling produced by the non-parametric classiﬁer proposed in this chapter eliminates
the need for erosion/dilation, and produces large regions of homogeneous surface
type that can be directly treated as initial segments. Furthermore, the improved ro-
bustness comes without additional computational expense.
4.8 Discussion and Conclusions
This chapter presented a new method for surface type classiﬁcation based on analy-
sis of principal curvatures (from the Gaussian image) and convexity, and utilized the
techniques for data-driven object modelling and classiﬁcation. The proposed non-
parametric classiﬁer eliminates the need for choosing arbitrary approximating sur-
face functions as required by methods based on the mean and Gaussian curvature.
The new method is comparable in computational expense to existing classiﬁers, but
exhibits signiﬁcantly greater robustness to noise and produces large regions of ho-
mogeneous surface type consistent with perceptual expectations. The proposed clas-

4.8 Discussion and Conclusions
83
siﬁer is used in a range segmentation algorithm to ﬁnd geometric primitives in a
range map. Common objects are then identiﬁed as speciﬁc collections of extracted
primitives. Experimental results conﬁrm that the proposed technique is capable of
successfully classifying and modelling a variety of domestic objects using planes,
spheres, cylinders and cones.
The main computational cost of segmentation is the ﬁnal iterative merging stage.
This involves ﬁtting every neighbouring pair of regions with all possible geometric
primitives to determine the pair to be merged. Since most pairs will not be merged,
a great deal of processing time is simply wasted. It may be possible to accelerate
the merging stage by introducing heuristics (based on, for example, analysis of the
Gaussian image) to quickly eliminate some pairs from consideration before com-
mitting to an expensive ﬁtting algorithm. However, the current processing speed is
reasonable for a service robot, particularly if the extracted objects are subsequently
tracked using a faster algorithm, which is the topic of the next chapter.
As with many segmentation algorithms, the methods employed here require sev-
eral empirical thresholds. These include selection of the window size for normal
vector and surface type calculations, and thresholds to detect creases, non-zero prin-
cipal curvatures and acceptable surface models. Parameter selection is driven not
only by consideration of the sensor noise, but also by the needs of object classiﬁ-
cation and robotic grasp planning. For example, cups with planar sides, embossed
patterns and curved proﬁles are common in practice, but can be recognized as gen-
erally cylindrical at an appropriate scale. Furthermore, features on the scale of the
hand (such as the height and radius of a cup) have a greater impact on grasping
than smaller surface variations. The window size for normal vector and surface type
calculations determines the smallest detectable features. Similarly, the sensitivity of
surface classiﬁcation is controlled by the threshold for detection of non-zero princi-
pal curvatures. The particular choice of parameters in the experimental results was
found to segment objects at a scale appropriate for classiﬁcation and grasp planning.
However, for applications such as CAD modelling, it may be appropriate to select
parameters which produce ﬁner details.
The ability of the proposed algorithm to model complex objects is clearly limited
by the selection of geometric primitives. An obvious direction for further work is to
consider additional primitives such as ellipsoids and tori. However, any addition of
new primitives must also consider the tradeoff between generality, complexity and
computational expense. Alternatively, a greater variety of domestic objects could be
classiﬁed by developing more sophisticated object modelling algorithms to handle
complex relationships between primitives, which would ultimately allow a robot to
perform a greater range of tasks. Another interesting research direction would be to
exploit the ability of the robot to interact with the world to aid scene understanding.
If the initial models provide sufﬁcient information to grasp an object, the robot could
then view the object from all sides to reﬁne or verify the initial classiﬁcation. Our
own tendency to explore the world in this way could provide valuable insight into
developing similar “active understanding” strategies.

5
Multi-cue 3D Model-Based Object Tracking
Once an object has been located and classiﬁed using the techniques in the previous
chapters, the system must continue to update the estimated pose for several reasons.
Clearly, the initial pose will quickly become invalid if the object is under internal
or external dynamic inﬂuences. However, even if the object is static, the motion of
the active cameras may dynamically bias the estimated pose through modelling er-
rors. Even a small pose bias is sufﬁcient to destabilize a planned grasp and cause the
manipulation to fail. Tracking is therefore an important component in a robust grasp-
ing and manipulation framework. If the range sensing and segmentation methods
described in Chapters 3 and 4 could be performed with sufﬁcient speed, the track-
ing could be implemented by continuously repeating this process. Unfortunately, the
current measurement rate (up to one minute per range scan) renders this approach
unsuitable for real-time tracking. However, the textured polygonal models and initial
pose information from range data segmentation present an ideal basis for 3D model-
based tracking. To close the visual feedback loop, this chapter now addresses the
problem of continuously updating the pose of modelled objects.
In Chapter 6, robust tracking of the end-effector is achieved by fusing kinematic
and visual measurements and exploiting the fact that that gripper is a known target.
Conversely, tracking unknown objects must rely solely on the detection of natural
cues. Object tracking is further complicated by lighting variations, background clut-
ter and occlusions that are likely to occur in domestic scenes. Furthermore, arbitrary
objects may contain too few or too many visual features for robust matching. Con-
ventional model-based tracking algorithms are typically based on a single class of
visual cue, such as intensity edges or texture, and fail when visual conditions eventu-
ally become unsuitable (as will be demonstrated in Section 5.5). However, it is also
observed that different cues often exhibit independent and complementary failure
modes. The problem of unpredictable visual conditions can therefore be alleviated
by fusing multiple visual cues. Multi-cue fusion achieves tracking robustness by re-
ducing the likelihood that all visual features fail simultaneously, allowing the tracker
handle a wide variety of viewing conditions. For example, if edge detection fails due
to a low contrasting background, the tracking ﬁlter can simply rely on cues such as
colour and texture.
G. Taylor and L. Kleeman: Visual Perception and Robotic Manipulation, STAR 26, pp. 85–113, 2006.
© Springer-Verlag Berlin Heidelberg 2006

86
5 Multi-cue 3D Model-Based Object Tracking
The following section examines the motivation for multi-cue visual tracking in
greater detail and reviews the common approaches to the problem. An overview of
the proposed framework is presented in Section 5.2, in which objects are tracked
by fusing colour, texture and edge cues in a Kalman ﬁlter. The ﬁlter is described in
Section 5.3, and Section 5.4 follows with details of the image processing and mea-
surement model for each cue. Finally, Section 5.5 provides implementation details
and experimental results. The performance of the proposed multi-cue algorithm is
compared with single-cue tracking ﬁlters to validate the increased robustness gained
through fusion. Video clips of the experimental tracking results can be found in the
Multimedia Extensions, along with source code to implement the algorithm.
5.1 Introduction
Visual tracking is employed in a variety of robotic applications that require the local-
ization of dynamic targets, including mobile robot navigation [30], human-machine
interaction [72] and machine learning [11]. Clearly, visual tracking is also neces-
sary in domestic manipulation tasks involving moving objects. In practice, domestic
objects are actually often static1, and a well calibrated robot could employ a sense-
then-move strategy (initial pose estimation followed by blind grasp) to perform many
tasks, as demonstrated in [10]. In practice, sense-then-move without visual tracking
is defeated by at least two sources of uncertainty: kinematic and camera calibration
errors, and dynamics imposed by the robot on the environment, as described below.
It was suggested in Chapter 1 that reliance on accurate calibration in domestic
robotics may be unsustainable or even undesirable, and emphasis should therefore
be placed on robustness to calibration errors. In the case of visual localization, cali-
bration errors introduce a varying bias in the estimated pose. As the active cameras
move, any stationary object will appear to undergo an opposite motion on the image
plane. When transformed from the camera frame to the world frame (see Figure 2.5),
the estimated pose of the object should remain constant despite the camera motion.
In practice, ego-motion creeps in as the transformation (equation (2.30)) is subject
to kinematic calibration errors. Motion of the active head therefore reduces any con-
ﬁdence in the previously sensed location of an object, whether static or dynamic.
Unstable grasps or collisions during the execution of a task can impose additional
dynamics and further increase the uncertainty in the expected pose of target objects.
Visually tracking target objects reduces these uncertainties, and therefore plays
an important role in hand-eye systems even in static domestic environments. As al-
ready noted, 3D model-based tracking is particularly suited to the system developed
in this monograph since object modelling (see Chapter 4) provides textured polygo-
nal models and accurate initial pose information.
A variety of approaches to object tracking have been proposed in the litera-
ture, and algorithms are typically based on the detection of a particular cue, such
as colour [11], edges [89,98,152] or feature templates [113,114]. However, individ-
ual cues can only provide reliable tracking under limited conditions. For example,
1In fact, this is a requirement of light stripe scanning (see Chapter 3).

5.1 Introduction
87
motion and colour require the presence of contrasting foreground and background
colours, and only recover partial pose information. Intensity edges similarly require
a highly contrasting background, and are distracted by surface textures [89]. View-
based approaches such as template tracking are distracted by reﬂections and light-
ing variations, while some objects contain insufﬁcient texture for reliable matching.
Single-cue tracking is therefore unsuitable in an unpredictable domestic setting, since
the above failure modes will inevitably be encountered at some time.
To address the problem of reliable tracking in unpredictable conditions, it should
be observed that different cues are subject to independent failure modes. Thus, the
simultaneous failure of all cues is much less likely than the loss of any single modal-
ity. Robustness can be improved by exploiting multi-cue visual tracking, which is
the basis of the framework proposed in this chapter. A 3D model-based algorithm
is developed to track textured polygonal object models in stereo video streams by
fusing colour, edge and texture cues in a Kalman ﬁlter framework.
Multi-cue tracking schemes generally adopt one of two basic approaches: se-
quential cue selection or cue integration. In the sequential cue selection framework,
the tracker may switch between a number of cues, but only one cue contributes to
the estimated state at any time. In cue integration, all cues are tracked simultaneously
and fused to estimate the state. Kragi´c and Christensen [87] present a classical two-
stage sequential cue algorithm to estimate the pose of an object for position-based
visual servoing. A view-based template matching algorithm is ﬁrst applied to select
a coarse region of pose space, after which the object is tracked using edge cues.
Toyama and Hager [153] generalize the sequential cue approach to a hierarchy
of selector and tracker algorithms. The hierarchy orders algorithms in terms of pre-
cision, although some may operate on the same visual cue. Selectors are deﬁned
as algorithms that may produce a false positive (erroneously identify a feature as
part of the object), while trackers never produce false positives. An associated state
transition graph determines which algorithm is active depending on current tracking
performance. For robotic applications, Prokopowicz et al. [125] suggest that knowl-
edge of the expected behaviour of visual features for the given task and environ-
ment should also be incorporated into the cue selection algorithm. Darrell et al. [29]
present an alternative approach to cue selection for tracking people in 2D images
using parallel rather than sequential cue processing. Skin colour, range segmentation
and face detection are processed independently for each frame, and state estimation
is based on an order of preference between detected cues.
The main drawback of sequential cue selection in multi-cue tracking is that con-
siderable visual information is simply discarded in any given frame. Furthermore,
accurate evaluation of tracking performance (in particular, detection of false posi-
tives) is required to guide the selection of the most suitable cue for current tracking
conditions. Conversely, the approach of cue integration avoids both of these issues,
but at the expense of greater computational load. In a typical cue integration scheme,
the pose of the object is estimated independently from each cue modality, and the
results are combined using a suitable fusion operator. A variety of operators have
been proposed in practice, and a review of common approaches is given in [13].

88
5 Multi-cue 3D Model-Based Object Tracking
Spengler and Schiele [141] present two typical Bayesian decision approaches to
cue integration: democratic voting and particle ﬁltering. In both cases, each cue maps
the input image into a conditional probability density for the state of the target. De-
mocratic voting then calculates the combined state as the maxima of the weighted
sum of the probability densities, where the weights are based on the past perfor-
mance of cues. Particle ﬁltering (also known as the CONDENSATION algorithm in
visual tracking literature) fuses state probability densities in a similar manner, but
represents the combined density as a small number of sampled states (known as par-
ticles). Particles are easily clustered to give multiple state hypotheses, which enables
tracking of multiple targets and provides robustness to association errors.
Cue integration can also be performed without explicitly modelling conditional
probability densities, and two such tracking schemes are presented in [87]: consensus
voting and fuzzy logic. Consensus voting, similar to the Hough transform, operates
by discretizing state space and allowing each cue to vote for zero or more states.
The supporting cues for the most popular state are then combined to reﬁne the es-
timated state. Fuzzy logic operates by assigning to each state a membership value
corresponding to each cue. Fusion is performed by maximizing the combined mem-
bership values using a min-max fuzzy logic operator. Uhlin et al. [157] present an
alternative non-Bayesian fusion framework for the speciﬁc task of tracking an arbi-
trary moving target represented by a binary image mask. Motion detection identiﬁes
pixels which are moving independently of the background, and disparity segmen-
tation distinguishes pixels that are moving consistently with the current target. The
new pixels are fused with the existing mask using simple binary logic operations.
The ICONDENSATION algorithm proposed by Isard and Blake [72] is a notable
compromise between the cue selection and integration schemes. In this work, a hu-
man hand is tracked in a particle ﬁltering framework by fusing skin colour and shape
template features. Skin colour detection forms the basis of an importance function
from which the particles are re-sampled at each update, while shape detection con-
tributes to the actual estimated state. Thus, while skin colour is not used directly to
estimate the state, both cues inﬂuence the ﬁnal outcome.
The multi-cue integration techniques described above work well for feature-
based applications in which the estimated state is a 2D position on the image plane,
but are less suited to 3D model-based tracking. In particular, fusion operators such
as voting and ICONDENSATION quickly become computationally intractable as the
number of estimated parameters increase. Furthermore, both approaches typically re-
quire each cue to estimate the state independently of other cues. While this is usually
possible in 2D tracking, cues such as motion and colour do not sufﬁciently constrain
the 3D pose estimation problem.
The framework proposed in the chapter overcomes these issues by integrating
multiple visual cues using the Kalman ﬁlter framework, which is commonly ex-
ploited in sensory fusion applications such as mobile robot navigation [84]. In this
approach, each cue has an associated measurement model to predict the observa-
tions for a given state. In each measurement cycle, the estimated state is iteratively
updated by minimizing the weighted error between the observed and predicted mea-
surements. Thus, any cue with a suitable measurement model can contribute to the es-

5.2 System Overview
89
timated state since explicit pose reconstruction is not necessary. Like other Bayesian
approaches, the Kalman ﬁlter also maintains an estimate of the state covariance,
which is useful for evaluating tracking performance. Furthermore, experimental re-
sults will demonstrate that the Kalman ﬁlter imposes minimal computational expense
compared to other components of the tracking framework, such as image processing.
The proposed method is now described in detail.
5.2 System Overview
The aim of visual tracking is to estimate the trajectory of an object in a sequence
of stereo images, given an initial estimate of the pose. The cues used for multi-
cue tracking are colour centroid, intensity edges and surface texture, which are cho-
sen to provide complementary failure modes and redundant pose information. The
pose of the object in the kth frame is represented by a 6 dimensional pose vector
p(k) = (X,Y,Z,φ,θ,ψ) (see Section 2.1.4). The tracked object is modelled as a 3D
polygonal mesh with textured facets, as detailed in Chapter 4. It will be demonstrated
below that a polygonal model provides a reasonable approximation for objects with
either ﬂat or curved surfaces. The N vertices of the facets are represented by the set
of points V = {OVi : i = 1,...,N}.
Figure 5.1 shows a typical frame from the video sequence with a moving rectan-
gular box, which will be used to illustrate the discussion below and in later sections.
Tracking is divided into three sub-tasks: feature prediction, detection/association and
state update. Feature prediction consists of estimating the expected visual cues from
the textured 3D model and predicted pose. As usual, the pose is predicted by evolv-
ing the estimated state from the previous frame according to the dynamics model in
the tracking ﬁlter. Using the central projection camera model introduced in Section
2.2.1, the projected vertices L,Rˆvi of the object in the predicted pose are calculated
as:
L,Rˆvi(ˆp) = L,RPWHO(ˆp)OVi
(5.1)
where L,RP are the camera projection matrices (given by equation (2.29)) and
WHO(ˆp) is the transformation from the object frame to the world frame for the pre-
dicted pose ˆp. For the test sequence in Figure 5.1, the predicted pose of the box is
indicated by the wireframe overlay. To predict the visual cues, the 3D model of the
target is rendered into an image buffer using standard computer graphics techniques.
The synthetic images in Figures 5.2(a) and 5.2(b) show the expected appearance of
textures and edges for the predicted pose of the tracked box.
The predicted visual features are used to guide the detection and association of
real cues in the captured frame. A rectangular region of interest (ROI) for visual
measurements is calculated as the bounding box containing all the projected vertices
of the model (with a ﬁxed margin around the points). The ROI for the test frame in
Figure 5.1 is indicated by the white rectangle around the object. To reduce compu-
tational load and background distractions, the colour centroid, texture and edge cues
are only calculated from pixels within the rectangular ROI.

90
5 Multi-cue 3D Model-Based Object Tracking
Fig. 5.1. Captured image, predicted pose and region of interest (ROI)
(a) Predicted appearance of texture
(b) Predicted appearance of edges
Fig. 5.2. Predicted appearance of object.
The prediction, detection and association algorithms for each cue are detailed in
Section 5.4. In brief, the colour centroid is predicted as the mean position of projected
vertices, and measured by processing the ROI with a colour ﬁlter constructed from
the appearance of the object in the initial frame. To measure edges, the ROI is ﬁrst
processed with an edge detection ﬁlter, and the resulting edge pixels are matched to
the predicted edges shown in Figure 5.2(b). The edges are parameterized by ﬁtting
a line to the pixels associated with each boundary segment. For texture cues, the
synthetic image in Figure 5.2(a) is processed using a texture quality ﬁlter to locate
salient features. The predicted features are then matched to the captured frame (using
sum of squared difference minimization) to ﬁnd the corresponding real texture cues.
Finally, all the measurements are fused to update the estimated pose of the object.
Many tracking algorithms achieve continuity by associating new measurements
with a ﬁxed set of features over several frames. The association problem becomes

5.3 Kalman Filter Framework
91
more difﬁcult as the appearance of features change with time, until eventually the
features must be replaced with a new set. Conversely, model-based tracking allows
for the selection of new features with every frame, since continuity is provided by
the underlying object model. The temporal association problem is replaced with the
simpler problem of associating predicted and measured features. Using each cue
only once eliminates the need to explicitly update the appearance and assess the
quality of features over time (as used by [138]). Furthermore, when feature detection
and association errors occasionally arise, the offending features do not perturb the
estimated pose for more than a single frame.
5.3 Kalman Filter Framework
The Kalman ﬁlter is a well known algorithm for optimally estimating the state of
a linear dynamic system from noisy measurements, and is used for many tracking
applications in both vision and robotics [84, 163]. The Kalman ﬁlter also provides
a statistically robust framework for fusing different measurement modalities, which
is exploited in the framework developed below. Furthermore, the ﬁlter maintains an
estimate of the uncertainty in the tracked parameters, which can be useful for eval-
uating tracking performance. The Iterated Extended Kalman Filter (IEKF) extends
the framework to non-linear systems, such as the present application, by solving the
ﬁlter equations numerically. Appendix C describes the basic equations of the IEKF,
and a detailed treatment of Kalman ﬁlter theory can be found in [8,74].
The variables we wish to estimate are encapsulated as the state of the tracking ﬁl-
ter. For 3D object tracking, the state is a 12 dimensional vector x(k) = (p(k), ˙r(k)),
where p(k) and ˙r(k) represent the pose and velocity screw of the target in the kth
video frame. The IEKF algorithm to estimate the state can be summarized in two
broad steps: state prediction based on a known dynamic model, followed by state
update based on the latest measurements. Assuming the object has smooth motion,
the evolution of the state is modelled by constant velocity dynamics:
p(k +1) = p(k)+ ˙r(k)Δt
(5.2)
˙r(k +1) = ˙r(k)
(5.3)
where Δt is the sample time between frames. The ﬁrst step in the IEKF algorithm
predicts the current state by applying this model to the previous state estimate.
In the update step, cues are fused with the predicted state through measurement
models that predict the observed features as a function of the predicted state. The new
state estimate is recovered by minimizing the weighted observation error between the
predicted and measured features. Colour, edge and texture patches extracted from
the captured stereo images are stored in a measurement vector y(k + 1), which has
variable dimension depending on the number of detected features. A measurement
model predicts these measurements ˆy(k + 1) given the predicted state ˆx(k + 1|k) in
the (k +1)th frame as:
ˆy(k +1) = h(ˆx(k +1|k))+w(k +1)
(5.4)

92
5 Multi-cue 3D Model-Based Object Tracking
where w(k + 1) is an estimate of the measurement noise. The non-linear measure-
ment equations, h(x), are detailed in Section 5.4 for each cue. For a linear Kalman
ﬁlter, the current state ˆx(k + 1) is then estimated as the sum of the predicted state
and observation error (difference between the predicted and observed measurement
vectors), weighted by the Kalman gain. The IEKF differs from the linear Kalman
Filter in that the updated state is computed numerically via iterative re-linearization
of the non-linear measurement model. Details of the IEKF equations can be found in
Appendix C.
The orientation component of the state vector must be handled carefully in the
IEKF as Euler angles are non-unique and even degenerate for some orientations (see
Section 2.1.3). To address this issue, the Euler angles in the state vector represent
only the inter-frame differential orientation, while the total orientation is stored as
a quaternion outside the IEKF [15, 161]. At each state update, the IEKF estimates
the differential orientation between the new pose and the external quaternion. The
differential Euler angles are then integrated into the external quaternion and reset to
zero for the next measurement cycle.
As noted earlier, the IEKF framework is extensible; any modality with a suitable
measurement model can be integrated, including cues that provide incomplete pose
information. The same mechanism allows features to be fused from multiple cameras
by supplying a measurement model for each image plane. Unlike conventional stereo
reconstruction, this approach eliminates the need to search for corresponding features
as all measurements are coupled implicitly through the ﬁlter state. Camera failures
are also tolerated as tracking continues even if the object is obscured from all but one
camera.
5.4 Feature Measurement
As described earlier, the robustness of multi-cue tracking arises from integrating cues
with independent and complementary failure modes. For this purpose, the proposed
ﬁlter is based on colour centroid, intensity edges and texture templates. As described
in Section 5.2, the measurement process begins by predicting the pose of the object in
the current frame and identifying the ROI. Each modality then requires a detection
process to extract features from the ROI (in both stereo ﬁelds) before assembling
the results into a measurement vector. Finally, the IEKF requires a measurement
model associated with each modality to predict the observed features for a given
pose. The following sections describe the detection process and measurement model
for each cue. For pedagogical purposes, the frame shown in Figure 5.1 will be used
to illustrate the detection algorithms.
5.4.1 Colour
The colour cue measures the centroid of pixels in the captured frame that are congru-
ent with the expected colour of the object. The measurement process is implemented

5.4 Feature Measurement
93
(a) ROI in initial image with outline showing pixels used
for creation of object colour ﬁlter
(b) Output of colour ﬁlter applied to current
frame ROI
(c) Measured colour centroid (black square)
Fig. 5.3. Colour cue initialization and measurement of colour centroid.
using a simple colour ﬁlter constructed from an image of the object, which is sufﬁ-
cient for tracking in scenes with minimal lighting variations. Adaptive colour track-
ing techniques [2, 101] provide greater robustness to lighting variations and clutter
and could easily replace the colour ﬁlter described below.
One of the difﬁculties associated with colour ﬁltering is the possibility that the
background contains distracting colours similar to those in the target. Thus, the ﬁl-
ter initialization algorithm is designed to promote unique colours in the target while
suppressing those colours common to the target and background. This is achieved
by exploiting the accuracy of the initial pose estimate furnished by range data seg-
mentation. The ﬁlter is constructed from the ROI in the initial frame of the tracking
sequence, and then applied to every subsequent frame.
For our box tracking example, the initial frame in the sequence and associated
ROI are shown in Figure 5.3(a). The boundary of the object is identiﬁed by projecting
the initial pose of the model onto the image plane (equation (5.1)), as shown by the
white outline in Figure 5.3(a). Colour information is compiled in an RGB histogram
by partitioning RGB space into uniform cells (16 cells per channel was found to be

94
5 Multi-cue 3D Model-Based Object Tracking
sufﬁcient). The histogram is compiled from all pixels in the ROI both inside and
outside the boundary of the object; pixels inside the object contribute positively to
the histogram while background pixels contribute negatively. Let Hi represent the set
of colours associated with the ith cell in the histogram, and hi represent the associated
tally. Furthermore, let C(x,y) represent the colour of the pixel located at (x,y) in the
captured frame, and O represent the set of pixel locations within the object boundary.
Then, the accumulated count for the ith histogram bin is
hi =
∑
(x,y)∈ROI



+1 if (x,y) ∈O, C(x,y) ∈Hi
−1 if (x,y) /∈O, C(x,y) ∈Hi
0 otherwise
(5.5)
After accumulating all pixels, cells with a negative count are set to zero. Outlier
rejection is applied by iteratively eliminating the cell with the lowest count until no
less than 90% of the original total tally remains. Finally, the colours associated with
the remaining non-zero cells form the pass-band of the colour ﬁlter. The ﬁlter is
stored as a binary look-up table indicating whether the RGB index value occupies
the pass-band.
During tracking, the colour ﬁlter is applied to the captured frame by replacing
each pixel in the ROI with value in the look-up table. Figure 5.3(b) shows the result
of colour ﬁltering for the ROI in Figure 5.1. The centroid of the object is recovered
by applying binary connectivity to the output of the ﬁlter and calculating the centroid
of the largest blob. Finally, the IEKF requires a measurement function to predict the
colour centroid for a given state. The colour centroid is approximated as the projected
centroid of the 3D model, after transforming to the given state:
L,Rˆc(x) = L,RPWHO(x) ∑
Vi∈V
Vi
(5.6)
where L,RP are the camera projection matrices, WHO(x) is the transformation from
the object frame to the world frame for state x, and L,Rˆc(x) are the predicted centroids
on the left and right image planes. This prediction equation actually introduces a sys-
tematic bias due to the non-linearity of the projective transformation; the projection
of the mean point (ie. the prediction) is not equal to the mean of the projected points
(ie. the image plane measurement). However, this effect is neglected since it is much
smaller than the random error arising from noise in the output of the colour ﬁlter,
and in any case the bias is readily compensated by the other visual cues.
5.4.2 Edges
The predicted edges for the tracking scenario in Figure 5.1 are shown in Figure
5.2(b). Only the jump boundaries outlining the object are considered in the detec-
tion process, since internal edges are easily distracted by surface textures (see Figure
5.4(a)). The jth predicted edge segment is represented by the 3D vertices Aj ∈V
and Bj ∈V of the end-points and their associated projections aj(x) and bj(x) on the
image plane for the current pose (via equation (5.1)). For each pair of end-points, the

5.4 Feature Measurement
95
normal direction nj = (nx j,ny j), orientation θj and perpendicular distance dj to the
image plane origin are calculated as:
nj =

0 −1
1 0

(bj −aj)
(5.7)
θj = tan−1(ny j/nx j)
(5.8)
dj = −n
j aj
(5.9)
These parameters guide the detection of intensity edges in the captured frame. As
usual, the ﬁrst step in edge detection is to calculate directional intensity gradients,
gx(x,y) and gy(x,y). A central difference approximation to these gradients is
gx(x,y) = I(x+1,y)−I(x−1,y)
(5.10)
gy(x,y) = I(x,y+1)−I(x,y−1)
(5.11)
where I(x,y) is the intensity channel. An edge is detected at pixel position ek when
gx(ek) > eth and gy(ek) > eth for the gradient threshold eth, and the orientation of the
detected edge is calculated as θk = tan−1(gx/gy).
Figure 5.4(a) shows the output of the edge detector applied to the ROI in Figure
5.1. Clearly, raw edge detection produces numerous spurious measurements due to
variations in lighting and surface texture. These distractions are eliminated by com-
bining edge detection with the output of the colour ﬁlter (see Figure 5.3). First, the
colour ﬁlter output is scanned to identify the pixels on the convex boundary of the
largest connected region. Edge pixels beyond a threshold distance of these bound-
ary pixels are eliminated from the output of the edge detector. The remaining edge
pixels are treated as possible candidates for the desired jump boundary outlining the
object. Finally, skeletonization is applied to reduce the edges to the minimal set of
candidates, shown in Figure 5.4(b).
Following image processing, an association algorithm attempts to match each
edge pixel to one of the predicted line segments. The kth edge pixel, at position ek
and with orientation θk, is associated the the jth line segment when the following
conditions are satisﬁed:
n
j ek +dj < rth
(5.12)
|θ j −θk| < θth
(5.13)
0 ≤(ek −aj)(bj −aj) ≤|bj −aj|2
(5.14)
Condition (5.12) requires the candidate pixel to satisfy the line equation to within an
error threshold rth. Condition (5.13) enforces a maximum angular difference θth be-
tween the orientation of the edge pixel and predicted line segment. Finally, condition
(5.14) ensures that the pixel lies within the end points of the segment. When the pixel
matches more than one predicted edges, the ambiguity is resolved by assigning the
candidate to the segment that gives the minimum residual error in condition (5.12).
After the association step, the jth predicted segment has mj matched pixels. For
segments with sufﬁcient matched points (mj > mth), two measurements are generated

96
5 Multi-cue 3D Model-Based Object Tracking
(a) Edge ﬁlter output (grey-level indicates
edge orientation)
(b) Skeletonized jump boundary pixels
(c) Detected edges (line indicates orientation and square indicates
mean position)
Fig. 5.4. Edge detection and matching.
for the IEKF: the average position of edge pixels mj, and the normal orientation θj
of a ﬁtted line using principal components analysis (PCA) [51]. The robustness to
mis-matched points is improved by applying an initial PCA, rejecting points with a
residue greater than two standard deviations, and re-applying PCA to the remaining
points. Figure 5.4(c) shows the mean position and orientation of detected edges for
the tracking example in Figure 5.1.
The mean of pixels associated with the jth segment is unlikely to coincide with
the midpoint between projected end-points ˆaj and ˆbj, as a result of occlusions and
other distractions. Furthermore, the observed mean is difﬁcult to predict for a given
state, so mj is not a useful measurement for the IEKF. Instead, the distance r j be-
tween m j and the measured edge, which is identically zero and invariant to occlu-
sions, is added to the measurement vector. The mean position m j is supplied as an
input to the IEKF, and the measurement model predicting ˆrj(x) for a given state x is:
ˆrj(x) = ˆn
j (x)[mj −ˆaj(x)]
(5.15)

5.4 Feature Measurement
97
where the normal vector ˆnj(x) of the segment is calculated from equation (5.7). Since
the measured rj is identically zero, the pose estimated by the IEKF minimizes the
perpendicular distance between the measured mean m j and the predicted edge.
To predict the orientation θj, the IEKF is supplied with the end-points Aj
and Bj of the associated segment in the 3D model. The image plane projections
ˆaj(x) = (ˆxaj, ˆyaj) and ˆbj(x) = (ˆxb j, ˆyb j) for a given state x are then calculated
using equation (5.1), the orientation is predicted as:
ˆθ(x) = tan−1
 ˆxb j −ˆxa j
ˆya j −ˆyb j

(5.16)
In summary, edge cues are parameterized by θj and rj ≡0, and equations (5.15)
and (5.16) serve as the measurement model for the IEKF, with inputs Aj, Bj and
mj. It should be noted that the measurement vector could have been parameterized
using the distance to the origin instead of the distance to the measured mean point.
However, the error variance of the distance to the origin varies signiﬁcantly with the
position and orientation of the line segment. Conversely, the measurement error vari-
ance for the distance to the mean point is small and independent of the position and
orientation of the segment. The latter formulation therefore leads to a simpler calcu-
lation of the measurement error covariance matrix required by the tracking ﬁlter.
5.4.3 Texture
Texture tracking algorithms typically represent cues as small greyscale image tem-
plates, and match these to the captured image using sum of squared difference (SSD)
or correlation-based measures. The templates are generally view-based and there-
fore dependent on the pose of the object. Most tracking algorithms address the issue
of view-dependence by maintaining a quality measure to determine when a tem-
plate no longer represents the current appearance of a feature. Conversely, the use
of a textured 3D object model allows a new set of view-dependent features to be
predicted in every frame. While this approach is computationally expensive, the fea-
ture selection calculations can be optimized to operate in real-time. As noted earlier,
model-based 3D tracking also removes the need for stereo reconstruction from corre-
sponding measurements. As with the other cues, the measurement process described
below is applied independently to each camera.
Figure 5.2(a) shows the predicted appearance of textures for the object in Figure
5.1. Clearly, some regions constrain the tracking problem better than others; areas
exhibiting omni-directional spatial frequencies such as corner and salt-and-pepper
textures are generally considered the most suitable. A widely accepted technique for
locating such features is the quality measure proposed by Shi and Tomasi [138]. For
each pixel in the rendered image, a matrix Z is computed as:
Z = ∑
x∈W
 g2
x
gxgy
gxgy g2
y

(5.17)

98
5 Multi-cue 3D Model-Based Object Tracking
(a) Texture quality from minimum
eigenvalue of Z matrix.
(b) Candidate templates (squares) and
displacement vectors (lines, x10 scale).
(c) Validated candidate templates
(d) Matched features in captured frame
Fig. 5.5. Texture feature selection and matching.
where gx and gy are spatial intensity gradients given by equations (5.10)-(5.11), and
W is the n×n template window centred on the pixel. Good texture features are iden-
tiﬁed as satisfying
min(λ1,λ2) > λth
(5.18)
where λ1 and λ2 are the eigenvalues of Z. Figure 5.5(a) shows the minimum eigen-
value at each pixel of the rendered object (Figure 5.2(a)). While the object is out-
lined by a high response to the quality measure, these areas usually straddle a jump
boundary and are not suitable for tracking. Thus, the feature selector only examines
pixels further than a window width from the jump boundary. A window-based search
locates local maxima in the quality measure, which are assembled into a set of can-
didate cue locations. The ith candidate is associated with a position mi and a window
of pixels in the rendered image Wi (centred on mi) that deﬁne an intensity template.
Candidate templates are matched to the captured image using SSD minimization,
which yields an offset di between the predicted and measured location of the ith cue.
The SSD εi(d) between the window centred at mi in the rendered image and the
window at mi +d in the captured image is evaluated as:

5.4 Feature Measurement
99
εi(d) = ∑
x∈Wi
[J(x)−I(x+d)−M(d)]2
(5.19)
where J is the rendered image, I is the captured image and M(d) compensates for
the mean difference between predicted and measured templates due to variations in
lighting. This last term is given by:
M(d) = 1
n2 ∑
x∈Wi
[J(x)−I(x+d)]
(5.20)
The minimum SSD within a ﬁxed search window Di around the ith template deter-
mines the displacement di:
di = argmind∈Diε(d)
(5.21)
Figure 5.5(b) shows the candidate features and measured displacement vectors (at
×10 scale) for the object tracked in Figure 5.1. It can be expected that some candi-
date templates will be incorrectly matched to the captured image, since the rendered
image is only an approximation. A two-stage validation process is therefore applied
before adding texture cues to the measurement vector of the IEKF.
Let the initial candidate displacement vectors be represented by the set C = {di}.
Assuming the error between the predicted and actual pose of the object is approx-
imately translational (a common requirement for template-based tracking), all cor-
rectly matched features will exhibit the same displacement. Thus, the ﬁrst validation
step ﬁnds the largest subset C ⊆C containing approximately equal displacements.
For each candidate displacement di, a set of similar candidates Si is constructed:
Si = {dj : ||dj −di|| < dth;di,dj ∈C}
(5.22)
where dth determines the maximum allowed distance between elements of Si. After
determining Si for all candidates, a new set Mi is calculated for each candidate as the
intersection of all sets of similar candidates that include di:
Mi =

{Sj : di ∈Sj}
(5.23)
The new set Mi will contain only mutually supporting vectors such that di ∈Sj and
dj ∈Si for all di,dj ∈Mi. Finally, the largest mutually supporting set of displace-
ments C = argmax|Mi| are taken as valid candidates.
A second validation test requires template matching to be invertible; for each
vector di ∈C, a template is extracted from the captured frame and matched to the
rendered image by SSD minimization, giving a reverse displacement ri. Valid fea-
tures should produce approximately equal forward and reverse displacements. Thus,
the valid displacements are the subset M ⊆C given by
M = {di : ||di +ri|| < rth,di ∈C}
(5.24)
where rth is an empirically determined error threshold. Finally, the corresponding
matched positions mi +di in the captured frame are added to the measurement vector

100
5 Multi-cue 3D Model-Based Object Tracking
b
a
m2
θ3
θ1
θ2
θ2
θ1
θ3
c
m1
Fig. 5.6. Construction of the occupancy condition (5.25) for a facet given by vertices a, b and
c. For point m1 within the facet, angles θ1, θ2 and θ3 given by equations (5.26)-(5.28) sum to
2π. For point m2 outside the facet, the sum is less than 2π.
in the IEKF. Figure 5.5(c) shows the ﬁnal set of validated features pruned from the
initial candidates in Figure 5.5(b), and Figure 5.5(d) shows the location of matched
features in the captured frame.
A measurement model to predict the location of texture features for a given state
requires the construction of a model of 3D points Mi on the surface of the object
corresponding to the projections mi in the rendered image. Let Aj ∈V, Bj ∈V and
Cj ∈V represent the 3D vertices of the jth triangular facet in the object model after
transforming to a given pose, and aj, bj and cj represent their projections onto the
image plane. Before calculating the 3D location of a texture feature, the facet con-
taining the projection mi must be identiﬁed. The following condition determines that
mi is bounded by the jth facet:
θ1 +θ2 +θ3 = 2π
(5.25)
where
θ1 = cos−1
(aj −mi)·(bj −mi)
|aj −mi||b j −mi|

(5.26)
θ2 = cos−1
(aj −mi)·(cj −mi)
|aj −mi||c j −mi|

(5.27)
θ3 = cos−1
(bj −mi)·(cj −mi)
|bj −mi||c j −mi|

(5.28)
Figure 5.6 illustrates the geometrical interpretation of the angles θ1, θ2 and θ3 in
condition (5.25). A search is performed over all visible facets to identify the bound-
ing facet for each texture feature. The associated 3D vertices Aj, Bj and Cj are then
used to calculate the plane parameters of each bounding facet:
n = (Aj −Bj)×(Cj −Bj)
(5.29)
d = −nAj
(5.30)

5.5 Implementation and Experimental Results
101
where n is the normal direction and d is the distance to the origin of the camera
frame. Finally, the 3D point Mi associated with the ith texture feature is given by the
simultaneous solution of:
PMi = mi
(5.31)
nMi +d = 0
(5.32)
where equation (5.31) constrains Mi to the back-projected ray through mi given the
camera projection matrix P (equation (2.26)), and equation (5.32) constrains Mi to
the surface of the facet. The solution Mi = (Xi,Yi,Zi) in non-homogeneous coordi-
nates is (with mi = (xi,yi) and n = (nx,ny,nz)):
Zi = −d f/(nxxi +nyyi +nz f)
(5.33)
Xi = xiZi/f
(5.34)
Yi = yiZi/f
(5.35)
Finally, Mi is transformed from the left or right camera frame (depending on the
camera in which mi was measured) to the object frame:
OMi = OHW(x)WHL,RL,RMi
(5.36)
where WHL,R is the transformation from the camera frame to the world frame, and
OHW(x) is the transformation to the object frame for the given state x. The surface
point OMi is supplied as an input to the IEKF for each texture feature in the measure-
ment vector. Finally, the predicted measurement mi(x) for a given state x is simply
calculated as the forward projection:
mi(x) = L,RPWHO(x)OMi
(5.37)
where either LP or RP is applied depending on the camera in which the feature was
measured (recalling that the cameras are processed independently). Since stereo cor-
respondences are not required, the above framework is extensible from one to any
number of cameras.
5.5 Implementation and Experimental Results
In this section, the performance of the proposed multi-cue tracking framework is
examined in a number of real tracking scenarios. As described in Chapter 7, stereo
images are captured at half PAL resolution (384 × 288 pixels) and PAL frame-rate,
and projective rectiﬁcation and radial distortion correction are applied to each frame
prior to feature extraction. Image processing and Kalman ﬁlter calculations are per-
formed on a dual 2.2 GHz Intel Xeon, and a number of optimizations are employed
including parallel processing of stereo images in separate threads and MMX/SSE
code optimizations. The current implementation achieves a processing rate of about
14 frames per second, and approximate processing times for each component of the

102
5 Multi-cue 3D Model-Based Object Tracking
Table 5.1. Typical processing times for components of the multi-cue tracking algorithm.
Component
Time (ms)
Image capture and preprocessing 20
Colour measurement
5
Texture measurement
35
Edge measurement
7
IEKF update
5
algorithm are shown in Table 5.1 (values indicate total time for stereo frames). Tex-
ture features consume the greatest proportion of processing time due to the com-
putationally expensive algorithms for perspectively correct rendering, texture qual-
ity measurement and normalized SSD minimization. Conversely, the IEKF fusion
process imposes minimal computational load.
For the ﬁrst three tracking scenarios, the performance of the multi-cue tracker is
compared to the result achieved using single-cue trackers with edge only and texture
only measurements2. The edge only and texture only trackers were implemented by
simply discarding the unused features from the measurement vector in the IEKF. To
enable a meaningful comparison, the video streams were captured from the experi-
mental hardware and processed off-line for each tracking ﬁlter. Off-line processing
also provides an indication of the tracking performance that would be achieved with
a full frame-rate (25 Hz) implementation. Four tracking scenarios were examined:
tracking in poor visual conditions (contrast and lighting), tracking in the presence
of occlusions, tracking motion about an axis of symmetry, and tracking to overcome
calibration errors in ego-motion. The results for each sequence are described in turn.
5.5.1 Sequence 1: Poor Visual Conditions
In the ﬁrst tracking sequence, visual conditions were deliberately contrived to pro-
mote the loss of visual cues by introducing motion, low contrast, and lighting vari-
ations. The target object is a textured yellow box, and selected frames (right camera
only) from the tracking sequence are shown in Figure 5.7 (see Multimedia Extensions
for the full sequence). The box is initially placed against a highly contrasting back-
ground to enable reliable detection of all cues. The intensity edges are then weakened
by moving the box to a low contrasting background. Finally, the box is moved back
to its initial position and rotated to place the front face in shadow and impede the
detection of texture cues. It should also be noted that the texture on the top face was
not detected in this ﬁnal orientation since it was hidden when the model was created.
As described earlier, the sequence is processed using both single-cue and multi-cue
tracking. Figure 5.7 shows the features observed by the multi-cue tracker, with the
estimated pose of the box is overlaid as a wireframe model.
2Colour only tracking was not implemented since colour does not provide complete pose
information.

5.5 Implementation and Experimental Results
103
(a) Frame 0
(b) Frame 80
(c) Frame 150
(d) Frame 250
(e) Frame 450
(f) Frame 700
Fig. 5.7. Selected frames (right camera only) from box tracking sequence. Features used by
the multi-cue tracker are overlaid along with the estimated pose. Frame numbers are only
approximate (see Multimedia Extensions for the full sequence).

104
5 Multi-cue 3D Model-Based Object Tracking
0
5
10
15
20
25
30
35
0
100
200
300
400
500
600
700
Number of Features
Frame
edge segments
texture patches
(a) Number of detected edge and texture cues (every ﬁfth frame shown)
0
0.5
1
1.5
2
2.5
3
3.5
4
0
100
200
300
400
500
600
700
Position Std Dev (mm)
Frame
multimodal
edges only
texture only
(b) Standard deviation of x-coordinate (every ﬁfth frame shown)
Fig. 5.8. Performance of multi-cue tracker in box tracking sequence.
The plot in Figure 5.8 summarizes the number of edge and texture features ob-
served for each frame in the sequence. As expected, the number of detected edges
drops signiﬁcantly between frames 100 and 200 when the box is placed in front of
the low contrasting background. A sharp drop in the number of texture features is
also evident between frames 70 and 100, which can be attributed to a blurring of
the box as it is moved quickly to its new position (see Figure 5.7(b)). After frame
550, the number of texture features drops to zero as the altered lighting conditions
hinder template matching. Despite the loss of individual cues at various times in the

5.5 Implementation and Experimental Results
105
(a) Frame 150: failure for edge only tracker
due to low contrast background
(b) Frame 560: failure for texture only
tracker due to lighting variations
Fig. 5.9. Failure of single-cue tracking ﬁlters.
sequence, the multi-cue algorithm reliably tracks the box through the entire sequence
by relying on available modalities.
To compare the performance multi-cue, edge only and texture only trackers, Fig-
ure 5.8(b) plots the standard deviation of the x-coordinate of the estimated pose for
each tracker. As expected, multi-cue tracking yields a smaller variance in the esti-
mated state than the single-cue trackers since it exploits all available constraints. The
plot also highlights the failure of edge only and texture only tracking exhibited by
divergence of the state covariance. As expected, failure of the edge tracker coincides
with the loss of edge features due to the low contrasting background, and failure of
the texture tracker occurs after the front face of the box is placed in shadow. The
output of the edge and texture trackers at the point of failure is shown in Figure
5.9. Only the multi-cue tracker provides sufﬁcient robustness in the presence of poor
visual conditions to maintain track of the box for the entire sequence.
5.5.2 Sequence 2: Occlusions
The second tracking sequence also involves the loss of visual features, in this case
caused by obstacles occluding the target. Selected frames from the tracking sequence
(left camera only) are shown in Figure 5.10 (see Multimedia Extensions for the full
sequence). Since the occlusions are introduced while the target is stationary, the ini-
tial 150 frames show the object in motion to demonstrate that the tracking ﬁlter is
indeed estimating the pose and velocity without constraining the motion model. The
ﬁrst obstacle is introduced between frames 200 and 300 and occludes the texture cues
on the upper surface of the box. Other obstacles are successively introduced between
frames 400 and 700 to occlude most of the edges. Finally, the obstacles are removed
and the object is again manipulated to verify the operation of the tracking ﬁlter.
Figure 5.11(a) plots the number of detected features in each frame for the multi-
cue tracker. As expected, the ﬁrst occlusion between frames 200 and 300 reduces
the number of detected texture cues to zero (see Figure 5.10(c)). Similarly, occlusion

106
5 Multi-cue 3D Model-Based Object Tracking
(a) Frame 0
(b) Frame 130
(c) Frame 280
(d) Frame 470
(e) Frame 670
(f) Frame 920
Fig. 5.10. Selected frames (left camera only) from occluded box tracking sequence. Features
used by the multi-cue tracker are overlaid along with the estimated pose. Frame numbers are
only approximate (see Multimedia Extensions for the full sequence).

5.5 Implementation and Experimental Results
107
0
5
10
15
20
25
30
35
0
100
200
300
400
500
600
700
800
900
Number of Features
Frame
edge segments
texture patches
(a) Number of detected edge and texture features (every ﬁfth frame shown)
0
0.5
1
1.5
2
2.5
3
3.5
4
0
100
200
300
400
500
600
700
800
900
Position Std Dev (mm)
Frame
multimodal
edges only
texture only
(b) Standard deviation of x-coordinate (every ﬁfth frame shown)
Fig. 5.11. Performance of multi-cue tracker in occluded box tracking sequence.
of the edges causes a steady decrease in the number of associated features between
frames 400 and 700, while the texture cues also decrease with the introduction of the
blue box between frames 600 and 700 (see Figure 5.10(e)). Despite these losses, the
multi-cue tracker robustly maintains the pose of the box through the entire sequence.
Figure 5.11(b) shows the standard deviation of the x-coordinate estimated us-
ing edge only, texture only and multi-cue trackers. As before, the multi-cue tracker
achieves a smaller variance than the single-cue trackers, and the occlusion of edges
and textures introduce large excursions in the error for the single-cue trackers. How-

108
5 Multi-cue 3D Model-Based Object Tracking
ever, unlike the previous sequence, the single-cue trackers actually recover the pose
of the object when the disturbances are removed. Since the target remains stationary
during occlusion, the texture only tracker maintains a predicted pose that drifts only
slowly from the actual pose, as shown in Figure 5.12(a). The accuracy of the pre-
dicted pose allows texture cues to be recovered once the object is removed (Figure
5.12(b)). When edges are later occluded, Figure 5.13(a) shows that the edge tracker
develops association errors: the edges due to the occluding cups are mistaken for
edges of the box, resulting in a pose bias. Figure 5.13(b) shows a similar error, in
this case caused by the occluding hand. In both cases, the correct associations are
eventually recovered after the occlusions are removed. Importantly, these associa-
tion errors are not present in the output of the multi-cue tracker.
(a) Frame 300: drift in estimated pose due to
occluded texture
(b) Frame 315: recovery of pose after
reappearance of texture
Fig. 5.12. Performance of texture only tracker in the presence of occlusions.
(a) Frame 670: pose biased by distracting
edges in occluding objects
(b) Frame 870: pose biased by distracting
edges in occluding hand
Fig. 5.13. Performance of edge only tracker in the presence of occlusions.

5.5 Implementation and Experimental Results
109
5.5.3 Sequence 3: Rotation About Axis of Symmetry
This tracking sequence was designed to highlight a particular failure of edge-based
tracking. In this case, a conical coffee mug is rotated anti-clockwise and then clock-
wise about its axis of symmetry (frames 0 to 250), and ﬁnally ﬂipped upside-down
(frames 250 to 500). As before, the sequence was processed using edge only, texture
only and multi-cue trackers. Selected frames from the multi-cue tracking result are
shown in Figure 5.14 (see Multimedia Extensions for the complete sequence). For
each tracker, the angle between the initial and current pose of the mug is calculated
from the angle/axis representation of the difference in orientation. Figure 5.15 shows
the magnitude of the estimated change in angle for each tracker.
In this experiment, edge tracking fails to estimate the orientation of the mug due
to insufﬁcient constraints on the pose about the axis of symmetry. This is clearly
demonstrated in the ﬁrst 250 frames of Figure 5.15. While the other trackers closely
follow the rotation of the mug during this period, the edge tracker simply obeys
constant velocity dynamics and maintains the initial orientation. The rotation in the
latter half of the sequence is successfully tracked by all cues since it does not involve
an under-constrained degree of freedom. The issue of symmetry is not peculiar to
edge-based tracking; an equivalent ambiguity arises for textures such as pin-stripes
or concentric circles that also exhibit an unconstrained translational or rotational de-
gree of freedom. Incidentally, texture only tracking fails in the ﬁnal 100 frames of
the sequence as the rotation of the mug causes a sufﬁciently large change in the ap-
pearance of textures. As in the previous results, only multi-cue tracking successfully
estimates the pose of the object over the entire sequence.
5.5.4 Sequence 4: Ego-Motion Compensation
The ﬁnal tracking sequence involves a stationary mug and a simple camera motion to
demonstrate the pose bias introduced by ego-motion compensation. Figure 5.16(a)
shows the scene when the mug was ﬁrst scanned and modelled. The initial pose of
the mug is overlaid as a black wireframe and the tracked pose (using multi-cue track-
ing) is shown in white. Obviously, the initial pose closely matches the target while
both mug and cameras remain stationary. In Figures 5.16(b) and 5.16(c), the cameras
are panned up and across the scene while the mug remains stationary. As described
earlier, calibration errors in the neck joints will bias the estimated pose when trans-
forming the object between the world frame and camera frame. This bias is clearly
evident as a displacement between the initial pose (stored in the world frame) and ac-
tual target in both Figures 5.16(b) and 5.16(c). The bias observed during camera mo-
tion (Figures 5.16(b)) is considerably larger than the ﬁnal bias due to the additional
effect of latency in the joint angle readings. The close match between the tracked
pose (white wireframe) and actual mug throughout the sequence demonstrates how
multi-cue tracking overcomes the systematic error. This result highlights the neces-
sity of object tracking even in static scenes for precision tasks such as grasping.

110
5 Multi-cue 3D Model-Based Object Tracking
(a) Frame 0
(b) Frame 100
(c) Frame 190
(d) Frame 250
(e) Frame 370
(f) Frame 500
Fig. 5.14. Selected frames from coffee mug tracking sequence. Features used by multi-cue
tracker are overlaid along with the estimated pose. Frame numbers are only approximate (see
Multimedia Extensions for the full sequence).

5.6 Discussion and Conclusions
111
0
0.5
1
1.5
2
2.5
3
3.5
0
100
200
300
400
500
Orientation (rad)
Frame
multimodal
edges only
texture only
Fig. 5.15. Observed orientation of mug for single and multi-cue trackers.
5.6 Discussion and Conclusions
This chapter presented a multi-cue 3D model-based tracking algorithm that fuses
colour centroid, intensity edge and texture template cues from stereo cameras in
a Kalman ﬁlter framework. Cues are fused through the measurement model in the
IEKF, which relate the observed features to the pose of the object. This approach al-
lows stereo measurements to be used without explicit stereo correspondences, since
all measurements interact through the ﬁlter state. Furthermore, the framework is
completely extensible; fusion of additional cameras and cues simply requires a suit-
able measurement model.
Experimental results demonstrate that multi-cue fusion enables robust tracking in
visual conditions that otherwise cause conventional single-cue tracking algorithms to
fail, including low contrasting backgrounds, motion blur, lighting variations and oc-
clusions. The robustness of multi-cue tracking relies on the constituent modalities
having independent and complementary failure modes, so that a given visual distrac-
tion does not suppress all cues. Furthermore, the additional constraints provided by
multi-cue tracking eliminate the problem of singularities in the estimated pose that
arise for certain symmetries. The obvious direction for future work is to include ad-
ditional modalities that have not yet been exploited. These might include new cues
such as motion and depth, or functions of existing cues including curved intensity
edges and higher order moments of colour blobs.
Calculation of the ﬁlter weight for optimal fusion requires accurate estimation
of the measurement errors. The current implementation employs empirical values
for the measurement errors, which are likely to produce a sub-optimal state esti-
mate. This situation could be improved by estimating the observation error as part of
the measurement process. In addition to providing a more accurate state covariance,

112
5 Multi-cue 3D Model-Based Object Tracking
(a) Initial and tracked pose of the object closely match
(b) Pose bias during camera motion
(c) Pose bias at ﬁnal camera position
Fig. 5.16. Selected frames from ego-motion tracking sequence. Tracked pose is shown by the
white wireframe model, and initial pose (compensated for ego-motion) is shown in black.
robustness would be improved by weighting the estimated pose towards the most
reliable features under current conditions.
The measurement process could be further improved by using the estimated state
covariance to guide the search for features. In the current implementation, measure-
ments are restricted to a ﬁxed ROI to reduce computational expense and eliminate
background clutter. Unfortunately, a small ﬁxed ROI also hinders recovery in the case
of a tracking loss. By modulating the size of the ROI by the state covariance, efﬁcient
tracking could be maintained in favourable conditions while automatically increas-
ing the search space when conditions degrade. The same applies to other thresholds
used in edge and texture matching.
The current IEKF implementation assumes smooth motion through a constant
velocity state transition model. This constraint is likely to be violated in an unstruc-
tured domestic environment where the dynamics are inherently unknown. Adaptive
Kalman ﬁltering (for example [164]) attempts to address this issue by estimating
the state transition matrix, and may be a better suited to service applications. How-
ever, once an object is grasped, which is the ultimate aim in this work, the motion

5.6 Discussion and Conclusions
113
of the target becomes completely constrained by the robot. In this case, the constant
velocity model should be replaced with a model describing the imposed motion.
Ultimately, Kalman ﬁltering is only one of many tracking frameworks, both para-
metric and non-parametric. Particle ﬁltering is another popular tracking framework
in robotic applications, in which the Gaussian noise assumption is replaced by a sam-
pled state distribution. Particle ﬁlters are generally more computationally expensive,
but can also provide more robust recovery from tracking errors. The multi-cue ap-
proach developed in this chapter could be implemented with particle ﬁltering among
many other tracking frameworks.

6
Hybrid Position-Based Visual Servoing
The preceding chapters developed a framework for perception based on automatic
extraction of 3D models from range data for object classiﬁcation and tracking. In
robotics, however, perception is only ever half the story! This chapter addresses the
complementary problem of controlling a robotic manipulator to interact with the
perceived world. Speciﬁcally, the controller must be able to drive an end-effector to
some desired pose relative to a detected object. The traditional solution is kinematic
control, in which joint angles form the control error and the pose of the end-effector
pose is reconstructed through inverse kinematics. This approach can be effective for
service robots when the camera parameters and kinematic model are well calibrated,
as demonstrated in [10]. However, it is generally accepted that kinematic control
deteriorates with increasing mechanical complexity [45]. Economic constraints im-
pose additional limitations on the accuracy of calibration, including low manufac-
turing tolerances, cheap sensors and lightweight, compliant limbs for efﬁciency and
safety. Achieving reliable, long term operation in an unpredictable environment rein-
forces the need to tolerate the effects of wear on sensors and mechanical components.
Clearly, a more robust control solution is required.
Visual servoing is a robot control technique that minimizes the effect of calibra-
tion errors by using a camera to directly observe the relative pose between the object
and end-effector. Section 2.4.2 reviewed contemporary visual servoing schemes, and
this chapter shows how position-based visual servoing is a suitable control frame-
work in the service robot domain. An extended approach, called hybrid position-
based visual servoing, is proposed to overcome the limitations of traditional position-
based visual servoing, namely a reliance on accurate camera calibration and loss of
control when the end-effector is obscured. The hybrid controller dynamically esti-
mates calibration-related parameters, and fuses visual tracking with kinematic mea-
surements of the end-effector. This chapter also introduces the use of active visual
cues to improve the robustness of end-effector tracking.
The following section motivates the proposed scheme by elaborating the chal-
lenges of visual servoing for service robots. Section 6.2 formalizes the servoing
problem and describes the control law for position-based control. Sections 6.3 and
6.4 describe how the pose of the end-effector can be estimated from visual and kine-
G. Taylor and L. Kleeman: Visual Perception and Robotic Manipulation, STAR 26, pp. 115–144, 2006.
© Springer-Verlag Berlin Heidelberg 2006

116
6 Hybrid Position-Based Visual Servoing
matic measurements, including an analysis of calibration errors. Fusion of visual
and kinematic measurements to estimate both pose and calibration parameters is dis-
cussed in Section 6.5. Section 6.6 describes image processing, including the use of
active cues, and other implementation details. Finally, experimental results presented
in Section 6.7 demonstrate the increased accuracy and robustness of hybrid position-
based visual servoing compared to similar control techniques.
6.1 Introduction
To summarize the discussion in Section 2.4.2, visual servoing is an approach to robot
control that uses visual measurements in the feedback loop. Visual servoing schemes
are broadly classiﬁed as image-based, in which the control error is expressed in terms
of features on the image plane, or position-based, in which the full pose of the robot
and target are reconstructed before applying Cartesian control (see Figure 2.6). Var-
ious other approaches, including 2-1/2-D visual servoing [97] and afﬁne approxima-
tions [24], have also been proposed to overcome the drawbacks of classical schemes.
Visual servoing has been applied to a variety of domains, including mobile robot-
ics [158], industrial manipulators [75] and unmanned aircraft [159]. Each application
presents distinct challenges that motivate a unique formulation of the controller. It is
therefore prudent to begin this chapter by considering the characteristics of service
robots, the challenges of performing manipulation tasks in a domestic environment,
and the inﬂuence of these factors on controller design.
Our ﬁrst consideration is the conﬁguration of the hand-eye system. As discussed
in Section 2.4.2, visually servoed manipulators are usually conﬁgured as eye-in-
hand, with the camera rigidly attached to the end-effector, or ﬁxed-camera, with
the camera and manipulator at opposite ends of a kinematic chain. The ﬁrst conﬁg-
uration eliminates the problem of tracking the end-effector, if the hand-eye trans-
formation is well calibrated. However, a ﬁxed-camera offers greater ﬂexibility to
control the viewpoint and manipulator independently. This can be advantageous for
service robots since the camera will likely serve many functions other than grasp
control. For aesthetic reasons, ﬁxed-camera is also the obvious conﬁguration for hu-
manoid service robots. The controller developed in this chapter is thus formulated for
a ﬁxed-camera hand-eye system. The application to an eye-in-hand implementation
is straightforward and left as an exercise for the reader.
One of the attractions of image-based visual servoing is that the effect of camera
calibration errors can be minimized by capturing and using an image of the robot in
the desired pose as the controller reference [63, 65, 97]. Unfortunately, the singular,
ad hoc nature of service tasks precludes any opportunity to directly observe the de-
sired pose in advance. Instead, the controller reference must be predicted from an
internal world model, which is constructed from the real world through an uncertain
camera model. Uncertainties thus affect the positioning accuracy of the controller
regardless of the selected visual servoing scheme, whether position-base or image-
based. As will be shown later in the chapter, accurate position-based control can still
be achieved by explicitly compensating camera calibration errors in the control loop.

6.1 Introduction
117
Another perceived advantage of classical image-based control is that tracked
features are easily constrained to remain within the ﬁeld of view. Position-based
schemes have also been developed to generate trajectories that maximize the avail-
ability of visual feedback [18]. However, the combination of ad hoc tasks and clut-
tered environments in service applications will likely introduce unavoidable occlu-
sions that invalidate any advantage of these approaches. For example, a large initial
pose error (when the end-effector is far from the target) may prevent the end-effector
and target from being viewed simultaneously, and obstacles near the target may intro-
duce occlusions when the end-effector should otherwise be visible. A control scheme
for service robots must therefore emphasize robustness to occlusions rather than the
impractical goal of maintaining continuous visual feedback. It will be shown that this
is achieved by fusing visual and kinematic measurements in the feedback loop.
Collision avoidance is important for reliable operation in a cluttered domestic en-
vironment, and relies on the controller to generate predictable trajectories around ob-
stacles in Cartesian space. Purely image-based control generates smooth image-plane
trajectories but leads to unpredictable Cartesian motion and a higher likelihood of
collisions. Conversely, position-based control, image-based control with decoupled
orientation and translation [83,129] and 2-1/2-D servoing [97] generate predictable
Cartesian trajectories, and all exhibit similar stability and robustness [31]. Recent
studies in applying biological reach-to-grasp strategies to robotic systems, which
suggest that humans use 3D structural cues rather than image-based features [67],
also hint at the importance of Cartesian control in human-like tasks. In fact, other
results suggest that human motions are planned in Cartesian space rather than joint
space [56], and that humans apply 3D interpretations to almost all visual information,
including monocular images [60]. These observations are reﬂected in the emphasis
on position-based visual servoing and other 3D techniques in the perception and
control framework presented in this book.
In most visual servoing schemes, a kinematic model of the manipulator is ulti-
mately required to transform the control error into joint velocities. The impact of
kinematic calibration errors depends on the whether visual servoing is implemented
as end open-loop (EOL) or end closed-loop (ECL) [68]. For a ﬁxed camera conﬁg-
uration, EOL control is similar to the classical look-then-grasp approach in which
a target object is tracked and the desired pose of the gripper is calculated through
inverse kinematics. While EOL control simpliﬁes the visual tracking problem, kine-
matic errors have a signiﬁcant impact on positioning accuracy. Conversely, ECL con-
trol tracks both the end-effector and target, which signiﬁcantly reduces the effect of
kinematic uncertainties since the control error reduces to zero only when the end-
effector is observed to reach the target. To further reduce the reliance on a kinematic
model, some visual servoing schemes attempt to estimate the control error to joint
velocity transformation directly [65,110].
While robustness to kinematic calibration errors appears to make ECL control
more suitable for service robots, this advantage is counterbalanced by a sensitivity to
occlusions. That is, both the end-effector and target must be continuously visible to
calculate the control error. Conversely, occlusions of the end-effector have no effect
on the convergence of EOL control. To simultaneously provide robustness to cali-

118
6 Hybrid Position-Based Visual Servoing
bration errors and occlusions, service robots would beneﬁt from a hybrid approach
borrowing aspects of both ECL and EOL visual servoing. This can be achieved by
fusing kinematic and visual measurements to track the end-effector. The controller
smoothly transits between visual and kinematic control as the observability of the
end-effector varies. Fusion also improves the robustness of the tracker to visual dis-
tractions since kinematic measurements add further constraints on the pose of the
end-effector. However, just as position-based ECL control is biased by camera cal-
ibration errors and EOL control is biased by kinematic errors, the hybrid approach
must address both issues. As will be shown below, calibration errors can be compen-
sated through on-line estimation of system parameters.
Yokokohji et al. [168] previously demonstrated fusion of kinematic and visual
measurements for dextrous manipulation of an object using a three ﬁngered hand. In
that case, fusion was proposed to overcome the low resolution and sampling rate of
visual control, while compensating for errors associated with a kinematic model de-
scribing the contact between the object and ﬁngertips. Fusion was achieved through a
least squares ﬁt of the system state to the measurements, weighted by their respective
errors. This approach is similar to the Kalman ﬁlter framework described below, but
without fusing previous states or estimating the state covariance. Furthermore, the
effects of occlusions and camera calibration errors were not considered. These deﬁ-
ciencies are addressed in the hybrid position-based visual servoing scheme described
in the following sections.
6.2 Visual Servo Controller Overview
Figure 6.1 illustrates the relevant coordinate frames and transformations for a simple
grasping task. In addition to the camera frame C and world frame W (introduced in
Section 2.2.3), B is the base frame of the manipulator, E is the end-effector frame,
object frame O describes the pose of the target, and the grasp frame G (rigidly at-
tached to E) describes the desired position of the target relative to the gripper. This
ﬁnal frame is introduced to facilitate planning; various grasps are readily generated
by setting the relative pose of G and E. For example, placing G between the ﬁnger-
tips of a hand-like gripper produces a precision grasp, while placing G near the base
of the ﬁngers results in a power grasp.
The basic task of visual servoing is to regulate the pose of the end-effector to
align the grasp frame G with the object frame O. The ECL position-based controller
described here is similar to the formulation in [68], with the addition of the grasp
frame. The pose of the object and end-effector in the world frame (denoted by the
transformations WHO and WHE in Figure 6.1) are estimated from visual measure-
ments. The control error is then the pose error between the grasp and object frames
(denoted as GHO), which is identity when the frames are aligned at the completion
of the control task. This control error is calculated from the measurements as
GHO = (WHEEHG)−1 ·WHO
(6.1)

6.2 Visual Servo Controller Overview
119
object
target
right
camera
left
camera
WHB
B
gripper
G
E
O
W,C
GHO
WHO
BHE
WHE
EHG
robot arm
base
Fig. 6.1. Deﬁnition of visual servoing task with relevant coordinate frames and transforma-
tions. Reprinted from [149]. c2004, IEEE. Used with permission.
where we recall that EHG is ﬁxed in a given task. Expanding the rotational and trans-
lational components of GHO via equation (2.3) yields:
GRO = (ERGWRE)−1WRO
(6.2)
GTO = (WREERG)−1[WTO −WREETG −WTE]
(6.3)
For each new observation of the object and end-effector, a proportional control
law generates the required velocity screw of the end-effector to drive the pose error
to identity. Denoting the rotational gain as k1 and representing the orientation error
GRO (equation (6.2)) as an angle of rotation GθO about axis GAO, the angular velocity
component of the required velocity screw (expressed in the grasp frame) is
GΩ = k1GθOGAO
(6.4)
The translational component of the velocity screw, using an independent gain of k2,
is
GV = k2GTO −GΩ× GTO
(6.5)
where the ﬁrst term drives the grasp frame towards the object and the second term
compensates for the relative translational motion of the object induced by the angular
velocity in equation (6.4). Transforming the above velocity screw to the robot base
frame via equations (2.19)-(2.20) yields the velocity command sent to the robot:1
1The experimental implementation employs a Puma robot controlled in “tool-tip” mode;
the desired velocity screw is actually speciﬁed in the end-effector frame, and the transforma-
tion to the robot frame in equations (6.6)-(6.7) is applied internally by the Puma controller.

120
6 Hybrid Position-Based Visual Servoing
BΩ = BREERGGΩ
(6.6)
BV = BREERG[GV−GΩ×(BREETG + BTE)]
(6.7)
To calculate the above command, the transformation BHE is reconstructed from mea-
sured joint angles and known kinematics. However, it is important to note that the
velocity screw is zero when the pose error (equations (6.2)-(6.3)) is identity inde-
pendently of BHE. Thus, kinematic calibration errors in the calculation of BHE have
no effect on the positioning accuracy of the controller (although dynamic perfor-
mance may degrade). More importantly, the above formulation does not involve the
transformation between the world frame and robot base frame (WHB in Figure 6.1),
meaning that the location of the robot need not be known if visual measurements are
available (unlike EOL control).
The accuracy and robustness with which WHE and WHO (the pose of the end-
effector and object) can be tracked imposes a fundamental limitation on the above
controller. A framework for robustly tracking the object using texture, edge and
colour cues was discussed in Chapter 5. For end-effector tracking, even greater ac-
curacy and robustness to visual distractions and occlusions can be achieved by ex-
ploiting the fact that the gripper is part of the robot. This has two important con-
sequences: ﬁrstly, we are free to design active, artiﬁcial visual cues that can be ro-
bustly detected, and secondly, kinematic measurements are also available to support
visual tracking and enable calibration errors to be compensated. The following sec-
tions present a framework for fusing visual and kinematic measurements to robustly
track the gripper. Section 6.3 presents measurement models and an error analysis for
visual pose estimation, and Section 6.4 presents a similar discussion for kinematic
measurements. Finally, a Kalman ﬁlter tracking framework to robustly estimate WHE
by fusing visual and kinematic measurements is presented in Section 6.5.
6.3 Visual Feedback
6.3.1 Gripper Model with Active Visual Cues
This section describes a framework for model-based visual tracking using point
cues at known locations on the end-effector. Artiﬁcial cues are commonly used in
visual servoing, typically implemented as coloured dots or similar passive mark-
ers [68]. This approach offers simplicity and robustness in a controlled environment,
but can be defeated in general applications where varying lighting conditions and
background clutter distract passive feature detection. Avoiding these problems is a
simple matter of replacing the passive cues with actively lit markers that maintain
a constant appearance in the presence of lighting variations. Light emitting diodes
(LEDs) provide a simple implementation of active cues, as shown in Figure 6.2(a).
Another advantage of artiﬁcial cues is the ability to independently activate individual
markers, which is exploited in Section 6.6) to solve association ambiguities.
Let the point set Gi, i = 1...n, represent the locations of the n cues in the end-
effector frame, which will be referred to as the gripper model. The gripper model for

6.3 Visual Feedback
121
(a) Active LED markers
G1
E
X
Y
P3
P5
G2
G7
G6
Z
θ5
θ3
G5
G3
G4
(b) Gripper model
Fig. 6.2. Active LED features and articulated model for visual gripper tracking.
our experimental robot, with n = 7, is shown in Figure 6.2(b). The hand-like exper-
imental gripper has a single DOF that allows the thumb and foreﬁnger to open and
close. The gripper model is similarly articulated so that G5, G2 and G3 reﬂect the
current location of the ﬁnger cues. Let P5 and P3 represent the pivot points about
which the thumb and foreﬁnger rotate, and let θ5 and θ3 represent the maximum
angles of rotation as shown in Figure 6.2(b). Furthermore, let "G5, "G2 and "G3 repre-
sent the nominal position of the ﬁnger cues for a fully open grasp, and let δ, where
0 ≤δ ≤1, represent the fraction by which hand is closed, as measured by a joint
encoder attached to the thumb. Then, the position of the ﬁnger cues are calculated as
G2 = P3 +Rx(−δθ3)["G2 −P3]
(6.8)
G3 = P3 +Rx(−δθ3)["G3 −P3]
(6.9)
G5 = P5 +Rx(δθ5)["G5 −P5]
(6.10)
where Rx(θ) is the rotation matrix for a canonical rotation about the x-axis by angle
θ. The gripper model, including opening angles and locations of LEDs and pivot
points are manually calibrated for the experiments in Section 6.7.
6.3.2 Pose Estimation
In a 3D model-based tracking framework, the pose of the gripper is estimated by
matching the gripper model to stereo image plane measurements of the artiﬁcial
cues. In the following discussion, let the measurements of the n markers on the left
and right image plane be represented by Lgi and Rgi, i = 1...n, respectively.
The matching and estimation problem can be solved using two distinct ap-
proaches. The ﬁrst approach is to reconstruct corresponding stereo measurements
of individual markers before ﬁtting the gripper model to the resulting set of 3D
points. The advantage of this approach is that stereo triangulation and model-based
pose recovery from point features are well studied problems with closed form so-
lutions [48, 54]. Furthermore, this approach leads to a linear measurement function

122
6 Hybrid Position-Based Visual Servoing
in the tracking ﬁlter since the pose forms both the measurement and state. How-
ever, this method also has several important drawbacks. Firstly, corresponding stereo
measurements of each point may not always be available due to occlusions (includ-
ing self-occlusions). When a marker is visible in only one camera, potentially useful
information simply discarded. Furthermore, the recovered pose will not be optimal
with respect to the measurement error if the points are uniformly weighted for model
ﬁtting. When 3D points are reconstructed from stereo measurements, the uncertainty
is greatest in the Z-direction and increases for distant points as the reconstruction
becomes less well-conditioned. Optimal pose recovery therefore requires each point
to be weighted by a non-uniform covariance that must be calculated for each point.
The second approach is to minimize the reprojection error on the image plane
between the measured markers and the projection of the gripper model, such that the
projected model point appear closest to the measurements at the optimal estimated
pose. This method leads to a more complex tracking ﬁlter since the measurement
equations involved a non-linear camera projection, and minimization of the image
plane error requires a numerical solution. However, this approach has several dis-
tinct advantages over the method described above. Firstly, a uniform error variance
can be assumed over all measurements, since the error function is formed on the
image plane. More importantly, this approach does not require explicit stereo corre-
spondences; rather, each measurement is only matched to the projection of a point in
the gripper model. Thus, all available measurements contribute to the estimated pose
without discarding markers visible in only one camera. In fact, this method extends
to a general n-camera conﬁguration by including an appropriate projection for each
camera. A monocular version is used for position-based visual servoing in [163], and
the following sections describe a stereo implementation.
Let pE represent the gripper pose in the world frame and H(pE) = WHE repre-
sent the equivalent homogeneous transformation matrix. Furthermore, let CHW rep-
resent the transformation from the world frame to the camera frame (due to the active
pan/tilt head), which is the inverse of the transformation in equation (2.30), and let
L,RP represent the left and right camera projection matrices given by equation (2.29).
Now, transforming the 3D model points Gi from the end-effector frame to the camera
frame and projecting onto the stereo image planes gives the predicted measurements
L,Rˆgi(pE) for the given pose pE:
L,Rˆgi(pE) = L,RPCHWH(pE)Gi
(6.11)
The optimal estimate of the pose, denoted by ˆpE, minimizes the reprojection error
between the predicted and actual measurements. The reprojection error D2(pE) for a
given pose pE is the sum of the squared distances between matching points given by
D2(pE) = ∑
i
d2(Lˆgi(pE), Lgi)+d2(Rˆgi(pE), Rgi)
(6.12)
where L,Rgi are the actual measurements, L,Rˆgi are given by equation (6.11), and
d(x1,x2) deﬁnes the Euclidean distance between x1 and x2. The estimated pose is
thus recovered by the minimization

6.3 Visual Feedback
123
ˆpE = argmin[D2(pE)]
(6.13)
The equal weighting of all measurements to form the reprojection error in equation
(6.12) is consistent with the assumption of uniform error variance for all image plane
measurements, and the estimated pose is optimal with respect to measurement error.
6.3.3 Compensation of Calibration Errors
Any real vision system will suffer some uncertainty in the estimated intrinsic and ex-
trinsic camera parameters (in addition to the inherent approximations in the camera
model). At best, the pose of the end-effector recovered from equations (6.12)-(6.13)
is some unknown transformation of the real pose. The following discussion develops
an error model to analyse the impact of uncertainties in camera calibration on the
accuracy of the estimated pose. This analysis will also reveal strategies to compen-
sate for calibration errors during visual servoing. A complete derivation of the error
model is given in Appendix D.
We begin this discussion by considering the reconstruction of a single point X
from corresponding stereo measurements Lx = (Lx, Ly) and Rx = (Rx, Ry). Let X
represent the estimated 3D position recovered from minimization of the reprojection
error (equations (6.12)-(6.13)) for a single point. It is easily shown (see Section D.1)
that the optimal reconstruction is
X =
b
Lx−Rx
Lx+ Rx, Ly+ Ry, 2 f

(6.14)
Now, let b∗, f ∗and ν∗represent the actual baseline, focal length and verge angle
of the stereo rig, and let b, f and ν represent the calibrated parameters of the camera
model. The task is to determine how the deviation between actual and calibrated
parameters affects the reconstruction in equation (6.14). We ﬁrst note that the verge
angle is used to projectively rectify non-rectilinear image plane measurements as
described in Section 2.2.4. When the estimated verge angle differs from the actual
verge angle (ν −ν∗= 0), projective rectiﬁcation over-compensates by (ν −ν∗). In
this case, the ideal rectilinear stereo pin-hole projection relating X and L,Rx given by
L,Rx = f ∗
Z (X ±2b∗,Y)
(6.15)
is replaced by the over-corrected model (see Section D.2)
L,Rx = f ∗((X ±2b∗)cos(ν −ν∗)Z sin(ν −ν∗), Y)
Z cos(ν −ν∗)±(X ±2b∗)sin(ν −ν∗)
(6.16)
where the top sign is taken for L and the bottom for R. The analysis proceeds by
applying a small angle approximation to equation (6.16) (assuming a mild verge
error, ν −ν∗ 1) and substituting the result into the reconstruction equation (6.14).
Taking a Taylor series expansion with respect to f, b and ν about the operating
point f = f ∗, b = b∗and ν = ν∗, it then follows that the relationship between the

124
6 Hybrid Position-Based Visual Servoing
actual point X, and point reconstructed via an imperfect camera model X can be
approximated by (see Section D.2):
X ≈

1+ b−b∗
2b∗
+ X2 +Z2
2Zb∗(ν −ν∗)


X
Y
Z

+ f −f ∗
f ∗


0
0
Z

+(ν −ν∗)


2Xb∗/Z
0
2b∗


(6.17)
Finally, we note that the distances between gripper features are generally much
smaller than the distance from the camera to the gripper. In this case, the follow-
ing ratios in equation (6.17) can be approximated by the constants k1 and k2:
X2 +Z2
2b∗Z
∼k1, 2b∗X
Z
∼k2
(6.18)
The relationship between X and X then reduces to a linear function of b, f and ν:
X(b, f,ν) ≈K1(b,ν)X+K2(f)(0, 0, Zi) +T(ν)
(6.19)
where
K1(b,ν) = 1+ b−b∗
2b∗
+k1(ν −ν∗)
(6.20)
K2(f) = f −f ∗
f ∗
(6.21)
T(ν) = (ν −ν∗)(k2, 0, 2b∗)
(6.22)
Equation (6.19) reveals that a verge angle error translates the reconstructed point
by T(ν) relative to X. Importantly, this bias has negligible effect on the accuracy of
ECL visual servoing. Since the object and gripper will suffer approximately equal
bias when aligned, the relative pose error remains unbiased. The focal length error
scales the reconstructed point in the Z direction by K2(f), which can be expected
from equation (6.14) where f appears in only the Z term. While this scaling can affect
model-based tracking, focal length is typically the most accurately known or readily
calibrated camera parameter. Thus, this source of error will also be neglected in the
remaining discussion. The ﬁnal source of error is a scaling of the reconstruction by
K1(b,ν) due to baseline and verge angle error. This is to be expected for the baseline,
since b is a factor of X in equation (6.14). Accurate calibration of the verge angle
and baseline require the cameras to be precisely mounted on the active pan/tilt head,
which is difﬁcult in practice. Furthermore, the baseline may change dynamically if
the camera centres are not aligned with the verge axes. Thus, we propose that the
main contribution to error in the reconstruction is the scale term K1(b,ν).
The above analysis involved only a single point X, and we now consider the
effect of the scale error K1(b,ν) on the estimated pose of an object represented by
multiple points. Intuition leads to the conclusion that scaling the reconstructed points
by K1(b,ν) accordingly scales the estimated pose. If this were true, visual servoing
would still converge since the pose error would still be zero when the (scaled) gripper

6.3 Visual Feedback
125
and object are aligned. However, it is shown below that K1(b,ν) does not lead to a
simple scale of the estimated pose.
The following analysis is simpliﬁed by considering only pure translation of the
gripper, represented by TE = (XE,YE,ZE). As before, let K1 represent the scale due
to camera calibration errors and let Gi = (Xi,Yi,Zi), i = 1...n, represent the actual
locations of n markers in the gripper frame. Without loss of generality, the mean
position of markers in the gripper frame is assumed to be zero (∑Gi = 0). Now, let
G∗
i represent the (inaccurately) measured location of markers in the world frame,
that is, after scaling by K1 due to calibration errors. The locations of these features
can be expressed as
G∗
i = K1(Gi +TE)
(6.23)
The actual measurements L,Rgi are the projection of these points onto the left and
right image planes (taking the top sign for L and bottom for R), given by:
L,Rgi =
f
K1(Zi +ZE)(K1(Xi +XE)±2b, K1(Yi +YE))
(6.24)
Now, let TE = (XE, YE, ZE) represent the estimated pose of the gripper by minimiza-
tion of the reprojection error (equations (6.12)-(6.13)). In calculating the reprojection
error, the unscaled model is translated by TE and projected onto the image plane to
give the predicted measurements:
L,Rˆgi =
f
Zi + ZE
(Xi + XE ±2b, Yi + YE)
(6.25)
The reprojection error between the predicted and actual measurements is calculated
by substituting equations (6.24) and (6.25) into equation (6.12). Solving equation
(6.13) analytically, it can be shown (see Section D.3) that the optimal estimate of the
gripper translation minimizing the reprojection error is given by:
TE =
XE ∑XiZi +YE ∑YiZi −ZE(∑X2
i +∑Y 2
i +4nb2)
K1(XE ∑XiZi +YE ∑YiZi)−ZE[K1(∑X2
i +∑Y 2
i )+4nb2]K1TE
(6.26)
The above equation shows that the intuitive result TE = K1TE (ie. the estimated pose
is simply scaled by K1) only occurs when n = 1 (a single point) or K1 = 1 (perfect
calibration). Otherwise, the pose bias caused by camera calibration errors is a func-
tion of the arrangement of points in the model itself. In other words, the estimated
pose for two different objects at the same position, in the presence of the same cal-
ibration errors, will be different! This result can be understood by considering the
effect of unknown scale in monocular images; large objects appear closer and small
objects further away, even though they may be equally distant from the camera2. This
has a serious and detrimental effect on visual servoing, since the pose error may not
converge to zero when the object and gripper frames are aligned due to a differing
bias in the estimated pose.
2A well-used effect in TV and movie production known as “forced perspective”.

126
6 Hybrid Position-Based Visual Servoing
The proposed solution to compensate for calibration errors is to estimate both the
pose pE and unknown scale K1 of the gripper in minimizing the reprojection error.
The predicted measurements in equation (6.11) are replaced with
L,Rˆgi(K1,pE) = L,RPCHWH(pE)·(K1EGi)
(6.27)
The pose is estimated as before, by substituting the predicted and actual measure-
ments into equation (6.12) and minimizing the observation error with respect to the
estimated parameters. In this case, the minimization is:
(K1, ˆpE) = argmin[D2(K1,pE)]
(6.28)
To sufﬁciently constrain the scale, four or more measurements are required with
at least one feature on each image plane. Monocular measurements do not sufﬁ-
ciently constrain the scale since the projective transformation in equation (2.23) is
only deﬁned up to an unknown scale (ie. central projection is a many-to-one map-
ping). Section 6.5 describes how unconstrained parameters are handled in the im-
plementation of the tracking ﬁlter. Finally, any remaining bias due to unmodelled
camera errors are assumed to be sufﬁciently small to be neglected, which is veriﬁed
experimentally in Section 6.7.
6.4 Kinematic Feedback
At the most basic level, kinematic feedback originates from the angle encoders at-
tached to the joints of the robot. As with the visual pose estimation, the kinematic
pose of the gripper is estimated by minimizing the error between the predicted mea-
surements (as a function of pose) and the actual measurements. Rather than formulat-
ing the error directly in terms of joint angles, the pose of the gripper is ﬁrst recovered
via a kinematic model that encodes the transformations between the links of the ma-
nipulator (see [102] for a discussion of robot kinematics), and the error is expressed
in terms of pose vectors. Let θθθ represent the joint angle vector, and let BHE(θθθ) rep-
resent the measured gripper pose (with respect to the robot base) obtained from the
kinematic model. Now, let BHE represent the predicted measurement, given by
BHE = WH−1
B ·W HE
(6.29)
Section 6.5 describes how the measurement error is calculated from the equivalent
6-dimensional pose vectors representing the transformations BHE and BHE(θθθ).
The predicted measurement in equation (6.29) requires knowledge of WHB, the
location of the robot base in the world frame3. Many methods for calibrating this
transformation are reported in the literature, including the use of special calibration
targets [156] and observation of known robot motions [6, 61]. Typically, the robot
position is calibrated once during initialization and thereafter assumed to remain
3This is equivalent to the so-called hand-eye transformation for eye-in-hand systems.

6.5 Fusion of Visual and Kinematic Feedback with Self-Calibration
127
constant. However, this approach suffers from several difﬁculties in the presence of
calibration errors. In particular, the visual measurements used for calibration may be
biased by camera calibration errors, while compliant/ﬂexible manipulators may pro-
duce dynamic and unpredictable deviations from the kinematic model. In the latter
case, static calibration will result in biased kinematic measurements even if the robot
position is well known.
A solution to this problem is to treat the robot position as a dynamic bias be-
tween the measured kinematic pose (in the robot base frame) and the pose estimated
from visual measurements (in the world frame). This is achieved by adding the pose
parameters for BHW to the state vector of the tracking ﬁlter. For each new set of kine-
matic and visual measurements, BHW is updated through the measurement function
in equation (6.29), providing continuous, on-line calibration. Solving this unknown
transformation requires the pose of the gripper to be sufﬁciently constrained by both
kinematic and visual measurements; kinematic pose is always available, while visual
pose recovery requires three or more image plane measurements as described above.
When the solution is unconstrained, BHW does not updated in the tracking ﬁlter, as
described in the next section.
6.5 Fusion of Visual and Kinematic Feedback with
Self-Calibration
As described in Section 6.2, position-based visual servoing requires accurate and
robust tracking of the end-effector. This is achieved through two mechanisms: fusion
of the visual and kinematic pose estimators described in the previous sections, and
on-line estimation of calibration parameters. Kinematic feedback allows servoing
to continue when the gripper is occluded or outside the ﬁeld of view, while visual
feedback improves positioning accuracy by observing the gripper and target directly.
The visual and kinematic estimators are both biased by calibration errors, which can
be compensated by estimating a few simple calibration parameters.
The Iterated Extended Kalman ﬁlter (IEKF) was introduced in Chapter 5 for
multi-cue model-based target tracking, and is also ideal for tracking the gripper from
visual and kinematic measurements. The Kalman ﬁlter is a well-known algorithm for
optimally estimating the state of a linear dynamic system from noisy measurements,
and the IEKF extends the framework to non-linear systems by solving the ﬁlter equa-
tions numerically. This framework is used in many visual and robotic tracking ap-
plications, including visual servoing (for example [163]). Appendix C describes the
basic equations of the IEKF, and a detailed treatment of Kalman ﬁlter theory can be
found in [8,74].
The Kalman ﬁlter is a statistically robust framework for sensor fusion, which is
exploited here to fuse visual and kinematic measurements. Let x(k) represent the
state (the parameters we wish to estimate) of the IEKF at time k. Following the
discussion in the Sections 6.3 and 6.4, the state can be summarized as
x(k) = (pE(k), ˙rE(k), BpW(k),K1(k))
(6.30)

128
6 Hybrid Position-Based Visual Servoing
where pE(k) is the pose of the end-effector and ˙rE(k) is the velocity screw of the end-
effector (both in the world frame), BpW(k) is the pose vector representing the inverse
of WHB (the location of the robot in the world frame, used in equation (6.29)), and
K1(k) is the scale parameter for camera calibration (used in equation (6.27)).
Now, let ˆx(k) represent the state estimated by the IEKF at time k. At the next
time step, a new state ˆx(k + 1) is estimated using a two stage process: state predic-
tion from a known dynamic model, followed by state update based on the new mea-
surements. For the ﬁrst step, the gripper pose is modelled by the familiar constant
velocity dynamics (assuming a smooth trajectory), while the calibration parameters
are modelled as static values. Thus, the predicted state variables at time k +1 based
on the estimated state at time k and the dynamic models are
pE(k +1) = pE(k)+Δt˙rE(k)
(6.31)
˙rE(k +1) = ˙rE(k)
(6.32)
BpW(k +1) = BpW(k)
(6.33)
K1(k +1) = K1(k)
(6.34)
where Δt is the sample time between state updates. The IEKF also requires an esti-
mate of the state transition noise (v(k) in equation (C.1), also known as the process
noise), to correctly weight predicted state against the measurements in the update
step. For this purpose, it is sufﬁcient to assume the state variables have independent
noise with ﬁxed variance.
The state update step then estimates the new state as a weighted average of the
predicted state and the state estimated from the new measurements at time k + 1.
These are represented by the vector y(k +1) given by
y(k +1) = (Lg1(k +1), Rg0(k = 1),..., Rgn(k +1), BpE(k +1))
(6.35)
where L,Rgi(k +1), i = 1...n, are the measured locations of the n active markers on
the stereo image planes, and BpE(k+1) is the gripper pose (in the robot base frame)
reconstructed from joint angles and the known kinematic model. As described earlier,
the state is estimated from the measurements by minimizing the observation error
between the predicted and actual measurements. The prediction models are provided
by equation (6.27) for visual measurements and (6.29) for kinematic measurements.
These models are the mechanism by which the IEKF achieves multimodal fusion, by
coupling the visual and kinematic measurements to a common underlying state.
To correctly weight the components of the observation error, an estimate of the
measurement noise (w(k + 1) in equation (C.3)) is required. As with the process
noise, we can assume the measurements have independent error with ﬁxed variance.
However, the tracking ﬁlter must also cope with the possibility that some markers
are occluded and produce no valid measurement. In Chapter 5, this problem was
solved using a variable dimension measurement vector that represented only visible
cues. An alternative approach used in this chapter is to maintain a ﬁxed dimension
measurement vector and set a very large error variance (low conﬁdence) for occluded
markers, which are thus associated with negligible weight in the state update.

6.5 Fusion of Visual and Kinematic Feedback with Self-Calibration
129
As described in Chapter 5, care must be taken when dealing with angular state
variables to avoid the problems of non-uniqueness and degeneracy in the state update
(see also Section 2.1.3). Again, the approach of [15, 161] successfully avoids these
problems. The orientation parameters of WpE and BpW are represented as differential
Euler angles in the state vector, with the total orientations stored outside the state
vector as quaternions. The transformations W HE and BHW in equations (6.27) and
(6.29) are therefore calculated in the following form:
H =

R(q)R(Δφ,Δθ,Δψ) T
01×3
1

(6.36)
where T is the translational component of the pose, q is the quaternion representing
the total orientation, Δφ, Δθ and Δψ are the differential Euler angles from the state
vector and R(·) is the rotation matrix representing the given orientation parameters.
For each state update, the incremental Euler angles are integrated into the external
quaternions using equations (2.7) and (2.12), and then set to zero in the state vector
before the next update.
When dealing with angular measurements, care must also be taken to ensure
the observation error is uniquely deﬁned (for example, the error between 0 and 2π
radians is 2πn for any integer n). Fortunately, representing the orientation of BpE by a
quaternion provides the necessary uniqueness. However, the quaternion components
cannot be assumed to have independent and ﬁxed error variance due to the constraint
imposed by normalization (see Section 2.1.3). This problem is resolved by assuming
independent errors for the equivalent Euler angles. The Euler angle variances are
then transformed to the quaternion covariance by linearizing the relationship between
Euler angles and quaternions (equation (2.7)).
As mentioned in the previous sections, special care is taken to ensure the calibra-
tion parameters in the estimated state are sufﬁciently constrained. It is well known
that the pose of a point-based model can be recovered from three monocular mea-
surements, although there may be up to four possible solutions [49]. However, three
points may be insufﬁcient for stereo cameras if two measurements correspond to the
same point on different image planes (providing only ﬁve constraints instead of six).
Furthermore, K1 is unconstrained for monocular visual measurements, while BpW(k)
requires the gripper pose to be constrained by visual measurements. Finally, avoid-
ing association errors in marker detection requires support across multiple consistent
measurements (see Section 6.6.1). To simplify the problem of constraining the esti-
mated state, the following hierarchy of estimators is adopted based on the number of
measured features, nL and nR, on each image plane:
nL ≥3 and nR ≥3: Sufﬁcient visual measurements are available to minimize the
likelihood of association errors and constrain all parameters of the state vector.
nL ≥3 or nR ≥3 (but not both): With fewer than three measurements, all measure-
ments on the given image plane are discarded due to possible association errors.
The monocular measurements on the remaining image plane are sufﬁcient to
constrain the visual pose of the end-effector and robot location. However, the
scale is unconstrained and thus excluded from the state update.

130
6 Hybrid Position-Based Visual Servoing
gripper
measurement
pose
estimation
controller
visual servo
active
vision
object
tracking
Cartesian
Arm controller
pE
L,Rgi
L,Rˆgi
EV
EΩ
BHE
pO
Fig. 6.3. Block diagram of hybrid position-based visual servoing control loop. Reprinted from
[149]. c2004, IEEE. Used with permission.
nL < 3 and nR < 3: All visual measurements are discarded due to possible associa-
tion errors. The end-effector is estimated using only kinematic measurements,
and all calibration parameters are excluded from the state update.
When necessary, K1 or BpW(k) are excluded from the state update by setting
the corresponding rows and columns of the measurement Jacobian (see equation
(C.8)) to zero. The experimental results in Section 6.7.2 demonstrate the successful
operation of the tracking ﬁlter under all of the above conditions.
The operation of the IEKF can now be summarized as follows. At each time
step, the ﬁlter ﬁrst predicts the current state using the dynamic model in equations
(6.31)-(6.34). The predicted state, which includes both the pose of the gripper pE and
the calibration parameters BpW and K1, is used to predict the visual and kinematic
measurements via equations (6.27) and (6.29). The observation error is then formed
by taking the difference between the predicted and actual measurements. The new
state is estimated by averaging the predicted state and observation error weighted
by the Kalman gain (calculated from the state and measurement covariances using
the Kalman ﬁlter update equations). As described above, care is taken to exclude the
calibration parameters from the state update if insufﬁcient visual measurements are
available. Finally, the differential orientation of pE and BpW in the state vector are
integrated into external quaternions representing total orientation, and reset to zero
for the next time step. Appendix C details the IEKF equations in each of these steps.
6.6 Implementation
This section presents the implementation of hybrid position-based visual servoing on
our experimental hand-eye platform (see Figure 7.1). A block diagram of the imple-

6.6 Implementation
131
mentation is shown in Figure 6.3. The active vision block controls the camera view
to maximize visual feedback using the proportional control laws in equations (2.31)-
(2.33). During servoing, different active vision strategies are employed depending on
the pose error. When the object and gripper cannot be observed simultaneously due
to large separation, active vision tracks only the object while visual servoing relies
on EOL control. Once the end-effector nears the object, active vision focuses on the
mid-point between the object and gripper to ensure both remain observable.
The visual servoing control loop is divided into three sets of blocks: measure-
ment, state estimation and actuation. The gripper measurement block detects the
markers using the process described below in Section 6.6.1. The pose estimation
block then estimates state of the IEKF presented in Section 6.5 and feeds the pre-
dicted gripper pose back to the measurement block to aid future detections. Similarly,
the object tracking block estimates the pose of the object as described in Chapter 5.
The visual servo controller then calculates the relative pose error and desired velocity
screw (see Section 6.2), which is sent to the Cartesian robot controller for actuation.
Controller gains k1 and k2 in equations (6.6)-(6.7) are chosen to minimize overshoot
and ringing due to measurement and actuation delays. The control cycle is repeated
for each new frame until the pose error (equations (6.2)-(6.3)) is sufﬁciently small:
|GTO| < dth and GθO < θth, where GθO is from the axis/angle representation of the
orientation error, and dth and θth are the pose error thresholds.
6.6.1 Image Processing
In this implementation, image processing is deliberately simpliﬁed to facilitate ro-
bust, real-time visual servoing. Stereo images are captured at PAL frame-rate (25
Hz), 384 × 288 pixel resolution and 16 bit RGB colour. Image processing is im-
plemented on a desktop PC, taking full advantage of the MMX/SSE instruction set
and multiple processors to parallelize operations where possible (see Section 7.1.2).
Also, image processing is restricted to a sub-image region of interest (ROI) most
likely to contain valid measurements. The ROI is constructed as the bounding box
enclosing the marker locations projected onto the image planes (equation (6.11)) at
the predicted pose of the end-effector (equation (6.31)). The main image processing
task during servoing is to measure the position of each active marker, implemented
as a red LED, while robustly handling the possibility of occlusions and background
clutter. LED detection is implemented as a two step process: colour ﬁltering identiﬁes
a set of candidate LED locations, and candidates are then matched to the predicted
LED positions using a global matching algorithm.
The colour ﬁltering step identiﬁes pixels similar in colour to the LEDs. The ﬁl-
ter is implemented as a look-up table (LUT) with a binary entry for each 16-bit
RGB value indicating membership in the pass-band. The LUT is constructed from
visual measurements in the following once-off calibration process. While viewing
the stationary gripper, the LEDs are alternately lit and darkened in successive cap-
tured frames, so that the inter-frame pixel difference identiﬁes foreground pixels on
the LEDs. The foreground colours are accumulated over several frames in a colour
histogram (one cell for each 16-bit RGB value). Finally, histogram cells with less

132
6 Hybrid Position-Based Visual Servoing
(a) Gripper against background clutter
(b) Output of colour/morphological ﬁlter
Fig. 6.4. Detection of LED candidates via colour ﬁltering.
than 20% of the maximum value are cleared, and the remaining non-empty cells are
thresholded to form the colour pass-band. The ﬁlter is applied to captured frames by
replacing each pixel with the corresponding entry in the LUT. The resulting binary
image is morphologically eroded and dilated to reduce noise, and binary connectivity
analysis is applied to identify connected blobs. The centre of mass of pixels in each
blob serve as the initial candidate LED locations.
Figure 6.4(a) shows a typical view of the gripper against a cluttered background.
The output of the colour/morphological ﬁlter for the same scene is shown in Figure
6.4(b); a number of LEDs remain undetected while background clutter introduces
spurious features. A global matching algorithm helps to identify the visible LEDs
while rejecting noise. The ﬁrst step in this algorithm is to predict the LED locations
by projecting the gripper model in the predicted pose. Let ˆgi, i = 1...n, represent the
predicted position of the n LEDs from equation (6.11), and gj, j = 1...m, represent
the m candidate measurements (noting that m may be larger or smaller than n). Now,
denoting the association of LED i with candidate measurement j as ai j, the aim of
the matching algorithm is to ﬁnd the most likely set of associations A = {ai j} in
which each of i and j appears zero or one times, ie. |A| ≤min(m,n).
A simple matching approach is to associate each LED with the closest candidate
measurement. However, this only works when the predicted pose is reliable and few
spurious features are present. At the other extreme, a brute force matching search
evaluates the likelihood of all possible associations, which can be very costly for
large m. The algorithm described below avoids both problems using a sub-optimal
global matching process that is more reliable than closest-point matching, but re-
quires fewer computations than a brute force search.
Global matching aims to identify LEDs and candidates that are similarly placed
relative to surrounding matched features, as illustrated in Figure 6.5. Let ˆdik = ˆgk −ˆgi
denote the position of the kth predicted feature relative to the ith predicted feature,
and djl = gl −gj denote an equivalent relative measure between candidates. To eval-
uate association aij in Figure 6.5, the algorithm searches for other LED/candidate

6.6 Implementation
133
ˆgi
ˆgk
gl
gj
measured features
predicted features
djl
ˆdik
ai j
Fig. 6.5. Solving the association between predicted and measured features. The association
ai j between the predicted LED ˆgi and candidate measurement g j is supported by other pre-
diction/measurement pairs with matching relative positions. In this example, three predic-
tion/measurement pairs (including ˆgk and gl) are found to support association aij. Reprinted
from [149]. c2004, IEEE. Used with permission.
pairs with similar positions relative to ˆgi and gj (three such pairs are shown in Figure
6.5). The number nij of pairs consistent with the association ai j is calculated as
nij = ∑
k

1 if minl |ˆdik −djl| < εth
0 otherwise
(6.37)
The threshold εth is the maximum allowed difference in relative position for a good
match. A residual error rij is calculated for association ai j as
rij = 1
nij ∑
k

minl |ˆdik −djl| if minl |ˆdik −djl| < εth
0
otherwise
(6.38)
The above formulation always gives nij ≥1, since the LED/candidate pair given by
k = i and l = j always matches the association aij. Treating nik as the likelihood of
the association aik, the best match for LED i is the candidate j with the maximum
score, nij = maxk nik. The best association for each LED is collected in the set L (note
that LED i does not appear in L if nij ≤1):
L = {aij : nij = max
k
nik, nij > 1}
(6.39)
If the maximum score is shared by more than one association for a given LED, the
ambiguity is resolved by selecting the association with the minimum residual error,
rij = mink rik. The association is then performed in reverse, ﬁnding the LED i that
best matches candidate j over all LEDs. The reverse associations are stored in set C:
C = {aij : nij = max
k
nk j, nij > 1}
(6.40)
The set A of mutually supporting associations aij, where candidate j is the best
match for LED i and vice versa, is calculated as the intersection

134
6 Hybrid Position-Based Visual Servoing
A = L∩C
(6.41)
Next, the associations in A are tested for self-consistency. Consider the associ-
ations aij and apq: if consistent, the displacement between LEDs i and p should be
similar to the displacement between candidates j and q. Using this rule, the set Ai j
of associations apq ∈A consistent with aij ∈A is calculated as
Aij = {apq : |ˆdip −djq| < εth, apq ∈A}
(6.42)
Finally, the largest self-consistent set A, given by
A = argmax|Aij|
(6.43)
is taken as giving the best associations between measurements and LEDs. LEDs not
appearing in A are assumed to be obscured, and successfully matched measurements
are passed to the measurement vector of the IEKF.
6.6.2 Initialization and Calibration
The IEKF solves the non-linear system equations numerically, so a good estimate of
the initial pose and robot position are required to initialize the ﬁlter at the start of a
new servoing task. Typically, these parameters should be known from the output of
the ﬁlter in previous tasks. When this information is unavailable, the autonomous cal-
ibration procedure described in this section is applied before servoing commences.
This process exploits the fact that the gripper features are actively controlled to solve
the measurement association problem without measurement prediction. Note, how-
ever, that active association cannot be used during servoing since it requires a static
end-effector.
The calibration process begins by locating the gripper. With all LEDs illumi-
nated, the cameras are scanned across the workspace to ﬁnd the direction giving
maximum output from the colour ﬁlter. The LEDs are then ﬂashed in sequence and
measured separately to avoid association errors. Colour ﬁltering and image differ-
encing are applied to successive frames to measure the location of individual LEDs.
Finally, the initial pose W ˆpE(0) and scale factor K1(0) of the gripper are estimated by
minimizing the reprojection error as described in Section 6.3 using the Levenberg-
Marquardt (LM) algorithm (as implemented in MINPACK [105]). Being a numeri-
cal method, LM optimization also requires an initial estimate of the pose and scale.
Reliable results are obtained by setting the scale to unity, orientation to zero, and
estimating the translation from equation (6.14) using the average position of LEDs
on each image plane.
Once the pose is known, the robot position is initialized as WHB = WHE(BHE)−1,
where BHE is estimated from the joint angles and kinematic model. Finally, the initial
pose and robot position are passed to the IEKF and the state error variances are set to
large values. This ensures that the estimated state is initially weighted towards new
measurements until the state error covariance settles to a steady-state value.

6.7 Experimental Results
135
G3
G5
G,O
B
A
Fig. 6.6. Experimental task for evaluation of hybrid position-based visual servoing.
6.7 Experimental Results
This section presents several experimental trials designed to evaluate the accuracy
and robustness of hybrid position-based visual servoing, and compare this approach
to classical techniques. Further experiments with visual servoing in practical do-
mestic tasks are presented in Chapter 7. All experiments in this chapter involve the
positioning task is illustrated in Figure 6.6. The task involves a simpliﬁed target con-
sisting of two coloured markers located at A and B. The goal is to position the thumb
and index ﬁnger of the gripper (G5 and G3) on the line intersecting A and B, placing
A at the midpoint between the ﬁngers.
Section 6.2 deﬁnes the grasp frame G and object frame O, which are aligned at
the completion of the servoing task. In this case, the origin of the grasp frame G is
set to the midpoint between the thumb and index ﬁngers, with the Y-axis oriented
towards the thumb. In this conﬁguration, the translational and rotational components
of EHG are
ETG = 1
2(EG5 + EG3)
(6.44)
ERG(θ,a) = R

cos−1

Y·
EG5 −EG3
|EG5 −EG3|

, Y×
EG5 −EG3
|EG5 −EG3|

(6.45)
where R(θ,a) is the rotation matrix for the given angle and axis, and Y = (0,1,0)
is a unit vector in the direction of the Y-axis. Similarly, the object frame O is centred
at A, with the Y-axis pointing towards B. Frame O is oriented so that the ﬁngers point
to the right in the desired pose (since the robot is left-handed!) as shown in Figure
6.6. The translational and rotational components of WHO are
WTO = A
(6.46)
WR0(θ,a) = R

cos−1

Y·
B−A
|B−A|

, Y×
B−A
|B−A|

·R(π/2,Y)
(6.47)
where A and B are the measured locations of the markers, reconstructed from stereo
measurements of the colour centroids (using equation (6.14)).

136
6 Hybrid Position-Based Visual Servoing
After completing each trial, the accuracy of the controller is measured as the
translational error eT between A and the origin of G:
eT =
####
1
2(G3 + G5)−A
####
(6.48)
where G3 and G5 are recovered by applying equation (6.14) to the stereo colour
centroids of the associated LEDs. The rotational accuracy is measured as the angular
error eθ between the line intersecting A and B and the line intersecting G3 and G5:
eθ = cos−1
 G5 −G3
|G5 −G3|
·
B−A
|B−A|

(6.49)
The pose errors eT and eθ are averaged over a number of frames to remove uncer-
tainties in the reconstructed features. Finally, it should be noted that while the errors
in equations (6.48)-(6.49) are not metric, these measurements expose any bias in
the IEKF which may affect the performance of the controller. The following section
evaluates the accuracy of hybrid position-based visual servoing compared to conven-
tional ECL and EOL control. Section 6.7.2 then evaluates the robustness of hybrid
control in the presence of clutter and occlusions. Lastly, Section 6.7.3 tests the limits
of camera calibration errors handled by the IEKF.
6.7.1 Positioning Accuracy
This series of experiments evaluates the accuracy of the hybrid position-based visual
servoing and conventional ECL and EOL position-based schemes under favourable
operating conditions (minimum visual clutter, accurate calibration and small initial
pose error). The positioning task was repeated over ﬁve trials for each controller,
from the initial pose shown in Figure 6.7(a). In all cases, the pose error thresholds for
convergence were set to dth = 5 mm for translation and θth = 0.2 rad for orientation,
which are suitable levels of accuracy for typical service tasks.
Evaluating the hybrid controller ﬁrst, Figure 6.7(b) shows the view at the end of
a typical trial. The estimated gripper is overlaid as a yellow wireframe, and coordi-
nate frames representing O and G are overlaid in red and blue respectively. Figure
6.8 plots the pose error during servoing, which exhibits typical behaviour for a pro-
portional velocity controller. Next, a conventional ECL controller was implemented
by excluding kinematic measurements from the IEKF measurement vector, exclud-
ing BHW and K1 from the state vector and setting the ﬁxed value K1 = 1. Figure 6.9
shows the ﬁnal pose of the gripper for a typical trial with the ECL controller. Finally,
an EOL controller was implemented by discarding the IEKF entirely and estimating
the pose of the end-effector purely from kinematics measurements (including a man-
ually calibrated WHB). For positioning trials using the EOL controller, a typical ﬁnal
pose is shown in Figure 6.10.
The average error and error variance calculated using equations (6.48)-(6.49) and
averaged over all trials for each controller are shown in Table 6.1. The low variance

6.7 Experimental Results
137
(a) Stereo images showing initial pose of object and gripper.
(b) Stereo images showing ﬁnal pose at completion of servoing task.
Fig. 6.7. Positioning task result with hybrid position-based servoing. The estimated pose of
the gripper is indicated by a yellow wireframe model, while the coordinate axes of the gripper
and object frames are shown in blue and red respectively.
for all controllers reﬂects the high repeatability of the results and indicates that the
residual pose errors eT and eθ are systematic rather than random. As expected, the
EOL controller is signiﬁcantly less accurate than the controllers using visual feed-
back, since the estimated pose is biased by calibration errors in the hand-eye trans-
formation. The full ECL controller compensates for both camera and kinematic cali-
bration errors and subsequently achieves the greatest accuracy. In fact, the ﬁnal trans-
lation error is bounded only by the servoing termination threshold (dth = 5 mm). The
orientation accuracy is better than the termination threshold since orientation con-
verges faster than translation for the chosen gains (see Figure 6.8). As expected, the
ECL control achieves an accuracy somewhere between the other methods. The high
accuracy achieved by hybrid control compared to ECL control is mainly due to the
estimation of K1 to compensate for camera calibration errors. This result highlights
the importance of camera calibration for accurate position-based visual servoing, and
demonstrates that this can be achieved on-line for mild errors by simply introducing
a scale parameter associated with the estimated pose.

138
6 Hybrid Position-Based Visual Servoing
-60
-40
-20
0
20
40
60
80
100
0
1
2
3
4
5
6
7
8
Translation Error (mm)
Time (sec)
x
y
z
(a) Translation error
-0.05
0
0.05
0.1
0.15
0.2
0.25
0
1
2
3
4
5
6
7
8
Orientation Error (rad)
Time (sec)
x
y
z
(b) Orientation error
Fig. 6.8. Pose error for hybrid position-based visual servoing.
Fig. 6.9. Positioning task result with conventional ECL position-based controller.
Fig. 6.10. Positioning task result with conventional EOL position-based controller.
6.7.2 Tracking Robustness
This second experiment evaluates the robustness of hybrid position-based servoing
in poor visual conditions including occlusions, background clutter and large initial

6.7 Experimental Results
139
Table 6.1. Comparison of positioning accuracy for experimental visual servo controllers.
Controller
eT (mm) var(eT ) (mm2) eθ (rad) var(eθ) (rad2)
Hybrid position-based
5.4
0.4
0.11
0.003
ECL: no scale correction 29.4
0.9
0.22
0.01
EOL: kinematics only
54.5
1.6
0.18
0.008
pose error. Figure 6.11(a) shows the initial pose of the object against a cluttered
background. A large initial pose error places the end-effector outside the ﬁeld of
view. To test the effect of occlusions, a virtual 3D obstacle is rendered near the
object on the left image plane.
After servoing commences and before the gripper enters the ﬁeld of view, the
pose is tracked using only kinematic feedback. Figure 6.11(b) shows the estimated
pose (as a wireframe overlay) during visual servoing, just after the gripper becomes
visible. At this point, the controller switches from EOL control to full hybrid control.
As the gripper approaches the target, measurements on the left image plane are oc-
cluded by the virtual obstacle. At this point, estimation of K1 is suspended since only
monocular measurements are available. Finally, Figure 6.11(c) shows the successful
completion of the task.
The performance of the controller is indicated by the plots in Figure 6.12, which
show the translational pose error, standard deviation of the gripper translation re-
ported by the IEKF and the estimated value of K1. During the ﬁrst three seconds, the
gripper is outside the ﬁeld of view and the scale remains at the initial value of K1 = 1.
The uncertainty in the end-effector pose during this period (see Figure 6.12(b)) is
over-optimistic, since the pose is biased by the imperfectly calibrated hand-eye trans-
formation. When stereo measurements ﬁnally become available, the estimated scale
quickly converges to the actual value and the translation error is mildly perturbed as
the ﬁlter adjusts to the new measurements. Furthermore, the additional constraints
cause the pose uncertainty to decrease.
The gripper is obscured from view by the virtual obstacle approximately six sec-
onds into the task. With only monocular measurements, K1 is no longer sufﬁciently
constrained and remains ﬁxed at the most recent estimate. However, the hand-eye
transformation (not shown) continues to be updated, based on the ﬁxed scale. The
controller achieves a ﬁnal translation error of eT = 10.9 mm and orientation error of
eθ = 0.087 radian (the virtual obstacle is removed to complete these measurements),
indicating only a mild reduction in accuracy from the case in Section 6.7.1 of stereo
measurements and favourable visual conditions.
6.7.3 Effect of Camera Calibration Errors
The effect of calibration errors on the accuracy of visual servoing was examined in
Section 6.3, and the experimental results in Section 6.7.1 veriﬁed that mild errors
can be compensated by incorporating a scale factor K1. This ﬁnal experiment tests
the bounds of the error model by observing the effect of deliberately introduced cal-

140
6 Hybrid Position-Based Visual Servoing
(a) Initial pose of object and virtual obstacle (shown in grey). The gripper is outside the
ﬁeld of view and initially tracked using only kinematic measurements.
(b) Gripper enters the ﬁeld of view and visual feedback is initiated.
(c) Final pose at completion of the servoing task. The gripper is obscured by the virtual
obstacle in the left visual ﬁeld, leaving only monocular feedback.
Fig. 6.11. Select frames from positioning task in the presence of occlusions and clutter.
ibration errors. Speciﬁcally, the calibrated baseline (not the mechanical baseline) is
scaled by 0.7 to 1.5 times the nominal value, and the verge angle is offset by -0.07
to 0.14 radian (-4 to +8 degrees) from the nominal measured value. The positioning
task was performed using the hybrid position-based controller with the initial con-

6.7 Experimental Results
141
-100
-50
0
50
100
150
200
250
0
2
4
6
8
10
Translation Error (mm)
Time (sec)
x
y
z
(a) Translation error
0
0.5
1
1.5
2
2.5
3
3.5
4
0
2
4
6
8
10
Translation Std Dev (mm)
Time (sec)
x
y
z
(b) End-effector translation variance
0
0.2
0.4
0.6
0.8
1
1.2
1.4
0
2
4
6
8
10
Scale
Time (sec)
(c) Estimated model scale
Fig. 6.12. Visual servoing performance in the presence of visual occlusions and clutter.
ﬁguration shown in Figure 6.7(a). At the completion of each trial, the ﬁnal estimated
scale was recorded along with the pose error.
Figure 6.13 shows the ﬁnal estimated scale and servoing accuracy for trials with
varying error in the baseline. The results support the expected linear relationship be-
tween baseline error and estimated K1, as predicted by equation (6.20). Furthermore,
Figures 6.13(b) and 6.13(c) show that the positioning accuracy of the hybrid is not
signiﬁcantly affected by the baseline error. This is also expected, since the scale-only
error model is exact for baseline errors (that is, b is a factor of X in equation (6.14)).
However, it should be noted that the constant position error reported in Figure 6.13(b)
actually corresponds to a linearly increasing real-space error, due to the variation in
the baseline (that is, the pose error is measured in a non-metric space).
The ﬁnal scale and pose error for trials with varying verge angle error are shown
in Figure 6.14. The main limitations of the verge angle error model arise from the
small angle approximations used to derive equation (6.17). As the verge angle in-
creasingly deviates from the calibrated value, non-linearities bias the estimated pose
and decrease the accuracy of the controller. This trend is reﬂected in the observed
pose errors in Figures 6.14(b) and 6.14(c), which exhibit a local minima. It should
be noted that the minimum pose error should occur at the offset for which the es-
timated scale is unity (at a verge offset of 0.04 rad from Figure 6.14(a)), which is

142
6 Hybrid Position-Based Visual Servoing
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
Estimated scale
Baseline error factor
(a) Estimated scale
0
1
2
3
4
5
6
7
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
Translation error (mm)
Baseline error factor
(b) Translation error
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.7
0.8
0.9
1
1.1
1.2
1.3
1.4
1.5
Orientation error (rad)
Baseline error factor
(c) Orientation error
Fig. 6.13. Performance of proposed controller
for baseline calibration error.
0.6
0.8
1
1.2
1.4
1.6
1.8
2
-0.05
0
0.05
0.1
Estimated scale
verge offset (rad)
(a) Estimated scale
0
2
4
6
8
10
12
14
-0.05
0
0.05
0.1
Translation error (mm)
verge offset (rad)
(b) Translation error
0
0.05
0.1
0.15
0.2
0.25
0.3
-0.05
0
0.05
0.1
Orientation error (rad)
verge offset (rad)
(c) Orientation error
Fig. 6.14. Performance of proposed controller
for verge calibration error.
clearly not the case. This discrepancy is most likely explained by the presence of an
unknown baseline error in addition to the verge offset.
6.8 Discussion and Conclusions
This chapter presented the development and implementation of a hybrid position-
based visual servoing framework suitable for service tasks. The proposed framework

6.8 Discussion and Conclusions
143
places particular emphasis on robustness to the loss of visual feedback, and on-line
compensation for calibration errors, which are important factors in an uncontrolled
environment. Conventional position-based visual servoing schemes employ either
kinematic (EOL) or visual (ECL) feedback, and are therefore susceptible to kine-
matic calibration errors and visual occlusions. The robust end-effector pose estimator
proposed in this chapter was designed to avoid both issues by optimally fusing both
kinematic and visual measurements. Kinematic measurements allow the controller to
operate (with reduced accuracy) while the gripper is occluded, and visual measure-
ments provide accurate pose control and on-line estimation of kinematic parameters.
Experimental results verify both the increased accuracy gained with visual feedback
and the robustness to occlusions gained with kinematic feedback.
Sensitivity to camera calibration errors is usually considered the primary draw-
back of position-based visual servoing, and it was shown in Section 6.3 that a visually
estimated pose is indeed biased by camera calibration errors. It was then shown that
the effect of mild baseline and verge angle errors can be modelled as an unknown
scale. While the error model considers only baseline and verge angle errors, this is
not an unreasonable limitation for many robotic implementations. The baseline and
verge angle both vary as a pan/tilt head is actuated, while other camera parameters
are ﬁxed and typically well known or readily estimated from visual measurements
using a variety of established calibration techniques. Experimental results validated
the error model and demonstrated that the accuracy of the controller is improved
by estimating the unknown scale along with the pose of the end-effector. The com-
bination of online calibration and kinematic/visual fusion described in this chapter
overcomes many of the classical problems associated with position-based visual ser-
voing.
The error model developed in this chapter could also be applied to the multi-
cue 3D model-based tracker developed in Chapter 5. However, new considerations
arise for tracking multiple objects (for example, the gripper and target object) in the
same image. Each object is associated with a different scale error, since the scale
depends on the current pose (through K1 in equation (6.20)). In addition, any change
in scale due to camera motion (in particular, the verge angle) will be correlated for
all objects. Tracking should account for this correlation to optimally estimate the
scale of each object. In practice, the objects are modelled from reconstructed 3D
measurements that are already biased by the scale error (unlike the gripper, which is
calibrated from metric measurements). Furthermore, the verge angle does not vary
signiﬁcantly during servoing after ﬁxating on the target. As a result, object tracking
using data-driven models is not signiﬁcantly affected by calibration errors.
Lastly, it should be mentioned that the controller presented in Section 6.2 over-
looks dynamic control issues such as steady state error (for moving targets) and sens-
ing/actuation delays. In the experimental work, these issues were avoided by using
sufﬁciently low gain and stationary targets. The system actually suffers from an ac-
quisition delay of at least 40 ms between capturing and processing stereo images,
due to internal buffering. Furthermore, an actuation delay of about 170 ms has been
observed between issuing a velocity command to the Puma robot arm and observing
the subsequent motion. Appendix E describes the simple calibration procedure that

144
6 Hybrid Position-Based Visual Servoing
was used to measure this and other latencies in the system. Clearly, dynamic control
issues must be addressed before a service robot can perform with the speed and ﬂex-
ibility of a human. A detailed treatment of dynamic control issues in visual servoing
can be found in [27].

7
System Integration and Experimental Results
In this chapter, a hand-eye service robot capable of performing interactive tasks in a
domestic environment is ﬁnally realized. The shape recovery, object modelling and
classiﬁcation, tracking and visual servoing components developed in earlier chapters
are integrated into a framework illustrated by the block diagram in Figure 1.2 (page
7). For the experiments in this book, the system is implemented on an upper-torso
humanoid robot. Humanoid robots are an appropriate test-bed since hand-eye co-
ordination will be the next vital skill once humanoid robot research moves beyond
bipedal motion. But just as importantly, a humanoid robot engenders a much greater
sense of personality than other conﬁgurations! It is important to note that the meth-
ods in this book also apply to a range of camera and manipulator conﬁgurations,
anthropomorphic or otherwise. The experimental platform, known as Metalman, is
detailed in Section 7.1.
The three fundamental steps in performing any task are: speciﬁcation, planning
and execution. Speciﬁcation is the problem of supplying the robot with sufﬁcient in-
formation to perform the required task, in a form that is unambiguous and easy to
interpret for both the user and robot. Section 7.2 outlines the common approaches
to task speciﬁcation and details the methods adopted in this chapter. The planning
component then uses the speciﬁcations to generate actions aimed at achieving the
desired goal. For this step, Section 7.3 describes a algorithm to plan a collision-free
path for the end-effector to grasp an object. Further details of the task planning cal-
culations are also presented in Appendix F. Finally, the planned path is passed to the
visual servo controller to execute the manipulation. Unlike industrial robots, service
robots may require several iterations between speciﬁcation, planning and execution
as unforeseen or ambiguous obstacles are encountered while performing a task.
Three domestic tasks are implemented to evaluate the performance of the pro-
posed framework. In the ﬁrst experiment, the robot is required to locate and grasp
a yellow box with minimal prior knowledge of the scene. The target object is only
speciﬁed in terms of shape class (box) and colour (yellow), and is placed amongst
several other objects on a table. The second experiment requires the robot to pick
up a cup that is interactively selected by the user, and pour the contents into a bowl.
Again, the robot has no prior knowledge of the scene except the class of the targets.
G. Taylor and L. Kleeman: Visual Perception and Robotic Manipulation, STAR 26, pp. 145–170, 2006.
© Springer-Verlag Berlin Heidelberg 2006

146
7 System Integration and Experimental Results
The third task explores the possibility of augmenting visual sensing with airﬂow and
odour sensors to increase the utility of the robot. This is demonstrated by performing
a recognition task that cannot be accomplished by vision alone; the robot is required
to autonomously locate and grasp a cup containing ethanol from amongst several
objects (including other cups).
The implementation and experimental results for the three task are presented in
Sections 7.4, 7.5 and 7.6, and additional VRML data and video clips can be found
in the Multimedia Extensions. It is important to note that a practical service robot
will require many additional skills, including dextrous manipulation, compliant joint
control, and high-level human interaction, that are beyond the scope of this book. By
necessity, the tasks considered in this chapter are also limited by the capabilities of
the experimental platform, and are appropriately contrived to avoid sophisticated task
planning. Despite these limitations, the experimental results presented in this chapter
demonstrate the signiﬁcant steps that this work has taken towards the development
of useful and general hand-eye robotic systems.
7.1 Experimental Hand-Eye Robot Platform
The following sections describe the hardware and software architecture of the exper-
imental upper-torso humanoid platform used in this work.
7.1.1 Hardware Conﬁguration
The experimental robotic platform, known as Metalman, is shown in Figure 7.1, and
a block diagram of the major components is illustrated in Figure 7.2. Metalman is
an upper-torso humanoid robot, with most of the control hardware located off-board.
The robot is approximately anthropomorphic in scale and conﬁguration, with six de-
gree of freedom (DOF) arms and stereo cameras on a three DOF active head. While
the lack of degrees of freedom about the waist limits the viewpoint and workspace of
the robot, the available motion is sufﬁcient for the study of manipulation in unstruc-
tured environments. It should also be noted that only one of the arms was used for
the tasks implemented in this chapter, leaving two arm coordination for future work.
While many humanoid robot platforms employ purpose-built hardware, Metalman is
mainly constructed from off-the-shelf components to aid simplicity and functional-
ity. The following sections detail the major components of the system:
Processing Platform
The processing platform is a dual 2.2GHz Intel Xeon PC operating Red Hat Linux
7.3. The two Xeon processors are each capable of simultaneously processing two
independent instruction streams, and the system is therefore capable of executing up
to four concurrent processes. Furthermore, integer and ﬂoating point SIMD opera-
tions in the Xeon allow up to 16 pixels to be processed in parallel. By exploiting both
features (see section 7.1.2), all signal processing and high-level hardware control is
implemented on board the PC.

7.1 Experimental Hand-Eye Robot Platform
147
prosthetic hand
(with LEDs)
Biclops
head
Puma 260
arm
right
camera
left camera
light stripe scanner
Fig. 7.1. Metalman: an experimental upper-torso humanoid robot. Reprinted from [150].
c2004, IEEE. Used with permission.
Stereo Cameras
Vision is provided by a pair of Panasonic CD1 CCD camera systems, which capture
images at PAL frame-rate (25 Hz) and half-PAL (384 × 288 pixel) resolution. The
Genlock mechanism is used to ensure the simultaneous exposure of stereo ﬁelds,
which is important when capturing dynamic scenes. Images are digitized in the host
PC by analogue video capture cards based on the Brooktree BT878 chipset.
Biclops Head
The stereo cameras are mounted on a three DOF Biclops head. In addition to the
usual pan and tilt axes, Biclops provides a non-linear verge control (see Appendix A).
The orientation of the head is recovered via optical encoder measurements on each
axis. Position and velocity control are implemented on a built-in PIC microcontroller,
which communicates with the host PC via an RS-232 link.
3D Scanner
A stereoscopic laser stripe scanner mounted on the Biclops head provides dense
range images of the workspace. The hardware consists of a 5 mW red laser (class
IIIa) with a cylindrical lens to generate a vertical stripe, a DC motor/optical encoder
and a PIC-based microprocessor to control the pan angle. The operation of the scan-
ner is detailed in Chapter 3.

148
7 System Integration and Experimental Results
Scanner Controller
PIC−based Laser
Active Head
Biclops
Stripe Scanner
5 mW Laser
CCD Cameras
Panasonic Stereo
Hand/LED Controller
Analogue PCI Capture Cards
Gen−Lock
CCD Camera Controllers
Panasonic CD1
Brooktree BT878−based
Series
PUMA 200
Prosthetic Hand
Otto Bock
Puma Controller
VAL II OS
ALTER Mode
HC11−based
Dual Intel XEON PC
LEDs
Multiport RS−232
Fig. 7.2. Block diagram of components in experimental platform.
Puma Robot Arms
The two PUMA 200 Series robots serving as Metalman’s arms are approximately an-
thropomorphic in conﬁguration and scale, and are operated from the original LSI-11
control cabinets running VAL II. The PUMA controller communicates with the host
PC over an RS-232 link in ALTER mode, accepting position/orientation corrections
to a static nominal pose at regular 30 ms intervals (approximately matching the 40
ms sensing cycle of the PAL cameras). Pose corrections are speciﬁed in the tool-tip
coordinate frame, and inverse kinematics and velocity proﬁling are provided by the
PUMA controller.
Prosthetic Hands
Appended to the tool-tip of each PUMA arm is a single-DOF Otto Bock prosthetic
hand with a parallel-jaw conﬁguration. The limited range of manipulations afforded
by this conﬁguration is sufﬁcient for simple pick-and-place tasks. The hands contain
no tactile feedback, leaving Metalman to rely on the measured opening angle and
visual feedback to determine the success of a grasp. Several LEDs are attached to
the hand to serve as active features for visual tracking (see Chapter 6). The hands
and LEDs are controlled by Motorola HC11 microcontrollers, which communicate
with the host PC over RS-232 links.

7.1 Experimental Hand-Eye Robot Platform
149
7.1.2 Software Architecture
The software architecture emphasizes robustness and real-time operation rather than
considerations of modularity and portability. In some cases, real-time image process-
ing has been achieved through design, such as employing coloured LEDs for tracking
the gripper and using prediction to identify a restricted region of interest for image
processing. For the remaining processes, acceleration was achieved by parallelizing
the computations where possible. The following sections describe the software tools
used for this purpose.
Hardware Accelerated Processing: MMX/SSE/SSE2
MMX (multimedia extensions), SSE (streaming SIMD extensions) and SSE2 are sin-
gle instruction, multiple data (SIMD) instruction sets for Intel processors that provide
similar functionality to digital signal processing hardware [71]. The integer SIMD
extensions provide 128-bit wide registers that can perform parallel operations on 1,
2, 4 or 8-byte integer operands, which is ideal for parallelizing many image process-
ing operations. All of the pixel-point operations (threshold, subtraction, colour con-
version, etc) and convolution operations (morphological ﬁltering, edge extraction,
template matching, etc) used in this research were parallelized using assembly coded
SIMD to achieve a 4-8 times reduction in execution time compared to non-parallel
implementations. However, not all image processing functions could be parallelized,
most prominently the look-up table operations (such as colour ﬁltering), and algo-
rithms involving dependencies between adjacent pixels (such as connectivity analy-
sis). SSE/SSE2 also support 4 packed single-precision or 2 double-precision ﬂoating
point operands. Single-precision SIMD instructions were used to accelerate image
processing functions such as radial correction/projective rectiﬁcation and eigenvalue
computations (see Chapter 5), while double-precision instructions were used to ac-
celerate matrix operations in the Kalman ﬁlter.
Parallelization Using POSIX Threads
As noted earlier, each Xeon processor supports two logical processor states and in-
creases pipeline efﬁciency by allowing instructions from one logical processor to
occupy the wait states of the other. On the dual-Xeon platform, the hardware is
presented to the operating system as four independent processors, and paralleliza-
tion can therefore be exploited by dividing software into a number of concurrent
processes (or threads). Thread management and inter-process communication was
implemented using the POSIX threads model [17]. Image processing for each cam-
era was handled by a separate thread, allowing the stereo ﬁelds to be processed con-
currently. High-level results were periodically collected by the main program thread,
which handled task planning and hardware control. Fine grained parallelization was
also employed for some processor intensive operations. For example, calculation of
the surface type for 3D range data (see Chapter 4) was accelerated by dividing the
range image into four regions and processing each in a separate concurrent thread.

150
7 System Integration and Experimental Results
The main drawback of POSIX threads in the 2.4 Linux kernel is the inability to allo-
cate threads to processors (known as assigning processor afﬁnity)1. Dividing a pro-
gram into threads did not guarantee a uniform distribution of load across all available
processors, and the beneﬁt of parallelization was often temporarily lost!
7.2 Task Speciﬁcation
The purpose of task speciﬁcation is to supply the robot with information regarding
target objects and manipulations necessary to plan and execute the task. The most
basic approach to task speciﬁcation is to program a ﬁxed manoeuvre either in soft-
ware or by manually guiding the robot through a series of set-points. This approach
is sufﬁcient for the repetitive tasks that industrial robots are usually required to per-
form. Conversely, domestic tasks are characterized by arbitrary and often unforeseen
manipulations involving possibly unknown objects in a dynamically changing envi-
ronment, which precludes the use of pre-programmed manoeuvres. Furthermore, it
cannot be assumed that the average user possesses sufﬁcient technical expertise (or
patience!) to program a new manoeuvre into the robot for every unique task.
The most promising solution to the task speciﬁcation problem is the develop-
ment of human-machine interfaces using natural modes of communication. Previ-
ous work on robotic task speciﬁcation using human modes of interaction include
verbal communication [131, 146], pointing gestures [24, 76], simple sign language
[10,66,107,122], and teaching by demonstration [32,70]. Feedback from the robot
to the user is also an important element of human-machine interaction. When an am-
biguous task speciﬁcation is encountered (for example, the requested object cannot
be uniquely identiﬁed), the robot may seek clariﬁcation by requesting the user to
choose between possible interpretations, as in [146].
The complexity of natural interaction techniques render them beyond the scope
of this book. For simplicity, the experimental tasks presented in this chapter are pre-
programmed as a scripted series of sub-tasks. Each sub-task describes an atomic
operation such as locating a target, grasping, lifting, or aligning the grasped object
with another object. The low-level set-points required for each manipulation sub-
task are planned autonomously by the robot depending on environmental conditions
(see Section 7.3). Object models are not provided directly, but are instead speciﬁed
by class (cup, box, bowl, etc.), general features (colour, size, etc) or selected inter-
actively by the user through a graphical user interface (GUI). While simplifying the
implementation, this framework also demonstrates the basic elements of interaction
and ﬂexibility to deal with unknown objects and adapt to environmental conditions
that are required by a useful service robot.
1Processor afﬁnity is now supported in version 2.6 and above of the Linux kernel.

7.3 Task Planning
151
7.3 Task Planning
In this work, task planning is the process of determining the desired pose of the robot
to grasp or place an object and generating a series of set-points to guide the gripper
on a collision-free path towards the goal. The two stages in the process are described
as grasp planning and trajectory planning.
For reach-and-grasp manipulations, a grasp planning algorithm searches for an
optimal hand pose that results in stable contacts between the ﬁngers and object. More
sophisticated planners may also constrain the planned pose to avoid collisions with
surrounding obstacles. Alternatively, the grasp planner may determine that the object
cannot be grasped. A grasp is described as stable if the object does not slide or rotate
about the contact points when lifted. For dextrous grippers and general objects, grasp
planning is complex and computationally expensive, and has no closed form solution
[38, 87]. Fortunately, grasp planning for Metalman is simpliﬁed by the single DOF
parallel-jaw structure of the prosthetic hands. The process is simpliﬁed even further
by developing a speciﬁc planner for each type of geometric primitive (box, cylinder,
etc.), rather than a general solution for arbitrary shapes. This approach can be applied
to any object modelled as a composite of primitives (see Chapter 4) by applying the
appropriate planner to a suitable primitive component. Finally, this implementation
does not consider collisions with surrounding obstacles when planning a grasp.
Referring to the coordinate frames deﬁned in Figure 6.1 (page 119), the purpose
of grasp planning is to determine the desired pose of the object (the grasp frame G) in
the frame of the end-effector, represented by the transformation EHG. The planners
use either a precision or power grasp depending on the geometry of the component. A
power grasp provides the greatest stability since the surface is contacted at multiple
points along the ﬁngers. However, a precision grasp (which makes contact only at
the ﬁngertips) is sometimes necessary when the ﬁngers cannot wrap entirely around
the object2. A power or precision grasp is generated by placing the origin of the
grasp frame at the appropriate location within the plane of the ﬁngers, represented
by Gpw and Gpr and shown in Figure 7.3. Details of the grasp planning calculations
to generate EHG for an arbitrary box and cup can be found in Appendix F, and are
based on the grasp stability principles established in [37,140].
Finally, trajectory planning generates a series of set-points to guide the end-
effector from an arbitrary initial pose to the planned grasp. For high DOF redundant
mechanisms such as a humanoid robot, trajectory planning in the presence of obsta-
cles is a complex problem. Kuffner and LaValle [92] recently presented a promising
solution based on Rapidly-exploring Random Trees. This algorithm searches through
high dimensional conﬁguration space using a process of random diffusion to plan
manipulations in complex environments. To avoid these complexities, the experi-
ments in this chapter are contrived to eliminate the problem of obstacle avoidance.
The simpliﬁed task planner generates only a single intermediate set-point between
the initial pose and planned grasp, as shown in Figure 7.4. This set-point ensures
2In general, a precision grasp facilitates dextrous manipulation, while a power grasp pro-
vides greater stability.

152
7 System Integration and Experimental Results
Gpr
Gpw
Fig. 7.3. Placement of grasp frame for power (Gpw) and precision (Gpr) grasps.
intermediate
planned grasp
initial
pose
set-point
Fig. 7.4. Simpliﬁed grasp and trajectory planning for a box.
the ﬁngers do not collide with the object as the gripper approaches the ﬁnal planned
grasp. Details of the intermediate set-point calculations are also provided in Appen-
dix F.
7.4 Experiment 1: Grasping an Unknown Object
In this ﬁrst experiment, Metalman is given the task of retrieving a speciﬁc object
from a cluttered scene. However, the object is only speciﬁed by a generic class (a
box) and particular feature (yellow colour). This task could have been conveyed to
the robot using the verbal command: “Metalman, please fetch the yellow box”. No
other information about the scene (such as the number and type of objects) is known,
and the end-effector is initially outside the ﬁeld of view. The initial pose of Metalman
and the arrangement of objects on the table are shown in Figure 7.5, with the target
object farthest to the left of the robot. It will be assumed that the Puma base frame
location and light stripe scanner have already been actively calibrated as described in
Chapters 6 and 3. To complete this task, Metalman must identify an object satisfying
the speciﬁcations and perform a grasp based on visual measurements of both the
object and end-effector.

7.4 Experiment 1: Grasping an Unknown Object
153
Fig. 7.5. Arrangement of objects for Experiment 1. Metalman is required to fetch the yellow
box located farthest to the left of the robot. Reprinted from [150]. c2004, IEEE. Used with
permission.
7.4.1 Implementation
While object retrieval is a relatively straightforward operation, all of control and
perception components developed in this book must be employed to achieve this
goal. The steps to complete the task are described below:
Initialization
In the current implementation, the initial gaze of the robot is manually directed to-
wards the table by interactively driving the Biclops head. For practical domestic ro-
bots, a more useful approach would combine visual perception with voice commands
and a map of the local environment, as in the example: “Metalman, please fetch the
yellow box from the kitchen bench”. After manually establishing the initial pose, the
remainder of the task is entirely autonomous.
Target Identiﬁcation
To identify the speciﬁed object, the stereoscopic light stripe scanner ﬁrst captures a
3D colour/range map of the scene as described in Chapter 3. The stripe is scanned in
both the forward and reverse directions to ensure a good coverage of range measure-
ments. The colour/range map is then segmented using the object classiﬁcation algo-
rithm in Chapter 4. The result is a list of convex objects and associated 3D polygonal
models, with each object classiﬁed as either box, ball, cup or bowl. Metalman is re-
quired to select the object that best matches the description of a “yellow box”, which
is achieved by consulting the texture information for all detected boxes. The number
of texture pixels (texels) within manually predeﬁned ranges of hue, saturation and
intensity for yellow are tallied, and the box with the highest number is selected as
the desired target.

154
7 System Integration and Experimental Results
Grasp Planning and Execution
Using the planning processes described in Section 7.3, a ﬁnal grasp and intermedi-
ate set-point are calculated for the target box. Finally, these are passed to the hy-
brid visual servo controller described in Chapter 6. Since the end-effector is initially
outside the ﬁeld of view, servoing commences using only kinematic measurements
(EOL control). As the hand nears the object, visual measurements become available
and the controller makes a transition to fused kinematic and visual measurements.
To close the visual feedback loop, the pose of the object is continuously updated us-
ing the multi-cue tracking algorithm described in Chapter 5. The target is tracked at
a reduced rate of 2 Hz, since the high computational expense of multi-cue tracking
degrades the performance of the visual servoing at higher sample rates. Fortunately,
a reduced rate is sufﬁcient for static objects since tracking is primarily employed
to overcome the pose bias due to calibration errors in ego-motion. Conversely, the
moving gripper is tracked at full frame-rate.
The task is completed when the gripper reaches the planned pose and the object
is grasped and lifted from the table. In the absence of tactile sensors, the joint angle
of the ﬁngers is monitored as the gripper is closed to determine when the object is
stably grasped. The stability of the grasp could also be evaluated by visually tracking
the object as it is lifted to detect slippage, although this technique is not used in the
current implementation.
7.4.2 Results
Figure 7.6 shows two views of the unprocessed light stripe during the initial ac-
quisition of range/colour measurements. Although this scene appears to pose sig-
niﬁcantly less challenge than the mirrored surface considered in Section 3.5.1, the
circled regions of Figure 7.6 demonstrate how strong reﬂections can still arise from
smooth cardboard and plastic to interfere with range data acquisition. Despite these
distractions, the stereoscopic scanner produces a dense, accurate colour/range map
as shown in Figure 7.7(a). Figure 7.7(b) shows the result of segmenting the range
map into geometric primitives. The wireframe models for detected convex objects
are overlaid on a captured frame in Figure 7.7(c), and the textured models are ren-
dered together in Figure 7.7(d). VRML models of the raw colour/range data and
extracted objects can be found in the Multimedia Extensions. Range data acquisition
and segmentation are completed in a total time of about one minute.
Segmentation identiﬁes two possible boxes: an orange box at the top right and a
yellow box at the left, the latter being the desired target. The texture information from
the extracted models is analysed to identify the yellow box as shown by the colour
charts in Figure 7.8. The hue and saturation (intensity not shown) of texels are plotted
as black points, and the predeﬁned colour range for bright yellow is indicated by the
black rectangle. The colour of the two boxes is clearly distinguished by the number
of texels within the yellow range, and the desired box is correctly and unambiguously
identiﬁed.

7.5 Experiment 2: Pouring Task
155
(a) Reﬂection from yellow box.
(b) Reﬂection from plastic funnel.
Fig. 7.6. Reﬂections (circled) during light stripe scanning in a simple domestic scene.
Figure 7.9 shows selected frames from the right camera during execution of the
grasping task (see also Multimedia Extensions). Figure 7.9(a) shows the initial view,
overlaid with tracking cues and the estimated pose of the box. Since the gripper is
initially outside the ﬁeld of view, EOL visual servoing commences using only kine-
matic measurements. Figure 7.9(b) shows the last stages of EOL control just before
the end-effector fully enters the visual ﬁeld. The large pose bias between the esti-
mated pose (indicated by the wireframe overlay) and the actual pose of the gripper
during EOL control in Figure 7.9(b)) is due to poor calibration of the camera to robot
base transformation. This bias is overcome in Figure 7.9(c) after the gripper enters
the ﬁeld of view and visual measurements can be fused with kinematic measure-
ments in the visual servo controller. At this stage, the end-effector has reached the
intermediate set-point (noting that the grasp frame and target frame are aligned over
the box) on approach to the planned grasp. Figure 7.9(d) shows the end-effector at
the planned pose for grasping, and 7.9(e) shows the ﬁngers in stable contact with the
target object. Finally, the box is lifted in Figure 7.9(f) and the task is complete.
7.5 Experiment 2: Pouring Task
In this experiment, Metalman is required to grasp an interactively selected cup (ﬁlled
with rice) and pour the contents into a bowl. The experimental arrangement for this
task is shown in Figure 7.10. As before, no prior knowledge of the number and type
of visible objects is assumed by the system. This experiment simulates the type of in-
teraction that might be necessary when ambiguous or incomplete task speciﬁcations
are encountered. For example, Metalman is given the command “Please pour the cup
of rice into the bowl” and, after scanning the scene, ﬁnds that the supplied parame-
ters do not sufﬁciently constrain the target. The robot consequently responds with:
“Please indicate which cup I should use.” In a practical application, the user may
employ verbal or gestural interaction to provide additional parameters. The imple-

156
7 System Integration and Experimental Results
(a) Raw colour/range scan.
(b) Segmentation result.
(c) Extracted objects (wireframes).
(d) Rendered textured 3D models.
Fig. 7.7. Scene analysis for grasping task (see also Multimedia Extensions).
saturation
hue
(a) Yellow box
saturation
hue
(b) Orange box
Fig. 7.8. Colour charts for identifying the yellow box. Texture pixels are plotted in black, and
hue and saturation ranges (intensity not shown) for yellow are indicated by the rectangle.
mentation in this chapter employs a GUI, which may be applicable for tele-operated
domestic robots (for example, see [146]).

7.5 Experiment 2: Pouring Task
157
(a) Tracking yellow box
(b) EOL servoing before gripper enters view
(c) Visual servoing to ﬁrst set-point
(d) Planned grasp, before closing ﬁngers
(e) Grasp closure
(f) Final pose at task completion
Fig. 7.9. Selected frames from grasping task (see Multimedia Extensions for a video of the
complete experiment).
7.5.1 Implementation
The sense-plan-action cycle for this task resembles that of the previous experiment,
and the details described in Section 7.4.1 also apply here. However, this experiment
introduces interaction and more complex motion planning to pour the contents of the
cup into the bowl, which are outlined below:

158
7 System Integration and Experimental Results
Fig. 7.10. Arrangement of objects for Experiment 2. Metalman is required to grasp one of the
cups (selected interactively) and pour the contents into the bowl.
Interaction
Interaction is facilitated through a point-and-click GUI. Light stripe scanning and
range data segmentation produce a list of cylindrical/conical objects, and the target
bowl is detected as the object with the largest radius. The centroid of each remaining
cup is projected onto the left image plane and rendered as a button over the possible
target. The robot then waits for the user to select a cup by clicking on the appropriate
button, and the selected object is tracked and grasped as described in Section 7.4.1.
Pouring Motion
The pouring motion is implemented as a set of predetermined set-points relative to
the tracked position of the bowl and measured height of the cup. After lifting the cup,
the ﬁrst set-point centres the cup directly above the bowl, at an elevation which places
the bottom of the cup slightly above the rim. The gripper is then rotated through 120
degrees and simultaneously lowered towards the bowl. The bowl is tracked at a rate
of 2 Hz during the pouring motion to ensure accurate placement of the cup. However,
the cup ceases to be tracked after the initial grasp (to reduce computational expense)
and is simply assumed to maintain a stable pose relative to the gripper.
7.5.2 Results
Figure 7.11 shows the result of light stripe scanning and scene analysis for the pour-
ing experiment. As in the previous experiment, secondary reﬂections caused by the
smooth surfaces of the objects were readily rejected by the robust stereoscopic scan-
ner, resulting in the colour/range scan in Figure 7.11(a). The segmented range map
is shown in Figure 7.11(b). Convex objects extracted from the scene are overlaid as
wireframe models in Figure 7.11(c) and rendered together in Figure 7.11(d). VRML
models of the raw colour/range scan and extracted objects can be found in the Mul-
timedia extensions. Again, these results highlight the accuracy and reliability of the

7.5 Experiment 2: Pouring Task
159
(a) Raw colour/range scan.
(b) Segmentation result.
(c) Extracted objects (wireframes).
(d) Rendered textured 3D models.
Fig. 7.11. Scene analysis for pouring task (see also Multimedia Extensions).
Fig. 7.12. Graphical interface for interaction: cup is selected by clicking on a rectangle.
proposed scanning/segmentation framework for modelling simple objects in unstruc-
tured scenes. However, segmentation required signiﬁcantly greater processing time
than the previous experiment (about one minute, in addition to the 30 seconds for

160
7 System Integration and Experimental Results
(a) Tracking cup, EOL servoing
(b) Hybrid visual servoing after gripper
enters view
(c) Planned pose for grasping cup
(d) Cup grasped, tracking bowl
(e) Alignment of cup above bowl
(f) Successful completion of the task
Fig. 7.13. Selected frames from pouring task (see Multimedia Extensions for a video of the
complete experiment).
range data acquisition) due to the large number of relatively expensive conical sur-
faces.
Scene analysis initially extracts three cylindrical/conical objects, and correctly
identiﬁes the bowl as having the largest radius. Figure 7.12 shows the GUI presented
to the user to select from the remaining two objects. The target cup is selected by

7.6 Experiment 3: Multi-sensor Synergy
161
Fig. 7.14. End-effector tracking loss due to background distraction (orange box).
clicking on the appropriate rectangle, in this case the cup marked “RICE” on the
far left. It is also important to note from Figure 7.12 that the system has correctly
discarded the box as a possible object of interest.
Selected frames from the right camera during execution of the task are shown in
Figure 7.13 (see also Multimedia Extensions). Figure 7.13(a) shows the initial view;
as in the previous experiment, the end-effector is initially unobservable, so visual
servoing must commence using only kinematic feedback and the tracked pose of the
cup. Figure 7.13(b) shows the captured image just after the end-effector fully en-
ters the visual ﬁeld and the controller initiates visual servoing using fused visual and
kinematic measurements. The end-effector reaches the planned pose for grasping in
Figure 7.13(c), and Figure 7.13(d) shows the cup grasped and lifted. At this point,
tracking is terminated and the cup is assumed to maintain a stable pose relative to
the end-effector. In any case, the signiﬁcant deformation suffered by the cup dur-
ing grasping would hinder model-based tracking. Tracking is initiated for the bowl,
which is still only partially visible in Figure 7.13(d). Figure 7.13(e) shows the align-
ment of the cup above the bowl prior to pouring. Finally, the task is successfully
completed in Figure 7.13(f), with the rice poured from the cup to the bowl.
After completing the task, Figure 7.14 shows that the system actually lost track
of the gripper. This was the result of two compounding effects: only two LEDs were
visible in the ﬁnal pose, while the distracting orange box in the background gave
the appearance of a third LED. However, it is important to note that bowl track-
ing was successful throughout the pouring manoeuvre, despite a poorly contrasting
background and signiﬁcant occlusions from the gripper and cup during servoing.
7.6 Experiment 3: Multi-sensor Synergy
Vision is usually considered one of the primary human senses, but it is easy to over-
look the important role played by our other senses in accomplishing many simple
tasks. For example, tactile and odour sensing also provide useful information for
classifying objects that simply cannot be recovered from vision alone (consider the

162
7 System Integration and Experimental Results
airﬂow whisker
sensor array
tin oxide
gas sensor
Fig. 7.15. Addition of airﬂow and odour sensors to Metalman for experiment 3.
difference between good and bad eggs!). Service robots of the future are likely to
have a rich set of complementary sensing modalities going well beyond vision, which
will enable them to maintain an increasingly complex world model and make better
informed decisions.
This ﬁnal experiment takes the ﬁrst steps towards integrating vision with comple-
mentary sensors to complete a task that cannot be solved by vision alone: Metalman
must locate and grasp a cup containing ethanol from among several objects on a
table, including other cups. Ethanol is used in this case due to its high volatility,
although one can imagine the analogous domestic task of distinguishing between a
cup of coffee and a cup of tea. To detect the concentration of ethanol vapour, Metal-
man is equipped an electronic nose based on a tin oxide gas sensor. To help locate
the source of the chemical plume, Metalman is also equipped with an airﬂow sensor
capable of measuring both air speed and direction. Environmental airﬂow is gener-
ated by a domestic cooling fan driven from a variable transformer. Figure 7.15 shows
the additional sensors mounted on Metalman. This collaborative work ﬁrst appeared
in [133].
7.6.1 Implementation
To accomplish this task, Metalman ﬁrst analyses the scene using the light stripe sen-
sor and object classiﬁcation framework developed in Chapters 3 and 4 to locate all
the cups on the table. The decision process is then triggered automatically when
Metalman ﬁrst detects the presence of ethanol, ie. when ethanol is poured onto one
of the cups. The likelihood that each cup contains ethanol is evaluated over a period
of time based on a recurrent stochastic process model, which takes air speed and di-
rection, chemical concentration and the location of the cup into consideration. After

7.6 Experiment 3: Multi-sensor Synergy
163
optical
switch
strip
plastic
35 mm
Fig. 7.16. Whisker sensor element for measuring airﬂow.
a ﬁxed decision period, Metalman selects and grasps the cup with the highest likeli-
hood of containing ethanol. Grasp planning and visual servoing are performed in the
same manner as the previous experiments. However, object tracking was not used
in this case as the workspace was limited and the camera experiences little motion.
Once grasped, the presence of ethanol is conﬁrmed by bringing the cup closer to the
nose and detecting the associated increase in chemical concentration.
The following sections describe the operation of the airﬂow and chemical sen-
sors, and the integration of sensor measurements in a recurrent stochastic model used
to evaluate the likelihood of the presence of ethanol.
Airﬂow Sensor
The airﬂow sensor was developed by Russell and Purnamadjaja [132] and is based on
the biologically inspired idea that airﬂow can be sensed as the disturbance of the ﬁne
hairs covering the skin. Based on this principle, Metalman uses an array of whisker
sensors to measure both air speed and direction. Each sensor element is composed
of a plastic ﬁlm whisker and an optical switch, as shown in Figure 7.16. With careful
alignment, the switch is capable of registering a 0.04 mm deﬂection in the tip of the
whisker. Eight whiskers are arranged in a halo above the Biclops head (see Figure
7.15), and a microcontroller registers the number of vibrations from each sensor in a
ﬁxed period of time. The eight measurements form an airﬂow feature vector.
As mentioned above, a variable speed airﬂow is directed towards Metalman us-
ing a domestic cooling fan running from a variable transformer. The circulation of
air around the whisker array depends critically on the location of both the fan and
any obstacles deﬂecting the airﬂow. A simple learning procedure must be conducted
to enable Metalman to predict the speed and direction of airﬂow in a given environ-
ment. The fan is moved to several known locations around the robot, and whisker
measurements are recorded at each location for both high and low-speed airﬂow to
form a set of training vectors. When Metalman is presented with an unknown airﬂow
in the same environment, the whisker measurements are correlated with the training
set and the highest result gives both the approximate direction and speed of airﬂow.

164
7 System Integration and Experimental Results
680
690
700
710
720
730
740
750
760
0
10
20
30
40
50
60
Chemical reading
Sample
0 degrees
45 degrees
90 degrees
Fig. 7.17. Chemical sensor response to a mug of ethanol placed at different bearings to the
direction of airﬂow.
θ = 0◦
θ = 45◦
θ = 90◦
chemical
wind
direction
sensor
θ
mug
Fig. 7.18. Deﬁnition of bearing angle θ of mug with respect to wind direction.
Odour Sensor
Electronic noses are typically constructed as an array of chemical sensors, each sen-
sitive to a broad but distinct range of chemicals (much like the different cones in
the human retina that distinguish different colours). The set of responses to a par-
ticular chemical form a feature vector and classical pattern recognition techniques
(commonly neural networks) can be applied to classify odours. Since classiﬁcation
is not required in this experiment, a single sensor is sufﬁcient to detect the presence
of ethanol. Metalman uses a Figaro tin oxide gas sensor, which presents a varying
resistance as a non-linear function of chemical concentration.
Figure 7.17 shows the response of Metalman’s odour sensor to a mug containing
a teaspoon of ethanol. A slow airﬂow was aimed directly towards Metalman and
the mug was placed at different bearings around the sensor. Figure 7.18 deﬁnes the
bearing angle θ of the mug with respect to the direction of the wind. The initial
transient in the response is due to the establishment of the stable down-wind chemical

7.6 Experiment 3: Multi-sensor Synergy
165
plume from the mug. The plot suggests that the location of the cup with respect to
the direction of airﬂow is best characterized by the peak-to-peak variation in sensor
measurements (after the initial transient), which in tun is determined by the location
of the sensor within the down-wind plume.
Sensor Coordination
The measurements from vision, airﬂow and odour sensing arise from distinctly dif-
ferent physical processes and are orthogonal in the sense that measurements from one
sensor cannot be recovered from any other. To integrate the sensors in a meaningful
way, we construct a probabilistic framework in which measurements provide evi-
dence supporting or disclaiming the hypothesis that a detected cup contains ethanol.
A likelihood function is maintained for each cup to evaluate the support for this
hypothesis. When asked to grasp the cup containing ethanol, Metalman naturally
chooses the cup with the highest likelihood.
The likelihood function is based on the following observations. If no airﬂow is
detected, the likelihood that a particular cup contains ethanol asymptotes towards the
a priori likelihood (set arbitrarily to 0.5) since there is no evidence to suggest oth-
erwise. Detecting a strong odour in the presence of airﬂow increases the likelihood
that a cup upwind from the sensor (at zero bearing) contains ethanol. Conversely,
detecting a weak odour decreases the likelihood that a cup upwind contains ethanol,
since we expect a strong odour. The contents of a cup at 90 degrees bearing cannot
be deduced (regardless of detected odour strength) since the chemical plume never
reaches the sensor. Thus, the likelihood of a cup at 90 degrees always remains at the
a priori likelihood. Cups at other bearings vary between these extreme cases.
To remove the effect of sensor noise and produce a time averaged result, the
likelihood function is formulated as a ﬁrst order, recurrent stochastic process. Let
Li
n represent the likelihood of the ith cup at the nth time step, θ i represent the angle
between the direction of the cup and the airﬂow with respect to the odour sensor,
Ai represent the a priori likelihood that a cup contains ethanol and m model the
time response of the likelihood function (for low-pass ﬁltering). Then, based on the
observations above, we can write the likelihood update equation as:
Li
n =



Li
n−1 +(Ai −Ai cosθ i −Li
n−1)/m
if low odour, airﬂow
Li
n−1 +(Ai −Li
n−1)/m
if med. odour, airﬂow or no airﬂow
Li
n−1 +(Ai +(1−Ai)cosθ i −Li
n−1)/m if high odour, airﬂow
(7.1)
7.6.2 Results
Figure 7.19(a) shows the arrangement of objects, namely two cups and a tennis ball,
and the initial conﬁguration of Metalman for this experiment. The cooling fan is
placed directly in front of Metalman so that the rightmost cup is almost directly up-
wind while the leftmost cup is away from the up-wind direction. Each trial began

166
7 System Integration and Experimental Results
(a) Ethanol poured into the left mug, away
from up-wind direction
(b) Classiﬁed objects: two cups and a tennis
ball (wireframe overlay)
Fig. 7.19. Experimental arrangement for experiment 3.
with a scan of the scene to recognize and locate the cups, as indicated by the wire-
frame models overlaid on Figure 7.19(b). The left and right cups are measured at
bearings of 36 and 6 degrees respectively relative to the wind direction, while the
ball is discarded as a possible source of ethanol. Ethanol was then poured into one of
the cups to trigger the decision process prior to grasping. The experiment was con-
ducted twice to test Metalman’s ability to detect ethanol in each cup (see Multimedia
Extensions for a video of the complete experiment).
For the ﬁrst trial, ethanol was poured into the leftmost cup away from the airﬂow
(see Figure 7.19(a)). Figure 7.20 shows the reading from the chemical sensor and the
evolution of the likelihood functions for each cup. After the initial rise in chemical
concentration above a ﬁxed threshold at 55 seconds from the start of the trial, the sys-
tem waits an additional minute while a stable plume is established. At 120 seconds,
two likelihood functions are initialized and updated with each new odour and airﬂow
reading. The small peak-to-peak variation in chemical concentration suggests that
very little ethanol is present, and the likelihood functions for both cups drop below
the a priori level of 0.5. The likelihood for the right cup drops furthest, since we
would expect the strongest response if this upwind cup contained ethanol.
The likelihoods are evolved for a ﬁxed period of 100 seconds, and the cup with
the greatest likelihood, in this case the left cup, is selected for grasping. Figure 7.21
shows the left cup successfully grasped and brought closer to the odour sensor. Con-
ﬁrmation that the correct cup was selected is established by the large jump in the
chemical sensor reading at about 270 seconds in Figure 7.20.
Figure 7.22(a) show the ethanol being poured into the right cup for the second
trial in this experiment, and Figure 7.23 shows the chemical sensor reading and evo-
lution of the likelihood functions. As before, the system waits for a minute after the
initial rise in chemical concentration (at 40 seconds) before evolving the likelihood
functions for each cup. In this case, the chemical readings show a high peak-to-peak
variation indicating the presence of an upwind ethanol source. The likelihoods cor-

7.6 Experiment 3: Multi-sensor Synergy
167
0
0.2
0.4
0.6
0.8
1
0
50
100
150
200
250
300
Likelihood
Time (sec)
rightmost cup
0
0.2
0.4
0.6
0.8
1
0
50
100
150
200
250
300
Likelihood
Time (sec)
rightmost cup
leftmost cup
680
700
720
740
760
780
800
Chemical reading
chemical
detected
start
likelihood
update
grasp
cup
Fig. 7.20. Chemical sensor readings and evolution of likelihood function for both cups (ethanol
in the left-hand cup).
(a) Visual servoing to planned grasp
(b) Correct cup grasped and lifted towards
nose
Fig. 7.21. Successful completion of the detection and grasping task for left-hand cup.
respondingly rise above the a priori value of 0.5. After 100 seconds of evolution, the
right cup is selected as the most likely to contain ethanol, being directly upwind. Fig-
ure 7.22 shows the cup successfully grasped and brought towards the nose. Again the

168
7 System Integration and Experimental Results
(a) Ethanol poured into the right mug,
directly upwind
(b) Correct cup grasped and lifted towards
nose
Fig. 7.22. Successful completion of the grasping task with ethanol in the rightmost cup.
0
0.2
0.4
0.6
0.8
1
0
50
100
150
200
250
Likelihood
Time (sec)
rightmost cup
0
0.2
0.4
0.6
0.8
1
0
50
100
150
200
250
Likelihood
Time (sec)
rightmost cup
leftmost cup
680
700
720
740
760
780
800
820
840
Chemical reading
chemical
detected
start
likelihood
update
grasp
cup
Fig. 7.23. Chemical sensor readings and evolution of likelihood function for both cups (ethanol
in the rightmost cup).
presence of ethanol in the cup is conﬁrmed by the jump in chemical concentration at
about 250 seconds in Figure 7.23.

7.7 Discussion and Conclusions
169
7.7 Discussion and Conclusions
In this chapter, three real-world tasks were experimentally implemented and tested
to evaluate the performance of the proposed framework for visual perception and
control for a hand-eye system. The tasks were based on supervisory commands and
involved classifying and manipulating objects with which the robot had no prior ex-
perience. All of the methods developed in the preceding chapters, including stereo
light stripe sensing, object modelling and classiﬁcation, multi-cue tracking and hy-
brid position-based visual servoing were integrated in the implementation. Although
the tasks were contrived to simplify scene analysis and avoid complex planning, their
successful completion demonstrates the effectiveness of this framework for perform-
ing simple real-world tasks with a hand-eye system on a service robot.
The range scanning, segmentation and object modelling results were successful
over many trials of each experiment. Importantly, signiﬁcant secondary reﬂections
of the laser stripe did not prevent the robust stereoscopic light stripe scanner from
producing accurate, dense colour/range data. However, gripper tracking was occa-
sionally distracted by background features despite the use of active cues and a global
association algorithm, as demonstrated at the end of the second experiment. This
problem was mainly compounded by the use of red LED markers, combined with
the red light stripe scanner that requires all target objects in the scene to have some
red component of colour. A possible solution to this problem may be the use of mul-
tiple coloured LEDs, which vary in colour across the gripper and between frames to
greatly reduce the likelihood of a sustained distraction in the background. Alterna-
tively, the robustness of multi-cue tracking in Chapter 5 suggests that a integration
of different cues could improve gripper tracking. For example, the gripper could be
augmented with distinctive line and texture markings in addition to multi-coloured
LEDs.
In Chapter 1, real-time operation was noted as an important aspect of practi-
cal service robots. The execution time for the tasks presented in this chapter were
approximately 80 s for the ﬁrst experiment and 130 s for the second. The greatest
proportion of time was consumed by light stripe scanning (25 s for bi-directional
scans) and range data segmentation (40-60 s depending on the complexity of the
scene). This is signiﬁcantly longer than the time required for a human to perform
a similar analysis. However, the current implementation is within an order of mag-
nitude of what might be considered acceptable for a practical domestic robot (ie. a
few seconds). With further optimization and high-speed hardware (such as CMOS
cameras and digital signal processors), it is likely that the proposed methods could
be realized in real-time using current technology.
Another aspect of real-time performance is the speed at which the arms are
moved. The visual servo controller employed here has a deliberately low gain to
avoid instabilities that arise from dynamic effects such as processing and actuation
delays (see Appendix E). Compensation for these effects by developing more so-
phisticated visual servoing control laws is an active area of research (see, for exam-
ple [27]), and will need to be solved before the robot can perform with the ﬂexibility

170
7 System Integration and Experimental Results
and speed for a human. In these experiments, the time required to move the gripper
towards a planned grasp over a distance of 40 cm was approximately 10 s.
The above experiments avoided complex scene analysis and task planning by
conveniently arranging simple objects. These concessions were also necessitated by
the limitations of the experimental platform. In particular, the parallel-jaw conﬁg-
uration of the gripper eliminates the possibility of dextrous manipulation and re-
duces the range of objects that can be grasped. Furthermore, the kinematics of the
Puma arm and lack of articulation about the waist of the robot signiﬁcantly limits the
available workspace. Since the kinematics were not explicitly considered in trajec-
tory planning or control, careful arrangement of the objects was necessary to avoid
joint limits. Clearly, more sophisticated planning (incorporating kinematics) must be
adopted for a practical service robot.
Finally, the important problem of grasp veriﬁcation was avoided for simplicity.
Stability analysis could have been implemented using tracking to ensure the grasped
object remains ﬁxed in the hand. Similarly, tracking could be used to verify that ob-
jects reach their planned pose after each manoeuvre. If any component of the task
fails, the system should be capable of planning steps to recover or alternatively re-
porting the mode of failure to the user. The above issues provide a starting point for
future research to improve the performance and ﬂexibility of hand-eye systems on
practical service robots.

8
Summary and Future Work
8.1 Answering the Challenge of Service Robotics
Service robotics is currently a very active area of research, with signiﬁcant progress
being made in areas such as locomotion, interaction, learning and manipulation. The
interest in service robots stems from the pressing needs of an aging society and di-
minishing workforce in many developed countries. Consequently, this book has fo-
cussed on developing manipulation skills that would be useful in applications such
as aged care and construction work. Many of the important characteristics of practi-
cal service applications are largely neglected in current robotics research, including
robust sensing in an unpredictable environment, tolerance to calibration errors, and
minimal reliance on prior knowledge for tasks such as object recognition. Solving
these problems contributes to the requirements of both high reliability and low cost
that will drive the acceptance and success of service robots.
This book has developed a perception and control framework based on visual
sensing to perform practical manipulation tasks with unknown objects in a domes-
tic environment. Robustness to noise, interference and calibration errors at all stages
from sensing through to actuation is a major theme of this framework. At the start of a
new task, the robot acquires a dense 3D map of the workspace using the stereoscopic
light stripe scanner described in Chapter 3. This sensor is capable of acquiring dense,
accurate range data despite reﬂections, cross-talk or other sources of interference.
Unknown objects are modelled as collections of data-driven geometric primitives
(planes, cylinders, cones and spheres) ﬁtted to the range data using the algorithms
described in Chapter 4. This process relies heavily on a new algorithm to robustly
classify the shape of local range patches in the presence of noise. The internal world
model, consisting of textured polygonal models of extracted objects, is updated using
the multi-cue 3D model-based tracking algorithm developed Chapter 5. Multi-cue in-
tegration allows objects to be tracked over a wide range of visual conditions, includ-
ing clutter, partial occlusions, lighting variations and low contrasting backgrounds.
Finally, the hybrid position-based visual servoing framework described in Chapter 6
was developed to control the end-effector with high tolerance to calibration errors,
occlusions, and other distractions. Chapter 7 demonstrated the successful applica-
G. Taylor and L. Kleeman: Visual Perception and Robotic Manipulation, STAR 26, pp. 171–175, 2006.
© Springer-Verlag Berlin Heidelberg 2006

172
8 Summary and Future Work
tion of this above framework to several simple tasks involving unknown objects in a
domestic setting.
Apart from forming the building blocks of a practical service robot, the tech-
niques developed in this book have a wide range of applications outside robotics.
As a prime example, light stripe scanners have been used in diverse ﬁelds including
industrial inspection, CAD modelling, historical preservation and medical diagno-
sis. Robust light stripe sensing opens new possibilities such as capturing data “in
the ﬁeld”, and capturing both colour and range while eliminating the need to pre-
pare objects with matte white paint. Visual tracking is another component with a di-
verse range of applications including surveillance, sports and entertainment, human-
machine interfaces and video compression. Multi-cue tracking has the potential to
improve performance in all of these areas.
In Chapter 1, four main challenges associated with visual perception and control
in a domestic environment were presented as the motivation for this research. It is
therefore prudent to conclude this book by revisiting these challenges and summa-
rizing how each has been addressed:
Imprecise task speciﬁcations and lack of prior knowledge
The experiments in Chapter 7 exempliﬁed how service tasks are typically speciﬁed
at a supervisory level, involving imprecisely described objects and manipulations.
To further illustrate this point, these tasks were only speciﬁed by simple commands
such as: “Please pass the yellow box” and “Please pour the cup of rice into the bowl.”
The robot was required to perform the tasks without necessarily having any prior
experience with the objects involved, or any further clue regarding their location and
identity. This lack of precise knowledge inﬂuenced all stages of sensing, planning
and actuation in the framework developed in this book.
In an unpredictable environment with unknown objects, visual sensing cannot
rely on the presence of suitable lighting conditions or particular visual cues. For low
level sensing, active light stripe ranging was thus chosen over passive stereo, since
the performance of the latter depends critically on the contents of the scene. Recog-
nizing meaningful objects in the range data must be performed without precise mod-
els for objects present in the scene. This problem is addressed by modelling classes
of objects as compound geometric primitives. Unknown objects are recognized by
matching these models to data-driven geometric primitives segmented from the range
data. Finally, object tracking cannot rely on the presence of intensity edges or unique
textures and colours to distinguish objects, which causes conventional single-cue
trackers to fail in the long term. Multi-cue tracking directly addresses this problem
and thus caters for a wide range of targets and visual conditions.
The second experiment in Chapter 7 illustrated how imprecise task speciﬁcations
might prevent a task from being successful executed. The cause in this case was the
detection of two objects satisfying the task speciﬁcations. To successfully resolve this
ambiguity, the robot asked for assistance using the current world model to compose a
suitable query. This example illustrates the importance of human-machine interaction
in robustly carrying out complex, cooperative domestic tasks.

8.1 Answering the Challenge of Service Robotics
173
Robust visual sensing in a cluttered environment
Data association, the task of identifying features in sensor data, is particularly prob-
lematic in a cluttered, dynamic and unpredictable domestic environment. This issue
drove the development of the robust stereoscopic light stripe scanner described in
Chapter 3. Conventional scanners rely on the brightness of the stripe exceeding all
other features in the scene to solve the data association problem. This assumption is
clearly violated under arbitrary lighting conditions and with the possibility of reﬂec-
tions and cross-talk. The stereoscopic light stripe scanner identiﬁes the true stripe in
the presence of interference by validating measurements using constraints provided
by redundancy.
Object and gripper tracking suffer similar association problems, and two differ-
ent solutions were demonstrated. Two critical techniques to improve object tracking
are the use of multiple cues and the detection of new cues in each frame. The for-
mer process reduces the signiﬁcance of single association errors, while the latter
ensures that association errors do not persist for more than a single frame. Alter-
natively, gripper tracking is improved through the use of easily detected active cues
and fusion with kinematic feedback. This latter approach fully exploits the additional
constraints that can be imposed by the gripper being controlled by the robot. Fusion
with kinematic measurements also overcomes the problem of losing visual feedback
due to occlusions, which are a signiﬁcant issue in an unpredictable, cluttered en-
vironment. The hybrid position-based controller implements pure EOL control or
fusion of visual and kinematic measurements depending on the availability of visual
information. Servoing is therefore possible over a wide range of conditions.
Robustness to operational wear and calibration errors
Accurate calibration of the sensors and mechanics in a service robot is hindered
by the requirements of low weight for minimal power consumption, cheap sensors,
low maintenance and operation in an unpredictable, hazardous environment. System
parameters are likely to change with operating conditions, wear and accidents. Many
of the techniques in this book were therefore developed to be self-calibrating rather
than relying on accurate manual calibration, to meet the requirements of reliability
and low maintenance.
Chapter 3 described an automatic procedure to calibrate the light stripe range
sensor from the scan of an arbitrary non-planar target, which allows the scanner to
be calibrated during normal operation. In Chapter 6 it was shown that mild verge and
baseline errors in the stereo camera model can be approximated as a scalar transfor-
mation. This was exploited in visual servoing to compensate for camera calibration
errors while tracking the pose of the gripper. The tracking ﬁlter also maintains an
on-line estimate of the location of the robot base to compensate for kinematic cali-
bration errors. Finally, the experimental results in Chapter 5 demonstrated that cal-
ibration errors cause camera motion to bias the estimated position of static objects,
despite ego-motion compensation. Object tracking overcomes this bias to improve
the success rate of grasps and other manipulations.

174
8 Summary and Future Work
Real-time operation
To interact and work cooperatively with humans, service robots must perform tasks
at a similar rate. Chapter 7 demonstrated that the algorithms presented in this book
enable an experimental humanoid robot to perform tasks within the constraints of
some practical applications, although performance is below what may be consid-
ered real-time. The current performance is achieved on a general purpose desktop
PC without specialized signal processing hardware. In the case of light stripe scan-
ning and visual servoing, real-time performance is limited by mechanical constraints
rather than computing power. However, all other components are likely to beneﬁt
from more efﬁcient optimizations, supplementary signal processing hardware and
exploitation of the steady increase in general purpose computing power. The results
of this book certainly suggest that a real-time implementation should be achievable
with current technology.
8.2 A Foundation for the Future
The visual perception and control framework developed throughout this book is by
no means complete. The development of practical service robots will require signif-
icant advancements in a variety of related ﬁelds including sensing, control, human-
machine interaction, learning and artiﬁcial intelligence. We chose to examine the
problem of visual perception and control for robotic grasping as it touches on many
of these areas, or at least highlights the need for further research. To achieve mean-
ingful results within the limited scope of this book, the real-world tasks in Chapter
7 were deliberately contrived to facilitate simple scene understanding, task planning
(including grasp planning and obstacle avoidance) and user interaction, and remove
the need for dextrous manipulation and cooperative manipulation with two (or more)
arms. These missing skills are important components of a useful and ﬂexible service
robot, and remain the subject of intense research elsewhere.
The ﬁnal experiment in Chapter 7 highlighted the effectiveness and utility of
multi-sensor fusion in service applications. The integration of smell and airﬂow sens-
ing with vision allowed the robot to perform a task that would be impossible with vi-
sion alone. One can imagine many other useful sensing modalities on a service robot
platform, both human based such as tactile and vestibular sensing, and non-human
based including sonar and GPS. Clearly, the reliability of grasping and manipulation
would be improved with tactile and force feedback. Tactile sensing can provide in-
formation about otherwise hidden surfaces of an object, and supplement visual grasp
planning with reactive grasp stabilization. Fragile objects, which are difﬁcult to dis-
tinguish visually, could be handled more conﬁdently with the aid of tactile feedback.
Tactile and wrist force sensors would alert the robot to collisions, which could be
used to both supplement obstacle detection and determine when an object has been
placed appropriately.
Just as the sensing capabilities of service robots should not be limited to an-
thropomorphic modalities, neither should the conﬁguration of the sensors be limited

8.2 A Foundation for the Future
175
to the human blueprint. For example, one can imagine many task-speciﬁc cameras
placed all around a robot for specialized functions such as user interaction, localiza-
tion and visual servoing. The eye-in-hand camera conﬁguration is already popular
in the visual servoing, and an interesting future research direction for position-based
visual servoing would be the fusion of both ﬁxed camera and eye-in-hand conﬁgura-
tions for hand-eye coordination.
A great deal of scope exists for the inclusion of learning algorithms to improve
the performance and ﬂexibility of the framework presented in this book. At a basic
level, segmentation and object modelling rely heavily on several thresholds including
patch size and surface ﬁtting tolerances. While these were chosen heuristically for the
experimental implementation in this book, parameter selection is particularly suited
to the application of unsupervised learning. By roughly predicting the contents of
the scene from the commanded task, learning could be conditioned by the consensus
between perception and prediction. Parameter selection could be further reﬁned with
supplemental inputs such as an estimate of the variance of range noise for current
visual conditions, which could also be learned autonomously. As the robot gains
operational experience, the ability to interpret range data could steadily improve in
response to changes in the environment.
At a higher level, object modelling and classiﬁcation is another suitable area for
the application of learning algorithms. The framework presented in chapter 4 classi-
ﬁes generic objects using predeﬁned attributed graphs of geometric primitives. How-
ever, a domestic robot will almost certainly be required to recognize special classes
of objects peculiar to its application. Through interaction and learning, new asso-
ciations could be formed between collections of primitives and meaningful tokens
(textual, vocal or otherwise). Attributed graphs could be created and reﬁned over
time through experience with multiple instances of each new class. This capability
would greatly increase the ﬂexibility of the service robot.
Ultimately, this research presented in this book forms only small component of a
practical, autonomous service robot. In the immediate future, integrating the percep-
tion and control skills developed here with progressive research in verbal/gestural
human-machine interaction would create a system that could already provide practi-
cal assistance in simple domestic tasks. With research continuing in a host of related
areas including locomotion, localization and mapping, dextrous manipulation and
learning, the future of service robots looks promising indeed.

A
Active Stereo Head Calibration
This appendix accompanies the discussion on active vision in Section 2.2.3, and
describes a calibration procedure for the Biclops stereo head. Biclops is a commer-
cially available robotic pan-tilt-verge camera platform that enables the viewpoint of
the cameras to be controlled for active tracking. Biclops provides active vision for
the experimental service robot detailed in Chapter 7 (see Figure 7.1).
When implementing model-based vision on an active vision head, accurate feed-
back of the pan, tilt and verge angles (see Figure 2.5) are important to determine the
current camera projection matrices. The Biclops head reports these quantities in their
measured units of encoder counts. Manual calibration is thus required to transform
these values into meaningful units such as radians or degrees. Fortunately, a linear
relationship exists between encoder count and angular position for the pan and tilt
axes, and the scale factor can be calculated by recording the encoder value at the
limits of motion for each axis and dividing by the factory speciﬁed angular range.
Conversely, the verge mechanism of the Biclops head does not possess such as lin-
ear relationship between encoder counts and angular travel. As illustrated in Figure
A.1, the Biclops verges the cameras from a single motor via a worm gear, nut and
transmission arm with leaf spring hinges. As the nut travels along the worm gear, the
cameras rotate with a symmetrical, non-linear change in verge angle and baseline.
Manufacturer speciﬁcations for these relationships are not provided, but the verge
angle can be calibrated manually using the image-based procedure described below.
The calibration procedure is based on placing a coloured target at varying known
distances from the head and controlling the verge angle to drive the measured colour
centroid to the centre of the left and right image planes. The smooth pursuit control
laws given by equations (2.31)-(2.33), with Lx = (Lx, Ly) and Rx = (Rx, Ry) repre-
senting the measured colour centroid, are used to centre the target at each known dis-
tance di, and the verge encoder value at the completion of each trial is also recorded.
Using a manually measured stereo baseline 2b, the verge angle νi at the ith target
position can be calculated as
νi = tan−1(b/di)
(A.1)
G. Taylor and L. Kleeman: Visual Perception and Robotic Manipulation, STAR 26, pp. 177–178, 2006.
© Springer-Verlag Berlin Heidelberg 2006

178
A Active Stereo Head Calibration
nut
camera
camera
leaf spring
leaf springs
arm and
left
right
motor/encoder
worm gear
Fig. A.1. Verge mechanism for Biclops head (top view).
0
0.05
0.1
0.15
0.2
1500
2000
2500
3000
3500
4000
4500
5000
5500
Verge angle (rad)
Encoder value (count)
manual verge measurements
least-squares calibration curve
Fig. A.2. Calibration of verge angle.
Figure A.2 plots the measured verge angle (from equation (A.1)) against the cor-
responding encoder value, when this procedure was performed on the experimental
Biclops head. Finally, an approximate quadratic calibration curve is calculated using
a least squares ﬁt, as shown by the dashed line in Figure A.2. While this approach
only provides approximate verge and baseline calibration, Chapter 6 shows how the
uncertainty in these parameters can be compensated during hybrid position-based
visual servoing.

B
Light Stripe Validation and Reconstruction
This appendix presents detailed calculations of the theoretical results for the robust
stereoscopic light stripe scanner described in Chapter 3.
B.1 Optimization of Light Plane Error Function
In Section 3.2.3, the following cost function is proposed to determine whether a pair
of stereo measurements, Lx and Rx, correspond to a point X on the light plane:
E = d2(Lx, LPX)+d2(Rx, RPX)
(B.1)
where d(x1,x2) is the Euclidean distance between x1 and x2 and L,RP are the projec-
tion matrices of the stereo cameras. The reconstruction X is constrained to the light
plane Ω by:
ΩX = 0
(B.2)
The minimum error E∗and optimal reconstruction X for a given measurement pair
is determined by the constrained optimization of equation (B.1) with respect to the
constraint in (B.2). As noted, a direct optimization is analytically cumbersome, but
the problem can be reduced to an unconstrained optimization by determining the
direct relationship between projections Lˆx and Rˆx for points on the light plane.
The following formulation is based on the observation that X lies at the intersec-
tion of the light plane with the image ray back-projected through Rˆx (or Lˆx). Pl¨ucker
matrices provide a concise notation for the intersection of planes and lines (see [52]).
If A and B represent the homogeneous vectors of two points on a line, the Pl¨ucker
matrix L describing the line is
L = AB −BA
(B.3)
Then, the intersection X of a plane Ω and the line described by L is simply
X = LΩ = (AB −BA)Ω
(B.4)
G. Taylor and L. Kleeman: Visual Perception and Robotic Manipulation, STAR 26, pp. 179–183, 2006.
© Springer-Verlag Berlin Heidelberg 2006

180
B Light Stripe Validation and Reconstruction
Now, the Pl¨ucker matrix LR for the back-projection of Rˆx can be constructed from
two known points on the ray: CR and RP+Rˆx, where RP+ is the pseudo-inverse of the
camera projection matrix RP, given by equation (2.35). Applying these to equation
(B.3), the Pl¨ucker matrix LR for the back-projection of Rˆx is
LR = CR(RP+Rˆx) −(RP+Rˆx)C
R
(B.5)
The intersection of LR with the laser plane Ω, can now be expressed using equation
(B.4) as:
X = LRΩ = [CR(RP+Rˆx) −(RP+Rˆx)C
R ]Ω
(B.6)
Equation (B.6) is the result quoted in equation (3.9) of Section 3.2.3. Finally, the
left projection Lˆx corresponding to Rˆx is obtained by projecting X, given by equation
(B.6), via LP:
Lˆx = LPX
= LP[CR(RP+Rˆx) −(RP+Rˆx)C
R ]Ω
= LPCR(RP+Rˆx)Ω−LP(RP+Rˆx)C
R Ω
(B.7)
Using the identity (RP+Rˆx)Ω = Ω(RP+Rˆx) (since both sides are scalar), and not-
ing that (C
R Ω) is scalar, the common factors are collected to simplify the above
expression to
Lx = LP(CRΩ)(RP+Rˆx)−LP(C
R Ω)(RP+Rˆx)
=

LP[CRΩ −(C
R Ω)I]RP+
ˆxR
(B.8)
Equation (B.8) is the desired relationship between projections Lˆx and Rˆx of points on
the light plane Ω, and is the result quoted in equation (3.10) of Section 3.2.3. Finally,
the error function in equation (B.1) can be written as
E = d2(Lx,HRˆx)+d2(Rx, Rˆx)
(B.9)
with H = LP[CRΩ −(C
R Ω)I]RP+ and the minimum error can be found by an un-
constrained optimization of equation (B.9) with respect to the projection Rˆx.
B.2 Optimal Reconstruction for Rectilinear Stereo and Pin-Hole
Cameras
Equation (B.8) will now be evaluated for the case of pin-hole cameras in a rectilin-
ear stereo conﬁguration. The camera centres CL,R and projection matrices L,RP for
rectilinear pin-hole cameras are given by (see Section 2.2.3)
CL,R = (b, 0,0, 1)
(B.10)
L,RP =


f 0 0 ±fb
0 f 0
0
0 0 1
0


(B.11)

B.2 Optimal Reconstruction for Rectilinear Stereo and Pin-Hole Cameras
181
where the top sign is taken for L and the bottom sign for R. Substituting RP into
equation (2.35), the pseudo-inverse for the right projection matrix is
RP+ = RP(RPRP)−1
=




1/[f(1+b2)]
0
0
0
1/ f 0
0
0
1
−b/[f(1+b2)]
0
0




(B.12)
Next, the term [CRΩ −(C
R Ω)I] in equation (B.8) evaluates to:
CRΩ −(C
R Ω)I =




−D
bB
bC
bD
0 −(Ab+D)
0
0
0
0
−(Ab+D)
0
A
B
C
−Ab




(B.13)
Finally, multiplying the matrices in equations (B.11), (B.13) and (B.12), equation
(B.8) is evaluated as:
Lˆx =


Ab−D
2Bb
2Cb f
0
−(Ab+D)
0
0
0
−(Ab+D)

Rˆx
(B.14)
which is the result quoted in equation (3.12) in Section 3.2.4. Evaluating equation
(B.14) with Lˆx = (L ˆx, L ˆy, L ˆw) and Rˆx = (R ˆx, R ˆy,1) gives


L ˆx
L ˆy
L ˆw

=


Ab−D
2Bb
2Cbf
0
−(Ab+D)
0
0
0
−(Ab+D)




R ˆx
R ˆy
1


=


(Ab−D)R ˆx+2BbR ˆy+2Cbf
−(Ab+D)R ˆy
−(Ab+D)


(B.15)
Expressed in inhomogeneous coordinates, the relationship between Lˆx and Rˆx is
L ˆx = −(Ab−D)R ˆx+2BbR ˆy+2Cbf
Ab+D
(B.16)
L ˆy = R ˆy
(B.17)
which is the result quoted in equations (3.13)-(3.14) in Section 3.2.4.
Now, with L,R ˆy = y where y = Ly = Ry is the height of the scan-line on which
Lx and Rx are measured (see the discussion in Section 3.2.4), the error function in
equation (B.9) can be evaluated from equations (B.16)-(B.17) as:
E = d2(Lx, Lˆx)+d2(Rx, Rˆx)
= (Lx−L ˆx)2 +(Ly−L ˆy)2 +(Rx−R ˆx)2 +(Ry−R ˆy)2
=

Lx+ Ab−D
Ab+D
R ˆx+
2Bb
Ab+Dy+
2Cb
Ab+D f
2
+
Rx−R ˆx
2
= (Lx+αR ˆx+βy+γ f)2 +(Rx−R ˆx)2
(B.18)

182
B Light Stripe Validation and Reconstruction
where the last line makes the change of variables
α = (Ab−D)/(Ab+D)
(B.19)
β = 2Bb/(Ab+D)
(B.20)
γ = 2Cb/(Ab+D)
(B.21)
Equations (B.18)-(B.21) are the result quoted in equations (3.15)-(3.18) in Section
3.2.4.
Equation (B.18) expresses the image plane error for points on the light plane
as a function of a single variable, R ˆx. Optimization now proceeds using standard
techniques, setting dE
dR ˆx to zero:
dE
dR ˆx = 2α(Lx+αR ˆx+βy+γ f)−2(Rx−R ˆx)
= 2R ˆx(α2 +1)+2[α(Lx+βy+γ f)−Rx]
= 0
Solving for R ˆx gives the optimal projection R ˆx∗:
R ˆx∗=
Rx−α(Lx+βy+γ f)
α2 +1
(B.22)
Substituting equation (B.22) into (B.18) gives the minimum error E∗for the optimal
reconstruction:
E∗=
αRx+(Lx+βy+γ f)
α2 +1
2
+
α2R ˆx+α(Lx+βy+γ f)
α2 +1
2
=
1
(α2 +1)2 (αRx+ Lx+βy+γ f)2 +
α2
(α2 +1)2 (αR ˆx+ Lx+βy+γ f)2
= (Lx+αRx+βy+γ f)2
α2 +1
(B.23)
Equations (B.22) and (B.23) are the results quoted in equations (3.25) and (3.26) in
Section 3.2.4. Finally, the optimal 3D reconstruction X∗is recovered by substituting
Rˆx∗into equation (B.6). The Pl¨ucker matrix LR for the back-projected ray through
Rˆx∗= (R ˆx∗,y) is evaluated by substituting equations (B.10)-(B.12) into (B.5):
LR = CR(RP+Rˆx∗) −(RP+Rˆx∗)C
R
=




0
by/f b −R ˆx∗/ f
−by/f
0
0 −y/ f
−b
0
0
−1
R ˆx∗/ f
y/ f 1
0




(B.24)
Multiplying LR by the plane parameters Ω = (A,B,C,D) and substituting the result
in equation (B.22) gives the optimal reconstruction X∗in homogeneous coordinates:

B.2 Optimal Reconstruction for Rectilinear Stereo and Pin-Hole Cameras
183
X∗= LRΩ
=




Bby/f +Cb−DR ˆx∗/ f
−(Ab+D)y/ f
−(Ab+D)
AR ˆx∗/ f +By/f +C




(B.25)
The x-coordinate of X∗in inhomogeneous notation is calculated by dividing the ﬁrst
row of equation (B.25) by the fourth row:
X∗= Bby+Cb f −DR ˆx∗
AR ˆx∗+By+C f
(B.26)
Multiplying the top and bottom row by 2b/(Ab + D), and then making the change
of variables in equations (B.19)-(B.21), noting that (α + 1) = 2Ab/(Ab + D) and
(α −1) = −2D/(Ab+D), gives X∗in terms of α, β and γ:
X∗= b[βy+γ f +(α −1)R ˆx∗]
(α +1)R ˆx∗+βy+γ f
(B.27)
The expressions for Y ∗and Z∗follow similarly
Y ∗=
−2by
(α +1)R ˆx∗+βy+γ f
(B.28)
Z∗=
−2bf
(α +1)R ˆx∗+βy+γ f
(B.29)
Equation (B.29) is used to derive the limits of valid image plane measurements in
Section 3.2.6. Finally, substituting R ˆx∗from equation (B.22) into equations (B.27)-
(B.29), the inhomogeneous coordinates of the optimal reconstruction X∗can be writ-
ten as a function of the image plane measurements Lx = (Lx,y) and Rx = (Rx,y),
and system parameters α, β, γ, b and f:
X∗= [(α −1)(αLx−Rx)−(α +1)(βy+γ f)]b
(α +1)(αLx−Rx)+(α −1)(βy+γ f)
(B.30)
Y ∗=
2by(α2 +1)
(α +1)(αLx−Rx)+(α −1)(βy+γ f)
(B.31)
Z∗=
2bf(α2 +1)
(α +1)(αLx−Rx)+(α −1)(βy+γ f)
(B.32)
which is the result quoted in equations (3.28)-(3.30) in Section 3.2.4.

C
Iterated Extended Kalman Filter
This Appendix presents an overview of the Iterated Extended Kalman Filter (IEKF)
equations, which are used in Chapters 5 and 6 for model-based object tracking. A
detailed treatment of Kalman ﬁlter theory and the IEKF can be found in [8] and
[74]. The purpose of the Kalman ﬁlter is to estimate the state and error covariance
matrix of a linear dynamic system from measurements with additive noise, and the
IEKF provides a near-optimal solution when the measurement model is non-linear.
The Kalman ﬁlter assumes that the measurement and system noise are Gaussian
distributed, zero mean and white with known covariance. When these assumptions
are not satisﬁed, such as for biased measurements, the ﬁlter equations under-estimate
the true state covariance.
Let x(k) represent the state vector (the variables we wish to estimate), P(k) repre-
sent the covariance matrix describing the uncertainty in the state, and y(k) represent
the measurements on a linear dynamic system at sample time k. Assuming the sys-
tem has no inputs, the evolution of the state can be described by a discrete time state
transition equation (also known as the dynamic model):
x(k +1) = Fx(k)+v(k)
(C.1)
where F is the state transition matrix and v(k) is the process noise, which absorbs
unmodelled dynamics. The measurements y(k) are related to the state by a measure-
ment model of the form:
y(k +1) = H(k +1)x(k +1)+w(k +1)
(C.2)
where H represents the measurement function, and w(k) is the measurements noise.
For non-linear sensors, such as the pin-hole camera used throughout this book, the
measurement model takes the form
y(k +1) = h(k +1,x(k +1))+w(k +1)
(C.3)
where h(k,x) is the vector-valued non-linear measurement function describing the
sensor model. As already noted, the IEKF assumes v(k) and w(k) are white, zero
G. Taylor and L. Kleeman: Visual Perception and Robotic Manipulation, STAR 26, pp. 185–187, 2006.
© Springer-Verlag Berlin Heidelberg 2006

186
C Iterated Extended Kalman Filter
mean, Gaussian distributed noise sources. The covariance matrices for the process
noise Q(k) and measurement noise R(k) are assumed to be known, and are given by
Q(k) = E[v(k)v(k)]
(C.4)
R(k) = E[w(k)w(k)]
(C.5)
where E[·] denotes expectation. When the state and measurement noise components
are assumed to be independent, as is the case for the systems in this book, R(k) and
Q(k) are diagonal.
Let ˆx(k|k) represent the estimated state at time k, and y(k + 1) represent a new
set of measurements observed at time k +1. The IEKF algorithm estimates the new
state ˆx(k+1|k+1) as a weighted mean of the state predicted by the system dynamics
and the new measurements, with the ﬁlter weight calculated from the state error and
measurement error covariances. In the ﬁrst step of the algorithm, a predicted state
vector ˆx(k +1|k) and covariance matrix P(k +1|k) are calculated as
ˆx(k +1|k) = Fˆx(k|k)
(C.6)
P(k +1|k) = FP(k|k)F +Q(k)
(C.7)
The predicted state is then updated using the new measurements, which is an iterated
process in the IEKF to solve the non-linear measurement equations. Let ni represent
the updated state estimate at the ith iteration, with n0 = ˆx(k +1|k). To calculate the
ﬁlter weight, the measurement function h(k + 1) is replaced by a linear approxima-
tion Mi(k +1), operating about the current updated state:
Mi(k +1,ni) = ∂h(k +1,x)
∂x
####
x=ni
(C.8)
Then, the ﬁlter weight Ki(k +1) at the ith iteration is given by
Ki(k +1) = P(k +1|k)M
i (k +1)[Mi(k +1)P(k +1|k)M
i (k +1)+R(k +1)]−1
(C.9)
Using the non-linear sensor model, the measurements associated with the current
state estimate are predicted as h(k+1,ni) and compared to the actual measurements
y(k +1) to form an observation error e(k +1):
e(k +1) = y(k +1)−h(k +1,ni)
(C.10)
Finally, a new estimate ni+1 of the state is recovered as the weighted mean of the
predicted state ˆx(k +1|k) and the observation error e(k +1) ( [74], page 279):
ni+1 = ˆx(k +1|k)+Ki(k +1){e(k +1)−Mi(k +1)[ˆx(k +1|k)−ni]}
(C.11)
Equations (C.8)-(C.11) are iterated until successive state estimates ni converge ac-
cording to suitable criteria. In the current implementation, the system is deemed to
have converged when the maximum absolute error between successive estimates of
all state variables is below a threshold cth:

C Iterated Extended Kalman Filter
187
max
j
|nj,i+1 −nj,i| < cth
(C.12)
where n j,i is the jth element of vector ni. The ﬁnal result is taken as the updated state
estimate ˆx(k + 1|k + 1). Finally, the state covariance is updated from the predicted
covariance (equation (C.7)) by
P(k +1|k +1) = [I−K(k +1)M(k +1)]P(k +1|k)
(C.13)
In practice, the Joseph form covariance update is used:
P(k+1|k+1) = A(k+1)P(k+1|k)A(k+1)+K(k+1)R(k+1)K(k+1) (C.14)
where
A(k +1) = I−K(k +1)M(k +1)
(C.15)
which is less sensitive to numerical round-off errors ( [8], pg. 216).

D
Stereo Reconstruction Error Models
This appendix presents error models describing the uncertainty in reconstructing a
single point from stereo measurement, and similarly estimating the pose of an object
modelled by multiple points. The error models are used in Chapter 6 for hybrid
position-based visual servoing.
D.1 Optimal Reconstruction of a Single Point
This section derives the optimal stereo reconstruction X of a point from measure-
ment Lx and Rx on the left and right image planes. Assuming pin-hole cameras in a
rectilinear conﬁguration, the projections of X on the stereo image planes are given
by
L,Rˆx = L,RPX
(D.1)
where L,RP are the projection matrices of the left and right cameras, given by equation
(2.29). The above transformation can be expressed in inhomogeneous coordinates as
 L,R ˆx
L,R ˆy

= f
Z
 X ±b
Y

(D.2)
taking the positive sign for L and the negative for R. Now, the optimal estimate is the
point X that minimizes the image plane error between the measurements L,Rx and the
projections L,Rˆx, calculated (in inhomogeneous coordinates) from equation (6.12) as:
D2 = |Lˆx−Lx|2 +|Rˆx−Rx|2
(D.3)
Substituting equation (D.2) into equation (D.3):
D2 =

f(X +b)
Z
−Lx
2
+

f Y
Z
−Ly
2
+

f(X −b)
Z
−Rx
2
+

f Y
Z
−Ry
2
(D.4)
G. Taylor and L. Kleeman: Visual Perception and Robotic Manipulation, STAR 26, pp. 189–194, 2006.
© Springer-Verlag Berlin Heidelberg 2006

190
D Stereo Reconstruction Error Models
Minimization of D2 with respect to X proceeds in the usual manner by setting partial
derivatives of D2 to zero and solving for the optimal reconstruction. The relevant
partial derivatives are:
∂D2
∂X
= 2 f
Z

f(X +b)
Z
−Lx

+ 2f
Z

f(X −b)
Z
−Rx

(D.5)
(D.6)
∂D2
∂Y
= 2 f
Z

f Y
Z
−Ly

+ 2 f
Z

f Y
Z
−Ry

(D.7)
(D.8)
∂D2
∂Z
= −2 f(X +b)
Z2

f(X +b)
Z
−Lx

−2 f Y
Z2

f Y
Z
−Ly

−2 f(X −b)
Z2

f(X −b)
Z
−Rx

−2 f Y
Z2

f Y
Z
−Ry

(D.9)
Setting the right hand side of equations (D.5) and (D.6) to zero and solving for X and
Y gives:
X =
Z
2f (Lx+ Rx)
(D.10)
Y =
Z
2f (Ly+ Ry)
(D.11)
Now, setting ∂D2
∂Z = 0 in equation (D.8) and collecting common factors yields:
0 = (X +b)( f(X +b)−LxZ)+ Y(f Y −LyZ)
+(X +b)(f(X −b)−RxZ)+ Y(f Y −RyZ)
= 2 f X2 +2f Y 2 +2 fb2 −Z X(Lx+ Rx)−ZY(Ly+ Ry)−Zb(Lx−Rx)
(D.12)
Then, substituting equations (D.10)-(D.11) into equation (D.12):
0 =
Z2
2 f (Lx+ Rx)2 +
Z2
2 f (Ly+ Ry)2 +2fb2
−
Z2
2 f (Lx+ Rx)2 −
Z2
2 f (Ly+ Ry)2 −Zb(Lx−Rx)
= 2fb−Z(Lx−Rx)
(D.13)
Finally, equation (D.13) can be solved for Z and the result substituted into equa-
tions (D.10)-(D.11). Finally, the components of the optimal reconstruction X can be
expressed as a function of the image plane measurements:

D.2 Error Model for Reconstruction of a Single Point
191
X =
b
Lx−Rx(Lx+ Rx)
(D.14)
Y =
b
Lx−Rx(Ly+ Ry)
(D.15)
Z =
2bf
Lx−Rx
(D.16)
The above result can be compactly represented in vector form as:
X =
b
Lx−Rx
Lx+ Rx, Ly+ Ry, 2 f

(D.17)
which is the result quoted in equation (6.14) in Section 6.3.
D.2 Error Model for Reconstruction of a Single Point
This section now considers the error introduced in the reconstruction X (given by
equation (D.17)) due to uncertainties in the parameters of the stereo camera model.
Let b∗, f ∗and ν∗represent the actual baseline, focal length and verge angle of the
stereo rig, and let b, f and ν represent the calibrated parameters of the camera model.
If the camera parameters are perfectly calibrated, the measurements L,Rx are related
to the actual point, X = (X,Y,Z), by (see equation(D.2)):
L,Rx = f ∗
Z (X ±2b∗,Y)
(D.18)
In practice, features are observed in non-rectilinear stereo cameras, and then cor-
rected using projective rectiﬁcation (see Section 2.2.4). However, any error in the
calibrated verge angle will cause projective rectiﬁcation to over or under-compensate
by angle (ν −ν∗) when correcting the raw measurements for rectilinear stereo. Thus,
in the presence of verge angle error, the ideal rectilinear measurements in equation
(D.18) are offset by angular error (ν −ν∗) (see equations (2.37)-(2.38)) to give the
actual rectilinear measurements:
L,Rx = f ∗((X ±2b∗)cos(ν −ν∗)Z sin(ν −ν∗), Y)
Z cos(ν −ν∗)±(X ±2b∗)sin(ν −ν∗)
(D.19)
Assuming the verge angle error is not severe, the small angle approximations cos(ν −
ν∗) ≈1 and sin(ν −ν∗) ≈(ν −ν∗) are introduced to reduce the above to
L,Rx = f ∗((X ±2b∗)Z(ν −ν∗), Y)
Z ±(X ±2b∗)(ν −ν∗)
(D.20)
Based on these measurements and calibrated camera parameters f, b and ν, the op-
timal reconstruction X is found by substituting equation (D.20) into equation (D.17).
Evaluating the intermediate terms in equation (D.17):

192
D Stereo Reconstruction Error Models
Lx−Rx = f ∗(X +2b∗)−Z(ν −ν∗)
Z +(X +2b∗)(ν −ν∗) −f ∗(X −2b∗)+Z(ν −ν∗)
Z −(X −2b∗)(ν −ν∗)
= 2 f ∗2Zb∗−(X2 +Z2 −4(b∗)2)(ν −ν∗)−2Zb∗(ν −ν∗)2
Z2 +4Zb∗(ν −ν∗)−(X2 −4(b∗)2)(ν −ν∗)2
(D.21)
Again assuming the verge angle error is small, the approximation (ν −ν∗)2  1 is
introduced to reduce equation (D.21) to
Lx−Rx = 2 f ∗2Zb∗−(X2 +Z2 −4(b∗)2)(ν −ν∗)
Z2 +4Zb∗(ν −ν∗)
(D.22)
The remaining intermediate terms follow similarly:
Lx+ Rx = 2 f ∗
XZ
Z2 +4Zb∗(ν −ν∗)
(D.23)
Ly+ Ry = 2 f ∗Y(Z +2b∗(ν −ν∗))
Z2 +4Zb∗(ν −ν∗)
(D.24)
Finally, the reconstruction X = (X, Y, Z) from equation (D.17) is
X =
XZb
2Zb∗−(X2 +Z2 −4(b∗)2)(ν −ν∗)
(D.25)
Y =
Yb(Z +2b∗(ν −ν∗))
2Zb∗−(X2 +Z2 −4(b∗)2)(ν −ν∗)
(D.26)
Z =
2bf(Z2 +4Zb∗(ν −ν∗))
2Zb∗−(X2 +Z2 −4(b∗)2)(ν −ν∗)
(D.27)
Again assuming small errors, the effect of calibration errors on X is most easily ex-
amined by taking a Taylor series expansion of equations (D.25)-(D.27) with respect
to f, b and ν about the operating point f = f ∗, b = b∗and ν = ν∗. Taking the ﬁrst
order expansion for each component:
Xi(f,b,ν) = X

1+ b−b∗
2b∗
+ X2 +Z2 −4(b∗)2
2Zb∗
(ν −ν∗)

(D.28)
Yi(f,b,ν) = Y

1+ b−b∗
2b∗
+ X2 +Z2
2Zb∗(ν −ν∗)

(D.29)
Zi(f,b,ν) = Z

1+ b−b∗
2b∗
+ f −f ∗
f ∗
+ X2 +Z2 +4(b∗)2
2Zb∗
(ν −ν∗)

(D.30)
Rearrange the above and expressing in vector notation, the relationship between a
real point X = (X,Y,Z) and its reconstruction X in the presence of camera calibra-
tion errors can be written as a linear function of f, b and ν:
X =

1+ b−b∗
2b∗
+ X2 +Z2
2Zb∗(ν −ν∗)


X
Y
Z

+ f −f ∗
f ∗


0
0
Z

+(ν −ν∗)


2Xb∗/Z
0
2b∗


(D.31)
which is the ﬁnal error model quoted in equation (6.17) in Section 6.3.

D.3 Error Model for Pose Estimation
193
D.3 Error Model for Pose Estimation
The previous section considered the effect of calibration errors on the reconstruction
of a single point. This section now examines the effect of errors on estimating the
pose of an object modelled by multiple points. For simplicity and without loss of
generality, the object is assumed to undergo pure translation. Let Gi, i = 1,...,N
represent the N points of the model, with ∑Gi = 0 as discussed in Section 6.3, and let
TE = (XE,YE,ZE) represent the actual position of the object. Now, let G∗
i represent
the visually reconstructed points in the model. As discussed in Section 6.3, the effect
of calibration errors is to scale each visually reconstructed point by K1. Thus, the
measured 3D points (after biasing by calibration errors) are effectively given by:
G∗
i = K1(Gi +TE)
(D.32)
and the actual image plane measurements are given by the projection in equation
(D.2):
L,Rgi =
f
K1(Zi +ZE)(K1(Xi +XE)±2b, K1(Yi +YE))
(D.33)
Now, let TE = (XE, YE, ZE) represent the estimated pose of the object. The points in
the model are estimated as Gi = Gi + TE (ie. without the unknown bias K1) and the
corresponding predicted measurements for the given pose are (from equation (D.2)):
L,Rˆgi =
f
Zi + ZE
(Xi + XE ±2b, Yi + YE)
(D.34)
An optimal estimate of the translation TE is obtained by minimizing the reprojection
error D2(TE) in equation (6.12) between the actual and predicted measurements.
Substituting equations (D.33)-(D.34) into (6.12) gives the reprojection error as:
D2(TE) = ∑
i

(L ˆxi −Lxi)2 +(L ˆyi −Lyi)2 +(R ˆxi −Rxi)2 +(R ˆyi −Ryi)2
= f 2∑
i



Xi + XE +2b
Zi + ZE
−K1(Xi +XE)+2b
K1(Zi +ZE)
2
+

Yi + YE
Zi + ZE
−K1(Yi +YE)
K1(Zi +ZE)
2
+

Xi + XE −2b
Zi + ZE
−K1(Xi +XE)−2b
K1(Zi +ZE)
2
+

Yi + YE
Zi + ZE
−K1(Yi +YE)
K1(Zi +ZE)
2

The optimal estimated pose can be found analytically by setting the partial deriva-
tives of D2(TE) to zero and solving for TE. Taking the partial derivative with respect
to XE yields:

194
D Stereo Reconstruction Error Models
∂D2
∂XE
= f 2∑
i

2

Xi + XE +2b
Zi + ZE
−K1(Xi +XE)+2b
K1(Zi +ZE)

1
Zi + ZE
+ 2

Xi + XE −2b
Zi + ZE
−K1(Xi +XE)−2b
K1(Zi +ZE)

1
Zi + ZE

(D.35)
Expressing equation (D.35) over a common denominator and equating the numerator
to zero gives
0 = ∑
i

K1(Zi +ZE)(Xi + XE +2b)−(Zi + ZE)[K1(Xi +XE)+2b]
+K1(Zi +ZE)(Xi + XE −2b)−(Zi + ZE)[K1(Xi +XE)−2b]

= K1∑(XiZi)+K1(XE +2b)∑Zi +K1ZE∑Xi +NK1ZE(XE +2b)
−K1∑(XiZi)−K1(XE +2b)∑Zi −K1ZE∑Xi −NK1ZE(XE +2b)
+K1∑(XiZi)+K1(XE −2b)∑Zi +K1ZE∑Xi +NK1ZE(XE −2b)
−K1∑(XiZi)−K1(XE −2b)∑Zi −K1ZE∑Xi −NK1ZE(XE −2b)
= ZE XE −ZEXE
where the last line is obtained after substituting ∑Gi = 0. Solving for XE:
XE = XE ZE
ZE
(D.36)
Taking the partial derivatives of D2(TE) with respect to YE, and following similar
algebraic manipulation leads to
YE = YE ZE
ZE
(D.37)
and ﬁnally for ZE:
ZE = K1[(XE−XE)∑XiZi+(YE−YE)∑YiZi+ZE(∑X2
i +∑Y 2
i +N X2
E+NY 2
E +4Nb2)]
K1(∑X2
i +∑Y 2
i +NXE XE+NYE YE)+4Nb2
(D.38)
Substituting equations (D.36) and (D.37) into equation (D.38) allows XE and YE to
be eliminated from the expression for ZE (left as an exercise for the reader), and the
result is substituted back into equations (D.36) and (D.37) to express the optimal
estimated pose as a function of the real pose, the model points and the scale error K1.
The ﬁnal solution can be expressed in vector notation as
TE =
XE ∑XiZi +YE ∑YiZi −ZE(∑X2
i +∑Y 2
i +4Nb2)
K1(XE ∑XiZi +YE ∑YiZi)−ZE[K1(∑X2
i +∑Y 2
i )+4Nb2]K1TE
(D.39)
which is the result quoted in equation (6.26) in Section 6.3.

E
Calibration of System Latencies
A service robot is a complex system requiring the coordination of multiple sensors
and actuators. Complications arise when independent sensors sample measurements
at different times, and actuators have an unknown delay between receiving and ex-
ecuting commands. Without compensation, these various latencies can have a nega-
tive impact on the stability and reliability of dynamic processes such as visual object
tracking and feedback control. One way to avoid these effects is to ensure that the
dynamics of the robot and environment remain within the region of stability. For the
experiments in Chapters 6 and 7, for example, stability was ensured by setting a de-
liberately low gain on the visual servo controller. Obviously, the drawback of this
approach is an increase in the total execution time. Other methods have been pro-
posed to deal with latencies directly (for example, see [27]) for cases in which this
side effect cannot be tolerated.
Clearly, methods that compensate for latency must possess good estimates of the
relevant delays. This appendix presents a simple calibration procedure that can be
used to estimate the latency between sensors and actuators. We illustrate this method
by applying it to the light stripe sensor (see Chapter 3); speciﬁcally, estimating the
delay between exposing the CCDs to a new image of the light stripe and sampling
the angle encoder for the position of the light stripe. However, the same method
also applies to the delay between sending a motion command to the Puma and ob-
servation of the subsequent motion on the CCD, or the delay between exposing the
left and right CCDs (negligible on our experimental platform since the cameras are
synchronized in hardware).
The calibration technique is based on minimizing the hysteresis induced by the
delay for some cyclic motion. For the light stripe scanner, the laser is scanned back-
ward and forward across a planar surface and the encoder value e(t) and laser po-
sition on the image plane x(t) (averaged over a few scan-lines) are recorded along
with a timestamp t. Figure E.1 shows the image plane position plotted against en-
coder value, with a visible hysteresis loop resulting from the lag between these two
variables. Now, the loop can be closed by phase shifting the encoder values by the
acquisition delay δt. Thus, the latency δt may be calculated as the phase shift that
minimizes the residual error from a linear regression applied to x(t) and e(t +δt). It
G. Taylor and L. Kleeman: Visual Perception and Robotic Manipulation, STAR 26, pp. 195–197, 2006.
© Springer-Verlag Berlin Heidelberg 2006

196
E Calibration of System Latencies
-150
-100
-50
0
50
100
150
150
200
250
300
350
400
450
500
550
600
650
image plane position (pixels)
encoder (counts)
Fig. E.1. Hysteresis due to acquisition delay between corresponding images and encoder mea-
surements of the light stripe.
0
2
4
6
8
10
12
14
-0.25
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
0.25
residual error (pixels)
acquisition delay (s)
Fig. E.2. Residual error for estimation of time delay from linear regression.
-150
-100
-50
0
50
100
150
150
200
250
300
350
400
450
500
550
600
650
image plane position (pixels)
encoder (counts)
Fig. E.3. Relationship between captured images and encoder measurements of the light stripe
after compensating for latency.

E Calibration of System Latencies
197
should be noted that the image plane position is not exactly a linear function of the
encoder value, but the approximation is valid over a small range of motion.
The phase shift is applied using linear interpolation to calculate the encoder value
e(t +δt) corresponding to each image sample x(t). Figure E.2 shows the residual er-
ror from a linear regression applied to x(t) and e(t + δt) for phase shifts in 5 ms
increments between ±200 ms. The plot exhibits a minima at about -90 ms (to the
nearest 5 ms), indicating that the shaft encoder is sampled about 90 ms after the
associated image was exposed. Finally, the relationship between encoder and CCD
measurements after compensating for the delay is shown in Figure E.3, and the hys-
teresis has disappeared.
The calibration procedure was also applied to the Puma arm by moving the end-
effector in a periodic motion and observing the image plane position of an active
marker. The analysis revealed that the effect of a particular motion command was
observed by the cameras about 170 ms after the command was issued to the Puma
controller.

F
Task Planning
This appendix details the task planning required guide the motion of the end-effector
for the experiments in Chapter 7. Task planning is divided into two phases: grasp
planning and trajectory planning. Grasp planning is the process of determining the
best contact points between the ﬁngers and the target object to stably grasp the ob-
ject. We simplify this process by formulating a speciﬁc algorithm for each known
object, rather than using a general solution as in [87]. Based on established princi-
ples [37, 140], grasp planners for rectangular prisms (boxes) and upright cylinders
(cups) are developed in Sections F.1 and F.2. Following grasp planning, a trajectory
planner generates a collision-free path to guide the gripper from the current pose
to the planned grasp. Details of the trajectory planner are presented in Section F.3.
Finally, Section F.4 describes a simple inverse kinematic model for the Puma to cal-
culate the angle of the wrist joint, which is required for grasp planning, and determine
whether the a planned grasp is reachable.
F.1 Grasping a Box
While a box may be grasped in numerous ways, the algorithm described here sim-
pliﬁes the problem by considering only the precision grasp shown in the Figure F.1.
This grasp is widely applicable and minimizes the possibility of collisions with other
objects. Optimal grasp planning requires knowledge of surface properties such as
friction the distribution of mass, both of which are typically unknown in service ap-
plications. However, general principles dictate that maximum stability is achieved
when the force applied by the ﬁngers is normal to the surface and the object is
grasped near or above the centre of mass to minimize load torque when lifted. These
rules are easily applied to a box, assuming a uniformly distributed mass.
Optimal grasp planning algorithms typically determine a set of candidate contact
points on the surface of the object using heuristics and then select the best set based
on a suitable cost function [38,140]. The fast but sub-optimal planner described here
considers only the two candidate contact pairs on orthogonal faces shown in Figure
F.1: (C1,C2) and (C3,C4), where C1 and C3 are on the faces nearest the robot. The
G. Taylor and L. Kleeman: Visual Perception and Robotic Manipulation, STAR 26, pp. 199–205, 2006.
© Springer-Verlag Berlin Heidelberg 2006

200
F Task Planning
V3
C1
C3
C4
δ
θ2
Gpr,C
C2
G5
G3
V2
V4
V1
G4
V7
V6
V8
V5
Fig. F.1. Grasp planning for a box.
contacts are placed a small distance δ below the top surface and above the midpoint
of opposite faces to minimize torque and slippage. The contacts are calculated from
the vertices V1 to V8 of the polygonal box model. For example, C1 is calculated as
C1 = 1
2(V1 +V2)+δ · V5 −V1
|V5 −V1|
(F.1)
For clarity, the remaining discussion considers only (C1,C2), and the equivalent cal-
culations for (C3,C4) follow similarly.
The ﬁrst step in the planning process is to determine whether the contacts are
reachable. Let G5 and G3 describe the location of the thumb and index ﬁngertips
when the gripper is fully opened. Candidates (C1,C2) are considered to ﬁt within the
grasp when |C1 −C2| < |G5 −G3|. If the contact points violate this condition, the
grasp candidate is considered no further. If successful, the algorithm calculates the
transformation that aligns the ﬁngertips with the contacts, by placing Gpr (deﬁned in
Figure 7.3) at the midpoint C = 1
2(C1 +C2). The desired rotation aligns the pair of
lines joining (G5,G3) and (C1,C2), and is calculated as an angle θ1 about axis A1,
given by:
A1 = (EG5 −EG3)×(C1 −C2)
|EG5 −EG3||C1 −C2|
(F.2)
θ1 = cos−1
(EG5 −EG3)(C1 −C2)
|EG5 −EG3||C1 −C2|

(F.3)
The translation of the end-effector that aligns Gpr with C after rotation is
T1 = C−R1(θ1,A1)EGpr
(F.4)
where R1(θ1,A1) is the rotation matrix corresponding to the axis/angle given in
equations (F.2)-(F.3).
While the above transformation aligns the ﬁngertips and contacts, the orientation
of the hand about the line through (C1,C2), represented as angle θ2 in Figure F.1,

F.2 Grasping a Cup
201
remains unconstrained. To avoid collisions, the grasp planner chooses θ2 such that
the palm is above the top surface of the box. The lowest point on the gripper is
G4 = (X4,Y4,Z4), and the grasp is planned to satisfy Y4 > (YC +δ +Yth), where YC
is the height of C and error threshold Yth ensures that G4 is well above the box. The
location of G4 as a function of the orientation of the hand is:
WG4(θ2) = R2(θ2,A2)R1(θ1,A1)(EG4 −EGpr)+C
(F.5)
where the rotation axis is A2 = (C2 −C1)/|C2 −C1|. In the current implementation,
θ2 is calculated numerically as
θ2 = argminθ|YC +δ +Yth −Y4(θ)|
(F.6)
where Y4(θ) is calculated from equation (F.5) for all angles in steps of one degree.
Finally, the two transformations are combined to give the planned orientation
WRE and translation WTE of end-effector in the planned grasp:
WRE = R2R1
(F.7)
WTE = C−R2R1EGpr
(F.8)
The condition developed in Section F.4 is used to determine whether the planned
grasp is within reach of the robot, and the grasp is discarded if this condition is
violated.
As noted earlier, a candidate grasp is calculated for both pairs of contact points,
(C1,C2) and (C3,C4). If more than one grasp is realizable, the algorithm arbitrarily
chooses the grasp that minimizes the wrist angle (Section F.4 describes the inverse
kinematic model for calculating wrist angle of the Puma robot). Finally, the planned
grasp is transformed to the frame of the end-effector as
EHG = WH−1
E
WHO
(F.9)
where WHO is the pose of the object, WHE is the planned pose of the end-effector
given by equations (F.7)-(F.8), and EHG is deﬁned in Figure 6.1. The rotational and
translational components of EHG are:
ERG = (R2R1)−1WRO
(F.10)
ETG = EGpr −(R2R1)−1(WTO +C)
(F.11)
F.2 Grasping a Cup
Grasping for a cylindrical or conical cup follows a similar development to the previ-
ous section, except that a power grasp is employed since a precision grasp is difﬁcult
to stabilize on a curved surface. Only one candidate power grasp is possible for a
cup, as shown in Figure F.2. The ﬁrst step in planning is to determine whether the
cup ﬁts within the grasp, ie. 2r < |G5 −G3|, where r is the radius of the cup and G5

202
F Task Planning
Vt
Vb
δ
θ2
Gpw,C
G5
G3
Fig. F.2. Grasp planning for an upright cup.
and G3 are the locations of the ﬁngertips when the grasp is fully open. Furthermore,
to ensure the hand does not collide with the ﬂoor, the height of the cup is required
to satisfy |Vt −Vb| > Hth, where Vt and Vb (shown in Figure F.2) are the centroids
of the top and bottom faces and Hth is the minimum height. For cups with sufﬁcient
height, the grasp centre C is positioned a small distance δ below the rim:
C = Vt +δ · Vb −Vt
|Vb −Vt|
(F.12)
To ensure the ﬁngers exert a force approximately perpendicular to the surface, the
hand is oriented to align the x-axis of the end-effector frame with the line joining Vt
and Vb. This alignment is achieved by rotating the end-effector by angle θ1 about
axis A1, given by:
A1 = X×(Vt −Vb)/|Vt −Vb|
(F.13)
θ1 = cos(X(Vt −Vb)/|Vt −Vb|)
(F.14)
where X = (1,0,0) is a unit vector in the direction of the x-axis.
To constrain the orientation of the hand about the axis of the cup (angle θ2 in
Figure F.2), the planning algorithm minimizes the angle of the wrist. Let θ2 and
A2 = (Vt −Vb)/|Vt −Vb| represent the angle/axis of rotation about the axis of the
cup. After rotation, the translation of the end-effector (aligning C and Gpw) is:
WTE(θ2) = C−R2(θ2,A2)R1(θ1,A1)EGpw
(F.15)
Now, let θ5(WTE) represent the angle of the wrist when the end-effector is positioned
at WTE (details of wrist angle calculations are provided in Section F.4). The desired
rotation that minimizes the wrist angle is given by:
θ2 = argminθ|θ5(WTE(θ))|
(F.16)
As in the previous section, the minimization is performed as a numerical search over
all angles in steps of one degree. Finally, the transformations are combined using
equations (F.7)-(F.8) (with EGpw in place of EGpr) to obtain the optimal pose of

F.3 Trajectory Planning
203
S
Δ
(a) Approach set-point for box.
S
Δ
(b) Approach set-point for cup.
Fig. F.3. Trajectory planning for approaching the grasp of a box and cup.
the hand, WHE. Finally, the reachability of the grasp is tested using the condition
developed in Section (F.4). If this test is satisﬁed, the planned grasp is transformed
to the end-effector frame EHG using equations (F.10)-(F.11). Otherwise, the cup is
considered ungraspable.
F.3 Trajectory Planning
In general, a planned grasp cannot be approached from an arbitrary initial pose. Tra-
jectory planning generates set-points to guide the motion of the end-effector along a
suitable path to minimize the likelihood of collisions. A complete collision-free path
planner should consider general obstacles, the target object itself and self-collisions.
The planner developed in this section simpliﬁes task planning by considering only
collisions between the hand (or grasped object) and the target object. In this case,
only a single set-point is required to plan a path from an arbitrary initial pose to the
desired grasp (see Figure 7.4).
Figure F.3 shows the set-points generated for grasping a box and cup with the left
hand. The target pose is approached away from the direction in which the ﬁngers are
pointing, which minimizes the chance of collision with the object. Let WRS and WTS
represent the rotation and translation of the set-point S, and WRE and WTE represent
the pose of the planned grasp. Then, the set-point (for a box or cup) is calculated as
WRS = WRE
(F.17)
WTS = WTE −ΔWREEF
(F.18)
where EF is the direction in which the ﬁngers are pointing (in the end-effector frame
E), and Δ is the distance between S and the ﬁnal grasp. Using the planned set-point,
the grasp is performed by visually servoing the hand ﬁrst to S and then to the ﬁnal
pose.

204
F Task Planning
y
D2
x
θ1
T5
T
5
(a) Calculation of Puma waist angle θ1
y
z
D1
D4
D3
T
3
T
E
T
5
(b) Calculation of elbow position T3
Fig. F.4. Puma kinematic model.
F.4 Puma Kinematic Model
This section presents a simple inverse kinematics model for the Puma arm to recover
the wrist angle (Puma joint 5) for a given pose of the end-effector, and also deter-
mine whether a planned pose is within the conﬁguration space of the arm. These
parameters are required by the grasp planning algorithms developed above.
Let TE and RE represent the position and orientation of the end-effector in the
robot base frame, and let T5 represent the position of the wrist, which is distance D1
from the origin of the end-effector frame (see Figure F.4). The location of the wrist
can be calculated as
T5 = TE −D1ZE
(F.19)
where ZE is a unit vector in the direction of the Z-axis of E, obtained from the third
column of RE. To calculate the position of the elbow (joint 3), the robot base frame
is ﬁrst rotated about its X-axis by angle θ1 so that the arm is parallel to the YZ-
plane, as shown in Figure F.4(a). The required angle of rotation is calculated from
T5 = (X5,Y5,Z5) as
θ1 = tan−1(Y5/X5)−tan−1
!
X2
5 +Y 2
5 −D2
2/D2

(F.20)
After rotation, the position of the wrist is T
5 = (D2,Y 
5,Z
5). Then, the position of
the elbow T
3 = (D2,Y 
3,Z
3) is at the intersection of two circles in the YZ-plane
of radius D3 and D4, centred at the origin and (Y 
E,Z
E) respectively (see Figure
F.4(b)). The point of intersection has Y and Z-coordinates:
Z
3 =
Z
5(Y 2
5 +Z2
5 +D2
3 −D2
4)+Y 
5
!
4D2
3(Y 2
5 +Z2
5 )−(Y 2
5 +Z2
5 +D2
3 −D2
4)2
2(Y 2
5 +Z2
5 )
(F.21)
Y 
3 =
!
D2
3 −Z2
3
(F.22)

F.4 Puma Kinematic Model
205
where the positive solutions are taken for the elbow pointing away from the body. Fi-
nally, the pose of the end-effector is beyond the reach of the Puma when the solution
to equation (F.21) is imaginary, and the planned grasp in this case is unreachable.
When the pose is reachable, the elbow position T
3 is rotated by −θ1 about the x-axis
to give T3 in the original frame. Finally, the wrist angle θ5(TE) for the desired pose
is calculated as the angle between the lines joining T5 to T3 and T5 to TE:
θ5(TE) = acos
(T3 −T5)(TE −T5)
|T3 −T5||TE −T5|

(F.23)
The distances D1 through to D4 are taken from manufacturer speciﬁcations.

,iviÀiViÃ
£° 	° `>Ã] 
° 	Ài>âi>] ,° ° 	ÀÃ] >` 	° -V>ÃÃi>Ì° Õ>` ÀLÌÃ\  iÜ `
v Ì°  Ìi}iÌ -ÞÃÌiÃ] £x­{®\ÓxqÎ£] Óäää°
Ó° ° ° }LÞ> >` ° ,iiÃ° ÕÌLiVÌ ÌÀ>V}  Û`i° ,i>/i >}}] x\Óxq
Îä{] £°
Î° ° ° } >` /° "° 	vÀ`° 
«ÕÌiÀ `iÃVÀ«Ì v VÕÀÛi` LiVÌÃ°  *ÀV° ÎÀ` Ì°
Ì 
v°  ÀÌwV> Ìi}iVi] «>}iÃ ÈÓqÈ{ä] £ÇÎ°
{° ° ° ` >` ° i 	ÀÃ° Li ÀLÌ V>â>Ì ÕÃ} > }Ì ÃÌÀ«i ÃiÃÀ° 
*ÀV° v Ìi Ìi}iÌ 6iViÃ ½{ -Þ«ÃÕ] «>}iÃ ÓxxqÓx] £{°
x° ° ° ° Ã>ÜÃ >` 
° ,° i° Î LiVÌ ÀiV}Ì ÕÃ} V`i` }Ì «ÀiVÌ
vÀ ÀLÌ >ÃÃiLÞ >««V>ÌÃ°  *ÀV° Ó£ÃÌ ÌiÀ>Ì> 
viÀiVi  `ÕÃÌÀ>
iVÌÀVÃ] 
ÌÀ >` ÃÌÀÕiÌ>Ì] ÛÕi Ó] «>}iÃ £{Óäq£Ó{Ç] £x°
È°  ° `Àivv] ,° À>Õ`] >` 	° Ã«>Õ° ,LÌ >`iÞi V>LÀ>Ì ÕÃ} ÃÌÀÕVÌÕÀi vÀ
Ì° ÌiÀ>Ì> ÕÀ> v ,LÌVÃ ,iÃi>ÀV] Óä­Î®\ÓÓnqÓ{n] Óää£°
Ç° ° 
° ÌiÃ] ° ° >i] ° *V] ° ,iÞ] -° ÌÃ>>] -° -V>>] /° -L>Ì>]
° /iÛ>Ì>] ° 1`i] -° 6>Þ>Õ>À] ° >Ü>Ì] >` ° >Ü>Ì° 1Ã} Õ>` ÀLÌÃ
Ì ÃÌÕ`Þ Õ> Li>ÛÕÀ°  Ìi}iÌ -ÞÃÌiÃ] £x­{®\{xqxÈ] Óäää°
n° 9° 	>À-> >` 8,° ° ÃÌ>Ì >` /À>V}\ *ÀV«iÃ] /iVµÕiÃ >` -vÌ
Ü>Ài° ÀÌiV ÕÃi] £Î°
° 
° ° 	>ÃÌÕÃViV° /iVµÕiÃ vÀ Ài>Ìi }iiÀ>Ì v À>}i >}iÃ°  *ÀV° 

«° -V° 
v°  
«ÕÌiÀ 6Ã >` *>ÌÌiÀ ,iV}Ì] «>}iÃ ÓÈÓqÓÈn] £n°
£ä° ° 	iViÀ] ° iv>i>] ° >di] 
° Û `iÀ >ÃLÕÀ}] ° *>}i] ° /ÀiÃV] ° 
°
6ÀLÀdÕ}}i] ,° *° 7dÕÀÌâ] >` -° <>`i° À«-ii\  }iÃÌÕÀiVÌÀi` ÀLÌ vÀ LiVÌ
«iÀVi«Ì >` >«Õ>Ì° ÕÌÕÃ ,LÌÃ] È\ÓäÎqÓ£] £°
££° °
° 	iÌÛi}>] ° 1`i] 
°° ÌiÃ] >` ° 
i}° Õ>` ÀLÌ i>À} >`
}>i «>Þ} ÕÃ} *
L>Ãi` ÛÃ°  *ÀV° É,- ÓääÓ ÌiÀ>Ì> 
viÀiVi
 Ìi}iÌ ,LÌÃ >` -ÞÃÌiÃ] ÛÕi Î] «>}iÃ Ó{{qÓ{x{] ÓääÓ°
£Ó° *° ° 	iÃ >` ,° 
° >° -i}iÌ>Ì ÌÀÕ} Û>À>LiÀ`iÀ ÃÕÀv>Vi wÌÌ}° 
/À>Ã>VÌÃ  *>ÌÌiÀ >ÞÃÃ >` >Vi Ìi}iVi] £ä­Ó®\£ÈÇq£Ó] >ÀV
£nn°
£Î° ° 	V° vÀ>Ì VL>Ì «iÀ>ÌÀÃ vÀ `>Ì> vÕÃ\  V«>À>ÌÛi ÀiÛiÜ ÜÌ
V>ÃÃwV>Ì°  /À>Ã>VÌÃ  -ÞÃÌiÃ] > >` 
ÞLiÀiÌVÃq*>ÀÌ \ -ÞÃÌiÃ
>` Õ>Ã] ÓÈ­£®\xÓqÈÇ] £È°
£{° 
° 	Ài>âi>° -V>Þ Ìi}iÌ ÀLÌÃ\ ÀiÃi>ÀV] `iÛi«iÌ] >` >««V>ÌÃ° 
*ÀV° Óää£  ÌiÀ>Ì> 
viÀiVi  -ÞÃÌiÃ] >] >` 
ÞLiÀiÌVÃ] ÛÕi {]
«>}iÃ Ó£Ó£qÓ£ÓÈ] Óää£°

Óän
,iviÀiViÃ
£x° /° ° 	À`> >` ,° 
i>««>° ÃÌ>Ì v LiVÌ Ì «>À>iÌiÀÃ vÀ ÃÞ 
>}iÃ°  /À>Ã>VÌÃ  *>ÌÌiÀ >ÞÃÃ >` >Vi Ìi}iVi] n­£®\äq] >
£nÈ°
£È° ,° ° 	ÀÃ°
Ìi}iVi ÜÌÕÌ Ài«ÀiÃiÌ>Ì°
ÀÌwV> Ìi}iVi ÕÀ>]
{Ç\£Îq£x] ££°
£Ç° ° ,° 	ÕÌiv° *À}À>} ÜÌ *"-8 ÌÀi>`Ã° ``Ã 7iÃiÞ] £Ç°
£n° ° 
iÀÛiÀ> >` *° >ÀÌiÌ° 6ÃÕ> ÃiÀÛ} ÜÌ `ÀiVÌ >}i VÌÀ >` > «Ài`VÌ>Li
V>iÀ> ÌÀ>iVÌÀÞ°  *ÀV° É,- ÌiÀ>Ì> 
viÀiVi  Ìi}iÌ ,LÌÃ
>` -ÞÃÌiÃ] «>}iÃ În£qÎnÈ] £°
£° /° 
>«iÀ >` ° ÕiÌÌi° ÝÌÀ>VÌ} VÞ`iÀÃ  vÕ Î `>Ì> ÕÃ} > À>` Ã>
«} iÌ` >` Ìi >ÕÃÃ> >}i°  *ÀV° 6Ã] `i} >` 6ÃÕ>â>Ì Óää£]
«>}iÃ Îxq{Ó] Óää£°
Óä° ° 
>ÕiÌÌi°
*ÌiÌ> «ÀLiÃ v ÃÌ>LÌÞ >` VÛiÀ}iVi  >}iL>Ãi` >`
«ÃÌL>Ãi` ÛÃÕ> ÃiÀÛ}°  ° Ài}>] ° >}iÀ] >` ° ÀÃi] i`ÌÀÃ] /i

yÕiVi  6Ã >` 
ÌÀ] ÛÕi ÓÎÇ v iVÌÕÀi  ÌiÃ  
ÌÀ >` vÀ
>Ì -ÞÃÌiÃ] «>}iÃ ÈÈqÇn° -«À}iÀ6iÀ>}]  iÜ 9À] £nn°
Ó£° ° 
i >` ° <iÃÞ°
*À}À>} LÞ `iÃÌÀ>Ì\ 
«} ÜÌ ÃÕL«Ì>
Ìi>V} >VÌÃ°
ÌiÀ>Ì> ÕÀ> v ,LÌVÃ ,iÃi>ÀV] ÓÓ­x®\ÓqÎ£] >Þ
ÓääÎ°
ÓÓ° ° -° 
}° -ÕÌ>iÕÃ >««} >` V>â>Ì vÀ > Li ,LÌ ÕÃ} ->À
-iÃ}° * ÌiÃÃ] >Ã 1ÛiÀÃÌÞ] ÕÃÌÀ>>] £Ç°
ÓÎ° ° 
Õ} >` /°  >}>Ì>° ,i>Ã} Ã«wi` ÛÕiÌÀV Ã>«iÃ vÀ ÀLÌV }À>Ã«}°
 *ÀV° £x  ÌiÀ>Ì> 
viÀiVi  Ìi}iÌ ,LÌVÃ >` -ÞÃÌiÃ] «>}iÃ
Î{nqÎxÎ] £x°
Ó{° ,° 
«> >`  ° }ÕÀÃÌ° 6ÃÕ>Þ }Õ`i` }À>Ã«}  ÕÃÌÀÕVÌÕÀi` iÛÀiÌÃ°
,LÌVÃ >` ÕÌÕÃ -ÞÃÌiÃ] £\ÎÎÇqÎ{È] £Ç°
Óx° ° 
>À] ° /ÀÕVV] >` ° 
iÕ}° «ÀÛ} >ÃiÀ ÌÀ>}Õ>Ì ÃiÃÀÃ ÕÃ} «
>Àâ>Ì°  *ÀV° xÌ Ì>Ì> 
viÀiVi  
«ÕÌiÀ 6Ã] «>}iÃ n£qnÈ]
£x°
ÓÈ° ° -° 
i >` ,° ° ,iÞ°  >ÝÕ i` >««À>V Ì Ãi}iÌ} À>}i `>Ì>°
 *ÀV° £nn  ÌiÀ>Ì> 
viÀiVi  ,LÌVÃ >` ÕÌ>Ì] ÛÕi Î]
«>}iÃ £ÈÈq£Çä£] £nn°
ÓÇ° *° ° 
Ài >` ° 
° `° Þ>V ivviVÌÃ  ÛÃÕ> VÃi`« ÃÞÃÌiÃ° 
/À>Ã>VÌÃ  ,LÌVÃ >` ÕÌ>Ì] £Ó­x®\ÈÇ£qÈnÎ] £È°
Ón° 
° >i >` ,° 	>VÃÞ° /Àii`iÃ> Ãi}iÌ>Ì ÕÃ} Ìi >ÕÃÃ> >}i >`
Ã«>Ì> vÀ>Ì°  *ÀV°  
«ÕÌiÀ -ViÌÞ 
viÀiVi  *>ÌÌiÀ ,iV}Ì
>` >}i *ÀViÃÃ}] «>}iÃ x{qxÈ] £n£°
Ó° /° >ÀÀi] ° À`] ° >ÀÛi] >` ° 7`w°
Ìi}À>Ìi` «iÀÃ ÌÀ>V} ÕÃ
} ÃÌiÀi] VÀ >` «>ÌÌiÀ `iÌiVÌ°
ÌiÀ>Ì> ÕÀ> v 
«ÕÌiÀ 6Ã]
ÎÇ­Ó®\£Çxq£nx] Óäää°
Îä° ° >ÛÃ° Li ,LÌ  >Û}>Ì 1Ã} VÌÛi 6Ã° * ÌiÃÃ] 1ÛiÀÃÌÞ v
"ÝvÀ`] £n°
Î£° ° i}] 7° ° 7Ã] >` ° >>L->Àw° 
>À>VÌiÀÃÌVÃ v ÀLÌ ÛÃÕ> ÃiÀÛ}
iÌ`Ã >` Ì>À}iÌ `i iÃÌ>Ì°  *ÀV°  ÌiÀ>Ì> -Þ«ÃÕ  Ìi
}iÌ 
ÌÀ] «>}iÃ Èn{qÈn] ÓääÓ°
ÎÓ° ° Ài>] ,° <iÀ] "° ,}>>] >` ,° >° *À}À>} ÃiÀÛVi Ì>ÃÃ 
ÕÃi` iÛÀiÌÃ LÞ Õ> `iÃÌÀ>Ì°  *ÀV° ££Ì  ÌiÀ>Ì>
7ÀÃ«  ,LÌ >` Õ> ÌiÀ>VÌÛi 
ÕV>Ì] «>}iÃ {Èäq{ÈÇ] ÓääÓ°

,iviÀiViÃ
Óä
ÎÎ° -° Û> >` ° vv> ° À>}V° "LiVÌ ÀiV}Ì >` «Ãi iÃÌ>Ì vÀ ÀLÌV
>«Õ>Ì ÕÃ} VÀ VVVÕÀÀiVi ÃÌ}À>Ã°  *ÀV° É,- ÌiÀ>Ì>

viÀiVi  Ìi}iÌ ,LÌÃ >` -ÞÃÌiÃ] «>}iÃ £Ón{q£Ón] ÓääÎ°
Î{° 	° Ã«>Õ] ° 
>ÕiÌÌi] >` *° ,ÛiÃ°  iÜ >««À>V Ì ÛÃÕ> ÃiÀÛ}  ÀLÌVÃ°
 /À>Ã>VÌÃ  ,LÌVÃ >` ÕÌ>Ì] n­Î®\Î£ÎqÎÓÈ] £Ó°
Îx° /° >] ° i`] >` ,°  iÛ>Ì>° -i}iÌi` `iÃVÀ«ÌÃ v Î ÃÕÀv>ViÃ° 
ÕÀ> v ,LÌVÃ >` ÕÌ>Ì] Î­È®\xÓÇqxÎn] iV £nÇ°
ÎÈ° "° >Õ}iÀ>Ã° /Àii`iÃ> V«ÕÌiÀ ÛÃ\ > }iiÌÀV ÛiÜ«Ì° / *ÀiÃÃ]
£Î°
ÎÇ° 
° iÀÀ>À >` ° 
>Þ° *>} «Ì> }À>Ã«Ã°  *ÀV° £Ó  ÌiÀ>Ì>

viÀiVi  ,LÌVÃ >` ÕÌ>Ì] «>}iÃ ÓÓäqx] £Ó°
În° ° ÃViÀ >` ° Àâ}iÀ° >ÃÌ «>} v «ÀiVÃ }À>Ã«Ã v Î LiVÌÃ°  *ÀV°
É,- ÌiÀ>Ì> 
viÀiVi  Ìi}iÌ ,LÌÃ >` -ÞÃÌiÃ] «>}iÃ £Óäq£ÓÈ]
£Ç°
Î° ° ° ÀÃÞÌ >` ° *Vi° 
«ÕÌiÀ ÛÃ\ > `iÀ >««À>V° *ÀiÌVi >] ÓääÎ°
{ä° ° ÕÌ] °
° <Õ] >` ° L`i>i° >}iL>Ãi` ÛÃÕ> ÃiÀÛ} vÀ }À>Ã«}
ÕÜ LiVÌÃ°  *ÀV° ÓÈÌ Õ> 
viÀiVi v Ìi  `ÕÃÌÀ> iVÌÀVÃ
-ViÌÞ] ÛÕi Ó] «>}iÃ nÇÈqnn£] Óäää°
{£° 6° >ÀÀV >` ° iÛÞ° Û>Õ>Ì v V>LÀ>Ì >` V>â>Ì iÌ`Ã vÀ ÛÃÕ>Þ
}Õ`i` }À>Ã«}°  *ÀV° É,- ÌiÀ>Ì> 
viÀiVi  Ìi}iÌ ,LÌÃ >`
-ÞÃÌiÃ] «>}iÃ ÎnÇqÎÎ] £x°
{Ó° 
° >ÃiÌÌ] *° 	ÀÜ] ° 
i}] >` ° <iÃÞ° i>À} «VÌ `iÃ `ÕÀ} Ì>À}iÌ
«ÕÀÃÕÌ°  *ÀV°  ÌiÀ>Ì> 
viÀiVi  ,LÌVÃ >` ÕÌ>Ì] ÛÕi Î]
«>}iÃ {£ÓÓq{£Ó] ÓääÎ°
{Î° 
ÀÃ >ÃiÌÌ >` À` 
i}° "i i>À} v > ÌÀ >« vÀ Õ>` À
LÌ Ài>V}°  *ÀVii`}Ã v Ìi Ó` ÌiÀ>Ì> 
viÀiVi  
«ÕÌ>Ì>
Ìi}iVi] ,LÌVÃ >` ÕÌÕÃ -ÞÃÌiÃ] ÓääÎ°
{{° ° ,° `ÃVi`iÀ >` ° +° ° 6>À>Ì> Ãi}iÌ>Ì LÞ «iViÜÃi v>ViÌ `iÃ
ÜÌ >««V>Ì Ì À>}i >}iÀÞ°  *ÀV° Óää£  ÌiÀ>Ì> 
viÀiVi 
>}i *ÀViÃÃ}] «>}iÃ n£Óqn£x] Óää£°
{x° ° ÀÃÃ] ° iÌÌ>] ° "``iÀ>] >` ° ->`° ,LÕÃÌ ÛÃÕ> ÃiÀÛ}  Î Ài>V}
Ì>ÃÃ°  /À>Ã>VÌÃ  ,LÌVÃ >` ÕÌ>Ì] £Ó­x®\ÇÎÓqÇ{Ó] £È°
{È° ° ÀÃÃ >` ° 6iÀVi° À>Ã«} ÃÌÀ>Ìi}iÃ vÀ ÀiVÃÌÀÕVÌi` ÕÜ Î LiVÌÃ°
 *ÀV° É,- ÌiÀ>Ì> 
viÀiVi  Ìi}iÌ ,LÌÃ >` -ÞÃÌiÃ] «>}iÃ
ÇäqÇx] ££°
{Ç° ° ÀÕÃÃ] ° ,° 
>ÀiÞ] >` /° >>`i° Ìi}À>Ìi` ÃiÃÀ >` À>}iw`} >>} Ã}>
«ÀViÃÃÀ°  ÕÀ> v -`-Ì>Ìi 
ÀVÕÌÃ] ÓÈ­Î®\£n{q££] >À ££°
{n° ,° ° >À>V] ° ] 
° ii] 8° <Õ>}] 6° ° 6>`Þ>] >` ° 	° ° *Ãi iÃÌ>Ì
vÀ VÀÀiÃ«`} «Ì `>Ì>°  /À>Ã>VÌÃ  -ÞÃÌiÃ] > >` 
ÞLiÀiÌVÃ]
£­È®\£{ÓÈq£{{x] £n°
{° ,° ° >À>V] 
° ii] ° "ÌÌiLiÀ}] >` °  di° >ÞÃÃ >` ÃÕÌÃ v Ìi ÌÀii
«Ì «iÀÃ«iVÌÛi «Ãi iÃÌ>Ì «ÀLi°  *ÀV°  
viÀiVi  
«ÕÌiÀ
6Ã >` *>ÌÌiÀ ,iV}Ì] «>}iÃ xÓqxn] ££°
xä° ,° ° >À>V] ° /° 7>ÌÃ] >` /° ° >vviÞ° /i Ì«}À>«V «À> ÃiÌV° /i
ÌiÀ>Ì> ÕÀ> v ,LÌVÃ ,iÃi>ÀV] Ó­£®\xäqÇÓ] -«À} £nÎ°
x£° ° ° >À>° `iÀ >VÌÀ >ÞÃÃ] V>«ÌiÀ n° /i 1ÛiÀÃÌÞ v 
V>} *ÀiÃÃ]
£È°
xÓ° ,° >ÀÌiÞ >` ° <ÃÃiÀ>° ÕÌ«i 6iÜ iiÌÀÞ  
«ÕÌiÀ 6Ã° 
>LÀ`}i
1ÛiÀÃÌÞ *ÀiÃÃ] Óäää°

Ó£ä
,iviÀiViÃ
xÎ° ,° ° >ÀÌiÞ° /iÀÞ >` «À>VÌVi v «ÀiVÌÛi ÀiVÌwV>Ì° ÌiÀ>Ì> ÕÀ> v

«ÕÌiÀ 6Ã] Îx­Ó®\££xq£ÓÇ] £°
x{° ,° ° >ÀÌiÞ >` *° -ÌÕÀ° /À>}Õ>Ì° 
«ÕÌiÀ 6Ã >` >}i 1`iÀÃÌ>`}]
Èn­Ó®\£{Èq£xÇ] £Ç°
xx° ° >ÕV] ° ,dÕÌÌ}iÀ] ° -À}] >` ° d>ÀLiÀ° 6ÃÕ> `iÌiÀ>Ì v Î }À>Ã«}
«ÌÃ  ÕÜ LiVÌÃ ÜÌ > LVÕ>À V>iÀ> ÃÞÃÌi°  *ÀV° É,- ÌiÀ>
Ì> 
viÀiVi  Ìi}iÌ ,LÌÃ >` -ÞÃÌiÃ] «>}iÃ ÓÇÓqÓÇn] £°
xÈ° ° >ÕV] ° -À}] ° >LiÀ] >` /° -Vi° 7>Ì V> Li i>Ài` vÀ Õ> Ài>V
Ì}À>Ã« ÛiiÌÃ vÀ Ìi `iÃ} v ÀLÌV >`iÞi ÃÞÃÌiÃ°  *ÀV°  ÌiÀ
>Ì> 
viÀiVi  ,LÌVÃ >` ÕÌ>Ì] «>}iÃ ÓxÓ£qÓxÓÈ] £°
xÇ° ° >ÛiÀi >` ° ,d}°  LÃÌ>Vi `iÌiVÌ ÃÞÃÌi ÕÃ} > }Ì ÃÌÀ«i `iÌw
V>Ì L>Ãi` iÌ`°  *ÀV°  ÌiÀ>Ì> Ì -Þ«ÃÕ  Ìi}iVi >`
-ÞÃÌiÃ] «>}iÃ ÓÎÓqÓÎÈ] £n°
xn° ° >ÛiÀi >` ° ,d}°  Î ÃV>iÀ V>«ÌÕÀ} À>}i >` VÕÀ vÀ Ìi ÀLÌVÃ
>««V>ÌÃ°  Ó{Ì 7ÀÃ« v Ìi *,] >Þ ÓxÓÈ] ÕÃÌÀ> Óäää°
x° ° iLiÀÌ° VÌÛi >` «>ÃÃÛi À>}i ÃiÃ} vÀ ÀLÌVÃ°  *ÀV°  ÌiÀ>Ì>

viÀiVi  ,LÌVÃ >` ÕÌ>Ì] «>}iÃ £äÓq££ä] Óäää°
Èä° ° ° vv>° 6ÃÕ> Ìi}iVi] V>«ÌiÀ Ó° 7° 7°  ÀÌ >` 
«>Þ] £n°
È£°  ° ° }ÕÀÃÌ° 1V>LÀ>Ìi` -ÌiÀi >` >`Þi 
À`>Ì° * ÌiÃÃ] 1
ÛiÀÃÌÞ v 
>LÀ`}i] 
>LÀ`}i] £Ç°
ÈÓ° ° ÛiÀ] ° i>	>«ÌÃÌi] 8° >}] *° ° Þ] ° 	Õi] ° 	° `}v] ° 	ÜÞiÀ]
° 7° }}iÀÌ] ° Ìâ}LL] >` ,° 	° ÃiÀ°  iÝ«iÀiÌ> V«>ÀÃ v À>}i
>}i Ãi}iÌ>Ì >}ÀÌÃ°  /À>Ã>VÌÃ  *>ÌÌiÀ >ÞÃÃ >` >Vi
Ìi}iVi] £n­Ç®\ÈÇÎqÈn] ÕÞ £È°
ÈÎ° ,° À>Õ` >` ° À>>° 6ÃÕ>Þ }Õ`i` LiVÌ }À>Ã«}°  /À>Ã>VÌÃ 
,LÌVÃ >` ÕÌ>Ì] £{­{®\xÓxqxÎÓ] £n°
È{° 	° ° * À° ÝÌi`i` >ÕÃÃ> >}iÃ° *ÀV° v Ìi ] ÇÓ­£Ó®\£ÈÇ£qnÈ] iV
£n{°
Èx° ° Ã`> >` ° Ã>`>° 6iÀÃ>Ìi ÛÃÕ> ÃiÀÛ} ÜÌÕÌ Üi`}i v Ìi ÌÀÕi >
VL>°  *ÀV° É,- ÌiÀ>Ì> 
viÀiVi  Ìi}iÌ ,LÌÃ >` -ÞÃÌiÃ]
«>}iÃ £nÈq££] £{°
ÈÈ° 
° Õ] ° +° i}] *° 8° Õ] >` 8° 7>}° 6ÃÕ> }iÃÌÕÀi ÀiV}Ì vÀ Õ>
>Vi ÌiÀv>Vi v ÀLÌ Ìii«iÀ>Ì°  *ÀV° É,- ÌiÀ>Ì> 
viÀiVi
 Ìi}iÌ ,LÌÃ >` -ÞÃÌiÃ] «>}iÃ £xÈäq£xÈx] ÓääÎ°
ÈÇ° 9° Õ] ,° >}iÃ] >` ° ° `>i° Õ> ÛÃÕ> ÃiÀÛ} vÀ Ài>V} >` }À>Ã«
}\ /i Ài v Î }iiÌÀV vi>ÌÕÀiÃ°  *ÀV°  ÌiÀ>Ì> 
viÀiVi 
,LÌVÃ >` ÕÌ>Ì] «>}iÃ ÎÓäqÎÓ£È] £°
Èn° -° ÕÌVÃ] ° ° >}iÀ] >` *° ° 
Ài°  ÌÕÌÀ>  ÛÃÕ> ÃiÀÛ VÌÀ° 
/À>Ã>VÌÃ  ,LÌVÃ >` ÕÌ>Ì] £Ó­x®\Èx£qÈÇä] £È°
È° ° +° ÕÞ] ,° ° "ÜiÃ] >` *° ° >ÀÌ>° 
>LÀ>Ì} > ÃÌÀÕVÌÕÀi` }Ì ÃÌÀ«i ÃÞÃ
Ìi\  Ûi >««À>V° ÌiÀ>Ì> ÕÀ> v 
«ÕÌiÀ 6Ã] ÎÎ­£®\ÇÎqnÈ] £°
Çä° ° iÕV] ° >Ü>`i] >` /° -ÕiÀ°
/Ü>À`Ã >ÃÃiLÞ «> vÀ LÃiÀÛ>Ì°
 *ÀV° É,- ÌiÀ>Ì> 
viÀiVi  Ìi}iÌ ,LÌÃ >` -ÞÃÌiÃ] «>}iÃ
ÓÓ{qÓÎä£] £Î°
Ç£° ÎÓ Ìi ÀVÌiVÌÕÀi -vÌÜ>Ài iÛi«iÀÃ >Õ>° Ìi 
À«À>Ì] ÓääÓ°
ÇÓ° ° Ã>À` >` ° 	>i° 
"  -/" \ 1vÞ} ÜiÛi >` }iÛi ÌÀ>V}
 > ÃÌV>ÃÌV vÀ>iÜÀ°
 *ÀV° xÌ ÕÀ«i> 
viÀiVi  
«ÕÌiÀ 6Ã]
ÛÕi £] «>}iÃ nÎqän] £n°
ÇÎ° ,° ° >ÀÛÃ°  «iÀÃ«iVÌÛi  À>}i w`} ÌiVµÕiÃ vÀ V«ÕÌiÀ ÛÃ° 
/À>Ã>VÌÃ  *>ÌÌiÀ >ÞÃÃ >` >Vi Ìi}iVi] x­Ó®\£ÓÓq£Î] £nÎ°

,iviÀiViÃ
Ó££
Ç{° ° ° >âÜÃ° -ÌV>ÃÌV *ÀViÃÃiÃ >` ÌiÀ} /iÀÞ° >Ìi>ÌVÃ  -ViVi
>` }iiÀ}° V>`iV *ÀiÃÃ]  iÜ 9À] £Çä°
Çx° ,° Ã >` ° 
° ->`iÀÃ° ««V>Ì v vi>ÌÕÀiL>Ãi` ÕÌÛiÜ ÃiÀÛ} vÀ >«
w>iÌ >}iÌ°  ,LÌVÃ >` ÕÌ>Ì >}>âi] «>}iÃ ÓxqÎ£] iViLiÀ
£n°
ÇÈ° ,° ° >] ° ° -Ü>] *°  ° *À«ÜVâ] >` ,° ° ÀLÞ° iÃÌÕÀi ÀiV}Ì ÕÃ}
Ìi *iÀÃiÕÃ ÃÞÃÌi°  *ÀV°  
«ÕÌiÀ -ViÌÞ 
viÀiVi  
«ÕÌiÀ 6Ã
>` *>ÌÌiÀ ,iV}Ì] «>}iÃ ÇÎ{qÇ{£] £È°
ÇÇ° /° >>`i >` ° ° ÀÀÃ° >VÌÀâ>Ì iÌ`Ã vÀ ÃÌÀÕVÌÕÀi vÀ Ì° *
Ã«V> /À>Ã>VÌÃ v Ìi ,Þ> -ViÌÞ v `] -iÀiÃ ] ÎxÈ­£Ç{ä®\££xÎqÇÎ]
£n°
Çn° /° >`>] ° Ã}ÕÀ] /° "] ° >] >` ,°  >>ÌÃÕ° iÛi«iÌ >` iÛ>Õ>Ì v
> ÌiÀ>VÌÛi Õ>` ÀLÌ »,LÛi»°  *ÀV° ÓääÓ  ÌiÀ>Ì> 
viÀiVi
 ,LÌVÃ >` ÕÌ>Ì] «>}iÃ £n{nq£nxx] ÓääÓ°
Ç° -° 	° >} >` ° iÕV° iÌiÀ} Î LiVÌ «Ãi ÕÃ} Ìi V«iÝ iÝÌi`i`
>ÕÃÃ> >}i°  *ÀV°  
«ÕÌiÀ -ViÌÞ 
viÀiVi  
«ÕÌiÀ 6Ã >`
*>ÌÌiÀ ,iV}Ì] «>}iÃ xnäqxnx] ££°
nä° ° iv>i>] ° >di] >` ,° *° 7dÕÀÌâ°  Ìi}À>Ìi` LiVÌ Ài«ÀiÃiÌ>Ì vÀ ÀiV}
Ì >` }À>Ã«}°  *ÀV° £ /À` ÌiÀ>Ì> 
viÀiVi  Üi`}i	>Ãi`
Ìi}iÌ vÀ>Ì }iiÀ} -ÞÃÌiÃ] «>}iÃ {ÓÎq{ÓÈ] £°
n£° ,° iÞ] ,° 
>Ài] "°  >ÃÃ] 	° ÕVi] >` ° ,iÞiÃ°
-Ì>Li ÛÃÕ> ÃiÀÛ} v
V>iÀ>>` ÀLÌV ÃÞÃÌiÃ°
 /À>Ã>VÌÃ  iV>ÌÀVÃ] x­£®\Îq{n]
Óäää°
nÓ° ° >`À>Õ] ° ÌÞ] *° >ÀÌiÌ] ° >Vi] >` ° 
>ÕiÌÌi° 6ÃÕ> ÃiÀÛ} 
ÀLÌVÃ ÃVii ÕÃ} > V>iÀ>É>ÃiÀÃÌÀ«i ÃiÃÀ°  /À>Ã>VÌÃ  ,LÌVÃ >`
ÕÌ>Ì] £Ó­x®\Ç{ÎqÇxä] £È°
nÎ° ° ] ° 
] >` ° Üi°  Ûi >}iL>Ãi` VÌÀ>Ü vÀ Ìi ÛÃÕ> ÃiÀ
Û} ÃÞÃÌi Õ`iÀ >À}i «Ãi iÀÀÀ°  *ÀV° É,- ÌiÀ>Ì> 
viÀiVi 
Ìi}iÌ ,LÌÃ >` -ÞÃÌiÃ] «>}iÃ ÓÈÎqÓÈÇ] Óäää°
n{° ° ii>° "«Ì> iÃÌ>Ì v «ÃÌ >` i>`} vÀ Li ÀLÌÃ ÕÃ} ÕÌÀ>
ÃV Li>VÃ >` `i>`ÀiV}°   ÌiÀ>Ì> 
viÀiVi  ,LÌVÃ >`
ÕÌ>Ì] «>}iÃ ÓxnÓqÓxnÇ] £Ó°
nx° ° L>Þ>Ã] ° }>Ü>] /° 	Ì] >` ° ÕV° Î LiVÌ ÀiV}Ì >` }À>Ã«} vÀ
Õ> ÃÕ««ÀÌ ÀLÌV ÃÞÃÌiÃ°  *ÀV° Óäää  ÌiÀ>Ì> 7ÀÃ«  ,LÌ
>` Õ> ÌiÀ>VÌÛi 
ÕV>Ì] «>}iÃ {Îäq{Îx] Óäää°
nÈ° ° ] /° 9Ãi] °  >}>Ã>] ° >L>] >` ° Õi° *Ài>ÀÞ iÝ«iÀiÌÃ
 Ì «À}À>} v Õ>` ÀLÌ LÞ Õ> `iÃÌÀ>Ì° - ÌiÀ>
Ì> ÕÀ>] {Î­Ó®\{ä£q{äÇ] Óäää°
nÇ° ° À>}AV° 6ÃÕ> -iÀÛ} vÀ >«Õ>Ì\ ,LÕÃÌiÃÃ >` Ìi}À>Ì ÃÃÕiÃ° *
ÌiÃÃ] ,Þ> ÃÌÌÕÌi v /iV}Þ] -ÌV] Óää£°
nn° ° À>}V >` ° ° 
ÀÃÌiÃi° `i L>Ãi` ÌiVµÕiÃ vÀ ÀLÌV ÃiÀÛ} >` }À>Ã«
}°  *ÀV° É,- ÌiÀ>Ì> 
viÀiVi  Ìi}iÌ ,LÌÃ >` -ÞÃÌiÃ]
«>}iÃ ÓqÎä{] ÓääÓ°
n° ° À>}AV >` ° ° 
ÀÃÌiÃi° -ÕÀÛiÞ  ÛÃÕ> ÃiÀÛ} vÀ >«Õ>Ì° /iVV>
,i«ÀÌ -, /É É*äÓÉä£-] /] ÓääÓ°
ä° *° ÀÃi] ° ÕA>VÃ] >` ,° ,° >ÀÌ° }ÀÌÃ vÀ V«ÕÌ} VÕÀÛ>ÌÕÀiÃ vÀ À>}i
`>Ì>°  /i >Ìi>ÌVÃ v -ÕÀv>ViÃ 6] «>}iÃ £q£È] £n°
£° ° ÀÕ«>] 
° }] ° >}vv] >` ° i >Ìi° 
L} >}iL>Ãi` >`
`i«Ì ÛÃÕ> ÃiÀÛ} >««i` Ì ÀLÌâi` >«>ÀÃV«V ÃÕÀ}iÀÞ°  É,- ÌiÀ>
Ì> 
viÀiVi  Ìi}iÌ ,LÌÃ >` -ÞÃÌiÃ] «>}iÃ ÎÓÎqÎÓ] ÓääÓ°

Ó£Ó
,iviÀiViÃ
Ó° ° ° ÕvviÀ >` -° ° >6>i° ,,/ViVÌ\  ivwViÌ >««À>V Ì Ã}iµÕiÀÞ
«>Ì «>}°  *ÀV°  ÌiÀ>Ì> 
viÀiVi  ,LÌVÃ >` ÕÌ>Ì]
ÛÕi Ó] «>}iÃ xq£ää£] Óäää°
Î° ,° >}i>] *° -iÌâ>] ° 	LiÀ>] >` ,° -VÜ>ÀÌiL° /ivy}Ì À>}i >}} ÜÌ
> VÕÃÌ Ã`ÃÌ>Ìi >}i ÃiÃÀ°  *ÀV° "-É-*
viÀiVi  >ÃiÀ iÌÀ}Þ
>` Ã«iVÌ] ÛÕi -* ÎnÓÎ] «>}iÃ £näq££] £°
{° ° ° >ÛiÃÌ] ° ,ÛiÃ] >` ° i° /Àii`iÃ> ÀiVÃÌÀÕVÌ LÞ â}°
*ÀV°  /À>Ã>VÌÃ  ,LÌVÃ >` ÕÌ>Ì] ­Ó®\£ÈqÓäÇ] £Î°
x° 
° -° ÛV >` ° ° vÌiÀ° /i ,L>ÕÌ >`\  `iÝÌÀÕÃ ÀLÌ >` vÀ Ã«>Vi°
 *ÀV°  ÌiÀ>Ì> 
viÀiVi  ,LÌVÃ >` ÕÌ>Ì] «>}iÃ äÇq£Ó]
£°
È° ° >}ii] ,° 7i}iÀ] >` ° ° À>i° V>Ì v vi>ÌÕÀiÃ v Ü i}Ì  Ìi
«ÀiÃiVi v ÀiyiVÌÛi >` ÀivÀ>VÌÛi Ãi ÕÃ} > ÃÌiÀiÃV«V }ÌÃÌÀ«} >««À>V°
"«ÌV> }iiÀ}] ÎÎ­{®\£äÓq£än] «À £{°
Ç° ° >Ã] ° 
>ÕiÌÌi] >` -° 	Õ`iÌ° Ó£ÉÓ ÛÃÕ> ÃiÀÛ}°  /À>Ã>VÌÃ 
,LÌVÃ >` ÕÌ>Ì] £x­Ó®\ÓÎnqÓxä] £°
n° ° >ÀV>`] *° 	ÕÌiÞ] ° 
>ÕiÌÌi] >` 6° Ài>Õ° ,LÕÃÌ Ài>Ìi ÛÃÕ> ÌÀ>V
} ÕÃ} > ÓÎ `iL>Ãi` >««À>V°  *ÀV° ÇÌ  ÌiÀ>Ì> 
viÀiVi
 
«ÕÌiÀ 6Ã] «>}iÃ ÓÈÓqÓÈn] £°
° ° >À>ÛV] 	° -V>ÃÃi>Ì] >` ° 7>Ã° -ivÌ>Õ}Ì ÛÃÕ>Þ}Õ`i` «Ì
} vÀ > Õ>` ÀLÌ°  *ÀV° ÕÀÌ ÌiÀ>Ì> 
viÀiVi  -Õ>Ì v
`>«ÌÛi 	i>ÛÕÀ] «>}iÃ Îxq{{] £È°
£ää° ° >ÀÃ>] ° Õ>VÃ] >` ,° >ÀÌ° ,LÕÃÌ Ãi}iÌ>Ì v «ÀÌÛiÃ vÀ À>}i
`>Ì>  Ìi «ÀiÃiVi v }iiÌÀV `i}iiÀ>VÞ°  /À>Ã>VÌÃ  *>ÌÌiÀ >ÞÃÃ
>` >Vi Ìi}iVi] ÓÎ­Î®\Îä{qÎ£{] >ÀV Óää£°
£ä£° -°° Vi>] 9° ,>>] >` -° }° "LiVÌ ÌÀ>V} ÕÃ} >`>«ÌÛi VÕÀ ÝÌÕÀi
`iÃ°  *ÀV° /À` Ã> 
viÀiVi  
«ÕÌiÀ 6Ã] «>}iÃ È£xqÈÓÓ] £Ç°
£äÓ° *° ° ViÀÀÜ° ÌÀ`ÕVÌ Ì ,LÌVÃ° ``Ã7iÃiÞ] ££°
£äÎ° *° Vi> >` *° i° À} V«iÝ `iÝÌÀÕÃ >«Õ>ÌÃ vÀ Ì>Ã «À
ÌÛiÃ°  *ÀV°  ÌiÀ>Ì> 
viÀiVi  ,LÌVÃ >` ÕÌ>Ì] ÛÕi {]
«>}iÃ ÎÎnÎqÎÎnn] £{°
£ä{° ° À>iÃ] ° 
i>Ì] ° ° >}}] >` ° *° `i *L° Ý«iÀiÌ> «Ài`VÌ v
Ìi «iÀvÀ>Vi v }À>Ã« Ì>ÃÃ vÀ ÛÃÕ> vi>ÌÕÀiÃ°  *ÀV° É,- ÌiÀ>Ì>

viÀiVi  Ìi}iÌ ,LÌÃ >` -ÞÃÌiÃ] «>}iÃ Î{ÓÎqÎ{Ón] ÓääÎ°
£äx° ° ° ÀAi] 	° -° >ÀLÜ] >` ° ° ÃÌÀ° 1ÃiÀÃ½ }Õ`i vÀ  *
£° /iVV>
,i«ÀÌ  näÇ{] ««i` >Ì° Û°] À}i  >Ì> >LÀ>ÌÀÞ] £nä°
£äÈ° ° Õ>``L] ° 	>Ìi>] >` ° ->Û>° ,iViÌ «À}ÀiÃÃ  ÃÌÀÕVÌÕÀi` }Ì  À`iÀ Ì
ÃÛi Ìi VÀÀiÃ«`iVi «ÀLi  ÃÌiÀiÛÃ°  *ÀV°  ÌiÀ>Ì> 
viÀ
iVi  ,LÌVÃ >` ÕÌ>Ì] «>}iÃ £Îäq£ÎÈ] £Ç°
£äÇ° ° 
° Þ° iÃÌÕÀiL>Ãi` ÌiÀ>VÌ ÜÌ > «iÌ ÀLÌ°  *ÀV° v Ìi -ÝÌiiÌ  >
Ì> 
viÀiVi  ÀÌwV> Ìi}iVi >` iÛiÌ 
viÀiVi  Û>ÌÛi «
«V>ÌÃ v ÀÌwV> Ìi}iVi] «>}iÃ ÈÓnqÈÎÎ] £°
£än° ° dÕiÀ >` ° 7dÀ° *>} v À>«` }À>Ã« «iÀ>ÌÃ  ÕÃÌÀÕVÌÕÀi` ÃViiÃ°
 *ÀV° Óäää É,- ÌiÀ>Ì> 
viÀiVi  Ìi}iÌ ,LÌÃ >` -ÞÃÌiÃ]
ÛÕi Î] «>}iÃ £Çxqnä] Óäää°
£ä° ÕV  >>] 9>ÃÕ 7>Ì>>Li] >` -ÕiÞ>ÃÕ >° ÝÌÀ>VÌ >` ÀiV}Ì v
Î`iÃ> vÀ>Ì LÞ «ÀiVÌ} > «>À v ÃÌÀ>Þ Li>Ã°  *ÀVii`}Ã v Ìi
Ì ÌiÀ>Ì> 
viÀiVi  *>ÌÌiÀ ,iV}Ì] «>}iÃ ÇÎÈqÇÎn] £nn°

,iviÀiViÃ
Ó£Î
££ä° °  >L> >`  ° >ÀÕ° Î i>À ÛÃÕ> ÃiÀÛ} vÀ Õ>` ÀLÌ°  ÓnÌ Õ>

viÀiVi v Ìi  `ÕÃÌÀ> iVÌÀVÃ -ViÌÞ] ÛÕi Î] «>}iÃ ÓÓÓxqÓÓÎä]
ÓääÓ°
£££° -° °  >Þ>À] ° 
° ->`iÀÃ] ° ° 7iÃÃ] >` ° ° -° -«iVÕ>À ÃÕÀv>Vi Ã«iVÌ
ÕÃ} ÃÌÀÕVÌÕÀi` }}Ì >` >ÕÃÃ> >}i°  /À>Ã>VÌÃ  ,LÌVÃ >`
ÕÌ>Ì] È­Ó®\ÓänqÓ£n] «À £ä°
££Ó° ° °  i`iÀ >` ,° i>`°  Ã«iÝ iÌ` vÀ vÕVÌ â>Ì° 
«ÕÌiÀ
ÕÀ>] Ç\ÎänqÎ£Î] £Èx°
££Î° 	°°  iÃ >` *°° Ã>°  iÝÌi`>Li vÀ>iÜÀ vÀ iÝ«iVÌ>ÌL>Ãi` ÛÃÕ> ÃiÀ
Û} ÕÃ} iÛÀiÌ `iÃ°  *ÀV°  ÌiÀ>Ì> 
viÀiVi  ,LÌVÃ
>` ÕÌ>Ì] «>}iÃ £n{q£n] £x°
££{° °  ViÃ >` -° ÕÌVÃ° `iL>Ãi` ÌÀ>V} v V«iÝ >ÀÌVÕ>Ìi` LiVÌÃ°
 /À>Ã>VÌÃ  ,LÌVÃ >` ÕÌ>Ì] £Ç­£®\ÓnqÎÈ] Óää£°
££x° °  ÕÀ> >` /°  >Ì° Ìi}À>Ìi` ÛÃÕ> ÃiÀÛ} ÃÞÃÌi Ì }À>Ã« `ÕÃÌÀ> «>ÀÌÃ Û
}  VÛiÞiÀ LÞ VÌÀ} È" >À°  *ÀV° É,- ÌiÀ>Ì> 
viÀiVi
 -ÞÃÌiÃ] > >` 
ÞLiÀiÌVÃ] «>}iÃ £ÇÈnq£ÇÇx] Óäää°
££È° °  Þ}>À`Ã] /° d}ÃÌÀd] >` Â° 7iÀiÀÃÃ° V} Ì «>iÌÃ ÜÌ vii`L>V vÀ
> ÃiiÌv}Ì À>}i V>iÀ>°  *ÀV° Óäää É,- ÌiÀ>Ì> 
viÀiVi 
Ìi}iÌ ,LÌÃ >` -ÞÃÌiÃ] «>}iÃ £nxÎq£nx] Óäää°
££Ç° °  Þ}>À`Ã >` Â° 7iÀiÀÃÃ° -«iVÕ>À LiVÌÃ  À>}i V>iÀ>Ã\ ,i`ÕV} >L}ÕÌiÃ
LÞ Ì°  *ÀV° v Ìi  ÌiÀ>Ì> 
viÀiVi  ÕÌÃiÃÀ ÕÃ >`
Ìi}À>Ì vÀ Ìi}iÌ -ÞÃÌiÃ] «>}iÃ ÎÓäqÎÓn] £{°
££n° ° ">`>] ° >L>] >` ° Õi° Ìi}À>Ì v Ài>Ìi LVÕ>À ÃÌiÀi ÛÃ >`
Üi L`Þ vÀ>Ì vÀ `Þ>V Ü>} >Û}>Ì v Õ>` ÀLÌ°  *ÀV°
 
viÀiVi  ÕÌÃÃÀ ÕÃ >` Ìi}À>Ì vÀ Ìi}iÌ -ÞÃÌiÃ] «>}iÃ
£Î£q£ÎÈ] ÓääÎ°
££° ° ">`>] 9°  >>ÕÀ>] >` -° 	>° iÃ} v «À}À>>Li «>ÃÃÛi V«>Vi
ÃÕ`iÀ iV>Ã°  *ÀV° Óää£  ÌiÀ>Ì> 
viÀiVi  ,LÌVÃ >`
ÕÌ>Ì] ÛÕi £] «>}iÃ Î{nqÎxÎ] Óää£°
£Óä° /° ">`>] ° ->] >` ° >i° /Àii`iÃ> LiVÌ ÀiV}Ì ÕÃ` Ã«iÀ
V> VÀÀi>Ì°  *ÀV° ££Ì *, ÌiÀ>Ì> 
viÀiVi  *>ÌÌiÀ ,iV}Ì]
ÛÕi £] «>}iÃ ÓxäqÓx{] £Ó°
£Ó£° /° ">Ì> >` ° i}ÕV° 
«ÕÌ>Ì v Ìi Ã} v Ìi >ÕÃÃ> VÕÀÛ>ÌÕÀi v > ÃÕÀ
v>Vi vÀ ÕÌ«i ÕÜ Õ>Ì >}iÃ ÜÌÕÌ Üi`}i v Ìi ÀiyiVÌ>Vi
«À«iÀÌÞ° 
«ÕÌiÀ 6Ã >` >}i 1`iÀÃÌ>`}] ÇÈ­Ó®\£Óxq£Î{]  ÛiLiÀ £°
£ÓÓ° 6° ° *>ÛÛV] ,° ->À>] >` /° -° Õ>}° 6ÃÕ> ÌiÀ«ÀiÌ>Ì v >` }iÃÌÕÀiÃ vÀ
Õ>V«ÕÌiÀ ÌiÀ>VÌ\  ÀiÛiÜ°
 /À>Ã>VÌÃ  *>ÌÌiÀ >ÞÃÃ >`
>Vi Ìi}iVi] £­Ç®\ÈÇÇqÈx] £Ç°
£ÓÎ° ° *iviÞÃ] ° 6> ] ° 6iÀ}>ÕÜi] ° 6iÀLiÃÌ] ° 
ÀiÃ] ° /«Ã] >` ,° V°
6ÃÕ> `i} ÜÌ > >`i` V>iÀ>° ÌiÀ>Ì> ÕÀ> v 
«ÕÌiÀ 6Ã]
x­Î®\ÓäÇqÓÎÓ] Óää{°
£Ó{° ° *iviÞÃ] ,° V] >` ° 6> ° -ivV>LÀ>Ì >` iÌÀV ÀiVÃÌÀÕVÌ 
Ã«Ìi v Û>ÀÞ} >` ÕÜ ÌiÀ> V>iÀ> «>À>iÌiÀÃ°  *ÀV°  ÈÌ ÌiÀ>
Ì> 
viÀiVi  
«ÕÌiÀ 6Ã] «>}iÃ äqx] £n°
£Óx° *°  ° *À«ÜVâ] ° ° -Ü>] >` ,° ° >° />Ã >` iÛÀiÌÃiÃÌÛi ÌÀ>V
}°  *ÀV° ,*É 7ÀÃ«  6ÃÕ> 	i>ÛÕÀÃ] «>}iÃ ÇÎqÇn] £{°
£ÓÈ° ° ,>] ° i`] ° Õ] >` ° ° 	iiÞ° ->«i `iÃVÀ«Ì >` }À>Ã«} vÀ ÀLÌ
>`iÞi VÀ`>Ì°  
ÌÀ -ÞÃÌiÃ >}>âi] «>}iÃ ÓÓqÓ] iLÀÕ>ÀÞ £n°
£ÓÇ° ,° *°  ° ,> >` ° ° 	>>À`° i>À} Ã>VV>`V iÞi ÛiiÌÃ ÕÃ} ÕÌÃV>i
Ã«>Ì> wÌiÀÃ° `Û>ViÃ   iÕÀ> vÀ>Ì *ÀViÃÃ} -ÞÃÌiÃ] Ç\nÎqää] £x°

Ó£{
,iviÀiViÃ
£Ón° ° ° ,ÌÌiÀ] /° ° >ÀÌiÌâ] >` ° ° -VÕÌi° /«}ÞVÃiÀÛ} >«Ã vÀ i>À}
ÛÃÕÌÀVÀ`>Ì°  iÕÀ>  iÌÜÀÃ] Ó­Î®\£xq£Èn] £n°
£Ó° *° ,ÛiÃ° 6ÃÕ> ÃiÀÛ} L>Ãi`  i««>À }iiÌÀÞ°  *ÀV° É,- ÌiÀ>Ì>

viÀiVi  Ìi}iÌ ,LÌÃ >` -ÞÃÌiÃ] «>}iÃ ÈäÓqÈäÇ] Óäää°
£Îä° ° ° ,`À}ÕiÃ] 9° ° ] ° ° ii] ° ° ,Ü>`] >` 
° }° ,LÌV }À>Ã«} v
V«iÝ LiVÌÃ ÜÌÕÌ vÕ }iiÌÀV> Üi`}i v Ã>«i°  *ÀV°  ÌiÀ>
Ì> 
viÀiVi  ,LÌVÃ >` ÕÌ>Ì] «>}iÃ ÇÎÇqÇ{Ó] £x°
£Î£° "° ,}>>] ° Ài>] ,° <iÀ] ,° 	iViÀ] >` ,° >° 1Ã} }iÃÌÕÀi >`
Ã«iiV VÌÀ vÀ V>`} > ÀLÌ >ÃÃÃÌ>Ì°
 *ÀV° ££Ì  ÌiÀ>Ì>
7ÀÃ«  ,LÌ >` Õ> ÌiÀ>VÌÛi 
ÕV>Ì] «>}iÃ {x{q{x] ÓääÓ°
£ÎÓ° ,° ° ,ÕÃÃi >` ° ° *ÕÀ>>`>>° "`À >` >ÀyÜ\ V«iiÌ>ÀÞ ÃiÃiÃ vÀ >
Õ>` ÀLÌ°  *ÀV°  ÌiÀ>Ì> 
viÀiVi  ,LÌVÃ >` ÕÌ>Ì]
ÛÕi Ó] «>}iÃ £n{Óq£n{Ç] ÓääÓ°
£ÎÎ° ,° ° ,ÕÃÃi] ° />ÞÀ] ° ii>] >` ° ° *ÕÀ>>`>>° ÕÌÃiÃÀÞ ÃÞiÀ
}iÃ  Õ>` ÀLÌVÃ° ÌiÀ>Ì> ÕÀ> v Õ>` ,LÌVÃ] £­Ó®\ÓnqÎ£{]
Óää{°
£Î{° 9° ° ,ÞÕ >` ° -° 
°  iÕÀ> iÌÜÀ >««À>V Ì iÝÌi`i` >ÕÃÃ> >}i L>Ãi`
Ã`iÀ Ì Ã«iVÌ° iV>ÌÀVÃ] Ç­Ó®\£xq£n{] £Ç°
£Îx° ° 
° ->`iÀÃ >` ° ° 7iÃÃ° >}iL>Ãi` ÛÃÕ> ÃiÀÛ VÌÀ ÕÃ} Ài>Ì>
}À>« iÀÀÀ Ã}>Ã°  *ÀV°  
viÀiVi  
ÞLiÀiÌVÃ >` -ViÌÞ] «>}iÃ £äÇ{q
£äÇÇ] £nä°
£ÎÈ° 	° -V>ÃÃi>Ì°  LVÕ>À] vÛi>Ìi` >VÌÛi ÛÃ ÃÞÃÌi° /iVV> ,i«ÀÌ £ÈÓn] /
ÀÌwV> Ìi}iVi >L] £n°
£ÎÇ° -° -V>>°
Ã Ì>Ì i>À} Ìi ÀÕÌi Ì Õ>` ÀLÌÃ°
/Ài`Ã  
}ÌÛi
-ViViÃ] Î­È®\ÓÎÎqÓ{Ó] £°
£În° ° - >` 
° />Ã° ` vi>ÌÕÀiÃ Ì ÌÀ>V°   
viÀiVi  
«ÕÌiÀ 6Ã
>` *>ÌÌiÀ ,iV}Ì] «>}iÃ xÎqÈää] £{°
£Î° 9° -À> >` ° -ÕÜ>° ,iV}Ì v «Þi`ÀÃ ÜÌ > À>}i w`iÀ°  *ÀV° Ó`
Ì° Ì 
v°  ÀÌwV> Ìi}iVi] «>}iÃ näqnÇ] £Ç£°
£{ä° ° -Ì] ° ii] ° `LiÀ}] ° 	dÀ}iÀ] >` ° 
À>}° 
«ÕÌ} «>À>i>Ü
}À«Ã°  *ÀV° £  ÌiÀ>Ì> 
viÀiVi  ,LÌÃ >` ÕÌ>Ì] «>}iÃ
£nÇq£äÎ] £°
£{£° ° -«i}iÀ >` 	° -Vii° /Ü>À`Ã ÀLÕÃÌ ÕÌVÕi Ìi}À>Ì vÀ ÛÃÕ> ÌÀ>V}°
>Vi 6Ã >` ««V>ÌÃ] £{\xäq£xn] ÓääÎ°
£{Ó° ° -Ì>ÛÌÃÞ >` ° 
>«Ã° ÕÌ«i V>iÀ> `iL>Ãi` Î ÛÃÕ> ÃiÀÛ° 
/À>Ã>VÌÃ  ,LÌVÃ >` ÕÌ>Ì] £È­È®\ÇÎÓqÇÎ] iV Óäää°
£{Î° 
° -Õ >` ° -iÀÀ>° Î ÃÞiÌÀÞ `iÌiVÌ ÕÃ} Ìi iÝÌi`i` >ÕÃÃ> >}i° 
/À>Ã>VÌÃ  *>ÌÌiÀ >ÞÃÃ >` >Vi Ìi}iVi] £­Ó®\£È{q£È] iLÀÕ>ÀÞ
£Ç°
£{{° ° />>Ã] ° Ã`>] 9° 9>>â>] >` ° >Ì° /i Ài>â>Ì v `Þ>V Ü>}
LÞ Ìi L«i` Ü>} ÀLÌ Ü£äÀ`°  *ÀV° ½nx ÌiÀ>Ì> 
viÀiVi  `Û>Vi`
,LÌVÃ] «>}iÃ {xq{ÈÈ] £nx°
£{x° ° />>Ã] ° ] ° /ÃÕ`>] >` ° >Ì° ,i>â>Ì v `Þ>V L«i` Ü>}
ÃÌ>Lâi` LÞ ÌÀÕ Ì  > Ã>}ÌÌ>Þ ÕiÛi ÃÕÀv>Vi°  *ÀV°  ÌiÀ>Ì>
7ÀÃ«  Ìi}iÌ ,LÌÃ >` -ÞÃÌiÃ] «>}iÃ ÎÓÎqÎÎä] £ä°
£{È° ° />â>Ü>] 9° >>À>]  ° ->`>] ° ÕÀ>] >` 9° -À>°  ÃiÀÛVi ÀLÌ ÜÌ
ÌiÀ>VÌÛi ÛÃ q LiVÌ ÀiV}Ì ÕÃ} `>} ÜÌ ÕÃiÀ q°  ÃÌ ÌiÀ>Ì>
7ÀÃ«  >}Õ>}i 1`iÀÃÌ>`} >` }iÌÃ vÀ ,i> 7À` ÌiÀ>VÌ] «>}iÃ
£ÈqÓÎ] ÓääÎ°

,iviÀiViÃ
Ó£x
£{Ç° ° />ÞÀ >` ° ii>° À>Ã«} ÕÜ LiVÌÃ ÜÌ > Õ>` ÀLÌ°  *ÀV°
ÓääÓ ÕÃÌÀ>>Ã> 
viÀiVi  ,LÌVÃ >` ÕÌ>Ì] «>}iÃ ££q£È] ÓääÓ°
£{n° ° />ÞÀ >` ° ii>° ,LÕÃÌ À>}i `>Ì> Ãi}iÌ>Ì ÕÃ} }iiÌÀV «ÀÌÛiÃ
vÀ ÀLÌV >««V>ÌÃ°  *ÀV° Ì -/ ÌiÀ>Ì> 
viÀiVi  -}> >`
>}i *ÀViÃÃ}] «>}iÃ {ÈÇq{ÇÓ] ÓääÎ°
£{° ° />ÞÀ >` ° ii>° ÞLÀ` «ÃÌL>Ãi` ÛÃÕ> ÃiÀÛ} ÜÌ i V>LÀ>Ì
vÀ > Õ>` ÀLÌ°  *ÀV° É,- ÌiÀ>Ì> 
viÀiVi  Ìi}iÌ ,LÌÃ
>` -ÞÃÌiÃ] «>}iÃ ÈnÈqÈ£] Óää{°
£xä° ° />ÞÀ >` ° ii>° Ìi}À>Ì v ÀLÕÃÌ ÛÃÕ> «iÀVi«Ì >` VÌÀ vÀ >
`iÃÌV Õ>` ÀLÌ°  *ÀV° É,- ÌiÀ>Ì> 
viÀiVi  Ìi}iÌ
,LÌÃ >` -ÞÃÌiÃ] «>}iÃ £ä£äq£ä£x] Óää{°
£x£° ° />ÞÀ >` ° ii>°
-ÌiÀiÃV«V }Ì ÃÌÀ«i ÃV>}\ ÌiÀviÀiVi ÀiiV
Ì] iÀÀÀ â>Ì >` V>LÀ>Ì° ÌiÀ>Ì> ÕÀ> v ,LÌVÃ ,iÃi>ÀV]
ÓÎ­£Ó®\££{£q££xÈ] iViLiÀ Óää{°
£xÓ° ° /] ° -Vd>viÀ] ° iiÃ] >` °°  >}i° /Ü>À`Ã ÛÃÕ>Þ ÃiÀÛi` >«
Õ>Ì v V>À i}i «>ÀÌÃ°  *ÀV°  ÌiÀ>Ì> 
viÀiVi  ,LÌVÃ >`
ÕÌ>Ì] «>}iÃ £ÎÈÈq£ÎÇ£] £Ç°
£xÎ° ° /Þ>> >` °° >}iÀ° VÀiiÌ> vVÕÃ v >ÌÌiÌ vÀ ÀLÕÃÌ ÛÃL>Ãi` ÌÀ>V
}° ÌiÀ>Ì> ÕÀ> v 
«ÕÌiÀ 6Ã] Îx­£®\{xqÈÎ] £°
£x{° ° /ÀÕVV >` ,° 	° ÃiÀ° 
«ÕÌ} ÃÕÀv>ViL>Ãi` Ài«ÀiÃiÌ>ÌÃ ÀÀ À>}i 
>}iÃ°  *ÀV°  ÌiÀ>Ì> -Þ«ÃÕ  Ìi}iÌ 
ÌÀ] «>}iÃ ÓÇxqÓnä]
£Ó°
£xx° ° /ÀÕVV >` ,° 	° ÃiÀ° VµÕÃÌ v VÃÃÌiÌ À>}i `>Ì> ÕÃ} V> V>LÀ>
Ì°  *ÀV° v Ìi £{  ÌiÀ>Ì> 
viÀiVi  ,LÌVÃ >` ÕÌ>Ì]
ÛÕi {] «>}iÃ Î{£äqÎ{£x] £{°
£xÈ° ,° 9° /Ã> >` ,° ° iâ°  iÜ ÌiVµÕi vÀ vÕÞ >ÕÌÕÃ >` ivwViÌ Î
ÀLÌV >`ÉiÞi V>LÀ>Ì°  /À>Ã>VÌÃ  ,LÌVÃ >` ÕÌ>Ì] x­Î®\Î{xq
Îxn] £n°
£xÇ° /° 1] *°  À`Õ`] ° >] >` °"° Õ`° /Ü>À`Ã > >VÌÛi ÛÃÕ> LÃiÀÛiÀ° 
*ÀV° v Ìi ÌiÀ>Ì> 
viÀiVi  
«ÕÌiÀ 6Ã] «>}iÃ ÈÇqÈnÈ] 
>LÀ`}i]
] £x°
£xn° ° 1ÃiÀ] *° ,`iÞ] >` *° 
Ài° 6ÃÕ> ÃiÀÛ} vÀ > V>Ài ÛiVi  > >««V>
Ì  `ÀiVÌ> ÛÃ°  *ÀV° ÓääÓ ÕÃÌÀ>>Ã> 
viÀiVi  ,LÌVÃ >`
ÕÌ>Ì] «>}iÃ ÎÇq{Ó] ÓääÓ°
£x° -° Û> `iÀ <Ü>] ° 	iÀ>À`] >` ° ->ÌÃ6VÌÀ° 6Ã L>Ãi` ÃÌ>Ì ii«}
>` `V} vÀ > >iÀ> L«°  É,- ÌiÀ>Ì> 
viÀiVi  Ìi}iÌ
,LÌÃ >` -ÞÃÌiÃ] ÛÕi £] «>}iÃ È£{qÈ£] Óäää°
£Èä° *° 6> >` ° ° iÃ° ,LÕÃÌ Ài>Ìi v>Vi `iÌiVÌ° ÌiÀ>Ì> ÕÀ> v

«ÕÌiÀ 6Ã] xÇ­Ó®\£ÎÇq£x{] Óää{°
£È£° ° 7iV >` ° 	Ã«° -
/\ VÀiiÌ> ÌÀ>V} ÜÌ V«iÌi vÀ>Ì°

«ÕÌiÀ À>«VÃ] Î£\ÎÎÎqÎ{{] £Ç°
£ÈÓ° ° -° 7iiiÀ] ° ° >}}] >` ,° ° ÀÕ«i° i>À} «ÀÃ«iVÌÛi «V >` «>Vi
Li>ÛÕÀ°  *ÀV° /i Ó` ÌiÀ>Ì> 
viÀiVi  iÛi«iÌ >` i>À}]
«>}iÃ £ÇqÓäÓ] ÓääÓ°
£ÈÎ° 7° 7Ã] 
° 7>Ã ÕÃ] >` ° 	i° ,i>ÌÛi i`ivviVÌÀ VÌÀ ÕÃ} V>ÀÌi
Ã> «ÃÌ L>Ãi` ÛÃÕ> ÃiÀÛ}°  /À>Ã>VÌÃ  ,LÌVÃ >` ÕÌ>Ì]
£Ó­x®\Èn{qÈÈ] £È°
£È{° *° 7À> >` ° *° 1ÀL>°  iÜ >`>«ÌÛi >> wÌiÀ >««i` Ì ÛÃÕ> ÃiÀÛ} Ì>ÃÃ° 
ÕÀÌ ÌiÀ>Ì> 
viÀiVi  Üi`}i	>Ãi` Ìi}iÌ }iiÀ} -ÞÃÌiÃ
>` ««i` /iV}iÃ] «>}iÃ ÓÈÇqÓÇä] Óäää°

Ó£È
,iviÀiViÃ
£Èx° *° 7ÕÃV >` ° Àâ}iÀ° ,i>Ìi ÛÃÕ> ÌÀ>V} v Î LiVÌÃ ÜÌ `Þ>V
>`} v VVÕÃ°  *ÀV°  ÌiÀ>Ì> 
viÀiVi  ,LÌVÃ >` Õ
Ì>Ì] «>}iÃ ÓnÈnqÓnÇÎ] £Ç°
£ÈÈ° ° 6>`i 7Þ}>iÀ` >` ° 6> ° 
>ÀÃi Ài}ÃÌÀ>Ì v ÃÕÀv>Vi «>ÌViÃ ÜÌ V>
ÃÞiÌÀiÃ°  *ÀV° ÇÌ ÕÀ«i> 
viÀiVi  
«ÕÌiÀ 6Ã] «>}iÃ xÇÓqxnÈ]
ÓääÓ°
£ÈÇ° ° -° 9>} >` ° 
° >° iÌiÀ>Ì v Ìi `iÌÌÞ] «ÃÌ >` ÀiÌ>Ì v Ìi
Ì«ÃÌ LiVÌ  > «i\ -i vÕÌiÀ iÝ«iÀiÌÃ°  *ÀV° £nÈ  ÌiÀ>Ì>

viÀiVi  ,LÌVÃ >` ÕÌ>Ì] ÛÕi £] «>}iÃ ÓÎqÓn] £nÈ°
£Èn° 9° 9] ° ->>Ì] >` /° 9Ã>Ü>° 6Ã>`i` LiVÌ >«Õ>Ì LÞ
> ÕÌw}iÀi` >` ÜÌ ÃvÌ w}iÀÌ«Ã°  *ÀV°  ÌiÀ>Ì> 
viÀiVi 
,LÌVÃ >` ÕÌ>Ì] «>}iÃ ÎÓä£qÎÓän] £°

`iÝ
>VÌÛi ÃiÃ}] Î£
>VÌÛi ÛÃ] Ç] £nqÓ£] £Î£] £ÇÇ
>ÀyÜ ÃiÃÀ] £ÈÓ] £ÈÎ] £ÈÎ
>ÌÌÀLÕÌi` }À>«] ÇÓ] ÇÓ
	V«Ã ÃÌiÀi i>`] Óä] {È] {È] £{Ç] £{Ç]
£ÇÇq£Çn
LÌÃÌÀ>««}] xÓ

 `iÃ] Ó{] xn
V>iÀ> V>LÀ>Ì >ÌÀÝ] £Ç
VÕÀ wÌiÀ}] Î] Óq{] £Î£] £ÎÓ] £{
V«ÕÌiÀ ÛÃ] Î] ÓÓqÓx
ViVÌÛÌÞ] ÈÓ] {] £ÎÓ] £{
VÛiÝÌÞ] ÈÎ] ÈÈ] ÈÈqÈÇ] ÇÓ
VÛ>À>Vi >ÌÀÝ] Èn] £nÈ] £nÇ
VÕÀÛ>ÌÕÀi
>ÕÃÃ>] x] ÈÎ] nÓ
i>] x] ÈÎ] nÓ
«ÀV«>] x] ÈÓqÈÈ
i`}i `iÌiVÌ] £n] {n] È] {qÇ] £{
i}Ì] Ó£] nÈ] £ä
ÕiÀ >}iÃ] £Îq£x] Ó] £Ó
vi>ÌÕÀi `iÌiVÌ] n] n
>ÕÃÃ> VÕÀÛ>ÌÕÀi] Ãii VÕÀÛ>ÌÕÀi
>ÕÃÃ> >}i] Èä] xqÈ£] ÈÎ] È{] Çä
iÝÌi`i` >ÕÃÃ> >}i] x
>ÕÃÃ> Ã«iÀi] x
iiÀ>âi` 
Þ`iÀÃ] Óx] xn
}iiÌÀV «ÀÌÛiÃ] Óx] xÇ] xn] nÎ
wÌÌ} Ì À>}i `>Ì>] ÈÇqÇ£
}À>Ã«
«>}] ÓÎ] £x£] £qÓäÎ
«ÜiÀ] £x£] £xÓ] Óä£
«ÀiVÃ] £x£] £xÓ] £
ÃÌ>LÌÞ] £x£] £x{
ÛiÀwV>Ì] £Çä
}iiÕÃ ÛiVÌÀÃ] ££q£Î
Õ>>Vi ÌiÀ>VÌ] {] £xä] £xx
Õ>` ÀLÌÃ] Ó] ÓqÎ] £{Ç
ÞÃÌiÀiÃÃ] £x
>}i }À>`iÌ] x
ÌiÀ«>Ì] {
>> wÌiÀ] Ãii ÌÀ>V}
i>ÌV VÌÀ] Óx] ££x
i>ÌV `i] ÓÇ] ££Ç] £ÓÈq£ÓÇ] Óä{
>ÌiVÞ] £ä] £x
iÛiLiÀ}>ÀµÕ>À`Ì >}ÀÌ] {Ó
iÛiLiÀ}>ÀµÕ>À`Ì >}ÀÌ] {x] ÈnqÇä]
£Î{
}Ì ÃÌÀ«i ÃV>}] Ç] ÎÎ] Î£qxÈ] £{Ç] £xÎ
V>LÀ>Ì] {{q{È
iÀÀÀ >>ÞÃÃ] ÎxqÎÈ] xÓqx{
ÀLÕÃÌiÃÃ] ÎÎqÎx] {qxÓ] £x{
i> VÕÀÛ>ÌÕÀi] Ãii VÕÀÛ>ÌÕÀi
 *
] Ãii iÛiLiÀ}>ÀµÕ>À`Ì
>}ÀÌ
8] £{
À«}V> wÌiÀ}] {Ç] £ÎÓ] £{

Ó£n
`iÝ
À>] Ãii VÛiÝÌÞ] Ãii >ÕÃÃ> >}i
v > i] x
v > «>i] Èn
v À>}i «>ÌV] È£
LiVÌ V>ÃÃwV>Ì] Ç] Óx] xn] Ç£qÇÎ] £xÎ]
£Çx
LiVÌ `i}] Ó{] xÇ] xn
«Þ}> `iÃ] ÇÎqÇx] n
LiVÌ ÀiV}Ì] Ç] ÓÎqÓx
LÃÌ>Vi >Û`>Vi] ££Ç] £x£] ÓäÎ
VVÕÃÃ] ÇÇ] È] £äxq£än] ££Ç] £Înq£Î
`ÕÀ ÃiÃÀ] £ÈÓ] £È{q£Èx
ÀiÌ>Ì ÃÌ}À>] Èä
ÕÌiÀ ÀiiVÌ] {x] {n] {
«i V>iÀ>] £È] £ÈqÓÓ] {äq{Ó] £nä] £n
*dÕViÀ >ÌÀÝ] £Ç
«Ãi] £x] £] £Ón
iÃÌ>Ì] £Ó£q£ÓÈ] £Îq£{] Ãii
ÌÀ>V}
«ÀV«> VÕÀÛ>ÌÕÀi] Ãii VÕÀÛ>ÌÕÀi
«ÀiVÌ >ÌÀÝ] £Ç] Ó£
«ÃiÕ`ÛiÀÃi] ÓÓ] £n£
«ÀiVÌÛi ÀiVÌwV>Ì] ÓäqÓÓ] £ÓÎ] ££
*Õ> ÀLÌ >À] £{Ç] £{n
i>ÌV `i] Óä{
µÕ>ÌiÀÃ] £Îq£x] Ó] £Ó
À>`> `ÃÌÀÌ] £n] £Çq£n
À>}i `>Ì>] Ãii }Ì ÃÌÀ«i ÃV>}
VÕÀ Ài}ÃÌÀ>Ì] ÎÓ] {
VÀi>ÃiÃ] È£] ÇÇ
`ÃVÌÕÌiÃ] {] È£] ÇÇ
Ãi}iÌ>Ì] xn] È£qÈÓ] Çxqnä
Ài>Ìi «iÀvÀ>Vi] È] £È
ÀiVÌi>À ÃÌiÀi] Ó£] {ä] £ÓÎ] £nä] ££
Ài«ÀiVÌ iÀÀÀ] £ÓÓ] £Î
ÀLÌ ÛÃ] Ãii V«ÕÌiÀ ÛÃ
Ãi}iÌ>Ì] x] ÓÎ] Ãii À>}i `>Ì>
Ài} }ÀÜ}] x] ÈÓ] nÓ
Ã«Ì>`iÀ}i] x
--É--Ó] £{
ÃÌiÀi ÀiVÃÌÀÕVÌ] £] £] Ó] £Ó£] £ÓÎ]
£nq£Ó
ÃÕÀv>Vi ÌÞ«i] ÈÎ] xqÈÇ
VÕÀÛ>ÌÕÀi ÛÃ° «>À>iÌÀV] näqnÓ
«>À>iâ>Ì] £{
Ü`Ü Ãâi] nÎ
Ì>Ã
«>}] £x£] £qÓäÎ
Ã«iVwV>Ì] x] £xä
Ìi«>Ìi >ÌV}] n
ÌiÝÌÕÀi VÕiÃ] n] Çq£ä£
ÌÀ>V}
>VÌÛi VÕiÃ] £Ó£] £Óäq£Ó£] £Î{
>> wÌiÀ] Ó] £qÓ] £ÓÇq£Îä]
£nxq£nÇ
ÕÌVÕi] nxq££Î
ÃiiVÌ ÛÃ° Ìi}À>Ì] nÇ
«>ÀÌVi wÌiÀ] ££Î
ÀLÕÃÌiÃÃ] nx] nÇ] £Óä] £ÓÇ] £ÇÎ
ÌÀ>iVÌÀÞ «>}] £x£] £xÓ] ÓäÎ] ÓäÎ
ÛiVÌÞ ÃVÀiÜ] £x] £] ££
ÛÃÕ> ÃiÀÛ}] ÓÈqÎä] ££Èq££n] ££
>VVÕÀ>VÞ] ÓÇ] Ó] ££È] £Óä] £ÎÈq£ÎÇ
V>LÀ>Ì] £ÓÎq£ÓÇ] £Î£] £Î{] £Îq£{Ó
VÌÀiÀ `iÃ}] ££nq£Óä
`Þ>V ivviVÌÃ] £È
ÞLÀ` «ÃÌL>Ãi`] ££xq£{{] £x{
«ÃÌL>Ãi` ÛÃ° >}iL>Ãi`] Ón
ÀLÕÃÌiÃÃ] ££Ç] £Óä] £Înq£Î


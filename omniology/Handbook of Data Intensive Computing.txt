
Handbook of Data Intensive Computing


Borko Furht • Armando Escalante
Editors
Handbook of Data Intensive
Computing
123

Editors
Borko Furht
Department of Computer
and Electrical Engineering
and Computer Science
Florida Atlantic University
Boca Raton, Florida
USA
bfurht@fau.edu
Armando Escalante
LexisNexis
Boca Raton, Florida
USA
armando.escalante@lexisnexis.com
ISBN 978-1-4614-1414-8
e-ISBN 978-1-4614-1415-5
DOI 10.1007/978-1-4614-1415-5
Springer New York Dordrecht Heidelberg London
Library of Congress Control Number: 2011941878
© Springer Science+Business Media, LLC 2011
All rights reserved. This work may not be translated or copied in whole or in part without the written
permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York,
NY 10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use in
connection with any form of information storage and retrieval, electronic adaptation, computer software,
or by similar or dissimilar methodology now known or hereafter developed is forbidden.
The use in this publication of trade names, trademarks, service marks, and similar terms, even if they are
not identiﬁed as such, is not to be taken as an expression of opinion as to whether or not they are subject
to proprietary rights.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

Preface
This handbook is carefully edited book – contributors are worldwide experts in
the ﬁeld of data intensive computing and their applications. The scope of the
book includes leading edge data intensive computing architectures and systems,
innovative storage, virtualization, and parallel processing technologies applied in
data intensive computing, and a variety of data intensive applications.
Data intensive computing refers to capturing, managing, analyzing, and under-
standing data at volumes and rates that push the frontiers of current technologies.
The challenge of data intensive computing is to provide the hardware architectures
and related software systems and techniques which are capable of transforming
ultra-large data into valuable knowledge. Data intensive computing demands a fun-
damentally different set of principles than mainstream computing. Data-intensive
applications typically are well suited for large-scale parallelism over the data and
also require extremely high degree of fault-tolerance, reliability, and availability.
In addition, most data intensive applications require real-time or near real-time
response. The objective of the project is to introduce the basic concepts of data
intensive computing, technologies and hardware and software techniques applied in
data intensive computing, and current and future applications.
The handbook comprises of four parts, which consist of 30 chapters. The ﬁrst part
on Architectures and Systems includes chapters dealing with network architectures
for data intensive computing, data intensive software systems, and high-level
programming languages and storage systems for data-intensive computing. The
second part on Technologies and Techniques covers load balancing techniques,
linking technologies, virtualization techniques, feature ranking methods and other
techniques applied in data intensive computing. The third part on Security includes
various aspects on privacy and security requirements and related techniques applied
in data intensive computing. The fourth part on Applications describes various data
intensive applications from earthquake simulations and geosciences to biological
systems, social information systems, and bioinformatics.
With the dramatic growth of data intensive computing and systems and their
applications, this handbook can be the deﬁnitive resource for persons working in
this ﬁeld as researchers, scientists, programmers, engineers, and users. The book is
v

vi
Preface
intended for a wide variety of people including academicians, designers, developers,
educators, engineers, practitioners, and researchers and graduate students. This book
can also be beneﬁcial for business managers, entrepreneurs, and investors. The book
can have a great potential to be adopted as a textbook in current and new courses on
Data Intensive Computing.
The main features of this handbook can be summarized as:
1. The handbook describes and evaluates the current state-of-the-art in a new ﬁeld
of data intensive computing.
2. It also presents current systems, services, and main players in this explosive ﬁeld.
3. Contributors to the handbook are the leading researchers from academia and
practitioners from industry.
We would like to thank the authors for their contributions. Without their expertise
and effort this handbook would never come to fruition. Springer editors and staff
also deserve our sincere recognition for their support throughout the project.
Editors-in-Chief
Borko Furht
Boca Raton, Florida
Armando Escalante

About the Editors-in-Chief
Borko Furht is a professor and chairman of the Department of Electrical &
Computer Engineering and Computer Science at Florida Atlantic University (FAU)
in Boca Raton, Florida. He is also director of recently formed NSF-sponsored
Industry/University Cooperative Research Center on Advanced Knowledge En-
ablement. Before joining FAU, he was a vice president of research and a senior
director of development at Modcomp (Ft. Lauderdale), a computer company of
Daimler Benz, Germany, a professor at University of Miami in Coral Gables,
Florida, and a senior researcher in the Institute Boris Kidric-Vinca, Yugoslavia.
Professor Furht received Ph.D. degree in electrical and computer engineering from
the University of Belgrade. His current research is in multimedia systems, video
coding and compression, 3D video and image systems, wireless multimedia, and
Internet and cloud computing. He is presently Principal Investigator and Co-PI
vii

viii
About the Editors-in-Chief
of several multiyear, multimillion dollar projects including NSF PIRE project and
NSF High-Performance Computing Center. He is the author of numerous books and
articles in the areas of multimedia, computer architecture, real-time computing, and
operating systems. He is a founder and editor-in-chief of the Journal of Multimedia
Tools and Applications (Springer). He has received several technical and publishing
awards, and has consulted for many high-tech companies including IBM, Hewlett-
Packard, Xerox, General Electric, JPL, NASA, Honeywell, and RCA. He has also
served as a consultant to various colleges and universities. He has given many
invited talks, keynote lectures, seminars, and tutorials. He served on the Board of
Directors of several high-tech companies.
Armando J. Escalante is Senior Vice President and Chief Technology Ofﬁcer of
Risk Solutions for the LexisNexis Group, a division of Reed Elsevier. In this
position, Escalante is responsible for technology development, information systems
and operations. Previously, Escalante was Chief Operating Ofﬁcer for Seisint, a
privately owned company, which was purchased by LexisNexis in 2004. In this
position, he was responsible for Technology, Development and Operations. Prior
to 2001, Escalante served as Vice President of Engineering and Operations for
Diveo Broadband Networks where he led world class Data Centers located in the
U.S. and Latin America. Before Diveo Broadband Networks, Escalante was VP
for one of the fastest growing divisions of Vignette Corporation, an eBusiness
software leader. Escalante earned his bachelors in electronic engineering at the
USB in Caracas, Venezuela and a master’s degree in computer science from Stevens
Institute of Technology as well as a master’s in business administration from West
Coast University.

Contents
Part I
Architectures and Systems
1
High Performance Network Architectures for Data
Intensive Computing .......................................................
3
Geng Lin and Eileen Liu
2
Architecting Data-Intensive Software Systems..........................
25
Chris A. Mattmann, Daniel J. Crichton, Andrew F. Hart,
Cameron Goodale, J. Steven Hughes, Sean Kelly,
Luca Cinquini, Thomas H. Painter, Joseph Lazio,
Duane Waliser, Nenad Medvidovic, Jinwon Kim,
and Peter Lean
3
ECL/HPCC: A Uniﬁed Approach to Big Data ..........................
59
Anthony M. Middleton, David Alan Bayliss,
and Gavin Halliday
4
Scalable Storage for Data-Intensive Computing ........................ 109
Abhishek Verma, Shivaram Venkataraman, Matthew Caesar,
and Roy H. Campbell
5
Computation and Storage Trade-Off
for Cost-Effectively Storing Scientiﬁc
Datasets in the Cloud....................................................... 129
Dong Yuan, Yun Yang, Xiao Liu, and Jinjun Chen
Part II
Technologies and Techniques
6
A Survey of Load Balancing Techniques for Data Intensive
Computing................................................................... 157
Zhiquan Sui and Shrideep Pallickara
ix

x
Contents
7
Resource Management for Data Intensive Clouds
Through Dynamic Federation: A Game Theoretic Approach ......... 169
Mohammad Mehedi Hassan and Eui-Nam Huh
8
Salt: Scalable Automated Linking Technology
for Data-Intensive Computing ............................................ 189
Anthony M. Middleton and David Alan Bayliss
9
Parallel Processing, Multiprocessors and Virtualization
in Data-Intensive Computing.............................................. 235
Jonathan Burger, Richard Chapman, and Flavio Villanustre
10
Challenges in Data Intensive Analysis at Scientiﬁc
Experimental User Facilities .............................................. 249
Kerstin Kleese van Dam, Dongsheng Li, Stephen D. Miller,
John W. Cobb, Mark L. Green, and Catherine L. Ruby
11
Large-Scale Data Analytics Using Ensemble Clustering ............... 285
Martin Hahmann, Dirk Habich, and Wolfgang Lehner
12
Speciﬁcation of Data Intensive Applications with Data
Dependency and Abstract Clocks......................................... 323
Abdoulaye Gamati´e
13
Ensemble Feature Ranking Methods for Data Intensive
Computing Applications ................................................... 349
Wilker Altidor, Taghi M. Khoshgoftaar, Jason Van Hulse,
and Amri Napolitano
14
Record Linkage Methodology and Applications ........................ 377
Ling Qin Zhang
15
Semantic Wrapper: Concise Semantic Querying
of Legacy Relational Databases ........................................... 415
Naphtali Rishe, Borko Furht, Malek Adjouadi,
Armando Barreto, Debra Davis, Ouri Wolfson,
Yelena Yesha, and Yaacov Yesha
Part III
Security
16
Security in Data Intensive Computing Systems ......................... 447
Eduardo B. Fernandez
17
Data Security and Privacy in Data-Intensive Computing Clusters ... 467
Flavio Villanustre and Jarvis Robinson
18
Information Security in Large Scale Distributed Systems ............. 485
Salvatore Distefano and Antonio Puliaﬁto

Contents
xi
19
Privacy and Security Requirements of Data Intensive
Computing in Clouds....................................................... 501
Arash Nourian and Muthucumaru Maheswaran
Part IV
Applications
20
On the Processing of Extreme Scale Datasets in the Geosciences ..... 521
Sangmi Lee Pallickara, Matthew Malensek,
and Shrideep Pallickara
21
Parallel Earthquake Simulations on Large-Scale
Multicore Supercomputers ................................................ 539
Xingfu Wu, Benchun Duan, and Valerie Taylor
22
Data Intensive Computing: A Biomedical Case Study
in Gene Selection and Filtering ........................................... 563
Michael Slavik, Xingquan Zhu, Imad Mahgoub,
Taghi Khoshgoftaar, and Ramaswamy Narayanan
23
Design Space Exploration for Efﬁcient Data Intensive
Computing on SoCs ........................................................ 581
Rosilde Corvino, Abdoulaye Gamati´e, and Pierre Boulet
24
Information Quality and Relevance in Large-Scale
Social Information Systems................................................ 617
Munmun De Choudhury
25
Geospatial Data Management with Terraﬂy............................. 637
Naphtali Rishe, Borko Furht, Malek Adjouadi,
Armando Barreto, Evgenia Cheremisina, Debra Davis,
Ouri Wolfson, Nabil Adam, Yelena Yesha,
and Yaacov Yesha
26
An Application for Processing Large and Non-Uniform
Media Objects on MapReduce-Based Clusters.......................... 667
Rainer Schmidt and Matthias Rella
27
Feature Selection Algorithms for Mining High
Dimensional DNA Microarray Data...................................... 685
David J. Dittman, Taghi M. Khoshgoftaar, Randall Wald,
and Jason Van Hulse
28
Application of Random Matrix Theory
to Analyze Biological Data................................................. 711
Feng Luo, Pradip K. Srimani, and Jizhong Zhou

xii
Contents
29
Keyword Search on Large-Scale Structured,
Semi-Structured, and Unstructured Data ............................... 733
Bin Zhou
30
A Distributed Publish/Subscribe System for Large Scale
Sensor Networks ............................................................ 753
Masato Yamanouchi, Ryota Miyagi, Satoshi Matsuura,
Satoru Noguchi, Kazutoshi Fujikawa, and Hideki Sunahara
Index ............................................................................... 777

Contributors
Nabil Adam U.S. Department of Homeland Security (DHS.gov), Washington DC,
USA
Malek Adjouadi NSF Industry-University Cooperative Research Center for
Advanced Knowledge, Enablement (CAKE.ﬁu.edu) at Florida International
University, Miami, Florida, USA
Wilker Altidor FAU, Boca Raton, FL, USA
Armando Barreto NSF Industry-University Cooperative Research Center for
Advanced Knowledge, Enablement (CAKE.ﬁu.edu) at Florida International
University, Miami, Florida, USA
David Alan Bayliss LexisNexis, Boca Raton, FL, USA
Pierre Boulet LIFL/CNRS and Inria, Parc Scientiﬁque de la Haute Borne,
Villeneuve d’Ascq, France
Jonathan Burger LexisNexis Risk Solutions, LexisNexis, Alpharetta, Georgia,
USA
Matthew Caesar Department of Computer Science, University of Illinois at
Urbana-Champaign, Urbana, IL, USA
Roy H. Campbell Department of Computer Science, University of Illinois at
Urbana-Champaign, Urbana, IL, USA
Richard Chapman LexisNexis Risk Solutions, LexisNexis, Alpharetta, Georgia,
USA
Jinjun Chen Faculty of Engineering and Information Technology, University of
Technology, Sydney, NSW, Australia
Evgenia Cheremisina NSF Industry-University Cooperative Research Center for
Advanced Knowledge, Enablement (CAKE.ﬁu.edu) at Florida International, Florida
Atlantic and Dubna University, Moscow, Russia
xiii

xiv
Contributors
Munmun De Choudhury Rutgers University, New Brunswick, NJ, USA
Luca Cinquini Instrument and Science Data Systems, NASA Jet Propulsion
Laboratory, California Institute of Technology, Pasadena, CA, USA
John W. Cobb Data Systems Group, Neutron Scattering Science Division, Oak
Ridge National Laboratory, Oak Ridge, TN, USA
Rosilde Corvino University of Technology Eindhoven, Eindhoven, AZ, The
Netherlands
Daniel J. Crichton Instrument and Science Data Systems, NASA Jet Propulsion
Laboratory, California Institute of Technology, Pasadena, CA, USA
Kerstin Kleese van Dam Fundamental and Computational Science Department,
Paciﬁc Northwest National Laboratory, Richland, WA, USA
Debra Davis NSF Industry-University Cooperative Research Center for Advanced
Knowledge, Enablement (CAKE.ﬁu.edu) at Florida International University,
Miami, Florida, USA
Salvatore Distefano Dipartimento di Matematica, Universit`a di Messina, Contrada
Papardo, S. Sperone, Messina, Italy
David J. Dittman FAU, Boca Raton, FL, USA
Benchun Duan Department of Geology & Geophysics, Texas A&M University,
College Station, TX, USA
Eduardo B. Fernandez Department of Computer & Electrical Engineering and
Computer Science, Florida Atlantic University, Boca Raton, FL, USA
Kazutoshi Fujikawa Graduate School of Information Science, Nara Institute of
Science and Technology 8916-5, Takayama-cho, Ikoma-shi, Nara, Japan
Borko Furht NSF Industry-University Cooperative Research Center for Advanced
Knowledge, Enablement (CAKE.ﬁu.edu) at Florida International, Florida Atlantic
and Dubna Universities, Boca Raton, Florida, USA
Abdoulaye Gamati´e LIFL/CNRS and Inria, Parc Scientiﬁque de la Haute Borne,
Villeneuve d’Ascq, France
Cameron Goodale Instrument and Science Data Systems, NASA Jet Propulsion
Laboratory, California Institute of Technology, Pasadena, CA, USA
Mark L. Green Systems Integration Group, Tech-X Corporation, Williamsville,
NY, USA
Dirk Habich Dresden University of Technology, Database Technology Group,
Dresden, Germany
Martin Hahmann Dresden University of Technology,Database Technology Group,
Dresden, Germany

Contributors
xv
Gavin Halliday LexisNexis, Boca Raton, FL, USA
Andrew F. Hart Instrument and Science Data Systems, NASA Jet Propulsion
Laboratory, California Institute of Technology, Pasadena, CA, USA
Mohammad Mehedi Hassan Department of Computer Engineering, Kyung Hee
University, South Korea
J. Steven Hughes Instrument and Science Data Systems, NASA Jet Propulsion
Laboratory, California Institute of Technology, Pasadena, CA, USA
Eui-Nam Huh Department of Computer Engineering, Kyung Hee University,
South Korea
Jason Van Hulse FAU, Boca Raton, FL, USA
Sean Kelly Instrument and Science Data Systems, NASA Jet Propulsion
Laboratory, California Institute of Technology, Pasadena, CA, USA
Taghi M. Khoshgoftaar Department of Computer & Electrical Engineering and
Computer Science, Florida Atlantic University, Boca Raton, FL, USA
Jinwon Kim Joint Institute for Regional Earth System Science and Engineering
(JIFRESSE), University of California, Los Angeles, Los Angeles, CA, USA
Joseph Lazio Instrument and Science Data Systems, NASA Jet Propulsion
Laboratory, California Institute of Technology, Pasadena, CA, USA
Peter Lean Department of Meteorology, University of Reading, Reading, UK
Wolfgang Lehner Dresden University of Technology,Database Technology Group
Dongsheng Li Fundamental and Computational Science Department, Paciﬁc
Northwest National Laboratory, Richland, WA, USA
Geng Lin Dell, IBM Alliance Cisco Systems
Eileen Liu Nominum, Inc., Wyse Technology, San Jose, California, USA
Xiao Liu Faculty of Information and Communication Technologies, Swinburne
University of Technology, Melbourne, Australia
Feng Luo School of Computing, Clemson University, Clemson, SC, USA
Muthucumaru Maheswaran McGill University, Montreal, Canada
Imad Mahgoub Department of Computer & Electrical Engineering and Computer
Science, Florida Atlantic University, Boca Raton, FL, USA
Matthew Malensek Department of Computer Science, Colorado State University,
Fort Collins, CO, USA
Satoshi Matsuura Graduate School of Information Science, Nara Institute of
Science and Technology 8916-5, Takayama-cho, Ikoma-shi, Nara, Japan

xvi
Contributors
Chris A. Mattmann Instrument and Science Data Systems, NASA Jet Propulsion
Laboratory, California Institute of Technology, Pasadena, CA, USA
Nenad
Medvidovic Computer
Science
Department,
Viterbi
School
of
Engineering, University of Southern California, Los Angeles, CA, USA
Anthony M. Middleton LexisNexis, Boca Raton, FL, USA
Stephen D. Miller Data Systems Group, Neutron Scattering Science Division, Oak
Ridge National Laboratory, Oak Ridge, TN, USA
Ryota Miyagi Graduate School of Information Science, Nara Institute of Science
and Technology 8916-5, Takayama-cho, Ikoma-shi, Nara, Japan
Amri Napolitano FAU, Boca Raton, FL, USA
Ramaswamy Narayanan Charles E. Schmidt College of Science, Florida Atlantic
University, Boca Raton, FL, USA
Satoru Noguchi Graduate School of Information Science, Nara Institute of
Science and Technology 89165, Takayama-cho, Ikoma-shi, Nara, Japan
Arash Nourian McGill University, Montreal, Canada
Thomas H. Painter Instrument and Science Data Systems, NASA Jet Propulsion
Laboratory, California Institute of Technology, Pasadena, CA, USA
Sangmi Lee Pallickara Department of Computer Science, Colorado State
University, Fort Collins, CO, USA
Shrideep Pallickara Department of Computer Science, Colorado State University,
Fort Collins, CO, USA
Makan Pourzandi Ericsson, Mississauga, Canada
Antonio Puliaﬁto Dipartimento di Matematica, Universit`a di Messina, Contrada
Papardo, S. Sperone, Messina, Italy
Matthias Rella Austrian Institute of Technology, Donau-City-Strasse 1, Vienna,
Austria
Naphtali Rishe NSF Industry-University Cooperative Research Center for
Advanced Knowledge, Enablement (CAKE.ﬁu.edu) at Florida International
University, Miami, Florida, USA
Jarvis Robinson LexisNexis, Alpharetta, GA, USA
Catherine
L.
Ruby Systems
Integration
Group,
Tech-X
Corporation,
Williamsville, NY, USA
Rainer Schmidt Austrian Institute of Technology, Donau-City-Strasse 1, Vienna,
Austria

Contributors
xvii
Michael Slavik Department of Computer and Electrical Engineering and
Computer Science, Florida Atlantic University, Boca Raton, FL, USA
Pradip K. Srimani School of Computing, Clemson University, Clemson, SC,
USA
Zhiquan Sui Department of Computer Science, Colorado State University, Fort
Collins, CO, USA
Hideki Sunahara Graduate School of Media Design, Keio University Kouhoku-ku,
Yokohama, Kanagawa, Japan
Valerie Taylor Department of Computer Science and Engineering, Texas A&M
University College Station, TX, USA
Shivaram Venkataraman Department of Computer Science, University of Illinois
at Urbana-Champaign, Urbana, IL, USA
Abhishek Verma Department of Computer Science, University of Illinois at
Urbana-Champaign, Urbana, IL, USA
Flavio Villanustre LexisNexis Risk Solutions, LexisNexis, Alpharetta, Georgia,
USA
Randall Wald FAU, Boca Raton, FL, USA
Duane Waliser Instrument and Science Data Systems, NASA Jet Propulsion
Laboratory, California Institute of Technology, Pasadena, CA, USA
Ouri Wolfson Computational Transportation Science Program (CTS.cs.uic.edu),
University of Illinois at Chicago, USA
Xingfu Wu Department of Computer Science & Engineering, Institute for Applied
Mathematics and Computational Science, Texas A&M University, College Station,
TX, USA
Masato Yamanouchi Graduate School of Media Design, Keio University
Kouhoku-ku, Yokohama, Kanagawa, Japan
Yun Yang Faculty of Information and Communication Technologies, Swinburne
University of Technology, Melbourne, Australia
Yaacov
Yesha NSF
Industry-University
Cooperative
Research
Center
for
Multicore Productivity Research (CHMPR.umbc.edu) at the University of
Maryland Baltimore County, Baltimore, Mayland, USA
Yelena
Yesha NSF
Industry-University
Cooperative
Research
Center
for
Multicore Productivity, Research (CHMPR.umbc.edu) at the University of
Maryland Baltimore County, Baltimore, Maryland, USA
Dong Yuan Faculty of Information and Communication Technologies, Swinburne
University of Technology, Melbourne, Australia

xviii
Contributors
Ling Qin Zhang LexisNexis Risk Solutions, Boca Raton, FL, USA
Bin Zhou Department of Information Systems, University of Maryland, Baltimore
County (UMBC), Baltimore, USA
Jizhong Zhou Institute for Environmental Genomics, University of Oklahoma,
Norman OK, USA
Xingquan Zhu Department of Computer and Electrical Engineering and Computer
Science, Florida Atlantic University, Boca Raton, FL, USA
Centre for Quantum Computation and Intelligent Systems, University of
Technology, Sydney, NSW, Australia

Part I
Architectures and Systems


Chapter 1
High Performance Network Architectures
for Data Intensive Computing
Geng Lin and Eileen Liu
1
Introduction
Data Intensive Computing is characterized by problems where data is the primary
challenger, whether it is the complexity, size, or rate of the data acquisition.
The hardware platform required for a data intensive computing environment
consists of tens, sometimes even hundreds, of thousands of compute nodes with
their corresponding networking and storage subsystems, power distribution and
conditioning equipment, and extensive cooling systems. An essential requirement
for processing exploding volumes of data is to move processing and analysis to data,
where possible, rather than data to processing and analysis [1]. It is also critical to
maximize the parallelism over the data and the efﬁciency of data movement between
discrete devices in a network.
This chapter focuses on the networking aspect in the data intensive computing
environment. The chapter is organized as follows. In Sect. 2, we discuss the different
applications of data intensive computing and their unique requirements on the
networks and the storage systems. In Sect. 3, we discuss the characteristics of the
storage architecture of data intensive computing. In Sect. 4, we focus on the network
architecture of the data intensive computing environment. In Sect. 5, we discuss our
conclusions and highlight the directions for future work in data intensive computing
network architectures.
G. Lin ()
Dell, IBM Alliance Cisco Systems
e-mail: gelin@cisco.com
E. Liu
Nominum, Inc.
e-mail: eileen.liu@nominum.com
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 1, © Springer Science+Business Media, LLC 2011
3

4
G. Lin and E. Liu
2
Data Intensive Computing Applications and the Network
Enormous digital data abound in all facets of our lives. According to IDC [2], the
size of the data universe has grown from 800,000 petabytes in 2009 to 1.2 zettabytes
in 2010, a 62% increase in the amount of data. Recent development in Internet-scale
data applications and services, combined with the proliferation of cloud computing,
has created a new wave of data intensive computing applications. The pace of data
production will only accelerate with the proliferation of Internet-scale applications
in social networking (e.g., Facebook, Tencent), social gaming (e.g., Zynga), and
eCommerce (e.g., Amazon, eBay, Alibaba), the digitization of all forms of media
including voice, TV, radio and print, and the continuing assimilation of computing
into everyday life (e.g., projects in smart grids and smart cities). IDC has estimated
that by 2020, the amount of data will have grown 44-fold from 2009 to 35 zettabytes.
The broad availability of data coupled with increased capabilities and decreased
costs of both storage and computing technologies has led us to rethink how we will
manage this abundance of data. Data intensive computing requires a fundamentally
different set of principles than traditional mainstream computing. We will discuss
the different applications of data intensive computing and their unique requirements
on the network and the underlying storage system in this section.
2.1
Large-Scale Data Parallelism Applications
Many data intensive applications allow for large-scale parallelism over their data
sets. As such, they are well suited to run on systems where the computing platform
has built-in capabilities to manage parallelism and data access. Data mining appli-
cations such as behavioral analysis for leveraging historical user behavior to select
the ads most relevant to users to display [3] and topic modeling for discovering the
abstract “topic” that occur in a collection of documents [4] are typical large-scale
data parallelism applications. Apache Hadoop MapReduce [5] and the LexisNexia
HPCC (High-Performance Computing Cluster) [6, 7] are examples data-intensive
computing platforms for this type of applications.
Large-Scale data parallelism applications pose challenging demands on the
underlying data storage system and the network infrastructure, and require us to
re-examine the architectural relationship between the compute, the storage, and the
network sub-architectures.
First, the network and the storage system must be highly scalable. The network
must host multiple services. The trafﬁc of one service should not be affected by
the trafﬁc of any other service. Also, Internet brings a huge audience of potential
users, traditional databases such as Oracle, MySql and Postgres are difﬁcult to scale
to hundreds millions of users with need to access data sets ranging in size from
hundreds of terabytes to dozens of petabytes. A complete redesign of the underlying
network and storage infrastructure is needed [8,9].

1
High Performance Network Architectures for Data Intensive Computing
5
Second, network should be designed in accordance with the data-intensive
computing platform. The data-intensive platforms for large-scale data parallelism
applications usually need to partition the data into multiple segments, which can
then be processed independently using the same application program in parallel.
Then the resulting processed data will be reassembled to produce the completed
output data. Large amount of data may need to be moved between the distributed
processing nodes and the result assembling nodes. The underlying network must be
designed in such a way that it can maximize the bandwidth between nodes where
large amount of data movement may occur. Traditional network architecture and
supporting products, following the typical 3-tier hierarchical data center network
design with signiﬁcant bandwidth over subscription between the adjacent layers, do
not meet the needs of large-scale parallel applications which require signiﬁcant data
movement between peer compute nodes.
Finally, the network and the storage system must be ﬂexible and be managed
easily to support reliable network-aware data-intensive computing platform. Most
of existing job schedulers for data-intensive computing frameworks do not take
the underlying storage system and the available network bandwidth into consid-
eration. Their performance is highly dependent on the behavior of underlying
network and the software layers such as IO scheduler and native ﬁle system
allocation algorithm, which are designed for general-purpose workloads, not data
intensive computing. As such, they produce excessive disk seeks and fragmentation,
congest network links and degrade storage bandwidth signiﬁcantly. Therefore,
the data-intensive computing platform must be aware of network topology, net-
work bandwidth and native storage system so that it can distribute the parallel
processing jobs and move data more efﬁciently. Since no single storage device
can hold all of the necessary data, multiple systems need cooperate to provide
reliability. Administrators should be able to update network and storage system
conﬁguration without taking any useful data ofﬂine. The data-intensive computing
platform must be notiﬁed automatically when network topology or bandwidth are
changed [10].
2.2
Interactive Online Services
Another major category of modern data intensive applications is the interactive
online services offered in the form of cloud services [11]. There are many
cloud-based online service providers in place today, offering services ranging
from infrastructure-as-as-service, to development-platform-as-a-service, to special
purpose application-as-a-services such as email, collaborative documents and social
networking. Amazon EC2, Force.com, Google App Engine, and Facebook, are
among some of the best-known examples. The size of the data that these services
have to manage keeps growing every day. For example, for Web search services, the
Web is growing by millions of pages every day, which increases the cost of building
and serving a Web index.

6
G. Lin and E. Liu
Interactive online services pose new architecture demands on the underlying data
storage system and the network infrastructure.
First, the network and the storage system must have low latency. Interactive
online services must be responsive and the result of an update should be visible
to users immediately and durably. The low latency requirement poses signiﬁcant
challenges to today’s data center switching architecture, which is built on a multi-
layer hierarchical architecture topology. In such network architecture, it is not
uncommon to see the communications between some end points going through
many “hops” (sometime nine hops or more). This introduces signiﬁcant delay in
the switching latency between the end points.
Second, the network and storage system must guarantee a consistent view of
data across multiple data centers. As organizations become increasingly global
and users demand low-latency to their data, multiple data centers must now hold
copies of data to maximize reliability and availability. This means data must
be replicated from one data center to another. Replicating data across distant
datacenters while providing low latency is challenging, as is guaranteeing a con-
sistent view of replicated data, especially during faults. Traditional data replication
techniques, such as the tiered replication [12], are not sufﬁcient to support such
requirements.
Finally, the network and the storage system must achieve high availability;
users typically expect online services to be up 24  7. Achieving fault-free
operation on a large collection of hardware and system software is hard and is
made more difﬁcult by the large number of servers involved [13]. The computing
platform for such services must be designed to gracefully tolerate large numbers
of component faults with little or no impact on the service level performance and
availability.
3
Storage Architectures for Data Intensive Computing
Database clustering techniques and master/slave deployment are the traditional
approaches to scale storage and data processing. While successful in supporting
client-server applications of the past a few decades, these architectural approaches
have begun to reach computational, operational, and economic limits in the face
of today’s rapidly growing Internet-scale applications and data sets. Modern
data intensive computing has to leverage the power of tens of thousands of
processors to manipulate large amount of distributed data sets. Although data
is accessible over the network, data is stored on disks local to the processors
whenever possible. Data intensive applications typically can only be run in data
centers where they can take advantages of computation, storage, power, and
cooling resources on a large scale [13]. In this section, we focus on two typical
storage architectures for data intensive applications: the Hadoop and MegaStore
architectures.

1
High Performance Network Architectures for Data Intensive Computing
7
Fig. 1.1 Hadoop cluster architecture
3.1
Hadoop Storage Architecture
The Hadoop architecture has gained tremendous popularity in modern data intensive
application design [5]. Hadoop is an open source data intensive computing frame-
work that supports large-scale data parallelism applications. The Hadoop framework
is based on the MapReduce programming model, which was originally introduced
by Google in its search application design [14]. In the MapReduce programming
model, the application computation is divided into a map stage and a reduce stage.
In the map stage, the input data is split into independent chunks and assigned to
the map tasks for processing in parallel. In the reduce stage, the output from the
map stage is read and combined to produce the ﬁnal output. A Hadoop framework
consists of a cluster of master and slave nodes as shown in Fig. 1.1. The master
nodes are either a Name Node or a Job Tracker. The slave nodes usually act as
both Data Node and Task Tracker. The Hadoop Map/Reduce components consists
a single master Job Tracker and one Task Tracker per slave node. The Job Tracker
is responsible for scheduling the jobs’ tasks on the slaves, monitoring them and re-
executing the failed tasks. The Task Tracker accepts map and reduce tasks from the
Job Tracker and executes tasks as directed.
Hadoop Distributed File System (HDFS) is the primary distributed storage
system used by Hadoop application. It primarily consists of a Name Node that
manages the ﬁle system metadata and multiple Data Nodes that store the actual
data. Files in HDFS are divided into large blocks, typically 64 MB, and each block
is stored as a separate ﬁle in the Data Node’s local ﬁle system.
The Hadoop HDFS and the Map/Reduce components can take a node’s physical
location into account when scheduling tasks and allocating storage. Nodes are
arranged in racks and network trafﬁc between different nodes within the same rack
is much more desirable than network trafﬁc across the racks. Administrator can
decide which rack a node belongs to through conﬁguration. Typically the compute
nodes and the storage nodes are the same, that is, the Map/Reduce framework and
the Hadoop DFS are running on the same set of nodes. This conﬁguration allows the

8
G. Lin and E. Liu
Fig. 1.2 MegaStore data partition
framework to effectively schedule tasks on the nodes where data is already present,
resulting in a very high aggregate bandwidth across the cluster.
For reliability, HDFS implements an automatic replication system. By default,
the Name Node will store two replicas of each data block in different Data Nodes
in the same rack and a third replica in a Data Node in a different rack for better
fault tolerance. Thus, each Data Node typically is servicing both local and remote
clients simultaneously. HDFS replication is transparent to the client application.
When writing to a block, a replication pipeline is established among all replica Data
Nodes. The client only communicates with the ﬁrst Data Node, which echoes the
data to a second Data Node, and so on, until the desired number of replicas have
been created. The write operation is ﬁnished only when all nodes in this replication
pipeline have successfully copied all data to disk. Data Nodes periodically report
a list of all blocks stored to the Name Node, which will verify that each ﬁle is
sufﬁciently replicated and, in the case of failure, instruct Data Nodes to make
additional copies.
3.2
MegaStore Storage Architecture
Megastore [11] is a scalable storage system that supports interactive online services
with strong consistency guarantees and high availability. In this system, data are
partitioned into a collection of entity groups as shown in Fig. 1.2. Data and the
transaction log of the data in each group are stored in a non-relational NoSQL data
store and are replicated independently over multiple data centers in the same entity
group. All the network trafﬁc between datacenters is from replicated operations,
which are synchronous and consistent.
Users can initiate read/write operation from any node. ACID (Atomicity, Con-
sistency, Isolation, Durability) transactions are guaranteed for operations within

1
High Performance Network Architectures for Data Intensive Computing
9
Fig. 1.3 MegaStore architecture
one single entity group. Operation across multiple entity groups can either rely on
expensive two-phase commits for atomic updates or asynchronous messaging for
looser consistency required operations. Megastore gives applications ﬁne-grained
control over their data’s partitioning and locality. To minimize latency, applications
should try to keep data near users and replicas closed to each other.
A Megastore node can either be a full replica or a witness replica. A full replica
contains all the entity data, index data and log. A witness replica only contains
log. It does not apply the log and do not store entity data or indices. Figure 1.3
shows the key components of a Megastore instance. Each node has a Megastore
client library invoked by an application server, a replication server, and a coordinator
server (if the node is a full replica). Each application server has a designated local
replica and a set of remote replicas. A coordinator server coordinates the write
operations on a set of entity groups. It keeps track of the states of all the writes
on its local replica to ensure read operations can be served locally. Coordinators
must communicate with each other to identify whether other coordinators are up,
healthy, and reachable. A replication server is stateless and is responsible to service
the read/write operations on the local data store from remote.
Megastore’s replication system use majority votes to reach consensus among a
group of replicas on a single value. Thus, it requires a majority of replicas to be
active and reachable and allows up to F faults with 2FC1 replicas. Witness replicas
are effectively tiebreakers and are used when there are not enough full replicas to
form a quorum for the replication system.
4
Network Architectures for Data Intensive Computing
Most of today’s Internet-scale online services involve multiple data centers for their
data intensive computing tasks. In such an environment, there are two principal
areas in which the network architecture is of critical importance to the data intensive
application service: (1) a data center network that interconnects the infrastructure
resources (e.g., servers and storage devices) within a data center, (2) a data center

10
G. Lin and E. Liu
Fig. 1.4 Traditional data center network architecture
interconnect network that connects multiple data centers to support the scaling,
availability, the distribution of such online interactive services. In this section, we
shall discuss these two architectures separately as each one is facing a unique set of
architectural challenges and requires distinct architectural solutions.
4.1
Data Center Network
Modern online service providers offer scalable and highly available services via
massive data centers. In such massive-scale data centers, Data Center Network
(DCN) is constructed to connect tens, sometimes hundreds, of thousands of servers
to deliver massive data intensive computing services to the public.
4.1.1
Traditional Data Center Network Design
Today’s data center network architecture design can be traced back to the later 1990s
and early 2000s. It started to support typical enterprise-class business applications.
Such data center network design is based on the hierarchical networking model
[15,16]. Figure1.4 show a conceptual view of a hierarchical data center network as
well as an example of mapping the reference architecture to a physical data center
deployment.
The access layer of a data center network provides connectivity for server
resource pool residing in the data center. Design of the access layer is heavily
inﬂuenced by the decision criteria such as server density, form factor, and server

1
High Performance Network Architectures for Data Intensive Computing
11
virtualization that can result in higher interface count requirements. The commonly
used approaches for data center access layer connectivity are end-of-row (EoR)
switch, top-of-rack (ToR) switch, and integrated switch (typically in the form
of blade switches inside a modular blade server chassis). Another form of the
integrated switch is the software switch in a server hypervisor or embedded in the
server Converged Network Adapter (CNA) card. Each design has pros and cons,
and is dictated by server hardware and application requirements [17]. For example,
in the ToR design servers are connected to switches that are located within the
same or adjacent racks, and in which these switches are connected to aggregation
switches typically using horizontal ﬁber-optic cabling. In EoR design, server racks
are typically lined up side by side in a row with one or two EoR racks providing
network connectivity to the servers within that row. Each server rack has a bundle
of twisted pair copper cables routed to the EoR rack(s). EoR design delivers higher
level of switch/port utilization and provides more ﬂexibility to support a broad range
of servers than ToR design. On the other hand, ToR design provides simpler cable
management and faster port-to-port switching for servers within the rack than EoR
design.
The aggregation layer of the data center serves as a consolidation point where
access layer switches are connected to. It provides connectivity between servers
for multi-tier applications, as well as connectivity across the core of the network
to the clients residing within the campus, WAN, or Internet. The aggregation
layer typically provides the boundary between Layer-3 routed links and Layer-2
Ethernet broadcast domains in the data center. The access switches are connected
to the aggregation layer using 802.1Q VLAN trunks to provide the capability
of connecting servers belonging to different VLANs and IP subnets to the same
physical switch.
The primary function of the core layer in a data center network is to provide
highly available, high performance Layer-3 switching for IP trafﬁc between the
data center and the Telco’s Internet edge and backbone. In some situations,
multiple geographically distributed data centers owned by a service provider may
be connected via a private WAN or a Metropolitan Area Network (MAN). The
typical network topology for this kind of geographically distributed data centers
is Layer-3 Peering Routing between the data center core switches. By conﬁguring
all links connecting to the network core as point-to-point Layer-3 connections,
convergence around any link failure is provided, and the control plane of the core
switches is not exposed to broadcast trafﬁc from end node devices or required to
participate in STP for Layer-2 network loop prevention. Readers can refer to [15]
for more detailed descriptions of today’s data center network design.
There are a few key architectural characteristics that are intrinsically associated
with this hierarchical data center network architecture model. First, the notion of
layers – the data center network is conceptually divided into three layers (access,
aggregation, and core layers) although in real world deployment the physical
topology can be more than three layers. Second, the concept of tree-like topology –
the network linking the server and storage end points forms a tree-like physical
topology as well as a trafﬁc forwarding logical topology to facilitate the data

12
G. Lin and E. Liu
exchange between the end points. (We say “tree-like” topology because in real
world implementation, there are typically dual paths provided – one active and
one inactive – for high availability purpose.) In such tree-like network topology,
prevailing Ethernet techniques, such as ARP broadcast, VLAN grouping, and STP
path selection, etc., form the foundation for data switching among the end points
[15]. Third, the assumption of “over subscription” – the aggregated trafﬁc bandwidth
for the layer below – i.e., the layer closer to the server and storage end points – is
signiﬁcantly larger than that for the layer above. This assumption was based on the
networking characteristics of the software applications supported in the early day
data centers, which typically were multi-tier enterprise applications running in a
client-server architecture model (e.g., J2EE applications). In such model, the data
exchange between layers (e.g., the web presentation tier and the application logic
layer) is much less than among the same layer. Hence it is not unusual to see the
over subscription ratio of 1:5 to 1:10 between the uplinks and the downlinks in most
of today’s commercial L2/L3 switching products. With a multi-layer topology, it is
common to see the communication paths going through the highest levels of the tree
oversubscribed by factors over 1:100.
4.1.2
Challenges to the Traditional Data Center Network Architecture
While the traditional data center network architecture, along with today’s com-
mercial L2/L3 networking products, was successful in supporting the enterprise-
class client server applications, it is facing signiﬁcant challenges dealing with
the emergence of the Internet-scale data intensive computing paradigm. In such
environment, application workloads are dynamically distributed over large server
pools, sometimes consisting of hundreds of thousands of commodity servers. In
order to achieve high utilization and dynamic scalability, the data center network
in such environment must be able to support any workload to be assigned to any
server in an agile and elastic manner. While the modern application middleware
in a data intensive computing data center, e.g., Hadoop/MapReduce as discussed
in Sect. 3, allows dynamic workload assignment to the appropriate server nodes,
the traditional data center network architecture, however, lacks some of the major
architectural characteristics needed to support such dynamic resource allocation and
hence fails to support such workload agility and elasticity [18,19]. We shall discuss
these architectural characteristics in details below.
Bandwidth Uniformity
Today’s data center network architecture does not provide uniform bandwidth
capacity between servers inside a data center. This is largely due to the tree-like
forward topology and the over subscription factor we discussed in the previous
section. As a result, the higher a communications path between two server-peers
needs to go (up the tree-structure), the less bandwidth is allocated to this com-

1
High Performance Network Architectures for Data Intensive Computing
13
Table 1.1 Study of latency
in large data center network
Component
Delay
Round-trip
Network switch
10–30 s
100–300 s
Network interface card
2.5–32 s
10–128 s
OS network stack
15 s
60 s
Speed of light (in ﬁber)
5 ns/m
0.6–1:2 s
munications path. As a theoretical exercise, imagine a pair of servers whose
communications path needs to go through three layers of 1:5 over subscription.
The bandwidth allocated to this communication path could be as few as 1/125
of that of a pair who share the same access switch. In the traditional data
center environment, this problem was largely minimized by the careful (and
time-consuming) pre-planning and pre-conﬁguration of servers supporting the
workloads with strong communication afﬁnity under the same ToR or in the
same VLAN. The situation is dramatically changed in an Internet-scale data
intensive computing environment where a programming framework like Hadoop
dynamically allocates workloads among a very large server pool. In practice, the
lack of bandwidth uniformity associated with the traditional data center network
has caused the fragmentation of the server pool. On one hand, congestion and
computation hotspots are prevalent in some local clusters; on the other hand,
spare capacity is available elsewhere in the data center and not utilized. Hence
the network for the data intensive computing environment should be able to ensure
that “all end points are equal” from a server-to-server communications bandwidth
perspective.
(It is worth pointing out that the same challenge exists in an enterprise data center
environment today due to the introduction of hypervisors to enable dynamic virtual
machine mobility. The evolution of enterprise data center networking, however,
is beyond the scope of this chapter. We shall focus only on the data intensive
computing environment.)
Low Latency
High latency is one of the major performance bottlenecks that the traditional data
center network architecture imposes on data intensive computing. Studies show
that in large data centers with tens of thousands of servers, round-trip times are
typically 200–500 s and congestion can cause spikes up to tens of milliseconds
[20,21]. Table 1.1 (from [20]) shows the major components of latency in data centers
today with the network switching delay being the largest single contributor. The
main reason for the network delay is the large number of hops introduced by the
hierarchical data center network architecture.
High latency imposes many limitations on data intensive computing. For exam-
ple, because the lack of low latency guarantee from the underlining data center
network, Hadoop/MapReduce framework has to organize large scale applications
as a series of parallel stages where data is accessed sequentially in large blocks.

14
G. Lin and E. Liu
This dependence on sequential data access makes MapReduce difﬁcult to be used
for applications that require random data accesses (for example, machine learning
applications [22] that perform interactive interrogation of large unstructured data set
such as Web image searching and complex ﬁle selection).
Reliability and Utilization
The traditional data center network design uses Spanning Tree Protocol (STP)
to ensure a loop-free topology for the bridged Ethernet layer. STP is also used
to prevent bridge loops and ensure broadcast radiation across the network [23].
In many of today’s data center networks, spanning tree is also used to include
redundant links to provide automatic backup paths if one or more active links fail,
avoiding the danger of bridge loops or the need for manual enabling/disabling of
these backup links.
In this design, the resilience model is based on the 1:1 provisioning between
active/backup links. See Fig. 1.4 for an illustration of a sample topology. This
resilience model results in 50% of the uplinks in a data center network in the idle
stage in order to ensure enough capacity to deal with the failure of the active links.
Although techniques such as Link Aggregation (LAG) can be used to improve
the situation, LAG itself requires extra backbone bandwidth for the inter-switch
connections for the two virtual port channel switches involved. (See [15] for design
details.) Also link aggregation cannot be used to create a fully redundant data
center, as it does not protect against the failure of a single switch. A more efﬁcient
mechanism for maintaining network resiliency and link utilization is to create
multiple paths between switches and end points and leverage Equal Cost Multi-
Path (ECMP) selection to prevent the degradation caused by the failures of single or
multiple devices (links or entire switches). We will discuss ECMP in more details
in the next section.
4.1.3
Design Principles for Data Center Networks for Data Intensive
Computing
The evolution of networking technology and architecture to support large-scale
data centers is most evident in the Internet-scale data intensive computing en-
vironment due to the scale of the computing environment, the size of the data
set, and the new programming model such as Hadoop/MapReduce framework.
While still in an experimental stage, early research work [18–20, 24, 25] have
revealed some common design principles that are critical to both the future data
center network architecture and the commercial switching products for the data
intensive computing environment. We expect more research to continue in these
areas.

1
High Performance Network Architectures for Data Intensive Computing
15
Non-interfering Network
The requirements for bandwidth uniformity and end-to-end low latency indicate
that the ideal network for data intensive applications should be a non-interfering
network [26]. A non-interfering network is the packet switching counterpart of
a non-blocking circuit switching network. (A network is said non-blocking if a
dedicated circuit can be formed from any input to any output without conﬂict with
the circuit requests for any other input-output pairs. See [27] for details.)
For packet switching applications, the notion of non-blocking network is largely
an overkill, because in a packet switch different sessions or packet ﬂows can share
a link without interference as long as (1) the link bandwidth is greater than the
trafﬁc load, and (2) the allocation of resources (buffers and channel bandwidths)
can be done in a such a manner that no single ﬂow denies service of another
ﬂow for more than a predetermined amount of time (to cause the dropping of
packets). The resource allocation constraint can be realized by an end-to-end ﬂow
control mechanism. A packet network is non-interfering if it satisﬁes the above two
conditions.
A non-interfering network delivers many beneﬁts to data intensive applications,
such as bandwidth uniformity and predicable end-to-end latency between any server
peers regardless of their locations. An immediate corollary for a non-interfering data
center network is that there should be no over subscription (or 1:1 over subscription)
between the aggregated uplink and downlink bandwidths in the network. However,
just to have 1:1 over subscription is not sufﬁcient for a non-interfering network.
In order to spread the trafﬁc uniformly (to avoid hot spots and congestion), proper
network topologies and routing algorithms have to be designed (just as in its non-
blocking circuit switching network counterpart [27]).
In general, a good routing algorithm for a non-interfering data center network
needs to spread the trafﬁc evenly to avoid the formation of hot spots.
Experiments in [18] show that there is little regularity or predictability of the
trafﬁc pattern in a data intensive data center environment. In such an environment,
Valiant Load Balancing (VLB) routing through randomization is demonstrated a
good method to achieve trafﬁc uniformity. The companion topology for the network
is typically a folded Clos Network (also called a fat tree). Figure 1.5 shows a folded
Clos .n; m; r/-network. Readers with further interests can refer [18,25,28] for more
details.
Flat Network Topology and Multi-pathing
A ﬂat network topology and multi-pathing capability between server end points are
needed to overcome the latency, reliability, and bandwidth utilization constraints
associated with today’s tree-like hierarchical network topology and the STP-based
Ethernet path selection. The two level leaf-spine design of the folded Clos network
described above can enable multiple paths between any server peers when using a
layer-3 link state routing protocol like IS-IS or OSPF for path selection. Such design

16
G. Lin and E. Liu
Fig. 1.5 Illustration of a folded Clos (8,4,8)-network
would require the leaf switches and the spine switches to operate as layer-3 routers
and use routing tables and leverage routing features such as Equal Cost MultiPath
(ECMP) to spread the trafﬁc along multiple paths between the server peers.
There are multiple proposals to add this multi-pathing capability to today’s layer
2 switches. TRILL (Transparent Interconnection of Lots of Links) is a proposed
standard currently under development in IETF [29]. TRILL proposes a new IETF
protocol implemented by devices called RBridges or Routing Bridges [30]. TRILL
combines the advantages of bridges and routers and is the application of link state
routing to the VLAN-aware customer-bridging problem. TRILL uses the IS-IS
link state routing protocol for path selection. SPB (Shortest Path Bridging) is a
standard developed by IEEE (IEEE 802.1aq) [31] aiming to solve the same issue
as TRILL but with a slightly different implementation. SPB also leverage IS-IS link
state routing protocol for route selection. (The evaluation of the relative merits and
difference of the two standard proposals is currently a hotly debated topic in the
networking industry and is beyond the scope of this chapter. Interested readers can
refer [32] for further details.)
In addition to the network-based TRILL and SPB approaches, researchers also
proposed other experimental solutions. For example, Greenberg, et al. [18, 19]
demonstrated an host-assisted solution by inserting a layer 2.5 shim in servers’ net-
working stack that combines end-system address resolution, OSPF-based routing,
Vilant Load Balancing (VLB) technique, and randomized route selection to achieve
uniform trafﬁc distribution and multi-pathing capability. These network-based or
host-assisted solutions ensure the data center networks to become a two-layer
topology with multiple redundant paths between any server peers, hence can help
reduce latency, increase link utilization and improve network reliability around link
and switch failures.

1
High Performance Network Architectures for Data Intensive Computing
17
Tiered Data Centers
Many Internet-scale online service providers have adopted a two-tier data center
design to optimize data center cost and service delivery [19]. In this architecture,
the creation and delivery of the service are accomplished by two tier data centers – a
front end tier and a back end tier – with signiﬁcant difference in their sizes. Take
the Web search service as an example, the massive data analysis applications (e.g.,
computing the web search index) is a natural ﬁt for the centralized mega data
centers (measured by hundreds of thousands of servers) while the highly interactive
user front-end applications (e.g., the query/response process) is a natural ﬁt for
geographically distributed micro data centers (measured by hundreds or thousands
of servers) each placed close to major population centers to minimize network
latency and delivery cost. Hence the data center network design principles discussed
above and their product implementations need to be scalable to support both mega
data centers and micro data centers. For example, there are known issues in the
current version of the TRILL and SPB proposals to scale up to the mega data center
environment.
4.2
Data Center Interconnect Network
Data center interconnect networks (DCINs) are used to connect multiple data
centers to support a seamless customer experience of data intensive computing
services. Geographically dispersed data centers provide added application resiliency
and workload allocation ﬂexibility to avoid demand hotspots and fully utilize
available capacity. To gain these beneﬁts, the network must provide Layer 2 and
3 and storage connectivity between data centers. Connectivity must be provided
without compromising the autonomy of data centers or the stability of the overall
network.
4.2.1
Requirements for Data Center Interconnect Network
While a conventional, dedicated, and statically provisioned virtual private network
can interconnect multiple data centers and offer secure communications, to meet
the requirements of dynamic workload and data mobility needed by data intensive
computing services, the DCIN has emerged as a special class of network architecture
which requires its own unique architecture solutions. Among the chief technical
requirements for the DCIN are:
IP address preservation: The IP address associated with the workload should
remain the same regardless which data center the workload is moved to. This is
critical to maintain the integrity of the data intensive applications.
Transport independence: The nature of the transport between data centers varies
depending on the location of the data centers, and the availability and quality

18
G. Lin and E. Liu
of services in the different areas. An application-effective solution for the
interconnection of data centers must be transport agnostic and give the network
designer the capability to hide the details of the transport characteristics from
the application environment. A solution capable of using the IP transport layer is
expected to provide the most ﬂexibility.
Bandwidth optimization: When connecting data centers, the use of available
bandwidth between data centers must be optimized to obtain the best connectivity
and achieve the optimal application performance. Balancing and accelerating the
loads across all available paths while providing resilient connectivity between
the data center and the transport network requires added intelligence above
and beyond that available in traditional Ethernet switching and Layer 2 VPNs.
Multicast and broadcast trafﬁc should also be replicated optimally to reduce
bandwidth consumption.
Simple operations: Static layer 2 VPNs can provide extended interconnections
across data centers, but is very cumbersome to deal with changes to support
scaling demands or new application distribution patterns. It usually involves a
mix of complex operations of protocol changes, distributed provisioning, and
adjustment of the operations-intensive hierarchical scaling model. A simple
overlay protocol with built-in capability and point-to-pointprovisioning is crucial
to providing the agility needed for the dynamic demands of data intensive
applications.
To address these technical challenges, industry development and research work
have been mainly focused on the following architecture areas.
4.2.2
Layer 2 Extension across Multiple Data Centers
Traditional data center network design calls for the termination of Layer 2 envi-
ronment at the data center core switch layer (see Sect. 4.1.1 for details). Having
the Layer 2 connectivity (i.e., LAN) extended beyond a single data center brings
tremendous beneﬁts to the data intensive computing as it allows workloads to be
dynamically reallocated to another data center without breaking all the dependency
it has on its native Layer 2 environment (e.g., VLAN membership). LAN extensions
can also help application designers to simplify the cross data center resiliency and
clustering mechanisms offered in different applications at the web, application, and
database layers. Overlay Transport Virtualization (OTV) is an architecture solution
used to provide LAN extensions across data centers [33,34].
OTV is an IP-based functionality designed to provide Layer 2 extension capabil-
ities over any transport infrastructure: Layer 2 based, Layer 3 based, IP switched,
label switched, and so on. The only requirement from the transport infrastructure is
providing IP connectivity between the data center sites. OTV provides an overlay
that enables Layer 2 connectivity between separate Layer 2 domains while keeping
these domains independent and preserving the fault-isolation, resiliency, and load-
balancing beneﬁts of an IP-based interconnection.

1
High Performance Network Architectures for Data Intensive Computing
19
Fig. 1.6 Data center interconnect LAN extension encapsulation options
OTV introduces the concept of “MAC routing,” which means a control plane
protocol is used to exchange MAC reachability information between network
devices providing LAN extension functionality. This is a signiﬁcant shift from
Layer 2 switching that traditionally leverages data plane learning. As such, Layer 2
communications between sites resembles routing more than switching. If the
destination MAC address information is unknown, then trafﬁc is dropped (not
ﬂooded), preventing waste of precious bandwidth across the WAN.
OTV also introduces the concept of dynamic encapsulation for Layer 2 ﬂows
that need to be sent to remote locations. Each Ethernet frame is individually
encapsulated into an IP packet and delivered across the transport network. This
eliminates the need to establish virtual circuits, called Pseudowires, between the data
center locations. Immediate advantages include improved ﬂexibility when adding or
removing sites to the overlay, more optimal bandwidth utilization across the WAN,
and independence from the transport characteristics (Layer 1, Layer 2 or Layer 3).
Lastly, OTV provides a native built-in multi-homing capability with automatic
detection, critical to increasing the high availability of the overall solution. Two
or more devices can be leveraged in each data center to provide LAN extension
functionality without running the risk of creating an end-to-end loop that would
jeopardize the overall stability of the design. This is achieved by leveraging the
same control plane protocol used for the exchange of MAC address information,
without the need of extending the STP across the overlay.
Figure 1.6 shows a high level architecture for the data center interconnect
network based on the Layer 2 network extension approach.

20
G. Lin and E. Liu
4.2.3
Location and Application-ID Separation
The current Internet architecture is based on using IP addresses in two distinctive
roles. From an application point of view, an IP address identiﬁes a host. That is, an IP
address is used as an identiﬁer for the peer host (e.g., in a socket communication). It
is expected that this identiﬁer remains stable as long as the association is active. This
role is often called the identiﬁer role. From the network point of view, an IP address
names the current topological location of an interface by which a host is attached
to the network. That is, an IP address is used as a name for the location where a
speciﬁc network interface can be found. If a host moves around and attaches its
network interface to a different location, the IP address associated with the interface
changes. This role is often called the locator role.
The dual-role played by the single IP address brings an architecture challenge
to the data intensive computing environment when an application workload needs
to be dynamically and seamlessly moved across data centers via the Internet. This
issue is not as severe when the application load is moved within a single data center
(typically represented as a single Layer 2 domain) because the Layer 2 network
forwards the application trafﬁc based on MAC address, not the IP address. However,
when Layer 3 routing is used in the data center interconnect network, this dual role
of IP addresses is becoming problematic.
One way to solve this problem is to extend a single LAN domain across the data
center interconnect network to avoid using IP address, as shown in the previous
section. Researchers and technology leaders also proposed solutions based on a
different architecture perspective [18,35,36].
In general, these solutions propose two addresses to be used – one to iden-
tify the application and one to identify the location. The network infrastructure
uses the location-speciﬁc addresses for routing purpose; the applications use the
application-speciﬁc addresses, which remain unaltered no matter how servers’
locations change due to virtual-machine migration or workload re-provisioning,
for application identiﬁcation and peering purpose. Depending on where the second
address is inserted, these solutions can be classiﬁed as host-based or network-
based.
The Host Identity Protocol (HIP) [35] solution and the VL2 [18] solution are
hosted-based solutions in that the host identiﬁer (application-speciﬁc address) is
injected from the host’s network stack, and intercepted by a directory service for
future bindings with the new location-speciﬁc address if the application is moved
later.
The Locator/ID Separation Protocol (LISP) [36] solution is a network-based
solution in that the interception of the host id, the generation of the network locator,
and the mapping between the two, are performed by the network – the LISP
“gateways” (called Ingress Tunnel Routers and Egress Tunnel Routers). The ITRs
and ETRs maintain the dynamic binding of the hosts regardless of their location
movements within the network.
There are pros and cons with regards to the network-based approach vs.
the host-based approach. In reality, HIP and LISP were proposed ﬁrst to solve

1
High Performance Network Architectures for Data Intensive Computing
21
the device multi-homing issue in a service provider environment. Overtime, we
expect to see more research and industry solutions specially designed for the
location-ID separation issue in the data intensive computing environment (similar
to [33]).
4.2.4
Layer 4 Network Services
Layer 4 network services play an important role in the DCIN architecture. Appli-
cation ﬁrewalls ensure the secure transport of user data and application workloads
between the data centers; server load balancers ensure the workloads distributed
evenly or according to operations policies; WAN accelerators provide WAN op-
timization that accelerates the targeted workloads over the WAN, and ensure a
transparent user experience regardless where the applications reside.
While these Layer 4 network services exist in today’s data center environments,
the data intensive computing environment has created a signiﬁcant challenge to
the traditional network service architecture, as the Layer 4 network services now
need to be aware of the workload location dynamically. For example, application
residing in one data center can be dynamically moved to another data center
for load balancing. How to ensure the WAN accelerator or security ﬁrewall to
recognize a new application without re-provision the network and operations
policies? We believe how to effectively leveraging Layer 4 network services
in the data intensive computing environment will be a fertile area for future
research.
5
Conclusions and Future Directions
Recent development in Internet-scale data applications and data intensive services,
combined with the proliferation of cloud computing, has created a new computing
model for data intensive computing best characterized by the MapReduce paradigm.
We believe this new model of data intensive computing is still in the early stage
and will bring tremendous impact to the next generation networking architecture.
In many ways, supporting data intensive computing represents a natural evolution
for the IP networking architecture; we see the Layer 2 domain in the data center
network becoming wider, ﬂatter and workload aware; we see the data center
interconnect network and Layer 4 network services becoming self-adaptable to
routing, security, and performance constraints; we see workload mobility and
service elasticity not only within a single data center but also across multiple
data centers. As more data intensive computing services are created and deployed,
more requirements will be put to the networks and more intelligence will be
implemented. We believe the guiding principles for the next generation network
architecture will be fast, ﬂat and ﬂexible. Unlike today’s network architecture
which often bundles the data plane and control plane together, the next gen-

22
G. Lin and E. Liu
eration network will have a clear separation of control plane and data plane
driven by software deﬁned networking. In such architecture, the network can be
virtualized where different application-speciﬁc service networks can be deﬁned at
will on top of the same physical network in a similar fashion to today’s server
virtualization.
References
1. C. Tanasescu and T. Reed, “Data Intensive Computing: How SGI Altrix ICE and Intel Xeon
Processor 5500 Series Help Sustain HPC Efﬁciency Amid Explosive Data Growth,” http://
www.sgi.com/pdfs/4154.pdf, 2009
2. J. McKendrick, “Size of the data universe:1.2 zettabytes and growing fast,” ZDNet, May 2010
3. Y. Chen, D. Pavlov and J. F. Canny, “Large-Scale Behavioral Targetting,” ACM KDD’09, Paris,
France, July 2009
4. D. Newman, A. Asuncion, P. Smyth and Max Welling, “Distributed Algorithms for Topic
Models,” Journal of Machine Learning Research, Aug., 2009
5. R. Shankar and G. Narendra, “MapReduce Programming with Apache Hadoop,” Java-
World.com, Sept. 2008
6. LexisNexis Risk Solutions, “LexisNexis HPCC: ECL Programmers Guide, ”http://www.
lexisnexis.com/risk/about/guides/programmers-guide.pdf, 2010
7. LexisNexis Risk Solutions, “High-Performance Cluster Computing,” http://www.lexisnexis.
com/government/solutions/literature/hpcc-das.pdf, 2010
8. G. Harrison, “10 Things You Should Know About NoSQL Databases,” http://www.
techrepublic.com/downloads/10-things-you-should-know-about-nosql-databases, Aug 2010
9. C. Strauch, “NoSQL Databases,” Stuttgart Media University, Feb 2011
10. HP, “HP Superdome 2: the Ultimate Mission-critical Platform,” http://www.compaq.com/cpq-
storage/pdfs/4AA1–7762ENW.pdf, June 2010
11. J. Baker, C. Bond, J. C. Corbett and etc., “Megastore: Providing Scalable, Highly Available
Storage for Interactive Services,” 5th Biennial Conference on Innovative Data Systems
Research (CIDR’11), January 2011
12. M. Diehl, “Database Replicatio with Mysql,” Linux Journal, May 2010
13. L. Barroso and U. H¨olzle “The Datacenter as a Computer: An Introduction to the Design of
Warehouse-Scale Machines,” 2009
14. J. Dean and S. Ghemawat, “MapReduce: Simpliﬁed Data Processing on Large Clusters,”
OSDI’04: Sixth Symposium on Operating System Design and Implementation,” San Francisco,
CA, Dec. 2004
15. Cisco Systems, “Data Center Design – IP Network Infrastructure,” http://www.cisco.com/en/
US/docs/solutions/Enterprise/Data Center/DC 3 0/DC-3 0 IPInfra.pdf, Oct. 2009
16. M. Arregoces and M. Portolani, “Data Center Fundamentals,” Cisco Press, 2004
17. B. Hedlund, “Top of Rack vs End of Row Data Center Designs,” http://bradhedlund.com/
2009/04/05/top-of-rack-vs-end-of-row-data-center-designs/, April 2009
18. A. Greenberg, J. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. A. Maltz, P. Patel
and S. Sengupta “VL2: A Scalable and Flexible Data Center Network,” ACM SIGCOMM,
Barcelona, Spain, Aug., 2009
19. A. Greenberg, P. Lahiri, D. A. Maltz, P. Patel and S. Sengupta. “Towards a next generation data
center architecture: Scalability and Commoditization,” PRESTO Workshop at SIGCOMM,
2008
20. S. M. Rumble, D. Ongaro, and R. Stutsman, M. Rosenblum and J. K. Ousterhout, “It’s Time for
Low Latency,” Proceedings of the 13th Workshop on Hot Topics in Operating Systems (HotOS
2011).

1
High Performance Network Architectures for Data Intensive Computing
23
21. J. Dean, “Designs, lessons and advice from building large distributed systems,” Keynote talk:
The 3rd ACM SIGOPS International Workshop on Large Scale Distributed Systems and
Middleware (October 2009)
22. S. Amershi, J. Fogarty, A. Kapoor and D. Tan, “Effectie End-User Interaction with Machine
Learning,” AAAI-11, Nector, 2011
23. R. Perlman, Interconnections, Second Edition, Addison-Wesley, 2000.
24. M. Al-Fares, A. Loukissas, and A. Vahdat, “A scalable, Commodity Data Center network
Architecture,” in Proceedings of SIGCOMM, 2008
25. S. Mahapatra and X. Yuan, “Load Balancing Mechanisms in Data Center Networks,” the
7th Int. Conf.& Expo on Emerging Technologies for a Smarter World (CEWIT), Sept. 2010.
(invited)
26. W. J. Dally and B. Towles, Principles and Practices of Interconnection Networks, Morgan
Kaufmann Publishers, 2004
27. G. Lin and N. Pippenger, “Parallel Algorithms for Routing in Non-Blocking Networks,” Math.
System Theory, Vol. 27 pp. 29–40, 1994
28. L. G. Valiant, “A scheme for fast parallel communication,” SIAM Journal on Computing,
Vol. 11, No. 2, pp. 350–361, 1982
29. J. Touch and R. Perlman, “Transparent Interconnection of Lots of Links (TRILL): Problem and
Applicability Statement,” RFC 5556, IETF, May 2009
30. R. Perlman, D. Eastlake, S. Switches, D. G. Dutt, S. Gai and A. Ghanwani, “RBridges: Base
Protocol Speciﬁcation,” Internet-Draft, IETF, Mar. 2010 (http://tools.ietf.org/html/draft-ietf-
trill-rbridge-protocol-16).
31. P. Ashwood-Smith, “Shortest Path Bridging IEEE 802.1aq Overview & Applications,” in UK
Network Operators Forum, Sept. 2010 (http://www.uknof.org.uk/uknof17/Ashwood Smith-
SPB.pdf).
32. D. Eastlate, P. Ashwood-Smith, S. Keesara, and P. Unbehagen, “The Great Debate:
TRILL Versus 802.1aq (SPB),” in North American Network Operators’ Group (NANOG)
Meeting
50, Oct. 2010
(http://www.nanog.org/meetings/nanog50/presentations/Monday/
NANOG50.Talk63.NANOG50 TRILL-SPB-Debate-Roisman.pdf).
33. Cisco Systems, “Data Center Interconnect: Layer 2 Extension between Remote Data,”
http://www.cisco.com/en/US/prod/collateral/switches/ps5718/ps708/white paper c11 493718.
pdf, July 2009
34. Cisco Systems, “Cisco Overlay Transport Virtualization Technology Introduction and De-
ployment Considerations,” Jan. 2011 http://www.cisco.com/en/US/docs/solutions/Enterprise/
Data Center/DCI/whitepaper/DCI3 OTV Intro WP.pdf)
35. R. Moskowitz and P. Nikander, “Host Identity Protocol (HIP) Architecture,” RFC 4423, IETF,
May 2006 (http://www.ietf.org/rfc/rfc4423.txt).
36. D. Farinacci, V. Fuller, D. Meyer and D. Lewis, “Locator/ID Separation Protocol (LISP),”
draft-farinacci-lisp-12, IETF, Mar. 2009 (http://tools.ietf.org/html/draft-farinacci-lisp-12).
37. D. Ferrucci, E. Brown, J. Chu-Carroll and etc., “Building Watson: An Overview of the DeepQA
Project,” Association for the Advancement of Artiﬁcial Intelligence, Fall, 2010
38. J. Qiu, J. Ekanayake, T. Guanarathene, and etc., “Data Intensive Computing for Bioinformat-
ics,” Bloomington, IN, Indiana University, December, 2009
39. J. Shafer, S. Rixner and A.L. Cox, “Datacenter Storage Architecture for MapReduce Applica-
tions,” Workshop on Architectural Concerns in Large Datacenters (ACLD 2009), Austin, TX,
June 2009.


Chapter 2
Architecting Data-Intensive Software Systems
Chris A. Mattmann, Daniel J. Crichton, Andrew F. Hart,
Cameron Goodale, J. Steven Hughes, Sean Kelly, Luca Cinquini,
Thomas H. Painter, Joseph Lazio, Duane Waliser, Nenad Medvidovic,
Jinwon Kim, and Peter Lean
1
Introduction
Data-intensive software is increasingly prominent in today’s world, where the
collection, processing, and dissemination of ever-larger volumes of data has become
a driving force behind innovation in the early twenty-ﬁrst century. The trend towards
massive data manipulation is broad-based, and case studies can be examined in
domains from politics, to intelligence gathering, to scientiﬁc and medical research.
The scientiﬁc domain in particular provides a rich array of case studies that
offer ready insight into many of the modern software engineering, and software
architecture challenges associated with data-intensive systems.
C.A. Mattmann () • D.J. Crichton • A.F. Hart • C. Goodale • J.S. Hughes • S. Kelly
• L. Cinquini • T.H. Painter • J. Lazio • D. Waliser
Instrument and Science Data Systems, NASA Jet Propulsion Laboratory
California Institute of Technology, Pasadena, CA, USA
e-mail: chris.a.mattmann@nasa.gov; daniel.j.crichton@jpl.nasa.gov; andrew.f.hart@jpl.nasa.gov;
cameron.e.goodale@jpl.nasa.gov; john.s.hughes@jpl.nasa.gov; sean.kelly@jpl.nasa.gov;
luca.cinquini@jpl.nasa.gov; thomas.painter@jpl.nasa.gov; joseph.lazio@jpl.nasa.gov;
duane.e.waliser@jpl.nasa.gov
N. Medvidovic
Computer Science Department, Viterbi School of Engineering
University of Southern California, Los Angeles, CA, USA
e-mail: neno@usc.edu
J. Kim
Joint Institute for Regional Earth System Science and Engineering (JIFRESSE),
University of California, Los Angeles, Los Angeles, CA, USA
e-mail: jkim@atmos.ucla.edu
P. Lean
Department of Meteorology, University of Reading Reading, UK
e-mail: p.w.lean@reading.ac.uk
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 2, © Springer Science+Business Media, LLC 2011
25

26
C.A. Mattmann et al.
Scientiﬁc domains such as climate research, bioinformatics, radio astronomy,
and planetary and Earth science all face challenges related to the effective manip-
ulation of large datasets, including the capture, generation, and distribution of the
often complex, heterogeneous data to support various domain-speciﬁc applications
including: decision support, modeling and prediction, virtual exploration and
simulation, and visualization, among others.
Capturing, generating and distributing data in meaningful ways is key to ensuring
that the data can be used by downstream consumers. One major impediment to these
activities in modern times is the sheer volume of the data involved. Data volumes
have steadily increased as the mechanisms for generating and capturing data with
increasing resolution have evolved. To take a recent example from the world of
radio astronomy [40], the LOw Frequency Array (LOFAR) instrument [1] currently
generates 138 PB (petabytes) of data per day [2]. Other examples include climate
models that produce 8 PB per run, NASA Earth science decadal missions, such as
the Orbiting Carbon Observatory, with projected mission data volumes well into the
hundreds of TB range [3], and the high-energy physics community’s Large Haldron
Collider (LHC) instrument, generating 2 PB of data per second [4] during operation.
Yet, despite its tremendous implications, data volume is only one of many
challenges that must be addressed to properly architect data-intensive systems. At
the same time, the rapid evolution of information technology in multiple dimensions
(storage capacity, computing power, network bandwidth, computing language, and
web protocols) is opening up an unprecedented set of possibilities for the large-scale
analysis of data, changing the very way researchers conduct their day-to-day work.
Scientiﬁc research is no longer conducted by small groups of scientists working
in adjacent ofﬁces or a single laboratory, using a few pieces of hardware equipment
and desktop software. Rather, scientiﬁc collaborations are increasingly distributed,
often global in nature, and typically involve tens to hundreds of scientists working
together from their home institutions, connected via virtual electronic environments,
and accessing and analyzing massive amounts of data archived at distributed
locations.
Scientiﬁc algorithms too are becoming increasingly complex, and are often
implemented using a combination of many different software modules working
together. The need to operate over larger and larger datasets, often in heterogeneous,
multi-unit environments, and the need to distribute results not just to peers, but
to a variety of audiences of vastly different backgrounds (e.g., global policy
makers, public ofﬁcials conducting regional and local assessments, and students
and researchers of all education levels) – facilitating the transformation of big data
into actionable knowledge – present signiﬁcant challenges as well. This modern
paradigm for scientiﬁc data analysis requires a technical infrastructure that can meet
these new challenges, and at the same time keep pace with the relentless evolution
of the underlying hardware and software capabilities.
In this chapter, we will survey the state of the art in software engineering, and
software architectural challenges associated with data-intensive systems, expressly
focusing on the domains of science data systems. In Sect. 2, we will restrict our
focus to seven key challenges for data-intensive systems in this domain, speciﬁcally:

2
Architecting Data-Intensive Software Systems
27
(1) data volumes, (2) data dissemination; (3) data curation; (4) use of open source
software; (5) search; (6) data processing and analysis and (7) information modeling.
These challenges are then illustrated via case studies in Sect. 3, in which we describe
our experience on several data-intensive science projects in the areas of Regional
Climate Modeling, Bioinformatics, Planetary.
Science, Radio Astronomy, and Snow Hydrology. After placing the challenges,
in context, in Sect. 4 we describe several emerging software and architectural
paradigms that we view as promising approaches to the construction of software
to support data-intensive science in the aforementioned domains, and Sect. 5 rounds
out the chapter.
Throughout the chapter, we strive to present information in an descriptive, rather
than prescriptive fashion, in order to align with this book’s goal of providing a
handbook in the area of data-intensive systems. We feel this approach will help
to guide the reader through important projects, and architectural challenges, rather
than provide a recipe applicable only to any one experience.
2
Key Challenges
Architecting data-intensive software systems in today’s highly connected, compu-
tationally demanding scientiﬁc research environments presents tremendous chal-
lenges that stretch to the limit the familiar software engineering approaches of
the last 10 years. Many of the canonical software architecture techniques must be
updated to accommodate the uniquely challenging circumstances encountered in
building modern, data-intensive software systems.
In this section, we present a representative cross-section of concerns and
architectural challenges inherent in modern, data-intensive systems. We do not
claim this to be a comprehensive list, but rather that it is indicative of the type and
complexity of the challenges that must be overcome in order to construct effective
software for data-intensive applications.
We will refer back to these challenges (shown graphically in Fig. 2.1) throughout
the remainder of the chapter. Speciﬁcally, in Sect. 3, we will illustrate the manifesta-
tion of these challenges in several real-world data systems for scientiﬁc research that
Data Volume
Data Dissemination
Data Curation
Use of Open Source
Data Processing and Analysis
Search
Information Modeling
Architectural Challenges
Consume
Produce
Model-driven
Exchange/Interoperability
Archive
Processing
Delivery
Huge-scale (TB, PB, XB)
Average (GB, MB)
(Near) Real Time
Offline
Fig. 2.1 Architectural challenges relevant to data-intensive systems

28
C.A. Mattmann et al.
Fig. 2.2 Architectural challenge areas and their relation to the data-intensive system lifecycle. FM
stands for ﬁle management, WM stands for workﬂow management, and RM stands for resource
management
we have constructed at NASA’s Jet Propulsion Laboratory. Further, in Sect. 4, we
will highlight how topical data-intensive system technologies provide a framework
for understanding these concerns, and for addressing them in various ways.
Before we dive into the discussion on challenges, we must ﬁrst frame them
within the context of data-intensive systems architecture. The following section will
revolve around the imagery seen in Fig. 2.2.
2.1
The Architecture of a Data-Intensive System
As can be seen from Fig. 2.2, the architecture of a data-intensive system is inherently
complex. If we scan the diagram from left to right, we see that data (and metadata)
enter into the system delivered (or disseminated as we will explain in Sect. 2.2.2 and
as shown in Fig. 2.1) via a number of potential protocols (e.g., FTP, GridFTP, etc.)
to a staging area (shown in the upper left periphery of Fig. 2.2). Once in the staging
area the data and metadata are curated (as we will explain further in Sect. 2.2.3)
which can involve both human-in-the-loop detective work to richly add metadata
and structure to the data, and can also involve automatic, software-based metadata
extraction. One key component of this effort is information modeling (as we will
explain further in Sect. 2.2.7) wherein which models of the data and metadata set to
be cataloged, and archived are derived.
Once curated, data is ingested (either manually or automatically) via some
sort of ﬁle management component (shown in the middle-left portion of Fig. 2.2
and labeled as FM). Ingestion involves both the cataloging of extracted metadata
associated with the data and data dissemination from the staging area to a controlled,

2
Architecting Data-Intensive Software Systems
29
rich archive1. The FM is also responsible for staging data and metadata to the
workﬂow manager component (labeled as WM in Fig. 2.2 and shown in the bottom-
left portion of the diagram) and for managing the total volume of data in the archive.
The workﬂow management component is responsible for data processing (as we
will learn more about in Sect. 2.2.6). Both of these components (FM and WM)
are available for consumption from a number of open source projects including
within our own efforts in the Apache Object Oriented Data Technology (OODT)
project [5,6], as well as within Apache Hadoop [7], Condor [8], Wings [9], Pegasus
[9], and from a number of other examples that we’ll cover in Sect. 4 (see [10] for
further information). The workﬂow management component orchestrates control
ﬂow (sequences of executions of work units or tasks), and data ﬂow (passing of
information in between those tasks). In science data systems, tasks traditionally
correspond to some sort of data ﬂow-style component, that takes input data ﬁles
and transforms them somehow (geolocates them; calibrates them, etc.) to produce
output data ﬁles.
The WM works together with a resource management component (shown as
RM (another common open source component) in the middle-portion of Fig. 2.2
and responsible for managing the underlying compute and storage resources shown
as the boxes below RM) to execute a ready-to-run task on a set of hardware and
software resources (e.g., a cluster, a grid, a cloud, etc.). The ﬁrst step once the
job has been batched out to a hardware node (provided there was enough disk
space, compute resources, etc., as calculated by the RM) is to stage the appropriate
data ﬁles required for input to the task. This involves communication with the FM
component usually in the form of metadata search as demonstrated in the middle-
portion of Fig. 2.2. Once the input is available, it is provided (along with any other
necessary run-time information) to the underlying task and the task is executed out
on the hardware node, the result of which is a set of output data ﬁles (and potentially
metadata). This information is then traditionally re-ingested (e.g., via a crawling
process [6]) and disseminated to the ﬁle management component for preservation,
and to make those output data ﬁles and metadata available to downstream tasks and
processing.
At some point during the lifecycle of the data-intensive system, data is delivered
from the processing archive to the long term-archive as illustrated in the transi-
tion from the left-to-middle-to-right portions of Fig. 2.2. This process is volume
intensive, and involves dissemination in its own right. There may also been further
data curation that occurs to enrich the metadata and structure of the data for the
long-term archive. Once archived, the data is presented externally to the scientiﬁc
user community via data portals (e.g., see [11] for some examples), where users
can interactively and programmatically search for the data and metadata, and
explore and consume the information model that describes the data. Users may also
download the data and metadata, so dissemination is an important factor here, as
shown in the right side of Fig. 2.2.
1We use “archive” and “repository” interchangeably throughout the chapter.

30
C.A. Mattmann et al.
In the next section, we will describe in detail the key challenges of data-intensive
systems as it relates to their canonical architecture that we have covered in this
section.
2.2
The Challenges
In this section, we will hone in on the seven challenges described in Fig. 2.1 and
illustrated from an architectural perspective in Fig. 2.2.
2.2.1
Total Volume
The amount of data analyzed by a single project has already reached the order of
several petabytes (PB), and the exabyte (EB) is rapidly approaching mainstream
vernacular. For example, a single run of a modern climate model simulation running
on a high-resolution grid (such as the CESM T341, which is global grid at roughly
40 km resolution) will generate several terabytes (TB) of output. The combined
total volume for all of the models comprising the Coupled Model Intercomparison
Project Phase 5 (CMIP5) [12] is on the order of 10 PB. Similarly, the next generation
of Earth observing satellite missions planned by NASA (such as the DESDynI [13]
mission), will generate a ﬁnal product stream of roughly 40 TB/day, resulting in
several tens of PB over the course of just a few years.
Massive data volumes present several challenges at the architectural level. From
our related experience, the fork in the road lies at the transition from gigabytes
(GB) of data (up to hundreds of GB) to terabytes, petabytes and exabytes of
information, as shown in the upper right portion of Fig. 2.1. Modern commodity
hardware traditionally ships with disks in the GB/low TB range (up to 1–2), and it is
relatively inexpensive to scale up to tens of TB. However, scaling much beyond the
tens of TB range not only raises costs signiﬁcantly, it also increases the complexity
of managing the resultant data system from an architectural perspective.
In science data systems, data is regularly stored as ﬁles on disk, and associated
metadata2 is stored in a ﬁle catalog such as a database, a ﬂat-ﬁle based index (such
as Lucene [14]), or simply as metadata ﬁles on disk alongside the data itself. As
the data volumes increase, the challenges of partitioning the data effectively on
disk, organizing the metadata efﬁciently for search, and providing access to both
data and metadata for processing and dissemination become more pronounced. In
average-scale volumes (up to hundreds of GB), the disk repositories and ﬁles can
be organized in an ad-hoc fashion without becoming prohibitively complicated to
traverse and explore. For repositories in the terabyte range and beyond, alternative
2Metadata refers to “data about data.” As an example, consider a book data ﬁle, and its associated
metadata, “author,” with potentially many values.

2
Architecting Data-Intensive Software Systems
31
approaches are often required, including: (1) metadata based ﬁle organization – a
partitioning of data based upon certain metadata attributes (e.g., for datasets that are
spatially located, ﬁles might be partitioned on disk by region); (2) ﬁle management
replication – replication of ﬁle management servers to partition the overall data
“namespace” and its access, search, and processing; (3) careful selection of data
dissemination technologies – taking into consideration the beneﬁts and limitations
(e.g., support for multi-delivery intervals, parallelized saturation of the underlying
network) [13] of distribution technologies such as bbFTP and GridFTP [15], among
others; and (4) the additional search, and processing challenges we discuss below.
All of these challenges are illustrated architecturally in Fig. 2.2.
The difﬁculty in dealing with massive data volumes permeates the strategies for
addressing all of the other concerns and challenges from Fig. 2.1, including open
source (in the sense that certain open source technologies are oriented speciﬁcally
for use with larger data volumes as we will describe further in Sect. 2.5). In the
following section we will cover data dissemination, a challenge directly related to
data volume.
2.2.2
Data Dissemination
The dissemination of data at the scales one might realistically expect to encounter
in modern data-intensive software systems is no longer a trivial matter and merits
careful consideration in the architectural planning phases of system development.
Data volumes, coupled with the (often internationally) distributed nature of modern
research teams, and the cross-domain nature of many scientiﬁc problems, imply
that data holdings analyzed by a single project can no longer be assumed to exist
at a single archive, but rather are likely to be distributed across multiple locations
that may be both geographically and technologically distinct. At the same time,
users often expect to ﬁnd and interact with data as if it were a single archive.
Meeting this expectation requires that data-intensive systems consider discovery
and access services that conform to a common Application Programming Interface
(API) to permit clients to seamlessly access data from arbitrary locations without
requiring specialized code. This is seen traditionally in many modern open source
ﬁle management, workﬂow management and resource management components in
the form of extension points or “plug in” services that allow composition of existing
data system components. This could be envisioned as in Fig. 2.2, for example, at
both ends of the diagram. On the one hand, for example, the distribution of upstream
data sets across the globe, that must be brought into a data-intensive system for
processing; and on the other, the data produced by a data-intensive system that must
be distributed to several geographically diverse archives.
Unfortunately, in many cases, the present state of the art involves providers
that run legacy services customized to the speciﬁc storage type, data format, and
control policies in use at the center. Middleware code that delivers data and metadata
conforming to the common API, while interfacing with the existing back-end
servers and applications is often necessary to bridge the gap.

32
C.A. Mattmann et al.
Typically, data holdings are not generated or collected on their ﬁnal storage area,
but they need to be moved there possibly through several stages of processing
(as shown in the left-side and middle portions of Fig. 2.2). This requires both the
availability of a network bandwidth that is able to keep up with the data generation
stream, and the utilization of data transfer protocols that are able to take advantage
of that bandwidth. Historically, network bandwidth has lagged behind with respect
to the continuous increase in storage capacity and computing power. Currently,
the fastest networks allow transfers of about 10 GB/s, but they are available only
between a few selected locations. New technologies (GridFTP, UDT, BFTP etc.
[13, 15] as shown in the left periphery of Fig. 2.2 and as could be imagined in the
right) continue to emerge that aim at maximizing data transfer rates by instantiating
multiple concurrent data streams, tweak the buffer size, and decompose each single
transferred ﬁle. But the beneﬁts of these technologies are still limited in most cases
by the availability of the underlying high speed network as our own prior studies
[13] and [15] have shown.
In the following section we will describe the importance and relevance of data
curation once data has reached a staging area as disseminated from a remote site,
and once data is sent from a processing system to a long-term archive (as shown in
the middle left to middle right portions of Fig. 2.2.
2.2.3
Data Curation
Data curation is a broad term for a set of processes designed to ensure that data in all
stages of a data-intensive software system, from raw input data to processed output,
exhibit properties that facilitate a common organization, uniﬁed interpretation, and
contain sufﬁcient supporting information, or metadata, so as to be easily shared and
preserved. Recall the prior discussion from Sect. 2.1 and the left-upper portion of
Fig. 2.2.
While the concept of data curation is not a new one, it has taken an increasingly
prominent role in the modern era of high-volume, complex data systems. The
proliferation of mechanisms, formats, and standards for generating, annotating,
distributing, and ultimately archiving data underscores the need to treat a policy
for data curation as an essential ingredient of a data intensive software system. Our
own speciﬁc experience in this area is within the context of bioinformatics systems
and we point the reader to [16] and [17] for further information.
The process for data curation varies across systems depending upon factors
such as the volumes of data involved, the degree of noise in the inputs, and the
downstream expectations for disseminating the data for future (and perhaps even
far-future) use. Furthermore, the degree of automation in the curation process is
also highly variable. Some systems maintain a human in the loop, whereas others
perform curation via an algorithmic, rule-based approach, without any inline human
intervention.
Data curation is closely related to efforts in information modeling (discussed
in Sect. 2.2.7). The curation process is very often one of the chief mechanisms by

2
Architecting Data-Intensive Software Systems
33
which the abstract information model is actually applied to the data in the system,
and through which any modeled constraints are enforced. From this perspective,
the data curation process can be viewed as a mechanism for quality assurance; it
provides an opportunity to perform sanity checks and corrective action on the data
as it moves throughout the system.
The beneﬁts of data curation are not limited to error detection, however.
The curation process is often viewed as a mechanism for adding value to the
data. Curation provides opportunities for enriching data with contextual metadata
annotations that facilitate its downstream discovery and use (e.g., via search).
Furthermore, the analysis steps in many data-intensive systems are often highly
context-sensitive, and depend upon information such as the provenance of the data
(the detailed history of processing steps employed to generate the current data
as shown in the middle-bottom portions of Fig. 2.2), as well as the operational
and environmental parameters for an accurate interpretation of the data. Here
again, the close relationship to information modeling becomes evident. Where
the information model deﬁnes concepts and relationships that must be present in
the data, the data curation process must implement a mechanism for satisfying the
requirements.
Finally, in addition to offering opportunities for error correction and for adding
value to generated data, the curation process often provides an opportunity to
address the requirements for sharing and for long-term preservation of the data
(for a detailed description of this area, we point the reader to [18] and further its
accompanying special issue of Nature magazine on “Big Data”). The contextual
annotations described earlier provide rich technical descriptions of the data. Aside
from enabling context-sensitive in-system processing, this additional detail is
extremely helpful in a distributed, collaborative environment where groups of end-
users may not otherwise have the same degree of detailed insight into the processes
by which their data was generated. A well-curated dataset with sufﬁciently detailed
annotations and descriptive metadata has a better chance of standing on its own as a
self-contained, scientiﬁcally valuable resource than does a simple collection of raw
data that lacks a curated context.
We will next change gears a bit and speciﬁcally describe the role that the use of
open source software plays within data-intensive systems, and how the consumption
and production of open source data management software components play a role
in the overall architecture of data-intensive systems.
2.2.4
Use of Open Source
With the tremendous existing challenges of constructing data-intensive systems,
the consumption and (re-)use, as well as the production of components for down-
stream (re-use) is an important challenge in data-intensive systems. Open-source
software is extremely common-place in data-intensive systems, and represents
the implementation-level reiﬁcation of (re-)use of software components. We will
elaborate below.

34
C.A. Mattmann et al.
Several areas within Fig. 2.2 demonstrate data-intensive components that already
exist and that can be consumed off-the-shelf from many open source market places.
Our purpose in this section is not to highlight the wealth of open source software
products for data systems that exist; nor is it to contrast and compare them. Instead,
we focus on the identiﬁcation and rationale behind consuming and producing open
source software components as identiﬁed in our data-intensive system architecture
within Fig. 2.2.
For the left side of Fig. 2.2, (the ﬁle management and curation areas) protocols
for data-delivery into a staging area are myriad, and can be consumed with
varying levels of effort. Some data dissemination components and connectors
[19] (the models of interaction between software components) vary in their non-
functional properties, as well as in functional areas such as scalability, consistency,
dependability, and efﬁciency [3]. In terms of production, the development of
reusable connectors and protocols within this portion of the data-intensive system
focuses on exploiting the underlying network, bandwidth, and hardware resources
in some fashion to achieve speeds in performance, and reductions in overall memory
footprint. Since connectors are inherently application independent, the development
of dissemination software for open source and downstream reuse is an active area
of open research.
Information modeling and curation (shown in the upper left, and upper right
portions of Fig. 2.2) are two areas where the stock of open source software
components are relatively small and much current work in data-intensive systems
focuses on the development of these components. This is in part due to the diversity
and heterogeneity of the data managed, and to the scientiﬁc domain of applicability,
as well as in part due to the relatively recent focus (over the past 10 years) on
the capture of rich descriptions of data and metadata. So, from a open-source
software production perspective, data-intensive systems are in need of (re-)usable
components focused on these two challenges.
Regarding workﬂow management and resource management as shown in the
middle portion of Fig. 2.2, many existing open source components are available
for consumption. We point the reader to [9] and [10] for further surveys, as well
as to our own work in the area of the Apache OODT [5] project. In terms of
developing open source components for workﬂow and resource management, effort
is mostly spent in the area of supporting a variety of complex control ﬂow (fan-in,
and fan-out, as deﬁned by directed acyclic graphs [20]) and underlying hardware
and compute resources (grids, clouds, etc.). The development of effective workﬂow
and resource management components is highly complex, and in our experience,
effort is best spent in this area consuming existing open source components and
leveraging existing models.
One thing worth pointing out regarding processing and resource management is
that the underlying scientiﬁc tasks (the tn tasks shown in the bottom-middle portion
of Fig. 2.2), regarded by many scientists as representative of some step in the overall
scientiﬁc process, are typically ﬁrst-of-breed algorithms and the result of many years
of research. Depending on the maturity of the data-intensive systems domain, these
algorithms may or may not be suitable for downstream open source dissemination.

2
Architecting Data-Intensive Software Systems
35
The algorithms often represent “research-grade” code and aren’t “hardened” to be
suitable for other contexts, and/or scientiﬁc domains. This is due to a number of
reasons that are beyond the scope of this chapter, one common of which is that
the algorithms are often tied to the scientiﬁc instrument or physical process that
produced the input data that they operate on. As for consuming these components
from the open source marketplace, as the algorithms are often unique, their (re-)use
is typically limited to the system at-hand.
Regarding the right-side of Fig. 2.2, there are a number of existing open-source
search components available for consumption and (re-)use within a data-intensive
system. We will discuss these more in Sect. 2.2.5. Data disseminated from long-
term archives can best leverage the same types of connector technologies and open
source software previously discussed in the beginning of this section.
Besides production and consumption of open source software, there are a
number of other challenges and concerns, including understanding open source
licenses, communities, development practices, and methodologies. We see this as
an emerging and important area to keep within arm’s reach in understanding data-
intensive systems.
In the next section, we will discuss search as a key challenge within the data-
intensive systems domain. From the existing discussion and from Fig. 2.2, search
crops up in both the data processing (left/bottom middle) and in user-facing data
dissemination from a long-term archive portions of the architecture.
2.2.5
Search
All of the effort towards collecting, ingesting, storing, and archiving is for naught
unless there is some way to get that data back. Strictly regurgitating out of system
all that was ingested does not sufﬁce, as there are multiple orders of magnitude more
data saved within a data-intensive system that are required for an individual analytic
problem. As a result, systems need a way to search through all that data to locate
items that match criteria.
Data-intensive systems therefore provide a search feature that accepts queries
for data and returns a set of matching results, both useful in data processing to
identify input data ﬁles and metadata to stage to an algorithm and shown in the
bottom/middle-left portion of Fig. 2.2, as well as in data dissemination to end-users
shown in the right side of Fig. 2.2. Stated informally, queries pose questions of
where certain data is, and the search feature satisﬁes those questions with the answer
of where to ﬁnd such data. A query in this sense means any form of declaration
of desiderata, such as “Where are readings of ocean temperature taken by an
orbiting infrared CCD between 2000-01-01 and 2001-12-31?” or “What humor
books published after 1950 in the northern hemisphere mention the term ‘loogie’?”
The desiderata necessarily require some foreknowledge of the data curated with a
data-intensive system.
Note well that retrieval and dissemination are separate operations from search.
Strictly speaking, a search operation serves to locate data matching criteria and

36
C.A. Mattmann et al.
nothing else. The results of search are the locations of the matching data, and
potentially other metadata recorded about it. What happens next is up to the user
demands and architects of the system. In some instances, a “results list” may be
presented, ranking the matching data by some metric, and a user may then select
speciﬁc data to retrieve.
Giving better morphology to results, a data-intensive system operating in a
system-independent matter may present results as:
•
A set of identiﬁers (often Uniform Resource Identiﬁers or URIs) that either name
or locate the matching data. These may be “resolved” in order to retrieve the
actual data.
•
A matching set of metadata that annotate each identiﬁer with descriptors indicate
context and, optionally, relevance. Such metadata serve to guide consumers of the
system towards the best result from a possibly massive set of matching results.
Queries presented to a search component of a data-intensive system often take one
of three forms: open, guided, and constrained.
Open queries are the easiest to use from a user standpoint and are the most
familiar in today’s web-enabled internet. Anyone from the most intimate and
advanced users of search engines to the casual “googler” takes advantage of the
open format of queries. The open query format is merely a sequence of textual
terms that the search component of a data-intensive system matches against the
system’s catalog of data. Such matching may be subject to certain processing (such
as conversion of terminology, synonym generation, and the like). Often, the matches
to open queries depend on the data containing the actual terms presented. Open
queries serve the requirements of all kinds of users, though those inexperienced or
unfamiliar with what is within a data-intensive system’s catalog may not be able to
present the right terms to gain any results.
Guided queries enable exploration of the corpus of data cataloged within a
data-intensive system. Guided queries present a series of high-level organizational
categories or “facets” of the cataloged data along with a set of applicable terms
in each category. By selecting an item, the set of matching data, as well as the
related categories, are constrained, presenting a narrower view. Repeating this, users
can narrow down towards the sought after data while also reﬁning their desiderata
to better mesh with the cataloged entries. It enables both a broad overview of the
available catalog while also providing for customized, use-case, a speciﬁc matching
of results through a guided progressive disclosure. This interactive approach is ideal
for new users who can gather, at a glance, a high-level view of the catalog. However,
its weakness is that it requires curators of the data-intensive system’s catalog to
choose categorizations that appeal to the system’s users. Inappropriate facets may
lead users nowhere.
Constrained queries enable the end user or client systems of the data-intensive
system to specify a series of logical constraints on any of the searchable indexes
in the system’s catalog. Such constraints specify exact or relative values, such as
temperature measurements equal to 273ı, or in the range of 273–279ı, and so forth.
Multiple constraints may be joined in a logical expression, such as temperature in

2
Architecting Data-Intensive Software Systems
37
the range of 273–279ı OR altitude greater less than 2 km YET with number of
observations greater than 20. Constrained queries allow for the most precise yet
also the widest ranging potential for search throughout a data-intensive system.
The architecture of the search component in a data-intensive system typically
takes the form of a distributed structure in which one or more catalogs (containers
for indexes and metadata, managed by ﬁle management components as identiﬁed in
Fig. 2.2) are populated with information about the corpus of data to be cataloged.
In this arrangement, the logical entities include the query handler, which accepts
desiderata in the three forms as described above; the indexer, that provides informa-
tion for queries and creates the catalog of data; the schema manager, that provides
the conﬁgurable set of metadata for queries; the converter, that accepts data to add to
the catalog, analyzes it, and extracts metadata, and passes such digested data to the
indexer; and the storage broker, that manages the persistence for all this information.
Ancillary components include replication management, controllers, authentication,
and authorization.
More speciﬁcally, the query handler accepts external queries, checks if they’re
well-formed, and uses the indexer for resolution. Query handlers may project a
number of client interfaces, from HTML forms, to HTTP-based web services
(including REST, XML-RPC, SOAP, and so forth), or language-speciﬁc application
programmer interfaces. The query handler gathers results and returns them to the
client, paginating as needed.
The schema manager maintains persistent metadata about indexed items. It is
equipped with the system curators’ selected metadata ﬁelds, their data types, and
so forth. For example, in a document data system, the schema manager would track
titles, authors, and abstracts. In a climate data system, it might be latitudes and
longitudes. Since search results contain only the locations of matching data and not
the data themselves, it’s the schema manager’s job to annotate those results with
enough useful information to inform end users of whether retrieving such data is
useful.
Finally, the indexer is the heart of the search component. It provides the logic
to handle queries, catalog data, and communicate with backend storage systems.
When data is entered into the system, it relies on metadata extraction and analysis
components to provide details to the schema manager. It relies on data conversion
components to translate foreign formats. It relies on the storage broker to maintain
such information over long periods of time. It builds high speed indexes for future
queries. And it services those queries.
In the ensuing section, we will build upon the existing challenges and describe
the relationship of data processing and analysis to them and to the data-intensive
system.
2.2.6
Data Processing and Analysis
Scientiﬁc data repositories have long focused on the need to capture data from
upstream data producers (instruments, sensors and other scientiﬁc activities),

38
C.A. Mattmann et al.
without carefully addressing science user needs for turning these repositories into
useful knowledge-bases. In light of this, many scientiﬁc domains have standardized
over time on science user pipelines or automated software workﬂows which
process and generate data from rich canonical science repositories, in order to
provide value-added answers and outputs to scientists, and to the broader decision-
support community. In a data-intensive system, this boils down to workﬂows, tasks,
workﬂow management systems, and resource management components as depicted
in the middle of Fig. 2.2.
Often, the large amounts of data generated by a project need to be post-processed
before it can be analyzed by the scientists. Such is the case for example for
remote sensed data, where the data stream from the ground station needs to be
transformed through several successive algorithms to generate the required data
products. Another example is that of global climate model output that needs to be
regridded and downscaled to provide data that can be used to predict the effect
of climate change on a regional and local scale. Along the way, the relationship
between maintaining data and metadata for the intermediate inputs and outputs
of such science workﬂows and pipelines is critical since this information in many
ways drives downstream processing, and ultimately other interesting areas of data-
intensive systems such as provenance of output and the like. There is also a close
relationship to other architectural components, including ﬁle management (to stage
ﬁles and metadata for the steps in each workﬂow), curation, and dissemination to
the node where the step is processing.
Post-processing these large volumes of data, often in real time, mandates
new requirements on the computing power of the hardware employed. When the
data cannot be processed on a single machine, the project needs to consider an
architecture that distributes the load over several servers, possibly conﬁgured to
perform different steps of the overall processing pipeline. This is demonstrated
through the resource management component and its main purpose as shown in the
middle of Fig. 2.2. Cloud computing environments have increasingly become under
consideration as a method to dynamically allocate computing resources to a data
processing task, due to their ability to support sporadic burst processing, and storage.
In practice, clouds, grids, clusters, and even desktop computing machines are all
used and leveraged during scientiﬁc data processing, and all should be available as
suitable resources for use.
The close relationship with processing and with ﬁle and metadata management
also begets an important connection to information modeling. Often, science
algorithms and workﬂow tasks are not necessarily concerned with maintaining
provenance, and other data system metadata inasmuch as they are concerned with
presenting the output science result or measurement. To ensure that the provenance
of executing these science algorithms is captured, many data-intensive systems
employ an architectural wrapper approach [21]. The wrapper orchestrates the
lifecycle of the individual step in the scientiﬁc processing, ensuring the appropriate
ﬁles, and metadata are provided, that the appropriate inputs are available and pre-
conditions met, and that metadata and output data ﬁles are cataloged and archived

2
Architecting Data-Intensive Software Systems
39
and made available for distribution. We point the reader to our own work in the
context of several science data processing systems in Earth science for further
information [6].
We close out this section and the discussion of key challenges below by
highlighting the importance of information modeling.
2.2.7
Information Modeling
In a data-intensive software system, especially one with requirements for long-term
data usability and persistence, the metadata should be considered as signiﬁcant as
the data. For example, a digital image is essentially useless to a planetary scientist
unless information about the locations of the light source, the imaging instrument,
and the target body are all known, preferably within a single frame of reference.
Metadata is often captured in a data-intensive system within a catalog or registry
(as demonstrated in the left upper portion of Fig. 2.2). In some scientiﬁc domains
(e.g., NASA planetary missions), the experiment is non-repeatable since it occurred
over a particular unique space/time sequence. Because of this, it is in the best interest
of the science community to collect as much information as possible about the
observation and the context within which it was performed for future reference.
Metadata is also required to index and classify the data for search and retrieval as
described in Sect. 2.2.5 and throughout Sect. 2.2 and as shown in the upper right and
lower left portions of Fig. 2.2.
Information models are used to deﬁne, organize, and classify metadata. These
include models for the actual data as well as models for other digital, physical and
conceptual things in the domain. For example in addition to a model for a digital
image ﬁle, others might be required to describe the digital calibration data required
to produce scientiﬁcally useful products3, the physical instrument that collected the
data, and the mission that managed it.
Data-intensive software systems also have system interoperability and data
correlation requirements. Shared information models [22] are needed to meet these
requirements by adding semantics, basically formally deﬁned relationships between
things in the models. Furthermore to achieve seamless connectivity important
assumptions must be made about the information models being used to provide
the semantics, including having a single shared ontology and the need for human
assistance in the development of the ontology. Without a single shared ontology
the effort to achieve connectivity across pre-existing repositories is essentially
“cryptography” and rapidly becomes intractable.
Information models help describe data as it is provided to the user through
data portals, search and curation tools, and to software programs looking for the
appropriate ﬁles to stage for data processing and for science algorithms (shown in
the upper right, and middle portions of Fig. 2.2).
3In the world of science data systems and data-intensive systems in general, “products” refer to the
output data ﬁle(s) along with their metadata.

40
C.A. Mattmann et al.
With the above understanding of the key challenges of data-intensive systems
as they relate to software architecture out of the way, in the next section we
will illustrate modern, real-world examples and manifestations of these challenges
within the architectures of science data systems for regional climate modeling,
astronomy and radio science, along with snow hydrology. In doing so, we will also
highlight strategies and approaches for dealing with these important challenges.
3
Representative Science Data Systems
Having covered the challenges that data-intensive computing presents, we will
expand on several examples from our collective project experience. It is our goal
to illustrate the commonality between these examples despite the extreme variance
in the area of scientiﬁc study from project to project.
The goal is two-fold. First, we demonstrate that the key challenges of data-
intensive systems manifest independent of the science domain; and second, we
strive to highlight successful architectural approaches and strategies (relating back
to Sect. 2) that have proved effective as a means for addressing these stringent
challenges.
3.1
Climate Modeling
Climate modeling is a computationally expensive task, both in terms of data
processing and data volumes (recall Sects. 2.2.6 and 2.2.1). While the accuracy of
climate projections is limited by both the level of understanding of the Earth system
and the available computer resources, currently the bottleneck lies primarily with
the computer resources [18].
By the early twentieth century, scientiﬁc understanding of the atmosphere had
reached a stage where accurate predictions could, in theory, have been made if
adequate computer resources had been available. This is illustrated by the fact that
during World War I, mathematician Lewis Fry Richardson, devised an atmospheric
model broadly similar to those in use today. He attempted to make a weather forecast
using this model, but limited by the computer resources of the day (he ran the model
by hand, performing calculations using pen and paper), it took him several months to
produce a 6-h forecast [23]. Consequently, routine weather predictions and the even
more computationally expensive problem of climate prediction were unfeasible until
the advent of modern computers later in the century.
Climate models work by splitting the entire Earth system (atmosphere, land
surface, ocean and sea ice) into a three dimensional grid and using the laws of
physics to predict the future evolution of various state variables at each grid box.
The models make projections up to 100 years ahead by calculating the evolution in
sequences of short time steps (each only a few tens of minutes long). Over time,

2
Architecting Data-Intensive Software Systems
41
the steady increase in computing power has allowed the model grid length to be
reduced (smaller boxes give a ﬁner representation of the atmospheric features) and
the complexity of the simulations to be increased.
Recently, the scientiﬁc community has faced a challenge in how to maintain this
performance increase in the face of a trend towards massive parallelization in HPC
architectures. Existing climate model codes do not scale well across many thousands
of CPU cores and major re-engineering may be required to take advantage of the
potential of massively parallel architectures. Currently, atmospheric models are
parallelized such that each core handles a geographically distinct region. However,
as changes in the atmosphere at one point depend on nearby conditions, considerable
communication between cores is required, reducing the scalability of the system.
As the resolution of the climate models increases, so does the volume of data
produced (Sect. 2.2.1). Today, climate research projects often involve international
collaboration between teams from many research centers, often involving data from
several models. The problems of disseminating and processing statistics (recall
Sects. 2.2.2 and 2.2.6) on such large datasets is becoming an increasingly serious
challenge to the community. In addition, different models use different ﬁle formats
and meta-data conventions posing signiﬁcant difﬁculties to researchers working on
output from more than one model. Currently, coordinated attempts are being made
to assist in the efﬁcient dissemination and processing (Sects. 2.2.2 and 2.2.6) of
climate model datasets through the creation of uniﬁed data portals which provide a
single source of data from multiple models in a common format.
Among the key computational aspects of climate modeling for predicting future
climate and its impact on human sectors is the fact that the assessment of climate
variations and change on regional sectors require high resolution information since a
number of important climate features vary according to regional-scale variations in
underlying surfaces. For example, precipitation and snow budget that play key roles
in the occurrence of ﬂooding and water resources in California is closely related with
the regional complex terrain (e.g., [24–26]). Climate simulations at such ﬁne spatial
resolutions for time scales of a century or more, minimum for resolving long-term
climate trend challenges computational infrastructure for both CPU cycles and the
handling (dissemination and storage) of model output. The CPU-cycle requirement
has been progressing well during the past three decades with the development of
massively parallel computer architectures and programming.
The data dissemination (recall Sect. 2.2.2) part has also been progressing with
the progresses in physical storage; however, it is still a bottle neck in climate
modeling. Because running global climate simulations at the spatial and time scales
to meet the needs for impact assessment and computing long-term climate trends,
respectively, remains cost-prohibitive, the climate modeling community employs
nested modeling in which regional climate models are used to spatially downscale
relatively coarse global model data.
Handling of massive data from regional model runs is a specially important
problem. For example, one snapshot of model ﬁelds for the ﬁne-resolution domain
is about 150 MB. Multiplying this with the frequency of model data sampling
(6 h, i.e., four times a day) and the simulation period (20 years for a minimum

42
C.A. Mattmann et al.
Fig. 2.3 The regional climate model evaluation system, or RCMES
to alleviate the effects of decadal variations), one regional climate run generates
about 4.5 TB of model output for its total volume. As model runs for at least two
periods, present-day and future, are needed for one set of climate change signals, one
scenario run generates about 10 TB of data. Note that this is a conservative estimate
based on the limited computational resources available to a small research group
consisted of a few researchers. Recent reduction in the cost of storage media allowed
regional climate modelers resolve the storage space problem somewhat; however,
transferring data from supercomputer centers to local storage and the performance
of local data storage in supporting data analysis remain major concerns.
Our most immediate experience in this area is centered around a project incepted
in 2009 funded by the American Recovery and Reinvestment Act (ARRA). The
project was focused on the assessment and generation of US Western regional
climate models using multiple observational datasets from e.g., the Atmospheric
Infrared Sounder (AIRS), MODerate resolution Imaging Spectrometer (MODIS),
and Tropical Rainfall Measurement Mission (TRMM) projects. Leveraging the open
source Apache OODT framework (that we will further discussion in Sect. 4), we
constructed a Regional Climate Model Evaluation System (RCMES) [27] with two
principal components, shown on the left and right sides of Fig. 2.3.
The Regional Climate Model Evaluation Database (RCMED) (left side of
Fig. 2.3) was built to warehouse data point tuples (of the form time, lat, lon, value)
from the aforementioned observational datasets and their respective ﬁle formats
ranging from NetCDF, HDF4/5, to GRIB [27]. The RCMED provided an external
user-facing web service allowing spatial/temporal searches (recall Sect.2.2.5 and
the right side of Fig. 2.1) of the relevant observational data for a particular region
of interest. Over the lifespan of the project, we have ingested over 14 billion data
points into RCMED at a rate over 60,000 tuple records per second.

2
Architecting Data-Intensive Software Systems
43
The web service’s primary consumer, the Regional Climate Model Evaluation
Toolkit or RCMET, complemented the RCMES by providing temporal and spatial
regridding of the observational data to match up to the provided climate model
outputs (shown in the right side of Fig. 2.3). This was a computationally intensive
data processing (recall Sect. 2.2.6) task that produced output bias calculations
(demonstrating the model or observational data biases when compared), and other
relevant decision-making outputs.
In the following section, we will discuss a relevant data-intensive system example
in the domain of astronomy.
3.2
Astronomy
Astronomy has had a long history of data intensive problems. Even observers in
the pre-telescopic era could generate stellar catalogs having 1,000 or more entries.
With the invention of the telescope, and then photographic and spectroscopic
systems, the data volume increased dramatically, to the point that it was necessary
to employ “human computers” in order to keep up. In the modern era, telescopes
and numerical simulations are being designed that challenge many aspects of data
intensive processing (Sect. 2.2.6), including processing, data storage, and curation
(Sect. 2.2.2).
Cosmology is the study of the origin and fate of the Universe, and it can
be a science driver for extreme processing. Recent observations have provided a
reckoning of the major constituents of the Universe, namely it is composed of
approximately 4% of baryonic matter, 21% of dark matter, and 75% of dark energy.
Baryonic matter is “normal” matter, composed of proton, neutrons, and electrons;
dark matter is largely unknown, though its nomenclature stems from the fact that
it does not appear to emit or absorb light and its presence can be inferred only
from its gravitational effects; and dark energy is described largely in name only,
as a component that may act effectively as if it has negative pressure but may also
indicate a fundamental breakdown of our understanding of gravity on the largest
scales.
Improving our knowledge of these constituents, and particularly probing the
extent to which their balance may have changed with cosmic time, requires surveys
of a signiﬁcant fraction of the volume of the Universe. There are a number of surveys
in which galaxies are used as point mass tracers of the Universe and which are either
underway or being planned for this decade, and into the next, for ground-based
telescopes. A partial listing includes the Baryon Oscillation Sky Survey (BOSS)
and its successor BigBOSS, the Evolutionary Map of the Universe (EMU), the all-
sky survey with the Large Synoptic Sky Telescope (LSST), and an all-sky survey
with the Square Kilometre Array (SKA).
The EMU survey on the Australian SKA Pathﬁnder (ASKAP) and the SKA
surveys could drive quite extreme processing requirements. Both ASKAP and
the SKA are interferometers, in which one does not obtain an image of the sky

44
C.A. Mattmann et al.
Table 2.1 Telescope parameters and resulting processing requirements
Australian SKA pathﬁnder
(ASKAP)
Square kilometre array
phase 1 (SKA1)
Square kilometre array
phase 2 (SKA2)
Nantenna D 30
Nantenna  250
Nantenna  1;000
Nbeams D 30
Nbeams D 1
Nbeams D 1‹
Nfrequency  16k
Nfrequency  16k?
Nfrequency  16k?
Ntime  4k
Ndata  1:8  1012
Ndata  4  1012
Ndata  65  1012
Nops  18  1010
Nops  40  1015
Nops  650  1015
directly. Rather, an interferometer consists of N individual apertures (telescopes).
Each unique pair of telescopes samples a particular Fourier component of the sky
brightness distribution, and the whole interferometer provides N.N  1//2 Fourier
samples. An image of the sky is obtained by Fourier inverting these N.N 1//2 sam-
ples. In practice, obtaining the image requires more than “just” a Fourier transform,
as various corrections for uneven sampling and instrumental effects must be applied.
Moreover, because of instrumental effects or galaxy emission characteristics or
both, it is desirable to obtain samples at multiple frequencies or wavelengths, and
the Universe is faint, necessitating long integrations. Finally, ASKAP will, and the
SKA may, have a “multi-beaming” technology deployed that allows multiple images
to be obtained simultaneously. These processing requirements are summarized in
Table 2.1.
The total data that must be processed in order to produce a single image is then
Ndata  Nantenna2NbeamNfrequencyNtime
where Nantenna is the number of antennas in the array, Nbeam is the number of “beams”
or independent ﬁelds of view generated by the “multi-beaming” technology,
Nfrequency is the number of frequencies or wavelengths processed, and Ntime is the
number of time samples collected. Signiﬁcantly, current community estimates are
that the number of operations required to correct for instrumental effects could be as
large as 104–105 per datum. Table 2.1 summarizes what the telescope parameters are
for ASKAP and the SKA and the resulting processing requirements. In the case of
the SKA, many of the design choices are not yet ﬁnalized; thus, the values listed in
Table 2.1 should be seen as indicative rather than deﬁnitive. Table 2.1 also assumes
the lower value for the number of operations, namely 104 per datum. Nonetheless,
it is clear that even the conservative assumptions yield processing requirements of
tens to hundreds of Peta-operations.
The Universe can provide a laboratory with which to test fundamental laws of
physics, which in turn can drive data storage requirements. One of the most famous
examples of using the Universe as a proving ground for fundamental theories was
the 1919 solar eclipse expeditions that were mounted, in part, to test a prediction of
Einstein’s recently published Theory of General Relativity (GR). Those expeditions

2
Architecting Data-Intensive Software Systems
45
demonstrated that the stars seen near the limb of the Sun, i.e., for which the line of
sight was strongly affected by the gravitational ﬁeld of the Sun, had their positions
shifted by an amount consistent with that of GR, and inconsistent with Newton’s
Universal Law of Gravitation. Today, of course, GR has entered modern life, as
GR corrections must be incorporated into satellite navigation (e.g., with the Global
Positioning System or GPS).
One modern test of GR uses binary star systems. A particular class of star is a
neutron star, and a subset of neutron stars is detectable as radio pulsars. These stars
are the condensed remnants of massive stars, containing perhaps 150% of the mass
of the Sun packed into a volume of about 10 km in radius, and they produce regularly
repeating pulses of radio radiation. In the best cases, the arrival time of a pulse from
a pulsar can be predicted to better than 100 ns precision. The combination of large
masses and high precision makes pulsars exquisite clocks for GR tests.
The exemplar for such tests of GR is the system PSR B1913C16, which consists
of two neutron stars in an approximately 8 h orbit4 about each other, with one of the
neutron stars detected as a pulsar. The discovery and subsequent precision timing of
the pulses from this system resulted in the 1993 Nobel Prize in Physics. Since the
discovery of PSR B1913C16, there have been a few other such neutron star-neutron
star binaries discovered, including the recent discovery of the double pulsar PSR
J0737  3039, in which both neutron stars have been detected as pulsars; there are
also neutron star-white dwarf5 binaries known that can probe other aspects of GR.
In GR, the most compact object that could exist is a black hole, and there is intense
interest in ﬁnding a black hole-pulsar binary as a number of previously inaccessible
tests of GR would then become possible.
One of the main difﬁculties in conducting such tests is that sufﬁciently useful
pulsars are rare. There are currently approximately 2,000 pulsars known. Of these,
fewer than 10% can be used for high precision GR tests of one kind or another;
indeed the very best pulsars may constitute only a few percent of the total known
population. Estimates of the total (detectable) pulsar population in the Galaxy are
as large as 20,000, suggesting that many more could be found. Moreover, for some
of the tests of interest, it is useful to have pulsars distributed widely on the sky,
which requires that one ultimately conduct a survey over the entire accessible sky
for pulsars.
Searching for pulsars requires large telescopes because pulsars are generally
quite faint. Obtaining sufﬁcient signal to noise on pulsars also often requires
collecting data over a large frequency (or wavelength) span and over a sufﬁcient time
interval. However, pulsar pulses are short durations, of order 1 ms, necessitating
rapid time sampling, and their radio signals are affected by propagation through the
4The size of their orbit is comparable to the diameter of the Sun.
5A white dwarf is the remnant of a star with a mass of about that of the Sun compressed into a
volume about the size of the Earth. The Sun will end its life some ﬁve billion years hence as a
white dwarf.

46
C.A. Mattmann et al.
Table 2.2 Illustrative data volumes generated by Pulsar surveys
Green bank telescope/Arecibo
Parameter
observatory/Parkes telescope
SKA
Dt
20–70 min
30 min
Dt
64–82 s
50 S
Dn
300–800 MHz
800 MHz
Dn
24–100 kHz
20 kHz
Ndata
44–2;200  109 samples
1;440  109 samples
Pixels in the sky
350  103
76  106
Full sky survey
20  1015 samples
4:6  1018 samples
interstellar medium, necessitating maintaining a narrow frequency sampling. For
each “pixel” on the sky, the number of data acquired is then
Ndata D
Dt
dt
 Dn
dn

where Dt is the total integration time, dt is the time sampling, Dn is the total
frequency bandwidth processed, and dn is the frequency channelization or sampling.
Table 2.2 presents illustrative data volume values for modern-day pulsar surveys,
such as those being conducted at the Green Bank Telescope (GBT), the Arecibo
Observatory, or the Parkes Telescope, and a future survey with the SKA.
For a single-dish telescope such as the GBT, Arecibo, or Parkes, a “pixel”
on the sky is deﬁned by the size of the ﬁeld of view; both Parkes and Arecibo
have implemented multi-feed system that effectively provides approximately 10
independent and simultaneous ﬁelds of view, increasing the data volumes by
approximately an order of magnitude. Speciﬁcally, the dimensions of a single pixel
could be typically 16 arcminutes (at a ﬁducial frequency of 800 MHz or a ﬁducial
wavelength of 37 cm), resulting in about 350  103 pixels in total in the sky.
Conversely, for the SKA, because it is an interferometer with much higher angular
resolution, a typical pixel dimension might be 1.2 arcminutes, resulting in as many
as 76  106 pixels in the sky. Table 2.2 also summarizes what the resulting data
volume would be for an all-sky pulsar survey, cast in terms of “samples.” Clearly,
if the data represented by only 1-byte samples, it would be quite easy to obtain
Exabyte data volumes.
Our focus here has been on pulsar surveys, which are conducted at radio
wavelengths. However, ground-based optical surveys, such as to be conducted by
the LSST, could easily generate tens of Terabytes of data per night. For instance,
the LSST envisions having a 3.2 Gpixel camera that is read out every 15 s. Over the
course of a night, 30 TB will be generated, or about 10 PB/year. During the LSST’s
nominal 10 year lifetime, the data volume begins to approach an Exabyte.
Finally, much of this discussion has focused on data to be generated by future
telescopes. Astronomy is a notable science in that the typical time scale of many
phenomena can exceed a human lifetime, often by a large factor. By retaining

2
Architecting Data-Intensive Software Systems
47
the data from a telescope for signiﬁcant durations, it can be possible to probe the
behavior of various kinds of objects on time scales that would not otherwise be
possible. For example, by collecting and combining data from photographic plates
and modern observations, it is possible to reconstruct the brightness variations of
some sources over durations that exceed 100 years. Clearly, data curation over such
durations both opens investigations that otherwise would not be possible – perhaps
not even imagined at the time that the original observations were taken – but also
poses signiﬁcant challenges.
Experience with existing instruments, such as the Hubble Space Telescope (HST)
and the Very Large Array (VLA), also has demonstrated the power of a data archive.
While the archive of neither telescope is yet particularly large, they have proven
quite useful, as the number of scientiﬁc papers being generated by re-analysis of
archival data is now equaling or exceeding the number of scientiﬁc papers being
generated from new observations. Moreover, major telescopes, such as the HST
and VLA, have lifetimes that are measured in decades. Thus, it is likely to become
necessary to provide for data curation on time scales of many decades.
Figure 2.4 demonstrates a recent data processing system that we developed
to process data from the Expanded Very Large Array (EVLA) instrument, a
data-archive centric instrument with more than ten times the VLA’s sensitivity.
EVLA data (the day2 TDEM 0003 10s norx) is disseminated (Sect. 2.2.2) across
the world-wide-web and delivered to a staging area, where it is automatically
curated and metadata is extracted. The data is then ingested into the ﬁle management
component (center of Fig. 2.4) labeled as FM where the extracted metadata is
stored in a catalog (labeled as cat in Fig. 2.4) and the data itself is moved to a
repository (labeled as rep in Fig. 2.4). Separately, a system operator sends an event
to being processing the EVLA Spectral Line Cube (evlascube in Fig. 2.4) task,
a Common Astronomy Software Applications (CASA) [28] program developed
for the EVLA summer school in 2010. The workﬂow management component
(labeled as WM in the middle-right of Fig. 2.4) is responsible for running the CASA
program, which is wrapped in a science algorithm wrapper called CAS-PGE, part
of the Apache OODT project that we will describe further in Sect. 4. The wrapper
communicates with the ﬁle management component, ﬁgures out the locations and
metadata associated with the day2 TDEM 0003 10s norx and then provides that
information to the CASA program so that it can process it and generate a spectral
line cube image. The result image, and calibration tables are ingested into the ﬁle
management component by the wrapper, and made available to external users via
a data portal (recall the upper right portion of Fig. 2.2) that provides search and
dissemination (recall Sects. 2.2.5 and 2.2.2, respectively) of the results to the science
community.
We will wrap up Sect. 3 by describing the domain of snow hydrology, and its
data-intensive challenges, along with a representative data system that we have
constructed to address them.

48
C.A. Mattmann et al.
Fig. 2.4 A prototype architecture demonstrating data processing, and archiving of extended very
large array (EVLA) data
3.3
Snow Hydrology
Snow cover and its melt dominate regional hydrology in many of the world’s
mountainous regions. One-sixth of Earth’s population depends on snow- or glacier-
melt for water resources, and people in these areas generate one-fourth of the global
domestic product [29, 30]. In the Western US, more than 70% of the freshwater
supply comes from snowmelt from the geographically limited mountain ranges.
Recent analyses of long-term surface observations show a declining snowpack
and snow cover in the western US attributable to increasing temperature [31–33].
Investigations in the Colorado River Basin show that radiative forcing by dust from
the Colorado Plateau in the mountain snow cover consistently enhances snowmelt
and leads to a month’s earlier loss of snow cover extent [34].
Today, the western US face signiﬁcant water resource challenges due to increas-
ing demands related to population growth and for economic and environmental

2
Architecting Data-Intensive Software Systems
49
Fig. 2.5 The SnowDS (data system) and its instantiation using Apache OODT
needs. If current trends continue, future demands are expected to increase by 40%
in the next 30 years according to the US Western Governor’s Association and
their 2006 report. The complexity of Western water management gives rise to
the signiﬁcant role of science and the need for improved observations to support
sustainability for generations to come [35].
Resource managers are tasked with projecting run-off amounts in order to
manage reservoirs, dams, and water allocations for several western States and
Mexico. In the past, planners have relied on information developed from historic
observations of stream ﬂow, snow pack, soil moisture and climate drivers coupled
with a sparse network of snow and stream gages as input to decision tools for
seasonal and yearly planning. However, it is becoming clear that this approach is
no longer viable, as historic observations perform poorly under a changing climate
[36], and changes in precipitation and snow patterns hinder accurate assessments of
snow and runoff conditions using the existing system of sensors. The result has been
a decline in the accuracy of water supply forecasts in the western United States.
The data system infrastructure developed to support this effort consists of a
scalable, end-to-end processing environment centered around custom algorithms to
perform enhanced ﬁltering of raw remote sensing observations and a distribution
architecture for the generation and delivery of data products to the National Snow
and Ice Data Center Distributed Active Archive Center (NSIDC-DAAC). The
increased coverage and enhanced accuracy of the data products generated by the
system ﬁll critical gaps in the present snow and ice record. The data system
infrastructure is show in Fig. 2.5.
The ﬁrst task was to obtain the required data to fully process and analyze the
Colorado River Basin (CRB) using a distinct set of MODIS MOD09GA tiles.
Using the Apache OODT data management framework, we immediately started
to download the data into our staging area, where the ﬁles were then crawled
and metadata ingested into the ﬁle management component. OODT’s ﬁle manager
requires a set of XML policy ﬁles to set up the metadata structure for ingestion of

50
C.A. Mattmann et al.
Fig. 2.6 The MODIS dust radiative forcing of snow (MOD-DRFS) algorithm and its processing
products. This “metadata ﬁrst” approach helps to ensure that a common information
model is in place before the data is actively placed in the archive. Figure 2.4
illustrates the data ingestion pipeline that was deployed to support the on-going
snow research project at JPL.
Utilizing the OODT framework offered a variety of time-saving beneﬁts that ex-
tended beyond metadata collection: by simple conﬁguration changes, we were also
able to archive the raw MODIS data ﬁles on disk within a directory structure what
was organized around temporal and geospatial considerations, making downstream
discovery and processing considerably easier.
After just over 2 weeks, the team had obtained over 9 years of MODIS satellite
observations from the upstream archive, and the focus shifted to the processing and
generation of MODIS Dust Radiative Forcing of Snow (MOD-DRFS) products. To
implement the specialized algorithms, we installed the OODT workﬂow manager
and wrapped the native Perl and IDL code used to transform MODIS products
into MOD-DRFS products. Figure2.6 shows the entire multi-staged process used
to generate MOD-DRFS. After a number of exploratory test runs, we estimated
that processing the entire dataset would take just over 16 days of uninterrupted
processing, and decided to investigate the potential time savings that increased
parallel processing might afford.
Apache OODT offers a resource management component that can be utilized to
map workﬂow jobs to remote batch nodes based upon node resource availability,
allowing workﬂow steps to be distributed and managed across multiple compute

2
Architecting Data-Intensive Software Systems
51
nodes. The team conﬁgured the software and installed batch stubs on another
machine, and in 2 days we started processing MOD-DRFS across eight nodes
using a standard conﬁguration. With additional testing and conﬁguration, the nodes
were further optimized for better performance. Less than 4 days (a 76% reduction
in processing time) later, all of the MOD-DRFS data was created, with metadata
extracted and archived back into the ﬁle manager.
In the very near term, we expect that the primary signiﬁcance of this system
will be a dramatic reduction in the amount of time necessary to process multiple
years of remote-sensing data for the purpose of determining snow and ice cover
in the western United States. The development of a full-featured data system
infrastructure will provide operational beneﬁts in the areas of scalability, reliability,
and repeatability that are not currently achievable. By increasing the efﬁciency with
which large volumes of remote-sensing data can be processed, we expect to be able
to generate a large volume of data products immediately relevant to ongoing policy
and decision-support efforts.
4
The Role of Middleware and the Road Ahead
The ﬁeld of data intensive software systems has blossomed recently as the relentless
advance of technology has begun to put massive amounts of computational power
and storage capacity within reach of projects with more modest budgets. As a
result, more and more organizations are discovering the possibilities, insights, and
competitive advantages to be had from the creative application of algorithms to
massive collections of data. As this proliferation of software (much of it open
source recall Sect. 2.2.5) for managing different aspects of the data management
process continues to expand, we believe that middleware software packages that
facilitate the integration of these disparate components into end-to-end data pro-
cessing pipelines like the ones discussed in the previous section will play an
increasingly prominent role. In this section we will describe one such middleware
framework, the Apache Software Foundation’s Object Oriented Data Technology
(OODT) project [5], to illustrate how middleware speciﬁcally designed with the
challenges of manipulating massive datasets in mind is becoming an essential
element for rapidly composing robust, reusable systems for data management and
analysis.
Apache OODT represents a Domain Speciﬁc Software Architecture (DSSA) [37]
that grew out of a more than a decade of effort NASA’s Jet Propulsion Laboratory
(JPL) in Pasadena, California, in the area of designing robust data management
solutions for NASA’s planetary and Earth science missions. The scientiﬁc diver-
sity of these missions, which include investigations into climate, physics, space
exploration, and even the medical ﬁelds (as you will recall from the descriptions
in Sect. 3), had, before OODT, led investigators to reinvent the data system from
scratch each time. Yet, despite their apparently unrelated scientiﬁc goals, each

52
C.A. Mattmann et al.
of these missions largely shared a core set of common data management and
processing needs (recall Sect. 2.2), and faced a largely similar set of fundamental
challenges. This insight, that underneath the differences in semantic interpretation
of the data, the basic processing needs could be addressed through common,
architecturally principled components, was the driving force behind the original
development of OODT.
OODT’s strength is that its architecture and evolution has never been driven
by any one particular domain, but rather has been informed over the years by
continuous input from a broad spectrum of scientiﬁc efforts. As a result, OODT has
implemented a ﬂexible set of domain-agnostic components that users can pick and
choose from to create a platform on which focused, domain-speciﬁc solutions can
be built. As a concrete example, OODT does not contain algorithms for processing
satellite imagery and generating higher order products from raw data. Rather, it
provides a highly conﬁgurable, wrappable workﬂow processing framework that can
be combined with a ﬁle management component to dramatically reduce the time
required to string together such domain-speciﬁc processing algorithms into a full-
ﬂedged data-intensive pipeline.
As mentioned before, the beneﬁt that middleware packages such as OODT pro-
vide to the modern data-intensive software system is that they enable such systems
to be composed out of reusable, loosely-connected components that communicate
among one another over standard interfaces and open protocols. This architecture
contrasts with the monolithic “silo” approach often adopted for “one-off” solutions,
and offers several distinct advantages.
In particular, because the component pieces of the data system are separable, the
system itself can be made more resilient to technological evolution and changing
requirements. As illustrated by the discussion from Sect. 2.2, data-intensive soft-
ware systems are expected to cope with increasing data volumes and processing
complexity, making this advantage particularly appealing.
The primary beneﬁt of leveraging a technology like OODT in the construction of
a data-intensive software system is its ability to act as a glue layer, facilitating com-
munication and interaction between distributed, possibly heterogeneous upstream
data sources, scientiﬁc processing algorithms, data archiving infrastructure, and data
dissemination technologies. In the following section we describe a representative
cross-section of examples where integration between OODT and a variety of
enterprise-class open source software facilitate the rapid development of massively
scalable pipelines for data management and processing.
With a ﬁrm understanding of the architectural challenges related to data-intensive
systems and with concrete examples of the manifestation of those challenges
and approaches to deal with them under our belts, we will describe in detail
the Apache Object Oriented Data Technology (OODT) project [5] and our ex-
perience using it as a framework for addressing the challenges of data-intensive
systems.

2
Architecting Data-Intensive Software Systems
53
4.1
Apache Object Oriented Data Technology
Rather than seeking to develop a domain-speciﬁc solution end to end from scratch,
the prevalence of industrial-strength open-source software, freely maintained by
communities of subject-matter experts, makes it easier than ever to obtain best-in-
class solutions to speciﬁc aspects of the data management pipeline, and to leverage
middleware packages like OODT to yoke together these individual workhorses into
a powerful, purpose-driven pipeline.
The major components that comprise Apache OODT can be broken down into
two families that we will discuss further below.
4.1.1
Information Integration Components
OODT’s information integration components help users search, access, and dis-
seminate data and metadata. The Proﬁle Server component delivers descriptions of
data, or metadata, including the Dublin Core [38] set of metadata elements, and cus-
tomized ISO-11179 [39] extensions for resource location, and for mission-speciﬁc
annotations (e.g., latitude and longitude for geo-located ﬁles; or mission and target
for planetary science ﬁles). Proﬁle servers describe data delivered back by Product
Server components. Product servers are responsible for hiding the uniqueness of
backend data stores and repositories of science data ﬁles, and delivering back the
data from them seamlessly. Query Servers unite the product and proﬁle servers by
using the proﬁle servers to locate resources (even other proﬁle servers and product
servers) that match a provided query and then packaging up the resultant data ﬁles
and metadata and delivering it back to the user automatically, or interactively. These
components directly deal with the key challenges of data dissemination, search,
open source and information modeling (recall Sects. 2.2.2, 2.2.4, 2.2.5, and 2.2.7
respectively). The information integration components are useful when write-access
to the underlying repositories and catalogs are not available, yet the use case calls for
unobtrusively exposing the collected data and metadata and presenting it externally
via search and dissemination techniques.
In the next section we will describe the OODT data processing components.
4.1.2
Data Processing Components
OODT’s data processing components include a triumverate of services. The ﬁle
management component, workﬂow management component and resource manage-
ment components catalog and archive ﬁles and metadata in repositories and catalogs
automatically, and interactively (directly supporting curation recall Sect. 2.2.3 and
the left-middle portion of Fig. 2.2 and dissemination recall Sect. 2.2.2 and the left-
middle portion of Fig. 2.2); data-ﬂow and control-ﬂow orchestration (recall the

54
C.A. Mattmann et al.
middle-bottom portions of Fig. 2.2 and Sect. 2.2.6) and management of underlying
hardware resources, be it grids, clouds, clusters and compute nodes (middle portions
of Fig. 2.2 and Sect. 2.2.1), respectively.
Several client frameworks are part of the data processing components. The
combination of a pushpull component that acquires remote data ﬁles and metadata
negotiating various protocols (FTP, SCP, etc.) to obtain it and an automatic ﬁle
identiﬁcation and crawling framework assist in getting ﬁles and metadata into the
ﬁle manager component. A science algorithm wrapper (recall the discussion in
Sect. 2.2.6) called CAS-PGE provides an unobtrusive interface to the ecosystem
of data processing services in order to stage ﬁles, metadata, and input into the
algorithm, to execute it, and to record its output and provenance.
It is no coincidence that the major elements of data-intensive processing,
ingestion, and dissemination all coincide with this family of OODT components.
The components were designed, from ﬁrst principles, with the goal of addressing
the key challenges of data-intensive systems, and with an eye towards ﬂexibility,
and extensibility to accommodate inevitable technology change and evolution.
5
Conclusion
In this chapter, we have strived to provide an overview of the relevant architectural
areas and of seven key challenges associated with data-intensive systems. The
chapter serves two primary purposes. First, it provides a contextual overview of
the important architectural components, techniques and architectural patterns for
science data systems, an important cross-section of the realm of data-intensive
systems that you will cover in this handbook. Second, the chapter frames the
discussion of these challenges and architectural patterns within the context of
three real world examples in regional climate modeling, astronomy, and in snow
hydrology. Each of these scientiﬁc domains presents many important challenges
in data ingestion, processing, curation, dissemination, search, delivery, and the
remainder of the relevant architectural areas discussed.
Throughout the chapter, our goal was to optimize for breadth rather than depth
in any one particular issue. As our chapter is an overview of architecture for data-
intensive systems, its goal is to ground the discussion of later relevant, speciﬁc data-
intensive examples discussed in later chapters.
The material presented therein including the architecture and the challenges
serve as a roadmap and concrete research agenda for areas of improvement and
fundamental research in data-intensive systems. All of these areas are being actively
explored by many world-class institutions and researchers and progress is being
made. The future in the data-intensive systems domain is bright!
Acknowledgements This work was conducted at the Jet Propulsion Laboratory, California
Institute of Technology under contract to the National Aeronautics and Space Administration. The
authors would like to thank the editors of the book for their resolve to publish the book and to work
with the authors’ tenuous work schedules to get this chapter published.

2
Architecting Data-Intensive Software Systems
55
References
1. H. Rottgering, LOFAR, a new low frequency radio telescope. New Astronomy Reviews, Volume
47, Issues 4–5, High-redshift radio galaxies - past, present and future, September 2003, Pages
405–409.
2. http://twitter.com/#!/chrismattmann/status/66141594474127361.
3. C. Mattmann. Software Connectors for Highly Distributed and Voluminous Data-Intensive
Systems. Ph.D. Dissertation. University of Southern California, 2007.
4. R. T. Kouzes, G. A. Anderson, S. T. Elbert, I Gorton, D. K. Gracio, The Changing Paradigm
of Data-Intensive Computing. Computer, vol.42, no.1, pp.26–34, Jan. 2009.
5. C. Mattmann, D. Crichton, N. Medvidovic and S. Hughes. A Software Architecture-Based
Framework for Highly Distributed and Data Intensive Scientiﬁc Applications. In Proceedings
of the 28th International Conference on Software Engineering (ICSE06), Software Engineering
Achievements Track, pp. 721–730, Shanghai, China, May 20th–28th, 2006.
6. C. Mattmann, D. Freeborn, D. Crichton, B. Foster, A. Hart, D. Woollard, S. Hardman,
P. Ramirez, S. Kelly, A. Y. Chang, C. E. Miller. A Reusable Process Control System Framework
for the Orbiting Carbon Observatory and NPP Sounder PEATE missions. In Proceedings of the
3rd IEEE Intl Conference on Space Mission Challenges for Information Technology (SMC-IT
2009), pp. 165–172, July 19–23, 2009.
7. T. White. Hadoop: The Deﬁnitive Guide. 2nd Edition, O’Reilly, 2010.
8. P. Couvares, T. Kosar, A. Roy, J. Weber, K. Wenger. Workﬂow Management in Condor. In
Workﬂows for e-Science. I. J. Taylor, E. Deelman, D. B. Gannon, M. Shields, eds. Springer
London, pp. 357–375, 2007.
9. Y. Gil, V. Ratnakar, K. Jihie, J. Moody, E. Deelman, P.A Gonz´alez-Calero, P. Groth. Wings:
Intelligent Workﬂow-Based Design of Computational Experiments. IEEE Intelligent Systems.
vol.26, no.1, pp.62–72, Jan.-Feb. 2011.
10. D. Woollard, N. Medvidovic, Y. Gil, and C. Mattmann. Scientiﬁc Software as Workﬂows: From
Discovery to Distribution. IEEE Software – Special Issue on Developing Scientiﬁc Software,
Vol. 25, No. 4, July/August, 2008.
11. Science Gateways Group, Indiana University Pervasive Technologies Institute, http://pti.iu.
edu/sgg,Accessed:July2011.
12. D. N. Williams, R. Ananthakrishnan, D. E. Bernholdt, S. Bharathi, D. Brown, M. Chen,
A. L. Chervenak, L. Cinquini, R. Drach, I. T. Foster, P. Fox, D. Fraser, J. Garcia, S. Hankin,
P. Jones, D. E. Middleton, J. Schwidder, R. Schweitzer, R. Schuler, A. Shoshani, F. Siebenlist,
A. Sim, W. G. Strand, M. Su, N. Wilhelmi, The Earth System Grid: Enabling Access to
Multi-Model Climate Simulation Data, in the Bulletin of the American Meteorological Society,
February 2009.
13. J. Tran, L. Cinquini, C. Mattmann, P. Zimdars, D. Cuddy, K. Leung, O. Kwoun, D. Crichton
and D. Freeborn. Evaluating Cloud Computing in the NASA DESDynI Ground Data System.
In Proceedings of the ICSE 2011 Workshop on Software Engineering for Cloud Computing -
SECLOUD, Honolulu, HI, May 22, 2011.
14. M. McCandless, E. Hatcher, and O. Gospodneti. Lucene in Action, Manning Publications, 532
pages, 2011.
15. C. Mattmann, D. Crichton, J. S. Hughes, S. Kelly, S. Hardman, R. Joyner and P. Ramirez.
A Classiﬁcation and Evaluation of Data Movement Technologies for the Delivery of Highly
Voluminous Scientiﬁc Data Products. In Proceedings of the NASA/IEEE Conference on Mass
Storage Systems and Technologies (MSST2006), pp. 131–135, College Park, Maryland, May
15–18, 2006.
16. A. Hart, C. Mattmann, J. Tran, D. Crichton, H. Kincaid, J. S. Hughes, S. Kelly, K. Anton,
D. Johnsey, C. Patriotis. Enabling Effective Curation of Cancer Biomarker Research Data. In
Proceedings of the 22nd IEEE International Symposium on Computer-Based Medical Systems
(CBMS), Albuquerque, NM, August 3rd–4th, 2009.

56
C.A. Mattmann et al.
17. A. Hart, J. Tran, D. Crichton, K. Anton, H. Kincaid, S. Kelly, J.S. Hughes and C. Mattmann. An
Extensible Biomarker Curation Approach and Software Infrastructure for the Early De- tection
of Cancer. In Proceedings of the IEEE Intl. Conference on Health Informatics, pp. 387–392,
Porto, Portugal, January 14–17, 2009.
18. C. Lynch. Big data: How do your data grow? Nature, 455:28–29, 2008.
19. N. R. Mehta, N. Medvidovic, and S. Phadke. 2000. Towards a taxonomy of software
connectors. In Proceedings of the 22nd international conference on Software engineering
(ICSE ’00). ACM, New York, NY, USA, 178–187.
20. J. Yu, R. Buyya. A Taxonomy of Workﬂow Management Systems for Grid Computing. J. Grid
Comput., 2005: 171200.
21. D. Woollard, C. Mattmann, and N. Medvidovic. Injecting Software Architectural Constraints
into Legacy Scientiﬁc Applications. In Proceedings of the ICSE 2009 Workshop on Software
Engineering for Computational Science and Engineering, pp. 65–71, Vancouver, Canada,
May 23, 2009.
22. M. Uschold and G. M., Ontologies and Semantics for Seamless Connectivity. SIGMOD
Record, vol. 33, 2004.
23. L. F. Richardson. Weather prediction by numerical process, Cambridge University Press, 1922.
24. J. Kim. Precipitation and snow budget over the southwestern United Sates during the
1994–1995 winter season in a mesoscale model simulation. Water Res. 33, 2831–2839, 1997.
25. J. Kim, R. T. Kim, W. Arritt, and N. Miller. Impacts of increased atmopheric CO2 on the
hydroclimate of the Western United States. J. Climate 15, 1926–1942, 2002.
26. F. M. Ralph, P.J. Neiman, and G.A. Wick, 2004. Satellite and CALJET aircraft observations of
atmospheric rivers over the eastern North Paciﬁc Ocean during the winter of 1997/1998, Mon.
Weather Rev., 132, 1721–1745.
27. A. Hart, C. Goodale, C. Mattmann, P. Zimdars, D. Crichton, P. Lean, J. Kim, and D. Waliser.
A Cloud-Enabled Regional Climate Model Evaluation System. In Proceedings of the ICSE
2011 Workshop on Software Engineering for Cloud Computing - SECLOUD, Honolulu, HI,
May 22, 2011.
28. J. P. McMullin, B. Water, D. Schiebel, W. Young, K. Golap. CASA Architecture and
Applications, Proceedings of Astronomical Data Analysis Software and Systems, Vol. 376,
p. 127, October 2006.
29. C. R. Bales., N. P. Molotch, T. H. Painter, M. D. Dettinger, R. Rice, and J. Dozie. Mountain
Hydrology of the Western United States, Water Resources Research, in press., 2006.
30. T. P Barnett, J. C. Adam, and D. P. Lettenmaier. Potential impacts of a warming climate on
water availability in snow-dominated regions, Nature, 438, doi:10.1038/nature04141, 2005.
31. T. P. Barnett et al. Human-induced changes in the hydrology of the western United States,
Science, 319(5866), 1080–1083, 2008.
32. P. W. Mote, A. F. Hamlet, M. P. Clark, and D. P. Lettenmaier. Declining mountain snowpack in
western North America, Bulletin of the American Meteorological Society, 86(1), 39–49, 2005.
33. D. W. Pierce, et al. Attribution of declining western U.S. snowpack to human effects, Journal
of Climate, 21, 6425–6444, 2008.
34. T. H. Painter, A. P. Barrett, C. C. Landry, J. C. Neff, M. P. Cassidy, C. R. Lawrence,
K. E. McBride, and G. L. Farmer. Impact of disturbed desert soils on duration of mountain
snow cover, Geophysical Research Letters, 34, 2007.
35. M. T. Anderson and J. Lloyd H. Woosley. Water availability for the Western United States –
Key Scientiﬁc Challenges, US Geological Survey Circular, 1261(85), 2005.
36. P. C. D. Milly, J. Betancourt, M. Falkenmark, R. Hirsch, Z. Kundzweicz, D. Lettenmaier, and
R. Stouffer. Stationarity is Dead, Wither Water Management?, Science, 319(5863), 573–574,
2008.
37. W. Tracz. 1995. DSSA (Domain-Speciﬁc Software Architecture): pedagogical example.
SIGSOFT Softw. Eng. Notes 20, 3 (July 1995), 49–62.

2
Architecting Data-Intensive Software Systems
57
38. S. Weibel, J. Kunze, C. Lagoze and M. Wolf, Dublin Core Metadata for Resource Discovery,
Number 2413 in IETF, The Internet Society, 1998.
39. Home Page for ISO/IEC 11179 Information Technology, http://metadata-stds.org/11179/,
Accessed:July2011.
40. National Radio Astronomy Observatory Innovations in Data-Intensive Astronomy Workshop,
http://www.nrao.edu/meetings/bigdata/,Accessed:06/27/11.


Chapter 3
ECL/HPCC: A Uniﬁed Approach to Big Data
Anthony M. Middleton, David Alan Bayliss, and Gavin Halliday
1
Introduction
As a result of the continuing information explosion, many organizations are
experiencing what is now called the “Big Data” problem. This results in the
inability of organizations to effectively use massive amounts of their data in datasets
which have grown too big to process in a timely manner. Data-intensive computing
represents a new computing paradigm [26] which can address the big data problem
using high-performance architectures supporting scalable parallel processing to
allow government, commercial organizations, and research environments to process
massive amounts of data and implement new applications previously thought to be
impractical or infeasible.
The fundamental challenges of data-intensive computing are managing and
processing exponentially growing data volumes, signiﬁcantly reducing associated
data analysis cycles to support practical, timely applications, and developing new
algorithms which can scale to search and process massive amounts of data. Re-
searchers at LexisNexis believe that the answer to these challenges are (1) a scalable,
integrated computer systems hardware and software architecture designed for paral-
lel processing of data-intensive computing applications, and (2) a new programming
paradigm in the form of a high-level declarative data-centric programming language
designed speciﬁcally for big data processing. This chapter explores the challenges of
data-intensive computing from a programming perspective, and describes the ECL
programming language and the open source High-Performance Cluster Computing
(HPCC) architecture designed for data-intensive exascale computing applications.
ECL is also compared to Pig Latin, a high-level language developed for the Hadoop
MapReduce architecture.
A.M. Middleton () • D.A. Bayliss • G. Halliday
LexisNexis, Boca Raton, FL, USA
e-mail: Tony.Middleton@lexisnexis.com; David.Bayliss@LexisNexis.com;
Gavin.Halliday@LexisNexis.com
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 3, © Springer Science+Business Media, LLC 2011
59

60
A.M. Middleton et al.
1.1
Data-Intensive Computing Applications
High-Performance Computing (HPC) is used to describe computing environments
which utilize supercomputers and computer clusters to address complex computa-
tional requirements or applications with signiﬁcant processing time requirements or
which require processing of signiﬁcant amounts of data. Computing approaches can
be generally classiﬁed as either compute-intensive, or data-intensive [19, 25, 40].
HPC has generally been associated with scientiﬁc research and compute-intensive
types of problems, but more and more HPC technology is appropriate for both
compute-intensive and data-intensive applications. HPC platforms utilize a high-
degree of internal parallelism and tend to use specialized multi-processors with
custom memory architectures which have been highly-optimized for numerical
calculations [15]. Supercomputers also require special parallel programming tech-
niques to take advantage of its performance potential.
Compute-intensive is used to describe application programs that are compute
bound. Such applications devote most of their execution time to computational
requirements as opposed to I/O, and typically require small volumes of data.
HPC approaches to compute-intensive applications typically involves parallelizing
individual algorithms within an application process, and decomposing the overall
application process into separate tasks, which can then be executed in parallel on an
appropriate computing platform to achieve overall higher performance than serial
processing. In compute-intensive applications, multiple operations are performed
simultaneously, with each operation addressing a particular part of the problem.
This is often referred to as functional parallelism or control parallelism [1].
Data-intensive is used to describe applications that are I/O bound or with
a need to process large volumes of data [18, 19, 25]. Such applications devote
most of their processing time to I/O and movement of data. HPC approaches to
data-intensive applications typically use parallel system architectures and involves
partitioning or subdividing the data into multiple segments which can be processed
independently using the same executable application program in parallel on an
appropriate computing platform, then reassembling the results to produce the
completed output data [32]. The greater the aggregate distribution of the data, the
more beneﬁt there is in parallel processing of the data. Gorton et al. [19] state
that data-intensive processing requirements normally scale linearly according to
the size of the data and are very amenable to straightforward parallelization. The
fundamental challenges for data-intensive computing according to Gorton et al. [19]
are managing and processing exponentially growing data volumes, signiﬁcantly re-
ducing associated data analysis cycles to support practical, timely applications, and
developing new algorithms which can scale to search and process massive amounts
of data.
Today a desktop PC has more computing power than the supercomputers which
existed ten years ago. This has led to a new trend in supercomputer design for
high-performance computing: using clusters of independent processors connected
in parallel [9]. Many computing problems are suitable for parallelization; often

3
ECL/HPCC: A Uniﬁed Approach to Big Data
61
problems can be divided in a manner that each independent processor can work
on a portion of the problem by simply dividing the data to be processed and
the processing results of each portion combined. This type of parallelism is often
referred to as data-parallelism, and data-parallel applications are a potential solution
to petabyte scale data processing requirements [32,38].
1.2
Data-Parallelism
According to Agichtein [3], parallelization is considered to be an attractive al-
ternative for processing extremely large collections of data such as the billions
of documents on the Web [2]. Nyland et al. [32] deﬁne data-parallelism as a
computation applied independently to each data item of a set of data which allows
the degree of parallelism to be scaled with the volume of data. According to Nyland
et al. [32], the most important reason for developing data-parallel applications is the
potential for scalable performance, and may result in several orders of magnitude
performance improvement. The key issues with developing applications using data-
parallelism are the choice of the algorithm, the strategy for data decomposition, load
balancing on processing nodes, message passing communications between nodes,
and the overall accuracy of the results [32, 39]. Nyland et al. [32] also note that
the development of a data-parallel application can involve substantial programming
complexity to deﬁne the problem in the context of available programming tools,
and to address limitations of the target architecture. Information extraction from
and indexing of Web documents is typical of data-intensive processing which can
derive signiﬁcant performance beneﬁts from data-parallel implementations since
Web and other types of document collections can typically then be processed in
parallel [2].
1.3
The “Big Data” Problem
The rapid growth of the Internet and World Wide Web has led to vast amounts of
information available online. In addition, business and government organizations
create large amounts of both structured and unstructured information which needs
to be processed, analyzed, and linked. Vinton Cerf of Google has described this
as an “Information Avalanche” and has stated “we must harness the Internet’s
energy before the information it has unleashed buries us.” [11] An IDC white
paper sponsored by EMC estimated the amount of information currently stored
in a digital form in 2007 at 281 exabytes and the overall compound growth rate
at 57% with information in organizations growing at even a faster rate [16]. In
another study of the so-called information explosion it was estimated that 95% of
all current information exists in unstructured form with increased data processing
requirements compared to structured information [29]. The storing, managing,

62
A.M. Middleton et al.
accessing, and processing of this vast amount of data represents a fundamental need
and an immense challenge in order to satisfy needs to search, analyze, mine, and
visualize this data as information [7]. These challenges are now simple described
in the literature as the “Big Data” problem. In the next section, we will enumerate
some of the characteristics of data-intensive computing systems which can address
the problems associated with processing big data.
2
Data-Intensive Computing Platforms
The National Science Foundation believes that data-intensive computing requires
a “fundamentally different set of principles” than current computing approaches
[31]. Through a funding program within the Computer and Information Science and
Engineering area, the NSF is seeking to “increase understanding of the capabilities
and limitations of data-intensive computing.” The key areas of focus are:
•
Approaches to parallel programming to address the parallel processing of data
on data-intensive systems
•
Programming abstractions including models, languages, and algorithms which
allow a natural expression of parallel processing of data
•
Design of data-intensive computing platforms to provide high levels of reliability,
efﬁciency, availability, and scalability.
•
Identifying applications that can exploit this computing paradigm and determin-
ing how it should evolve to support emerging data-intensive applications.
Paciﬁc Northwest National Labs has deﬁned data-intensive computing as “captur-
ing, managing, analyzing, and understanding data at volumes and rates that push the
frontiers of current technologies.” [26, 37] They believe that to address the rapidly
growing data volumes and complexity requires “epochal advances in software,
hardware, and algorithm development” which can scale readily with size of the
data and provide effective and timely analysis and processing results. The ECL
programming language and HPCC architecture developed by LexisNexis represents
such an advance in capabilities.
2.1
Cluster Conﬁgurations
Current data-intensive computing platforms use a “divide and conquer” parallel
processing approach combining multiple processors and disks conﬁgured in large
computing clusters connected using high-speed communications switches and
networks which allows the data to be partitioned among the available computing
resources and processed independently to achieve performance and scalability based
on the amount of data (Fig. 3.1). Buyya et al. [10] deﬁne a cluster as “a type of
parallel and distributed system, which consists of a collection of inter-connected

3
ECL/HPCC: A Uniﬁed Approach to Big Data
63
Fig. 3.1 Commodity hardware cluster [33]
stand-alone computers working together as a single integrated computing resource.”
This approach to parallel processing is often referred to as a “shared nothing”
approach since each node consisting of processor, local memory, and disk resources
shares nothing with other nodes in the cluster. In parallel computing this approach
is considered suitable for data processing problems which are “embarrassingly
parallel”, i.e. where it is relatively easy to separate the problem into a number of
parallel tasks and there is no dependency or communication required between the
tasks other than overall management of the tasks. These types of data processing
problems are inherently adaptable to various forms of distributed computing
including clusters and data grids and cloud computing.
2.2
Common Platform Characteristics
There are several important common characteristics of data-intensive computing
systems that distinguish them from other forms of computing. First is the principle
of collocation of the data and programs or algorithms to perform the computation.
To achieve high performance in data-intensive computing, it is important to
minimize the movement of data [20]. In direct contrast to other types of computing
and high-performance computing which utilize data stored in a separate repository
or servers and transfer the data to the processing system for computation, data-
intensive computing uses distributed data and distributed ﬁle systems in which data
is located across a cluster of processing nodes, and instead of moving the data,
the program or algorithm is transferred to the nodes with the data that needs to be
processed. This principle – “Move the code to the data” – is extremely effective
since program size is usually small in comparison to the large datasets processed
by data-intensive systems and results in much less network trafﬁc since data can
be read locally instead of across the network. This characteristic allows processing

64
A.M. Middleton et al.
algorithms to execute on the nodes where the data resides reducing system overhead
and increasing performance [19].
A second important characteristic of data-intensive computing systems is the
programming model utilized. Data-intensive computing systems utilize a machine-
independent approach in which applications are expressed in terms of high-level
operations on data, and the runtime system transparently controls the scheduling,
execution, load balancing, communications, and movement of programs and data
across the distributed computing cluster [8]. The programming abstraction and
language tools allow the processing to be expressed in terms of data ﬂows and
transformations incorporating new dataﬂow programming languages and shared
libraries of common data manipulation algorithms such as sorting. Conventional
high-performance computing and distributed computing systems typically utilize
machine dependent programming models which can require low-level programmer
control of processing and node communications using conventional imperative
programming languages and specialized software packages which adds complexity
to the parallel programming task and reduces programmer productivity. A machine
dependent programming model also requires signiﬁcant tuning and is more sus-
ceptible to single points of failure. The ECL programming language described
in this chapter was speciﬁcally designed to address data-intensive computing
requirements.
A third important characteristic of data-intensive computing systems is the focus
on reliability and availability. Large-scale systems with hundreds or thousands of
processing nodes are inherently more susceptible to hardware failures, communica-
tions errors, and software bugs. Data-intensive computing systems are designed to
be fault resilient. This includes redundant copies of all data ﬁles on disk, storage of
intermediate processing results on disk, automatic detection of node or processing
failures, and selective re-computation of results. A processing cluster conﬁgured
for data-intensive computing is typically able to continue operation with a reduced
number of nodes following a node failure with automatic and transparent recovery
of incomplete processing.
A ﬁnal important characteristic of data-intensive computing systems is the inher-
ent scalability of the underlying hardware and software architecture. Data-intensive
computing systems can typically be scaled in a linear fashion to accommodate
virtually any amount of data, or to meet time-critical performance requirements by
simply adding additional processing nodes to a system conﬁguration in order to
achieve high processing rates and throughput. The number of nodes and processing
tasks assigned for a speciﬁc application can be variable or ﬁxed depending on the
hardware, software, communications, and distributed ﬁle system architecture. This
scalability allows computing problems once considered to be intractable due to the
amount of data required or amount of processing time required to now be feasible
and affords opportunities for new breakthroughs in data analysis and information
processing.

3
ECL/HPCC: A Uniﬁed Approach to Big Data
65
3
HPCC Platform
3.1
Background
A variety of system architectures have been implemented for data-intensive and
large-scale data analysis applications including parallel and distributed relational
database management systems which have been available to run on shared nothing
clusters of processing nodes for more than two decades [35]. These include database
systems from Teradata, Netezza, Vertica, and Exadata/Oracle and others which
provide high-performanceparallel database platforms. Although these systems have
the ability to run parallel applications and queries expressed in the SQL language,
they are typically not general-purpose processing platforms and usually run as
a back-end to a separate front-end application processing system. Although this
approach offers beneﬁts when the data utilized is primarily structured in nature
and ﬁts easily into the constraints of a relational database, and often excels for
transaction processing applications, most data growth is with data in unstructured
form [16] and new processing paradigms with more ﬂexible data models were
needed. Internet companies such as Google, Yahoo, Microsoft, Facebook, and others
required a new processing approach to effectively deal with the enormous amount of
Web data for applications such as search engines and social networking. In addition,
many government and business organizations were overwhelmed with data that
could not be effectively processed, linked, and analyzed with traditional computing
approaches.
Several solutions have emerged including the MapReduce architecture pioneered
by Google and now available in an open source implementation called Hadoop
used by Yahoo, Facebook, and others. LexisNexis developed and implemented a
scalable platform for data-intensive computing called HPCC (High-Performance
Computing Cluster) also available in open source and used by LexisNexis and
other commercial and government organizations to process large volumes of
structured and unstructured data. Similar approaches using commodity computing
clusters including Sector/Sphere [21–23], SCOPE/Cosmos [12], DryadLINQ [42],
Meandre [28], and GridBatch [27] recently described in the literature are also
suitable for data-intensive cloud computing applications and represent additional
alternatives.
3.2
HPCC System Architecture
The development of the open source HPCC computing platform by the Seisint
subsidiary of LexisNexis began in 1999 and applications were in production by late
2000. The conceptual vision for this computing platform is depicted in Fig. 3.2.
The LexisNexis approach also utilizes commodity clusters of hardware running
the Linux operating system as shown in Fig. 3.1. Custom system software and

66
A.M. Middleton et al.
Fig. 3.2 LexisNexis vision for a data-intensive supercomputer
middleware components were developed and layered on the base Linux operating
system to provide the execution environment and distributed ﬁlesystem support
required for data-intensive computing. Because LexisNexis recognized the need
for a new computing paradigm to address its growing volumes of data, the design
approach included the deﬁnition of a new high-level language for parallel data
processing called ECL (Enterprise Control Language). The power, ﬂexibility, ad-
vanced capabilities, speed of development, and ease of use of the ECL programming
language is the primary distinguishing factor between the LexisNexis HPCC and
other data-intensive computing solutions. The following provides an overview of
the HPCC systems architecture and the ECL language.
LexisNexis developers recognized that to meet all the requirements of data-
intensive computing applications in an optimum manner required the design and
implementation of two distinct processing environments, each of which could be
optimized independently for its parallel data processing purpose. The ﬁrst of these
platforms is called a Data Reﬁnery whose overall purpose is the general processing
of massive volumes of raw data of any type for any purpose but typically used for
data cleansing and hygiene, ETL processing of the raw data (extract, transform,
load), record linking and entity resolution, large-scale ad-hoc analysis of data, and
creation of keyed data and indexes to support high-performance structured queries
and data warehouse applications. The Data Reﬁnery is also referred to as Thor, a
reference to the mythical Norse god of thunder with the large hammer symbolic
of crushing large amounts of raw data into useful information. A Thor system is
similar in its hardware conﬁguration, function, execution environment, ﬁlesystem,
and capabilities to the Hadoop MapReduce platform, but offers signiﬁcantly higher
performance in equivalent conﬁgurations.

3
ECL/HPCC: A Uniﬁed Approach to Big Data
67
Fig. 3.3 HPCC Thor processing cluster
The Thor processing cluster is depicted in Sect. 4.3. In addition to the Thor
master and slave nodes, additional auxiliary and common components are needed
to implement a complete HPCC processing environment. The actual number of
physical nodes required for the auxiliary components is determined during the
conﬁgurations process.
The second of the parallel data processing platforms designed and implemented
by LexisNexis is called the Data Delivery Engine. This platform is designed as an
online high-performance structured query and analysis platform or data warehouse
delivering the parallel data access processing requirements of online applications
through Web services interfaces supporting thousands of simultaneous queries and
users with sub-second response times. High-proﬁle online applications developed
by LexisNexis such as Accurint utilize this platform. The Data Delivery Engine
is also referred to as Roxie, which is an acronym for Rapid Online XML Inquiry
Engine. Roxie uses a special distributed indexed ﬁlesystem to provide parallel
processing of queries. A Roxie system is similar in its function and capabilities to
Hadoop with HBase and Hive capabilities added, but provides signiﬁcantly higher
throughput since it uses a more optimized execution environment and ﬁlesystem for
high-performance online processing. Most importantly, both Thor and Roxie sys-
tems utilize the same ECL programming language for implementing applications,

68
A.M. Middleton et al.
Fig. 3.4 HPCC Roxie processing cluster
increasing continuity and programmer productivity. The Roxie processing cluster is
depicted in Fig. 3.4.
The implementation of two types of parallel data processing platforms (Thor
and Roxie) in the HPCC processing environment serving different data processing
needs allows these platforms to be optimized and tuned for their speciﬁc purposes
to provide the highest level of system performance possible to users. This is a
distinct advantage when compared to Hadoop where the MapReduce architecture
must be overlaid with additional systems such as HBase, Hive, and Pig which have
different processing goals and requirements, and don’t always map readily into the
MapReduce paradigm. In addition, the LexisNexis HPCC approach incorporates
the notion of a processing environment which can integrate Thor and Roxie clusters
as needed to meet the complete processing needs of an organization. As a result,
scalability can be deﬁned not only in terms of the number of nodes in a cluster,
but in terms of how many clusters and of what type are needed to meet system
performance goals and user requirements. This provides signiﬁcant ﬂexibility when
compared to Hadoop clusters which tend to be independent islands of processing.
For additional information and a detailed comparison of the HPCC system platform
to Hadoop, see [30].

3
ECL/HPCC: A Uniﬁed Approach to Big Data
69
3.3
2 HPCC Thor System Cluster
The Thor system cluster is implemented using a master/slave approach with a
single master node and multiple slave nodes which provide a parallel job execution
environment for programs coded in ECL. Each of the slave nodes is also a data
node within the distributed ﬁle system for the cluster. Multiple Thor clusters can
exist in an HPCC system environment, and job queues can span multiple clusters
in an environment if needed. Jobs executing on a Thor cluster in a multi-cluster
environment can also read ﬁles from the distributed ﬁle system on foreign clusters
if needed. The middleware layer provides additional server processes to support the
execution environment including ECL Agents and ECL Servers. A client process
submits an ECL job to the ECL Agent which coordinates the overall job execution
on behalf of the client process.
An ECL program is compiled by the ECL server which interacts with an
additional server called the ECL Repository which is a source code repository
and contains shared, reusable ECL code. ECL code can also be stored in local
source ﬁles and managed with a conventional version control system. ECL programs
are compiled into optimized CCC source code, which is subsequently linked into
executable code and distributed to the slave nodes of a Thor cluster by the Thor
master node. The Thor master monitors and coordinates the processing activities of
the slave nodes and communicates status information monitored by the ECL Agent
processes. When the job completes, the ECL Agent and client process are notiﬁed,
and the output of the process is available for viewing or subsequent processing.
Output can be stored in the distributed ﬁlesystem for the cluster or returned to the
client process.
The distributed ﬁlesystem (DFS) used in a Thor cluster is record-oriented which
is somewhat different from the block format used in MapReduce clusters. Records
can be ﬁxed or variable length, and support a variety of standard (ﬁxed record size,
CSV, XML) and custom formats including nested child datasets. Record I/O is
buffered in large blocks to reduce latency and improve data transfer rates to and
from disk ﬁles to be loaded to a Thor cluster are typically ﬁrst transferred to a
landing zone from some external location, then a process called “spraying” is used
to partition the ﬁle and load it to the nodes of a Thor cluster. The initial spraying
process divides the ﬁle on user-speciﬁed record boundaries and distributes the data
as evenly as possible with records in sequential order across the available nodes
in the cluster. Files can also be “desprayed” when needed to transfer output ﬁles
to another system or can be directly copied between Thor clusters in the same
environment. Index ﬁles generated on Thor clusters can also be directly copied to
Roxie clusters to support online queries.
Nameservices and storage of metadata about ﬁles including record format
information in the Thor DFS are maintained in a special server called the Dali
server. Thor users have complete control over distribution of data in a Thor cluster,
and can re-distribute the data as needed in an ECL job by speciﬁc keys, ﬁelds, or
combinations of ﬁelds to facilitate the locality characteristics of parallel processing.

70
A.M. Middleton et al.
The Dali nameserver uses a dynamic datastore for ﬁlesystem metadata organized
in a hierarchical structure corresponding to the scope of ﬁles in the system. The
Thor DFS utilizes the local Linux ﬁlesystem for physical ﬁle storage, and ﬁle
scopes are created using ﬁle directory structures of the local ﬁle system. Parts of
a distributed ﬁle are named according to the node number in a cluster, such that
a ﬁle in a 400-node cluster will always have 400 parts regardless of the ﬁle size.
Each node contains an integral number of records (individual records are not split
across nodes), and I/O is completely localized to the processing node for local
processing operations. The ability to easily redistribute the data evenly to nodes
based on processing requirements and the characteristics of the data during a Thor
job can provide a signiﬁcant performance improvement over the blocked data and
input splits used in the MapReduce approach.
The Thor DFS also supports the concept of “superﬁles” which are processed as
a single logical ﬁle when accessed, but consist of multiple Thor DFS ﬁles. Each
ﬁle which makes up a superﬁle must have the same record structure. New ﬁles
can be added and old ﬁles deleted from a superﬁle dynamically facilitating update
processes without the need to rewrite a new ﬁle. Thor clusters are fault resilient and
a minimum of one replica of each ﬁle part in a Thor DFS ﬁle is stored on a different
node within the cluster.
3.4
HPCC Roxie System Cluster
Roxie clusters consist of a conﬁgurable number of peer-coupled nodes functioning
as a high-performance, high availability parallel processing query platform. ECL
source code for structured queries is pre-compiled and deployed to the cluster. The
Roxie distributed ﬁlesystem is a distributed indexed-based ﬁlesystem which uses a
custom BCTree structure for data storage. Indexes and data supporting queries are
pre-built on Thor clusters and deployed to the Roxie DFS with portions of the index
and data stored on each node. Typically the data associated with index logical keys
is embedded in the index structure as a payload. Index keys can be multi-ﬁeld and
multivariate, and payloads can contain any type of structured or unstructured data
supported by the ECL language. Queries can use as many indexes as required for
a query and contain joins and other complex transformations on the data with the
full expression and processing capabilities of the ECL language. For example, the
LexisNexis Accurint R comprehensive person report which produces many pages
of output is generated by a single Roxie query.
A Roxie cluster uses the concept of Servers and Agents. Each node in a Roxie
cluster runs Server and Agent processes which are conﬁgurable by a System
Administrator depending on the processing requirements for the cluster. A Server
process waits for a query request from a Web services interface then determines
the nodes and associated Agent processes that have the data locally that is needed
for a query, or portion of the query. Roxie query requests can be submitted from a
client application as a SOAP call, HTTP or HTTPS protocol request from a Web

3
ECL/HPCC: A Uniﬁed Approach to Big Data
71
application, or through a direct socket connection. Each Roxie query request is
associated with a speciﬁc deployed ECL query program. Roxie queries can also
be executed from programs running on Thor clusters. The Roxie Server process that
receives the request owns the processing of the ECL program for the query until it
is completed. The Server sends portions of the query job to the nodes in the cluster
and Agent processes which have data needed for the query stored locally as needed,
and waits for results. When a Server receives all the results needed from all nodes,
it collates them, performs any additional processing, and then returns the result set
to the client requestor.
The performance of query processing on a Roxie cluster varies depending on
factors such as machine speed, data complexity, number of nodes, and the nature of
the query, but production results have shown throughput of 5,000 transactions per
second on a 100-node cluster. Roxie clusters have ﬂexible data storage options with
indexes and data stored locally on the cluster, as well as being able to use indexes
stored remotely in the same environment on a Thor cluster. Nameservices for Roxie
clusters are also provided by the Dali server. Roxie clusters are fault-resilient and
data redundancy is built-in using a peer system where replicas of data are stored
on two or more nodes, all data including replicas are available to be used in the
processing of queries by Agent processes. The Roxie cluster provides automatic
failover in case of node failure, and the cluster will continue to perform even if
one or more nodes are down. Additional redundancy can be provided by including
multiple Roxie clusters in an environment.
Load balancing of query requests across Roxie clusters is typically implemented
using external load balancing communications devices. Roxie clusters can be sized
as needed to meet query processing throughput and response time requirements, but
are typically smaller that Thor clusters.
4
ECL Programming Language
4.1
Background
Several well-known companies experiencing the big data problem have imple-
mented high-level programming or script languages oriented toward data analysis.
In Google’s MapReduce programming environment, native applications are coded
in CCC [13]. The MapReduce programming model allows group aggregations in
parallel over a commodity cluster of machines similar to Fig. 3.1. Programmers
provide a Map function that processes input data and groups the data according
to a key-value pair, and a Reduce function that performs aggregation by key-value
on the output of the Map function. According to Dean and Ghemawat in [13, 14],
the processing is automatically parallelized by the system on the cluster, and takes
care of details like partitioning the input data, scheduling and executing tasks across
a processing cluster, and managing the communications between nodes, allowing
programmers with no experience in parallel programming to use a large parallel

72
A.M. Middleton et al.
processing environment. For more complex data processing procedures, multiple
MapReduce calls must be linked together in sequence.
Google also implemented a high-level language named Sawzall for performing
parallel data analysis and data mining in the MapReduce environment and a work-
ﬂow management and scheduling infrastructure for Sawzall jobs called Workqueue
[36]. For most applications implemented using Sawzall, the code is much simpler
and smaller than the equivalent CCC by a factor of 10 or more. Pike et al. in [36]
cite several reasons why a new language is beneﬁcial for data analysis and data
mining applications: (1) a programming language customized for a speciﬁc problem
domain makes resulting programs “clearer, more compact, and more expressive”;
(2) aggregations are speciﬁed in the Sawzall language so that the programmer does
not have to provide one in the Reduce task of a standard MapReduce program; (3)
a programming language oriented to data analysis provides a more natural way to
think about data processing problems for large distributed datasets; and (4) Sawzall
programs are signiﬁcantly smaller that equivalent CCC MapReduce programs and
signiﬁcantly easier to program.
An open source implementation of MapReduce pioneered by Yahoo! called
Hadoop is functionally similar to the Google implementation except that the
base programming language for Hadoop is Java instead of CCC. Yahoo! also
implemented a high-level dataﬂow-oriented language called Pig Latin and execution
environment ostensibly for the same reasons that Google developed the Sawzall
language for its MapReduce implementation – to provide a speciﬁc language
notation for data analysis applications and to improve programmer productivity
and reduce development cycles when using the Hadoop MapReduce environment.
Working out how to ﬁt many data analysis and processing applications into the
MapReduce paradigm can be a challenge, and often requires multiple MapReduce
jobs [41]. Pig Latin programs are automatically translated into sequences of
MapReduce programs if needed in the execution environment.
Both Google with its Sawzall language and Yahoo with its Pig system and
language for Hadoop address some of the limitations of the MapReduce model
by providing an external dataﬂow-oriented programming language which translates
language statements into MapReduce processing sequences [17, 34, 36]. These
languages provide many standard data processing operators so users do not have
to implement custom Map and Reduce functions, improve reusability, and provide
some optimization for job execution. However, these languages are externally
implemented executing on client systems and not integral to the MapReduce
architecture, but still rely on the on the same infrastructure and limited execution
model provided by MapReduce.
4.2
ECL Features and Capabilities
The open source ECL programming language represents a new programming
paradigm for data-intensive computing. ECL was speciﬁcally designed to be a
transparent and implicitly parallel programming language for data-intensive appli-

3
ECL/HPCC: A Uniﬁed Approach to Big Data
73
cations. It is a high-level, declarative, non-procedural dataﬂow-oriented language
that allows the programmer to deﬁne what the data processing result should be and
the dataﬂows and transformations that are necessary to achieve the result. Execution
is not determined by the order of the language statements, but from the sequence of
dataﬂows and transformations represented by the language statements. It combines
data representation with algorithm implementation, and is the fusion of both a query
language and a parallel data processing language.
ECL uses an intuitive syntax which has taken cues from other familiar languages,
supports modular code organization with a high degree of reusability and extensi-
bility, and supports high-productivity for programmers in terms of the amount of
code required for typical applications compared to traditional languages like Java
and CCC. Similar to the beneﬁts Sawzall provides in the Google environment, and
Pig Latin provides to Hadoop users, a 20 times increase in programmer productivity
is typical which can signiﬁcantly reduce development cycles.
ECL is compiled into optimized CCC code for execution on the HPCC system
platforms, and can be used for complex data processing and analysis jobs on a Thor
cluster or for comprehensive query and report processing on a Roxie cluster. ECL
allows inline CCC functions to be incorporated into ECL programs, and external
programs in other languages can be incorporated and parallelized through a PIPE
facility. External services written in CCC and other languages which generate
DLLs can also be incorporated in the ECL system library, and ECL programs can
access external Web services through a standard SOAPCALL interface.
The basic unit of code for ECL is called an attribute deﬁnition. An attribute
can contain a complete executable query or program, or a shareable and reusable
code fragment such as a function, record deﬁnition, dataset deﬁnition, macro, ﬁlter
deﬁnition, etc. Attributes can reference other attributes which in turn can reference
other attributes so that ECL code can be nested and combined as needed in a reusable
manner. Attributes are stored in ECL code repository which is subdivided into
modules typically associated with a project or process. Each ECL attribute added
to the repository effectively extends the ECL language like adding a new word
to a dictionary, and attributes can be reused as part of multiple ECL queries and
programs. ECL can also be stored in local source ﬁles as with other programming
languages. With ECL a rich set of programming tools is provided including an
interactive IDE similar to Visual CCC, Eclipse (an ECL add-in for Eclipse is
available) and other code development environments.
The ECL language includes extensive capabilities for data deﬁnition, ﬁltering,
data management, and data transformation, and provides an extensive set of built-
in functions to operate on records in datasets which can include user-deﬁned
transformation functions. Transform functions operate on a single record or a pair
of records at a time depending on the operation. Built-in transform operations
in the ECL language which process through entire datasets include PROJECT,
ITERATE, ROLLUP, AGGREGATE, JOIN, COMBINE, FETCH, NORMALIZE,
DENORMALIZE, and PROCESS. The transform function deﬁned for a JOIN
operation for example receives two records, one from each dataset being joined, and
can perform any operations on the ﬁelds in the pair of records, and returns an output
record which can be completely different from either of the input records. Example

74
A.M. Middleton et al.
Fig. 3.5 ECL sample syntax for JOIN operation
syntax for the JOIN operation from the ECL Language Reference Manual is shown
in Fig. 3.5. Other important data operations included in ECL which operate across
datasets and indexes include TABLE, SORT, MERGE, MERGEJOIN, DEDUP,
GROUP, APPLY, ASSERT, AVE, BUILD, BUILDINDEX, CHOOSESETS, COR-
RELATION, COUNT, COVARIANCE, DISTRIBUTE, DISTRIBUTION, ENTH,
EXISTS, GRAPH, HAVING, KEYDIFF, KEYPATCH, LIMIT, LOOP, MAX,
MIN, NONEMPTY, OUTPUT, PARSE, PIPE, PRELOAD, PULL, RANGE, RE-
GROUP, SAMPLE, SET, SOAPCALL, STEPPED, SUM, TOPN, UNGROUP, and
VARIANCE.
The Thor system allows data transformation operations to be performed either
locally on each node independently in the cluster, or globally across all the nodes in
a cluster, which can be user-speciﬁed in the ECL language. Some operations such
as PROJECT for example are inherently local operations on the part of a distributed
ﬁle stored locally on a node. Others such as SORT can be performed either locally or
globally if needed. This is a signiﬁcant difference from the MapReduce architecture
in which Map and Reduce operations are only performed locally on the input split
assigned to the task. A local SORT operation in an HPCC cluster would sort the

3
ECL/HPCC: A Uniﬁed Approach to Big Data
75
Fig. 3.6 ECL code example
records by the speciﬁed key in the ﬁle part on the local node, resulting in the
records being in sorted order on the local node, but not in full ﬁle order spanning all
nodes. In contrast, a global SORT operation would result in the full distributed ﬁle
being in sorted order by the speciﬁed key spanning all nodes. This requires node to
node data movement during the SORT operation. Figure 3.6 shows a sample ECL
program using the LOCAL mode of operation, and Fig. 3.7 shows the corresponding
execution graph. Note the explicit programmer control over distribution of data
across nodes. The colon-equals “:=”operator in an ECL program is read as “is
deﬁned as”. The only action in this program is the OUTPUT statement, the other
statements are deﬁnitions.
An additional important capability provided in the ECL programming language
is support for natural language processing (NLP) with PATTERN statements and
the built-in PARSE function. The PARSE function cam accept an unambiguous
grammar deﬁned by PATTERN, TOKEN, and RULE statements with penalties
or preferences to provide deterministic path selection, a capability which can
signiﬁcantly reduce the difﬁculty of NLP applications. PATTERN statements allow
matching patterns including regular expressions to be deﬁned and used to parse
information from unstructured data such as raw text. PATTERN statements can be
combined to implement complex parsing operations or complete grammars from
BNF deﬁnitions. The PARSE operation function across a dataset of records on a
speciﬁc ﬁeld within a record, this ﬁeld could be an entire line in a text ﬁle for
example. Using this capability of the ECL language it is possible to implement
parallel processing for information extraction applications across document ﬁles
including XML-based documents or Web pages.
4.3
ECL Compilation, Optimization, and Execution
The ECL language compiler takes the ECL source code and produces an output
with three main elements. The ﬁrst is an XML representation of the execution graph,
detailing the activities to be executed and the dependencies between those activities.

76
A.M. Middleton et al.
Fig. 3.7 ECL code example execution graph
The second is a CCC class for each of the activities in the graph, and the third
contains code and meta information to control the workﬂow for the ECL program.
These different elements are embedded in a single shared object that contains all the
information about the particular query. That shared object is passed to the execution
engines, which take that shared object and execute the program it contains.
The process of compiling, optimizing, and executing the ECL is broken into
several stages: (1) parsing, (2) optimization, (3) transforming, (4) generating, and
(5) execution.
4.3.1
Parsing
The sources for an ECL program can come from a local directory tree, an external
repository, or a single-source archive. The ECL compiler reads the ECL source,
parses it, and converts it into an abstract graph representation of the program. The
representation is then normalized to resolve ambiguities and ensure is it is suitable

3
ECL/HPCC: A Uniﬁed Approach to Big Data
77
for subsequent processing. All of the subsequent operations within the compiler
work on, and create, this same abstract representation.
4.3.2
Optimizations
The design of the ECL language provides abundant scope for optimizations. When
reusable attributes are combined it often creates the scope for optimizations that
would be hard, if not impossible, to be spotted by a programmer. Its declarative
design allows many optimizations without the concerns about side-effects associ-
ated with imperative languages. Many different optimizations are performed on the
program, some of the key ones are:
•
Constant folding. This includes simple purely constant expressions like 123 D>
36, and more complex changes e.g. IF(a, ‘b’, ‘c’) IN [‘a’,’c’] => NOT a
•
Tracking and propagating constant ﬁeld values. This can often lead to further
constant folding, or reduce the lifetime of a ﬁeld. Minimizing the ﬁelds in a row
at each stage of the processing. This saves the programmer from unnecessary
optimization, and often beneﬁts from the other optimizations (e.g., constant
propagation).
•
Reordering operations. Sometimes changing the order of operations can sig-
niﬁcantly reduce the data processed by complex activities. Examples include
ensuring a ﬁlter is done before a sort, or replacing a ﬁlter on a joined dataset
with a ﬁlter on one (or both) of the inputs.
•
Tracking meta information including sort orders and record counts, and remov-
ing redundant operations. This is an example of an optimization which often
comes in to play when reusable attributes are combined. A particular sort order
may not be part of the speciﬁcation of an attribute, but the optimizer can make
use of the current implementation.
•
Minimizing data transferred between slave nodes.
There is sufﬁcient scope for many additional optimizations. For example, a currently
planned optimization would analyze and optimize the distribution and sort activities
used in a program to maximize overlap and minimize data redistribution.
A key design goal is for the ECL programmer to be able to describe the problem,
and rely on the ECL compiler to solve the problem efﬁciently.
4.3.3
Transforming
The ECL compiler needs to transform the abstract declarative ECL (what it should
do) to a concrete imperative implementation (how it should do it). This again has
several different elements:
•
Convert the logical graph into an execution graph. This includes introducing
activities to split the data stream, ensure dependencies between activities will be
executed in the correct order, and resolving any global resourcing constraints.

78
A.M. Middleton et al.
•
Extracting context-invariant expressions to ensure they are evaluated a minimal
number of times. This is similar to spotting loop invariant code in an imperative
language.
•
Selecting between different implementations of a sequence of activities. For
example generating either inline code or a nested graph of activities.
•
Common sub-expression elimination. Both globally across the whole program,
and locally the expressions used within the methods of the activity classes.
•
Mapping complex ECL statements into the activities supported by the target
engine. For instance a JOIN may be implemented differently depending on how
the inputs are sorted, distributed, and the likely size of the datasets. Similarly an
ECL DEDUP operation may sometimes be implemented as a local dedup activity
followed by a global dedup activity.
•
Combining multiple logical operations into a single activity. Compound activi-
ties have been implemented in the engines where it can signiﬁcantly reduce the
data being copied, or because there are likely to be expressions shared between
the activities. One of the commonest examples is disk read, ﬁlter and project.
4.3.4
Generating
Following the transforming stage, the XML and CCC associated with the ECL
program is generated. The CCC code is built using a data structure that allows
peephole optimizations to be applied to the CCC that will be generated. Once the
processing is complete, the CCC is generated from the structure, and the generated
source ﬁles are passed to the system CCC compiler to create a shared object.
In practice, the optimization, transforming and generation is much more of an
iterative process rather than sequential.
4.3.5
Execution
The details of executing ECL program vary depending on the speciﬁc HPCC system
platform and its execution engine, but they follow the same broad sequence.
The engine extracts resources from the shared object that describe the workﬂow
of the query. The workﬂow can include waiting for particular events, conditionally
re-evaluating expressions, and executing actions in a particular order. Each work-
ﬂow item is executed independently, but can have dependencies on other workﬂow
items. A workﬂow item may contain any number of activity graphs which evaluate
a particular part of the ECL program.
To execute a graph of activities the engine starts at the outputs and recursively
walks the graph to evaluate any dependencies. Once the graph is prepared the graph
of activities is executed. Generally multiple paths within the graph are executed
in parallel, and multiple slave nodes in a cluster will be executing the graphs on
different subsets of the data. Records are streamed through the graphs from the

3
ECL/HPCC: A Uniﬁed Approach to Big Data
79
inputs to the outputs. Some activities execute completely locally, and others co-
ordinate their execution with other slave nodes.
4.4
ECL Log Analysis Programming Example
Analysis of log data collected by Web servers, system servers, and other network
devices such as routers and ﬁrewalls is an important application for generating
statistical information and reports on system and network utilization and other
types of analysis such as intrusion detection and misuse of network resources.
Log data is usually collected in unstructured text ﬁles which must be parsed using
NLP to extract key information for reporting and analysis. This is typical of many
data processing applications which must process data in a raw form, extracting,
transforming, and loading the data for subsequent processing and is commonly
referred to as ETL processing. The volume of log data generated by a large
network of system and network servers can be enormous and is representative of
applications which require a data-intensive computing solution like the LexisNexis
HPCC platform.
Since log ﬁles from various system servers and networks devices can have
varying formats, but a network generally includes multiples of the same types of
devices which use common log formats, a useful design approach is to generate a
function or macro for each type of device. The ECL programming language includes
both functions and macros, and a macro format was selected for this example.
A macro in a programming language accepts parameters similar to a function, and
substitutes the parameter values to replace parts of the code generated by the macro,
generating new inline code each time it is referenced.
The example log ﬁle data contains lines of text which include a date, time,
log source, message type, and additional log information formatted as key value
pairs. An ECL macro (MAC Parse DTSM Keyval Format) was implemented for
this speciﬁc type of log ﬁle format and is shown in Fig. 3.8. The macro accepts
parameters deﬁning the input raw log ﬁle, the output formatted log ﬁle, and an
output error ﬁle which will contain lines from the raw log ﬁle data which had an
invalid format.
The steps used by the ECL macro shown in Fig. 3.9 to process the raw log ﬁle
data transforming the data to a formatted output ﬁle are as follows:
1. The raw input log ﬁle (inlogﬁle) is projected to a new format which adds a
sequential line number in a separate ﬁeld to each log line for reference in macro
lines 5–13. Individual ECL statements are terminated by a semicolon character,
and whitespace can be used freely to improve readability of the code.
2. NLP patterns are deﬁned using the ECL PATTERN statement to represent the
data to be extracted from the raw log lines in macro lines 15–20. Note references
to other patterns such as Text.Date and Text.ISO Time which are shared pattern
deﬁnitions stored in the Text module in the ECL repository.

80
A.M. Middleton et al.
Fig. 3.8 ECL programming example – log ﬁle analysis macro

3
ECL/HPCC: A Uniﬁed Approach to Big Data
81
Fig. 3.9 ECL programming example – log ﬁle analysis execution graph
3. The output record format for parsed log lines is shown in macro lines 22–30 and
include separate ﬁelds for the date, time, log source, message type, and additional
log information.
4. Parsing of the raw log data into the format described in step 3 is shown in macro
line 33. This parse statement as well as other ECL statements operate on the
entire ﬁle. Each node in a Thor processing cluster operates on the part of the ﬁle
locally stored on the node.
5. The log info ﬁeld parsed in the operation described in step 4 includes additional
key-value pairs. This information is then parsed into a separate dataset in macro
line 46, using pattern statements deﬁned in macro lines 35–38, and the output
record deﬁnition deﬁned in macro lines 40–44.
6. The ﬁnal formatted output from the log ﬁle is designed to include the ﬁeld,
date, time, log source, and message type, and a child dataset for each log line
containing the key-value pairs extracted from the log info ﬁeld. This output
record format is deﬁned in macro line 49 which references a separate ECL
attribute containing the record deﬁnition stored in the ECL repository in the
Log Analysis module named Layout DTSM Keyval which is shown in Fig. 3.10.
7. The initially parsed log ﬁle from macro line 33 (log init) is projected to the output
format in lines 51–55. To complete the output ﬁle, the key-value pairs for each
log line generated in step 5 (keyvals init) are added to the initialized output ﬁle
(log out init) using the ECL DENORMALIZE statement in macro lines 67–74.
Both ﬁles are distributed across the available nodes in the cluster by log line
number so this operation can be performed locally. The key-value pairs are sorted
by the linenum and key ﬁelds and the ﬁnal output is sorted in order by the linenum
ﬁeld.
8. Lines which had invalid formats which failed to parse properly are identiﬁed and
written to a separate dataset in lines 57–64 using the ECL JOIN operation to join
the initial sequenced log ﬁle (log seq) to the initial log data parse (log init) by the
log line number (linenum). Lines which appear in the log seq ﬁle and not in the
log init ﬁle are written to the error dataset. This is facilitated by the ECL JOIN
option LEFT ONLY which generates records which appear in the left dataset of
the join operation and not in the right dataset (Fig. 3.11).

82
A.M. Middleton et al.
Fig. 3.10 ECL programming example – log ﬁle analysis output
The MAC Parse DTSM Keyval Format ECL macro can now be used to process
any raw log ﬁle with the deﬁned format. An example of using this ECL macro
is shown in Fig. 3.12. This code can be executed from the ECL IDE as an ECL
job. The code includes a dataset deﬁnition of the raw input log ﬁle (lines 1–
7), an output statement to display a sample of the raw log data (line 10), a
MAC Parse DTSM Keyval Format macro call to process the raw log data (line 13),
an output statement to display a sample of invalid format raw log lines, and an
output statement to display a sample of the processed log data. Figure 3.9 shows the
job execution graph for the example job. 4.12 shows a sample of the raw log ﬁle
input data and the formatted log data output for the example job.
4.5
ECL Development Tools and User Interfaces
The HPCC platform includes a suite of development tools and utilities for data
analysts, programmers, administrators, and end-users. These include ECL IDE, an
integrated programming development environment similar to those available for
other languages such as CCC and Java, which encompasses source code editing,
source code version control, access to the ECL source code repository, and the
capability to execute and debug ECL programs. Figure3.13 shows the Query
Builder IDE application.
ECL IDE provides a full-featured Windows-based GUI for ECL program
development and direct access to the ECL repository source code. ECL IDE allows
you to create and edit ECL attributes which can be shared and reused in multiple
ECL programs or to enter an ECL query which can be submitted directly to a Thor
cluster as an executable job or deployed to a Roxie cluster. An ECL query can
be self-contained or reference other sharable ECL code in the attribute repository.
ECL IDE also allows you to utilize a large number of built-in ECL functions from

3
ECL/HPCC: A Uniﬁed Approach to Big Data
83
Fig. 3.11 ECL programming
example – log ﬁle output
format

84
A.M. Middleton et al.
Fig. 3.12 ECL programming example – log ﬁle analysis job

3
ECL/HPCC: A Uniﬁed Approach to Big Data
85
Fig. 3.13 ECL IDE
included libraries covering string handling, data manipulation, ﬁle handling, ﬁle
spray and despray, superﬁle management, job monitoring, cluster management,
word handling, date processing, auditing, parsing support, phonetic (metaphone)
support, and workunit services.
ECLWatch is a Web-based utility which provides a set of tools for monitoring and
managing HPCC clusters which is shown in Fig. 3.14. ECLWatch allows you see
information about workunits including a graph displaying a visual representation of
the dataﬂows for the workunit complete with statistics which are updated as the job
progresses. The graph is interactive and you can drill down on nodes and connectors
to see more detailed information and statistics. This information is retained in the
workunit even after the job has completed so it can be reviewed and analyzed. An
example of an ECL execution graph corresponding to the code example in Fig. 3.6
is shown in Fig. 3.7. In addition with ECLWatch, you can monitor cluster activity,
browse through or search for previously submitted workunits, use DFU functions to
search for ﬁles and see information including record counts and layouts and display
data from the ﬁle, spray and despray ﬁles from available landing zones to and from
clusters, check the status of all system servers, view log ﬁles, change job priorities,
and much more.
The HPCC platform also provides an ECL Attribute Migration Tool which allows
ECL source code to be copied from one ECL repository to another. For example,

86
A.M. Middleton et al.
Fig. 3.14 ECLWatch web-based utility
in most HPCC conﬁgurations there are separate development and production
environments. AMT allows newly developed ECL attributes to be migrated from
development to production in a controlled manner.
4.6
ECL Advantages and Key Beneﬁts
ECL a heavily optimized, data-centric declarative programming language. It is a
language speciﬁcally designed to allow data operations to be speciﬁed in a manner
which is easy to optimize and parallelize. With a declarative language, you specify
what you want done rather than how to do it. A distinguishing feature of declarative
languages is that they are extremely succinct; it is common for a declarative
language to require an order of magnitude (10) less code than a procedural
equivalent to specify the same problem [5]. The SQL language commonly used
for data access and data management with RDBMS systems is also a declarative
language. Declarative languages have many beneﬁts including conciseness, freedom
from side effects, parallelize naturally, and the executable code generated can
be highly optimized since the compiler can determine the optimum sequence of
execution instead of the programmer.

3
ECL/HPCC: A Uniﬁed Approach to Big Data
87
ECL extends the beneﬁts of declarative in three important ways [5]: (1) It is
data-centric which means it addresses computing problems that can be speciﬁed
by some form of analysis upon data. It has deﬁned a simple but powerful data
algebra to allow highly complex data manipulations to be constructed; (2) It is
extensible. When a programmerdeﬁnes new code segments (called attributes) which
can include macros, functions, data deﬁnitions, procedures, etc., these essentially
become a part of the language and can be used by other programmers. Therefore
a new ECL installation may be relatively narrow and generic in its initial scope,
but as new ECL code is added, its abilities expand to allow new problems and
classes of problems to be stated declaratively; and (3) It is internally abstract. The
ECL compiler generates CCC code and calls into many ‘libraries’ of code, most of
which are major undertakings in their own right. By doing this, the ECL compiler
is machine neutral and greatly simpliﬁed. This allows the ECL compiler writers to
focus on making the language relevant and good, and generating highly-optimized
executable code. For some coding examples and additional insights into declarative
programming with ECL, see [6].
One of the key issues which has confronted language developers is to ﬁnd
solutions to the complexity and difﬁculty of parallel and distributed programming.
Although high-performance computing and cluster architectures such have ad-
vanced to provide highly-scalable processing environments, languages designed for
parallel programming are still somewhat rare. Declarative, data-centric languages
because the parallelize naturally represent solutions to this issue [24]. According
to Hellerstein, declarative, data-centric languages parallelize naturally over large
datasets, and programmers can beneﬁt from parallel execution without modiﬁca-
tions to their code. ECL code, for example can be used on any size cluster without
modiﬁcation to the code, so performance can be scaled naturally.
The key beneﬁts of ECL can be summarized as follows:
•
ECL is a declarative, data-centric, programming language which can expressed
concisely, parallelizes naturally, is free from side effects, and results in highly-
optimized executable code.
•
ECL incorporates transparent and implicit parallelism regardless of the size
of the computing cluster and reduces the complexity of parallel programming
increasing the productivity of application developers.
•
ECL enables implementation of data-intensive applications with huge volumes
of data previously thought to be intractable or infeasible. ECL was speciﬁcally
designed for manipulation of data and query processing. Order of magnitude
performance increases over other approaches are possible.
•
ECL provides a more than 20 times productivity improvement for programmers
over traditional languages such as Java and CCC. The ECL compiler generates
highly optimized CCC for execution.
•
ECL provides a comprehensive IDE and programming tools that provide a
highly interactive environment for rapid development and implementation of
ECL applications.

88
A.M. Middleton et al.
•
ECL is a powerful, high-level, parallel programming language ideal for im-
plementation of ETL, Information Retrieval, Information Extraction, and other
data-intensive applications.
4.7
ECL Versus SQL for Aggregated Data Analysis
It is useful to compare and contrast the traditional Relationship Database Man-
agement System (DBMS)/Structured Query Language (SQL) solution to the one
offered by the HPCC ECL platform. While many of the comparison points made
here are applicable to data processing in general, the integration of huge amounts of
heterogeneous data will be discussed here. It will be argued that the relational data
model is excellent for data which is generated, collected and stored under relational
constraints. However for data which is not generated or collected under relational
constraints, the attempt to force the data into the relational model involves crippling
compromises. The model-neutral nature of ECL obviates these concerns.
The relational database is the most prevalent database management system
available today; however, it is not the most suitable system for the integration and
analysis of massive amounts of data from heterogeneous data sources. A premise
of the RDBMS concept is that the data is generated, stored and delivered according
to the same data model. For those in the business of collecting data from external
sources, this premise is fundamentally broken. Each data source that is collected
will, at best, have been generated according to a different data model or no data
model at all. The procedures in place to ensure a RDBMS has integrity simply do not
apply for the majority of data that is available today. Some examples of constraints
that are typically placed on an RDBMS that would be violated by most ingested data
are required ﬁelds (ﬁelds may not be populated), unique ﬁelds are unique (a ﬁeld
like social security number could have been mistyped), an entity can be represented
by a single foreign key (many ﬁelds related to a person could have multiple valid
values meaning the same thing), a single foreign key can refer to only one entity
(a city name can be replicated in many different states), a single ﬁeld can take
one discrete set of values (misspellings and variations between different external
systems mean the standard ﬁeld lookup is invalid). The result of the above is that it
is impossible to construct a normalized relational model that accurately reﬂects the
data that is being ingested without producing a model that will signiﬁcantly affect
system performance. For a detailed example of this, see [4].
The SQL data base administrator usually adopts one or a hybrid of the following
strategies to address these problems: (1) Normalize the data fully, investing in
enough hardware and manpower to get the required performance. However, this
approach can result in a single large ﬁle of ingested data containing multiple
terabytes of data into tens or even hundreds of sub-ﬁles. In addition, the data
architecture team potentially has to alter the model for every new ingested ﬁle.;
(2) Abandon normalization and move the data manipulation logic down into the
application layer. With this approach, the ﬁelds contain the data as collected and the

3
ECL/HPCC: A Uniﬁed Approach to Big Data
89
Fig. 3.15 RDBMS/SQL data aggregation model
task of interpreting the data is moved down to the programmers. The application
typically has to fetch a lot of data in multiple steps for a process that should have
been executed atomically on the database server; and (3) Add a signiﬁcant data
ingest phase where the data is ‘bashed’ into a format that has been pre-deﬁned by
the data architects using a separate ETL (extract, transform, load) system. This is
the best in terms of performance of the query system but has the twin downsides
of creating a signiﬁcant delay during the data ingest phase and also throwing
away potentially vital data that was not compatible with the pre-deﬁned ingest
data architecture. The RDBMS/SQL architecture for data aggregation is shown in
Fig. 3.15.
LexisNexis, in order to overcome the limitations of DBMS for data aggregation
and data analysis developed a new data-centric approach with the HPCC system
platform and the ECL programming language. ECL was designed to have all
of the data processing capabilities required by the most advanced SQL or ETL
systems but also to have the code encapsulation mechanisms demanded by systems
programmers. The model for data aggregation used with the HPCC/ECL platform
is shown in Fig. 3.16. The advantages of the ECL approach to the data aggregation
problem are (1) The original data sources are stored unmodiﬁed, even though they
are modiﬁed as part of delivery. Thus there is never any “loss” of information or
signiﬁcant pain in re-mapping the incoming ﬁles to the target formats; (2) the data
teams can be segmented by data type rather than language skill. This allows for
every ﬁle type to be handled by individuals skilled in that ﬁeld; (3) if required, a
storage point between a batch ingest facility and a real-time delivery mechanism
is available without a need to signiﬁcantly recode the processing logic; and (4)
introducing parallelism is natural and can even be done between remote processing
sites.

90
A.M. Middleton et al.
Fig. 3.16 HPCC/ECL data aggregation model
For some case studies that further demonstrate the new horizons that can be
opened by a HPCC/ECL implementation, see [4]
4.8
Pig Versus ECL Feature Comparison
Although many Hadoop installations implement applications directly in Java, the
Pig Latin language is now being used to increase programmer productivity and
further simplify the programming of data-intensive applications at Yahoo! and
other major users of Hadoop [17]. ECL is the base programming language used
for applications on the HPCC platform even though it is compiled into CCC
for execution. When comparing the Hadoop and HPCC platforms which can
typically be executed on the same commodity cluster hardware conﬁguration, it is
useful to compare the features, functionality, and performance of these high-level
languages.
Both Pig and ECL are intrinsically parallel, supporting transparent data-
parallelism on the underlying platform. Pig and ECL are translated into programs
that automatically process input data for a process in parallel with data distributed
across a cluster of nodes. Programmers of both languages do not need to know the
underlying cluster size or use this to accomplish data-parallel execution of jobs.
Both Pig and ECL are dataﬂow-oriented, but Pig is an imperative programming
language and ECL is a declarative programming language. A declarative language
allows programmers to focus on the data transformations required to solve an
application problem and hides the complexity of the underlying platform and
implementation details, reduces side effects, and facilitates compiler optimization

3
ECL/HPCC: A Uniﬁed Approach to Big Data
91
of the code and execution plan. An imperative programming language dictates the
control ﬂow of the program which may not result in an ideal execution plan in a
parallel environment. Declarative programming languages allow the programmer to
specify “what” a program should accomplish, instead of “how” to accomplish it.
For more information, refer to the discussions of declarative (http://en.wikipedia.
org/wiki/Declarative programming) and imperative (http://en.wikipedia.org/wiki/
Imperative programming) programming languages on Wikipedia.
The source code for both Pig and ECL is compiled or translated into another
language – Pig source programs are translated into Java language MapReduce jobs
for execution and ECL programs are translated into CCC source code which is then
compiled into a DLL for execution. Pig programs are restricted to the MapReduce
architecture and HDFS of Hadoop, but ECL has no ﬁxed framework other than the
DFS (Distributed File System) used for HPCC and therefore can be more ﬂexible in
implementation of data operations. This is evident in two key areas: (1) ECL allows
operations to be either global or local, where standard MapReduce is restricted to
local operations only in both the Map and Reduce phases. Global operations process
the records in a dataset in order across all nodes and associated ﬁle parts in sequence
maintaining the records in sorted order as opposed to only the records contained in
each local node which may be important to the data processing procedure; (2) ECL
has the ﬂexibility to implement operations which can process more than one record
at a time such as its ITERATE operation which uses a sliding window and passes two
records at a time to an associated transform function. This allows inter-record ﬁeld-
by-ﬁeld dependencies and decisions which are not available in Pig. For example the
DISTINCT operation in Pig which is used to remove duplicates does not allow this
on a subset of ﬁelds. ECL provides both DEDUP and ROLLUP operations which
are usually preceded by a SORT and operate on adjacent records in a sliding window
mode and any condition relating to the ﬁeld contents of the left and right record of
adjacent records can be used to determine if the record is removed. ROLLUP allows
a custom transformation to be applied to the de-duplication process.
An important consideration of any software architecture for data is the underlying
data model. Pig incorporates a very ﬂexible nested data model which allows non-
atomic data types (atomic data types include numbers and strings) such as set, map,
and tuple to occur as ﬁelds of a table [34]. Tuples are sequences of ﬁelds, bags
are collections of tuples, and maps are a collection of data items where each data
item has a key with which it can be looked up. A data record within Pig is called
a relation which is an outer bag, the bag is a collection of tuples, each tuple is an
ordered set of ﬁelds, and a ﬁeld is a piece of data. Relations are referenced by a
name assigned by a user. Types can be assigned by the user to each ﬁeld, but if
not assigned will default to a bytearray and conversions are applied depending on
the context in which the ﬁeld is used. The ECL data model also offers a nested
data structure using child datasets. A user-speciﬁed RECORD deﬁnition deﬁnes
the content of each record in a dataset which can contain ﬁxed or variable length
ﬁelds or child datasets which in turn contain ﬁelds or child datasets etc. With this
format any type of data structure can be represented. ECL offers speciﬁc support

92
A.M. Middleton et al.
for CSV and XML formats in addition to ﬂat ﬁle formats. Each ﬁeld in a record has
a user-speciﬁed identiﬁer and data type and an optional default value and optional
ﬁeld modiﬁers such as MAXLENGTH that enhance type and use checking during
compilation. ECL will perform implicit casting and conversion depending on the
context in which a ﬁeld is used, and explicit user casting is also supported. ECL
also allows in-line datasets allowing sample data to be easily deﬁned and included
in the code for testing rather than separately in a ﬁle.
The Pig environment offers several programmer tools for development, execu-
tion, and debugging of Pig Latin programs (Pig Latin is the formal name for the
language, and the execution environment is called Pig, although both are commonly
referred to as Pig). Pig provides command line execution of scripts and an interactive
shell called Grunt that allows you to execute individual Pig commands or execute
a Pig script. Pig programs can also be embedded in Java programs. Although Pig
does not provide a speciﬁc IDE for developing and executing PIG programs, add-ins
are available for several program editing environments including Eclipse, Vim, and
Textmate to perform syntax checking and highlighting [41]. PigPen is an Eclipse
plug-in that provides program editing, an example data generator, and the capability
to run a Pig script on a Hadoop cluster.
The HPCC platform provides an extensive set of tools for ECL development
including a comprehensive IDE which allows program editing, execution, and
interactive graph visualization for debugging and proﬁling ECL programs. The
common code repository tree is displayed in the ECL IDE and tools are provided for
source control, accessing and searching the repository. ECL jobs can be launched
to an HPCC environment or speciﬁc cluster, and execution can be monitored
directly from the ECL IDE. External tools are also provided including ECLWatch
which provides complete access to current and historical workunits (jobs executed
in the HPCC environment are packaged into workunits), queue management and
monitoring, execution graph visualization, distributed ﬁlesystem utility functions,
and system performance monitoring and analysis.
Although Pig Latin and the Pig execution environment provide a basic high-
level language environment for data-intensive processing and analysis and increases
the productivity of developers and users of the Hadoop MapReduce environment,
ECL is a signiﬁcantly more comprehensive and mature language that generates
highly optimized code, offers more advanced capabilities in a robust, proven,
integrated data-intensive processing architecture. Table 3.1 provides a feature to
feature comparison between the Pig and ECL languages and their execution
environments.

3
ECL/HPCC: A Uniﬁed Approach to Big Data
93
Table 3.1 Pig versus ECL feature comparison
Language feature
or capability
Pig
ECL
Language type
Data-ﬂow oriented, imperative, parallel language for
data-intensive computing. All Pig statements
perform actions in sequentially ordered steps. Pig
programs deﬁne a sequence of actions on the data
Data-ﬂow oriented, declarative, non-procedural, parallel language for
data-intensive computing. Most ECL statements are deﬁnitions of the
desired result which allows the execution plan to be highly optimized by
the compiler. ECL actions such as OUTPUT cause execution of the
dataﬂows to produce the result deﬁned by the ECL program
Compiler
Translated into a sequence of MapReduce Java
programs for execution on a Hadoop Cluster. Runs
as a client application
Compiled and optimized into CCC source code which is compiled into DLL
for execution on an HPCC cluster. Runs as a server application
User-deﬁned
Functions
Written in Java to perform custom processing and
transformations as needed in Pig language
statements. REGISTER is used to register a JAR ﬁle
so that UDFs can be used
Processing functions or TRANSFORM functions are written in ECL. ECL
supports inline CCC in functions and external Services compiled into
DLL libraries written in any language
Macros
Not supported
Extensive support for ECL macros to improve code reuse of common
procedures. Additional template language for use in macros provides
unique naming and conditional code generation capabilities
Data model
Nested data model with named relations to deﬁne data
records. Relations can include nested combinations
of bags, tuples, and ﬁelds. Atomic data types include
int, long, ﬂoat, double, chararray, bytearray, tuple,
bag, and map. If types not speciﬁed, default to
bytearray then converted during expressions
evaluation depending on the context as needed
Nested data model using child datasets. Datasets contain ﬁelds or child
datasets containing ﬁelds or additional child datasets. Record deﬁnitions
describe the ﬁelds in datasets and child datasets. Indexes are special
datasets supporting keyed access to data. Data types can be speciﬁed for
ﬁelds in record deﬁnitions and include Boolean, integer, real, decimal,
string, qstring, Unicode, data, varstring, varunicode, and related operators
including set of (type), typeof(expression) and recordof(dataset) and
ENUM (enumeration). Explicit type casting is available and implicit type
casting may occur during evaluation of expressions by ECL depending on
the context. Type transfer between types is also supported. All datasets
can have an associated ﬁlter express to include only records which meet
the ﬁlter condition, in ECL a ﬁltered physical dataset is called a recordset
(continued)

94
A.M. Middleton et al.
Table 3.1 (continued)
Language feature
or capability
Pig
ECL
Distribution of
data
Controlled by Hadoop MapReduce architecture and
HDFS, no explicit programmer control provided.
PARALLEL allows number of Reduce tasks to be
speciﬁed. Local operations only are supported,
global operations require custom Java MapReduce
programs
Explicit programmer control over distribution of data across cluster using
DISTRIBUTE function. Helps avoid data skew. ECL supports both local
(operations are performed on data local to node) and global (operations
performed across nodes) modes
Operators
Standard comparison operators; standard arithmetic
operators and modulus division, Boolean operators
AND, OR, NOT; null operators (is null, is not null);
dereference operators for tuples and maps; explicit
cast operator; minus and plus sign operators;
matches operator
Supports arithmetic operators including normal division, integer division, and
modulus division; bitwise operators for AND, OR, and XOR; standard
comparison operators; Boolean operators NOT, AND, OR; explicit cast
operator; minus and plus sign operators; set and record set operators;
string concatenation operator; sort descending and ascending operator;
special operators IN, BETWEEN, WITHIN
Conditional
Expression
Evaluation
The bincond operator is provided (condition ?
true value: false value)
ECL includes an IF statement for single expression conditional evaluation,
and MAP, CASE, CHOOSE, WHICH, and REJECTED for multiple
expression evaluation. The ASSERT statement can be used to test a
condition across a dataset. EXISTS can be used to determine if records
meeting the speciﬁed condition exist in a dataset. ISVALID determines if
a ﬁeld contains a valid value
Program Loops
No capability exists other than the standard relation
operations across a dataset.
FOREACH. . . GENERATE provides nested
capability to combine speciﬁc relation operations
In addition to built-in data transform functions, ECL provides LOOP and
GRAPH statements which allow looping of dataset operations or iteration
of a speciﬁed process on a dataset until a loopﬁlter condition is met or a
loopcount is satisﬁed
Indexes
Not supported directly by Pig. HBase and Hive provide
indexed data capability for Hadoop MapReduce
which is accessible through custom user-deﬁned
functions in Pig
Indexes can be created on datasets to support keyed access to data to improve
data processing performance and for use on the Roxie data delivery
engine for query applications

3
ECL/HPCC: A Uniﬁed Approach to Big Data
95
Language
Statement
Types
Grouped into relational operators, diagnostic operators,
UDF (user-deﬁned function) statements, Eval
functions, and load/store functions. The Grunt shell
offers additional interactive ﬁle commands
Grouped into dataset, index and record deﬁnitions, built-in functions to deﬁne
processing and dataﬂows and workﬂow management, and actions which
trigger execution. Functions include transform functions such as JOIN
which operate on data records, and aggregation functions such as SUM.
Action statements result in execution based on speciﬁed ECL deﬁnitions
describing the dataﬂows and results for a process
External
Program Calls
PIG includes the STREAM statement to send data to an
external script or program. The SHIP statement can
be used to ship program binaries, jar ﬁles, or data to
the Hadoop cluster compute nodes. The DEFINE
statement, with INPUT, OUTPUT, SHIP, and
CACHE clauses allow functions and commands to
be associated with STREAM to access external
programs
ECL includes PIPE option on DATASET and OUTPUT and a PIPE function
to execute external third-party programs in parallel on nodes across the
cluster. Most programs which receive an input ﬁle and parameters can
adapted to run in the HPCC environment
External Web
Services Access
Not supported directly by the Pig language.
User-deﬁned functions written in Java can provide
this capability
Built-in ECL function SOAPCALL for SOAP calls to access external Web
Services. An entire dataset can be processed by a single SOAPCALL in
an ECL program
Data
Aggregation
Implemented in Pig using the GROUP, and
FOREACH. . . GENERATE statements performing
EVAL functions on ﬁelds. Built-in EVAL functions
include AVG, CONCAT, COUNT, DIFF, ISEMPTY,
MAX, MIN, SIZE, SUM, TOKENIZE
Implemented in ECL using the TABLE statement with group by ﬁelds
speciﬁed and an output record deﬁnition that includes computed ﬁelds
using expressions with aggregation functions performed across the
speciﬁed group. Built-in aggregation functions which work across
datasets or groups include AVE, CORRELATION, COUNT,
COVARIANCE, MAX, MIN, SUM, VARIANCE
Natural
Language
Processing
The TOKENIZE statement splits a string and outputs a
bag of words. Otherwise no direct language support
for parsing and other natural language processing.
User-deﬁned functions are required
Includes PATTERN, RULE, TOKEN, and DEFINE statements for deﬁning
parsing patterns, rules, and grammars. Patterns can include regular
expression deﬁnitions and user-deﬁned validation functions. The PARSE
statement provides both regular expression type parsing or Tomita parsing
capability and recursive grammars. Special parsing syntax is included
speciﬁcally for XML data
(continued)

96
A.M. Middleton et al.
Table 3.1 (continued)
Language feature
or capability
Pig
ECL
Scientiﬁc
Function
Support
Not supported directly by the Pig language. Requires
the deﬁnition and use of a user-deﬁned function
ECL provides built-in functions for ABS, ACOS, ASIN, ATAN, ATAN2,
COS, COSH, EXP, LN, LOG, ROUND, ROUNDUP,SIN, SINH, SQRT,
TAN, TANH
Hashing
Functions for
Dataset
Distribution
No explicit programmer control for dataset distribution.
PARALLEL option on relational operations allows
the number of Reduce tasks to be speciﬁed
Hashing functions available for use with the DISTRIBUTE statement include
HASH, HASH32 (32-bit FNV), HASH64 (64-bit FNV), HASHCRC,
HASHMD5 (128-bit MD5)
Creating
Sample Datasets
The SAMPLE operation selects a random data sample
with a speciﬁed sample size
ECL provides ENTH which selects every nth record of a dataset, SAMPLE
which provides the capability to select non-overlapping samples on a
speciﬁed interval, CHOOSEN which selects the ﬁrst n records of a dataset
and CHOOSESETS which allows multiple conditions to be speciﬁed and
the number of records that meet the condition or optionally a number of
records that meet none of the conditions speciﬁed. The base dataset for
each of the ENTH, SAMPLE, CHOOSEN, and CHOOSETS can have an
associated ﬁlter expression
Workﬂow
Management
No language statements in Pig directly affect Workﬂow.
The Hadoop cluster does allow Java MapReduce
programs access to speciﬁc workﬂow information
and scheduling options to manage execution
Workﬂow Services in ECL include the CHECKPOINT and PERSIST
statements allow the dataﬂow to be captured at speciﬁc points in the
execution of an ECL program. If a program must be rerun because of a
cluster failure, it will resume at last Checkpoint which is deleted after
completion. The PERSIST ﬁles are stored permanently in the ﬁlesystem.
If a job is repeated, persisted steps are only recalculated if the code has
changed, or any underlying data has changed. Other workﬂow statements
include FAILURE to trap expression evaluation failures, PRIORITY,
RECOVERY, STORED, SUCCESS, WHEN for processing events,
GLOBAL and INDEPENDENT

3
ECL/HPCC: A Uniﬁed Approach to Big Data
97
PIG Relation
Operations:
COGROUP
The COGROUP operation is similar to the JOIN
operation and groups the data in two or more
relations (datasets) based on common ﬁeld values.
COGROUP creates a nested set of output tuples
while JOIN creates a ﬂat set of output tuples.
INNER and OUTER joins are supported. Fields
from each relation are speciﬁed as the join key. No
support exists for conditional processing other than
ﬁeld equality
In ECL, this is accomplished using the DENORMALIZE function joining to
each dataset and adding all records matching the join key to a new record
format with a child dataset for each child ﬁle. The DENORMALIZE
function is similar to a JOIN and is used to form a combined record out of
a parent and any number of children
CROSS
Creates the cross product of two or more relations
(datasets).
In ECL the JOIN operation can be used to create cross products using a join
condition that is always true
DISTINCT
Removes duplicate tuples in a relation. All ﬁelds in the
tuple must match. The tuples are sorted prior to this
operation. Cannot be used on a subset of ﬁelds. A
FOREACH. . . GENERATE statement must be used
to generate the ﬁelds prior to a DISTINCT operation
in this case
The ECL DEDUP statement compares adjacent records to determine if a
speciﬁed conditional expression is met, in which case the duplicate record
is dropped and the remaining record is compared to the next record in a
sliding window manner. This provides a much more ﬂexible deduplication
capability than the Pig DISTINCT operation. A SORT is required prior to
a DEDUP unless using the ALL option. Conditions can use any
expression and can reference values from the left and right adjacent
records. DEDUP can use any subset of ﬁelds
DUMP
Displays the contents of a relation
ECL provides an OUTPUT statement that can either write ﬁles to the
ﬁlesystem or for display. Display ﬁles can be named and are stored in the
Workunit associated with the job. Workunits are archived on a
management server in the HPCC platform
FILTER
Selects tuples from a relation based on a condition.
Used to select the data you want or conversely to
ﬁlter out remove the data you don’t want
Filter expressions can be used any time a dataset or recordset is referenced in
any ECL statement with the ﬁlter expression in parenthesis following the
dataset name as dataset name(ﬁlter expression). The ECL compiler
optimizes ﬁltering of the data during execution based on the combination
of ﬁltering expressions
(continued)

98
A.M. Middleton et al.
Table 3.1 (continued)
Language feature
or capability
Pig
ECL
FOREACH . . .
GENERATE
Generates data transformations based on columns of
data This action can be used for projection,
aggregation, and transformation, and can include
other operations in the generation clause such as
FILTER, DISTINCT, GROUP, etc
Each ECL transform operation such as PROJECT, JOIN, ROLLUP, etc.
include a TRANSFORM function which implicitly provides the
FOREACH . . . GENERATE operation as records are processed by the
TRANSFORM function. Depending on the function, the output record of
the transform can include ﬁelds from the input and computed ﬁelds
selectively as needed and does not have to be identical to the input record
GROUP
Groups together the tuples in a single relation that have
the same group key ﬁelds
The GROUP operation in ECL fragments a dataset into a set of sets based on
the break criteria which is a list of ﬁelds or expressions based on ﬁelds in
the record which function as the group by keys. This allows aggregations
and transform operations such as ITERATE, SORT, DEDUP, ROLLUP
and others to occur within deﬁned subsets of the data as it executes on
each subset individually
JOIN
Joins two or more relations based on common ﬁeld
values. The JOIN operator always performs an inner
join. If one relation is small and can be held in
memory, the “replicated” option can be used to
improve performance
The ECL JOIN operation works on two datasets or a set of datasets. For two
datasets INNER, FULL OUTER, LEFT OUTER, RIGHT OUTER, LEFT
ONLY and RIGHT ONLY joins are permitted. For the set of datasets
JOIN, INNER, LEFT OUTER, LEFT ONLY, and MOFN(min, max) joins
are permitted. Any type of conditional expression referencing ﬁelds in the
datasets to be joined can be used as a join condition. JOIN can be used in
both a global and local modes also provides additional options for
distribution including HASH which distributes the datasets by the
speciﬁed join keys, and LOOKUP which copies one dataset if small to all
nodes and is similar to the “replicated” join feature of Pig. Joins can also
use keyed indexes to improve performance and self-joins (joining the
same dataset to itself) is supported. Additional join-type operations
provided by ECL include MERGEJOIN which joins and merges in a
single operation, and smart stepping using STEPPED which provides a
method of doing n-ary join/merge-join operations

3
ECL/HPCC: A Uniﬁed Approach to Big Data
99
LIMIT
Used to limit the number of output tuples in a relation.
However, there is no guarantee of which tuples will
be output unless preceded by an ORDER statement
The LIMIT function in ECL is to restrict the output of a recordset resulting
from processing to a maximum number or records, or to fail the operation
if the limit is exceeded. The CHOOSEN function can be use to select a
speciﬁed number of records in a dataset
LOAD
Loads data from the ﬁlesystem
Since ECL is declarative, the equivalent of the Pig LOAD operation is a
DATASET deﬁnition which also includes a RECORD deﬁnition. The
examples shown in Figs. 3.7 and 3.9 demonstrate this difference
ORDER
Sorts a relation based on one or more ﬁelds. Both
ascending and descending sorts are supported.
Relations will be in order for a DUMP, but if the
result of an ORDER is further processed by another
relation operation, there is no guarantee the results
will be processed in the order speciﬁed. Relations
are considered to be unordered in Pig
The ECL SORT function sorts a dataset according to a list of expressions or
key ﬁelds. The SORT can be global in which the dataset will be ordered
across the nodes in a cluster, or local in which the dataset will be ordered
on each node in the cluster individually. For grouped datasets, the SORT
applies to each group individually. Sorting operations can be performed
using a quicksort, insertionsort, or heapsort, and can be stable or unstable
for duplicates
SPLIT
Partitions a relation into two or more relations
Since ECL is declarative, partitions are created by simply specifying ﬁlter
expressions on the base dataset. Example for dataset DS1, you could
deﬁne DS2:= DS1(ﬁlter expression 1), DS3:= DS1(ﬁlter expression 2),
etc
STORE
Stores data to the ﬁle system
The OUTPUT function in ECL is used to write a dataset to the ﬁlesystem or
to store it in the workunit for display. Output ﬁles can be compressed
using LZW compression. Variations of OUTPUT support ﬂat ﬁle, CSV,
and XML formats. Output can also be written to a PIPE as the standard
input to the command speciﬁed for the PIPE operation. Output can write
not only the ﬁlesystem on the local cluster, but to any cluster ﬁlesystem in
the HPCC processing environment
UNION
The UNION operator is used to merge the contents of
two or more relations into a single relation. Order of
tuples is not preserved, both input and output
relations are interpreted as an unordered bag of
tuples. Does not eliminate duplicate tuples
The MERGE function returns a single dataset or index containing all the
datasets or indexes speciﬁed in a list of datasets. Datasets must have the
same record format. A SORTED option allows the merge to be ordered
according to a ﬁeld list that speciﬁes the sort order. A DEDUP option
causes only records with unique keys to be included. The REGROUP
function allows multiple datasets which have been grouped using the
same ﬁelds to be merged into a single dataset
(continued)

100
A.M. Middleton et al.
Table 3.1 (continued)
Language feature
or capability
Pig
ECL
Additional ECL
Transformation
Functions
ECL includes many additional functions providing important data
transformations that are not available in Pig without implementing custom
user-deﬁned processing
AGGREGATE
Not Available
The AGGREGATE function allows arbitrary aggregation operations to be
performed on pairs of records. It is similar to the ROLLUP function
except that the input record format and output record format can be
different, and includes both a process transform function and an optional
merge transform function. Grouping ﬁelds can be speciﬁed so the
aggregation operates on matching groups within a dataset
COMBINE
Not available
The COMBINE function combines two datasets into a single dataset on a
record-by-record basis in the order in which they appear in each. Records
from each are passed to the speciﬁed transform function, and the record
format of the output dataset can contain selected ﬁelds from both input
datasets and additional ﬁelds as needed
FETCH
Not available
The FETCH function processes through all the records in an index dataset in
the order speciﬁed by the index fetching the corresponding record from
the base dataset and passing it through a speciﬁed transform function to
create a new dataset
ITERATE
Not available
The ITERATE function processes through all records in a dataset one pair of
records at a time using a sliding window method performing the transform
record on each pair in turn. If the dataset is grouped, the ITERATE
processes each group individually. The ITERATE function is useful in
propagating information and calculating new information such as running
totals since it allows inter-record dependencies to be considered

3
ECL/HPCC: A Uniﬁed Approach to Big Data
101
NORMALIZE
Use of FOREACH. . . GENERATE is required
The NORMALIZE function normalizes child records out of a dataset into a
separate dataset. The associated transform and output record format does
not have to be the same as the input
PROCESS
Not available
The PROCESS function is similar to ITERATE and processes through all
records in a dataset one pair of records at a time (left record, right record)
using a sliding window method performing the associated transform
function on each pair of records in turn. A second transform function is
also speciﬁed that constructs the right record for the next comparison
PROJECT
Use of FOREACH . . . GENERATE is required
The PROJECT processes through all the records in a dataset performing the
speciﬁed transform on each record in turn
ROLLUP
Not available
The ROLLUP function is similar to the DEDUP function but includes a
speciﬁed transform function to process each pair of duplicate records.
This allows you to retrieve and use valuable information from the
duplicate record before it is thrown away. Depending on how the
ROLLUP is deﬁned, either the left or right record passed to the transform
can be retained, or any mixture of data from both
Diagnostic
Operators
Pig includes diagnostic operators to aid in the
visualization of data structures. The DESCRIBE
operator returns the schema of a relation. The
EXPLAIN operator allows you to review the logical,
physical, and MapReduce execution plans that are
used to compute an operation in a Pig script. The
ILLUSTRATE operator displays a step-by-step
execution of a sequence of statements allow you to
see how data is transformed through a sequence of
Pig Latin statements essentially dumping the output
of each statement in the script
The DISTRIBUTION action produces a crosstab report in XML format
indicating how many records there are in a dataset for each value in each
ﬁeld in the dataset to aid in the analysis of data distribution in order to
avoid skews. The ECL IDE and ECLWatch program development
environment tools provide a complete visualization tool for analyzing,
debugging, and proﬁling execution of ECL jobs. During the execution of
a job, the dataﬂows expressed by ECL can be viewed as a directed acyclic
graph (DAG) which shows the execution plan, dataﬂows as they occur,
and the results of each processing step. Users can double click on the
graph to drill down for additional information. An example of the graph
corresponding to the ECL code shown in Fig. 3.9 is shown n Fig. 3.10

102
A.M. Middleton et al.
Fig. 3.17 PigMix L3 code converted to ECL
4.9
Pig Versus ECL Performance
To test the performance of HPCC using ECL versus Hadoop and Pig, the PigMix
set of benchmark tests was used. This is a set of 17 Pig programs within the
Pig community that have been used to measure the comparative performance
of Pig and Java/Hadoop since 11/8/2008. To produce ECL versions of the Pig
programs, an automatic Pig ! ECL translator called Bacon was used. This allowed
the Pig programs to be converted without using any special ECL programming
techniques to achieve the result, and basically duplicated the steps deﬁned in the
Pig benchmark. This was necessary because the PigMix benchmark essentially tests
speciﬁc Pig language features, and the individual programs are not intended to solve
a particular real world problem. This approach resulted in three versions of the
tests which could be executed: a Pig language version, a Java version which the
Pig version is usually tested against, and an ECL version for comparison. As an
example, Fig. 3.17 shows PigMix Test L3 in the Pig language in the comment block,
and shows the same PigMix Test L3 in the ECL version as translated by the Bacon
converter program.
A 25-node commodity hardware cluster was selected to perform the benchmark.
Each node had 4 GB of memory, and 600 MB of disk storage. Nodes were
interconnected through a non-blocking Gigabit switch backplane using 1 Gbps
connections. The cluster was ﬁrst installed with Hadoop and Pig, the PigMix data
was generated into the Hadoop ﬁlesystem, and the benchmark test was performed;
the data was then copied to temporary storage, HPCC and ECL was installed

3
ECL/HPCC: A Uniﬁed Approach to Big Data
103
Fig. 3.18 PigMix comparison benchmark results
on the test cluster, and the data was loaded to the HPCC ﬁlesystem, and the
benchmark was performed. Pig programs were used to ﬂatten the data structures
prior to loading in the HPCC ﬁlesystem, and although data is represented differently,
nested relationships were maintained. The data was generated using the Perl data
generation script generate data.pl found in issue PIG-200 on the Apache site. The
script allows varying sizes of the base data set page views to be generated depending
on the size of the available cluster, with a default of 625M rows.
The timings provided on the PigMix Wiki page are based on only 10M page view
rows. The results presented here are using a page views dataset with 156.25M
rows which more effectively demonstrates the handling of big data on the 25-node
cluster tested. Additional datasets generated by the script include page views sorted
(156.25M rows), power users (500 rows), power users samples (252 rows), users
(1,599,555 rows), users sorted (1,599,555rows), widegroupbydata(156.25M rows),
and widerow (10M rows). The generated data was translated from the Pig data
model to the ECL data model where needed for the ECL version of the benchmark
but maintained the identical data content, size, and nested relationships. The
benchmark results are shown in Fig. 3.18.
It can be seen that there is signiﬁcant variation from test to test. The outlier is
L17 where the Pig time is much worse than Java and worse than on the ofﬁcial Pig
website. This is a newer test so it could indicate a new optimization has not yet
made it into release code and that Pig number will improve. L9 and L10 are also
interesting in that it shows that the Pig code (which generates Java) substantially
beats the original Java baseline. This demonstrates the work that has been put into
enhancing the Pig performance on these benchmarks.

104
A.M. Middleton et al.
Comparing ECL to Pig we see that it substantially wins every single comparison.
The weakest win is L15 where it is 1.46 faster than Pig. The strongest win
(ignoring the outlier 24 fold improvement on L17) is the 4.74 improvement
granted by the patented SORT algorithm used by ECL on L10. Across all tests
ECL was an average 4.45 faster than Pig. Comparing ECL to Java, the gap shrinks
some but ECL still wins every single test. The weakest is again L15 with a 1.175
speedup and the strongest is L10 with a 6.35 speedup. Across all tests ECL is an
average 3.23 faster than native coded Java/Hadoop.
5
Conclusions
As a result of the continuing information explosion, many organizations are
drowning in data and are experiencing the “Big Data” problem making it harder and
harder to process and gain useful insights from their data. Data-intensive computing
represents a new computing paradigm which can address the big data problem
and allow government and commercial organizations and research environments to
process massive amounts of data and implement applications previously thought
to be impractical or infeasible. Several organizations developed new parallel-
processing architectures using commodity computing clusters including Google
who initially developed the MapReduce architecture and LexisNexis who developed
the HPCC architecture and the ECL programming language. An open source
version of MapReduce called Hadoop was developed with additional capabilities
to enhance the platform including a data-oriented programming language and
execution environment called Pig. The open source HPCC platform and the ECL
programming language are described in this chapter, and a direct comparison
of the Pig language of Hadoop to the ECL language was presented along with
a representative benchmark. Availability of a high-level declarative, data-centric,
dataﬂow-oriented programming language has proven to be a critical success factor
in data-intensive computing.
The LexisNexis HPCC platform is at the heart of a premier information services
provider and industry leader, and has been adopted by government agencies, com-
mercial organizations, and research laboratories because of its high-performance
cost-effective implementation. Existing HPCC applications implemented using the
ECL language include raw data processing, ETL, and linking of enormous amounts
of data to support online information services such as LexisNexis and industry-
leading information search applications such as Accurint; entity extraction and
entity resolution of unstructured and semi-structured data such as Web documents
to support information extraction; statistical analysis of Web logs for security appli-
cations such as intrusion detection; online analytical processing to support business
intelligence systems (BIS); and data analysis of massive datasets in educational and
research environments and by state and federal government agencies.
There are many factors in choosing a new computer systems architecture and
programming language, and usually the best approach is to conduct a speciﬁc bench-

3
ECL/HPCC: A Uniﬁed Approach to Big Data
105
mark test with a customer application to determine the overall system effectiveness
and performance. A comparison of the Hadoop MapReduce architecture using a
public benchmark for the Pig programming language to the HPCC architecture
and ECL programming language on the same system hardware conﬁguration in
this chapter reveals signiﬁcant performance advantages for the HPCC platform
with ECL. Some additional advantages of choosing the LexisNexis HPCC platform
with ECL include: (1) an open source architecture which implements a highly
integrated system environment with capabilities from raw data processing to high-
performance queries and data analysis using a common language; (2) a scalable
architecture which provides equivalent performance at a much lower system cost
based on the number of processing nodes required compared to other data-intensive
computing architectures such as MapReduce; (3) an architecture which has been
proven to be stable and reliable on high-performance data processing production
applications for varied organizations over a 10-year period; (4) an architecture that
uses a declarative, data-centric programming language (ECL) with extensive built-
in capabilities for data-parallel processing, allows complex operations without the
need for extensive user-deﬁned functions, and automatically optimizes execution
graphs with hundreds of processing steps into single efﬁcient workunits; (5) an
architecture with a high-level of fault resilience and language capabilities which
reduce the need for re-processing in case of system failures; and (6) an architecture
which is available in open source from and supported by a well-known leader
in information services and risk solutions (LexisNexis) who is part of one of the
world’s largest publishers of information ReedElsevier.
References
1. Abbas, A. (2004). Grid computing: A practical guide to technology and applications.
Hingham, MA: Charles River Media, Inc.
2. Agichtein, E. (2004). Scaling information extraction to large document collections: Microsoft
Research.
3. Agichtein, E., & Ganti, V. (2004). Mining reference tables for automatic text segmentation.
Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, Seattle, WA, USA, 20–29.
4. Bayliss, D. A. (2010a). Aggregated data analysis: The paradigm shift (Whitepaper): Lexis-
Nexis.
5. Bayliss, D. A. (2010b). Enterrprise control language overview (Whitepaper): LexisNexis.
6. Bayliss, D. A. (2010c). Thinking declaratively (Whitepaper).
7. Berman, F. (2008). Got data? A guide to data preservation in the information age. Communi-
cations of the ACM, 51(12), 50–56.
8. Bryant, R. E. (2008). Data intensive scalable computing. Carnegie Mellon University.
Retrieved August 10, 2009, from http://www.cs.cmu.edu/$nsim$bryant/presentations/DISC-
concept.ppt
9. Buyya, R. (1999). High performance cluster computing. Upper Saddle River, NJ: Prentice Hall.
10. Buyya, R., Yeo, C. S., Venugopal, S., Broberg, J., & Brandic, I. (2009). Cloud computing and
emerging it platforms: Vision, hype, and reality for delivering computing as the 5th utility.
Future Generation Computer Systems, 25(6), 599–616.

106
A.M. Middleton et al.
11. Cerf, V. G. (2007). An information avalanche. IEEE Computer, 40(1), 104–105.
12. Chaiken, R., Jenkins, B., Larson, P.-A., Ramsey, B., Shakib, D., Weaver, S., et al. (2008).
Scope: Easy and efﬁcient parallel processing of massive data sets. Proceedings of the VLDB
Endowment; 1, 1265–1276.
13. Dean, J., & Ghemawat, S. (2004). Mapreduce: Simpliﬁed data processing on large clusters.
Proceedings of the Sixth Symposium on Operating System Design and Implementation
(OSDI).
14. Dean, J., & Ghemawat, S. (2010). Mapreduce: A ﬂexible data processing tool. Communications
of the ACM, 53(1), 72–77.
15. Dowd, K., & Severance, C. (1998). High performance computing. Sebastopol, CA: O’Reilly
and Associates, Inc.
16. Gantz, J. F., Reinsel, D., Chute, C., Schlichting, W., McArthur, J., Minton, S., et al. (2007).
The expanding digital universe (White Paper): IDC.
17. Gates, A. F., Natkovich, O., Chopra, S., Kamath, P., Narayanamurthy, S. M., Olston, C., et al.
(2009, Aug 24–28). Building a high-level dataﬂow system on top of map-reduce: The pig
experience. Proceedings of the 35th International Conference on Very Large Databases (VLDB
2009), Lyon, France.
18. Gokhale, M., Cohen, J., Yoo, A., & Miller, W. M. (2008). Hardware technologies for high-
performance data-intensive computing. IEEE Computer, 41(4), 60–68.
19. Gorton, I., Greenﬁeld, P., Szalay, A., & Williams, R. (2008). Data-intensive computing in the
21st century. IEEE Computer, 41(4), 30–32.
20. Gray, J. (2008). Distributed computing economics. ACM Queue, 6(3), 63–68.
21. Grossman, R., & Gu, Y. (2008). Data mining using high performance data clouds: Experi-
mental studies using sector and sphere. Proceedings of the 14th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, Las Vegas, Nevada, USA.
22. Grossman, R. L., Gu, Y., Sabala, M., & Zhang, W. (2009). Compute and storage clouds using
wide area high performance networks. Future Generation Computer Systems, 25(2), 179–183.
23. Gu, Y., & Grossman, R. L. (2009). Lessons learned from a year’s worth of benchmarks of
large data clouds. Proceedings of the 2nd Workshop on Many-Task Computing on Grids and
Supercomputers, Portland, Oregon.
24. Hellerstein, J. M. (2010). The declarative imperative. SIGMOD Record, 39(1), 5–19.
25. Johnston, W. E. (1998). High-speed, wide area, data intensive computing: A ten year
retrospective, Proceedings of the 7th IEEE International Symposium on High Performance
Distributed Computing: IEEE Computer Society.
26. Kouzes, R. T., Anderson, G. A., Elbert, S. T., Gorton, I., & Gracio, D. K. (2009). The changing
paradigm of data-intensive computing. Computer, 42(1), 26–34.
27. Liu, H., & Orban, D. (2008). Gridbatch: Cloud computing for large-scale data-intensive batch
applications. Proceedings of the Eighth IEEE International Symposium on Cluster Computing
and the Grid, 295–305.
28. Llor, X., Acs, B., Auvil, L. S., Capitanu, B., Welge, M. E., & Goldberg, D. E. (2008).
Meandre: Semantic-driven data-intensive ﬂows in the clouds. Proceedings of the Fourth IEEE
International Conference on eScience, 238–245.
29. Lyman, P., & Varian, H. R. (2003). How much information? 2003 (Research Report): School
of Information Management and Systems, University of California at Berkeley.
30. Middleton, A. M. (2009). Data-intensive computing solutions (Whitepaper): LexisNexis.
31. NSF. (2009). Data-intensive computing. National Science Foundation. Retrieved August 10,
2009, from http://www.nsf.gov/funding/pgm summ.jsp?pims id=503324&org=IIS
32. Nyland, L. S., Prins, J. F., Goldberg, A., & Mills, P. H. (2000). A design methodology for
data-parallel applications. IEEE Transactions on Software Engineering, 26(4), 293–314.
33. O’Malley, O. (2008). Introduction to hadoop. Retrieved August 10, 2009, from http://wiki.
apache.org/hadoop-data/attachments/HadoopPresentations/attachments/YahooHadoopIntro-
apachecon-us-2008.pdf
34. Olston, C., Reed, B., Srivastava, U., Kumar, R., & Tomkins, A. (2008, June 9–12). Pig latin: A
not-so foreign language for data processing. Proceedings of the 28th ACM SIGMOD/PODS

3
ECL/HPCC: A Uniﬁed Approach to Big Data
107
International Conference on Management of Data/Principles of Database Systems, Vancouver,
BC, Canada, 1099–1110.
35. Pavlo, A., Paulson, E., Rasin, A., Abadi, D. J., Dewitt, D. J., Madden, S., et al. (2009, June
29–July 2). A comparison of approaches to large-scale data analysis. Proceedings of the 35th
SIGMOD international conference on Management of data, Providence, RI, 165–168.
36. Pike, R., Dorward, S., Griesemer, R., & Quinlan, S. (2004). Interpreting the data: Parallel
analysis with sawzall. Scientiﬁc Programming Journal, 13(4), 227–298.
37. PNNL. (2008). Data intensive computing. Paciﬁc Northwest National Laboratory. Retrieved
August 10, 2009, from http://www.cs.cmu.edu/$nsim$bryant/presentations/DISC-concept.ppt
38. Ravichandran, D., Pantel, P., & Hovy, E. (2004). The terascale challenge. Proceedings of the
KDD Workshop on Mining for and from the Semantic Web.
39. Rencuzogullari, U., & Dwarkadas, S. (2001). Dynamic adaptation to available resources for
parallel computing in an autonomous network of workstations. Proceedings of the Eighth ACM
SIGPLAN Symposium on Principles and Practices of Parallel Programming, Snowbird, UT,
72–81.
40. Skillicorn, D. B., & Talia, D. (1998). Models and languages for parallel computation. ACM
Computing Surveys, 30(2), 123–169.
41. White, T. (2009). Hadoop: The deﬁnitive guide (First ed.). Sebastopol, CA: O’Reilly Media
Inc.
42. Yu, Y., Gunda, P. K., & Isard, M. (2009). Distributed aggregation for data-parallel computing:
Interfaces and implementations. Proceedings of the ACM SIGOPS 22nd Symposium on
Operating Systems Principles, Big Sky, Montana, USA, 247–260.


Chapter 4
Scalable Storage for Data-Intensive Computing
Abhishek Verma, Shivaram Venkataraman, Matthew Caesar,
and Roy H. Campbell
1
Introduction
Persistent storage is a fundamental abstraction in computing. It consists of a named
set of data items that come into existence through explicit creation, persist through
temporary failures of the system, until they are explicitly deleted. Sharing of data in
distributed systems has become pervasive as these systems have grown in scale in
terms of number of machines and the amount of data stored.
The phenomenal growth of web services in the past decade has resulted in many
Internet companies needing to perform large scale data analysis such as indexing
the contents of the billions of websites or analyzing terabytes of trafﬁc logs to mine
usage patterns. A study into the economics of distributed computing [1] published
in 2008, revealed that the cost of transferring data across the network is relatively
high. Hence moving computation near the data is a more efﬁcient computing model
and several large scale, data-intensive application frameworks [2, 3] exemplify this
model.
The growing size of the datacenter also means that hardware failures occur more
frequently making such data analysis much harder. A recent presentation about a
typical Google datacenter reported that up to 5% of disk drives fail each year and
that every server restarts at least twice a year due to software or hardware issues [4].
With the size of digital data doubling every 18 months [5], it is also essential that
applications are designed to scale and meet the growing demands.
To deal with these challenges, there has been a lot of work on building large
scale distributed ﬁle systems. Distributed data storage has been identiﬁed as one
of the challenges in cloud computing [6]. An efﬁcient distributed ﬁle system
needs to:
A. Verma () • S. Venkataraman • M. Caesar • R.H. Campbell
Department of Computer Science, University of Illinois at Urbana-Champaign,
201 N. Goodwin Avenue, Urbana, IL 61801, USA
e-mail: verma7@illinois.edu; venkata4@illinois.edu; caesar@illinois.edu; rhc@illinois.edu
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 4, © Springer Science+Business Media, LLC 2011
109

110
A. Verma et al.
1. Provide large bandwidth for data access from multiple concurrent jobs
2. Operate reliably amidst hardware failures
3. Be able to scale to many millions or billions of ﬁles and thousands of machines
The Google File System (GFS) [7] was proposed to meet the above requirements
and has since been cloned in open source projects such as Hadoop Distributed File
System (HDFS)1 and Kosmos File System (KFS)2 that are used by companies such
as Yahoo, Facebook, Amazon, Baidu, etc.
The GFS architecture was picked for its simplicity and works well for hundreds
of terabytes with few millions of ﬁles [8]. One of the direct implications of storing
all the metadata in memory is that the size of metadata is limited by the memory
available. A typical GFS master is capable of handling a few thousand operations per
second [8] but when massively parallel applications like a MapReduce [2] job with
many thousand mappers need to open a number of ﬁles, the GFS master becomes
overloaded. Though the probability of a single server failing in a datacenter is low
and the GFS master is continuously monitored, it still remains a single point of
failure for the system. With storage requirements growing to petabytes, there is a
need for distributing the metadata storage to more than one server.
Having multiple servers to handle failure would increase the overall reliability
of the system and reduce the downtime visible to clients. As datacenters grow
to accommodate many thousands of machines in one location, distributing the
metadata operations among multiple servers would be necessary to increase the
throughput. Handling metadata operations efﬁciently is an important aspect of
the ﬁle system as they constitute up to half of ﬁle system workloads [9]. While
I/O bandwidth available for a distributed ﬁle system can be increased by adding
more data storage servers, scaling metadata management involves dealing with
consistency issues across replicated servers.
Peer-to-peer storage systems [10], studied previously, provide decentralized con-
trol and tolerance to failures in untrusted-Internet scale environments. Grid Com-
puting has been suggested as a potential environment for peer-to-peer ideas [11].
Similarly we believe that large scale cloud computing applications could beneﬁt by
adopting peer-to-peer system designs. In this work, we address the above mentioned
limitations and present the design of Ring File System (RFS), a distributed
ﬁle system for large scale data-intensive applications. In RFS, the metadata is
distributed among multiple replicas connected using a distributed hash table (DHT).
This design provides better fault tolerance and scalability while ensuring a high
throughput for metadata operations from multiple clients.
The major contributions of our work include:
1. A metadata storage architecture that provides fault tolerance, improved through-
put and increased scalability for the ﬁle system.
2. Studying the impact of the proposed design through analysis and simulations.
1http://hadoop.apache.org/
2http://kosmosfs.sourceforge.net/

4
Scalable Storage for Data-Intensive Computing
111
3. Implementing and deploying RFS on a 16-node cluster and comparison with
HDFS and KFS.
The rest of this chapter is organized as follows: We ﬁrst provide a background
on two popular traditional distributed ﬁle systems NFS and AFS in Sect.2. Then,
we discuss how peer-to-peer system ideas have been used to design distributed
storage systems in Sect. 3. Section 4 discusses how current cloud storage systems
are designed based on the amalgamation of ideas from traditional and P2P-based
ﬁle systems. We describe the design of our system, RFS, in Sect. 5 and analyze its
implications in Sect. 6. We then demonstrate the scalability and fault tolerance of our
design through simulations followed by implementation results in Sect. 7. Section 8
summarizes the metadata management techniques in existing distributed ﬁle system
and their limitations. We discuss possible future work and conclude with Sect. 5.
2
Traditional Distributed Filesystems
In this section, we examine two traditional distributed ﬁlesystems NFS and AFS
and focus on their design goals and consistency mechanisms. Traditional distributed
ﬁlesystems are typically geared towards providing sharing capabilities among
multiple (human) users under a common administrative domain.
2.1
NFS
The NFS [12] protocol has been an industry standard since its introduction by
Sun Microsystems in the 1980s. It allows remote clients to mount ﬁle systems
over the network and interact with those ﬁle systems as if they were mounted
locally. Although the ﬁrst implementation of NFS was in a Unix environment, NFS
is now implemented within several different OS environments. File manipulation
primitives supported by NFS are: read, write, create a ﬁle or directory, remove a
ﬁle or directory. The NFS protocol is designed to be machine, operating system,
network architecture, and transport protocol independent. This independence is
achieved through the use of Remote Procedure Call (RPC) primitives built on top of
an eXternal Data Representation (XDR).
NFS uses the Virtual ﬁle system (VFS) layer to handle local and remote ﬁles.
VFS provides a standard ﬁle system interface and allows NFS to hide the difference
between accessing local or remote ﬁle systems.
NFS is a stateless protocol, i.e., the server does not maintain the state of
ﬁles – there are no open or close primitives in NFS. Hence, each client request
contains all the necessary information for the server to complete the request and
the server responds fully to every client’s request without being aware of the
conditions under which the client is making the request. Only the client knows the

112
A. Verma et al.
state of a ﬁle for which an operation is requested. If the server and/or the client
maintains state, the failure of a client, server or the network is difﬁcult to recover
from.
NFS uses the mount protocol to access remote ﬁles, which establishes a local
name for remote ﬁles. Thus, the users access remote ﬁles using local names, while
the OS takes care of the mapping to remote names. Most NFS implementations
provide session semantics for performance reasons – no changes are visible to other
processes until the ﬁle is closed. Using local caches greatly improves performance
at the cost of consistency and reliability. Different implementations use different
caching policies. Sun’s implementation allows cache data to be stable for up to 30 s.
Applications can use locks in order to ensure consistency.
Unlike earlier versions, the NFS version 4 protocol supports traditional ﬁle access
while integrating support for ﬁle locking and the mount protocol. In addition,
support for strong security (and its negotiation), compound operations, client
caching, and internationalization have been added. Client checks cache validity
when the ﬁle is opened. Modiﬁed data is written back to the server when the ﬁle
is closed.
Parallel NFS (pNFS) is a part of the NFS v4.1 standard that allows clients to
access storage devices directly and in parallel. The pNFS architecture eliminates
the scalability and performance issues associated with NFS servers in earlier
deployments. This is achieved by the separation of data and metadata, and moving
the metadata server out of the data path.
2.2
AFS
The Andrew File System (AFS) was developed as a part of the Andrew project at
Carnegie Mellon University. AFS is designed to allow users with workstations to
share data easily. The design of AFS consists two components: a set of centralized
ﬁle servers and a communication network, called Vice, and a client process named
Venus that runs on every workstation. The distributed ﬁle system is mounted as a
single tree in every workstation and Venus communicates with Vice to open ﬁles
and manage the local cache.
The two main design goals of AFS are scalability and security. Scalability is
achieved by caching relevant information in the clients to support a large number
of clients per server. In the ﬁrst version of AFS, clients cached the pathname preﬁx
information and directed requests to the appropriate server. Additionally the ﬁle
cache used in AFS-1 was pessimistic and veriﬁed if the cache was up to date every
time a ﬁle was opened.
AFS-2 was designed to improve the performance and overcome some of the
administrative difﬁculties found in AFS-1. The cache coherence protocol in AFS-2
assumes that the cache was valid unless notiﬁed by a callback. AFS-2 also
introduces the notion of having data volumes to eliminate the static mapping from
ﬁles to servers. Volumes consist of a partial subtree and many volumes are contained

4
Scalable Storage for Data-Intensive Computing
113
in a single disk partition. Furthermore, using volumes helps the design of other
features like read-only snapshots, backups and per-user disk quotas. AFS-2 was
used for around four years at CMU and experiments showed that its performance
was better than NFS [13].
The third version of AFS was motivated by the need to support multiple
administrative domains. Such a design could support a federation of cells but present
users with a single uniﬁed namespace. AFS was also commercialized during this
time and the development of AFS-3 was continued at Transarc Corporation in
1989. The currently available implementation of AFS is a community supported
distribution named OpenAFS.
AFS has also played an important role in shaping the design of other distributed
ﬁle system. Coda a highly-available distributed ﬁle system, also developed at CMU,
is a descendant of AFS-2. The design of NFSv4 published in 2003 was also heavily
inﬂuenced by AFS.
3
P2P-Based Storage Systems
The need for sharing ﬁles over the Internet led to the birth of peer-to-peer systems
like Napster and Gnutella. In this section, we describe the design of two storage
systems based on distributed hash tables (DHTs).
3.1
OceanStore
OceanStore [10] is a global persistent data store that aims to provide a consistent,
highly available storage utility. There are two differentiating design goals:
1. The ability to be constructed from untrusted infrastructure
2. Aggressive promiscuous caching
OceanStore assumes that the infrastructure is fundamentally untrusted – any
server may crash without warning, leak information to third parties or be com-
promised. OceanStore caches data promiscuously anywhere, anytime, in order to
provide faster access and robustness to network partitions. Although aggressive
caching complicates data coherence and location, it provides greater ﬂexibility to
optimize locality and trades off consistency for availability. It also helps to reduce
network congestion by localizing access trafﬁc. Promiscuous caching requires
redundancy and cryptographic techniques to ensure the integrity and authenticity
of the data.
OceanStore employs a Byzantine-fault tolerant commit protocol to provide
strong consistency across replicas. The OceanStore API also allows applications
to weaken their consistency restrictions in exchange for higher performance and
availability.

114
A. Verma et al.
A version-based archival storage system provides durability. OceanStore stores
each version of a data object in a permanent, read-only form, which is encoded with
an erasure code and spread over hundreds or thousands of servers. A small subset
of the encoded fragments are sufﬁcient to reconstruct the archived object; only a
global-scale disaster could disable enough machines to destroy the archived object.
The OceanStore introspection layer adapts the system to improve performance
and fault tolerance. Internal event monitors collect and analyze information such
as usage patterns, network activity, and resource availability. OceanStore can then
adapt to regional outages and denial of service attacks, pro-actively migrate data
towards areas of use and maintain sufﬁciently high levels of data redundancy.
OceanStore objects are identiﬁed by a globally unique identiﬁer (GUID), which
is the secure hash (for e.g., SHA1 [14]) of the owner’s key and a human readable
name. This scheme allows servers to verify and object’s owner efﬁciently and
facilitates access checks and resource accounting.
OceanStore uses Tapestry [15] to store and locate objects. Tapestry is a scalable
overlay network, built on TCP/IP, that frees the OceanStore implementation from
worrying about the location of resources. Each message sent through Tapestry is
addressed with a GUID rather than an IP address; Tapestry routes the message to a
physical host containing a resource with that GUID. Further, Tapestry is locality
aware: if there are several resources with the same GUID, it locates (with high
probability) one that is among the closest to the message source.
3.2
PAST
PAST is a large scale, decentralized, persistent, peer-to-peer storage system that
aims to provide high availability and scalability. PAST is composed of nodes
connected to the Internet and an overlay routing network among the nodes is
constructed using Pastry [16]. PAST supports three operations:
1. Insert: Stores a given ﬁle a k different locations in the PAST network. k
represents the number of replicas created and can be chosen by the user. The ﬁle
identiﬁer is generated using a SHA-1 hash of the ﬁle name, the owner’s public
key and a random salt to ensure that they are unique. k nodes which have an
identiﬁer closest to the ﬁleId are selected to store the node.
2. Lookup: Retrieves a copy of the ﬁle from the nearest available replica.
3. Reclaim: Reclaims the storage of k copies of the ﬁle but does not guarantee that
the ﬁle is no longer available.
Since node identiﬁers and ﬁle identiﬁers are uniformly distributed in their
domains, the number of ﬁles stored by each node is roughly balanced. However,
due to variation in the size of the inserted ﬁles and the capacity of each PAST node
there could be storage imbalances in the system.
There are two schemes proposed to handle such imbalances. In the ﬁrst scheme,
called replica diversion, if one of the k closest nodes to the given ﬁleId does not

4
Scalable Storage for Data-Intensive Computing
115
have enough space to store the ﬁle, a node from the leafset is chosen and a pointer
is maintained in the original node. To handle failures, this pointer is also replicated.
If no suitable node is found for replica diversion the entire request is reverted and
the client is forced to choose a different ﬁleId by using a different random salt.
This scheme is called ﬁle diversion and is a costly operation. Simulations show that
ﬁle diversion is required only for up to 4% of the requests when the leaf set size
is 32.
The replication of a ﬁle in PAST, to k different locations, is to ensure high
availability. However, some popular ﬁles could require more than k replicas to
minimize latency and improve performance. PAST uses any space unused in the
nodes to cache ﬁles which are frequently accessed. When a ﬁle is routed through a
node during an insert or lookup operation, if the size of the ﬁle is less than a fraction
of the available free space, it is cached. The cached copies of a ﬁle are maintained
in addition to the k replicas and are evicted when space is required for a new ﬁle.
4
Cloud Storage Systems
Motivated by the need for processing terabytes of data generated by systems, cloud
storage systems need to provide high performance at large scale. In this section, we
examine two cloud computing storage systems.
4.1
Google File System
Around 2000, Google designed and implemented the google ﬁle system (GFS) [7]
to provide support for large, distributed, data-intensive applications. One of the
requirements was to run on cheap commodity hardware and deliver good aggregate
throughput to a large number of clients. GFS is designed for storing a modest
(millions) number of huge ﬁles. It is optimized for large streaming reads and writes.
Though small random reads and writes are supported, it is a non-goal to perform
them efﬁciently.
The GFS architecture comprises of a single GFS master server which stores the
ﬁle metadata of the ﬁle system and multiple slaves known as chunkservers which
store the data. The GFS master stores the metadata, which consists of information
such as ﬁle names, size, directory structure and block locations, in memory. The
chunkservers periodically send heartbeat messages to the master to report their state
and get instructions.
Files are divided into chunks (usually 64 MB in size) and the GFS master
manages the placement and data-layout among the various chunkservers. A large
chunk size reduces the overhead of the client interacting with the master to ﬁnd out
its location. For reliability, each chunk is replicated on multiple (by default three)
chunkservers.

116
A. Verma et al.
Having a single master simpliﬁes the design and enables the master to make
sophisticated chunk placement and replication decisions using global knowledge.
It’s involvement in reads and writes is minimized so that it does not become a
bottleneck. Clients never read and write ﬁle data through the master. Instead, a client
asks the master which chunkservers it should contact. Clients cache this information
(for a limited time) and directly communicate with the chunkservers for subsequent
operations.
GFS supports a relaxed consistency model that is simple and efﬁcient to
implement at scale. File namespace mutations at the master are guaranteed to be
atomic. Record append causes data to be appended atomically at least once even in
the presence of concurrent mutations. Since clients cache chunk locations, they may
read stale data. However, this window is limited by the cache entry’s timeout and the
next open of the ﬁle. Data corruption (like bit rot) is detected through checksumming
by the chunkservers.
4.2
Dynamo
Dynamo [17] is a distributed key-value store used at Amazon and is primarily built
to replace relational databases with a key-value store having eventual consistency.
Dynamo uses a synthesis of well known techniques to achieve the goals
of scalability and availability. It is designed taking churn into account: storage
nodes can be added or removed without requiring any manual partitioning or
redistribution. Data is partitioned and replicated using consistent hashing, and
consistency is provided using versioning. Vector clocks with reconciliation dur-
ing reads are used to provide high availability for writes. Consistency among
replicas during failures is maintained by a quorum-like replica synchronization
protocol. Dynamo uses a gossip based distributed failure detection and membership
protocol.
The basic consistent hashing algorithm assigns a random position for each node
on the ring. This can lead to non-uniform data and load distribution and is oblivious
to heterogeneity. Hence, Dynamo uses “virtual nodes”: a virtual node looks like a
single node in the system, but each node can be responsible for more than one virtual
node. When a new node is added to the system, it is assigned multiple positions in
the ring.
Dynamo provides eventual consistency, which allows for updates to be propagated
to all replicas asynchronously. A write request can return to the client, before the
update has been applied at all the replicas, which can result in scenarios where a
subsequent read operation may return stale data. In the absence of failures, there
is a bound on the update propagation times. However under certain failures (like
network partitions), updates may not propagate to all replicas for a long time.
Dynamo shows that an eventually consistent storage system can be a building block
for highly available applications.

4
Scalable Storage for Data-Intensive Computing
117
5
RFS Design
In this section, we present the design of Ring File System (RFS), a distributed
ﬁle system for large scale data-intensive applications. In RFS, the metadata is
distributed among multiple replicas connected using a distributed hash table (DHT).
This design provides better fault tolerance and scalability while ensuring a high
throughput for metadata operations from multiple clients.
Our architecture consists of three types of nodes: metaservers, chunkservers and
clients as shown in the Fig. 4.1. The metaservers store the metadata of the ﬁle system
whereas the chunkservers store the actual contents of the ﬁle. Every metaserver has
information about the locations of all the other metaservers in the ﬁle system. Thus,
the metaservers are organized in a single hop Distributed Hash Table (DHT). Each
metaserver has an identiﬁer which is obtained by hashing its MAC address.
Chunkservers are grouped into multiple cells and each cell communicates with
a single metaserver. This grouping can be performed in two ways. The chunkserver
can compute a hash of its MAC address and connect to the metaserver that is its
successor in the DHT. This makes the system more self adaptive since the ﬁle system
is symmetric with respect to each metaserver. The alternative is to conﬁgure each
chunkserver to connect to a particular metaserver alone. This gives more control
over the mapping of chunkservers to metaservers and can be useful in conﬁguring
geographically distributed cells each having its own metaserver.
The clients distribute the metadata for the ﬁles and directories over the DHT
by computing a hash of the parent path present in the ﬁle operation. Using the
parent path implies that the metadata for all the ﬁles in a given directory is
present at the same metaserver. This makes listing the contents of a directory
efﬁcient and is commonly used by MapReduce [2] and other cloud computing
applications.
Fig. 4.1 Architecture of
ring FS

118
A. Verma et al.
5.1
Normal Operation
We demonstrate the steps involved in the creation of a ﬁle, when there are no failures
in the system. The sequence of operations shown in Fig. 4.1 are:
1. Client wishes to create a ﬁle named /dir1/dir2/ﬁlename. It computes a hash of
the parent path, /dir1/dir2, to determine that it has to contact metaserver M0 for
this ﬁle operation.
2. Client issues a create request to this metaserver which adds a record to its
metatable and allocates space for the ﬁle in Cell 0.
3. Before returning the response back to the client, M0 sends a replication request
to r of its successors, M1; M2;    ; Mr in the DHT to perform the same operation
on their replica metatable.
4. All of the successor metaservers send replies to M0. Synchronous replication is
necessary to ensure consistency in the event of failures of metaservers.
5. M0 sends back the response to the client.
6. Client then contacts the chunkserver and sends the actual ﬁle contents.
7. Chunkserver stores the ﬁle contents.
Thus, in all r metadata Remote Procedure Calls (RPCs) are needed for a write
operation. If multiple clients try to create a ﬁle or write to the same ﬁle, consistency
is ensured by the fact that these mutable operations are serialized at the primary
metaserver for that ﬁle.
The read operation is similarly performed by using the hash of the parent path
to determine the metaserver to contact. This metaserver directly replies with the
metadata information of the ﬁle and the location of the chunks. The client then
communicates directly with the chunkservers to read the contents of the ﬁle. Thus,
read operations need a single metadata RPC.
5.2
Failure and Recovery
Let us now consider a case now where metaserver M0 has failed as shown in
Fig. 4.2. The chunkservers in cell 0, detect the failure through heartbeat messages
and connect to the next server M1 in the DHT. When a client wishes to create a ﬁle
its connection is now handled by M1 in place of M0. We replicate the metadata to r
successive servers for this request. M1 also allocates space for the ﬁle in cell 0 and
manages the layout and replication for the chunkservers in cell 0.
Once M0 recovers, it sends a request to its neighboring metaservers M1; M2;    ;
Mr to obtain the latest version of the metadata. On receipt of this request, M1
sends the metadata which belongs to M0 and also closes the connection with the
chunkservers in cell 0. The chunkservers now reconnect to M0 which takes over
the layout management for this cell and veriﬁes the ﬁle chunks based on the
latest metadata version obtained. Also, Mr lazily deletes the .r C 1/th copy of

4
Scalable Storage for Data-Intensive Computing
119
Fig. 4.2 Tolerating metaserver failures
the metadata. Thus, our design guarantees strict consistency through the use of
synchronous replication. In order to withstand the failure of a machine or a rack
in a datacenter, a suitable number of replicas can be chosen using techniques from
Carbonite [18].
If instead, Mk, one of the r successors of M0 fails in step 3, then M0 retries
the replication request a ﬁxed number of times. In the mean time, the underlying
DHT stabilization protocol updates the routing table and MkC1 handles the requests
directed to the namespace previously serviced by Mk. If M0 is unable to replicate
the metadata to r successors, then it sends an error message back to the client.
6
Analysis
In this section, we present a mathematical analysis comparing the design of GFS
and RFS with respect to the scalability, throughput followed by failure analysis.
6.1
Design Analysis
Let the total number of machines in the system be n. In GFS, there is exactly 1
metaserver and the remaining n  1 machines are chunkservers that store the actual
data. Since there is only one metaserver, the metadata is not replicated and the ﬁle
system cannot survive the crash of the metaserver.
In RFS, we have m metaservers that distribute the metadata r times. RFS can thus
survive the crash of r  1 metaservers. Although a single Remote Procedure Call
(RPC) is enough for the lookup using a hash of the path, r RPCs are needed for the
creation of the ﬁle, since the metadata has to be replicated to r other servers. Since
m metaservers can handle the read operations for different ﬁles, the read metadata

120
A. Verma et al.
Table 4.1 Analytical comparison of GFS and RFS
Metric
GFS
RFS
Metaserver failures that can be tolerated
0
r  1
RPCs required for a read
1
1
RPCs required for a write
1
r
Metadata throughput for reads
R
R  m
Metadata throughput for writes
W
W  m=r
m number of metaservers, R; W baseline Read and Write through-
puts, r number of times the metadata is replicated
throughput is m times that of GFS. Similarly, the write metadata throughput is m=r
times that of GFS, since it is distributed over m metaservers, but replicated r times.
This analysis is summarized in Table 4.1.
6.2
Failure Analysis
Failures are assumed to be independent. This assumption is reasonable because we
have only tens of metaservers and they are distributed across racks and potentially
different clusters. We ignore the failure of chunkservers in this analysis since it has
the same effect on both the designs and simpliﬁes our analysis. Let f D 1=MTBF
be the probability that the meta server fails in a given time, and let Rg be the time
required to recover it. The ﬁle system is unavailable for Rg  f of the time. If GFS
is deployed with a hot standby master replica, GFS is unavailable for Rg  f 2 of the
time, when both of them fail. For example, if the master server fails once a month
and it takes 6 h for it to recover, then the ﬁle system availability with a single master
is 99:18% and increases to 99:99% with a hot standby.
Let m be the number of metaservers in our system, r be the number of times the
metadata is replicated, f be the probability that a given server fails in a given time t
and Rr be the time required to recover it. Since the recovery time of a metaserver is
proportional to the amount of metadata stored on it and we assume that the metadata
is replicated r times, Rr will be roughly equal to r  Rg=n. The probability that
any r consecutive metaservers in the ring go down is mf r.1  f /mr. If we have
m D 10 metaservers, r D 3 copies of the metadata and M TBF is 30 days, then
this probability is 0:47%. However, a portion of our ﬁle system is unavailable if and
only if all the replicated metaservers go down within the recovery time of each other.
This happens with a probability of Fr D m  f 

f Rr
t
r1
 .1  f /mr, assuming
that the failures are equally distributed over time. The ﬁle system is unavailable for
Fr Rr of the time. Continuing with the example and substituting appropriate values,
we ﬁnd that the recovery time would be 1.8 h and the availability is 99:9994%.

4
Scalable Storage for Data-Intensive Computing
121
7
Experiments
In this section, we present experimental results obtained from our prototype
implementation of RFS. Our implementation is based on KFS and has modiﬁed
data structures for metadata management and the ability for metaservers to recover
from failures by communicating with its replicas. To study the behavior on large
networks of nodes, we also implemented a simulation environment.
All experiments were performed on sixteen 8-core HP DL160 (Intel Xeon
2.66 GHz CPUs) with 16 GB of main memory, running CentOS 5.4. The MapRe-
duce implementation used was Hadoop 0.20.1 and was executed using Sun’s Java
SDK 1.6.0. We compare our results against Hadoop Distributed File System (HDFS)
that accompanied the Hadoop 0.20.1 release and Kosmos File system (KFS) 0.4. For
the HDFS and KFS experiments, a single server is conﬁgured as the metaserver and
the other 15 nodes as chunkservers. RFS is conﬁgured with three metaservers and
ﬁve chunkservers connecting to each of them. We replicate the metadata three times
in our experiments.
7.1
Simulation
Fault tolerance of a design is difﬁcult to measure without a large scale deployment.
Hence, we chose to model the failures that occur in datacenters using a discrete
iterative simulation. Each metaserver is assumed to have a constant and independent
failure probability.
The results show that RFS has better fault tolerance than the single master
(GFS) design. In the case of GFS, if the metaserver fails, the whole ﬁle system is
unavailable and the number of successful lookups is 0 till it recovers after some time.
In RFS, we conﬁgure ten metaservers and each fails independently. The metadata
is replicated on the two successor metaservers. Only a part of the ﬁle system is
unavailable only when three successive metaservers fail. Figure 4.3 shows a plot
of the CDF of the number of successful lookups for GFS and RFS for different
probabilities of failure. As the failure probability increases, the number of successful
lookups decreases. Less than 10% of the lookups fail in RFS in all the cases.
7.2
Fault Tolerance
The second experiment demonstrates the fault tolerance of our implementation.
A client sends 150 metadata operations per second and the number of successful
operations is plotted over time for GFS, KFS and RFS in Fig. 4.4. HDFS achieves
a steady state throughput, but when the metaserver is killed, the complete ﬁle

122
A. Verma et al.
0
20
40
60
80
100
0
20
40
60
80
100
Percentage of requests
Percentage of successful lookups
GFS, f=0.03
GFS, f=0.04
GFS, f=0.05
RFS, f=0.03
RFS, f=0.04
RFS, f=0.05
Fig. 4.3 CDF of number of successful lookups for different failure probabilities
0
100
200
300
400
500
0
50
100
150
200
Successful operations per second
Time (in seconds)
HDFS
KFS
RFS
Fig. 4.4 Fault tolerance of HDFS, KFS and RFS
system become unavailable. Around t D 110s, the metaserver is restarted and it
recovers from its checkpointed state and replays the logs of operations that couldn’t
be checkpointed. The spike during the recovery happens because the metaserver
buffers the requests till it is recovering and batches them together. A similar trend is
observed in the case of KFS, in which we kill the metaserver at t D 70s and restart
it at t D 140s.
For testing the fault tolerance of RFS, we kill one of the three metaservers at t D
20s and it does not lead to any decline in the throughput of successful operations. At

4
Scalable Storage for Data-Intensive Computing
123
0
200
400
600
800
1000
0
200
400
600
800
1000 1200 1400 1600
Successful operations per second
Client operations sent per second
RFS
KFS
HDFS
Fig. 4.5 Comparison of
throughput under different
load conditions
t D 30s, we kill another metaserver, leaving just one metaserver leading to a drop
in the throughput. At t D 60s, we restart the failed metaserver and the throughput
stabilizes to its steady state.
7.3
Throughput
The third experiment demonstrates the metadata throughput performance. A multi-
threaded client is conﬁgured to spawn a new thread and perform read and write
metadata operations at the appropriate frequency to achieve the target qps. We then
measure how many operations complete successfully each second and use this to
compute the server’s capacity. Figure 4.5 shows the load graph comparison for
HDFS, KFS and RFS. The throughput of RFS is roughly twice that of HDFS and
KFS and though the experiment was conducted with three metaservers, the speed
is slightly lesser due to the replication overhead. Also, the performance of HDFS
and KFS are quite similar and RFS with no metadata replication has the same
performance as KFS.
7.4
MapReduce Performance
We ran a simple MapReduce application that counts the number of words on
a Wikipedia dataset and varied the input dataset size from 2 GB to 16 GB. We
measured the time taken for the job to compute on all three ﬁle system and a plot of
the same is shown in Fig. 4.6. We observed that for a smaller dataset the overhead
of replicating the metadata increased the time taken to run the job, but on larger
datasets the running times were almost the same for KFS and RFS.

124
A. Verma et al.
 0
 50
 100
 150
 200
 250
2
4
8
12
Time (in seconds)
Input dataset size (in GB)
HDFS
KFS
RFS
Fig. 4.6 MapReduce application – wordcount
8
Comparison with Related Work
Metadata management has been implemented in ﬁle systems such as NFS and
AFS [13] by statically partitioning the directory hierarchy to different servers. This,
however, requires an administrator to assign directory subtrees to each server but
enables clients to know easily which servers have the metadata for a give ﬁle name.
Techniques of hashing a ﬁle name or the parent directory name to locate a server
have been previously discussed in ﬁle systems such as Vesta [19] and Lustre [20].
Ceph [21], a petabyte scale ﬁle system, uses a dynamic metadata distribution scheme
where subtrees are migrated when the load on a server increases. Hashing schemes
have been found to be inefﬁcient while trying to satisfy POSIX directory access
semantics as this would involve contacting more than one server. However studies
have shown that most cloud computing applications do not require strict POSIX
semantics [7] and with efﬁcient caching of metadata on the clients, the performance
overhead can be overcome.
8.1
Peer-to-Peer File systems
File systems such as PAST [22] and CFS [23] have been built on top of DHTs like
Pastry [16] and Chord [24], but they concentrate on storage management in a peer-
to-peer system with immutable ﬁles. Ivy [25] is a read/write peer-to-peer ﬁle system
which uses logging and DHash. A more exhaustive survey of peer-to-peer storage
techniques for distributed ﬁle systems can be found here [26].

4
Scalable Storage for Data-Intensive Computing
125
Our work differs from these existing ﬁle systems in two aspects: (1) Consistency
of metadata is crucial in a distributed ﬁle system deployed in a datacenter and our
design provides stricter consistency guarantees than these systems through syn-
chronous replication. (2) Existing peer-to-peer ﬁle systems place blocks randomly,
while some can exploit locality. Our system can implement more sophisticated
placement policies (e.g.: placing blocks on the closest and the least loaded server),
since the group of servers which store the metadata have global information about
the ﬁle system.
8.2
Distributed Key-Value Stores
Recently, there have been some efforts in deploying peer-to-peer like systems in
distributed key-value stores used in datacenters. Cassandra [27] is a distributed
key-value store that has been widely used and provides clients with a simple data
model and eventual consistency guarantees. Cassandra provides a fully distributed
design like Dynamo with a column-family based data model like Bigtable [28].
Key-value stores are often useful for low latency access to small objects that can
tolerate eventual consistency guarantees. RingFS on the other hand, tries to address
the problems associated with storing the metadata for large ﬁles in a hierarchical ﬁle
system and provides stronger consistency guarantees.
9
Conclusion
Today’s cloud computing storage systems need to be scalable, elastic and fault
tolerant. We surveyed how storage systems have evolved from traditional distributed
ﬁlesystems (NFS and AFS) and peer-to-peer storage systems (OceanStore and
PAST), and how these ideas have been synthesized in current cloud computing
storage systems (GFS and Dynamo).
We presented and evaluated RFS, a scalable, fault-tolerant and high throughput
ﬁle system that is well suited for large scale data-intensive applications. RFS can
tolerate the failure of multiple metaservers and it can handle a large number of
ﬁles. The number of ﬁles that can be stored in RFS and the throughput of metadata
operations scales linearly with the number of servers. RFS performs better than
HDFS and KFS in terms of fault tolerance, scalability and throughput.
Peer-to-peer systems are decentralized and self-organizing. Thus, they are
attractive for datacenters built with commodity components with high failure rates,
especially as the size of datacenters increases. We have shown how using a single-
hop Distributed Hash Table to manage its metadata from peer-to-peer systems can be
combined together with the traditional client server model for managing the actual
data. We envision that more ideas from peer-to-peer systems research can be applied

126
A. Verma et al.
for building systems that scale to large datacenters with hundreds of thousands of
machines distributed at multiple sites.
Acknowledgements This work was funded in part by NSF IIS grant 0841765 and in part by NSF
CCF grant 0964471. The views expressed are those of the authors only.
References
1. J. Gray, “Distributed computing economics,” Queue, vol. 6, no. 3, pp. 63–68, 2008.
2. J. Dean and S. Ghemawat, “MapReduce: simpliﬁed data processing on large clusters,”
Commun. ACM, vol. 51, no. 1, pp. 107–113, 2008.
3. M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly, “Dryad: Distributed data-parallel
programs from sequential building blocks,” in EuroSys ’07: Proc. of the 2nd ACM SIGOPS,
New York, NY, USA, 2007, pp. 59–72.
4. J. Dean, “Large-Scale Distributed Systems at Google: Current Systems and Future Directions,”
2009.
5. J. Gantz and D. Reinsel, “As the economy contracts, the Digital Universe expands,” IDC
Multimedia White Paper, 2009.
6. M. Armbrust, A. Fox, R. Grifﬁth, A. D. Joseph, R. H. Katz, A. Konwinski, G. Lee,
D. A. Patterson, A. Rabkin, I. Stoica, and M. Zaharia, “Above the Clouds: A Berkeley View of
Cloud Computing,” EECS Department, University of California, Berkeley, Tech. Rep., 2009.
7. S. Ghemawat, H. Gobioff, and S.-T. Leung, “The Google ﬁle system,” SIGOPS Oper. Syst.
Rev., vol. 37, no. 5, pp. 29–43, 2003.
8. M. K. McKusick and S. Quinlan, “GFS: Evolution on Fast-forward,” Queue, vol. 7, no. 7,
pp. 10–20, 2009.
9. D. Roselli, J. Lorch, and T. Anderson, “A comparison of ﬁle system workloads,” in Proceedings
of the annual conference on USENIX Annual Technical Conference.
USENIX Association,
2000.
10. J. Kubiatowicz, D. Bindel, Y. Chen, S. Czerwinski, P. Eaton, D. Geels, R. Gummadi, S. Rhea,
H. Weatherspoon, W. Weimer, C. Wells, and B. Zhao, “Oceanstore: An architecture for global-
scale persistent storage,” in Proc. of the 9th International Conference on Architectural Support
for Programming Languages and Operating Systems, 2000.
11. J. Ledlie, J. Shneidman, M. Seltzer, and J. Huth, “Scooped, again,” Lecture notes in computer
science, pp. 129–138, 2003.
12. R. Sandberg, D. Goldberg, S. Kleiman, D. Walsh, and B. Lyon, “Design and implementation of
the sun network ﬁlesystem,” in Proceedings of the Summer 1985 USENIX Conference, 1985,
pp. 119–130.
13. J. Howard, M. Kazar, S. Menees, D. Nichols, M. Satyanarayanan, R. Sidebotham, and M. West,
“Scale and performance in a distributed ﬁle system,” ACM Transactions on Computer Systems
(TOCS), vol. 6, no. 1, pp. 51–81, 1988.
14. D. Eastlake and P. Jones, “US secure hash algorithm 1 (SHA1),” RFC 3174, September, Tech.
Rep., 2001.
15. B. Zhao, L. Huang, J. Stribling, S. Rhea, A. Joseph, and J. Kubiatowicz, “Tapestry: a resilient
global-scale overlay for service deployment,” in IEEE J. Selected Areas in Communications,
January 2003.
16. A. Rowstron and P. Druschel, “Pastry: scalable, decentralized object location and routing for
large-scale peer-to-peer systems,” in ACM Middleware, November 2001.
17. G. DeCandia, D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, A. Pilchin, S. Sivasub-
ramanian, P. Vosshall, and W. Vogels, “Dynamo: Amazon’s highly available key-value store,”
ACM SIGOPS Operating Systems Review, vol. 41, no. 6, p. 220, 2007.

4
Scalable Storage for Data-Intensive Computing
127
18. B. Chun, F. Dabek, A. Haeberlen, E. Sit, H. Weatherspoon, M. Kaashoek, J. Kubiatowicz, and
R. Morris, “Efﬁcient replica maintenance for distributed storage systems,” in Proc. of NSDI,
vol. 6, 2006.
19. P. Corbett and D. Feitelson, “The Vesta parallel ﬁle system,” ACM Transactions on Computer
Systems (TOCS), vol. 14, no. 3, pp. 225–264, 1996.
20. P. Schwan, “Lustre: Building a ﬁle system for 1000-node clusters,” in Proceedings of the 2003
Linux Symposium, 2003.
21. S. Weil, S. Brandt, E. Miller, D. Long, and C. Maltzahn, “Ceph: A scalable, high-performance
distributed ﬁle system,” in Proceedings of the 7th Symposium on Operating Systems Design
and Implementation (OSDI), 2006.
22. P. Druschel and A. Rowstron, “PAST: A large-scale, persistent peer-to-peer storage utility,” in
Proc. HotOS VIII, 2001, pp. 75–80.
23. F. Dabek, M. Kaashoek, D. Karger, R. Morris, and I. Stoica, “Wide-area cooperative storage
with CFS,” ACM SIGOPS Operating Systems Review, vol. 35, no. 5, pp. 202–215, 2001.
24. I. Stoica, R. Morris, D. Karger, M. Kaashoek, and H. Balakrishnan, “Chord: a scalable peer-
to-peer lookup service for Internet applications,” in ACM SIGCOMM, August 2001.
25. T. M. G. Athicha Muthitacharoen, Robert Morris and B. Chen, “Ivy: A Read/Write Peer-to-
Peer File System,” in OSDI, December 2002.
26. R. Hasan, Z. Anwar, W. Yurcik, L. Brumbaugh, and R. Campbell, “A survey of peer-to-peer
storage techniques for distributed ﬁle systems,” in ITCC, vol. 5, pp. 205–213.
27. A. Lakshman and P. Malik, “Cassandra: structured storage system on a P2P network,” in Proc.
of the 28th ACM symposium on Principles of distributed computing, 2009.
28. F. Chang, J. Dean, S. Ghemawat, W. Hsieh, D. Wallach, M. Burrows, T. Chandra, A. Fikes, and
R. Gruber, “Bigtable: A distributed storage system for structured data,” in Proceedings of the
7th USENIX Symposium on Operating Systems Design and Implementation (OSDI06), 2006.


Chapter 5
Computation and Storage Trade-Off
for Cost-Effectively Storing Scientiﬁc
Datasets in the Cloud
Dong Yuan, Yun Yang, Xiao Liu, and Jinjun Chen
1
Introduction
Scientiﬁc applications are usually data intensive [1,2], where the generated datasets
are often terabytes or even petabytes in size. As reported by Szalay and Gray in
[3], science is in an exponential world and the amount of scientiﬁc data will double
every year over the next decade and future. Producing scientiﬁc datasets involves
large number of computation intensive tasks, e.g., with scientiﬁc workﬂows [4],
hence taking a long time for execution. These generated datasets contain important
intermediate or ﬁnal results of the computation, and need to be stored as valuable
resources. This is because: (1) data can be reused – scientists may need to re-analyze
the results or apply new analyses on the existing datasets [5]; (2) data can be shared –
for collaboration, the computation results may be shared, hence the datasets are used
by scientists from different institutions [6]. Storing valuable generated application
datasets can save their regeneration cost when they are reused, not to mention
the waiting time caused by regeneration. However, the large size of the scientiﬁc
datasets is a big challenge for their storage.
D. Yuan () • Y. Yang • X. Liu
Faculty of Information and Communication Technologies, Swinburne University of Technology,
Melbourne, Australia
e-mail: dyuan@swin.edu.au; yyang@swin.edu.au; xliu@swin.edu.au
J. Chen
Faculty of Engineering and Information Technology, University of Technology,
Sydney, Australia
e-mail: Jinjin.Chen@uts.edu.au
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 5, © Springer Science+Business Media, LLC 2011
129

130
D. Yuan et al.
In recent years, cloud computing is emerging as a latest distributed computing
paradigm which provides redundant, inexpensive and scalable resources on demand
to system requirements [7]. Meanwhile, cloud computing adopts a pay-as-you-go
model where users are charged according to the usage of cloud services such as
computing, storage and network services like conventional utilities in everyday life
(e.g., water, electricity, gas and telephony) [8].
Cloud computing systems offer a new way for deploying large-scale data and
computation intensive applications. As IaaS (Infrastructure as a Service) is a very
popular way to deliver computing resources in the cloud [9], the heterogeneity
of computing systems [10] of one service provider can be well shielded by
virtualization technology. Hence, users can deploy their applications in uniﬁed
resources without any infrastructure investment, where excessive processing power
and storage can be obtained from commercial cloud service providers. With the
pay-as-you-go model, the total application cost in the cloud highly depends on the
strategy of storing the application datasets, e.g., storing all the generated application
datasets in the cloud may result in a high storage cost since some datasets may be
seldom used but large in size; in contrast, if we delete all the generated datasets and
regenerate them every time when needed, the computation cost may be very high
too. A good strategy is to ﬁnd a balance to selectively store some popular datasets
and regenerate the rest when needed [11–14]. However, sometimes users may
have certain preferences on storing some particular datasets due to various reasons
rather than cost, e.g., guaranteeing immediate access to certain datasets. Hence,
users’ preferences should also be considered in a storage strategy. Furthermore,
because of the scalability and the dynamic provisioning mechanism of the cloud
computing system, the application cost in the cloud would change from time to time
whenever new datasets are generated or the datasets’ usage frequencies are changed.
The cloud service provider should be able to provide benchmarking services to
users, who wish to know the minimum cost of running their applications in
the cloud.
Datasets in scientiﬁc applications often have dependencies, i.e., computation task
can operate on one or more datasets and generate new one(s). Hence we create
a Data Dependency Graph (DDG) [15] based on data provenance, which records
the generation relationship of all the datasets. Based on DDG, we know how to
regenerate datasets in the cloud system and can further calculate their generation
costs. In this chapter, by comparing the generation costs and storage costs of the
datasets, we develop cost-effective strategies for storing scientiﬁc datasets in the
cloud with the pay-as-you-go model. We also design novel algorithms to ﬁnd
the best trade-off of computation and storage, based on which we propose an
approach for minimum cost benchmarking in the cloud.
The remainder of this chapter is organized as follows. Section 2 discusses
the related work. Section 3 gives a motivating example of scientiﬁc workﬂow
and analyses the research problems. Section 4 introduces some important con-
cepts about the DDG and the datasets storage cost model in cloud comput-
ing. Section 5 presents our cost-effective datasets storage strategies in detail.
Section 6 presents our minimum cost benchmarking approach for evaluating the cost

5
Computation and Storage Trade-Off...
131
effectiveness of the storage strategies. Section 7 demonstrates some experimental
results and the evaluation. Section 8 summarizes the entire chapter and points out the
future work.
2
Related Work
Today, research on deploying applications in the cloud becomes popular [16–19].
Cloud computing system for scientiﬁc applications, i.e., science cloud, has already
commenced [20–22]. Comparing to the traditional distributed computing systems
like cluster and grid, a cloud computing system has a cost beneﬁt [23]. Assunc¸˜ao
et al. [24] demonstrate that cloud computing can extend the capacity of clusters with
a cost beneﬁt. With Amazon clouds’ cost model and BOINC volunteer computing
middleware, the work in [25] analyzes the cost beneﬁt of cloud computing versus
grid computing. The work by Deelman et al. [1] also applies Amazon clouds’ cost
model and demonstrates that cloud computing offers a cost-effective way to deploy
scientiﬁc applications. Especially, Cho and Gupta [26] further propose planning
algorithms of how to transfer large bulks of scientiﬁc data to commercial clouds
in order to run the applications. The above works mainly focus on the comparison
of cloud computing systems and the traditional distributed computing paradigms,
which shows that applications running in the cloud have cost beneﬁts, but they do
not touch the issue of computation and storage trade-off in the cloud.
Nectar system [27] is designed for automatic management of data and computa-
tion in data centers, where obsolete datasets are deleted in order to improve resource
utilization. In [1], Deelman et al. present that storing some popular intermediate data
can save the cost in comparison to always regenerating them from the input data. In
[11], Adams et al. propose a model to represent the trade-off of computation cost
and storage cost, but have not given the strategy to ﬁnd this trade-off. In this chapter,
we investigate how to ﬁnd the computation and storage trade-off in the cloud. Based
on the trade-off, we propose cost-effective strategies for storing scientiﬁc datasets
as well as the approach for minimum cost benchmarking in the cloud.
The research works on data provenance are important foundation for our work.
Due to the importance of data provenance in scientiﬁc applications, many works
about recording data provenance of the system have been done [28, 29]. Recently,
research on data provenance in cloud computing systems has also appeared [30].
More speciﬁcally, Osterweil et al. [31] present how to generate a data derivation
graph for the execution of a scientiﬁc workﬂow, where one graph records the
data provenance of one execution, and Foster et al. [32] propose the concept of
Virtual Data in the Chimera system, which enables the automatic regeneration
of datasets when needed. Our DDG is based on data provenance in scientiﬁc
applications, which depicts the dependency relationships of all the datasets in the
system. With DDG, we know where the datasets are derived from and how to
regenerate them.

132
D. Yuan et al.
3
Motivation and Research Problems
3.1
Motivating Example
Swinburne Astrophysics group has been conducting pulsar searching surveys using
the observation data from Parkes Radio Telescope, which is one of the most famous
radio telescopes in the world1. Pulsar searching is a typical scientiﬁc application.
It contains complex and time consuming tasks and needs to process terabytes
of data. Figure 5.1 depicts the high level structure of the pulsar searching work-
ﬂow, which is currently running on Swinburne high performance supercomputing
facility2.
At the beginning, raw signal data from Parkes Radio Telescope are recorded at a
rate of 1 GB/s by the ATNF3 Parkes Swinburne Recorder4. Depending on different
areas in the universe that the scientists want to conduct the pulsar searching survey,
different sizes of beam ﬁles (1–20GB) are extracted from the raw data ﬁles and
compressed for initial preparation. The beam ﬁles contain the pulsar signals which
are dispersed by the interstellar medium. The De-dispersion step is to counteract
this effect. Since the potential dispersion source is unknown, a large number of de-
dispersion ﬁles needs to be generated with different dispersion trials. In the current
pulsar searching survey, 1,200 is the minimum number of the dispersion trials.
For the 20 GB input beam ﬁle, this De-dispersion step takes about 13 h to ﬁnish
and generate up to 90 GB of de-dispersion ﬁles. The Accelerate step is for binary
pulsar searching, which generates accelerated de-dispersion ﬁles with the similar
size of the original de-dispersion ﬁles. Based on these generated de-dispersion ﬁles,
different seeking algorithms can be applied to search pulsar candidates, such as
FFT Seeking, FFA Seeking, and Single Pulse Seeking. A candidate list of pulsars
is generated after the Seeking step which is saved in a text ﬁle. Furthermore, by
comparing the candidates generated from different beam ﬁles in a same time session,
Candidates
Candidates
Beam
Beam
De-disperse
Acceleate
Record
Raw
Data
Extract
Beam
Pulse
Seek
FFT
Seek
FFA
Seek
Get
Candidates
Elimanate
candidates
Fold to
XML
Extract
Beam
Get
Candidates
…
…...
…...
…...
Make
decision
Trial Measure 1
Trial Measure 1200
Trial Measure 2
…...
Compress
Beam
…...
…...
Fig. 5.1 Pulsar searching workﬂow
1http://www.parkes.atnf.csiro.au/
2http://astronomy.swin.edu.au/supercomputing/
3http://www.atnf.csiro.au/
4http://astronomy.swin.edu.au/pulsar/?topic=apsr

5
Computation and Storage Trade-Off...
133
some interference may be detected and some candidates may be eliminated. With
the ﬁnal pulsar candidates, we need to go back to the de-dispersion ﬁles to ﬁnd
their feature signals and fold them to XML ﬁles. At last, the XML ﬁles will be
visually displayed to users for making decisions on whether a pulsar has been found
or not.
At present, all the generated datasets are deleted after having been used, and
the scientists only store the raw beam data, which are extracted from the raw
telescope data. Whenever there are needs of using the deleted datasets, the scientists
will regenerate them based on the raw beam ﬁles. The generated datasets are not
stored, mainly because the supercomputer is a shared facility that cannot offer
unlimited storage capacity to hold the accumulated terabytes of data. However, some
datasets are better to be stored. For example, the de-dispersion ﬁles are frequently
used. Based on them, the scientists can apply different seeking algorithms to ﬁnd
potential pulsar candidates. For the large input beam ﬁles, the regeneration of the
de-dispersion ﬁles will take more than 10 h. It not only delays the scientists from
conducting their experiments, but also requires a lot of computation resources. On
the other hand, some datasets need not be stored. For example, the accelerated de-
dispersion ﬁles, which are generated by the Accelerate step, are not often used. The
Accelerate step is an optional step that is only for the binary pulsar searching. In
light of this and given the large size of these datasets, they are not worth storing
as it would be more cost effective to regenerate them from the de-dispersion ﬁles
whenever used.
3.2
Problem Analysis and Research Issues
Traditionally, scientiﬁc applications are deployed on the high performance comput-
ing facilities, such as clusters and grids. Scientiﬁc applications are often complex
with huge datasets generated during their execution. How to store these datasets
is normally decided by the scientists who use the scientiﬁc applications. This is
because the clusters and grids only serve for certain institutions. The scientists may
store the datasets that are most valuable to them, based on the storage capacity
of the system. However, for many scientiﬁc applications, the storage capacities are
limited, such as the pulsar searching workﬂow introduced above. The scientists have
to delete all the generated datasets because of the storage limitation.
The storage limitation should not be the case in the cloud, because the com-
mercial cloud service providers can offer virtually unlimited storage resources.
However, due to the pay-as-you-go model, users are responsible for the cost of
both storing and regenerating datasets in the cloud. It makes cost-effectiveness an
extremely important factor for the data storage strategies. Hence, we need to ﬁnd the
trade-off of computation and storage in the cloud. Furthermore, data accessing delay
should also be considered for datasets storage. Users have different preferences of
storing the datasets, e.g., some users may want to store some datasets with higher
storage cost to guarantee the immediate availability; some users may have tolerance

134
D. Yuan et al.
of computation delay with a certain time. Hence, the best trade-off of computation
cost and storage cost may not be the best strategy for datasets storage. Based on the
analysis, there are two research issues that we need to investigate:
1. Cost-effective datasets storage strategies.
We need to develop runtime storage strategies in the cloud that can store not only
the existing datasets, but also the newly generated datasets. The storage strategies
should also be able to reﬂect the preferences from users about the computation
delay in the cloud.
2. Minimum cost benchmarking.
We need to design an algorithm to ﬁnd the best trade-off of computation and
storage in the cloud, which forms the minimum cost storage strategy. The
minimum cost is the benchmark for evaluating the cost-effectiveness of all other
datasets storage strategies.
4
Concepts and Cost Model of Datasets Storage in the Cloud
In this section, we introduce some important concepts and represent the datasets
storage cost model of scientiﬁc applications in the cloud.
4.1
Classiﬁcation of Scientiﬁc Application Data in the Cloud
In general, there are two types of data stored in the cloud storage, original data and
generated data:
1. Original data are the data uploaded by users, and in scientiﬁc applications they
are usually the raw data collected from the devices in the experiments. For these
data, the users need to decide whether they should be stored or deleted, since
they cannot be regenerated by the system once deleted.
2. Generated data are the data produced in the cloud computing system while the
applications run. They are the intermediate or ﬁnal computation results of the
application which can be used in the future. For these data, their storage can be
decided by the system, since they can be regenerated.
For the original data, the users decide whether they should be stored or deleted,
since they cannot be regenerated once deleted. Hence, our datasets storage strategy
is only applied to the generated data in the cloud computing system that can auto-
matically decide the storage status of generated datasets in scientiﬁc applications.
In this chapter, we refer generated data as dataset(s).

5
Computation and Storage Trade-Off...
135
d1
d2
d3
d4
d5
d6
d7
d8
Fig. 5.2 A simple data dependency graph (DDG)
4.2
Data Provenance and Data Dependency Graph (DDG)
Scientiﬁc applications have many computation and data intensive tasks that generate
many datasets of considerable size. There exist dependencies among these datasets.
Data provenance is a kind of important metadata in which the dependencies between
datasets are recorded [33]. The dependency depicts the derivation relationship be-
tween datasets. For scientiﬁc applications, data provenance is especially important
because after the execution, some datasets may be deleted, but sometimes the
scientists have to regenerate them for either reuse or reanalysis [5]. Data provenance
records the information of how the datasets were generated, which is very important
for the scientists. Furthermore, regeneration of the datasets from the input data may
be very time consuming, and therefore carry a high cost. On the contrary, with data
provenance information, the regeneration of the demanding dataset may start from
some stored datasets. In the cloud, data provenance is recorded along the execution
of the applications. Taking the advantage of data provenance, we can build DDG.
All the datasets once generated (or modiﬁed) in the cloud, whether stored or deleted,
their references are recorded in the DDG as different nodes.
In DDG, every node denotes a dataset. Figure 5.2 shows a simple DDG, where
every node in the graph denotes a dataset. Dataset d1 pointing to d2 means that d1
is used to generate d2; and d2 pointing to d3 and d5 means d2 is used to generate d3
and d5 based on different operations; dataset d4 and d6 pointing to d7 means d4 and
d6 are used together to generate d7.
DDG is a directed acyclic graph (DAG). This is because DDG records the
provenances of how datasets are derived in the system as time goes on. In another
word, it depicts the generation relationships of datasets. When some of the deleted
datasets need to be reused, we do not need to regenerate them from the original input
data. With DDG, the system can ﬁnd the predecessors of the demanding dataset, so
they can be regenerated from their .
We denote a dataset di in DDG as di 2 DDG, and a set of datasets S D
fd1; d2: : :dhg in DDG as S  DDG. To better describe the relationships of datasets
in DDG, we deﬁne two symbols ! and ½:
•
! denotes that two datasets have a generation relationship, where di ! dj
means di is a predecessor dataset of dj in the DDG. For example, in the DDG in
Fig. 5.2, we have d1 ! d2; d1 ! d4; d5 ! d7; d1 ! d7, etc. Furthermore, !
is transitive, where di ! dj ! dk , di ! dj ^ dj ! dk ) di ! dk.

136
D. Yuan et al.
•
½ denotes that two datasets do not have a generation relationship, where di ½
dj means di and dj are in different branches in DDG. For example, in the DDG
in Fig. 5.2, we have d3 ½ d5; d3 ½ d6, etc. Furthermore, ½ is commutative,
where di ½ dj , dj ½ di.
4.3
Datasets Storage Cost Model in the Cloud
In a commercial cloud computing environment, if the users want to deploy and run
applications, they need to pay for the resources used. The resources are offered by
cloud service providers, who have their cost models to charge the users. In general,
there are two basic types of resources in the cloud: storage and computation5.
Popular cloud services providers’ cost models are based on these types of resources
[9]. For example, Amazon cloud services’ prices are as follows6:
•
$0.15 per Gigabyte per month for the storage resources;
•
$0.1 per CPU instance hour for the computation resources7;
In this chapter, we deﬁne our datasets storage cost model in cloud computing system
as follows:
Cost D C C S;
where the total cost of the system, Cost, is the sum of C, which is the total cost of
computation resources used to regenerate datasets, and S, which is the total cost of
storage resources used to store the datasets.
To utilize the cost model, we deﬁne some important attributes for the datasets in
DDG. For dataset di, its attributes are denoted as: < xi, yi, f i, vi, provSeti, CostRi >,
where
•
xi denotes the generation cost of dataset di from its direct predecessors. To
calculate this generation cost, we have to multiply the time of generating dataset
di by the price of computation resources. Normally the generation time can be
obtained from the system logs.
5Bandwidth is another common kind of resource in the cloud. In [1], the authors state that the
cost-effective way of doing science in the cloud is to upload all the application data to the cloud
storage and run all the applications with the cloud services. So we assume that the scientists upload
all the original data to the cloud to conduct their experiments. Because transferring data within one
cloud service provider’s facilities is usually free, the data transfer cost of managing the application
datasets is not counted. In [15], the authors discussed the scenario of running scientiﬁc applications
among different cloud service providers.
6The prices may ﬂuctuate from time to time according to market factors.
7Amazon cloud service offers different CPU instances with different prices, where using expensive
CPU instances with higher performance would reduce computation time. There exists a trade-off
of time and cost [34], which is different with the trade-off of computation and storage, hence is out
of this chapter’s scope.

5
Computation and Storage Trade-Off...
137
…...
…...
…...
…...
…...
…...
…...
dk
…...
Stored dataset
Deleted dataset
…...
…...
provSetj
provSeti
provSetk
…...
…...
…...
di
dj
Fig. 5.3 Datasets’ provSets in a general DDG
•
yi denotes the cost of storing dataset di in the system per time unit. This storage
cost can be calculated by multiplying the size of dataset di and the price of
storage resources.
•
f i is a ﬂag, which denotes the status whether this dataset is stored or deleted in
the system.
•
vi denotes the usage frequency, which indicates how often di is used. In cloud
computing systems, datasets are shared by many users; hence vi should be an
estimated value from di’s usage history recorded in the system logs.
•
provSeti denotes the set of stored provenances that are needed when regenerating
dataset di, in another word, it is the set of references of stored predecessor
datasets that are adjacent to di in the DDG. If we want to regenerate di, we
have to ﬁnd its direct predecessors, which may also be deleted, so we have to
further ﬁnd the stored predecessors of datasets di. provSeti is the set of the nearest
stored predecessors of di in the DDG. Figure 5.3 shows the provSet of a dataset
in different situations.
Formally, we can describe a dataset di’s ProvSeti as follows:
provSeti D fdjj8dj 2 DDG ^ fj D “stored00 ^ dj ! di
^..:9dk 2 DDG ^ dj ! dk ! di/
_.9dk 2 DDG ^ dj ! dk ! di ^ fk D “deleted00//g
provSet is a very important attribute of a dataset in calculating its generation
cost. When we want to regenerate a dataset in DDG, we have to start the
computation from the dataset in its provSet. Hence, for dataset di, its generation
cost is:
genCost.di/ D xi C
X
fdkjdj 2 provSeti ^dj !dk!dig xk
This cost is a total cost of (1) the generation cost of dataset di from its direct
predecessor datasets and (2) the generation costs of di’s deleted predecessors that
need to be regenerated.
•
CostRi is di’s cost rate, which means the average cost per time unit of the dataset
di in the system. If di is a stored dataset, then CostRi D yi. If di is a deleted
dataset in the system, when we need to use di, we have to regenerate it. So we

138
D. Yuan et al.
multiply the generation cost of di by the frequency of its usages and use this
value as the cost rate of di in the system. CostRi D genCost.di/vi. The storage
statuses of the datasets have strong impact on their cost rates.
CostRi D
 yi;
fi D stored
genCost.di/vi;
fi D deleted
Hence, the total cost rate of storing a DDG, is the sum of CostR of all the datasets
in it, which is P
di 2 DDG CostRi. Given a time duration, the total cost of storing
a DDG is the integral of the cost rate in this duration as a function of time t,
which is
Total Cost D
Z
t
X
di 2DDG CostRi

 dt
We further deﬁne the storage strategy of a DDG as S, where S  DDG, which
means storing the datasets in S in the cloud and deleting the rest. We denote the
cost rate of storing a DDG with the storage strategy S as
X
di 2DDG CostRi

S
Based on the deﬁnition above, different datasets storage strategies will lead to
different cost rates to the system. Our work aims at reducing this cost rate.
5
Cost-Effective Datasets Storage Strategies
In a commercial cloud computing environment, theoretically, the system can offer
unlimited storage resources. All the datasets generated by the applications can
be stored, if the users are willing to pay for the required resources. Hence, for
applications in the cloud, whether to store or delete the datasets is not an easy
decision anymore. The datasets vary in size, and have different generation costs
and usage frequencies. On one hand, it is most likely not cost effective to store all
these datasets in the cloud. On the other hand, if we delete them all, regeneration
of frequently used datasets would normally impose a high computation cost.
Meanwhile, the storage strategy should also consider the users’ tolerance of data
accessing delay. Based on the factors above, we present two storage strategies
developed in this section.
5.1
Cost Rate Based Storage Strategy
In this strategy, for every dataset in the DDG, we compare its generation cost
rate and storage cost rate to decide its storage status [12]. When new datasets are

5
Computation and Storage Trade-Off...
139
generated or the datasets’ usage frequencies are changed, the strategy dynamically
checks the cost rates of datasets and adjusts their storage status accordingly.
Furthermore, we introduce a parameter  to reﬂect users’ cost related tolerance of
data accessing delay, which is a value between 0 and 1 [13]. Sometimes, users prefer
storing the datasets in the cloud to regenerating them even with a higher storage
cost because of the accessing delay. To reﬂect this preference of users, the storage
cost rate of the datasets will be multiplied by this parameter . The value of  is
set by the system manager based on users’ preference. The two extreme situations:
i D 0 indicates that users have no tolerance of data accessing delay, which means
no matter how large the dataset’s storage cost is, it has to be stored; i D 1 indicates
that users are fully tolerant of data accessing delay, which means the storage status
of the dataset only depends on its generation cost and storage cost in order to reduce
the total system cost.
The cost rate based strategy is presented in [12] and [13] in detail, we brieﬂy
describe it in this subsection as follows:
1. If di is a newly generated dataset in the cloud, ﬁrst we add its information to the
DDG. We ﬁnd the provenance datasets of di in the DDG, and add edges pointing
to di from these datasets. Then we initialize its attributes. As di does not have
a usage history yet, we use the average value in the system as the initial value
of di’s usage frequency. Next, we check if di needs to be stored or not. As di is
newly added in the DDG, it does not have successors, which means no datasets
are derived from di at this moment. For deciding whether to store or delete di,
we only compare the generation cost rate of di itself and its storage cost rate
multiplied by the delay tolerance parameter , which are genCost.di/  vi and
yi  i. If the cost of generation is larger than the cost of storing it, we save di
and set CostRi D yi, otherwise we delete di and set CostRi D genCost.di/  vi.
2. If di is a stored dataset in the cloud. In this situation, we set a threshold time ti,
where ti D genCost.di//yi. This threshold time indicates how long this dataset
can be stored in the system with the cost of generating it. If di has not been
used for the time of ti, we will check whether it should be stored anymore.
We compare di’s storage cost rate and generation cost rate to decide whether
di should be stored or not.
3. If di is a deleted dataset in the cloud. Whenever di is reused, we check di’s cost
rate to decide whether it should be stored or not. If we store di, the change of
di’s storage status will impact the cost rate of di’s predecessors and successors.
Hence we also need to adjust the storage status of di’s stored predecessors and
successors according to their cost rates.
By utilizing this strategy, we can guarantee that all the stored datasets in the cloud
are necessary, which means deleting any datasets in the cloud will bring a cost
increase. This strategy is highly efﬁcient and scalable, because when deciding the
storage status of a dataset, we only consider the cost rate of the dataset itself. More
detailed information of this strategy can be found in [12] and [13].

140
D. Yuan et al.
5.2
Local-Optimization Based Storage Strategy
In order to further improve the cost-effectiveness of the cost rate based storage
strategy, we design a Cost Transitive Tournament Shortest Path (CTT-SP) based
algorithm that can ﬁnd the minimum cost storage strategy for a linear DDG. Based
on this algorithm, we introduce a local-optimization based storage strategy in this
subsection [14].
5.2.1
CTT-SP Algorithm for Linear DDG
Linear DDG means a DDG with no branches, where all the datasets in the DDG
only have one predecessor and one successor except the ﬁrst and last datasets. The
basic idea of CTT-SP algorithm is to construct a Cost Transitive Tournament (CTT)
based on the DDG. In a CTT, the paths from the start dataset to the end dataset have
a one-to-one mapping to the storage strategies, and the length of the path equals to
the total cost rate. Then we can use the well known Dijkstra algorithm to ﬁnd the
shortest path, which is the minimum cost storage strategy.
Given a linear DDG, which has datasets fd1, d2 ...dng. The CTT-SP algorithm
has the following four steps:
Step 1: We add two virtual datasets in the DDG, ds before d1 and de after dn, as
the start and end datasets, and set xs D ys D 0 and xe D ye D 0.
Step 2: We add new directed edges in the DDG to construct the transitive
tournament. For every dataset in the DDG, we add edges that start from it and point
to all its successors. Formally, for dataset di, it has out-edges to all the datasets
in the set of
˚
dj
ˇˇ8dj 2 DDG ^ di ! dj

, and in-edges from all the datasets in
the set of fdk j8dk 2 DDG ^ dk ! di g. Hence, for any two datasets di and dj in
the DDG, we have an edge between them, denoted as e < di; dj >. Formally,
8di; dj 2 DDG ^ di ! dj ) 9e < di; dj >.
Step 3: We set weights to the edges. The reason we call the graph Cost Transitive
Tournament is because the weights of its edges are composed of the cost rates of
datasets. For an edge e < di; dj >, we denote its weight as ! < di; dj >, which
is deﬁned as the sum of cost rates of dj and the datasets between di and dj,
supposing that only di and dj are stored and rest of the datasets between di and
dj are all deleted. Formally, ! < di; dj >
D yj C P
fdkj8dk2 DDG^di!dk!dj g
.genCost.dk/  vk/. Since we are discussing the linear DDG, for the datasets
between di and dj, di is the only dataset in their provSets. Hence we can further get:
! < di; dj > D yj C
X
fdkj8dk2DDG^di!dk!dj g

xk C
X
fdhj8dh2DDG^di !dh!dk g xh

 vk


5
Computation and Storage Trade-Off...
141
Fig. 5.4 An example of constructing CTT
In Fig. 5.4, we demonstrate a simple example of constructing the CTT for a DDG
that only have three datasets, where ds is the start dataset that only has out-edges
and de is the end dataset that only has in-edges.
Step 4: We ﬁnd the shortest path of CTT. From the construction steps, we can
clearly see that the CTT is an acyclic complete oriented graph. Hence we can use
the Dijkstra algorithm to ﬁnd the shortest path from ds to de. The Dijkstra algorithm
is a classic greedy algorithm to ﬁnd the shortest path in graph theory. We denote the
shortest path from ds to de as Pmin.
Based on the steps above, we can clearly see that given a linear DDG with
datasets fd1, d2 ... dng, the length of Pmin of its CTT is the minimum cost rate of
the system to store the datasets in the DDG, and the corresponding storage strategy
is to store the datasets that Pmin traverses.
5.2.2
Local-Optimization Based Storage Strategy with Improved
CTT-SP Algorithm
The storage strategy needs to reﬂect users’ tolerance of data accessing delay, so
that we improve the CTT-SP algorithm by introducing a new attributes Ti for every
datasetdi in the DDG. Ti is the minimum duration of delay that users can tolerate
when accessing dataset di. In the improved linear CTT-SP algorithm, the edge
e < di; dj > has to further satisfy the condition
8dk 2 DDG ^ .di ! dk ! dj/ ^
genCost.dk/
CostCPU
< Tk

;
where CostCPU is the price of CPU instances in the cloud. With this condition,
many cost edges are eliminated from the CTT. It guarantees that in all storage
strategies of the DDG found by the algorithm, for every deleted dataset di, its
regeneration time is smaller than Ti. The pseudo code of this algorithm is shown
in Fig. 5.5.
Based on the improved CTT-SP algorithm, we can develop the local-optimization
based datasets storage strategy. The basic idea is to partition the general DDG

142
D. Yuan et al.
Fig. 5.5 Pseudo-code of CTT-SP algorithm
into small linear segments on which we utilize the CTT-SP algorithm to achieve
a localized optimum. The strategy contains the following four rules:
1. Given a general DDG, the datasets to be stored ﬁrst are the ones that users have
no tolerance of accessing delay on them. This is to guarantee the immediate
availability when these datasets are needed.
2. Then, the DDG is partitioned into separate sub DDGs by the stored datasets.
For every sub DDG, if it is a linear one, we use the CTT-SP algorithm to ﬁnd
its storage strategy; otherwise, we ﬁnd the datasets that have multiple direct
predecessors or successors, and use these datasets as the partitioning points to
divide it into sub linear DDGs, as shown in Fig. 5.6. Then we use the improved
linear CTT-SP algorithm to ﬁnd their storage strategies. This is the essence of
local optimization.
3. When new datasets are generated in the system, they will be treated as a new
sub DDG and added to the old DDG. Correspondingly, its storage status will be
calculated in the same way as the old DDG.

5
Computation and Storage Trade-Off...
143
...
...
...
...
Linear DDG1
Linear DDG2
Linear DDG3
Linear DDG4
Partitioning
point dataset
Partitioning
point dataset
Fig. 5.6 Partitioning a DDG into sub linear DDGs
4. When a dataset’s usage frequency is changed, we will re-calculate the storage
status of the sub linear DDG that contains this dataset.
In the strategy introduced above, the computation time complexity is well controlled
within O.mn4
i / by dividing the general DDG into sub linear DDGs, where m is the
number of the sub linear DDGs and ni is the number of datasets in the sub linear
DDGs.
Because of the utilization of the CTT-SP algorithm, the local-optimization based
storage strategy is more cost-effective but less efﬁcient and scalable than the cost
rate based strategy. More details about this strategy can be found in [14].
6
Minimum Cost Benchmarking of Datasets Storage
in the Cloud
As cost-effectiveness is an extremely important factor for the data storage strategy
of scientiﬁc applications in the cloud, users need to evaluate the cost effectiveness of
their storage strategies. Hence the cloud service providers should be able to provide
benchmarking services that can inform the minimum cost of storing the application
datasets in the cloud. As we discussed in previous sections, there is a trade-off
between computation and storage in the cloud. The benchmarking algorithms are
to ﬁnd this trade-off, which form the minimum cost storage strategy for scientiﬁc
applications in the cloud.
Finding the minimum cost storage strategy for a general DDG is a complicated
problem. In this subsection, we extend the CTT-SP algorithm to a recursive
algorithm with polynomial computation complexity that can be used on general
DDGs to ﬁnd the minimum cost benchmark.

144
D. Yuan et al.
de
ds
DDG
CTT
Block
Main Branch
Sub-branch
d1
d1
d2
d2
d3
d3
d4
d4
d5
d5
d6
d6
d7
d7
d8
d8
Fig. 5.7 An example of constructing CTT for a DDG with a block
6.1
Construct CTT for DDG with a Block
Block is a set of sub-branches in the DDG that split from a common dataset and
merge into another common dataset. We denote the block as B. Figure 5.7 shows
an DDG with a simple block B D fd3; d4; d5; d6g. We will use it as the example
to illustrate the construction of CTT.
To construct the CTT, we need the datasets in DDG to be totally ordered.
Hence, for the DDG with a block, we only choose one branch to construct
the CTT, as shown is Fig. 5.7. We call the linear datasets which are chosen
to construct the CTT “main branch,” denoted as MB, and call the rest of the
datasets “sub-branches,” denoted as SB. For example, in Fig. 5.7’s DDG, MB D
fd1; d2; d5; d6; d7; d8g and SB D fd3; d4g. Due to the existence of the block, the
edges can be classiﬁed into four categories. The deﬁnition of this classiﬁcation is as
follows:
•
In-block edge: e < di, dj > is an in-block edge meaning that the edge starts
from di, which is a dataset outside of the block, and points to dj, which is a
dataset in the block, such as e < d2, d5 >, e < d1, d6 > in Fig. 5.7. Formally,
we deﬁne e < di, dj > as an in-block edge, where
9dk 2 DDG ^ di ! dk ^ dj ½ dk

5
Computation and Storage Trade-Off...
145
•
Out-block edge: e < di, dj > is an out-block edge meaning that the edge starts
from di, which is a dataset in the block, and points to dj, which is a dataset
outside of the block, such as e < d6, d7 >, e < d5, d8 > in Fig. 5.7. Formally,
we deﬁne e < di, dj > as an out-block edge, where
9dk 2 DDG ^ di ½ dk ^ dk ! dj
•
Over-block edge: e < di, dj > is an over-block edge meaning that the edge
crosses over the block, where di is a dataset preceding the block, dj is a dataset
succeeding the block, such as e < d2, d7 >, e < d1, d8 > in Fig. 5.7. Formally,
we deﬁne e < di, dj > as an over-block edge, where
9dk; dh 2 DDG ^ dh ½ dk ^ di ! dh ! dj ^ di ! dk ! dj
•
Ordinary edge: e < di, dj > is an ordinary edge meaning that datasets between
di and dj are totally ordered, such as e < ds, d2 >, e < d5, d6 >, e < d7, d8 >
in Fig. 5.7. Formally, we deﬁne e < di, dj > as an ordinary edge, where
:9dk 2 DDG ^

.di ! dk ^ dk ½ dj/ _ .di ½ dk ^ dk ! dj /
_.dh 2 DDG ^ dh ½ dk ^ di ! dh ! dj ^ di ! dk ! dj /
	
6.2
General CTT-SP Algorithm for Minimum Cost
Benchmarking
For a general DDG, we ﬁrst choose a main branch to construct the CTT and start the
Dijkstra algorithm. During the algorithm, we recursively call the CTT-SP algorithm
to calculate the minimum cost storage strategy of the sub-branches of the DDG. The
pseudo code of this algorithm is shown in Fig. 5.8. The main steps of the algorithm
are as follows:
Step 1 (lines 1–22): Construct the initial CTT of the DDG. We choose an arbitrary
branch in the DDG as the main branch and add cost edges to construct the CTT.
In the CTT, for the ordinary edges and in-block edges, we set their weights based
on same formula as the linear CTT-SP algorithm. For the over-block edges, we
recursively call the CTT-SP algorithm on the sub-branches of the block and add
their cost rate to the weight of the over-block edge. For the out-block edges, we set
their weights as inﬁnity at the initial stage. We create a set of CTTs and add the
initial CTT to it.
Step 2 (lines 23–51): We start the Dijkstra algorithm to ﬁnd the shortest path from
ds to de. We use F to denote the set of datasets discovered by the Dijkstra algorithm.
When a new edgee < di, dj > is discovered, we ﬁrst add dj to F , and then check
whether e < di, dj > is an in-block edge or not. If not, we continue to ﬁnd the next

146
D. Yuan et al.
Fig. 5.8 Pseudo code of general CTT-SP algorithm

5
Computation and Storage Trade-Off...
147
edge. If e < di, dj > is an in-block edge, create a new CTT (see steps 2.1–2.3 next)
and add it to the set of CTTs. We continue to ﬁnd the next edge from the set of CTTs
by the Dijkstra algorithm until de is reached which would terminate the algorithm.
Step 2.1 (lines 29–34): In the case where in-block edge e < di, dj > is discovered,
we create a new CTT(e < di, dj >) based on the current CTT. First, we copy
all the information of the current CTT to the new CTT(e < di, dj >). Second, we
update the weights of all the in-block edges in CTT(e < di, dj >) as inﬁnity, except
e < di, dj >. This guarantees that dataset di is the stored adjacent predecessor of
the sub-branch in all the paths of CTT(e < di, dj >). Third, we update the weights
of all the out-block edges in CTT(e < di, dj >) as described next.
Step 2.2 (lines 35–50): In order to calculate the weight of an out-block edge e < dh,
dk > in CTT(e < di, dj >), we need to recursively call the CTT-SP algorithm on
the sub-branches of the block assuming that di is the start dataset and dk is the
end dataset. After we calculate the weights of all the out-block edges, we add new
CTT(e < di, dj >) to the CTTs set.
For more details of the general CTT-SP algorithm, please refer to our prior work
[15]. In [15], we further prove that the general CTT-SP algorithm is polynomial
with the worst case computation complexity of O.n9/. This algorithm can be used
as an on-demand minimum cost benchmarking approach in the cloud. Whenever
users want to know the minimum cost of storing the dataset, the general CTT-SP
algorithm is called to calculate the minimum cost benchmark for users.
7
Evaluation
We conduct simulation in the SwinCloud [35] system to evaluate our datasets
storage strategies and minimum cost benchmarking approach. For the general
performance evaluation, please refer to the detailed experimental results in our prior
work [12–15]. In this section, we demonstrate the simulation results of utilizing our
approach in the pulsar searching application introduced in Sect. 2. It shows how our
approach works in the real world scientiﬁc application.
7.1
Simulation Environment and Strategies
SwinCloud [35] is a cloud computing simulation environment built on the comput-
ing facilities in Swinburne University of Technology which takes advantage of the
existing SwinGrid system [36]. We install VMWare8 on SwinGrid, so that it can
offer uniﬁed computing and storage resources. By utilizing the uniﬁed resources,
8http://www.vmware.com/

148
D. Yuan et al.
Fig. 5.9 Structure of SwinCloud
we set up data centers that can host applications. In the data centers, Hadoop9 is
installed that can facilitate the MapReduce computing paradigm and distributed data
management. The structure of SwinCloud is depicted in Fig. 5.9.
To evaluate the cost-effectiveness of our datasets storage strategy, we com-
pare the total costs of different storage strategies with ours. The representative
strategies are:
1. Usage based strategy, in which we store the datasets that are most often used.
2. Generation cost based strategy, in which we store the datasets that incur the
highest generation cost.
3. Cost rate based strategy reported in [12, 13], in which we store the datasets by
comparing their own generation cost rate and storage cost rate.
4. Local-optimization based strategy reported in [14], in which we utilize the
CTT-SP algorithm on linear segments of a general DDG.
5. On-demand minimum cost benchmarking approach reported in [15], in which we
propose the general CTT-SP algorithm.
9http://hadoop.apache.org/

5
Computation and Storage Trade-Off...
149
Raw
beam
data
Accelerated
De-
dispersion
files
De-
dispersion
files
Extracted &
compressed
beam
Seek
results
files
Candidate
list
XML
files
Size:
Generation time:
20 GB
245 mins
1 mins
80 mins
300 mins
790 mins
27 mins
25 KB
1 KB
16 MB
90 GB
90 GB
Accelerated
De-
dispersion
files
Seek
results
files
Candidate
list
XML
files
Seek
results
files
Candidate
list
XML
files
245 mins
1 mins
80 mins
300 mins
25 KB
1 KB
16 MB
90 GB
245 mins
1 mins
80 mins
25 KB
1 KB
16 MB
New sub
DDG1
New sub
DDG2
Initial
DDG
Fig. 5.10 DDG of the pulsar searching workﬂow
7.2
Pulsar Searching Application Simulation and Results
In the pulsar searching application, for one execution of the workﬂow, six datasets
are generated. Scientists may need to re-analyze these datasets, or reuse them
in new workﬂows and generate new datasets. The DDG of this pulsar searching
workﬂow is shown in Fig. 5.10, as well as the sizes and generation times of these
datasets. The generation times of the datasets are from running this workﬂow on
Swinburne Astrophysics Supercomputer, and for simulation, we assume that in
the cloud computing system, the generation times of these datasets are the same.
Furthermore, we assume that the prices of cloud services follow Amazon clouds’
cost model.
From Swinburne Astrophysics research group, we understand that the
“De-dispersion ﬁles” is the most useful dataset. Based on these ﬁles, many
accelerating and seeking methods can be used to search pulsar candidates. Based
on the scenario, we set the “De-dispersion ﬁles” to be used once every 4 days and
other datasets to be used once every 10 days. Furthermore, we assume new datasets
are generated on the 10th day and 20th day, indicated as sub DDG1 and DDG2 in
Fig. 5.10. Based on this setting, we run the above mentioned simulation strategies
and calculated the total costs of the system for one branch of the pulsar searching
workﬂow of processing a piece of one hour’s observation data in 30 days as shown
in Fig. 5.11.
From Fig. 5.11 we can see that (1) the cost of the “store all” datasets strategy
is a polyline, because all the datasets are stored in the system that is charged at a
ﬁxed rate, and the inﬂection points only occur when new datasets are generated;
(2) the cost of the “store none” datasets strategy is a ﬂuctuated line because in this
strategy all the costs are computation cost of regenerating datasets. For the days that
have fewer requests of the data, the cost is low, otherwise, the cost is high; (3–4)
the costs of the generation cost based strategy and the usage based strategy are in
the middle band, which are much lower than the “store all” and “store none” storage

150
D. Yuan et al.
Total cost of 30 days - Pulsar case simulation
0
5
10
15
20
25
30
35
40
45
1
3
5
7
9
11 13
15
17
19
21
23
25 27
29
Days
Cost (USD)
Store all datasets
Store none
Usage based strategy
Generation cost based
strategy
Cost rate based strategy
Local-optimisation based
strategy & minimum cost
benchmark
Fig. 5.11 Cost-effectiveness of our strategy in the pulsar case DDG
Table 5.1 Storage status of datasets in the pulsar searching workﬂow with different strategies
Datasets
Strategies
Extracted
beam
De-dispersion
ﬁles
Accelerated
de-dispersion
ﬁles
Seek
results
Pulsar
candidates
XML
ﬁles
(1) Store all
Stored
Stored
Stored
Stored
Stored
Stored
(2) Store
none
Deleted
Deleted
Deleted
Deleted
Deleted
Deleted
(3) Genera-
tion cost
based
strategy
Deleted
Stored
Stored
Deleted
Deleted
Stored
(4) Usage
based
strategy
Deleted
Stored
Deleted
Deleted
Deleted
Deleted
(5) Cost rate
based
strategy
Deleted
Stored
(deleted
initially)
Deleted
Stored
Deleted
Stored
(6) Local-
optimization
based
strategy
Deleted
Stored
Deleted
Stored
Deleted
Stored
(7) Minimum
cost
storage
strategy
Deleted
Stored
Deleted
Stored
Deleted
Stored
strategies. The cost lines are slightly ﬂuctuated because the datasets are partially
stored; (5–7) the cost rate based strategy also has a good performance in this pulsar
searching application and the most cost-effective datasets storage strategy is our
local-optimization based strategy which performs the same as the minimum cost
benchmarking storage strategy in this speciﬁc application. Table 5.1 shows how the
datasets are stored with different strategies in detail.

5
Computation and Storage Trade-Off...
151
As shown in Fig. 5.10, the high-level pulsar searching workﬂow is not very
complicated, hence we can do some intuitive analyses on how to store the
generated datasets. For the dataset of “Accelerated de-dispersion ﬁles,” although
their generation cost is quite high, comparing to their huge sizes, it is not worth
storing them in the cloud. However, in the generation cost based strategy, the
“Accelerated De-dispersion ﬁles” are stored. For the “Final XML ﬁles,” they are not
very often used, but comparing to their high generation costs and small sizes, they
should be stored. However, in the usage based strategy, these ﬁles are not stored.
For the dataset of “De-dispersion ﬁles,” by comparing their own generation cost
rates and storage cost rates, the cost rate based strategy did not store them at the
beginning, but store them after they are used in the regeneration of other datasets.
In conclusion, the local-optimization based strategy is the most cost-effective and
appropriate datasets storage strategy which performs the same as the minimum cost
benchmarking storage strategy for this application in the cloud.
8
Conclusions and Future Directions
In this chapter, based on an astrophysics pulsar searching scenario, we have
examined the unique features of storing scientiﬁc datasets in the cloud. Based on
investigating the computation and storage trade-off of cloud resources, we have
proposed two cost-effective storage strategies and an on-demand minimum cost
benchmarking approach for storing scientiﬁc datasets in the cloud. The storage
strategies are efﬁcient and scalable for users to store their application datasets
in runtime which also consider users’ tolerance of data accessing delay. The
benchmarking approach is to ﬁnd the minimum cost of storing the scientiﬁc datasets
in the cloud, which users can facilitate on-demand to evaluate the cost-effectiveness
of their storage strategies. Evaluation with simulations indicates how our approach
works in the real scientiﬁc application in the cloud.
Our current work is based on Amazon clouds’ cost model and assumes that all
the application datasets are stored with a single cloud service provider. However,
sometimes scientiﬁc applications may have to run in a more distributed manner since
some application datasets may be distributed with ﬁxed locations by nature. In the
future, we will incorporate the data transfer cost into our cost models. Furthermore,
models of forecasting dataset usage frequency can be further studied, with which
our approach can be easily adapted to different scientiﬁc applications in the cloud.
References
1. Deelman, E., G. Singh, M. Livny, B. Berriman, and J. Good. The Cost of Doing Science on the
Cloud: the Montage Example. in ACM/IEEE Conference on Supercomputing (SC’08). pp. 1–
12. 2008. Austin, Texas, USA.

152
D. Yuan et al.
2. Ludascher, B., I. Altintas, C. Berkley, D. Higgins, E. Jaeger, M. Jones, and E.A. Lee, Scientiﬁc
Workﬂow Management and the Kepler System. Concurrency and Computation: Practice and
Experience, 2005. 18(10): pp. 1039–1065.
3. Szalay, A.S. and J. Gray, Science in an Exponential World. Nature, 2006. 440: pp. 23–24.
4. Deelman, E., D. Gannon, M. Shields, and I. Taylor, Workﬂows and e-Science: An Overview
of Workﬂow System Features and Capabilities. Future Generation Computer Systems, 2009.
25(5): pp. 528–540.
5. Bose, R. and J. Frew, Lineage Retrieval for Scientiﬁc Data Processing: A Survey. ACM
Computing Survey, 2005. 37(1): pp. 1–28.
6. Burton, A. and A. Treloar. Publish My Data: A Composition of Services from ANDS and
ARCS. in 5th IEEE International Conference on e-Science, (e-Science ’09) pp. 164–170. 2009.
Oxford, UK.
7. Foster, I., Z. Yong, I. Raicu, and S. Lu. Cloud Computing and Grid Computing 360-Degree
Compared. in Grid Computing Environments Workshop (GCE’08). pp. 1–10. 2008. Austin,
Texas, USA.
8. Buyya, R., C.S. Yeo, S. Venugopal, J. Broberg, and I. Brandic, Cloud Computing and Emerging
IT Platforms: Vision, Hype, and Reality for Delivering Computing as the 5th Utility. Future
Generation Computer Systems, 2009. 25(6): pp. 599–616.
9. Amazon Cloud Services: http://aws.amazon.com/.
10. Zaharia, M., A. Konwinski, A.D. Joseph, R. Katz, and I. Stoica. Improving MapReduce
Performance in Heterogeneous Environments. in 8th USENIX Symposium on Operating
Systems Design and Implementation (OSDI’2008). pp. 29–42. 2008. San Diego, CA, USA.
11. Adams, I., D.D.E. Long, E.L. Miller, S. Pasupathy, and M.W. Storer. Maximizing Efﬁciency
by Trading Storage for Computation. in Workshop on Hot Topics in Cloud Computing
(HotCloud’09). pp. 1–5. 2009. San Diego, CA, USA.
12. Yuan, D., Y. Yang, X. Liu, and J. Chen. A Cost-Effective Strategy for Intermediate Data Storage
in Scientiﬁc Cloud Workﬂows. in 24th IEEE International Parallel & Distributed Processing
Symposium (IPDPS’10). pp. 1–12. 2010. Atlanta, Georgia, USA.
13. Yuan, D., Y. Yang, X. Liu, G. Zhang, and J. Chen, A Data Dependency Based Strategy
for Intermediate Data Storage in Scientiﬁc Cloud Workﬂow Systems. Concurrency and
Computation: Practice and Experience, 2010. (http://dx.doi.org/10.1002/cpe.1636)
14. Yuan, D., Y. Yang, X. Liu, and J. Chen. A Local-Optimisation based Strategy for Cost-Effective
Datasets Storage of Scientiﬁc Applications in the Cloud. in 4th IEEE International Conference
on Cloud Computing (Cloud2011). pp. 1–8. 2011. Washington DC, USA.
15. Yuan, D., Y. Yun, X. Liu, and J. Chen, On-demand Minimum Cost Benchmarking for
Intermediate Datasets Storage in Scientiﬁc Cloud Workﬂow Systems. Journal of Parallel and
Distributed Computing, 2011. 72(2): pp. 316–332.
16. Chiba, T., T. Kielmann, M.d. Burger, and S. Matsuoka. Dynamic Load-Balanced Multicast for
Data-Intensive Applications on Clouds. in IEEE/ACM International Symposium on Cluster,
Cloud and Grid Computing (CCGrid2010). pp. 5–14. 2010. Melbourne, Australia.
17. Juve, G., E. Deelman, K. Vahi, and G. Mehta. Data Sharing Options for Scientiﬁc Workﬂows
on Amazon EC2. in ACM/IEEE Conference on Supercomputing (SC’10). pp. 1–9. 2010.
New Orleans, Louisiana, USA.
18. Li, J., M. Humphrey, D. Agarwal, K. Jackson, C.v. Ingen, and Y. Ryu. eScience in the Cloud: A
MODIS Satellite Data Reprojection and Reduction Pipeline in the Windows Azure Platform. in
24th IEEE International Parallel & Distributed Processing Symposium (IPDPS’10). pp. 1–12.
2010. Atlanta, Georgia, USA.
19. Yuan, D., Y. Yang, X. Liu, and J. Chen, A Data Placement Strategy in Scientiﬁc Cloud
Workﬂows. Future Generation Computer Systems, 2010. 26(8): pp. 1200–1214.
20. Eucalyptus. Available from: http://open.eucalyptus.com/.
21. Nimbus. Available from: http://www.nimbusproject.org/.
22. OpenNebula. Available from: http://www.opennebula.org/.

5
Computation and Storage Trade-Off...
153
23. Armbrust, M., A. Fox, R. Grifﬁth, A.D. Joseph, R. Katz, A. Konwinski, G. Lee, D. Patterson,
A. Rabkin, I. Stoica, and M. Zaharia, A View of Cloud Computing. Commun. ACM, 2010.
53(4): pp. 50–58.
24. Assuncao, M.D.d., A.d. Costanzo, and R. Buyya. Evaluating the Cost-Beneﬁt of Using Cloud
Computing to Extend the Capacity of Clusters. in 18th ACM International Symposium on High
Performance Distributed Computing (HPDC’09). pp. 1–10. 2009. Garching, Germany.
25. Kondo, D., B. Javadi, P. Malecot, F. Cappello, and D.P. Anderson. Cost-Beneﬁt Analysis of
Cloud Computing versus Desktop Grids. in 23th IEEE International Parallel & Distributed
Processing Symposium (IPDPS’09). pp. 1–12. 2009. Rome, Italy.
26. Cho, B. and I. Gupta. New Algorithms for Planning Bulk Transfer via Internet and Shipping
Networks. in IEEE 30th International Conference on Distributed Computing Systems (ICDCS).
pp. 305–314. 2010. Genova, Italy.
27. Gunda, P.K., L. Ravindranath, C.A. Thekkath, Y. Yu, and L. Zhuang. Nectar: Automatic
Management of Data and Computation in Datacenters. in 9th Symposium on Operating
Systems Design and Implementation (OSDI’2010). pp. 1–14. 2010, Vancouver, Canada.
28. Bao, Z., S. Cohen-Boulakia, S.B. Davidson, A. Eyal, and S. Khanna. Differencing Prove-
nance in Scientiﬁc Workﬂows. in 25th IEEE International Conference on Data Engineering
(ICDE’09). pp. 808–819. 2009. Shanghai, China.
29. Groth, P. and L. Moreau, Recording Process Documentation for Provenance. IEEE Transac-
tions on Parallel and Distributed Systems, 2009. 20(9): pp. 1246–1259.
30. Muniswamy-Reddy, K.-K., P. Macko, and M. Seltzer. Provenance for the Cloud. in 8th
USENIX Conference on File and Storage Technology (FAST’10). pp. 197–210. 2010. San Jose,
CA, USA.
31. Osterweil, L.J., L.A. Clarke, A.M. Ellison, R. Podorozhny, A. Wise, E. Boose, and J. Hadley.
Experience in Using A Process Language to Deﬁne Scientiﬁc Workﬂow and Generate Dataset
Provenance. in 16th ACM SIGSOFT International Symposium on Foundations of Software
Engineering. pp. 319–329. 2008. Atlanta, Georgia: ACM.
32. Foster, I., J. Vockler, M. Wilde, and Z. Yong. Chimera: A Virtual Data System for Representing,
Querying, and Automating Data Derivation. in 14th International Conference on Scientiﬁc and
Statistical Database Management, (SSDBM’02). pp. 37–46. 2002. Edinburgh, Scotland, UK.
33. Simmhan, Y.L., B. Plale, and D. Gannon, A Survey of Data Provenance in E-Science. SIGMOD
Rec., 2005. 34(3): pp. 31–36.
34. Garg, S.K., R. Buyya, and H.J. Siegel, Time and Cost Trade-Off Management for Scheduling
Parallel Applications on Utility Grids. Future Generation Computer Systems, 2010. 26(8):
pp. 1344–1355.
35. Liu, X., D. Yuan, G. Zhang, J. Chen, and Y. Yang, SwinDeW-C: A Peer-to-Peer Based Cloud
Workﬂow System, in Handbook of Cloud Computing, B. Furht and A. Escalante, Editors. 2010,
Springer. pp. 309–332.
36. Yang, Y., K. Liu, J. Chen, J. Lignier, and H. Jin. Peer-to-Peer Based Grid Workﬂow Runtime
Environment of SwinDeW-G. in IEEE International Conference on e-Science and Grid
Computing. pp. 51–58. 2007. Bangalore, India.


Part II
Technologies and Techniques


Chapter 6
A Survey of Load Balancing Techniques
for Data Intensive Computing
Zhiquan Sui and Shrideep Pallickara
1
Introduction
Data volumes have been increasing substantially over the past several years. Such
data is often processed concurrently on a distributed collection of machines to
ensure reasonable completion times. Load balancing is one of the most important
issues in data intensive computing. Often, the choice of the load balancing strategy
has implications not just for reduction of execution times, but also on energy usage,
network overhead, and costs.
Applications that are faced with processing large data volumes have a choice
of relying on frameworks (often cloud-based) that are increasingly popular or
designing algorithms that are suited for their application domain. Here, we will
cover both. Our focus is a survey of the frameworks, APIs, and schemes used to load
balance processing of voluminous data on a collection of machines while processing
large data volumes in settings such as analytics (MapReduce), stream based settings,
and discrete event simulations.
In Sect. 2 we discuss several popular data intensive computing frameworks. APIs
available to for the development of cloud-scale applications are discussed in Sect. 3.
In Sect. 4, we describe both static and dynamic load balancing schemes and how the
latter is used in different settings. Section 5 outlines our conclusions.
2
Data Intensive Computing Frameworks
2.1
Google MapReduce Framework
MapReduce [1] is a framework introduced by Google that is well suited for
concurrent processing of large datasets (usually more than 1 Tb) on a collection
Z. Sui () • S. Pallickara
Department of Computer Science, Colorado State University, Fort Collins, CO, USA
e-mail: simonsui@cs.colostate.edu; shrideep@cs.colostate.edu
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 6, © Springer Science+Business Media, LLC 2011
157

158
Z. Sui and S. Pallickara
Map
Map
Map
Map
Storage
Reduce
Reduce
Reduce
Final Results
Fig. 6.1 A MapReduce computation
of machines. The framework is suited for problems where the same processing
is applied to apportioned sections of the data: these include grep, histogramming
words, sort, web log analysis, and so on. MapReduce underpins several capabilities
within Google. For example, Google uses the MapReduce framework to index
the web.
The basic concepts within this framework are Map and Reduce. A large task
can be divided into two phases: Map and Reduce. The Map phase divides the
large task into smaller pieces and dispatches each small piece of work onto one
active node in the cluster. The Reduce phase collects the results from the Map
phase and processes these results to get the ﬁnal result. A typical MapReduce
computation is shown in Fig. 6.1. In general, the Map and Reduce functions
divide the data that they operate on for load balancing purposes. However, slow
or bogged down machines may lead to straggler computations within the system
that might lead to longer completion time: the completion time is only as fast as
the slowest computation. If one straggler computation is twice as slow as other
computations in that phase, then the total elapsed time in this step would be twice
that of the case without the straggler computation. In such a case, a straggler
detection and avoidance mechanism becomes necessary. This is done by launching
speculative tasks towards the end of a processing phase on other machines for those
computations that have not yet ﬁnished. It is possible for computation imbalances
to exist in a MapReduce computation depending on the rate at which the results are
generated in the Map phase and the rate at which these results are processed (or
consumed) within the Reduce phase. An imbalance in this rate could result in the
reducing machines idling or for the mapped data to be backlogged or queued in the
reducing phase. Other optimizations of MapReduce framework are introduced in
Sect. 4.2.2.

6
A Survey of Load Balancing Techniques for Data Intensive Computing
159
2.1.1
Hadoop Framework
Hadoop [2] is the most popular implementation of the MapReduce framework. It
incorporates support for reliability and scalability while scheduling tasks within a
distributed system. Hadoop identiﬁes straggler machines within a cluster and then
speculative re-launches tasks that were hosted on the machines. This model works
well in homogeneous system where machines have very similar conﬁgurations in
terms of hardware and software. In a heterogeneous setting involving machines
with disparate conﬁgurations the scheme of launching speculative tasks does not
result in the performance gains that one see in homogeneous settings. In [3], the
authors improve the Hadoop framework and introduce a new algorithm called
LATE. This algorithm adjusts the re-launch mechanism within Hadoop. First, it re-
launches speculative tasks only on non-straggler machines. Intuitively, re-launching
speculative tasks is waste of resources. Secondly, this approach focuses on re-
launching tasks that will be the most delayed rather than the ones that are about
to complete. By targeting tasks that will be the last to ﬁnish this algorithm targets
tasks that will slow down the whole system. Last but not the least, the number of
speculative tasks is reduced. This is important because launching speculative tasks
has resource costs and includes additional overheads. The new algorithm works
just as well as the basic Hadoop framework in homogeneous setting, but produces
substantially better results in heterogeneous settings.
2.2
Microsoft Dryad Framework
Dryad [4] is a framework developed at Microsoft that uses graphs to express
computations. The focus within Dryad is to evaluate SQL queries concurrently on
a distributed collection of machines. Each query operation is a read-only operation
that avoids the write-conﬂict problem: this, in turn, allows operations to be done
in parallel. Each speciﬁed SQL query can be divided into several steps involving
concurrent SQL query operations. The topology of the computational graph is con-
ﬁgured before runtime and is levelized with each level being responsible for an SQL
operation. The structure of Dryad is depicted in Fig. 6.2. There are several different
relationships between two adjacent levels. Users can choose the one corresponding
to the logic of computation. Also users have to specify the functionality of each
node. During execution, each node relies on the operations before it. Once it receives
all data from nodes in the preceding stage it initiates processing.
The issue of load balancing in Dryad is exacerbated by the presence of levels
within the computational graph. Stragglers within a level inﬂuence not just that level
but also all subsequent levels in the graph. There can also be some optimizations in
the Dryad framework. For example, a many-to-one mapping (or a fan-in) may result
in a bottleneck because of the bandwidth available at the destination node. Here,
the topology can be reﬁned by replicating the destination node and distributing the
input data. The Dryad framework address reliability issues by incorporating support
for failure detection and re-launch mechanisms for affected computations.

160
Z. Sui and S. Pallickara
Mapping Function 2
Mapping Function 1
Input Files
R
R
R
R
Level 1
Operation R
X
X
X
X
X
X
Level 2
Operation X 
M
M
M
M
Level 3
Operation M
Output Files
Fig. 6.2 Structure of dryad jobs
2.3
Processing Data Streams
A data stream [5] is sequence of tuples generated at run time. The unique
characteristic of this model [6, 7] is that the data is unknown before execution.
However, the operations are ﬁxed. For example, in an online purchasing system,
the operations that the users can perform are ﬁxed. Users can only put the items
into their carts, pay the bill for their choices, and so on. But the system can never
predict which user will purchase which item at what time. This is a typical data
stream based model. The users’ choices will be sent as tuples. The ﬁxed operations
have been programmed in the system. When thousands of such tuples come into the
query network at the same time, the system becomes fairly busy and issues such as
load balancing, consistency, and reliability need to be addressed.
There are several systems that address the load balancing issues within this
model. Aurora and Medusa [8, 9] are representative of such systems. Aurora is
an intra-participant distributed system. During initializations the system relies on a
simplistic distribution of the load. However, once a node is overloaded either in CPU
utilization, memory usage or network bandwidth, it will try to migrate its load to all
the other nodes. Medusa, on the other hand, is based on inter-participant federated
operation. Each action in the Medusa system is made by a collection of users. The
system relies on agoric computing [10] which focuses on applying schemes from the

6
A Survey of Load Balancing Techniques for Data Intensive Computing
161
area of economics (such as auctions) to managing computational resources. Each
message stream in the system has a positive value which represents its price. Each
node will “sell” their stream to the other participants so that it can earn some money
from these transactions. The objective is to run this mechanism within the system so
that it will anneal to a stable economic status. Intuitively, a heavy-loaded node will
not have enough money to buy more message streams so the load balancing issue
will be addressed as well.
Consistency is rather important for data stream based system. Users tend to
be more than tolerant of network stability problems than they are of problems
with their purchases. There are basically three mechanisms for recovery in the
presence of failures: precise recovery, rollback recovery, and gap recovery. Gap
recovery ignores lost information and as such is not acceptable in data stream based
system. Precise recovery focuses on recovering transactions perfectly to what it was
before failures and provides strict correctness for transactions so many important
commercial activities rely on this kind of recovery. However, this mechanism has
rather high performance costs and overheads exist even when there are no failures.
Rollback recovery provides an equivalent but not necessarily the same output as
precise recovery after failures. The output may have duplicated tuples after failures.
Such as scheme works well in situations where the system cannot lose information
but can withstand duplicated information. This mechanism has less overhead than
precise recovery.
3
Developing Cloud-Scale Applications
In this section we describe two popular approaches to developing cloud-scale
applications: the Google App Engine and Microsoft Azure.
3.1
The Google App Engine
The Google App Engine [11, 12] is a popular framework for data intensive
computations that allows users to run their web applications without having to
worry about the maintenance of servers. All a user needs to do is to upload their
application and the framework automatically provisions resources by proﬁling the
application’s storage and data ﬂow. Issues related to reliability and load balancing
issues are handled transparently.
The Google App Engine allows developer to develop and deploy web appli-
cations. The App Engine includes support for programming in Java and Python.
Developers can develop applications that use Java features such as the Java Virtual
Machine (JVM) and Java servlets besides languages such as Javascript and Ruby
that rely on a JVM-based interpreter or compiler. The App Engine supplies one
Python interpreter and a standard Python library, which supports development using
Python.

162
Z. Sui and S. Pallickara
The Google App Engine allows development of applications that have signiﬁcant
processing requirements due to the data volumes involved. The main functionalities
of this framework include:
•
Support for most popular web technologies.
•
A consistent storage space that includes support for queries, transactions and
sorting.
•
Local simulation of the Google App Engine that supports developing and
debugging applications prior to deployments in the cloud.
•
A mechanism for scheduling tasks.
•
Identity veriﬁcation and email services using Gmail accounts.
•
Load balancing framework.
3.2
Microsoft Azure
The Azure Platform [13] from Microsoft is a framework for developing cloud
applications. All data are stored in a distributed ﬁle system and accessible online;
the data is replicated to account for failures that might take place. This precludes the
need to store data locally. In the Azure model users do not buy software, rather they
buy services are a charged based on usage.
Microsoft Azure Platform supports three types of roles: Web, Worker, and Virtual
Machine (VM). Underpinning support for the web role is the Internet Information
Services (IIS) 7 and ASP.Net. Users can use native code in PHP or Java to create
applications. Compared to the Web role, the Worker role focuses on background
processing and is not hosted by IIS. Users can use the.NET framework or some
other application running in Windows with the Worker role. The VM role is used
to deploy a custom Windows Server 2008 R2 image to Windows Azure. In the VM
role, users have more control over their environment and can manage their own
conﬁgurations of the OS, use Windows Services, schedule tasks and so on.
4
Load Balancing Schemes
In practice there are two broad classes of load balancing schemes: static and
dynamic. The suitability of these schemes depends on the application characteristics
and objectives that must be met.
4.1
Static Load Balancing Schemes
In the static scheme load balancing decisions are made before execution. The system
also typically performs several experiments to collect information such as execution
time on a single processor, memory usage and so on. In [14], the authors describe

6
A Survey of Load Balancing Techniques for Data Intensive Computing
163
several static load balancing algorithms that can be divided into three categories:
greedy, searching algorithms, and machine learning algorithms. The objective is
to dispatch a set of subtasks with dependencies within a cluster. These algorithms
consider both execution time and energy cost.
Greedy algorithms basically set up a criteria followed by the dispatch. The
criterion is a function that is the combination of execution time and battery usage.
For the Min-Min algorithm, the ﬁrst Min is to ﬁnd the minimum ﬁtness value of all
the machines for each subtask. The second Min is to ﬁnd the minimum ﬁtness value
among the results of the ﬁrst step. The algorithm repeats these steps until all the
subtasks have been dispatched. The Levelized Weight Tuning (LWT) algorithm and
Bottom Up algorithm are similar. They both rely on the DAG (Directed Acyclic
Graph) that represents the dependencies of the subtasks and dispatch subtasks
level-by-level with the LWT algorithm relying on processing these levels in a top-
to-bottom scheme with the BU algorithm being bottom-up.
Search algorithms look for the best solution using a search tree. However, when
using search, there must be a pruning optimization to ensure that the complexity
is acceptable. The A algorithm works well in this situation. The depth of the tree
corresponds either to the number of sub-tasks for a given task or the number of
available machines. In each level of the search tree, it stores a ﬁxed number (100)
of statuses. It expands (multiples of 100) these statuses and then selects the 100 best
statuses in the next level. This process is repeated depending on the depth of the tree
and a best solution is then found.
Machine learning algorithms are widely used in the load balancing area. Genetic
algorithm tends to be the most convenient for static load. The main idea here is to
randomly generate some dispatch patterns and generate new patterns from them. In
each step the ﬁttest patterns survives which then go on to generate new patterns in
the next step. The suitability of the pattern is function of the metrics of interest such
as execution time and energy cost. There are also some mutations in each step. One
often ends with efﬁcient patterns that would not have been generated otherwise. In
each round the patterns tend to be more and more suitable (or ﬁt). The algorithm
stops when the ﬁtness function does not change for several steps or is acceptable.
The overhead for static load balancing algorithms comes before the execution
starts. For the algorithms described in [14], the most costly overhead is no more
than 20% of the whole execution time.
4.2
Dynamic Load Balancing Schemes
Compared to static load balancing, the costs for dynamic load balancing are
interleaved with the execution time; here, the scheduling decisions are being
computed while the tasks are executing. These algorithms must have less complexity
than static load balancing algorithms. Using a computationally intensive algorithm
to come up with the best dispatch scheme may not be the best choice. This is because
the best solution at given instant may not be the best solution when new events occur.
At any given instant, it is much more useful to arrive at a good solution in a short
time.

164
Z. Sui and S. Pallickara
In this section, we will introduce different dynamic load balancing algorithms in
different scenarios.
4.2.1
Dynamic Load Balancing Schemes in Stream Based Scenarios
Load balancing algorithms in stream based scenarios is quite different from those
in discrete event simulations. Stream based scenarios usually involve a levelized
network and each level of network is responsible for particular operations. Each
task, which is like a stream in this network, contains a series of operations and each
operation can be performed by one level in the network. The goal is to ﬁnish all the
tasks that arrive in this network as soon as possible. A signiﬁcant characteristic of
the tasks is that they are unpredictable. Because the users submit the tasks most of
time, the network does not know beforehand how many tasks will arrive over the
next few seconds. Unlike discrete event simulation there is also no synchronization
point during execution; thus there is no intuitive point at which migrations may
be coordinated. In this case, traditional load balancing algorithms often fall short.
However, machine learning algorithms tend to perform much better.
In [15], the authors introduce an ant-colony algorithm for dynamic load balanc-
ing in stream-based scenarios. The classic ant-colony algorithm has been modiﬁed
to account for the speciﬁcity of the problem. The algorithm relies on three types of
ants with different functionalities. The ants are also more intelligent than the classic
ants in ant-colony algorithm in that they keep store more information. However,
the main idea still involves searching the path randomly and leaving pheromones
in the path while passing by; the stronger the pheromone the more the number
of ants that will be attracted to selecting that path. In this algorithm, each task
will choose the current best solution and this might introduce bottlenecks into
the whole system. Extremely selﬁsh behavior might introduce greater latency to the
other tasks. However, in this system, such behavior is acceptable. The goal of the
algorithm is no longer to ﬁnish all the tasks in the shortest time but to make sure that
the average latency is minimum. The ﬁrst task should be served as fast as it can
because nobody knows how many other tasks may arrive in the near future. Also,
the algorithm usually takes time to learn the arrival patterns and does not work
as effectively in the beginning. Such learning algorithms work extremely well for
tasks where the arrival patterns are regular and the self-tuning characteristic of the
machine-learning algorithm can accommodate slow changes to the arrival patterns.
However, frequent changes to the task arrival patterns may lead to deteriorating
performance in such a scheme. In general, machine learning algorithms underpin
load balancing schemes in stream based scenarios.
4.2.2
Dynamic Load Balancing in Cloud Computing
In cloud computing, the frameworks introduced in Sects. 2 and 3 are widely
used. There are also some dynamic load balancing algorithms that build on these
frameworks. One such dynamic load balancing algorithm that builds on the Hadoop

6
A Survey of Load Balancing Techniques for Data Intensive Computing
165
Fig. 6.3 MapReduce bottleneck example
framework is described in [16]. There exists a threshold in the balancer, which
controls the rate at which a node should spread some of its work to the other
nodes. This threshold determines how much imbalance would be tolerated before
tasks would be redistributed for balancing purposes. The smaller the threshold is the
more balanced the system is because the balancer will respond to small imbalances;
however, this also results in more overhead due to the balancing operations. In
contrast to traditional clusters, the communication overhead in cloud settings is
slightly more expensive so the load migrations will only target neighboring nodes.
Once a node exceeds the threshold, it sends a request to the controller called
the NameNode. The NameNode in turn returns the most idle neighboring node’s
information back to the node. The node then compares whether the migration
is reasonable based on the information. If so, it will send the migration to the
destination node.
Some more dynamic load balancing optimizations [17] have been applied to
MapReduce framework. The authors have focused their effort on the detection of
critical paths in the network. The optimization mechanisms are workﬂow priority,
stage priority, and bottleneck elimination. The workﬂow priority optimization is
to set a Workﬂow Priority which is speciﬁed by the users. The users can set this
parameter depending on whether it is in the test or production phase, proximity
to deadline, or the urgency for an output. The more important the application is
the higher its priority is and the better its performance. Stage priority optimization
is similar to workﬂow priority but is applicable to different stages within a task.
Depending on how much work each stage has, users can also set the Stage Priority,
and the system will then set aside corresponding resources for each of the stages.
This scheme avoids bottlenecks and situations where several stages are waiting for
the output of one stage. The bottleneck elimination strategy is to balance the load
within each stage. A typical load imbalance is depicted in Fig. 6.3. The optimization
here is to redistribute the load from the active bottleneck nodes to the passive idle
nodes. With this mechanism, the overall progress of the whole stage will be gained.

166
Z. Sui and S. Pallickara
4.2.3
Dynamic Load Balancing Schemes in Discrete Event Simulation
Discrete event simulation is an effective technology for stochastic systems in
domains such as economics, epidemic modeling, and weather forecasting. In
discrete event simulations that rely on modeling phenomena that have geographical
underpinnings (such as disease spread), the whole system models a region and the
focus of the load balancing algorithm is to divide this region for different nodes.
The simplest scheme is to divide the region into spatially equal pieces. However,
this scheme usually results in imbalances because the distributing density of the
population being modeled not uniform throughout the region.
Another policy may focus on dividing the region and make sure that the
population is equal for each subdivided region. This policy while better than the
equal sized spatial splits wills still results in an imbalance. Events are not equally
distributed among all the entities and during the course of the simulation there is a lot
of ﬂux in the number of active individuals. Other commonly used schemes include
random distributions and explicit spatial scattering [18] which has been explored in
the context of a trafﬁc simulation problem. The main idea in these schemes is to
divide each complex computational region into smaller pieces. This works well in
many situations but it also increases the communication footprint within the system.
The communication overheads may become a bottleneck in situations where a large
number of messages are being exchanged and also in situations where the network
connecting the processing elements is bogged down resulting in higher latencies. In
such situations dynamic load balancing is needed to reduce this imbalance.
The two core issues in dynamic load balancing are how to detect computational
imbalances and how to migrate parts of the load to the other nodes. Detection of
imbalance can be implemented either in a control node or in each of the individual
nodes.
The controller-worker pattern [19] works well for a wide range of problems.
In this pattern, the detection of imbalance is the responsibility of the controller.
One approach to detecting the load imbalance is using the current execution time
as the basis for what the future execution times would be. A system that relies on
using load patterns being sent by each worker would be more accurate than just
the execution time, however this can result in more processing and communication
overheads.
The approach is to use a decentralized strategy. Here, there is no centralized
controller in the system. Rather, the workers communicate directly with each other
to determine whether their relative loads. In this scenario, each worker has a
threshold that lets it judge whether it is (or has transitioned into) a heavily or lightly
loaded worker. This threshold changes during the course of execution. For each time
step the workers broadcast their own load and autonomously make decisions about
whether they have breached threshold bounds.
The decision on migrating tasks is predicated on identify the task that needs
to be migrated, the destination for the migrated task, and the process migration
mechanisms that involve state synopsis and serialization. Identiﬁcation of over-
loaded workers and new destination nodes is easier in the controller-worker pattern

6
A Survey of Load Balancing Techniques for Data Intensive Computing
167
because the controller has information about all workers. However, there might
be some restrictions on this migration. For instance, some simulations require
that the geographical regions being modeled must be contiguous and, furthermore,
in some cases the geometry of the modeled regions might be constrained which
often can make the problem much harder. In [20], the authors describe an efﬁ-
cient regional split policy that splits the regions into strips which make it much
easier for an overloaded region to migrate parts of its load to its neighbors. The
scheme also incorporates an algorithm that balances the computational load at
each synchronization point for all the workers. While algorithm may not give the
most optimal solution, it makes fast decisions with excellent overall completion
times.
In a decentralized scheme the lack for a centralized controller means that the
overloaded node is responsible for ﬁnding a suitable node for load shedding. One
effective rule for achieving load shedding is based on the fraction of the nodes that
are heavily loaded or lightly loaded; here, a heavy loaded node would push its load
onto a lightly loaded node in a system where most nodes are lightly loaded, while
a lightly loaded node would pull load away from a heavily loaded node in a system
where most nodes have a high load. The random destination algorithm is particularly
effective in such settings. First, the algorithm is not compute intensive and does
not introduce additional overheads and the probability of the load migration being
successful is high. Secondly, even if the load migrates to a bad destination, the
impact of this migration is limited to the next synchronization point at which
point the destination will detect itself as heavily loaded or lightly loaded and take
corrective measures.
The mechanism for migrating processes is works well for dynamic load bal-
ancing in discrete event simulations. For spatially explicit problems an effective
implementation of the regional split primitive makes dynamic subdivisions of
regions easier. Even in the case of classic task dispatching problems, current VMs
are generally provide excellent support for process migration.
5
Conclusion
Dynamic load balancing is an important mechanism for data intensive computing. In
this chapter we discussed popular mechanisms in different scenarios. The choice of
load balancing algorithm plays an important role in the overall system performance.
Static and dynamic loading balancing algorithms have applicability in different
settings and often cannot be interchanged without adversely impacting system
performance. The emergence of streaming data and the corresponding increase in
data volumes mean that more systems would need to rely on dynamic load balancing
algorithms.

168
Z. Sui and S. Pallickara
References
1. J. Dean and S. Ghemawat. 2008. MapReduce: simpliﬁed data processing on large clusters.
Commun. ACM 51, 1 (January 2008), 107–113.
2. http://hadoop.apache.org/
3. M. Zaharia, A. Konwinski, A.D. Joseph, R.H. Katz, and I. Stoica. Improving MapReduce
Performance in Heterogeneous Environments. In Proceedings of OSDI. 2008, 29–42.
4. M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. “Dryad: distributed data-parallel
programs from sequential building blocks,” presented at the Proceedings of the 2nd ACM
SIGOPS/EuroSys European Conference on Computer Systems 2007, Lisbon, Portugal, 2007.
5. J.-H. Hwang, M. Balazinska, A. Rasin, U. Cetintemel, M. Stonebraker, and S. Zdonik.
2005. High-Availability Algorithms for Distributed Stream Processing. In Proceedings of the
21st International Conference on Data Engineering (ICDE ’05). IEEE Computer Society,
Washington, DC, USA, 779–790.
6. B. Babcock, S. Babu, M. Datar, R. Motwani, and J. Widom. 2002. Models and issues in
data stream systems. In Proceedings of the twenty-ﬁrst ACM SIGMOD-SIGACT-SIGART
symposium on Principles of database systems (PODS ’02). ACM, New York, NY, USA,
1–16.
7. D. Carney, U. C¸ etintemel, M. Cherniack, C. Convey, S. Lee, G. Seidman, M. Stonebraker,
N. Tatbul, and S. Zdonik. 2002. Monitoring streams: a new class of data management
applications. In Proceedings of the 28th international conference on Very Large Data Bases
(VLDB ’02). VLDB Endowment 215–226.
8. M. Cherniack, H. Balakrishnan, M. Balazinska, D. Carney, U. C¸ etintemel, Y. Xing, and
S. Zdonik. Scalable distributed stream processing. In Proc. of the First Biennial Conference
on Innovative Data Systems Research (CIDR’03), Jan. 2003.
9. S.B.
Zdonik,
M.
Stonebraker,
M.
Cherniack,
U.
C¸ etintemel,
M.
Balazinska,
and
H. Balakrishnan. “The Aurora and Medusa Projects”, presented at IEEE Data Eng. Bull., 2003,
pp.3–10.
10. M.S. Miller and K.E. Drexler. “Markets and Computation: Agoric Open Systems,” in The
Ecology of Computation, B.A. Huberman, Ed.: North-Holland, 1988.
11. http://code.google.com/intl/en/appengine/docs/
12. A. Bedra. “Getting Started with Google App Engine and Clojure,” Internet Computing, IEEE,
vol.14, no.4, pp.85–88, July-Aug. 2010.
13. http://www.microsoft.com/windowsazure/
14. S. Shivle, R. Castain, H.J. Siegel, A.A. Maciejewski, T. Banka, K. Chindam, S. Dussinger,
P. Pichumani, P. Satyasekaran, W. Saylor, D. Sendek, J. Sousa, J. Sridharan, P. Sugavanam,
and J. Velazco. “Static mapping of subtasks in a heterogeneous ad hoc grid environment,” in
Proc. of 13th HCW Workshop, IEEE Computer Society, 2004.
15. G.T. Lakshmanan and R. Strom. Biologically-inspired distributed middleware management for
stream processing systems. ACM Middleware conference, 2008.
16. http://www.ibm.com/developerworks/cloud/library/cl-mapreduce/index.html
17. T. Sandholm and K. Lai. MapReduce optimization using regulated dynamic prioritization.
In Proceedings of the 11th International Joint Conference on Measurement and Modeling of
Computer Systems (SIGMETRICS), pages 299–310, 2009.
18. S.Thulasidasan, S.P. Kasiviswanathan, S. Eidenbenz, P. Romero. “Explicit Spatial Scattering
for Load Balancing in Conservatively Synchronized Parallel Discrete Event Simulations,”
Principles of Advanced and Distributed Simulation (PADS), 2010 IEEE Workshop on, vol.,
no., pp.1–8, 17–19 May 2010.
19. Z. Sui, N. Harvey, and S. Pallickara. Orchestrating Distributed Event Simulations within the
Granules Cloud Runtime, Technical Report CS-11 Colorado State University, June 2011.
20. E. Deelman. and B.K. Szymanski. “Dynamic load balancing in parallel discrete event
simulation for spatially explicit problems,” Parallel and Distributed Simulation, 1998.
PADS 98. Proceedings. Twelfth Workshop on, vol., no., pp.46–53, 26–29 May 1998.

Chapter 7
Resource Management for Data Intensive
Clouds Through Dynamic Federation: A Game
Theoretic Approach
Mohammad Mehedi Hassan and Eui-Nam Huh
1
Introduction
In recent years deploying data-intensive applications in the cloud are gaining a lot
of momentum in both research and industrial communities [11, 14, 16, 21, 22, 24].
As the data rates and the processing demands of these applications vary over time,
the on-demand cloud paradigm is becoming a good match for their needs. The
cloud computing models that directly applicable to data-intensive computing char-
acteristics are Infrastructure as a Service (IaaS) and Platform as a Service (PaaS).
IaaS typically includes a large pool of conﬁgurable virtualized resources which can
include hardware, operating systems, middleware, and development platforms or
other software services which can be scaled to accommodate varying processing
loads [26]. The computing clusters typically used for data-intensive processing can
be provided in this model. Processing environments such as Hadoop MapReduce
and LexisNexis HPCC which include application development platform capabilities
in addition to basic infrastructure implement the Platform as a Service (PaaS) model.
Applications with a high degree of data parallelism and a requirement to process
very large datasets can take advantage of cloud computing and IaaS or PaaS using
hundreds of computers provisioned for a short time instead of one or a small number
of computers for a long time [5].
However, the prevalent commercial cloud providers (CPs), operating in isola-
tion (i.e., proprietary in nature), may face resource over-provisioning, degraded
performance, and service level agreement (SLA) violations (i.e., cloud service
outages) [28] to meet the storage, communication, and processing demands of some
data-intensive applications, characterized by the processing and storage of data
produced by high-bandwidth sensors or streaming applications [16]. Present trends
in cloud service providers’ capabilities give rise to the interest in federating Clouds
M.M. Hassan () • E.-N. Huh
Department of Computer Engineering, Kyung Hee University, Global Campus, South Korea
e-mail: hassan@khu.ac.kr; johnhuh@khu.ac.kr
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 7, © Springer Science+Business Media, LLC 2011
169

170
M.M. Hassan and E.-N. Huh
[6, 7, 9, 23], thus allowing providers to revel on increased scale and reach than that
is achievable individually. In this chapter, we argue that data intensive CPs can form
dynamic federation with other CPs to gain economies of scale and an enlargement of
their virtual machine (VM) infrastructure capabilities (i.e., storage and processing
demands) to meet the requirements of data intensive applications.
Nevertheless, there is a need to develop effective dynamic resource management
mechanism for data intensive CPs to model the economics of VM resource
supplying in federating environment. Such a mechanism needs to be fair and
ensures mutual beneﬁts so that the other collaborating CPs are encouraged to join
the federation. In this dynamic federation environment, there are two types of
participants: a buyer data intensive CP called primary CP (pCP) and a seller or
cooperating CP called cCP. We consider the scenario for IaaS data intensive pCPs.
The pCPs initiate a dynamic federation platform and can pay cCPs for VM resource
consumption to complete jobs. We assume that all the cCPS are rational (self-
interested and welfare maximizing) and they will refuse to offer their VM resources
to each other unless they can recover their costs.
The market-based approaches [2,13] are recently proposed for cloud computing
for building a distributed resource allocation mechanism since it provides economic
incentives. However, the introduction of economic incentives tends to induce
rational and revenue-maximizing providers to alter their bids or prices in order to
increase their revenue. In such a setting where we have to ensure truth-elicitation,
game theory becomes immediately applicable [15]. Game-theory based distributed
resource allocation mechanisms have received a considerable amount of attention
in different areas like grid computing [8, 15, 18], P2P network [20], and recently
in cloud computing area [4, 17, 25, 27]. However, most of these works in cloud
computing area mainly focus on optimal resource allocation using game theory in
a single provider scenario except [3] where the authors focus on using a coalition
game theory to ﬁnd the proﬁt share and the notion of diversity in a existing static
cloud federation (PlanetLab) scenario.
Besides, in computational grids environment, He et al. [15] proposed a coalition
formation-based resource allocation mechanism using game theory. They used
automated multi-party explicit negotiation for resource allocation. However, they
did not evaluate the social welfare among the agents which reﬂects the level
of satisfaction of the participants in the coalition. To create dynamic virtual
organizations in Grids, Carroll et al. [8] used a coalition game theory based resource
composition framework among self interested grid service providers. The authors
tried to compute the worth of each coalition for an agent that maximizes its proﬁt.
However, a service provider cannot possibly compute the worth of all coalitions
and thus has limited information to base its decision. Khan et al. [18] proposed
non-cooperative, semi-cooperative and cooperative games of resource allocation for
computational grid environment. However, the proposed cooperative method has
high computational complexity and difﬁcult to implement. Contrary to these works,
our approach targets IaaS data intensive CPs and not the Grid. Hence, there is still
need of a practicable solution for resource management for data intensive clouds to
effectively encourages other CPs to participate in federating platform.

7
Resource Management for Data Intensive Clouds Through Dynamic Federation...
171
In this chapter, we analyze game theory based distributed resource management
mechanisms for data intensive IaaS CPS in a federation environment. Our contribu-
tions are summarized as follows:
•
We study the design of distributed resource allocation games to model the eco-
nomics of the VM resource supplying for data intensive CPs (pCPs) in a dynamic
federation environment. The games motivate different cCPs with heterogeneous
cost function to cooperate with pCPs. The objective of a distributed resource
allocation game is to maximize the utility of the system, deﬁned as the sum of
the buyer data intensive pCPs’ utilities, without exceeding the resource capacity
and expense price. We use price-based resource allocation strategies in the game.
•
We propose two resource allocation games – non-cooperative and cooperative
games to analyze the utility of a pCP in a dynamic federation platform. Both
centralized and distributed algorithms are also presented for the games to achieve
optimal solutions. These algorithms have low overhead and robust performance
against dynamic pricing and stability. Various simulations were conducted to
measure the effectiveness of these algorithms.
•
The simulation results demonstrate that in non-cooperative environment, the
optimal aggregated beneﬁt of the dynamic federation may not be guaranteed.
So under the cooperative resource allocation game, cCPs have strong motivation
to participate in a dynamic federation with pCPs. Also, this game enables the
best set of cCPs to supply VM resources in the federation.
The paper is organized as follows: In Sect. 2, we present the overall system
architecture of a dynamic federation platform, and the mathematical problem
formulation. In Sect. 3, we describe the two resource allocation games in detail.
In Sect. 4, we evaluate the effectiveness of the proposed resource allocation games
in a dynamic federation environment and ﬁnally Sect. 5 concludes the paper.
2
System Model and Problem Formulation
In this section, ﬁrst we present the overall system architecture of a dynamic
federation platform for data intensive IaaS CPs. Then we describe our mathematical
problem formulation of resource allocation games.
2.1
Overview of Dynamic Federation Platform for Data
Intensive Clouds
Let us provide an overview of dynamic cloud federation platform for data intensive
IaaS CPs in the light of current works as described in [9, 10]. We assume that
CPs are rational (self-interested and well fare maximizing) and make their own

172
M.M. Hassan and E.-N. Huh
Fig. 7.1 A formed dynamic federation platform for a data intensive pCP with cCPs
decisions according to their budgets, capabilities, goals and local knowledge. The
formation of a dynamic federation platform is initiated by a IaaS data intensive
CP, called pCP, when it realizes that at certain time in future it cannot continue
providing services to some data intensive applications requiring both time varying
resource needs and real-time performance demands. Consequently, it transparently
and dynamically enlarges its own virtualization infrastructure capabilities by asking
further VM resources to other collaborating clouds, called cCPs, for a speciﬁc period
of time.
The Fig. 7.1 shows a formed dynamic cloud federation platform with a pCP. We
can see that the pCP is dynamically collaborating with other CPs, that is, cCP’s,

7
Resource Management for Data Intensive Clouds Through Dynamic Federation...
173
to enlarge its capability when it realizes that its virtualization infrastructure would
be unable to continue providing services to its clients. Thus a dynamic federation
platform allows data intensive IaaS pCPs to cooperatively achieve greater scale and
reach, as well as service quality and performance, than they could otherwise attain
individually. Its signiﬁcance can be better understood by the following example
applications:
•
Emerging high bandwidth sensor systems like weather radars, astronomical radio
telescopes, networks of pan-tilt-zoom video cameras being deployed across both
the southern and northern border by the U.S. border patrol etc. deployed on a
IaaS/PaaS CP’s platform produce streaming data that require both time varying
resource needs and real-time performance demands [16]. Since sensors collect
data in the real world, their needs are driven primarily by unpredictable real
world events. For instance, weather radars may produce more data and require
higher-bandwidth during intense thunderstorms than during periods of calm.
Likewise, pan-tilt-zoom cameras may require low latency network connections
during times of intense border activity, but may not require network resources
at all when performing conventional monitoring functions. In addition, modern
high-energy physics experiments deployed on cloud platform, such as DZero1,
typically generate more than one TeraByte of data per day [24]. Thus these data
intensive applications need a huge amount of compute and storage resources from
a IaaS CP dynamically on demand basis in real time. However, the IaaS/PaaS
CP that hosts these applications may fail provisioning of resources for these
applications for a certain period of time due to load spikes (cloud bursting). This
result in an SLA violation and end up incurring additional costs for the IaaS/PaaS
CP. This necessitates building mechanisms for dynamic federation of IaaS CPs
for seamless provisioning of VM resources.
•
Data intensive applications like Social networks (e.g., Face book, MySpace etc.)
deployed on a IaaS/PaaS CP serve dynamic content to millions of users, whose
access and interaction patterns are hard to predict. In addition, the dynamic
creation of new plug-ins by independent developers may require additional
resources which may not be provided by the hosting cloud provider at certain
period in time. In this situation, load spikes (cloud bursting) can take place
at different locations at any time, for instance, whenever new system features
become popular or a new plug-in application is deployed. Thus result in an SLA
violation and end up incurring additional costs for the CP [7]. So there is a need of
building dynamic federation IaaS CPs with other CPs for seamless provisioning
of VM resources.
•
Other example of data intensive applications that need dynamic cloud federation
are massively multiplayer online role-playing games (MMORPGs). World of
Warcraft (http://www.worldofwarcraft.com/cataclysm/), for example, currently
has 11.5 million subscribers; each of whom designs an avatar and interacts with
other subscribers in an online universe. Second Life (http://secondlife.com/) is
an even more interesting example of a social space that can be created through
dynamic Cloud collaboration. Any of the 15 million users can build virtual

174
M.M. Hassan and E.-N. Huh
Table 7.1 Summary of notations
Parameters
Description
Rt
VM
Total VM resources supplied in a dynamic federation platform in
period t
P D fP t
i ji D 1:::mg
Total Number of Cloud providers present in period t
rt
i
VM resource supplied by provider i in period t
QC t
i
Total VM capacity of provider i in period t
Cost.rt
i /
Cost of supplying rt
i unit of VM resource by provider i in period t
M t
i
Cost of the ﬁrst unit of VM resource by provider i in period t
˛i
Learning factor of provider i where 0:75 < ˛t
i < 0:9
!
Parameter deﬁning the rate of revenue in a dynamic federation
platform
Revt
cCP.Rt
VM/
Revenue function estimated by a pCP for cCPs in period t
Prt
cCP.Rt
VM/
Price per hour given to cCPs by a pCP for each unit of VM resource
supplied in period t
Util.rt
i /
Utility of any cCP i by providing rt
i unit of VM resources in
period t
objects, own virtual land, buy and sell virtual goods, attend virtual concerts,
bars, weddings, and churches, and communicate with any other member of
the virtual world. These MMORPGs certainly require huge amount of Cloud
resources/services which cannot be provided by a single cloud provider at that
time. Thus this necessitates building mechanisms for seamless collaboration of
different CPs supporting dynamic scaling of resources across multiple domains
in order to meet QoS targets of MMORPGs customers.
However, the economics of VM resource supplying that encourage cCPs to
join in a dynamic federation platform need to be addressed. We study different
game theory based distributed VM resource allocation mechanisms for the cCPs
in a dynamic federation platform. From the pCP’s prospective, it wants a higher
contribution of VM resources because it can support more clients at a lower
operating system cost. It deﬁnes a price function, which speciﬁes how much price
per hour should be given to cCPs for each unit of VM resource supplied in a
federation platform. Ideally, the price should match the demand and supply of VM
resource such that social optimality is achieved.
2.2
Mathematical Model Formulation in a Dynamic
Federation Platform
In a dynamic federation environment, there are two types of player, pCP and cCP.
The notations used in the paper are summarized in Table 7.1. Consider a pCP
requires VM resources with speciﬁc QoS requirements during a certain period t
to continue providing services to its clients. A set of cCPs P D fP t
i ji D 1:::mg is
available during that period which can form a dynamic federation platform with pCP

7
Resource Management for Data Intensive Clouds Through Dynamic Federation...
175
by providing VM resources with required QoS. Let Rt
VM be the total VM resources
supplied in a dynamic federation platform in period t, rt
i be the units of VM resource
supplied by a CP i in period t, and QC t
i be its maximum capacity in that period. The
sum of the VM resources supplied to any pCP should be
m
P
iD1
rt
i D Rt
VM . We know
that the pCP can buy these VMs cheaper than the revenue it obtains for selling them
to clients [12]. Now, we present various deﬁnitions used for the mathematic model
formulation.
Deﬁnition 1 (Proﬁt). Let Revt
cCP .Rt
VM/ be the revenue a pCP can provide for
getting Rt
VM resources from cCPs at certain period t and Prt
cCP .Rt
VM / be the price
per hour set by a pCP to cCPs for each unit of VM resource supplied in period t.
Then, the expected proﬁt of a pCP obtained from executing tasks on Rt
VM resources
from cCPs is deﬁned as follows:
Proﬁtt
pCP .Rt
VM / D Revt
cCP .Rt
VM /  Rt
VM 
t
Pr
cCP.Rt
VM /
(7.1)
A shown in Eq. 7.1, the total proﬁt is determined by the total VM resource Rt
VM
supplied in the dynamic federation platform and a pCP can only inﬂuence the value
of Rt
VM by setting a proper price function Prt
cCP .Rt
VM /. So the pCP can strategically
deﬁne the price function Prt
cCP .Rt
VM/ such a way that it can motivate available cCPs
for contributing resources in a dynamic federation platform as well as get proﬁt.
Deﬁnition 2 (Cost Function). Let M t
i be the production cost of the ﬁrst unit of
VM resource for any provider i during a certain period tand ˛i is its learning factor.
Then, in order to supply rt
i units of VM resource, any cCP i has to pay Cost.ri t/,
which is deﬁned as follows [1]:
Cost.rt
i / D M t
i  rt1Clog2˛
i
1 C log2˛
(7.2)
s:t: 0  rt
i  QC t
i
(7.3)
The cost function can be heterogeneous for different cCPs based on ˛ and M.
The higher the value of ˛ and M, the higher the production cost for a provider. It
has been reported [1] that for a typical CP, as the total number of servers in house
doubles, the marginal cost of deploying and maintaining each server decreases 10–
25%, thus the learning factors are typically within the range (0.75, 0.9).
Deﬁnition 3 (Revenue Function). Let ! be the increasing rate of revenue. Now,
a pCP can estimate the revenue function of the dynamic federation platform as
follows:
Revt
cCP .Rt
VM / D
M 

1  eRt
VM !
!
(7.4)

176
M.M. Hassan and E.-N. Huh
The function Revt
cCP .Rt
VM / is a non-decreasing and concave function which
means that the more resources supplied by cCPs, the higher the revenue. However,
the marginal revenue is decreasing as the resource increased. The product of the
price and the total available VM unit Rt
VM should not exceed the corresponding
revenue. The pCP has its freedom to decide how much revenue is to be provided to
the cCPs by varying the parameters M and !.
Deﬁnition 4 (Price Function). Based on the revenue function of Eq. 7.4, the price
per hour given to cCPs by a pCP for each unit of VM resource supplied in period t
is deﬁned as follows:
t
Pr
cCP.Rt
VM / D M  eRt
VM !
(7.5)
s:t: Rt
VM > 0
(7.6)
The function Prt
cCP .Rt
VM / is the marginal gain of the dynamic federation platform.
When the amount of VM resource increase, the price per hour of each unit of
VM resource decrease. Also this function represents the proportional fairness of
contributing resources by cCPs.
3
Resource Allocation Games in a Dynamic Federation
Platform
The objective of a resource allocation game in a dynamic federation platform
is to dynamically allocate VM resources to a pCP from the available cCPs on
demand basis so that all of the providers are satisﬁed. We study two resource
allocation games in a dynamic federation platform. These games are repeated and
asynchronous games. In one game, cCPs can supply Rt
VM resources to a pCP
in non cooperative manner and in the other game they supply VM resources in
cooperative manner. In both games, each cCP can make or change its decision
about the amount of VM resources at the beginning of each round. To be realistic
and scalable, we assume imperfect knowledge of each cCP, meaning that the cCP
only knows about the VM resources supplied to the dynamic federation platform by
other cCPs and the price function Prt
cCP .Rt
VM /. The pCP publicizes the current price
function Prt
cCP .Rt
VM / and the total amount of VM resources such that other cCPs
can obtain the information easily. Based on these information, a cCP can update its
own strategy in each move so to maximize its utility.
3.1
Non-Cooperative Resource Allocation Game
In the non-cooperative game of resource allocation, the cCPs make decision to
maximize their own utilities regardless of other cCPs. They choose rt
i based on
the public information: the aggregated VM resource Rt
VM and the price function
Prt
cCP .Rt
VM /. Formally, cCP i needs to perform:

7
Resource Management for Data Intensive Clouds Through Dynamic Federation...
177
Max Util.rt
i / D rt
i 
t
Pr
cCP.Rt
VM /  Cost.rt
i /
(7.7)
s:t:
0  rt
i  QC t
i
(7.8)
The Nash equilibrium for pure strategy is obtained as the solution of this game (that
is, the amount of VM resource rt
i
offered by each cCP to a pCP). If the net utility
of a cCP is less than or equal to zero, it will not participate in the game, and it will
be removed from the list of cCPs. Note that Rt
VM is implicitly depends on rt
i . If the
value of rt
i is changed, the value of Rt
VM , as well as Prt
cCP .Rt
VM /, will be changed
accordingly. Thus, in the optimization, the value of Rt
VM would be better presented
in terms of rt
i .
Let rt
i be the amount of VM resource collectively supplied by the cCPs except
cCP i, then rt
i D Rt
0
VM  rt
0
i , where Rt
0
VM and rt
0
i
are the total amount of VM
resource and the amount of VM resource supplied by cCP i respectively in the
previous round. The equivalent optimization problem of Eq. 7.7 can be re-written
by using Eqs. 7.5 and 7.2 as follows:
Max Util.rt
i / D rt
i  M  e.rt
i Crt
i/!  M t
i  rt1Clog2˛i
i
1 C log2˛i
(7.9)
s:t:
0  rt
i  QC t
i
(7.10)
Now to obtain the Nash equilibrium (rt
i ) of the game, we take the derivative of
Eq. 7.9 with respect to rt
i as follows:
Util0.rt
i / D M  e.rt
i Crt
i /!  rt
i  M  !  e.rt
i Crt
i /!  M t
i  rtlog2˛i
i
D 0
(7.11)
Now from Eq. 7.11, it is difﬁcult to ﬁnd the close form solution of rt
i . We can use
direct search method like pattern search method [19] with multiple initial guesses to
the optimal VM quantity rt
i . The procedure is as follows:
•
In the pattern search method, an initial step size f is chosen and the search is
initiated from a starting point rt
i . The method involves the steps of exploration
and pattern search.
•
In the exploration step, it tries to probe the value of the utility by increasing or
decreasing the rt
i . Let rt
0
i
D rt
i , the objective function is evaluated at rt
0
i
Cf .
If the utility value increases, then rt
0
i
is updated to rt
0
i
C f . Otherwise,the
function is evaluated at rt
0
i
 f . If the utility value increases, rt
0
i
is updated to
rt
0
i
f . In case both of them fail in the test, the original value of rt
0
i
is retained.
An exploration is said to be successful if the function valued at rt
0
i
is higher than
rt
i
by a predetermined amount.
•
The pattern search algorithm starts from a quantity rt
i . The exploration step is
made in rt
i . If the exploration fail, the step size is reduced by a factor of j, that
is, f  j  f . Otherwise, a new base point of rt
i
is established according to the
exploration. The search continues until the VM resource quantity rt
i
converged.

178
M.M. Hassan and E.-N. Huh
However, for a deﬁned price function, the Nash equilibrium of this non-
cooperative game may not be unique as the order of move will inﬂuence the
equilibrium point. The game may converge to different equilibrium, depending on
the sequence of move of the cCPs. So there is no guarantee that the equilibrium is
socially optimal. Under the non-cooperative situation, each cCP seems to optimize
their individual beneﬁt, but actually the system-wide behavior does not reﬂect
the optimization of any objective. So the non-cooperative game is not desired in
maximizing a pCP’s proﬁt because it does not lead to a unique Nash equilibrium.
3.2
Cooperative Resource Allocation Game
In the cooperative game of resource allocation, we jointly consider the beneﬁts
of both the pCP and cCPs. Being a pCP, the objective is to maximize its total
proﬁt in the dynamic federation platform. However, for a deﬁned price function
Prt
cCP .Rt
VM /, the system may converge to different equilibrium, depending on the
sequence of move of the cCPs. There is no guarantee for the pCP to set a particular
marginal pricing function that leads to a desirable outcome which maximizes its
total proﬁt. So a pCP can set a initial constant price and can choose a proper price
P rt
cCP to maximize its total proﬁt.
For cCPs, they try to maximize their beneﬁts based on rt
i and the initial constant
price P rt
cCP . So the objective of any cCP i is deﬁned as follows:
Max Util.rt
i / D rt
i  P rt
cCP  Cost.rt
i /
(7.12)
s:t:
0  rt
i  QC t
i
(7.13)
Since there is a boundary constraint for the variable rt
i , that is,
0  rt
i  QC t
i ,
the problem in Eq. 7.12 can be formulated as a constrained optimization, which can
be solved by the method of Lagrangian Multiplier.
L D Util.rt
i / 
m
X
iD1
rt
i C
m
X
iD1
'.rt
i  QC t
i /
(7.14)
where  and ' are the Lagrangian constant. The Karush Kuhn Tucker (KKT)
condition is as follows:
@L
@rt
i
D Util0.rt
i /   C ' D 0; i D 1; :::::; m
(7.15)
Util0.rt
i / D M t
i  rtlog2˛i
i
D P rt
cCP
(7.16)
rt
i D
 M t
i
P rt
cCP

1
log2˛i
(7.17)

7
Resource Management for Data Intensive Clouds Through Dynamic Federation...
179
By solving rt
i of Eq. 7.16, we can obtain the solution of a cCP’s optimization
problem of choosing rt
i
as follows:
rt
i
D
8
<
:
rt
i ;
0  rt
i  QC t
i ;
QC t
i ; rt
i > QC t
i ;
0;
rt
i  0
(7.18)
Thus, given the value of proper price Prt
cCP , a pCP can predict the total VM
resource RVM contributed to the system, that is
Rt
VM D
m
X
iD1
rt
i
(7.19)
Now, let’s consider the optimization problem of a pCP. If the pCP knows the
parameters M and ˛ of all the cCPs, it can formulate its own maximization, which
aims at maximizing the total proﬁt with respect to Rt
VM
Max Proﬁtt
pCP .Rt
VM / D Revt
cCP .Rt
VM /  Rt
VM 
t
Pr
cCP
(7.20)
s:t:
t
Pr
cCP  0
(7.21)
0  rt
i  QC t
i
(7.22)
Since the total amount of VM resource Rt
VM solely depends on the value of
price Prt
cCP through Eqs. 7.18 and 7.19, one can rewrite the objective function by
substituting Rt
VM in terms of Prt
cCP as follows:
Max Pr oﬁtt
pCP

t
Pr
cCP

D Revt
cCP
0
@
m
X
iD1
 M t
i
Prt
cCP

1
log2˛i
1
A 
0
@
m
X
iD1
 M t
i
Prt
cCP

1
log2˛i
1
A 
t
Pr
cCP
(7.23)
D
M 
0
B@1  e

m
P
iD1

Mt
i
Prt
cCP

1
log2˛i !
1
CA
!

0
@
m
X
iD1
 M t
i
Prt
cCP

1
log2˛i
1
A 
t
Pr
cCP
(7.24)
s:t:
0  rt
i  QC t
i
(7.25)

180
M.M. Hassan and E.-N. Huh
Although it is difﬁcult to ﬁnd the close-form solution of Prt
cCP for Eq. 7.24,
we can solve this optimization efﬁciently using numerical method, for example,
Newton method is applied to solve the value of the optimal price Prt
cCP . Once the
pCP ﬁnds the optimal price, it can calculate the value of all rt
i
using Eq. 7.18.
However, it may happen that the boundary constraints in Eq. 7.25 may violate and
in that case the problem becomes more complicated. Still we can ﬁnd the solution
mathematically using Lagrangian multiplier. Without the constraints it can be shown
that the objective function in Eq. 7.24 is a concave function. So there exists a unique
solution that satisﬁes the KKT-conditions of Eq.7.24 as follows:
L D Pr oﬁtt
pCP .
t
Pr
cCP/ 
m
X
iD1
rt
i C
m
X
iD1
'.rt
i  QC t
i /
(7.26)
@L
@ Prt
cCP
D
@
h
Pr oﬁtt
pCP

Prt
cCP
	i
@ Prt
cCP

@
 m
P
iD1
rt
i

@ Prt
cCP
C
@
 m
P
iD1
'.rt
i  QC t
i /

@ Prt
cCP
D 0
(7.27)
 > 0;
' > 0; rt
i D 0; '.rt
i  QC t
i / D 0; 0 6 rt
i 6 QC t
i ; i D 1; :::::; m
(7.28)
From the KKT conditions, if  D 0; and ' D 0, then all the rt
i lie between
[0; QC t
i ]. When the boundary constraints are violated ( ¤ 0; or ' ¤ 0), the value
of rt
i are forced to be the boundary value [either 0 or QC t
i ]. If rt
i is less than or equal
to zero for certain cCP i, we are sure that the cCP is not eligible for contributing
as the cost of supplying the VM is comparatively high. Similarly, if rt
i is greater
than QC t
i , we are sure that the cCP has optimal value of rt
i . This cCP should provide
as much VM resource as possible since the cost is comparatively low. Thus, we
can eliminate some cCPs, whose value of rt
i is known already, from the problem
formulation and resolve the rt
i for the remaining cCPs.
Until now, we assume the pCP knows the characteristic of the cost function of
each cCP such that it can determine the behavior of the cCPs, and it can construct
its own objective function. However, in distributed environment, the pCP can only
observe the action of each cCP by setting a probing price. The cCPs choose the best
rt
i to maximize their net utility. The pCP keeps adjusting the price gradually until
a desirable proﬁt is obtained. Now we present a distributed algorithm to ﬁnd the
optimal value of Prt
cCP .
The algorithm is described step by step as follows:
Step 1: Initialize the probing price Prt
cCP D 0:1 and frt
i gP
iD1 D 0.
Step 2: Send Prt
cCP D 0:1 to all cCPs and receive corresponding Rt
VM D
PP
iD1
rt
i .
Step 3: if
Proﬁtt
pCP .Rt
VM/ D Revt
cCP .Rt
VM /  Rt
VM 
t
Pr
cCP is maximized or
@
h
Pr oﬁtt
pCP .Prt
cCP /
i
@ Prt
cCP
D 0, the optimal Prt
cCP is found and break. Otherwise update
Prt
cCP based on old price and the percentage change of the net proﬁt.

7
Resource Management for Data Intensive Clouds Through Dynamic Federation...
181
Table 7.2 Parameters used
in the resource allocation
games for cCPs
Production cost of
ﬁrst unit per h
Learning factor
Total capacity
cCPs i
M t
i
˛i
QC t
i
1
2.8
0.79
300
2
2.7
0.84
302
3
2.0
0.83
305
4
2.3
0.80
304
5
2.9
0.78
303
6
2.4
0.78
301
Step 4: If 0  rt
i  QC t
i for all i 2 P , then break.
Step 5: Now for some cCP, i 2 P , If rt
i  0, remove those cCPs from the list of P .
Also for some cCP, i 2 P , If rt
i  QC t
i , set rt
i D QC t
i .
4
Simulation and Discussion
In this section, we focus on evaluating the effectiveness of the proposed resource
allocation games in a dynamic federation platform. We focus on the case of one
data intensive pCP and six cCPs. Both the non-cooperative and cooperative games,
the performance measures are the social welfare, total proﬁt, cost effectiveness and
scalability. The experimental parameters are shown in Table 7.2. Using Amazon
(EC2) as the example, we assume the range of production cost of ﬁrst unit varies
from 2$/h to 3$/h and service availability for all provider is 99.95% [1]. For
simplicity, we consider each cCP has almost same amount of VM resource capacity
in period t. Also the evaluation of two resource allocation games were done based
on mathematical simulation, which was implemented in MATLAB 7.0.
4.1
Convergence of the Resource Allocation Games
Convergence is a basic requirement that the resource allocated by the cCPs should
converge in each game. In all the experiments, we consider ! D 0:01, M D 3 and
 D 0:3. We analyze the behavior of each game based on the VM resource supplied
at the steady state. Figures 7.2 and 7.3 depict the quantity of VM resource supplied
by the six cCPs in each iteration of two games. It demonstrates that the resource
allocation games converge to a steady state after a number of iterations. As shown
in the graph, the non-cooperative game converges fast, while it takes more iterations
for the cooperative game to stabilize. However, the converging speed does not affect
the performance of the games.

182
M.M. Hassan and E.-N. Huh
Fig. 7.2 VM resource supplied by each cCP in non-cooperative game
Fig. 7.3 VM resource supplied by each cCP in cooperative game

7
Resource Management for Data Intensive Clouds Through Dynamic Federation...
183
Fig. 7.4 Total proﬁt in each resource allocation game
4.2
Performance Analysis of Resource Allocation Games
In this subsection, we ﬁrst evaluate the total proﬁt in the proposed resource
allocation games. Figure 7.4 plots the total proﬁt of the pCP in the two resource
allocation games. We can see that the cooperative game generated the highest total
proﬁt (204) as compared to the non-cooperative game (140).
Now we evaluate the social welfare in the resource allocation games. Figures 7.5
and 7.6 demonstrate the individual utility of the cCPs under non-cooperative and
cooperative games. Note that only the cCPs having positive utility suppled VM
resources to the dynamic federation platform.
Figure 7.7 shows the social welfare in the two resource allocation games. It can
be seen that the social welfare achieved by the non-cooperative game is much higher
(135) as compared to the cooperative game (81). The reason is that the cooperative
game is designed to maximize the total proﬁt in a dynamic federation platform by
trading off the social welfare. Now the key discussion here is which approach, the
cooperative or the non-cooperative is better. To compare the performance of these
games, we can use the total utility achieved in a dynamic federation platform that
is the sum of the total proﬁt and social welfare. For this, We evaluate the total
utility in the two games having different revenue function ! as shown in Fig. 7.8.
Note that larger the !, the higher the revenue is for the same quantity of VM
resource. It can be seen that the cooperative game performs well as compared to the

184
M.M. Hassan and E.-N. Huh
Fig. 7.5 Individual utility of cCPs in non-cooperative game
Fig. 7.6 Individual utility of cCPs in cooperative game

7
Resource Management for Data Intensive Clouds Through Dynamic Federation...
185
Fig. 7.7 Social welfare of cCPs in each resource allocation game
Fig. 7.8 Total utility in a dynamic federation platform for each resource allocation game under
different revenue function

186
M.M. Hassan and E.-N. Huh
Fig. 7.9 Performance of two resource allocation games in terms of total utility with different
number of cCPs
non-cooperative game. Also the cooperative game is cost effective as few low-
cost cCPs provide more VM resources (see Fig. 7.3). Hence, we conclude that the
cooperative game provides a cost-effective resource supply to a dynamic federation
platform and thus admits the best set of cCPs to participate.
Also to evaluate the effect of federation system size (scalability) in two games,
we vary the number of cCPs in the HDCF system from 6 to 24 for ! value 0.01.
Note that the small ! implies more quantity of VM resource is required to obtain
the same amount of revenue. And large number of cCPs means they can supply
VM resource with less cost. The result is shown in Fig. 7.9. We can see that in
cooperative game the total utility increases with the number of cCPs as compared to
the non-cooperative game. Obviously, large cooperating system can provide a better
choice of VM allocation (in terms of operational cost) among the cCPs. However,
in steady state, only the cCPs that provide cost-effective VMs are remained in
cooperative game.
5
Conclusions
In this paper, we study game theory based distributed resource management
mechanisms for data intensive IaaS CPS in a federation environment. We propose
two resource allocation games – non-cooperative and cooperative. Different data

7
Resource Management for Data Intensive Clouds Through Dynamic Federation...
187
intensive IaaS CPs (pCPs) interact with different cCPs with heterogeneous cost
function in these games. It is shown that desirable outcome (e.g., total utility,
cost effectiveness etc.) cannot be achieved under a non-cooperative environment.
Both centralized and distributed algorithms are presented to ﬁnd optimal solutions.
Also we carried out extensive simulations to measure the effectiveness of these
algorithms in a dynamic federation platform. Under the cooperative resource
allocation game, the cCPs have a strong motivation to contribute VM resources
to the pCPs. Also, this game is cost-effective and scalable as only the collaborators
with low-cost participate in a dynamic federation platform with pCPs. In future,
we will study the performances of these games in a simulated environment where
hundreds of clouds will dynamically join and leave the federation.
References
1. Amit, G., Xia, C.H.: Learning Curves and Stochastic Models for Pricing and Provisioning
Cloud Computing Services. Service Science 3, 99–109 (2011)
2. An, B., Lesser, V., Irwin, D., Zink, M.: Automated negotiation with decommitment for dynamic
resource allocation in cloud computing. In: Proceedings of the 9th International Conference on
Autonomous Agents and Multiagent Systems: volume 1 - Volume 1, AAMAS ’10, pp. 981–988
(2010)
3. Antoniadis, P., Fdida, S., Friedman, T., Misra, V.: Federation of virtualized infrastructures:
sharing the value of diversity. In: Proceedings of the 6th International COnference, Co-NEXT
’10, pp. 12:1–12:12. ACM (2010)
4. Ardagna, D., Panicucci, B., Passacantando, M.: A game theoretic formulation of the service
provisioning problem in cloud systems. In: Proceedings of the 20th international conference
on World wide web, WWW ’11, pp. 177–186 (2011)
5. Armbrust, M., Fox, A., Grifﬁth, R., Joseph, A.D., Katz, R.H., Konwinski, A., Lee, G.,
Patterson, D.A., Rabkin, A., Stoica, I., Zaharia, M.: Above the clouds: A berkeley view of cloud
computing. Tech. Rep. UCB/EECS-2009-28, EECS Department, University of California,
Berkeley (2009)
6. Bittman, T.: The evolution of the cloud computing market. Gartner Blog Network, http://
blogs.gartner.com/thomasbittman/2008/11/03/theevolution-of-the-cloud-computing-market/
(November, 2008)
7. Buyya, R., Ranjan, R., Calheiros, R.: Intercloud: Utility-oriented federation of cloud com-
puting environments for scaling of application services. In: Algorithms and Architectures for
Parallel Processing, Lecture Notes in Computer Science, vol. 6081, pp. 13–31 (2010)
8. Carroll, T.E., Grosu, D.: Formation of virtual organizations in grids: a game-theoretic approach.
Concurr. Comput. : Pract. Exper. 22, 1972–1989 (2010)
9. Celesti, A., Tusa, F., Villari, M., Puliaﬁto, A.: How to enhance cloud architectures to enable
cross-federation. Cloud Computing, IEEE International Conference on 0, 337–345 (2010)
10. Celesti, A., Tusa, F., Villari, M., Puliaﬁto, A.: Three-phase cross-cloud federation model: The
cloud sso authentication. Advances in Future Internet, International Conference on 0, 94–101
(2010)
11. Chiba, T., den Burger, M., Kielmann, T., Matsuoka, S.: Dynamic load-balanced multicast for
data-intensive applications on clouds. In: Cluster, Cloud and Grid Computing (CCGrid), 2010
10th IEEE/ACM International Conference on, pp. 5 –14 (2010)
12. Goiri, I., Guitart, J., Torres, J.: Characterizing cloud federation for enhancing providers’ proﬁt.
Cloud Computing, IEEE International Conference on 0, 123–130 (2010)

188
M.M. Hassan and E.-N. Huh
13. Gomes, E.R., Vo, Q.B., Kowalczyk, R.: Pure exchange markets for resource sharing in
federated clouds. Concurrency and Computation: Practice and Experience pp. n/a–n/a (2010).
10.1002/cpe.1659. http://dx.doi.org/10.1002/cpe.1659
14. Grossman, R.L., Gu, Y.: On the varieties of clouds for data intensive computing. IEEE Data
Eng. Bull. 32(1), 44–50 (2009)
15. He, L., Ioerger, T.R.: Forming resource-sharing coalitions: a distributed resource allocation
mechanism for self-interested agents in computational grids. In: Proceedings of the 2005 ACM
symposium on Applied computing, SAC ’05, pp. 84–91 (2005)
16. Irwin, D., Shenoy, P., Cecchet, E., Zink, M.: Resource management in data-intensive clouds:
Opportunities and challenges. In: Local and Metropolitan Area Networks (LANMAN), 2010
17th IEEE Workshop on, pp. 1 –6 (2010). 10.1109/LANMAN.2010.5507156
17. Jalaparti, V., Nguyen, G.D., Gupta, I., Caesar, M.: Cloud Resource Allocation Games.
Technical Report, University of Illinois, http://hdl.handle.net/2142/17427 (Dec, 2010)
18. Khan, S.U., Ahmad, I.: Non-cooperative, semi-cooperative, and cooperative games-based grid
resource allocation. In: Proceedings of the 20th international conference on Parallel and
distributed processing, IPDPS’ 06, pp. 121–121 (2006)
19. Kolda, T.G., Lewis, R.M., Torczon, V.: Optimization by direct search: New perspectives on
some classical and modern methods. SIAM Review 45, 385–482 (2003)
20. Kumar, C., Altinkemer, K., De, P.: A mechanism for pricing and resource allocation in peer-
to-peer networks. Electron. Commer. Rec. Appl. 10, 26–37 (2011)
21. Liu, H., Orban, D.: Gridbatch: Cloud computing for large-scale data-intensive batch appli-
cations. In: Cluster Computing and the Grid, 2008. CCGRID ’08. 8th IEEE International
Symposium on, pp. 295 –305 (2008). 10.1109/CCGRID.2008.30
22. Middleton, A.M.: Data-intensive technologies for cloud computing. Chapter 5, Handbook of
Cloud Computing (2010)
23. Rochwerger, B., Breitgand: The reservoir model and architecture for open federated cloud
computing. IBM J. Res. Dev. 53(4), 535–545 (2009)
24. Sakr, S., Liu, A., Batista, D., Alomari, M.: A survey of large scale data management
approaches in cloud environments. Communications Surveys Tutorials, IEEE PP(99), 1–26
(2011). 10.1109/SURV.2011.032211.00087
25. Teng, F., Magouls, F.: A new game theoretical resource allocation algorithm for cloud
computing. In: Advances in Grid and Pervasive Computing, Lecture Notes in Computer
Science, vol. 6104, pp. 321–330. Springer Berlin / Heidelberg (2010)
26. Vaquero, L.M., Rodero-Merino, L., Caceres, J., Lindner, M.: A break in the clouds:
towards a cloud deﬁnition. SIGCOMM Comput. Commun. Rev. 39, 50–55 (2008).
http://doi.acm.org/10.1145/1496091.1496100. http://doi.acm.org/10.1145/1496091.1496100
27. Wei, G., V., V.A., Yao, Z., Xiong, N.: A game-theoretic method of fair resource allocation for
cloud computing services. J. Supercomput. 54, 252–269 (2010)
28. Williams, A.: Top 5 cloud outages of the past two years: Lessons Learned. http://www.
readwriteweb.com/cloud/2010/02/top-5-cloud-outages-of-the-pas.php (Feb, 2010)

Chapter 8
Salt: Scalable Automated Linking Technology
for Data-Intensive Computing
Anthony M. Middleton and David Alan Bayliss
1
Introduction
One of the most complex tasks in a data processing environment is record
linkage, the data integration process of accurately matching or clustering records
or documents from multiple data sources containing information which refer to the
same entity such as a person or business. The massive amount of data being collected
at many organizations has led to what is now being called the “Big Data” problem
which limits the capability of organizations to process and use their data effectively
and makes the record linkage process even more challenging [3, 13]. New high-
performance data-intensive computing architectures supporting scalable parallel
processing such as Hadoop MapReduce and HPCC allow government, commercial
organizations, and research environments to process massive amounts of data and
solve complex data processing problems including record linkage. A fundamental
challenge of data-intensive computing is developing new algorithms which can
scale to search and process big data [17]. SALT (Scalable Automated Linking
Technology) is new tool which automatically generates code in the ECL language
for the open source HPCC scalable data-intensive computing platform based on a
simple speciﬁcation to address most common data integration tasks including data
proﬁling, data cleansing, data ingest, and record linkage.
SALT incorporates some of the most advanced technology and best practices of
LexisNexis Risk Solutions, a subsidiary of Reed-Elsevier, one of the world’s largest
publishers of information. LexisNexis currently has 30 patents pending related
to record linkage and other technology included in SALT, including innovative
new approaches to approximate string matching (a.k.a. fuzzy matching), automated
calculation of matching weights and thresholds, automated selection of blocking
criteria, automated calculation of best values for ﬁelds in an entity, propagation of
A.M. Middleton () • D.A. Bayliss
LexisNexis, Boca Raton, FL, USA
e-mail: Tony.Middleton@lexisnexis.com; David.Bayliss@LexisNexis.com
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 8, © Springer Science+Business Media, LLC 2011
189

190
A.M. Middleton and D.A. Bayliss
ﬁeld values in entities to increase likelihood of matching, automated calculation
of secondary relationships between entities, automated splitting of entity clusters
to remove bad links, automated cleansing of data to improve match quality, and
automated generation of batch and online applications for entity resolution and
search applications including an online search application including a Uber key
which allows searches on any combination of ﬁeld input data.
SALT is an ECL code generator for use with the open source HPCC platform
for data-intensive computing. SALT is provided as an executable program that runs
from a Windows command prompt or can be executed automatically from the ECL
IDE programmer’s interactive development environment. The input to the SALT
tool is a user-deﬁned speciﬁcation stored as a text ﬁle with a.spc ﬁle extension
which includes declarative statements describing the user input data and process
parameters. The output of SALT is a text ﬁle containing ECL code which can be
imported and executed in an HPCC system environment. The SALT tool can be
used to generate complete applications ready to-execute for data proﬁling, data
hygiene (also called data cleansing, the process of cleaning data), data source
consistency monitoring (checking consistency of data value distributions among
multiple sources of input), data ﬁle delta changes, data ingest, and record linking
and clustering. SALT record linking and clustering capabilities include (1) internal
linking – the batch process of linking records from multiple sources which refer
to the same entity to a unique entity identiﬁer; (2) external linking – also called
entity resolution, the batch process of linking information from an external ﬁle
to a previously linked base or authority ﬁle in order to assign entity identiﬁers to
the external data, or an online process where information entered about an entity
is resolved to a speciﬁc entity identiﬁer, or an online process for searching for
records in an authority ﬁle which best match entered information about an entity;
and (3) remote linking, an online capability that allows SALT record matching to be
incorporated within a custom user application.
This chapter explores the challenges of data integration and record linkage in
a data-intensive computing environment, and describes how the SALT tool can be
used to automatically generate executable code for the complete data integration
process including record linkage. All data examples used in this chapter are ﬁctitious
and do not represent real information on any person, place, or business unless stated
otherwise.
2
Background
2.1
Record Linkage: Deﬁnition, Approaches, and Historical
Perspective
The basic deﬁnition of record linkage or the record matching problem in the
literature is remarkably consistent and is variously described as: the methodology of
bringing together corresponding records from two or more ﬁles or ﬁnding duplicates

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
191
within ﬁles [24]; accurately identifying records corresponding to the same entity
from one or more data sources [12]; to identify records in the same or different data
sources or databases that refer to the same real world entity even if the records do
not match completely [4]; recognizing those records in two ﬁles which represent
identical persons, objects or events (said to be matched) [10]; the task of accurately
labeling record pairs corresponding to the same entity from different sources [16];
to determine if two database records taken from different source databases refer to
the same entity [5]; match and aggregate all records that refer to the same entity
[3]; and the bringing together of information from two records that are believed to
relate to the same entity or to identify duplicate records within a single ﬁle [13].
A common thread within these deﬁnitions is the concept of a real-world entity about
which information is collected or clustered, such as a person, family, or business.
When using SALT, an entity can be a real-world entity like a person or a more
abstract entity like passages from legal documents related to the same legal concept,
essentially any type of object with descriptive attribute data in a structured or semi-
structured format which can be matched to link similar or matching objects. SALT is
data-agnostic, meaning it can be used with any type of data in any format supported
by the ECL programming language.
Record linking has been deﬁned as the task of matching and clustering records
that refer to the same entity in the same or multiple ﬁles. A record contains data
stored in ﬁelds about a speciﬁc entity. Normally an entity has a unique identiﬁer
which is assigned as one of the ﬁelds in the record. The processing techniques for
determining if one or more records refer to the same entity are varied, but can be
grouped into two general categories: rule-based and probabilistic record linkage
which is typically a machine learning approach.
In a rule-based approach to record linking, reasonable matching rules are de-
veloped and reﬁned as common exceptions are recognized. This approach requires
a signiﬁcant amount of testing and tuning of the rules, but offers a very high
level of precision. The primary disadvantages of rule-based approaches is that the
initial development time is substantial and to maintain high precision over time, an
increasing number of special cases and exceptions must be handled, and the number
of rules can become too complex to maintain reliably. Also rule-based approaches
are not as tolerant of inconsistent data which can result in lower recall.
An alternate approach using machine learning methods is the probabilistic record
linkage approach. This typically requires a large calibration set of records to be
hand-labeled as matching or non-matching pairs to be used for training of the
matching algorithm. Statistics are calculated from the agreement of ﬁelds on the
available matching and non-matching pairs of records to determine weights on
each ﬁeld. For some approaches, a value-speciﬁc frequency-based method is used
for calculating weights. This allows values which are rare to be assigned a higher
weight than values which are common [21]. Frequencies can be calculated from all
the available data using a large base reference ﬁle [13]. This method also does not
require a calibration data set [21]. When the record linking process is executed, the
weight for each ﬁeld is added to get a combined score that represents the probability
that the records refer to the same entity. There is usually a tunable threshold above

192
A.M. Middleton and D.A. Bayliss
which the combined score for a pair of records is considered a match, and another
threshold score below which would be a non-match. Between the thresholds a record
pair is a possible match which can be linked or not linked, processed through rules,
or ﬂagged for human intervention. Machine learning approaches are also typically
more tolerant of inconsistent data which results in improved recall.
Record linkage has been an important data processing task for more than 60
years. It was used as early as 1946 by the Bureau of Vital Statistics of the U.S. Public
Health Service and is described by H. L. Dunn as a means to identify and link vital
records about people including births and deaths [9]. The earliest computer-based
work in record linkage using modern probabilistic matching techniques is credited
to Newcombe et al in 1959 who also introduced the concept of using the frequencies
of data values in determining probabilities and matching weights [19]. Fellegi and
Sunter are credited with formalizing a mathematical model for probabilistic record
linkage using a Bayesian decision model. For a more in-depth perspective on prob-
abilistic record linking techniques and the underlying mathematical concepts and
machine learning methods, the reader is referred to [13] and also Chap. 22, Record
Linkage on a High Performance Data-Intensive Computing Platform in this book.
2.2
Record Linkage: Process
Record linkage ﬁts into a general class of data processing known as data integration,
which can be deﬁned as the problem of combining information from multiple
heterogeneous databases [6]. Data integration can include data preparation [4] steps
such as parsing, proﬁling, cleansing, normalization, and parsing and standardization
of the raw input data prior to record linkage to improve the quality of the input
data [13] and to make the data more consistent and comparable [3, 11] (these data
preparation steps are sometimes referred to as ETL or extract, transform, load).
The data preparation steps are followed by the actual record matching or clustering
process which can include probability and weight computation, data ingest of source
data, blocking/searching, weight assignment and record comparison, and weight
aggregation and match decision to determine if records are associated with the same
entity [4,6,7,12,22,23,25]. Figure 8.1 shows the phases typical in a data integration
processing model.
2.2.1
Proﬁling
Data proﬁling or exploratory data analysis [13] is a step usually performed by data
analysts on raw input data to determine the characteristics of the data including
type, statistical, and pattern information as well as ﬁeld population counts. The goal
of proﬁling is to fully understand the characteristics of the data and identify any
bad data or validity issues and any additional cleansing, ﬁltering, or de-duplication
that may be needed before the data is processed further. Data proﬁling can also

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
193
Additional
Data Ingest
Data Sources
Profiling
Parsing
Cleansing
Normalization
Standardization
Data Preparation Processes (ETL)
Matching Weights
and Threshold
Computation
Blocking/
Searching
Weight Assignment
and Record
Comparison
Record Match
Decision
Linking Iterations
Linked Data
File
Record Linkage Processes
Fig. 8.1 Data integration process model
provide information on the changing characteristics of data over time as new data is
linked. Data proﬁling can occur prior to the parsing step if needed to identify raw
data ﬁelds which need to be parsed, but is usually performed once the input data has
been projected into a structured format for the record linkage process.
2.2.2
Parsing
Raw input data ﬁles may need additional parsing as part of the ETL process to
create the individual ﬁelds to be use for matching in the record linkage process. For
example, unstructured raw text documents or semi-structured documents in HTML
or XML format with sections containing text, or where match data is combined into
a single ﬁeld, will need to be further parsed into a structured format. Prior to parsing,
each document or record should be assigned a unique record identiﬁer so that there
is a link to the original data record or document. Only ﬁelds for matching or carried
for inspection or validation purposes need to be included in data records used in the
record linkage process. The ECL language on the HPCC platform includes natural
language processing capability with language statements including PATTERN and
PARSE to facilitate any parsing requirements.
2.2.3
Cleansing
Data cleansing, also called data hygiene, is the process of cleaning the raw input
data so that it can be used effectively in a subsequent process like record linkage.
The cleanliness of data is determined by whether or not a data item is valid within

194
A.M. Middleton and D.A. Bayliss
the constraints speciﬁed for a particular ﬁeld. For example, if a particular data ﬁeld
is constrained to the numeric characters 0–9, then any data item for the ﬁeld which
contains characters other than 0–9 would fail a cleansing validity check for the data
ﬁeld. So 5551212 would be a valid value for a data item, but 555–1212 would
not. Data which has not been cleansed properly can have adverse effects on the
outcome of the record linkage process [25]. Some data issues can be identiﬁed
and corrected through the cleansing process, for others such as misspellings or
character transpositions or deletions, the record linkage process will need to support
comparison methods such as edit-distance, phonetic, and other forms of fuzzy
matching to allow for common typographical errors and then scale match weights
appropriately.
2.2.4
Normalization
Normalization is required when multiple source data ﬁles are utilized in a record
linkage process. Each source ﬁle may contain information about the entity to
be linked, but the ﬁles may have different formats and varying content. The
purpose of normalization is to extract the data that will be used for matching from
each individual source ﬁle, and map this data into a common layout or uniform
presentation [4]. Since each source ﬁle may have information about the entity other
than the ﬁelds needed for record linkage, it is important to include a ﬁeld to identify
the source ﬁle type and a unique record identiﬁer from the source ﬁle in the common
layout. This provides a foreign key to the original data source and also facilitates
adding the entity identiﬁers to the original source ﬁles once the record linkage
process is complete. When the normalization process is completed and all source
ﬁles have been projected to the common layout, the records to be linked can be
merged into a single ﬁle for the record linkage process.
2.2.5
Standardization
Standardization of certain types of input data is essential to achieving high-quality
results in a record linkage process [3]. Names can be represented differently in
different sources or misspelled. Addresses can be represented differently or be
incomplete. Data in any ﬁeld can be missing or have erroneous values. For example,
the data below shows a set of potentially matching records with inconsistent data:
Name
Address
City
St ZIP
Phone
--------- -----------------
------------ -- ---------- ------------
1 MIDDLETON
10 SW 10TH AVE
BOCA RATON
FL 33487-7312 (561)999 9990
2 MIDDLETON
10 10TH SOUTHWEST BOCA RATON
FL 33437
3 MIDDLTN
10 SOUTHWEST 10TH BOCA
FL
561-999-9990
4 MIDDLETON
10 SW TENTH AVE
BOCA RATON
FL 33437
999-9990
5 MIDDLTON
TEN SOTHWEST 10TH BOCA FL
33487
561-999-9991

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
195
Although all of these records were intended to refer to the same person entity,
no two records are exactly alike, and would be problematic for most matching
approaches [23]. However, if the ﬁelds in these are ﬁrst standardized prior to
matching, the ability to match these records improves dramatically, and the example
records above become:
Name
Address
City
St ZIP
Phone
--------- ----------------- ------------ -- ---------- ----------
1 MIDDLETON 10 SW 10TH AVE
BOCA RATON
FL 33487-7312 5619999990
2 MIDDLETON 10 SW 10TH AVE
BOCA RATON
FL 33487-7312
3 MIDDLTN
10 SW 10TH AVE
BOCA RATON
FL 33487-7312 5619999990
4 MIDDLETON 10 SW 10TH AVE
BOCA RATON
FL 33487-7312 0009999990
5 MIDDLTON
10 SW 10TH AVE
BOCA RATON
FL 33487-7312 5619999991
Names and addresses are often standardized into component ﬁelds prior to
matching which can also improve matching capability [22]. For example the
addresses above would become:
prim
prim
addr
unit
range predir name
suffix postdir desig city
----- ------ -------- ------ ------- ----- ---------
10
SW
10TH
AVE
BOCA RATON
state zip
zip4
----- ----- ----
FL
33487 7312
Consider this example using person names:
Name
-------------------------------------
William Smith
Bill R. Smith
Mr. William R. Smith Jr.
Dr. W. R. Smith
William Robert Smith
Dr. W. Smith PhD.
These names can be standardized into the following ﬁelds:
Title
First
Middle
Last
Suffix
------ ---------- --------- ----------- ------
WILLIAM
SMITH
BILL
R
SMITH
MR
WILLIAM
R
SMITH
JR
DR
W
R
SMITH
WILLIAM
ROBERT
SMITH
DR
W
SMITH
PHD
There are several third-party software packages available to perform name and
address standardization which can be incorporated into the record linkage process.

196
A.M. Middleton and D.A. Bayliss
2.2.6
Matching Weight and Threshold Computation
An important step in probabilistic record linkage is calculating the match proba-
bilities and weights of individual ﬁelds used in the matching process as well as
the threshold value for a record match. Fellegi and Sunter developed the original
mathematical model for this process based on earlier work by Newcombe [10,19].
This model relies on a large calibration dataset of observed matches representative
of the distribution of the actual population. Field match probabilities are typically
converted to logarithms so they can be added across the ﬁelds used for matching
which form a match vector to compute a total score, which is then compared to
the match threshold estimated by the model required for a record match. Fellegi
and Sunter also included in their model a method of using the relative frequency of
data values in the computation of the match probabilities. Several machine learning
methods have been used to estimate the match weights including the Bayesian
decision model [10, 16] the EM (expectation maximization) algorithm [24] and
support vector machine (SVM) methods [3]. Winkler later extended the Fellegi and
Sunter model to use the value-speciﬁc frequencies of all the available data which did
not rely on calibration data sets [21]. Although matching weights and thresholds are
usually pre-computed prior to the matching process, in some implementations made
feasible by high-performance data-intensive computing platforms, the weights are
calculated and adjusted on-the-ﬂy during the matching process [13].
2.2.7
Data Ingest
The data ingest step is the merging of additional standardized input data source ﬁles
with an existing base ﬁle or with each other to create the base ﬁle on which the
record linkage process will be performed. If a linked base or authority already ﬁle
exists, the data ingest process functions as an update and merges the new or updated
record information into the base ﬁle. The subsequent record linkage process can
add any new records to existing entity clusters, form new entity clusters, splitting
and collapsing entity clusters as required based on the matching results and new
information included in the input ﬁles to create a new linked version of the base ﬁle.
2.2.8
Blocking/Searching
In order to perform an exhaustive search for all possible record matches in the input
data for the record linkage process, each record must be compared to every other
record in the data, an n2 process. Although feasible when the number of records
is small, in the big data scenarios of data-intensive computing this approach is
impractical [4], and only a small number of record pairs will be matching records
[13]. To reduce the potentially large number of comparisons, some form indexing or
ﬁltering technique is needed to reduce the pairs of records that need to be examined
[3, 12, 13, 18]. The most frequently used approach is called blocking. In blocking,

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
197
a set of blocking keys to access or join the data are deﬁned based on the match
criteria to partition the database into mutually exclusive blocks or subsets of data
to subdivide the ﬁle and increase the number of matches found while decreasing
the number of pairs of records which have to be examined [4, 13]. Blocking keys
are usually based on the values of one or more ﬁelds in the matching criteria and
comparisons are then restricted to the record pairs in each block [13]. Enough
blocking keys must be deﬁned which are as independent as possible to insure
that missed record matches are minimized. For an in-depth discussion on blocking
strategies, refer to [13] and [23].
2.2.9
Weight Assignment and Record Comparison
Using the blocking criteria described in the previous section, data is accessed from
the source data ﬁle to be linked with the blocking keys using joins or other methods
such as sorted neighborhoods [4], and each record in the block is compared to the
other records in the block, by comparing the values of individual ﬁelds deﬁned
as part of the linking criteria for equality or similarity. Each ﬁeld is assigned
a matching weight based on the quality of the match using matching weights
calculated as described previously which may be scaled when using approximate
string matching (fuzzy matching) techniques if not an exact match. There are many
fuzzy matching approaches to address data problems such as typographical errors,
missing data, abbreviations, nicknames, including various edit distance algorithms,
phonetic algorithms such as Soundex or Metaphone and TF-IDF cosine similarity
[1,2,4,8,12,22]. The individual ﬁeld match weights are then aggregated to create a
total record match score for the record match decision.
2.2.10
Record Match Decision
The record match decision is made using the total score computed from matching
two records and aggregating individual ﬁeld match scores, compared to a pre-
computed threshold value described previously. In some record linkage systems,
thresholds are computed as an upper and a lower threshold such that if the total
score for a record match is at or above the upper threshold it is considered a match,
and if at or lower than the lower threshold, is considered a non-match. Records
between the upper and lower thresholds are possible matches, and are left for human
examination [10, 21, 23, 24]. Other systems such as SALT have a single threshold
and record matches at or above the threshold are considered a match, and those
below the threshold are considered a non-match. Records close to the threshold
value can be examined to determine if under-matching (false negatives) or over-
matching (false positives) are occurring. Once a record match has been determined,
the record linkage process can assign the same entity ID to the matching pair of
records.

198
A.M. Middleton and D.A. Bayliss
The record matching process may require multiple iterations of matching for
convergence as entity clusters of records are formed, or may use some method
of transitive closure to combine multiple pairs or clusters of matching records. In
an iterative process, a data analyst can examine results to determine if the target
precision and recall has been achieved, and whether or not under-matching or over
matching is occurring. Based on observation and examination of results, the process
and parameters can be adjusted if needed to improve results.
3
SALT: Basic Concepts
SALT is an acronym for Scalable Automated Linking Technology and is designed
to run on the open source HPCC scalable data-intensive computing platform. It
is a programming environment support tool which functions as an ECL code
generator on the HPCC platform to automatically produce ECL code for a variety
of applications. Although the primary use of SALT is for record linkage and clus-
tering applications, SALT offers auxiliary capabilities including data proﬁling, data
hygiene, data source consistency monitoring, data ingest and updating of base data
ﬁles, and ﬁle comparison to determine delta changes between versions of a data ﬁle.
SALT is an executable program coded in CCC which can be executed from
a Windows command prompt or directly from the ECL IDE development tool
(previously called QueryBuilder) in the HPCC environment. The SALT program
reads as its input a text ﬁle containing user-deﬁned speciﬁcation statements, and
produces an output ﬁle containing the generated ECL code to import into the user
ECL code repository. SALT provides many command line options to control its
execution, and to determine the type of ECL code to produce for a target application.
SALT offers many advantages when developing a new data-intensive application.
SALT encapsulates a signiﬁcant amount of ECL programming knowledge, experi-
ence, and best practices gained at LexisNexis for the types of applications supported,
and can result in signiﬁcant increases in developer productivity. It affords signiﬁcant
reductions in implementation time and cost over a hand-coded approach. SALT can
be used with any type of data in any format supported by the ECL programming
language to create new applications, or to enhance existing applications.
3.1
SALT Process
The SALT process begins with a user deﬁned speciﬁcation ﬁle for which an example
is shown in Fig. 8.2. This is a text ﬁle with declarative statements and parameters
that deﬁne the data ﬁle and ﬁelds to be processed, and associated processing options
such as the module into which the generated code is imported.
Figure 8.3 shows the basic steps in using SALT: (1) a speciﬁcation ﬁle for the
data and application is created by the user; (2) the SALT program is executed using

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
199
Fig. 8.2 SALT speciﬁcation ﬁle example
Fig. 8.3 SALT basic process
a command line with speciﬁc options depending on the type of application for which
the code is being generated and includes an input ﬁle with a .spc extension and an
output ﬁle with a .mod extension; (3) the SALT program produces an output ﬁle
in a special .mod format with the ECL coded needed for the application; (4) the
generated code is imported into the ECL code repository; and (5) the ECL code is
now available for execution using the ECL IDE.

200
A.M. Middleton and D.A. Bayliss
3.2
Record Matching Field Weight Computation
SALT calculates record matching ﬁeld weights based the concept of term speciﬁcity
and matching weights are referred to within SALT as speciﬁcities. The measure of
term speciﬁcity for documents was ﬁrst proposed by Karen Sp¨ark Jones in 1972 in
a paper titled “A Statistical Interpretation of Term Speciﬁcity and its Application in
Retrieval” [14], but later became known as inverse document frequency (IDF) [20].
It is based on counting the documents in a collection or set of documents which
contain a particular term (indexed by the term). The basic idea is that a term that
occurs in many documents is less speciﬁc as an index term and should be given less
weight than a term which occurs in only a few documents. The IDF is frequently
used in combination with the term frequency or TF which is the frequency of a term
within a single document. The combination called TF-IDF is used as a weight or
statistical measure to evaluate how important a term is to a document that is part of a
set of documents. The use of frequencies in calculating weights for record matching
was ﬁrst proposed by Newcombe et al. [19], formalized in a mathematical model
by Fellegi and Sunter [10], and extended by Winkler [21]. TF-IDF calculations
for matching weights have also been used by Bilenko and Mooney [1], Cohen [7],
Cohen et al. [8], Koudas et al. [15], and Gravano et al. [11].
SALT applies the concept of term speciﬁcity to the unique ﬁeld values for a
ﬁeld deﬁned for a record in the input dataset(s) to be matched to calculate a ﬁeld
value speciﬁcity for each unique value contained in the ﬁeld across all records in
the dataset. The rarer a ﬁeld value is in the input dataset, the higher the speciﬁcity
value. SALT also calculates a weighted average ﬁeld speciﬁcity taking into account
the distribution of unique values for each ﬁeld which is used when individual ﬁeld
values are not available during processing and for internal code generation and
processing decisions. The ﬁeld value speciﬁcities are calculated by dividing the
total number of unique entities in the dataset by the number of entities containing a
non-null unique ﬁeld value in a ﬁeld and taking the logarithm base 2 .log2/ of the
quotient. Note that initially in an unlinked dataset the number of entities is equal to
the number of records. SALT recalculates ﬁeld value speciﬁcities and the weighted
average ﬁeld value speciﬁcity which are used directly as matching weights for each
iteration of linking based on all available data. The weight computation for ﬁeld
value speciﬁcity is represented by the following equation:
wfvs D log 2nent
nval
(8.1)
where wfvs is the ﬁeld value speciﬁcity, nent is the number of unique entities in
the dataset, and nval is the number of entities containing a non-null unique ﬁeld
value for the ﬁeld for the current iteration of linking. The average ﬁeld speciﬁcity is
calculated by the following equation:

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
201
Fig. 8.4 Example average and ﬁeld value speciﬁcities for state
wavg D log 2
 nP
iD1
nvali
2
nP
iD1
nvali 2
(8.2)
SALT uses the ﬁeld value speciﬁcities as weights for determining record matches
in a record linking/clustering process. For example, when two separate records
are being matched, SALT compares each ﬁeld in the two records for similarity
based on the deﬁnition of the ﬁeld in the SALT speciﬁcation ﬁle. If the ﬁeld
values match between the two records, the speciﬁcity for the ﬁeld value (scaled
for fuzzy matches and otherwise adjusted based on the editing options for the
ﬁeld) is added to a total speciﬁcity to help determine a record match. Each ﬁeld
deﬁned in the speciﬁcation ﬁle for the record can make a positive, negative, or no
contribution to the total speciﬁcity. If the total speciﬁcity exceeds a pre-determined
record matching threshold, then the two records are considered a match. The SALT
record linking/clustering technology operates on a dataset of records containing
information about a speciﬁc entity type. As records are linked in an iterative process
to form entity clusters, speciﬁcities are recalculated based on the number of entities
that have a ﬁeld value as a proportion of the total entities represented in the dataset.
As clustering occurs speciﬁcities converge to a more accurate value based on the
number of entities represented. Figure 8.4 shows an example of the speciﬁcity values

202
A.M. Middleton and D.A. Bayliss
Table 8.1 Thresholds
computed for various levels
of precision
P (%)
T
99
log2.N/ C 5:64
99.9
log2.N/ C 5:64
99.99
log2.N/ C 12:28
for state codes calculated on a large dataset. Note that the state code with the largest
count of records (CA – California) has the lowest speciﬁcity, and the state code with
the fewest records (MH – Marshall Islands) has the highest speciﬁcity.
3.3
Record Matching Threshold Computation
In a record linkage process, SALT scores the likelihood that two records reference
the same entity using the following formula:
S .r1; r2/ D
X
f
wf;v
(8.3)
In the above formula, S.r1; r2/ represents a score assigned to records r1 and r2.
The sum is taken over all ﬁelds f common to both r1 and r2, of the ﬁeld value
speciﬁcities represented by wf;v which can be positive, negative, or zero and also
scaled for near or fuzzy matches. In some cases, the average ﬁeld speciﬁcity may be
used when the speciﬁcity for a ﬁeld value is not known.
The total speciﬁcity threshold T for a record match is calculated using the
formula below:
T D log 2.N /  log 2.1  P /  1
(8.4)
The term N represents the number of entities in the dataset. This is initially based
on the estimated number of entities speciﬁed by the POPULATION statement in
the SALT speciﬁcation ﬁle, but may be adjusted by the SALT process if required.
The term P represents the target precision percentage required by the record linkage
process and is speciﬁed in SALT using the NINES statement. The precision required
expressed as a number of nines such that a value of 2 means 2 nines or a precision
percentage of 99%. A value of 3 means 3 nines or 99.9%. A table of thresholds
computed for various levels of precision is: (Table 8.1).
The total speciﬁcity threshold for a record match can also be directly speciﬁed
by a user of SALT using the THRESHOLD statement. The POPULATION, NINES,
and THRESHOLD statements are described further in Sect. 3.7.

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
203
3.4
Record Linkage
The record linking approach used by SALT is an automatic statistical approach
based entirely on the input data. The ﬁeld value speciﬁcities (described in Sect. 3.2)
calculated for each unique value in a ﬁeld are used as weights to determine record
matches. As records are compared, the value in each ﬁeld in one record is compared
to the values in corresponding ﬁelds in the other record. If a ﬁeld matches, the
speciﬁcity for the ﬁeld is added to a total for the record and compared to a
pre-computed or user-speciﬁed threshold. Each ﬁeld can make a positive (ﬁeld
values match), negative (ﬁeld values do not match), or no contribution to the total
speciﬁcity. If the total speciﬁcity is at or above the threshold value, the two records
are considered to be a match. The amount of speciﬁcity added per ﬁeld for a match
is variable, based on the actual matching ﬁeld value, and ﬁeld speciﬁcities are
automatically scaled for fuzzy matches and otherwise adjusted based on the editing
options for the ﬁeld deﬁned in the speciﬁcation ﬁle.
SALT includes three types of record linkage processes. Internal linking is the
classic process of matching and clustering records that refer to the same entity and
to assign entity identiﬁers to create a base or authority ﬁle. An entity is typically
a real-world object such as a person or business, but can be anything about which
information is collected in ﬁelds in a record where each record refers to a speciﬁc
entity. The goal is to identify all the records in a ﬁle that are related to the same
entity. This process is useful in many information processing applications including
identifying duplicate records in a database and consolidating account information
for example. Input records are matched using the ﬁelds and process parameters
deﬁned by FIELD, CONCEPT, ATTRIBUTEFILE and other statements in the
speciﬁcation SALT speciﬁcation ﬁle.
External linking is the process of matching an external ﬁle or an online query
to an existing, linked base or authority ﬁle which has been previously linked by
an internal linking process, or some other linking process. The goal of external
linking is to determine if a record in the external ﬁle is a match to an entity cluster
in the internal base ﬁle and assign it the unique identiﬁer for the matching entity.
This process is also referred to as entity resolution. External linking is useful in
establishing foreign key relationships between an external ﬁle and an existing ﬁle
based on the unique entity identiﬁer. For example, a person may have a unique
identiﬁer in a base ﬁle that contains general information about the person entity,
and the external ﬁle may have information on vehicles which are or have been
owned or leased by a person entity. SALT external linking also supports a base
ﬁle search mode in which all records which are similar to the search criteria are
returned.
SALT can be used to generate code to perform record matching and scoring
and link together records that are completely independent from a base ﬁle without
directly using the base ﬁle during the linking process. This capability is called
remote linking. For remote linking, SALT still generates statistics from the base
ﬁle data which can be used to signiﬁcantly improve the quality of record to record

204
A.M. Middleton and D.A. Bayliss
matching/linking for any application assuming the records contain ﬁelds with the
same type of data in the base ﬁle. The remote linking capability is implemented as
a compare service, which compares the ﬁelds in two records and generates scoring
information similar to SALT internal linking.
The record linking approach used by SALT is similar to the classic probabilistic
record linkage approach. However, the SALT approach has some signiﬁcant
advantages over the typical probabilistic record linkage approach. The amount of
speciﬁcity added per ﬁeld for a match is variable, based on the actual matching
ﬁeld value. This effectively assigns higher weights automatically to the more rare
values which have higher speciﬁcity. This in turn allows record matches to occur
even when the data in a record is sparse or inconsistent (i.e. ﬁelds with missing
values) increasing recall signiﬁcantly, when the remaining matching ﬁeld values
are sufﬁciently rare. In addition, ﬁeld speciﬁcities are automatically scaled for
fuzzy matches and other editing constraints speciﬁed for a ﬁeld improving overall
precision. Since speciﬁcities are also effectively trained on all the available data, and
not just a hand-labeled sample of the data, the SALT approach can provide higher
precision and recall than other machine learning approaches.
3.5
Attribute Files
Sometimes there are additional ﬁelds related to an entity identiﬁer which may
help in record linkage except these ﬁelds do not exist in the input ﬁle being
linked. Examples from the LexisNexis public records are properties, vehicles, and
bankruptcies which contain information relating to person entities. These are ﬁles
external to the linking process that contain a person entity identiﬁer and some
form of data or attribute that is associated with that entity identiﬁer. For example,
a unique property id, vehicle identiﬁcation number (VIN), or bankruptcy ﬁling
number. SALT refers to these external ﬁles as attribute ﬁles and they are deﬁned
in the SALT speciﬁcation ﬁle using an ATTRIBUTEFILE statement.
The properties needed for these external ﬁelds are that they have high speciﬁcity
(usually a unique identiﬁer about something like a vehicle which could be associated
with more than one entity) and low variability (some variability in value for a
given entity is permissible, i.e., one person entity could be associated with multiple
vehicles). This implies looking for things which are associated with an entity
and which are shared by relatively few entities (one vehicle hasn’t had too many
owners), and where a single entity doesn’t have too many. By default only the
best of the matching entity identiﬁers from each attribute ﬁle is allowed to score
towards matching one pair of entity identiﬁers in the input ﬁle. Attribute ﬁles
can contain additional ﬁelds from the external ﬁle which can be used by SALT
in search applications. For example if appropriate ﬁelds are included, a search for
persons who own or have owned red Corvette convertibles living in Florida could
be done.

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
205
3.6
Linkpaths
External linking in SALT requires the deﬁnition of ﬁelds to be used for searching
for candidate records for matching. In SALT, these search deﬁnitions are called
linkpaths and deﬁned in the SALT speciﬁcation ﬁle using the LINKPATH statement.
Linkpaths deﬁne various combinations of ﬁelds which are used to inform the
external linking process how the internal data should be searched for a potential
match. Linkpaths are analogous to deﬁning indexes on a data base, and result in the
generation of an index on the base ﬁle data to support the external linking process.
User-deﬁned linkpaths are speciﬁed using the LINKPATH statement with a name
and a ﬁeld list. The ﬁeld list can be grouped into required ﬁelds, optional ﬁelds,
and extra-credit ﬁelds. The required ﬁelds deﬁned in a linkpath must match exactly
during external linking. Optional ﬁelds must match if provided, and fuzzy matches
are acceptable. Extra-credit ﬁelds do not need to match, but add to the total matching
score if they do and can also include any of the fuzzy matching edit characteristics.
Each linkpath deﬁned results in the creation of an HPCC ECL index (key) ﬁle which
is used in the matching process.
Although the user is primarily responsible for deﬁning appropriate linkpaths
based on knowledge of the data and user query patterns, SALT includes a capability
to suggest possible linkpaths based on the data in the base ﬁle and a sample external
ﬁle or ﬁles. The output of the Data Proﬁling Field Combination Analysis report on
an external ﬁle can be used as an additional input ﬁle to the SALT tool along with a
speciﬁcation ﬁle deﬁning the ﬁelds in the base ﬁle to create a Linkpath Generation
Report with suggested linkpaths. Figure 8.5 is an example of a speciﬁcation ﬁle for
external linking which includes LINKPATH statements.
3.7
Speciﬁcation Language
The SALT speciﬁcation language is a declarative language which describes the
input ﬁle data and the process parameters to be used in a SALT generated ECL
language application. Each speciﬁcation ﬁle language statement must appear on a
single line of text in the speciﬁcation ﬁle. The basic syntax for language statements
is as follows:
KEYWORD:parameter:KEYWORD(parameter)[:OPTIONALjWORD]
Keywords are not case-sensitive and optional parameters can appear in any order
within a speciﬁcation ﬁle statement. Keywords are shown in caps for emphasis in
all examples in this chapter. Although statements can generally appear in any order,
deﬁnitions are usually ordered in a similar manner to the example in Fig. 8.2 for
readability and consistency. A complete language reference is not presented here,
but can be found at http://hpccsystems.com.

206
A.M. Middleton and D.A. Bayliss
Fig. 8.5 External linking speciﬁcation ﬁle example
MODULE:modulename[.submodule]
The MODULE statement speciﬁes a module name (folder) in the ECL repository
(directory) where the source code generated by SALT will reside. The code
generated by SALT uses the speciﬁed modulename with optional submodule as the
base for the ECL code generated and is used for external references the code.
OPTIONS:option switches
The OPTIONS statement allows the.spc ﬁle to override or add in command line
options normally speciﬁed on the SALT command line when using SALT directly
from the ECL IDE.
FILENAME:name]
The FILENAME statement allows a logical name for the input ﬁle to be speciﬁed
and processed by the code generated by SALT. The name parameter is incorporated
into various attribute names including attributes which identify the input dataset
and the input record layout for the process, and additional temporary and output
ﬁlenames in the ECL code generated by SALT.
PROCESS:processname[:UBER(ALWAYSjREQUIREDjNEVER)]
The PROCESS statement speciﬁes an overall name for an external linking or remote
linking process generated by SALT, but is not required for other processes. The

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
207
processname is arbitrary and used for symbol naming in the generated code. The
UBER option deﬁnes how the UBER key is used in an external linking process. The
default is the UBER key is used if searching using all of the LINKPATHs speciﬁed
for external linking could satisfy the query.
IDFIELD:
IDFIELD:EXISTS:fieldname
The IDFIELD identiﬁes the ﬁeld to be used as the entity ID for record linkage. If
IDFIELD: is speciﬁed with nothing following, then it is assumed that no ID exists
and the generated code will be used to cluster the input ﬁle records and assign a
clustering ID based on the record id ﬁeld speciﬁed in the RIDFIELD statement. If
IDFIELD:EXISTS: ﬁeldname is speciﬁed, then the input ﬁle is assumed to have a
ﬁeld previously deﬁned identifying matching records for entity clusters. When used
in a record linkage process, this allows additional records to be clustered with the
existing IDs.
IDNAME:fieldname
The IDNAME statement speciﬁes the ﬁeldname to be used for the ID ﬁeld in the
output of a record linkage process. If an ID ﬁeld does not already exist in the input
data, then IDFIELD: is used with IDNAME: ﬁeldname which speciﬁes the name of
the output ﬁeld for the ID.
RIDFIELD:fieldname
The RIDFIELD statement speciﬁes the name of the numeric ﬁeld containing the
record identiﬁer or RID. Each record in the input dataset should have a unique RID
value. The RIDFIELD is used as the basis for the record linkage process when no
IDFIELD:EXISTS is speciﬁed. The entity cluster ID for each matched set of records
will be the lowest value RID in the group at the end of the record linkage process.
RECORDS:record count
The RECORDS statement speciﬁes the expected number of records at the end of a
record linkage process. The record count value is the expected number of records at
the end of the process which initially can be speciﬁed as the input record count.
The RECORDS statement in combination with the NINES and POPULATION
statements in a speciﬁcation ﬁle allow SALT to compute a suitable matching score
threshold for record linkage as well as a block size for the number of records to
compare for the blocking process.
POPULATION:entity count
The POPULATION statement speciﬁes the expected number of entities at the end
of a record linkage process. When the matching process is complete, entity clusters
or records are formed, each identiﬁed by a unique entity ID. The entity count value
is the expected number of entities or unique entity IDs that will be generated by the
matching process.

208
A.M. Middleton and D.A. Bayliss
NINES:precision value
The NINES statement speciﬁes the precision required for a SALT generated record
linkage process. The precision value parameter speciﬁes the precision required
expressed as a number of nines such that a value of 2 means 2 nines or a precision
of 99%. A value of 3 means 3 nines or 99.9%.
FIELDTYPE:typename:[ALLOW(chars):] [SPACES(chars):]
[IGNORE(chars):][LEFTTRIM:][CAPS:][LENGTHS(length list):]
[NOQUOTES():][LIKE(fieldtype):][ONFAIL(IGNOREjCLEANjBLANKj
REJECT):][CUSTOM(functionname[< j >n][,funcparam1, funcparam2 ,
...funcparamn]:]
The FIELDTYPE statement allows ﬁeld editing and validity checking require-
ments used for data hygiene processing to be deﬁned and grouped into common
deﬁnitions which can then be associated with any ﬁeld. A FIELDTYPE ﬁeld does
not really exist; it is used to assign editing constraints to a ﬁeld. The FIELDTYPE
parameters are essentially assertions deﬁning what the given ﬁeld must look
like. The LIKE parameter speciﬁes a base or parent for the ﬁeld type allowing
FIELDTYPEs to be nested. All of the restrictions of the parent ﬁeld type are then
applied in addition to those of the ﬁeld type being speciﬁed. The ONFAIL parameter
allows the user to select what occurs when an editing constraint is violated. These
include ignoring the error, cleaning the data according to the constraint, blanking
or zeroing the ﬁeld, or rejecting the record. The CUSTOM parameter allows a user
deﬁned function to be referenced to perform validity checking.
BESTTYPE:name:BASIS(fixed fields:[?j!]:optional fields):
construction method:construction modifiers:
propagation method
The BESTTYPE statement is used to deﬁne a best value computation for a ﬁeld or
concept for a given basis for an entity. The calculated best value can be used for
propagation during record linkage, and is available for external application use. The
basis is typically the entity identiﬁer speciﬁed by the IDFIELD, but a more complex
basis can be speciﬁed consisting of multiple ﬁelds. Multiple BESTTYPEs can be
associated with a ﬁeld or concept, and all are evaluated, but the leftmost non-null
best value is considered the overall best value for the ﬁeld. SALT generates code for
calculating the best values in the Best module and exported dataset deﬁnitions are
provided which allow output of a dataset of best values for each ﬁeld or concept and
associated BESTYPE deﬁnitions. In addition SALT provides several aggregate ﬁles
using whatever ﬁelds are deﬁned in the basis.
BESTTYPE construction methods provided are COMMONEST (most fre-
quently appearing value), VOTED (a user-deﬁned function is provided to weight
the ﬁeld value by source type), UNIQUE (best value is produced if there is only one
unique value for the ﬁeld in the entity cluster), RECENT (uses the most recent
value speciﬁed by a date ﬁeld parameter), LONGEST (picks the longest value
for a ﬁeld). Construction modiﬁers include MINIMUM (candidates must have a
minimum number of occurrences in an entity cluster), FUZZY (speciﬁes that the

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
209
fuzzy matching criteria of the target ﬁeld are used to allow less common values
to support candidates for best value), and VALID (speciﬁes that only those values
considered valid will be considered available for BEST computation). Propagation
methods include PROP (copy the best value into null ﬁelds with a matching basis),
EXTEND (copy the best value into null ﬁelds and those that are partial exact
matches to the best value), FIX (copy the best value onto null ﬁelds and overwrite
those ﬁelds which are fuzzy matches to the best value), and ENFORCE (copy the
best value into the ﬁeld regardless of the original data content).
Note that the BESTTYPE statement is a powerful capability and interested
readers are referred to the SALT User’s Guide for a more in-depth explanation.
FIELD:fieldname[:PROP][:CONTEXT(context fieldname)]
[:BAGOFWORDS[(MANYjALLjANYjMOSTjTRIGRAM)]][:CARRY]
[:TYPE(datatype)][:LIKE(fieldtype)] [:EDIT1][:EDIT2]
[:PHONETIC][:INITIAL][:ABBR][:HYPHEN1[(n)]][:HYPHEN2[(n)]]
[:fuzzy function...[:fuzzy function]]
[:MULTIPLE][:RECORDDATE(FIRSTjLAST[,YYYYMM])]
[:besttype...[:besttype]] [:FLAG][:OWNED]
[:FORCE[(+j--[n])]:specificity,switch value1000
The FIELD statement deﬁnes a data ﬁeld in the input ﬁle record including its type
and other characteristics which affect hygiene, validity, and matching. The PROP
parameter speciﬁes a default propagation for the ﬁeld if there is no associated BEST-
TYPE. If the CONTEXT parameter is speciﬁed, then a match occurs only if both
the values in ﬁeldname and the context ﬁeldname match. If the BAGOFWORDS
parameter is speciﬁed then the string ﬁeld is treated as a sequence of space delimited
tokens. The LIKE parameter speciﬁes additional editing characteristics of the ﬁeld
deﬁned by the named FIELDTYPE statement. EDIT1 and EDIT2 specify edit-
distance fuzzy matching, PHONETIC speciﬁes phonetic fuzzy matching, INITIAL
allows a partial string to match the ﬁrst characters of another string, ABBR allows
the ﬁrst character of tokens in one string appended together to match another string,
HYPHEN1 and HYPHEN2 provide for partial and reverse matching of hyphenated
ﬁelds, MULTIPLE allows multiple values to be speciﬁed for entity resolution,
RECCORDATE allows a date ﬁeld to be speciﬁed as FIRST or LAST in context
and YYYYMM allows dates to be year and month only. fuzzy function speciﬁes
the name of a custom fuzzy matching function deﬁned by the FUZZY statement.
The besttype parameters refer to BESTTTYPE deﬁnitions associated with the
ﬁeld, FLAG allows statistics to be calculated about the ﬁelds when using BEST-
TYPE, OWNED with FLAG implies the best value should only appear in a single
entity cluster. The FORCE parameter is used to require a match on the ﬁeld for a
record match, or specify the minimum ﬁeld match score needed for a record match,
and can also specify that no negative contribution to the record score is allowed. The
speciﬁcity and switch value1000 are computed by SALT and added to the FIELD
statements prior to record linkage. Speciﬁcity is the weighted average ﬁeld score for
matching and the switch value1000 is the average variability of ﬁeld values across
all entity clusters (fraction  1000).

210
A.M. Middleton and D.A. Bayliss
FUZZY:name:RST:TYPE(FuzzyType):CUSTOM(FunctionName)
The FUZZY statement speciﬁes a custom user-supplied fuzzy matching function
for a FIELD. SALT automatically handles other requirements such as scaling of
the ﬁeld value speciﬁcity. The name parameter associates a name with the custom
fuzzy processing. Once deﬁned, the name can be used as a parameter of a FIELD
deﬁnition. The FuzzyType parameter allows the return type of the fuzzy function to
be speciﬁed as a valid ECL datatype. The FunctionName parameter deﬁnes an ECL
function which performs the fuzzy processing.
DATEFIELD:fieldname [:PROP][:SOFT1][:YEARSHIFT][:MDDM]
[:CONTEXT(context fieldname)][:FORCE[(+j--[n]
[,GENERATION])]:specificity,switch value1000
The DATEFIELD statement speciﬁes a numeric string ﬁeld in the format YYYYM-
MDD. It functions in an identical manner to the FIELD statement except for
requiring the speciﬁc date format. The FORCE parameter includes a special option
GENERATION which applies only to a DATEFIELD. If used the YEAR portion of
the date has to be within 13 years of the other (or null). The SOFT1, YEARSHIFT,
and MDDM options provide some fuzzy matching capabilities for dates.
SOURCEFIELD:fieldname[:CONSISTENT[(checkfieldname,
checkfieldname,...)]]
The SOURCFIELD statement speciﬁes the name of the ﬁeld containing the input
data source type. The source ﬁeld is not processed as a normal ﬁeld deﬁnition
for matching, but is used for the data source consistency checking process. If the
CONSISTENT parameter is provided then SALT generates code into the hygiene
module to check for consistency of ﬁeld values between the various sources
represented in the input ﬁle.
SOURCERIDFIELD:fieldname
The SOURCERIDFIELD statement speciﬁes the name of a ﬁeld in the input ﬁle
which contains a unique identiﬁer for a corresponding record in source or ingest ﬁle
which has been merged into the base ﬁle. This value in combination with the value
of the SOURCEFIELD provides a link to the original source record for the data.
LATLONG:name:LAT(latitude field):LONG(longitude field):
[DISTANCE(n)][DIVISIONS(n)]
The LATLONG statement speciﬁes a geo-point ﬁeld for the location associated
with a record based on latitude and longitude ﬁelds included in the speciﬁcation
ﬁle. If a LATLONG is speciﬁed, the geo-point is made up of the combined latitude
ﬁeld and longitude ﬁeld and is treated as one single ‘pin-point’ location instead
of the two separate measures during a record linkage process. LATLONG ﬁeld
values are treated fuzzily for matching records. The LATLONG geo-points must
also be within DISTANCE(n) as deﬁned above from each other to make a positive
contribution to the match score, otherwise it can make a negative contribution.
The population density of entities in the grid as deﬁned by the DISTANCE and

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
211
DIVISIONS parameters for the grid around all geo-points is calculated giving the
ﬁeld match score for a given distance from a geo-point.
CONCEPT:fieldname[:+]:child1[+]:child2[+]:childn[+]...
[:FORCE[(+j--[n])]:[:SCALE(NEVERjALWAYSjMATCH)][:BAGOFWORDS]:
specificity,switch value1000
The CONCEPT statement allows a group of related or dependent ﬁelds to be
deﬁned and is used so that dependent ﬁelds are not over weighted in the record
linkage process. SALT makes an implicit assumption of ﬁeld independence which
can lead to under or over weighting during the matching process when the ﬁelds
only really have meaning in the context of other ﬁelds. This can be corrected by
appropriately deﬁning CONCEPT ﬁelds. A CONCEPT replaces the child ﬁelds only
if matched between records during the record matching process. If the Concept ﬁeld
does not match, the child ﬁelds are independently evaluated in the record matching
and scoring process. A Concept ﬁeld is a computed ﬁeld and does not appear in the
input ﬁle.
ATTRIBUTEFILE:name[:NAMED(modulename.filename)]:
IDFIELD(id field name):VALUES(attribute field name[,LIST])
[:KEEP(njALL)][:WEIGHT(value)][SEARCH(list of fields)]
[:specificity,switch value1000]
An ATTRIBUTEFILE statement deﬁnes a special type of ﬁeld which provides a
set of values for matching from an external ﬁle, child dataset which is part of the
main input ﬁle, or a child dataset which is part of the external ﬁle. Each matching
value must be paired with an ID value of the same type as deﬁned for the input ﬁle
in the IDFIELD or IDNAME statement. During the matching process, if attribute
values match between records being compared, the match will contribute to the
overall score of the record match. The VALUES ﬁeld list allows additional ﬁelds to
be included which can then be used in search applications. The KEEP parameter
allows the user to specify how many matching attribute values are allowed to
contribute to a record match.
INGESTFILE:name:NAMED(module.attribute name)
The INGESTFILE statement speciﬁes the name to be used for an ingest ﬁle to
be appended/merged with the base ﬁle as part of a SALT record linkage process.
The module.attribute name speciﬁed in the NAMED() parameter speciﬁes
the module and attribute name of a dataset attribute. The dataset is assumed to be
in the same format as the base ﬁle. Ingest ﬁles are appended to and merged with
the base ﬁle speciﬁed in the FILENAME attribute for a record linkage process.
Typically these ﬁles are generated from external source ﬁles or base ﬁles for other
types of entities.
LINKPATH:pathname[:fieldname:fieldname:fieldname...:
fieldname]
The LINKPATH statement speciﬁes the name of a search path for an external linking
entity resolution process generated by SALT. The pathname is arbitrary and used for

212
A.M. Middleton and D.A. Bayliss
symbol naming. A ﬁeldname references either a ﬁeld deﬁned in the speciﬁcation ﬁle,
an ATTRIBUTEFILE value ﬁeld, or is a ‘?’ or ‘C’ character separating groups of
ﬁelds. A linkpath can be divided into 3 groups: required ﬁelds which immediately
follow the pathname and must match, optional ﬁelds which follow the ‘?’ character
used as a ﬁeldname and must match if data is present in both records for the ﬁeld,
and extra credit ﬁelds which follow a ‘C’ character used as a ﬁeldname and are
not required to match but will add to the match score if they do. The ﬁeldnames
used in a linkpath typically correspond to ﬁeld combinations used frequently in user
queries.
RELATIONSHIP:relationshipname:BASIS(FieldList):DEDUP
(FieldList)[:SCORE(FieldList)][:MULTIPLE(n)][:SPLIT(n)]
[:THRESHOLD(n)]
RELATIONSHIP:relationshipname: RelationshipList)]
[:MULTIPLE(n)] [:THRESHOLD(n)]
SALT record linkage provides the capability to cluster together records to form
an entity. In some situations, the objective is not to determine that two records
or clusters are close enough to become part of the same entity, but to determine
if a statistically signiﬁcant link exists between the two clusters and to record this
relationship. The RELATIONSHIP statement provides this function. Relationships
provide a way to record instances when multiple occurrences of speciﬁc set of ﬁelds
(the BASIS ﬁeld list) matching between clusters provide information that a speciﬁc
relationship exists or evidence that the clusters may need to be linked. The second
form of the RELATIONSHIP statement deﬁnition above allows a relationship to be
formed as the sum of other relationships.
THRESHOLD:threshold value
The THRESHOLD statement overrides the default record matching threshold
calculated by the SALT code generation process. The threshold value speciﬁes a
new value for the speciﬁcity matching threshold which is the minimum amount of
total speciﬁcity needed for a record match.
BLOCKLINK:NAMED(modulename.attribute)
The BLOCKLINK statement is used to deﬁne a ﬁle which will be used to
block linking of speciﬁc matching records during an internal linking process.
BLOCKLINK provides a user-speciﬁed unlink capability which prevents certain
records from being combined in an entity cluster. This may be required as part of a
linking process for compliance or other reasons.
4
SALT: Applications
SALT provides data proﬁling and data hygiene applications to support the data
preparation process. In addition SALT provides a general data ingest application
which allows input ﬁles to be combined or merged with an existing base ﬁle. SALT
also provides application for several different types of record linking including

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
213
Data Profiling
FIELD definition
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Summary Report
Detail Report
Data Source Consistency Report
Statistics
Data Hygiene
FIELDTYPE Definitions
Hygiene Report
Statistics
Data Ingest
New/Updated Data
Merge to Base File
Statistics
Linking?
No
Delta Compare
•
Compare to previous cycle
•
Delta Change Reports
•
Statistics
Yes
Generate Specificities
Specific Field Value Weights
Average Field Value Weights
Internal Linking
External Linking
Remote Linking
LINKPATH defintion
Entity Resolution
Base File Searching
CONCEPT definition
BESTTYPE defintion
RELATIONSHIP definition
BLOCKLINK definition
Entity ID clustering
•
Custom user record matching
•
Weights calculated from base
file
Data Preparation
Record Linking
Fig. 8.6 SALT data integration process
internal, external, and remote described later in this chapter. In addition SALT
includes a data source consistency application to check the ﬁeld value consistency
between different sources of data for an input ﬁle and delta ﬁle comparison
application so that a new version of a ﬁle can be compared to a previous version
to determine what has changed. Figure 8.6 shows the SALT user data integration
process and application ﬂow.

214
A.M. Middleton and D.A. Bayliss
Fig. 8.7 SALT data proﬁling summary report example
4.1
Data Proﬁling
SALT data proﬁling is a process which provides important type, statistical, and
pattern information on the data ﬁelds and concepts and their contents in any input
data ﬁle. This information is essential in analyzing the content and shape (patterns)
of the data in the source data ﬁles and facilitates important decisions concerning data
quality, cleansing, de-duping, and linking of records, and to provide information
on the changing characteristics of data over time. Data proﬁling is a task usually
performed by data analysts as exploratory data analysis [13], and is an important
preparatory step for the record linkage process.
SALT data proﬁling provides by ﬁeld breakdowns of all the characters, string
lengths, ﬁeld cardinality (the number of unique values a ﬁeld contains), top data
values, and word counts for every data ﬁeld or concept (dependent groups of data
ﬁelds) deﬁned in the speciﬁcation ﬁle. In addition, SALT calculates and displays the
top data patterns to help analyze the shape of the data. The data proﬁling capability
also provides summary statistical data such as the number of records in the input
ﬁle, and the percentage of non-blank data, maximum ﬁeld length, and average ﬁeld
length for every ﬁeld and concept. This summary information provides a quick view
which can be compared with previous versions of a data ﬁle to identify anomalies
or to verify anticipated changes in the content of a data ﬁle.
The data proﬁling information can also be used as input data to a change tracking
system. If any of the data proﬁling information is not consistent with expectations, it
may be an indication of bad data in the source ﬁle which may need further cleansing.
Figure8.7 shows a partial data proﬁling summary report produced by SALT for a
sample input ﬁle of business data.
Figure 8.8 shows a partial data proﬁling detail ﬁeld report for the phone ﬁeld
in the same sample ﬁle. SALT can run data proﬁling ﬁeld proﬁles on all ﬁelds or
selected ﬁelds.
SALT data proﬁling also provides the capability to analyze ﬁelds existing in an
external ﬁle which correspond to ﬁelds in an internal base ﬁle. When executed, a
report is produced which shows the top combinations of ﬁelds which are non-blank
sorted by frequency. The output of this report is an indicator of which ﬁelds may be

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
215
Fig. 8.8 SALT data proﬁling ﬁeld detail report example
Fig. 8.9 SALT ﬁeld combination analysis report example
more sparsely populated in the external ﬁle which could indicate problems with the
data source and can also help identify the type of data represented in the external
ﬁle. If the external ﬁle is representative of typical data requiring entity resolution
using the base ﬁle, this report helps determine the best ﬁeld combinations to use
to deﬁne the linkpaths required, and SALT can automatically generated suggested
linkpaths using the data from this report. Figure 8.9 shows partial sample output
from this report.
4.2
Data Hygiene
Once the initial data proﬁling process is complete, SALT can be used to check
the cleanliness of the data. SALT uses the term data hygiene to refer to both the
cleanliness of the data and the process by which data is cleansed so that it can be

216
A.M. Middleton and D.A. Bayliss
used effectively in a subsequent data integration process such as record linkage.
Cleanliness of data is determined by whether or not a data item is valid within the
constraints speciﬁed for a particular data ﬁeld. For example, if a particular data ﬁeld
is constrained to the numeric characters 0–9, then any data item for the ﬁeld which
contains characters other than 0–9 would fail a hygiene validity check for the data
ﬁeld.
SALT includes capabilities to deﬁne hygiene constraints on its input data,
identify invalid data in ﬁelds, and cleanse the data if needed. However, by default,
no error checking will occur unless speciﬁed for ﬁeld deﬁnitions in the speciﬁcation
ﬁle. SALT includes standard syntax using the FIELDTYPE statement to support
most common types of validity checks on data in ﬁelds. Custom user-deﬁned
functions which perform user-speciﬁc validity checks can also be included.
SALT data hygiene can be used as an independentprocess to check the input data,
and if appropriate, the user can correct any problems identiﬁed to create a cleansed
input ﬁle before continuing with other SALT processes like record linkage. SALT
can also automatically cleanse bad data before proceeding in which is controlled
by the ONFAIL parameter of the FIELDTYPE statement. If the value in a ﬁeld is
not valid according to the editing constraints imposed, ONFAIL actions include:
IGNORE (data is accepted as is), BLANK (the value in the data ﬁeld is changed
to blank or zero depending on the type of the ﬁeld), CLEAN (removes any invalid
characters), or REJECT (removes/ﬁlter out records with the invalid ﬁeld data). The
following are sample FIELDTYPE statements:
FIELDTYPE:DEFAULT:LEFTTRIM:NOQUOTES("’):
FIELDTYPE:NUMBER:ALLOW(0123456789):
FIELDTYPE:ALPHA:CAPS:ALLOW(ABCDEFGHIJKLMNOPQRSTUVWXYZ):
FIELDTYPE:WORDBAG:CAPS:ALLOW
(ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789’):SPACES
( <>{}[]-ˆ=!+{\&},./):ONFAIL(CLEAN):
FIELDTYPE:CITY:LIKE(WORDBAG):LENGTHS(0,4..):ONFAIL(BLANK):
The DEFAULT ﬁeldtype applies to all ﬁelds unless overridden, and the LIKE
parameter allows ﬁeldtypes to be nested in a hierarchical manner. If the name of a
FIELDTYPE also matches the name of a ﬁeld like CITY, then the ﬁeld automatically
assumes the hygiene constraints of the FIELDTYPE with the same name. This
facilitates building a library of FIELDTYPE statements which can be used to insure
consistency across data models. Figure 8.10 shows a partial example of the SALT
data hygiene report.
4.3
Data Source Consistency Checking
SALT has the capability to check the ﬁeld value consistency between different
sources of data for an input ﬁle. This capability requires that the input ﬁle being
checked has a ﬁeld on each record designating from which unique source the

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
217
Fig. 8.10 SALT data hygiene report example
data was provided for the record. The SALT speciﬁcation ﬁle includes a special
SOURCEFIELD statement which provides the information needed to perform the
consistency checking. Consistency checking can be speciﬁed for all ﬁelds in the
record, or only speciﬁc ﬁelds. Typically consistency checking is used only on
speciﬁc ﬁelds where a consistent distribution of data values across all sources
is expected. For example, for an input ﬁle containing person names, we expect
data values in the last name ﬁeld would generally be consistently represented in
terms of its distribution within sources. For example, the last name Smith would be
represented in all sources and no source would have this data value in abnormally
high numbers compared to the average across all sources.
The output of the data source consistency checking process is a list of outliers,
data values whose distribution is not consistently represented across all sources.
This list contains the name of the data ﬁeld(s) being checked, the data value of
the outlier, the unique source identiﬁer, and the number of records containing the
outlier. These outliers could represent bad data values being introduced from a
speciﬁc source, missing data, or other anomalies and inconsistencies related to the
data source containing the outliers. Some outliers may be legitimate, for example if
the source ﬁeld contains a geographic identiﬁer, there may be high concentrations of
a particular last names in certain geographical areas which could be ﬂagged by the
consistency checking. Figure 8.11 shows partial sample output from a data source
consistency report.
4.4
Delta File Comparison
SALT includes the capability to compare two versions of a ﬁle and provides two
reports showing the differences. A differences summary report which outputs ﬁve

218
A.M. Middleton and D.A. Bayliss
Fig. 8.11 SALT data source consistency report example
records similar to the data proﬁling summary report for the records in the new ﬁle,
records in the old ﬁle, updated/changed records in the new ﬁle, records added to the
new ﬁle, and records deleted from the old ﬁle. The differences summary provides
the number of records for each of these categories (New, Old, Updates, Additions,
Deletions), and the percentage of non-blank data, maximum ﬁeld length, and
average ﬁeld length for every ﬁeld for each of the categories. The Changed category
is only available if an RIDFIELD statement is included in the speciﬁcation ﬁle.
A differences detail report which outputs any record (Added, Deleted, Changed)
which is different in the new ﬁle from the old ﬁle with additional columns to ﬂag
the type of change. Added and Changed records are shown from the new ﬁle, and
Deleted records are shown from the old ﬁle. The Changed category is only available
if an RIDFIELD statement is included in the speciﬁcation ﬁle and otherwise a
change is shown as an addition and deletion.
The delta difference reports show the differences between two versions of the
same ﬁle which has been updated through an ETL type of process, for example
a monthly update of a data source. Even though summary statistics are normally
generated in a typical ETL update process, the statistics for the delta difference
reports may highlight smaller errors that may be obscured by the statistics on the
full ﬁles. Figure8.12 shows partial sample output from delta difference summary
and detail reports.
4.5
Data Ingest
Data processing applications which maintain a base or authority ﬁle with informa-
tion on an entity typically require periodic updates with new or updated information.
The reading and processing of new information to add or update the base ﬁle is
usually referred to as a data ingest process. The SALT data ingest process applies
the ingest records to the base ﬁle and determines which records are: new, never
seen before; updates, identical record to an existing record in the base ﬁle but with
newer record dates; unchanged, identical to an existing record in the base ﬁle but not

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
219
Fig. 8.12 SALT delta difference summary and detail reports
altering a record date; and old, records exist in the base ﬁle but not in the ingest ﬁle.
SALT can generate code which will automatically perform data ingest operations as
an independent process, or as part of and combined with an internal record linking
process described later in this chapter.
The SALT data ingest process requires the ingest ﬁle format to match the record
layout of the base ﬁle. The base ﬁle record must include a numeric record id ﬁeld
speciﬁed by the RIDFIELD statement which uniquely identiﬁes any record in the
base ﬁle. The GENERATE option on the RIDFIELD statement allows fresh record
IDs to be automatically generated by the data ingest process. The base ﬁle may also
include a ﬁeld which indicates the external source ﬁle type for a record and a ﬁeld
which is the unique identiﬁer of the record from the data ingest ﬁle identiﬁed by the
source type speciﬁed by the SOURCEFIELD and SOURCERIDFIELD statements
in the speciﬁcation ﬁle. Including these ﬁelds allows SALT to provide additional
functionality including enhanced statistics. The base ﬁle and ingest ﬁle records may
also include speciﬁc date ﬁelds which indicate the ﬁrst date and the last date that the
data meets some condition such as being valid for the speciﬁed source, or when the
data ﬁrst added entered and last entered the base ﬁle for the speciﬁed source.
Three reports are produced by the data ingest process in addition to the
updated base ﬁle: (1) statistics by ingest change type and source deﬁned by the

220
A.M. Middleton and D.A. Bayliss
Fig. 8.13 SALT data ingest sample updated base ﬁle
SOURCEFIELD statement with record counts where type indicates old, new,
updated, or unchanged as described previously; (2) ﬁeld change statistics between
old and new records where the source ﬁeld as deﬁned by the SOURCEFIELD
statement and the unique id as deﬁned by the SOURCERIDFIELD statement
(vendor id for the sample data example shown below) match between old and new
records; and (3) record counts by ingest ﬁle source deﬁned by the SOURCEFIELD
statement. The updated base ﬁle will be identical in format to the previous base ﬁle
but can include an additional ﬁeld which will contain a numeric value corresponding
to the ingest change type: 0-unknown, 1-unchanged, 2-updated, 3-old, 4-new.
Figure8.13 shows a partial sample of an updated base ﬁle for a data ingest operation.
4.6
Generating Speciﬁcities
The ﬁrst step in running a SALT record linking process is to generate the ﬁeld value
and average ﬁeld speciﬁcities described in Sect. 3.2 that will be used as weights
for matching during the linking process. There are two different modes which can

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
221
Fig. 8.14 SALT speciﬁcities and speciﬁcity shift sample reports
be used: (1) a single-step mode in which speciﬁcity values are stored in persisted
ﬁles on the HPPC processing cluster, and (2) a two-step mode in which speciﬁcity
values are stored in key/index ﬁles. Speciﬁcities can take a large amount of time
to calculate when the base data is extremely large depending on the size (number
of nodes) of the processing cluster. The two-step mode allows the option of not
recalculating speciﬁcities each time a process like internal or external linking is run
based on updates to the base data. This can save a signiﬁcant amount of processing
time when data is updated and linked on a processing cycle such as a monthly build
to add new or changed data.
Initially in the SALT speciﬁcation ﬁle, the speciﬁcity and switch value infor-
mation is unknown for the FIELD, CONCEPT, and ATTRIBUTEFILE statements
(refer to the description of the FIELD statement in Sect. 3.7 for a further descrip-
tion). Once speciﬁcities have been calculated using the SALT generation process,
the average ﬁeld speciﬁcity and switch values can be added to the speciﬁcation ﬁle.
This information allows SALT to generate optimized code and set various thresholds
appropriately for the record linkage processes. SALT produces two reports when
speciﬁcities are generated: (1) the speciﬁcities report displays an average ﬁeld
speciﬁcity value, maximum speciﬁcity value, and switch value for each FIELD,
CONCEPT, and ATTRIBUTE statement in the speciﬁcation ﬁle. In addition, SALT
shows which values if any for each ﬁeld will also be treated as nulls (other than
blanks and zeros) by SALT in the matching process. The speciﬁcities shift report
shows the change (positive or negative) in speciﬁcity from the previous value in
speciﬁcation ﬁle. The ﬁeld value speciﬁcities are stored in either persisted data ﬁles
or index/key ﬁles depending on the generation mode selected. Persisted ﬁles are an
HPCC and ECL feature that allow datasets generated by ECL code to be stored, and
if a process is run again, and the code or other data affecting the persisted ﬁle has
not changed, it will not be recomputed. Figure 8.14 shows a partial speciﬁcities and
speciﬁcities shift report example for a sample data ﬁle.
4.7
Internal Linking
The goal of the internal linking process is SALT is to match records containing
data about a speciﬁc entity type in an input ﬁle and to assign a unique identiﬁer to

222
A.M. Middleton and D.A. Bayliss
records in the ﬁle which refer to the same entity. For example, in a ﬁle of records
containing customer information such as a customer order ﬁle, internal linking could
be used to assign a unique customer identiﬁer to all the records belonging to each
unique customer. Internal linking can also be thought of as clustering, so that records
referring to the same entity are grouped into clusters, with each cluster having a
unique identiﬁer.
SALT uses the ﬁeld value speciﬁcities as weights for determining record matches
in the internal linking process. For example, when two separate records are being
matched, SALT will compare each ﬁeld, concept, and attribute ﬁle in the two records
for similarity based on the deﬁnition of the ﬁeld speciﬁed by the FIELD, CONCEPT,
and ATTRIBUTEFILE statements in the SALT speciﬁcation ﬁle. If the values match
between the two records, the speciﬁcity for the value (scaled for fuzzy matches
and otherwise adjusted based on the editing options for the ﬁeld) will be added
to a total speciﬁcity to help determine a record match. Each ﬁeld deﬁned in the
speciﬁcation ﬁle for the record can make a positive, negative, or no contribution
to the total speciﬁcity. If the total speciﬁcity exceeds the pre-determined matching
threshold, then the two records are considered a match.
The tendency is to think of the record match decision as a yes/no question as in
many rule-based systems. However, since SALT uses speciﬁcity values for match
scores based on every ﬁeld value available in the input data, a record match score of
n C 1 denotes a link which is 2x less likely to be false than a score of n. In addition,
during an iteration of SALT internal linking, entity links are only generated (a) if
they are above the calculated threshold (either the default automatically calculated
by SALT or user-speciﬁed); and (b) are the highest scoring linkage for both records
involved in the link.
The internal matching process is iterative beginning with the input base ﬁle
and any additional ingest ﬁles which are merged with the input base ﬁle, with
each processing iteration attempting additional matches of records to records and
entity clusters formed in the previous iteration. As new entity clusters are formed or
expanded during each iteration, more information becomes available about an entity.
In a successive iteration, this may allow additional records or entire clusters to be
merged with an existing cluster. The output of each iteration effectively becomes the
training set for the next iteration, effectively learning from the previous iteration, as
new entity clusters are formed or extended and matching weights are recalculated.
Multiple iterations are usually required for convergence (no additional matches
occur) and to achieve high levels of precision and recall for a given population
of entities. A typical SALT-generated record linkage system will be iterated quite
extensively initially, but may only need additional iterations once or twice a month
as new or updated data is ingested.
The results from each iteration should be reviewed to determine if the record
matching results have met precision and recall goals or if under-matching or over-
matching has occurred. Adjustments may need to be made to ﬁeld and concept
deﬁnitions or the speciﬁcity matching threshold and the entire process repeated. If
the goals of the linking process have been met, the result of ﬁnal iteration becomes
the new linked base ﬁle. This result will contain the same number of records as

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
223
Fig. 8.15 SALT internal linking output results
the original input ﬁle, but the entity identiﬁer ﬁeld speciﬁed by the IDFIELD or
IDNAME statement on each record will now contain a unique identiﬁer for the
entity cluster to which the record belongs.
SALT produces a wealth of information to assess the quality of the results
for each iteration of the internal linking process. This information includes match
sample records, ﬁeld speciﬁcities used in the current iteration, pre- and post-iteration
ﬁeld population stats, pre- and post-iteration clustering stats showing the number
of clusters formed by record count for the cluster, the number of matches that
occurred, rule efﬁcacy stats showing how many matches occurred as a result of
each blocking/matching rule (each rule is implemented as an ECL join operation),
conﬁdence level stats showing total speciﬁcity levels for matches and how many
matches for each level, percentages of records where propagation assisted or was
required for a match, validity error ﬂags which can indicate an internal problem
with the process or data, a match candidates debug ﬁle which contains all the records
in the input ﬁle with individual ﬁeld value speciﬁcities appended and propagation
ﬂags appended, a match sample debug ﬁle which contains a record for each match
attempted with both left and right ﬁeld data and scores for ﬁeld matches and the
total match score, an iteration result ﬁle sorted in order of the entity identiﬁer, and
a patched match candidates ﬁle with the entity identiﬁer appended to each record.
SALT also produces various debug key ﬁles and a ID compare online service which
can be deployed to a HPCC Roxie cluster (refer to Chap. 4 for more information on
Roxie and the HPCC technology) that allows you to compare the data for two entity
identiﬁers to debug matches and non-matches. Figure8.15 shows an example of the
output results produced for each iteration of linking.
Figure 8.16 shows a partial result example of the post iteration cluster statistics
after the ﬁrst iteration of internal linking.

224
A.M. Middleton and D.A. Bayliss
Fig. 8.16 SALT internal
linking cluster statistics
The input base ﬁle for the internal linking process is speciﬁed by the FILENAME
statement in the speciﬁcation ﬁle. Other data ingest ﬁles can also be included which
will be appended and merged with the base ﬁle prior to the linking process. The
INGESTFILE statement allows you to deﬁne a dataset which provides records to
be ingested in the same format of the base ﬁle. Typically these ﬁles are generated
from external source ﬁles or base ﬁles for other types of entities. The data ingest is
executed automatically if the speciﬁcation ﬁle includes INGESTFILE statements.
After analyzing match sample records generated by the internal linking process
on each iteration, the results may indicate the system is overmatching (too many
false positives), or under matching (too many false negatives). False positive
matches are evidenced by entity clusters that have records which should not have
been included. False negative matches are evidenced by records which should have
been matched and included in an entity cluster, but were not. There are many reasons
why either of these conditions could be occurring including the need or adjustment
of parameters in the speciﬁcation ﬁle such as the FORCE on the FIELD statements,
and the overall deﬁnition of statements and concepts. If the matching criteria in you
speciﬁcation ﬁle appears to be correct, then the match threshold value may need to
be adjusted manually using the THRESHOLD statement.
The match sample records generated by the internal linking process include
samples of record matches at and above the match threshold, and also matches in
the range of the match threshold value within three points of speciﬁcity. If matches
below the threshold appear to actually be valid, then the match threshold may need
to be lowered. If records above the current match threshold appear to be invalid, then
you may need to raise the match threshold. A sufﬁcient number of records need to
be examined at the match threshold, below, and above before making a decision. It is
not uncommon to have some false positives and false negatives in a linking process.

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
225
SALT automatically generates an ID Compare Service for use with internal
linking. Once the service has been deployed to a HPCC Roxie cluster, the query
can be accessed manually through the WsECL interface. The query allows you to
look at all the data associated with two identiﬁers to see if they should be joined. It is
also useful for looking at all the data associated with a speciﬁc entity identiﬁer if you
only enter one identiﬁer. SALT also automatically generates an ID sliceout service
for use with internal linking. This query allows examination of records which the
internal linking process has identiﬁed as sliceouts.
4.8
External Linking
The goal of the external linking process of SALT is to match records containing
data about a speciﬁc entity type in an external ﬁle or online query to a previously
linked base ﬁle of entities and to assign a unique entity identiﬁer from the base ﬁle
to records in the external ﬁle or to the query which refer to the same entity. External
linking is also useful in establishing foreign key relationships between an external
ﬁle and an existing ﬁle based on the unique entity identiﬁer. For example, in an
external ﬁle of records containing property information for people, external linking
could be used to assign a unique person entity identiﬁer to all the property records
associated with a base ﬁle of people. External linking can also be thought of as
entity resolution, so that records or online queries containing information about an
entity are resolved by matching the records to a speciﬁc entity in an authority ﬁle,
and assigning the corresponding unique entity identiﬁer.
The external linking capability requires a previously linked input ﬁle in which
all the records have been clustered for a speciﬁc entity type. The linked input ﬁle is
used to build keys required for external matching. The linked ﬁle is a single ﬂat ﬁle
that functions as the authority or base ﬁle to be used for matching corresponding
ﬁelds from an external ﬁle to perform the entity resolution process. The records in
this authority ﬁle should contain all the ﬁelds to be used for matching with an entity
identiﬁer (unique ID for the associated entity cluster) assigned to each record. The
authority ﬁle can be the output of a previous SALT internal linking process.
The key to implementing an efﬁcient external linking capability with high pre-
cision and recall using SALT is the choice of linkpaths deﬁned by the LINKPATH
statement in the speciﬁcation ﬁle. Linkpaths are introduced in Sect. 3.6. Figure8.17
shows an example of LINKPATH statements used for external linking of a base ﬁle
of person entities.
Each LINKPATH statement will result in the creation of an ECL Index (key)
ﬁle which is used in the external matching process. The ultimate responsibility
for choosing linkpaths to be used for external linking entity resolution rests with
the developer. Linkpath deﬁnitions in the speciﬁcation ﬁle can be divided into
required (compulsory for a match) and non-required ﬁelds. User-deﬁned linkpaths
are speciﬁed using the LINKPATH statement beginning with a linkpath name,

226
A.M. Middleton and D.A. Bayliss
Fig. 8.17 SALT LINKPATH deﬁnitions example
followed by a speciﬁc ﬁeld list with the required ﬁelds ﬁrst, followed by optional
ﬁelds and then extra-credit ﬁelds as described in Sect. 3.6.
Each ﬁeld in the authority ﬁle to be used for external linking is deﬁned in the
speciﬁcation ﬁle using either the FIELD or CONCEPT statement, or can be a value
ﬁeld in an attribute ﬁle speciﬁed in an ATTRIBUTEFILE statement, and the entity
identiﬁer is deﬁned using the IDFIELD statement. The speciﬁcity of each ﬁeld,
concept, or attribute ﬁle value ﬁeld must be included, so speciﬁcities on the authority
ﬁle need to be generated if the speciﬁcities are not already known from a previous
internal linking process. If the ﬁeld deﬁnition includes a BESTTYPE deﬁnition with
a propagation method, propagation of ﬁelds within entity clusters in the authority
ﬁle will be automatically handled to improve matching results. Field deﬁnitions used
for external linking can include the MULTIPLE parameter which speciﬁes that the
external ﬁle matching ﬁeld contains multiple values. FIELDTYPE statements can
also be used in a speciﬁcation ﬁle used for external linking, and if included are used
to clean the data for the external linking keybuild process, and also to clean external
ﬁle data or queries for the search process.
The required ﬁelds deﬁned in a linkpath must match exactly during external
linking. Optional ﬁelds must match if provided, and if the ﬁeld is not deﬁned as
MULTIPLE, then fuzzy matches are adequate. Extra-credit ﬁelds do not need to
match, but add to the total matching score if they do and can also included any of
the fuzzy matching edit characteristics.
SALT also automatically creates an additional key called the UBER key using
all the ﬁelds and concepts deﬁned in your speciﬁcation ﬁle. By default, UBER
key is not used unless an external record or query fails to match any records
using the linkpaths you have deﬁned, essentially providing a “fallback” alternative
for searching. The default behavior can be changed by using a parameter on the
PROCESS statement in your speciﬁcation ﬁle. The parameters include ALWAYS
(the search process will always use the UBER key as well as any other linkpath
speciﬁed), REQUIRED (same as the default, the UBER key will be used if none
of the other linkpaths could satisfy the query), and NEVER (the UBER key is not
used for external linking or searching). The UBER key can provide recall lift when
the data in the external record or query does not match any existing linkpath, but
at a higher processing cost. The UBER key does not support any form of fuzzy
matching, all ﬁelds provided much match exactly for a search to be successful.
Another interesting feature of the UBER key which can raise recall signiﬁcantly
is that it works entirely at the entity level. Thus if any entity record has a particular
middle name and any entity record has a particular address, then the entity will be

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
227
Fig. 8.18 SALT external linking keybuild results
Fig. 8.19 SALT City,State,Company Name LINKPATH key example ﬁle
returned; even if both did not originally appear on the same record. This feature
allows an UBER key search to work with many multiple ﬁelds. You can search, for
example, for someone with two different last names who have lived in two different
counties.
The SALT external linking process will mandate that some ﬁelds deﬁned for a
linkpath become required for a link to occur based on the total speciﬁcity required
for a match. The SALT external linking process will also automatically divide
the non-required ﬁelds in a linkpath into optional and extra-credit ﬁelds if the
speciﬁcation ﬁle has not done that already.
Before the SALT external linking capability can be used, a keybuild process on
the internal base ﬁle must be run. The speciﬁcation ﬁle must be edited to ensure that
all FIELD, CONCEPT, ATTRIBUTEFILE, and LINKPATH statements required
for the matching process are deﬁned and ﬁeld speciﬁcities are included. Figure 8.5
shows an example of a speciﬁcation ﬁle deﬁned for external linking. Figure8.18
shows the results of a keybuild process. Figure 8.19 is a partial sample of a key ﬁle
but for the LINKPATH:CS shown in Fig. 8.5 which begins with CITY and STATE
as required ﬁelds and COMPANY NAME as an optional ﬁeld.

228
A.M. Middleton and D.A. Bayliss
Once the external linking keybuild is completed, record matching using an
external ﬁle to your internal/base ﬁle can be processed. Batch mode external linking
allows you to perform the external linking function on a HPCC Thor cluster as
a batch process (refer to Chap. 4 for more information on Thor and the HPCC
technology). SALT automatically generates a macro which can be used in ECL code
implemented to perform the actual matching process.
The output dataset from the external linking batch process contains a corre-
sponding record for any external ﬁle record which contained sufﬁcient data for
matching to a deﬁned linkpath. This is determined by ﬁltering the external ﬁle
records to ensure that the records contained data in the required ﬁelds in the linkpath.
Each record in the output dataset contains a parent record with a reference ﬁeld
corresponding to a unique id assigned to the external input ﬁle prior to the external
linking process, a set of Boolean result ﬂags, and a child dataset named results
containing the results of the matching process. Resolved records (successful linking
to an entity in the base ﬁle) in the output dataset are indicated by the Boolean
resolved ﬂag set to true. The reference ﬁeld for each record in the child dataset is the
same as the reference on the parent record. The matching process will return one or
more result records with scores in the child dataset depending on how many viable
matches to different entities in the internal base are found. The identiﬁer speciﬁed
by the IDFIELD statement in your speciﬁcation ﬁle will contain the matching entity
identiﬁer. The output recordset can be used to append resolved entity identiﬁers
to the external input ﬁle based on the reference ﬁeld, or for other application
uses such as to display the candidate matches for a query when the record is not
resolved.
SALT external linking automatically generates two deployable Roxie services to
aid in debugging the external linking process which also can be used for manual
examination of data to evaluate linkpaths, as well as to support the online mode
external linking capability described later in this section. These services also provide
an example for incorporating online external linking and searching the base ﬁle into
other online queries and services.
Online mode external linking allows the external linking function to be per-
formed as part of an online query on a HPCC Roxie cluster. This capability can
be utilized to incorporate external linking into other Roxie-based online queries
and applications or you can use the provided online service for batch mode linking
from a Thor. SALT automatically generates a macro can be used in the ECL code
implemented to perform the actual matching process for an online mode batch
external linking application. Figure 8.20 shows an example of the automatically
generated online service and manual query for entity resolution. Figure 8.21 shows
the entity resolution result for this query.
The same Boolean ﬂags used for batch mode external linking including the
resolved ﬂag are displayed along with the weight ﬁeld which contains the score
for the match and ID ﬁeld for the resolved entity (bdid in this example).

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
229
Fig. 8.20 SALT external linking online query example input
Fig. 8.21 SALT external linking entity resolution result example
4.9
Base File Searching
SALT provides an additional query which displays all the entity records from the
internal base ﬁle matching the input information. This query is useful in debugging
the external linking process to assess how a particular record was resolved or not
resolved to an entity. The ECL function called by this service provides a base ﬁle
search capability that can be incorporated into other HPCC online applications.
The base ﬁle search is intended to return records organized with the records
which best match the search criteria ﬁrst. All data returning from the search is

230
A.M. Middleton and D.A. Bayliss
Table 8.2 Search criteria ﬁeld match grading
Value
Description
2
Search criteria supplied, but does not match this record
1
Search criteria supplied, but this record has a blank
0
Search criteria not supplied
1
Search criteria is fuzzy match to this record
2
Search criteria is a match to this record
Fig. 8.22 SALT sample base ﬁle search results
graded against the search criteria, and for each ﬁeld in the data a second ﬁeld is
appended which will contain one of the following values: (Table 8.2).
Figure 8.22 shows an example of the base ﬁle search results using the same
query shown in Fig. 8.20. Each record will have two scores. Weight is the speciﬁcity
score allocated to the IDFIELD identiﬁer (bdid for the example). The record score
is the sum of all the values listed above for each ﬁeld. Records with the highest
record score are sorted and displayed ﬁrst. Additional Boolean status ﬁelds show
if the record is a full match to the search criteria if true, and if the value for the
IDFIELD has at least 1 record which fully matches the search criteria.
Depending on the search criteria, the SALT will use the deﬁned LINKPATHs
and the UBER key to perform the search. Specifying extra credit ﬁelds in the
LINKPATH statements is beneﬁcial to ensure that the best records are included
in the search results and returned ﬁrst. If attribute ﬁles have been included in the
external linking process, their contents are also displayed by the base ﬁle search.

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
231
The base ﬁle search can also be run using only an entity id, and all records matching
the entity id are displayed.
4.10
Remote Linking
Although the primary purpose of SALT is to create entity clusters in a base ﬁle
through internal linking and to provide entity resolution of external records using
external linking to an existing base ﬁle, it is also possible to use SALT to generate
code that will perform record matching and scoring and link together records that are
completely independent from a base ﬁle without directly using the base ﬁle during
the linking process. This capability is called remote linking within SALT. For remote
linking, SALT still generates speciﬁcity weights from the base ﬁle data which can
be used to signiﬁcantly improve the quality of record to record matching/linking
assuming the records contain ﬁelds with the same type of data in the base ﬁle.
The remote linking capability is implemented as an online compare service
for the HPCC Roxie cluster, which compares the ﬁelds in two records and
generates scoring information similar to SALT internal linking. This allows user-
deﬁned matching to be implemented in a Roxie query, using the power of SALT
generated statistics, speciﬁcity weights, and ﬁeld editing features on the independent
records to improve the matching result. Remote linking requires the deﬁnition of a
speciﬁcation ﬁle for the ﬁelds that will be matched from the base ﬁle The base ﬁle
is used only for calculating the speciﬁcities needed for remote matching, the base is
not actually used during the remote linking process.
The remote linking code works by constructing two input records from input data
to the service which are then passed to the internal linking process to determine
if they would link using the following steps: (1) the normal cleaning process is
performed as required on input data for ﬁelds deﬁned with editing constraints
using FIELDTYPE statements in the speciﬁcation ﬁle; and (2) the weighting and
scoring is done exactly as if an internal linking process was executed without any
propagation. In this manner, remote linking can be added to a conventional record
linking application to provide improved matching decisions.
5
Summary and Conclusions
Data integration and data analysis are fundamental data processing requirements
for organizations. Organizations now collect massive amounts of data which has
led to the Big Data problem and the resulting need for data-intensive computing
architectures, systems, and application solutions. Scalable platforms such as Hadoop
and HPCC which use clusters of commodity processors are now available which
can address data-intensive computing requirements. One of the most complex and
challenging data integration applications is record linkage [13]. Record linkage
allows information from multiple sources that refer to the same entity such as a

232
A.M. Middleton and D.A. Bayliss
person or business to be matched and identiﬁed or linked together. The record
linkage process is used by organizations in many types of applications ranging from
maintaining customer ﬁles for customer relationship management, to merging of all
types of data into a data warehouse for data analysis, to fraud detection.
This chapter introduced SALT, a code generation tool for the open source HPCC
data-intensive computing platform, which can automatically generate executable
code in the ECL language for common data integration applications including
data proﬁling, data hygiene, record linking and entity resolution. SALT provides
a simple, high-level, declarative speciﬁcation language to deﬁne the data and
process parameters in a user-deﬁned speciﬁcation ﬁle. From the speciﬁcation ﬁle,
SALT generates ECL code which can then be executed to perform the desired
application. SALT encapsulates some of the most advanced technology and best
practices of LexisNexis Risk Solutions, a leading aggregator of data and provider
of information services signiﬁcantly increasing programmer productivity for the
applications supported. For example, in one application used in LexisNexis Risk
Solutions for processing insurance data, a 42-line SALT speciﬁcation ﬁle generates
3,980 lines of ECL code, which in turn generates 482,410 lines of CCC. ECL code
is compiled into CCC for efﬁcient execution on the HPCC platform.
SALT speciﬁc record linking capabilities presented in this chapter include
internal linking, a batch process to link records from multiple sources which refer
to the same entity to a unique entity identiﬁer; external linking, the batch process
of linking information from an external ﬁle to a previously linked base or authority
ﬁle in order to assign entity identiﬁers to the external data (entity resolution), or an
online process where information entered about an entity is resolved to a speciﬁc
entity identiﬁer, or an online process for searching for records in an authority ﬁle
which best match entered information about an entity; and remote linking, an online
capability that allows SALT record matching to be incorporated within a custom
user application. The key beneﬁts of using SALT can be summarized as follows:
•
SALT automatically generates executable code for the open source HPCC
data-intensive computing platform to address the Big Data problems of data
integration.
•
SALT provides important data preparation applications including data proﬁling,
data hygiene, and data source consistency checking which can signiﬁcantly
reduce bugs related to data cleanliness and consistency.
•
SALT provides record linking applications to support clustering of data referring
to the same entity, entity resolution of external data to a base or authority ﬁle, and
advanced searching capabilities to ﬁnd data related to an entity, and generates
code for both batch and online access.
•
SALT automatically generates ﬁeld matching weights from all the available
data, and calculates default matching thresholds and blocking criteria for record
linking applications.
•
SALT incorporates patent-pending innovations to enhance all aspects the record
linkage process including new approaches to approximate string matching such
as BAGOFWORDS which allows matching to occur with no order dependency

8
Salt: Scalable Automated Linking Technology for Data-Intensive Computing
233
of word tokens and using the speciﬁcity of the individual words contained in the
ﬁeld as weights for matching.
•
SALT data hygiene supports standard and custom validity checking and auto-
matic cleansing of data using ﬁeld editing constraints deﬁned by FIELDTYPE
statements which can be standardized for speciﬁc data ﬁelds.
•
SALT record linking applications are data neutral and support any data type
available in the ECL programming language, support both real-world and
abstract entity types, can provide higher precision and recall than hand-coded
approaches in most cases, can handle relationships and dependencies between
individual ﬁelds using CONCEPT statements, support calculation of best values
for a ﬁeld in an entity cluster using the BESTTYPE statement which can
be used to propagate ﬁeld values to increase matching precision and recall,
support additional relationship detection for non-obvious relationships between
entity clusters using the RELATIONSHIP statement, provide many built-in fuzzy
matching capabilities, and allow users to deﬁne custom fuzzy-matching functions
using the FUZZY statement.
•
SALT applications are deﬁned using a simple, declarative speciﬁcation language
edited in a standard text ﬁle, signiﬁcantly enhancing programmer productivity
for data integration applications.
•
SALT automatically generates statistics for processes which can be utilized to
analyze cyclical changes in data for repeating processes and quickly identify
problems.
•
SALT is provided and supported by LexisNexis Risk Solutions, a subsidiary of
Reed Elsevier, one of the largest information companies in the world.
Using SALT in combination with the HPCC high-performance data-intensive
computing platform can help organizations solve the complex data integration
and processing issues resulting from the Big Data problem, helping organizations
improve data quality, increase productivity, and enhance data analysis capabilities,
timeliness, and effectiveness.
References
1. Bilenko, M., & Mooney, R. J. (2003, August 24–27). Adaptive duplicate detection using
learnable string similarity measures. Proceedings of the KDD ’03 Ninth ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, Washington, D.C.,
39–48.
2. Branting, L. K. (2003). A comparative evaluation of name-matching algorithms. Proceedings
of the ICAIL ’03 9th International Conference on Artiﬁcial Intelligence and Law, Edinburgh,
Scotland, 224–232.
3. Christen, P. (2008). Automatic record linkage using seeded nearest neighbor and support
vector machine classiﬁcation. Proceedings of the KDD ’08 14th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, Las Vegas, NV, 151–159.
4. Cochinwala, M., Dalal, S., Elmagarmid, A. K., & Verykios, V. V. (2001). Record matching:
Past, present and future (No. Technical Report CSD-TR #01-013): Department of Computer
Sciences, Purdue University.

234
A.M. Middleton and D.A. Bayliss
5. Cohen, W., & Richman, J. (2001). Learning to match and cluster entity names. Proceedings of
the ACM SIGIR’01 workshop on Mathematical /Formal Methods in IR.
6. Cohen, W., & Richman, J. (2002). Learning to match and cluster large high-dimensional data
sets for data integration. Proceedings of the KDD ’02 Eighth ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, Edmonton, Alberta, Canada.
7. Cohen, W. W. (2000). Data integration using similarity joins and a word-based information
representation language. ACM Transactions on Information Systems, 18(3).
8. Cohen, W. W., Ravikumar, P., & Fienberg, S. E. (2003, August). A comparison of string
distance metrics for name matching tasks. Proceedings of the IJCAI-03 Workshop on
Information Integration, Acapulco, Mexico, 73–78.
9. Dunn, H. L. (1946). Record linkage. American Journal of Public Health, 36, 1412–1415.
10. Fellegi, I. P., & Sunter, A. B. (1969). A theory for record linkage. Journal of the American
Statistical Association, 64(328), 1183–1210.
11. Gravano, L., Ipeirotis, P. G., Koudas, N., & Srivastava, D. (2003, May 20–24). Text joins in an
RDBMS for web data integration. Proceedings of the WWW ’03 12th international conference
on World Wide Web, Budapest, Hungary.
12. Gu, L., Baxter, R., Vickers, D., & Rainsford, C. (2003). Record linkage: Current practice
and future directions (No. CMIS Technical Report No. 03/83): CSIRO Mathematical and
Information Sciences.
13. Herzog, T. N., Scheuren, F. J., & Winkler, W. E. (2007). Data quality and record linkage
techniques. New York: Springer Science and Business Media LLC.
14. Jones, K. S. (1972). A statistical interpretation of term speciﬁcity and its application in
information retrieval. Journal of Documentation, 28(1), 11–21.
15. Koudas, N., Marathe, A., & Srivastava, D. (2004). Flexible string matching against large
databases in practice. Proceedings of the 30th VLDB Conference, Toronto, Canada,
1078–1086.
16. Maggi, F. (2008). A survey of probabilistic record matching models, techniques and tools (No.
Advanced Topics in Information Systems B, Cycle XXII, Scientiﬁc Report TR-2008-22): DEI,
Politecnico di Milano.
17. Middleton, A. M. (2010). Data-intensive technologies for cloud computing. In B. Furht & A.
Escalante (Eds.), Handbook of cloud computing (pp. 83–136). New York: Springer.
18. Newcombe, H. B., & Kennedy, J. M. (1962). Record linkage. Communications of the ACM,
5(11), 563–566.
19. Newcombe, H. B., Kennedy, J. M., Axford, S. J., & James, A. P. (1959). Automatic linkage of
vital records. Science, 130, 954–959.
20. Robertson, S. (2004). Understanding inverse document frequency: On theoretical arguments
for IDF. Journal of Documentation, 60(5), 503–520.
21. Winkler, W. E. (1989). Frequency-based matching in Fellegi-Sunter model of record linkage.
Proceedings of the Section on Survey Research Methods, American Statistical Association,
778–783.
22. Winkler, W. E. (1994). Advanced methods for record linkage. Proceedings of the Section on
Survey Research Methods, American Statistical Association, 274–279.
23. Winkler, W. E. (1995). Matching and record linkage. In B. G. Cox, D. A. Binder, B. N.
Chinnappa, M. J. Christianson, M. J. Colledge & P. S. Kott (Eds.), Business survey methods.
New York: John Wiley & Sons.
24. Winkler, W. E. (1999). The state of record linkage and current research problems: U.S. Bureau
of the Census Statistical Research Division.
25. Winkler, W. E. (2001). Record linkage software and methods for merging administrative lists
(No. Statistical Research Report Series No. RR/2001/03). Washington, D.C.: US Bureau of the
Census.

Chapter 9
Parallel Processing, Multiprocessors
and Virtualization in Data-Intensive Computing
Jonathan Burger, Richard Chapman, and Flavio Villanustre
1
Introduction
Efﬁcient use of hardware resources is the cornerstone to achieve the highest possible
performance out of any Data Intensive cluster [1]. Utilization levels of subsystems
within each node and across all the nodes in the cluster, such as CPU, Memory, I/O
and disk vary at different phases of the execution plan. Balancing these resources
during the architectural design can be challenging.
One of the most interesting aspects of performance scalability is associated
with the CPU subsystem. In the early days, performance improvements obeyed
mostly to CPU frequency scaling over previous generations. Once frequency scaling
wasn’t feasible anymore, superscalar CPU’s took advantage of technologies such
as speculative and out of order execution [2, 3] and branch prediction [4] to
parallelize instructions and still appear as a single processing pipe to the software.
After encountering scalability limitations in this paradigm, modern systems rely on
multiple apparent execution paths through the use of Symmetric Multiprocessing
(SMP), Multi-core and Hyper-threading.
The open source HPCC platform is a Data Intensive Computing system platform
developed by LexisNexis Risk Solutions. The HPCC platform incorporates a
software architecture implemented on commodity computing clusters and provide
for high-performance, data-parallel processing for applications utilizing Big Data.
The HPCC platform includes system conﬁgurations which support both, parallel
batch data processing (Thor), and high-performance online query applications
using indexed data ﬁles (Roxie). The HPCC platform also includes a data-centric
declarative programming language for parallel data processing called ECL (see
Chap.4 for a more extensive description of the HPCC platform).
J. Burger () • R. Chapman • F. Villanustre
LexisNexis Risk Solutions, LexisNexis, GA, USA
e-mail: Jon.Burger@lexisnexis.com; Richard.Chapan@lexisnexis.com;
Flavio.Villanustre@lexisnexis.com
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 9, © Springer Science+Business Media, LLC 2011
235

236
J. Burger et al.
Thor, LexisNexis answer to large scale data extraction, transformation, loading
and linking [5], has been designed from the grounds up as a distributed processing
platform, able to run on a share nothing loosely coupled architecture. As a
distributed processing platform, different nodes can process independently without
the need for exhaustive synchronization primitives. The obvious natural evolution
path is to provide for parallel execution within the same node, maximizing the
efﬁcient use of all the available processing units within the node itself.
There are several different architectural approaches. In the rest of this chapter
we’ll present the beneﬁts and disadvantages of each of these approaches together
with our experience with the LexisNexis Thor cluster architecture in particular.
2
Segmentation, Isolation, and Virtualization
One of the most important roles of modern Operating System kernels is the
arbitration of hardware resources in multitasking and multiprocessing environments.
Different processes execute in isolated memory spaces and, with very few excep-
tions, have little interaction with other processes alternatively or simultaneously
running on the same hardware. In addition to this, virtual memory managers provide
the illusion of a contiguous virtual memory space, even if the underlying hardware
sparsely assigns memory pages from a unique global pool. In any case, all of these
processes do share certain components such as parent processes, system libraries
and a common kernel.
Over the years, certain techniques have been developed to provide further
isolation on shared hardware resources. The most relevant ones can be classiﬁed into
the following categories: Emulation, Hardware level Virtualization and Operating
System level Virtualization (also known as partitioning).
Emulation involves the complete re-implementation in software of the underlying
hardware components of a system. It allows for running applications on non-native
architectures (for example, an x86 operating system running on a PowerPC system).
With very few exceptions [6] the performance penalty paid in emulation is so
high, that this technique is relegated to academic, experimental and development
environments, with little application in production systems.
Hardware level Virtualization represents the segmentation of the underlying
physical hardware to ensure that a complete Operating System (system virtual
machine) or a process (process virtual machine) [7] can run in complete isolation
from the rest of the software components running on the same hardware without
incurring in the overhead that true emulation has. Certain capabilities are required
in the underlying hardware to allow for virtualization, and most modern processor
architectures include this functionality [8–10]. Virtual machines can run on top of
the bare hardware (Type 1 or native VM) [11] or on top of an Operating System
(Type 2 or hosted VM) [12]. The overhead that virtualization has, varies largely
depending on the particular subsystem, and different factors inﬂuence the extra

9
Parallel Processing, Multiprocessors and Virtualization in Data-Intensive Computing
237
computing time required by this technique (a cache may need to be ﬂushed, memory
pages may require to be wiped, an execution pipeline may be discarded, branch
prediction could be invalidated, etc.).
Operating System level Virtualization, also known as partitioning or container-
based virtualization [13], segments the process space into compartments also known
as jails, guests, zones, etc. It provides for isolation beyond the level normally
provided by the Operating System kernel (processes cannot communicate with other
processes outside of their own partition), and has relatively low overhead when
compared to Virtualization and Emulation (although it is generally slower than the
standard process isolation).
3
Why Parallelize?
Traditionally, only high end architectures used to be equipped with more than one
CPU, so software design was modeled after the assumption that only one thread
of execution would ever be running at any given time (even if the Operating
System provided the illusion of multi-processing and multi-threading). After the
CPU frequency race came to a halt, and CPU manufacturers starting packing
multiple CPU cores in the same die, Symmetric Multi-Processing (SMP) became
commonplace [14].
With multiple execution engines, general purpose systems running dozens of
programs at the same time automatically increased the overall amount of work done
by unit of time (even if the execution time of each individual program was about the
same as before). The bigger challenge resided in speeding up algorithms originally
designed to be executed serially in a single execution thread. Several computing
models have been proposed and special synchronization primitives were developed
to support these models [15,16].
Data oriented algorithms, on the other hand, are fortunately well suited for
parallelization; data problems range from those that can be considered embarrass-
ingly parallel to those that require a signiﬁcant amount of synchronization. An
example of the former is a direct and context independent transformation applied
to individual data records; an example of the latter is a sort operation where
execution threads need to know about the others (excluding Radix sort and similar
methods).
3.1
Why Use Multiple Nodes?
The cost of scaling beyond a few processors in a single computer system can be
substantial, as high end systems have a premium cost associated with them. In
addition to this, there are certain resources such as system bus, I/O subsystems and
memory interconnect that cannot be scaled beyond a certain practical limit.

238
J. Burger et al.
For this reason and due to the above mentioned fact that data algorithms tend to ﬁt
parallel execution models well, is that the data intensive supercomputing paradigm
was born [17].
The days of the large and often expensive single server, packed with as much
processing power as the engineers could design were beginning to dwindle. Busi-
nesses that adopted the parallel processing approach suddenly had a powerful edge
over their competition. The alternative centralized approach was at a disadvantage,
being limited by the boundaries of a single large machine. In a centralized model,
in order to increase the processing power, companies are required to often replace
this large expensive server every few years at great expense, while companies using
the parallel processing design could simply add small inexpensive servers to their
existing cluster.
The open source movement and the growing stability of free operating systems
such as Linux and BSD further increased the advantage of the parallel processing
design model over the single large server. Cluster expansion became mostly a
“hardware only” expense as the Operating System cost was no longer a factor
to consider. This gave companies adopting parallel data processing models a
further edge.
3.2
Why Use Multiple Cores?
As hardware manufacturers continued to improve server efﬁciency by packing more
and more processing cores into a smaller space while reducing the overall power
consumption, the effective use of these additional computing resources became
paramount. Software design needed to take into account this trend in order to fully
utilize the total computing power offered by a single server. This led to greater
efﬁciency and, in turn, the opportunity for greater proﬁt in business.
As discussed in the introduction to this chapter, there are different approaches to
multiprocessing within a server. While traditional software could be re-engineered
to use lightweight threads, also known as multithreading, which could parallelize
execution across multiple CPU cores, a more beneﬁcial strategy can be applied if the
software was already designed for a distributed parallel execution model. Multiple
computing cores within a single server are analogous to multiple processors
distributed across a network with the exception of that, in the case of the multi-core
server, intercommunication between these cores has higher bandwidth, they may
have some cache in common and access to shared memory is extremely fast [18].
When a large number of threads of execution need to be created and de-
stroyed, the lightweight multithreading computing model provides a well-deﬁned
advantage over the more traditional multiprocessing model. Lightweight threads
have signiﬁcantly less overhead as certain requirements related with isolation
and segmentation can be bypassed. Due to the fact that threads normally run
in a common memory space sharing resources, expensive operations such as
memory cleanup, inter-process communication and full isolation mechanisms can

9
Parallel Processing, Multiprocessors and Virtualization in Data-Intensive Computing
239
be avoided; context switching among threads becomes inexpensive too since most of
the protection mechanisms are unnecessary. On the counter side, lightweight threads
are vulnerable to the fact that any thread in the pool misbehaving can affect every
other thread. Since there is no memory protection, data structure corruption, stack
overﬂow and race conditions can compromise the entire thread pool [19].
The beneﬁts of the multithreading model described above are clear if the system
needs to either create threads at a signiﬁcant rate or have a large pool of inter-
dependent execution threads alternatively or simultaneously executing at any given
time. However, if the number of threads of execution is fairly constant and there are
enough processing cores to guarantee that no pre-emption ever needs to be made,
the beneﬁts of lightweight threads are quickly washed out and their disadvantages
prevail.
In the speciﬁc case where the software has been designed for parallel execution
across a cluster, there may be further reasons why considering a multiprocessing
approach is even more beneﬁcial. Most likely operational considerations such
as fault tolerance, redundancy, and monitoring have been factored in during the
application design. Re-implementing these features while porting the software to a
multithreading model can be substantially difﬁcult; it will also be signiﬁcantly more
expensive than maintaining the multi-process approach.
LexisNexis Thor has been designed as a distributed processing engine from the
ground up and matches the requirements mentioned above to ﬁt a multi-processing
strategy better. Thor processes, which would normally run in independent nodes, can
execute in different cores. Since all Thor processes are started early and they execute
in separate cores, impact from process forking and pre-emption is negligible to non-
existent. Existing mechanisms for fault tolerance, redundancy and monitoring can
be utilized, with the only difference being that now each physical node will contain
multiple “sub nodes” or “virtual nodes”.
Since Thor processes are self-contained and run independently from the rest,
full isolation through virtualization and containers is not needed, thus eliminating
the overhead that these solutions require. In addition to this, it’s beneﬁcial to
allow Thor processes to take advantage of shared resources, allowing for moderate
oversubscription which can produce the best performance based on concurrency
during normal workloads.
4
Bottlenecks
One of the biggest challenges when designing parallel distributed data intensive
platforms is to ensure adequate resource balance across the cluster [20]. This
is especially true when intra-node multiprocessing is required, as certain shared
resources such as data busses can be easily overlooked as subsystems prone to
congestion. Unlike numerical distributed clusters, where contention will almost
exclusively occur due to processor utilization (workloads in these systems are
characterized by very small data and a signiﬁcant number of integer and ﬂoating

240
J. Burger et al.
point vector operations) contention in data intensive platforms can arise from a
wide range of components. Data intensive workloads are characterized by a constant
movement of relatively large data around the system, and the execution of mostly
string and integer operations [21]. Although LexisNexis Thor takes special care
of minimizing the amount of data transfers between processes by exploiting data
locality as much as possible, data ingress and egress from and to the ﬁlesystem in
each process is still one of the most prevalent factors when determining performance
of the system.
As data ﬂows from the ﬁlesystems and into the Processor to be parsed and
transformed, latencies in the memory subsystem and efﬁciency in the local CPU
caches become a signiﬁcant component of overall performance. The move from the
Core and Core 2 architectures to Nehalem, with the integrated memory controller,
larger caches and faster memory with a three-way interleave using DDR3 brought
considerable performance improvement in itself [22].
Even though concurrency can be thought as simultaneous execution of the
same operations across the cluster at the exact same time, factors such as natural
divergence due to varying latencies and caching provide opportunity for efﬁcient
use of different system resources (for example, when a process is reading data from
the drive into memory, a second process may be moving data from memory to CPU
cache and a third process may be accessing the CPU L1/L2 cache). Hyper-threading,
a hardware based threading implementation from Intel to further utilize unused
resources within the CPU by exposing two individual hardware based threads of
execution to the Operating System, can further provide a moderate performance
improvement; however, since certain CPU hardware resources are shared, the
opportunity for improvement is normally up to 10–15% in Nehalem processors [23].
One of the most interesting considerations that need to be made when intending
to run multiple Thor processes in a single node is around the disk subsystem. Due
to the nature of the Thor workload, most of the disk access is sequential. With a
single Thor process requesting disk access on a reasonably defragmented ﬁlesystem,
the number of disk head seeks is kept to a minimum. Since disk seeks can be a
major source for latency in the disk subsystem, particularly when using mechanical
hard drives (as opposed to Solid State Drives), disk seeks can affect the overall
performance in a very signiﬁcant way; latency introduced by the disk subsystem
implies that any operations serialized after synchronous disk access are subject to
blocking until the data is retrieved. When there are multiple processes competing for
the same disk resources, the opportunity for non-sequential data access increases
signiﬁcantly as different process will attempt to read data residing in different
sectors on the platters; this can have an important detrimental impact on the overall
performance of the platform due to the extra latency arising from the random disk
access. Fortunately there are techniques to reduce the contention that drive access
can impose; some of the most common methods include reordering disk requests,
de-coupling disk access through more efﬁcient caching strategies and moving to
drives with better random access proﬁles (i.e., SAS vs SATA, Solid State Drives vs
Hard Drives, etc.).

9
Parallel Processing, Multiprocessors and Virtualization in Data-Intensive Computing
241
Since the co-existence of multiple processes in the same node changes the disk
access pattern from mostly, or exclusively, sequential to random as described above,
and mechanical storage subsystems can exhibit signiﬁcant latency during this access
pattern due to the increased number of disk seeks, special emphasis needs to be
put on improving the disk subsystem in order to take full advantage of extra CPU
cores. One of the most common ways to improve overall disk performance is to
move from 3:50 SATA drives (usually the choice for sequential access systems due
to their lower cost and larger capacities) to 2:50 SAS drives which tend to have a
signiﬁcantly better response proﬁle to random I/O seeks. The smaller form factor
also allows for a larger number of spindles in the same physical footprint, providing
for the possibility of spreading disk activity over a larger number of drives.
The aggregation of multiple drives into logical units using RAID can decrease
the overall storage subsystem latency in response to random I/O activity as long
certain key principles are met. Hardware based RAID is always preferable as parity
calculation can tax system CPU’s signiﬁcantly, particularly for RAID 5 and RAID
6. The number of drives per container can also have an impact on the performance
of the overall group, as most controllers will exhibit performance degradation with
a large number of drives; generally a three drives RAID 5 or six drives RAID 6
container will achieve the best performance. In a similar way, a large number of
containers per controller can be detrimental to the general performance; in this
case, the use of multiple controllers would be an appropriate workaround. The
width of the stripe slices is also critical: narrower slices have the best random
I/O performance when there are bursts of activity, but may hamper sustained
performance; as a rule of thumb, wider slices are usually preferred, as the cache
residing in the hard drives and the RAID controller itself can adequately manage
bursts of activity with very little performance degradation (in a traditional RDBMS
where very small bursts of random I/O are frequent, narrow slices would still
be preferred). It is important to remark that battery backed controllers must be
used in order to enable write-back strategy; write-through is so detrimental to
write performance that it cannot be considered for any serious use. Lastly, it is
recommended that RAID 5 or RAID 6 (depending on the data protection proﬁle
required) are used; RAID 0 would provide the best performance, but the failure of
any single drive would render that container (and possibly computing node) void,
and wasting 50% of usable space in a RAID 10 container is usually not practical.
Solid State Drives also represent an opportunity for potential performance gains
in the storage subsystem. Their reduced latencies due to the lack of a spinning
mechanical plate combined with their increased overall throughput can make them
good alternatives to hard drives, particularly when taking into account their lower
power consumption. However there are certain factors to be considered before the
current generation of Solid State Drives can be considered to replace hard drives:
cost, capacity, longevity, and write performance. Solid State Drives tend to cost
more than their mechanical counterparts, and their capacities are still limited by
the density that can be achieved at the current scale of integration. Longevity of
Solid State Drives is in direct relationship to the amount of write/delete activity
and continuous writing to a SSD can shorten its life considerably, especially for

242
J. Burger et al.
MLC (Multi-Level Cell) units. Write performance tends to be non-stellar; due to
the way SSD work, writing, particularly when associated to deletion, usually under-
performs top end mechanical drives. Certain manufacturers of PCI-e based ﬂash
drives developed special drivers that work around these limitations by using main
memory as a buffer to write operations, but this could take signiﬁcant amounts
of valuable main memory which otherwise could be used to speed up the overall
system performance by, for example, avoiding to spill temporary ﬁles to disk in the
ﬁrst place.
Multiple Thor processes running on each node also put a strain on resources
external to the nodes; networking bandwidth can constitute a bottleneck if not
sized properly. A signiﬁcantly oversubscribed network leads to packet collisions
and retransmissions which can severely impair performance of TCP connections
by forcing them to throttle down and renegotiate transmission windows too often.
A single 1 Gbps link per node, while sufﬁcient for a single Thor process, can
become a signiﬁcant contention point when ten Thor processes are running in
that node, particularly if those ten processes can be kept consistently fed from
the underlying ﬁlesystem. In addition to this, environments with ten or more Thor
processes per node, and hundreds of physical nodes can quickly grow to have
thousands of virtual nodes which will be challenging for any switch frame that is not
completely non-blocking. And even in a non-blocking environment, multiple data
transmissions to individual nodes can oversubscribe the output port on the network
switch; Thor takes special care in the arbitration of network resources to minimize
the opportunity for such conditions by interleaving data transmissions and relying
on information provided by the recipient node to determine the optimum time to
transfer data [24].
In order to alleviate the effect that congestion has over TCP transmissions, there
are a couple of conﬁguration options that need to be considered. On the one hand,
there is the congestion avoidance mechanisms RED (Random Early Detection) and
WRED (Weighted Random Early Detection) that can be conﬁgured in the outbound
queues of the switch; on the other hand, there is TCP ECN (Explicit Congestion
Notiﬁcation) which is synergistic with RED and WRED. RED and WRED prevent
congestion by starting to drop select TCP packets early when port utilization reaches
certain thresholds with the goal of forcing throttling on those TCP sessions to
prevent congestion. If TCP ECN is enabled in the hosts, RED and WRED will not
drop any packets and rather ﬂag those TCP sessions as congested for the hosts at
both ends to negotiate a lower transmission rate. In theory, TCP ECN should be
able to achieve a better performance proﬁle across all TCP sessions traversing that
particular port.
As part of the strategy to move to multiprocessing in each node, a review and
potential upgrade of the network infrastructure should be considered. The two
technologies that show the biggest promise are 10 GE and InﬁniBand [25,26]. The
former maintains layer 2 compatibility with Ethernet, but this is normally not a
decisive factor when the intention is to replace the switch fabric altogether. The
latter offers signiﬁcantly lower latencies (under 1 s latencies measured between
network stacks) and provides for a better opportunity for ofﬂoading processing to the

9
Parallel Processing, Multiprocessors and Virtualization in Data-Intensive Computing
243
adapter (using, for example RDMA). Depending on the average frame size, moving
millions of frames per second can have a very noticeable impact on CPU caches
and overall CPU utilization; this negative effect can be exacerbated if the Operating
System forces all or a sizeable portion of this processing into a single processor
core. Inﬁniband takes special care of ofﬂoading data copies from the CPU, which
not only reduces CPU utilization compared to Ethernet, but also prevents polluting
the CPU caches with data that is only loaded there in order to be transferred over
the network. Inﬁniband also offers for asynchronous zero copy network transfers,
reducing the transmission latencies and helping better parallelize network activity.
5
Achieving Greater Parallelism
Some processes are straightforward to parallelize – typically cases where there
is little or no interaction between the different elements of the problem; such
problems are termed “embarrassingly parallel” [27]. These types of algorithms
usually have an execution time that scales linearly in inverse proportion to the
number of processor cores, which is known as “strong scaling” [28], if suitably
divided up using one of the approaches described above.
For cases that are not embarrassingly parallel, achieving parallelism may not easy
task, and there is usually a limit to how much speedup will be possible to gain. In
1967, Gene Amdahl put forth an argument regarding the speedup obtain by parallel
processing. This argument is now known as Amdahl’s law [29] and sustains that
even when the fraction of serial work in a given problem is small, the maximum
speedup expectable by inﬁnite parallelism cannot exceed from the inverse of the
amount of serial work. However, the validity of Amdahl’s law has been recently
criticized [30,31] and it may still be possible to achieve a certain degree of parallel
execution using one or more of the approaches below.
5.1
Parallelize a Different Dimension
It may not be possible to parallelize all the tasks you need to run on a system,
but if there are multiple independent tasks, running them all at the same time will
introduce a degree of parallelism to the system [32]. One advantage of this approach
is that it can assist in sidestepping Amdahl’s law, particularly due to the fact that
the different tasks are likely to reach their bottlenecks at different times even if
they tax the same subsystems at different times during their execution. It is also
important to mention that if these tasks are truly independent of each other and
there are no barrier and/or synchronization operations across them, the serialization
within each one is independent of the others. However the amount of parallelism
you can introduce using this method is limited by the number of independent tasks
that need to be run, and there is often a limit to how much scalability can be gained
this way.

244
J. Burger et al.
5.2
Use Better Tools
Automatic or semi-automatic parallelizing compilers for traditional imperative
computer languages are available, which can do a reasonable job of parallelizing the
inner loops of some algorithms. However these tend to be limited to shared-memory
systems rather than the distributed cluster systems we have been discussing in this
chapter.
Declarative languages are much easier to parallelize automatically, and some
such as ECL [33–35], Data Parallel Haskell [36], or Schemik [37, 38] have been
designed from the start to be used on distributed memory clusters. These declarative
languages, by the fact that they are not imperative, also conveniently offer the
opportunity for the compiler to perform optimizations speciﬁc to the environment
which could result in further speedup by reordering critical sections, ensuring that
resource utilization of the different subsystems is interleaved across different tasks
to guarantee that tasks don’t starve for a single common resource at a given time,
and of course other beneﬁts that declarative programming paradigms bring, such as
lazy execution and overall better programmer efﬁciency by focusing on what needs
to be done, rather on how to do it [39].
Without changing programming language, there are toolsets and APIs available
that make the task of implementing parallel algorithms easier and avoid “reinventing
the wheel.” Intel’s thread building blocks [40], OpenMP [41], MPI [42], and
MapReduce [43] are all good examples of toolsets, programming models, and APIs
that can help with the parallelization of algorithms.
5.3
Employ Smarter Programmers
And be sure to pay them lots of money! Many things CAN be parallelized with a
bit of work : : : or a ton of it. Many times an equivalent parallel algorithm can be
designed. Or a heuristic can be applied to do MOST of the work in parallel and mop
up a few remnants at the end.
For some applications, it may be acceptable to use a parallel implementation that
gives a “good enough” answer, where a perfect answer might not be possible to
parallelize. For example, the traveling salesman problem is only fully solvable with
an exhaustive search requiring polynomial time, but algorithms exist that can be
guaranteed to give an answer within a few percentage points of the optimum, that
can be evaluated using parallel techniques and without the polynomial order [44].
“Good enough” need not even be inferior; it’s a question of examining your
requirements carefully to understand what your needs really are. For example,
one process that was run on the LexisNexis Thor engine early in its development
involved adding a sequence number to each record in a data ﬁle. This was a task
that was hard to parallelize because each node needed to know how many records
were going to be processed by every other node before it could start, and where

9
Parallel Processing, Multiprocessors and Virtualization in Data-Intensive Computing
245
this was not known the operation became an entirely serial one. This could have
been implemented as a two pass parallel process (each node counts, all nodes
propagate the count information, then each node assigns sequence numbers), but
an examination of what was really needed revealed that it was not a requirement
for the sequence numbers to be strictly sequential with no gaps, merely that they
be unique. By having each node start numbering at .n node limit/, where n is the
node number and node limit is an upper bound on how many records a node could
ever be expected to hold, you can generate unique ids in a single pass, in a complete
parallel manner, without ever needing nodes to exchange information.
6
Conclusions and Future Work
Performance optimization in Data-Intensive Computing Clusters requires a delicate
balance of potential throughput across the multiple components, both internal and
external to the nodes, which depends not only on the speciﬁc platform, but also on
the expected particular workloads. The modern use of multicore CPU’s and SMP
to increase performance of individual nodes adds a new dimension which needs to
be considered when trying to make use of all the existing resources in the most
efﬁcient manner. Although the initial reaction could be to re-engineer the software
to distribute workloads across multiple CPU cores, this strategy may not always be
viable, depending on the degree of effort involved with the software rewrite.
Data-Intensive Computing Platforms, such as LexisNexis Thor, are well suited
for distributed environments, and can already parallelize workloads efﬁciently
across multiple execution units residing in different computing nodes. It is a logical
evolution to extend the paradigm to spread workloads across multiple execution
threads within the nodes themselves. While the use of virtualization and containers
could seem initially attractive, upon further analysis, the overhead that these
isolation techniques carry is unjustiﬁed if the platform already allows for multiple
processes to co-exist in the same node without conﬂicts.
While CPU resources are important to the overall performance of the sys-
tem, Data Intensive Computing’s own characteristics also could create important
contention points around other subsystems, particularly disk, network, memory
bandwidth, and CPU cache coherency. Paying attention to this can help move
beyond the plateau observed when adding more computing cores seem not to render
any additional performance.
Moreover, when all these techniques reach a point where bottlenecks are
balanced across multiple subsystems, and upgrading hardware or adding multiple
nodes is not viable due to budgetary, space, power or thermal concerns, there are
other more extreme measures involving partial or complete rewrite of the application
level algorithms and/or the underlying platform to achieve further speedups. These
are usually non obvious and could require a considerable amount of effort, but may
be well worth as the payback could be signiﬁcant in many cases.

246
J. Burger et al.
References
1. A.S. Szalay, G. Bell, J. Vandenberg, A. Wonders, R. Burns, D. Fay, J. Heasley, T. Hey, M.
Nieto-SantiSteban, A. Thakar, C. Van Ingen, and R. Wilton. “Gray Wulf”: Scalable Clustered
Architecture for Data Intensive Computing,” Proceedings of the 42nd Hawaii International
Conference on System Sciences. 2009.
2. D. Lee, J.-L. Baer, B. Calder, and D. Grunwald. “Instruction Cache Fetch Policies for
Speculative Execution,” 22nd Annual International Symposium on Computer Architecture.
Italy, June 1995.
3. M.F. Younis, T.J. Marlowe, A.D. Stoyen, and G. Tsai. “Statically Safe Speculative Execu-
tion for Real-Time Systems,” IEEE Transactions on software engineering, Vol. 25, No. 3,
May/June 1999.
4. B.L. Deitrich, B.C. Chen, and W.W. Hwu. “Improving static branch prediction in a compiler,”
Proceedings of the International Conference on Parallel Architectures and Compilation
Techniques. 1998.
5. B. Furht, and A. Escalante. “Data Intensive Technologies for Cloud Computing,” Handbook of
Cloud Computing, Springer, 2010.
6. J.C. Dehnert, B. K. Grant, J.P. Banning, R. Johnson, T. Kistler, A. Klaiber, and J. Mattson.
“The Transmeta Code Morphing
TM Software: Using Speculation, Recovery, and Adaptive
Retranslation to Address Real-Life Challenges,” Proceedings of the First Annual IEEE/ACM
International Symposium on Code Generation and Optimization. 27–29 March 2003.
7. C. Li, N.K. Jha, and A. Raghunathan. “Secure Virtual Machine Execution under an Untrusted
Management OS,” International Conference on Cloud Computing, IEEE, 2010.
8. R. Uglig, G. Neiger, D. Rodgers, A.L. Santoni, F.C.M. Martins, A.V. Anderson, S.M. Bennett,
A. Kagi, F.H. Leung, and L. Smith. “Intel Virtualization Technology,” Computer Vol. 38 No.
5, May 2005, pp. 48–56.
9. R. Figueiredo, P.A. Dinda, and J. Fortes. “Resource Virtualization Renaissance.” Computer,
Vol. 38 No. 5, May 2005, pp. 28–31.
10. G. Strongin. “Trusted computing using AMD “Paciﬁca” and “Presidio” secure virtual machine
technology,” Journal Information Security Tech. Report, Vol. 10 No. 2, January 2005.
11. K. Nance, B. Hay, and M. Bishop. “Virtual Machine Introspection,” IEEE Computer Soci-
ety, 2008.
12. S.T. King, G.W. Dunlap, and P.M. Chen. “Operating System support for virtual ma-
chines,” Proceedings of the annual conference on USENIX Annual Technical Conference
(ATEC’03), 2003.
13. S. Soltesz, H. Potzl, M.E. Fiuczynski, A. Bavier, and L. Peterson. “Container-based
operating system virtualization: a scalable, high-performance alternative to hypervisors,”
Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference on Computer Systems
(EuroSys’07), 2007.
14. T. Jones, S. Dawson, R. Neely, W. Tuel, L. Breener, J. Fier, R. Blackmore, P. Caffrey,
B. Maskell, P. Tomlinson, and M. Roberts. “Improving the Scalability of Parallel Jobs by
adding Parallel Awareness to the Operating System,” ACM/IEEE Conference on Supercom-
puting, 2003.
15. M. Herlihy. “A methodology for implementing highly concurrent data structures,” Proceedings
of the second ACM SIGPLAN symposium on Principles & practice of parallel programming
(PPOPP’90), 1990.
16. H. Franke, R. Russell, and M. Krikwood. “Fuss, Futexes and Furwocks: Fast Userlevel Locking
in Linux,” Proceedings of the 2002 Ottawa Linux Summit, 2002.
17. M. Cannataro, D. Talia, and P.K. Srimani. “Parallel data intensive computing in scientiﬁc and
commercial applications,” Journal of Parallel Computing – Parallel data-intensive algorithms
and applications. Vol. 28 No. 5, May 2002.
18. A. Sohn, M. Sato, N. Yoo, and J.L. Gaudiot. “Data and Workload Distribution in a Multi-
threaded Architecture.” Journal of Parallel and Distributed Computing, Vol. 40, No. 2, Feb.
1997, pp. 256–264.

9
Parallel Processing, Multiprocessors and Virtualization in Data-Intensive Computing
247
19. K.B. Wheeler, R.C. Murphy, and D. Thain. “Qthreads: An API for programming with
millions of lightweight threads,” IEEE International Symposium on Parallel and Distributed
Processing, 2008.
20. Q. Wu, and Y. Gu. “Optimizing end-to-end performance of data-intensive computing pipelines
in heterogeneous network environments,” Journal of Parallel and Distributed Computing, Vol.
71, No. 2, February 2011.
21. M. Gokhale, J. Cohen, A. Yoo, W.M. Miller, A. Jacob, C. Ulmer, and R. Pearce. “Hardware
Technologies for High-Performance Data-Intensive Computting,” Computer, Vol. 41, No. 4,
April 2008.
22. S. Saini, A. Naraikin, R. Biswas, D. Barkai, and T. Sandstrom. “Early performance evaluation
of a “Nehalem” cluster using scientiﬁc and engineering applications,” Proceedings of the Con-
ference on High Performance Computing Networking, Storage and Analysis (SC’09), 2009.
23. W.M. Hassanein, M.A. Hammad, and L. Rashid. “Characterizing the Performance of Data
Management Systems on Hyper-Threaded Architectures,” Proceedings of the 18th Interna-
tional Symposium on Computer Architecture and High Performance Computing (SBAC-
PAD’06), 2006.
24. S. Soudan, R. Guillier, L. Hablot, Y. Kodama, T. Kudoh, F. Okazaki, R. Takano, and P. Primet.
“Investigation of ethernet switches behaviour in presence of contending ﬂows at very high
speed,” PFLDnet 2007, Feb. 2007.
25. S. Hansen, T. Wilcox, and D. Stanzione. “Inﬁniband routing and switching: improving fabric
scalability, distance, and fault isolation,” Proceedings of the 2006 ACM/IEEE conference on
Supercomputing (SC’06), 2006.
26. D.K. Panda, and P. Balaji. “Designing high-end computing systems with InﬁniBand and 10-
Gigabit Ethernet iWARP,” Proceedings of the 2007 IEEE International Conference on Cluster
Computing (CLUSTER’07), 2007.
27. H. Stockinger, M. Pagni, L. Cerutti, and L. Falquet. “Grid Approach to Embarrassingly Parallel
CPU-Intensive Bioinformatics Problems,” Proceedings of the Second IEEE International
Conference on e-Science and Grid Computing (E-SCIENCE’06), 2006.
28. O. Sahni, C.D. Carothers, M.S. Shephard, and K.E. Jansen. “Strong scaling analysis of a
parallel, unstructured, implicit solver and the inﬂuence of the operating system interference,”
Scientiﬁc Programming, Vol. 17, No. 3, Aug. 2009.
29. G.M. Amdahl. “Validity of the single-processor approach to achieving large scale computing
capabilities,” AFIPS Conference Proceedings, Vol. 30, 1967, pp. 483–485.
30. J.L. Gustafson. “Reevaluating Amdahl’s Law,” Communications of the ACM, Vol. 31, No.
5, 1988.
31. S. Krishnaprasad. “Uses and abuses of Amdahl’s law,” Journal of Computing Sciences in
Colleges, Vol. 17, No. 2, Dec. 2001.
32. M. Ivanova, and T. Risch. “Customizable parallel execution of scientiﬁc stream queries,”
Proceedings of the 31st international conference on Very large data bases (VLDB’05), 2005.
33. A. Yoo, and I. Kaplan. “Evaluating use of data ﬂow systems for large graph analysis,”
Proceedings of the 2nd Workshop on Many-Task Computing on Grids and Supercomputers
(MTAGS’09), 2009.
34. D. Bayliss, R. Chapman, J. Smith, O. Poulsen, G. Halliday, and N. Hicks. “System and method
for conﬁguring a parallel processing database system,” US Patent 7240059, 2007.
35. D. Bayliss, R. Chapman, J. Smith, O. Poulsen, G. Halliday, and N. Hicks. “Query scheduling
in a parallel-processing database system,” US Patent 7185003, 2007.
36. M.M.T. Chakravarty, R. Leshchinskiy, S. Peyton Jones, G. Keller, and S. Marlow. “Proceedings
of the 2007 workshop on Declarative aspects of multicore programming (DAMP’07), 2007.
37. P. Krajca, and V. Vychodil. “Data Parallel Dialect of Scheme,” Proceedings of the 24th Annual
ACM Symposium on Applied Computing (SAC’09), 2009, pp. 1938–1940.
38. P. Krajca, and V. Vychodil. “Software transactional memory for implicitly parallel func-
tional language,” Proceedings of the 25th Annual ACM Symposium on Applied Computing
(SAC’10), 2010, pp. 2123–2130.

248
J. Burger et al.
39. J.W. Lloyd. “Practical advantages of declarative programming,” Joint Conference on Declara-
tive Programming (GULP-PRODE’94), 1994.
40. T. Willhalm. “Putting intel
TM threading building blocks to work,” Proceedings of the 1st
international workshop on Multicore software engineering (IWMSE’08), 2008.
41. J.P. Hoeﬂinger. “Programming with cluster openMP,” Proceedings of the 12th ACM SIGPLAN
symposium on Principles and practice of parallel programming (PPoPP’07), 2007.
42. W. Gropp, R. Thakur, and E. Lusk. “Using MPI-2: Advanced Features of the Message Passing
Interface,” MIT Press, ISBN: 026257134X, 1999.
43. J. Dean, and S. Ghewmawat, “MapReduce: simpliﬁed data processing on large clusters,”
Communications of the ACM, 50th anniversary issue, Vol. 51, No. 1, Jan. 2008.
44. T. Chun-Wein, T. Shih-Pang, C. Ming-Chao, and Y. Chu-Sing. “A fast parallel genetic algo-
rithm for traveling salesman problem,” Proceedings of the Second Russia-Taiwan conference
on Methods and tools of parallel programming multicomputers (MTPP’10), 2010.

Chapter 10
Challenges in Data Intensive Analysis
at Scientiﬁc Experimental User Facilities
Kerstin Kleese van Dam, Dongsheng Li, Stephen D. Miller, John W. Cobb,
Mark L. Green, and Catherine L. Ruby
1
Introduction
Today’s scientiﬁc challenges such as routes to a sustainable energy future, materials
by design or biological and chemical environmental remediation methods, are
complex problems that require the integration of a wide range of complementary
expertise to be addressed successfully. Experimental and computational science
research methods can hereby offer fundamental insights for their solution. Exper-
imental facilities in particular can contribute through a large variety of investigative
methods, which can span length scales from millions of kilometers (radar) to the
sub-nucleus (LHC1). These methods are used to probe structure, properties, and
function of objects from single elements to whole communities. Hereby direct
imaging techniques are a powerful means to develop an atomistic understanding of
scientiﬁc issues [1,2]. For example, the identiﬁcation of mechanisms associated with
chemical, material, and biological transformations requires the direct observation
of the reactions to build up an understanding of the atom-by-atom structural and
chemical changes. Computational science can aid the planning of such experiments,
correlate results, explain or predict the phenomena as they would be observed and
1http://public.web.cern.ch/public/en/lhc/lhc-en.html.
K.K. van Dam () • D. Li
Fundamental and Computational Science Department, Paciﬁc Northwest National Laboratory,
Richland, WA, USA
e-mail: Kerstin.KleeseVanDam@pnl.gov; dongsheng.li@pnnl.gov
S.D. Miller • J.W. Cobb
Data Systems Group, Neutron Scattering Science Division, Oak Ridge National Laboratory, Oak
Ridge, TN, USA
e-mail: millersd@ornl.gov; cobbjw@ornl.gov
M.L. Green • C.L. Ruby
Systems Integration Group Tech-X Corporation, Williamsville, NY, USA
e-mail: mlgreen@txcorp.com; rlruby@txcorp.com
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 10, © Springer Science+Business Media, LLC 2011
249

250
K.K. van Dam et al.
thus aid their interpretation. Furthermore computational science can be essential
for the investigation of phenomena that are difﬁcult to observe due to their scale,
reaction time or extreme conditions. Combining experimental and computational
techniques provides scientists with the ability to research structures and processes at
various levels of theory, e.g. providing molecular ‘movies’ of complex reactions that
show bond breaking and reforming in natural time scales, along with the intermedi-
ate states to understand the mechanisms that govern the chemical transformations.
Advances in experimental and computational technologies have lead to an
exponential growth in the volumes, variety and complexity of data derived from
such methodologies. For example the experimental data rates at Oak Ridge National
Laboratory (ORNL) Spallation Neutron Source (SNS) vary from around 200
MB/day to around 4.7 GB/day per instrument with an average of around 1.3
GB/day/instrument for its 23 instruments. The Advanced Photon Source (APS)
has almost 60 beamlines with one to three instruments per beamline and rapidly
produces copious amounts of data. Typical experiments will produce between a few
KB to 100 GB, while imaging experiments (such as tomography) produce more, on
the order of 1–10 TB of data per experiment. Data rates for some instruments, such
as X-ray Photon Correlation Spectroscopy and 3D X-ray Diffraction Microscopy
will approach 300 MB/s on a continuous basis. At the Linac Coherent Light Source
(LCLS) there will be six sets of versatile high data bandwidth instruments installed
in two hatches of the LCLS experimental area. Some instruments will be capable
of producing up to tens of GB/s of data in peak. In the ﬁnal implementation of the
system up to two of those instruments can be used simultaneously. This will result
in multi-terabyte data volumes to be handled on daily basis. The data rate will ramp
up over the next several years. The ﬁrst experiments will produce up to 1 TB of data
per day. In 3 years the amount of data to be stored per day will increase up to 15
TB from only one of the instruments, and that would correspond to nearly 2–3 PB
of data per year. Next generation facilities such as the X-Ray Free Electron Laser in
Germany (XFEL2) expect data rates of up to 3.5 PB a day, compressed and reduced
for long time storage to 1–4 PB a month [3], in comparison the much quoted LHC
particle physics experiment is expecting to store 5 PB a year. However it is not just
the large scale facilities that have experienced this increase in data rates, facilities
with laboratory based equipment such as the Environmental Molecular Sciences
Laboratory (EMSL) with over a hundred different instruments have seen similar
increases. A 2010 Ion Mobility Spectroscopy Time of Flight instruments produces
10x as much data as comparable systems in 2006 i.e., an increase from 1 to 10 TB
per day.
Similarly submission rates at leading community repositories for experimental
data results such as the European Molecular Biology Laboratory Nucleotide
Sequence Database (EMBL-Bank – comparable to the US GenBank) have strongly
2www.xfel.eu.

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
251
increased. EMBL is currently growing at a rate of 200% per annum, requiring a
doubling in its storage capacity every year (5 PB in 2009). EMBL’s web pages,
which give access to 63 distinct databases, received 3:5 million hits per day in
2009, and programmatic access to their data via web services was at 1 million
requests per month and growing [4]. The number of databases reported in Nucleic
Acids Research jumped from 218 to 1,170 in the years 2001–2009 [5]. Overall
experimental data rates have signiﬁcantly increased in line with the general trend
for new experimental instruments; as a consequence, whilst the data deluge might
not be happening everywhere in an absolute sense, it is in a relative one for
most research groups. However it is not only the volume that has increased it is
also the complexity of the data that is rapidly growing. Ever new investigative
methods are being developed, with each method and vendor of instruments for
such a method creating new data formats and results representations. Therefore
experimental science and its analysis is overall now a very data intensive ﬁeld of
science.
This exceptional growth in data volumes and complexity has presented re-
searchers with signiﬁcant challenges, foremost how to effectively analyze the results
of their research both for single experiments and increasingly across different
investigative methods. The availability of underpinning data management facilities
and tools play hereby a crucial role throughout the experimental and analysis
processes.
Data management challenges include issues such as data storage, access, and
movement. Ever growing volumes no longer allow facilities or researchers to store
all the collected raw and derived data in perpetuity and hard decisions might have
to be taken in terms of what is worthwhile retaining. Even when it is possible to
store the data collected, its volume and diversity requires expert management to
enable immediate and timely analysis as well as long term access and usability of
the data, this data management knowledge is not always available at the researcher
or even the facilities level, leaving large volumes of data destitute and inaccessible.
Similarly it can be quite difﬁcult for scientists or facilities to support the basic
functions necessary for the correlation of research results. Data transfers between
organizations can be fraud with problems such as unreliability, speed and lack of
data integrity throughout the transfer, and so many facilities and their users still rely
on the shipping of hard drives for their data movement [6].
An even greater challenge however is the analysis of the data itself, with the
increasing variety of instruments used at experimental facilities; the variety of
(often proprietary) data formats and analysis software packages has increased
dramatically. This plethora of investigative methods and data formats has prevented
the community thus far form working collaboratively on advancing their analytical
methods. As a result traditional analysis methods are often not scalable enough
to deal with either the increasing volume or complexity of the results of both
experimental and computational research results. In response researchers often
either do not use the full capabilities of the instruments or only analyze a very
small subset of the data they collected. Where full analysis is possible it can take

252
K.K. van Dam et al.
many hours or weeks for an experiment which might have only lasted minutes or
seconds. The lack of suitable tools, advanced computing techniques, and storage are
key limiting factors. Furthermore these hinder the progression of the ﬁeld to address
one of the main requirements for the future of experimental science: the ability to
analyze experimental results in real time, and actively inﬂuence these. To achieve
this next level of experimental research, a new generation of analysis methods needs
to be developed.
Experimental science today is highly specialized on the individual level, driven
by ever more complex investigative methods, but very collaborative and interna-
tional in its project work [7], driven by the complexity of the scientiﬁc challenges.
It is therefore necessary to correlate, integrate, and synthesize local results with
other experimental and computational work world wide to improve the quality,
accuracy, and completeness of the analysis. Therefore a critical challenge for
experimental science is the need to empower the scientist with computational tools
to perform analysis across a high volume, diverse and complex set of experimental
and simulation data, to extract the desired knowledge and meaning that leads to
scientiﬁc discovery.
This chapter will discuss the critical data intensive analysis challenges faced by
the experimental science community at large scale and laboratory based facilities.
The chapter will highlight current solutions and lay out perspectives for the future,
such as methods to achieve real time analysis capabilities and the challenges and
opportunities of data integration across experimental scales, levels of theory, and
varying techniques.
2
Challenges
The experimental sciences community faces a wide range of challenges, both in their
day to day work, as well as in their endeavor to progress the scientiﬁc capabilities
of this domain in general. While many challenges are related to the instrumentation,
the speciﬁc science domains or physical research objects, an increasing number stem
from the data intensive nature of the processes involved in experimental analysis. In
the following we will elaborate further on the key challenges which include the
following principle areas:
•
Metadata Generation and Association
•
Data Formats
•
Data Integrity
•
Data Analysis of Single Experiments
•
Co-analysis of the Collection
•
Data Provenance, Collaboration, and Data Sharing
•
Data Ownership and Data Citation

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
253
2.1
Metadata Generation and Association
Often experiments require the use of specialized sample environment equipment
to control the conditions in which data are collected. The nature of the sample
environment equipment may vary widely though the most common equipment
controls temperature, pressure, or magnetic ﬁeld, and sometimes a combination of
these. Other common types of sample environment equipment may include pulsed
lasers, vibrational stress to materials, dynamically mixing gases or chemicals at
varying ratios, and the sequencing of a number of samples to be studied. The timing
for when conditions change on the sample must be recorded in order to correlate
sample environment changes with those observed in the data, and are critical to
the reliable analysis of the results. The how precise the correlation needs to be
can depend upon the time scale for the rate of change anticipated. This being the
case, it is possible that the sample environment metadata can also be an appreciable
size.
Similarly, sample positioning is another important piece of metadata which must
be recorded. The position information is necessary to have for experiments which
rely upon probing beam path lengths and angular relationships to detectors. The
position information is also necessary for classes of samples which have positional
information as part of their composition, such as crystalline materials. Often these
crystals must be oriented according to a known position so that structure can
be studied. Another class of experiments fall into the category of rotating the
sample such as in the case of tomography or neutron spectroscopy inelastic single
crystal energy transfer measurements. Moving or rotating the sample can create a
corresponding data ﬁle for each positional change, else the dynamic nature of the
repositioning must be indicated within the data.
There are a wide variety of other metadata which should also be associated
with the experiment and which metadata are recorded can be a function of
facility capabilities and data policies. Often one primary key is the experiment
proposal number. Along with this, associating the experiment team members can
be important. Other important metadata include: measurement start and end times,
instrument status and operating conditions, and data acquisition related metadata
used to properly identify and segment data such as measurement frame numbers or
experiment run numbers.
Many of these metadata are not only vital for the immediate analysis process,
but also support the long term exploitation of the results. The quality and com-
prehensiveness of the metadata will directly inﬂuence the accuracy and quality
of the analysis process. Due to the wide variety of metadata and its sources its
structured and quality controlled capture is a major challenge for any scientist or
facility.

254
K.K. van Dam et al.
2.2
Data Formats
Data can appear in a wide variety of formats ranging from unformatted, proprietary,
to self-describing such as HDF53 or NetCDF4. Data management professionals
can be challenged to select the best data format to use for a number of reasons
ranging from ease of use to ﬁle format performance. Scientiﬁc communities can
engrain on particular data formats, which can cause challenges when data systems
professionals seek to use a new format. There can be little motivation to adapt
community developed legacy applications to utilize new ﬁle formats, particularly
if these new formats are not readily available via the software language used
by the legacy application. Scientists may be familiar with their own tools for
examining data and if new formats cause any additional burden, these will ﬁnd low
acceptance.
However one should not give up on deﬁning ﬁle formats, particularly in
scientiﬁc communities lacking data format standards. Moving the community
towards standards has the beneﬁt of potentially opening up software development
to a broader segment of the community once data interchange formats have been
established. In some cases, it may be necessary to move the community to more
advanced data formats to address issues that might have already been solved by these
standard data formats. For example, if higher performance is needed, utilizing the
parallel access or inherent data compression/decompression capabilities may be of
beneﬁt.
The longer term beneﬁt of using a self-describing data formats are many, as the
ﬁle can collect and store metadata pertaining to the data which may otherwise be
lost over time if these are maintained as separate ﬁles. A self-describing format
also offers the potential to engage a larger community of researchers wishing to
collaborate on the data. Thus there are many advantages for deﬁning data formats
for a scientiﬁc community.
Another challenge for establishing data formats may be to capture data with non-
traditional data formats. Typical data acquisition may utilize histogramming to bin
data. However, the binning process can reduce the resolution of the data. To avoid
this problem, some state of the art instruments are utilizing event mode data which
works by concatenating detected data to a growing list. One such example would
be an event data format which records detected events in position and time thus
providing the maximum possible resolution of the detection system. However some
data storage formats may not respond well to varying length data sets, especially if
data need to be appended during an acquisition.
3The HDF Group produces and maintains software for self-describing scientiﬁc data via the
Hierarchical Data Format. http://www.hdfgroup.org/.
4The Network Common Data Form (NetCDF) self-describing data format developed by the
University Corporation for Atmospheric Research (UCAR). http://en.wikipedia.org/wiki/Netcdf.

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
255
2.3
Data Integrity
Ensuring the quality of the data is a challenge as part of the data acquisition process
for one primary reason – ensuring data integrity may take as much time as the
time required to produce the data, and the data acquisition system may not be
capable of performing this task during rapid data acquisition sequences. However
the challenge remains that data integrity must be ensured as close to the source as
possible.
There are a number of data integrity mechanisms that can be employed, some
implicit while others are explicit. Implicit mechanisms include computer memory
parity checking and correction, network handshaking protocols such as TCP/IP, and
data parity checking and correcting methods such as disk systems which utilize
RAID parity checking and correction, such as RAID 1, 5, 6, or 10. These methods
are commonly utilized today and are fairly reliable and robust enough that one
may take for granted that additional data integrity mechanisms may need to be
employed.
However if one does not explicitly determine the integrity of the data, one may
not know for sure the integrity of the data. Considering the vast sizes of data sets
today, the probability for some type of data corruption is on the increase. These
errors may arise from faulty memory, RAID system failures or single disks not
operating in RAID conﬁguration, or faulty networking equipment that corrupts data
during transfer. In the case of an unnoticed error resulting in data corruption, the
corrupted data may be perpetuated into the future beyond a point where it can be
recovered.
To help explicitly identify data, methods for producing checksums have been
developed. A checksum is typically a ﬁxed-size number computed from a data set
of interest that, to some high degree of certainty, uniquely identiﬁes that particular
data set. Thus a change in one of the datum will result in a different checksum. A
variety of checksum methods are in use with the more common ones being MD55,
SHA6, and CRC7.
When examining the dataﬂow, ideally the checksum process must be performed
as early in the dataﬂow as possible and as previously mentioned ideally when the
data are created. To be useful, the checksum must be stored somewhere where it
can be referred to at a later time. Data production systems often employ catalogs to
store metadata for search, and the checksum value for the data should be stored in
this catalog. Though the challenge remains today, to create checksums in a timely
fashion for large ﬁles.
5http://en.wikipedia.org/wiki/MD5.
6http://en.wikipedia.org/wiki/Secure Hash Algorithm.
7http://en.wikipedia.org/wiki/Mathematics of CRC.

256
K.K. van Dam et al.
Fig. 10.1 Example crystallography analysis workﬂow [8]
2.4
Data Analysis of Single Experiments
The analysis of the raw data produced by single experiment is often a complex
process, incorporating many different steps, some of which will need to be repeated
several times over after review, to achieve the best possible results (see Fig. 10.1 for
an exemplary analysis workﬂow).
The steps taken can in general be classiﬁed as: data capture, calibration,
data compression, data reduction (Reduce noise and smooth data, reconstructions
will contain the most signiﬁcant information, are feature-accentuated), image
reconstruction (Accurate re-construction of high volume data, combine correlation
functions with parallelized ﬁltered back projection), segmentation and feature
association (identiﬁcation of application-speciﬁc chemical signature and feature
recognition), visualization of results. Some of the analysis steps might be repeated
several times to identify all required features in the data and ﬁlter out enough
background information to make these clearly visible.
The increase in data and repetition rates on many instruments has caused severe
problems for the subsequent analysis. The analysis take much longer than before
e.g., up to 18 h for a basic analysis for a mass spectrometry experiment that takes

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
257
itself under 1 h. But more importantly many of the existing tools are no longer able
to cope with the data volumes, requiring the scientists to collect data with less
precision than the instrument could offer or being able to examine only on small
subsets of the sample (i.e., 15  1;000 rather than 1;000  1;000) thus hampering
their scientiﬁc research signiﬁcantly. The problem in the existing methods are
hereby not only the data throughput, but the mathematical methods used, many
of which do not scale or are not at the appropriate level of theory. An example
repetition rates at leading, large laser facilities have increased from one shot an
hour to one a second, whereas in the past direct analysis methods where appropriate
scientists now need to investigate much more complex methods, separating the
effects of different shots, but also consider more statistics based approaches to their
analysis. Later stages of the analysis such as segmentation and feature detection
face similar challenges due to the increased volumes and complexity of the results.
Current more interactive methods of feature identiﬁcation need to be replaced by
automated ones. More importantly the representation of results has become more
challenging; high levels of details in single visualization (e.g., 3D volume rendering
of dense bio ﬁlms) make it difﬁcult for users to locate features of interest. The data
volumes are so big that only very advanced visualization tools can cope, however,
these are often difﬁcult to handle and require specialist support, traditionally not
present at experimental facilities or the scientists home organization. Similarly if
the users want to interact with the visualization, the data volumes require signiﬁcant
processing power to support this interaction. This processing power can no longer be
provided in the traditional analysis setting at the researcher’s desktop, but requires
dedicated visualization clusters and specialist software e.g. for remote visualization
back to the researchers desktop. While such methods exist, these tools are made for
visualization specialist in main and not for the use by scientiﬁc end users.
Driven by the need for science-enabling tools and technologies, researcher are
increasingly interested in real-time analysis and visualization e.g. of protein crystal
and enzyme structures and functions to enable intelligent instrument and experiment
control. It has proven particularly successful to pair researchers with computer
and computational scientists. These can guide researchers through structured re-
quirement gathering exercises, to identify enabling technologies that would address
their needs and provide a real step change in the possible scientiﬁc analysis. In
the recent commissioning of the neutron science instruments at ORNL’s SNS the
computing team heard questions like the following that could be answered by the
right computational infrastructure:
•
If I could plot in real time R-factors and observations-to-parameters ratios, they
should asymptotically approach values limited by the sample. I could then see
when the best time is to stop the data collection and move to the next temperature
or the next sample.
•
Say I want to know the O  H; O hydrogen bond distances with a precision of
0.01 ˚A. If I could evaluate bond distances their esd’s in real time, I could see if
and when this is achievable.

258
K.K. van Dam et al.
•
Parametric studies with single crystals – observing the dependence of a structural
parameter versus time, temperature, pressure, magnetic ﬁeld, electric potential,
laser excitation, and gas diffusion.
•
Observe Fourier density maps in real time.
•
Follow an order parameter in an order-disorder phase transition in real time.
•
Follow the intensities of superlattice and satellite peaks and diffuse scattering in
real time (reciprocal space).
However, the understanding of how to use leadership computing facilities as part
of such a computational infrastructure can be extremely time consuming for the
scientists to learn. Moreover, the access to these world-class resources is highly
dependent on the physical location, network connectivity, local computer resources,
or other resource-limited device availability. Leading community support facilities
now provide scalable user access through thin- and thick-client computing models.
Thin-client access is generally suitable for handheld resource-limited devices and/or
local computer resources where it is advantageous for the application software,
data, and CPU power to reside on the network server rather than the local client.
In most cases this will require that the scientist has network access and can
access a web browser, and no further application software installation or support
is required. Conversely, thick-client access is highly desirable when application
software, data, and CPU power is provided by the local resources that are able
to function independently from the network server. Portals and Science Gateways
can supply a robust support infrastructure for clients of this type by providing
resource and application availability dependent on the user requirements and level
of sophistication.
There exist several neutron instrument simulation packages such as MCSTAS,
IDEAS, NISP, and VITESS, which are used by instrument scientists to develop
detailed simulations of their beamlines. While in some cases several man-years of
effort are invested into these simulations, these valuable models are not routinely
being used by the neutron scientists for virtual experiment optimization and
planning due to computational and data workﬂow complexities. Furthermore, a
current bottleneck of efﬁcient data collection is the lack of software allowing
for real-time tracking of diffraction patterns as they are being collected in an
integrated manner. Current single crystal diffraction instrumentation designs will
be able to survey a vast amount of reciprocal space within a short time period. The
data produced, composed of short Bragg reﬂections and diffuse scattering, carry
important information on static and dynamic interactions of molecules and atoms
in the crystalline state. Computer programs such as GSAS, Fullprof, and SHELXL
are readily available for post-mortem data analysis interpreting Bragg diffraction
data and reﬁning ordered structures. However, a real-time system would enable
biomedical structure reﬁnement to occur while samples are still in the instrument.
This would provide a real-time determination of experiment duration based on
reﬁnement criteria provided by the scientist and veriﬁed by the real-time analysis
of live experimental data, which has never before been attainable. Enabling a real-
time decision support system for neutron beam experiment users has the potential
to dramatically advance the state-of-the-art and lead to not only the more efﬁcient

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
259
use of facility resources but may also lead to a better understanding to the dynamics
of data collection within an instrument.
2.5
Co-analysis of the Data Collection
As in the medical ﬁeld where a doctor may order a number of tests and scans to
examine and diagnose a patient, so do scientists and researchers utilize a number
of experimental techniques to study objects. In the case of large experimental user
facilities, it is quite common for scientists to perform complementary experiments
at both X-ray and neutron scattering user facilities, or combine laboratory based
experiments with X-ray experiments. Often the X-ray data can give good informa-
tion regarding the structure of a material, while the neutron scattering data can give
valuable information on the placement of hydrogen atoms within this structure that
X-rays see quite poorly. Laboratory based instruments might give more information
on the chemical composition of the object or its functions. In other imaging
technique combinations one technique might provide a cost effective ﬁrst quick look
at an object, whereas another one is used to examine objects of interest identiﬁed in
the initial quick look in much more detail with a higher precision method. Therefore
these techniques can complement each other quite well in providing different pieces
to the puzzle much like the various tests that a medical doctor would have performed
for a patient. Furthermore the results of one experiment might not only inform the
planning of another experiment, but can help in its direction and analysis.
While a few software programs are emerging for speciﬁc imaging technique
combinations e.g., to co-analyze some X-Ray and Neutron experiments, most of
the co-analysis is currently carried out in an ad-hoc fashion by the researchers and
thus is very time consuming and error prone. The challenges in this type of co-
analysis do not only lie in the analysis algorithms, but equally in the vital logistics
support for this type of analysis, with data often residing in different institutes and
potentially owned by different users.
The required data management of such complementary data sets are typically
left completely to the user. Often these experimentalists must manage copying data
to portable disk drives while they continue to acquire more experiment data. The
portable disks tend to be low performing and may be the only copy of the data
resulting from the experiment as facilities typically have a short time on the order of
two weeks when they will keep data available for the facility users. Some facilities
have developed more mature data management practices and systems, and retain
data for longer periods of time to better facilitate the data reduction and analyses
processes inherent in the publications process.
Thus the experimentalist must deal with a variety of factors including:
•
Different or no data management systems at one or both user facilities
•
Different computing access mechanisms for each facility perhaps resulting in
multiple passwords to manage
•
Managing where data reside

260
K.K. van Dam et al.
•
Single copies of data are vulnerable to errors and loss
•
Resource limitations for slow performing data systems and computers
Once the results of all experiments are available, the real analysis challenges start,
as there are hundreds of different investigative methods, the user has to determine
how the different imaging techniques relate to each other and thus how they need
to be treated. Do the results need to be integrated, compared, correlated etc. The
representation of the results from different techniques varies signiﬁcantly, as does
their scale, accuracy, and measured property. To compare two experimental results,
experts in both techniques need to be present to determine the relationship and
necessary analysis steps for their co-analysis, giving the lack of available tools, they
would then need to develop the algorithms to carry out the analysis and evaluation
of the results. Increasing data volumes and experimental complexity have made this
type of co-analysis ever more challenging and thus deterring many. Where scientists
embark on this journey, it will take them many weeks or months to complete. Given
that their foremost interest is the scientiﬁc outcome, the tools produced are ad-hoc
solutions, usually ﬁt only for this speciﬁc analysis and not ready to be shared with
others. More importantly, most of the time they will have no means or interest to
share their methods, thus other researchers will have to start again from scratch,
would they decide to follow in a similar direction.
Thus it is evident that there are many barriers to multi-facility data collection
and analyses. However, the rewards for improving inter-facility data management
and co-analysis software tools may likely yield an accelerated pace for scientiﬁc
discovery for many science areas. Though this has been an area which has been
slow to advance due to the complexity of coordination required for inter-facility
data management, and in some cases, it is more a matter of policy than technology
which may impede this integration.
2.6
Data Provenance, Collaboration, and Data Sharing
Research projects are increasingly looking for ways to effectively keep abreast with
everyone’s progress, discuss ﬁndings, share data, applications and workﬂows and
manage the documents that are exchanged, before publishing their results to a wider
community. Tools like dropbox, Google groups, Google docs, megaloader, etc., do
allow exchange of data, but they fall short in the following areas:
•
Limited space with paying a subscription fee. This becomes difﬁcult when team
members all need to subscribe.
•
These tools do not provide the version control and tracking capabilities that are
needed.
•
These tools do not allow you to annotate the data ﬁles and attach discussion
threads to datasets.

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
261
Wiki’s, another popular choice, become unwieldy very quickly and are not suitable
for large data exchange. Existing repositories at experimental or computational
facilities or community archives, provide access to data, but offer no support for
wider reaching scholarly exchanges and collaboration. There are only a few notable
exceptions where this kind of project based management, exchange and sharing of
data is supported by a community wide infrastructure, these are: the long standing
Earth Systems Grid (ESG8) offering data exchange and sharing for climate simula-
tion data worldwide, the relatively new NFS funded I-Plant9 Infrastructure’s project
and community based collaborative data sharing environment and the planned DOE
Systems Biology Knowledgebase.10 Therefore at present most research groups have
to rely on ad-hoc solutions, which are often difﬁcult to maintain and not efﬁcient.
A further challenge in collaborating both within research projects and across
projects is the lack of ability to transfer the increasing amounts of data produced
at experimental facilities. Due to the complexity of the network infrastructure, ad
hoc nature of the transfers, data sizes and the interaction of end user applications
and networking are all contributing factors to this situation. The severity of the
networking challenges faced by the users varies depending on the size and rate of the
data to be transferred and the regularity of the transfer. Small scale transfers (MBs to
a few GBs) are relatively well supported today, although data collection in the ‘ﬁeld’
is still a challenge. Medium range transfers (few tens of GBs) can be unreliable (e.g.
lost connection), even more so when these are used for data streaming (sequence
of experimental or observational measurements). For large-scale data transfers it
can be very difﬁcult and time consuming to resolve network problems between any
two sites. There are usually multiple carriers that are participating in the end-to-
end network path, and it is difﬁcult to get any one carrier to take ownership of the
problem. Experiences have shown that to “clean-up” a connection can take in the
worst case several months. So if a connection is not of useful quality, it is usually
going to take days if not weeks to resolve the problem. In this case the researcher
would probably either ﬁnd a work-around (i.e. send it in the post) long before the
problem was resolved or give up, if this was an ad hoc requirement [6]. Therefore
new means would be required to co-analyze the results, without the need to move
the data.
When data ﬁnally ends up in a publication perhaps as a chart, graph, or image,
the researcher needs to feel a high degree of conﬁdence in being able to reproduce
the results. To do so, the researcher not only needs to be able to refer back from the
publication data to the analysis data, to the reduced data, and ﬁnally to the acquired
data, but also to the processes used to derive the different results – thus the researcher
needs access to the provenance of any published data. This is quite a complex chain
once one takes into consideration that a large number of separate data products that
may have been used in conjunction with the experiment dataﬂow. Facilities can help
8www.earthsystemgrid.org.
9http://www.iplantcollaborative.org/.
10http://genomicscience.energy.gov/compbio/#page=news.

262
K.K. van Dam et al.
with the cataloging of acquired data and accompanying provenanceinformation, and
possibly with the cataloging of reduced data if this data was reduced using facility
resources on-site. However data analysis is typically on the leading edge of scientiﬁc
discovery and often this is where scientist and researchers utilize a wide variety of
tools including software they produce for themselves which is almost impossible to
keep track of.
2.7
Data Ownership and Data Citation
So who owns the data? This is a commonly asked, and sometimes hotly debated
question. In the case of national user facilities, the government funded the operation
of the facilities and one may think that this makes a clear case for data ownership.
However oftentimes the people performing the experiments also apply signiﬁcant
skill, labor, and expertise to produce the sample they place in the beam in order
to produce their data. Thus making the data openly available immediately could
be a signiﬁcant demotivating factor perhaps fostering a counter-culture of parasitic
research.
The case of data ownership and access typically needs to be established by
the facility via the data practices and policies which they assert. A one-size-ﬁts-
all policy across user facilities may not be appropriate as there may be different
factors to consider such as the reproducibility of the experiment and the data, the
data volumes produced, the longevity of usefulness of the data, and the typical
publication lifecycle for a particular technique – there are many more considerations
here. However it is typically agreed upon that at some point experimental data
should become publicly available after some predetermined amount of time, though
this means for opening data to the public is not universally applied.
Should these data become public, there typically are no standards pertaining to
how to cite this data. One means has been to keep the data closed and perhaps
only provide it to collaborators who agreed to include the experiment team or the
Principal Investigator on the resulting paper. This method has its merits for ensuring
data citation, however working this way could also impede the scientiﬁc discovery
process by not allowing broader access to the data. Data ownership and data citation
become most contentious when “hot topics” in science emerge. For example,
in the current situation of working to ﬁnd high temperature superconductors,
competing research teams do not want to give away their advantages – or their
data.
Stepping back and surveying the scientiﬁc data management community, there
are emerging standards called a Digital Object Identiﬁer11 (DOIs) which is a
character string that is used to identify a data product. The DOI and its metadata
may also include the data URL where a researcher may locate the data. The DOI
11http://en.wikipedia.org/wiki/Digital object identiﬁer.

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
263
system is implemented via federated agencies coordinated by the International
DOI Foundation12. Some thought needs to be given for how to deﬁne DOIs for
data products as one may easily deﬁne the DOIs either as too ﬁne grained or too
coarse grained. Other complications are what to do with DOIs in the case where
data are adapted from the original – should a new DOI be deﬁned or should the
original DOI stand? The answer depends upon the context of the data situation.
However there is a well-established method via DOIs that could be employed to
help with data citation, though it is far from being universally adopted amongst user
facilities.
2.8
Summary
Data intensive analysis at experimental facilities faces a wide array of challenges,
chief amongst them:
•
Current algorithms are often unable to handle the increasing volumes and
diversity of the data either at all or in a timely fashion.
•
The community requirement for real time analysis cannot be met with present
solutions.
In addition experimental analysis relies heavily on the integration, correlation, com-
parison and synthesize of the single experimental results with other experimental
and computational efforts, requiring not only multi-modal analysis and visualization
solutions that can span scales, levels of theory, and investigative methods, but also a
supporting eco-system of data management, movement, security, and collaboration
services to enable this type of co-analysis.
3
Current Solutions and Standardization Efforts
Many of the challenges described in the previous section have been known to the
community for a considerable time; however the pressure to address them has only
increased in recent years due to the exponential increase in data volumes and the
drive for co-analysis of results. Community efforts so far have largely concentrated
on the improvement of data management support at experimental facilities and the
optimization of single experiment analysis. A few small developments are emerging
at present in the ﬁeld of collaboration support for experimental sciences. In this
section we will describe some key developments in these areas, exemplary for
the ﬁeld.
12http://www.doi.org/index.html.

264
K.K. van Dam et al.
3.1
Data Management Practices and Policies
As data are a fundamental product of the user facilities, by default de facto
data management practices will evolve, but in more deliberate and formalized
situations, data policies are deﬁned and put into practice. In surveying a number
of DOE user facilities, it quickly became apparent that as of this writing the data
practices and policies vary widely. Generalizing across the big data producing
facilities, the newer facilities appear to be taking on some form of data management
for their facility users while the more established facilities (over 10 years in
operation), tend to provide less data management resources. It is important to
keep in mind that data storage capacity and network bandwidth has increased
dramatically over the past 10 years, and this increased value per unit capacity
allows facilities to consider providing more services to users, with the goal being
to accelerate the rate of user produced publications via data management and
data analysis services. To this end, some of the newer facilities have created data
centers for their storage needs. The Linac Coherent Light Source (LCLS) at the
SLAC National Accelerator Laboratory13 has a 2 PB parallel ﬁle system in their
instrument hall [9]. Similarly, the NSLS-II data facility once fully built estimates
that aggregating across its 58 beamlines that the facility could produce up to
500 TB per day which via the technologies available today would be completely
impractical to utilize data practices based upon portable media for dissemination
of experimental data. The Spallation Neutron Source14 at Oak Ridge National
Laboratory has had a functioning data portal coupled with computing infrastructure
since 2006 which utilizes a data management system layered upon centralized data
storage [10].
Also important to consider are country speciﬁc guidelines and policies such as
the US Federal guidelines and standards on information management as put forth in
FIPS Publication 199 [11]. The security objectives which are of concern:
•
Conﬁdentiality – “Preserving authorized restrictions on information access and
disclosure, including means for protecting personal privacy and proprietary
information: : :”
•
Integrity – “Guarding against improper information modiﬁcation or destruction,
and includes ensuring information non-repudiation and authenticity: : :”
•
Availability – “Ensuring timely and reliable access to and use of information: : :”
The impact of a breach of conﬁdentiality, integrity, or availability is assessed to
be either: low, moderate, or high, depending upon the level of adverse affect on
the organizations operations, assets, or individuals. Typically in the case of open
research, the impact is assessed as low impact.
13SLAC: http://slac.stanford.edu/.
14SNS: http://neutrons.ornl.gov/facilities/SNS/.

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
265
Standards to deﬁne harmonized data policies across user facilities are forming in
Europe via the PaN-data a Photon and Neutron Data Infrastructure collaboration.15
Currently there are 11 partners in the PaN-data collaboration from across Europe.
The PaN-data collaboration strives to produce a sustainable data infrastructure for
European Neutron and Photon laboratories with goals to produce a common data
infrastructure across the large European user facilities that supports the scientiﬁc
communities in utilizing these facilities. The work being done by PaN-data includes
standardization activities in the areas of: data policy, information exchange, data
formats, interoperability of data analysis software, and science lifecycle integration
of publications and data.
The PaN-data policy document,16 under development for approximately 18
months, was ﬁnalized in December of 2010. The document standardized upon
NeXus/HDF5 for data formats. The document also strives to strike a balance
between the competitive and collaborative nature of scientiﬁc discovery. The open
access data policy is intended to provide raw data for use and scrutiny by other
researchers, enable data re-use without the need (and additional cost) for re-
measuring, and facilitate data mining to facilitate new research.
Examining key PaN-data policy elements:
•
Data produced at publicly funded facilities are open access with the facility acting
as the custodian.
•
Data are to be curated in well-deﬁned formats.
•
Automatically captured metadata shall be stored and utilized to form a data
catalog which will be on-line searchable.
•
Data are provided as read-only.
•
Ideally each data set will have a unique identiﬁer.
•
Access to raw data and associated metadata becomes open access after 3 years
from the end of the experiment.
•
Appropriate facility staff will have access to the data.
•
The Principal Investigator has the right to copy and distribute the raw data and
can grant access to others.
•
Ownership of results from the analysis of the raw data depends upon the
contractual obligations of the researchers who performed the analysis.
•
The facility will provide the ability for users to upload associated results and
metadata.
•
The facility cannot be made liable in the event of data loss or unavailability.
•
Publications related to experiments performed at these facilities are to be made
known to the facility within 3 months of the publication date.
In the case of proprietary data where the user does not wish for the data to be made
publicly available, beam time is to be purchased at a rate to be determined by the
facility. One could expect such fees to be on the order of some number of thousands
15PaN-data: http://www.pan-data.eu/PaN-data Europe.
16PaN-data Data Policy: http://www.pan-data.eu/imagesGHD/0/08/PaN-data-D2--1.pdf.

266
K.K. van Dam et al.
of dollars per hour keeping in mind that an experiment typically lasts from 1 to
3 days.
To support operations, US user facilities either formally or informally have
developed data management practices and policies. Typically the biggest difference
from the PaN-data policy has been in the areas of data ownership and access, as the
raw data are not obliged to become openly available. However advancements are
being made in the area of Scientiﬁc Data Management (SDM) as an inter-agency
working group has been producing recommendationsand guidelines [12]. Outcomes
from this working group include:
•
Agencies should stimulate cultural change through a system of incentives to
stakeholders. SDM policy should motivate agency researchers to move from the
ownership mindset of data hoarding to a data sharing approach.
•
Each agency should develop a data policy within a federal policy context.
•
Agencies should manage scientiﬁc data for appropriate control while ensuring
appropriate access.
•
Agencies should establish the roles of chief data ofﬁcer and should clarify roles
and responsibilities.
3.2
Data Management Infrastructures
Experimental facilities support a signiﬁcant stretch of the experimental research
process (see Fig. 10.2).
After a successful proposal for experimental time, the researcher will work
with the facility on the experimental design, including instrument conﬁguration
and mode of experimental work. For more standardized measurements such as
crystallography or proteomics, samples are usually sent to the facility, experimental
data is collected, raw data analyzed, and processed data is returned to the user.
The majority of experimental work however requires the presence of the scientists
at the facility, working hand in hand with the local instrument expert on the
experimental set-up, data taking and analysis. Key to the effective support of these
processes is the easy availability of information, tools, and data that are required
for each step. This required information can include not only data and metadata
generated at the facility itself, but also other resources such as data from previous
experiments at other facilities, new tools or discussions about experimental set up.
The increasing complexity of the processes, and a drive to higher efﬁciency in
the facilities operation lead in the early part of this century to the development
of concepts for integrated infrastructures to support the full experimental research
cycle at experimental facilities.
Metadata is hereby seen as the key integrating factor for the different processes
and data products, allowing for the easy management, discovery and access of data
and tools. The Core Scientiﬁc Meta-Data Model (CSMD) developed by the Science

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
267
Fig. 10.2 Life cycle support at experimental facilities
and Technology Facilities Council (STFC) in the UK has hereby emerged as a de-
facto standard [13–16]. CSMD is a study based metadata model, capturing high
level information about experiments, instruments, samples and their resulting raw
and derived data incl. the analysis process (see Fig. 10.3).
Its ﬂexible structure of property lists, allows the model to be customized for any
instrument type. It provides the necessary integration across investigative methods
at a particular institute to support discovery and access, as well as co-analysis tasks.
Scientists have furthermore the ability to link in material from related activities,
which can incorporate other experiments as well as publications and presentations
about the experiment. CSMD is currently used by a wide range of experimental
facilities worldwide to capture and manage their scientiﬁc metadata.
Many of these institutes have developed customized infrastructure solutions for
their particular facility or laboratory based around the core CSMD model. One well
known example is the STFC developed integrated infrastructure for its Neutron
Source ISIS, Central Laser Facility and DIAMOND Lightsource, based around the
Information Catalogue (ICAT17). The software was made open source in 200818 and
was the only one available for distribution and usage by others in this ﬁeld. Since its
release it has been adopted by a range of other facilities in Europe, Australia, and
the US. The complete infrastructure supports all process from proposal submission
17http://www.icatproject.org/.
18http://code.google.com/p/icatproject/wiki/IcatMain.

268
K.K. van Dam et al.
Fig. 10.3 CSMD general structure [17]
to data acquisition and distribution. A 2010 funded UK effort ‘Infrastructure for
Integration in Structural Sciences’19 extended the infrastructure to support and
manage the creation of derived data.
ICAT provides however only the central component of a much more complex
network of services required to support the experimental process, as Fig. 10.4 below
of the infrastructure set up at the UK DIAMOND facility shows.
Key challenges in such infrastructure developments are the integration of the
different components, in this case facilitated through the central ICAT system and
the monitoring of the correct operation and interoperation of the many different
tasks. Newer infrastructure development efforts such as those at the Paciﬁc North-
west National Laboratory (PNNL) Environmental Molecular Sciences Laboratory20
have started to explore the usage of high performance workﬂow systems such as
MeDICi21. Other infrastructure developments based around the CSMD model are
found at the US ORNL Spallation Neutron Source (SNS), the Australian CIMA
19http://www.ukoln.ac.uk/projects/I2S2/.
20http://www.emsl.pnl.gov/emslweb/.
21http://dicomputing.pnnl.gov/demonstrations/medici/.

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
269
Fig. 10.4 Core data management infrastructure components
[18], Archer,22 and eCrystals (UK).23 All of these infrastructures aim to provide
an improved support for their users throughout the experimental process, delivering
improved access to information and data, as well as supporting long term access and
sharing of results.
3.3
Standardization Efforts
Within the X-ray and Neutron scattering communities, there is an emerging data
format standard named NeXus24 which is based upon the HDF5 self-describing data
format. This is a community lead collaboration to deﬁne a suitable data standard
commensurate with the needs of experimental user facilities. Undertaking such an
initiative is no small task as the experimental techniques vary widely across the
user facilities. Some considerations include accommodating the large variations
in detector technologies and geometries, the wide variety of sample environment
22http://archer.edu.au/about/.
23http://ecrystals.chem.soton.ac.uk/.
24http://www.nexusformat.org/Main Page.

270
K.K. van Dam et al.
data to associate with the experiment data, as well as the variety of beam spectrum
monitor information and beam power information. Initially the NeXus format only
supported the histogram data format. NeXus was well suited to this as the data
were written as a ﬁnal step of creating the ﬁle. The intrinsic compress capabilities
of HDF5 were employed which could result in signiﬁcantly reduced ﬁle sizes.
However with the advent of event based data acquisition, it was necessary to extend
the NeXus format to support a list based, or streaming data format. Initially NeXus
was not well suited to supporting arbitrary length data sets, though signiﬁcant effort
was expended to adapt NeXus to better accommodate the intrinsic unformatted
nature of event data.
The data ﬁle creation occurs via a process which has been named data translation
which takes raw input data ﬁles from various sources, massages, and produces a
NeXus ﬁle. The granularity of the data contained within the NeXus ﬁle can be
somewhat arbitrary, however for the sake of convenience;a ﬁle typically will contain
the results from one data acquisition start/stop interval which is often called a “run.”
The raw input data produced during a run are typically comprised of the event data
list, the event pulse information (for pulsed sources such as a spallation neutron
source), or the histogram data in the case of X-ray instruments where individual
X-ray photons occur too rapidly to be counted individually via today’s detector
technology.
The construction of the NeXus ﬁle must take into consideration the mapping
of the pixel information as detector pixel locations may need to be abstracted
to represent a uniform ordering rather than the order which may have resulted
from producing the detector. For example, the lower left corner of the detector
as viewed from the sample may be deﬁned as the origin, however the detector
as created may not deﬁne these pixels in a similar fashion. In these cases, it
is necessary to re-map the pixels to a desired orientation. The mapping process
places the pixels in a relative space, however it is also necessary to locate these
pixels in an absolute space. To do so requires applying instrument geometry
information such as path lengths, orientation angles, and measured information
such as pixel spacing within the detector. Standard samples (such as silicon,
diamond, or other material) can be used to ﬁne-tune the instrument geometry
information.
Experiment related metadata must also be captured and incorporated within
the NeXus ﬁles. There is a wide variety of metadata to consider here and in-
corporate properly. The most important information pertains to the parametric
conditions which the test subject, or sample, was under and called the sample
environment metadata. In some cases, the sample environment data can be con-
siderably large itself. Pressure, temperature, and magnetic ﬁeld are the primary
sample environment data collected. These data must be time-correlated with the
measurements, particularly for event data, to best take advantage of the experiment
information.

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
271
Fig. 10.5 Proteomics mass spectrometry pipeline at PNNL
3.4
Optimization of Single Experiment Analysis
The challenges in chemical imaging analysis stem from large data sets and fast
data generation rates, as well as the drive to faster processing to move from
post analysis to real time analysis and experimental steering. To achieve this
goal the community has principally concentrated on two separate approaches, the
optimization of speciﬁc analysis steps or software systems and the automation of
process chains to support smother turn around.
PNNL developed for example a componentized pipeline for the control and
real time analysis of proteomics mass spectrometry experiments (Fig. 10.5), using
its MEDICi workﬂow system [19]. This is a highly standardized process at the
laboratory and thus lends itself to automation via scientiﬁc workﬂows. The pipeline
combines data intensive analytical software modules, visualization software and
very large databases within a single integration framework (MeDICi). Incoming
spectra from a simulated mass spectrometer are analyzed in real time to determine
the course of processing for individual samples based on comparing them to an
existing results and updating the database of observed mass and time values.
The same spectra are visualized within a central software component along with
additional results of analytical processing. A feedback based on the results of the
analytical processing is initiated back to the instrument which decides whether the
samples have been fragmented already.
This capability provides a spectrum of beneﬁts:
•
Processing of already analyzed features is avoided, which allows more efﬁcient
instrument usage and reduces the amount of redundant data generation. This has
positive impact in data richness and speeds the results to the end user.

272
K.K. van Dam et al.
•
Without the smart instrument control method, experimental results of interest are
usually the hardest to acquire. The described method will lead to more intelligent
data gathering, which will improve analysis quality, reduce costs, and increase
knowledge of the biological systems being studied.
Similarly the Medical College of Wisconsin25 created e.g. an open-source cloud
based environment for the analysis of proteomics results, as their own computing
capacity was insufﬁcient to serve all their users.
Complementary to these automation approaches the community has started to
develop more sophisticated tools for core analysis functions. The US–UK collab-
oration Mantid26 is working for example to consolidate the data analysis software
for neutron scattering experiments by creating a ﬂexible framework that provides a
set of common services, algorithms and data objects. For robustness, the software
aims to be experiment technique independent and supported on Windows, Linux,
and Mac OS platforms. The goal is to provide essential functionality combined with
an easy to use scripting interface, to allow scientists the ability to extend and create
applications. The software is open source and the project currently has 24 active
contributors with almost 600,000 lines of code written. For performance, the data
objects and computation are implemented via the CCC while a python scripting
interface is provided for ease of use and integration. A key feature of Mantid is its
ability to read and process HDF5 based NeXus ﬁles which contain event data while
heavily utilizing multi-threading to accelerate performance.
Other efforts focus on the optimization of speciﬁc analysis methods. Hereby data
reduction methods can expedite the processing and subsequent feature based anal-
ysis [20, 21], including fusion, segmentation, and visualization. Similarly effective
data compression at different levels of image analysis can aid the faster extraction
of useful information and transfer of data to next analysis step. Segmentation
algorithms must be general and intuitive for the non-expert to utilize, and to
be effective in the ﬁeld, the algorithms must also exhibit real-time performance.
Current approaches include e.g. advanced suites of tools for segmentation based on
energy minimization with Graph Cuts, Random Walks, and the Power Watershed
frameworks. More work remains to be done to decrease both the computational
complexity and the memory footprint of these approaches. Feature detection is
another critical component in the analysis process, allowing to emphasize feature of
interest by removing disturbing artifacts and background noise [22,23]. In general,
much effort still needs to be expended to address the efﬁciency of the analysis
algorithms themselves, many of which remain to date sequential algorithms,
which are not adapted to meet the needs of the data intensive requirements of
experimental analysis. Parallelized algorithms are crucial to real time measurement,
analysis, and control of scientiﬁc experiments. Initial efforts at e.g., Lawrence
25http://www.genomeweb.com/informatics/mcw-insilicos-enable-open-source-proteomics-tools-
data-analysis-cloud.
26Mantid Project home page: http://www.mantidproject.org/Main Page.

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
273
Berkley National Laboratory and ORNL are focusing on the usage of parallel
algorithms and high performance computing to speed up the analysis algorithms
themselves [24].
3.5
Data Exchange, Sharing, and Publication
At the same time that technology transformed how we do research and anal-
ysis, science also transformed with whom we work, research today is much
more collaborative, international and interdisciplinary than it was 50 years ago.
Geographically dispersed collaborations are common practice today, even across
several continents and the single researcher or closed local collaborations are
a rarity nowadays [25, 26]. It is clear that with the advent of a much deeper
understanding of scientiﬁc subjects and increasingly complex experimental and
computational technologies a strong individual specialization, not only along the
lines of scientiﬁc topics but also research methodologies has taken place [27,28]. On
the other hand societal problems drive funders to encourage science to help with the
solution for much more complex challenges requiring interdisciplinary (integration
of different science domains) and multidisciplinary (several disciplines make a
separate contribution) projects or borrowing (usage of technologies from a different
discipline), thus a much broader, non domain speciﬁc scientiﬁc knowledge and
information exchange [29]. This exchange forms the basis for the more important
collaborative tasks of co-analysis and visualization of results across techniques and
disciplines.
The general working practices around the sharing of research results have
however not changed much over the past centuries, research publications are still
the main sources of information exchange. Unfortunately publications have certain
limitations in conveying comprehensive information on a particular subject, there
is the restriction in length and thus detail, as well as that its main purpose is
to convey ones point of view rather than necessarily a comprehensive, objective
representation of all facts [30–33]. Publications thus provide at best a very coarse
and high level summary of the research work undertaken by the authors, but are
not suitable in supporting co-analysis tasks. The associated raw and derived data
would be rich source of supporting information, in particular if coupled with the
appropriate metadata and documented scientiﬁc workﬂows [34].
In recognition of the desire by the research community to get access not
only to the summary of a research project, but also the underpinning data, more
publishers today require from their authors that they share their raw and derived
data by depositing it into publicly accessible archives or by providing it on request.
However, recent studies have shown [35, 36] that few authors comply with the
latter requirement, and only the enforced deposition before publication seems to
work. This seems to indicate a continued reluctance to share in-depth research
results with the general research community. Nevertheless a growing awareness
of the value of the produced data as research record in its own right has given

274
K.K. van Dam et al.
rise to the creation of a large number of new institutional and community data
collections [37] and an exponential growth of the existing ones [38]. The drivers
for the creation of these collections are usually organizations and funders, rather
than researchers themselves, this is demonstrated by the low data deposition rates
even in highly regulated data publication subjects such as Crystallography were only
around 20% of all determined structures are publicly accessible, a mere 1.3% of all
know Chemical compounds [39].
The advent of citable data publications is however slowly turning the tide in
a number of research communities, in particular organizations such as DataCite27
work to increase the acceptance of research data as legitimate, citable contributions
to the scientiﬁc record. The ORCID28 collaboration on the other hand, works on
removing the ambiguity in attributing such research records reliably to speciﬁc
scientists.
While researchers may still be reluctant in many ﬁelds to share their data
more globally, it is a core necessity for them to share their data and progress
with their fellow collaborators. In 2008 a NSF workshop on “New Models for
scholarly Communication in Chemistry” investigated the merits of introducing new
web based methods of collaboration and communication into chemistry and thus
experimental sciences. Whilst methods such as semantic web, semantic publishing,
open notebook science and data publishing were seen as embryonic at the time
and had not yet found a broader user base, their undoubted potential to enhance
scientiﬁc communication was clearly identiﬁed [40]. Since then technology has
progressed and a number of interesting developments have emerged in particular
from the former e-Science community. The international LabTrove29 development
combines integrated data management infrastructures for experimental sciences
with online blogging to create a smart research framework. LabTrove integrates
a number of key developments: Electronic Laboratory Notebook, Collaborative
research support through MyExperiment,30 an experimental ontology and a blog
factory [41]. Similarly the PNNL development Velo [42] combines a classical
content management system (Alfresco) with a semantic media Wicki and the
collaborative analytical toolbox (CAT)31 to provide project based collaborative
environment for discussions, data sharing and co-analysis. Velo is currently used
by a wide range of different communities including a number of experimental
groups.
27http://www.datacite.org/.
28http://www.orcid.org/.
29http://www.labtrove.org/.
30http://www.myexperiment.org/.
31http://omics.pnl.gov/software/CAT.php.

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
275
4
Future Directions
A key medium term challenge is the routine co-analysis of scientiﬁc results and
the improvement of analysis tools in general to move towards more sophisticated
community tools that are suitable for both high data volumes and real or near-
real time analysis. Initial efforts are emerging to build the necessary infrastructure
and tools that would offer such capabilities. In the longer term data intensive
analysis for experimental facilities should become an integral part of a more general
data intensive environment that combines both experimental and computational
approaches.
4.1
Co-Analysis Across Different Investigative Methods
Today’s scientiﬁc challenges are complex and usually require the integration of
a wide range of complementary expertise to be addressed successfully. Research
results from a wide range of experimental imaging technologies, ranging from
nano to macroscale, need to be brought together to form a coherent synergistic
picture. At present, however, scientists are usually only familiar with a very limited
range of experimental technologies. Each of these different technologies currently
requires in-depth domain knowledge to enable the user to use the technique correctly
and to be able to interpret the results correctly. Each scientist can therefore only
make use of a very limited palette of experimental technologies. They are thus
limited in their ability to synthesize and connect their own research with the work
of others, who are investigating the same or related topics, but with different
experimental technologies. The ability to go beyond such limitations through a clear
understanding of what each of these technologies delivers in terms of scientiﬁc
insights, and the ability to synthesize results across a wide spectrum of imaging
technologies would be a powerful catalyst for the quality and pace at which scientiﬁc
research and discovery can be carried out. In addition, it would be crucial for the
faster exploitation of those results by industry and academics.
Image informatics is a developing interdisciplinary ﬁeld of research that encom-
passes computer science, statistics, engineering, mathematics, information science,
as well as the natural sciences. The primary challenge is to maximize experimental
outcomes by enabling the correct end to end analysis. If an important bit of data
or metadata is lost or converted into the wrong form for preservation, it is gone
and expensive experiments do not reach their potential or have to be repeated.
The focus of current research in PNNL’s Chemical Imaging Initiative32 is to
deﬁne a framework for chemical imaging co-analysis (Fig. 10.6). This framework
32http://www.pnl.gov/science/research/chemicalimaging/.

276
K.K. van Dam et al.
Fig. 10.6 High level framework overview
will necessarily include capabilities for data preservation, data description, data
management, and data analysis. There is currently no suitable framework available
or under development worldwide that the authors are aware of that appropriately
handles the multitude of chemical imaging methodologies and the petabytes of
data expected to be generated. The routine co-analysis of experimental results
from different imaging technologies has so far not been addressed. The proposed
framework will bring together a range of existing research concepts in the areas
of semantic mapping and rules, workﬂows, and core technologies for the capture,
analysis and integration of experimental data, integrate and develop these further to
create this unique capability.
The workﬂow architecture and the semantic framework will ensure the coherence
of the knowledge capture, exploitation and usage by the different components. The
framework raises the integration needs with emerging requirements on functions,
data types, semantics and real-time properties of the workﬂow to be addressed at
an overarching level. The group anticipates that the data exchange between imaging
technologies will be complex and intensive (petabytes of data to be generated), with
the rapid growths of data sets spanning different spatial and temporal scales. In
response to the challenges of data intensive integration of imaging technologies,
this architecture is being built by leveraging PNNL’s MeDICi (Middleware for Data
Intensive Computing). MeDICi is a middleware platform for building complex, high

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
277
performance analytical applications. It has been proven efﬁcient and successful
as the communication backbone and execution environment for scientiﬁc and
analytic domain applications. Leveraging MeDICi, the group aims to explore the
transformations needed to take data from one technique, tool, or application and feed
it into another during the workﬂow execution. The project focuses on identifying the
intrinsic linkage of the imaging technologies and understanding data characteristics.
The semantic component of the framework will consist of four components:
Characterization, Relation, Analysis, and Representation. Basic concepts for these
areas have been developed and tested for a range of projects [43–45]. However,
these were never applied collectively, nor integrated to capture domain knowledge
in an easily usable form.
Starting with six key imaging technologies initially chosen for the initiative, the
group will develop formal characterizations of the methods, instruments, samples,
analysis processes, and data products associated with each of these, detailing
in particular what each method contributes to the overall domain knowledge
Furthermore, they will determine how each of these methodologies relates to the
others (for example A reﬁnes B, A complements B by adding X), thus building a
formalized topology of the methods, their contribution, and constraints. Based on
these initial characterizations they formalize their functionalities, so that it can be
extended to other techniques and utilized by a wider community.
A further enabling technology identiﬁed for the success of the framework is the
ability for distributed analysis. The instrumentation used to collect experimental
data is expected to continue to improve in resolution and size, thus resultant data
sets can grow into the multi petabyte range. Furthermore the facilities housing these
results will be geographically distributed. While it is possible today to transfer a
few terabytes of data across thousands of miles in a day, poor and unpredictable
data transfer rates are the norm over long distances on wide area networks. If the
performance of long-distance ﬁle transfers cannot be assured, the best alternative
is to minimize the quantity of data that must be transferred. Failing that, the
computation must be brought to the data. The initiative will therefore investigate
new analysis methods that can work across distributed data sets.
In this light there has also been a recent proposal to establish a user facility
network (UFnet) which would facilitate inter-facility data movement and manage-
ment [46]. An initial focus would be the routine integration of multi-technique data
from X-ray and neutron sources. Tech-X Corporation’s open source tool Orbiter33
provides in this context for example:
•
Secure User and Management Interface: Users, managers, and resource providers
demand a rich environment of tools, tutorials, documentation, and customizable
interfaces that can be accessed from Internet capable mobile phones, laptops, and
workstations.
33https://orbiter.txcorp.com.

278
K.K. van Dam et al.
•
Scalable Virtual Organization and Community Management: We envision not
only capitalizing on role-based infrastructures but also providing federated
community identity management capabilities. It is essential that a scalable
management infrastructure provide the ability for DOE stakeholders to audit and
organize their personnel usage by project, department, or service type.
•
Dynamic User-Centric Compute and Storage Resource Status: Up-to-date re-
source status, state, and load is required to dynamically scale enterprise service
infrastructures to meet the stake holder throughput, storage, and bandwidth
requirements for all resources.
•
Reliable Resource Conﬁguration and Management: Efﬁcient and reliable in-
frastructure application deployment and conﬁguration management provides
the feedback necessary for optimizing the deployed applications and services
required to differentiate the Orbiter UFnet production and productivity systems.
•
Easy to use access to HPC mechanisms, via thick and thin clients supporting
Service Oriented Architecture (SOA) based services consist of standards-based
components that are reusable and extensible for accessing high performance
computing, data and computational grid infrastructure, and cluster-based re-
sources easily from a user conﬁgurable interface.
•
A prototype network node services to enable off-line and online simultaneous
multi-technique experiment and analysis for X-ray scattering at APS and neutron
scattering at SNS is shown in Fig. 10.7.
4.2
Long Term Perspective
For the future it is hoped that data will work for scientists rather than scientists
working for their data – network, data, computing infrastructure, and software will
be synergistically integrated to better enable collaborative pursuit of scientiﬁc dis-
coveries resulting from experiments performed at user facilities. Data management
and analysis would hereby be a central component of any such solution and data
issues would be considered as an integral component of any system design [47].
A range of forward looking white papers on data intensive science have discussed
the issues involved in establishing such wide reaching infrastructures and proposed
options for the way forward [48–52]. Each of these is focused on seamless access
to research data and the provision of advanced analysis capabilities.
Open Access and Data Curation (long term preservation for reuse) issues have
long driven the development of standards, methods and infrastructures for data
intensive science in Europe. The 2008 update to the roadmap of the European
Strategy Forum on Research Infrastructures (ESFRI) lists for the ﬁrst time the
need not only for leading edge experimental and computational facilities to drive
future scientiﬁc progress, but also adds the importance of an underpinning e-
infrastructure consisting of integrated communication networks, distributed grids,
high performance computing, and digital repositories components. ESFRI states
further that data in their various forms (from raw data to scientiﬁc publications)

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
279
Fig. 10.7 User facility network prototype science case study features and functionality
will need to be stored, maintained and made available and openly accessible to all
scientiﬁc communities. They are placing a new emphasize on digital repositories
as places to capture and curate the scientiﬁc data both for the good of science and
the economy. Intellectual and Technological progress in these areas has particularly
been driven by centers of excellence, large scale long term infrastructure projects
and organizations with visionary leadership and an in-depth understanding of data
intensive sciences. Key examples for international centre’s and projects are: UK
Data Curation Centre,34 US SciDAC SDM centre, the Earth Systems Grid35 and its
international Partners, e-Infrastructure for Large Scale Experimental Facilities [13]
and the Biomedical Informatics Research Network (BIRN36). These projects have
clearly demonstrated the potential of data intensive science technologies; however
as the report ‘Data-Intensive Research Theme’ [49] notes ‘Current strategies for
supporting it demonstrate the power and potential of the new methods. However,
they are not a sustainable strategy as they demand far too much expertise and help in
addressing each new data-intensive task’. This and other recent publication [48,51]
clearly show the community consensus that more generalized, easy to use solutions
34http://www.dcc.ac.uk/.
35http://www.earthsystemgrid.org/.
36http://www.birncommunity.org/.

280
K.K. van Dam et al.
need to be developed to make a more wide spread use of these basic data intensive
technologies possible. Thought leaders are also pointing out, that while the current
developments of infrastructure surrounding the management of data continue to be
important, it is time to go beyond these basic approaches and focus on the data
itself – developing the means to transform data into an infrastructure in its own
right. In response the European Union announced in 2010 a high level funding
opportunity to develop new seamless infrastructure demonstrators across a wide
range of computational and experimental resources, with the ﬁrst projects set to
start in late 2011.
In the US the National Science Foundation has proved to be a driving force for
change, by requiring structured data management plans from all grant applicants.
Furthermore the NSF is in regular discussions with its European counter parts to
explore the potential for a harmonization of policies and infrastructures. A recent
NSF-OCI Task Force on Data and Visualization recommended to [53]:
•
Identify and share best-practices for the critical areas of data management
•
Effectively and securely offer data services/access to various stakeholder com-
munities
•
Associate scientiﬁc publications with the underlying data and software assets (to
improve the reproducibility of science)
5
Conclusions
Experimental research methods can offer fundamental insights, gained through
a large variety of investigative methods, to help address pressing, complex sci-
entiﬁc challenges. Hereby direct imaging methods are used to probe structure,
properties, and function of objects from single elements to whole communities,
helping to develop an atomistic understanding of scientiﬁc issues. Advances in
the underlying experimental technologies have lead to an exponential growth in
the volumes, variety and complexity of data derived from such methodologies,
making experimental science a very data intensive ﬁeld of science. This exceptional
growth in data volumes and complexity has presented researchers with signiﬁcant
challenges, foremost how to effectively analyze the results of their research both
for single experiments and increasingly across different investigative methods. Key
issues are:
•
Algorithms are often unable to handle the increasing volumes and diversity of
the data either at all or in a timely fashion
•
The community requirement for real time analysis cannot be met with present
solutions
•
While it has been acknowledged that scientiﬁc discovery, like medical diagnosis
of a patient’s condition, require integration of inputs and ﬁndings from a number
of sources there are no routine co-analysis techniques available to the community

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
281
Furthermore experimental analysis relies heavily on the availability of a support-
ing eco-system of data management, movement, security, and collaboration services
to be successful.
Community efforts so far have largely concentrated on the improvement of
data management eco-system at experimental facilities by developing policies,
standards, and integrated infrastructures. The optimization of single experiment
analysis through improved methods and automated analysis pipelines has been a
more recent focus of the community’s research efforts, with a number of exemplary
successes in the area of automation. Only a few small developments are currently
emerging in the ﬁeld of collaboration support for experimental sciences. Initial
research work is emerging focused on building the necessary infrastructure and tools
to support routine co-analysis of scientiﬁc results; however, these projects are still in
their infancy and so this domain is seen as a fertile growth area with many research
challenges still ahead.
Overall, while progress is being made on the development of supportive data
management eco-systems, the key data intensive analysis challenges for experi-
mental facilities remain. There is a critical lack of analytical methods that can
routinely and reliably handle the growing volume and diversity of data, support real
time and co-analysis. Image informatics the interdisciplinary ﬁeld of research that
encompasses computer science, statistics, engineering, mathematics, information
science and natural sciences, as well as data intensive science research itself would
seem to offer the most promising approaches to solving these analysis challenges
and enable the crucial progress for experimental sciences.
Acknowledgements S.D.M. acknowledges that the research at Oak Ridge National Laboratory’s
Spallation Neutron Source was sponsored by the Scientiﬁc User Facilities Division, Ofﬁce of Basic
Energy Sciences, U. S. Department of Energy.
S.D.M and J.W.C. acknowledge that the submitted manuscript has been co-authored by a
contractor of the U.S. Government under Contract No. DE-AC05-00OR22725. Accordingly, the
U.S. Government retains a non-exclusive, royalty-free license to publish or reproduce the published
form of this contribution, or allow others to do so, for U.S. Government purposes.
J.W.C. acknowledges that this material is based upon work supported by the National Science
Foundation under Grant No. 050474. This research was supported in part by the National Science
Foundation through TeraGrid resources provided by the Neutron Science TeraGrid Gateway.
References
1. National Research Council. Visualizing Chemistry: The Progress and Promise of Advanced
Chemical Imaging, The National Academies Press, Washington, DC, 2006.
2. Basic Energy Science Advisory Committee, Subcommittee on Facing Our Energy Challenges
in a New Era of Science, “Next Generation Photon Sources for Grand Challenges in Science
and Energy”, Technical Report, U.S. Department of Energy, May 2009.
3. F. Maia, P. van der Meulen, A. Ourmazd, I. Vartanyes, G. Bortel, K. Wrona, M. Altarelli, G.
Huldt, D. Larsson, R. Abela, V. Elser, T. Ekeberg, K. Cameron, D. van der Spoel, H. Kono,
F. Wang, P. Thibault, and A. Mancuso, “Data Analysis and its needs @ European Xfel”.
Presentation SPB-Workshop 2008 Working Group 3. http://www.xfel.eu/events/workshops/
2008/spb workshop 2008/ (accessed May 6th 2011)

282
K.K. van Dam et al.
4. C. Southan and G. Cameron, “Beyond the Tsunami: Developing the Infrastructure to Deal
with Life Sciences data” In The Fourth Paradigm: Data-Intensive Scientiﬁc Discovery, 2009,
Microsoft Research.
5. C. Goble and D. De Roure, “The Impact of Workﬂow Tools on Data-centric Research” In The
Fourth Paradigm: Data-Intensive Scientiﬁc Discovery, 2009, Microsoft Research.
6. K. Alapaty, B. Allen, G. Bell, D. Benton, T. Brettin, S. Canon, R. Carlson, S. Cotter, S. Crivelli,
E. Dart, V. Dattoria, N. Desai, R. Egan, J. Flick, K. Goodwin, S. Gregurick, S. Hicks, B.
Johnston, B. de Jong, K. Kleese van Dam, M. Livny, V. Markowitz, J. McGraw, R. McCord,
C. Oehmen, K. Regimbal, G. Shipman, G. Strand, B. Tierney, S. Turnbull, D. Williams, and J.
Zurawski, “BER Science Network Requirements”, Report of the Biological and Environmental
Research Network Requirements Workshop, April 29 and 30, 2010, Editors E. Dart and B.
Tierney, LBNL report LBNL-4089E, October 2010.
7. B.F. Jones, S. Wuchty, and B. Uzzi, “Multi-University Research Teams: Shifting Impact,
Geography, and Stratiﬁcation in Science” in Science Express on 9 October 2008, Science 21
November 2008: Vol. 322. no. 5905, pp. 1259–1262
8. E. Yang, “Martin Dove’s RMC Workﬂow Diagram”, a supplementary requirement report, Work
Package 1, November 2009 – June 2010, JISC I2S2 project, July 2010, available at: http://www.
ukoln.ac.uk/projects/I2S2/documents/ISIS%20RMC%20workﬂow.pd
9. E. Dart and B. Tierney, “BES Science Network Requirements – Report of the Basic Energy
Sciences Network Requirements Workshop Conducted September 22 and 23, 2010”.
10. S.D. Miller, A Geist, K.W. Herwig, P.F. Peterson, M.A. Reuter, S. Ren, J.C. Bilheux, S.I.
Campbell, J.A. Kohl, S.S. Vazhkudai, J.W. Cobb, V.E. Lynch, M. Chen, J.R. Trater, B.C. Smith,
T. Swain, J. Huang, R. Mikkelson, D. Mikkelson, and M.L. Green, “The SNS/HFIR Web Portal
System – How Can it Help Me?” 2010 J. Phys.: Conf. Ser. 251 012096. doi:10.1088/1742-
6596/251/1/012096.
11. Federal Information Processing Standards Publication – FIPS PUB 199, “Standards for
Security Categorization of Federal Information and Information Systems” February 2004.
12. Scientiﬁc Data Management (SDM) for Government Agencies: Report from the Workshop to
Improve SDM. “Harnessing the Power of Digital Data: Taking the Next Step. June 29-July
1, 2010.
13. D. Flannery, B. Matthews, T. Grifﬁn, J. Bicarregui, M. Gleave, L. Lerusse, S. Suﬁ, G.
Drinkwater, and K. Kleese van Dam, “ICAT: Integrating data infrastructure for facilities based
science”. Proc. 5th IEEE International Conference on e-Science (e-science 2009), Oxford, UK,
09–11 Dec 2009
14. S. Suﬁ, B. Matthews, and K. Kleese van Dam. (2003) An Interdisciplinary Model for the
Representation of Scientiﬁc Studies and Associated Data Holdings. UK e-Science All Hands
meeting, Nottingham, 02–04 Sep 2003
15. S. Suﬁand B.M. Matthews. (2005) The CCLRC Scientiﬁc Metadata Model: a metadata model
for the exploitation of scientiﬁc studies and associated data. In Contributions in Knowledge and
Data Management in Grids, eds. Domenico Talia, Angelos Bilas, Marios Dikaiakos, CoreGRID
3, Springer-Verlag, 2005.
16. E. Yang, B. Matthews, and M. Wilson, “Enhancing the Core Scientiﬁc Metadata Model to
Incorporate Derived Data,” eScience, IEEE International Conference on, pp. 145–152, 2010
IEEE Sixth International Conference on e-Science, 2010
17. B. Matthews, “Using a Core Scientiﬁc Metadata Model in Large-Scale Facilities”. Presentation
at 5th International Digital Curation Conference (IDCC 2009), London, UK, 02–04 Dec 2009
18. I.M. Atkinson, D. du Boulay, C. Chee, K. Chiu, T. King, D.F. McMullen, R. Quilici, N.G.D.
Sim, P. Turner, and M. Wyatt, “CIMA Based Remote Instrument and Data Access: An
Extension into the Australian e-Science Environment.” Proceedings of IEEE International
Conference on e-Science and Grid Computing (e-Science 2006) Amsterdam, The Netherlands,
December 2006.
19. I. Gorton, A. Wynne, Y. Liu, and J. Yin, “Components in the Pipeline,” IEEE Software, vol. 28,
no. 3, pp. 34–40, May/June 2011, doi:10.1109/MS.2011.23

10
Challenges in Data Intensive Analysis at Scientiﬁc Experimental User Facilities
283
20. D. Li, M. Tschopp, X. Sun and M. Khaleel, Comparison of reconstructed spatial microstructure
images using different statistical descriptors. Submitted to Computational Materials Science
21. D. Li Application of chemical image reconstruction on materials science and technology.
accepted by Proceeding of 2011 World Congress of Engineering and Technology, IEEE, and
will present the paper in October 2011
22. L.M. Kindle, I.A. Kakadiaris, T. Ju, and J.P. Carson (2011) A semiautomated approach for
artefact removal in serial tissue cryosections. Journal of Microscopy. 241(2):200–6.
23. J.P. Carson, D.R. Einstein, K.R. Minard, M.V. Fanucchi, C.D. Wallis, and R.A Corley (2010)
High resolution lung airway cast segmentation with proper topology suitable for computational
ﬂuid dynamic simulations. Computerized Medical Imaging and Graphics. In Press.
24. M. Hohn, G. Tang, G. Goodyear, P.R. Baldwin, Z. Huang, P.A. Penczek, C. Yang, R.M. Glaeser,
P.D. Adams, and S.J. Ludtke, “SPARX, a new environment for Cryo-EM image processing” in
J Struct Biol. 157, 47–55, 2007
25. B.F. Jones, S. Wuchty, and B. Uzzi, 2008. ‘Multi-University Research Teams: Shifting Impact,
Geography, and Stratiﬁcation in Science’ in Science Express on 9 October 2008, Science 21
November 2008: Vol. 322. no. 5905, pp. 1259–1262
26. R. Guimera, B. Uzzi, J. Spiro, and L.A.N. Amaral, 2005. ‘Team Assembly Mechanisms Deter-
mine Collaboration Network Structure and Team Performance’ in Science, 308, 697 (2005).
27. M. Pianta and D. Archibugi, 1991. ‘Specialization and size of scientiﬁc activities: A bibliomet-
ric analysis of advanced countries’ in Scientometrics Volume 22, Number 3/November, 1991
28. W. West and P. Nightingale, 2009. ‘Organizing for innovation: towards successful translational
research’ in Trends in Biotechnology, Volume 27, Issue 10, 558–561, 17 August 2009
29. Committee on Facilitating Interdisciplinary Research, National Academy of Sciences, National
Academy of Engineering, Institute of Medicine. 2004. ‘The Drivers for Interdisciplinary
Research’ in Facilitating interdisciplinary Research p 26–40, 2004
30. D. Shotton, K. Portwin, G. Klyne, and A. Miles, 2009. ‘Adventures in Semantic Publishing:
Exemplar Semantic Enhancements of a Research Article’ in Publication Library of Science
Computational Biology. 2009 April; 5(4).
31. A. de Waard, L. Breure, J.G. Kircz, and H. van Oostendorp, 2006. ‘Modeling rhetoric in
scientiﬁc publication’ in Proceedings of the International Conference on Multidisciplinary
Information Sciences and Technologies, pp 1–5, InSciT2006; 25–28 October 2006; Merida,
Spain. http://www.instac.es/inscit2006/papers/pdf/133.pdf.
32. T. Kuhn, 1962. The Structure of Scientiﬁc Revolutions (Chicago: University of Chicago
Press, 1962)
33. B. Latour, 1987. ‘Science in Action’ in How to Follow Scientists and Engineers through
Society, Cambridge, Ma.: Harvard University Press, 1987.
34. C. Goble and D. deRoure, 2009. “The impact of Workﬂow tools on data-centric research” In
The Fourth Paradigm: Data-Intensive Scientiﬁc Discovery, 2009, Microsoft Research.
35. C.J. Savage and A.J. Vickers (2009) Empirical Study of Data Sharing by Authors Publishing
in PLoS Journals. PLoS ONE 4(9): e7078. doi:10.1371/journal.pone.0007078.
36. J.M. Wicherts, D. Borsboom, J. Kats, and D. Molenaar, 2006. ‘The poor availability of
psychological research data for reanalysis’ in American Psychologist 61: 726–728.
37. D. De Roure, C. Goble, S. Aleksejevs, S. Bechhofer, J. Bhagat, D. Cruickshank, D.
Michaelides, and D. Newman, 2009. ‘The myExperiment Open Repository for Scientiﬁc
Workﬂows’ in: Open Repositories 2009, May 2009, Atlanta, Georgia, US. (Submitted).
38. C. Southan and G. Cameron, 2009. “Beyond the Tsunami: Developing the Infrastructure to
Deal with Life Sciences data” In The Fourth Paradigm: Data-Intensive Scientiﬁc Discovery,
2009, Microsoft Research.
39. S. Coles and L. Carr, 2008. ‘Experiences with Repositories & Blogs in Laboratories’ in
Proceedings of: Third International Conference on Open Repositories 2008, 1–4 April 2008,
Southampton, United Kingdom.
40. T. Velden and C. Lagoze, The Value of new Communication Models for Chemistry, White
Paper 2009, eCommens@Cornell, http://hdl.handle.net/1813/14150.

284
K.K. van Dam et al.
41. J.D. Blower, A. Santokhee, A.J. Milsted, and J.G. Frey, BlogMyData: a Virtual Research
Environment for collaborative visualization of environmental data. All Hands Meeting 2010,
Cardiff UK 13–16 Sep 2010 http://eprints.soton.ac.uk/164533/.
42. I. Gorton, C. Sivaramakrishnan, G. Black, S. White, S. Purohit, M. Madison, and K.
Schuchardt, 2011. Velo: riding the knowledge management wave for simulation and modeling.
In Proceeding of the 4th international workshop on Software engineering for computational
science and engineering (SECSE ’11). ACM, New York, NY, USA, 32–40.
43. L.E.C. Roberts, L.J. Blanshard, K. Kleese Van Dam, L. Price, S.L. Price, and I. Brown,
Providing an Effective Data Infrastructure for the Simulation of Complex Materials. Proc. UK
e-Science Programme All Hands Meeting 2006 (AHM 2006).
44. A.M. Walker, R.P. Bruin, M.T. Dove, T.O.H. White, K. Kleese van Dam, and R.P. Tyer. Inte-
grating computing, data and collaboration grids: the RMCS tool. Philosophical Transactions
of The Royal Society A 367 (1890) 1047–1050 (2009) [doi:10.1098/rsta.2008.0159]
45. A. Woolf, B. Lawrence, R. Lowry, K. Kleese van Dam, R. Cramer, and M. Gutierrez. Data
integration with the Climate Science Modelling Language Proc. European Geosciences Union
General Assembly 2005, Vienna, Austria, 24–29 Apr 2005, Geophysical Research Abstracts,
Volume 7, 08775, 2005 (2005), Fourth GO-ESSP meeting, RAL, UK, 06–08 Jun 2005,
Workshop on Grid Middleware and Geospatial Standards for Earth System Science Data,
NESC workshop, Edinburgh, Scotland, 06–08 Sep 2005.
46. S.D. Miller, K.W. Herwig, S. Ren, S.S. Vazhkusai, P.R. Jemian, S. Luitz, A.A. Salnikov,
I. Gaponenko, T. Proffen, P. Lewis, and M.L. Green, “Data Management and Its Role in
Delivering Science at DOE BES User Facilities – Past, Present, and Future.
47. J. Ahrens, B. Hendrickson, S. Miller, R. Ross, and D. Williams, “Data Intensive Science in the
Department of Energy” October 2010, LA-UR-10-07088.
48. K. Koski, C. Gheller, S. Heinzel, A. Kennedy, A. Streit, and P. Wittenburg. Strategy for a
European Data Infrastructure: White Paper. Technical report, Partnership for Advanced Data
in Europe (PARADE), September 2009.
49. M. Atkinson, M. Kersten, A. Szalay, and J. van Hemert. Data Intensive Research Theme. NESC
Technical Report, May 2010.
50. J. Wood, T. Anderson, A. Bachem, C. Best, F. Genova, D. Lopez, W. Los, M. Marinucci, L.
Romary, H. Van de Sompel, J. Vigen, P. Wittenburg, D. Giaretta, R.L. Hudson. Riding the
Wave – How Europe can gain from the rising tide of scientiﬁc data, October 2010.
51. J. Ahrens, B. Hendrickson, G. Long, S. Miller, R. Ross, and D. Williams. Data Intensive
Science in the Department of Energy, October 2010.
52. K. Kleese van Dam, T. Critchlow, J. Johnson, I. Gorton, D. Daly, R. Russell, and J. Feo.
The Future of Data Intensive Science Experimenting in Data - Across the Scales, Across
Technologies, Across the Disciplines. PNNL White Paper, November 2010. https://sites.
google.com/site/dataintensivesciencecommunity/home
53. D. Atkins, T. Detterich, T. Hey, S. Baker, S. Feldman, and L. Lyon, NSF-OCI Task Force on
Data and Visualization, March 7, 2011.
54. P.
Rich,
“Infrastructure
III”,
I/O
Tutorial,
An
Advanced
Simulation
&
Comput-
ing (ASC) Academic Strategic Alliances Program (ASAP) Center at The University
of Chicago, 2009, http://ﬂash.uchicago.edu/website/codesupport/tutorial talks/June2009/IO
tutorial.pdf (accessed May 6th 2011)
55. Scientiﬁc Grand Challenges – Discovery in Basic Energy Sciences: the Role of Computing at
the Extreme Scale, Report of DOE workshop, August 13–15, Washington DC.
56. B. Fultz, K.W. Herwig, and G.G. Long, “Computational Scattering Science 2010”, Workshop
held at Argonne National Laboratory July 7–9 2010. Workshop report. http://neutronscattering.
org/2011/01/computational-scattering-science

Chapter 11
Large-Scale Data Analytics Using Ensemble
Clustering
Martin Hahmann, Dirk Habich, and Wolfgang Lehner
1
Introduction
The analysis technique clustering is described as the problem of partitioning a
set of objects into groups, called clusters, so that objects in the same cluster are
similar, while objects in different clusters are dissimilar [21]. The identiﬁcation of
similarities between data objects and their arrangement into groups in accordance
to these similarities, are essential tools to gain understanding and acquire novel,
previously unknown knowledge. Although this approach is quite powerful, its
application by humans is limited by the magnitude of the data.
The relevance of clustering as an analysis technique is more and more increasing,
due to the ongoing data gathering trend in different areas of research and industry.
As a result of this trend, large datasets occur in multiple domains, ranging from
gene-expression data in biomedical science to customer-panel data in market
research. Despite being designed for the processing of sizable amounts of data,
clustering as a technique is still challenged by the large-scale of todays analytic
tasks. Before, we begin the discussion of large-scale analytics and how it can be
done using clustering, we ﬁrst outline what large-scale means for us in the context
of clustering, because it is a multifaceted phenomenon.
First and foremost, large-scale concerns the data that is to be analyzed. On the
one hand, dataset volume is constantly increasing, generating challenges regarding
storage, computation, and ultimately runtime. On the other hand, the number of
attributes/dimensions can reach the hundreds or even thousands, which makes
it hard to identify clusters having a high similarity for all features of the data.
M. Hahmann () • D. Habich • W. Lehner
Dresden University of Technology, Database Technology Group
e-mail: martin.hahmann@tu-dresden.de; dirk.habich@tu-dresden.de;
wolfgang.lehner@tu-dresden.de
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 11, © Springer Science+Business Media, LLC 2011
285

286
M. Hahmann et al.
In addition, high dimensionality leads to problems regarding the interpretation and
evaluation of discovered structures as humans have problems grasping spaces with
more than three or four dimensions.
Another aspect of large-scale can be found in the algorithmic diversity of the
clustering domain. Although ofﬁcially unknown, the actual number of available
algorithms is expected to be higher than thousand. To get an idea of the algorithm
landscape, in terms of size, we looked through the proceedings of the SIGKDD
and ICDM data-mining conferences from 2005 to 2010 and counted more than 120
papers that introduced new algorithms or variants resp. optimizations of existing
techniques. This multitude of algorithms exists because it is impossible to design
an algorithm that automatically produces optimal results for any dataset, thus a
lot of techniques are highly specialized and custom-made for speciﬁc application
scenarios or types of datasets.
The last facet of large-scale comprises the application of clustering. We already
mentioned that data gathering is an ongoing trend spanning more and more domains.
This leads to an increasing number of application domains, which calls for versatile
clustering algorithms. Furthermore, the user audience grows, which means that
many people will employ clustering techniques, who are application domain experts
but have no experience regarding clustering. As clustering is an expert domain and
the clustering procedure is not trivial due to its high degree of specialization, this
emerging new class of users is faced with a lot of problems with today’s algorithms.
In this chapter, we discuss the problems arising from the different aspects
of large-scale we have outlined and offer some approaches to tackle them. In
Sect. 2, we give an overview of the mentioned algorithmic diversity and illustrate
issues regarding application and user-friendliness. Then, we describe our conceptual
approach to the aspects of large-scale algorithmics and large-scale application by
proposing a novel feedback-driven clustering process in Sect. 3. Afterwards, we
present an infrastructure which deals with the actual processing of large-scale data
in Sect. 4. We conclude this chapter with a short summary in Sect. 5.
2
Contemporary Clustering Practice: An Assessment
The contemporary clustering practice usually requires the completion of four
steps, during which different decisions must be made by the user. Each of these
decisions strongly inﬂuences the result. In general, the four steps can be described
as follows:
1. Selection of an algorithm.
2. Conﬁguration of the algorithm and subsequent execution.
3. Interpretation of the obtained result.
4. Adjustment of algorithm and parameters to improve result quality.
All of these steps are already demanding for experts and therefore form substantial
challenges for non-expert users. The main problems during most of these steps can

11
Large-Scale Data Analytics Using Ensemble Clustering
287
be subsumed as: the huge amount of choices and the lack of background-knowledge.
Take for example the algorithm selection, during which the user is faced with the
large-scale of available algorithms we already mentioned in the introduction. To
choose the optimal algorithm, the user would not only need to know all existing
algorithms but also which one of them ﬁts best, for the dataset that is to be analyzed.
These problems remain during algorithm conﬁguration, when similarity measures
and parameters must be chosen and also during result interpretation when the user
must select a method for cluster validation.
In this section, we describe and assess several clustering approaches and their
associated algorithms with regard to the four presented steps. Due to the vast
number of algorithms that have been developed and published during the last
years, the ﬁrst two subsections will mostly deal with the algorithm selection and
conﬁguration steps, while the third subsection covers result interpretation and
adjustment. This distribution shows a disproportion between the algorithm-centered
and user-centered steps, that is responsible for many problems non-expert users face
in the area of clustering.
2.1
Traditional Clustering
We use the term traditional clustering to subsume all clustering techniques that
comply with the following mode of application: one algorithm is executed using one
set of parameters, generating one clustering solution for the given data. Later in this
section, we introduce approaches that vary in some points of this application mode,
therefore the term ‘traditional’ is used for differentiation. This group contains the
main body of available clustering algorithms. For giving an overview, we divide it
into three main classes: partitional, density-based and hierarchical methods. Please
note that this classiﬁcation is neither ﬁxed nor universally valid, as the diversity of
the ﬁeld allows for many other.
2.1.1
Partitional Clustering
Members of this class use the following basic approach to group a set of n objects
into k clusters: During object assignment, each cluster is represented by a so
called prototype. Using a similarity/distance measure like the euclidean distance,
the similarity between each object and each prototype is determined and in doing
so, a n  k similarity matrix is constructed. Following similarity evaluation, each
object is assigned to the prototype with the highest similarity and its corresponding
cluster. The initialization of prototypes often incorporates random factors and thus is
most likely not optimal. Therefore, most partitional algorithms iteratively optimize
prototypes with regard to a speciﬁc objective function.

288
M. Hahmann et al.
Fig. 11.1 K-means clustering results for different k and cluster initializations
The most well-known member of this algorithm class is k-means. Although it
was developed more than 50 years ago by [32] resp. [24], due to its simplicity
and ease of implementation, it is still the most popular and most used clustering
algorithm in general. For its execution, the k-means algorithm needs two user-
speciﬁed parameters: the number of clusters k and a cluster initialization. Both
choices have a critical impact on the clustering result. To illustrate this impact we
prepared a toy dataset and some example clusterings, that are shown in Fig. 11.1.
The ﬁrst two clusterings were done with equal values for k but different cluster
initializations—in this case seed values for a random generator drawing the initial
prototype positions—and show very different results. Even for k D 7, which the
correct number of clusters in our example dataset, an improper initialization can
lead to a non-satisfying result. As there exits no reliable analytical way to determine
the right choices for both parameters, in practice k-means is run repeatedly with
different parameters being tested. From the results obtained in this ‘trial and error’
procedure the one that appears to be most meaningful to a domain expert is chosen.
2.1.2
Density-Based Clustering
The algorithms of this class deﬁne clusters as areas of high density in d-dimensional
space, that are separated by areas of lower density. All methods working on that
deﬁnition try to identify dense regions and connect them to clusters, in doing so
different algorithms utilize different ways to model density and connections.
As an example, we introduce DBSCAN by Kriegel et al. [10]. This algorithm
models density using two parameters " and minPts, with " deﬁning the size of a
neighborhood around a data object and minPts specifying the number of objects
that must be located inside this neighborhood in order to be counted as a high
density region. Each object that satisﬁes the given density requirements is called
a core object and thus is part of a cluster. These core objects are then connected
based on the overlap of their "-neighborhoods i.e. if two core objects are located
in each others "-neighborhood they are connected and are members of the same
cluster. In Fig. 11.2, some example clusterings are illustrated. While the density

11
Large-Scale Data Analytics Using Ensemble Clustering
289
Fig. 11.2 DBSCAN clustering results with different densities
speciﬁcations made in Fig. 11.2a lead to a result which is similar to the k-means
clustering in Fig. 11.1a, differing results are obtained if the threshold for high-
density areas is raised by reducing " and/or increasing minPts. For the clustering
depicted in Fig. 11.2b " was halved, which lead to a correct identiﬁcation of clusters
c1 and c2. The density threshold was raised even further for Fig. 11.2c by increasing
minPts. In doing so the obtained clustering properly detects clusters c6 and c7 but
also breaks up the L-shaped cluster into smaller parts as the objects that fulﬁll the
high-density criterion are no longer fully connected using the given parametrization.
Besides the changes in the discovered structures, a high density-threshold also leads
to an increasing amount of noise, that is depicted as dots without ﬁlling in Fig. 11.2.
Density-based methods like DBSCAN have many advantages: they are able to
identify arbitrary shaped clusters, handle noise by ﬁltering objects in low density
regions and they do not need a pre-speciﬁed number of clusters. On the other hand
they are susceptible to the curse of dimensionality as high-dimensional data spaces
are often sparsely populated, which hinders the differentiation between high- and
low-density regions.
2.1.3
Hierarchical Clustering
This class of algorithms creates a hierarchy of clusters in the dataset from which
different clustering solutions can be extracted. This hierarchy is often presented
in a tree structure, a so called dendrogram, whose root contains a single cluster
including all objects of the dataset, while each leave corresponds to a cluster
with one object. Two ways exist to generate such structures: The bottom-up or
agglomerative ways starts at the leaf-level and successively merges the two most
similar clusters until the root is reached. In contrast, divisive techniques start a the
root and iteratively split the most heterogeneous cluster. Examples for these two
approaches are the agglomerative nesting and divisive analysis [23] algorithms.
To determine the similarity between clusters any distance measure can be used.
Because hierarchical clustering algorithms measure the similarity between groups

290
M. Hahmann et al.
of objects and not between a pair of objects, the similarity measurement can be
carried out in different ways, assuming that the distance between two clusters
corresponds to the: (1) maximum, (2) minimum, or (3) average element-to-element
distance between the clusters. In order to generate an actual clustering result from
the dendogramm, it needs to be cut at a speciﬁed height, thus collapsing into
disconnected sub-trees, that represent the clusters. The granularity of the clustering
is inﬂuenced by the cutting height. While a cut near the root will produce a coarse
clustering with few but large clusters, a cut near the leaf-level will produce a ﬁner
grouping with many but small clusters. Besides the employed distance measure and
mode, the cut is the third parameter that must be speciﬁed by the user.
2.1.4
Assessment
Traditional clustering offers a wide selection of algorithms, thus making the steps
of algorithm selection and conﬁguration challenging for users. As algorithms are
often tailored for speciﬁc application scenarios, an optimal selection can only
be made if each methods suitability for the task hand is known. The provided
algorithm examples show that the setup of parameters is a non-trivial and algorithm-
speciﬁc task. As reliable analytic ways for parameter determination are virtually
non-existent, support for this step is only provided by heuristics, rules of thumb or a
data-mining experts assistance. This also affects the adjustment step, as adjustments
are made via switching of algorithms and/or re-parametrization. All these problems
lead to cluster-analysis becoming a trial-and-error procedure.
2.2
Ensemble Clustering
As previously described, traditional clustering often leads to multiple iterations in
which different parameters or algorithms are tried out until a satisfactory result is
obtained. This practice implicitly generates multiple clustering solutions for the
analyzed dataset. The concept of Ensemble clustering takes up this characteristic
and utilizes it explicitly, whereas the mode of application changes to: multiple
algorithms are executed using multiple sets of parameters, generating multiple
clustering solutions that are combined into one ﬁnal robust clustering.
This utilization of different traditional clustering algorithms with different pa-
rameter values aims to tackle the problem, that some algorithms or parametrizations
can fail to work with certain datasets. The set of these multiple clusterings is
called ensemble, while the ﬁnal clustering solution generated from it is called
consensus-clustering. Therefore, this approach is also called consensus-clustering.
Approaches of this class can be divided into: pairwise-similarity approaches and
approaches based on cluster-labels. At this point we focus on the pairwise-similarity
approaches, as these are more relevant for our work.

11
Large-Scale Data Analytics Using Ensemble Clustering
291
Fig. 11.3 An example cluster ensemble
2.2.1
Pairwise-Similarities
In order to generate a single consensus-clustering from an ensemble, a so called
consensus function is needed. This function uses the information regarding cluster
assignment of all ensemble members and incorporates them into a new clustering.
Algorithms working on the basis of pairwise similarities, model the cluster assign-
ments by evaluating the grouping of each object-pair over the whole ensemble
[13, 33]. There are two cases of pairwise similarity: (1) a pair objects is part of
the same cluster or (2) a pair of objects is part of different clusters. For each
clustering of the ensemble, these similarities are represented in the form of a so
called coassociation matrix.
Let us assume the small example of a clustering-ensemble with four clusterings
for a dataset consisting of ten objects, which is shown in Fig. 11.3. All clusterings
differ in number of clusters or cluster composition as they are generated using e.g.
different parameters. In Fig. 11.3e, the local coassociation matrix for clustering C4
is shown. A cell containing 1 shows that the respective pair of objects is located
in the same cluster e.g. .x4; x5/ while a 0 indicates that an object pair is assigned
to different clusters. For the generation of the consensus clustering, at ﬁrst a global
coassociation matrix is build by adding up all local matrices and then normalizing
each cell using the ensemble size. Thus, the global coassociation matrix contains
the relative frequency in which each pair of objects is located in the same cluster.

292
M. Hahmann et al.
Fig. 11.4 An example clustering-ensemble
For our example, the global coassociation is depicted in Fig. 11.4a, as the ensemble
contains four clusterings, the resulting values are multiples of one quarter. Based
on this matrix, different consensus functions can be employed to extract the ﬁnal
solution.
For our example, we use a very simple function based on [12], which aims to
generate a consensus-clustering that shows minimal dissimilarities to all clusterings
of the ensemble in terms of pairwise similarities. This basically means that if a pair
of objects is located in the same cluster in the majority of the ensemble it should also
be part of the same cluster in the consensus solution. Vice versa, this also holds for
object pairs mostly located in different clusters. To achieve these goals, we simply
remove all cells from the coassociation matrix that contain a value smaller than 0:5
and use the remaining cells to generate the clustering. In Fig. 11.4a, the cells that
fulﬁll the ﬁlter requirement are highlighted by a bold outline. These cells show that
.x1; x2; x3/, .x4; x5/ and .x6; x7; x8; x9/ are members of the same clusters in at least
ﬁfty percent of the ensemble, thus forming the clusters of the consensus clustering
depicted in Fig. 11.4b.
An alternative way to generate the consensus-clustering is to interpret the
coassociation matrix as an edge-weighted graph, use an algorithm for graph-
partitioning like METIS [22] to cut edges with a weight smaller 0:5 and build
the consensus clusters from the disconnected subgraphs. Further approaches and
examples of consensus functions can be found in [12,33].
2.2.2
Assessment
Ensemble clustering features some interesting beneﬁts for the algorithm selec-
tion and conﬁguration steps. One the one hand, the use of multiple algorithms

11
Large-Scale Data Analytics Using Ensemble Clustering
293
and parameter values relieves the user from the necessity to ﬁnd the single
optimal algorithm-parameter combination. On the other hand, ensemble clustering
determines more robust results and thus leads to an expanded applicability. Un-
fortunately, these beneﬁts come with a huge drawback concerning adjustments. If
adjustments are necessary due to an unsatisfactory result, the user not only has to
decide on switching one algorithm or one set of parameters but has to conﬁgure
a whole set of algorithms including new choices like ensemble size and algorithm
composition.
2.3
Result Interpretation and Adjustments
While the previous subsections dealt mostly with the algorithmic side of the
clustering practice, this subsection focuses on the ways of communication between
the clustering algorithms and the user. Basically, there are two directions for
this communication: (1) from algorithm to user, which is covered by the result
interpretation step and (2) from user to algorithm in the form of the adjustment step.
We discuss both steps in combination, as they fundamentally depend on each other.
During result interpretation, the user evaluates and decides whether a clustering
solution is satisfying or not. In the latter case, it is necessary to identify what makes
the clustering unsatisfactory and derive what must be done to improve the result i.e.
what adjustments must be made. We begin our discussion with available techniques
for result interpretation, which can be divided into the two main areas of quality
measures and visualization.
2.3.1
Quality Measures
The methods of this area try to answer the question ‘How good is the obtained
clustering?’. Answering this question is nearly impossible as there is no universally
valid deﬁnition of clustering quality. As a result of this situation, multiple quality
metrics exist [5, 7, 27]. One extremely popular approach to clustering quality,
that is often used in publications to evaluate the performance of the proposed
algorithm, is comparison to a known solution. This known solution is usually build
by application-domain experts, who manually label the data, and is considered
optimal. The quality of a clustering solution is then measured by quantifying the
deviation from this gold standard e.g. by using the Rand Index [27]. Obviously this
approach is not usable in real-world applications, as the optimal clustering of a
dataset is typically unknown, which is the fundamental reason to utilize clustering
in the ﬁrst place. Therefore, most quality measures are based on the general goals of
clustering, namely high intra-cluster similarity and high inter-cluster dissimilarity.
Typically, a quality measure models these two goals and uses the ratio between
them to express quality. Examples for such quality measures are Dunns Index [7]
or the Davis Bouldin Index [5]. Each of these methods uses an individual deﬁnition

294
M. Hahmann et al.
of clustering quality, thus their expressiveness depends on the clustered data, the
employed algorithm and parameters and the application scenario in general. This
lack of universality means that quality measures can only be applied for an absolute
result interpretation in well-known scenarios and in combination with application
domain knowledge. Otherwise they can only be used for orientation or the relative
comparison of clustering results. In addition, the coarse granularity of quality
measures makes them inappropriate for the derivation of adjustments, as typically
whole clusterings or clusters are mapped to a single numerical value, which means
informations concerning the actual cluster structures are lost.
2.3.2
Visualization
The human being has exceptional visual perception capabilities, that can be
addressed for result interpretation by employing visualization techniques. Via
graphical presentation of the dataset and the obtained cluster assignments it is
possible to communicate ﬁne grained information about the identiﬁed structures
to the user. Displaying the raw dataset and its assignment to the identiﬁed groups
allows the user the subjective interpretation of the obtained result without the bias
added by the speciﬁc deﬁnitions of quality measures. As the user is often an expert
of the application domain, he/she possesses background-knowledgethat permits the
evaluation of the clustering solution.
We already used a visualization technique in the Sect. 2.1 regarding traditional
clustering, by depicting clustering results as scatterplots e.g. in Fig. 11.1. These plots
show each data object, its location in the two-dimensional space of our example
dataset and its cluster assignment. Although scatterplots are a very convenient
technique for the interpretation of our small examples, its usefulness suffers when
it comes to large-scale datasets. Data-driven techniques like scatterplots always
display all objects of a dataset, which can be a problem for high-volume data as
there may be too much objects for the available display space leading to occlusions
and other unfavorable effects. Besides that, the biggest problem is the presentation
of an arbitrary number of dimensions on a two-dimensional medium like a computer
monitor or this book. In addition to these technical problems, most humans have
problems to grasp high-dimensional spaces as our physical universe just spans three
dimensions.
2.3.3
Adjustments
In contrast to the previous steps, little can be said regarding approaches to the
adjustment of clusterings. In general it is accepted that adjustments are made
by changing the clustering algorithm or the algorithm-speciﬁc parameters. The
problem with this approach is, that the user must explicitly know how the clustering
algorithm works and how its work is inﬂuenced by each parameter. Otherwise it is

11
Large-Scale Data Analytics Using Ensemble Clustering
295
not possible to achieve the intended adjustment. An opposite approach to this very
technical low-level way of adjustment is proposed in [2]. In their highly theoretical
work the authors propose two high-level parameters split and merge, with which the
user can adjust a clustering. They prove that each clustering C can be adjusted into
an arbitrary Clustering C 0 just by the subsequent splitting and merging of clusters
and present some observations regarding upper bounds for the necessary number of
split and merge steps. This approach can only be seen as a proof of concept as the
assumed setting was very limited i.e. the dataset was of small volume and only one-
dimensional while the known optimal solution was presented to the user in order to
allow the selection of the appropriate adjustment steps. Nonetheless this approach
shows a way of adjusting clusterings that is very interesting for non-expert users.
2.3.4
Assessment
Similar to the algorithm-centered steps of selection and conﬁguration the area of
result interpretation offers a huge variety of techniques. Again selection of a certain
technique is not trivial as the choice of an unsuitable quality measure can prevent
that an actually satisfying result is recognized as such. As visualizations do not
explicitly state the clustering quality but leave the interpretation to the user, selection
is a less critical problem. However, nearly all data-driven visualization techniques
become less useful for high-volume and especially high-dimensional datasets, as
they communicate more information than the user can process/comprehend.
2.4
Summary
In this section we described the current clustering practice, its four basic steps and
their available approaches and methods. We also outlined the limitations regarding
applicability and usability for non-expert users(large-scale application). In general
we can state that the vast amount of choices for clustering algorithm, parameters
and interpretation methods (large-scale algorithmic) in combination with a lack
of background-knowledge are the main problems concerning the creation of a
clustering. Thus the described clustering practice is carried out in an iterative
fashion, that includes—more or less random—variations during each step which
effectively leads to a ‘trial-and-error’ mode of operation. Needless to say, this has
a negative inﬂuence on result quality, runtime/number of iterations and ultimately
user satisfaction. To tackle these problems and handle the different notions of large-
scale it is necessary to reconsider the current practice of clustering with its separate
steps and to evolve the available approaches in order to achieve a tight inter-step
coupling that leads to a process, covering clustering creation and the associated
issues of large-scale in an end-to-end fashion.

296
M. Hahmann et al.
3
From Trial-Driven Practice to Feedback-Driven Process
In order to tackle today’s clustering limitations and to enable an efﬁcient large-scale
clustering, we propose a feedback-driven clustering process covering all essential
clustering steps. Our main process idea is to present an initial data clustering
result to the user and offer him/her opportunities to adjust the result in an iterative
fashion by giving feedback to the process. The initial clustering is generated by
an algorithmic platform based on ensemble clustering providing an advantageous
starting point due to the robustness of a consensus clustering. For the feedback
mechanism, instead of using low-level technical parameters as mentioned in the
previous section, we introduce more high-level effects like those described in [2].
Therefore, we deﬁne the following feedback operations:
Merge This operation fuses clusters, that are interpreted as too similar by the user,
into a single new cluster.
Split Using this operation, a cluster with low internal homogeneity can be split into
multiple clusters. The resulting number of clusters depends on the underlying
clustering-ensemble.
Reﬁne This operation removes possible outlier objects from a cluster and classiﬁes
them as noise.
Restructure Up to now, our proposed feedback operations have modiﬁed clusters.
The restructure operation is special since it does not directly change the cluster it
is applied to but its underlying clustering-ensemble. In some cases, it may happen
that a cluster cannot be adjusted as intended, because the underlying cluster
ensemble does not permit it. With the restructure operation, a new clustering-
ensemble can be generated for the objects of the selected cluster, which forms a
new basis for cluster adjustments.
In one iteration of our feedback-driven ensemble clustering process, the user
assigns one of these operations to a cluster and then triggers the next iteration, where
the speciﬁed adjustments are implemented. In the following iteration, the adjusted
clusters are evaluated and further adjustments are applied if necessary. In the case
that an applied feedback operation did not have the desired effect, the user can
execute an undo that reverts the respective cluster to the state before the feedback
operation was applied.
For the implementation of our high-level feedback, we developed a special
algorithmic platform which is described in the following Sect. 3.1. Besides this
platform, our concept of iterative result adjustment also needs a criterion for
optimization, that allows users to evaluate, whether the clustering result is improved
or not. To overcome the described problem, that there is no universally valid
deﬁnition for clustering quality (see Sect. 2.3), we introduce a different perspective
to the problem. When interpreting the clustering result, we evaluate how “good”
the underlying dataset ﬁts to the clustering scheme according to the constraints
dictated by the applied algorithm. In other words, we look at how “good” the data

11
Large-Scale Data Analytics Using Ensemble Clustering
297
was clustered from the clustering algorithm’s point of view [18]. The integration of
this notion of ﬁt into our process, requires a way to communicate the ﬁt between the
data and the current clustering schema, which we describe in Sect. 3.2.
3.1
Algorithmic Platform
As already mentioned, our algorithmic platform corresponds to an ensemble clus-
tering approach. For the subsequent explanations, we assume the following setting:
let D be a dataset fx1;    ; xng consisting of n objects and CE be a clustering-
ensemble fC1;    ; Ceg, from which a consensus clustering OC is constructed. Each
Cl 2 CE.1  l  e/ has kl clusters c1;    ; ckl , satisfying Skl
iD1 ci D D. Until
now, we assumed that CE consists of clusterings with crisp/hard assignments, that
assign each point xi 2 D to exactly one cluster, that is denoted by a unique label.
Based on this, we denote the pairwise-similarities of two points in Cl with: (1) a+
for objects with equal cluster labels/assignments, and (2) a- for object-pairs with
different assignments in Cl.
Besides these kind of assignments, there exist algorithms that generate so called
soft cluster assignments, which assign each point xi 2 D to all clusters of Cl to a
certain degree. Thus, the cluster assignments provide more ﬁne-grained information
than their crisp counterpart. The utilization of this additional information enables
our algorithmic platform to fulﬁll the requirements of our proposed clustering
process. In the following, soft cluster assignments are denoted as vectors #»vi with
the components vip.1  p  kl/, describing the relation between xi and the p-th
cluster of Cl. The construction of a consensus clustering from soft assignments is
challenging, because it requires the determination of pairwise-similarities based on
vectors. A simple way to do this, would be to state that xi and xj are members
of the same cluster if their assignment vectors #»
vi and #»
vj are equal by components.
Unfortunately, this strict condition most likely leads to nearly no a+ assignments.
Therefore, another approach is to soften the equality constraint and assume that
a+ holds for objects with similar assignment vectors. This principle is already
employed by some existing ensemble clustering concepts for soft input sets [17,35].
Both approaches use well-known distance measures—e.g. the euclidean distance in
[17]—to calculate the similarity between vectors and derive pairwise-similarities. If
the calculated similarity exceeds a certain threshold, the resp. points are considered
as a+ or else as a-. The major problem of these approaches is the use of common
distance measures. We illustrate this problem by assuming the following example:
a clustering Cl with kl D 2 and a set V of 11 vectors #»
vi; i 2 f0; 1; 2; : : : ; 10g that
represent different soft assignments, satisfying P2
pD1 vip D 1; 0  vip  1, and
8vi1 D i=10. As we want to examine pairwise similarities, we generate 121 vector
pairs .#»
vi; #»
vj / via the cartesian product V  V .
We start by applying the L2 norm resp. euclidean distance to V V . In Fig. 11.5a,
the obtained results are shown; (1) via x- and y-coordinates a vector pairing .#»
vi; #»
vj /

298
M. Hahmann et al.
Fig. 11.5 Different distance measures applied to two-dimensional vectors
is speciﬁed, while (2) the corresponding z-value represents the L2 distance for
this pair. For example, the pair #»
vi > D .1; 0/ and #»
vj >.0; 1/ in the left corner of
Fig. 11.5a has a distance of
p
2. When measuring the distance between two vectors,
L2 only considers their norm but not their direction, which is a major drawback
in our scenario. Thus it is possible, that pairs #»viI #»
vj have an equal L2 distance,
regardless of xi and xj actually being in the same cluster or not. For example, the
pair #»
vi > D .0:1; 0:9/ and #»
vj >.0:3; 0:7/ is located in cluster 2, i.e. a+ holds, while
#»
vk> D .0:6; 0:4/; #»vl >.0:4; 0:6/ is separated in clusters 1 and 2, i.e. a-. Although
the pairwise-similarities are actually different, both pairs have the same L2 distance
of
p
0:08. It is obvious that this can lead to incorrect decisions in the construction
of OC, especially if thresholds or clustering algorithms are employed. Consequently,
vector direction is vital for an accurate interpretation of pairwise-similarities.
In order to tackle this issue, we examine distance metrics that take the direction
resp. composition of vectors into account. At ﬁrst, we look at the Pearson correla-
tion coefﬁcient (%) assuming a+ for positive and a- for negative linear dependency
between #»
vi and #»
vj . In Fig. 11.5b, we can see two pairs of separated planes as results
of our experiment. When examining vector pairs and their corresponding %, we
can conﬁrm our assumption about the relation between the value of %.#»
vi; #»
vj / and
pairwise-similarity. The correlation coefﬁcient has two advantages: (1) direction
awareness and (2) a direct link between the pairwise-similarity and the algebraic
sign of the %-value. Regarding Fig. 11.5b, we notice gaps between the planes. These
originate from vector pairs where at least one member has zero variance (2 D 0).
The Pearson correlation coefﬁcient is deﬁned as the ratio of the covariance of two
vectors and the product of their standard deviations. Therefore, 2 D 0 leads to a
division by zero, making % undeﬁned. To get rid of this problem, we exclude the
mentioned division from %, reducing it to the covariance. The results for this last
experiment are shown in Fig. 11.5c, where a behavior similar to % can be observed,
with the difference, that there are no undeﬁned areas and continuous values. The
last two experiments have shown a special behavior of % and covariance for vectors
with 2 D 0. While % is not deﬁned for these cases, the covariance yields zero.
Vectors #»vi with 2=0 are an interesting phenomenonin the area oft soft clustering
assignments. They satisfy 8vipjvip D
1
kl , stating that the respective object xi
has equal relations with all clusters of Cl. Thus, it is impossible to determine an

11
Large-Scale Data Analytics Using Ensemble Clustering
299
explicit cluster afﬁliation for this object. We refer to such cases as fully balanced
assignments. Because it is impossible to decide to which cluster an object xi with a
fully balanced assignment belongs, it is also not possible to determine the pairwise-
similarity of any pair containing xi.
3.1.1
Triadic Pairwise-Similarity
The existence of fully balanced assignments and the connected issue of undecid-
able pairwise-similarity, require the expansion of the present notion of pairwise-
similarity. Until now, existing ensemble clustering approaches assume that pairwise-
similarity is dyadic, i.e. has two values: a+ and a-. To handle object pairs with
undecidable assignments, an additional value must be deﬁned for these cases. We
denote this value as a?, thus making pairwise-similarity triadic. In order to correctly
determine the pairwise-similarity for any pair .xi; xj / in a clustering Cl, we need to
know if #»
vi and/or #»
vj is fully balanced. This is the case if each component of #»
vi equals
1
kl . An additional form of undecidable assignments, which we denote as balanced,
occurs with vectors having more than one maximum component vip. Assume e.g.
an object xi with #»vi > D .0:4; 0:4; 0:2/ for a clustering Cl with kl D 3 clusters.
Although it can be stated, that xi is not a member of cluster 3, it is impossible to
specify whether the object effectively belongs to cluster 1 or 2. In contrast, a vector
#»
vi > D .0:6; 0:2; 0:2/ containing multiple equal but not maximal components vip is
not critical. As long as the maximum vip is singular, we can derive a clear cluster
afﬁliation. Based on this observation, we deﬁne a balance-detection function f b.#»
vi/
testing if an object xi has a fully balanced or balanced assignment. If #»
vi contains
multiple maxima, hence showing no clear cluster afﬁliation, the function f b.#»
vi/
results in true; otherwise f b.#»vi/ yields false.
In addition, we need to decide whether two objects xi and xj belong to the same
partition of Cl or not. Therefore, we regard the strongest cluster afﬁliation of xi i.e.
the maximum vip. If the maximum components vip and vjq of two vectors #»
vi; #»
vj ,
are located in the same dimension of their respective vectors, xi and xj belong
to the same cluster. In contrast, objects with maximum components in different
dimensions of #»vi are located in different clusters. Based on this, we deﬁne a co-
occurrence function fc.#»
vi ; #»
vj/, stating whether .xi; xj/ is part of the same cluster:
sim.xi; xj / D
8
ˆˆ<
ˆˆ:
1;
if fc.#»
vi ; #»
vj/ D 1 and :.f b.#»
vi/ _ b.#»
vj//
1;
if fc.#»
vi ; #»
vj/ D 1
0;
otherwise
(11.1)
Now, we can create a function sim() (11.1) that determines the pairwise-similarity
of any object pair in a given clustering Cl. If no object has a balanced or fully
balanced #»
vi and if both objects are clearly related with the same cluster of Cl,
our function sim.xi; xj / returns 1, signifying a+ for xi and xj . By contrast, the

300
M. Hahmann et al.
Fig. 11.6
!
vi with different
signiﬁcances
result 1 denotes a-, in which it is not relevant if balanced objects are part of
the pair in question. Assume for Cl with kl D 3 a balanced #»
vi D .0:4; 0:4; 0:2/
and #»
vj
D .0:1; 0:1; 0:8/. Because the maximum components are in different
dimensions, a- holds. Although we cannot decide to which cluster xi belongs, it is
deﬁnitely not the cluster xj belongs to. The result 0 is only obtained for object pairs
containing fully balanced objects or pairs with balanced assignments that co-occur
(fc.#»
vi; #»
vj / D 1) and indicates a?. Our function sim() solves the problems described
at the beginning of this section and allows the correct determination of one of the
three values of our triadic pairwise-similarity for any arbitrary object pair.
3.1.2
A Notion of Signiﬁcance
By deﬁnition, all values of our triadic pairwise-similarity are absolute, which means
that all pairwise-similarities are equal regarding their signiﬁcance. In the following,
we propose that in the context of our scenario, the calculated pairwise-similarities
for certain pairs of objects, can have different levels of conﬁdence.
Consider the example shown in Fig. 11.6, of a clustering Cl with kl D 3 clusters
and their respective centroids c1;
c2 and c3. The grey lines show the borders
of the area of inﬂuence each cluster has. An object located on those lines or at
intersection points has an equal degree of similarity with adjacent clusters and has
thus a balanced resp. fully balanced assignment. The two depicted objects x1 and
x2 have a very strong relation with c1 and only negligible links with the remaining
clusters of Cl. For this example, our function sim.x1; x2/ results in 1, hence a+ for
x1 and x2. Now regard object x3: it still has the strongest degree of similarity with
c1 but it also has a nearly equal similarity with c2 and c3, bringing x3 very close
to a fully balanced assignment. Nevertheless, sim.x1; x3/ determines that x1 and x3
both belong to cluster c1, which is correct in this example. However, comparing both
pairwise-similarity results, we would intuitively say that the one made for x1; x2 is
more conﬁdent.

11
Large-Scale Data Analytics Using Ensemble Clustering
301
This subjective way of interpreting the signiﬁcance of the pairwise-similarity,
takes two properties into account: (1) #»
vi and #»
vj should show an explicit cluster
relationship i.e. show high dissimilarity to the fully balanced assignment, like
.x1; x2/ in Fig. 11.6; (2) #»
vi and #»
vj should have a high component-wise similarity,
which is also the case for .x1; x2/ in Fig. 11.6. The degree of satisfaction of these
requirements, constitutes our notion of signiﬁcance. Some examples of pairs that
maximize both criterions, are the corners of the planes, shown in Fig. 11.5. It is
plausible to assume that, starting from these locations, the signiﬁcance decreases
when approaching the middle of the plane or one of its bisectors—the grey lines
in Fig. 11.6—, where balanced or fully balanced assignments are located. As we
can see in Fig. 11.5c, the covariance partly shows this desired behavior of high
values at the corners and low resp. zero values in the middle of the plane. Based
on this observation, we deﬁne a scoring function, score.#»
vi ; #»
vj/—similar to the
covariance—that returns a signiﬁcance score for the pairwise-similarity determined
for a pair .xi; xj / in a clustering Cl.
score.#»
vi; #»
vj / D
kl
X
p;qD1

jvip  1
kl
j  jvjq  1
kl
j

.i ¤ j; p D q; 1  .p; q/  kl/
(11.2)
A high value from score.#»
vi ; #»
vj/ indicates a high signiﬁcance for the determined
pairwise-similarity. To calculate the pairwise-similarity including the signiﬁcance
scoring for any pair of objects, we combine score.#»
vi; #»
vj/ and sim./ into one
single function: sim0.xi; xj / D sim.xi; xj /  score.#»
vi; #»
vj/. With this, the result
interpretation changes slightly. Now, the pairwise-similarity is denoted by the
algebraic sign of the result, while its absolute value represents the signiﬁcance score.
For the undecidability result a? the function simply yields 0.
At this point, we can only evaluate the signiﬁcance score in relation to other
signiﬁcance scores, stating e.g that the pairwise-similarity a+ for .x1; x2/ has a
higher signiﬁcance than for .x1; x3/. Assumptions on an absolute scale require, the
normalization of results, so that sim0.xi; xj / yields 1 resp. 1 if a+ resp. a- hold
with maximum signiﬁcance. To achieve this, we examined those pairs for which
maximum signiﬁcance occurs, and derived a normalized function sim00 [20] that
meets this goal.
3.1.3
Building Flexible Consensus Clusterings
Subsequently, we describe how our introduced expansions are integrated into
ensemble clustering, to make it ﬂexible and enable result adjustments. As basic
consensus procedure, we adopt the idea described by Gionis et al. in [12]. Using
our function sim00, we determine the pairwise-similarities of every object pair in
all clusterings of CE. When deciding on the pairwise-similarity for .xi; xj / in the
consensus result OC, we enact a majority decision and choose the pairwise-similarity

302
M. Hahmann et al.
Fig. 11.7 Results of sim00 with and without ﬁltering
occurring the most for .xi; xj /. If no majority can be identiﬁed e.g. due to equal
occurrences of different pairwise-similarities, we assume a? for the corresponding
pair in the consensus clustering, because the ﬁnal assignment is effectively unde-
cidable. Although this method, allows the construction of a consensus-solution, it is
still lacking ﬂexibility resp. a method of control.
To achieve this control, we utilize the signiﬁcance scores provided by sim00 and
ﬁlter all pairwise-similarities according to their score. This can be done with a
simple ﬁltering function that returns 0 if sim00.xi; xj /j  t and casekCk.xi; xj /
otherwise. Whereas, the threshold t speciﬁes the minimum signiﬁcance score, that
a pairwise-similarity needs to be considered in the consensus procedure. Therefore,
all pairwise-similarities, not exceeding t are replaced by a? with zero signiﬁcance.
With this, it is possible, to create an area of undecidability that allows us to mark
not only balanced/fully balanced assignments as a?, but also those assignments in
their close proximity. Lets regard our example in Fig. 11.6 again, where undecidable
assignments are located on the grey lines and the pairwise-similarity for .x1; x3/ is
a+ with low signiﬁcance due to x3. If we apply ﬁltering, the grey lines of Fig. 11.6
expand and form an area of undecidability that can be described as a union of circles
centered at intersection points and broadened lines/stripes. With increasing t the
circles radii and width of stripes also increase. If the t-deﬁned area is big enough
to enclose x3, its assignment becomes undecidable. Under these conditions, the pair
.x1; x3/ is classiﬁed as a?. Basically, via ﬁltering we guarantee a minimal conﬁdence
for all decidable pairwise-assignments.
In Fig. 11.7a, the results of our function sim00 for the experimental setting
from the beginning of this section are shown. We observe the desired behavior
of maximum signiﬁcance scores at the plane’s corners. Take for example the left
corner at #»
vi > D .1; 0/ and #»
vj > D .1; 0/, the pairwise-similarity for this pair is a+
with maximum signiﬁcance, so sim00 yields 1 at this point. The signiﬁcance drops
linearly towards and equals zero at the planes middle and its bisectors. The middle
of the plane is speciﬁed by #»
vi > D .0:5; 0:5/ and #»
vj > D .0:5; 0:5/. This pair is

11
Large-Scale Data Analytics Using Ensemble Clustering
303
Fig. 11.8
OC using existing pairwise consensus procedure
composed of two objects with fully balanced assignments, making it undecidable
i.e. sim00 yields zero. When we apply ﬁltering with threshold t D 0:3, the results
change to Fig. 11.7b. A ﬂat area has formed around the center of the plane and its
bisectors. None of the object pairs in this area satisﬁes the ﬁltering criterion, and
hence they are classiﬁed as a?.
With the methods proposed so far, we are able to determine one of the triadic
pairwise-similarity values on the ensemble-level and can control the amount of a?
via t. In doing so, stable cores can be deﬁned in the consensus-clustering—a+,a- that
are robust against t—, while areas of undecidable a?’s are located in their vin-city.
These areas are the key to result adjustments via our proposed feedback operations.
To illustrate the workings of our control method, we again use the synthetic
dataset introduced in Sect. 2. This data contains seven clusters, of which two
are very close but not linked and two cluster pairs are connected via bridges of
different length and width. Figure 11.8 depicts the consensus clustering, obtained
by employing the consensus procedure introduced in [12]. The clustering-ensemble
was generated using k-means [11] with different parametrizations. Due to some
characteristics of the dataset, it is very unlikely that single runs of k-means will
produce the optimal clustering, even if many iterations with different parameters
are made. By applying existing aggregation approaches, the result can be already
improved even for disadvantageous algorithm-dataset combinations. The obtained
result consists of ﬁve clusters, where three clusters c1; c2, and c5 might be divided
further while the remaining two clusters should be merged. As this aggregation

304
M. Hahmann et al.
Fig. 11.9 Several aggregation results
result is not optimal and requires adjustments, a user needs to: (1) modify
parameters/algorithms of CE, (2) recreate CE and the consensus solution, and (3)
evaluate OC again until the desired adjustments occur.
For our approach, we use the same setup as before but change the algorithm
to FCM [3], a soft clustering version of k-means that produces the necessary
soft cluster assignments. Concerning the handling of a?, we have to regard two
alternatives, since we cannot determine if undecidable pairs are in the same cluster
or not. Therefore, we deﬁne two strategies: one mapping a? to a+ and another one
that maps it to a-.
3.1.4
Implementing Feedback
By modifying t and the a?-handling, we are able to adjust the consensus-clustering
without making changes to CE remains untouched. To implement our proposed
merge feedback-operation, we choose a? ! a+ as handling strategy and increase
t. With t D 0:1, the result shown in Fig. 11.9a is obtained, where the two clusters
in the lower right have been fused. This fusion happens due to the points along
the border between both former clusters, that have nearly equal afﬁliations—
balanced assignment—to both clusters which leads to pairwise-similarities with low
signiﬁcance. Therefore, a? starts to occur in this area near the border when t D 0:1
is applied. The mapping of a? to a+ then leads to both clusters being connected. If t
is increased further, more clusters are connected leading to a uniﬁcation of all data
points at t D 0:4. If this merge strategy is applied to the whole dataset, it can be
delicate as one single pair classiﬁed as a+ is enough to merge whole clusters, that
could otherwise be very dissimilar.
The second of our feedback operations split is implemented in a similar way, by
using a? ! a-, which yields the result shown in Fig. 11.8 at t D 0. In order to split
up the clusters of this result, we set t D 0:8 and employ the mentioned handling

11
Large-Scale Data Analytics Using Ensemble Clustering
305
strategy for undecidable pairs, resulting in the consensus clustering depicted in
Fig. 11.9b. We observe that the upper clusters are each split in two small cluster,
just as it was intended. In addition, a sizable number of objects that are not
assigned to any cluster—denoted by white circles—has emerged. For these objects,
our algorithm was unable to determine a clear cluster assignment, because their
pairwise-similarity is a?, which is mapped to a- in all of CE. This actually means,
that each object forms a singleton cluster for itself, because no afﬁliations to other
objects or existing clusters can be determined. For convenience these singletons are
put into a larger cluster that we interpret as noise. The emergence of such noise is
a novel phenomenon that cannot occur in existing consensus procedures and grows
as t increases.
Until now we have only described how two of our feedback operations are
implemented in our ensemble clustering approach. Furthermore we only applied
them to the clustering as a whole, which can be problematic as merging and
splitting are mutually exclusive. To ﬁx this issue we need a ﬁner granularity for
the application of our feedback than the whole clustering. This effectively means
that our feedback operations are applied on the cluster-level. For this only minor
adjustments to our algorithm must be made. Let us assume the consensus clustering
shown in Fig. 11.8 again. To execute e.g. a split for cluster c3, the respective handling
strategy is chosen in the algorithm and a value for t is supplied. In order to keep the
adjustments local, our algorithm now just executes the consensus procedure for the
object pairs of the selected cluster c3. Merge operation are processed accordingly.
This allows it, to apply the opposing feedback operation split and merge together in
the same clustering.
To complete the implementation of our feedback operations, we also need to
regard reﬁne and restructure that are also executed on the cluster-level and can both
be regard as special variants of the split. With reﬁne a user can explicitly put objects
of a cluster, that are regarded as outliers into the noise cluster. Thus, these objects are
split apart from the cluster but instead of forming new clusters they are effectively
removed from the clustering.
The last feedback operation restructure addresses an issue, we can illustrate with
cluster c1 in Fig. 11.9b. We observe, that it was not accurately split, but reduced in
size due to a lot of its object becoming noise. This event results from the cluster
assignments in CE, where the small rectangular and the bigger ‘L’-shaped group
of objects, that make up c1 are always identiﬁed as just one cluster, which allows
no ﬂexibility for this area of the dataset. With restructure, it is possible to take an
existing cluster like c1 and create a new clustering-ensemble for this part of the data.
For our example this would mean, that the objects of c1 are clustered again, which
will most likely allow better adjustments for this area.
In this subsection we have proposed an algorithmic platform, that is able to
implement our introduced feedback operations and allows easy result adjustments
by the user. We have also stated that our feedback operations can be most effectively
applied on the cluster-level. To complete our proposed clustering process it is now
necessary to develop a user-interface that allows the evaluation and interpretation of
clusters, and furthermore the identiﬁcation and application of appropriate feedback.

306
M. Hahmann et al.
3.2
Visual-Interactive Interface
For the application of feedback operations it is essential to decide whether a
cluster needs adjustments or not and, if the former case is given, what adjustments
are appropriate. This decision is made by the user and takes place during the
interpretation of the clustering result. Therefore, to enable the user to make decisions
in the ﬁrst place, it is necessary to provide an instrument for the interpretation
of clustering results. In this subsection we introduce a visualization approach,
that fulﬁlls this role in our clustering process, by describing its input, its single
views, the information visualized, and its interpretation. The input consists of the
consensus clustering provided by our algorithm [20] that was described in the
previous subsection and provides information about cluster centroids and sizes,
soft cluster assignments, and signiﬁcance scores for object pairs. Based on this
input, additional information is computed for certain views, which will be explained
during the description of the respective view.
As mentioned, our goal is to optimize clusters in reference to the algorithm-
speciﬁed ﬁt of clustering and dataset. Therefore, our visualization must express how
satisfying a certain cluster is, from the point of view of the employed clustering
algorithm(-class) [19]. Despite the mandatory adaption to certain algorithm-classes,
we believe that a general template can be derived from the abstract core objective of
clustering. This objective is the partitioning of data into clusters, so that each cluster
has high internal similarity, while all clusters can be clearly separated from each
other. Although different algorithms implement these two conditions in different
ways, they can be found in almost every method. Therefore, our visualization
communicates information about the composition of clusters and the relations
between them. As we employ fuzzy c-means for ensemble generation, we construct
our template with reference to this algorithm.
On the basis of Shneiderman’s mantra, ‘overview ﬁrst, zoom and ﬁlter, then
details-on-demand’ [30], our visualization features three views: overview, cluster
composition and relations, and the attribute view. With this, we want to enable the
user to determine the clusters that need no adjustment and to decide which ones
should be merged or split, with the goal to improve the quality of the result. For
illustration, we use the consensus clustering depicted in Fig. 11.8 as an example
again. In all following ﬁgures, clusters are identiﬁed via color.
3.2.1
Overview
The overview is the ﬁrst view presented to the user and depicted in Fig. 11.10. This
view is completely result-driven, i.e. only characteristics of the clustering aggregate
are shown. The dominant circle represents the clusters of the aggregate, whereas
each circle segment corresponds to a cluster whose percental size correlates with
the segment’s size. The radar-like gauge located on the left shows the distances
between the prototypes (centroids) of all clusters. The mapping between centroids

11
Large-Scale Data Analytics Using Ensemble Clustering
307
Fig. 11.10 Overview showing clusters and inter-cluster distances
in the radar and circle segment is done via color. The radar shows a distance graph,
where vertices represent centroids, and edges—invisible in our visualization—
represent the Euclidean distance between centroids in the full dimensional data
space. Therefore, the radar is applicable for high-dimensional data. Since all our
views are basically result-driven, we can also handle high-volume datasets without
problems. The overview provides the user with a visual summary of the clustering
result, allowing a ﬁrst evaluation of the number of clusters and relations between
clusters expressed by distance and size.
3.2.2
Cluster Composition and Relations
If the user identiﬁes clusters of interest in the overview e.g. two very close clusters
like the red (c3) and blue (c4) ones in Fig. 11.8, they can be selected individually
to get more information about them, thus performing ‘zoom and ﬁlter’. Cluster
selection is done by rotation of the main circle. As soon as a cluster is selected,
the composition and relations (c&r) view depicted in Fig. 11.11 (for cluster c4) is
displayed. The selected cluster’s composition is shown by the row of histograms
on the right. All histograms feature the interval Œ0; 1 with ten bins of equal width.
From the left to the right, they show the distribution of: (1) soft assignment values,
(2) signiﬁcance scores for all object-centroid pairs, and (3) signiﬁcance scores for
all object-object pairs in the selected cluster. For details concerning these scores,
refer to [20] or the previous subsection. Certain histogram signatures indicate certain
cluster states e.g. a stable and compact cluster is given if all three histograms show
a unimodal distribution with the mode—ideally containing all objects—situated in
the right-most (highest signiﬁcance) bin.
Let us regard the signature of the example depicted in Fig. 11.11. The histograms
show that many of the object-centroid and pairwise assignments are not very
strong. This indicates that there are other clusters (c3 in the example) that strongly
inﬂuence the selected cluster objects, which leaves the chance that these clusters
could be merged. To support such assumptions, the relations between clusters

308
M. Hahmann et al.
Fig. 11.11 C&R view showing composition and relations for cluster c4
have to be analyzed. For this, the two ‘pie-chart’ gauges and arcs inside the main
circle are used. The smaller gauge shows the degree of ‘self-assignment’ of the
selected cluster, while the other one displays the degree of ‘shared assignment’
and its distribution among the remaining clusters. These degrees are calculated as
follows: each fuzzy object assignment is a vector with a sum of 1, consisting of
components ranged between 0 and 1, indicating the relative degree of assignment
to a certain cluster, i.e. each vector-dimension corresponds to a cluster. The degree
of self-assignment is calculated by summing up all components in the dimension
corresponding to the selected cluster. This sum is then normalized and multiplied
with 100 to get a percental score. The shared assignment is generated in the same
fashion for each remaining cluster/dimension. The target and strength of relations
between the selected cluster and others is described by the color and size of the
shared-assignment slices. For easy identiﬁcation, the displayed arcs show these
cluster-to-cluster relations by connecting clusters, where the stroke width shows
the strength of the relation.
If a cluster is not inﬂuenced by others, it shows a very high degree of self-
assignment with no outstanding relations to other clusters. In contrast, the example
in Fig. 11.11 shows that the selected cluster has a noticeable relation to the cluster c3.
This supports the merge assumption and furthermore indicates which other cluster
should be part of a possible merge. To get additional information, the inter-cluster
distances can be analyzed. For this, the user can employ the ‘radar’, showing that
both clusters in our example are relatively close to each other (the selected cluster
is encircled), or switch on additional distance indicators (‘details-on-demand’), as
shown in Fig. 11.12. These display the ratio of centroid-to-centroid distances—
like the radar—and minimum object-to-object distances between the selected and
the remaining clusters. If this ratio approaches 1, the respective clusters are well
separated and the colored bars are distant. In our example, this is the case for all
clusters except for the blue one, where both bars nearly touch each other, showing

11
Large-Scale Data Analytics Using Ensemble Clustering
309
Fig. 11.12 C&R view with activated distance indicators
that the minimal object distance between the clusters c3; c4 is much smaller than
the centroid distance. With this, the user can now savely state that the both clusters
should be merged. To double-check, cluster c3 can be selected and should show
similar relations to c3.
With the c&r view, it is also possible to evaluate whether or not a cluster should
be split. Candidates for a split show the following: In all three histograms, the mode
of the distribution is located in one of the medium-signiﬁcance bins. Additionally,
they feature a reduced degree of self-assignment, but in contrast to the merge case,
they have equally strong relations to the remaining clusters and are well separated in
terms of the radar and distance indicators. Unfortunately, these characteristics are no
clear indication for a split e.g. non-spherical clusters can exhibit the same properties.
To gain more certainty in decisions for split candidates, the attribute view has been
developed.
3.2.3
Attribute View
When we look at attributes in terms of clustering, we can state the following: If
an attribute has a uniform or unimodal distribution (in the following ˚), it is not
useful for clustering because the objects of the dataset cannot be clearly separated
in this dimension. In contrast, bi- or multi-modal distributions are desired, since
they can be used for object separation. When we look at attributes on the cluster
level, this is inverted. Regarding a cluster, it is desirable that all of its attributes
have unimodal distributions, since this shows high intra-cluster homogeneity. A
multimodal-distributed attribute would imply that the cluster could be further
separated in this dimension. Generally, we desire the following: On the dataset level,
attributes should be dissimilar to ˚, while on the cluster level, they should resemble
it as closely as possible. These are the basics for our attribute view.

310
M. Hahmann et al.
Fig. 11.13 Attribute view indicating a split in dimension 2(y) for c5
To calculate the similarity to ˚, we use a straightforward approach. We generate
histograms, on the dataset and cluster level, for each attribute. From the histogram
bins, those that are local maxima are selected. From each maximum, we iterate
over the neighboring bins. If a neighboring bin contains a smaller or equal number
of objects, it is counted and the next bin is examined; otherwise, the examination
stops. With this, we can determine the maximum number of objects and bins of
this attribute that can be ﬁtted under ˚. This is the value we display in the attribute
view. In Fig. 11.13, the attribute view is depicted for the cluster c5 from our example.
There are two hemispheres and a band of numbers between them. The band shows
the attributes of the dataset, ordered by our computed values, and is used to select
an attribute for examination (selection has a darker color). The small hemisphere on
the right shows the global behavior of attributes. Each curve represents an attribute,
while for the selected attribute, the area under its curve is colored. The hemisphere
itself consists of two 90ı scales, the upper for the percentage of objects and the
lower for the percentage of bins that can be ﬁtted under ˚. The start and end point
of each curve show the values for the attribute on these scales. If all objects and bins
ﬁt under ˚, a vertical line is drawn and there is no color in the hemisphere. All this
also applies to the left hemisphere showing the attribute in the selected cluster. For
our example in Fig. 11.13, we selected attribute 1.
We can see a large colored area, showing that more than 50% of the objects and
bins do not ﬁt under ˚. If, in addition, the selected cluster shows split characteristics
in the c&r view, the user may assume that this cluster should be split. The beneﬁt of
this view lies in the fast and easy interpretability. More color in the left hemisphere
indicates a higher split possibility, while the amount of color in the right hemisphere
acts as a measure of conﬁdence for the left. In terms of Shneiderman’s mantra, this
view can either be considered as ‘details-on-demand’ or as an ‘overview’ and ‘zoom
and ﬁlter’ for the attribute space.

11
Large-Scale Data Analytics Using Ensemble Clustering
311
4
An Infrastructure for Feedback-Driven Clustering
In the previous section, we have presented our novel feedback-driven clustering
process. This feedback-driven process enables users to efﬁciently analysis large-
scale data, whereas this process only considers the analytical perspective. However,
in order to efﬁciently execute those feedback-driven processes, a large-scale infras-
tructure is also required. Based on our previous work [14,28], we are now presenting
our current infrastructure approach which has its foundation in the service-oriented
architecture [9]. The service-oriented architecture is a widely accepted and engaged
paradigm for the realization of business processes incorporating several distributed,
loosely coupled partners. Today, Web services and the Business Process Execution
Language for Web Services (BPEL4WS, BPEL for short) [9,34] are the established
technologies to implement such a service-oriented architecture [9, 34]. The func-
tionality provided by business applications is enclosed within Web service software
components. Those Web services can be invoked by application programs or by
other Web services via Internet without explicitly binding them. On top of that,
BPEL has been established as the de-facto standard for implementing business
processes based on Web services [9,34].
Generally, our proposed feedback-driven process is not a regular business
process, but a data-intensive process, whose efﬁcient service-oriented execution is
a challenging task. We deﬁne the notion of a data-intensive process as collection
of related structured activities—like regular business processes—, where huge data
sets have to be exchanged between several loosely coupled services. Therefore, we
see data-intensive processes as a subclass of business processes with a special data
ﬂow property. The implementation of such data-intensive processes in a service-
oriented environment offers some advantages, but the efﬁcient realization of data
ﬂows is difﬁcult. In this case, the exchange of massive data is challenging and
several papers have shown that the preferred XML-based SOAP protocol [34] for
the communication between Web services is not efﬁcient enough in those scenario
settings [4, 8, 15, 25]. Therefore, we developed a novel SOA-aware approach with
a special focus on the data ﬂow, whereas our core concept to optimized the data
ﬂows is based on data clouds. The tight interaction of new cloud technologies with
SOA technologies enables us to optimize the execution of data-intensive service
applications by reducing the data exchange tasks to a minimum.
4.1
Preliminaries
To tackle this data exchange issue in service-oriented environments on a conceptual
level, we have already proposed speciﬁc extensions on the Web service [15] as well
as the BPEL levels [16] for data-intensive service applications.

312
M. Hahmann et al.
Fig. 11.14 Operation method of Data-Grey-Box Web services
4.1.1
Service-Level Extension
The concept of Data-Grey-Box Web Services (DGB-WS) is a speciﬁc extension
of the Web service technology for data-intensive service applications [15]. Each
DGB-WS exhibits its own storage system—e.g. a database system as illustrated in
Fig. 11.14—that can be used to efﬁciently process large amounts of data within the
service. However, in contrast to the original black-box Web service approach [34],
the Web service interface of the Data-Grey-Box Web services is enhanced with an
explicit data aspect offering exhaustive information about the data semantics. Aside
from the separation of functional parameters and data in the interface description,
a novel binding format for structured data was introduced. Through this new data
binding, services signal that data has not been transferred via SOAP and that there
is a separate data layer instead. As before, regular functional parameters are handed
over via SOAP when calling the Web service.
To handle this newly introduced data binding, the SOAP framework was
extended with the integration of a novel data layer component. On the client side,
enhanced Web service call semantics are necessary. Besides the transmission of the
endpoint information and regular parameters in the SOAP message, the client has
to deliver access information as references for (1) where the input data is available
(input reference) and (2) where the output data should be stored (output reference).
Thus, the new data binding is translated into no more than two additional parameters
with access information for input and output data on the client side. These new
parameters are included in the SOAP message for the invocation of Web services.
The advantage of this procedure is that instead of propagating the pure data in an
XML-marshaled SOAP message, only the access information in the form of data
pointers are delivered in SOAP. This property is depicted in Fig. 11.14, where the
client delivers data pointers to its storage system to the service.

11
Large-Scale Data Analytics Using Ensemble Clustering
313
On the service side, the extended SOAP framework receives the SOAP message
and conducts a separation into the functional aspect and the data aspect. The
associated data layer calls an appropriate mediator for the data propagation based
on the access information of the client and the service. While the client’s data
access information can be found in the received SOAP message, the data access
information for the service instance must be queried from the extended service
infrastructure [15]. Fundamentally, a mediator is a neutral third party member
that is responsible for the data propagation between client and Web service.
Such mediators have to be accessible via a standard Web service interface. Using
this mediator approach, the heterogeneity of the storage system is tackled on a
conceptual level.
If a DGB-WS receives and returns a data set as illustrated in Fig. 11.14, two
data propagation tasks via mediators at the service side will be initiated. The
ﬁrst propagation task for the input data is conducted before the pure functionality
of the service is invoked. The correlation between such input data and the Web
service instance is realized by our extended service infrastructure. If the input data
propagation task is ﬁnished, the functionality is automatically invoked. The last step
is the initiation of the data propagation task to deliver the output data to the client.
Fundamentally, this proposed concept of Data-Grey-Box Web services offers
several drawbacks. One drawback is that the client has to deliver access information
to its internal storage system. Another drawback is the restricted usability of
Data-Grey-Box Web services, which is linked to the availability of appropriate
mediator services to propagate data between the participating storage systems. If no
appropriate mediator service is available, either the client cannot invoke the service
or the client has to change its own storage system. The data exchange will also be
initiated when the storage systems of client and service are equal.
4.1.2
Process-Level Extension
In order to efﬁciently realize comprehensive data-intensive service applications, the
next step is the orchestration of Data-Grey-Box Web services. Therefore, BPEL data
transitions, as a data-aware extension of BPEL, have been proposed in [16]. These
data transitions are explicit link types connecting several Data-Grey-Box services
on the data level. Fundamentally, those data transitions are an orthogonal data ﬂow
concept compared to the control ﬂow.
In the left part of Fig. 11.15, a simple process consisting of two service
invocations (DGB-WS), W S1 and W S2, with a user interaction between these
invocations, is illustrated, where the user’s input is necessary to call service W S2.
Furthermore, both services, W S1 and W S2, are also explicitly connected on the
data level with a data transition (illustrated by a solid line). The meaning of the data
transition is that the output data of W S1 is used as input data for W S2. Moreover,
the data transition includes a schema transformation speciﬁcation to construct the
input data for W S2 according to the necessary schema from the output data of
W S1 (illustrated by a circle containing the character T ). As a consequence, data

314
M. Hahmann et al.
Fig. 11.15 BPELDT —Modeling and execution perspective
transitions include a speciﬁcation of how the output data of a source service has to
be transformed to the input data schema of the target service.
Aside from the process deﬁnition, Fig. 11.15 illustrates the process execution
on an abstract level as well (using dashed lines). An adapted BPEL engine also
has an explicit storage system as a temporary storage location. As depicted, the
output data of W S1 is stored at this position. Then, the schema transformation
of the data transition is executed (T-Service) with the help of the temporary
storage system. Afterwards, Web service W S2 gets its input data from this storage
system. In summa, three explicit data propagation (by value) tasks using specialized
propagation tools are conducted during the process execution. In [16], we have
shown that such an execution strategy performs better than the traditional SOAP-
based approach.
4.1.3
Discussion
Using these extended technologies, the exchange of large data sets between partic-
ipating services in a process is conducted with speciﬁc data-oriented approaches,
such as ETL tools for the data exchange between databases [31]. The incorporation
of such speciﬁc data propagation tools is done in a transparent way using service
technologies during the process execution. However, the main disadvantages are:
1. The speciﬁc data propagation/exchange services have to be available at process
execution time. If this demand is not met, the process execution is not possible;
this clearly restricts the practicability and applicability of our approach. There-
fore, the overall approach depends on the availability of such specialized data
propagation services.
2. The performance of the data exchange is dramatically improved by this approach,
as proven in [16]. However, the data exchange is still a time-consuming activity,
which should not be underestimated. Therefore, further improvements have to be
done to optimize the overall performance of data-intensive service applications.

11
Large-Scale Data Analytics Using Ensemble Clustering
315
Up to now, these extensions perfectly ﬁts the main realistic assumption: each
loosely coupled service has its own storage system, e.g. a database system. This
storage system is utilized to efﬁciently process large amounts of data within
the service. However, based on work in the direction of cloud technologies, the
assumption can be reﬁned in a novel way. Fundamentally, the cloud is a metaphor
for the Internet, and it is an abstraction for the complex infrastructure it conceals.
Therefore, a data cloud is a widely available abstract data layer offering scalable,
reliable, cheap and speedy access to data over the Internet. Examples of a data cloud
are Amazon SimpleDB [29] or HadoopDB [1]. According to this, we are able to
deﬁned our new main assumption as follows: Each loosely coupled service uses the
same data cloud as common, shared storage and data processing system.
Using our so-called data cloud assumption, we are able to simplify the execution
of data-intensive service applications in two ways: (1) speciﬁc data propagation
services do not have to be available at process execution time, and (2) the data
exchange between participating services in a process is completely replaced by the
exchange of data references as pointers. Based on this issue, the overhead of the
data propagation—by value exchange—is reduced to a minimum. Fundamentally,
we only want to propagate data in those situations where we cannot avoid it at all.
In order to reach the desired properties, we subsequently introduce the concept of
Data Cloud Web Services as an extension of [15] and present all methods for the
process modeling and execution level.
4.2
Data Cloud Web Services
In order to be able to efﬁciently execute our feedback-driven clustering process, we
ﬁrst introduce our concept of Data Cloud Web services (DC services). Our concept is
a specialized variant of Data-Grey-Box Web services, but we start with assumption,
where every service—e.g. services for k-means, DBSCAN, clustering aggregation,
etc.—works in combination with a commonly shared data cloud as storage system
for the efﬁcient processing of massive data. In this scenario, we see a data cloud as
a widely open, scalable and accessible data layer with a well-deﬁned interface for
efﬁcient data access. In contrast to the concept of Data-Grey-Box Web services, this
is a main shift that allows more ﬂexibility, as we will describe later.
Based on the foundation of Data-Grey-Box Web services, DC services have
the same enhanced interface description with a separation of functional and data
aspects. In this case, the already introduced new data binding format signals that
data has not been transferred via SOAP and that there is a separate data layer
instead [15]. According to the use of the data binding format by Data-Grey-Box Web
services, Fig. 11.16 illustrates the adjusted mapping for DC services. Again, on the
client side, enhanced service invocation semantics are essential. In such invocation
semantics, the endpoint information, regular parameters, and data references are
delivered in a SOAP message to the DC service. As depicted in Fig. 11.16, data
references are pointers into the data cloud for (1) where the input data is available

316
M. Hahmann et al.
Fig. 11.16 Operation method of data cloud web services
and (2) where the output data should be stored. As we can see, there are no
differences in the invocation semantics between Data Cloud Web services and Data-
Grey-Box Web services on the client side.
However, in contrast to Data-Grey-Box Web services, the internal operation
method on the service side is completely different. This difference is clearly
noticeable when we compare Figs. 11.14 and 11.16. Using our main assumption
of a commonly shared data cloud for all DC services, we do not have to propagate
data from the client storage system to the service storage system. Therefore, we
are able to remove the invocation of appropriate mediators from the internal service
processing procedure, and the delivered data references from the client are directly
passed to functional aspects of the service as main data pointers. These main
working data pointers are used to read the input data (READ operation) and to store
the output data at the corresponding location in the data cloud (WRITE operation).
As already illustrated in Fig. 11.16, the interactions between DC services and the
data cloud are restricted to the operations READ and WRITE, and the delivered
data references of the client are used to instantiate these operations in an appropriate
way. At the moment, these operations are abstract and should allow efﬁcient access
to data in the cloud. Thus, we prefer an SQL-like interface as offered by Amazon
SimpleDB [29] or HadoopDB [1] at the moment. Aside from efﬁcient descriptive
access to data, such an interface offers further data processing capabilities like
grouping or aggregation of data as well. However, this introduce some limitations,
which have to addressed in further research activities.
In contrast to the concept of Data-Grey-Box Web services, the internal service
processing procedure is simpliﬁed. The main advantage of our DC service concept
is that the data propagation itself is completely eliminated by the use of a commonly
shared data cloud. Instead of exchanging data in a by-value manner—either using

11
Large-Scale Data Analytics Using Ensemble Clustering
317
SOAP messages or with specialized mediators—, DC services exchange data in a
pure by-reference manner, and the internal service procedure guarantees the correct
data processing. For data-intensive service applications, this makes a lot of sense and
strongly reduces the execution time of services. Another advantage of DC services
is the clear deﬁnition of the requirements for the invocation. If a client wants to
invoke a DC service, the client has to ensure that (1) the input data is available in
the data cloud and/or (2) a storage position for output data is available in the data
cloud. In comparison to Data-Grey-Box Web services, these requirements are more
strict and the degrees of freedom are fewer.
To summarize, Data Cloud Web services are a specialized variant of Data-
Grey-Box Web services. The tight coupling with a data cloud is an outstanding
property and enables the elimination of all data propagation tasks within the service
invocation. In this case, the service invocation corresponds to a pure functional
invocation without any data delivery. If the essential input data is available in
the data cloud, DC services are able to access this data directly within the data
cloud. Furthermore, the output data of DC services is also put in the data cloud.
In the following section, we show how to efﬁciently exploit these aspects to
process data-intensive service applications without the execution of any explicit data
propagation task.
4.3
Process Perspective
In order to create comprehensive data-intensive service applications, e.g. our
feedback-driven clustering process using several DC services, we employ the
BPELDT approach [16]. From the modeling perspective, we do not have to
modify anything in the already proposed approach. The left part of Fig. 11.17
illustrates a subprocess of our feedback-driven clustering process—analyzing data
with traditional clustering algorithms—consisting of the following steps:
1. The ﬁrst DC service provideData—after starting the subprocess—provides the
data to be analyzed by the subsequent services.
2. The second DC service normalizeData conducts a normalization of data. The
ﬁrst and the second DC services are connected by a control link as well as by a
data link. The data link—BPEL data transition—signals a data exchange between
these services. This data link also includes a speciﬁcation of how the output data
of the DC service provideData has to be transformed to the input data schema of
the DC service normalizeData.
3. In the third step, we set essential clustering parameters for the subsequent step.
4. The fourth step invokes a DC service analyseData, which is responsible for
determining the clustering result of the normalized data. Therefore, the DC
service normalizeData and this DC service are connected by a data link including
a schema mapping speciﬁcation. Afterwards, the subprocess is ﬁnished.

318
M. Hahmann et al.
Fig. 11.17 Data-cloud-aware process execution
4.3.1
Process Execution
Generally, we assume that the relevant data to be analyzed is already available
in the data cloud and is made accessible by the provideData DC service. The
sample subprocess execution is depicted in detail in the right part of Fig. 11.17.
As in the original BPELDT approach [16], the process engine is responsible for
the data reference handling during process execution. Thus, the process engine is
tightly coupled to the same data cloud as the participating DC services. The process
execution can be described as follows:
•
Based on the functionality of the DC service provideData, the data is copied
to an engine-determined location outRef 1 in the data cloud. Those reference
information is delivered to the DC service during the service invocation. If the
DC service has ﬁnished (synchronous invocation), the next process tasks can be
executed.
•
The engine-determined output data reference of the DC service provideData
is then used as input data reference for the subsequent DC service normal-
izeData (outRef1 equals inRef 2). The DC service normalizeData reads the
corresponding data from this location within the data cloud and writes the
normalized data to an engine-determined location outRef 2 using our deﬁned
READ/WRITE interface.
•
Afterwards, the user interaction is processed and the DC service analyseData will
be invoked. During this service invocation, the delivered output data reference
outRef 2 of normalizeData is used as input data reference inRef 3. This DC
service reads and writes the data according to the engine-determined data cloud
locations.

11
Large-Scale Data Analytics Using Ensemble Clustering
319
To determine the data reference equalities of input and output data for several
DC services, the explicitly available data transitions within the process deﬁnition
are highly practicable and usable in this case. As we can observe by this example,
the process execution is conducted without any explicit data propagation between
participating DC services. Instead of propagating data between heterogeneous
storage systems, as proposed in [15, 16], we efﬁciently employ the developed
techniques of data clouds and propagate only data cloud reference information
between services. Each DC service then operates directly on the delivered data
reference locations. Up to now, the described process execution has been simpliﬁed
by removing the execution of possible data transformation tasks speciﬁed within the
data transitions. However, we are able to include such tasks easily. One possible way
is to invoke a special transformation DC service at any time to execute the delivered
transformation. From our point of view, such a functionality should be offered in a
data cloud approach.
4.4
Discussion
With our proposed concept of DC services including the adjusted BPELDT process
execution, the data ﬂows within data-intensive service applications are optimized.
Instead of propagating massive data sets between heterogeneous systems, we
suggest handling all data with a data cloud like Amazon SimpleDB [29], without
losing the property of distributed services. Therefore, we are able to create data-
intensive process with the property that we do not need to exchange massive data
between the participating services. This results in a hugh performance beneﬁt in
contrast to existing service-oriented approaches.
However, the tight coupling of all services with one commonly shared data
cloud comes with some drawbacks. In [14], we already have described several
ideas how to cope with several heterogeneous data clouds. The handling these
heterogeneous data clouds is one research topic of us. A further major drawback
of our current approach is our limited READ-WRITE interface to access data
clouds. Today, several developed data processing standards on top of specialized
data clouds exist. The most common ones are Map/Reduce [6] and PigLatin [26]
with several assets and drawbacks. In further research work, we are going to enhance
our access interface with those standards to optimize the data processing within
DC-services. Generally, this are several steps to efﬁciently utilize the full power
of cloud technologies in combination with SOA technologies. From our point of
view, the combination is right way for data-intensive processes from perspectives of
implementation and execution.

320
M. Hahmann et al.
5
Summary
In this chapter, we have discussed challenges arising from different aspects of large-
scale data clustering. In order to tackle almost all those aspects, we have presented
our novel feedback-driven clustering process as well as a new infrastructure. Unlike
existing clustering approaches, the end user is the central point of our process and
he/she has to execute a number of iterations to determine a satisfying result with
stable clusters. To alleviate the traversal through iterations for end users, we use the
well-known idea of relevance feedback. Our feedback is speciﬁed in a user-friendly
way and contains effect-oriented operations like split or merge. To enable the
selection of an appropriate operation foreach cluster, we introduced an information
visual interface, that communicates several statistics and measures. Aside from
these theoretical clustering concepts, our proposed infrastructure is able to handle
increasing data volumes due to the tight coupling of service-oriented concepts and
cloud technologies.
References
1. Abouzeid, A., Bajda-Pawlikowski, K., Abadi, D.J., Rasin, A., Silberschatz, A.: Hadoopdb: An
architectural hybrid of mapreduce and dbms technologies for analytical workloads. PVLDB
2(1), 922–933 (2009)
2. Balcan, M.F., Blum, A.: Clustering with interactive feedback. In: Proc. of ALT, pp. 316–328
(2008)
3. Bezdek, J.C.: Pattern Recognition with Fuzzy Objective Function Algorithms.
New York:
Plenum (1981)
4. Chiu, K., Govindaraju, M., Bramley, R.: Investigating the limits of soap performance for
scientiﬁc computing. In: Proceedings of the 11th IEEE International Symposium on High
Performance Distributed Computing, pp. 246–254 (2002)
5. Davies, D.L., Bouldin, D.W.: A cluster separation measure. IEEE Transactions on Pattern
Analysis and Machine Intelligence PAMI-1, 224–227 (1979)
6. Dean, J., Ghemawat, S.: Mapreduce: Simpliﬁed data processing on large clusters. In: OSDI,
pp. 137–150 (2004)
7. Dunn, J.C.: A fuzzy relative of the isodata process and its use in detecting compact well-
separated clusters. Cybernetics and Systems (1974)
8. van Engelen, R.: Pushing the soap envelope with web services for scientiﬁc computing. In:
Proceedings of the International Conference on Web Services, pp. 346–352 (2003)
9. Erl, T.: Service-Oriented Architecture (SOA): Concepts, Technology, and Design. Prentice
Hall PTR (2005)
10. Ester, M., Kriegel, H.P., Sander, J., Xu, X.: A density-based algorithm for discovering clusters
in large spatial databases with noise. In: Proc. of KDD (1996)
11. Forgy, E.W.: Cluster analysis of multivariate data: Efﬁciency versus interpretability of classiﬁ-
cation. Biometrics 21 (1965)
12. Gionis, A., Mannila, H., Tsaparas, P.: Clustering aggregation. In: Proc. of ICDE (2005)
13. Gionis, A., Mannila, H., Tsaparas, P.: Clustering aggregation. TKDD 1(1) (2007)
14. Habich, D., Lehner, W., Richly, S., Assmann, U.: Using cloud technologies to optimize data-
intensive service applications. In: Proceedings of the 2010 IEEE 3rd International Conference
on Cloud Computing, pp. 19–26 (2010)

11
Large-Scale Data Analytics Using Ensemble Clustering
321
15. Habich, D., Preißler, S., Lehner, W., Richly, S., Aßmann, U., Grasselt, M., Maier, A.: Data-
grey-box web services in data-centric environments.
In: Proceedings of the 2007 IEEE
International Conference on Web Services, pp. 976–983 (2007)
16. Habich, D., Richly, S., Grasselt, M., Preißler, S., Lehner, W., Maier, A.: BpelDT - data-aware
extension of bpel to support data-intensive service applications. In: Proceedings of the 2nd
ECOWS07 Workshop on Emerging Web Services Technology, pp. 111–128 (2007)
17. Habich, D., W¨achter, T., Lehner, W., Pilarsky, C.: Two-phase clustering strategy for gene
expression data sets. In: Proceedings of the 2006 ACM Symposium on Applied Computing,
pp. 145–150 (2006)
18. Hahmann, M., Habich, D., Lehner, W.: Evolving ensemble-clustering to a feedback-driven
process. In: Proceedings of the IEEE ICDM Workshop on Visual Analytics and Knowledge
Discovery (VAKD) (2010)
19. Hahmann, M., Habich, D., Lehner, W.: Visual decision support for ensemble-clustering.
In: Proceedings of the 22nd International Conference on Scientiﬁc and Statistical Database
Management (SSDBM) (2010). (to appear)
20. Hahmann, M., Volk, P., Rosenthal, F., Habich, D., Lehner, W.: How to control clustering
results? ﬂexible clustering aggregation.
In: Advances in Intelligent Data Analysis VIII,
pp. 59–70 (2009)
21. Jain, A.K., Murty, M.N., Flynn, P.J.: Data clustering: a review. ACM Comput. Surv. 31(3)
(1999)
22. Karypis, G., Kumar, V.: A fast and high quality multilevel scheme for partitioning irregular
graphs. SIAM Journal on Scientiﬁc Computing 20(1) (1998)
23. Kaufman, L., Rousseeuw, P.: Finding Groups in Data An Introduction to Cluster Analysis.
Wiley Interscience (1990)
24. Lloyd, S.P.: Least squares quantization in pcm. IEEE Transactions on Information Theory 28,
129–137 (1982)
25. Ng, A.: Optimising web services performance with table driven xml. In: Proceedings of the
17th Australian Software Engineering Conference (2006)
26. Olston, C., Reed, B., Srivastava, U., Kumar, R., Tomkins, A.: Pig latin: a not-so-foreign
language for data processing. In: Proceedings of the ACM SIGMOD International Conference
on Management of Data, pp. 1099–1110 (2008)
27. Rand, W.: Objective criteria for the evaluation of clustering methods. Journal of the American
Statistical Association 66(336), 846–850 (1971)
28. Richly, S., Habich, D., Thiele, M., Goetz, S., Hartung, S.: Supporting gene expression analysis
processes by a service-oriented platform.
In: Proceedings of the 2007 IEEE International
Conference on Services Computing, pp. 739–746 (2007)
29. Services, A.W.: Amazon SimpleDB. http://aws.amazon.com/simpledb/ (2009)
30. Shneiderman, B.: The eyes have it: A task by data type taxonomy for information visualiza-
tions. In: VL ’96: Proceedings of the 1996 IEEE Symposium on Visual Languages, p. 336.
IEEE Computer Society, Washington, DC, USA (1996)
31. Simitsis, A.: Modeling and managing etl processes. In: roceedings of the VLDB 2003 PhD
Workshop. Co-located with the 29th International Conference on Very Large Data Bases (2003)
32. Steinhaus, H.: Sur la division des corp materiels en parties. Bull. Acad. Polon. Sci. C1. III
vol IV, 801–804 (1956)
33. Strehl, A., Ghosh, J.: Cluster ensembles a knowledge reuse framework for combining
partitionings. In: Proc. of AAAI (2002)
34. Weerawarana, S., Curbera, F., Leymann, F., Storey, T., Ferguson., D.F.: Web Services Platform
Architecture : SOAP, WSDL, WS-Policy, WS-Addressing, WS-BPEL, WS-Reliable Messag-
ing, and More. Prentice Hall PTR (2005)
35. Zeng, Y., Tang, J., Garcia-Frias, J., Gao, G.R.: An adaptive meta-clustering approach:
Combining the information from different clustering results. In: Proc. of CSB (2002)


Chapter 12
Speciﬁcation of Data Intensive Applications
with Data Dependency and Abstract Clocks
Abdoulaye Gamati´e
1
Introduction
Data intensive processing concerns applications operating on large sets of data
or streams of data. During the execution of such an application, a major part of
the processing consists of data read/write and data manipulation. The range of
domains concerned by data intensive processing is very wide [10]: civil and military
defense and security (e.g., radar and sonar systems), transportation (e.g., GPS and
satellite data processing, collision detection system), consumer electronics (e.g.,
audio and video processing in televisions or cameras), medical imaging processing
(e.g., computer-assisted surgery), computational science for simulating complex
physical phenomena (e.g., climate modeling, seismic waves, behaviors of biological
systems), business information processing from databases, and entertainment (e.g.,
realistic games). In a major part of the mentioned areas, the data amounts of
applications are expected to double every 2 years in the future [6].
A number of common characteristics can be observed in data intensive
applications.
•
The sets of data are represented by multidimensional data structures, typically
multidimensional arrays. The dimensions express various metrics according to
an application context: time, space, frequency, temperature, magnetic ﬁeld, etc.
The information stored in these data structures are accessed either point-wise
or block-wise. The ﬁrst case usually happens when data are read or written via
indexes of arrays, while the second case concerns accesses to monolithic subsets
of data that are themselves multidimensional data structures.
A. Gamati´e ()
LIFL/CNRS and Inria, Parc Scientiﬁque de la Haute Borne,
Park Plaza – Bˆatiment A – 40 avenue Halley, Villeneuve d’Ascq, France
e-mail: abdoulaye.gamatie@liﬂ.fr
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 12, © Springer Science+Business Media, LLC 2011
323

324
A. Gamati´e
•
The computation of output data is achieved by applying operations such as
ﬁlters in multimedia signal processing, to input data independently from the
order in which input data are taken by the operations. The set of output data
is often smaller than the set of input data. An example is an image downscaling
transformation, which is used to reduce the size of high resolution video images
into small size images for video display on screens of mobile phones.
The particularity of data intensive applications calls for well-suited design para-
digms addressing their ﬁeld-related design requirements: high degree of parallelism,
temporal performance, timing predictability for real-time constraints, reliability and
safety for critical applications. When these applications are embedded in Systems-
on-Chip (SoCs), as it is often the case in multimedia domain, further requirements
such as high system throughput and low power and energy consumption come into
play. Due to the data-parallel nature of data intensive computations, the successful
design paradigms must provide programming models that adequately deal with
concurrency, and in particular with parallelism.
According to their adopted style, existing programming languages of interest
describe the parallelism either explicitly or implicitly. In the former case, special
directives are used to characterize a parallel execution, while in the latter case, the
potential parallelism degree of an application is expressed by language constructs
and can be exploited automatically by compilers. One of the most popular explicit
parallel programming model is the message passing interface (MPI) [30]. It provides
a set of directives and library routines to express the distribution of computations
and data on processing and memory units respectively. To obtain high performance
execution of programs, very careful manual optimizations are often required
however. The implicit parallel programming model can be achieved, e.g., with the
Sisal [16] and SaC [35] languages. The repetitive structure modeling advocated in
this chapter also embraces the same principle. Besides the need of programming
models, dedicated frameworks are required, which facilitate the overall design
activity by providing designers with a well-deﬁned methodology and a tool-set.
1.1
A Modeling Paradigm for Data Intensive Applications
This chapter presents a data intensive application modeling paradigm based on
the notions of repetitive structure modeling (RSM) [19] and abstract clock spec-
iﬁcations [8, 13]. RSM proposes a rich set of constructs allowing for an implicit
description of regular parallelism inherent to computations. It is inspired by the
domain-speciﬁc array-oriented language, named Array-OL [9], in which data are
manipulated in the form of multidimensional arrays. Both data-parallelism and task-
level parallelisms are enabled by the compositional design. The potential parallelism
expressed by an RSM model can be reﬁned with abstract clock relations to reﬂect
different execution models of an application given some platform or environment

12
Speciﬁcation of Data Intensive Applications with RSM and Abstract Clocks
325
Fig. 12.1 Overview of Marte design packages: gray boxes represent the concepts used in
Gaspard2
constraints, e.g., fully parallel, pipelined or hierarchical (mixing of the previous
two). Such abstract clocks are inspired by the synchronous reactive approach [3].
The presented modeling paradigm is deﬁned in a framework, named Gaspard2
(Graphical Array Speciﬁcation for Parallel and Distributed Computing) [11,12,38],
dedicated to the design of data intensive applications on SoCs. From a practical
point of view, Gaspard2 promotes a graphical modeling via the UML Marte standard
proﬁle [33,34], used to describe applications. Marte stands for Model and Analysis
Real-Time Embedded system. It is an evolution of the UML SPT proﬁle devoted
to the modeling of time, schedulability, and performance-related aspects of real-
time systems. It also borrows a few concepts from SysML, another UML proﬁle for
system design in general.
Of course, since the scope of Marte is larger than the sole data intensive
computation modeling, Fig. 12.1 identiﬁes in gray color the subset of packages
considered for Gaspard2 from the whole Marte proﬁle. This subset comprises:
•
The GCM (Generic Component Model) package for application design. This
package contains basic concepts such as data ﬂow ports, components and
associated connectors.
•
The HRM (Hardware Resource Modeling) package for execution architecture
speciﬁcation. It specializes the concepts of GCM into hardware resource such as
memory, processor and buses.
•
The Alloc (Allocation) package for the representation of the mapping between
software applications and hardware architectures.

326
A. Gamati´e
•
The RSM (Repetitive Structure Modeling) package for the speciﬁcation of
regular parallelism in applications, architectures and allocations. Its main con-
cepts are presented in this chapter.
•
The NFP (Non Functional Properties) package for the speciﬁcation of non
functional properties such as frequencies of processors.
•
The VSL (Value Speciﬁcation Language) package for the structuring and
speciﬁcation of values.
•
The Time package for the speciﬁcation of temporal properties, e.g., clock
constraints shown in this chapter. It is often used together with its companion
clock constraint speciﬁcation language (CCSL) [27].
Beyond application and hardware architecture modeling, the Gaspard2 frame-
work also includes automatic model transformations (implementing either refactor-
ing via loop transformations, or compilation) towards various targets: simulation
at different abstraction levels with SystemC, hardware synthesis with VHDL,
formal validation with synchronouslanguages, scientiﬁc computation with OpenMP
Fortran and C. These facilities can be applied according to a methodology presented
in [11,12]. The entire design activity is supported by a seamless tool-chain deployed
within the Eclipse environment [38].
1.2
Outline
The rest of this chapter is organized as follows: Sects. 2 and 3 respectively discuss
some relevant monodimensional and multidimensional speciﬁcation models for
data intensive applications. Section 4 presents the main concepts of a repetitive
model of computation and an illustrative data-intensive algorithm modeled in Marte.
Section 5 introduces abstract clocks and their combination with repetitive models
in Marte. Section 6 reports possible design analysis based on such a modeling
paradigm. Finally, a summary is given in Sect. 7.
2
Monodimensional Data Speciﬁcation Models
Over recent decades, there have been several speciﬁcation models proposed for
describing data intensive applications. Among these models, Kahn process networks
(KPN) [22] and synchronous dataﬂow (SDF) networks [25] appear as the main-
streams. A KPN consists of a set of nodes or processes representing (sequential)
programs that are linked by communication mechanisms deﬁned as FIFO channels.
When a process attempts to read an empty channel, it is blocked, i.e., blocking read
request. On the contrary, the write requests are not blocking, i.e., FIFO channels
have inﬁnite size. KPNs hold a mathematical semantics, which favors a formal

12
Speciﬁcation of Data Intensive Applications with RSM and Abstract Clocks
327
Fig. 12.2 SDF speciﬁcation of image resizing
reasoning. One important property is that they are deterministic from the purely
functional point of view. KPNs offer a high ﬂexibility for compositional design.
The SDF model is close to the KPN model. It also consists of a set of nodes
interconnected by oriented edges denoting FIFO queues. Each node consumes (resp.
produces) data tokens from (resp. to) its incoming (outgoing) edges. The rate of
data token consumption and production is deﬁned statically and remains unchanged.
These data tokens are monodimensional. Their value has no effect on the control
ﬂow of an SDF. Figure 12.2 illustrates an SDF speciﬁcation example composed
of three nodes. The Pixel generator node produces pixels of some images to be
modiﬁed by the Resize node, and the result is used to reconstruct new images by
the Image builder node. The number of input or output data tokens for each node
is speciﬁed. For instance, the Resize node consumes 480 tokens and produces 180
tokens at the same time.
All these features of SDFs make them well-suited for static analysis of their
behavior. In particular, an SDF can be statically scheduled at compile time based
on the balance equations associated with the numbers of produced/consumed
tokens and the node activation rates. This is not the case of KPNs, which are
scheduled dynamically in general. A recent interesting language sharing several
features with SDFs is StreamIt [39]. It is dedicated to streaming applications. The
deﬁnition of this language relies on the following application characteristics: large
streams of data, independent stream ﬁlters, stable computation patterns, occasional
modiﬁcation of stream structure, occasional out-of-stream communication and
high-performance expectations. To deal with all these aspects, StreamIt proposes
speciﬁc constructs that aim to facilitate the job of a programmer. A compiler is also
provided for an efﬁcient execution of programs.
3
Multidimensional Data Speciﬁcation Models
The need of manipulating multidimensional data structures in data intensive appli-
cations leads to a few extensions of SDF, such as: the multidimensional synchronous
dataﬂow (MDSDF) [24], its generalization GMDSDF [32], and the windowed
synchronous dataﬂow (WSDF) [23]. MDSDFs aim to specify SDFs in which edges
are multidimensional arrays with one possible inﬁnite dimension at most.

328
A. Gamati´e
Fig. 12.3 MSDF speciﬁcation of image resizing
Figure 12.3 shows another way to specify the previous image resizing example
with an MSDF. Here, the shape of tokens exchanged by nodes is bidimensional,
i.e., a matrix of pixels. In MDSDFs, the way data are consumed from or produced
in these multidimensional structures is required to be done only according to
the parallel of their dimension axes. This restriction is removed in GMDSDFs,
by introducing a rule according to which data are produced on a lattice deﬁned
and modiﬁed via speciﬁc actors: a source actor speciﬁes the form of this lattice
while it is modiﬁed by the decimator and expander actors. While GMDSDFs bring
more expressivity, their usage can be tedious in practice because of difﬁculties in
their balance equations solving. WSDF is another extension of MDSDF allowing
for expression of overlapping sliding windows, which is often useful in image
processing algorithms.
The programming of data intensive applications manipulating multidimensional
arrays is also addressed by considering streams and iterations in other languages
such as: single assignment language (Sisal) [16] and the single assignment C (SAC)
language [35]. Both languages are functional and provide an implicit expression of
parallelism. A comparison between them and the Array-OL language can be found
in [19,21].
Beyond the above speciﬁcation models, we also mention Alpha [29, 40], a
functional language devoted to the expression of regular data intensive algorithms.
Alpha relies on systems of afﬁne recurrence equations. The manipulated data
type consists of convex polyhedral index domains. This allows for the description
of algorithms operating on multidimensional data. Alpha is very close to the
Array-OL language [19]. It is associated with a few program transformations for
optimal implementations. In [36], authors proposed a combination of Alpha and
the synchronous language Signal [13,20] to design and validate data intensive real-
time systems. The data intensive computations are speciﬁed in Alpha whereas the
control ﬂow resulting from Alpha program transformations is described in Signal
via abstract clock constraints. The solution advocated in this chapter is very similar
to this combination: the regularity of the target data-intensive algorithms is speciﬁed
with the repetitive structure modeling, and reﬁned with component interaction
properties modeled with the clock constraint speciﬁcations in Marte.
Note that the synchronous dataﬂow languages Signal and Lustre [7] propose a
few useful constructs for the expression of data-parallelism. In Signal, the notion of
array of processes [4] enables the application of the same algorithm computation on
different subparts of ﬁnite multidimensional arrays. In [31], new array manipulation
constructs are proposed for Lustre. They are used to describe iterative algorithms as
one can found in data intensive applications.

12
Speciﬁcation of Data Intensive Applications with RSM and Abstract Clocks
329
4
Repetitive Structure Modeling
The repetitive structure modeling (RSM) relies on the Array-OL domain-speciﬁc
language [5, 9], dedicated to the speciﬁcation of intensive multidimensional sig-
nal processing applications. Among the basic characteristics of Array-OL are
the following: manipulation of multidimensional arrays, true data dependency
expressions, determinism, absence of causality cycles and single assignment in spec-
iﬁcations. Below, we present the main constructs of RSM based on an operational
semantics deﬁned previously in [14].
The algorithms speciﬁed with RSM consist of multidimensional array manip-
ulations via different kinds of tasks. Here, we distinguish four kinds: elementary,
composed, hierarchical and repetitive tasks.
The grammar shown in Fig. 12.4 gives an overview of RSM concepts. By
convention, the notation x W X denotes that x is of type X, and fXg represents
a set of elements of type X. Also, for syntactical convenience, a ”dot” notation is
used to designate parts of a concept, e.g., if T is a task, then T:Interface denotes the
set of input and output ports in its interface.
Rule (12.1) describes the features that are common ton all tasks in RSM:
•
An interface deﬁned in Rule (12.2), which specifying input and output ports,
respectively represented by i and o. Ports are characterized in Rule (12.3) by
their identiﬁer, by the type of the array elements they transmit, and by the shape
(i.e., dimension) of those arrays. Here, array elements are considered over basic
types (integers, Booleans, ...).
•
A body, deﬁned in Rule (12.4), describing the function deﬁned by the task.
Task WWD InterfaceI Body
(12.1)
Interface WWD i; o W fPortg
(12.2)
Port WWD idI typeI shape
(12.3)
Body WWD Bodyej BodycjBodyh j Bodyr
(12.4)
Bodye WWD some function 
(12.5)
Bodyc WWD Task1I Task2I fCnxg
(12.6)
Cnx WWD i; o W Port
(12.7)
Bodyh WWD TaskI fCnxg
(12.8)
Bodyr WWD ti; to W fTilergI sI TaskI fIrdg
(12.9)
Tiler WWD CnxI .FI o I P/
(12.10)
Ird WWD CnxI d
(12.11)
Fig. 12.4 A grammar of the repetitive structure modeling concepts

330
A. Gamati´e
In the next, a few preliminary deﬁnitions are introduced before the explanations
of remaining rules in Fig. 12.4. They serve as a semantical model for concept
deﬁnitions. Let V denote the set of multidimensional arrays.
Deﬁnition 12.1 (Environment). For a set of ports P , an environment for P is a
function " W P ! V.

The set of environments associated with P is noted "P . The fact that a port (or a
set of ports) p takes a value v in the environment " is denoted by ".p/ D v.
Deﬁnition 12.2 (Environment composition). Let "1 2 "P1 and "2 2 "P2 denote
two environments. They are composable iff 8p 2 P1 \ P2, "1.p/ D "2.p/. Their
composition, noted ˚, is:
˚ :
"P1  "P2
!
"P1[P2
."1; "2/
7!
"1 [ "2

A task T2 is a sub-task of a task T1 if either T1 D T2, or T2 is a sub-task of a task
occurring in the body of T1 (cf. Rules (12.6) in our grammar). Two tasks T1; T2 are
disjoint if the set of sub-tasks of T1 is disjoint from the set of sub-tasks of T2, or,
equivalently, if the sets of the respective elementary sub-tasks of T1; T2 are disjoint.
As a general assumption, we require that for all disjoint tasks T1 and T2, or
s.t. T2 is a strict sub-task of T1, the interfaces T1:Interface and T2:Interface are
disjoint, hence, all environments "1 2 "T1:Interface and "2 2 "T2:Interface are compatible.
Similar “semantical” constraints are also associated with some of the semantical
rules deﬁned below. Such a rule consists of the form:
C
"
T
! "0
where T
2 T , "; "0 2 "T:Interface, and C is a condition on T , " and "0. The
environment " gives the current values to the ports of the task T , and, the
environment "0 gives the next values for these ports, i.e., after the task T is executed.
The condition C must be satisﬁed in order to perform the transition between "
and "0 according to T . We sometimes denote by ŒŒT  the semantics of a task T
as follows: let T:Interface D .i; o/; then, for all environments "; "0: "
T
! "0 iff
("0.o/ D ŒŒT .".i//).
4.1
Elementary, Composed and Hierarchical Tasks
An elementary task E-Rule (12.5) – consists of a body E:Body performing some
function  and an interface E:Interface D .i; o/.

12
Speciﬁcation of Data Intensive Applications with RSM and Abstract Clocks
331
Deﬁnition 12.3 (Elementary task). Let E be an elementary task. Its operational
semantics is given by the rule:
"0.o/ D .".i//
"
E
! "0
where .i; o/ D E:Interface.

A composed task K-Rule (12.6) – has a body that consists in two tasks T1; T2,
and a set of connections C, where each connection is a pair of ports, as speciﬁed by
Rule (12.7).
We assume that the interface of the composed task K is equal to T1:Interface [
T2:Interface. This ensures that the graph induced by the connections C is acyclic.
A simple inductive argument shows that by iterating the composition operator, the
resulting graph of connection remains acyclic. This is consistent with the absence
of causality cycles in RSM.
Deﬁnition 12.4 (Composed task). The semantics of a composed task K, whose
body is T1I T2I C, is as follows:
"1
T1
! "0
1;
"2
T2
! "0
2
8.p1; p2/ 2 C; "0
1.p1/ D "2.p2/
"1 ˚ "2 K
! "0
1 ˚ "0
2

The above task composition is associative [14].
A hierarchical task H-Rule (12.8) – has a body that consists of a task T and
a set of connections C. We assume that C connects interfaces of H and T , i.e.,
for all .p1; p2/ 2 C, .p1; p2/ 2 .H:Interface  T:Interface/ [ .T:Interface 
H:Interface/. We also assume that H “hides” T from the outside, i.e., 8p 2
T:Interface; 9.p1; p2/ 2 C such that p D p1 or p D p2.
Deﬁnition 12.5 (Hierarchical task). The semantics of a hierarchical task H,
whose body is T I C, is given by the rule:
Q" T! Q"0;
8.p1; p2/ 2 C; ".p1/ D Q".p2/ ^ Q"0.p1/ D "0.p2/
" H
! "0

4.2
Repetitive Tasks
A repetitive task R-Rule (12.9) – expresses data-parallelism. Figure 12.5 illustrates
a repetitive task R in which a task T is repeated. Each instance of T takes as
input a sub-array or pattern pi extracted from a multidimensional array of data
i, and produces a pattern po stored in another multidimensional array of data o.

332
A. Gamati´e
i
o
po
pi
to
Fo
P
ti :
R
T
sr
Fig. 12.5 Model of a
repetitive task
The resulting instances of T are assumed to be independent and schedulable
following any order. The vector s denotes a (multidimensional) repetition space
from which the number of executed T instances is calculated. The values of its
components deﬁne the bounds of corresponding parallel loops.
4.2.1
Array Tiling and Paving
The tilers connectors ti and to – Rule (12.10) – contain the useful information that
enable to extract and store the patterns respectively from i and in o: F is a ﬁtting
matrix describing how array elements ﬁll patterns; o is an origin of the reference
pattern; and P is a paving matrix specifying how patterns tile an array. We brieﬂy
recall below the basic principles for pattern ﬁtting and array paving (for further
details, see also [5]).
Given a pattern within an array, let its reference element denote the origin
point from which all its other elements are extracted. The ﬁtting matrix is used to
determine these elements. Their coordinates, represented by ei, are built as the sum
of the coordinates of the reference element and a linear combination of the ﬁtting
vectors, the whole modulo the size of the array (since arrays are toroidal) as follows:
8 i; 0  i < spattern; ei D .ref C F  i/ mod sarray
(12.12)
where spattern is the shape of the pattern, sarray is the shape of the array and F is the
ﬁtting matrix.
Figure 12.6 illustrates the ﬁtting result for a .2; 3/-pattern with the tiling
information indicated on the same ﬁgure. The ﬁtting index-vector i, indicated in
each point-wise element of the pattern, varies between  0
0
 and  1
2
. The reference
element is characterized by index-vector  0
0
.
Now, for each task repetition instance, one needs to specify the reference
elements of the input and output tiles. The reference element of the ﬁrst repetition
are given by the origin vector, o, of each tiler. The reference elements of the other
repetition instances are built relatively to this one. Their coordinates are built as a
linear combination of the vectors of the paving matrix as follows:
8 k; 0  k < srepetition; refk D .o C P  k/ mod sarray
(12.13)

12
Speciﬁcation of Data Intensive Applications with RSM and Abstract Clocks
333
Fig. 12.6 Array paving and ﬁtting
where srepetition is the shape of the repetition space, P the paving matrix and sarray the
shape of the array. The paving illustrated by Fig. 12.6 shows how a .2; 3/-patterns
tile a .6; 6/-array. Here, the paving index-vector k varies between
 0
0

and
 2
1

.
A more general formula expressing the calculation of patterns k can be deﬁned
by combining the previous formulas (12.12) and (12.13) as follows:
8 k; 0  k < srepetition;
k D f.o C .P F / 
 k
i

/ mod sarrayj0  i < spatterng
(12.14)
We denote by ˛ D U
tfk j 0  k < srepetitiong the tiling operation of the array ˛
according to tiler t and repetition space srepetition, by which the array ˛ is partitioned
into a set of patterns according to the general formula (12.14).

334
A. Gamati´e
4.2.2
Deﬁnition of Repetitive Tasks
Deﬁnition 12.6 (Repetitive task). Let R be a repetitive task, denoted by the
following information: R:Interface D .fi1; : : : ing; fo1; : : : ; omg/, R:Body:Tiler D
.fti1; : : : ting; fto1; : : : ; tomg/,  D ŒŒR:Body:Task, and R:Body:Task:Interface D
.i; o/: The semantics of the task R is deﬁned below:
8k 2 0::s; "0
k.o/ D ."k.i//;
Vn
j D1 ".ij/ D U
tijf"k.i0
j/j0  k < s; .ij; i0
j/ D tij:Cnxg;
Vm
lD1 "0.ol/ D U
tolf"0
k.o0
l/j0  k < s; .ol; o0
l/ D tol:Cnxg
"
R
! "0
where s D R:Body:s.

In the above deﬁnition, the condition of the transition speciﬁed in Deﬁnition 12.6
has three parts. The ﬁrst part requires that all the repetitions (indexed by the
index-vector k) of the tasks in the repetitive task’s body complete their execution,
with the effect that their output ports o are valuated according to some next-state
environments "0
k; these values depend on the values of the input ports i according to
some current environment "k.
The second part of the transition condition in Deﬁnition 12.6 describes how the
current environment " for the repetitive task R is related to the current environments
"k of the repetitions. Essentially, the condition says that for each input port ij, for
j D 1; : : : n, ".ij/ is “tiled” by the corresponding input tiler tij , into the set of tiles:
f"k.i0
j/j0  k < R:Body:sg;
where i0
j is the port connected to ij by the input tiler tij. The third part of the
condition is similar to the second part - it describes how the next environment "0 for
the repetitive task R is related to the next environments "0
k according to the output
tilers tol (l D 1; :::; m).
Within a repetition, task instances may depend on each other. A typical example
is the computation of the sum of array elements by considering the partial sums,
until all elements are added. Such an algorithm induces an execution order between
task instances. Figure 12.7 illustrates a repetitive task with an inter-repetition
dependency – Rule (12.11) – connecting the output ports po of instances to the
input ports p of data-dependent instances, according to a dependency vector d. An
speciﬁc connector speciﬁes initialization values (for instances that do not depend on
any other instance), denoted by def, of input port p involved in the inter-repetition
dependency.
There can be several inter-repetition dependencies within a task, since an instance
may require values from more than one instances to compute its outputs. This is
why Rule (12.9) allows for a set of dependency link vectors fIrdg. The semantics of
repetitive tasks with inter-repetition dependency can be found in [14].

12
Speciﬁcation of Data Intensive Applications with RSM and Abstract Clocks
335
F
P
ti :
i
o
pi
to
o
R
p
sr
d
def
po
T
Fig. 12.7 Model of a
repetitive task with
inter-repetition dependency
Fig. 12.8 Image downscaling
4.3
Modeling of a Downscaler in Marte
In order to illustrate the RSM modeling of data-intensive algorithms, we consider a
simple downscaling transformation on images as illustrated in Fig. 12.8. The reduc-
tion process is divided into two phases: image size is ﬁrst reduced horizontally, then
vertically. The transformed images are assumed to be produced by a frame generator
component. The resized images frame are consumed by another component that
reconstruct a user-friendly video shown on a small-scale screen.

336
A. Gamati´e
Fig. 12.9 Marte speciﬁcation of an image downscaling application
In the next, we use the modeling concepts of RSM presented previously to
describe this very simple image downscaling application. Figure 12.9 illustrates a
corresponding model speciﬁed in a UML Marte-like notation. It consists of one
main task, named Application, in which three sub-tasks are composed:
•
An instance P of a FrameProducer component, which generates image
frames of size 1920  1080. P is assumed to be an elementary task producing an
inﬁnity of frames. Its output port named outFrameP, is deﬁned as an inﬁnite
tridimensional array. The shape of this array is noted Œ1920; 1080;  where the
star symbol “” denotes “inﬁnity” in the UML notation. Notice that at this design
level, the inﬁnite dimension does not necessarily denote the temporal dimension
even though this is the case in general. In Marte, each concept that holds a
multidimensional topology is characterized with the stereotype <<shaped>>.1
Here, it is the case of the outFrameP port
•
An instance D of a Downscaler component, which reduces the size of image
frames received from P. D is a hierarchical task detailed in Fig. 12.10a. The shape
of its input and output ports inFrameD and outFrameD are respectively those
of the output and input ports of P and C (presented next)
•
An instance C of a FrameConsumer component, which consumes image
frames in order to display the images. C is also assumed to be an elementary
task. The shape of its unique input port inFrameC equals to Œ720; 480; ,
representing an inﬁnity of reduced image frames.
The Downscaler component itself is detailed in Fig. 12.10a. It contains two
sub-tasks HF and VF, which reduce the received image frames according to
their horizontal and vertical dimensions respectively. HF and VF are instances
of HorizontalFilter and VerticalFilter components that consist of
repetitive tasks.
Only the HorizontalFilter component is shown in Fig. 12.10b (the
VerticalFilter component is deﬁned in a very similar way). The shape of
its output port Œ720; 1080;  shows that only one dimension (the horizontal one)
1For the sake of simplicity, the shape information is not always shown in all ﬁgures.

12
Speciﬁcation of Data Intensive Applications with RSM and Abstract Clocks
337
Fig. 12.10 A downscaler model
is reduced. To achieve this reduction, a task instance H is replicated in parallel
on different pixel patterns of the input image frames in order to reduce them
into smaller patterns. The shape Œ240; 1080;  of the repetition space associated
with H gives the number of replicated H instances: 240  1080  1. Since the
H task is repeated according to its repetition space, it is also associated with the
<<shaped>> stereotype in Marte.
The shapes of input and output pixel patterns corresponding to H are respectively
Œ13 and Œ3. These patterns are extracted based on input and output tiler speciﬁ-
cations. The Marte <<tiler>> stereotype characterizes a connection denoting a
tiler in a repetitive task.
The model represented in Fig. 12.10a, b is one possible speciﬁcation of the
downscaling algorithm. It is absolutely correct from the functional point of view
expressed by the data dependencies. However, such a model is not realistic from
an execution point of view. As a matter of fact, the HorizontalFilter
component is supposed to produce an inﬁnite output array as an input for the
VerticalFilter component. The production of this output array will probably
require an inﬁnite execution duration for HorizontalFilter, which means that
VerticalFilter will not be executed. In fact, what a designer may need here
is to consider the inﬁnite dimension of arrays as the temporal dimension. Then, a
pipelined execution becomes possible between both components.

338
A. Gamati´e
Fig. 12.11 A downscaler model

12
Speciﬁcation of Data Intensive Applications with RSM and Abstract Clocks
339
An alternative model of the Downscaler is therefore proposed after a model
refactoring that introduces an additional hierarchy level in Fig. 12.11a. In terms
of loop transformations, this corresponds to a fusion operation [17]. The inﬁnite
dimension of arrays is now present only at the introduced highest hierarchy level. It
can be consistently and straightforwardly interpreted as the temporal dimension.
The granularity (i.e., units) of such a temporal dimension should be chosen
according to environment and/or execution platform constraints. This is expressed
by using abstract clock in the next section.
One can notice that the speciﬁcation of the HorizontalFilter (see
Fig. 12.11c) and VerticalFilter (see Fig. 12.11d) has been modiﬁed, and
mainly concerns their input and output data sizes. Now, both components are
successively applied to sub-arrays of size .14  13/ to produce sub-arrays of size
.3  4/ (see Fig. 12.11b).
The above Marte speciﬁcation of the downscaling application expresses the
entire data-dependency relations between the different arrays corresponding to
image frames. It captures completely the (deterministic) functional behavior of the
application. In addition, it is an implicit description of the potential parallelism
inherent to the downscaling application. An effective implementation of such
an application will require a choice of an execution model: pipeline strategy,
parallelism degree induced by available number of processors for the data-parallel
part, etc. Such choices lead to various interactions between application components.
The modeling of temporal behaviors plays a key role in the reﬁnement of the
RSM speciﬁcation towards the mentioned execution models. For this purpose, we
consider abstract clock constraints presented in the next section.
5
Abstract Clocks
We introduce a notion of abstract clock inspired by the synchronous reactive
programming [3]. Such clocks offer an interesting way to reﬁne an implicit
description of parallelism (obtained with the repetitive model of computation) by
deﬁning activation rules between application components. As a result, the resulting
descriptions can be associated with a clear execution model.
5.1
Basic Deﬁnitions
We deﬁne a few basic notions that will serve to deﬁne abstract clocks and related
constraints. The deﬁnition of these notions is inspired by [20].
Let us consider the following sets:
•
X a countable set of variables,
•
V a value domain,

340
A. Gamati´e
•
T a dense set equipped with a partial order relation , and holding a lower bound
for the  relation. The elements of T are called tags [26].
We introduce the notion of observation point.
Deﬁnition 12.7 (Observation points). A set of observation points is a set of tags
T  T such that:
1. T is countable,
2. T holds a lower bound for the  relation,
3.  is well-founded on T , i.e., there exists no inﬁnite series .tn/ such that 8n 2
N; tnC1  tn.

The set T provides a discrete time dimension that corresponds to logical instants
according to which the presence and absence of events can be observed during a
system execution. The set T provides a continuous time dimension that may also
serve as a physical time scale.
A chain C  T is a totally ordered set admitting a lower bound. The set of
chains is denoted by C. For a set of observation points T , we denote by CT the set
of all possible chains in T .
Deﬁnition 12.8 (Event). Given a set of observation points T , an event e is a pair
.t; v/ 2 T  V.

Deﬁnition 12.9 (Signal). Given a set of observation points T and a chain C 2 CT ,
a signal s is a partial function C * V, which associates values with observation
points that belong to C.

The set of signals on T is noted ST . We denote by tags.s/ the set of tags
associated with a signal s, i.e., the domain of the signal s.
Deﬁnition 12.10 (Clock). Given a signal s, its corresponding abstract clock is the
set tags.s/.

From the above deﬁnition, a clock is a totally ordered set of tags (or instants).
In order to determine the relative distance from any tag to its neighbors, a time
reference, e.g., N the set of natural numbers, is required.
In the next, we deﬁne the notion of afﬁne clocks, as ordered sets of instants
with positions identiﬁed by an afﬁne enumeration with respect to a reference clock
T D N.
Deﬁnition 12.11 (Afﬁne clock). Given a set of observation points T , an abstract
clock is said to be afﬁne if its associated tags can be characterized with an afﬁne
function according to T , i.e., the set of tags in such a clock is of the form:
f C  j  2 Nnf0g;  2 Z;  2 T g:


12
Speciﬁcation of Data Intensive Applications with RSM and Abstract Clocks
341
Fig. 12.12 Trace of
1 D f3 C 2 j  2 T g;
2 D f2 C 1 j  2 T g
and 3 D f j  2 T g
Fig. 12.13 Binary trace of
the sets of instants 1; 2
and 3
Figure 12.12 shows an example of trace characterizing the instant occurrences
over a reference time scale, of three afﬁne clocks deﬁned by the sets of instants:
1 D f3 C 2 j  2 T g; 2 D f2 C 1 j  2 T g and 3 D f j  2 T g:
For speciﬁcation convenience, a notation is used as in CCSL [27] to describe
an abstract clock as a binary word. Given a signal s and a chain C 2 CT such
that tags.s/  C, the binary encoding of the abstract clock corresponding to s is
deﬁned by a function clock W C ! f0; 1g deﬁned as follows:
8t; clock.t/ D
 1 if t 2 C \ tags.s/;
0 if t 2 Cntags.s/:
For instance, considering C D N, the abstract clocks characterized by the sets of
instants 1; 2 and 3 are speciﬁed in the binary notation as shown in Fig. 12.13.
A more compact binary notation for the sets 1, 2 and 3 is respectively
00.100/!; 0.10/! and .1/! where ! 2 N.
5.2
Abstract Clock Relations
We distinguish various clock relations amongst which is the .n; ; /-afﬁne clock
relation between two clocks clk1 and clk2. Such a relation is obtained by inserting
.n1/ tags between any two successive elements of clk1, then clk2 is characterized
by considering each th instant in the resulting set of tags, starting from the . C
1/th tag.
Deﬁnition 12.12 (Afﬁne clock relation). Given two abstract clocks clk1 and clk2,
they are said to be in .n; ; /-afﬁne relation, noted as clk1
.n;;/
!
clk2 if the
Cartesian product clk1  clk2 is the co-domain of the following function f :
f W N  N ! clk1  clk2
.t; t0/ 7! .nt; t0 C /
where n;  2 N and  2 Z (the set of integers).


342
A. Gamati´e
As an example, we can notice that in Fig. 12.12, the following afﬁne clock
relations hold:
3
.1;2;3/
! 1 and 3
.1;1;2/
! 2:
These afﬁne clock relations can be speciﬁed in Marte by using the CCSL
concepts. In that way, a given model speciﬁed with RSM can be enriched with more
precise temporal properties in order to model an operational design of data intensive
applications as suggested in the next example.
Beyond afﬁne clock relations, there are further interesting clock relations such
as those deﬁned in the CCSL language [2, 27]. A brief description of such binary
relations is given below:
•
Inclusion: the expressions c1 isSubClockOf c2 and c1 isSuperClockOf
c2 respectively mean that an abstract clock c1 is a subset (in terms of set of tags)
of another abstract clock c2, and c1 is a superset of c2.
•
Relative speed: the expression c1 isFasterThan c2 (resp. c1 isStrict-
lyFasterThan c2) means that for each i within the range of all possible tag
index values, the ith instant of clock c1 (resp. strictly) precedes the ith instant
of clock c2. In particular, these relations are very interesting when modeling
causality relations, hence data dependencies, between some events observed on
different clocks.
•
Clocks with alternating instants: the expression c1 alternatesWith c2
speciﬁes that instant occurrences in a clock c1 alternates with instant occurrences
in a clock c2.
The next section provides a enhancement of the Downscaler model with afﬁne
clock relations.
5.3
Example of Model in Marte
Let us consider again the downscaling application speciﬁed in Marte in Fig. 12.11a.
We enrich this speciﬁcation with clock constraint information in order to model
some interaction resulting from a possible implementation of the application. For
this purpose, the three tasks composing the application, i.e., image frame producer
P, downscaling task D and image frame consumer C, are associated with abstract
afﬁne clocks, respectively denoted by cp; cd and cc.
In CCSL, we ﬁrst deﬁne a basic clock type, named ActivationClock as
shown in Fig. 12.14. This is introduced by the <<clockType>> stereotype. Here,
the type represents discrete logical clocks (attributes nature and isLogical).
The unit of the clock type is a tick corresponding to an instantaneous execution of a
task upon activation.

12
Speciﬁcation of Data Intensive Applications with RSM and Abstract Clocks
343
Fig. 12.14 Deﬁnition of a
clock type
Fig. 12.15 Parametrized clock relations for the downscaling application
Three clocks, c p, c d and c c, are created now as instances of the basic
clock ActivationClock type (see Fig. 12.15). They are associated with the
components P, D and C of the downscaling application model via the on concept of
Marte. Notice that P, D and C hold a timed behavior property after this association,
speciﬁed via the stereotype timedProcessing.

344
A. Gamati´e
The intended behavior of this application is a pipelined execution of the
following steps:
1. P produces a set of frames upon each activation of clock c p
2. D is activated after certain amount of frames are produced by P (i.e., after a
number of activations of P) in order to reduce these frames. According to the
speciﬁcation in Fig. 12.15, these activations of component D happen periodically
every p1 activations of P, starting from the (d1+1)th activation of P:
cp
.1;p1;d1/
!
cd
3. C gets activated after a number of frames has treated by D (i.e., after a number of
activations of D). In the speciﬁcation of Fig. 12.15, the activations of component
C happen periodically every p2 activations of D, starting from the (d2+1)th
activation of D:
cd
.1;p2;d2/
!
cc
In the above pipelined execution of the downscaling application speciﬁed with
abstract clocks, we can notice that the inﬁnite dimension of arrays exchanged by P,
D and C components is implicitly interpreted as time.
A simulation tool, referred to as Timesquare [37], has been developed in order
to provide a designer with clock-based simulation traces from a speciﬁed CCSL
model, so that behaviors of a considered system can be analyzed via the traces.
There is a study that focuses on the use of CCSL to model timed causality models
[28]. Authors propose to capture the causal relations expressed by SDFs in the form
of CCSL expression. The result is typically used as an execution semantics for UML
activity diagrams. In [18], authors consider a similar approach by extending the
causal relation expression to MDSDFs.
6
Analysis
The model transformation mentioned earlier in Sect. 1.1, towards synchronous
languages aims at verifying key properties of data intensive application mod-
els designed in Gaspard2. The basic idea of this transformation consists of a
structural translation of RSM speciﬁcations into synchronous dataﬂow programs
[15]. Concretely, this yields a semantically-equivalent system of equations in the
target languages (Lustre and Signal). Typically for repetitive tasks, each equation
corresponds to a repeated task instance, while the global repetition is obtained via
the parallel composition of equations associated with all instances. Afterwards, the
correctness of the resulting models is addressed by using the formal validation
techniques and tools of synchronous languages.

12
Speciﬁcation of Data Intensive Applications with RSM and Abstract Clocks
345
A well-designed data intensive application model in Gaspard2 must satisfy some
basic data dependency properties, enumerated in the following:
•
Absence of causality cycles from data dependency speciﬁcations.
•
Absence of uninitialized array to be read.
•
Single assignment: in the RSM paradigm, values are supposed to be produced
only once.
•
Functional determinism: any scheduling scenario during executions of data
intensive application model respecting the speciﬁed data dependencies in RSM,
necessarily leads to the same functional results.
All these properties can be veriﬁed on the synchronous models obtained from
the transformation of given Gaspard2 models. Indeed, the compilation process of
synchronous languages includes speciﬁc causality analysis techniques taking into
account both data dependency information and abstract clock properties.
The abstract clock properties speciﬁed in a data intensive application as shown
in previous sections are adequately analyzed with compilers of synchronous lan-
guages. Typically, the synchronizability between different components that hold
clocks with different rates is checked with respect to the constraints imposed by
execution platforms or environment [15]. From such an analysis, one can study the
suitable clock rate values to have a synchronous interaction between components.
One can infer useful information about memory buffer sizing so that no exchanged
data is lost during the interaction between components.
The considered abstract clocks also give a basis to study different scheduling
scenarios of data intensive applications on execution platforms. We have deﬁned
clock projections that capture the dynamic behavior of an application upon a
multiprocessor architecture with variable frequencies [1]. The result of these clock
projections is a set of simulation traces. Depending on a conﬁguration of an
execution platform, we can easily observe whether the clock relations speciﬁed
at the functional level are preservation. For instance, the synchronizability results
presented in N-synchronous Kahn networks [8] and in Signal-Alpha combination
[36] applies to the obtained traces. This makes it possible to identify the platform
conﬁgurations that meet as much as possible the memory amount requirement of a
system. These simulation traces also serve to determine the temporal performance
for each system conﬁguration. Furthermore, one can distinguish conﬁgurations
that would reduce energy consumption regarding considered real-time constraints.
In these conﬁgurations, the processors have the lowest frequency values while
respecting system execution deadlines.
7
Summary
In this chapter, we presented the speciﬁcation of data intensive applications within
a framework, named Gaspard2 and dedicated to the graphical design with the
UML Marte standard proﬁle, of these applications on Systems-on-Chip (SoCs). A

346
A. Gamati´e
repetitive structure modeling (RSM) has been used in combination with an abstract
clock notion to represent the applications. RSM proposes a rich set of constructs
that allow for an implicit description of the parallelism inherent to data intensive
applications. The models speciﬁed with RSM are reﬁned to reﬂect different
execution scenarios according to possible constraints imposed by interaction modes
of application components based on conﬁgurations of an execution platform or
the application environment. Here, such constraints are expressed in the form of
activation rate relations, adequately captured by abstract clock constraints. An
example of design about a typical multimedia image processing have shown the
way modeling concepts of Marte are used. The obtained designs can be therefore
considered for analysis based on the formal validation technologies reachable after
automatic model transformations in Gaspard2.
Acknowledgements The author would like to thank his colleagues from the DaRT group of
LIFL/CNRS and Inria, who also contributed to the ideas presented in this Chapter.
References
1. Adolf Abdallah, Abdoulaye Gamati´e and, and Jean-Luc Dekeyser. Correct and energy-efﬁcient
design of socs: The h.264 encoder case study. In System on Chip (SoC), 2010 International
Symposium on, pages 115–120, Sept. 2010.
2. Charles Andr´e and Fr´ed´eric Mallet. Clock Constraints in UML/MARTE CCSL. Research
Report RR-6540, INRIA, 2008.
3. Albert Benveniste, Paul Caspi, Steven Edwards, Nicolas Halbwachs, Paul Le Guernic, and
Robert de Simone. The synchronous languages twelve years later. Proceedings of the IEEE,
91(1):64–83, January 2003.
4. Lo¨ıc. Besnard, Thierry Gautier, and Paul Le Guernic. Signal reference manual., 2007. www.
irisa.fr/espresso/Polychrony.
5. Pierre Boulet. Formal Semantics of Array-OL, a Domain Speciﬁc Language for Intensive
Multidimensional Signal Processing. Research report, INRIA, France, March 2008. available
online at http://hal.inria.fr/inria-00261178/fr.
6. Surendra Byna and Xian-He Sun. Special issue on data intensive computing. Journal of
Parallel and Distributed Computing, 71(2):143–144, 2011. Data Intensive Computing.
7. Paul Caspi, Daniel Pilaud, Nicolas Halbwachs, and John Plaice. Lustre: a declarative language
for real-time programming. In Proceedings of the 14th ACM SIGACT-SIGPLAN symposium on
Principles of programming languages (POPL’87), pages 178–188. ACM Press, 1987.
8. Albert Cohen, Marc Duranton, Christine Eisenbeis, Claire Pagetti, Florence Plateau, and
Marc Pouzet. N-sychronous Kahn networks. In ACM Symp. on Principles of Programming
Languages (PoPL’06), Charleston, South Carolina, USA, January 2006.
9. Alain Demeure and Yannick Del Gallo. An array approach for signal processing design.
In Sophia-Antipolis conference on Micro-Electronics (SAME’98), System-on-Chip Session,
France, October 1998.
10. Marc Duranton, Sami Yehia, Bjorn De Sutter, Koen De Bosschere, Albert Cohen, Babak
Falsaﬁ, Georgi Gaydadjiev, Manolis Katevenis, Jonas Maebe, Harm Munk, Nacho Navarro,
Alex Ramirez, Olivier Temam, and Mateo Valero. The hipeac vision. Report, European
Network of Excellence on High Performance and Embedded Architecture and Compilation,
2010.

12
Speciﬁcation of Data Intensive Applications with RSM and Abstract Clocks
347
11. Abdoulaye Gamati´e, S´ebastien Le Beux, ´Eric Piel, Anne Etien, Rabie Ben-Atitallah, Philippe
Marquet, and Jean-Luc Dekeyser. A model driven design framework for high performance
embedded systems. Research Report 6614, INRIA, 2008. http://hal.inria.fr/inria-00311115/en.
12. Abdoulaye Gamati´e, S´ebastien Le Beux, ´Eric Piel, Rabie Ben Atitallah, Anne Etien,
Philippe Marquet, and Jean-Luc Dekeyser. A model driven design framework for massively
parallel embedded systems. ACM Transactions on Embedded Computing Systems (TECS),
2011. To appear.
13. Abdoulaye Gamati´e. Designing Embedded Systems with the Signal Programming Language -
Synchronous, Reactive Speciﬁcation. Springer, 2010.
14. Abdoulaye Gamati´e, Vlad Rusu, and ´Eric Rutten. Operational semantics of the marte repetitive
structure modeling concepts for data-parallel applications design. In ISPDC, pages 25–32.
IEEE Computer Society, 2010.
15. Abdoulaye Gamati´e, ´Eric Rutten, Huafeng Yu, Pierre Boulet, and Jean-Luc Dekeyser. Syn-
chronous modeling and analysis of data intensive applications. EURASIP J. Emb. Sys., 2008,
2008.
16. Jean-Luc Gaudiot, Thomas DeBoni, John Feo, A. P. Wim B¨ohm, Walid A. Najjar, and Patrick
Miller. The sisal project: Real world functional programming. In Santosh Pande and Dharma P.
Agrawal, editors, Compiler Optimizations for Scalable Parallel Systems Languages, volume
1808 of Lecture Notes in Computer Science, pages 45–72. Springer, 2001.
17. Calin Glitia, Pierre Boulet, ´Eric Lenormand, and Michel Barreteau. Repetitive model refac-
toring strategy for the design space exploration of intensive signal processing applications.
Journal of Systems Architecture, In Press, Corrected Proof:–, 2011.
18. Calin Glitia, Julien DeAntoni, and Fr´ed´eric Mallet. Logical time at work: Capturing data
dependencies and platform constraints. In Adam Morawiec and Jinnie Hinderscheit, editors,
FDL, pages 241–. ECSI, Electronic Chips & Systems design Initiative, 2010.
19. Calin Glitia, Philippe Dumont, and Pierre Boulet. Array-ol with delays, a domain speciﬁc
speciﬁcation language for multidimensional intensive signal processing. Multidimensional
Systems and Signal Processing, 21:105–131, 2010. 10.1007/s11045-009-0085-4.
20. Paul Le Guernic, Jean-Pierre Talpin, and Jean-Christophe Le Lann, and Projet Espresso.
Polychrony for system design. Journal for Circuits, Systems and Computers, 12:261–304,
2002.
21. Jing Guo, Antonio Wendell De Oliveira Rodrigues, Jerarajan Thiyagalingam, Fr´ed´eric Guy-
omarch, Pierre Boulet, and Sven-Bodo Scholz. Harnessing the Power of GPUs without Losing
Abstractions in SaC and ArrayOL: A Comparative Study. In HIPS 2011, 16th Interna-
tional Workshop on High-Level Parallel Programming Models and Supportive Environments,
Anchorage (Alaska) United States, 05 2011.
22. Gilles Kahn. The semantics of simple language for parallel programming. In IFIP Congress,
pages 471–475, 1974.
23. Joachim Keinert, Christian Haubelt, and J¨urgen Teich. Simulative buffer analysis of local
image processing algorithms described by windowed synchronous data ﬂow. In Holger Blume,
Georgi Gaydadjiev, C. John Glossner, and Peter M. W. Knijnenburg, editors, ICSAMOS, pages
161–168. IEEE, 2007.
24. Edward A. Lee. Mulitdimensional streams rooted in dataﬂow. In Michel Cosnard, Kemal
Ebcioglu, and Jean-Luc Gaudiot, editors, Architectures and Compilation Techniques for Fine
and Medium Grain Parallelism, volume A-23 of IFIP Transactions, pages 295–306. North-
Holland, 1993.
25. Edward A. Lee and David G. Messerschmitt. Synchronous data ﬂow: Describing signal
processing algorithm for parallel computation. In COMPCON, pages 310–315, 1987.
26. Edward A. Lee and Alberto Sangiovanni-vincentelli. A framework for comparing models
of computation. IEEE Transactions on Computer-Aided Design of Integrated Circuits and
Systems, 17(12):1217–1229, 1998.
27. Fr´ed´eric Mallet. Clock constraint speciﬁcation language: specifying clock constraints
with uml/marte. Innovations in Systems and Software Engineering, 4:309–314, 2008.
10.1007/s11334-008-0055-2.

348
A. Gamati´e
28. Fr´ed´eric Mallet, Julien DeAntoni, Charles Andr´e, and Robert de Simone. The clock constraint
speciﬁcation language for building timed causality models. Innovations in Systems and
Software Engineering, 6:99–106, 2010. 10.1007/s11334-009-0109-0.
29. Christophe Mauras. Alpha : un langage ´equationnel pour la conception et la programmation
d’architectures parall`eles synchrones. PhD thesis, Universit´e de Rennes I, France, December
1989.
30. Message Passing Interface Forum. MPI Documents. http://www.mpi-forum.org/docs/docs.
html, 2009.
31. Lionel Morel. Array iterators in lustre: From a language extension to its exploitation in
validation. EURASIP Journal on Embedded Systems, 2007:Article ID 59130, 16 pages, 2007.
32. Praveen K. Murthy and Edward A. Lee. Multidimensional synchronous dataﬂow. IEEE
Transactions on Signal Processing, 50:3306–3309, 2002.
33. OMG. The uml proﬁle for marte: Modeling and analysis of real-time and embedded systems.
http://www.omgmarte.org, 2011.
34. Laurent Rioux, Thierry Saunier, S´ebastien G´erard, Ansgar Radermacher, Robert de Simone,
Thierry Gautier, Yves Sorel, Julien Forget, Jean-Luc Dekeyser, Arnaud Cuccuru, C´edric Du-
moulin, and Charles Andr´e. Marte: A new omg proﬁle rfp for the modeling and analysis of
real-time embedded systems. In DAC Workshop UML for SoC Design, UML-SoC’05, Anaheim
CA, USA, June 2005.
35. Sven-Bodo Scholz. Single assignment c: efﬁcient support for high-level array operations in a
functional setting. J. Funct. Program., 13(6):1005–1059, 2003.
36. Irina Smarandache, Thierry Gautier, and Paul Le Guernic. Validation of mixed Signal-
Alpha real-time systems through afﬁne calculus on clock synchronisation constraints. In World
Congress on Formal Methods (2), pages 1364–1383, 1999.
37. The Aoste Team. Timesquare. http://www-sop.inria.fr/aoste, 2011.
38. The DaRT Team. Gaspard2 design environment. http://www.gaspard2.org, 2011.
39. William Thies, Michal Karczmarek, Michael Gordon, David Maze, Jeremy Wong, Henry Hoff-
mann, Matthew Brown, and Saman Amarasinghe StreamIt: A compiler for streaming applica-
tions. MIT/LCS Technical Memo MIT/LCS Technical Memo LCS-TM-622, Massachusetts
Institute of Technology, Cambridge, MA, December 2001.
40. Doran Wilde. The Alpha language. Technical Report 827, IRISA - INRIA, Rennes, 1994.
available at www.irisa.fr/bibli/publi/pi/1994/827/827.html.

Chapter 13
Ensemble Feature Ranking Methods for Data
Intensive Computing Applications
Wilker Altidor, Taghi M. Khoshgoftaar, Jason Van Hulse,
and Amri Napolitano
1
Introduction
Feature selection, which consists of selecting the most relevant features from a larger
set of features, is considered one of the most critical problems researchers face
today in data mining and machine learning. Feature selection has made learning
from a high dimensional feature space possible by separating relevant features from
irrelevant and redundant ones. A well-studied type of feature selection is known
as the ﬁlter method, which evaluates feature relevance by examining the intrinsic
characteristics of the data without the use of a classiﬁer [28]. Feature ranking [12],
a type of ﬁlter method, determines the relevancy of the features by their correlations
to the class and ranks them according to their degrees of relevance.
Traditionally, the effectiveness of feature ranking techniques is measured by the
classiﬁcation performance of a subset of the most relevant features identiﬁed by
these techniques. Given the performance differences of inductive algorithms, the
classiﬁcation performance of the relevant subset depends on the inductive algorithm.
Naturally, many studies have focused on improving the classiﬁcation performance.
One approach that receives much attention is to combine multiple classiﬁers to
form what is known as an ensemble or a committee. To increase the diversity of
the classiﬁers in an ensemble, numerous works have proposed combining feature
selection techniques and a classiﬁer ensemble. This combining approach is referred
to as ensemble feature selection [5]. This approach uses feature selection techniques
to vary the feature subsets used by each member of the classiﬁer ensemble [20].
Thus, the goal is not only to reduce the dimensionality of the feature space but also
to promote diversity among the members of the classiﬁer ensemble.
W. Altidor • T.M. Khoshgoftaar () • J. Van Hulse • A. Napolitano
FAU, Boca Raton, FL
e-mail: wilker.altidor@gmail.com; khoshgof@fau.edu; jvanhulse@gmail.com;
amrifau@gmail.com
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 13, © Springer Science+Business Media, LLC 2011
349

350
W. Altidor et al.
This paper focuses on a novel approach to feature selection that only recently
has been examined by some researchers [1, 26, 27]. It is based on the intuitive
concept of ensemble learning, which has the advantage of the integration of multiple
learning approaches over one single approach in classifying new instances. This
novel approach extends the ensemble concept from an ensemble of classiﬁers
to an ensemble of feature ranking techniques, whereby multiple distinct feature
ranking techniques are combined to give a single ranking. This is different from
manipulating training datasets through feature selection techniques to help an
ensemble of classiﬁers. Instead, our procedure combines the results of multiple
feature ranking techniques to obtain a ﬁnal ranked list that is potentially better than
the individual components.
This study uses six commonly known ﬁlter-based feature ranking techniques,
which are referred to as standard ﬁlter-based feature ranking techniques (see
Sect. 3.1). It also includes a set of eleven novel and innovative ﬁlter-based feature
selection techniques called TBFS recently proposed by our research group (see
Sect. 3.2). Moreover, we consider six ensembles based on combinations of standard
and threshold-based ﬁlters (see Sect. 3.3). These ensembles, whose construction
is based solely on the types of the ranking techniques, are referred to as general
ensembles (see Sect. 3.3.1). In addition to these general ensembles, we present four
other ensembles, whose construction is based not only on the types of the individual
ranking techniques but also on their robustness to class noise. These ensembles are
referred to as focused ensembles (see Sect. 3.3.2).
These ensemble feature ranking techniques are evaluated on seven binary
classiﬁcation datasets. We compare all the ensemble feature rankings in terms of
their similarity to one another and to the individual components, using Kendall
Tau correlations. The classiﬁcation performance resulting from a reduced feature
space is not considered, nor is the construction of the optimal subset vis-a-vis
classiﬁcation performance. Hence, the experiments are performed independently of
any inductive learner’s performance on selected features. The empirical results show
that the correlation strength is impacted by the types of the techniques comprising
the ensemble feature ranking, and the ranking obtained from an ensemble tends to be
more comparable to that of its components than the ranking from a non-component
technique. We also assess the ensembles with respect to their robustness to class
noise as we seek to provide answers to the following questions:
1. How does the robustness of individual components affect the ensemble?
2. Can ‘unstable’ feature selection techniques help each other?
3. Can the addition of ‘stable’ techniques to an ensemble of ‘unstable’ techniques
help?
4. How do the number and variety of the individual feature selection techniques
affect the ensemble feature performance?
5. How should an ensemble feature ranking be built? Is there value in a systematic
process for choosing the components of an ensemble?
The remainder of this paper is organized as follows. Section 2 discusses related
work. Section 3 introduces the feature ranking techniques and the ensembles utilized

13
Ensemble Feature Ranking Methods for Data Intensive Computing Applications
351
in this work. Section 4 provides details about the experimental setup. Section 5
presents our experimental results along with analysis of the results. Finally, our
conclusion and future work are discussed in Sect. 6.
2
Related Work
Traditional feature selection techniques focus on identifying a relevant subset of
features that best describe the target concept [25]. Feature ranking, a well studied
type of feature selection, is very effective at separating relevant from irrelevant
features. It removes a major impediment to successful machine learning of real
world data: irrelevant/redundant information. In addition to irrelevant information,
real world data often contain various types of errors. These errors may be random
or systemic, and they are commonly referred to as noise. Noise can creep into
the dependent feature when an instance is assigned to a wrong class and into the
independent features when there are incorrect or missing values in the independent
features [40]. The former is called class or label noise while the later is known as
attribute noise. There are three ways for handling noisy data. One can rely on robust
learners [40] (i.e. those that are less affected by noise in the data). Also, one can
clean [41] the data prior to learning by correcting erroneous values and imputing
values to missing ones. Finally, one can ﬁlter the noise by detecting [16, 24] and
removing noisy instances prior to learning.
While feature selection has been the target of many works, very little study has
been done to systematically address the issue of feature relevancy in the presence
of noisy data. With the various feature selection methods studied and proposed as
new applications and research problems in data mining emerge, there is arguably
an implicit assumption on noise handling, given that many have been analyzed on
real world data. However, prior to our study [3], no known research has assessed
the impact of noisy data on feature selection techniques. A work worth mentioning
is that of Daza et al., which proposed two new feature selection methods that used
a quality measure to evaluate the quality of an instance. The measure is based on
an instance’s similarity with other members of the same class and its dissimilarity
with instances of different classes [8]. While a quality measure is used to evaluate
feature relevance, the effect of the quality of data or its lack thereof on the feature
selection method is not considered.
Ensemble feature selection has recently become a topic of interest. The ensemble
feature selection approach for the most part consists of combining feature selection
techniques and a classiﬁer ensemble with the objective of increasing the diversity
of the classiﬁers in the ensemble. Generally, the methods for combining feature
selection techniques with classiﬁer ensembles fall into three categories: random,
nonrandom, and optimized feature selection approaches. In the random case, we
consider for example the work of Tsymbal et al. [32] that presented the random
selection of feature subsets from the original feature space to build an ensemble
consisting of diverse simple Bayesian classiﬁers. Tumer et al. [33] presented a

352
W. Altidor et al.
nonrandom approach to selecting different subsets of features for the base classiﬁers
of an ensemble. An example of the optimized feature selection approach is the work
of Opitz et al. in [20], which proposed an ensemble feature selection approach that
uses a genetic algorithm to obtain a set of feature subsets for classiﬁer ensembles.
The focus of most works [5, 29] on ensemble feature selection is on improving
classiﬁcation performance through the combination of feature selection techniques
and a classiﬁer ensemble. These works assess how an ensemble of feature selection
techniques can offer diversity, thereby improving ensembles of classiﬁers. Not
enough work has considered how the results of multiple feature selection techniques
may be combined to enhance feature selection itself. Only a few have taken that
direction. For example, Rokach et al. [25] examine how ensemble feature selection
can improve feature selection rather than classiﬁer ensemble; however, they only
consider non-ranking ﬁlters. Jong et al. [14] introduce an ensemble feature ranking
that combines multiple feature rankings from the same dataset and show a boost in
stability for the ensemble feature ranking as the ensemble size increases. However,
their experimental results are based solely on artiﬁcial datasets. This new concept
of ensemble feature selection is also presented in [1,26,27]. These works show that
ensemble feature selection techniques yield more robust feature subsets. While they
look at the robustness of ensemble feature selection, the stability is demonstrated
by variation due to changes in the data size. It is more desirable to understand the
variation due to class noise as noisy data is considered more pertinent. For this
reason, our work considers the effect of class noise on ensemble feature rankings
based off standard ﬁlters, TBFS techniques, and a combination of the two. Until
now, no work has examined the effect of class noise on ensemble feature ranking
techniques.
3
Feature Ranking Techniques
We consider 17 ﬁlter-based feature ranking techniques. Six of these, which are
more frequently used in the literature, are referred to as the standard ﬁlter-based
feature ranking techniques. The remaining eleven techniques, recently proposed
by our research group, are referred to as threshold-based feature selection (TBFS)
techniques. Each of the 17 techniques examines feature relevancy with respect to
the class value without the use of a learner. Given that all the datasets in this study
are of binary class, these techniques determine the relevancy of each feature to the
class of interest (the positive P class) or the other class (the negative N class).
3.1
Standard Filter-Based Feature Ranking Techniques
The standard ﬁlter-based feature ranking techniques include 2-statistic, informa-
tion gain, gain ratio, two versions of ReliefF, and symmetric uncertainty.

13
Ensemble Feature Ranking Methods for Data Intensive Computing Applications
353
1: Chi-Squared (2) is based on the 2-statistic, and it evaluates features indepen-
dently with respect to the class labels. The larger the Chi-squared, the more
relevant the feature is with respect to the class. The values of the features must
ﬁrst be discretized into a number of intervals using some discretization method
such as an entropy-based one [11, 19]. The Chi-squared value of each feature is
computed as:
2 D
I
X
iD1
B
X
j D1
ŒAij  Ri Bj
N
2
Ri Bj
N
(13.1)
Where I denotes the number of intervals, B the number of classes, N the total
number of instances, Ri the number of instances in the ith interval, Bj the
number of instances in the jth class, and Aij the number of instances in the ith
interval and jth class. Note that for the Chi-squared approximation to be valid,
the test requires a sufﬁcient sample size.
2: Information Gain (IG) is a commonly used measure in the ﬁelds of information
theory and machine learning. IG measures the number of bits of information
gained about the class prediction by knowing the value of a given feature when
predicting the class [13,39]. To calculate the information gain of a given feature
X with respect to the class attribute Y, one must know the uncertainty about the
value of Y and the uncertainty about the value of Y when the value of X is known.
The former is measured by the entropy of Y, H.Y /, and the latter is measured by
the conditional entropy of Y given X, H.Y jX/. The entropy of Y (which consists
of classes Y1 and Y2) is given by:
H.Y / D 
k
X
iD1
P.Y D Yi/ log2.P.Y D Yi//
(13.2)
The conditional entropy of Y given X (consisting of values X1; X2; :::; Xr) is:
H.Y jX/ D 
r
X
j D1
P.X D Xj/H.Y jX D Xj/
(13.3)
The information gain of feature X is deﬁned as:
IG.X/ D H.Y /  H.Y jX/
(13.4)
Thus, the level of a feature’s signiﬁcance is determined by how great is the
decrease in entropy of the class when considered with the corresponding feature
individually.
3: Gain Ratio (GR) is a reﬁnement to Information Gain. While IG favors features
that have a large number of values, GR’s approach is to maximize the feature’s

354
W. Altidor et al.
information gain while minimizing the number of its values [23,38]. The intrinsic
value of attribute X is given as follows:
IV.X/ D 
r
X
iD1
.jXij=N/log.jXij=N/
(13.5)
where jXij is the number of instances where attribute X takes the value of Xi,
r is the number of distinct values of X, and N is the total number of instances
in the dataset. The gain ratio of X is thus deﬁned as the information gain of X
divided by its intrinsic value:
GR.X/ D IG.X/=IV.X/
(13.6)
4: ReliefF (RF) is a feature selection method which, for a given instance, I0,
estimates the quality of a feature according to how the instance’s nearest hit, H
(one from the same class) differs from its nearest miss, M (one from a different
class) [17,18]. Relief’s estimate W ŒX of attribute X is obtained by:
W ŒX D P.different value of X j nearest instance from different class/
P.different value of X j nearest instance from same class/
D
N
X
iD1
diff.X; I0; M/
i
 diff.X; I0; H/
i

(13.7)
Where diff.X; I1; I2/ calculates the difference between the values of X for both
instances I1 and I2.
ReliefF, which is an extension of the Relief algorithm that can handle
noise and multiclass datasets, ﬁnds one near miss, M.B/ for each different
class instead of one miss M from a single opposite class and averages their
contribution for updating estimates W ŒX. The average is weighted with the prior
probability of each class, P.B/ [17]:
W ŒX D
N
X
iD1
2
4
X
B¤class.X/
P.B/  diff.X; I0; M/
i

 diff.X; I0; H/
i
3
5 (13.8)
RF is implemented in the Weka data mining tool [38] with the “weight nearest
neighbors by their distance” parameter set to false.
5: ReliefF-W (RFW) is similar to ReliefF, except that in ReliefF-W, the “weight
nearest neighbors by their distance” parameter is set to true.
6: Symmetric Uncertainty (SU) is a correlation measure and is calculated as
follows [38]:

13
Ensemble Feature Ranking Methods for Data Intensive Computing Applications
355
SU D 2  H.X/ C H.Y /  H.X; Y /
H.X/ C H.Y /
D 2  H.Y /  H.Y jX/
H.X/ C H.Y /
(13.9)
where H.X/ and H.Y / are the entropies based on the probability associated with
each feature and class value respectively and H.X; Y / is the joint probability of
all combinations of values of X and Y .
3.2
Threshold-Based Feature Selection Techniques
For the 11 TBFS techniques, each independent feature’s relevance is evaluated using
a performance metric on a reduced dataset containing the independent feature paired
with the dependent one. Note that none of the TBFS techniques uses a classiﬁer in
determining feature relevancy. To determine the relevancy of a feature, the feature’s
values are normalized between 0 and 1, and the normalized values are treated as
posterior probabilities in each performance metric calculation. Thus, feature Xj is
transformed to OXj via:
Xj 7! OXj D
Xj  min.Xj/
max.Xj /  min.Xj /
(13.10)
Each performance measure is calculated at various points of the posterior proba-
bility, OXj . Accordingly, at each threshold t 2 Œ0; 1, the performance metrics True
Positive (TPR), True Negative (TNR), False Positive (FPR), and False Negative
(FNR) rates are obtained. As an example, the formula for TPR.t/ is given as follows:
TPR.t/ D
ˇˇˇfx 2 Dj. OXj.x/ > t/ \ .c.x/ D P /g
ˇˇˇ
jfx 2 Djc.x/ D P gj
(13.11)
where P indicates the positive class. In a similar fashion, TNR.t/, FPR.t/, and
FNR.t/ can be calculated. The TBFS techniques (originally described by Van Hulse
et al. [35]) are listed below.
1: F-Measure (FM) is a single measure that combines both precision and recall.
In particular, FM is the harmonic mean of precision and recall. Using a tunable
parameter ˇ to indicate the relative importance of precision and recall, it is
calculated as follows [38]:
FM D
max
t2Œ0;1
.1  ˇ2/R.t/PRE.t/
ˇ2.R.t/ C PRE.t//
(13.12)

356
W. Altidor et al.
where R.t/ and P.t/ are Recall and Precision at threshold t, respectively.
Recall, R.t/, is equivalent to TPR(t) while Precision, PRE.t/, represents the
proportion of positive predictions that are truly positive at each threshold
t 2 Œ0; 1. More precisely, PRE.t/ is deﬁned as the number of positive instances
with OXj > t divided by the total number of instances with OXj > t.
2: Odds Ratio (OR) is a measure used to describe the strength of association be-
tween an independent variable and the dependent variable [11]. It is deﬁned as:
OR D
max
t2Œ0;1
TP.t/  TN.t/
FP.t/  FN.t/
(13.13)
where TP.t/ and TN.t/ represent the number of true positives and true negatives
at threshold t, respectively while FP.t/ and FN.t/ represent the number of false
positives and false negatives at threshold t, respectively.
3: Power (Pow) is a measure that avoids false positive cases while giving stronger
preference for positive cases [11]. It is deﬁned as:
Pow D
max
t2Œ0;1 ..1  FPR.t//k  .1  TPR.t//k/
(13.14)
where k D 5.
4: Probability Ratio (PR) is the sample estimate probability of the feature given
the positive class divided by the sample estimate probability of the feature given
the negative class [11].
PR D
max
t2Œ0;1
TPR.t/
FPR.t/
(13.15)
5: Gini Index (GI) is derived from a decision tree construction process where
a score is used as a splitting criterion to grow the tree along a particular
branch [7]. It measures the impurity of each feature towards categorization,
and it is obtained by:
GI D
min
t2Œ0;1 Œ2P.t/.1  P.t// C 2NPV.t/.1  NPV.t//
(13.16)
where NPV.t/, the negative predicted value at threshold t, is the percentage of
examples predicted to be negative that are actually negative. GI of a feature is
thus the minimum at all decision thresholds t 2 Œ0; 1.
6: Mutual Information (MI) computes the mutual information criterion with
respect to the number of times a feature value and a class co-occur, the
feature value occurs without the class, and the class occurs without the feature
value [6,21,39]. The mutual information is deﬁned as:
MI D
max
t2Œ0;1
X
Oyt 2fP;Ng
X
y2fP;Ng
p. Oyt; y/ log p. Oyt; y/
p. Oyt/p.y/
(13.17)

13
Ensemble Feature Ranking Methods for Data Intensive Computing Applications
357
where y.x/ is the actual class of instance x, Oyt.x/ is the predicted class based
on the value of the attribute Xj at a threshold t,
p. Oyt D ˛; y D ˇ/ D
ˇˇˇfxj. OXj.x/ D ˛/ \ .y.x/ D ˇ/g
ˇˇˇ
jP j C jNj
;
p. Oyt D ˛/ D jfxjy.x/ D ˛gj
jP j C jN j
;
p.y D ˛/ D jfxjy.x/ D ˛gj
jP j C jN j
;
˛; ˇ 2 fP; N g:
Note that the class (actual or predicted) can be either positive (P) or nega-
tive (N).
7: Kolmogorov-Smirnov Statistic (KS) measures a feature’s relevancy by dividing
the data into clusters based on the class and compares the distribution of
that particular attribute among the clusters [9]. It is effectively the maximum
difference between the curves generated by the true positive and false positive
rates (TPR(t) and FPR(t)) as the decision threshold changes from 0 to 1, and its
formula is given as follows:
KS D max
t2Œ0;1 jTPR.t/  FPR.t/j :
(13.18)
8: Deviance (Dev) is the minimum residual sum of squares based on a threshold t.
It measures the sum of the squared errors from the mean class given a
partitioning of the space based on the threshold t [36]. It is deﬁned as:
Dev D
min
t2Œ0;1
hX
.v.x/  v.St//2 C
X
.v.x/  v. NSt//2i
(13.19)
where St D fxj OXj > tg, NSt D fxj OXj  tg, v.St/ D jStj1 P v.x/,
v. NSt/ D j NStj
1 P v.x/, and v.x/ D 1 if instance x belongs to the positive
class, otherwise, v.x/ D 0.
9: Geometric Mean (GM) is a single-value performance measure obtained by
calculating the square root of the product of the true positive rate, TPR.t/, and
the true negative rate, TNR.t/ [31]. GM ranges from 0 to 1, with a value of 1
attributed to the feature that is perfectly correlated to the class.
GM D
max
t2Œ0;1
p
TPR.t/  TNR.t/
(13.20)
Thus, a feature’s predictive power is determined by the maximum value of
GM as different GM values are obtained, one at each value of the normalized
attribute range.

358
W. Altidor et al.
10: Area Under ROC (AUC), the area under the receiver operating characteristic
curve, is a single-value measure based on statistical decision theory and
was developed for the analysis of electronic signal detection. It is the result
of plotting TPR.t/ against FPR.t/ [10, 31]. In this research, ROC is used
to determine each feature’s predictive power. ROC curves are generated by
varying the decision threshold t used to transform the normalized attribute
values into a predicted class. That is, as the threshold for the normalized
attribute varies from 0 to 1, the true positive and false positive rates are
calculated.
11: Area Under PRC (PRC), the area under the precision-recall characteristic curve,
is a single-value measure depicting the trade-off between precision and recall.
It is the result of plotting TPR.t/ against precision, P.t/ [31]. Its value ranges
from 0 to 1, with 1 denoting a feature with highest predictive power. The PRC
curve is generated by varying the decision threshold t from 0 to 1 and plotting
the recall (y-axis) and precision (x-axis) at each point in a similar manner to
the ROC curve.
3.3
Ensemble Feature Ranking Techniques
There are effectively three steps in an ensemble of feature ranking techniques.
The ﬁrst is to determine the ensemble components (the individual techniques that
will form the ensemble). Second is to select the aggregate function (the method
that will be used to transform the rankings obtained in the ﬁrst step into one
result). This is also referred to as the combination method. There are three types
of combination methods: fusion-based, selection-based, and hybrid [29]. Fusion-
based methods make use of the results of all individual components to come up
with the ﬁnal outcome. Selection-based methods, on the other hand, choose one of
the components as the ﬁnal outcome. In a hybrid-based combination method, the
ﬁnal outcome is obtained after both selection and fusion methods have been used.
The third step is to identify the operands for, or the parameters of, the combination
methods. For ensembles of ranking techniques, the combination method can use
either the relevancy scores or the rank orders of the features in the calculation.
More formally, let us consider a dataset D D x1; x2; :::; xN with N instances
having M features each. Let EFR be an ensemble comprising of r feature
ranking techniques, EFR D fF1; F2; :::; Frg. Let T be the combination method
and assume that the parameters of the aggregate function are the rank orders of
the features according to each feature ranking technique, such that RankEFR D
T .Rank1; Rank2; :::; Rankr/. Let’s further assume that f j
i denotes the rank of feature
i from ranking j, such that the set of rankings for feature i is given by Si D
ff 1
i ; f 2
i ; :::; f r
i g. The new relevancy score of feature i is then obtained by:
Lfi D T .f 1
i ; f 2
i ; :::; f r
i /
(13.21)

13
Ensemble Feature Ranking Methods for Data Intensive Computing Applications
359
Table 13.1 Characteristics of the general ensembles
Aggregate
Name
Components
function
ESA
2, IG, GR, RF, RFW, SU
Average
ESM
2, IG, GR, RF, RFW, SU
Median
ETA
FM, OR, Pow, PR, GI, MI, KS, Dev, GM, AUC, PRC
Average
ETM
FM, OR, Pow, PR, GI, MI, KS, Dev, GM, AUC, PRC
Median
EAA
2, IG, GR, RF, RFW, SU, FM, OR, Pow, PR, GI,
Average
MI, KS, Dev, GM, AUC, PRC
EAM
2, IG, GR, RF, RFW, SU, FM, OR, Pow, PR, GI,
Median
MI, KS, Dev, GM, AUC, PRC
In this study, two aggregate functions are used, arithmetic mean (average) and
median. These aggregate functions are applied to the rank orders of the features.
Finally, the ranking for the assemble is obtained by arranging the features from the
lowest to the highest Lfi and assigning rank 1 to the feature with the lowest Lfi and
rank M to that with the highest Lfi.
3.3.1
General Ensembles
For these general ensembles, no signiﬁcant information beyond the type of the
feature ranking techniques is required in the selection of the components. We
consider three sets of ensemble components resulting in three different ensemble
sizes. In the ﬁrst one, all the standard ﬁlters are combined together (r D 6,
representing all six standard ﬁlter-based techniques). In the second one, all the
11 threshold-based techniques (r D 11) are combined. Finally, the third ensemble
contains all 17 ranking techniques (r D 17). Table 13.1 shows the characteristics of
the general ensembles used in this study.
3.3.2
Focused Ensembles
In the construction of our second group of ensembles, which are referred to as
focused ensembles, robustness of the ﬁlters is considered to determine which
techniques to use in the ensembles. With mean absolute error (MAE) and sum
of squared errors (SSE) (refer to [3] for usage of these measures) as robustness
measures, both AUC and Pow, which top the robustness chart, are selected. PRC
is then chosen from the next three stable threshold-based techniques, which are
similar in robustness measure. Next, considering both robustness and diversity, we
select PR and MI, which are also the top two moderately stable techniques. Finally,
the top two most robust standard ﬁlters, 2 and RF, are selected. On one hand,
we construct two ensembles with the ﬁve TBFS techniques (three stable plus two

360
W. Altidor et al.
Table 13.2 Characteristics of the focused ensembles
Name
Components
Aggregate function
F1A
AUC, Pow, PRC, PR, MI
Average
F1M
AUC, Pow, PRC, PR, MI
Median
F2A
AUC, Pow, PRC, PR, MI, 2, RF
Average
F2M
AUC, Pow, PRC, PR, MI, 2, RF
Median
Table 13.3 Dataset characteristics
Dataset
#Attributes
#Instances
%Positives
%Negatives
Lung cancer
12;534
181
17:1
82.9
Ovarian cancer
15;155
253
36:0
64.0
Liver cancer
122
166
47:0
53.0
Optdigits-8
37
6;435
9:7
90.3
Satimage-4
65
5;620
9:9
90.1
Internet advertisements
1;559
3;279
14:0
86.0
Musk
167
6;598
15:4
84.6
moderately stable TBFS techniques). On the other hand, we form an ensemble by
adding to these ﬁve TBFS techniques the two most stable standard ﬁlters. Table 13.2
shows the characteristics of the focused ensembles.
4
Experiments
This section provides the details related to this study’s experimental setup. The
experiments are conducted on seven real-world datasets, representing different
application domains (see Sect. 4.1 below). The procedure for injecting class noise
into the datasets is explained in Sect. 4.2. For the similarity-based comparisons,
Kendall Rank Correlations, as described in Sect. 4.3, are used.
4.1
Datasets
Table 13.3 lists the seven datasets used in this study and provides their character-
istics in terms of the total number of attributes, number of instances, percentage of
positive instances, and percentage of negative instances. They are all binary class
datasets. That is, for all the datasets, each instance is assigned one of two class
labels.
We consider three cancerous gene expression datasets: lung cancer, ovarian
cancer, and liver cancer. The Lung Cancer dataset is a classiﬁcation of malignant
pleural mesothelioma (MPM) vs. adenocarcinoma (ADCA) of the lung, and consists
of 181 tissue samples (31 MPM, 150 ADCA) [37]. The ovarian cancer dataset

13
Ensemble Feature Ranking Methods for Data Intensive Computing Applications
361
consists of proteomic spectra derived from analysis of serum to distinguish ovarian
cancer from non-cancer [22]. The liver cancer microarray dataset consists of 78
tumor samples and 88 normal samples. Also used in the experiments for this
study are two datasets from the domain of image recognition, Optidigits-8 and
Satimage-4. Both are from the UCI machine learning repository [4]. The Internet
Advertisements dataset comes from the UCI Machine Learning Repository [4] as
well. It contains 3,279 instances, representing images along with some keywords
embedded in web pages. The independent features consist of image dimensions,
phrases in the URL of the document or the image, and text occurring in or near the
image’s anchor tag while the dependent feature determines whether an image is an
advertisement (“ad”) or not (“noad”). Finally, the Musk data (also available from
the UCI Machine Learning Repository [4]) is a set of different conformations of
various molecules for predicting drug activity, i.e. whether new molecules will be
musks or non-musks.
As can be seen, these datasets represent different application domains and show
a good distribution of class distribution levels. They are also chosen because they
are relatively noise free (see Sect. 4.2). Injecting noise into datasets that are already
noisy is thereby avoided. This ensures the proper and effective assessment of the
different ranking techniques (see Sects. 3.1 and 3.2 below) with respect to their
robustness to class noise.
4.2
Noise Injection Mechanism
The seven datasets that are used in our experiments originated from different
application domains. Class noise is injected into all seven. Injecting class noise into
unprocessed real-world data can be problematic, given that real-world data may
already contain a relatively high level of class noise. No noise injection mechanism
should ignore the prior quality of the data. These speciﬁc datasets were chosen for
this study after models built with them as training datasets showed near-perfect
classiﬁcation. Note that other methods can be used to ensure that the datasets are
relatively clean prior to class noise injection. One such method is to obtain a subset
of the data from which models with near-perfect classiﬁcation are built.
For the noise injection mechanism, the levels of class noise are regulated by
two noise parameters. The ﬁrst parameter, denoted ˛ (˛ D 10; 20; 30; 40; 50%),
is used to determine the overall class noise level in the data. Precisely, ˛ is the
noise level relative to the number of instances belonging to the positive class. This
ensures that the positive class is not drastically impacted by the level of corruption,
especially if the data is highly imbalanced. The second parameter, denoted ˇ (ˇ D
0; 25; 50; 75; 100%), represents the percentage of class noise injected in the positive
instances. These parameters serve to ensure systematic control of the training data
corruption. Note that with ˇ D 50%, the class noise injection is identical to the
Proportional Random Corruption (PRC) procedure [41].

362
W. Altidor et al.
Given ˛ and ˇ, the cardinality of the set of randomly selected instances intended
for corruption is calculated as follows:
ST D 2  ˛  N P
(13.22)
where N P represents the number of positive instances in the dataset. From this,
the number of positive instances intended to be corrupted to the negative class is
obtained by:
SP D ST  ˇ
(13.23)
Likewise, the number of negative instances intended to be corrupted is obtained by:
SN D ST  .1  ˇ/
(13.24)
SP instances are randomly selected from the minority class and their member-
ship class is switched to the majority one. Similarly, SN instances are randomly
selected from the majority class and their membership is switched to the minority
one. For each noise corruption scheme, the noise injection process is performed 30
times on each dataset. That is, the random selection of the same number of instances
of each class (but not necessarily the same instances) are repeated for a total of 30
times, thereby obtaining 210 corrupted datasets for each noise corruption scheme.
Table 13.4 shows the noise injection details for the ovarian cancer dataset, which
contains 91 positive and 162 negative instances. The last row represents the noise
injection scheme where ˛ D 50% and ˇ D 75%. This results into a total of 91
(2  0:50  91) instances corrupted, from which 68 (91  0:75) randomly selected
positive instances change class label to negative and 23 (91  .1  0:75/) randomly
selected negative instances are changed to positive. Hence, a corrupted dataset with
46 positive instances and 207 negative instances is created. The noise injected to the
positive and negative classes amounts to 74:7% and 14:2% respectively. The total
number of noisy instances adds up to 91, which represent 36% of the total size.
After all the precessing of the noise injection scheme, 30 corrupted datasets with 46
positive instances and 207 negative instances are generated.
This corruption mechanism is the same as that reported by Van Hulse et al. [34].
With it, the total number of instances remains the same. The proportion of the
minority class changes for all values of ˇ, except 50%. Note that other noise
injection mechanisms can be used, and we recommend that future works consider
other methods.
4.3
Kendall Rank Correlations (KRC)
Kendall Rank Correlations [15] are used to assess the similarities among the
different rankings as proposed in [2]. For each pair of rankings, the KRC coefﬁcient
is computed to measure the degree of similarity between two sets of ranking

13
Ensemble Feature Ranking Methods for Data Intensive Computing Applications
363
Table 13.4 Ovarian cancer noise injection details
Noise injection
#
#
#
#
% Noisy
% Noisy
Total #
Total #
Total
Total %
˛(%)
ˇ(%)
P >N
N >P
Clean P
Clean N
Noisy P
Noisy N
P
N
P
N
noise
noise
10
0
0
18
91
144
0
18
0:0
11:2
109
144
18
7.2
25
5
13
86
149
5
13
5:5
8:1
99
154
18
7.2
50
9
9
82
153
9
9
9:9
5:7
91
162
18
7.2
75
14
4
77
158
14
4
15:4
2:6
81
172
18
7.2
100
18
0
73
162
18
0
19:8
0:1
73
180
18
7.2
20
0
0
36
91
126
0
36
0:0
22:5
127
126
36
14.4
25
9
27
82
135
9
27
9:9
16:9
109
144
36
14.4
50
18
18
73
144
18
18
19:8
11:4
91
162
36
14.4
75
27
9
64
153
27
9
29:7
5:8
73
180
36
14.4
100
36
0
55
162
36
0
39:6
0:2
55
198
36
14.4
30
0
0
55
91
107
0
55
0:0
33:7
146
107
55
21.6
25
14
41
77
121
14
41
15:4
25:1
118
135
55
21.6
50
27
28
64
134
27
28
29:7
17:0
92
161
55
21.6
75
41
14
50
148
41
14
45:1
8:4
64
189
55
21.6
100
55
0
36
162
55
0
60:4
0:2
36
217
55
21.6
40
0
0
73
91
89
0
73
0:0
44:9
164
89
73
28.8
25
18
55
73
107
18
55
19:8
33:8
128
125
73
28.8
50
36
37
55
125
36
37
39:6
22:7
92
161
73
28.8
75
55
18
36
144
55
18
60:4
11:0
54
199
73
28.8
100
73
0
18
162
73
0
80:2
0:1
18
235
73
28.8
50
0
0
91
91
71
0
91
0:0
56:2
182
71
91
36.0
25
23
68
68
94
23
68
25:3
42:0
136
117
91
36.0
50
46
45
45
117
46
45
50:5
27:8
90
163
91
36.0
75
68
23
23
139
68
23
74:7
14:2
46
207
91
36.0

364
W. Altidor et al.
Table 13.5 Strength of
correlations
Range
Strength
1.00–0.81
Very strong
0.80–0.61
Strong
0.60–0.41
Moderate
0.40–0.21
Weak
< 0:21
Very weak
obtained by two different techniques on the same set of features (or attributes). This
coefﬁcient is determined by the number of inversions of pairs of features which
would be needed to transform one ranked feature list into the other. Each ranked
feature list is represented by the set of all pairs of features where a value of 0 or 1 is
assigned to each pair if the order is discordant or concordant respectively. A function
can be deﬁned as follows:
Coefﬁcient D KRC.Ranki; Rankj /
(13.25)
where Ranki and Rankj denote the rankings obtained by two ranking techniques on
the same dataset. The KRC coefﬁcient ranges from 1 to C1, with 1 indicating
that one list is the exact reverse of the other list, a 0 indicates there is no relationship
between the two lists, and C1 indicating that both orders are identical. KRC is an
appropriate measure, given our desire to examine the relationships among ordered
lists obtained by different ranking techniques. It is used to compare the feature
ranking techniques in terms of the similarities or dissimilarities among their ranked
lists.
In our study, SAS [30] is used to calculate the KRC coefﬁcients, and the fol-
lowing scales (see Table 13.5) are used to determine the strength of the correlations
among the ordered lists. For example, a KRC value of 0:79 indicates a Very Weak
correlation while a KRC value of C0:79 indicates a Strong correlation (note that
different researchers may use different scales).
4.4
Setup
Two scenarios are considered in the experimental setup. The ﬁrst is referred to as
the clean scenario where the original datasets, prior to noise injection, are used to
obtain different ranked lists. The second is referred to as the noisy scenario where
the ranked lists are obtained from the corrupted datasets.
4.4.1
Clean Scenario
A ranked list for each ﬁlter-based feature ranking technique is obtained on each
clean dataset. A clean dataset, in this context, is any of the original seven datasets

13
Ensemble Feature Ranking Methods for Data Intensive Computing Applications
365
prior to noise injection. This leads to 17 ranked lists from each clean dataset, and
they are represented by Rankd
i , where d and i represent one of the original datasets
and one of the 17 feature ranking techniques respectively. The six ranked lists from
the standard ﬁlters are combined to form two general ensembles, one for each of
the aggregate functions (mean and median). Similarly, two general ensembles are
also formed with the eleven threshold-based feature selection techniques. Finally,
the last pair of general ensembles comprises all 17 ﬁlter-based ranking techniques.
In a similar fashion, we get the results from the focused ensembles by combining
the appropriate ranking techniques. Given ﬁve different ensemble sizes and two
different aggregate functions, ten ordered lists are obtained for each clean dataset as
follows:
Rankd
18 D Mean.Rankd
1 ; Rankd
2 ; :::; Rankd
6 /
Rankd
19 D Median.Rankd
1 ; Rankd
2 ; :::; Rankd
6 /
Rankd
20 D Mean.Rankd
7 ; Rankd
8 ; :::; Rankd
17/
Rankd
21 D Median.Rankd
7 ; Rankd
8 ; :::; Rankd
17/
Rankd
22 D Mean.Rankd
1 ; Rankd
2 ; :::; Rankd
17/
Rankd
23 D Median.Rankd
1 ; Rankd
2 ; :::; Rankd
17/
Rankd
24 D Mean.Rankd
9 ; Rankd
10; Rankd
12; Rankd
16; Rankd
17/
Rankd
25 D Median.Rankd
9 ; Rankd
10; Rankd
12; Rankd
16; Rankd
17/
Rankd
26 D Mean.Rankd
1 ; Rankd
4 ; Rankd
9 ; Rankd
10; Rankd
12; Rankd
16; Rankd
17/
Rankd
27 D Median.Rankd
1 ; Rankd
4 ; Rankd
9 ; Rankd
10; Rankd
12; Rankd
16; Rankd
17/
Using KRC, these ranked lists are assessed in terms of ranking similarity to
each other and to the individual ensemble components (and in some cases, to
individual non-componentranking techniques). The KRC coefﬁcients are calculated
as follows:
C d
i;j D KRC.Rankd
i ; Rankd
j /
(13.26)
where d 2 f1; 2; 3; :::; 7g represents a dataset, and i; j 2 f1; 2; :::; 27g represents a
feature ranking technique (including the ensembles). The average KRC coefﬁcient
over all seven datasets for each pair of ranking techniques is obtained as follows:
Ci;j D
P7
dD1 C d
i;j
7
(13.27)
4.4.2
Noisy Scenario
For the noisy scenario, we obtain a ranked list for each feature ranking technique on
each corrupted dataset and denote it Rankd
i;n, where n represents a noise injection

366
W. Altidor et al.
scheme, and d and i are as previously deﬁned. Subsequently, ten ranked lists, one
for each ensemble, are generated for each corrupted dataset. Given thirty corrupted
datasets for each corruption scheme from one original dataset, the average rank
order for each ensemble is calculated on all thirty ranked orders from the corrupted
datasets for each ensemble. Similar to the clean scenario, the ordered lists resulted
from the corrupted datasets are denoted by:
Rankd
18;n D Mean.Rankd
1;n; Rankd
2;n; :::; Rankd
6;n/
Rankd
19;n D Median.Rankd
1;n; Rankd
2;n; :::; Rankd
6;n/
Rankd
20;n D Mean.Rankd
7;n; Rankd
8;n; :::; Rankd
17;n/
Rankd
21;n D Median.Rankd
7;n; Rankd
8;n; :::; Rankd
17;n/
Rankd
22;n D Mean.Rankd
1;n; Rankd
2;n; :::; Rankd
17;n/
Rankd
23;n D Median.Rankd
1;n; Rankd
2;n; :::; Rankd
17;n/
Rankd
24;n D Mean.Rankd
9;n; Rankd
10;n; Rankd
12;n; Rankd
16;n; Rankd
17;n/
Rankd
25;n D Median.Rankd
9;n; Rankd
10;n; Rankd
12;n; Rankd
16;n; Rankd
17;n/
Rankd
26;n D Mean.Rankd
1;n; Rankd
4;n; Rankd
9;n; Rankd
10;n; Rankd
12;n; Rankd
16;n; Rankd
17;n/
Rankd
27;n D Median.Rankd
1;n; Rankd
4;n; Rankd
9;n; Rankd
10;n; Rankd
12;n; Rankd
16;n; Rankd
17;n/
We then measure the correlation between the ‘clean’ scenario and the ‘corrupted’
scenario associated with that particular noise injection scheme. Thus, for each noise
injection scheme and a feature ranking technique, the KRC coefﬁcients between the
‘clean’ scenario with each of the 24 noisy scenarios are measured by:
C d
i;n D KRC.Rankd
i ; Rankd
i;n/
(13.28)
where d and i are deﬁned as before, and n represents one of the 24 noise injection
schemes. We obtain a single correlation for each ranking technique and noise
injection scheme by taking the average of the correlations over all seven datasets:
Ci;n D
P7
dD1 C d
i;n
7
(13.29)
The ensemble feature ranking techniques are then evaluated in terms of their
robustness to class noise. More speciﬁcally, the robustness of each ensemble ranking
technique against class noise is assessed using the correlation measures for each
noisy scheme and the corresponding clean scenario.

13
Ensemble Feature Ranking Methods for Data Intensive Computing Applications
367
5
Results
This section presents the experimental results of our assessment of the ensemble
feature ranking techniques. For this assessment, we use the seven datasets as
described in Sect. 4.1, and the individual techniques comprising the different
ensembles as described in Sects. 3.1 and 3.2. We ﬁrst assess the ensembles in terms
of how they are correlated to one another and how similar they are compared to the
individual feature ranking techniques. Finally, we compare the ensembles in terms
of their robustness to class noise.
5.1
Ensembles’ Correlations
Table 13.6 shows the KRC values for all pairings of the ensemble techniques. These
results, in general, show a Very Strong or Strong correlation for the ensembles,
except among ESA and ESM. Table 13.7 shows the KRC correlations between the
general ensembles and the individual feature ranking techniques while Table 13.8
shows the KRC correlations between the focused ensembles and the individual
ranking techniques. In both Tables 13.7 and 13.8, a KRC value in bold face indicates
the comparison is between an ensemble and one of its components. From these
results, we can make the following observations:
•
As expected, the correlation strength between an ensemble and one of its
components generally tends to be higher than that between the same ensemble
and a non-component technique. More speciﬁcally, the standard-ﬁlter ensembles
are more correlated to the individual standard ﬁlters than they are to the
individual threshold-based ﬁlters. Likewise, the threshold-based ensembles are
more correlated to the individual threshold-based ﬁlters than they are to the
individual standard ﬁlters. Furthermore, the pairing of F1A with any of its
components produces a higher correlation than the pairing of F1A with either
Table 13.6 Ensembles’ correlations with one another
KRC
ESM
ETA
ETM
EAA
EAM
F1A
F1M
F2A
F2M
ESA
0.844
0.595
0.603
0.723
0.683
0.544
0.569
0.659
0.635
ESM
0.612
0.622
0.716
0.704
0.563
0.591
0.650
0.651
ETA
0.899
0.871
0.859
0.861
0.873
0.856
0.862
ETM
0.851
0.889
0.814
0.870
0.826
0.869
EAA
0.893
0.790
0.812
0.876
0.853
EAM
0.785
0.830
0.841
0.881
F1A
0.889
0.874
0.849
F1M
0.861
0.908
F2A
0.883

368
W. Altidor et al.
Table 13.7 General
ensembles’ correlations
with individual components
KRC
ESA
ESM
ETA
ETM
EAA
EAM
2
0.787
0.901
0.591
0.604
0.684
0.678
GR
0.706
0.813
0.559
0.557
0.644
0.626
IG
0.800
0.904
0.597
0.610
0.691
0.682
RF
0.537
0.406
0.378
0.391
0.449
0.426
RFW
0.534
0.403
0.358
0.374
0.433
0.410
SU
0.774
0.888
0.615
0.617
0.705
0.689
FM
0.484
0.468
0.658
0.702
0.644
0.671
OR
0.437
0.455
0.500
0.471
0.505
0.476
Pow
0.434
0.462
0.651
0.633
0.607
0.616
PR
0.376
0.401
0.541
0.500
0.508
0.498
GI
0.083
0.098
0.139
0.083
0.122
0.089
MI
0.520
0.525
0.698
0.726
0.696
0.708
KS
0.584
0.593
0.812
0.874
0.794
0.837
Dev
0.622
0.652
0.841
0.875
0.832
0.859
GM
0.532
0.540
0.758
0.794
0.724
0.756
AUC
0.495
0.505
0.754
0.788
0.703
0.738
PRC
0.583
0.604
0.874
0.874
0.824
0.841
Table 13.8 Focused
ensembles’ correlations
with individual components
KRC
F1A
F1M
F2A
F2M
AUC
0.721
0.752
0.696
0.743
Pow
0.750
0.733
0.691
0.700
PR
0.578
0.566
0.558
0.543
PRC
0.859
0.916
0.851
0.891
MI
0.671
0.688
0.686
0.689
2
0.538
0.570
0.622
0.628
RF
0.346
0.365
0.447
0.419
2 or RF (see Table 13.8). Similarly, F2A has a higher correlation with 2 and
RF, and for the most part a lower correlation with AUC, Pow, PR, PRC, MI
than F1A.
•
The correlation strength is impacted by the type (TBFS vs standard) of techniques
comprising the ensemble feature ranking. The ‘purer’ is the ensemble (i.e., the
more techniques of the same type that comprise the ensemble), the better is
its correlation strength with its individual component. For instance, the KRC
coefﬁcient between a standard ﬁlter ensemble and any individual standard
ﬁlter is higher than that between the ‘all’ ensemble and that same individual
standard ﬁlter, which in turn is higher than the KRC coefﬁcient between the
corresponding threshold-based ensemble and the same individual standard ﬁlter
(see Table 13.7). In the same way, the KRC between F1M and AUC for example
is higher than that between F2M and AUC (see Table 13.8).
•
Whether a ranking technique is a component of the ensemble is a better
indicator of the correlation strength than the ensemble size. For example, ESM is
comprised of six components while ETM has 11 components, yet the pairing of
ESM with any of its components yields a stronger KRC than the pairing of ETM
with any of the ESM components.

13
Ensemble Feature Ranking Methods for Data Intensive Computing Applications
369
Table 13.9 Types of
correlation strength according
to the ensembles
(a) General
Strength
ESA
ESM
ETA
ETM
EAA
EAM
Very strong
0
4
3
3
2
3
Strong
5
1
6
6
9
9
Moderate
10
8
5
5
5
3
Weak
1
3
2
2
0
1
Very weak
1
1
1
1
1
1
(b) Focused
Strength
F1A
F1M
F2A
F2M
Very strong
1
1
1
1
Strong
3
3
4
4
Moderate
2
2
2
2
Weak
1
1
0
0
Very weak
0
0
0
0
•
Considering together the aggregate function and the type of ﬁlters comprising the
general ensembles, the standard ﬁlter ensemble with median generally registers
higher correlations with standard ﬁlters, except for RF and RFW. In general,
the threshold-based ﬁlter ensemble with median registers higher correlations
with threshold-based ﬁlters, except for OR, Pow, PR, and GI. For the all-
ﬁlter ensembles, the results are practically split, with EAA registering higher
correlations for nine (all six standard ﬁlters and three of the threshold-based ones)
out of the 17 individual ﬁlters.
Characterizing how diverse the ensembles are from the individual components,
Table 13.9 shows the characteristics of both the general and focused ensembles
in terms of the strength of their relationships with the associated components.
Conversely, Table 13.10 summarizes the strength of the individual ﬁlters to the
ensembles. The results conﬁrm the close relationship between the ensembles and
the individual components. The results also show that the ordered lists from GI and
RFW are the farthest apart from those of the general ensembles, and the most diverse
individual ranking technique from the focused ensembles is RF.
5.2
Ensembles’ Robustness
Figures 13.1 and 13.2 provide the results for the general and focused ensembles
respectively. They show the Kendall Tau correlations between the ‘corrupted’ and
the ‘clean’ scenarios for each ensemble. That is, for each noise injection scheme
and a given ensemble, we measure the correlation between the ‘clean’ scenario
and the ’corrupted’ scenario associated with that particular noise injection scheme.
Both groups of ensembles show that the ensembles and the individual components
share some common characteristics (as reported in [3]). With the ensembles as well

370
W. Altidor et al.
Table 13.10 Types of correlation strength according to the individual components
(a) General with standard
Strength
2
GR
IG
RF
RFW
SU
Very strong
1
1
1
0
0
1
Strong
3
3
3
0
0
5
Moderate
2
2
2
3
2
0
Weak
0
0
0
3
4
0
Very weak
0
0
0
0
0
0
(b) General with TBFS
Strength
FM
OR
Pow
PRC
GI
MI
KS
Dev
GM
AUC
PRC
Very strong
0
0
0
0
0
0
3
4
0
0
4
Strong
4
0
3
0
0
4
1
2
4
4
0
Moderate
2
6
3
4
0
2
2
0
2
2
2
Weak
0
0
0
2
0
0
0
0
0
0
0
Very weak
0
0
0
0
6
0
0
0
0
0
0
(c) Focused
Strength
AUC
Pow
PR
PRC
MI
2
RF
Very strong
0
0
0
4
0
0
0
Strong
4
4
0
0
4
2
0
Moderate
0
0
4
0
0
2
2
Weak
0
0
0
0
0
0
2
Very weak
0
0
0
0
0
0
0
as with their individual components, an increase in noise level has a decreasing
correlation for a ﬁxed noise distribution while an increase in noise distribution
has varied impacts. Similarly, the range in correlations between the lowest and the
highest noise level increases as the noise distribution increases.
The results also show that an ensemble robustness can be predicted from
knowledge of the individual components. On one hand, the ensembles comprising
solely of the standard ﬁlters are very unstable. This implies that the combination
of non-robust techniques just makes a non-robust ensemble. On the other hand, the
most stable ensembles are those comprising solely of TBFS techniques, and they
are more stable than the ensembles consisting of ﬁlters of mixed types (standard and
threshold based ﬁlters together). Consequently, the stability of an ensemble of non-
robust techniques is more likely to be enhanced with the addition of more robust
techniques. Conversely, the stability of an ensemble of more robust techniques is
more likely to degrade with the addition of non-robust techniques. We can observe
that the 17-member ensemble (ensemble of both standard and threshold-based
techniques) is in general more robust than the ensemble of standard ﬁlters but less
stable than that of threshold-based ﬁlters. In general the ensembles with the standard
ﬁlters perform poorly compared to the individual TBFS techniques (except for OR
and GI) in terms of robustness to class noise.
Furthermore, the ensembles show different levels of ﬂuctuation as the noise
injection scheme varies. The variation observed in the correlations as the noise
injection schemes vary is measured by taking the average difference between an

13
Ensemble Feature Ranking Methods for Data Intensive Computing Applications
371
Fig. 13.1 KRC with respect to noise injection scheme for the general ensembles
Fig. 13.2 KRC with respect to noise injection scheme for the focused ensembles

372
W. Altidor et al.
Table 13.11 Robustness measures
Ranking
Combined
Lung
Ovarian
Liver
Internet
techniques
robustness
cancer
cancer
cancer
Optdigits-8
Satimage-4
Ad
Musk
F1A
0.123
0.144
0.155
0.252
0.170
0.053
0.049
0.040
ETM
0.127
0.161
0.171
0.282
0.131
0.063
0.047
0.037
ETA
0.133
0.146
0.174
0.299
0.132
0.084
0.054
0.043
F1M
0.136
0.145
0.163
0.286
0.188
0.058
0.069
0.046
F2M
0.152
0.173
0.189
0.340
0.148
0.102
0.068
0.043
F2A
0.157
0.186
0.204
0.335
0.149
0.088
0.095
0.043
EAM
0.173
0.229
0.235
0.377
0.157
0.108
0.061
0.044
EAA
0.185
0.236
0.267
0.399
0.169
0.104
0.083
0.040
ESM
0.355
0.492
0.586
0.590
0.370
0.215
0.141
0.094
ESA
0.358
0.483
0.524
0.554
0.367
0.235
0.243
0.103
identical order comparison and the pairwise similarity comparisons of the clean
scenario and the different noise injection schemes. This is similar to the robustness
measure reported in [26]. It is essentially the mean absolute error (MAE), which is
obtained by:
P24
nD1 1  Ci;n
24
(13.30)
Table 13.11 shows shows the robustness measure for each ensemble. The
robustness measures are given in terms of MAE for the combined datasets as well
as the individual datasets, and the ranking techniques are ranked based on the
combined results (shown in bold font) from the most to the least robust. Clearly, the
11-member ensembles (ensembles of TBFS techniques) are the most robust among
them. They are better than the six-member ensembles (ensembles of standard ﬁlters)
and the 17-member ensembles.
Thus, whether an ensemble feature ranking is as robust as the individual
components depends on the types of the individual components. This study suggests
that not all ensemble feature selection, which consists of different feature ranking
techniques, can result in a more robust method. Careful attention must be paid to
the choice of the individual components in building an ensemble feature ranking.
5.3
ANalysis Of VAriance (ANOVA)
Here, a pair (one for the clean scenario and the other for the noisy one) of three-
way ANOVA tests is performed to compare the two choices for aggregate function.
For the clean scenario, the KRC values obtained when comparing the ensembles
with the individual ﬁlters are considered for each aggregate function. The three
factors include IRT for the individual ranking techniques, in which all 17 ﬁlters
are considered, Factor ERT (ensemble ranking techniques), which include ﬁve

13
Ensemble Feature Ranking Methods for Data Intensive Computing Applications
373
Table 13.12 ANOVA table for the aggregate function comparison – Clean scenario
Sum
Percentage
Degrees
Mean
Component
of squares
of variation (%)
of freedom
square
F-computed
F-table
Total
5.9967
100.0
169
0.0355
IRT
4.8602
81.0
16
0.3038
43.45
1.52
ERT
0.0954
1.6
4
0.0238
3.41
1.98
AF
0.0064
0.1
1
0.0064
0.91
2.74
Residual
1.0348
17.3
148
0.0070
Residual standard deviation D 0.0836
Table 13.13 ANOVA Table for the aggregate function comparison – Noisy scenario
Sum
Percentage
Degrees
Mean
Component
of squares
of variation (%)
of freedom
square
F-computed
F-table
Total
4.2722
100.0
239
0.0179
NS
2.2884
53.6
23
0.0995
88.93
1.43
ERT
1.7473
40.9
4
0.4368
390.43
1.97
AF
0.0004
0.0
1
0.0004
0.39
2.73
Residual
0.2361
5.5
211
0.0011
Residual standard deviation D 0.0334
different ensembles, and Factor AF (aggregate function) contains two levels. The
ANOVA for the aggregate function comparison on the clean scenario is shown in
Table 13.12. Meanwhile, the ANOVA for the aggregate function comparison on the
noisy scenario is shown in Table 13.13. For the noisy scenario, three factors are
used. However, IRT is replaced with NS (representing the noise scheme with 24
levels), and Factors ERT and AF are as previously deﬁned. Thus, for each aggregate
function, the KRC values obtained when comparing the ensembles with each noise
injection scheme are considered.
The F-test is used to check whether the allocation of variation by Factor AF
is signiﬁcantly higher than that of the residuals. The signiﬁcance of this variation
(whether AF has a signiﬁcant effect on the response, KRC) is determined by
comparing its contribution to the variation with that of the residuals. For both
scenarios, AF is assumed to explain no signiﬁcant fraction of the KRC variation,
given the value from the F-table is greater than its computed F for both scenarios.
On the other hand, the results show that the other two factors (IRT and ERT for
the clean scenario and NS and ERT for the noisy scenario) explain a signiﬁcant
fraction of the KRC variation. This implies that the aggregate function median is
not signiﬁcantly better than the mean. The results also validate our observation that
the key to the ensembles’ robustness is the individual components that comprise
these ensembles.

374
W. Altidor et al.
6
Conclusion
In our empirical study, we examine two types of ensemble feature rankings: general
ensembles (13.6) and focused ensembles (13.4). The ensembles’s components are
drawn from 17 ﬁlter-based feature ranking techniques, of which 11 are threshold-
based and six are standard ﬁlters. We use seven datasets from different domain
applications, with different dimensions and different levels of class imbalance.
We compare both the general and focused ensembles in terms of their similarity
to one another and to the individual components, using Kendall Tau correlations.
We empirically show that the correlation strength is impacted by the type of the
techniques comprising the ensemble feature ranking, and the correlation strength
between an ensemble and one of its components generally tends to be better than
that between the same ensemble and a non-component technique. We also assess
the ensembles with respect to their robustness to class noise.
The results also show that an ensemble’s robustness can be predicted from
knowledge of the individual components. The ensemble’s robustness is greatly
impacted by the robustness of the individual components. The greater the percentage
of robust techniques in the ensemble, the more robust is the ensemble. Conversely,
the greater the percentage of unstable techniques in the ensemble, the less robust
is the ensemble. This further implies that the addition of unstable techniques to
an ensemble of robust techniques renders the ensemble less robust. Likewise, the
robustness of an ensemble of unstable techniques is enhanced with the addition of a
robust component.
The underlying hypothesis is that the size of the ensemble has less to do with its
robustness, and the robustness of the individual components is more indicative of the
robustness of the ensemble. If an ensemble’s robustness is desirable, it takes more
than just combining multiple feature ranking techniques together. More attention
should be paid to the ensemble’s components. The combination of standard ﬁlters
yields the worst results in terms of robustness to class noise.
Given that the classiﬁcation performance resulting from a reduced feature space
was not the object of this study, future work could consider such an analysis.
Different noise injection mechanisms could also be used in future work.
References
1. T. Abeel, T. Helleputte, Y. Van de Peer, P. Dupont, and Y. Saeys. Robust biomarker
identiﬁcation for cancer diagnosis with ensemble feature selection methods. Bioinformatics,
26(3):392–398, February 2010.
2. W. Altidor, T. M. Khoshgoftaar, and J. Van Hulse. An empirical study on wrapper-based feature
ranking. In Proceedings of the 21st IEEE International Conference on Tools with Artiﬁcial
Intelligence, ICTAI ’09, pages 75–82, Newark (New York Metropolitan Area), New Jersey,
USA, 2009.
3. W. Altidor, T. M. Khoshgoftaar, and J. Van Hulse. Impact of class noise on ﬁlter-based feature
ranking. Department of Computer and Electrical Engineering and Computer Science, Florida
Atlantic Univerisy, 2010. Technical Report.

13
Ensemble Feature Ranking Methods for Data Intensive Computing Applications
375
4. A. Asuncion and D. Newman. UCI machine learning repository [http://www.ics.uci.
edu/mlearn/MLRepository.html]. University of California, Irvine, School of Information and
Computer Sciences, 2007.
5. M. Attik. Using ensemble feature selection approach in selecting subset with relevant features.
In Proceedings of the 3rd International Symposium on Neural Networks, ISNN ’06, pages
1359–1366, Chengdu, China, 2006.
6. R. Battiti. Using mutual information for selecting features in supervised neural net learning.
IEEE Transactions on Neural Networks, 5:537–550, July 1994.
7. L. Breiman, J. Friedman, C. J. Stone, and R. A. Olshen. Classiﬁcation and Regression Trees.
Chapman & Hall/CRC, January 1984.
8. L. Daza and E. Acuna. Feature selection based on a data quality measure. In Proceedings of
the World Congress on Engineering - Vol II, WCE ’08, pages 1095–1099, London, U.K., 2008.
9. D. J. Dittman, T. M. Khoshgoftaar, R. Wald, and J. Van Hulse. Comparative analysis of DNA
microarray data through the use of feature selection techniques. In Proceedings of the Ninth
IEEE International Conference on Machine Learning and Applications, ICMLA ’10, pages
147–152, Washington, DC, USA, December 2010. IEEE Computer Society.
10. T. Fawcett. ROC graphs: Notes and practical considerations for data mining researchers. HPL-
2003-4, Intelligent Enterprise Technologies Lab, 2003.
11. G. Forman, I. Guyon, and A. Elisseeff. An extensive empirical study of feature selection
metrics for text classiﬁcation. Journal of Machine Learning Research, 3:1289–1305, 2003.
12. I. Guyon and A. Elisseeff. An introduction to variable and feature selection. Journal of Machine
Learning Research, 3:1157–1182, 2003.
13. M. A. Hall and G. Holmes. Benchmarking attribute selection techniques for discrete class data
mining. IEEE Transactions on Knowledge and Data Engineering, 15(6):1437–1447, 2003.
14. K. Jong, J. Mary, A. Cornu´ejols, E. Marchiori, and M. Sebag. Ensemble feature ranking.
Lecture Notes In Computer Science, pages 267–278, 2004.
15. M. G. Kendall and J. D. Gibbons. Rank Correlation Methods. Oxford University Press, New
York, 5th edition, 1990.
16. J. L. Y. Koh, M.-L. Lee, W. Hsu, and K.-T. Lam. Correlation-based detection of attribute
outliers. In Proceedings of the 12th International Conference on Database Systems for
Advanced Applications, DASFAA ’07, pages 164–175, Berlin, Heidelberg, 2007. Springer-
Verlag.
17. I. Kononenko. Estimating attributes: Analysis and extensions of RELIEF. In Proceedings of the
European conference on Machine Learning, ECML ’94, pages 171–182, Secaucus, NJ, USA,
1994. Springer-Verlag.
18. H. Liu, J. Li, and L. Wong. A comparative study on feature selection and classiﬁcation methods
using gene expression proﬁles and proteomic patterns. Genome Informatics, 13:51–60, 2002.
19. H. Liu and R. Setiono. Chi2: Feature selection and discretization of numeric attributes. In
Proceedings of the Seventh International Conference on Tools with Artiﬁcial Intelligence,
ICTAI ’95, pages 388–391, 1995.
20. D. W. Opitz. Feature selection for ensembles. In Proceedings of the 16th National Conference
on Artiﬁcial Intelligence, AAAI ’99, pages 379–384. Press, 1999.
21. H. Peng, F. Long, and C. Ding. Feature selection based on mutual information: criteria of max-
dependency, max-relevance, and min-redundancy. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 27:1226–1238, 2005.
22. Petricoin, A. M. Ardekani, B. A. Hitt, P. J. Levine, V. A. Fusaro, S. M. Steinberg, G. B. Mills,
C. Simone, D. A. Fishman, E. C. Kohn, and L. A. Liotta. Use of proteomic patterns in serum
to identify ovarian cancer. The Lancet, 359(9306):572–577, February 2002.
23. J. R. Quinlan. C4.5: programs for machine learning. Morgan Kaufmann Publishers Inc., San
Francisco, CA, USA, 1993.
24. S. Ramaswamy, R. Rastogi, and K. Shim. Efﬁcient algorithms for mining outliers from large
data sets. Proceedings of the ACM SIGMOD Conference on Management of Data, 29(2):427–
438, 2000.

376
W. Altidor et al.
25. L. Rokach, B. Chizi, and O. Maimon. Feature selection by combining multiple methods. In
Advances in Web Intelligence and Data Mining, pages 295–304. Springer, 2006.
26. Y. Saeys, T. Abeel, and Y. de Peer. Towards robust feature selection techniques. In Proceedings
of Benelearn, pages 45–46, 2008.
27. Y. Saeys, T. Abeel, and Y. Van De Peer. Robust feature selection using ensemble feature
selection techniques. In Proceedings of the European conference on Machine Learning and
Knowledge Discovery in Databases - Part II, ECML PKDD ’08, pages 313–325, Berlin,
Heidelberg, 2008. Springer-Verlag.
28. Y. Saeys, I. Inza, and P. Larra˜naga. A review of feature selection techniques in bioinformatics.
Bioinformatics, 23(19):2507–2517, 2007.
29. L. E. A. Santana, D. F. de Oliveira, A. M. P. Canuto, and M. C. P. de Souto. A comparative
analysis of feature selection methods for ensembles with different combination methods. In
Proceedings of the 20th International Joint Conference on Neural Networks, IJCNN ’07, pages
643–648. IEEE, August 2007.
30. SAS Institute. SAS/STAT user’s guide. SAS Institute Inc., 2004.
31. N. Seliya, T. Khoshgoftaar, and J. Van Hulse. A study on the relationships of classiﬁer
performance metrics. In Proceedings of the 21st IEEE International Conference on Tools with
Artiﬁcial Intelligence, ICTAI ’09, pages 59–66, Washington, DC, USA, November 2009. IEEE
Computer Society.
32. A. Tsymbal, S. Puuronen, and D. Patterson. Ensemble feature selection with the simple
Bayesian classiﬁcation. Information Fusion, 4(2):87–100, 2003.
33. K. Tumer and N. C. Oza. Input decimated ensembles. Pattern Analysis and Applications,
6(1):65–77, 2003.
34. J. Van Hulse and T. Khoshgoftaar. Knowledge discovery from imbalanced and noisy data. Data
and Knowledge Engineering, 68(12):1513–1542, December 2009.
35. J. Van Hulse, T. M. Khoshgoftaar, A. Napolitano, and R. Wald. Threshold-based feature
selection techniques for high-dimensional bioinformatics data. Department of Computer and
Electrical Engineering and Computer Science, Florida Atlantic Univerisy, 2010. Technical
Report.
36. H. Wang, T. M. Khoshgoftaar, and J. Van Hulse. A comparative study of threshold-based
feature selection techniques. In Proceeding of the IEEE International Conference on Granular
Computing, pages 499–504, San Jose, CA, USA, August 2010. IEEE Computer Society.
37. X. Wang and O. Gotoh. Accurate molecular classiﬁcation of cancer using simple rules. BMC
Medical Genomics, 2(64), October 2009.
38. I. H. Witten and E. Frank. Data Mining: Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, 2nd edition, 2005.
39. Y. Yang and J. O. Pedersen. A comparative study on feature selection in text categorization.
In Proceedings of the 14th International Conference on Machine Learning, ICML ’97, pages
412–420, San Francisco, CA, US, 1997. Morgan Kaufmann Publishers.
40. X. Zhu and X. Wu. Class noise vs. attribute noise: a quantitative study of their impacts.
Artiﬁcial Intelligence Review, 22(3):177–210, 2004.
41. X. Zhu and X. Wu. Cost-guided class noise handling for effective cost-sensitive learning. In
ICDM ’04: Proceedings of the Fourth IEEE International Conference on Data Mining, pages
297–304, Washington, DC, USA, 2004. IEEE Computer Society.

Chapter 14
Record Linkage Methodology and Applications
Ling Qin Zhang
1
Introduction
As information technology advances rapidly and Internet blooms, a lot of business
tends to electronization and globalization. Individuals and organizations have more
channels or methods to expose information and gather information. The result is
that individuals and organizations face the increasing challenges to process the large
volumes of data and ﬁnd the relevant quality information to ﬁt their speciﬁc business
needs. In addition, the data gathered from multiple resources usually contains errors
and duplicate information. There is a strong need to detect duplicates and remove
them in data preparation phase before performing advanced data mining [1–4].
In other cases, data gathered from one data source is not enough to provide a
complete view about a person or entity. Therefore, data needs to be linked or
integrated together to provide a single complete view about a person, a product,
a object, a geographical area or any entity to meet a speciﬁc business application
need [5,6].
The above case can be illustrated by a scenario: Joe visited a family doctor one
day because of headache and visited a dermatologist at another day because of skin
problem. Joe also visits his dentist regularly. One day he was hospitalized because
of diagnosed lung cancer.
As Joe’s personal information is requested and documented at each of his doctor
visits, he left his personal records at multiple places at different times. In order
to correctly diagnose the real cause of his disease, the current doctor needs all of
his health history. But all the information is scattered around different databases.
Therefore integrating Joe’s illness history becomes very vital. Also it is possible that
Joe has left inconsistent personal information at different doctor visits because of
different patient registration form formats or human errors such as he may misspell
L.Q. Zhang
LexisNexis Risk Solutions, Boca Raton, FL, USA
e-mail: Ling.Zhang@lexisnexis.com
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 14, © Springer Science+Business Media, LLC 2011
377

378
L.Q. Zhang
his name or write a wrong birth date. It’s also possible that he has lived at different
addresses, but his personal identity does not change at all. How can we link all those
inconsistent pieces of information together and make sure all refer to the same Joe
with high accuracy? This is what record linkage is trying to do, linking all pieces of
records referring to a single person regardless of data discrepancies.
Another scenario is that a company has a customer database that includes
customer’s personal data, e.g. contact information, mailing address for billing
purposes. As the information is collected from different channels such as online
registration, phone calls or emails, some of the information is unavoidably repeated
in the database. For efﬁciency and accuracy, from time to time the company has to
remove the duplicate entries from database. Otherwise, it will increase the mailing
cost because of mailing the same address for more than one time.
The above pinpoints a need to aggregate distributed information from diverse
resources referring to the same entity, it also addresses the need to remove duplicates
from a single database for operational efﬁciency and improve data quality for
advanced data processing and analysis. Usually those records lack a universal
identiﬁer. Otherwise, a simple join operation will be an easier solution. Record
Linkage is a data mining technology in pursuit of the above solution. It identiﬁes
if two or more records refer to the same entity from more than one data source and
links them together or detects duplicate records in one ﬁle and removes the duplicate
from database or groups them as clusters.
In the early stages, record linkage was employed in health care sector and vital
statistics [7–9] for solving problems as mentioned in the ﬁrst scenario. In the current
stage, as information technology advances, Internet explosion speeds up information
distribution with added methods and devices. It results in more demands for record
linkage to aggregate distributed information referring to a unique world entity.
The integrated content can support organizations to develop proﬁt driven business
intelligence across diverse industries. Record linkage is also a commonly used data
mining tool for improving data quality in a single dataset by removing or grouping
duplicates. Section 2 provides a comprehensive summary of business applications
that use record linkage and the related business beneﬁts.
The term record linkage was ﬁrst used by Halbert L. Dunn of the United States
National Bureau of Statistics for solving people identiﬁcation in US public health
records, when he gave a talk in Canada in 1946 and he described: Each person in the
world creates a Book of Life. This Book starts with birth and ends with death. Its
pages are made up of the records of the principal events in life. Record linkage is the
name given to the process of assembling the pages of this book into a volume [7].
Record Linkage has gone through over seven decades since it was ﬁrst introduced
in 1946 by Dr. Dunn [7]. During decades, there are variant deﬁnitions of record
linkage but the core remains same. Section 3 provides a general deﬁnition about
record linkage.
In 1959, Computerized Record Linkage was ﬁrst undertaken by the Canadian
geneticist Howard Newcombe [8] and he recognized the full implications of extend-
ing the principle to the arrangement of personal ﬁles and into family histories [10].

14
Record Linkage Methodology and Applications
379
The basic probability linking method was introduced by Newcombe and Kennedy
in 1962 [9] and Fellegi and Sunter in 1969 developed a formal mathematical theory,
the classical probabilistic record linkage approach [11]. Later Winkler extended and
enhanced the probability model [12,67,69]. Section 3 introduces a commonly used
notation from Fellegi and Sunter.
After the probability linking method had been prospered over two decades,
researchers started to review record linkage problem as a pattern classiﬁcation
problem and they applied machine learning methods for record linking such as unsu-
pervised clustering method [13–15] and supervised learning method, Support Vector
Machine (SVM) [2,16], regression [5] or hybrid methods of both unsupervised and
supervised learning method [17]. LexisNexis SALT linking technology, introduced
in Chap. 8, is a combination of probability based method and clustering method
when it’s used for detecting duplicate records, referred as Internal Linking [18] and
it’s also a pure probability based method when it performs content aggregation,
referred as External Linking [19].
Broadly speaking, there are four types of record linkage techniques, determin-
istic, probabilistic method, modern approach [3] and their combination like SALT
[18]. Section 4 reviews some of the commonly used methods.
As regards the probability linking framework, a generalized linkage system was
ﬁrst developed in 1980 at Statistics Canada [20]. Since then more record linkage
systems have been developed for research purpose or commercial uses such as Febrl
[21] and TAILOR [22]. Section 5 outlines the main components of a typical linking
system. Section 6 addresses challenges that a record linkage system faces, especially
when handling large volume of datasets. Section 6.1 outlines the chapter summary
and conclusion.
2
Business Beneﬁt and Value
When Dr. Dunn, the chief of US National Ofﬁce of Vital Statistics, ﬁrst used record
linkage to create a book of life for a person, record linkage was used in medical
and health care for supporting disease research and improving treatment by linking
together patient’s records collected from multiple resources [7,20,23]. Later on, the
demand for record linkage was expanded in pharmaceutical companies for medicine
researches [24].
Up to now, as data are scattered across many different resources and are
heterogeneity [25], record linkage has been used in business to improve data quality
and enrich content. Enterprise applications such as Master Data Management,
Customer Data Integration and data warehouse project use record linkage in data
cleaning process by removing duplicate data entries. It’s also used for content
integration. Record linkage has demonstrated tremendous business value and will
continue to demonstrate its value in the future. Following is a summary of its
business beneﬁts to organizations.

380
L.Q. Zhang
2.1
Customer Relationship Management (CRM)
Customer Relationship Management (CRM) is a strategy used by companies for
managing customer relationships in an organized way. It aims to ﬁnd, attract,
and win new clients and retain existing customers as well as reduce the costs of
marketing and client service.
For supporting CRM program, a company needs a customer database to develop
a customer centric management system. The database provides a 360ı view of cus-
tomers by consolidating the existence of many representations of a single individual
into only one representation and it also describes relationships in sufﬁcient details
so that management, salespeople or other persons can directly access to information,
match customer needs with product plans and offerings, remind customers of
service requirements, know what other products a customer has purchased and
so forth.
Record linkage is used to integrate customer data and build knowledge vistas
around customers [26]. For example, in order to effectively manage claims,
insurance companies integrate their clients’ records registered in multiple regis-
tration ofﬁces or different insurance companies; for saving customer mailing cost,
manufacturers usually remove redundant customer contact records.
Marketing
In a corporation, marketing department can identify and target their best customers,
manage marketing campaigns and generate quality leads for the sales team. A key
marketing capability is to track and monitor multichannel campaigns including
phone, email or direct mail, online search and social media. Metrics should be built
from the multiple channels and monitored from responses, clicks, leads, deals, and
revenue. Companies can save costs associated with direct mail by means of mailing
only one promotion to any given household.
Collaboration
In large companies, different departments tend to function with little collaboration
and sometimes they interact with same customers. Integrating information shared by
multiple departments or employees can help an organization to improve cooperation
among sales, service, and marketing through streamlining processes in accordance
with the integrated customer data. For example, a feedback from the customer
support department can enlighten marketers about speciﬁc services and product
features that clients are asking for. Representatives, in their turn, will be able to
pursue these opportunities without the burden of re-entering records and contact
data into a separate sale force automation system.

14
Record Linkage Methodology and Applications
381
2.2
Fraud Detection, Law Enforcement, Government
Administration
Fraud can happen at anywhere and anytime, when electric transaction increases,
the fraud rate has increased tremendously. Record linkage can be used to detect the
duplicate transactions or users as compared to previously known proﬁles.
The Financial Crimes Enforcement Network (FINCEN) AI system (FAIS) [27] is
used to identify potential money laundering by linking and evaluating all reports of
large cash transactions for indications of suspicious activity characteristic of money
laundering. Its objective is to identify previously unknown, potentially high value
transaction that leads to follow-up investigation and prosecution if warranted (Wall
Street Journal 1993).
In an accounting system, linking is used to detect duplicate payments that save
millions of dollars. Department of Veterans Affairs’ ﬁnancial audit discovered two
duplicate payments, amounting to $1.08 million; Medicaid identiﬁed about $10
million in duplicate payments in a 2-year audit period, and estimated the actual
amount to be around $31 million [28–30]. At the same time, name matching is also
used to law enforcement and counter-terrorism [31].
For supporting government administration, government agencies use record
linkage to identify people who register for assistance or welfare beneﬁt multiple
times, or who collect unemployment beneﬁts despite being employed [16]. Security
agencies often require fast access to ﬁles of a particular individual in order to solve
crimes or to prevent terror threats through early intervention [32].
2.3
Inventory and Cost Management
Record linkage is widely used to improve supply chain and inventory efﬁciency
by matching inventory in a warehouse that is physically identical, but seemingly
unconnected in databases, so that the company can lower carrying costs. There are
fewer inventories to store, less space to rent, lower insurance and taxes on inventory,
and lower costs to physically count inventory as well as lower risk from obsolete
inventory. Chemtracker [33] is a system built with record linkage for Chemical
Inventory Management System.
When inventory data are clean, buyers can have more accurate information on
the amount purchased from any given vendor and they can apply pressure on the
vendor to lower costs.
2.4
Content Enrichment and Intelligence
Information of individuals and organizations is usually scattered in diverse content
such as text based articles of news, word documents or structured content in an

382
L.Q. Zhang
authority database and the information is sometimes partly overlapped. Record
linkage is used to aggregate the information from heterogeneous sources and build
a complete proﬁle of an individual, an organization or any entity and then they are
further contributed to develop intelligent business products.
For example, legal professionals like attorneys and judges, usually have limited
information when presented in legal case law documents. Those documents may
only include their names or employer information. However, end users usually want
more information about an attorney such as education background and the cases
they have handled before, etc. Without an ISLN in legal document, it’s hard to pull
other important information from attorney authority database. In such case, there is
a need to link an attorney in legal cases to the corresponding entries in an attorney
authority directory so that end users can access to a complete proﬁle of an attorney
with a single access to a legal document.
Record linkage is used by legal information providers to enrich their content by
matching an attorney in legal cases to their legal professional authority database.
Once a match is found, the attorney’s ISLN will be pulled and used as a unique
identiﬁer to mark up an attorney’s names in case law documents. The presence of
such an identiﬁer will help customer quickly and easily access to an attorney’s
additional information online and build attorney’s professional proﬁles and add
them to authority directory [5,6].
2.5
Search Result Improvement
When using a search system, sometimes we found there are duplicates in the
answers and it seriously affects the quality of a search system. Record linkage has
been used to remove duplicates in online search applications [14] and shorten end
user time for information seeking.
In digital libraries, it is estimated there is about 50% duplicates in citations,
linking is used to remove the duplicates and improve work efﬁciency [34].
The above is not a complete list about record linkage usage in business and its
beneﬁt to business. For example, record linkage is also used to clean data errors
and remove duplicates when creating a data warehouse or before performing any
knowledge discovery process [1, 24]. It’s also used in Geocoding for applications
such as market segmentation, demographics, geo-spatial distribution of plants, sales
territories [35]. Even record linkage is used for government election and politics,
where it links donors who made more than one donations in an election cycle and
improves the study of campaign contributors [36].
The business beneﬁt record linkage can bring to organizations continues to grow
because there are many potential business applications waiting to explore.

14
Record Linkage Methodology and Applications
383
3
Deﬁnition and Notation
The term record linkage was originated in the public health area when it was ﬁrst
used by Halbert Dunn for linking a person’s medical record to create a “book of
life” [7], the records of individual patients that were brought together by using
name, date-of-birth and other information. Winkler described Record linkage is the
methodology of bringing together corresponding records from two or more ﬁles or
ﬁnding duplicates within the same ﬁle [1], while more often record linkage was
described as a process that brings two or more records relating to the same record
from more than one ﬁle together.
Since then, the progress in technology and science has made computer systems
incorporate sophisticated ideas from computer science, statistics, and operation
research, many variant of deﬁnitions has been given depending how and where
it’s used in industries, business and applications. However the two major functions
have kept unchanged: identifying and removing duplication records in one dataset;
matching same records in multiple datasets and linking them together. The ﬁrst is to
focus on improving data quality, while the second aims to content integration and
enrichment.
In summary, Record linkage (RL) is a task that identiﬁes if multiple records are
the same one when they are presented in one or more data sources. It includes a
decision method to decide if multiple records are same record and a process that
applies the method in matching and links the same records together.
Record Linkage is called Record Matching because performing record linkage
needs a matching process to recognize the same records, it’s also called Record
Linking due to the fact that the goal of ﬁnding the same records in multiple data
sources is not to delete one or more of those records, instead, in business, it is usually
to link those records together by creating a unique link key that is used to represent
a cluster of similar records. When record linkage is used to resolve a record identity
by matching the record to an underlining authority database with unique identiﬁers
and tie a unique identiﬁer to the external record, record linkage is also called Entity
Resolution.
Since Felligi and Sunter [11] deﬁned a formal mathematical notation, it is widely
used by the record linkage community. For convenience, the standard notation is
described below and it’s referenced through the following part of the chapter.
Two record sets A and B, the record pair sets is deﬁned as AxB D f.a; b/I a 2
A; b 2 Bg, where a; b are records with k ﬁelds and they are represented in
Rk space. The set is a union of two disjoint sets, the match pairs M and
non-matches U .
M D f.a; b/ 2 AxBj a D bg
(14.1)
U D f.a; b/ 2 AxBj a ¤ bg
(14.2)

384
L.Q. Zhang
A comparison vector of any two records from A and B, is denoted as .a; b/ that
deﬁnes a comparison space denoted as  , that includes all comparison features.
  .a; b/ D f1.a; b/; 2.a; b/; : : : i.a; b/; : : : K.a; b/g
(14.3)
Each i deﬁnes a comparison function that compares the value of a record pair (a, b)
of the ith ﬁeld. Examples of simple comparison function like
i.a; b/ D
 1 if ai D bi
0 if ai ¤ bi
(14.4)
i.a; b/ D
8
<
:
1 if ai D bi
0 if ai ¤ bi
1 if either value is missing
(14.5)
Formula (14.4) deﬁnes a binary comparison function with binary values in the
result and formula (14.5) deﬁnes a category comparison function. Both of them
produce category values. There are other types of comparison function that produces
continuous values like edit distance or cosine distance functions, etc. Those
functions are introduced in Sect. 5.3.
Once the comparison space  has established, the next goal is to determine a
record pair .a; b/ is a match or a non-match according to the comparison vector
value .a; b/ by applying a decision method, also referred to as a decision model
that will be introduced in Sect. 4.
4
Record Linkage Methods and Decision Models
As mentioned earlier in Sect. 1, broadly speaking, there are four types of record
linkage techniques, deterministic, probabilistic method, modern approach [16]
and their combinations such as LexisNexis SALT linking approach for external
linking
(Chap.
8.3.4)
and
they
are
further
discussed
below
in
details
respectively.
4.1
Deterministic Linkage
Deterministic record linkage is also called a rule based linking method as it
usually employs a set of rules for exact match or partial match results between
corresponding ﬁelds in record pairs. The rules may be in a decision table format,
a hierarchy of tree, or a variety of different sets of matching criteria. This approach

14
Record Linkage Methodology and Applications
385
is usually used when reliable unique identiﬁers exist in records such as people’s
social security number or account numbers and they are used in a join-operation.
When Dr. Halbert L. Dunn ﬁrst introduced the record linkage term, he advocated
the use of a unique identiﬁer like birth registration number to do people linking [7].
But usually the unique identiﬁer is neither available nor in fact stable or trust worthy.
In such case, a set of business rules are developed and applied in the linking process
e.g. the rule can be hand-coded like same SSN, or birth date match and soundex of
ﬁrst name and last name matches. A linking key is created by using a combination
of more than one attribute.
However, the business rules are usually complex and hard to tune and maintain
[37]. In addition, the persons who develop the business rules have to be proﬁcient
with the datasets and rules. Research shows that non-deterministic linkage such as
probabilistic method performs better than deterministic rule based approach [38].
Therefore it’s widely used and became the dominant record linkage method.
4.2
Probabilistic Linkage Model
The probability based linking frame work was established by Fellegi and Sunter
in 1969 [11] by extending Newcombe and Kennedy’s prior work in 1967. It uses
Bayesian decision model to develop matching rules from a well labeled training
dataset. The method is summarized as follows:
In a comparison space  deﬁned in Sect. 3, with the probability model, for each
comparison vector j; i
j D 1 if ﬁeld i of the record pair matches, otherwise the
value is 0.
As ” can be considered as a random event, the match and non-match probabilities
are deﬁned as:
m.O D P..j.a; b/m.mM /
(14.6)
u.O D P ..ja; b/m.mU /
(14.7)
For a speciﬁc value of ”, the goal is to decide a pair .a; b/, according to ”, as A1, a
match, A3 a non-match, or A2, a possible match by Felligi and Sunter.
A linking rule is applied to help make the three decisions and it deﬁnes a mapping
from  to the three decisions that assign a probability denoted as
d./ D fP.A1j/; P.A3j/; P.A2j/g;
P3
iD1 P.Aij/ D 1
(14.8)
With the above framework, three types of methods have been explored by re-
searchers to formulate a decision method: Error based, EM and Cost based
probability method [39]. Since Error and EM based methods are widely used

386
L.Q. Zhang
in linking systems such as Canada Record Linkage system [20], Febrl [21], and
TAILOR [22], their introduction are provided below.
4.2.1
Error Based
For each linkage rule, there are two types of error associated with: classify an
actually not match pair as a match and classify an actually match as a non-matched.
The ﬁrst type error is statistically referred to as Type I Error, the second as Type
II Error. They are also called Precision error and Recall error respectively according
to Information Retrieval (IR) concept.
Felligi and Sunter proved mathematically, that there exists an optimal linkage
rule that minimizes the probability that labels a record pair with A2 for a given
desired precision and recall error rate. A ratio deﬁned in ﬁnding such a decision
function,
l.”j/ D m.”j/
u.”j / ; for all ”j;
(14.9)
m.”j/ D p.”j jM / and .”j / D p.”j jU /
(14.10)
For a comparison vector ”j, randomly draw from  , the goal is to label a record pair
.a; b/ as M D A1 or U D A2 [ A3. The probability rule is
.a; b/ 2
(
M if p Mj j
 > p U j j
 ;
U otherwise
(14.11)
With Bayes’ theorem, the rule can be rewritten as
.a; b/ 2
(
M if l

j

> p.U/
p.M/;
U otherwise
(14.12)
The ratio p.U/
p.M/ is the threshold value for a decision. Equation (14.12) is referred to as
Bayes’ test for minimum error and it can be only useful when there exists a labeled
dataset to calculate p.M /; p.U /, and the distribution of p.”jjM / and p.”j jU / are
known. However they are usually not known [40].
Another approach is to compute p.j jM / and p.jjU / by using Na¨ıve Bayes’
method in assuming that record ﬁelds’ independence holds.
For each j , deﬁne mi and i as the probability of the ith ﬁeld of j agrees given
it’s a match and non-match pair respectively,
mi D p

”i
jjM

and i D p

”i
jjU

(14.13)

14
Record Linkage Methodology and Applications
387
With the assumption that ﬁelds of ”j are independent of each other, the following
can be derived
p.j jM / D
K
Y
iD1
m
i
j
i .1  mi/1i
j
(14.14)
p.jjU / D
K
Y
iD1
u
i
j
i .1  ui/1i
j
(14.15)
The likelihood is transformed as logarithms of the ratio, wi D log2

mi
i

when
i
j D 1 and wi is the ith ﬁeld weight. With (14.12) and (14.13), formula (14.9) is
rewritten as
l.”j/ D
Y mi
i
(14.16)
w.O D w1 C w2 C : : : C wk, is called as record weight, or composite weight The
ratio deﬁned by (14.9) is sorted in decreasing, it proved that there is existing n and
n’ so that
T D l.n/ and T D l.n0/ when n < n0
(14.17)
Or in their logarithm form as
T D log2 l.n/ and T D log2 l.n0/
(14.18)
For each comparison vector ”j, a decision rule can be formulated according to T
and T as
If w./ > T;  is a match .A1/
If w./  T and ./  T; ” is a possible match .A2/;
If w./ < T;  is a non-match .A3/
(14.19)
As T and T can be estimated by minimizing the probability of the error that
makes a correct match of a record pair, the method is therefore called as error
based method. Felligi and Sunter proposed two approaches to calculate the weight
for each ”, one is based on prior knowledge of comparison pairs in M and U .
Those record pairs are labeled as matches or non-matches from a training dataset,
another approach is to use probability estimation based on random sampling ” in a
match space.
Under the probability frame work, LexisNexis SALT linking technology employs
an error based method in computing a record ﬁeld value weight (also referred to as
speciﬁcity) deﬁned in (12.1). In implementation, SALT uses Inversed Document

388
L.Q. Zhang
Frequency (IDF), a concept used in Information Retrieval (IR), as an approximation
of wi or log2

mi
i

that can be interpreted in theorem as follows.
In the weight calculation Method I introduced by Felligi and Sunter [11], as mi
and ui can be approximated by (14.20), wi can be approximated by (14.21)
mi Š PA\B

i
j

and i Š PA

i
j

PB

i
j

(14.20)
wi Š log2 PA\B

i
j
.
PA

i
j

PB

i
j

(14.21)
When dataset A and B are same,
wi D log2

1=PA

i
j

(14.22)
(14.22) is the IDF of the ith ﬁeld value! The approximation makes the implemen-
tation much more efﬁciency, SALT can calculate the ﬁeld value speciﬁcities in
advance and store them as indexed ﬁles on HPCC hence, therefore it boosts run
time efﬁciency since the speciﬁcities are not calculated at run time in a matching
process, they are only accessed at run time instead.
4.2.2
EM Based
EM method was proposed by Winkler in 1988 [41] for solving record linkage
problem based on likelihood estimators. Given the probability model deﬁned by
Felligi and Sunter, EM method is the very right method for estimating m./; u./
and p, the proportion of matched pairs in M.
The process includes two steps iteratively, Expectation .E/ and Maximization
.M /, it starts with an initial guess about < m; u; p >.
In E step, it estimates P.M jj/ and P.U jj/ by
P.M jj/ D
p  p.jjM /
p  p.jjM / C .1  p/  p.j jU /;
(14.23)
P.U jj/ D
.1  p/  p.jjU /
p  p.jjM / C .1  p/  p.j jU /;
(14.24)
where p.j jM / and p.jjU / are deﬁned in (14.14) and (14.15)
In the M step, mi; ui and p are estimated by
mi D
NP
j D1
p.M jj/i
j
NP
j D1
p.M jj/
;
(14.25)

14
Record Linkage Methodology and Applications
389
ui D
NP
j D1
p.U jj/

1  i
j

NP
j D1
p.U jj/
;
(14.26)
p D
NP
j D1
p.M jj/
N
(14.27)
The iteration continues until the required p is reached. With this method, there is no
need for training data that will save a lot of time.
4.3
Modern Approach
As classical probabilistic linkage relies on degrees of agreement of a record pair,
it requires that the comparison results of each ﬁeld are nominal data types like
agreement, disagreement, unknown, etc. With the approach, ﬁrst there is a need
to predeﬁne all the possible value categories for each ﬁeld. However, the similarity
measure of two records, and their corresponding ﬁelds are usually numerical data
types resulted from a package of mathematic comparison functions or they are
adjusted by weight [9, 10, 18, 42, 64, 65] that measures the speciﬁc probability of
a ﬁeld value according to their relative frequency in a dataset. When comparison
vector ” is weighted, it’s usually referred to as a weight vector.
In order to use the probabilistic method, match thresholds for each ﬁeld
should be trained and further are used as decision criteria to label a ﬁeld as an
agreement, disagreement or missing, etc., depending on the comparison result of
two corresponding ﬁelds of two records. In this way, a record with numerical value
ﬁelds has to be transferred into a pattern based record consists of ﬁelds’ value in
nominal data type.
In order to overcome the weakness of probabilistic linking method, researchers
from data mining communities view record linkage as a pattern classiﬁcation prob-
lem and they employed widely used machine learning methods for classiﬁcation
such as Decision tree [13,22] and Support Vector Machine (SVM) [16,65] for record
pair classiﬁcation. Additive Logistic Regression [43], a statistical interpretation
of boosting based learning has been used by LexisNexis for matching legal
professionals extracted from legal documents to a master authority database [5] and
it’s introduced in Sect. 4.3.1. These methods are proved to achieve better linkage
result.
Furthermore, in order to overcome the disadvantage of relying on existing
training data that are usually hard to create or it takes much more human efforts,
ﬁrst, non-supervised machine learning method, clustering, is used such as K-Means

390
L.Q. Zhang
(KM), Farthest-First (FF) and other clustering methods [2, 13, 15] or a hybrid
method is employed [44, 66] where class labels are assigned by clustering method
and then the result dataset is used as a training set to train supervised classiﬁers.
Second, active learning has been explored by researchers [45], basically the method
iteratively adds representative samples from unclassiﬁed data to a training dataset
with manual review. In this way, review of less than 100 training examples can
provide better results than from over thousands. Third, two-steps method [2, 16] is
recently explored by Christen. The method starts with initializing a small number
training samples, then iteratively adds more classiﬁed record pairs into training
datasets and it’s introduced in more details in Sect. 4.3.2.
4.3.1
Additive Logistic Regression
Additive Logistic Regression [43] is a statistical interpretation of boosting based
learning algorithm, developed by Friedman. The algorithm from [43] is described
below in details, Algorithm 3, an adaptive Newton algorithm for ﬁtting an additive
logistic regression model, where ” represents a record pair vector from comparison
space and K is the number of ﬁelds used for matching process.
1. Start with weights wi D 1=K; i D 1; 2; : : :; K; F./ D 0 and probability
estimates p.i/ D 1=2
2. Repeat for m D 1; 2; : : :; M :
(a) Compute the working response and weights
zi D
y
i  p.i/
p.i/.1  p.i//
wi D p.i/.1  p.i//
(14.28)
(b) Fit the function fm(”) by a weak learning method of zi to ”i using weights
wi
(c) Update F./  F./ C 1=2fm./ and
p./  
eF./
eF./ C eF./
3. Output the classiﬁer
signŒF./ D sign
XM
mD1 fm./

(14.29)
Algorithm 1. An adaptive Newton algorithm for ﬁtting an additive logistic regres-
sion model

14
Record Linkage Methodology and Applications
391
The algorithm, in step 3, outputs a decision function, to help make match
decisions in a matching process. As described in Algorithm 3, the method requires a
weak machine learning method in 2-(b), it can be any supervised machine learning
method.
In the implementation [5], three weak learning methods are chosen in (b):
Decision Stump, Simple Linear Regression and General Linear Regression.
Decision Stump [46,47] does classiﬁcation according to class probability distri-
bution. It chooses a single ﬁeld with minimum entropy conditioned on rows of a
contingency table. The entropy calculated by
m
X
iD0
0
@pi
k
X
j D0
.pij log2 pij/
1
A; where pi D
k
X
j D0
pij
	
m
X
iD0
k
X
j D0
pij
(14.30)
With the Algorithm 3, at each round of step 2, a best split point is found from a ﬁeld
by minimizing the entropy deﬁned as above.
Simple Linear Regression is used by choosing the best ﬁeld that can result in
minimum residual loss at each round of the iteration.
General Weighted Linear Regression uses, instead of one ﬁeld, more ﬁelds in
each round to construct a linear function which results in minimum residual loss.
During the record matching process, it takes the classiﬁer trained from the
learning Algorithm 3 on a training dataset and a record pair to make the matching
decisions. For each record vector ”, the matching process ﬁts the classiﬁer,
sign
hPM
mD1 fm./
i
, if the sign is positive, it is a match, otherwise, it’s a non-match.
The method can produce good match results because it can recognize rare match
patterns at each iteration and it reweights the record pairs that are classiﬁed wrong
in previous iteration to reach a more accurate decision.
4.3.2
Two Step Methods for Unsupervised Record Linkage
In order to overcome the challenge that lacks of training dataset, Christen [2, 16]
introduced a two-step method, the ﬁrst step is to automatically choose a small
number of initially trained weight vectors with high quality, the second step is to
use it to train a supervised learning classiﬁer. The classiﬁer can be learned from
any supervised machine learning method though Christen has only experimented
k-nearest neighbor and Support Vector Machine (SVM) methods. The approach is
illustrated in Fig. 14.2 below by using SVM as an example. SVM is proved to be a
good method for handling both complex patterns of data and high dimensional data.
The method is also robust to a dataset that is full of noise.
With the approach, the ﬁeld value of each comparison vector ” is assumed to
fall in the range [0, 1]. Value 1 represents for the exact matches and 0 for exact
non-matches. Suppose K ﬁelds are chosen for matching, give a weight vector

392
L.Q. Zhang
m that represents an exact match and a weight vector nthat represents an exact
non-match:
m D .m1; m2; : : : mK/ and mj D 1:0 for 1 <D j <D K;
(14.31)
n D .n1; n2; : : : nK/ and nj D 0 for 1 <D j <D K;
(14.32)
In Step1, the initial training examples are chosen based on the fact that weight
vectors generated from record comparison process are strongly biased to non-
matches and two records have high similarity value when they are referred to the
same record and the two records have very low similarity value when they are not
refer to the same record. Based on the above, training samples are chosen from high
likelihood weight vectors for matches and low likelihood for non-matches.
Christen [2, 16] introduced two methods for choosing training samples, one
is Nearest-neighbor based method, another is threshold based method. Here the
Nearest-neighbor based method is described as an example.
The weight vectors from  that are the closest to m are selected into WM, and
the closest to n are selected into WU , the training dataset is initialized by:
WM D fj	; k	/ WM W dist.m; j/ < dist.m; k/g;
(14.33)
WU D fj	; k	/ WU W dist.j ; n/ < dist.k; n/g;
(14.34)
Where, dist(i; j) deﬁnes a distance function and xm D jWMj and xn D jWU j,
represents the total number of vectors in the match and non-match sets respectively.
Because the number of true matches is much less than the number of true non-
matches, the training samples selected should reﬂect the point. Usually xm and xn
are chosen according to an estimate ratio, r and it is calculated by the number of
records in two datasets jAj and jBj, and the number of weight vectors j j,
r D .min.jAj; jBj//=.j.j  min.jAj; jBj//
(14.35)
Once the initial training samples are created, Step 2 is to apply a machine learning
method to train a classiﬁer by taking the seeded training samples. Figure 14.1 below
introduces SVM method.
The number of iterations in the algorithm is decided by the values of ip and tp
set before. When they are set bigger, the algorithm takes less iterations, otherwise,
it takes more iterations. The method is seminal in the way that it actually achieves
the effect of a non-supervised learning method with a supervised learning approach
without any human interaction. The algorithm is showed to produce higher quality
matching result.

14
Record Linkage Methodology and Applications
393
Fig. 14.1 Seeded iterative SVM record linking
4.4
SALT Approach
LexisNexis SALT [18] linking technology is a combination of probability based
method and clustering method when used for Internal Linking, (Chap. 8.3.4) and
the clustering method employs a bottom-up approach.

394
L.Q. Zhang
First, the method assumes that record ﬁeld is independent of each other and a
ﬁeld value weight is computed by IDF (14.22), an approximation of wi in Felligi
and Sunter method, therefore, it is a probability based method in calculating a match
weight.
Second, the method does not perform one time linking in one run, instead, it
iteratively merges two most similar records at each run even if a record has found
more than one match over a predeﬁned threshold. The method starts with each
record in a dataset as an individual cluster, which means the number of clusters
is the number of records in a dataset at the beginning. At each iteration, it groups
matched records into one cluster by assigning a unique id and each cluster is
represented as a single entity in the next iteration no matter how many records in it.
SALT recalculates ﬁeld value speciﬁcities based on the number of records currently
available at each iteration. The linking process continues until it reaches predeﬁned
criteria.
In order to classify two records in one cluster, SALT uses a similarity function
deﬁned in (12.3) and the ﬁnal similarity is the sum of all matched ﬁelds’ value
speciﬁcities between two records (see detail in Chap. 8.3.3).
SALT approach is very innovative in combining probability based method and
clustering methods and it greatly boosts matching precision and recall.
5
Record Linkage System
A typical record linkage process comprises three steps: searching, record compar-
ison and decision making, matching & linking. The ﬁrst step is to search all the
potential match records for a given record; the second step is to generate weight
vectors by performing ﬁeld-wise comparison between the record and all its potential
match candidates; ﬁnally, the third step is to decide if each comparison vector
represents a match or non-match according to a chosen decision rule or model.
Figure 14.2 below illustrates a typical linking process in a record linkage system
employed by Febrl [21], AutoMatch [48], TAILOR [44] and SALT [18].
As a typical record linkage system serves for two main business objectives:
duplicates detection in one dataset and resolving entity identity of an external record
by matching to an underlining authority dataset. When serving for de-duplication,
SALT refers linking as internal linking, Data A and B in the Fig. 14.2 are the same
dataset but it may includes records from multiple data sources; when serving for
entity resolution, SALT refers the linking as external linking, where A and B is two
different datasets. In such case, one of the dataset is the base ﬁle or master dataset
with unique identiﬁer and another one does not include the unique identiﬁers.
The search process is performed based on indexing methods so that only
potentially matched records are pulled out for comparison. In such a way, the
number of comparisons is reduced, so is the time used for comparison. In the
comparison phase, weight vectors are produced and the dimension of the vectors
is determined by the number of ﬁelds predeﬁned for matching and they are passed
to the match process.

14
Record Linkage Methodology and Applications
395
Fig. 14.2 Linking process ﬂow chart
In the matching process, a decision model is applied and it decides if the weight
vector represents a match or non-match. A decision model may be some business
rules if deterministic method is applied, a threshold value if probabilistic method is
used, a classiﬁer if a supervised learning is applied and the classiﬁer has been trained
in advance, or a distance function if a non-supervised learning method, clustering,
is used. Those models have already been discussed in Sect. 4.
For successful linkage, data source A or B has to be cleaned or standardized
before performing record linking because the real world data are usually dirty. Data
pre-processing is important and necessary.
Incorporate with a typical linking process that consists of three steps, a typical
record linkage system usually includes six software components: Data Preprocess-
ing, Indexing or Blocking, Record Comparison, Matching and Linking plus Decision
Model Development and Result Evaluation. Their relationship is illustrated in the
following Fig. 14.3.
5.1
Data Pre-Processing
Data Pre-processing aims to understand data and ensure that data is in good quality
before performing record linking. It includes data proﬁling and data cleaning and
standardization.
Proﬁling data helps understand data quality, data patterns and characteristics.
It provides information about missing values, data types, value ranges, and most
common ﬁeld values. In the proﬁling phase, mean, medium, or maximum and
minimum value of data ﬁelds can be calculated and their standard deviation or

396
L.Q. Zhang
Data Preprocessing
Profiling
Cleaning
Standardization
Indexing
Blocking
Record
Comparison
Matching &
Linking
Building
Decision Model
Decision Rule
or Classifier
Result
Evaluation
Fig. 14.3 Components of a record linkage system
variance in a population is calculated as well. Those statistics provide insights in
choosing right ﬁelds for matching and help set right strategies for creating index
keys or data blocks.
Data cleaning aims to transform different ﬁeld data types to one uniform data
type when data are from different sources and they use different data types of same
ﬁelds. It includes transforming data values into same ranges so that data integrity
is guaranteed. Data cleaning also performs dependency checking of different ﬁelds
such as statistical correlation when necessary.
Standardization, also called Normalization, is to convert raw data with different
format into well deﬁned format for quality linking so that data has a uniform
representation. Examples like person name can be presented as ﬁrst, middle, last
names and name sufﬁx. Those components may be in different orders or different
formats from raw data. The three components, ﬁrst, middle and last names, may be
in one ﬁeld; or part of them only has initials. For quality matching, there is a need
to convert them into a unique form. Other examples like addresses and company
names should also be standardized before matching. Basically there are two steps
in normalizing a composite component into one or more simple components, ﬁrst
it recognizes individual component and extracts each component and then converts
each component into a standard format. The method employed can be rule based or
machine learning based methods [5,42]. A comprehensive data transform methods
can be found in [49].
Data cleaning and standardization may be an optional component for a linkage
system if a linkage system is built in an enterprise data management system like
data warehouse, data may have been already cleaned and standardized during a data
fabrication process.

14
Record Linkage Methodology and Applications
397
5.2
Indexing or Blocking
Indexing or Blocking aims to reduce computation cost and improve matching
performance. It is a technique used to partition a big dataset into subsets or clusters
of records according to the similar features of record ﬁelds, e.g., the phonetic code
of the last name and gender code. Those ﬁelds usually have higher quality.
When linking two datasets A and B, conceptually, each record in dataset A is
compared to each record in dataset B to form record pairs of all possible com-
parisons, there are a total jAj  jBj comparisons. Usually the two datasets include
millions even billions of records that therefore demands very high computational
costs. With the pair-wise comparison, the time to execute grows exponentially as
data volume grows linearly. However, the true matches for each record in A may be
limited to only one in B.
No matter what decision model is used for matching, the task for pair-wise
comparison is challenging because it demands resources of both time and memory
space to perform the comparison. In order to reduce the number of comparison
between two datasets, indexing or blocking method is employed to ﬁlter out non-
potential match candidates for each record in A from B so that only potential match
records are pulled out for comparison [2,16,25,40,50].
Blocking is typically a procedure that divides a dataset on disk into exclusive sub
sets or blocks under the assumption that no matches occur across different blocks so
that similar records are grouped together and matching records are ended up in one
block. Blocking can be implemented by sorting records according to a block key
[51]. In such case, only records in the same block are fetched and compared, that
greatly reduces comparison space and increases the speed of a comparison process.
A block key can be formed by using a function of soundex or metaphone of
ﬁelds or a combination of one or more ﬁelds and those ﬁelds usually have higher
discrimination power like ﬁrst name or date of birth. A commonly used method to
implement blocking is to use hash function.
Sorted Neighborhood [52] is another commonly used blocking method. The
method includes three steps: ﬁrst a key is created for each record by one or more
ﬁelds; second, the records are sorted by those keys, third, a ﬁxed size of window
is moved through records and only the records that belong to the current window
are compared. The records are compared in such a way: suppose the window size
is w, for every new record that enters the window, it is compared with previous w-1
records. As usually no single key can be sufﬁcient to sort records in such a way that
all the potential match records can be compared, some of the match records may be
ended up missed. In order to overcome the problem, multipass strategy is suggested
and applied by deﬁning different sorting keys. The matching process executes more
than one run of Sorted Neighborhood method.
In contrast to the blocking method that partitions a dataset into exclusive subsets,
Canopies [53, 68] method is introduced by McCallum for speeding up duplicate
detection process. The method basically groups records into overlapping clusters
called canopies through a cheap comparison function. Later on the records that are

398
L.Q. Zhang
ended up on the same cluster are further compared with more expensive comparison
function that leads to higher quality matching result. The cheap function can be
string length function or tf-idf similarity, etc.
No matter what blocking method is applied, the design goal for blocking is to
optimize searching result for higher matching recall through maximizing the number
of potentially match records and minimizing the number of non-potentially match
records.
Maximizing the number of potentially match records can be measured by record
Pair Completeness (PC) [44], also referred as sensitivity, deﬁned as (14.36)
Pair Completeness P C D SM
NM
(14.36)
Where SM is the number of true match record pairs in the set of record pairs resulted
from searching phase by using a blocking method and NM is the total number of
true match record pairs. PC is actually a search recall.
Minimizing the number of non-potentially match records is measured by record
Reduction Ratio (RR) [44], which is deﬁned as (14.37)
Reduction Ratio
RR D 1  S
N
(14.37)
Where S is the number of record pairs produced by blocking method for comparison
and N is the total number of possible record pairs in the entire datasets, jAj  jBj
The overall quality of indexing can be measured by a F-Score, a combination of
the above two and it’s deﬁned as (14.38)
F-Score
F D 2.P C RR/
P C C RR
(14.38)
In designing indexes, high quality ﬁelds are usually identiﬁed from data proﬁling
phase and one ﬁeld or a combination of several ﬁelds can be selected and grouped
as indexing keys that split datasets into blocks. When the blocks are mutually
exclusive, a single pass search can fetch all the similar records from one block.
However, sometimes, a record may end up into more than one block when a record
has more than one key value from different ﬁelds’ combination. In such case, for a
given record, the searching process performs multiple passes fetching on more than
one block and then merge results together.
LexisNexis Linking technology SALT (Chap. 8) is implemented with optimized
indexing design based on data proﬁling and it chooses quality ﬁelds as index
keys. SALT blocking method may partition dataset into subsets with overlaps thus
SALT uses multiple passes for fetching and those fetches are run in parallels [54]
on HPCC platform and it therefore promises fast delivery and accurate matching
result.

14
Record Linkage Methodology and Applications
399
5.3
Record Comparison and Weight Vectors
After the searching process, that results in one or more potential match candidate
records for each record in A from B, the next step is to compare a record in A
with each candidate from B. As each record usually includes one or more ﬁelds for
comparison that makes the record comparison become ﬁeld-by-ﬁeld comparison
(a ﬁeld may be a combination of two or more ﬁelds, called a composite ﬁeld). The
ﬁelds are compared with similarity functions. Usually several different similarity
functions can be applied depending on record data type or a speciﬁc application
requirement.
For text or string ﬁelds, string comparison functions are applied. As a string
ﬁeld usually includes typographical variation like David and Dave, record link-
age requires effective string comparison functions that can handle typographical
variation well. Distance functions like edit, phonetic or typewriter distance [55]
perform well in handling typos. Other distance functions are widely used like
Levenshtein [56] distance that accounts for insertion, deletion and transposition.
The most popular one is Jaro/Winkler weighted string comparison [57–60] and
it’s similar to Levenshtein distance but it gives more weight for the characters
at the beginning. N-gram distance forms the set of all the substrings of length n
for each string and the distance is deﬁned as Euclidean 1-norm distance based
on the number of occurrence of a substring. N-grams have been extended to Q-
grams [56] for computing approximate string joins effectively. For comparing
or grouping names that sounds similar, Soundex Code function [61] is used. It
uses phonetic algorithm and encodes the ﬁrst vowel letter and consonants of a
string.
In order to handle names that are ﬂipped like ﬁrst name and last name, composite
ﬁeld can be formed as a ﬁeld for comparison such as, by combing ﬁrst name,
last name and even middle name, and a word vector based comparison function
can be used like cosine function [5, 62] or bag of words function implemented by
SALT [18].
No matter what comparison function is used, the comparison result for each ﬁeld
usually results in a numeric value. All the ﬁeld comparison values are represented
in a vector that indicates the similarity of two records.
In order to increase the discriminate power of record ﬁelds, the probabilities
that ﬁeld values are presented in the whole population [18], or in matched versus
non-matched datasets are calculated [9]. The former is referred to as ﬁeld value
Speciﬁcity and the later is referred as binit weight [9], coefﬁcient of speciﬁcity or
discriminating power [10]. When they are adjusted by ﬁeld similarity, they are
referred to as weight vectors (Sect. 4.3) or matching weights [2,18]. The technique
produces the higher match weight when a rare ﬁeld value agrees than a common
ﬁeld value agrees. For example, a person named Prezed will have much higher
weight than a person named John.
It needs sound consideration to choose right ﬁelds for matching and right method
for comparison according to speciﬁc application requirements and content speciﬁc

400
L.Q. Zhang
characteristics. Sometimes a speciﬁc comparison function or weighting method
has to be invented for producing high quality weight vectors that contributes to a
matching decision process.
5.4
Matching and Linking
Once the record comparison has done and weight vectors have been created, the
linkage process goes to Matching and Linking component that designates if a weight
vector represents a match or non-match. Once a match has found, the process links
the two records together by assigning a unique identiﬁer to them.
Matching is to make decisions whether two records refer to the same record
depending on a decision model and the weight vector. A weight vector represents
the closeness of two records at ﬁeld level. In matching phase, the probability based
decision model calculates a composite weight that indicates the degree of similarity
of two records and it uses the composite weight to classify if the record pair is a
match or non-match. Other decision models may incorporate each ﬁeld weight to
perform classiﬁcation such as decision tree, regression method or support vector
model that is discussed in Sect. 4.
Linking is to integrate or merge two records that refer to the same record as one
when they are matched. Usually it appends a unique identiﬁer from one data source
with it to another source without it. The linking process may also include enhancing
some other ﬁelds. For example, for ﬁelds are empty or not correct, it replaces them
with the mean or median of all accurate ﬁeld’ values.
5.5
Building Decision Model
The component is to develop a decision model by using method discussed in
Sect. 4. With supervised learning, model development is not at real time. Only
the classiﬁers that are developed from the component are used at real time in the
matching component. In developing a decision model, it involves labeling a training
dataset and testing dataset, and then iteratively building a model and validating it
until developed classiﬁer reaches a predeﬁned criteria, precision and recall. The
evaluation criteria are discussed in Result Evaluation, Sect. 5.6.
5.6
Result Evaluation
The Evaluation component aims to measure the overall quality of a record linkage
system that includes measuring the completeness of the system in the percentage

14
Record Linkage Methodology and Applications
401
Table 14.1 Confusion matrix f record pairs classiﬁcation
Labeled as match (M)
Labeled as non-match (U)
Actually match
True match
False non-match
True positive (TP)
False negative (FN)
Actually non-match
False match
True non-match
False positive (FP)
True negative (TN)
of record pairs that actually are matched over all the matching pairs in records and
the accuracy of the system in the percentage of correctly matched and non-matched
record pairs over all the record pairs.
Section 4.2 described that a typical record linkage system considers two types
of errors, Type I and Type II error. Type I error happens whenever it links together
two records that are not the same record (also referred to as false positive); Type II
error happens whenever it misses linking together two records that actually refer to
the same record (also referred as false negative). These errors can be described by a
confusion matrix [13,63] as deﬁned in Table 14.1.
FP represents the number of pairs labeled as a match when they are actually
not matched. If FP or precision error happens, the number of accurate matches is
decreased; FN represents the number of pairs labeled as a non-match when they are
actually matched. If FN or a recall error happens, the number of real matches is
decreased.
Precision error is usually caused by a decision model. When a threshold based
decision method is used for classifying a record pair as a match or non-match,
the precision error happens when comparison value of non-matches falls above
the given threshold. The recall error is caused ﬁrst by a searching process when
it misses some potentially matching records so that they are not able to compare
with the target record, similar to precision error, recall error is also caused by a
decision model when the decision model has bias to certain matching patterns. For
example, with the threshold method, recall error happens when comparison value
of real matches is below a given threshold that addresses the cases that an indexing
method is able to ﬁnd the right candidates but the decision model does not make the
right decision.
The quantity measurement of matching performance of a record linkage system
is deﬁned by accuracy, precision and recall based on the confusion matrix in
Table 14.1.
Accuracy
Acc D
TP C TN
TP C FP C FN C TN
(14.39)
Precision
P D
TP
TP C FP
(14.40)
Recall
R D
TP
TP C FN
(14.41)

402
L.Q. Zhang
As Christen [4] pointed out that the number of matches and non-matches are
strongly imbalanced in record pairs and there are much more non-match pairs than
match pairs, the accuracy measurement usually produces an optimistic result. The
most common use for measuring the quality of a matching system is precision and
recall or F-measure, a harmonic mean of Precision and Recall, deﬁned as follows.
F-Measure F C 2.P R/
P C R
(14.42)
The numbers in Table 14.1 can be obtained through a labeled testing dataset in
case of using supervised classiﬁcation method. In practice, human interaction is
necessary for manually review part of the linking result so that the precision and
recall can be estimated.
6
Record Linkage Challenge and LexisNexis Solution
As Internet omnipresence makes individuals and organization tremendously easier
to share information, humans are now living in a big data era where there is
profound business intelligence fueled by big data. Record Linkage, as a technology
for improving data quality and enriching enterprise content, like other advanced
data mining technology, faces the challenge of extremely high task complexity in
implementation, especially at real time operation. This is because when data size
increases linearly, the comparison can grow in quadratic if indexing is not used
[3]. Though supervised machine learning methods can be implemented ofﬂine, the
record pair comparison still needs to be done at real time.
For a typical linking process, the computation cost is mainly affected by ﬁeld
comparison, record comparison, search process as well as matching and linking
process.
Suppose there are k (usually > 10) ﬁelds chosen in a linking process, each record
pair comparison takes k ﬁelds’ comparison and some of ﬁeld’s comparison may be
very expensive. If Levenshtein edit distance is used for comparing two string ﬁelds
with length š1 and š2, the computation cost is O.jš1jjš2j/. Suppose only check if two
strings are in an edit distance £, the cost can be reduced to O.min fjš1j; jš2jg£/.
For two datasets, A and B, theoretically each record in A should be compared
with each record in B. The total comparison is jAjjBj, suppose n D min (jAj; jBj),
then at least n2 comparison should be performed. When performing de-duplicate,
there is at least n.n  1/=2 comparisons. If there are one billion records in A, it
needs 5  108.109  1/ record pair comparisons. If a high speed linking system can
take 107 ms for comparing two records and it will need about 5  107 s to get all
comparison done, that’s 1 year and half or 579 days!
In order to overcome the record pair comparison challenge, indexing or blocking
technique is employed for reducing the number of comparisons by ﬁltering out the
records that have little probability to be matches. Even in such case, for a dataset

14
Record Linkage Methodology and Applications
403
A with n records matches to a dataset B, suppose there is no duplicate in A and
there is at least one match in B for each record in A, also suppose A has one
billion .109/ records and indexing technology is perfect enough to ﬁlter out all
non-matches except for the only one record that is matched in B. It still needs
109 comparisons that need a half day work or 12 h! Considering it’s not possible
to have such a perfect indexing and that the process of searching and matching
also take time, the actual task complexity for a linking process as a whole is much
longer.
Slow delivery of right information to customer’s request seriously affects
customers’ satisfaction and the business performance. It addresses a strong demand
for a faster computing platform to process high volume data on which a record
linkage system can be implemented. LexisNexis proprietary High Performance
Computing Cluster (HPCC), introduced in Chap. 9, that coupled with a powerful
programming language, ECL, introduced in Chap. 3, provides the best solution to
the challenge as mentioned before. LexisNexis SALT linking technology is built on
the HPCC platform. Its technology details and competitive advantage are introduced
in Chap. 8. In the session we will further demonstrate the SALT linking performance
in run time, indexing and matching through the experiment results from real world
data used for developing LexisNexis products.
For resolving entity identity problem, LexisNexis uses SALT [18] to match each
person record in a dataset A without a unique identiﬁer, collected from business, to
a base authority dataset B that has the unique identiﬁers. If there is a match found
in B for a record in A, the matching process links the matched record in B to the
record in A by appending its unique identiﬁer to the record in A. For the project, 21
attributes have been selected from person records. Those ﬁelds are person names,
date of birth, gender, and addresses, etc. Some of ﬁelds are composite attributes like
full name that includes ﬁrst, middle and last name. When working on the project,
the person records without unique identiﬁer are collected from multiple sources
such as utility daily, student list, and bankruptcy. The base ﬁle at LexisNexis is a
comprehensive people records that are collected daily from multiple data resources
and also integrated with SALT internal linking method as a master base dataset with
over ten billion records.
6.1
Indexing Quality
The index quality is measured by the criteria, Pair Completeness (PC), Reduction
Ratio (RR) or their combination F-Score (Sect. 5.2). Table 14.2 illustrates the
performance of four different datasets when they match against the same base
dataset of ten billion records. The four external datasets are from Bankruptcy, Utility
Daily, Student List and Colorado State person records.
Table 14.2 shows the four datasets have very high RR value, 99.99999999. It
means that only one record out of every 100 million is compared, therefore the
actually compared records have been greatly reduced.

404
L.Q. Zhang
Table 14.2 SALT linking indexing quality
Dataset
Total records
count
PC
RR
F-score
Actual compared
record pairs count
Bank
37794692
98:815077
99:99999999
99:40400747
401739281
Utility
292369
99:45206229
99:99999999
99:72527849
2965806
Student
41706308
99:09411066
99:99999999
99:5449944
420951565
CO State
53306783
99:99605116
99:99999999
99:99802554
498731740
If SALT linking system compares each record in A with each record in the base
dataset B with ten billion records, suppose, for utility daily with about 3  105
records, then there will be total 31015 comparisons. With the indexing technology,
the number of record pair comparison is greatly reduced. The actually compared
record count is only about three millions!!
The above PC values shows that the reduced record pair comparison does not
hurt the search recall too much. For a PC value, 99.45%, the search process fails
about only 6 good candidates for every 1,000 records. For bankruptcy and student
list datasets, the loss is slightly higher. Our result analysis showed the main reason
causing a good candidate missed in the result is that the datasets include relative new
records, some have data errors or the master dataset is not most recent and complete.
This is acceptable as there is latency between data collected most recently and the
data well integrated before in an enterprise content system.
The datasets from Colorado State are sampled from part of the base ﬁle and the
search process only fails about one record for every 10,000 records because there is
no gap between dataset and master dataset.
6.2
Run Time Performance
SALT run time performance is demonstrated from three perspectives, (1) compare
run time of an external dataset matches against two base dataset with different
data sizes and see how run time changes when external dataset size increases, (2)
compare records and record pairs processed by second, (3) how throughput, the
number of records processed per second, changes as dataset size varies.
Figure 14.4 above shows two charts of SALT linking execution time, the red chart
shows execute time that external dataset A matches against the base dataset B with
ten billion records, the blue chart shows that A matches against a base dataset with
about 124 million records sampled from the ﬁst base dataset B by Colorado State.
The blue chart shows that processing a 3 M person records takes 2 min 56 s and
8 million seconds (02:56.8), a 45 M person records takes about 18min 48 s and
491 million seconds. The average time taken for matching each 1 M records is
about 27 s.
The charts also show the run time does not grow linearly as dataset grows in
linear, especially when the base dataset size is bigger. The red chart shows strong

14
Record Linkage Methodology and Applications
405
Fig. 14.4 SALT run time performance by external records matches against different base ﬁles
vibrations. Because LexisNexis HPCC uses parallel processing with multiple nodes
and SALT linking is performed on 400 nodes. If a dataset is evenly distributed across
different nodes, the processing time becomes shorter when more parallel processes
run simultaneously, which tells why a bigger dataset does not necessarily take more
time than a smaller dataset as thought in general. Though the bigger base dataset
is about 80 times bigger than the smaller one, the run time that an external dataset
matches to the bigger base dataset is only about 6 times longer on average than it
matches to the smaller base dataset. This demonstrates the strong unique power of
HPCC platform.
Figure 14.5 shows two charts by the number of records and record pairs
processed with time, the red chart from record pairs processed and the blue chart
from external records. The charts show, when processing time linearly increases,
the number of external records processed increases in the power of 2065 X1:4069 and
the number of record pairs increases in the power of 7427 X1:4069 that is about 3.6
times faster than the former. It once again demonstrates that the processing time
doesn’t vary in linear with the number of records processed and the speed of record
pairs processed is actually much faster. It implies that the throughput, the number of
record processed by time, increases with a dataset increasing. Figure 14.6 illustrates
the point.
Figure 14.6 is drawn by throughput when external dataset with variant size
matches against a base ﬁle. The chart shows, that the throughput is not a constant,
instead, it tends to increase when the dataset size increasing though there are some
vibrations.

406
L.Q. Zhang
Fig. 14.5 SALT run time performance by number of records and record pairs processed
Fig. 14.6 SALT run time performance by throughput by second

14
Record Linkage Methodology and Applications
407
Table 14.3 SALT linking performance
DataSet
Records
count
Matches
Correct
matched
Recall
Precision
F-score
Bank
37794692
37794692
36908653
96:2250969
98:5351
97:3664
Utility
292369
286599
284681
97:3704462
99:33077
97:35454
Student
41706308
40686232
40106439
96:163964
98:57497
98:34084
CO State
53306783
52557600
52402398
98:3034335
99:7047
98:9991
The experiments show the minimum number of record pairs processed is 81,099
per second and the maximum number is 154,447 per second, that means SALT
linking system takes approximately from 0.012 to 0.006ms to link a record pair
on average that as always includes searching potential matching record candidates,
performing record pair comparison, making a match decision then linking two
records together once two records are matched, a complete linking process.
6.3
Linking Performance
SALT matching performance is evaluated by precision, recall and F-score deﬁned
in session (14.40)–(14.42). Table 14.3 shows three stats from four different datasets
that are matched against the same base dataset, ten billion records. The statistics
demonstrates that recall is lower than precision because recall is calculated by taking
two losses into account, the real matches that are missed from searching process
and classiﬁed as non-matches from a decision model. Considering those datasets
are from real world, the linking performance is very competitive with the minimum
recall 96.2, precision 98.5, and F-Score 97.4. Colorado State dataset has a better
performance because it’s sampled from part of the base dataset and there is no
latency between the dataset and the base dataset.
Further, the linking performance is evaluated by matching weight, Fig. 14.7,
drawn based on the result from Colorado State. The chart shows that both matching
recall and precision is 0 when matching weight falls below 40, they grow as the
matching weight grows and converges to almost to 1 when weight value is over 100.
Figure 14.8 demonstrates the distribution of matches versus correct matches by
matching weight. The chart shows that the number of matches is almost in a normal
distribution with most matches happen from weight 60 to 150. When match weight
is lower, there is a bigger discrepancy between record pairs classiﬁed as matches and
the real or correct matches. When weight increases, the discrepancy decreases, and
they tend to merge as one once the weight is over 100 that tells why the matching
precision and recall converge when weight increases in Fig. 14.7.
LexisNexis SALT linking system on HPCC demonstrates strong competitive
advantages in both run time and linking performance and it lays solid foundations
for many LexisNexis products in business. SALT continues to show its business

408
L.Q. Zhang
Fig. 14.7 SALT linking performance by matching weight
Fig. 14.8 Matches versus correct matches distribution by matching weight
value to LexisNexis as its demand increases daily and it continues to expand and
progress in technology. Actually up to the writing, SALT is the only linking system
that builds on a super computer platform with an amazing processing power.

14
Record Linkage Methodology and Applications
409
7
Conclusion
In the big data era, while organizations realize that data quality and richness are the
driven force of information business operations, they face the challenge of managing
ﬂood of data, efﬁciently integrating data and solving data inconsistency gathered
from multiple sources, there is a strong demand for an automated record linkage
system to improve data quality and enrich content before performing any advanced
pattern mining and drawing any business insight upon data in supporting their daily
business decision making. The chapter provides a comprehensive overview about
Record Linkage technology, its history, concept and methods as well as applications.
The chapter also demonstrates the strong power of a LexisNexis SALT linking
system built on LexisNexis High Performance Computing Platform.
Record linkage started with building a book of a person’s life by Halbert L. Dunn
in 1946. Gradually it’s used to build a book of any entity’s life such as a business
unit, a product, a transaction or an object, etc.
Section 2 introduces the business value record linkage brings to an organization.
Though record linkage has gone through about seven decades, its business value
continues to grow when more data are gathered through Internets and rich media.
The technology has contributed to build many core business products in diverse
business applications. It also provides solid quality data foundation for developing
an enterprise content management system through cleaning, removing duplicates as
well as aggregating content from multiple sources.
Section 3 gives a rigor deﬁnition about record linkage and the commonly used
standard notion.
Section 4 outlines the main linking methods on market or most recently proposed
by researchers. Broadly speaking, there are four types of record linkage techniques,
deterministic, probabilistic method, and modern approach as well as any of their
combination. Experiments showed probabilistic method outperformed deterministic
method since it was invented in late 1960s and became dominant over two decades.
Then researchers took matching problem as a pattern classiﬁcation problem. Many
diverse machine learning methods have been used for record linkage such as
decision tree, support vector model, regression, clustering method or the combi-
nation of both supervised learning and clustering. For overcoming the challenges of
developing a training dataset with pure supervised learning method, Christen [2,16]
introduced two steps method to reach the effect of unsupervised learning method
with a supervised learning approach.
Section 5 outlines a standard record linkage process and the main components in
a typical linkage system, data preprocessing, indexing or blocking, record compar-
ison, matching & linking and model development as well as result evaluation. As
the quality of indexing is very critical to the success of a linkage system, indexing
has to ensure that good candidates are not missed in searching phase and also
guarantee it minimizes the number of non-potentially match records and maximizes
the number of potentially match records, otherwise it will jeopardize matching recall
or precision.

410
L.Q. Zhang
Finally Sect. 6 addresses the challenges in implementing linking technology on
a general platform because of its high computational cost with a big dataset and it
shows SALT linking technology implemented on LexisNexis HPCC platform can
conquer the scalability challenge and deliver the impressive match results.
As data volume grows daily and new challenges also emerge, record linkage will
exploit more statistical and machine learning methods to overcome the unexpected
challenges and it will continue to demonstrate its business value in the era of
information intelligence. LexisNexis SALT linking technology will also continue
to incorporate more advanced methods in the system and make it more robust and
competitive.
References
1. W. E. Winkler. Record Linkage Software and Methods for Merging Administrative Lists,
BUREAU OF THE CENSUS STATISTICAL RESEARCH DIVISION, Statistical Research
Report Series
2. P. Christen, A two-step classiﬁcation approach to unsupervised record linkage. In AusDM’07,
CRPIT vol. 70, pages 111–119, Gold Coast, Australia, 2007.
3. P. Christen and K. Goiser (2005), Assessing deduplication and data linkage quality: What to
measure, in ‘Proceedings of the fourth Australasian Data Mining Conference (AusDM 2005)’,
Sydney.
4. P. Christen and K. Goiser, Quality and Complexity Measures for Data Linkage and Deduplica-
tion, Accepted for Quality Measures in Data Mining, Springer, 2006.
5. L. Zhang and M. Wasson, TEMPLAR, Valentina, METHODS AND SYSTEMS FOR
MATCHING RECORDS AND NORMALIZING NAMES, WO/2010/088052
6. C. Dozer and R. Haschart, Automatic Extraction and Linking of Person Names in Legal Text,
RIAO-2000 Proceedings
7. H. L. Dunn, (1946) Record Linkage, American Journal of Public Health, 36, 1412–1415
8. H. B. Newcombe, J. M. Kennedy, S. J. Axford, and A. P. James, 1959, Automatic Linkage of
vital records, Science 150(1959), 954–959
9. H. B. Newcombe and J. M. Kennedy, Record Linkage, Making Maximum Use of the
Discriminating Power of Identifying Information, Communications of ACM, 1962, Vol. 5,
Issue11
10. H. B. Newcombe, Record linking: The design of efﬁcient systems for linking records into
individual and family histories, Am J Hum Genet. 1967 May; 19(3 Pt 1): 335–359.
11. I. Fellegi and A. Sunter, A theory for record linkage, Journal of the American Statistical
Society, 64(328):1183–1210, 1969.
12. W. Winkler. The State of Record Linkage and Current Research Problems. U.S. Bureau of the
Census, Research Report, 1999.
13. K. Goiser and P. Christen, Towards Automated Record Linkage, In ACM KDD’08Proc. Fifth
Australasian Data Mining Conference (AusDM2006)
14. J. S. Lawson, Record Linkage Techniques for Improving Online Genealogical Research using
Census Index Records, ASA Section on Survey Research Methods.
15. W. W. Cohen and J. Richman, Learning to Match and Cluster Large High-Dimensional Data
Sets For Data Integration, SIGKDD’ 02
16. P. Christen, Automatic Record Linkage using Seeded Nearest Neighbour and Support Vector
Machine Classiﬁcation, In ACM KDD’08, Pages 151–159, Las Vegas, 2008

14
Record Linkage Methodology and Applications
411
17. M. G. Elfeky, T. M. Ghanem, V. S. Verykios, A. R. Huwait, and A. K. Elmagarmid, Record
Linkage: A Machine Learning Approach, A Toolbox, and A Digital Government Web Service,
Technical Report CSD-TR 03–024
18. D. A. Bayliss, Database systems and methods for linking records and entity representations
with sufﬁciently high conﬁdence, US 2009/0271424 A1
19. D. A. Bayliss, DATABASE SYSTEMS AND METHODS FOR LINKING RECORDS,
WO/2010/003061
20. M. Fair, Generalized Record Linkage System – Statistics Canada’s Record Linkage Software,
Austrian Journal of Statistics, Volume 33 (2004), Number 1&2, 37–53
21. P. Christen. Febrl-An Open Source Data Cleaning, Deduplication and Record Linkage System
with a Graphical User Interface. August 2008.
22. M. Elfeky, V. Verykios, and A. Elmagarmid. TAILOR: A record linkage toolbox. In ICDE’02,
pages 17–28, San Jose, 2002.
23. C. W. Kelman, J. Bass, and D. Holman, (2002), ‘Research use of linked health data – A best
practice protocol’, Aust NZ Journal of Public Health, vol. 26, pp. 251–255.
24. W. E. Winkler. Methods for evaluating and creating data quality, Elsevier Information Systems,
29(7):531–550, 2004.
25. M. Cochinwala, S. Dalal, A. K. Elmagarmid, V. S. Verykios, Record Matching, Past, Present
and Future, Submitted to ACM Computing Surveys, 2003
26. D. Loshin, Ed Allburn, Customer Data Integration, Linkage Precision and Match Accuracy,
Information Management Magazine, November 2004
27. E. Ted, H. Goldberg, J. Wooton, M. Cottini, and A. Khan, (1995). Financial Crimes Enforce-
ment Network AI System (FAIS) Identifying Potential Money Laundering from Reports of
Large Cash Transactions. AI Magazine, 16(4), 21–39. Retrieved from http://www.aaai.org/ojs/
index.php/aimagazine/article/viewArticle/1169
28. H. Issa, Application of Duplicate Records detection Techniques to Duplicate Payments in a
Real Business Environment, Rutgers Business School, Rutgers University 2010
29. Inspector General. (1997), Audit report duplicate payments. Retrieved from http://www.va.
gov/oig/52/reports/1997/7AF-G01-035--duppay.pdf.
30. A. C. Novello, (2004). Duplicate Medicaid Transportation Payments, 1–4. Retrieved from
http://www.osc.state.ny.us/audits/allaudits/093004/04f2.pdf
31. L. Karl Branting, BAE Systems, Inc, Name Matching in Law Enforcement and CounterTerror-
ism, Columbia, MD 21046, USA, karl.branting@baesystems.com
32. J. Jonas and J. Harper, Effective counterterrorism and the limited role of predictive data mining,
Policy Analysis, (584), 2006.
33. http://www.stanford.edu/dept/EHS/prod/researchlab/chem/inven/new chem inven instr.html
34. M.-Y. Kan and Y. F. Tan, Record Matching in Digital Library Metadata, Technical Opinion,
Communications of The ACM, Vol. 51, No. 2, 02/2008
35. O. Charif z, H. Omraniz, O. Kleinz, M. Schneiderz, and P. Trigano, A method and a tool for
geocoding and record linkage, Working Paperking Paper, No 2010-17, 07/2010
36. C. Giraud-Carrier, J. Goodliffe, and B. Jones, Improving the Study of Campaign Contributors
with Record Linkage
37. S. J. Grannis, J. M. Overhage, C. J. McDonald M, Analysis of Identiﬁer Performance using a
Deterministic Linkage Algorithm
38. S. Gomatam, R. Carter., M. Ariet, and G. Mitchell, An empirical comparison of record linkage
procedures. Statistics in Medicine, vol. 21, no. 10, pp. 1485–1496, May 2002.
39. F. Maggi, A Survey of Probabilistic Record Matching Models, Techniques and Tools, Scienti c
Report TR-2008-22
40. A. Elmagarmid, P. Ipeirotis, and V. Verykios. Duplicate record detection: A survey. IEEE
Transactions on Knowledge and Data Engineering, 19(1):1–16, 2007.
41. W. E. Winkler, Using the EM algorithm for weight computation in the Fellegi-Sunter model of
record linkage. Technical Report RR2000/05, US Bureau of the Census, 2000

412
L.Q. Zhang
42. P. Christen, T. Churches, and J. X. Zhu, Probabilistic Name and Address Clearning and
Standardization, the Australasian Data Mining Workshop 2002
43. J. Friedman, T. Hastie, and R. Tibshirani, Additive logistic regression: a statistical view of
boosting, the Annals of Statistic, 28(2):337–407, 2000.
44. M. Elfeky, V. Verykios, and A. Elmagarmid. TAILOR: A record linkage toolbox. In ICDE’02,
pages 17–28, San Jose, 2002.
45. S. Sarawagi and A. Bhamidipaty, Interactive deduplication using active learning. Proceedings
of the 8th ACM SIGKDD conference, Edmonton, July 2002.
46. J. Rennie, Boosting with decision stumps and binary features, 2003
47. R. E. Schapire, The Boosting Approach to Machine Learning. An Overview Nonlinear
Estimation and Classiﬁcation, Springer, 2003
48. M. Jaro. Software Demonstrations. In Proc. of an International Workshop and Exposition –
Record Linkage Techniques, Arlington, VA, USA, 1997.
49. E. Rundensteiner (Ed.), Special Issue on Data Transformation, IEEE Data Engineering
Bulletin, March 1999.
50. L. Gu, R. Baxter, D. Vickers, and C. Rainsford. Record linkage: Current practice and future
directions. Technical Report 03/83, CSIRO Mathematical and Information Sciences, Canberra,
Australia, April 2003.
51. D. Knuth, The Art of Computing Programming, Volume III, Addison-Wesley 1973.
52. M. Hernandez and S. Stolfo. Real World Data is Dirty: Data Cleansing and the Merge/Purge
Problem. Journal of Data Mining and Knowledge Discovery, 2(1), pages 9–37, 1998.
53. A. McCallum, K. Nigam, and L. H. Ungar, “Efﬁcient Clustering of High-Dimensional
Data Sets with Application to Reference Matching,” Proc. Sixth ACM SIGKDD Int’l Conf.
Knowledge Discovery and Data Mining (KDD’00), pp. 169–178, 2000.
54. R. K. Chapman, D. A. Bayliss, G. C. Halliday, METHODS AND SYSTEMS FOR DYNAMI-
CALLY CREATING KEYS IN A DATABASE SYSTEM, US 7739287 B1
55. M. A. Hernandez and S. J. Stolfo. The Merge/Purge Problem for Large Databases. In Proc. of
1995 ACT SIGMOD Conf., pages 127–138, 1995.
56. V. I. Levenshtein. Binary codes capable of correcting deletions, insertions and reversals. Soviet
Physics Doklady, 10:707–710, 1966.
57. http://cpansearch.perl.org/src/SCW/Text-JaroWinkler-0.1/strcmp95.c
58. M. A. Jaro, 1989. Advances in record-linkage methodology as applied to matching the 1985
census of Tampa, Florida. Journal of the American Statistical Association 84:414–420.
59. M. A. Jaro, 1995. Probabilistic linkage of large public health data ﬁles (disc: P687–689).
Statistics in Medicine 14:491–498
60. W. E. Winkler, 1999. The state of record linkage and current research problems. Statistics of
Income Division, Internal Revenue Service Publication R99/04. Available from http://www.
census.gov/srd/www/byname.html.
61. L. Gravano, P. G. Ipeirotis, H. V. Jagadish, N. Koudas, S. Muthukrishnan, and D. Srinivasta.
Approximate string joins in a database. In Proc. 27th Int. Conf. on Very Large Data Bases,
pages 491–500, 2001.
62. W. W. Cohen, P. Ravikumar, and S. E. Fienberg. A comparison of string distance metrics for
name-matching tasks. In Proceedings of the IJCAI-2003 Workshop on Information Integration
on the Web (IIWeb-03), 2003. To appear
63. Journal of the American Statistical Association, 84(406), pages 414–420, 1989.
64. S. Tejada, C. Knoblock, and S. Minton, Learning domain-independent string transformation
weights for high accuracy object identiﬁcation, In ACM KDD’02, pages 350–359, dmon-
ton, 2002.
65. U. Y. Nahm, M. Bilenko, and R. J. Mooney. Two approaches to handling noisy variation in text
mining. In TextML’02, pages 18–27, Sydney, 2002.
66. L. Gu and R. Baxter. Decision models for record linkage. In Selected Papers from AusDM,
Springer LNCS 3755, pages 146–160, 2006.
67. W. E. Winkler, (1995), Advanced methods for record linkage, American Statistical Association,
Proceedings of the Section on Survey Research Methods, pp. 467–472.

14
Record Linkage Methodology and Applications
413
68. R. Baxter, P. Christen, and T. Churches. A comparison of fast blocking methods for record
linkage. In ACM SIGKDD workshop on Data Cleaning, Record Linkage and Object Consoli-
dation, pages 25–27, Washington DC, 2003.
69. W. E. Winkler, (1995), Matching and Record Linkage, in B. G. Cox et al. (ed.) Business Survey.
Methods, New York: J. Wiley, 355–384.


Chapter 15
Semantic Wrapper: Concise Semantic Querying
of Legacy Relational Databases
Naphtali Rishe, Borko Furht, Malek Adjouadi, Armando Barreto,
Debra Davis, Ouri Wolfson, Yelena Yesha, and Yaacov Yesha
1
Introduction
From business to education to research, one of the most common needs in today’s
world is the ability to efﬁciently and effectively store and organize information.
In the past several decades, the amount of information required to perform even
everyday tasks has dramatically increased. In addition, there has been a substantial
increase in the availability of more sophisticated and complex data, such as imagery,
multimedia, geospatial and remotely sensed data. As our need for more extensive
and complex information has increased, so has the size and complexity of databases
used to store this information. To even further complicate the situation, needed data
often resides in various independent, distributed databases as well as in unstructured
forms such as social media and web sites. This has resulted in a greater need for
more ﬂexible, scalable and efﬁcient database technology that can be used to store,
query, and combine massive amounts of various types of data that are distributed
over multiple structured and unstructured data sources [1–3].
N. Rishe () • M. Adjouadi • A. Barreto • D. Davis
NSF Industry-University Cooperative Research Center for Advanced Knowledge Enablement
(CAKE.ﬁu.edu) at Florida International and Florida Atlantic Universities, Miami, FL, USA
e-mail: rishe@ﬁu.edu; adjouadi@ﬁu.edu; Armando.Barreto@ﬁu.edu; dledavis@cs.ﬁu.edu
B. Furht
NSF Industry-University Cooperative Research Center for Advanced Knowledge Enablement
(CAKE.ﬁu.edu) at Florida International and Florida Atlantic Universities, Boca Raton, FL, USA
e-mail: borko@cse.fau.edu
O. Wolfson
Computational Transportation Science Program (CTS.cs.uic.edu) at the University of Illinois
at Chicago, Miami, FL, USA
e-mail: wolfson@cs.uic.edu
Y. Yesha • Y. Yesha
NSF Industry-University Cooperative Research Center for Multicore Productivity Research
(CHMPR.umbc.edu) at the University of Maryland Baltimore County, Baltimore, MD, USA
e-mail: yeyesha@umbc.edu; yayesha@umbc.edu
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 15, © Springer Science+Business Media, LLC 2011
415

416
N. Rishe et al.
1.1
Relational Database Systems and Structured Query
Language (SQL)
The majority of commercial data systems in use to today are relational databases.
These database systems have been developed based on the Relational Data Model
proposed by Codd in 1970 and subsequently updated in 1990 [4, 5]. A relational
database is composed of one or more two-dimensional tables, each of which
contains data ﬁelds, or attributes, in the form of columns, and data records, or tuples,
in the form of rows. Relationships between tables are implicit, by cross-referencing
data values in ﬁelds found on different tables. The Relational Data Model has a
number of advantages that have contributed to the prevalence of these systems
including: (1) it has rigorous design methodologies (e.g., normalization), alleviating
redundancy and inconsistency, (2) the database schema is easily modiﬁable by the
addition of tables and ﬁelds, and (3) the database has ﬂexible and powerful well-
deﬁned operations, such as join, that is based on the algebraic set theory [6].
One of the biggest advantages of relational databases is the use of Structured
Query Language (SQL) as the standard query syntax in most relational database
products. In its basic form, the syntax is simple and relatively easy to understand and
use: Select fields From tables Where conditions. SQL can be
used directly on a database, or as embedded SQL within general-purpose languages,
or as an intermediary language via a standard communication protocol such as
Open Database Connectivity (ODBC) protocol, Java Database Connectivity (JDBC)
protocol or Object Linking and Embedding (OLE).
Relational database systems have become a staple in modern technology. How-
ever, the data needs of today’s world are increasingly changing at a very rapid
pace. As a result, today’s technology is becoming overtaxed and obsolete at an
increasingly faster rate as more sophisticated solutions are required. As this pressure
on technology has increased, the limitations of relational databases have come
more to the forefront. For example, relational databases are designed for organizing
information that is easily categorized by common characteristics, and described by
simple string or number data. Complex data, such as images, spatial and remotely
sensed data, and multimedia products, are not easily described or categorized in
this manner. Increased storage and utilization of massive amounts of complex data
tends to lead to the implementation of more complex database schemas. Due to the
difﬁculty in effectively handling these complex structures, this can lead, in turn, to
isolation of complex relational database systems where information cannot be easily
shared between those systems [1].
There are also challenges involved in the use of SQL. Although in its basic
form, SQL is a relatively simple and easy to understand syntax, rarely will a simple
query be sufﬁcient to provide users with the data results that they require. This is
particularly true for the more common, complex systems in use today. The effective
use of SQL in most real-world relational databases requires technology specialists
who have extensive training in the principles of relational databases, and complex

15
Semantic Wrapper: Concise Semantic Querying of Legacy Relational Databases
417
and efﬁcient SQL query design. They must also have a thorough understanding of
the structure of the database of interest. This is because the relationship between
tables in a relational database is implicit rather than explicit, and data of interest of is
often found across various tables in the database. When creating a query, users must
explicitly identify each table and provide a formula relating the various tables in
order to effectively achieve the desired data results. The more abstract and complex
the relationships are between tables in the database, the more difﬁcult it becomes to
create accurate and efﬁcient queries.
These highly skilled database technology specialists are also needed when
designing and implementing a new relational database or making changes to the
structure of an existing system. Because relational databases are so easy to modify,
some untrained users mistakenly believe that one can simply add new tables and
ﬁelds without much thought. However, this ﬂexibility makes it easy to create poor,
inefﬁcient database designs that do not meet the needs of end users. These complex
solutions require thorough analysis and planning, and are quite expensive and time
consuming to implement or change.
1.2
Overcoming the Limitations of Relational Databases
In recent years, there have been a number of efforts to overcome the limitations
of relational database systems. Semantic and object-oriented databases have been
introduced that provide improvement on the structure of the database itself, such
as allowing explicit relationships between classes of objects, and the inclusion of
super-categories, sub-categories and inheritance. However, to be effective, these im-
provements to the structure of the database have historically required the addition of
enhanced query language [7]. Although this can provide more powerful capabilities,
it has also precluded the ability to use standard SQL for effective querying of these
databases, as well as the use of standard relational database interface tools.
Approaches that allow semantic and object oriented query interfaces over rela-
tional databases have also been proposed, including SQL3, Object query Language
(OQL), and various graphical query languages [7, 8]. While these provide the
advantages of the object approach, changes to the data models and syntax of SQL
are required to enable syntax expressiveness. This requires users to study new syntax
and does not allow the utilization of the very extensive existing middleware and end-
user GUIs that interface in standard SQL. Also, while the new languages work well
with new database implementations based on object models, it does not provide
improvement to existing relational databases. Further, these approaches require
additional training and a change in how users must approach their programming.
Standard SQL is a purely declarative language, whereas SQL3 and OQL, in
particular, are semi-procedural.
Graphical query languages attempt to make query interfaces more user friendly.
Through the use of approaches such as hypertext language, menu-driven queries

418
N. Rishe et al.
and Query-By-Example (QBE), users theoretically need not learn SQL syntax. In
real world applications, however, these approaches only work well for very simple
databases and very simple queries. Attempting to use these types of approaches
for complex queries often leads to frustration when trying to follow the required
procedures, and produces irrelevant results once the user completes the process. In
order to be successful when creating complex queries, users must still have full
knowledge of the logical structures of the database of interest, and understand how
to express the needed joins correctly.
It is clear that there is a need for a more dynamic solution that overcomes
the increasing pressures and limitations of relational databases, while, at the
same time, avoids some of the obstacles encountered by other object oriented
approaches. To address and overcome these many challenges, Semantic Wrapper
technology has been created. The Semantic Wrapper is a middleware system that
provides semantic views over legacy relational databases. As middleware, this
system provides straight-forward, easy access to legacy relational databases without
requiring users to switch from their existing GUI to a new, unfamiliar GUI. It further
greatly improves usability by allowing the use of standard SQL syntax to access
the semantic layer via more simpliﬁed and concise SQL queries than what would
be required for direct querying of a standard relational database. This approach is
also applicable in a heterogeneous multi-database environment that can include both
structured (relational and semantic databases) and unstructured data (social media
and related Internet sites).
The remainder of this article presents an overview of the Semantic Wrapper and
its major components. Section 2 discusses the Semantic Wrapper’s primary features
and compares it to other systems. This is followed by discussions of the capabilities
and integration of its three major components in Sect. 3.
2
Semantic Wrapper Overview
The Semantic Wrapper is a set of middleware tools developed to quickly and easily
access and query relational databases, semantic databases and unstructured data
such as Internet data sources. To accomplish this, the Semantic Wrapper creates a
semantic schema view over relational and legacy databases. This schema is then
used by the system to provide users with the ability to query the data quickly
and easily. This technology allows the use of standard SQL queries to access the
semantic schema via simple and concise syntax. It enables the use of third-party
interfaces to formulate queries, allowing users continued use of familiar interfaces
while still providing them with the Semantic Wrapper’s powerful capabilities.
The system is based on the semantic modeling approach [9], and employs
the Semantic Binary Object Data Model (Sem-ODM) and Semantic SQL query
language (Sem-SQL) for improved data querying and usability [10]. Various aspects
and application of this and related technology are discussed in [11–15]. This

15
Semantic Wrapper: Concise Semantic Querying of Legacy Relational Databases
419
Fig. 15.1 High-level architectural view of the Semantic Wrapper
platform provides the system with powerful and adaptive capabilities. When the
semantic features of these technologies are employed, they have been found to
provide a high level of efﬁciency when compared to relational databases [16].
2.1
Capabilities of the Semantic Wrapper
There are a number of object oriented approaches that are focused on overcoming
the problems inherent in the Relational Model’s inability to effectively deal with
modern data needs. To address these issues, other approaches have implemented
improvements to database structures, enhancements to standard query languages,
and the use of graphical user interfaces to improve ease of use. However, each of
these attempts has created their own challenges and limitations. These challenges
include:
•
Inability to be implemented with existing relational databases and legacy systems
•
Increased complexity and decreased user friendliness due to changes to query
languages
•
Attempts to improve ease of use that also result in signiﬁcantly decreased
functionality
The Semantic Wrapper has been designed to provide a powerful, yet easy to use,
system that improves our ability to work with various types of heterogeneous data
without encountering the above challenges.
The Semantic wrapper can be implemented either independently, or, as can be
seen in Fig. 15.1, over an existing relational database. A number of the object
oriented approaches discussed in Sect. 1.2 incorporate improvement on the structure
of the database itself to provide a greater capability to handle various types of data.

420
N. Rishe et al.
While this was a major step towards more efﬁciently and effectively dealing with the
shortcomings of a relational schema, it does not allow for the ability to implement
the system with existing relational and legacy database systems whose structures
are not be amenable to structural change. The Semantic Wrapper has overcome this
obstacle by avoiding any changes to existing database structures. Instead, a separate
semantic schema is constructed that allows a compact and logical semantic view
while still providing an accurate reﬂection of the original database. This, in essence,
provides the system with the improved database structure sought by other systems,
without the same limitations. Thus, the Semantic Wrapper can be used to construct
new applications, or as an additional interface to existing relational databases.
The Semantic Wrapper uses Semantic SQL, which is syntactically identical to the
standard relational SQL, but exploits the semantics of relationships. This, in turn,
makes Semantic SQL queries much more concise and clear. Speciﬁcally, Semantic
SQL queries refer to a virtual relational schema that consists of inferred tables
deﬁned as spanning trees containing all reachable relations from a given category.
Complications of multiple table references and keys are eliminated. As a result,
users have no need to explicitly express joins. Instead, when a user queries the
database, it is as if there is a universal table for each category. This signiﬁcantly
reduces the complexity of queries, and signiﬁcantly increases the user friendliness
of the system’s querying capabilities [17].
Attempts by other approaches, such as graphical query languages, to improve
user friendliness of database systems have succeeded in terms of simplifying the
process needed for very basic queries. However, these have also lead to signiﬁcant
reductions in the ability to create even moderately complex queries that are more
commonly used in the real world. Even though the Semantic Wrapper’s increased
user friendliness is partially through the use of more simpliﬁed query syntax,
because of the use of Semantic SQL in combination with the Semantic Wrapper’s
structure and the various tools that are part of the Semantic Wrapper, there is no
decrease in functionality. Speciﬁcally, the mapping and query translation processes
that are used to create the semantic schema (see Sect. 3.1) and subsequently query
the database (see Sect. 3.3) merely hide the complexity from the user without
actually limiting functionality. In other words, the system does much of the work
for the user, without loss of functionality or an increase in errors.
In sum, the Semantic Wrapper is a solution that provides users with the ability to
view multiple data sources as one, centralized virtual database through a semantic
database schema, and access needed data via a standard, easy-to-use and ﬂexible
interface. This standard interface allows users to interact and query data in a more
intelligent and friendlier manner that is based on the stored meaning of the data.
Further, third party tools that the user is already familiar with can be deployed with
the system. The system can be implemented independently for new applications,
or over an existing legacy relational database. Other advantages of using the
Semantic Wrapper include comprehensive enforcement of integrity constraints,
greater ﬂexibility, and substantially shorter application programs [10].

15
Semantic Wrapper: Concise Semantic Querying of Legacy Relational Databases
421
3
Technical Background
3.1
Semantic Schemas
A key component of Semantic Wrapper technology is the use of a semantic schema
that is functionally equivalent to a corresponding relational database. Use of an
equivalent semantic schema has numerous advantages over a relational schema,
including:
•
Knowledge is described at a conceptual level rather than at a table layout level.
•
Meaning of information in a semantic schema is retained and stored within the
schema itself.
•
Relationships between categories are explicitly represented.
•
It is signiﬁcantly easier to formulate queries as any relationship can be queried
and joins are not required to be explicitly deﬁned.
For example, a subschema of a Hydrology database that was developed for the
Everglades National Park can be seen in Fig. 15.2 and its normalized relational
counterpart can be seen in Fig. 15.3. As the semantic subschema is a conceptual
representation, its structure is more analogous to how the real world is conceptual-
ized by humans. This makes it easier for users to understand the types of information
contained in the database, as well as how data within the database is conceptually
related.
As can be seen in Fig. 15.4, Semantic SQL queries are an order of magnitude
shorter than the corresponding relational SQL query. This query is representative
of the types of real world information that might be needed by a user. In this case,
the user would be a scientist or environmentalist needing data about environmental
conditions over time (see Figs. 15.2 and 15.3 for the semantic and relational
subschemas). The question of interest “Give me all of the observations with all
of their attributes since January 1, 1993 and the location of the observing stations,”
requires a very short and easy to compose query to the semantic schema. As can
be seen, if the same query were written for a relational schema, the query would be
substantially longer and more complex.
3.2
The Semantic Binary Model
The semantic schemas used by the Semantic Wrapper are based on the Semantic
Binary Model (SBM) [9]. The SBM is a ﬂexible, new generation data model that
is natural, simple, non-redundant, and implementation-independent. Its strength lies
in its ability more accurately capture the meaning of information, while, at the same
time, providing a succinct, high-level description of that information. Its use of

422
N. Rishe et al.
Fig. 15.2 Semantic subschema for physical observations
objects, categories, and their relationships is very easy for users to conceptualize as
they are reﬂective of the manner in which users already think about the real world.
A sample semantic schema can be seen in Fig 15.5.
Objects – The central notion of semantic models is the concept of an object.
Objects are deﬁned as any real world object or entity that we wish to store infor-
mation about in the database. Examples of objects include a student, department,
course, and course name. Objects can be further classiﬁed as concrete objects, which
are printable objects such as course name, or abstract objects, which are non-value
objects in the real world such as a course itself.
Categories – Objects that have common properties are considered in the same
category in the database. As with objects, categories can be concrete (consisting of
only concrete objects) or abstract (consisting of only abstract objects). Objects may
also belong to more than one category at a time. For example, an object can be both
a student and instructor at the same time (see Fig 15.5). A schema may also contain
subcategories. A category is a subcategory of another category if every object in that
category is always an object in the latter category. For example, a student is always
also person. Therefore, STUDENT is a subcategory of PERSON. On the opposite
end of the spectrum, categories can be disjoint. Two categories are disjoint if no
object can ever be a member of both categories at the same time. For example, a
student can never also be a course; therefore, STUDENT and COURSE are disjoint
categories.

15
Semantic Wrapper: Concise Semantic Querying of Legacy Relational Databases
423
PHYSICAL-OBSERVATION-STATION
physical-observation-station-id-key:Integer      1:1;      comments:String;
housing:String;      structure:String;
is-part-of--physical-observation-station-id:Integer; 
_______________________________________
_______________________________________
_______________________________________
_______________________________________
_______________________________________
_______________________________________
_______________________________________
_______________________________________
_______________________________________
_______________________________________
_______________________________________
LOCATION
north-UTM-in-key:Number; east-UTM-in-key:Number; elevation-ft:Number;
description:String; 
ORGANIZATION
name-key:String 1:1; description:String;
PROJECT
name-key:String 1:1; description:String; comments:String; starting-date:Date;
ending-date:Date; 
MEASUREMENT-TYPE
name-key:String 1:1; measurement-unit:String; upper-limit:Number; lower-
limit:Number; 
FIXED-STATION
physical-observation-station-id-key:Integer  1:1;  platform-height-ft:0..50.000;
located-at--north-UTM:Number;
located-at--east-UTM:Number; 
MEASUREMENT
observation-id-key:Integer      1:1;      comment:String;      time:Date-time;
value:Number;      of--name:String;
by--physical-observation-station-id:Integer; 
IMAGE
observation-id-key:Integer       1:1;       comment:String;       time:Date-time;
image:Raw;       subject:String;
direction-of-view:0..360; comments:String; type:Char(3); by--physical-
observation-station-id:Integer;
PHYSICAL-OBSERVATION-STATION--BELONGS-TO--
ORGANIZATION
physical-observation-station-id-in-key:Integer; organization--name-in-
key:String;
ORGANIZATION--RUNS--PROJECT
organization--name-in-key:String; project--name-in-key:String;
PHYSICAL-OBSERVATION-STATION--SERVES--PROJECT
physical-observation-station-id-in-key:Integer; project--name-in-key:String;
ORGANIZATION--IS-PART-OF--ORGANIZATION
organization--name-in-key:String; organization-2--name-in-key:String;
Fig. 15.3 Relational subschema for physical observations

424
N. Rishe et al.
Fig. 15.4 Comparison of a semantic SQL query to a corresponding relational SQL query
Relationships – A binary relationship is a connection between two objects
indicating that they are related by a certain property. Such a property is called a
binary relation. At every moment in time, a binary relation R is descriptive of a
set of pairs of objects .x; y/ which are related at that time. This is denoted as xRy.
For example, an instructor WORKS-IN a department (see Fig 15.5). The relation is
WORKS-IN, and is denoted as i WORKS-IN d.
Binary relations may be m:1 (many-to-one), 1:m (one-to-many), m:m (many-to-
many) or 1:1 (one-to-one). A binary relation R is m:1 if there is never a time when
xRy and xRz where y ¤ z. For example, every person has only one birth year, there-
fore, BIRTH-YEAR is m:1. A binary relation R is 1:m if there is never a time when
xRy and zRy where x ¤ z. For example, every student can have at most one major.
If we had a relation MAJOR-STUDENT (instead of MAJOR-DEPARTMENT, which is
m:1), then that relation would be 1:m.). A binary relation R is m:m if it is neither m:1
or 1:m. For example, every instructor can work in more than one department, and
every department can employ more than one instructor. Thus, WORKS-IN is a m:m
relation. A binary relation R is 1:1 if it is always both m:1 and 1:m. For example, if
every course is uniquely identiﬁed by its name (there is no character string that can
be the name of two or more courses), then COURSE-NAME is 1:1.
The domain of a relation is the smallest category such that for every xRy, x always
belongs to the category. The range of a relation is the smallest category such that for
every xRy, y always belongs to the category. For example, the domain of WORKS-
IN is INSTRUCTOR and the range is DEPARTMENT. A binary relation is total
if for every object x in its domain, there always exists an object y such that xRy.

15
Semantic Wrapper: Concise Semantic Querying of Legacy Relational Databases
425
PERSON
last_name:String
first_name:String
birth_year:String
address:String
STUDENT
INSTRUCTOR
DEPARTMENT
name:String 1:m
COURSE_
ENROLLMENT
final_grade:Integer
COURSE
name:String 1:1
QUARTER
year: Integer
season:String
the_student
(m:1)
major
(m:1)
minor
(m:1)
works-in
(m:m)
the_instruct
or (m:1)
the_course
(m:1)
the_offer
(m:1)
the_quarter
(m:1)
COURSE_
OFFERING
Fig. 15.5 Semantic schema of a University application
For example, the domain of the relation BIRTH-DATE is PERSON. Although every
person has a date of birth, that date of birth is not always known. Therefore, BIRTH-
DATE is not total.
A binary relation whose range is a concrete category is called an attribute.
Thus, the phrase “a is an attribute of C” means that a is an attribute and its
domain is category C. For example, LAST-NAME, FIRST-NAME and BIRTH-YEAR
are attributes of PERSON.

426
N. Rishe et al.
3.3
Semantic SQL
The Semantic SQL language paradigm is the core interface of the Semantic Wrapper
both to the end user and as a middleware. Semantic SQL was originally designed to
query semantic and object oriented databases, and, as part of the Semantic Wrapper,
it is used to query a semantic schema that is reﬂective of the underlying relational
schema. Semantic SQL is syntactically identical to Standard SQL but assumes a
virtual schema comprised of inﬁnitely-wide tables, one table per semantic category
with all the ﬁelds reachable from it. In the middleware mode, the fact that the
user refers to this virtual schema is technically transparent to third-party tools, thus
allowing for standard protocols, such as ODBC and JDBC. Thus, for example, the
user can utilize a third-party GUI to create a query. The input is user clicks, the GUI
output is SQL. The complexity of the input is proportional to the complexity of the
output. Because the formulation of a query in Semantic SQL is very concise in the
output, so is the user’s input as measured by the number of clicks and the intellectual
complexity of the task.
Because Semantic SQL syntax is identical to the relational SQL syntax, it
supports standard database access interfaces such as ODBC and JDBC. Thus,
Semantic SQL, like standard SQL, is a purely declarative query language. This
is particularly advantageous for users as there no need to learn new query syntax
or a new programming approach. Where Semantic SQL and relational SQL differ
is in the simplicity of Semantic SQL queries as compared to relational queries.
Because semantic databases use real world concepts such as objects and categories,
Semantic SQL is able to query schemas at the conceptual level instead of the table
layout level.
Semantic SQL queries refer to a virtual relational schema that consists of inferred
tables deﬁned as spanning trees containing all reachable relations from a given
category. A virtual table is implicitly deﬁned for each category in the semantic
schema, where all related data is grouped together. Appendix 1 provides a formal
deﬁnition of this grouping. Referring back to our semantic schema in Fig. 15.5, the
following is an example of some of the ﬁelds in the virtual table for the category
STUDENT.
For ease of use, every attribute in a virtual table has a short semantic name and a
long semantic name. A short semantic name is created by removing preﬁxes in the
long attribute name, and can be used for queries so long as there is no ambiguity
within that virtual table. For example, in the virtual table STUDENT, the long
semantic name for one attribute is the student
the offer the quarter year and
its short semantic name is year. As there is no other attribute of the same depth
with the name year in this virtual table, no ambiguity arises from use of this short
semantic name. Conversely, using the abbreviated attribute name would lead to
ambiguity as there would be two possible attributes of the same depth that this could
represent: the student major name and the student minor name. In this case, it
can be disambiguated by using either major name or minor name.

15
Semantic Wrapper: Concise Semantic Querying of Legacy Relational Databases
427
Full Attribute Name
Abbreviation
Type
Sample
STUDENT
–
surrogate
123235
last name
–
string
Smith
birth-year
–
integer
1990
the student
the offer the quarter year
year
integer
2011
the student
the offer the quarter season
season
string
Spring
the student
ﬁnal grade
ﬁnal grade
integer
75
Major
–
surrogate
CS
Minor
–
surrogate
ECE
major name
–
string
CompSci
minor name
–
string
Electrical
In the virtual table for STUDENT, the attribute STUDENT major name (where STUDENT is
the category, MAJOR is the relationship, and NAME is the attribute) refers to “the name of the
department in which a student majors.”
With relational SQL, users usually need to deﬁne a join operation to link two
tables together. While inner joins are hard enough to deﬁne, most realistic queries
require an outer join (left outer join), which is very hard to deﬁne in relational
SQL. As can be seen in the above example, Semantic SQL relieves users of the
need to explicitly express joins. Instead, joins are expressed in the names of the
attributes found in the virtual table. Because of the semantic information inherent
in the schema, there is no ambiguity in the query. This results in simpler query
construction for the user. For example, if a user wants to retrieve the ﬁrst name and
last name of a student whose major department is “computer science,” the Semantic
SQL query is a follows:
select
ﬁrst name, last name from STUDENT
where
major name D ‘computer science’
In the relational SQL, however, the same query might be composed as follows
(depending on the relational schema):
select
ﬁrst name, last name from STUDENT, DEPARTMENT
where
STUDENT.deptID D DEPARTMENT.deptID and
DEPARTMENT.name D ‘computer science’
Semantic SQL queries requesting the retrieval of more complex combinations of
information are still simple. For example, a query to retrieve the student’s last name,
ﬁrst name, address, major, name of each course, ﬁnal grade for the course, year and
semester the course was taken, for every student would be:
select
last name, ﬁrst name, address, major name,
the course name, ﬁnal grade, season, year
from
STUDENT

428
N. Rishe et al.
Should a user prefer to explicitly express join conditions, they can still do so. As
Semantic SQL is completely compatible with relational SQL, the syntax is exactly
the same for both query languages.
Queries to update against a virtual relational database are inherently ambiguous.
Semantic SQL provides disambiguating semantics on the underlying semantic
schema (see Appendix 2 for formal deﬁnitions). As with queries for retrieving data,
standard SQL syntax, such as insert, delete, and update, is used. An example of a
simple update is:
Delete students whose ﬁnal grade is less than 50:
Structure:
delete from C where condition
Example:
delete from STUDENT where FINAL GRADE< 50
4
Components of the Semantic Wrapper
The Semantic Wrapper is primarily comprised of three engineering components
that can be used as either a standalone application or as middleware. This section
provides a high-level description of each of these components, a description of how
these components interact with each other to produce the desired results, and an
example that illustrates the Semantic Wrapper’s capability to be implemented as
middleware.
4.1
The Knowledge Base Tool: Reverse Engineering
of a Semantic Schema
In order to interpret Semantic SQL queries to a relational database, a semantic view
of the database must be created. This is the primary function of the Knowledge Base
Tool (KDB Tool). By using this tool, the user is able to create semantic information
for a relational database of interest via the construction of a semantic schema that
accurately reﬂects the information in and structure of data in the relational database.
This can include the speciﬁcation of inheritance of categories and many-to-many
relations.
The KDB Tool’s capabilities are designed to ensure the integrity of the mapping
between the original relational database and the corresponding semantic schema.
This is accomplished by enforcing a rule at every step of the creation process
that keeps speciﬁc mapping information between the relational database tables and
semantic schema categories and relations intact. The system will not allow changes
to be made to the semantic schema that would damage the integrity of the mapping
information.

15
Semantic Wrapper: Concise Semantic Querying of Legacy Relational Databases
429
Use of the KDB Tool on a standard relational database involves a possible total
of eight steps, four of which are automated and four of which involve the skills
and knowledge of the relational database’s administrator [18]. To automatically
generate the initial semantic schema, it is assumed that metadata regarding tables,
attributes, primary keys, and other constraints are available via the relational
database management system in use. The ﬁrst four steps needed to create the
semantic schema are as follows:
1. A category in the semantic schema is created for each table in the relational
database.
2. Within each semantic category, an attribute is created for every ﬁeld (i.e.,
column) in the corresponding relational table.
3. A semantic relation is created for each functional dependency in the relational
database.
4. Within each semantic category, attributes that correspond to foreign keys in
the relational database are removed (these are reﬂected in the relations that are
created in step 3, rendering these attributes redundant).
A sample relational database structure can be seen in Fig. 15.6, along with its
transformation through step 4 to the semantic schema seen in Fig. 15.7. Perusing
both ﬁgures, it is easy to discern the correspondence between the relational and
semantic schemas. For example, the relational table CURRENCY FOR COUNTRY
corresponds to the semantic category CURRENCY-FOR-COUNTRY. The functional
dependency between CURRENCY FOR COUNTRY and CURRENCY relational tables
transformed into a semantic relationship, “the-currency,” and the foreign key,
THE CURRENCY
CODE KEY, was removed as it is reﬂected in the semantic rela-
tionship between CURRENCY-FOR-COUNTRY and CURRENCY semantic categories.
Once steps 1–4 are completed, a valid semantic view of the database has been
created. This view, however, can be further reﬁned to create a more accurate
reﬂection of the application’s semantics by completing steps 5–8, which require
human intervention, i.e., the skills and knowledge of the relational database’s
administrator (DBA). The reasons for this include the ease at which a database
domain expert can understand semantic databases, the DBA’s in-depth knowledge
of the structure of the relational database, the DBA’s intimate understanding of
needed userviews and end-user needs, and the DBA’s responsibility for the correct
functioning of the database tools.
Steps 5–8 of the semantic schema creation process are as follows:
5. Any semantic categories that correspond to a relational table whose sole purpose
is to represent many-to-many relations should be replaced with actual many-to-
many relationships in the semantic schema.
In our sample schema, this type of transformation can be seen in Fig. 15.8. In
short, the relational table CURRENCY FOR COUNTRY, contains only foreign keys
and has two many-to-one relationships. This is reﬂected in the semantic schema as
a category, CURRENCY-FOR-COUNTRY, that has no attributes and two many-to-one

430
N. Rishe et al.
Fig. 15.6 Relational schema of a geography database
relationships. Thus, it is clear that the sole purpose of this category is to represent a
many-to-many relationship in the relational schema, which can be directly replaced
in the semantic schema as a many-to-relationship.
6. Any semantic categories that correspond to relational tables whose sole purpose
is to represent a recursive reference should be replaced with an is-part-of
relationship in the corresponding semantic category. This relationship may have
a cardinality of many-to-one or many-to-many.
In our sample schema, this type of transformation can be seen in Fig. 15.9. In short,
the relational table CITY NEAR CITY, contains only foreign keys and has two many-
to-one relationships with the CITY relational table. This is reﬂected in the semantic
schema as a category, CITY-NEAR-CITY, that has no attributes and two many-to-

15
Semantic Wrapper: Concise Semantic Querying of Legacy Relational Databases
431
Fig. 15.7 Geography
semantic schema after
transformation through step 4
COUNTRY-
OTHER-NAME
the-other-name
CURRENCY
code_key
name 
COUNTRY
code
name
CURRENCY-
FOR-COUNTRY
the-currency
for
the-country
CITY-NEAR-
CITY
AIRPORT
name
CITY
name
from-city
of
in
to-city
one relationships with the semantic category CITY. Thus, it is clear that the sole
purpose of theCITY-NEAR-CITY category is to represent a recursive reference in the
relational schema.
7. Any semantic categories that correspond to relational tables whose sole purpose
is to represent a one-to-many relationship should be replaced with a one-to-many
attribute in the corresponding semantic category.
In our sample schema, this type of transformation can be seen in Fig. 15.10. In
short, the relational tables COUNTRY OTHER NAME andCOUNTRY have attributes
that contain the same information (the name of a country). Further, the table COUN-
TRY OTHER NAME has no other attributes and only a one-to-many relationship with
the COUNTRY relational table. This is similarly reﬂected in the semantic schema.
Thus, it is clear that the sole purpose of theCOUNTRY-OTHER-NAME category is to
represent a one-to-many relationship in the relational schema and should be replaced
with a one-to-many attribute, other-name, in the COUNTRY semantic category.
8. Include the relevant category inheritance hierarchy into the semantic schema.
Step 8 introduces an additional level of abstraction to the semantic schema. In our
sample schema, this type of transformation can be seen in Fig. 15.11. In short, the

432
N. Rishe et al.
CURRENCY
code_key
name 
COUNTRY
code
name
CURRENCY-
FOR-COUNTRY
the-currency
for
for
(m:m)
COUNTRY
code
name
CURRENCY
code
name 
is replaced by:
Fig. 15.8 Step 5 of the KDB
tool on the geography
database
CITY-NEAR-
CITY
CITY
name
to-city
from-city
CITY
->near (m:m)->
name 
is replaced by:
Fig. 15.9 Step 6 of the KDB tool on the geography database
COUNTRY-
OTHER-NAME
the-other-name
COUNTRY
code
name
the-
country
COUNTRY
code
name 
other-name (1:m)
is replaced by:
Fig. 15.10 Step 7 of the KDB tool on the geography database

15
Semantic Wrapper: Concise Semantic Querying of Legacy Relational Databases
433
COUNTRY
code
name
AIRPORT
name
CITY
name
of
in
GEOGRAPHICAL
ENTITY
name 
AIRPORT
CITY
of
in
COUNTRY
code
is replaced by:
Fig. 15.11 Step 8 of the
KDB tool on the geography
database
relational tables COUNTRY, CITY and AIRPORT all have an attribute in common;
that is, they all have a name. Because of this commonality, a supercategory,
GEOGRAPHICAL-ENTITY, with the attribute name can be introduced into the
schema, and the name attribute can be removed from the aforementioned categories.
In addition, prior to deﬁning a virtual table, the name of every category and
relation is “cleaned” as follows:
1. Replace all non-alphanumeric characters with “ ”
2. If the name begins with a digit or “ ”, prepend “A”
3. If the name ends with “ ” append “Z”
4. Collapse multiple “ ” into a single “ ”
Once this is completed, the systems checks to ensure that no ambiguity has been
introduced. If this process does introduce any ambiguity, the schema is rejected.

434
N. Rishe et al.
4.2
Knowledge Base (KB)
The Semantic Wrapper’s Knowledge Base (KB) is the interface between the KDB
Tool and the Query Translator. All of the mapping information that is generated
by the KDB Tool during semantic schema creation is saved in the KB. The KB is
an Extensible Markup Language (XML) [19] ﬁle that will subsequently be used to
translate Semantic SQL queries into standard SQL queries by the Query Translator
(see Sect. 3.3). Speciﬁcally, the KB stores all the needed information for both the
relational and semantic database schemas, as well all derivation rules for query
translation. Its XML format, in particular, provides a ﬂexible, robust and easy to
use avenue for capturing complex semantic information in conjunction with the
relational and semantic schemas.
Along with the KDB Tool, the KB includes sets of inference rules that can
be used to generate new knowledge that is needed during query translation.
This is particularly useful when there is not enough information to complete the
transformation of the semantic query, such as when the Semantic Wrapper is being
used to integrate data from heterogeneous multi-database environments.
4.3
Query Translator: Automatic Query Conversion
The Query Translator is the central processor of the Semantic Wrapper. This
component is responsible for transforming the easy to use Semantic SQL queries
that are based on the semantic schema into the more complex corresponding
relational SQL queries on the relational database. While this component translates
Semantic SQL queries into relational SQL queries that are signiﬁcantly more
complex, they are still semantically equivalent to the original query posed by
the user.
To accomplish this, the Query Translator interfaces with the KB to retrieve and
use the semantic and relational schema information recorded during the semantic
schema creation (see Sect. 4.1), as well as any needed derivation rules. Once
the Query Translator retrieves needed information from the KB, it generates the
appropriate projections on virtual tables via the use of temporary views. Query
results are generated through the application of outer joins or sub-queries explicitly
in the Where clause between these temporary views. The constructed relational SQL
queries are subsequently transmitted to the relational database management system
via a standard interface.
Semantic SQL queries are often an order of magnitude shorter than the cor-
responding relational SQL queries. The complexity involved in transforming a
Semantic SQL query into a relational SQL query is best seen by a direct comparison
of SQL statements on a real world semantic schema and its relational counterpart.
Referring back to Figs. 15.2 and 15.3, some examples of real world Semantic

15
Semantic Wrapper: Concise Semantic Querying of Legacy Relational Databases
435
SQL queries and their functionally equivalent relational SQL queries on these
subschemas include:
Example 15.1. List of the time and housing of temperature measurements
over 50ı
Semantic SQLStatement
Relational SQL Statement
select housing, time from
select housing, time
MEASUREMENT where
from
of name D ’Temperature’ and
PHYSICAL OBSERVATION STATION,
value > 50
MEASUREMENT
where exists
(select  from MEASUREMENT-TYPE
where name key D of name and
name key D ’Temperature’ and
by physical observation station id D
physical observation station id key
and
value > 50)
Example 15.2. The descriptions of organizationsand locations of their ﬁxed stations
Semantic SQLStatement
Relational SQL Statement
select description, LOCATION
select description,
from ORGANIZATION
LOCATION.north UTM in key,
LOCATION.east UTM in key from
ORGANIZATION, LOCATION where
exists (select  from FIXED STATION
where exists (select  from
PHYSICAL OBSERVATION STATION
BELONGS TO ORGANIZATION where
name key D organization name in key and
PHYSICAL OBSERVATION STATION
BELONGS TO ORGANIZATION.
physical observation station id in yy D
FIXED STATION.
physical observation station id key
and located at north UTM D
north UTM in key and
located at east UTM D
east UTM in key))

436
N. Rishe et al.
Example 15.3. The observations since January 1, 1993 (including images,
measurements, and their types) with location of the stations
Semantic SQLStatement
Relational SQL Statement
select OBSERVATION ,
of , LOCATION from
OBSERVATION where
time > ’1993/01’
(select MEASUREMENT TYPE. ,
LOCATION.north UTM in key,
LOCATION.east UTM in key,
MEASUREMENT., NULL, NULL, NULL, NULL,
NULL, NULL, NULL, NULL, NULL from
MEASUREMENT TYPE, LOCATION,
MEASUREMENT where time > ’1993/01’ and
exists (select  from FIXED STATION where
by physical observation station id D
physical observation station id key and
located at north UTM D north UTM in key and
located at east UTM D east UTM in key and
of name D name key)) union (select
MEASUREMENT TYPE., NULL, NULL,
MEASUREMENT., NULL,NULL, NULL, NULL,
NULL, NULL, NULL, NULL, NULLfrom
MEASUREMENT TYPE, MEASUREMENT where
time > ’1993/01’ and not exists (select  from
FIXED STATION
whereby physical observation station id D
physical observation station id key and of name
D name key)) union (select NULL, NULL, NULL,
NULL, LOCATION.north UTM in key,
LOCATION.east UTM in key, NULL, NULL,
NULL, NULL, NULL, NULL, IMAGE. from
LOCATION, IMAGE where time > ’1993/01’ and
exists (select  from FIXED STATION where
by physical observation station id D
physical observation station id key and
located at north UTM D north UTM in key and
located at east UTM D east UTM in key))
union (select NULL, NULL, NULL, NULL, NULL,
NULL, NULL, NULL, NULL, NULL, NULL,
NULL, IMAGE. from IMAGE where time >
’1993/01’ and not exists (select  from
FIXED-STATION where
by physical observation station id D
physical observation station id key))
As can be seen in these three examples, the relational SQL queries that are
constructed by the Query Translator are often substantially larger and more complex
than the semantic SQL queries created by users.

15
Semantic Wrapper: Concise Semantic Querying of Legacy Relational Databases
437
Fig. 15.12 Overall architecture of the Semantic Wrapper as middleware for a web application
4.4
Semantic Wrapper as Middleware
The Semantic Wrapper is a middleware system that provides semantic views over
legacy relational databases. As middleware, this system provides straightforward,
easy access to legacy relational databases without requiring users to switch from
their existing interfaces to a new, unfamiliar interface. As illustrated in Fig. 15.12,
the Semantic Wrapper can be employed in many environments and for numerous
applications, including as middleware for web applications.
The Semantic Wrapper greatly improves usability by allowing the use of standard
SQL syntax to access the semantic layer via more simpliﬁed and concise SQL

438
N. Rishe et al.
queries than what would be required for direct querying of a standard relational
database. This approach is also applicable in a heterogeneous multi-database
environment that can include both structured (relational and semantic databases)
and unstructured data (social media and related Internet sites).
5
Conclusion
The Semantic Wrapper is a middleware system that can be used to greatly improve
the ability to meet the intense and ever-changing data management needs of today’s
world. The Semantic Wrapper provides an easy to use method for accessing legacy
and relational databases, while still maintaining the ability to be implemented as a
standalone solution. It allows users to continue to use familiar GUIs and greatly
decreases the complexity of SQL syntax needed from users to fulﬁll their data
requests. Finally, the system can be used over both structured and unstructured,
heterogeneous data sources, providing a set of tools that can easily incorporate new
and diverse sources of data.
Acknowledgement This research was supported in part by NSF grants CNS-0821345, CNS-
1126619, HRD-0833093, IIP-0829576, CNS-1057661, IIS-1052625, CNS-0959985, OISE-
1157372, CNS-1042341, CNS-0837556, DGE-0549489, IIS-0957394, IIS-0847680, IIP-0934339,
and OISE-0730065. We are most grateful to NSF Program Directors Rita Virginia Rodriguez,
Maria Zemankova, Richard Smith, Rathindra DasGupta, Demetrios Kazakos, and Alex
Schwarzkopf.
Appendix 1: Semantic SQL Virtual Tables Formal Deﬁnition
The virtual table T .C/ for a category C, recursive deﬁnition:
1. The ﬁrst attribute of T:
C — attribute of T , range: C.m W 1/
2. For every attribute A of T , for every relation r applicable to the range of A:
A r — attribute of T , range: range(r) (m:1)
Note: this virtual table is inﬁnite. When interpreting a speciﬁc query, a ﬁnite
projection of this table is assumed as further explained in Technical Notes.
The name of T is the same as of C.
Note: to-many original relations are reduced to to-one attributes of the virtual table.
If the semantic relation r is many-to-many or one-to-many, the new attribute would
be many-to-one, but many virtual rows would exist in the table T , one for each
instance of the tree. If r has no value for an object, a null value will appear in the
virtual relational table.

15
Semantic Wrapper: Concise Semantic Querying of Legacy Relational Databases
439
The relation r may be inferred. The range of a virtual attribute may be of multi-
media types: numbers with unlimited varying precision and magnitude, texts of
unlimited size, images, etc.
Abbreviation of Preﬁxes
•
Every component relation r in the virtual attribute name may be named by its
full semantic name or, if no ambiguity arises, by its short semantic name.
•
The attribute names of T contain long preﬁxes. These preﬁxes can be omitted
when no ambiguity arises, i.e.: attribute y is an abbreviated synonym of the
unabbreviated attribute x y of T if T has no other unabbreviated attribute z y
where depth (z)  depth (x).
depth (x) is the number of relations present in x.
Surrogates
All attributes of T .C/ of type Abstract are replaced by their surrogates of type
String.
Deﬁnition of the Extension of a Table
The virtual table T for a category C is logically generated as follows:
1. Initially, T ŒC D C, i.e. T contains one column called C, whose values are the
objects of the category.
2. For every attribute A of T , for every schema relation or attribute r whose domain
may intersect range.A/, let R be the relation r with its domain renamed A and
range renamed A r, let T be the natural right-outer-join of T with R. (Unlike a
regular join, the outer join creates A r D null when there is no match.)
3. For a given query q the virtual table against which q is interpreted, T ŒC; q, is a
projection of T ŒC on the following virtual attributes:
•
the virtual attributes that appear in the query,
•
the unabbreviated preﬁxes of said attributes (including the surrogate
attribute C),
•
and the attributes p r where p is any of said preﬁxes and r is an original
printable-type to-one attribute of the semantic schema.
Note: the projection operation here is a set operation with duplicated tuples
eliminated.

440
N. Rishe et al.
User-Control of Table Depth
(Used only by sophisticated users trying to outsmart $MAXDEPTH deﬁned by a
graphical user interface; not needed by users posing direct SQL queries without
a GUI.)
•
For each category C, in addition to the default table named C, of depth limited
by $MAXDEPTH, there are also tables called C i for any positive integer i, with
the depth limited by i rather than $MAXDEPTH. Also, there is a table C 0 which
includes only the original to-one attributes and relations whose domain is C or a
supercategory of C and the surrogate attribute of C.
ODBC Schema Queries
•
The ODBC request for the names of all tables is interpreted as: for every category
get the primary virtual table C and the tables C 0 and C 1.
•
The ODBC request for the names of all attributes of a given virtual table T
returns all attributes maximally abbreviated. If the request is for the virtual table
corresponding to a category C, only attributes of C 2 are returned
•
The ODBC request to browse the virtual table is denied. (Browsing of C 0 is
permitted. Browsability of C 1 is not guaranteed)
Appendix 2: Disambiguation of Arbitrary Semantic SQL
Updates
Let C be a category against which an update operation is performed.
Notation:
T D T .C/ – the virtual table of C.
A – the list of full names of attributes of T that are assigned values in the
operation.
R1; : : :; Rn – the set of relations of C such that for some sufﬁx s; Ri
s is in A.
(That is, Ri
s is a two-step or deeper attribute.)
C1; : : :; Cn – the ranges of R1; : : :; Rn.
Si – list .sjRi
s in A) in the order of appearance in A.
V.a/ – For every attribute a in A let V.a/ be the value being assigned to the
attribute a. For every s in Si let V.s/ be the value assigned to Ri
s. Let V.Si/
be the list of V.s/ where s in Si.
Ei – the list of assignments s D V.s/ for s in Si.

15
Semantic Wrapper: Concise Semantic Querying of Legacy Relational Databases
441
(1) delete from C where condition
(a) Perform: select C from C where condition
(b) For every resultant object o in C: remove o from C.
Example:
delete from STUDENT where FINAL GRADE<50
(2) insert into C (attributes) values (assignments)
(c) Create a new object in C. Let this object be denoted o. Its one-step
relationships are assigned values from the assignments. If a one-step
relationship is m:m or 1:m then only one value may be assigned.
(d) For every category Ci in C1: : :Cn do:
(1) if Ri
Ci is in A and V (Ri Ci) D“new”
then recursively perform:
insert into Ci(Si) values (V (Si));
let v be the object inserted above
else do:
(2.1) perform: select Ci from Ci where Ei
(2.2) if the above select results in exactly one object,
then denote that object v
else abort with an error message
(2) relate: o Riv
Example: create a new student James in the department in which Johnson works and
enroll Jim in the only existing offering of “Magnetism”:
insert into STUDENT
(FirstName, Major WorksIn
LastName, Enrollment, The Course)
values (’James’,’Johnson’,’new’,’Magnetism’)
(3) insert into C (attributes) query
(e) Evaluate the query, resulting in a set of rows.
(f) For each row r perform: insert into C(A) values (r)
Example: For every instructor create a department named after him and make him
work there:
insert into DEPARTMENT
(Name, WorksIn )
select
LastName, Instructor
from
Instructor

442
N. Rishe et al.
(4) update C set assignments where condition
(g) perform:
select C from C where condition
(h) for every object o in the result of the above query perform:
(1) The object’s one-step relationships are assigned values from the assign-
ments, i.e.: for every one-step attribute Ai in A perform: o:Ai WD V (Ai)
(2) For every category Ci in C1: : :Cn do:
(2a) if Ri
Ci is in A and V (Ri Ci) D“new”
then recursively perform:
(2a1) insert into Ci(Si) values (V (Si));
(2a2) let v be the object inserted above
(2b) else do:
(2b1) perform: select Ci from Ci where Ei
(2b2) if the above select results in exactly one object,
then denote that object v
else abort with an error message
(2c) o:Ri WD v
(5) insert into C
R: : :
Allows creation of multiple relationships R. This cannot be accomplished with
previous commands when R is many-to-many and many values need to be assigned.
Note: C
R has been deﬁned as a virtual table.
Example: let Johnson work in Physics
insert into INSTRUCTOR WorksIn (INSTRUCTOR,DEPARTMENT)
select distinct INSTRUCTOR, DEPARTMENT
from INSTRUCTOR, DEPARTMENT
where INSTRUCTOR.LastNameD’Johnson’ and
DEPARTMENT.NameD’Physics’
Example: let Johnson work in every department
insert into INSTRUCTOR WorksIn (INSTRUCTOR,DEPARTMENT)
select distinct INSTRUCTOR, DEPARTMENT
from INSTRUCTOR, DEPARTMENT
where INSTRUCTOR.LastNameD’Johnson’
(6) delete from C Rwhere condition
Allows deletion of multiple relationships R.
Example: do not let Johnson work in any department Smith works in.
delete from INSTRUCTOR WorksIn
where LastNameD’Johnson’ and WorksIn (
select WorksIn from INSTRUCTOR where LastNameD’Smith’)
(7) Object surrogate assignment: if in an insert statement there is an assignment of
a user-supplied value to an object being created, that value becomes the object’s
surrogate, overriding surrogates generated by other algorithms. In the database it
is entered into the attribute UserSuppliedSurrogate, which is enforced to be 1:1.

15
Semantic Wrapper: Concise Semantic Querying of Legacy Relational Databases
443
Further, if this value begins with the character “#” the database will derive the
internal object id from this value — it may have effect only on efﬁciency. If this
value begins with a “$” it will be automatically erased at the end of the session.
Example:
insert into INSTRUCTOR (Instructor, FirstName)
values (’John’,’John’)
Note: any expression producing an abstract object is automatically converted into
that object’s surrogate.
References
1. M. Egenhofer. “Why not SQL!” International Journal on Geographical Information Systems,
vol. 6, no.2, 1992, p. 71–85.
2. H.V. Jagadish, A. Chapman, A. Elkiss, M. Jayapandian, Y. Li, A. Nandi, and C. Yu.
“Making Database Systems Usable”. ACM’s Special Interest Group on Management of Data
(SIGMOD), Beijing, China, June 11–14, 2007.
3. A. E. Wade. “Hitting the Relational Wall”. Objectivity Inc. White Paper. [Online] 2005. http://
www.objectivity.com/pages/object-oriented-database-vs-relational-database/default.html.
4. E.F. Codd. “A Relational Model of Data for Large Shared Data Banks”. Communications of
the ACM, vol. 13, no. 6, 1970, pp. 377–387.
5. E.F. Codd. “The Relational Model for Database Management”. Reading, MA, Addison-
Wesley, 1990.
6. G. Russell. “Database eLearning”. [Online] http://db.grussell.org/index.html.
7. M. Berler, J. Eastmen, D. Jordan, C. Russell, O. Schadow, T. Stanienda, F. Velez. “The Object
Data Standard: ODGM 3.0”. [ed.] K.D. Barry, R.G.G. Cattell. San Francisco, CA, Morgan
Kaufmann, 2000.
8. A. Eisenberg, J. Melton. “SQL: 1999, formerly known as SQL3”. ACM SIGMOD Record,
vol.28, no.1, 1999.
9. N. Rishe. “Database Design: The Semantic Modeling Approach”. New York, NY, McGraw-
Hill, 1992.
10. N. Rishe, J. Yuan, R. Athauda, S.C. Chen, X. Lu, X. Ma, A. Vaschillo, A. Shaposhnikov,
D. Vasilevsky. “Semantic Access: Semantic Interface for Querying Databases”. Proceedings of
the 26th International Conference On Very Large Data. Cairo, Egypt, September 10–14, 2000.
pp. 591–594.
11. A. Cary, Y. Yesha, M. Adjouadi, N. Rishe. “Leveraging Cloud Computing in Geodatabase
Management”. Proceedings of the 2010 IEEE Conference on Granular Computing GrC-2010.
Silicon Valley, CA, August 14–16, 2010. pp. 73–78.
12. L. Yang, N. Rishe. “Formal Representation and Transformation of DTDs to Sem-ODM”. The
2006 International Conference on Foundations of Computer Science, FCS06. Las Vegas, NV,
June 26–29, 2006, pp.182–188.
13. L. Yang, N. Rishe. “Transforming Sem-ODM Semantic Schemas to DTDs”. Proceedings of the
43rd ACM Southeast Conference, ACMSE 2005. Kennesaw, GA, vol. 1, March 18–20, 2005,
pp. 237–242.
14. N. Rishe, A. Barreto, M. Chekmasov, D. Vasilevsky, S. Graham, S. Sood. “Semantic Database
Engine Design”. Proceedings of the 7th International Conference on Enterprise Information
Systems, ICEIS 2005. Miami, FL, May 24–28, 2005, pp. 433–436.
15. N. Rishe, A. Barreto, M. Chekmasov, D. Vasilevsky, S. Graham, S. Sood. “Object ID Dis-
tribution and Encoding in the Semantic Binary Engine”. Proceedings of the 7th International
Conference on Enterprise Information Systems, ICEIS 2005. Miami, FL, May 24–28, 2005,
pp. 279–284.

444
N. Rishe et al.
16. G. Aslan, D. McLeod. “Semantic Heterogeneity Resolution in Federated Databases by
Metadata Implantation and Stepwise Evolution”. The VLDB Journal, vol. 8, no. 2, 1999,
pp.120–132.
17. N. Rishe. “Semantic SQL”. Internal Document, High Performance Database Research Center,
School of Computer Science, Florida International University, 1998.
18. N. Rishe, T. Huang, M. Chekmasov, S. Graham, L. Yang, S. Himelsback. “Semantic Wrapping
Tool for Internet”. Proceedings of the 6th World Multiconference on Systemics, Cybernetics
and Informatics SCI-2002, Orlando, FL, July 14–18, 2002, pp. 441–445.
19. Extensible Markup Language (XML) 1.0 (Fifth Edition). W3C Recommendation. [Online]
November 26, 2008. http://www.w3.org/TR/REC-xml/.

Part III
Security


Chapter 16
Security in Data Intensive Computing Systems
Eduardo B. Fernandez
1
Introduction
Many applications, e.g., scientiﬁc computing, weather prediction, medical image
processing, require the manipulation of large amounts of data [6]. Analysis of
web trafﬁc, sales, travel, and all kinds of human activities can bring valuable
insights for business and science [27]. This work has been done until now in large
multiprocessors in the computer centers of large institutions, whose increasing
power allows more and more aspects to be analyzed and with more detail [29].
Recently, the cloud has brought the possibility of processing and storing large
amounts of data at a relatively low cost and from anywhere in the world. However,
this wide accessibility increases the vulnerability of their systems and the emphasis
on fast processing leads often to sacriﬁcing security. We survey here the security
implications of data intensive applications in the new environments. A more speciﬁc
discussion, considering just clouds, is given in [35].
Data is a valuable resource and many attacks are being reported every day. We
don’t want the data handled by institutions to be seen by those who could misuse
it; enterprises want their information to be hidden from competitors, and patients
don’t want their medical records to be seen by unauthorized people. We don’t want
that information changed either. Data and other resources are considered assets;
security is the protection of assets, including institution and individual information
[1,12,17].
We have to accept the fact that there are people who intentionally try to
misuse information either for their own gain, to make a point, or for the sake
E.B. Fernandez ()
Department of Comp. and Elect. Eng. and Comp. Science,
Florida Atlantic University, Boca Raton, FL, USA
e-mail: ed@cse.fau.edu
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 16, © Springer Science+Business Media, LLC 2011
447

448
E.B. Fernandez
of doing damage. Some of their actions include viruses and similar attacks that
corrupt information; some actions are to access or modify information. Attacks can
be external or from employees looking for revenge or monetary gain.
Clearly, all the usual aspects of security apply to data intensive systems, we
concentrate here only on those aspects peculiar to them, i.e., due to the nature
of these applications and corresponding platforms. In particular, we leave out the
privacy aspects of data mining (see [45] in this book). Also, we concentrate on
recent developments since much has been written already about these subjects.
We provide ﬁrst a short introduction to security for those readers who are not
experts on this topic. This is followed by a brief discussion of privacy. The next
section is a discussion of web architectures. We then look at a few data-intensive
applications, including a discussion of their security requirements. This is followed
by a look at the platform architectures used to execute these applications to see what
security they provide for the applications. We end with some conclusions where we
summarize the situation and propose some actions for the future.
2
Security
Security is the protection against:
•
Unauthorized data disclosure (conﬁdentiality or secrecy) – Loss of secrecy may
bring economic losses, privacy violations, and other types of damage.
•
Unauthorized data modiﬁcation (integrity) – Unauthorized modiﬁcation of data
may result in inconsistencies or erroneous data. Data destruction may bring all
kinds of losses.
•
Denial of service – Users or other systems may prevent the legitimate users from
using their system. Denial of service is an attack on the availability of the system.
•
Lack of accountability – Users should be responsible for their actions and should
not be able to deny what they have done (non-repudiation).
This list deﬁnes the objectives of a secure system which are how to avoid or
mitigate the effects of these attacks or misuses of information. These aspects,
sometimes called security goals or properties, try to anticipate the possible risks
of the system, that is, the types of attacks against which we have to be prepared.
Different applications may emphasize one or more of these aspects over others.
For example, the military worry more about conﬁdentiality, a ﬁnancial institution
may emphasize accountability and integrity, homeland security systems usually
emphasize availability.
The generic attacks mentioned above are realized in two basic ways: by direct
attacks from a person trying to exploit a vulnerability or ﬂaw in the system, or
using malware, software that contains code that exploits one or more of these ﬂaws.
Sometimes, a combination of the two is used some are insider attacks. We need to
ﬁnd defenses or countermeasures against these attacks.

16
Security in Data Intensive Computing Systems
449
The deﬁnition of security above describes security as defense against some types
of attacks. The generic types of defenses that we can use include:
•
Identiﬁcation and Authentication (I& A) – Identiﬁcation is a user or system action
where the user provides an identity. Authentication implies some proof that a user
or system is the one he/it claims to be. The result of authentication may be a set
of credentials, which later can be used to prove identity and may describe some
attributes of the authenticated entity.
•
Authorization and Access control (A & A) – Authorization deﬁnes permitted
access to resources depending on the accessor (user, executing process), the
resource being accessed, and the intended use of the resource. Access control
implies some mechanism to enforce authorization.
•
Logging and Auditing – These functions imply keeping a record (log) of actions
that may be relevant for security and analyzing it later. They can be used to collect
evidence for prosecution (forensics) and to improve the system by analyzing why
the attack succeeded.
•
Hiding of information – It is usually performed by the use of cryptography but
steganography is another option. The idea is to hide the information in order to
protect it.
•
Intrusion detection – Intrusion Detection Systems (IDS) alert the operators or
users in real time when an intruder is trying to attack the system.
Before studying possible defenses we need to know the types of attacks that we
will receive and why they can succeed. A vulnerability is a state or condition that
may be exploited by an attack. People study source code and ﬁnd ﬂaws that can
lead to possible attacks (threats). An attack is an attempt to take advantage of
a vulnerability and it may result in a misuse, a violation of security. An actual
attack is an incident. The outcome of misuse can be loss of conﬁdentiality or
integrity, theft of services, denial of service, or repudiation of some action. An
attack has a perpetrator, who has a motivation (goal) and possibly appropriate
skill. This motivation may be monetary gain (lucre), need to show off his skills,
or political/religious motivation. The attack has a method of operation (modus
operandi) to accomplish a mission with respect to a target. The damage of a mission
can be loss of assets, money, or even lives.
Vulnerabilities can be the result of design or implementation errors, although
not every program error is a vulnerability. Design vulnerabilities are very hard to
correct (one needs to redesign and reimplement the system), while implementation
vulnerabilities can be corrected through patches. Examples of wrong design include
using lack of checks, lack of modularity, or not using appropriate defenses. Exam-
ples of bad implementation include not checking size bounds for data structures and
giving too much privilege to a process. Deployment, conﬁguration, administration,
and user errors are also sources of vulnerabilities.
In a direct attack, the attacker takes advantage of a vulnerability to gain access to
information. A typical attack is SQL injection, where the attacker inserts a database
query in an input form. Direct attacks often are prepared through malicious code.

450
E.B. Fernandez
The objective of an attack is a misuse of the information to reach some goal
for the attacker. An external attacker may try to ﬁnd a way to transfer the list of
customers of an institution; an insider can take advantage of incomplete authoriza-
tion to modify some patient’s prescription; an external attacker can inﬁltrate many
computers and convert them into zombies that will be used later for a denial of
service attack; a legitimate customer of a ﬁnancial institution may deny having given
an order to buy some type of stock.
A variety of programs have been written with the purpose of preparing or
performing a misuse. They are what is called malicious software or malware.
These include worms, viruses, and Trojan Horses. Other varieties of malicious code
include logic bombs, rootkits, etc. [12].
To secure a system, we need ﬁrst to enumerate its possible threats. The misuse
activities approach [5] consists of a systematic way to identify system threats, and
determining policies to stop and/or mitigate their effects. To do that, two stages
are applied. The ﬁrst stage is an analysis of the ﬂow of events in a use case
or a group of use cases, in which each activity is analyzed to uncover related
threats. This analysis should be performed for all the system uses cases. The
second stage entails a selection of appropriate security policies which can stop
and/or mitigate the identiﬁed threats. Realization of the policies leads to security
mechanisms.
3
Privacy
Privacy is the right of individuals to decide about the use of information about
themselves. This right is recognized by all civilized societies and is considered a
fundamental human right. While there are laws to protect privacy against govern-
ment intrusion, in the US there is little to protect people against business collecting
private information, except the voluntary decisions of these businesses. Obviously,
privacy laws mean nothing without having ways to enforce their restrictions in
the institutions that handle individuals’ data. Their approaches to data security are
then fundamental to any protection of privacy. Without effective authentication
and access control systems, privacy is left to the good will of employees and
other individuals. It should be noted that attempts to make information more
readily available for improved use as well as the large amounts of information
now collected may have a negative effect on privacy. For example, in the US the
Health Insurance Portability and Accountability Act (HIPAA) regulations require
the computerization and exchange of medical records over the Internet to improve
service to patients. However, this easier availability makes this data also easier to be
accessed by unauthorized parties. Unless there is a correlated investment on security,
privacy will be negatively affected. Social networks are another source of privacy
violations.

16
Security in Data Intensive Computing Systems
451
4
Web Architectures
There is a tradeoff here between speed and security. Two protocols commonly
used are:
SOAP is a protocol for exchanging structured information in the implementation
of web services. It relies on Extensible Markup Language (XML) for its message
format, and usually relies on other application protocols, most notably Remote
Procedure call (RPC) and HTTP, for message negotiation and transmission. SOAP
provides a basic messaging framework to allow web services to communicate.
Representational State Transfer (REST) is a style for web systems. REST-style
architectures consist of clients and servers. Clients initiate requests to servers;
servers process requests and return appropriate responses. Requests and responses
are built around the transfer of representations of resources. A resource can be
essentially any meaningful entity that may be addressed. A representation of a
resource is typically a document that captures the current or intended state of a
resource.
Most of the web functionality on the Internet now uses REST: Twitter, Yahoo’s
web services use REST, others include Flickr, del.icio.us, pubsub, bloglines,
technorati, and several others. eBay and Amazon have web services for both REST
and SOAP. SOAP is mostly used for enterprise applications to integrate a wide
variety of types and number of applications and another trend is to integrate with
legacy systems, etc. The emphasis of REST is speed and it has almost no provisions
for security. SOAP and web services provide a rich set of security standards at the
expense of adding more complexity and losing speed.
4.1
Service-Oriented Architectures (SOA)
SOA is an architectural style in which a system is composed from a set of loosely
coupled services that interact with each other by sending messages. In order to
interoperate, each service publishes its description, which deﬁnes its interface and
expresses constraints and policies that must be respected in order to interact with
it. A service (set of services) represents a business activity and thus it is a building
block for service-oriented applications. Applications are built by coordinating and
assembling services. A key principle about services is that they should be easily
reusable and discoverable, even in an inter-organizational context. Furthermore,
the channels of communication between the participating entities in a service-
oriented application are much more vulnerable than in operating systems or within
the boundaries of an organization’s intranet, since they are established on public
networks
Web services (WS) are the most popular approach to implementing SOA; they
deﬁne an abstract framework, called the Web services platform which is comprised

452
E.B. Fernandez
of several parts. Many of these parts address a particular aspect of the common
SOA and are deﬁned by standards organizations and implemented on proprietary
technology platforms
These organizations have deﬁned a rich set of security standards for web services
interaction. We have represented most of those standards as patterns [10,13].
4.2
Grid Computing
A computing grid is typically heterogeneous in nature (nodes can have differ-
ent processor, memory, and disk resources), and consists of multiple disparate
computers distributed across organizations and often geographically using wide-
area networking communications usually with relatively low-bandwidth. Grids
are typically used to solve complex computational problems which are compute-
intensive requiring only small amounts of data for each processing node. A variation
known as data grids allow shared repositories of data to be accessed by a grid
and utilized in application processing, however the low-bandwidth of data grids
limit their effectiveness for large-scale data-intensive applications. In contrast, data-
intensive computing systems are typically homogeneous in nature (nodes in the
computing cluster have identical processor, memory, and disk resources), use high-
bandwidth communications between nodes such as gigabit Ethernet switches, and
are located in close proximity in a data center using high-density hardware. Grid
computing is a distributed computing environment composed of resources from
different administrative domains. Grids focus on integrating existing resources
with their hardware, operating systems, local resources management, and security
infrastructure [16].
Some grid applications for data intensive applications are shown in [44]. GDIA,
built on top of a meta-scheduler and a data grid, is a scalable grid infrastructure for
data intensive applications that applies security controls [25]. Hameurlain et al. [21]
indicate that security is one of the challenges of grid computing for data intensive
systems but no solutions are discussed.
4.3
Cloud Computing
Cloud computing is the latest computational paradigm and one of its main uses
is data intensive computing. Clouds consist of three layers: Software as a Service
(SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS).
Figure 16.1 shows the layers within the cloud.
In Software as a Service the user rents software from the cloud provider instead
of buying it. The software is hosted in the data center managed by the provider, and
it is available over the Internet. Typical examples of SaaS include word processors,

16
Security in Data Intensive Computing Systems
453
Fig. 16.1 Cloud computing
layers
spreadsheets, database applications, etc. Salesforce Customer Relationship Manage-
ment (CRM) system and Google Aps are two examples for SaaS.
Platform as a Service offers a development environment for developers where
they can run their applications. A programming platform is offered to the user
(developer), which provides a complete software lifecycle management, from
planning to design to building applications to testing to maintenance. Two good
examples are Google App Engine and the Force.com. Google App Engine offers
APIs for storage, platform, and database. It supports applications written in different
programming languages such as Java and Python. The Force.com platform is
similar, but it uses a custom programming language called Apex. It offers cloud
platform where users can develop, package, and deploy applications without any
infrastructure.
Infrastructure as a Service is the foundation layer that provides computing
infrastructure to customers rather than buying servers, software, or networking com-
ponents. IaaS layer virtualizes computing power, storage and network connectivity
of the data centers, and offers it as services to customers. Amazon, GoGrid, and
Google are some of the companies that offer infrastructure services. Amazon’s EC2
provides resizable compute capacity in the cloud. GoGrid offers cloud computing
services that enables automated provisioning of virtual and hardware infrastructure
over the Internet. A discussion of cloud use for data intensive computing is in [20].
Service Level Contracts (SLAs) are fundamental to specify the duties of the
provider and the customer. They deﬁne the beneﬁts and responsibilities of each
party. The only means the provider can gain trust of client is through the SLA. Due
to the dynamic nature of the cloud, continuous monitoring on Quality of Service
(QoS) attributes is necessary to enforce SLAs [36].
5
Data Intensive Applications
Purely data-intensive applications process multiterabyte-to-petabyte-sized datasets.
This data may come in several different formats and is often distributed across
multiple locations. Processing these datasets typically takes place in multistep

454
E.B. Fernandez
analytical pipelines that include transformation and fusion stages. Research issues
involve data management, ﬁltering and fusion techniques, and efﬁcient querying
and distribution [18]. This, and most other studies don’t mention security as an
important issue. We enumerate below a few important applications and their security
requirements.
5.1
Scientiﬁc Collaboration
An important use of large amounts of data is in the collaborative analysis and
transformation by groups of scientists performing experiments or analyzing the
results of experiments. A similar example is manipulation of climate model
data. Researchers must assemble and analyze large datasets that are archived in
different formats on disparate platforms and must extract portions of datasets to
compute statistical or diagnostic metrics in place. This leads to a common virtual
environment in which to access both climate model datasets and analysis tools is
provided to support not only ready access to climate data but also facilitate the
use of visualization software, diagnostic algorithms, and related resources. Another
application of this type is the analysis of planetary data [30–32].
Given that this data is to be used in scientiﬁc research, its main security concern
is that the data is used by legitimate registered scientists and their associates, which
calls for strong authentication. Read-only enforcement is also important because
alterations to the data could make it useless, which calls for a basic authorization
system. Logging and non-repudiation are not important in this environment, avail-
ability is important but not critical.
5.2
Computerization and Exchange of Medical Information
Medical information is one of the most sensitive types of information. Its misuse
could have a very serious effect on an individual’s life; leakage of information
about a psychiatric treatment could ruin a career, an incorrect change in a medical
record may result in a wrong prescription with damage to the patient. In past
times this information was collected and stored at physicians’ ofﬁces and hospitals
and relatively few people even knew it existed. In most instances it was not
computerized and was protected by its isolation and the ignorance of its existence.
All this is fast changing, most or all of the doctor ofﬁces use computers, hospitals
have large information systems, and a good part of this information is becoming
accessible through distributed systems, including the Internet. Specialized databases
containing information about prescriptions or treatments are being linked together
through the Internet. Genomic databases are starting to appear. Recent plans from
President Obama call for interconnecting all medical records. This means that

16
Security in Data Intensive Computing Systems
455
the number of people that can potentially access information about patients has
increased by orders of magnitude. This increases dramatically the potential for
misuse. There is a clear requirement here for privacy, which indicates a strict need
for authentication, access control, and logging. A problem is the lack of common
standards to represent medical records, which makes the application of security
controls much more complicated than it should be. Logging and non-repudiation
enforcement is also needed to make health care providers responsible for their
decisions. Availability is also very important because of the potential need for a
speciﬁc medical record at any time and in any place.
5.3
Social Networks
Social networks provide facilities for collaboration between people who have some
common interest, belong to a similar group, or have some social connection.
A social network service includes representations of each user (a proﬁle), his social
links, and a variety of additional services. These networks are characterized by
constant activity and their users put in them large amounts of personal information.
The large amount of information about individuals held by social networks
creates a signiﬁcant privacy problem. There is also a variety of security threats
to social networks and several serious incidents have already occurred. The state
of security in social networks is rather primitive; although they contain mostly
information about individuals there is little protection against privacy violations.
Their platforms are also easy to penetrate by external attackers. We deﬁned in [11]
what security requirements should be imposed on these organizations to protect
their users. In summary, there is a need for authentication and for some basic
authorization controls. Availability is not an important aspect, although its lack may
be annoying. There is hardly a need for logging and non-repudiation.
5.4
Data Warehousing
A data warehouse by nature is an open, accessible system to support managerial
decision-making [24]. However, because of its sensitive information there is a need
for security. A UML proﬁle is a UML package that can extend either a metamodel
or another proﬁle. Villarroel et al. [43] showed how to incorporate security aspects
into conceptual modeling of data warehouses using UML proﬁles. Commercial
data warehouses may not have strict security controls, however. The need here is
for enforcing read-only access and also authorization controls for sensitive data.
Logging and non-repudiation are normally not needed. Availability is important but
not critical.

456
E.B. Fernandez
5.5
Financial Systems
Financial systems include stock and commodities trade, tax records analysis,
ﬁnancial planning, and analysis of stock data. In addition to the large amount of
data, this data may be highly distributed and needs to be collected in one place for
its analysis. Financial information is very sensitive. Because it refers to individuals,
there is a big privacy concern. But also these applications handle money which
brings a big incentive for attackers. We need here authentication, authorization,
availability, logging, and non-repudiation control. The last two are needed for legal
reasons and because of regulations such as Sarbanes Oxley.
Other applications of this type include oil exploration, analysis of sensor network
information, and SPAM ﬁlters. As we can see, the security requirements vary widely
depending on the type of data used in the application. This means that instead of a
one-size-ﬁts-all kind of security, we need to analyze their speciﬁc threats, using an
approach such a [5] or similar. The next section discusses how the existing platforms
satisfy these requirements.
6
Data Intensive Architectures
We separate complete architectures from database systems. Database systems are
available through the Internet and can be used directly. A complete architecture
provides facilities for storage, data ﬁltering, query, and development tools.
6.1
Database Systems
Most databases used in institutions are until now relational databases, which
store their data in the form of tables, where each entry (tuple) represents a record.
The relational model uses several basic (relational algebra) operations to manipulate
data: selection, projection, union, minus, Cartesian product, and join. There are also
rules for transaction integrity (ACID rules) and the basic operations can be used
to deﬁne views that present the users a subset of the database. These views are
normally used to enforce access control [8]. Almost all relational databases use
SQL as data manipulation language (SQL is an ANSI and ISO standard).
NoSQL is a new category of data management technologies that uses non-
relational database architectures. The main reason for the emergence of NoSQL
systems is that typical relational databases have shown poor performance on certain
data-intensive applications, including indexing a large number of documents, re-
trieving pages from high-trafﬁc web sites, and delivering streaming media. NoSQL
databases are often better suited to handle the requirements of high-performance,

16
Security in Data Intensive Computing Systems
457
web-scalable systems and intensive data analysis. Organizations like Facebook,
Twitter, Netﬂix, and Yahoo have used NoSQL solutions to gain greater scale and
performance, and at lower cost than relational database systems [33]. Some of these
systems may also have some SQL capabilities.
In a broader sense this deﬁnition includes any system that does not exclusively
rely on SQL for data selection. These systems include native XML databases,
graph stores, column stores, object stores, in-memory caches and multi-dimensional
OLAP cubes using MDX. Typically, NoSQL systems do not attempt to provide
ACID guarantees.
SQL databases have well-developed security features, including authentication,
authorization, logging, and encryption [8, 12]. As we will see below, the NoSQL
databases lack strong security features.
A common programming model for the new databases is MapReduce, which
allows programmers to use a functional programming style to create a map function
that processes a key-value pair associated with the input data to generate a set of
intermediate key-value pairs, and a reduce function that merges all intermediate
values associated with the same intermediate key [20]. MapReduce programs
can be used to compute derived data from documents such as inverted indexes
and the processing is automatically parallelized by the system which executes
on large clusters of machines, usually scalable to thousands of machines. Since
the system automatically takes care of details like partitioning the input data,
scheduling and executing tasks across a processing cluster, and managing the
communications between nodes, nonspecialized programmers can use conveniently
a large distributed processing environment [38].
Used together with the MapReduce architecture is the Google File System (GFS).
GFS was designed to be a high-performance, scalable distributed ﬁle system for
very large data ﬁles and data-intensive applications providing fault tolerance and
running on clusters of commodity hardware. Apache has produced an open source
Map/Reduce platform called Hadoop.
Hadoop includes two software components (each administered by the open
source Apache Software Foundation) – a distributed ﬁle manager known as HDFS
(Hadoop Distributed File System), and a parallel programming framework derived
from MapReduce. HDFS runs on a large cluster of commodity nodes and uses Java
as programming language.
Hadoop is being built and used by a global community of contributors, using
Java. Yahoo has been the largest contributor to the project, and uses Hadoop exten-
sively. Other users include IBM, Facebook, The New York Times, and e-Harmony.
MapReduce doesn’t seem to have any predeﬁned security provisions [37]. It
is up to the implementers to add security features; for example, Amazon Elastic
MapReduce starts data instances in two Amazon EC2 security groups, one for the
master and another for the slaves. The master security group has a port open for
communication with the service. It also has the Unix Secure Shell (SSH) port open
to allow to access the instances using the SSH, using the key speciﬁed at startup.
The slaves start in a separate security group, which only allows interaction with the

458
E.B. Fernandez
master instance. By default both security groups are set up to not allow access from
external sources including Amazon EC2 instances belonging to other customers.
Amazon S3 also provides authentication mechanisms. Amazon Elastic MapReduce
customers can also choose to send data to Amazon S3 using the HTTPS protocol for
secure transmission. In addition, Amazon Elastic MapReduce always uses HTTPS
to send data between Amazon S3 and Amazon EC2. For added security, customers
may encrypt the input data before they upload it to Amazon S3. However, there is
no full authorization system, no availability protection, no logging, and no control
of repudiation.
As of Version 0.19, Hadoop has security ﬂaws that limit how data can be handled,
and what kind of data can be handled. The HDFS ﬁle system of Hadoop, has no
authorization system, any authenticated user can read or write anything. Hadoop
authenticates a user for access control to the system by using the output of the
“whoami” command, which is not secure. Third, HBase, which is the “database”
that Hadoop uses, has no access control at all. Any job running on a Hadoop cluster
can access any data on that cluster. Schlesinger [38] provides some advice, one of
which is: “don’t store very sensitive data in Hadoop.” This is an advice not heeded by
some companies who store medical records and other sensitive information. Hadoop
0.20.2 has Unix access control lists for MapReduce; these are implemented as Java
libraries, which are well known to be deﬁcient as ways to enforce authorization
[12, 17], so there is no much improvement. In the Unix security model there is
no way to deﬁne access controls for individual users; for example. A typical Java
library for Hadoop security looks like:
java.lang.Object
org.apache.hadoop.security.UserGroupInformation
org.apache.hadoop.security.UnixUserGroupInformation
A specialized system for web applications in which the database component is
the bottleneck, was presented in [28]. A Database Scalability Service Provider
(DSSP) caches application data and supplies query answers on behalf of the
application. Cost-effective DSSPs need to cache data from many applications,
which brings security concerns. The chapter studies the security-scalability tradeoff,
both formally and empirically, by providing a method for statically identifying
segments of the database that can be encrypted without impacting scalability.
Another specialized system is a metadata catalog for data intensive applications
[40]. This catalog supports authentication and authorization functions. Subsystems
like these are useful but they don’t solve the lack of security in the main databases.
Secure search of large ﬁles is discussed in [41].
In summary, other than for specialized functions, the situation does not look good
here. The security of MapReduce and Hadoop cannot satisfy the requirements of
applications handling medical or ﬁnancial records for example.

16
Security in Data Intensive Computing Systems
459
6.2
Complete Architectures
A complete data handling architecture must have components to store and ﬁlter
information, to query information, and tools for developers; a few of them exist or
are appearing. Some were intended for grid systems but they could be adapted for
use in the cloud.
An architecture proposed for scientiﬁc work is the Virtual Data Grid [15]. In this
environment, data, procedures, and computations are entities that can be published,
discovered, and manipulated. Their security emphasis is on authentication mecha-
nisms.
A system for planetary information, also used for medical research, is OODT
[30, 32]. OODT objectives include access to data, discover speciﬁc data, correlate
data models, and integrate software interfaces. For these purposes it includes
product, proﬁle, and query servers but there is no mention of any security facilities.
Siebenlist et al. [39] describe the Earth System Grid (ESG), which includes an
advanced authentication system.
Legrand et al. [26] describe MonALISA (Monitoring Agents using a Large
Integrated Services Architecture), a framework intended to control and optimize
data intensive applications. It uses Java and it is intended for work in High-Energy
Physics (HEP). No security aspects are discussed.
6.2.1
Lexis Nexis HPCC
The LexisNexis approach utilizes clusters of hardware and includes custom system
software and middleware components developed and layered on a base Linux
operating system to provide the execution environment and distributed ﬁle system
support required for data-intensive computing [4, 23]. Its architecture uses the
following components:
•
Thor, (the Data Reﬁnery Cluster), is responsible for consuming vast amounts of
data, transforming, linking, and indexing that data. It functions as a distributed
ﬁle system with parallel processing power spread across the nodes. A cluster can
scale from a single node to thousands of nodes.
•
Roxie (the Query Cluster), provides separate high-performance online query
processing and data warehouse capabilities.
•
ECL (Enterprise Control Language) is a programming language that manipulates
the data. It is a non-procedural and dataﬂow oriented language.
•
There are also appropriate tools to support programmers, to interface with web
services, and to apply web services security standards.
There is no explicit description of security in their web documents so it is impossible
to judge their security. Of course, for web services applications, one can use
standards such as SAML and XACML, but their support in the architecture is
not clear. ECL has not been published in the research community so it is not clear

460
E.B. Fernandez
how convenient it is to enforce security. One of the nice aspects of SQL is the
possibility of using queries to deﬁne views to enforce access control [8, 12]; it is
not clear if something like this is possible in ECL.
7
How to Improve Security in Current Systems?
What do we need? We need systematic ways to build secure data intensive systems.
The works of Mattmann [31] and Gorton [19] were a big advance in that before
them, all these applications were only concerned with performance and provided ad
hoc solutions for security. Mattmann and Gorton emphasized the need for systematic
architectures. Past work on metadata catalogs is also important in this context [40].
The next step is to add security to these architectures. It is possible to do the same
with some commercial offering, e.g., HPCC.
Patterns are a way to simplify software development which we have been long
using in our work. A pattern is an encapsulated solution to a software problem in
a given context and a good catalog of patterns can improve the quality of software.
Patterns are fundamental when dealing with complex systems, security patterns can
help apply security in complex systems [13]. Reference architectures are important
to provide a context for the patterns. A reference architecture or domain-speciﬁc
architecture is a standardized, generic architecture, valid for a particular domain. It
is reusable, extendable, and conﬁgurable, that is, it is a kind of pattern for whole
architectures and it can be instantiated into a speciﬁc software architecture by
adding implementation-oriented aspects [3]. Architectural styles are larger patterns
and can be used as building blocks; for example [30] uses: product, proﬁle, and
query services. There is also a need for access control models combining different
expressions for rules, using different security models and languages. Most modern
security systems use Role-Based Access Control, there is nothing like that in the
surveyed systems. These models can be used as part of a systematic methodology.
We are now building a reference architecture for cloud security; we started by
deﬁning patterns to describe attacks to clouds [22].
We have proposed a systematic methodology for building secure applications
[9], which can be adapted to data intensive systems. This methodology is based on
patterns. A main idea in the proposed methodology is that security principles should
be applied at every stage of the software lifecycle and that each stage can be tested
for compliance with those principles. We also consider all the architectural levels of
the system. We have considered the use of databases in the lifecycle, we only need
to take into consideration the special aspects of very large databases. Its stages are
shown in Fig. 16.2 and include:
Domain analysis stage: A business model is deﬁned. Legacy systems are identiﬁed
and their security implications analyzed. Domain and regulatory constraints are
identiﬁed and use as global policies that should be enforced in all the applications
derived from this model [14]. The suitability of the development team is assessed,

16
Security in Data Intensive Computing Systems
461
Fig. 16.2 Secure systems design methodology
possibly leading to added training. This phase may be performed only once for
each new domain or team. The need for specialized database architectures should
be determined at this point. The approach (general DBMS or application-oriented
system) should also de deﬁned at this stage.
Requirements stage: Use cases deﬁne the required interactions with the system.
Each activity within a use case is analyzed to see which threats are possible.
Activity diagrams indicate created objects and are a good way to determine which
data should be protected [5]. Since many possible threats may be identiﬁed, risk
analysis helps to prune them according to their impact and probability of occurrence.
Any requirements for degree of security should be expressed as part of the use
cases.
Analysis stage: Analysis patterns can be used to build the conceptual model in a
more reliable and efﬁcient way. The policies deﬁned in the requirements can now
be expressed as abstract security models, e.g., access matrix. The model selected
must correspond to the type of application; for example, multilevel models have
not been successful for medical applications. One can build a conceptual model
where repeated applications of a security model pattern realize the rights determined
from use cases. In fact, analysis patterns can be built with predeﬁned authorizations
according to the roles in their use cases. Patterns for authentication, logging, and
secure channels are also speciﬁed at this level. Note that the model and the security
patterns should deﬁne precisely the requirements of the problem, not its software
solution. UML is a good semi-formal approach for deﬁning policies, avoiding
the need for ad-hoc policy languages. The addition of OCL (Object Constraint
Language) can make the approach more formal.
Design stage: When one has deﬁned the policies needed, one can select mecha-
nisms to stop attacks that would violate these policies. A speciﬁc security model,
e.g., RBAC, is now implemented in terms of software units. User interfaces should

462
E.B. Fernandez
DBMS
Operating
System
User
Interface
1
1
*
*
*
*
OS calls
shell 
commands
DBMS 
language
OS Security
Services
I/O Services
I/O
1
1
DBMS Security
Services
1
1
1
1
1
I/O
Fig. 16.3 The operating system controls the storage of databases
correspond to use cases and may be used to enforce the authorizations deﬁned in the
analysis stage. Secure interfaces enforce authorizations when users interact with the
system. Components can be secured by using authorization rules for Java or.NET
components. Distribution provides another dimension where security restrictions
can be applied. Deployment diagrams can deﬁne secure conﬁgurations to be used by
security administrators. A multilayer architecture is needed to enforce the security
constraints deﬁned at the application level. In each level one can use patterns to
represent appropriate security mechanisms. Security constraints must be mapped
between levels.
The persistent aspects of the conceptual model are typically mapped into
relational databases [7]. The design of the database architecture is done according
to the requirements from the uses cases for the level of security needed and the
security model adopted in the analysis stage. Two basic choices for the enforcement
mechanism include query modiﬁcation as in INGRES and views as in System R.
Data intensive systems do not generally follow this model so we need to modify
this stage. A tradeoff is using an existing DBMS as a Commercial Off-the-Shelf
(COTS) component, although in this case security will depend on the security of
that component.
Database systems architectures can be structured in different ways. Traditional
systems had the operating system controlling the ﬁle system that stored the whole
database as shown in Fig. 16.3. In the newer architectures there is a web application
server (Fig. 16.4) that combines a web server and an application integration server

16
Security in Data Intensive Computing Systems
463
Web Application
Server
DBMS
Local User
Interface
Web User
Interface
Security
Services
Security
Services
Operating
System
Security
Services
I/O Services
I/O
1
*
1
1
1
1
1
1
1
1
1
1
1
*
*
*
OS calls
OS calls
Fig. 16.4 Architecture using a web application server
(WAS). The WAS contains a common model of the possibly many databases and
applies security controls. The architectures surveyed earlier, e.g. [23, 30, 34], have
ad-hoc architectures using some type of middleware resembling a WAS.
Implementation stage: This stage requires reﬂecting in the code the security
rules deﬁned in the design stage. Because these rules are expressed as classes,
associations, and constraints, they can be implemented as classes in object-oriented
languages. In this stage one can also select speciﬁc security packages or COTS,
e.g., a ﬁrewall product or a cryptographic package. Some of the patterns identiﬁed
earlier in the cycle can be replaced by COTS (these can be tested to see if they
include a similar pattern). Performance aspects become now important and may
require iterations. As indicated, a whole DBMS could be such component.
8
Conclusions
The same security constraints and principles that apply to more general environ-
ments also apply to data intensive systems. Their objectives with respect to security
are also similar. What is different is the fact that large amounts of information may
exist scattered in diverse places and that many people may need legitimate access to
them; in other words, there is a large data exposure. New architectures, such as the
cloud, bring new types of attacks [22]. In addition, convenient use requires access
through the Internet, which is a rather insecure environment.

464
E.B. Fernandez
The needs of the applications we considered are quite different, social networks
are (or should be) concerned with user privacy, some scientiﬁc computing has few
security needs, ﬁnancial and medical applications are highly sensitive. In other
words they span all degrees of security. This indicates that one should study the
speciﬁc threats of each environment and then apply some security approach.
If we look at the current ways to manipulate large amounts of data we come to
the conclusion that the situation with respect to security is not good and that much
needs to be done before we can trust in these systems. There is almost no concern for
security and privacy in the recent commercial offerings. For example, the security
of a very popular system like Hadoop is now rather poor. Also, many changes are
happening, SciDb is toted as an improvement for scientiﬁc databases [42] but again
there is no mention of security. In traditional systems, security has been applied in a
reasonable way but in the new systems the only emphasis until now is performance.
When many serious breaches happen, we will start worrying about security. At that
moment, complete solutions will become important because they are the only ones
which can provide a high degree of security.
We made some recommendations to secure the new systems. Our approach to
security is based on application semantics. We believe that there is no security sys-
tem that ﬁts all applications, one should analyze the speciﬁc threats of applications
and then build the application and its platform together. Emphasis on protecting the
lower levels of the architecture have not been effective in the past and if we don’t
understand the applications there is no much hope we will be able to build secure
systems.
The use of Service Level Agreements (SLAs) and other legal restrictions are
necessary to protect customers and individuals whose information is kept in these
systems. More work is needed to deﬁne precise SLAs together with ways to monitor
that the prescriptions of the SLAs are followed. Service certiﬁcation may be needed
in some cases [2]; not much has been done in this direction and it should become
important for critical services. In fact, SLAs can be a byproduct of secure application
development and they can be integrated in the system life cycle.
References
1. R. Anderson, Security Engineering (2nd. Ed.), Wiley, 2008.
2. M. Anisetti, C.A. Ardagna, and E. Damiani, “Container-level security certiﬁcation of services”,
International Workshop on Business System Management and Engineering (BSME 2010).
3. P. Avgeriou, “Describing, instantiating and evaluating a reference architecture: A case study”,
Enterprise Architecture Journal, June 2003.
4. D. Bayliss, HPCC systems: Aggregated data analysis: The paradigm shift”, Lexis Nexis white
paper, May 2011, http://hpccsystems.com
5. F. Braz, E.B. Fernandez, and M. VanHilst, “Eliciting security requirements through misuse
activities” Procs. of the 2nd Int. Workshop on Secure Systems Methodologies using Patterns
(SPattern’08). Turin, Italy, September 1–5, 2008. 328–333.
6. R.E. Bryant, “Data intensive supercomputing”, Slide presentation, http://www.cs.cmu.edu/
bryant

16
Security in Data Intensive Computing Systems
465
7. S. Ceri, P. Fraternali, and M. Matera, “Conceptual modeling of data-intensive web applica-
tions”, IEEE Internet Computing, July-August 2002, 20–30.
8. R. Elmasri, and S. Navathe, Fundamentals of Database Systems, Sixth Edition. Pearson. 2010.
9. E.B. Fernandez, M.M. Larrondo-Petrie, T. Sorgente, and M. VanHilst, “A methodology to
develop secure systems using patterns”, Chapter 5 in “Integrating security and software
engineering: Advances and future vision”, H. Mouratidis and P. Giorgini (Eds.), IDEA Press,
2006, 107–126.
10. E.B. Fernandez, “Security patterns and a methodology to apply them”, in Security and Depend-
ability for Ambient Intelligence, G. Spanoudakis and A. Ma˜na (Eds.), Springer Verlag, 2009.
11. E.B. Fernandez, C. Marin, and M.M. Larrondo Petrie, “Security requirements for social
networks in Web 2.0”, in the Handbook of Social Networks: Technologies and Applications,
B. Furht (Editor), Springer 2010.
12. E.B. Fernandez, E. Gudes, and M. Olivier, The design of secure systems, Addison-Wesley, to
appear.
13. E.B. Fernandez, Designing secure architectures using security patterns, under contract with J.
Wiley. To appear in the Wiley Series on Software Design Patterns.
14. E.B. Fernandez and S. Mujica, “Model-based development of security requirements”, accepted
for the CLEI (Latin-American Center for Informatics Studies) Journal.
15. I. Foster, J. Voeckler, M. Wilde, and Y. Zhao, “The Virtual Data Grid: A new model
and architecture for data-intensive collaboration”, Proceedings of the 15th International
Conference on Scientiﬁc and Statistical Database Management (SSDBM ’03), IEEE Computer
Society, Washington, DC, USA, 2003.
16. I. Foster, Y. Zhao, I. Raicu, and S. Lu, “Cloud Computing and Grid Computing 360-Degree
Compared”, CoRR, Vol. 0901, 2009.
17. D. Gollmann, Computer security (2nd Ed.), Wiley, 2006.
18. Gorton, I., Greenﬁeld, P., Szalay, A., & Williams, R. (2008). “Data-intensive computing in the
21st century”. IEEE Computer, 41(4), 30–32.
19. I. Gorton, “Software Architecture Challenges for Data Intensive Computing”, Procs. Seventh
Working IEEE/IFIP Conference on Software Architecture, WICSA 2008, 4–6.
20. R.L. Grossman and Y. Gu, “On the varieties of clouds for data intensive computing”, Bull. of
the IEEE Comp. Soc. Tech. Comm. on Data Eng., 209, 1–7. http://sites.computer.org/debull/
A09mar/issue1.htm
21. A. Hameurlain, F. Morvan, and M. El Samad, “Large scale data management in grid systems: a
survey”, Information and Communication Technologies: From Theory to Applications, ICTTA
2008. 1–6.
22. K. Hashizume, E.B. Fernandez, and N. Yoshioka, “Misuse patterns for cloud computing”,
accepted for the Twenty-Third International Conference on Software Engineering and Knowl-
edge Engineering (SEKE 2011), Miami Beach, USA, July 7–9, 2011.
23. Lexis Nexis HPCC, Data-Intensive Computing Solutions, http://wpc.423a.edgecastcdn.
net/00423A/whitepapers/wp data intensive computing solutions.pdf (last retrieved June 30,
2011).
24. N. Katic, G. Quirchmayr, J. Schiefer, M. Stolba and A. M. Tjoa. “A Prototype Model for Data
Warehouse Security Based on Metadata”, Int. Workshop on Security and Integrity of Data
Intensive Applications in conjunction with the 9th Int. Conf. on Database and Expert Systems
Applications (DEXA’98), University of Vienna, Austria, 24–28 August, 1998.
25. B. Lang, I. Foster, F. Siebenlist, R. Ananthakrishnan, T. Freeman A Flexible Attribute Based
Access Control Method for Grid Computing 2009 GSC.
26. J. Legrand et al., “Monitoring and control of large systems with MonALISA”, Comm. of the
ACM, vol. 52, No 9, Sept. 2009, 49–55.
27. S. Lohr, “New ways to exploit raw data may bring surge of innovation, a study says”, The New
York Times, Friday, May 13, 2011, B3.
28. A. Manjhi, A. Ailamaki, B.M. Maggs, T.C. Mowry, C. Olston, and A. Tomasic “Simultaneous
scalability and security for data intensive web applications”, Procs. of SIGMOD 2006, June
27–29, 2006, Chicago, Illinois, USA.

466
E.B. Fernandez
29. J. Markoff, “Digging deeper, seeing farther: Supercomputers alter science”, The New York
Times, Tuesday, April 26, 2011, D1and D3.
30. C. Mattmann, D. Crichton, J. S. Hughes, S.C. Kelly, and P. M. Ramirez, “Software architecture
for large-scale, distributed, data-intensive systems”, Procs. of the 4th Working IEEE/IFIP Conf.
on Software Architecture (WICSA’4). Osto, Norway, June 2004.
31. C. Mattmann, D. Crichton, N. Medvidovic and S. Hughes, “A Software Architecture-Based
Framework for Highly Distributed and Data Intensive Scientiﬁc Applications”. In Proceedings
of the 28th International Conference on Software Engineering (ICSE06), pp. 721–730,
Shanghai, China, May 20th-28th, 2006.
32. C. Mattmann, D. Crichton, A. Hart, S. Kelly, and J.S. Hughes, “Experiments with Storage and
Preservation of NASA’s Planetary Data via the Cloud”, IEEE IT Professional – Special Theme
on Cloud Computing, Vol. 12, No. 5, September/October, 2010, 28–35.
33. D. McCreary and D. McKnight, The CIO’s guide to NoSQL, http://www.Dataversity.net
34. C. Miceli et al., “Programming abstractions for data intensive computing on clouds and grids”,
Procs. 9th IEEE/ACM Int. Symp. on Cluster Computing and the Grid, 2009, 478–483.
35. A. Nourian, M. Maheswaran, and M. Pourzandi, “Privacy and Security Requirements of Data
Intensive Applications in Clouds”, Chapter 20, this book.
36. P. Patel, A. Ranabahu, and A. Sheth, “Service Level Agreement in Cloud Computing”, Cloud
Workshops at OOPSLA, 2009.
37. I. Roy, S.T.V. Setty, A. Kilzer, V. Shmatikov, and E. Witchel, Airavat: Security and Privacy for
MapReduce http://www.cs.utexas.edu/shmat/shmat nsdi10.pdf
38. J. Schlesinger, “Cloud security in MapReduce: An analysis”, http://www.defcon.org/images/
defcon-17/dc-17-presentations/defcon-17-jason schlesinger-cloud security.pdf
39. F. Siebenlist, R. Ananthakrishnan, D.E. Bernholdt, L. Cinquini, I.T. Foster, DE Middleton, and
N. Miller, DN Williams Enhancing the earth system grid security infrastructure through single
sign-on and autoprovisioning Proceeding GCE ’09 Proceedings of the 5th Grid Computing
Environments Workshop ACM New York, NY, USA c 2009.
40. S. Singh et al., “A metadata catalog service for data intensive applications”, Procs. ACM/IEEE
Sc 2003 Conference, ACM 2003.
41. A. Singh, M. Srivatsa, and L. Liu, “Efﬁcient and secure search of enterprise ﬁle systems”,
Procs. of WWW 2007, May 2007, Banff, CA.
42. M. Stonebraker, “SciDB: An Open Source Data Base Project “, presentation 2008.
43. R. Villarroel, E. Fernandez-Medina, M. Piattini, and J. Trujillo, “A UML 2.0/OCL Extension
for Designing Secure Data Warehouses”, Journal of Research and Practice in Information
Technology, Vol. 38, No. 1, February 2006, 31–43.
44. X. Wei et al., GDIA: A Scalable Grid Infrastructure for Data Intensive Applications, in Int
Conf. on Hybrid Information Technology, 2006. ICHIT ’06. Nov. 2006.
45. B. Zhou and J. Pei, Privacy Preserving Data Mining and Social Computing in Large-Scale
Social Networks, chapter 13 of this book.

Chapter 17
Data Security and Privacy in Data-Intensive
Computing Clusters
Flavio Villanustre and Jarvis Robinson
1
Introduction
Data security is of utmost importance, especially in cases when improper disclosure
would compromise corporate trade secrets, expose Personal Identiﬁable Information
(PII) and lead to ﬁnes and legal recourse.
The Data-Intensive Computing paradigm is based on parallelizing the execution
of data transformation algorithms by spreading them among a set of execution
threads running on different computing nodes, which are part of the same cluster.
This model tends to adapt very well to cloud deployments, either private or public.
Many of these algorithms scale linearly with the number of nodes; a ﬂexible cloud
model where nodes can be allocated on demand, is efﬁcient and thus, very attractive.
By the same token, leveraging a public or private cloud for Data-Intensive Com-
puting involves loading considerable amounts of data on a number of computing
nodes, which are not necessarily dedicated to this particular function. Even if the
data store and some computing subsystems (RAM, CPU cores, etc.) are dedicated
to this data during the execution of the job, they will be reused as soon as the job
ﬁnishes (or even once the execution time slice is exhausted). Moreover, several
implementations allow for concurrent and simultaneous use of computing and
storage resources by different operators on multiple datasets.
The fact that data labeled under different classiﬁcation levels can co-reside in
permanent (hard drive) or volatile/transient (RAM, CPU cache, CPU registers)
storage, or even be executed using the same computing resources, poses several
challenges from a security and a compliance standpoint. In addition to that, access
to the code that performs the transformation by unauthorized people can potentially
disclose information that should only be available to the legit owner. Last, by virtue
F. Villanustre • J. Robinson
LexisNexis, Alpharetta, GA, USA
e-mail: ﬂavio.villanustre@lexisnexis.com; Jarvis.robinson@lexisnexis.com
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 17, © Springer Science+Business Media, LLC 2011
467

468
F. Villanustre and J. Robinson
of the aggregated technical capabilities provided by Data-Intensive Computing,
the resources must be safeguarded against risks such as denial of service and
misappropriation of resources.
In this chapter article we brieﬂy describe and analyze common exposures facing
Data-Intensive Computing. In addition, we indicate some of the best practices as
generally accepted within the global IT Security community, to mitigate those risks.
Such practices account for security milestones including conﬁdentiality, integrity,
availability, and accountability.
2
Secure Design
A practice well-acknowledged by security and privacy practitioners is to integrate
security, early and often, throughout the software development life cycle. Accord-
ingly, the design phase marks a key stage to enact sound security and privacy
practices. Given the aforementioned security milestones, the concept of threat
modeling lends well. Speciﬁcally, threat modeling allows the thorough analysis
of the Data-Intensive Computing architecture, the classiﬁcation of the exposures
therein, and the design of the appropriate safeguards [1].
After the formulation of security milestones, threat modeling involves analyzing
the architecture according to its distinct parts. A key approach is to identify
trust-boundaries, ingress points and egress points. Given the various data ﬂows,
trust-boundaries help denote changes in reliance levels; typically between logical
or physical components. With respect to Data-Intensive Computing, the interaction
between the web service provider and the web service consumer is an example.
Speciﬁcally, the multifaceted nature of the public networks such as the Internet
often leads one to treat client-side data as untrusted by default. As a result, it could
be said that the demarcation between the web service consumers (e.g. any possible
caller on the front-end side) and the web service provider could be considered a trust
boundary. And the trust boundary provides a visual cue that technologists may use
to implement appropriate safeguards. Figure 17.1 provides a high-level depiction of
a data ﬂow diagram with focus on threat modeling.
Identifying threats, misuse cases, and vulnerabilities is a key step in the threat
modeling process. In Information Security terms, vulnerability is a weakness which
allows a threat to manifest. Armed with the aforementioned knowledge of the
Data-Intensive Computing environment (e.g. trust boundaries, ingress points, egress
points, etc.), one may begin to arrive at misuse cases. A misuse case involves
scenarios gone bad in which an actor (i.e. threat) attempts to compromise the overall
Data-Intensive Computing environment, the processing task or the data therein.
Coupled with an attack vector (i.e. email access, remote database query) and a

17
Data Security and Privacy in Data-Intensive Computing Clusters
469
Fig. 17.1 Data ﬂow diagram
possible vulnerability for the actor to exploit, a misuse case can be a practical utility
to quickly identify an exposure.
After the exposures have been outlined, one should design appropriate safe-
guards. Sometimes the sheer volume of possible exposures may pose a signiﬁcant
challenge. Hence, taking a risk-based approach can be helpful by prioritizing those
risks exposures that lend into the higher risks. This may allow one to effectively
prioritize the design of safeguards while balancing against other factors including
usability and cost.
3
Human Resources
Appropriate management of human resources is a key area to ensure overall
security and privacy. This aspect includes selecting the right talent, ensuring that
resources are well-trained in the fundamentals of security, and maintaining adequate
awareness levels [2]. Clear identiﬁcation of the roles and responsibilities for

470
F. Villanustre and J. Robinson
personnel who will have direct or indirect access to the Data-Intensive Computing
environment is critical. Often this begins with the development of clear job
descriptions. Additionally, security policies and procedures should be used to
reference roles and responsibilities related to the ﬁrm’s security program. Moreover,
selected candidates must be adequately screened prior to employment with the
organization [3]. Common methods include criminal background checks, reference
checks, and drug screening. Following a successful screening, appropriate non-
disclosure agreements must be signed before the employee can be assigned to
perform the designated role.
For an information security program to be effective, awareness needs to be
raised and maintained across the organization. Personnel need to understand the
governance process and the overarching security policies and the consequences
of not complying. The extent of training should include operating procedures
for data access and data custodianship, and reporting violations contrary to the
existing policies. To ensure continuity in this process, adequate yet regular employee
outreach is recommended to maintain proper level of awareness. The scope should
include part-time employees, full-time employees, contractors, and third parties.
The overall underlying goal is to employ administrative controls to mitigate
insider risks, but to also help minimize some of the outsider attack vectors through
better trained personnel that could help deﬂect, for example, attempts at social
engineering.
4
Data Value Classiﬁcation and Data Labeling
Since a signiﬁcant portion of the risk around these systems resides on the data being
processed, adequate mitigation cannot happen unless the data is properly classiﬁed
and labeled, and that this process happens as close to the inception or ingestion
point as possible. While the process governing the data value classiﬁcation is highly
dependent on the speciﬁc aspects of the business and the operating regulatory and
legal frameworks, the implementation of the data classiﬁcation and labeling can be
still be subject of general best practices.
Several schemes have been proposed to protect data privacy and conﬁdentiality
in distributed systems, including randomization, [4,5] but a sensible, consistent and
simple process of data classiﬁcation and labeling can go a long way to identify
sensitive data and track it throughout its entire lifetime in the system, ensuring that
adequate protections are applied at each stage.

17
Data Security and Privacy in Data-Intensive Computing Clusters
471
The data classiﬁcation process usually involves recognizing the data source and
the contents of the particular dataset to adequately categorize it. Since most of the
data classiﬁcation schemes are multi-dimensional, other attributes will need to be
added to the metadata to properly reﬂect each one of those additional dimensions
(for example, creation date, expiration date, audience, blacklist of entities that must
not access it, etc.).
While the classiﬁcation process can have a coarse granular nature and be applied
to complete datasets rather than individual records, best practices indicate that
the labeling should be performed at the record level, even at the expense of the
extra storage space needed to handle these descriptors. A level of indirection
through normalization can be used, and these can be just annotations which
reference an external metadata information resource, particularly when the amount
of information to be added is signiﬁcant when compared to the original size of
the record. Special care needs to be applied to ensure that both, the internal record
labels and the external resources that some of these labels could depend on are
transformed, linked, and moved together with the records themselves.
In certain cases, as a result of the linking process across multiple datasets,
resulting recordsets could be subject to a value classiﬁcation different from that
of their parents. A good example would be for someone to use the source data
from the Netﬂix(tm) challenge, which provides anonymized data on movie viewers
and link it against public data sources such as the IMDB database, which contains
comments about movies from viewers. A potential resultset could constitute de-
anonymized movie viewing habits data for certain consumers, which should clearly
fall on a value classiﬁcation level higher than those of its own parents [6]. A solid
data classiﬁcation policy needs to account for these cases, where the result is more
than the mere sum of its parts, and provide for ﬂexibility in the classiﬁcation of
derivatives, even if that means that the derivatives need to be rated at a more sensitive
level than their data sources.
5
Access Controls and Authorization Levels
In a distributed environment, it is critical for access controls to be consistent
throughout the environment. By the nature of a Data-Intensive Computing system,
data and code are distributed across all nodes, and access to these resources must be
controlled by a uniform set of deﬁnitions.

472
F. Villanustre and J. Robinson
A Data-Intensive Computing system is built upon different building blocks.
While the best known overall functional component could be the ETL engine
(HDFS/MapReduce in Apache Hadoop and Thor in LexisNexis HPCC), there could
also be an scalable distributed delivery engine (Roxie in the LexisNexis HPCC).
From a detailed architectural standpoint, individual servers can be assigned speciﬁc
functions (for example, namenode and taskserver for an Apache Hadoop cluster, or
Dali and ECLserver for a LexisNexis HPCC cluster) or just form part of the more
generic processing node category.
In addition to the intrinsic components, external entities should be considered as
well, as they interact with and access the Data-Intensive Computing environment
by design. An example of an external entity includes a client on the Internet
interacting with the Data-Intensive Computing cluster via an Application Program-
ming Interface (API), or an internal client deploying code to a cluster through an
Integrated Development Environment (IDE). In each case, these external entities
could represent a noteworthy exposure as unwarranted access levels through them
could lead to direct or indirect compromise of the cluster and data therein.
Possibly the most effective control is around ensuring least-based privilege to
the aforementioned components; place scrutiny on subjects including users, calling
code, and functions. An administrator, for example, may implement access controls
on batch processing clients so that they are only able to consume a particular
batch service and only for the data sets commensurate to their permissible use.
Similarly, emphasis must be placed on limited access at the operating system
level. This practice is normally referred as “the principle of least privilege” which
holds that a subject should only be assigned the access it needs and nothing
more. Equally important, procedures for provisioning access to the Data-Intensive
Computing cluster should be employed. This includes promptly terminating user
access following separation of employment or in situations in which access is no
longer warranted (e.g. change of job function).
Streamlining account management and account policy enforcement is also a
keystone to overall security posture. Depending on the size of the environment,
manual account management processes such as paper-based approval systems could
quickly become challenging to security administrators responsible for ongoing
access reconciliation. This may lead to exposures such as stale and orphaned
accounts, which an attacker may leverage. These exposures are exacerbated in
the case of a distributed environment consisting of a large number of cluster
nodes. Hence, tools and techniques to help automate account management and
policy enforcement can signiﬁcantly mitigate this risk. This is usually achieved
through the deployment of a centralized access control facility, which may support
efﬁcient management of subjects and objects over the lifespan of the environment.
A directory service together with an adequate authorization ticket facility (for
example, a combination of LDAP with Kerberos) is a good starting point, which
could be combined with a global identity management system at a later stage.
Couple user provisioning capabilities with administrative processes that support
audit capabilities. Assign data owners to support the access approval and re-
veriﬁcation processes. Each one of these processes serves as checks and balances,

17
Data Security and Privacy in Data-Intensive Computing Clusters
473
where access requests are concerned.In addition, maintain an audit trail to document
decisions related to access reviews across the life cycle of each subject’s access.
A hidden beneﬁt is that this also provides supporting evidence in case of incident
response triage activity.
Ensure separation of duties [7] across components in the Data-Intensive Com-
puting environment and key job functions. This measure helps minimize the risk
of fraudulent access or misuse of assets. For example, a common best practice
is to utilize separation of duties for developer and production support resources.
Speciﬁcally, this safeguard helps minimize the risk of ill-code promotion from the
development environment and to the production environment. Failure to do so leads
to exposure where system and data integrity are concerned. In special cases, if
some level of overlapping access is required, access monitoring and object change
auditing may be appropriate compensating controls.
6
Security of the Data At Rest
Persistent data in the Data-Intensive Computing cluster, either resident in the
ﬁlesystem as an independent object or combined with other elements (for example,
as a result of memory paging/swapping to the persistent storage device), can be
susceptible to attacks at the physical layer. Data theft and tampering are both
possible through an unsophisticated physical attack. With respect to volatile storage
of the data, which commonly exists in the form of RAM, CPU cache and CPU
registers, attacks at the physical layer are also a possible exposure, although
usually more difﬁcult to exploit [8]. Physical access controls, perimeter alarms, and
surveillance are necessary to fully protect a Data-Intensive Computing cluster and
its content.
In addition to this, in concurrent execution environments, the possibility of data
leakage through CPU cache and/or CPU registers can expose small portions of data
to unauthorized people through side channel analysis or otherwise. Although side
channel attacks can usually only recover small bits and pieces of data at a time,
repeated application of this technique can expose signiﬁcant portions of sensitive
information which could even contribute to further data loss, in the case that, for
example, the recovered information is a decryption key.
There are several industry best practices around preserving conﬁdentiality,
integrity, and availability of the data at rest in the system. The ﬁrst and possibly

474
F. Villanustre and J. Robinson
most comprehensive measure is to tightly control physical access to the data storage
media in online and ofﬂine modes, and the system as a whole. This is one of the best
defenses as inability to physically access system or media leaves logical attacks as
the only possible vector (i.e. attacks via the network). Examples of physical controls
include guarded access points, monitored security cameras, and limited number
of human resources with access to the protected datacenter environment, together
with multiple authentication factors required for entry. Keep also in mind that local
console access, in the majority of the systems, could equal to a defeat of logical
controls, as many times these logical access controls are oriented more towards
remote access (for example, in the case where the attacker at the local console can
subvert the execution of the “init” process in a Unix system to bypass the loading
of the normal Unix access control mechanisms). In addition to this, periodic re-
veriﬁcation of employees and contractors with access to the protected environments
is paramount to ensure that only authorized parties continue to have physical access
to the system.
Remote logical access to the system also needs to be controlled, and there are
several layered mechanisms that can mitigate the risk of attacks. The attack surface
can be limited through ensuring that only necessary services and network ports are
active, and that access to these can only originate from approved networks whenever
possible. Moreover, appropriate authentication credentials should be required to
ensure that only authorized users have access to these ports and services. Ability
to execute unapproved code can also create exposure of leakage of data at rest.
Several Operating System standard controls, including rendering writable memory
non-executable .W^X/ and process and library address space layout randomization
can help mitigate this risk.
With respect to the threat of rogue programs running in the Data-Intensive
Computing environment, a best practice is to ensure consistency in the deployment
artifact build process. Having a pristine image of each of the binary artifacts for
the system would greatly reduce the duration for which a compromised host may
remain resident and/or downtimes during the restoration process. In this respect, it
is convenient to ensure that the system boots to a consistent state every time by using
a centralized booting facility through, for example, PXE boot deployment of RAM-
based images. As such, the system would be initialized each time to a fresh state,
minimizing the potential for bad code to stay resident between general reboots.
System monitoring is another best practice in the sense that it allows for
anomalous events to be checked against good known states. One form of system
monitoring includes integrity checking, which helps administrators detect changes
in subsystems against a well-known prior state; changes contrary to the norm
would demand further investigation. Integrity check capabilities can be made
extensive to provide for scrutiny and veriﬁcation of resident objects and components
using industry standard routines and algorithms. While these precautions pose an
additional hurdle for attackers to overcome, it should be noted that they are by no
means a complete protection, but merely a part of a defense-in-depth paradigm.
Physical access needs to be protected even beyond the lifetime of the assets. For
those assets which could hold persistent information, including but not limited to
magnetic media as sometimes sensitive information could be held in nonvolatile

17
Data Security and Privacy in Data-Intensive Computing Clusters
475
memory, for example, a consistent and thorough disposal program needs to be in
place. Once a particular piece of hardware is determined to require replacement
or disposal, appropriate methods need to be applied to render any data contained
inaccessible. Depending on the speciﬁc storage mechanism, this could require
degaussing, overwriting and even physical destruction such as pulverization [9].
As a specialized area of asset disposal management, a properly implemented data
disposal program would leverage data owners and data custodians, together with
data labeling and tracking practices to identify and classify data throughout is
lifecycle. In concert with data disposal, data retention policies, normally required
in the process to fulﬁll business, legal and regulatory requirements are an integral
part of an information security program.
7
Security of the Data in Transit
In a distributed computing environment, the scope of data in transit includes
data loaded and/or unloaded from the cluster along with data transferred in
between computing nodes (and storage nodes if they are present) during execution.
Ultimately, data in transit adds another layer of complexity and potential exposure
as it introduces the threat of data eavesdropping, data interception, and data
manipulation. Even inside seemingly trusted boundaries, data can be subject to such
exposure. Detection of certain adversaries can be challenging in unconstrained and
ﬂat networks, and even in a properly segmented and architected network, identifying
attack vectors may not be exempt of complexities.
As mentioned in the section on Threat Modeling, it is important to start the
analysis with a comprehensive assessment of all data ﬂows, including all ingress
and egress points including those used for data loading and unloading, external
and internal interfaces, and access to persistent data storage. This includes any
components and subsystems used by administrators and support staff.
For each data ﬂow, examine the communication protocol and channels used,
and ensure it establishes adequate authentication between entities. Speciﬁcally, the
source and destination of the data ﬂows should include an authentication layer such
that entities may adequately verify the origin of the speciﬁc ﬂow. Ensure also that the

476
F. Villanustre and J. Robinson
communication protocol allows for data conﬁdentiality methods to mitigate the risk
of eavesdropping. Where possible, session management between entities should be
difﬁcult for an attacker to compromise. In this respect, session identiﬁers should be
randomly selected from a possible universe signiﬁcantly larger than the maximum
number of sessions and difﬁcult to replay, normally obtainable by including a nonce.
In addition to this, sessions should be ﬁnite in duration whenever possible and
a mechanism for session expiration due to inactivity should be present if viable.
A possible compensating control, especially in the case that some of the above
measures cannot be employed, is to maintain adequate network segmentation. This
includes segregating the production environment from the non-production areas by
establishing well-deﬁned security boundaries.
As a general rule of thumb, always leverage standard communication protocols,
especially those that have been scrutinized in the public space. Besides advantages
such as code reuse and lower implementation cost, the inherent beneﬁt is that
standard protocols are often subject to review by the global IT community, so
they are well known and their ﬂaws are usually quickly recognized and corrected.
Through more comprehensive security review efforts, standard protocols can reach
maturity faster than their closed and proprietary counterparts. The broad exposure
aids also with their reliability and overall security.
8
Security of the Data Transformation Code or Algorithm
The data transformation algorithms are usually at an exceptional vantage point if
they can be subverted. By the nature of their job, these algorithms (for example,
a MapReduce java task in the Apache Hadoop world, and an ECL job in the
LexisNexis HPCC platform) normally have unfettered access to large portions of
the data store, and they can represent an especially attractive target.
Compromise of the data transformation code lends to exposed intellectual
property due to the leak of the algorithms themselves, together with the possibilities
for data tampering, data loss and data exposure, with the potential consequences
in terms of ﬁnancial loses and even lawsuits. As such, adequate detection and

17
Data Security and Privacy in Data-Intensive Computing Clusters
477
prevention methods are required. From the standpoint of the execution of modiﬁed
code, the use of mandatory cryptographic signatures should minimize the exposure
if the platform can refuse to execute unsigned code, or code that doesn’t pass
an integrity check. In order to prevent potential Intellectual Property leak due to
unauthorized access and distribution of the algorithms themselves, it is possible to
implement detection and/or prevention mechanism that operate on code or algorithm
characteristics.
Since a reliance chain is as weak as the weakest link, and that the overall
platform code (and the Operating System) could be eventually be modiﬁed to defeat
the mandatory cryptographic veriﬁcation mechanisms, it is recommended that a
hardware based solution such as Intel Trusted Execution Technology is used [10].
9
Security of the Data Retrieval Process
The data retrieval process normally involves a component which serves as an
interface between the user and those subsystems that provide data retrieval functions
and the data store. It is this ‘broker’ component, the subsystem that provides the
functions required by interactive, semi-interactive and batch access to the data
results. As a result, this component poses as a single surface of attack where data
access is concerned. In order to effectively protect the system against attacks at this
layer, you should ensure that adequate authentication between the data consumer
and the provider exists. For highly sensitive transactions, consider veriﬁcation of
the origin of the access attempt and the data therein. Often this can be done at the
session and application layers, but don’t necessarily discard the veriﬁcation at the
network layer by matching the address identiﬁers carried (more speciﬁcally, source
IP address in the case of IP based protocols), especially when protocol used require a
three way handshake to initiate a connection (which should minimize the likelihood
for spooﬁng of network addresses).

478
F. Villanustre and J. Robinson
Data validation is another element critical to the retrieval process. Inadequate
data validation or lack thereof is a key weakness underpinning many successful
attacks today. For example, malformed input passed to the interactive interface
could lead erroneous interpretation on the cluster nodes leading to unauthorized
data disclosure and/or compromise of the system integrity as a whole (a technique
known as xml injection, in the case of an xml transport, is an example of a potential
attack in this area). As such, it is critical to ensure that input and output conforms to
predeﬁned schema. Common data attributes to inspect include length, range, format,
and type [11]. In the case of non-conformity, exception handling techniques may
include discarding and transforming (i.e. escaping) such data, including logging and
alerting therein.
10
Data Extracts/Summaries and Individual Record
Retrieval
Similar to individual record retrieval, data extracts are commonly facilitated in
a Data-Intensive Computing architecture by a query component responsible for
brokering transactions between the data consumer and the provider. While data
extracts could be perceived as having a lower potential risk when compared to
retrieval of complete individual records, it is very difﬁcult to generalize around the
intrinsic value of the data in a particular extract without knowing details around
the data. Paradigms such as Statistical Data Control (SDC) provide for formal
representation of the possible inherent risk. Speciﬁcally, SDC indicates that one
should achieve a balance between useful information and data conﬁdentiality [12].
There are several safeguards that should be considered when offering data
extracts or individual record retrieval as options for the consumer. Based on to the
principle of least privilege, as referenced in the preceding section on Access Control,
risk of data leak should be balanced with speciﬁc use cases. In this respect, the

17
Data Security and Privacy in Data-Intensive Computing Clusters
479
decision to provide data extracts versus the delivery of complete individual records
will depend on the tolerance to risk by the organization. The scope of least privilege
must include data consumers in both, the external and the internal to the organization
groups.
It is important to analyze the potential options for data control. A simple method
is to plot each of the dimensions along a plane, which includes the following: desired
data loss level or transformation (i.e. security) and tolerable level of data disclosure
(i.e. useful information, usability, etc). The most feasible solution should provide an
adequate mix between these two factors and any other additional factors that play a
key role in analysis.
Select a data control method to lower the risk. Methods to lower the risk may
include micro aggregation, tokenization, obfuscation, and truncation. When micro
aggregation is employed, data that needs to remain conﬁdential is replaced with
one or more values shared by aggregated records or data elements. Tokenization
is a technique, which replaces select data elements by a token. This lends to a
mapping between the token and the original data, which may persist in a centralized
yet protected store. As such, tokenization may help minimize further propagation
of sensitive data. Obfuscation is a separate technique, that’s typically used when
there is a need to hide the meaning of data. Obfuscation can be used in tandem
with cryptography to serve as a deterrent to data disclosure methods such as reverse
engineering. Last, truncation merely allows one to reduce or eliminate unwarranted
portions of the data by redacting them out.
Regardless of the chosen method, it is equally important to understand that data
control is never foolproof. In fact, it may merely serve as a temporary deterrent to
data disclosure, in many cases, until the attacker either ﬁnds a different attack vector
or identiﬁes an external data source which, when combined with the reduced version
of the data, it can provide for the missing data elements thus nullifying the effect of
the data extracts.
Nonetheless, at a minimum, it is imperative to conduct adequate quality control
to verify that the expected results are met by the technique employed. Speciﬁcally,
the method should achieve the desired result and nothing more. Typically this means
assessing the possible vectors for exposures such as data reversibility, even through
side-channel methods or inference. Data extracts and data redaction in general
can have other unintended consequences. For example, when using truncation, one
should consider the negative implication on referential integrity which could have
implications if certain controls around data, such as labeling, rely on the integrity of
these links.

480
F. Villanustre and J. Robinson
11
Network Threat and Vulnerability Management
Threat and vulnerability management continues to gain attention as ease of com-
promise has paved the way for crime and illicit proﬁt. By design, the nature of
Data-Intensive Computing is that it serves as an aggregation point for systems and
data therein. This provides an attractive path for cyber criminals in cases where
a viable attack vector presents itself. Examples of traditional attacks that can also
affect these types of environments include phishing attacks and distributed denial of
service attacks.
Additionally, and by the fact that the Data-Intensive Computing platform is
usually built upon hundreds of homogeneous nodes, it could serve as a powerful
vehicle for task execution. For that reason, it could be said that it provides a valuable
resource for attackers to leverage. For example, social networking technology has
been re-purposed for botnet agents and also for command and control capabilities.
Hence, it is plausible that attackers may view the resource as more attractive than
the data therein [13].
As part of the risk mitigation, it is critical to map the network including ingress
and egress points. Identify areas of exposure while considering interfaces, bespoke
channels, and APIs [14]. Given these exposures, analyze the different data ﬂows
including their dependencies on the network and other systems therein.
Develop applications that leverage the Data-Intensive Computing platform
according to an industry accepted framework or collective guidance, such as the
Open Web Application Security Project (OWASP). Review and assess changes with
a security focus in mind. The extent of this assessment should include the early

17
Data Security and Privacy in Data-Intensive Computing Clusters
481
Fig. 17.2 Software development life cycle (SDLC)
stages in the Software Development Life Cycle (SDLC), such as development and
Quality Assurance and continue through staging and production to include static
code analysis and penetration testing where applicable.
Figure 17.2 illustrates a typical SDLC where the waterfall method is concerned.
Continuous threat monitoring, including network and host based intrusion
detection systems, together with continuous surveillance of security vulnerabilities
across the platform, including both, software developed in house and third party
software is vital to an integral security posture. Proactive patch management and
regular security testing are also a valuable resource to ensure overall security.
It is a must to correlate security event data across all sources and in a long
timescale. Given the potential for large volumes of raw data, the use of a data
intensive computing platform to perform the correlation itself can allow the security
staff to arrive at useful information within reasonable timeframes, and promptly take
actionable steps in the case of required remediation.

482
F. Villanustre and J. Robinson
12
Security Auditing
An audit may help support many objectives. Common objectives may include
fostering a greater sense of trust where clients are concerned (i.e. agency theory) and
helping a ﬁrm to achieve compliance with ﬁduciary responsibility. In other cases, a
ﬁrm may merely leverage an audit as a means to discretely report the effectiveness
of its security controls to clients (e.g. attestation). In any case, it is best to adopt or
employ a compliance framework and an audit framework, which leverage industry
accepted practices. An industry accepted framework may help a ﬁrm reuse best
practices that have been developed, scrutinized, and generally accepted by the global
IT community.
Leverage the internal staff to audit the Data-Intensive Computing platform on
a routine basis [15]. Conventionally, a ﬁrm may employ a dedicated internal audit
team to audit accordingly. However, consider possible synergy between the internal
audit staff and technologists such as the information security team, architects, and
engineers. For example, technologists may help streamline the audit process based
on their knowledge of the environment and knowledge of the perceived risks. Also,
they may possess tools and techniques, which the audit team may leverage while
maintaining adequate separation of duty during the audit process.
Consider engaging an independent party to audit the Data-Intensive Computing
platform on a periodic yet consistent basis. This can be advantageous for several rea-
sons: in some cases, an independent auditor may provide needed staff augmentation
in cases where internal resources are not available or lack the needed skills; in other
cases, due to an auditor’s outside-in view of environment, they may aid internal
resources with enhancing the security posture of the Data-Intensive Computing
environment, as they help discover unaccounted anomalies given stated controls
and actual implementation. To that end, properly vet prospective auditors to help
ensure an appropriate balance between compliance and security know-how.

17
Data Security and Privacy in Data-Intensive Computing Clusters
483
13
Conclusions and Trends
The current information explosion, together with the limitations of the traditional
RDBMS model continuously pushes for innovative ways to extract actionable
intelligence from massive amounts of raw data. As a natural consequence of this
trend, emergent technologies have appeared in this ﬁeld, the most noticeable being
Data Intensive Computing. We analyzed the current state of this technology from an
Information Security standpoint, with emphasis on practical safeguards around data
protection, privacy and overall security.
As we indicated, data and systems security is a process, which begins early in
the Software Development Life Cycle with the deﬁnition of security milestones,
and continues with the analysis during the formal risk assessment. We referenced
Threat Modeling as a valuable utility to aid practitioners in the examination process.
Privacy requires adequate data classiﬁcation and a solid governance process to
identify derived data elements and classify them accordingly.
Although Data Intensive Computing is an emerging technology, industry stan-
dards and best practices can be effectively applied to implement controls that can
properly mitigate the diverse risks. Although there are some promising technologies
that could become viable in the next few years, such as homomorphic encryption
[16], we don’t currently recommend utilizing methodologies that haven’t been
vetted by a large community.
The different risks within Data Intensive Computing have been outlined although
the speciﬁc details largely depend on the particular environment and the intrinsic
sensitivity around the type of data involved. Because of this fact, the appropriate
response shall depend on a variety of factors. For example, it could be said that
each organization faces distinct risk tolerance levels, and corporate and ﬁduciary
responsibilities [17] in the case of security and privacy. As a result, the appropriate
balance between security and usability may vary across organizations. Ultimately,
the controls that we have provided should be considered judiciously and prioritized
for implementation accordingly.
When it comes to risks speciﬁc to Data Intensive Computing and future state,
we described those that appear as a result of the ability to harness the performance
advantages of this platform and the massive amounts of data to extract intelligence
that could be used by a cybercrime organization to commit fraud and/or to leverage
more traditional attacks. Although Data Intensive Super-Computing is still mostly
in its infancy, Opensource platforms such as LexisNexis HPCC [18] and Apache
Hadoop will eventually make this technology pervasive, increasing its relevance as
a viable target.
Conversely, Data-Intensive Computing holds positive implication on security and
privacy objectives facing organizations. For example, there is promise in leveraging
Data-Intensive Computing to deliver counter threat intelligence and manage risk.
For example, the immense analytic capabilities of these platforms provide a
noteworthy resource for reaching well-informed decisions rapidly while processing
the universe of events [15]. And the sheer depth and breadth of resources attributed

484
F. Villanustre and J. Robinson
to Data-Intensive Computing clusters offer an attractive alternative to traditional
computing platforms. And in the case of risk management, prominent ﬁrms have
turned to Data-Intensive Computing in order to deliver risk mitigation capabilities
to their clients. Overall, given the appropriate design and implementation of
security and privacy safeguards, we believe that Data-Intensive Computing can be a
formidable asset.
References
1. J.D. Meier, A. Mackman, and B. Wastell, “How to: Create a Threat Model for a Web
Application at Design Time,” http://msdn.microsoft.com/en-us/library/ff647894.aspx
2. M. Wilson, J. Hash, “Building an Information Technology Security Awareness and Training
Program,” http://csrc.nist.gov/publications/nistpubs/800-50/NIST-SP800-50.pdf
3. “Information technology — Security techniques — Code of practice for information security
management,” http://www.iso27001security.com/html/27002.html
4. A. Evﬁmievski, “Randomization in privacy preserving data mining,” ACM SIGKDD Explo-
rations Newsletter, Vol. 4, No. 2, Dec. 2002.
5. Y. Zhu, and L. Liu, “Optimal randomization for privacy preserving data mining,” Proceedings
of the tenth ACM SIGKDD international conference on Knowledge discovery and data
mining, 2004.
6. A.S. Narayanan, “How to break anonymity of the Netﬂix prize data set,” http://arxiv.org/abs/
cs/0610105, 2007.
7. K. Knorr and H. Weidner, “Analyzing Separation of Duties in Petri Net Workﬂows,” Depart-
ment of Information Technology University of Zurich, 2001.
8. J.A. Halderman, S.D. Shoen, N. Heninger, W. Paul, J.A. Calandrino, A.J. Feldman,
J. Appelbaum, and E.W. Felten “Lest we remember: cold-boot attacks on encryption keys,”
Communications of the ACM – Security in the Browser, Vol. 52, No. 5, May 2009.
9. R. Kissel, M. Scholl, S. Skolochenko, and X. Li, “Guidelines for Media Sanitization,” http://
csrc.nist.gov/publications/nistpubs/800-88/NISTSP800-88 rev1.pdf
10. Intel Corp. “Trusted Execution Technology” http://www.intel.com/technology/malwarer
eduction/index.htm
11. R. Araujo, “Microsoft MVP – Data Validation – Step One in Improving the Security of Your
Web Applications,” http://technet.microsoft.com/en-us/library/dd699463.aspx
12. J. Domingo-Ferrer and V. Torra2, “Aggregation Techniques for Statistical Conﬁdentiality,”
http://vneumann.etse.urv.es/webCrises/publications/bcpi/domingotorra calvomesiar.pdf
13. B. Prince, “How attackers use social networks,” http://securitywatch.eweek.com/botnet cncs/
how attackers use social networks for command and control operations.html
14. Cloud Security Alliance, “Cloud Security Matrix,” https://cloudsecurityalliance.org/research/
projects/cloud-controls-matrix-ccm/
15. Cloud Security Alliance, “Top Threats to Cloud Computing,” https://cloudsecurityalliance.org/
topthreats/csathreats.v1.0.pdf
16. C. Gentry, and S. Halevi, “A working implementation of fully homomorphic encryption,” http://
eurocrypt2010rump.cr.yp.to/9854ad3cab48983f7c2c5a2258e27717.pdf, EuroCrypt, 2010.
17. J.H. Langbein ‘Questioning the Trust Law Duty of Loyalty’ (2005) 114 Yale Law Journal
929–990.
18. J.E. West “LexisNexis brings its data management magic to bear on scientiﬁc data,”
http://www.hpcwire.com/hpcwire/2009-07-23/lexisnexis brings its data management magic
to bear on scientiﬁc data.html, HPC Wire, 2009.

Chapter 18
Information Security in Large Scale
Distributed Systems
Salvatore Distefano and Antonio Puliaﬁto
1
Introduction
The actual IT trend deﬁnitely moves towards network-distributed computing
paradigms. From clusters to data-centers built on multi-core systems, such trend
focuses on mechanisms and tools for aggregating distributed resources through
the Internet. Peer to peer systems exploit overlay networks in order to connect
peers, volunteer computing adopts a client-server approach to aggregate volunteer
contributors, Grids specify a resource broker (or workload management system)
for managing resources and requests, Clouds implement speciﬁc interface for
aggregating, managing and interfacing (physical and virtual) resources according
to a service oriented computing paradigm, as a service. All such mechanisms
and tools allow to aggregate from thousands (Grid, Cloud) to millions (P2P,
volunteer computing) resources into large scale distributed systems. Even though
the ﬁnal purpose of such paradigms differs, ranging from business-commercial
applications (Grid, Cloud) to scientiﬁc-academic-free of charge contexts (Grid,
volunteer computing) and to ﬁle/resource sharing (P2P, volunteer computing),
there are some problems and issues common to all of them. One is information
security.
In fact, sharing data in distributed multi-user environments triggers problems of
security concerning data conﬁdentiality and integrity. Some of the middleware im-
plementing the above referred paradigms usually provide resources management’s
capabilities, ensuring security on accessing services and on communicating data,
but they often lacks of data protection from direct malicious accesses, at system
level. In other words, the fact that data are disseminated and stored in remote
nodes, directly accessible from their administrators, constitutes one of the main
S. Distefano () • A. Puliaﬁto
Dipartimento di Matematica, Universit`a di Messina, Contrada Papardo, S. Sperone,
98166 Messina, Italy
e-mail: sdistefano@unime.it; apuliaﬁto@unime.it
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 18, © Springer Science+Business Media, LLC 2011
485

486
S. Distefano and A. Puliaﬁto
risk for data security in such environments, known as insider abuse/attack. It is
therefore mandatory to introduce an adequate data protection mechanisms, which
denies data intelligibility to unauthorized users, also if they are (local) system
administrators.
The problem of a secure storage access has been mainly faced in literature as
deﬁnition of access rights [3], in particular addressing problems of data sharing,
whilst the coding of the data is demanded to the user, since no automatic mechanism
to access to a secure storage space in a transparent way has been deﬁned.
In [6] the authors propose a technique for securing data disseminated over Grid
gLite environment based on symmetric cryptography (AES). The key security is
entrusted to a unique keystore server that stores it, to which all the data access
requests must be notiﬁed in order to decrypt the data. This algorithm implements
a spatial security policy: the security lies in physically hiding and securing
the keystore server, and the access to the keystore is physically restricted and
monitored in order to protect from malicious users, external attacks and insider
abuses.
In [7] the authors studied in depth the problem of data access, and propose
a solution based on symmetric keys. In order to prevent non-authorized accesses
to the symmetric key the authors propose to subdivide it on different servers. A
similar technique has been speciﬁed by Shamir in [8], used in PERROQUET [2]
to modify the PARROT middleware [9] by adding an encrypted ﬁle manager. The
main contribution of such work is that, by applying the proposed algorithm, the
(AES) symmetric key, split in N parts, can be recomposed if and only if all the N
parts are available.
All the proposals above mentioned are based on symmetric cryptography. Most
of them implement keys splitting algorithms. The underlying idea of the key
splitting approach is that at least a subset of the systems (key servers) over which
the keys are distributed will be trustworthy. However this approach is weak from
three points of views: the security, since the list of servers with key parts must
be adequately secured, the system administrators can always access the keys and
it is really hard to achieve trustworthy on remote and distributed nodes for users;
the reliability/availability, since if one of the server storing a part of the key is
unavailable, the data cannot be accessed; the performance, since there is an initial
overhead to rebuild a key, depending on the number of parts in which the key is
split. A solution for improving reliability/availability is to replicate the key servers,
but this contrasts with security challenges.
The goal of our work is to provide a mechanism to store data in large scale
distributed system, overcoming the above discussed problems and issues. Starting
from literature, our solution will base on cryptography. In order to address per-
formance issues and challenges, especially in high performance and data intensive
computing contexts, we propose to combine both the symmetric and the asymmetric
cryptography, into a lightweight and effective technique that conjugates the high
security goal with performance issues, as also suggested in [10]. The technique we

18
Information Security in Large Scale Distributed Systems
487
propose has been really implemented into the gLite Grid middleware in order to
demonstrate the feasibility of the approach, supported by the encouraging results
obtained through the GS3 performance evaluation. Other interesting contributions
of GS3 to the state of the art are the organization of the Grid data into a ﬁle
system, the protection of both data/ﬁles and of the ﬁle system structure, and the
introduction of the capability of ﬁle rewriting in gLite storage systems, not actually
implemented.
The remainder of the chapter is organized as follows: in Sect. 2 we describe the
main algorithms of the proposes technique, and its implementation into the gLite
Grid middleware (Sect. 3). Then, in Sect. 4 the results obtained by evaluating our
implementation are discussed. A ﬁnal discussion on our work, some ﬁnal remarks
and possible future work are instead reported in Sect. 5.
2
The Algorithm
The main goal of this work is to achieve data security in large scale distributed
systems speciﬁcally conceived for providing users with access to an enormous
amount of storage space. In such context, data conﬁdentiality and integrity must
be pursued avoiding both outsider and, in particular, insider attacks: no one except
the user/owner can access data, including system administrators.
In order to achieve data security, the best solution is the cryptography. As
discussed in Sect. 1, till now, the most successfully approach adopted for solving
the problem is the symmetric cryptography due to its performance against the
asymmetric one. The best approach is therefore to encrypt data by exploiting
a symmetric cryptography algorithm, moving the problem of security towards
a problem of symmetric key (DataKey) securing-hiding, as also stated by the
Kerckhoffs’ principle [4].
As above introduced, the most successful approach for addressing the key-
security problem is to split the key (DataKey) among different KeyServers. The
main drawback of such approach is the total failure of the protection mechanism
in case a malicious user obtains the key. This technique does not protect the owner
of data from whose have (legally or illegally) the privileges of administrator, since
administrators can access to all the key components/splits. For this reason we retain
it is necessary to use a more effective solution.
An adequate one could be to implement a two-stage encryption algorithm: at the
ﬁrst stage data are encrypted by exploiting a fast symmetric encryption algorithm
(AES), at the second stage the symmetric key used for encrypting data is in its
turn encrypted. In order to ensure exclusive data accesses to users only asymmetric
cryptographyalgorithms, such as the public key infrastructure (PKI) [5], can provide
adequate guaranties since, for example, the combination of PKI infrastructure and
smartcard device ensures keys inaccessibility.

488
S. Distefano and A. Puliaﬁto
A whole asymmetric cryptography algorithm, also applied to data encryption,
has a heavy impact on data access times, since this requires greater computational
resources and therefore is slower than the symmetric algorithms. This usually does
not allow to encrypt large amounts of data in acceptable time for users, considerably
slowing down the overall system’s performance. Thus, it becomes necessary to
implement a solution representing a trade-off between the two requirements of
security and performance. The only condition to satisfy is that no other than the
owner of data can access to the private key, that is the only exclusive way to decrypt
data encrypted by the coupling public key. In other words we assume that the owner
of data is sure that his or her private key is securely stored in a trust location (for
example a physical token such as a smartcard device).
The proposed technique combines both symmetric and asymmetric cryptog-
raphy into a hierarchical approach, ensuring high security. An authorized user,
authenticated by his/her own X509 certiﬁcate through the user interface, contacts
the storage system where his/her data are located. Data in the storage system are
encrypted by a symmetric cryptography algorithm whose symmetric DataKey (K)
is also stored in the storage system, in its turn encrypted by the user/owner public
key KPUB, obtaining the encrypted DataKey KPUB.K/. In this way, only the user
that has the matching private key KPRIV can decrypt the symmetric DataKey and
therefore the encrypted data. The encrypted DataKey KPUB.K/ is stored together
the data in order to allow the user-owner to access data from any node of the
infrastructure. He/she only needs the private key, hosted into the physical token.
In order to implement data sharing, the DataKey K is saved into the storage,
replicated into as many copies as the users authorized to access data. In this
way a copy of DataKey encrypted by the ith authorized user public key Ki
PUB
(Ki
PUB.K/) must be stored into the storage system in order that the user can access
the data.
Notice that, in the proposed algorithm, the decryption is exclusively performed
into the authorized users’ node where the corresponding X509 certiﬁcate is hosted,
and the decrypted symmetric key, the data and all the other information concerning
these latter are kept into unswappable memory locations of such node to avoid
malicious accesses. In this way the highest layer of security is achieved and
ensured: data and keys are always encrypted when they are far from the user,
both in the remote storage and in transfers; they are in clear only when reach
the trust user host, always and exclusively kept in the user space unswappable
memory.
From an algorithmic point of view, the security logic architecture just de-
scribed can be decomposed into two steps: pictorially depicted in Fig. 18.1: (1)
the symmetric DataKey K is encrypted through the user public key KPUB, and
it is written in the storage system; then (2) K is ready to be used for data
encryption. The algorithm implementing this mechanism can be better rationalized
in three phases: initialization, data I/0, and ﬁnalization/termination, detailed in the
following sections.

18
Information Security in Large Scale Distributed Systems
489
Fig. 18.1
Cryptography security algorithm
Fig. 18.2 Initialization algorithm
2.1
Initialization
The ﬁrst phase of the algorithm is devoted to the initial setting of the distributed
environment. The step by step algorithm describing the initialization phase is
reported in form of activity diagram in Fig. 18.2.
Once a user logs in the distributed environment trough the user interface, the
algorithm requests to the storage system the symmetric DataKey K encrypted by
the public key of the user KPUB. If the storage system has been already initialized,

490
S. Distefano and A. Puliaﬁto
a
b
c
Fig. 18.3 Data I/O algorithms: read (a), write (b) and generic ops (c)
its answer contains the encrypted DataKey KPUB.K/, that is decrypted by the user
private key KPRIV and then saved in a safe memory location of the user interface.
Otherwise, a the ﬁrst access to the storage system, a DataKey K must be created by
the user interface side of the algorithm and therefore encrypted and sent to the other
side.
2.2
Data I/O
The data stored in the storage system are organized according to a ﬁle system
structured in directories. The data are managed and accessed by well-known
primitives such as open, close, read, write, delete, list, rename, etc. In Fig. 18.3,
the algorithms implementing read, write and generic operations (delete, rename,
list, etc) are represented by activity diagrams. In particular the read algorithm of
Fig. 18.3a implies the decryption of data received by the storage system, while the
write algorithm of Fig. 18.3b requires the encryption of data before they are sent to
the storage system. A generic operation instead only sends a command or a signal,
as shown in Fig. 18.3c.
2.3
Termination
The termination phase algorithm is described by the activity diagram of Fig. 18.4.
Before the user logs out the system, it is necessary to remove the symmetric Datakey
and the other reserved information from the user interface memory. But, since a user
could still have one or more data I/O operations active/alive, it is possible he/she
wants to know the status of such operations, and therefore asks to the storage system
about that. Then, evaluating the obtained answer he/she can choose to terminate the
current session or to wait for the completion of some of them. Finally the user logs
out the infrastructure.

18
Information Security in Large Scale Distributed Systems
491
Fig. 18.4 GS3 termination
phase algorithm
3
The Implementation
The idea of combining symmetric and asymmetric cryptography in the data security
algorithm detailed in Sect. 2, has been implemented as a service in the gLite Grid
middleware into the grid secure storage system (GS3).
Since the GS3 implementation must be integrated in the gLite environment which
uses its own storage libraries (GFAL), the best solution available to simplify the use
of the Grid secure storage and to better integrate such implementation into the gLite
middleware is to base on GFAL. In order to ensure high security it is also necessary
that the secure storage service must be available in interactive mode from the UI,
that exclusively performs data decryption.
In such implementation, we choose the AES [1] algorithm for symmetric en-
cryptions, and the public key infrastructure (PKI) [5] for asymmetric cryptography.
Moreover, for the sake of simplicity and portability towards other paradigms a
POSIX interface has been implemented.
3.1
Storage Architecture
The architecture implementing the GS3 algorithm in the gLite middleware, satisfy-
ing the requirements and speciﬁcations above described, is depicted in Fig. 18.5a.
Thus, GS3 is implemented as a layer working on top of GFAL, providing a ﬁle
service with security/cryptography capability by means of POSIX interface.
The GS3 storage service creates a virtual ﬁle system structuring the data in
ﬁles, directories and subdirectories without any restrictions on levels and number
of ﬁles per directory. Since we build this architecture on top of GFAL, in GS3
all data objects are seen as ﬁles stored on the SE, accessible by users through

492
S. Distefano and A. Puliaﬁto
GFAL
GS3
gLite
UI
UI
a
b
SE
K(GS3FI)
K(BLK[1])
K(BLK[n])
SE
FILE 
BLOCKS 
CACHE 
GS3FBC
GS3 
FILE
Unswappable Mem
KPUB(K)
Fig. 18.5 GS3 gLite implementation architecture (a) and ﬁle system (b)
the GFAL interface (LFN, SRM, GUID, etc). Thanks to the storage architecture
and the internal organization, this GS3 implementation provides all the beneﬁts
of a ﬁle system. One of the most interesting is the capability of ﬁle modiﬁcation
and/or rewriting, operation not implemented by the GFAL library. GFAL only
allows to create/write new ﬁles, without any possibilities of modifying those after
creation.
A GS3 ﬁle can be entirely stored in the SE in one chunk with variable length or
it can be split into two or more blocks with ﬁxed, user deﬁned length, speciﬁed in
the GS3 setup conﬁguration, as reported in Fig. 18.5b. To avoid conﬂicts among
ﬁle names, we univocally identify each chunk of data stored on the SE by a
UUID identiﬁer. The ﬁle index shown in Fig. 18.5b (GS3FI), maps a ﬁle to the
corresponding blocks in the SE. Such ﬁle index is encrypted through the symmetric
DataKey and is kept in UI unswappable memory locations. In this way the user
operates on a virtual ﬁle system whose logic structure usually does not correspond
with its physical structure in the SE, since each ﬁle can be split into many blocks
stored in the SE as ﬁles. But the main goal of ﬁle indexing is the optimization
of the ﬁle I/O operations, since it reduces the data access time. Moreover, since
the GS3 ﬁle rewriting and modiﬁcation on the SE has to be implemented through
GFAL primitives, these operations are performed by deleting the ﬁle and rewriting
its modiﬁed version; splitting a GS3 ﬁle into several chunks/blocks ﬁles in the SE is
the only feasible way to reach the goal.
The ﬁle system is created and stored on the SE when the GS3 initialization is
performed. Each ﬁle referring to data stored on the SE is encrypted by a symmetric
DataKey stored on the same SE and encrypted by the user public key.
In order to optimize the ﬁle I/O operations performance, a local cache of
encrypted blocks/chunks (GS3FBC) is held in the UI unswappable memory. All the
operations involving blocks/chunks already loaded in the UI cache are performed
locally, varying the content of such blocks/chunks. When a ﬁle is closed, the blocks
stored in cache are updated to the SE. A speciﬁc GS3 primitive (gs3 flush) has
been speciﬁed to force the ﬂushing of data from the UI cache to the SE storage. This
remarkably speeds-up the performance of the storage system, reducing the number
of accesses to the SE. Problems of cache coherence may arise if there are more than
one simultaneously active access on the Grid storage working on the same data.

18
Information Security in Large Scale Distributed Systems
493
At the moment, we apply a relaxed consistency protocol allowing to have different
copies of the same data on local caches.
3.2
GS3 Interface Library and API
Since the library commands implement a POSIX.1 interface, the access to a ﬁle on
the virtual encrypted ﬁle system is similar to the access to a local ﬁle. GS3 speciﬁes
the same library functions set of GFAL: in the former case the functions are preﬁxed
by “gs3 *” while in the latter case by “gfal *”. The main difference between GS3
and a POSIX interface is constituted by the initialization and the termination phases
as described in Sect. 2. In the following we specify the GS3 primitives starting from
the same phases characterization identiﬁed above.
3.2.1
Initialization
The initialization phase is the most important phase of the GS3 gLite implemen-
tation. In this phase the library context is initialized with the user preferences set
on environment variables: GS3 PATH (URL base where the data ﬁles are stored),
GS3 PUBKEY (user’s public key used to encrypt), GS3 PRVKEY (user’s private
key used to encrypt).
A user needing to access the SE must invoke the gs3 init function in order to
read from storage space the symmetric DataKey K encrypted by the user public key
KPUB. As shown in Fig. 18.6, and also introduced in Sect. 2.1, two cases distinguish
the ﬁrst from successive accesses. In the ﬁrst initialization phase, gs3 init
generates the symmetric key K as sequence of random numbers, returned by an
OPENSSL function. In the following accesses gs3 init loads K and the ﬁle
index GS3FI from the storage elements. The algorithms in both cases are similar:
ﬁrstly the UI checks the presence of the encrypted key KPUB.K/ in the SE by a
gfal stat, then, in case it does not exist, a new key is created (Fig. 18.6a) and
sent to the SE, otherwise the key and the ﬁle index are loaded in the UI by two
consecutive gfal read operations (Fig. 18.6b) The encrypted Datakey KPUB.K/
is therefore decrypted by the user private key and placed into an unswappable
memory location of the UI to avoid malicious accesses.
3.2.2
Data I/O
GS3 data I/O operations are implemented through I/O POSIX primitives such as:
open, read/write and close. Files are always encrypted in memory, the encryption
is performed at runtime. To improve the GS3 performance and the usability of its
library the accessed ﬁles’ chunks are locally buffered into a cache in the UI until the
corresponding ﬁles are closed. At ﬁle closing, the UI cache is synchronized with SE.

494
S. Distefano and A. Puliaﬁto
gfal_stat(KPUB(K),*Stat)
SE
gfal_write(KPUB(KNEW)FD,
*KPUB(KNEW),C)
UI
a
b
Stat,Res1
Res2
FILE 
BLOCKS 
CACHE 
GS3FBC
FILE 
INDEX  
GS3FI
KNEW
KPUB(KNEW)
KPUB(K)
Unswappable Mem
Stat
gfal_stat(KPUB(K),*Stat)
SE
gfal_read(KPUB(K)FD,
*KPUB(K),C)
Stat,Res1
KPUB(K),Res2
K(GS3FI)
gfal_read(K(GS3FI)FD,
*K(GS3FI),C)
K(GS3FI),Res3
UI
FILE 
BLOCKS 
CACHE 
GS3FBC
FILE 
INDEX  
GS3FI
K
Unswappable Mem
Res2
Res2
Res3
Stat
Res1
Res1
K(BLK[1])
K(BLK[n])
Fig. 18.6 gs3 init() Library initialization: ﬁrst (a) and following uses (b)
More speciﬁcally, the gs3 read(int fd, void *buf, int c) prim-
itive reads c bytes of data of the ﬁle referred by the fd ﬁle descriptor placing
that in the local UI buffer buf. As pictorially described in Fig. 18.7a, by using
the ﬁle index and the input parameters, the corresponding SE blocks descriptor set
(BLKD1) is obtained. The blocks not present in the cache, identiﬁed by the set
BLKD2  BLKD1, are loaded from the SE by a gfal read call. Such data,
with the data loaded from cache, are placed in the output buffer, and the ﬁle
blocks cache is updated with the data just loaded from the SE. The sets BLKD1
and BLKD2 correspond to the vectors BLKD1[] and BLKD2[] of Fig. 18.7a.
The gs3 write(int fd, const void *buf, int c) is an operation
entirely performed locally to the UI, as shown in Fig. 18.7b. The data blocks to
modify in the SE are temporarily saved into the ﬁle blocks cache. When the ﬁle is
closed, renamed, moved, deleted, the ﬂush of the cache is forced, or the gLite GS3
session is terminated, the data in cache are synchronized with the corresponding one
in the SE.
gs3 <op>(int fd, <par>) is a generic data I/O operation mapped into
the corresponding GFAL operation gfal <op>(int fd, <par>). When
a gs3 <op>(int fd, <par>) modiﬁes the ﬁle system structure (delete,
rename, move, mkdir, etc) it is necessary to update the ﬁle index in the SE.

18
Information Security in Large Scale Distributed Systems
495
FD
C
K(BLKD1[])
C1[]
gfal_read(K(BLKD2[]),*BUF,C2[])
K(GS3FI)
K(BLK[n])
K(BLK[1])
K(BLK2[]),Res
BUF
UI
SE
K(BLKD2[])
C2[]
Res
FILE 
INDEX  
GS3FI
KPUB(K)
Unswappable Mem
FILE 
BLOCKS 
CACHE 
GS3FBC
FD
C
K(BLKD1[])
C1[]
BUF
UI
a
b
c
SE
FILE 
INDEX  
GS3FI
K(GS3FI)
K(BLK[n])
K(BLK[1])
KPUB(K)
FILE 
BLOCKS 
CACHE 
GS3FBC
Unswappable Mem
Uns Mem
K(BLKD[])
gfal_<op>(K(BLKD[]),
<pars>)
UI
SE
Res1
FD
Res1
gfal_write(K(GS3FI)FD,
*K(GS3FI),C2[])
Res2
Res2
FILE 
INDEX  
GS3FI
K(GS3FI)
K(BLK[n])
K(BLK[1])
KPUB(K)
FILE 
BLOCKS 
CACHE 
GS3FBC
Fig. 18.7 GS3 data I/O primitives: gs3 read (a), gs3 write (b) and gs3 <op> (c)
Uns Mem
gfal_write(K(BLKD[]),
*K(GS3FBC[]),C1[])
SE
gfal_write(K(GS3FI)FD,
*K(GS3FI),C2[])
UI
K(BLKD2[])
K(BLKD1[])
C1[]
C2[]
Res2
Res1
FILE 
BLOCKS 
CACHE 
GS3FBC
Res1
Res2
FILE 
INDEX  
GS3FI
K(GS3FI)
K(BLK[n])
K(BLK[1])
KPUB(K)
Fig. 18.8 gs3 finalize primitive implementation
3.2.3
Termination
The main goal of the termination operation is the synchronization of data between
the UI cache and the SE. This is implemented by the gs3 finalize() function,
a simpliﬁed version of which is detailed in Fig. 18.8. It describes two separated
gfal write operations into the SE: the ﬁrst writes all data of the UI ﬁle blocks
cache (GS3FBC), the other writes the UI ﬁle index (GS3FI).
This sequence implements a gs3 flush function, called each time a ﬁle is
closed, deleted, renamed, etc. Moreover, this is a simpliﬁed version of gs3 flush,
since the GFAL libraries do not implement the rewriting capability: a ﬁle can be
written only when created. Thus, if a ﬁle already exists in the SE, GFAL does not
allow to modify it. In order to implement this capability in GS3, using the GFAL

496
S. Distefano and A. Puliaﬁto
library, it is necessary to bypass the problem of rewriting by deleting and creating
a new ﬁle each time the ﬁle is modiﬁed. This mechanism is a little bit complex
and hard to pictorially depict, so we only show a simpliﬁed version in Fig. 18.8.
However, the rewriting algorithm has been entirely implemented in the GS3 library.
4
Performance Evaluation
In order to evaluate the performance of the GS3 speciﬁc experiments have been
executed. In such tests a ﬁle has been created/written and then we have performed
read and delete operation over it. The tests have been performed by varying the
ﬁle size from 28 to 217 bytes, doubling its size in each experiment. Therefore in
total we made ten different tests. By these, we evaluate the performance of the GS3
primitives.
The behavior of GS3 has been compared with that of the GFAL, and of an
enhanced version of GFAL in which we added only the encryption feature (we call
it CGFAL). In order to provide a complete picture of the GS3 performance we have
made the same measures on the local ﬁle system (LOCAL).
In the tests, we have evaluated the performance of operations directly performed
into the SE by considering the different environments. This could be considered as
the worst case for GS3, in which the operation is directly synchronized with the SE,
without taking into account the cache.
As performance metric we consider the elaboration time, i.e. the time elapsed
from the operation’s launching until the results are fed back to the user. In order to
provide signiﬁcant measures, we have repeated each test 1; 000 times, and calculated
the average of the results thus obtained.
4.1
Write
The results obtained by evaluating the elaboration time of the create/write calls
are reported in Fig. 18.9. Such results show similar trends for all the considered
environment. As can be easily expected, the elaboration times of write operations
from UI to SE are affected by the ﬁle size since it hardly affect the data transferring
and storing.
By comparing the results of the different libraries, we can observe that, without
considering the impact of cache, GS3 is considerably slower than GFAL, CGFAL
and obviously than LOCAL calls. This is due to the fact that, each time a GS3
write into the SE is performed, it is also necessary to update the SE ﬁle index, and
therefore two consecutive gfal write are needed, as shown in Fig. 18.8. But, as
we can note by observing the performance of the encrypted CGFAL and the GFAL
ones, the time spent to access the communication network is orders of magnitude
greater than the computational time spent for encrypting data. This justiﬁes the

18
Information Security in Large Scale Distributed Systems
497
Fig. 18.9 Write time
performance gap among GS3 and the others: in the former case two network storage
accesses are required, the ﬁrst for storing data, the second for storing the ﬁle index,
while in the other cases only one access is needed. This is the cost of rewriting: to
implement such important feature, GS3 introduces the ﬁle splitting and therefore the
ﬁle index table.
4.2
Read
The performance obtained by the read tests are showed in Fig. 18.10. Similarly to
the write operation, the elaboration time trends increase by increasing the data size
due to the role of the interposed network. But, in this case, the results of the worst
case gs3 read elaboration are comparable to the gfal read ones and also to
those obtained by the CGFAL. This is due to the fact that in read operations we
don’t have to update the ﬁle index, and the computational overhead is mainly due
to the encryption. The trends also conﬁrm that the time spent in encryption tasks is
negligible with regards the time spent in communication.
4.3
Delete
Figure 18.11 reports the results obtained by the evaluating delete operations.
Obviously, the performance of delete operations do not vary with ﬁle size, since

498
S. Distefano and A. Puliaﬁto
Fig. 18.10 Read time
Fig. 18.11 Delete time
only a signal, few bytes, are sent. As in the write case, also in this case there is a
great gap between the GS3 performance and the others, due to similar motivations:
a GS3 ﬁle deletion, as shown in Fig. 18.7c, needs to update the SE ﬁle index
after removing the ﬁle from the SE. This introduces a further gfal write
operation of such ﬁle index, increasing the overall elaboration time of the GS3 ﬁle
deletion.

18
Information Security in Large Scale Distributed Systems
499
It can be also noticed that, the performance of delete operation do not vary with
ﬁle size.
5
Final Remarks
Information security is a hot open problem in distributed computing environments,
particularly felt in large scale distributed systems. Protecting data from malicious
accesses is the fundamental goal for different users: companies and enterprises,
that try to reduce the loss of information for avoiding industrial espionage;
research centers, that have the necessity to hide their results to competitors; public
administrations, hospitals and medical centers, that must ensure the conﬁdentiality
of their data; and so on.
The use of a distributed storage allows to remarkably reduce costs also allowing
to re-use obsolete resources with limited computing power.
In this work we described a secure (encrypted) storage system for distributed
systems. In this chapter we detail both the security algorithm and its implementation
into the gLite middleware as GS3.
The security algorithm is based on the idea of combining symmetric and
asymmetric cryptography. The symmetric cryptography is directly applied to data,
generating encrypted stored data. The symmetric key decrypting such encrypted
data is in its turn encrypted by the user-owner public key (asymmetric cryptography)
and stored into the Grid remote storage system. Decryption is performed by the user
interface node, and both the key and the data are allocated into unswappable memory
locations of such node. In this way the data can be accessed exclusively by the data
owner. In order to share such data with other users it is necessary to store in the Grid
storage copies of the DataKey encrypted by such users private keys.
The strength point of the GS3 implementation into the gLite middleware is
the deﬁnition of a speciﬁc secure ﬁle system on top of the GFAL library. This
choice allows to protect both data/ﬁles and also their structure, the whole ﬁle
system. Moreover, the gLite GS3 implementation introduces a new capability:
the ﬁle modiﬁcation and rewriting. This implementation has been evaluated, in
particular with regards the three main operations: read, write and delete. The
tests are performed by considering the worst case, in which the GS3 always
operates directly to the SE, comparing GS3 to other libraries (GFAL, CGFAL,
LOCAL) The results obtained shown higher elaboration times of GS3 than the
others in write and delete operations, due to the ﬁle index writing for each of
these operations, while in read operations they are very close to the GFAL and
CGFAL ones.
A deeper investigation on the GS3 performance by also considering the impact
of cache is one of the imminent/short term development. Other interesting points to
further investigate are: security improvements, cache coherence, data sharing, fault
tolerance, Quality of Service, system optimization and jobs batch.

500
S. Distefano and A. Puliaﬁto
References
1. AES (2001) Federal information. processing strandard pubblication 197.
2. Blanchet C, Mollon R, Deleage G (2006) Building an encrypted ﬁle system on the egee grid:
Application to protein sequence analysis. In: ARES ’06: Proceedings of the First International
Conference on Availability, Reliability and Security, IEEE Computer Society, Washington, DC,
USA, pp 965–973
3. Junrang L, Zhaohui W, Jianhua Y, Mingwang X (2004) A secure model for network-attached
storage on the grid. In: SCC ’04: Proceedings of the 2004 IEEE International Conference on
Services Computing, IEEE Computer Society, Washington, DC, USA, pp 604–608
4. Kerckhoffs A (1883) La cryptographie militaire. Journal des sciences militaires IX:5–83
5. Rivest RL, Shamir A, Adelman LM (1978) A method for obtaning digital signatures and
public-key cryptosystems. Commun ACM 21(2):120–126
6. Scardaci D, Scuderi G (2007) A secure storage service for the glite middleware. In: Inter-
national Symposium on Information Assurance and Security, IEEE Computer Society, Los
Alamitos, CA, USA, pp 261–266
7. Seitz L BL Pierson J-M (2003) Key management for encrypted data storage in distributed
systems. In: IEEE Security in Storage Workshop, Washington DC, USA, October 2003, IEEE
Computer Society, pp 20–30
8. Shamir A (1979) How to share a secret. Commun ACM 22(11):612–613
9. Thain D, Livny M (2004) Parrot: An application environment for data-intensive computing.
Journal of Parallel and Distributed Computing Practices
10. Tu M, Li P, Yen IL, Thuraisingham B, Khan L (2009) Secure Data Objects Replication in Data
Grid. Transaction on Dependable and Secure Computing To appear

Chapter 19
Privacy and Security Requirements of Data
Intensive Computing in Clouds
Arash Nourian and Muthucumaru Maheswaran
1
Introduction
Cloud computing is evolving into a popular computing paradigm for deploying
variety of data-intensive applications. Although there are many deﬁnitions of cloud
computing [1], it is largely an evolution of computing paradigms in use for many
years. Cloud computing offers the opportunity to commoditize several information
technology (IT) services. As more services are ported to the clouds, the amount of
data held by the clouds increase.
When migrating to the cloud, customers need to understand how it differs from
their existing environments. Clouds are shared and largely virtual environments.
Data owners should understand the implications of their data residing in the cloud
service provider’s data center under the protection of its security policies [2].
Further, the data security policies adopted by a cloud provider change over
time. With changing data security policies, customers need to comprehend policy
evolution to understand potential breaches in the security of their data.
Cloud computing systems emerged from the need for creating a platform that
can provide on-demand computing resources at low management cost. Therefore,
service level agreements (SLAs) that deﬁne the resource characteristics are central
to cloud computing. The SLAs are used to measure the compliance of the cloud
resource management activities and providers or consumers violating them are
A. Nourian
School of Computer Science, McGill University, McConnell Engineering Building,
Room 318, 3480 University Street, Montreal, QC, H3A 2A7 Canada
e-mail: nourian@cs.mcgill.ca
M. Maheswaran ()
School of Computer Science, Department of Electrical and Computer Engineering,
McGill University, McConnell Engineering Building, Room 754, 3480 University Street,
Montreal, QC, H3A 2A7 Canada
e-mail: maheswar@cs.mcgill.ca
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 19, © Springer Science+Business Media, LLC 2011
501

502
A. Nourian and M. Maheswaran
penalized accordingly. Because SLAs are crafted by mainly considering resource
management concerns, they are not suitable for dealing with the safety of data stored
and processed by cloud computing over long time scales.
A cloud ecosystem consists of many different participants. Currently, most
important participants are cloud providers (e.g., Amazon, Rackspace) and cloud
consumers (e.g., banks, e-commerce sites). Currently, the cloud model does not
include an independent regulator to enforce best practices or arbitrate conﬂicts
among the participants. Unlike computations, data processing can have security
concerns that span across long time spans. Such concerns are impacted by variations
of the policies adopted by the cloud providers and the trust among the participants.
Because cloud computing is still taking shape, regulatory aspects are still recom-
mendatory in nature (e.g., FedRAMP [3]).
Trust in clouds is still a ﬂedgling concept that is yet to be deﬁned in a widely
accepted manner. Consequently, customers do not have standard ways of evaluating
and comparing the trust and reputation scores of different cloud operators. This
means a cloud provider might not be equivalent and replaceable with another cloud
provider in terms of the trust that can placed on the resources.
2
Data Cloud Computing
With data-intensive applications, cloud computing becomes an even compelling
proposition. Most data-intensive applications gather data from sources such as
sensor devices (e.g., cameras), diagnostic services (e.g., medical imaging systems),
transaction management systems (e.g., e-commerce archives), and social network
systems. In these applications, rate at which data is generated increases with time.
Also, many data intensive computing applications have archival requirements at
least for a given duration. Therefore, the data load presented by these applications
increase with time. The dynamic resource allocation model where the consumer
pays based on the amount of resource usage is ideal for data-intensive applications
because it allows the consumer to keep the resource utilization very high. This
helps small companies with limited resources to launch data-intensive services at
low cost with minimum launch time [4]. They can successfully compete with high
investment companies because the underlying infrastructure needs are taken care by
the clouds [5].
2.1
Model for Data Clouds
Cloud computing can be categorized into four different types [6] known as public
cloud, private cloud, community cloud, and hybrid cloud. In a public cloud, resource
allocations are dynamically provisioned over the Internet. In these clouds different
customers’ data sit on cloud machines which are shared between customers. Private
cloud is built using private computing resources and networks. These type of

19
Privacy and Security Requirements of Data Intensive Computing in Clouds
503
clouds are built by organizations that need full control over their data and services.
A community cloud is a cloud which is shared by group of organizations. A hybrid
cloud environment combines multiple public and private clouds.
Although cloud computing brings scalability, high quality of service, user
friendly interface, and low cost,
[7–9], it is still suffering from several fun-
damental security drawbacks. The security requirements for cloud computing
providers typically include ﬁrewall isolation, network segmentation, intrusion
detection and prevention systems, and security monitoring tools. Realizations of
these requirements in clouds are based on virtual machines and are vulnerable
for attacks targeting virtual machine operating systems. Attackers can remotely
exploit the vulnerabilities in OSes to intrude into the cloud infrastructure and
siphon off valuable data from clouds. Often, virtual machines are co-located in
physical machines on which they are instantiated, which increase the effectiveness
of the intrusion attacks [10]. To address this type of attacks, either traditional
security techniques should be promoted to address security breaches in the virtual
machine level or new paradigms should be introduced to tackle the cloud security
issues.
2.2
Example Data Clouds
Cloud Computing has various ﬂavours of implementation that are relevant to data
clouds. Basically, it is composed of three service models such as Software as a
Service which are (SaaS), Platform as a Service (PaaS) and Infrastructure as a
Service (IaaS).
1. Software as a Service (SaaS)
Software as a Service refers to a software that is deployed over the Internet by
a provider which customers are able to subscribe on demand. Customers are
charged based on the number of services they use along with the duration of
usage. The most successful examples of such services are customer relationship
management (CRMs) (e.g., Saleforce.com) which can be used by anyone
regardless of the number of customers they might have without spending lot of
money for deploying such a system. Basically, in SaaS, software developers use
the clouds to rent their softwares instead of selling it and customers are capable
of renting a software until they need it by paying a ﬁxed subscription fees. This
model is highly appropriate for generic applications and is still not suitable for
customized applications.
2. Platform as a Service (PaaS)
Platform as a Service refers to platforms that enable users to create their own
cloud applications by providing the business logic. PaaS provides high level
access to system functions such as database management. Users are equipped
with tools for creating new applications rapidly with low cost by PaaS. The major
concern here is to avoid application lock-in as migrating between platforms is not
an easy task. Although there are many popular PaaS stacks such as Amazon EC2,

504
A. Nourian and M. Maheswaran
Eucalyptus, Microsoft Azure, and Google App Engine, an application developed
for one platform may require signiﬁcant effort to migrate to another platform.
3. Infrastructure as a Service (IaaS)
Infrastructure as a Service refers to computing resources as a service. An
IaaS cloud provides virtual processing, network, and data storage elements on
top of which customers can deploy custom system images to implement their
applications. Companies need not create their own data centers for their services.
They can deploy their services on top of an IaaS provider and get billed for the
resources according to their usage. The IaaS does not have the problem of lock-
in as providers normally export compatible virtual machine interfaces. Amazon
Web Services and Rackspace are two key players providing IaaS services.
Data-intensive computing applications can be implemented on clouds falling into
the three models given above. IaaS can be preferred approach for deploying data
intensive applications when precise control is needed on managing the computing
facilities provided by the clouds. For instance, parallel processing requirements of
data intensive applications can favour IaaS clouds. In contrast, PaaS clouds pro-
vide complete development environments for cloud applications. Various services
essential for web-oriented applications such as database service and, web service
are typically made available by PaaS clouds.
Besides the above cloud computing model that is generally adopted, other
cloud computing models have been proposed such as the Linthicum [11] and the
Jericho Cloud Cube [12] models. The Linthicum model consists of ten categories as
follows:
•
Information as a Service: using any type of information hosted in the cloud.
•
Database as a Service: using and sharing hosted databases in the cloud.
•
Storage as a Service: using and sharing physical storage provided by the cloud.
•
Application as a Service: using and sharing any hosted application inside the
cloud.
•
Platform as a Service: using and sharing the platform provided by the cloud.
•
Process as a Service: using hosted resources to create required resource for
business processes.
•
Security as a Service: using and sharing security services provided by the cloud.
•
Testing as a Service: using testing softwares and services hosted in cloud to test
inside and outside cloud provided services.
•
Management as a Service: using hosted controllers to manage cloud resource and
services.
•
Integration as a Service: using the stack provided by the cloud to control
integration processes.
While the Linthicum model reﬁnes the simple cloud model discussed previously,
the Jericho Cloud Cube model incorporates idea of dimensions to the traditional
model as follows.
•
Insourced Versus Outsourced: services which are under consumers control called
insourced whereas services controlled by third party are called outsourced.

19
Privacy and Security Requirements of Data Intensive Computing in Clouds
505
•
Internal Versus External: describes the physical location of data.
•
Perimeterized Versus De-perimeterized: inside or outside of traditional IT domain.
•
Proprietary Versus Open: using open technology or black box one.
3
Security Concerns in Data Clouds
This section describes some of the key security concerns in data clouds. These
concerns arise in the context of the overall safety of the data stored and processed
by cloud systems.
3.1
Data Conﬁdentiality
Conﬁdentiality refers to the prevention of unauthorized disclosure of users’ in-
formation. It requires cloud providers to ensure that unauthorized disclosure of
the information is prohibited. Conﬁdentiality concerns include two major issues:
authentication and access control. Most cloud installations use weak forms of
authentication like username and password to anchor the credentials that are used for
controlling access to data and resources. Further, the access control mechanisms in
most clouds do not support ﬁne grained control. Authentication and access control
work only in un-compromised systems. In a compromised cloud computing system,
preventing unauthorized information disclosure is much harder. A data cloud that
deals with highly sensitive data use encryption and/or data segmentation to limit the
conﬁdentiality breach when the cloud computing system is compromised.
Data segmentation helps minimize the amount of data disclosure in case any
of the servers that stores the data gets compromised. Data segmentation brings the
following advantages to the cloud’s security capabilities:
•
Consumers’ sensitive data leaks only when an entire system of cloud servers are
compromised.
•
Downtime associated with the compromise of an individual node is negligible.
Covert channel is another issue in the conﬁdentiality of data inside a cloud.
A covert channel is an unauthorized communication path that enables information
leakage. Covert channels can be used through timing of messages or inappropriate
use of storage mechanisms.
3.2
Data Integrity
Information held by data clouds is often used by many parties. In addition, many
parties often have the capability to update the information held in data clouds.

506
A. Nourian and M. Maheswaran
To maintain the integrity of the data, the capability to update the information
should be carefully controlled. Like data conﬁdentiality, data integrity depends on
the authentication and access control. If the authentication is based on credentials
that are anchored on weak usernames and passwords, the authentication process
can be easily compromised. This can lead to unauthorized updates to the data.
Similarly, if the cloud is compromised,data becomes unprotected from unauthorized
updates.
3.3
Data Provenance
Data has integrity if it is not changed in an unauthorized manner or by an
unauthorized person. Provenance means not only that the data has integrity, but
also that it was computed in the correct manner. Data clouds are used to hold data
that undergo series of computational transformations to gather useful results. The
computations are often applied in a data driven manner where data sets are piped
through computational elements. In this process, the meta data associated with the
data sets is used in applying the appropriate transformations. The meta data indicates
the “computational history” of a data set.
3.4
Data Availability
Data availability is one of the major beneﬁts provided by the cloud computing
systems. Using massive scale and sufﬁcient safeguards against various forms of
denial of service attacks, clouds are able to provide very high availabilities that rival
the best provided by dedicated systems. However, several highly publicized data
loses have happened in data clouds. Therefore, data availability is an issue that still
needs careful consideration when data cloud deployments are rolled out.
3.5
Data Accountability
Data accountability is the ability to verify the agents behind the different actions
in data clouds. To implement accountability, identifying information of the agents
need to be logged at different locations of the data clouds. Simply associating all
the actions with a customer account may not be always sufﬁcient. For instance, if an
account is compromised, then the customer may not be accountable for the actions
performed after the compromise. Therefore, sufﬁcient information should be logged
to detect the anomalous behaviour after the account compromise.

19
Privacy and Security Requirements of Data Intensive Computing in Clouds
507
3.6
Data Placement
Large cloud operators such as Amazon have resource installations in different
continents. Some countries for certain sensitive data (e.g., healthcare data [13])
stipulate that the data is actually stored within certain geographical locations. Such
stipulations should be considered by the data clouds while replicas of the stored
data are created. The data placement concerns arise because cloud operators can be
forced by court orders to disclose data stored in their servers.
3.7
Data Remnance
Data remnance is the residual of data left behind after a delete operation. It is created
because the delete operation just unlinked an allocated block and included it in the
free list. Although data remnance is an important issue, data cloud operators are
yet to explicitly state in their policies how their system is handling this concern. In
an IaaS cloud, the customer can stuff zeros in the storage volume before releasing
it to reduce data remnance. However, the customer has less control over data
remnance in SaaS and PaaS clouds. Data encryption or segmentation can help with
remnance.
4
Threat Modelling in Data Clouds
Threats in data clouds can be categorized as external or internal. External threats
emerge from parties outside the cloud or customers’ applications running on
the cloud. Internal threats emerge from the cloud itself or third party system or
management services that are integrated into the cloud.
When external threats emerge from outside parties, countermeasures developed
for protecting network assets on the Internet (e.g., intrusion detection and prevention
systems) can be useful in handling the threats. Threats emerging from application
programs are little harder to handle. In IaaS clouds, applications run inside
virtual machines, which can be used as capability constrained sandboxes to limit
the data that can accessed and manipulated by the application. In PaaS clouds, the
application programs use the APIs provided by the cloud provider to access the data
and services managed by the cloud. To safeguard sensitive information, APIs often
support API keys that restrict access to data to authorized programs.
Cloud computing and in particular the PaaS and SaaS variants assume that
the customer trusts the cloud service provider to keep the data secure. In many
situations, the cloud service providers have peering arrangements with other service
providers to share information regarding customers. This means a customer’s

508
A. Nourian and M. Maheswaran
activity information could be shared with others without customer’s consent.
If a customer prefers high level of privacy, such information sharing among the
providers can cause security and privacy concerns. Internal threats such as those
mentioned above are difﬁcult to using existing security measures. Two main
approaches can be used to handle internal threats: trust modelling on clouds and
distributing data across independent clouds.
Trust modelling is an idea that is yet to be explored fully on clouds. Although
trust modelling is well studied in online transaction management systems, cloud
computing poses new challenges. The primary challenge is the lack of observability
in the clouds. Customers can pay careful attention to cloud providers’ data
conﬁdentiality and availability records while selecting a cloud provider for their
needs. While data availabilities can be measured by outside parties, quantifying
data conﬁdentiality is a hard problem. One approach is to examine the data
conﬁdentiality policies offered by a provider, evolution of such policies, historical
data on data conﬁdentiality violations of the provider. Another approach is to make
cloud computing auditable. Many cloud computing providers are large companies
with multi-country operations. These cloud providers are not likely to open their
installations for a customer’s examination. However, a regulatory organization with
sufﬁcient mandate can act as an audit and examine cloud provider’s activities on
behalf of the customers.
Cloud computing systems have threat scenarios that do not exist in traditional
computing systems. In clouds, multiple users can share a physical device (com-
puting element) because their virtual machines are mapped onto the machine.
So attackers can by pass the intricate access control mechanisms they can have
co-tenancy as the victims. Once the attackers gain co-tenancy they could monitor
various activities of the victims.
5
Data Intensive Applications on Clouds
5.1
E-Banking
E-banking is a broad sector with many different institutions and users managing
money in the clouds. For example, banks, credit card companies, and personal
banking institutions create web-based portals so that their users can manage trans-
actions through them. Personal banking is becoming a popular sector where people
use applications running on desktops or mobile devices to manage their ﬁnancial
portfolio. Unlike traditional applications that existed for money management for a
long time, cloud based money management applications are active and available all
the time. This means the user is able to access the application from any device all
the time. Further, the application is able to gather information from sources such as
banks and credit card agencies regarding the client’s transactions.

19
Privacy and Security Requirements of Data Intensive Computing in Clouds
509
Most e-banking services fall into the SaaS cloud model. Cloud computing
provides many beneﬁts to e-banking such as always on access, professionally
managed secure computing environments, and seamless connectivity with other
services. Instead of running the money management software in the client’s desktop,
there are signiﬁcant advantages by running it on the clouds. A cloud resident
application can receive notiﬁcations from banks and credit card agencies without
any connectivity issues. In addition to providing a facility to manage money for its
users, several money management applications have started sharing analytics among
its users. These analytics are measures computed over anonymized transactions its
users perform with their banks, credit card agencies, and other business partners.
These analytics measures can be useful for users as well as businesses.
For e-banking, data conﬁdentiality is a key concern. User authentication remains
one of the heavily exploited vulnerabilities to break data conﬁdentiality in e-banking.
Data integrity is not much of a concern here because users do not generate content
in e-banking and the opportunities to update existing information is very limited
for the users. Data accountability and provenance are important concerns as well.
Information access should be carefully logged to detect fraud and address any legal
conﬂicts that may arise. Similarly, data provenance is important because different
services obtain data from other services. For instance, personal banking applications
can obtain credit card transactions and bank accounts on behalf of the user from
respective institutions. Data provenance is important to ensure that the data is
appropriately used as it ﬂows from one service to another one.
5.2
E-Health
E-health clouds are emerging as many countries are streamlining their healthcare
platforms by removing unnecessary information exchange blockages. In the sim-
plest scenario, an e-health cloud will have patients, healthcare providers, and patient
record hosting services. The simple scenario is sufﬁcient only if the patient is trusted
to maintain the health record. Such trust is not feasible in many situations such
as when insurance claims are made for healthcare services. Therefore, a practical
e-health cloud will be more complex. It will have health care providers responsible
for maintaining the health records. There will be billing services and insurance
services in addition to previous three entities.
The key challenge is to maintain patient privacy while allowing efﬁcient
information interchange for the purposes of providing the necessary healthcare to
the patient. In most cases, the e-health cloud will be a SaaS type cloud. With
multiple parties in the e-health cloud, information access needs to be restricted
according to the need-to-know principle. For instance, an insurance service provider
need not know the particular type of ailment the patient is suffering from. Instead,
the insurance service provider needs to know the particulars for the types of
healthcare services rendered to the patient and the cost associated with those
services.

510
A. Nourian and M. Maheswaran
E-health clouds are subjected to various governmental regulations on the way
information is stored, transferred, archived, and released to different parties. If the
cloud service provider fails to meet the regulations, there might be penalties imposed
against the operator.
5.3
Video Surveillance (VSaaS)
Video surveillance services can beneﬁt from the processing and storage capabilities
of cloud computing. In traditional implementations, videos obtained from surveil-
lance cameras are fed to a local storage server. The capacity of the storage server can
limit the number of cameras and consequently the surveillance area. Further, with
limited storage, old video feeds are overwritten by new video feeds. This means the
duration of surveillance available is often limited by the amount of storage available
at the servers. With an elastic storage facility, clouds provide cheap but high volumes
of storage that is suitable for the video surveillance application. Also, clouds provide
a remote and secure facility for archiving the surveillance videos that is not affected
by the insecurities of the site that is being monitored by the surveillance system.
The processing capabilities of the clouds can be handy for performing compu-
tations on the surveillance videos. In the simplest form, video surveillance needs
a video storage and retrieval facility. With the advancements in video analysis
software, it will be possible search through videos for particular scenes. A cloud-
based video surveillance system can facilitate such search functions by providing the
necessary computational capability to run the video processing algorithms included
in the search functions.
Videos taken for surveillance purposes should be accessible only to authorized
users. Because videos are large ﬁles, traditional encryption techniques cannot be
applied to implement conﬁdentiality. New techniques for efﬁciently handling large
datasets created by the video surveillance application is needed to implement data
conﬁdentiality. Data integrity is not a major concern here. The integrity aspect is
covered by the data provenance concerns that deals with the injection of false feeds
or loopbacks. The video surveillance too has the availability concern as the other
applications featured in this section.
5.4
E-Government
Most federal agencies having aging computing systems based on traditional in-
house approach. To cut cost, quickly revamp the infrastructure, and remain unlocked
to a particular vendor, they are beginning to consider cloud computing as a feasible
approach [3]. If cloud computing is adopted by federal agencies, several mission-
critical applications will be loaded into remote cloud servers that are managed by
third party cloud providers. Along with the application code, the cloud servers will

19
Privacy and Security Requirements of Data Intensive Computing in Clouds
511
Fig. 19.1 Comparison of the security and privacy requirements of the different applications
be holding valuable data. Therefore, security and privacy of the data is a top priority.
By adapting the cloud platform, the responsibility for implementing data protection
is passed to the vendor [14], but enforcing accountability is retained with the federal
agencies that own the data.
With Internet access becoming ubiquitous, most international corporations have
consolidated their IT services and are using private or public clouds for their
purposes. Many governments are challenged to use the lessons learned by the
corporations to roll out their own systems in an cost efﬁcient and secure way.
To address the security and privacy concerns of data in the cloud which is turning
out to be the most important barrier for the government agencies to adopt the
cloud services, a project called FeDRAMP [3] is initiated by the US government.
The Federal Risk and Authorization Management Program or FedRAMP has
been established to provide a standard approach to assessing and authorizing
cloud computing services and products. FedRAMP allows joint authorizations and
continuous security monitoring services for government and commercial cloud
computing systems intended for multi-agency use. Joint authorization of cloud
providers results in a common security risk model that can be leveraged across
the federal government. The use of this common security risk model provides a
consistent baseline for cloud-based technologies.
5.5
Comparison of the Different Application Requirements
Figure 19.1 summarizes the data security and privacy requirements of the applica-
tions discussed in this section. From the ﬁgure it can be observed that conﬁdentiality,

512
A. Nourian and M. Maheswaran
availability, and remnance are universally important for all applications. Concerns
such as integrity, accountability, and placement are important for certain applica-
tions and not so much for others.
6
Example Cloud Systems and their Security Policies
The lack of a standardized approach for establishing security and privacy parameters
and enforcing them on the clouds is one of the fundamental concerns. Currently,
security and privacy parameters are evaluated in an ad hoc basis according to the
requirements of the customers. For example, a customer might ask the following
questions when choosing a cloud service provider:
•
How customers’ data are protected from an unauthorized access?
•
What forms of authentication schemes are supported to authorize access?
•
How is data privacy ensured?
•
What tools are offered to notify customers about access to their data?
•
If customers decide to change clouds, what happens to their data and is there a
secure way of exporting the data?
•
Is their any data segmentation process to limit the amount of unintended data
exposure if one of the servers gets compromised?
•
What are the security policies and how often have they changed in the past?
This section analyzes various cloud offerings on the Internet and examines the
mechanisms provided by them to address customers’ security and privacy concerns.
6.1
Amazon Web Services (IaaS)
Amazon [15] is in the forefront of cloud computing offering variety of services in
this domain based on the IaaS cloud service model [16].
1. Amazon EC2 (Amazon Elastic Compute Cloud) which provides virtual environ-
ments with dynamic computing capacities.
2. Amazon S3 (Amazon Simple Storage Service) which provides a scalable data
storage as a web service.
3. Amazon SimpleDB which provides core database functions as a web service.
4. Amazon CloudFront which provides a content delivery as a web service for
content providers.
5. Amazon Elastic MapReduce [17] which provides a hosted Hadoop [18] frame-
work for processing large amounts of data.
Although Amazon’s cloud offerings are well established in the industry, they
do not have a dedicated privacy policy. They use the same privacy policy used by
their e-commerce site in Amazon.com. The AWS S3 does not encrypt a customer’s

19
Privacy and Security Requirements of Data Intensive Computing in Clouds
513
data. Customers are able to encrypt their data by themselves prior to uploading, but
S3 does not provide encryption. Stored information is not guaranteed to be kept
conﬁdential and can be released in the following cases:
•
Afﬁliated businesses not controlled by the cloud provider
•
Third-party service providers
•
Promotional offers
•
Business transfers
•
With customer’s consent
Some of the data sharing scenarios mentioned above apply more to e-commerce
than clouds. Because Amazon does not have a different set of policies for their cloud
offering, same policies apply to the clouds as well. It is interesting to note that the
policy does not cover data placement and purging. As cloud computing continues to
gain popularity, they may need cloud speciﬁc data sharing policies instead of using
policies from B2C or B2B services.
6.2
Google AppEngine (PaaS)
Google AppEngine [19] privacy policy does not address all the security and privacy
concerns delineated above. Google is one of the few cloud operators to provide an
archive of its privacy policies so that customers can investigate the evolution of
Google’s privacy policies. According to the recent version of the privacy policy,
Google may share customer’s information with other companies outside Google if
they have contractual agreements or collaborations with them. Any party receiving
the information is required to abide by Google’s terms of service that requires them
to comply with Google’s privacy policy and any other appropriate conﬁdentiality
and security measures. But there is no information on how they could verify
the compliance of the collaborators. In addition, they also disclose the customer
information under the following circumstances:
•
Satisfy any applicable law, regulation, legal process or enforceable governmental
request.
•
Enforce applicable terms of service, including investigation of potential violations.
•
Detect, prevent, or otherwise address fraud, security or technical issues.
•
Protect against harm to the rights, property or safety of Google, its users or the
public as required or permitted by law.
Also if Google’s cloud operations becomes involved in any part of sale, they ensure
the conﬁdentiality of the transferred data and provide a notice before doing such
a transfer. Google employees, contractors and agents are subjected to discipline,
including termination and criminal prosecution, if they fail to meet information
disclosure obligations. However, there is no way for customers to verify such
breaches took place or not. In most cases, customers need to trust the procedures
Google has put in place to implement their policies.

514
A. Nourian and M. Maheswaran
6.3
Microsoft Azure (PaaS)
Microsoft Azure [20] is a member of the TRUSTe privacy seal program. TRUSTe
is an independent organization whose mission is to build trust and conﬁdence
in the Internet by promoting the use of fair information practices. There is no
indication of cloud based privacy policy for Azure in both small version and
complete version of the privacy policy. It uses the Microsoft general privacy policy
for Azure as it is explicitly mentioned in the Azure website. Customers’ information
may be revealed to third parties who are working with Microsoft. In addition to the
information provided by users, Microsoft webpages contain web beacons which
help them to gather information without customer’s awareness. Such information
may be shared with third parties who are collaborating with Microsoft. Microsoft
informs customers if any changes occur in its privacy policy by updating the
corresponding pages. However, there is no archive of privacy policy for customers
to review.
6.4
Proofpoint (SaaS, IaaS)
Proofpoint [21] provides cloud based solutions for enterprise email services such as
email storage management, archival service, protection against spam and phishing
using its cloud infrastructure as well as other services. While the common use case
is outbound email ﬁltering to restrict information leakage, it can also be used to
detect sensitive information in inbound trafﬁc from business partners as well.
Proofpoint provides a solution called “smart identiﬁer” that automatically scans
for sensitive information in the outbound messages based on customizable rules and
block or encrypt the messages as appropriate. The problem with this approach is that
Proofpoint needs to analyze conﬁdential corporate data to apply the rules. Therefore,
for this approach to work, customers (enterprises hosting their email services in
Proofpoint clouds) need to trust Proofpoint.
Proofpoint has a privacy policy in effect since September 2007. This policy
has not changed substantially since it was put in effect. According to the policy,
Proofpoint reserves the right to use and process customer’s information without any
limitation and disclose and transfer information within their organization or among
their afﬁliates. Also, in the privacy policy it is mentioned that contact information
of the customers may be shared with their partners who can provide the customers
with goods or services that may be of interest to them.
According to their data placement policy, customer’s information may be
transferred or maintained outside the customer’s state, province, or country, where
the privacy laws may not be as protective as those in the customer’s jurisdiction.
Also, for customers residing outside the US, their information can be transferred to
US and processed there.

19
Privacy and Security Requirements of Data Intensive Computing in Clouds
515
Although the privacy policy mentions that customers can delete their contact
information by contacting Proofpoint, there is no information regarding data purging
and data remnance.
6.5
Salesforce (SaaS, PaaS)
Salesforce [22] provides customer relationship management (CRM) services based
on cloud solutions. With more than two million satisﬁed customers, it has service
and sales clouds as its core services offered to businesses including small businesses,
enterprises, and healthcare providers. They are a member of TRUSTe as indicated
in their privacy policy. Salesforce may share customer’s information with other
companies that work on Salesforce’s behalf. Data conﬁdentiality is guaranteed by
Salesforce and they maintain appropriate administrative, physical, and technical
safeguards for enforcing the conﬁdentiality and integrity of their customers’ data.
Salesforce may use customer’s data for marketing purposes. The privacy policy also
mentions that the security measures can be customized by customers to protect
their data from unauthorized access, maintain data accuracy, and help ensure its
appropriate use.
There is no information regarding data purging or data remnance. However, it is
mentioned that Salesforce will return customer’s data upon request with in 30 days
after the effective date of termination.
6.6
Sun Open Clouds
Sun provides an open cloud platform to minimize vendor lock-in (data lock-in) as
long as they use the open standards APIs [23] provided as part of the cloud. They
provide storage and compute clouds. Their storage cloud is set of web services that
provide programmatic access to their storage infrastructure. Developers can build
and run their own cloud based data center from pre-built components by dragging
and dropping servers, switches, and ﬁrewalls. They can deﬁne server’s processor
features such as core memory and clock speed by a pay-as-you-go model. The cloud
created using this platform is compatible with Amazon S3 and EC2 platforms at the
API level, which makes migration easy between these two cloud providers.
The Sun cloud platform does not have a dedicated privacy policy. It uses the
privacy policy used by other Oracle services. Following are some of the salient
features of the Oracle privacy policy with regard to cloud computing.
Oracle is under a TRUSTe privacy program. The policy regarding collection,
usage, and sharing of customer information is very similar to other cloud providers.
The Sun cloud uses various security measures to protect customer data from
unauthorized access. All employees are required to sign an agreement to hold
customers’ data conﬁdential. However, the privacy policy does not cover data
purging, remnance, and placement.

516
A. Nourian and M. Maheswaran
Fig. 19.2 Comparison of the supported features by different cloud providers
6.7
Rackspace
Rackspace [24] is a storage cloud provider that hosts online content. With more
than 40,000 cloud customers, they offer unlimited online storage for ﬁles and media
objects and by using Limelight Network’s content delivery network (CDN), they
deliver the content at high speeds to the clients. Signing up with Rackspace implies
that the customers are in agreement with their privacy policies. If customers refuse
to provide certain proﬁle information while signing up, some features are disabled
based on the missing information. All employees of Rackspace as well as their
agents have access to customers’ data. They use best practice approaches to prevent
the loss, misuse and alteration of the information in their possession. All of their
employees as well as their agents have access to the customers data. Data integrity
is provided but no information regarding data purging, data remnance, and data
exporting.
6.8
Comparison of Operating Policies of Different Clouds
One interesting observation that can be made from Fig. 19.2 is that almost all clouds
support data conﬁdentiality while no clouds support data purging. If a customer
deems data purging important, the customer needs to encrypt the data before
storing it in the clouds. Data placement is another important parameter that is not
supported by most of the cloud providers. This can bring both legal issues as well

19
Privacy and Security Requirements of Data Intensive Computing in Clouds
517
as security risks to consumers’ data. Data placement can also create other burdens
to consumers’ businesses such as different taxes. Data exportability is also among
the missing features in the studied clouds. This feature is related to the issue of data
lock-in which is not favourable to most consumers who want to retain the freedom
of changing the cloud services. Consumers certainly prefer to choose a cloud which
is able to give them more freedom over their data rather joining to a cloud which
locks them up.
7
Trust Management in Clouds
Trust certiﬁcation is becoming an important issue for cloud computing. To facilitate
the trust certiﬁcation process, the Federal Trade Commission has formulated the
Fair Information Principles (FIC). The FIC sets out the following requirements:
•
Need to notify the consumer when personal data is collected and used.
•
Need to obtain the consent from consumer before data collection.
•
Need to provide a facility for the consumer to examine the collected data for
accuracy and misrepresentation.
•
Need to protect the consumers’ data against unauthorized access, destruction,
and disclosure.
•
Need to provide mechanisms to enforce compliance.
TRUSTe is a private certiﬁcation company that certiﬁes online businesses for
their best practices with regard to customer privacy. Recently, TRUSTe has started a
notiﬁcation service that allows alerts the customers on events that can impact their
privacy. TRUSTe also allows the customers to set limits to the personal information
distribution. Any disputes with regard to privacy could be addressed directly with
the service provider or with TRUSTe.
TRUSTe services started with websites that collect user proﬁle information. With
clouds coming in different types (IaaS, PaaS, and SaaS), it is not clear how TRUSTe
would be able to detect privacy violations. In particular, in IaaS clouds, different
entities might be involved in providing a service. While in SaaS, a single cloud
provider can be held responsible for the service even though multiple parties could
have contributed to the deployment of the service.
8
Conclusions
Cloud computing is becoming an increasingly attractive alternative to large in-
house data centers. For cloud computing to maintain its relevance for data intensive
applications, it is important that related standards are developed and adopted
especially in the security domain. This chapter highlighted various data privacy
concerns that emerge in the context of representative data intensive applications.

518
A. Nourian and M. Maheswaran
Further, data privacy measures supported by existing cloud computing systems were
examined. Although signiﬁcant work has been done in securing cloud computing
systems, lot more work is needed to ensure true data privacy. In particular, long-term
security of the massive amounts of data accumulated by data intensive applications
is still weak.
References
1. “National Institute of Standards and Technology,” http://www.nist.gov/.
2. M. Armbrust, A. Fox, R. Grifﬁth, A. D. Joseph, R. Katz, A. Konwinski, G. Lee, D. Patterson,
A. Rabkin, I. Stoica, and M. Zaharia, “Above the Clouds: A Berkeley View of Cloud
Computing,” University of California at Berkeley, Tech. Rep., Feb. 2009.
3. “FedRAMP: Governmentwide Approach to Cloud Security,” http://www.cio.gov/pages-
nonnews.cfm/page/Federal-Risk-and-Authorization-Management-Program-FedRAMP.
4. D. Abramson, R. Buyya, and J. Giddy, “A computational economy for grid computing and
its implementation in the nimrod-g resource broker,” in Future Generation Computer Systems
(FGCS) Journal, vol. 18, no. 8, 2002, pp. 1061–1074.
5. R. T. Kouzes, G. A. Anderson, S. T. Elbert, I. Gorton, and D. K. Gracio, “The changing
paradigm of data-intensive computing,” Computer, vol. 42, pp. 26–34, 2009.
6. P. Mell and T. Grace, “The nist deﬁnition of cloud computing, national institute of standards
and technology,” 2009.
7. C. Wang, Q. Wang, K. Ren, and W. Lou, “Ensuring data storage security in cloud computing,”
Cryptology ePrint Archive, Report 2009/081, 2009.
8. R. L. Grossman, “The case for cloud computing,” It Professional, vol. 11, pp. 23–27, 2009.
9. R. L. Grossman and Y. Gu, “On the varieties of clouds for data intensive computing,” Mar.
2009.
10. T. Wood, G. Tarasuk-levin, P. Shenoy, P. Desnoyers, E. Cecchet, and M. D. Corner, “Memory
buddies: Exploiting page sharing for smart colocation,” in 5th ACM International Conference
on Virtual Execution Environments, 2009.
11. D. S. Linthicum, Cloud Computing and SOA Convergence in Your Enterprise: A Step-by-Step
Guide, 1st ed.
Addison-Wesley Professional, 2009.
12. “Jericho Cloud Cube Model,” http://www.opengroup.org/jericho/cloud cube model v1.0.pdf.
13. C. A. Yfoulis and A. Gounaris, “Tc3 health case study: Amazon web services,” 2009.
14. R. Gellman, “Privacy in the clouds: Risks to privacy and conﬁdentiality from cloud comput-
ing,” Tech. Rep., Feb. 2009.
15. D. Nurmi, R. Wolski, C. Grzegorczyk, G. Obertelli, S. Soman, L. Youseff, and D. Zagorodnov,
“Eucalyptus: A technical report on an elastic utility computing architecture linking your
programs to useful systems,” 2008.
16. “Amazon Web Services.” http://aws.amazon.com.
17. J. Dean and S. Ghemawat, “Mapreduce: simpliﬁed data processing on large clusters,”
Commununications of the ACM, vol. 51, January 2008.
18. D. Borthakur, The Hadoop Distributed File System: Architecture and Design, The Apache
Software Foundation, 2007.
19. “Google AppEngine.” http://code.google.com/appengine.
20. “Microsoft Azure: Windows Azure.” http://www.microsoft.com/windowsazure/.
21. “Proofpoint.” http://www.proofpoint.com.
22. “Salesforce: CRM & Cloud Computing.” http://www.salesforce.com.
23. “Sun Open cloud Platform.” http://www.sun.com.
24. “Rackspace: Rackspace Managed Hosting,” http://www.rackspace.com/.

Part IV
Applications


Chapter 20
On the Processing of Extreme Scale Datasets
in the Geosciences
Sangmi Lee Pallickara, Matthew Malensek, and Shrideep Pallickara
1
Introduction
Observational measurements and model output data acquired or generated by the
various research areas within the realm of Geosciences (also known as Earth
Science) encompass a spatial scale of tens of thousands of kilometers and temporal
scales of seconds to millions of years. Here geosciences refers to the study of
atmosphere, hydrosphere, oceans, and biosphere as well as the earth’s core. Rapid
advances in sensor deployments, computational capacity, and data storage density
have been resulted in dramatic increases in the volume and complexity of data in
geosciences. Geoscientists now see the data-intensive computing approach as part
of their knowledge discovery process alongside traditional theoretical, experimental,
and computational archetype [1]. Data-intensive computing poses unique challenges
to the geoscience community that is exacerbated by the sheer size of the datasets
involved.
Data intensive computing in Geoscience has a unique set of challenges. First
many of geo-phenomena are naturally correlated by their geospatial location and
time. To cope with increasing volumes and data resolutions in the modeling process,
which is spatial and chronological, a wide variety of data analysis technologies
have been (or are being) developed. Cluster analysis has proven very useful for
segmentation, network analysis, change detection, and feature extraction [2]. Block
entropy can be used as a classiﬁer for a dynamical system. Spectral methods
are frequently employed for decomposing periodic phenomena. Artiﬁcial neural
networks and model tree ensembles have been used to reﬁne models and to
empirically up-scale and extrapolate point measurements [3].
S.L. Pallickara () • M. Malensek • S. Pallickara
Department of Computer Science, Colorado State University, Fort Collins, CO, USA
e-mail: sangmi@cs.colostate.edu; malensek@cs.colostate.edu; shrideep@cs.colostate.edu
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 20, © Springer Science+Business Media, LLC 2011
521

522
S.L. Pallickara et al.
Second, providing access to geo-data for scientists in various domains is not
straightforward. Managing high volumes of data from the remote sensors such
as satellites or radars poses interesting data management issues such as curation,
provenance, metadata creation, and public distribution. It is even more difﬁcult for
small datasets from in-situ observational instruments (e.g., measurements for the
ecological sampling) to be collected, preserved, distributed, and accessed for further
processing [4]. The National Ecological Observatory Network (NEON), a 30-year
nationwide study of climate and ecology [5,6], is in their initial development phase
to provide infrastructure for distributing observational data collections that enables
continental scale analysis and forecasting in ecology and related areas. NEON
collects data from electronic sensors mounted on towers (e.g., eddy ﬂux towers)
and aquatic array over the continental US. These datasets are useful for monitoring
physical and chemical climate properties such as air pollution, carbon concentration,
and freshwater sources.
Third, interoperability and data integration is needed in geosciences. There is a
wide range of geospatial data repositories, many of which are readily accessible over
the Internet. A high degree of interoperability among these systems is imperative.
The Open Geospatial Consortium (OGC) has led the effort on standards for
geospatial content and services [7]. A key concept underlying this approach is
providing a standard way to interoperate with diverse geospatial data management
infrastructures and to access heterogeneous forms of geospatial data in a uniform
and transparent fashion. Another challenge in data integration is the integration of
real-time streaming data, which is becoming a predominant source of geospatial
data. OGC’s Sensor Web Enablement (SWE) program [8] initiative seeks to
provide interoperability between disparate sensors and sensor processing systems by
establishing a set of standard protocols to enable a “Sensor Web”. In this scenario,
sensors in the Web are discoverable, accessible, and usable.
In this chapter, we present current trends and technologies in support of data
intensive computing in geosciences. We will focus on the ﬂow of the data, which
enables interactions between the data and the computation/analysis. The remainder
of this chapter is organized as follows. In Sect. 2, we discuss major data processes
involved in geoscience research projects. This section will cover collection and
capture of data from the source, to the visualization and analysis processes.
Section 3 will review the geospatial data models and representations. The metadata
standards effort will be included as well. In Sect. 4, we discuss current technologies
and systems to manage geospatial data and how they interact with computing
resources in various applications.
2
Data Process in Geosciences
Data intensive computing in geosciences involves various processes including
data collection, analysis, visualization, and computation. Researchers require a

20
On the Processing of Extreme Scale Datasets in the Geosciences
523
combination of these data processes to achieve their goal. This section describes
data processes used for knowledge discovery in the geosciences research based on
the characteristics of each process.
2.1
Data Collection from Observational Instruments
Measurement of the natural phenomena is one of the critical tasks in the geo-
sciences. There are two general approaches for performing atmospheric measure-
ments. First, in-situ measurements involve a direct sampling of the atmosphere.
Second, remote sensing instruments allow estimation of interesting parameters by
measuring changes of related atmospheric radiations. Observations do not have to
sample the air directly.
Measurements from in-situ or remote sensing instruments are processed through
multiple steps before it is made available to the users. Data processing levels for
data products generated as part of a research investigation are categorized [9] as:
•
Level 0: Reconstructed and unprocessed instrument payload data at full resolu-
tion. This includes any and all communications artifacts.
•
Level 1A: Time-referenced and annotated ancillary information that includes
radiometric and geometric calibration coefﬁcients besides georeferencing param-
eters.
•
Level 1B: Level 1A data that has been processed while accounting for sensor
units.
•
Level 2: This includes derived geophysical variables at the same resolution and
location as the Level 1 source data.
•
Level 3: Variables that are mapped on to uniform space-time grid scales, usually
with some completeness and consistency.
•
Level 4: Model output or results from analyses of lower level data.
Most users access the datasets in levels 1  4. Since level-4 is the model output,
users of our system will access datasets in levels 1  3.
Unlike remote sensing observations, in-situ measurements require additional
infrastructure to collect and distribute the data from each of the sites. For example,
data collectors such as the WMO’s Global Telecommunication Systems(GTS)
[10] collect observational data from the global participants and distributes them
at the national level. Within the GTS network, Datasets are collected from the
World Meteorological Centers (Melbourne, Moscow, and Washington), 15 Regional
Telecommunication Hubs (including Beijing, Brasilia, Cairo, and Tokyo), and
satellite data centers. These datasets are processed and published by authorized
organizations such as the National Centers for Environmental Prediction (NCEP)
[11]. NCEP also hosts remote sensing data provided by the National Environmental
Satellite and Information Service (NESDIS) and the NEXRAD radar. These datasets
can be in different levels of data processing. For example, wind data from NEXRAD
radar includes level-2 and level-3 data. NCEP packages datasets based on data

524
S.L. Pallickara et al.
similarity (but it still maintains the original structure of reports) and observational
cycles. Finally, integrated and encoded datasets are published periodically. Most of
these datasets are available through the Internet.
2.2
Data Capture
In geosciences the output of high-throughput computations such as global climate
simulations is used for various analysis or visualization steps. These simulations
often take hours with thousands of compute nodes. A key challenge is deciding
how to extract the large amount of data being generated by these computations
off the compute nodes at runtime and over to the data collecting nodes for further
processing such as monitoring, analysis, and archival. This process is referred to
as data capturing and it is important that this has minimal impact on the execution
while ensuring reliability.
Parallel ﬁle systems improve I/O performance for HPC applications. These in-
clude Panasas [12], PVFS [13], Lustre [14], and GPFS [15]. A common goal of these
systems is to provide general purpose, multi-user ﬁle services. HPC applications
can sometimes demand instantaneous and sole access to a large fraction of the
parallel ﬁle system; this can waste CPU cycles on compute nodes that are waiting
for completion of I/O accesses rather than making progress on their scientiﬁc
simulation. This delay in the shared ﬁle system is caused by interference between
processes within a single application, or between the applications demanding a
higher rate of I/O access [16].
MPI-IP introduced the abstract device I/O (ADIO) layer [17] as a way to install
system-speciﬁc optimizations of the general MPI-IO implementation. ADIO is a
user-level parallel I/O interface that provides a portable mechanism to implement a
parallel layer within multiple ﬁle systems. Collective IO provides a level of data-
size driven adaption by aggregating small writes into single, larger writes to obtain
better performance. However, this does not address the issue of large writes from
all of the processes. The Google File System [18] is focused on higher aggregate
throughput, but does not address the interference caused by the shared ﬁle system.
In geoscience community, HDF-5 [19], and NetCDF4 [20] are popular and these
have relied on the underlying IO layer, typically using MPI-IO. PnetCDF [21]
provides subﬁling to address the need to decompose the output to gain greater
parallelism.
There have been adaptive I/O methods for improving IO performance by dynam-
ically shifting work from heavily used areas of the storage system to those that are
more lightly loaded. ADIOS [22] is one of the adaptive I/O APIs which maintains
I/O graphs representing the relationships between nodes for the application, and
dynamically schedules the storage based on the I/O cost calculated from the graph.
This system has been applied to an astrophysics supernova code, chimera.

20
On the Processing of Extreme Scale Datasets in the Geosciences
525
2.3
Visualization
The visualization process is closely related to the data model and representation.
Visualization algorithms take vast amounts of input produced by the simulation,
observation or experiments, and then transform that data into imagery. Modern par-
allel visualization tools use a data ﬂow network processing design [23]. A dataﬂow
network contains components such as ﬁlters, sources, or sinks. A data ﬂow network
is composed by relating data objects and components. For example, ﬁlters have
set of inputs and a set of outputs, both of which are data objects. Sinks have only
data inputs and sources have only data outputs. A data ﬂow network is a pipeline
of data with an ordered collection of components. When a pipeline is executed,
data comes from the source and ﬂows through ﬁlters until it reaches the sink. The
majority of data processing in visualization operations are embarrassingly parallel –
these process can occur in parallel with no communication between the parallel
processes.
VisIt is an example of a visualization application implementing a data ﬂow
network with parallel data processing [24]. VisIt is designed for visualization and
graphical analysis of terascale scientiﬁc simulations such as climate modeling. In
VisIt, each component of the data ﬂow network can specify optimizations through
contracts and communicate with each other to improve performance. For example,
many ﬁlters specify limits on the amount of data being processed in their contracts.
VisIt uses two approaches for rendering. First, surfaces with a relatively small
number of geometric primitives are sent to users and rendered locally. Surfaces
with a relatively large amount of geometric primitives remain on the server and
are rendered in parallel. In terms of the data model, VisIt processes all of the mesh
types and ﬁeld types while tracking and preserving information about data layout
and ordering.
2.4
Data Analysis
Data analysis in geosciences involves complex processing and access to a large
amount of data. Analysis of atmospheric phenomena involves two important
aspects: the physical laws that govern the atmospheric circulation and the spatial
and temporal spectra of the atmospheric phenomena [25]. Physical laws indicate
how it might be possible to determine one variable from another; for example,
analyzing wind from temperature measurements. The governing equations of the
atmosphere can be written in terms of independent variables (three dimensional
variables for spatial location and time) and dependent variables (temperature,
humidity, or chemical species). In general, these equations are nonlinear partial
differential equations of the variables described above.
Scientists in geosciences have encountered various challenges that increase
complexity [26]. First, there is the issue of scale selection. Choosing the best

526
S.L. Pallickara et al.
scale for an analysis is an important decision involving experience and also
trial-and-error. To achieve this scientists adopt a multi-step approach, in which
intermediate results are used to evaluate the next decision in the analysis. The
second source of complexity is the uncertainty in the measured data. Data are
restricted by trade-offs and practical limitations. A primary objective in data analysis
is to ﬁgure out random ﬂuctuations from deterministic components. If the data
distribution (e.g. Gaussian, exponential, logarithmic, etc.) is known this would is
not a problem; however, ﬁnding the appropriate data distribution function is a
challenging task. Finally, spatial and temporal interactions add to the complexity
of the modeling system. For instance, in ecology research, most of the ecosystems
are dependent on environmental conditions (e.g. elevation or humidity). Biological
variables are altered based on the changing conditions and become space-dependent.
If there is no general dependency in space, a local phenomenon may exist. Space-
autocorrelation, which is the phenomena where geospatially neighboring samples
are more similar than one would expect from a known ecological condition, is
often observed. Similarly, there is the issue of temporal dependence and temporal
autocorrelation.
3
Data Models and Representations
As we discussed in Sect. 2, spatial coordinate information and temporal information
of the data are keys to organizing geospatial datasets for subsequent processing.
Based on the characteristics of the applications and datasets, there have been various
data models and types of data representations. In this section, we will review
fundamental concepts and techniques for managing geospatial time series datasets
in the geosciences.
3.1
Object-Based Model and Field-Based Model
There have been active efforts in the modeling and representation of geospatial
data in GIS (Geographic Information System) based systems. The two most popular
approaches are an object-based model and a ﬁeld-based model [27,28]. In an object-
based model, geographic objects corresponding to real-world entities are deﬁned
by a spatial component and a descriptive component. The spatial component is
speciﬁed by the shape and location of the object in the embedding space. For the
object with a given spatial component, a descriptive component provides non-spatial
properties in the form of attributes.
In ﬁeld-based approaches, the space is partitioned into two or multidimensional
cells. Each cell has one or more attribute values associated with it and each attribute
describes a continuous function in space. An example of a ﬁeld-based data model is
a multispectral or hyperspectral raster imagery obtained from a radar or satellite. In

20
On the Processing of Extreme Scale Datasets in the Geosciences
527
Table 20.1 Geographic data models
Data model
Example application
Computer-aided design (CAD)
Automated engineering design and drafting
Graphical (non-topological)
Simple mapping
Image
Image processing and simple grid analysis
Raster/grid
Spatial analysis and modeling, especially in
environmental and natural resource applications
Vector/Geo-relational topology
Many operations on vector geometric features in
cartography, socio-economic and resource
analysis, and modeling
Network
Network analysis in transportation, hydrology, and
utilities
Triangulated irregular network (TIN)
Surface/terrain visualization
Object
Many operations on all types of entities
(raster/vector/TIN etc.) in all types of application
a ﬁeld-based approach, there is no notion of objects but observations of phenomena
described by attribute values (e.g. measurements).
Longley et al. [29] provide a summary of spatial data models used in GIS and
example applications as depicted in Table 20.1. Conversion between models and
combining models for more efﬁcient expression of the dataset is common in GIS
applications.
3.2
N-dimensional Array Model
Similar to other scientiﬁc data, geospatial data is naturally represented by a
multi-dimensional, array-based data model [19, 20, 30]. Since most commercial
databases do not support the array data type, or allow ﬂexible access to large array
datasets, there have been many approaches in the geoscience area to cope with this
challenge.
First, there are data formats widely used in geosciences: netCDF [30] and HDFS
[19]. These formats follow the Common Data Model (CMD) [31] that represent
data as a set of multi-dimensional arrays, with sharable dimensions, and additional
metadata attached to individual arrays or the entire ﬁle. There are various software
available for scientists to access data ﬁles that conform to these data formats,
including data analysis tools and visualization applications. We will discuss this
is Sect. 3.5.
The second approach to dealing with N -dimensional array model for geosciences
involves building a scientiﬁc storage system supporting multi-dimensional arrays or
APIs for accessing multi-dimensional data array stored in existing storage systems.
SciDB [32] which is a database management system for scientiﬁc research supports
a multi-dimensional, nested array model with array cells containing records.

528
S.L. Pallickara et al.
SciHadoop provides multi-dimensional array access through their query interface
while MapReduce tasks are being executed [33].
3.3
Data Formats: NetCDF, HDF5, and FITS
The Network Common Data Format (NetCDF) [30] is a self-describing data
storage format for multi-dimensional data. NetCDF provides a generic and machine-
independent interface for storing and accessing scientiﬁc data. NetCDF represents
data as ﬁles containing three properties: dimensions, variables, and attributes.
Dimensions are named values used to describe the shape of the variables contained
in the ﬁle, which could be application-speciﬁc attributes such as time, elevation,
or position. Dimensions do not necessarily have to be bounded. A variable is
represented as a multi-dimensional array that is a collection of homogeneous
values. Finally, attributes describe a variable’s metadata or other application-speciﬁc
information to aid in processing and analysis. NetCDF data format is widely used
in scientiﬁc data analysis tools (e.g., MATLAB, R) and GIS applications (e.g.,
ArcGIS), and large-scale simulations.
Hierarchical Data Format 5 (HDF5) [34] was originally developed by the
National Center for Supercomputing Applications (NCSA) as a general-purpose,
machine-independent scientiﬁc data format. HDF5 is the latest version of the
format, representing a major architectural redesign over previous versions. An
HDF5 ﬁle consists of two different components: groups and datasets. Groups allow
users and applications to create a hierarchy for data in a tree-based form similar
to the POSIX logical ﬁle system layout. Datasets are also divided into two parts: a
header and a data array. Headers describe the data type, dimensionality, and storage
method for the array. They also contain metadata and other information on how the
data should be interpreted, which allows HDF ﬁles to be self-describing. Data can be
stored in a contiguous manner or can be broken into separate pieces of data, called
chunks, to improve performance.
Developed in the 1970s for astronomical data, Flexible Image Transport System
(FITS) [35, 36] has evolved to support generic scientiﬁc datasets. Since FITS is
designed for two- or three-dimensional images, it is naturally well-suited for other
forms of multi-dimensional scientiﬁc data. A FITS ﬁle is composed of a number
of Header C Data Units (HDUs). The ﬁrst HDU is an n-dimensional array of
pixel data, followed by an arbitrary number of FITS extensions. Extensions include
additional n-dimensional images, ASCII tables, or binary tables. Headers contain
80-character key-value entries that describe a FITS ﬁle’s metadata.
FITS has wide support for programming language bindings, including C, Fortran,
Java, Python, and R. Many image-processing applications also support reading
image data from FITS ﬁles. Since a large use case of FITS is archival storage,
additions to the format must not make ﬁles produced from a previous version of
the format unreadable by newer software implementations.

20
On the Processing of Extreme Scale Datasets in the Geosciences
529
While the simplicity of the FITS format is a major strength, it may also be a
limiting factor for some applications. An 80-character maximum for header records
could hinder self-description for some datasets, resulting in additional ASCII or
binary data tables needing to accompany data.
4
Data Access in Geosciences
In geosciences data processing schemes are evolving to cope with the large
volumes of data generated by observational measurements or from large-scale
simulations. Here we discuss approaches taken in the geoscience domain to address
this challenge. Our discussion will address distributed data access infrastructure,
GIS, sensor data networks, Grid infrastructure, database management systems, and
computational platforms.
4.1
OPeNDAP: Domain Speciﬁc Distributed Data Access
The Open Source Project for Network Data Access Protocol (OPeNDAP) [37]
provides access to the oceanographic data stored in remote sources. Datasets are
stored in several formats at the servers. Users can access and download the data
directly from their analysis programs. OPeNDAP provides both server software to
make data available to remote users, and client software to access this data. Some
of the client software provides data access API libraries for translating between
different geospatial models such as NetCDF [30], HDF5 [19], JGOFS [38], and
others. Although it was originally designed for oceanographic data, it has been used
for other geospatial research areas such as atmospheric science and ecology.
The OPeNDAP system provides two types of metadata in the returning dataset.
The ﬁrst type is the syntactic metadata generated from the accompanying data. The
second is the semantic metadata that is generated based on the client’s understanding
of the dataset. OPeNDAP uses these metadata for its translational process and
attribute-based search process. The syntactic metadata is useful during translation
of the data format and provide a consistent semantic description about the dataset.
OPeNDAP also provides an attribute-based search interface to describe the dataset.
This includes parameter, range, location, and descriptive search over the metadata.
4.2
PostGIS: GIS Approach
As an extension to the PostgreSQL relational database, PostGIS [39] adds support
for geospatial datatypes and queries. This permits users and applications to work
with geospatial data using standard SQL syntax. PostGIS supports the OpenGIS

530
S.L. Pallickara et al.
Simple Features Speciﬁcation, which deﬁnes a set of geometric datatypes and
methods for geospatial analysis. This speciﬁcation provides a level of standardiza-
tion in the geospatial database ﬁeld and is supported by industry players including
Oracle and ESRI.
To use PostGIS, a new database must be created with the extension enabled.
Once created, data can be added to the database using SQL statements. PostGIS
also includes a tool that converts ESRI shape ﬁles to and from SQL statements.
Standard PostgreSQL datatypes are also available in PostGIS-enabled databases.
The combination of geospatial datatypes and built-in geospatial methods allow some
processing to be ofﬂoaded from the clients to the database server itself.
For large datasets, PostgreSQL supports indexing database tables. In the case of
multi-dimensional geospatial datatypes, PostGIS uses R-trees based on Generalized
Search Tree (GiST) project. R-trees are similar to the common B-tree used in
databases and ﬁlesystems, and split a geospatial range into a hierarchical set of
bounding boxes [40]. This allows for quick nearest-neighbor and window lookups.
One problem with this approach is that the PostgreSQL query planner does not
always optimize GiST indexes well, resulting in a query that scans the entire
table [39].
PostGIS provides a standardized, SQL-based interface that allows for compat-
ibility with other database systems or data formats. Despite this compatibility,
if an application’s storage requirements do not ﬁt the OpenGIS Simple Feature
Speciﬁcation, then its data cannot be easily migrated to other systems. Depending on
workload, scaling PostgreSQL and PostGIS may also be problematic. To distribute
processing across a number of machines, data must be transferred from the database
server (or servers) to clients, incurring IO latency costs. To alleviate latencies
and distribute data more effectively it is possible to replicate the database or
manually split the data across multiple databases, but this complicates queries and
the application logic that interacts with the data. However, in cases where geospatial
clustering is being performed over objects that span a large geographical area or
the entire dataset, the centralized database approach could be more efﬁcient than a
distributed (and therefore communication-heavy) approach.
4.3
DataTurbine: Access to the Real-Time Sensor Data
Geoscience communities are now actively engaged in large scale sensor-based
observational systems. DataTurbine is a real-time data streaming engine [41].
It is an open-source middleware product providing programming abstractions over
heterogeneous devices, and integrated network services for managing streaming
data. DataTurbine provides a common Application Programming Interface (API)
for disparate devices. For example, data streams from accelerometers and video
cameras are integrated and managed through a common API, so that users can
integrate heterogeneous data streams.

20
On the Processing of Extreme Scale Datasets in the Geosciences
531
Reliable delivery of sensor data is critical requirement for the large scale sensor
network infrastructure because network delays and partitions are common in sensor
networks. DataTurbine maintains q ring queue spanning both memory and disk
to store the data. It provides a level of reliability that is between plain TCP and
application dependent transactional systems. This is also designed to accommodate
other functionalities such as real-time data archival and distribution over local
and wide area networks. Client applications can use the ring buffers for their
on-demand features including data stream subscription, data capture, rewind, and
replay.
Routing and topology management in DataTurbine is conﬁgurable based on the
characteristics of the observational system. For example, a tree-style topology could
be applied in situations involving ﬁrewalls. Users can conﬁgure the sensor nodes
behind the ﬁrewall as child nodes and deﬁne their parent node as nodes located
outside the ﬁrewall to enable communication between the nodes.
For the geospatial sensor data, DataTurbine provides an online data mapping
interface: converting incoming data from sensors to overlays of data. Streamed data
is transformed into the KML format and data displays can be layered on the Google
Earth interface. This provides a visual interface to the domain experts for easy-to-
use navigation over the complex data streams.
4.4
The Earth System Grid: Data Access in Grid Settings
A cyberinfrastructure project, the Earth System Grid (ESG) has developed an
environment for managing climate data from multiple climate model sources, obser-
vation data, and analysis/visualization tools used by the climate research community
[42]. Participating institutes publish datasets in ESG. The ESG publication API
manages the publishing process. Based on the THREDDS catalog [43], the ESG
publication API provides easy-to-use command line tools to package and transfer
the newly published datasets. ESG extends the THREDDS metadata speciﬁcation
to organize metadata.
The Berkeley Storage Manager (BeStMan) is an implementation of the Storage
Resource Manager (SRM) system used in ESG [44]. BeStMan provides interfaces
to various storage systems, including the Mass Storage System (MSSs). High
Performance Storage System(HPSS) in LBNL, ORNL, and MSS at NCAR are
accessible through a uniﬁed ESG gateway portal. BeStMan enables users to access
storage systems with different security mechanisms.
Datasets stored at multiple institutes across North America can be moved to the
user’s site for simulation, analysis or visualization. DataMover-Lite (DML) [42]
from the Berkeley Lab is a simple ﬁle transfer tool with a graphical user interface
that supports various data transfer protocols including http, https, gridftp, ftp, and
scp. In ESG data sites are equipped with DML to move ﬁles over GridFTP/HTTPS
with built-in grid security mechanisms. DML splits ﬁles for multiple HTTP
connections for faster downloads. For large datasets, the Bulk Data Mover (BDM)

532
S.L. Pallickara et al.
[42] provides a scalable data movement solution. BDM manages the transfer of
millions of data ﬁles those have a total size of hundreds of TB reliably.
GridFTP [45] is used as the underlying technology for data download by users,
data movement between resources, and data synchronization between replications.
Globus Online [46] is a data transfer management software-as-a-service built on top
of GridFTP. This software provided by ANL and University of Chicago enables the
ESG community to transfer their data. Features include performance monitoring,
retrying failed transfers, and automatic fault recovery. Globus Online optimizes the
transfer to ensure best performance based on the transfer size and number of ﬁles
per transfer.
4.5
SciDB: Database Management System
for Multi-dimensional Data
Instead of building on existing relational databases, SciDB [47] is a different type
of database designed speciﬁcally for large-scale scientiﬁc applications. Storing and
processing data from sensor arrays is the primary use case SciDB was designed
around, and has been harnessed for applications in seismology, astronomy, and
climate research. The creators of SciDB identiﬁed three main differences between
business data and scientiﬁc data: ﬁrst, in scientiﬁc data, the sources of data generally
have a location associated with them, which could include adjacency to other
sensors or coordinates in space. Second, the data being collected in scientiﬁc
applications also usually requires complex processing before and/or after storage.
Finally, sensors and other scientiﬁc instruments generate data on the petabyte-scale,
so the database must be able to cope with these massive storage needs.
The logical data storage model in SciDB is different from traditional tabular
database systems. SciDB databases consist of n-dimensional arrays of cells. The
datatypes held by cells are deﬁned at an array’s creation, and they can be either
scalars or sub-arrays. These combinations of datatypes and values are called
attributes. The arrays in SciDB do not necessarily have to be contiguous; cells can
be empty, creating a sparse or “jagged” array. Storing data in this manner helps
arrays match a variety of scientiﬁc data, and also allows data to be grouped in a
fashion that is most optimal for processing later. The SciDB storage manager also
supports “in situ” data; a large amount of time is spent in the scientiﬁc community
simply loading and moving data, so SciDB allows manipulating external data that is
not part of the database to eliminate load times. While the external data must be in
the SciDB format, it is also possible to write adaptors that can allow importing data
from formats such as NetCDF or HDF5.
SciDB is a shared nothing system, meaning each node in the system runs the
SciDB engine and operates independently of other nodes. Collections of data are
decomposed into “chunks” and compressed before being stored on a node’s local
disk. Nodes can be removed or added to the system without affecting other nodes,

20
On the Processing of Extreme Scale Datasets in the Geosciences
533
and data processing operations will continue to run as long as they do not reference
data that is unavailable. Upon entry into the system, nodes contact a central catalog
to inform the system of their presence, which represents a single point of failure.
The catalog is implemented as a PostgreSQL database [48] which also contains
information about data chunks stored in the system [47].
To facilitate the division of data, SciDB uses “chunking” and “vertical parti-
tioning.” Since cells can contain multiple attributes, vertical partitioning splits the
attributes up into their own separate arrays. This division is beneﬁcial because
computations often only involve a subset of the overall attributes an array might
have. Once the attributes are split, they can be broken up into chunks. Depending
on the application, chunks can “overlap,” meaning a chunk may contain parts
of adjacent chunks. This behavior is conﬁgurable by the database administrator
and can greatly reduce communication costs if chunks overlap in a way that
ﬁts the processing that will be applied to them, at the cost of using more disk
space.
While most relational database tables are updateable by default, SciDB’s arrays
are not. For many scientiﬁc applications, updates are undesirable because keeping
record of original results before processing is necessary. To support subsequent
processing after the initial import, SciDB records changes to the dataset and exposes
this to users in the form of an additional “history” dimension [48]. The history also
allows users to change calculations applied to data in a past history snapshot and
then have the rest of the values in subsequent processing recalculated as well.
For processing data, SciDB is modular in nature. It includes a base set of
functions for working with data, but also allows user-deﬁned array operators to be
written in CCC, following a model similar to PostgreSQL’s user-deﬁned functions.
These array operators can be used to create “enhanced” arrays, where a function is
applied to an array and then both the original and enhanced cells can be accessed.
Enhanced arrays allow users to access the same data but from different perspectives.
Writing array operators facilitates pushing computations to data instead of having
to query and transfer information from the database, mitigating the costs of disk
and network latencies. In addition, the database’s distributed nature also provides
parallelism when running array operators across a number of nodes. Extensible
language bindings are also provided to allow users to query data with their project’s
native programming language, if necessary.
4.6
Hadoop MapReduce: Computing Platform
Utilizing the MapReduce paradigm [49] and a form of structured storage is another
solution for processing and storing large amounts of geospatial data. As proposed
in [50] and [51], Apache Hadoop [52] and HDFS (Hadoop Distributed File System)
[53] can be used to execute geospatial queries in parallel.
Many geospatial queries involve computing data surrounding individual points
and then processing their combined results, which ﬁts the MapReduce paradigm

534
S.L. Pallickara et al.
well. For example, queries often require determining the nearest neighboring points
of some arbitrary target point in a geographical area. During the map phase of
MapReduce, each data point in the area is distributed to a mapper, which then
determines the nearest neighbor of its point. In the reduce phase, any results
matching the target point are collected to form a ﬁnal set of nearest neighbors.
Since standard R-trees represent a global index, they are not suitable for a
distributed environment. Wang and Wang [51] propose building indexes for each
data ﬁle that enters the system to alleviate this problem. Alternatively, instead
of the standard tree-based indexing approaches, Akdogan et al. [50] utilize a ﬂat
spatial index using Voronoi diagrams. Voronoi diagrams partition a region into a
collection of polygons. By removing the tree hierarchy, data coupling is reduced
while also allowing the data to be spread evenly across available nodes for better
load balancing. This storage model also means that computations can be pushed to
the data they operate on rather than requiring programs to request data and have it
transferred to them.
While using cloud computing frameworks such as Hadoop can provide beneﬁts
for large, data-intensive geospatial applications, there are some downsides to this
approach. First, using HDFS results in data ﬁles being broken up into pieces called
‘splits.’ If a query requires data from a neighboring split, an additional MapReduce
phase may be required to reconcile results from the two splits. In addition, using a
cloud framework for geospatial applications places more burden on the end user
to decide how to query and store data, whereas extensions built atop relational
databases generally support a common set of geospatial datatypes and procedures
that can be exploited with SQL.
4.7
Kepler: Scientiﬁc Workﬂow
While Hadoop and the MapReduce paradigm can provide immense processing
beneﬁts for scientiﬁc users, there is also a considerable learning curve involved
with using the Hadoop framework. The Kepler Project [54], a scientiﬁc processing
workﬂow tool, allows users to create workﬂows using a graphical user interface.
Kepler is a type of “actor-oriented modeling,” where actors are components that are
designed to perform various processing tasks. In the case of Kepler, MapReduce is
implemented as an actor that can be added to workﬂows.
In a workﬂow, actors have “ports,” which either produce or consume data. Actors
generally take data items in, process them, and then pass the results on to the next
actor in the workﬂow. Data may take different paths through the workﬂow and can
execute both serially and in parallel.
Kepler provides a good solution for users wanting to beneﬁt from MapReduce
without having to use it for every step in their processing. By default, data ﬁles are
not stored in HDFS and instead are copied into HDFS from the ﬁlesystem before the
MapReduce actor runs, so large changes to an existing workﬂow are not necessary.
It is also possible to conﬁgure Kepler to use data that is already stored in HDFS,

20
On the Processing of Extreme Scale Datasets in the Geosciences
535
but then other actors would need to support HDFS as well if they need access to the
data. The implications of copying ﬁles into HDFS before processing are not entirely
clear, but could be a large bottleneck when working with massive datasets.
Kepler is particularly well-suited for applications that don’t always ﬁt the
MapReduce model due to MapReduce being simply one of many possible actors
in a workﬂow. It also makes MapReduce much more accessible for scientiﬁc
applications.
5
Conclusions
This chapter provided a survey of challenges involved in analyzing geospatial
datasets. The proliferation of networked sensors, measurement devices, instruments,
and simulations have resulted in large data volumes. Processing such data requires
addressing several challenges that include ensuring efﬁcient representations, mod-
els, formats, transfers, and computational frameworks.
References
1. T. Hey, et al., The Fourth Paradigm: Data-Intensive Scientiﬁc Discovery. Redmond, Washing-
ton: Microsoft Corporation, 2009.
2. F. M. Hoffman, et al., “Multivariate Spatio-Temporal Clustering (MSTC) as a data mining tool
for environmental applications,” in the iEMSs Fourth Biennial Meeting: International Congress
on Environmental Modelling and Software Society (iEMSs 2008), 2008, pp. 1774–1781.
3. F. M. Hoffman, et al., “Data Mining in Earth System,” in the International Conference on
Computational Science (ICCS), 2011, pp. 1450–1455.
4. O. J. Reichman, et al. (2011) Challenges and opportunities of open data in ecology. Science.
703–705.
5. M. Keller, et al., “A continental strategy for the National Ecological Observatory Network,”
Front. Ecol. Environ Special Issue on Continental-Scale Ecology, vol. 5, pp. 282–284, 2008.
6. D. Schimel, et al., “NEON: A hierarchically designed national ecological network,” Front.
Ecol. Environ, vol. 2, 2007.
7. June, 17, 2011). The Open Geospatial Consortium (OGC) Available: http://www.opengeo
spatial.org
8. G. Percivall and C. Reed, “OGC Sensor Web Enabliment Standards,” Sensors and Transducers
Journal, vol. 71, pp. 698–706, 2006.
9. MTPE EOS Reference Handbook the EOS Project Science Ofﬁce, code 900, NASA Goddard
Space Flight Center, 1995.
10. The Global Telecommunication System. Available: http://www.wmo.int/pages/prog/www/
TEM/GTS/index en.html
11. National Center for Environmental Prediction (NCEP). Available: http://www.ncep.noaa.gov/
12. Panasas: Parallel File System for HPC Storage. Available: http://www.panasas.com/
13. M. M. Kuhn, et al., “Dynamic ﬁle system semantics to enable metadata optimizations in
PVFS,” Concurrency and Computation: Practice and Experience, vol. 21, 2009.
14. P. J. Braam, “Lustre: a scalable high-performance ﬁle system,” 2002.

536
S.L. Pallickara et al.
15. F. B. Schmuck and R. L. Haskin, “GPFS: A Shared-Disk File System for Large Computing
Clusters,” in the Conference on File and Storage Technologies, 2002, pp. 231–244.
16. J. Lofstead, et al., “Managing Variability in the IO Performance of Petascale Storage Systems,”
presented at the ACM/IEEE International Conference for High Performance Computing,
Networking, Storage and Analysis, 2010.
17. M. P. I. Forum, “MPI-2: Extensions to the Message-Passing Interface,” 1997.
18. S. Ghemawat, et al., “The Google File System,” ACM SIGOPS Operating Systems Review,
vol. 37, 2003.
19. HDF-5. Available: http://hdf.ncsa.uiuc.edu/products/hdf5/
20. NetCDF4. Available: http://www.hdfgroup.org/projects/netcdf-4/.
21. J. Li, et al., “Parallel netCDF: A high-performance scientiﬁc I/O interface,” in ACM Supercom-
puting (SC03), 2003.
22. H. Abbasi, et al., “DataStager: scalable data staging services for petascale applications,” in
ACM international Symposium on High Performance Distributed Computing, 2009.
23. J. Craig Upson, et al., “The Application Visualization System: A computational environment
for scientiﬁc visualization,” IEEE Computer Graphics and Applications, pp. 30–42, 1989.
24. VisIt Visualization Tool. Available: https://wci.llnl.gov/codes/visit/home.html
25. R. Daley, Atmospheric Data Analysis: Cambridge atmospheric and space science series, 1993.
26. O. Wildi, Data Analysis in Vegetation Ecology Willey, 2010.
27. P. Rigaux, et al., Spatial Databases with Application to GIS: Morgan Kaufmann, 2002.
28. S. Shekhar and S. Chawla, Spatial Database: A Tour: Prentice Hall, 2002.
29. P. Longley, et al., Geographic Information Systems and Science, 3 ed.: John Wiley &
Sons, 2011.
30. R. Rew and G. Davis, “NetCDF: an interface for scientiﬁc data access,” IEEE Computer
Graphics and Applications, vol. 10, pp. 76–82, 1990.
31. Common Data Model. Available: http://www.unidata.ucar.edu/software/netcdf-java/CDM/
32. P. Cudre-Mauroux, et al., “A Demonstration of SciDB: A Science-Oriented DBMS,” in the
2009 VLDB Endowment 2009.
33. J. Buck, et al., “SciHadoop: Array-based Query Processing in Hadoop,” UCSC2011.
34. (2010, The HDF Group. Hierarchical data format version 5. http://www.hdfgroup.org/HDF5.
35. (2011, FITS Support Ofﬁce. http://ﬁts.gsfc.nasa.gov/.
36. D. C. Wells, et al., “FITS: A Flexible Image Transport System,” Astronomy & Astrophysics,
vol. 44, pp. 363–370, 1981.
37. P. Cornillon, et al., “OPeNDAP: Accessing data in a distributed, heterogeneous environment,”
Data Science Journal, vol. 2, pp. 164–174, 2003.
38. D. M. Karl, et al., “Building the long-term picture: U.S. JGOFS Time-series Programs,”
Oceanography, pp. 6–17, 2001.
39. P. Ramsey, “PostGIS Manual,” ed: Refractions Research.
40. A. Guttman, “R-trees: a dynamic index structure for spatial searching,” in Proceedings of
the 1984 ACM SIGMOD international conference on Management of data, ed. Boston,
Massachusetts: ACM, 1984, pp. 47–57.
41. S. Tilak, et al., “The Ring Buffer Network Bus (RBNB) DataTurbine Streaming Data
Middleware for Environmental Observing Systems,” in IEEE e-Science, 2007, pp. 125–133.
42. D. N. Williams, et al., “The Earth System Grid: Enabling Access to Multi-Model Climate
Simulation Data,” Bulletin of the American Meteorological Society, vol. 90, pp. 195–205, 2009.
43. B. Domenico, et al., “Thematic Real-time Environmental Distributed Data Services
(THREDDS): Incorporating Interactive Analysis Tools into NSDL,” Journal of Interactivity
in Digital Libraries, vol. 2, 2002.
44. A. Shoshani, et al., “Storage Resource Managers (SRM) in the Earth System Grid,” Earth
System Grid2009.
45. G. Khanna, et al., “A Dynamic Scheduling Approach for Coordinated Wide-Area Data
Transfers using GridFTP,” in the 22nd IEEE International Parallel and Distributed Processing
Symposium (IPDPS 2008), 2008.

20
On the Processing of Extreme Scale Datasets in the Geosciences
537
46. Globus Online j Reliable File Transfer. No IT Required. Available: https://www.globuson
line.org/
47. P. G. Brown, “Overview of sciDB: large scale array storage, processing and analysis,” in
Proceedings of the 2010 international conference on Management of data, ed. Indianapolis,
Indiana, USA: ACM, 2010, pp. 963–968.
48. M. S. Mit, et al. (2009, Requirements for Science Data Bases and SciDB.
49. J. Dean and S. Ghemawat, “Mapreduce: Simpliﬁed data processing on large clusters,”
Communications of the ACM, vol. 51, pp. 107–113, 2008.
50. A. Akdogan, et al., “Voronoi-Based Geospatial Query Processing with MapReduce,” in Cloud
Computing Technology and Science (CloudCom), 2010 IEEE Second International Conference
on, ed, 2010, pp. 9–16.
51. Y. Wang and S. Wang, “Research and implementation on spatial data storage and operation
based on Hadoop platform,” in Geoscience and Remote Sensing (IITA-GRS), 2010 Second
IITA International Conference on vol. 2, ed, 2010, pp. 275–278.
52. Apache Hadoop. Available: http://hadoop.apache.org/
53. Hadoop Distributed File System. Available: http://hadoop.apache.org/hdfs/
54. J. Wang, et al., “Kepler C Hadoop: a general architecture facilitating data-intensive applica-
tions in scientiﬁc workﬂow systems,” in Proceedings of the 4th Workshop on Workﬂows in
Support of Large-Scale Science, ed. Portland, Oregon: ACM, 2009, pp. 12:1–12:8.


Chapter 21
Parallel Earthquake Simulations on Large-Scale
Multicore Supercomputers
Xingfu Wu, Benchun Duan, and Valerie Taylor
1
Overview of Earthquake Simulations
Earthquakes are one of the most destructive natural hazards on our planet Earth.
Hugh earthquakes striking offshore may cause devastating tsunamis, as evidenced
by the 11 March 2011 Japan (moment magnitude Mw9:0/ and the 26 December
2004 Sumatra .Mw9:1/ earthquakes. Earthquake prediction (in terms of the precise
time, place, and magnitude of a coming earthquake) is arguably unfeasible in
the foreseeable future. To mitigate seismic hazards from future earthquakes in
earthquake-prone areas, such as California and Japan, scientists have been using
numerical simulations to study earthquake rupture propagation along faults and
seismic wave propagation in the surrounding media on ever-advancing modern
computers over past several decades. In particular, ground motion simulations for
past and future (possible) signiﬁcant earthquakes have been performed to under-
stand factors that affect ground shaking in populated areas, and to provide ground
shaking characteristics and synthetic seismograms for emergency preparation and
design of earthquake-resistant structures. These simulation results can guide the
development of more rational seismic provisions for leading to safer, more efﬁcient,
and economical structures in earthquake-prone regions.
X. Wu ()
Department of Computer Science & Engineering, Institute for Applied Mathematics
and Computational Science, Texas A&M University, College Station, TX, USA
e-mail: wuxf@cse.tamu.edu
B. Duan
Department of Geology & Geophysics, Texas A&M University, College Station, TX, USA
e-mail: bduan@tamu.edu
V. Taylor
Department of Computer Science & Engineering, Texas A&M University,
College Station, TX, USA
e-mail: taylor@cse.tamu.edu
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 21, © Springer Science+Business Media, LLC 2011
539

540
X. Wu et al.
1.1
Large-Scale Ground Motion Simulations
Most earthquakes occur on tectonically active faults. A fault is a fracture in the
Earth’s crust or lithosphere on which one block of rock can slide past another.
Although each of the two blocks moves with respect to the other due to plate
tectonic processes, some areas of the fault may be locked by friction. Over decades,
centuries, or even millennia, the shear (tangential) stress on these locked areas builds
up, and elastic energy accumulates and is stored in deformed rocks. When the shear
stress exceeds the frictional strength somewhere along the fault surface, the fault
breaks suddenly and slip (relative displacement of the two sides of the fault) occurs
there. Under favorable conditions, the rupture propagates along the fault within
seconds or minutes. During the rupture propagation on the fault, the shear stress on
the fault drops to a lower level, and the stored elastic energy is suddenly released.
Some of the energy is radiated as seismic waves, which propagate within the Earth.
When seismic waves arrive at the Earth’s surface, they cause ground motion.
The governing equations for seismic wave propagation, thus ground motion
simulations, are equations of motion for a continuous medium. Consider a material
volume V of a continuum with surface S, equations of motion can be written as
Ru D r   C b;
(21.1)
where,  is the stress tensor, u is the displacement vector, b is the body force vector,
 is density, and double dots on u represent the second derivative in time (thus
the acceleration). The ﬁrst term on the right-hand side of Eq. 21.1 with the dot
product of the operator r and the stress tensor gives the divergence of the stress
ﬁeld, resulting in a vector. With boundary conditions (either prescribed traction or
displacement) on the surface S and initial conditions of displacement and velocity in
the volume V, Eq. 21.1 governs wave propagation in the medium. Given a speciﬁc
type of continuum (e.g., elastic or viscoelastic), a constitutive law that speciﬁes how
stress relates to strain (or strain rate) can be substituted into Eq. 21.1.
Earthquake sources in ground motion simulations are commonly characterized
by kinematic models, rather than dynamic models. A kinematic source model
speciﬁes slip distribution on the fault and temporal evolution of slip at a given
point on the fault, without considering driving forces that cause them. In contrast,
a dynamic source model speciﬁes initial stress conditions on the fault, and slip
evolution and distribution are a part of the solution. In particular, in a spontaneous
rupture model (this is the type of the dynamic source model we will refer to hereafter
in this chapter), rupture propagation is governed by a failure criterion (e.g., Mohr-
Coulomb) and a friction law that speciﬁes how frictional strength varies with slip,
slip rate, and/or state variables during fault slipping. Rupture propagation and its
radiated wave ﬁeld are decoupled in a kinematic source model, while they are
coupled in a dynamic source, which is more realistic.
Numerical methods are needed for ground motion simulations for realistic geo-
logic structures. Commonly used numerical methods in ground motion simulations

21
Parallel Earthquake Simulations on Large-Scale Multicore Supercomputers
541
are the ﬁnite difference method (FDM) and the ﬁnite element method (FEM).
Other numerical methods in use include the boundary element method (BEM) and
spectral element method (SEM). Generally speaking, FDM is simpler and easier
to implement in the computer codes than FEM. Thus, FDM is widely used, and is
the most popular method in ground motion simulations. A comprehensive review
of FDM and FEM with application in seismic wave propagation and earthquake
ground motion can be found in [1].
Large-scale ground motion simulations with parallel computing have been
performed in some of earthquake-prone areas, particularly in Southern California.
During the past decade, a group of researchers working on large-scale simulations
has collaborated in building a modeling community within the Southern California
Earthquake center (SCEC). Ground motion prediction from possible scenario
earthquakes on the San Andreas Fault (SAF) is one of research activities in this
community. These large-scale 3D ground motion simulations are very challenging:
they are not only computationally intensive but also data intensive. To capture
higher frequencies of ground motion that are engineering’s interest (i.e., up to tens
Hertz) requires enormous computational resources. Each factor of 2 improvement
in frequency resolution roughly requires an increase in spatial grid (mesh) size by a
factor of 8 and an increase in the number of simulation time steps by a factor of 2,
for a total increase of 16 in computational resources. Meantime, the size of required
input data (e.g., rock properties) and output data increase dramatically, imposing
signiﬁcant challenges for the I/O. Thus, some of these simulations were performed
on the largest supercomputers available at the time of simulations, such as TeraGrid.
A group of researchers at San Diego State University (SDSU) and San Diego
Supercomputer Center (SDSC) used a FDM approach to perform a series of large-
scale ground motion simulations in Southern California for scenario earthquakes
on the southern SAF, called TeraShake. The code used in TeraShake is the AWM
(Anelastic Wave Model), which solves the 3D velocity-stress wave equation by a
staggered-grid FDM with fourth-order spatial accuracy and second-order temporal
accuracy. Anelastic wave propagation effects are accounted for by a coarse-
grained implementation of the memory variables for a constant-Q solid [2] and
Perfectly Matched Layers (PML) absorbing boundary conditions on artiﬁcial model
boundaries are implemented in the code [3]. The TeraShake1 calculations [4]
simulate 4 min of 0–0.5 Hz ground motion in a 180; 000 km2 area of southern
California, for Mw 7.7 scenario earthquakes along the 200 km long section of the
SAF between Cajon Creek and Bombay Beach at the Salton Sea. The source
models in TeraShake1 are kinematic source models modiﬁed from the 2002 Mw
7.9 Denali, Alaska, earthquake. The SCEC Community Velocity Model (CVM)
[5, 6] Version 3.0 is used for material properties in the models. The main scientiﬁc
ﬁndings from TeraShake1 include (1) the chain of sedimentary basins between San
Bernardino and downtown Los Angeles forms an effective waveguide to produce
high long-period ground motions over much of the greater Los Angeles region,
and (2) northwestward rupture propagation is much more efﬁcient in exciting
the above waveguide effects than southeastward rupture propagation. The model
domain in TeraShake1 is 600 km (NW) by 300 km (NE) by 80 km (depth). With a

542
X. Wu et al.
grid spacing of 200 m, the volume is divided into 1.8 billion cubes. The simulations
were performed on the 10 teraﬂops IBM Power4+ DataStar supercomputerat SDSC,
using 240 processors and up to 19,000 CPU hours. Each scenario took about 24 h
wall clock time for 4 min of wave propagation.
The TeraShake2 simulations [7] use a more complex source derived from
spontaneous rupture models with small-scale stress-drop heterogeneity on a scale
consistent with inferences from models of the 1992 Landers earthquake. These
simulations predict a similar spatial pattern of peak ground velocity (PGV), but
with the PGV extremes decreased by factors of 2–3 relative to TeraShake1, due to
a less coherent waveﬁeld radiated from the more complex source. The AWM code
can also perform spontaneous rupture modeling, but limited to planar fault surfaces
aligned with Cartesian coordinate planes normal to the free surface (vertical fault
planes), which is common for standard FDMs. Thus, Olsen et al. [7] use a two-step
approximate procedure to perform these simulations. In the ﬁrst step, they perform
spontaneous rupture modeling for a simpliﬁed planar fault geometry. The second
step is essentially a separate kinematic simulation, using as a source the space-time
history of fault slip from the ﬁrst step, mapping the latter onto the ﬁve-segment
SAF geometry. They use the same system DataStar as in TeraShake1. The ﬁrst step,
dynamic rupture simulations with a high-resolution (a grid spacing of 100 m), took
36,000 CPU hours using 1,024 processors, and the second step of wave propagation
runs with a grid spacing of 200 m took 14,000 CPU hours on 240 processors. Cui
et al. [8] present detailed discussions on optimization of the AWM code, the I/O
handling, and initialization, on scaling ability of the code up to 40 k processors on
the Blue Gene/L machine at IBM TJ Watson Research center, and on challenges for
data archive and management.
Several groups of researchers perform ground motion simulations for the great
Southern California ShakeOut exercise [9]. The ShakeOut is a hypothetical seismic
event of Mw 7.8 developed by a multidisciplinary group from the US Geological
Survey (USGS), with the collaboration of SCEC and the California Geological
Survey to improve public awareness and readiness for the next great earthquake
along the southern SAF. Graves et al. [10] simulate broadband ground motion for the
scenario earthquake with a kinematic source description [11] and examined ground
motion sensitivity to rupture speed. Olsen et al. [12] simulate ground motion from
an ensemble of seven spontaneous rupture models of Mw 7.8 northwest-propagating
earthquakes on the southern SAF. They found a similar difference in ground
motion extremes between kinematic and dynamic source models to that between
TeraShake1 and TeraShake2, attributable to a less coherent waveﬁeld excited by the
complex rupture paths of the dynamic sources. Bielak et al. [13] present veriﬁcation
of the ShakeOut ground motion simulations with a kinematic source description
by three groups with three independently-developed codes. One group CMU/PSC
uses a FEM approach known as Hercules [14] with an octree-based mesher. The
other two groups, SDSU/SDSC and URS/USC, use a staggered-grid FDM approach
[4,7,10]. All three codes can run on parallel computers. They ﬁnd that the results are
in good agreement with one another, with small discrepancies attributed to inherent
characteristics of the various numerical methods and their implementations.

21
Parallel Earthquake Simulations on Large-Scale Multicore Supercomputers
543
The most recent large-scale ground motion simulation in southern California,
called “M8”, was performed on the Jaguar Cray XT 5 at the National center for
Computational Sciences, using 223,074 cores with sustained 220 Tﬂop/s for 24 h
by Cui et al. [15]. M8 uses 436 billion 40-m3 cubes to represent the 3D lithosphere
structure (the SCEC CVM Version 4.0) in a volume of 810 km  405 km  85 km,
providing ground motion synthetics in southern California with frequencies up to
2 Hz. The code used in M8, called AWP-ODC, is a highly scalable, parallel version
of the AWM with addition of other components, in particular for data-intensive I/O
treatments (e.g., MPI-IO). The same two-step procedure in TeraShake2 is used in
M8 to account for source complexities revealed by spontaneous rupture models.
Above TeraShake and ShakeOut simulations [7, 12] have shown that dynamic
source models with small-scale stress-drop heterogeneity predict more realistic
ground motion extremes than commonly used kinematic source models in ground
motion simulations. More importantly, dynamic rupture models provide a means for
scientists to explore physical processes that control earthquake rupture propagation,
and thus earthquake sizes and rupture paths in realistically complex fault systems,
which are important inputs for seismic hazard analysis in earthquake-prone regions.
1.2
Dynamic Rupture Simulations
As mentioned in the above section, dynamic rupture models specify initial stress
conditions on the fault and invoke a failure criterion and a friction law on the
fault to solve for rupture propagation. Thus, rupture propagation in these dynamic
rupture models is spontaneous (thus referring to spontaneous rupture models) and it
obeys physical laws, including the failure criterion, the friction law, and principles
of continuum mechanics. In addition, rupture propagation on the fault is coupled
with wave propagation in the surrounding medium through the stress ﬁeld. For
example, waves reﬂected from layer boundaries and the Earth’s free surface may
alter the stress state on the fault, thus affecting rupture propagation that radiates
seismic waves. Thus, spontaneous rupture models have been becoming increasingly
important in earthquake physics studies and ground motion simulations.
Two frictional laws have been widely used in spontaneous rupture models. One
is the slip-weakening friction law [16–18], in which the frictional coefﬁcient on
the fault plane drops linearly from a static value s to a dynamic (sliding) value
d over a critical slip-weakening distance D0. The other is the rate- and state-
dependent friction law derived from laboratory experiments of rock friction [19,20],
in which frictional coefﬁcient is a function of slip velocity and state variables.
Because of involvement of fault friction, there are no analytical solutions for
spontaneous rupture problems and numerical methods are required. Among the
earliest spontaneous rupture models were those constructed by Andrews [17] in
2D and Day [18] in 3D using the FDM approach. The former examined variability
of rupture speed on an idealized 2D fault and predicted a regime of supershear
rupture that is observed in natural earthquakes later. The latter explored effects

544
X. Wu et al.
of nonuniform prestress on rupture propagation. The standard FDM is limited to
simulate spontaneous rupture propagation on vertical planar fault planes. FEM,
BIEM (boundary integral equation method), and SEM have also been used in
spontaneous rupture simulations. For example, Oglesby et al. [21–23] used a FEM
approach to study effects of dipping fault geometry on rupture dynamics and ground
motion. Aochi and Fukuyama [24] used a BIEM to study spontaneous rupture
propagation of the 1992 Lander earthquake along a non-planar strike-slip fault
system. Kame et al. [25] studied effects of prestress state and rupture velocity on
dynamic fault branching using a BIEM approach.
One challenging issue in spontaneous rupture simulations is veriﬁcation and
validation of computer codes implemented by different researchers based on the
above methods. Veriﬁcation refers to comparison of results from different codes on
an identical problem, while validation generally means comparison of simulation
results against ground motion recordings from natural events and involves validation
of not only the source process, but also the path effect (including the velocity
structure and the local site condition). A broad, rigorous community-wide exercise
on veriﬁcation of dynamic rupture codes has been underway in the SCEC/USGS
community [26]. This exercise is to compare computer codes for rupture dynamics
used by SCEC and USGS researchers to verify that these codes are functioning as
expected for studying earthquake source physics and ground motion simulations
[27]. More than 15 computer codes have been involved in the exercise and results
of some benchmark problems from some of these codes are publicly accessible on
the web site, http://scecdata.usc.edu/cvws.
As analyzed by Day et al. [28], the 3D spontaneous rupture simulations can be
quite challenging in terms of required memory and processor power, because of
a spatial resolution requirement for the cohesive zone at the rupture tip. Very few
codes in the community can perform large-scale spontaneous rupture simulations
on hundreds to thousands processors, which are needed to construct spontaneous
rupture models of large to huge earthquakes with a reasonable resolution. The
FDM code AWP-ODC used in M8 simulations, with its earlier version of AWM
used in TeraShake2 simulations, is one that can run on thousands of processors
to our knowledge. However, the code is limited to simulate spontaneous rupture
propagation on a vertical planar fault. Most of large to huge earthquakes occur on
shallow-dipping thrust faults and often involve segmented faults with non-planar
geometry, such as the 2004 Sumatra and the 2011 Japan earthquakes.
Duan and co-workers have been developing an explicit FEM code, EQdyna,
to simulate spontaneous rupture propagation on geometrically complex faults and
seismic wave propagation in an elastic or elastoplastic medium [29–32]. The code
has been veriﬁed in the SCEC/USGS dynamic code veriﬁcation exercise on many
benchmark problems. An OpenMP version of the code [33] was used to investigate
effects of prestress rotations along a shallow-dipping fault on rupture dynamics and
near-ﬁeld ground motion, motivated by relevant observations in the 2008 Wenchuan
(China) Mw 7.9 earthquake [34]. Figure 21.1 shows snapshots of near-ﬁeld ground
motion from a spontaneous rupture model. In Fig. 21.1, the black line is the trace
of a shallow dipping fault in the model, and circle, triangular, plus, and cross

21
Parallel Earthquake Simulations on Large-Scale Multicore Supercomputers
545
Fig. 21.1 Snapshots of horizontal ground velocity from a simpliﬁed dynamic model of the
2008 Ms 8.0 Wenchuan earthquake
signs denote the epicenter, Chengdu, Beichuan, and Wenchuan cities, respectively.
The ﬁgure illustrates that the distribution of near-ﬁeld ground velocity is strongly
affected by the shallow dipping fault geometry with higher ground motion on the
hanging wall side of the fault (below the black line in the ﬁgure).
We have been parallelizing EQdyna since 2008, aiming to perform large-scale
spontaneous rupture and ground motion simulations for realistically complex fault
systems and geologic structures. Based on what we learned from the OpenMP
implementation [33], we developed an initial hybrid MPI/OpenMP implementation
with a 3D mesh as an input, which was generated by a 3D mesh generator separately
before the simulation execution [35]. In this chapter, we integrate the 3D mesh
generator into the simulation, and use MPI to parallelize the 3D mesh generator.
Then we illustrate an element-based partitioning scheme for explicit ﬁnite element
methods, and evaluate its performance on Quad- and Hex-core Cray XT systems
at Oak Ridge National Laboratory [36] using the SCEC benchmark TPV 210. The
experimental results indicate that the hybrid MPI/OpenMP implementation has the
accurate output results and the good scalability on these systems.
1.3
Earthquake Simulations and Data-Intensive Computing
The main input to large-scale earthquake simulations are large datasets which
describe geologic structures (e.g., faults and slip or stress on them) and rock
properties (e.g., seismic velocities). These datasets drive a simulation pipeline which

546
X. Wu et al.
includes mesher, solver, and visualizer [14, 37, 38]. Generally, a mesh is generated
to model the property or geometry of an earthquake region. Then, a solver takes the
mesh as input to conduct numerical computation. The numerical results produced
by the solver are correlated to the mesh structure by a visualizer to create earthquake
ground motion images or animations. As earthquake simulations target hundred
million to multi-billion element simulations, signiﬁcant performance bottlenecks
remain in storing, transferring and reading/writing multi-tera-/petabytes ﬁles be-
tween these components. In particular, I/O of multi-tera-/petabytes ﬁles remains a
pervasive performance bottleneck on large-scale multicore supercomputers.
Generally speaking, large-scale earthquake simulations are data-intensive sim-
ulations, such as TeraShake simulations [8] which produced more than 40 TB of
data. These simulations revealed new insights into large-scale patterns of earthquake
ground motion, including where the most intense impacts may occur in Southern
California’s sediment-ﬁlled basins during a magnitude 7.7 southern San Andreas
Fault earthquake. Parallel I/O techniques such as MPI-IO and I/O extension of the
standardized MPI library [39] are used to overcome I/O performance bottleneck
occurred in large-scale earthquake simulations by increasing the overall I/O band-
width via using more disks in parallel, decreasing the I/O latency via reducing the
number of disk accesses, and/or overlapping computation, communication and I/O
operations [40]. For post-processing component visualizer in earthquake ground
motion simulations, there are some visualization challenges such as huge output
data, time varying data, unstructured mesh, multiple variables and vector and
displacement ﬁelds [38].
In this chapter, we focus on discussing the earthquake simulation compo-
nents mesher and solver. The remainder of this chapter is organized as follows.
Section 2 illustrates an element-based partitioning scheme, discusses our hybrid
MPI/OpenMP parallel ﬁnite element Earthquake rupture simulations in detail.
Section 3 describes the architecture and memory hierarchy of quad- and hex-core
Cray XT systems used in our experiments. Section 4 discusses the benchmark
problem TPV210 and veriﬁes our simulation results. Section 5 evaluates and
explores performance characteristics of our hybrid MPI/OpenMP implementation,
and presents the experimental results. Section 6 concludes this chapter.
2
Hybrid MPI/OpenMP Parallel Finite Element Earthquake
Simulations
In the ﬁnite element method, the data dependence is much more irregular than
the ﬁnite difference method, so it is generally more difﬁcult to parallelize. Ding
and Ferraro [41] discussed node-based and element-based partitioning strategies,
found that main advantage for element-based partitioning strategy over node-based
partitioning strategy was its modular programming approach to the development
of parallel applications, and developed an element-based concurrent partitioner for

21
Parallel Earthquake Simulations on Large-Scale Multicore Supercomputers
547
partitioning unstructured ﬁnite element meshes on distributed memory architec-
tures. Tu et al. [14] parallelized an octree-based ﬁnite element simulation of
earthquake ground motion to demonstrate the ability of their end-to-end approach
to overcome the scalability bottlenecks of the traditional approach.
Mahinthakumar and Saied [42] presented a hybrid implementation adapted for
an implicit ﬁnite-element code developed for groundwater transport simulations
based on the original MPI code using a domain decomposition strategy, and added
OpenMP directives to the code to use multiple threads within each MPI process
on SMP clusters. Nakajima [43] presented a parallel iterative method in GeoFEM
for ﬁnite element method which was node-based with overlapping elements on
the Earth Simulator, and explored a three-level hybrid parallel programming
model, including message passing (MPI) for inter-SMP node communication, loop
directives by OpenMP for intra-SMP node parallelization and vectorization for each
processing element.
In this section, based on what we learned from our previous work [33, 35], we
integrate a 3D mesh generator into the simulation, and use MPI to parallelize the
3D mesh generator, illustrate an element-based partitioning scheme for explicit
ﬁnite element methods, and discuss how efﬁciently to use hybrid MPI/OpenMP
implementations in the earthquake simulations for not only achieving multiple levels
of parallelism but also reducing the communication overhead of MPI within a
multicore node, by taking advantage of the globally shared address space and on-
chip high inter-core bandwidth and low inter-core latency on large-scale multicore
systems.
2.1
Mesh Generation and Model Domain Partitioning
In our previous work [33, 35], we developed an initial hybrid MPI/OpenMP
implementation of the sequential earthquake simulation code EQdyna with a 3D
mesh as an input, which was generated by a 3D mesh generator separately before
the simulation execution. As we discussed in our previous work, the earthquake
simulation code is memory bound, when the number of elements increases, the
required system memory for storing large arrays associated with the entire model
domain increases dramatically. In order to overcome the limitation, in this chapter,
we integrate the 3D mesh generator into the simulation, and use MPI to parallelize
the 3D mesh generator.
To parallelize the 3D mesh generator, based on the number of MPI processes
used, we partition the entire model domain by the coordinate along fault strike (e.g.,
the x-coordinate in a Cartesian coordinate system) shown in Fig. 21.2 so that we
can deﬁne small arrays for each MPI process independently. Figure 21.2 gives a
schematic diagram for the 3D mesh partitioning. Thus, memory requirements by
large arrays that are associated with the entire model domain in a previous version
of the code [35] signiﬁcantly decrease.

548
X. Wu et al.
Fig. 21.2 Schematic diagram to show mesh and model domain partitioning
To facilitate message passing between adjacent MPI processes, based on the
partitions of the entire model domain by the coordinate along fault strike, during
the mesh generation step, we create a sub-mesh for each MPI process and record
shared boundary nodes between two adjacent MPI processes. This converts reading
initial large input mesh data to computing and generating small mesh data for each
MPI process. Note that, in this partitioning scheme, the maximum number of MPI
processes that can be used is bounded by the total number of nodes along the
x-coordinate.
2.2
Element-Based Partitioning
In the FEM, elements are usually triangles or quadrilaterals in two dimensions, or
tetrahedral or hexahedral bricks in three dimensions. In our explicit ﬁnite element
earthquake simulation, we primarily use trilinear hexahedral elements to discretize
a 3D model for computational efﬁciency, with wedge-shaped elements along the
fault to characterize dipping fault geometry as illustrated in Fig. 21.2. We use a
large buffer region with increasingly coarser element sizes away from the fault to
prevent reﬂections from artiﬁcial model boundaries from contaminating examined
phenomena.
For simplicity, we discuss our partitioning scheme with a hypothetical 2D mesh
shown in Fig. 21.3, where there are 12 elements (boxes) and each element has 4

21
Parallel Earthquake Simulations on Large-Scale Multicore Supercomputers
549
Fig. 21.3 2D geometry for the EQdyna: 12 elements (boxes) and each element with 4 nodes
(circles)
nodes (circles) adjacent to it. We propose an element-based partitioning scheme
because most time-consuming computation in the earthquake rupture simulation
code is element-based. Within one timestep, element contribution (both internal
force and hourglass force) to its nodes’ nodal force is ﬁrst calculated. Then,
contributions to a node’s nodal force from all of its adjacent elements are assembled.
For instance, the nodal force at node 1 only involves element 1, while the nodal force
at node 5 involves elements 1, 2, 3, and 4. The nodal force at node 5 is the sum of
contributions from all these four elements.
Figure 21.4 illustrates the element-based partitioning scheme for the ﬁnite
element method, where the 2D domain is split into three components. In this
scheme, we essentially partition the model domain based on element numbers. Each
component consists of four elements and the nodes adjacent to them. A node that lies
on the boundary between two components is called a boundary node. For example,
nodes 7, 8, and 9 are the boundary nodes between the ﬁrst two components, and
nodes 13, 14, and 15 are the boundary nodes between the last two components. To
update the nodal force at a boundary node such as node 8, it needs contributions
from elements 3 and 4 in the ﬁrst component and those from elements 5 and 6
in the second component. This requires the data exchange between the ﬁrst two
components.
Similarly, the above element-based partitioning scheme can be extended to large
3D datasets. The element-based partitioning method described in this section is
applicable to more irregular meshes as well.
2.3
Hybrid Implementations
Multicore clusters provide a natural programming paradigm for hybrid programs.
Generally, MPI is considered optimal for process-level coarse parallelism and
OpenMP is optimal for loop-level ﬁne grain parallelism. Combining MPI and

550
X. Wu et al.
Fig. 21.4 Element-based partitioning scheme
OpenMP parallelization to construct a hybrid program is not only to achieve
multiple levels of parallelism but also to reduce the communication overhead of
MPI at the expense of introducing OpenMP overhead due to thread creation and
increased memory bandwidth contention. Therefore, we use hybrid MPI/OpenMP
to parallelize the ﬁnite element code for exploring the parallelism of the code at
node level (OpenMP) and the parallelism of the code between nodes (MPI) so that
the parallel earthquake simulation can be run on most supercomputers. Note that, in
the hybrid MPI/OpenMP implementations, we separate MPI regions from OpenMP
regions, and OpenMP threads cannot call MPI subroutines.
Figure 21.5 shows the parallelism at MPI and OpenMP levels within one
timestep for the hybrid implementation of the earthquake simulation. As we
discussed in the previous section, using the element-based partitioning scheme,
we can partition the 2D mesh geometry into three components, and dispatch each
component to a MPI process for MPI level parallelism. So each MPI process is in
charge of four elements and the nodes adjacent to them. Because the earthquake
simulation is memory-bound, each MPI process is created on a different node
as illustrated in Fig. 21.5. MPI process 0 is run on Node 0; process 1 is on
Node 1; process 2 is on Node2. On each node, OpenMP level parallelism can

21
Parallel Earthquake Simulations on Large-Scale Multicore Supercomputers
551
Fig. 21.5 Parallelism at MPI and OpenMP levels within one timestep
be achieved by using element-based partitioning scheme and OpenMP. Each MPI
process (the master thread) forks several new threads to take advantage of the shared
address space and on-chip high inter-core bandwidth and low inter-core latency on
the node.
To manipulate and update nodal forces at these boundary nodes, it requires the
data exchange between two adjacent MPI processes via message passing. For each
boundary node such as node 7 shown in Fig. 21.5, to update its nodal force at the
end of each timestep, we sum the nodal force at node 7 from process 0 and the nodal
force at node 7 from process 1, then use the sum to update the nodal forces at node
7 for the processes 0 and 1.
To implement updating the nodal force at each boundary node at the end of each
timestep, we propose the following algorithm to deal with the problem.

552
X. Wu et al.
Algorithm: Update the nodal forces at boundary nodes:
Step 1: Partition the initial data mesh based on the number of MPI processes
to ensure load balancing, get the information about shared boundary nodes
between MPI processes i and i C 1 from the mesh generator discussed in
Sect. 1, and allocate a temporal array btmp with the nodal forces at the shared
boundary nodes.
Step 2: The MPI process i sends the array btmp to its neighbor process i C 1 using
MPI Sendrecv.
Step 3: The MPI process i C 1 receives the array from process i using MPI
Sendrecv. For each shared boundary node, it sums the nodal force from the
array and the local nodal force at the shared boundary node, then assigns the
summation to the nodal force at the shared node locally.
Step 4: The MPI process i C1 updates the array locally, and sends the updated array
back to the MPI process i.
Step 5: The MPI process i receives the updated array and update the nodal forces
at the shared nodes locally, and deallocates the temporal array at the end of the
timestep.
Step 6: Repeat the above Steps 1–5 for the next timestep.
The algorithm implements the straightforward data exchanges illustrated in
Fig. 21.5, and it is efﬁcient because of sending/receiving smaller messages. This
can simplify the programming efforts and reduce the communication overhead.
3
Experimental Platforms
Our hybrid parallel earthquake simulations were tested on several systems [33, 35,
44]. In this chapter, we only conduct our experiments using Jaguar (Cray XT5 and
XT4) supercomputers from Oak Ridge National Laboratory [36]. Table 21.1 shows
Table 21.1 Speciﬁcations of quad- and hex-core Cray XT supercomputers
Conﬁgurations
JaguarPF (XT5)
Jaguar (XT4)
Total cores
224,256
31,328
Total nodes
18,688
7,832
Cores/socket
6
4
Cores/node
12
4
CPU type
AMD 2.6 GHz hex-core
AMD 2.1 GHz quad-core
Memory/node
16 GB
8 GB
L1 Cache/core, private
64 KB
64 KB
L2 Cache/core, private
512 KB
512 KB
L3 Cache/socket, shared
6 MB
2 MB
Compiler
ftn
ftn
CompilerpOptions
-O3 -mp D nonuma -fastsse
-O3 -mp D nonuma -fastsse

21
Parallel Earthquake Simulations on Large-Scale Multicore Supercomputers
553
Fig. 21.6 Jaguar and JaguarPF System Architecture [36]
their speciﬁcations and the same compilers used for all experiments. All systems
have private L1 and L2 caches and shared L3 cache per node. Jaguar is the primary
system in the ORNL Leadership Computing Facility (OLCF). It consists of two
partitions: XT5 and XT4 partitions shown in Fig. 21.6.
The Jaguar XT5 partition (JaguarPF) contains 18,688 compute nodes in addition
to dedicated login/service nodes. Each compute node contains dual hex-core AMD
Opteron 2435 (Istanbul shown in Fig. 21.7) processors running at 2.6 GHz, 16 GB
of DDR2-800 memory, and a SeaStar 2C router. The resulting partition contains
224,256 processing cores, 300 TB of memory, and a peak performance of 2.3
petaﬂop/s. The Jaguar XT4 partition (Jaguar) contains 7,832 compute nodes in
addition to dedicated login/service nodes. Each compute node contains a quad-
core AMD Opteron 1354 (Budapest) processor running at 2.1 GHz, 8 GB of
DDR2-800 memory, and a SeaStar2 router. The resulting partition contains 31,328
processing cores, more than 62 TB of memory, over 600 TB of disk space, and
a peak performance of 263 teraﬂop/s. The SeaStar2C router (XT5 partition) has
a peak bandwidth of 57.6 GB/s, while the SeaStar2 router (XT4 partition) has a
peak bandwidth of 45.6 GB/s. The routers are connected in a 3D torus topology,
which provides an interconnect with very high bandwidth, low latency, and extreme
scalability.

554
X. Wu et al.
Fig. 21.7 AMD hex-core Opteron chip architecture [36]
4
Result Veriﬁcation and Benchmark Problems
4.1
Benchmark Problem SCEC TPV210
To validate the hybrid MPI/OpenMP earthquake simulation code, we apply it to
a SCEC/USGS benchmark problem TPV210, which is the convergence test of the
benchmark problem TPV10 [26,45]. In TPV10, a normal fault dipping at 60ı (30 km
long along strike and 15 km wide along dip) is embedded in a homogeneous half
space. Pre-stresses are depth dependent and frictional properties are set to result
in a subshear rupture. This benchmark problem is motivated by ground motion
prediction at Yucca Mountain, Nevada, which is a potential high-level radioactive
waste storage site [27, 32]. In TPV10, modelers are asked to run simulations at an
element size of 100 m on the fault surface. We refer the edge length of the trilinear
hexahedral elements near the fault as the element size in our study.
In TPV210, we conduct the convergence test of the solution by simulating the
same problem at a set of element sizes, i.e., 200, 100, 50, 25 m, and so on, where
m stands for meters. Table 21.2 summarizes model parameters for TPV210. The

21
Parallel Earthquake Simulations on Large-Scale Multicore Supercomputers
555
Table 21.2 Model parameters for TPV210
Parameters
Values
Element size
200 m
100 m
50 m
25 m
Total elements
6,116,160
24,651,088
98,985,744
419,554,200
Time step (s)
0.016
0.008
0.004
0.002
Termination time (s)
15
15
15
15
Required memory (GB)
6
24
94
380
nxt
281
477
829
1483
benchmark requires larger memory sizes with ﬁner element sizes, because the
decrease of element size means the increase of numbers of elements and nodes.
For example, for the element size of 50 m, it requires the number of elements
be approximately 100,000,000, and the memory requirement is around 94 GB.
The simulation is memory-bound. Our hybrid MPI/OpenMP parallel simulations
discussed in Sect. 2 target the limitation to reduce large memory requirements.
Because the number of elements with a discretization varies a little bit with the
number of MPI processes used, Table 21.2 only gives a rough estimate of this
number. In the table, nxt is the node number along the x-coordinate in a sequential
simulation, which limits how many MPI processes one can use in a hybrid parallel
simulation. For the sake of simplicity, we only use TPV210 with 50 m element size
as an example in this chapter.
4.2
Result Veriﬁcations
Figure 21.8 shows the rupture time (in seconds) contours on the 60ı dipping fault
plane. Red star denotes the hypocenter of simulated earthquakes. Results from two
simulations are plotted in the ﬁgure. One (black) is the result from a previous run
with 50 m element size that was veriﬁed in the SCEC/USGS code validation exercise
[45]. The other is the result from a run performed in this study on JaguarXT5
with 50 m element size using 256 MPI processes with 12 OpenMP threads per
MPI process. These two results essentially overlap, indicating our current hybrid
implementation gives accurate results.
Figures 21.9 and 21.10 compare time histories of the dip-slip component of slip
velocity at an on-fault station and the vertical component of particle velocity at
an off-fault station from the two simulations with 50 m of element size discussed
above. The locations of the stations are described in the ﬁgures. The result from
the current hybrid implementation matches that from the veriﬁed one very well.
This indicates that our hybrid MPI/OpenMP implementation is validated and has
the accurate output results of fault movement and ground shaking.

556
X. Wu et al.
Fig. 21.8 Rupture time contours on the dipping fault plane for TPV210 with 50 m element size
Fig. 21.9 The dip-slip component of slip velocity on a fault station for TPV210 with 50 m
element size
5
Performance Analysis
In this section, we analyze and compare the performanceof the hybrid MPI/OpenMP
ﬁnite element earthquake simulation on quad- and hex-core Cray XT systems. Note
that TPN stands for Threads Per Node.
Figure 21.11 presents the function-level performance of the hybrid MPI/OpenMP
ﬁnite element earthquake simulation with 50 m on Cray XT4, where there are seven
main functions in the code; the functions Input and qdct2 are called once, and the

21
Parallel Earthquake Simulations on Large-Scale Multicore Supercomputers
557
Fig. 21.10 The vertical component of particle velocity at an off-fault station for TPV210 with
50 m element size
Fig. 21.11 Function-level performance for TPV210 with 50 m on Cray XT4
functions updatedv, qdct3, hourglass, faulting and communication are within the
main timestep loop. The function communication means the MPI communication,
and the MPI communication overhead was measured on each master MPI process
for all hybrid executions.
Figure 21.12 shows the relative speedup for TPV210 with 50 m on Cray XT4
from Fig. 21.11, where we assume that the relative speedup for TPV210 with 50 m
executed on 1,024 cores is 1,024, then calculate the relative speedup for up to 3,200

558
X. Wu et al.
Fig. 21.12 Relative speedup for TPV210 with 50 m on Cray XT4
Fig. 21.13 Function-level performance for TPV210 with 50 m on Cray XT5
cores. In fact, for 1,024 cores, the hybrid execution on Cray XT4 utilizes 256 MPI
processes with 1 MPI process per node and 4 OpenMP TPN. We observe that the
hybrid execution on Cray XT4 has good scalability with the increase of the number
of cores.
Figure 21.13 presents the function-level performance of the hybrid MPI/OpenMP
ﬁnite element earthquake simulation with 50 m on Cray XT5. Fig. 21.14 shows the
relative speedup for TPV210 with 50 m on Cray XT5 from Fig. 21.13, where we
assume that the relative speedup for TPV210 with 50 m executed on 3,072 cores
(256 nodes with 12 cores per node) is 3,072, then calculate the relative speedup
for up to 9,600 cores. In fact, for 3,072 cores, the hybrid execution on Cray XT5
utilizes 256 MPI processes with 1 MPI process per node and 12 OpenMP TPN.

21
Parallel Earthquake Simulations on Large-Scale Multicore Supercomputers
559
Fig. 21.14 Relative speedup for TPV210 with 50 m on Cray XT5
We observe that the hybrid execution on Cray XT4 has better scalability than that
on Cray XT5. For strong scaling scientiﬁc applications like our hybrid earthquake
simulation, with increasing number of cores, some parallelization loop sizes become
very small, which may cause more OpenMP overhead. The other reason is related
to memory subsystems and how efﬁciently they support OpenMP programming
[44,46].
6
Conclusions
In this chapter, we reviewed large-scale ground motion and earthquake rupture
simulations in the earthquake simulation community and discussed the relationships
between data-intensive computing and earthquake simulations. We used a different
approach to convert a data-intensive earthquake simulation into a computation-
intensive earthquake simulation to signiﬁcantly reduce I/O operations at the input
stage. We integrated a 3D mesh generator into the simulation, and used MPI to par-
allelize the 3D mesh generator. We illustrated an element-based partitioning scheme
for explicit ﬁnite element methods. Based on the partitioning scheme and what
we learned from our previous work, we implemented a hybrid MPI/OpenMP ﬁnite
element earthquake simulation code to achieve multiple levels of parallelism of the
simulation. The experimental results demonstrated that the hybrid MPI/OpenMP
code has the accurate output results and the good scalability on Cray XT4 and XT5
systems.
Because we partitioned the entire model domain by the coordinate along fault
strike (e.g., the x-coordinate in a Cartesian coordinate system), the maximum
number of MPI processes that can be used is bounded by the total number of
nodes along the x-coordinate. This limits the scalability of the hybrid simulation.
We also found that we could not use any number of MPI processes for the hybrid

560
X. Wu et al.
execution because load imbalance could cause large MPI communication overhead.
For the future work, we plan to further improve the memory requirements of the
hybrid simulation code by partitioning the entire model domain in X-, Y- and
Z-dimensions, and consider some load balancing strategies discussed in [47] and
some optimization strategies discussed in [48].
Acknowledgments This work is supported by NSF grants CNS-0911023, EAR-1015597, and
the Award No. KUS-I1-010-01 made by King Abdullah University of Science and Technology
(KAUST). The authors would like to acknowledge National Center for Computational Science at
Oak Ridge National Laboratory for the use of Jaguar and JaguarPF under DOE INCITE project
“Performance Evaluation and Analysis Consortium End Station.”
References
1. P. Moczo, J. Kristek, M. Galis, et al., The ﬁnite-difference and ﬁnite-element modeling of
seismic wave propagation and earthquake motion, Acta Phys. Slovaca, 57(2), 177–406, 2007.
2. S. M. Day and C. R. Bradley, Memory-efﬁcient simulation of anelastic wave propagation,
Bulletin of the Seismological Society of America, 91, 520–531, 2001.
3. C. Marcinkovich, and K. Olsen, On the implementation of perfectly matched layers in a
three-dimensional fourth-order velocity-stress ﬁnite difference scheme, Journal of Geophysical
Research, 108(B5), 2276, 2003.
4. K. B. Olsen, S. M. Day, J. B. Minster, et al., Strong shaking in Los Angeles expected from
southern San Andreas earthquake, Geophysical Research Letters, 33, 1–4, 2006.
5. H. Magistrale, S. M. Day, R. W. Clayton, and R. W. Graves, The SCEC southern California
reference three-dimensional seismic velocity model version 2, Bulletin of the Seismological
Society of America, 90, S65–S76, 2000.
6. M. Kohler, H. Magistrale, and R. Clayton, Mantle heterogeneities and the SCEC three-
dimensional seismic velocity model version 3, Bulletin of the Seismological Society of
America, 93, 757–774, 2003.
7. K. B. Olsen, S. M. Day, J. B. Minster, et al., TeraShake2: Simulation of Mw7.7 earthquakes
on the southern San Andreas fault with spontaneous rupture description, Bulletin of the
Seismological Society of America, 98, 1162–1185, 2008.
8. Y. Cui, R. Moore, K. Olsen, et al., Toward Petascale Earthquake Simulations, Acta Geotech-
nica, DOI 10.1007/s11440-008-0055-2, 2008.
9. L. Jones, et al., The ShakeOut scenario, U.S. Geol. Survey Open File Rep., 2008–1150, 2008.
10. R. W. Graves, B. Aagaard, K. Hudnut, L. Star, J. Stewart, and T. H. Jordan, Broadband
simulations for Mw 7.8 southern San Andreas earthquakes: Ground motion sensitivity to
rupture speed, Geophysical Research Letters, 35, L22302, 2008.
11. K. W. Hudnut, B. Aagaard, R. Graves, L. Jones, T. Jordan, L. Star, and J. Stewart, ShakeOut
earthquake source description, surface faulting and ground motions, U.S. Geol. Surv. Open File
Rep., 2008–1150, 2008.
12. K. B. Olsen, S. M. Day, L. A. Dalguer, et al., ShakeOut-D: Ground motion estimates using
an ensemble of large earthquakes on the southern San Andreas fault with spontaneous rupture
propagation, Geophysical Research Letters, 36, L04303, 2009.
13. J. Bielak, R. Graves, K.B. Olsen, et al., The ShakeOut Earthquake Scenario: Veriﬁcation of
Three Simulation Sets, Geophysical Journal International, 180 (1), 375–404, 2010.
14. T. Tu, H. Yu, L. Ramırez-Guzman, et al., From mesh generation to scientiﬁc visualization:
an end-to-end approach to parallel supercomputing, in Proceedings of 2006 ACM/IEEE
International Conference for High Performance Computing, Networking, Storage and Analysis
(SC06), IEEE Computer Society, Tampa, Florida, 2006.

21
Parallel Earthquake Simulations on Large-Scale Multicore Supercomputers
561
15. Y. Cui, K. B. Olsen, T. H. Jordan, et al., Scalable earthquake simulation on petascale
supercomputers, SC10, 2010.
16. Y. Ida, Cohesive force across the top of a longitudinal shear crack and Grifﬁth’s speciﬁc surface
energy, Journal of Geophysical Research, 77, 3796–3805, 1972.
17. D. J. Andrews, Rupture velocity of plane strain shear cracks, Journal of Geophysical Research,
81, 5679–5687, 1976.
18. S. M. Day, Three-dimensional simulation of spontaneous rupture: The effect of nonuniform
prestress, Bulletin of the Seismological Society of America, 72, 1881–1902, 1982.
19. J. H. Dieterich, Modeling of rock friction, 1. Experimental results and constitutive equations,
Journal of Geophysical Research, 84, 2169–2175, 1979.
20. A. Ruina, Slip instability and state variable friction laws, Journal of Geophysical Research, 88,
10,359–10,370, 1983.
21. D. D. Oglesby, R. J. Archuleta, and S. B. Nielsen, Earthquakes on dipping faults: the effects of
broken symmetry, Science, 280, 1055–1059, 1998.
22. D. D. Oglesby, R. J. Archuleta, and S. B. Nielsen, The three-dimensional dynamics of dipping
faults, Bulletin of the Seismological Society of America, 90, 616–628, 2000.
23. D. D. Oglesby, R. J. Archuleta, and S. B. Nielsen, The dynamics of dip-slip faults: Explorations
in two dimensions, Journal of Geophysical Research, 105,13643–13653, 2000.
24. H. Aochi, and E. Fukuyama, Three-dimensional non-planar simulation of the 1992 Landers
earthquake, Journal of Geophysical Research, 107(B2), 2035, doi:10.1029/2000JB000061,
2002.
25. N. Kame, J. R. Rice, and R. Dmowska, Effects of prestress state and rupture velocity on
dynamic fault branching, Journal of Geophysical Research, 108(B5), 2265, 2003.
26. R. A. Harris, M. Barall, et al., The SCEC/USGS Dynamic Earthquake-rupture Code Veriﬁca-
tion Exercise, Seismological Research Letters, Vol. 80, No. 1, 2009.
27. R. A. Harris, M. Barall, D. J. Andrews, et al., Verifying a computational method for predicting
extreme ground motion, Seismological Research Letters, Vol. 82, No. 5, 2011.
28. S. M. Day, L. A. Dalguer, N. Lapusta, and Y. Liu, Comparison of ﬁnite difference and boundary
integral solutions to three-dimensional spontaneous rupture, Journal of Geophysical Research,
110, B12307, doi:10.1029/2005JB003813, 2005.
29. B. Duan and D. D. Oglesby, Heterogeneous Fault Stresses From Previous Earthquakes and
the Effect on Dynamics of Parallel Strike-slip Faults, Journal of Geophysical Research, 111,
B05309, 2006.
30. B. Duan and D. D. Oglesby, Nonuniform Prestress From Prior Earthquakes and the effect on
Dynamics of Branched Fault Systems, Journal of Geophysical Research, 112, B05308, 2007.
31. B. Duan and S. M. Day, Inelastic Strain Distribution and Seismic Radiation From Rupture of a
Fault Kink, Journal of Geophysical Research, 113, B12311, 2008.
32. B. Duan and S. M. Day, Sensitivity study of physical limits of ground motion at Yucca
Mountain, Bulletin of the Seismological Society of America, 100 (6), 2996–3019, 2010.
33. X. Wu, B. Duan and V. Taylor, An OpenMP Approach to Modeling Dynamic Earthquake Rup-
ture Along Geometrically Complex Faults on CMP Systems, ICPP2009 SMECS Workshop,
September 22–25, 2009, Vienna, Austria
34. B. Duan, Role of initial stress rotations in rupture dynamics and ground motion: A case
study with Implications for the Wenchuan earthquake, Journal of Geophysical Research, 115,
B05301, 2010.
35. X. Wu, B. Duan and V. Taylor, Parallel simulations of dynamic earthquake rupture along
geometrically complex faults on CMP systems, Journal of Algorithm and Computational
Technology, 5 (2), 313–340, 2011.
36. NCCS Jaguar and JaguarPF, Oak Ridge National Laboratory, http://www.nccs.gov/computing-
resources/jaguar/
37. T. Tu, D. R. O’Hallaron, and O. Ghattas, Scalable Parallel Octree Meshing for Terascale
Applications, Proceedings of 2005 ACM/IEEE International Conference for High Performance
Computing, Networking, Storage and Analysis (SC05), November 12–18, 2005, Seattle,
Washington, USA.

562
X. Wu et al.
38. K. Ma, A. Stompel, et al., Visualizing Very Large-Scale Earthquake Simulations, SC’03,
November 15–21, 2003, Phoenix, Arizona, USA.
39. W. Gropp, E. Lusk, R. Thakur, Using MPI-2: Advanced Features of the Message-Passing
Interface, MIT Press, Cambridge, MA, 1999.
40. M. Cannataro, D. Talia, and P. K. Srimani, Parallel Data Intensive Computing in Scientiﬁc and
Commercial Applications, Parallel Computing 28, 2002.
41. H. Ding and R. Ferraro, An Element-based Concurrent Partitioned for Unstructured Finite
Element Meshes, IPPS’96, 1996.
42. G. Mahinthakumar and F. Saied, A Hybrid MPI-OpenMP Implementation of An Implicit
Finite-Element Code on Parallel Architectures, the International Journal of High Performance
Computing Applications, Vol. 16, No. 4, 2002.
43. K. Nakajima, OpenMP/MPI Hybrid vs. Flat MPI On the Earth Simulator: Parallel Iterative
Solvers for Finite Element Method, ISHPC2003, LNCS 2858, 2003.
44. X. Wu, B. Duan and V. Taylor, Parallel Finite Element Earthquake Rupture Simulations on
Quad- and Hex-core Cray XT Systems, the 53rd Cray User Group Conference (CUG2011),
May 23–26, 2011, Fairbanks, Alaska.
45. The SCEC/USGS Spontanous Rupture Code Veriﬁcation Project, http://scecdata.usc.edu/cvws.
46. X. Wu and V. Taylor, Performance Characteristics of Hybrid MPI/OpenMP Implementations
of NAS Parallel Benchmarks SP and BT on Large-Scale Multicore Supercomputers, ACM
SIGMETRICS Performance Evaluation Review, Vol. 38, Issue 4, March 2011
47. V. Taylor, E. Schwabe, B. Holmer, and M. Hribar, Balancing Load versus Decreasing
Communication: Parameterizing the Tradeoff, Journal of Parallel and Distributed Computing,
Vol. 61, 567–580, 2001.
48. X. Wu, V. Taylor, C. Lively, and S. Sharkawi, Performance Analysis and Optimization
of Parallel Scientiﬁc Applications on CMP Clusters, Scalable Computing: Practice and
Experience, Vol. 10, No. 1, 2009.

Chapter 22
Data Intensive Computing: A Biomedical Case
Study in Gene Selection and Filtering
Michael Slavik, Xingquan Zhu, Imad Mahgoub, Taghi Khoshgoftaar,
and Ramaswamy Narayanan
1
Introduction
The rapid growth of the Internet and numerous networking technologies have led
to vast amounts of information which needs to be processed, analyzed, and utilized
for different purposes. For many applications, the volumes of the underlying data
can reach terabytes or event petabytes which requires the underlying data analyzing
framework to be highly parallel and effective in processing such large volume data.
Data intensive computing is commonly refereed to as the applications or approaches
which aim at processing such large volume data by using parallel computing
systems or architectures, such as distributed computing [5], cloud computing
The project described was supported by Award Number R01GM086707 from the National
Institute Of General Medical Sciences (NIGMS) at the National Institutes of Health (NIH). The
content is solely the responsibility of the authors and does not necessarily represent the ofﬁcial
views of NIGMS or NIH.
M. Slavik • I. Mahgoub • T. Khoshgoftaar
Department of Computer & Electrical Engineering and Computer Science,
Florida Atlantic University, Boca Raton, FL 33431, USA
e-mail: mslavik@fau.edu; imad@cse.fau.edu; taghi@cse.fau.edu
X. Zhu ()
Centre for Quantum Computation and Intelligent Systems, University of Technology,
Sydney, NSW 2007, Australia
e-mail: xqzhu@cse.fau.edu
R. Narayanan
Charles E. Schmidt College of Science, Florida Atlantic University,
Boca Raton, FL 33431, USA
e-mail: rnarayan@fau.edu
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 22, © Springer Science+Business Media, LLC 2011
563

564
M. Slavik et al.
etc. [1,2]. Among all these applications and domains, biomedical research [3,4] is
the ﬁeld which constantly pushes for effective data processing platform to analyze
large volume structured or unstructured data.
For example, microarray gene expression data are important sources of infor-
mation for many molecular biology and clinical studies such as genetic diseases
proﬁling [10], identifying potential biomarker signatures for cancers [11], and
gene regulatory network reconstruction [13]. Typical microarray experiments output
the express values for a large number of genes (e.g., more than 20,000) which
impose signiﬁcant challenges to any tools which intend to interpret the interactions
between genes or link the correlations between the genes and diseases. Most
machine learning methods, however, are known to be inaccurate or unstable for
high dimensional data, especially when the number of samples is relatively small.
Consequently, selecting a number of genes relevant to the tasks at hand has
recently received a lot of attentions [12, 16–18]. Numerous methods [21, 22] have
been proposed using computational methods such as Bayes errors [16], Random
Forest [17], and Receiver Operating Characteristics (ROC) [18] for gene selection.
Research also exists on how to determine the optimal number of genes for a given
microarray data set [15], determining proper sample size for microarray experiments
[19], theoretical analysis of the gene selection from microarray expression data [20],
and selecting informative genes for sample sets with biased distributions [7].
In order to select important genes, traditional approaches, from feature selection
perspective, roughly fall into the following two categories: ﬁlter and wrapper
approaches. In the ﬁlter approach, each gene is individually investigated based
on its relevance to the targets (e.g. diseased or normal tissues), and all genes are
ranked based on their relevance values. Typical ﬁltering approaches include ReliefF
[25], 2 [24], Information Gain [14], and many others. The genes selected from the
ﬁltering approach are considered important regardless of any classiﬁcation models.
On the other hand, the wrapper approach [27] employs a classiﬁcation model into
the selection process, so instead of producing a gene ranking list (like the ﬁlter
approaches do), it produces a gene subset which is regarded as the best gene set
with respect to the current classiﬁcation model. Typical wrapper selection starts
from one single gene (or the whole set of genes), then continuously adds (or drops)
genes and investigates the best set which can help build the most accurate models.
Because a wrapper approach faces a combinatorial search space, heuristics are
usually employed to speed up the search process. But even so, the wrapper methods
are far more expensive than the ﬁltering methods. In addition the gene subsets
selected from the wrapper approaches are customized to the given classiﬁer only.
For either ﬁlter or wrapper gene selection approaches, the selection process
is usually time consuming, especially for high dimensional large size data sets.
From data intensive computing perspective, one of the cheapest ways to speed up
computationally expensive algorithms is parallelizing them to execute on super-
computers. Cluster computers are becoming the primary means of supercomputing
due to its great and improving cost effectiveness. A cluster supercomputer usually
consists of a number nodes, each of which is typically a stand-alone computer with
independent memory, hard disk, and operating systems. The clusters are usually
connected in a high speed Local Area Network (LAN) environment based on the

22
A Biomedical Case Study in Gene Selection and Filtering
565
IP network technology, so all clusters can communicate with each others through
high speed switches with Gigabytes speeds. Designing algorithms to run in such a
distributed fashion can be challenging, given that the program instances are running
independently and communicate infrequently and at high cost. An application that
can be perfectly distributed will execute N times faster on an N node cluster than
on a single machine and is thus said to have linear speedup.
In this chapter, we report our recent research activities in designing effective
gene selection approaches for data intensive computing environments. We propose
two methods, PReliefFg and PReliefFp, to carry out parallel gene selection in an
independent and a cooperative manner, respectively [6]. Our results indicate that
both methods have close to linear speedup compared to the single machine based
PReliefF. The gene selection results from our methods are as good as or even
outperform the results of the PReliefF.
The remainder of the paper is structured as follow. Section 2 brieﬂy introduces
the ReliefF algorithm as a gene selection method. The parallel algorithms, PReliefFp
and PReliefFg, are introduced in Sect. 3. In Sect. 4, these two algorithms are
compared against the single machine based ReliefF method in terms of the runtime
performance and the quality of the selected genes on a cluster computer. The
conclusions and remarks are given in Sect. 5.
2
Gene Selection: An Introduction to ReliefF
Out of all 30,000 or more genes in human genome only a small subset of genes
are found to be responsible for causing cancer. As an example, scientists at the
University of Illinois at Chicago have found that there are about 57 genes involved in
breast cancer growth [8]. Another team of researchers at the University of Michigan
Comprehensive Cancer Center reports to ﬁnd 158 genes speciﬁc to pancreatic
cancer [9]. Their ﬁndings are thought to be the most accurate to date as they were
able to distinguish genes involved in pancreas cancer from those involved in chronic
inﬂammatory diseases like pancreatitis, a disease often mistaken for cancer. The
team later narrowed down the list of genes from 158 to 80 that were three times more
expressed in pancreatic cancer cells than in non-cancerous or pancreatitis cells.
When regarding genes as features for classifying or predicting diseases, gene
selection is equivalent to the feature selection which is fundamental to many
classiﬁcation problems in Data Mining and Machine Learning area. Intuitively, a
large number of irrelevant features may introduce noise into the data sets and may
even lead to wrong classiﬁcation of the target sample. Moreover, a large number
of potentially unnecessary features or genes make it difﬁcult for domain experts to
extract signiﬁcant and relevant information from the classiﬁcation model. It makes
senses to rely on effective techniques to select a small number of attributes or genes
to help build more accurate classiﬁcation model with less computing resources [7].
In Fig. 22.1, we list the main procedure of the ReliefF algorithm [25]. Given a
data set with m instances RŒi; i D 1;    ; m, each of which has a attributes (genes)

566
M. Slavik et al.
Fig. 22.1 ReliefF Algorithm
and a class label (the type of the sample e.g. diseased or normal tissue), the output
of the ReliefF is a ranking list of all genes with the most important genes ranked
on the top of the list. The main loop of the algorithm follows three steps for each
instance RŒi
1. Find the k nearest instances to RŒi in the same class as RŒi
2. Find the k nearest instances to RŒi in each other class
3. For each attribute, subtract from the weight the distance (relative to the attribute)
between RŒi and its closest neighbors in the same class, and add the distance
between RŒi and its closest neighbors in different classes
In order to calculate the difference between two instances with respect to a speciﬁc
gene A, ReliefF employs the following fomulars.
diff.A; I1; I2/ D
(
0
if value.A; I1/ D value.A; I2/
1
if value.A; I1/ ¤ value.A; I2/
(22.1)
for categorical attributes and
diff.A; I1; I2/ D jvalue.A; I1/  value.A; I2/j
maxfAg  minfAg
(22.2)
for numeric attributes. Here value.A; I/ is the value of gene A in instance I. R is the
vector of instances, each having a class class.RŒi/. P.C/ is the prior probability of
class C occurring, and W is the output vector of attribute weights.
The time complexity of the ReliefF is O.m2a/ which is quadratic with respect to
the number of instances. In this study, we consider gene expression data. A typical

22
A Biomedical Case Study in Gene Selection and Filtering
567
example data set in this class may have m D 500, c D 2, and a D 20; 000.
Such a data set has time complexity on the order of 10,000 million, which is
computationally expensive for even powerful computers. When gene selection must
be performed repeatedly, this can lead to unacceptable delays in calculating results.
Thus a need to expedite the algorithm is motivated.
3
Parallel Gene Selection Algorithms for Data Intensive
Computing Environments
In this section, we introduce two distributed variants of the ReliefF: PReliefFp and
PReliefFg, where the theme of the algorithms is to divide the calculation of the
ReliefF on a cluster computer composed of n nodes.
3.1
PReliefFp: Parallel ReliefF with Private Weighting
The ﬁrst variant, PReliefFp (Fig. 22.2), is the most straightforward subdivision of
work across the nodes in the cluster. In PReliefFp, the set of instances is split in to
n subsets, each with approximately equal size. Each node in the cluster processes
the instances in one subset, so each node maintains its local weight for all genes
based on the set of instances assigned to the current node. At the last stage, all the
private weight values are summed at a master node which gives the resulting weight
vectors for all genes. This algorithm is called the private weighting version (p) of
the algorithm because each node has only private weight values of the elements in
the weight vector during execution.
The time complexity analysis of PReliefFp is similar to ReliefF. In this case, the
main loop is executed m=n times on each node. Each node iterates the main loop
in parallel so the main loop requires O.a  c  m2=n/ steps. The ﬁnal step, summing
the partial weight vectors, is O.n  a/. Since in most cases n < m, we say the time
complexity of PReliefFp is
O
a  c  m2
n
C n  a

D O
a  c  m2
n
C n2  a
n

D O
a  c  m2
n

(22.3)
3.2
PReliefFg: Parallel ReliefF with Global Weighting
PReliefFg (Fig. 22.3) builds on PReliefFp by introducing a boosting process to
reduce the number of genes used to ﬁnd the distance between two instances. In
PReliefFg, the weight calculation for all genes are carried out in a cooperative
manner, where each node updates a global weight vector after processing each

568
M. Slavik et al.
Fig. 22.2 PReliefFp: Parallel gene selection with private weighting
instance, and further utilizes the global weight vector to fulﬁll the weight calculation
in the next round. Different from ReliefF and PReliefFp where all genes are used
to ﬁnd nearest neighoburs for each instance, as the algorithm progresses, PReliefFg
begins to exclude the least important genes in terms of weight when it calculates
the distance between instances. This boosting decreases the practical run time (but
not the time complexity) by reducing the number of steps involved in computing a
distance, and may also increase the quality of the genes selected from the algorithm.
This version is referred to as the global weight version (g) since each node has an
updated copy of the weight vector during execution.
The time complexity analysis of PReliefFg differs from PReliefFp in two places.
First, the main loop of PReliefFg has an implicit fourth step: selecting the l least
important features for exclusion. This step has O.a  log.a// complexity and is
executed once for each of the m=n iterations through the main loop. Second, in
PReliefFg, l (the number of attributes to exclude when computing a distance) ranges
from 0 to m during run time. Therefore the operations involved in computing
a distance between two instances is reduced on average from a in PReliefFp to
a  m=2 in PReliefFg. Thus the overall time complexity of PReliefFg is

22
A Biomedical Case Study in Gene Selection and Filtering
569
Fig. 22.3 PReliefFg: Parallel gene selection with global weighting
O
 
m 

a  m
2

 c  m C a  log.a/

n
!
(22.4)
The effect of this change has on the practical runtime will depend on the
relationship between a and m. If m  a, the ﬁrst term in the numerator reduces
(.a  m=2/  c  m  a  c  m) and the overall run time will be similar to PReliefFp
as long as log a is on the same order or smaller than c  m. If m  a, the ﬁrst term
in the numerator reduces to 0 and the overall complexity will become O.m=n/.
This will be signiﬁcantly faster than PReliefFp. When m D a, the time complexity
of PReliefFg is equivalent to PReliefFp, but in the limit it will run twice as fast
because the number of computations per distance calculations is cut in half.
When m > a, a small practical problem arises. l in this case will grow greater
than a, so in later stages of the algorithm no attributes will be considered in
distance calculations. This is undesirable, so the number of attributes excluded
should be capped at a certain proportion, for example 90%, of the total number of
attributes.

570
M. Slavik et al.
The ﬁnal point to consider is the communication overhead. In the case of
PReliefFp, the weight vector is gathered at the root node once, requiring O.n  a/
bytes to be transferred. In PReliefFg, the weight vector is sent and received once per
iteration of the main loop. The main loop is executed m times total (considering
all node), resulting in O.m  a/ bytes transferred. Since n is typically small in
comparison to m and a, this difference is signiﬁcant, similar to the difference
between linear and quadratic algorithms.
4
Experimental Results
4.1
Benchmark Gene Expression Data sets
Our testbed consists three gene expression data sets collected from different
resources, including the popular Kent Ridge Biomedical Data Set Repository [28].
The data characteristics of the benchmark data sets are reported in Table 22.1 (TIS is
a sequence data set for translation initialization site prediction, we include this data
set mainly for the purposes of demonstrating the algorithm performances on large
size high dimensional data). All benchmark data sets contain two types of examples,
i.e., positive and negative examples. Using these sets we evaluate the accuracy and
execution time performance of the new algorithms, in comparison with the single
machine based ReliefF method.
4.2
Data Intensive Computing Environments
The algorithms are implemented in C on top of the MPICH2 MPI library [26]. This
library provides a framework to easily build cluster-based applications. Resulting
programs are then executed on an IBM BladeCenter cluster computer consisting of
8 HS20 blade servers, each with Intel Xeon EM64T CPUs operating at 3.8 GHz
and 2 GB of RAM. Servers are linked with two 1 GB ethernet controllers. Inter-
node communication, handled in large part by the MPICH2 system, is built on an IP
network backbone.
Table 22.1 Data characteristics of the benchmark data sets
Name
# of Samples
# of Genes
AML-ALL Leukemia
72
7,130
Lung Cancer
203
12,601
Translation initialization site (TIS)
13,375
928

22
A Biomedical Case Study in Gene Selection and Filtering
571
4.3
Runtime Evaluation Results
The runtime performance of the new algorithms is evaluated on 1–8 node clusters.
Distributed programs are traditionally compared in terms of two metrics, speedup
and efﬁciency, deﬁned below in terms of T .n/, the running time on a cluster of n
nodes.
Speedup(n) D T .1/
T .n/
(22.5)
Efﬁciency(n) D Speedup(n)
n
D
T .1/
n  T .n/
(22.6)
Figures 22.4, 22.5 and 22.6 show the performance results. The left axis shows
the running time of the algorithm and the right axis shows the speedup, each plotted
vs the number of nodes in the cluster. Perfect speedup is achieved when the speedup
equals the number of nodes.
The execution times of the algorithms are summarized in Table 22.2. This shows
the execution time of the algorithms in seconds on a cluster with 8 nodes.
Efﬁciency gives a measure of how close to perfect speedup an algorithm gets.
Under normal circumstances, the maximum efﬁciency is 100% (efﬁciency greater
than 100%, or super-linear speedup, is possible but rare). Table 22.3 summarizes the
efﬁciency of the different algorithms.
In short, the results reported in this section suggest that
•
The data shows PReliefFp exhibits high efﬁciency across all data sets, with better
efﬁciency for larger data sets. This relationship is expected, since larger data sets
spend a larger proportion of their time in the main loop of the algorithm where
the gains from parallelization are made, rather than at serial tasks such as reading
the data set from disk and building results.
•
In the data sets where a  m (Leukemia and Lung Cancer), PReliefFg also
exhibits high efﬁciency, and PReliefFp is marginally faster than PReliefFg. This
result is predicted from the time complexity analysis.
•
In the data set where m  a (TIS), PReliefFg is much faster than PReliefFp.
This is also expected, since during most of the algorithm run time, only the top
weighted genes are used in computing distances between instances, so the main
loop executes much quicker. It is interesting to see that the efﬁciency of PReliefFg
is very low on this data set. This implies that non-parallelized overhead occupies
the majority of the processor time. Further investigation shows that importing
the data from disk uses about 6 s of the total time, and extrapolating the curve in
Fig. 22.6b shows about 630 s of other overhead will remain no matter how many
nodes are used. This 630 s represents the overhead resulting from data transfer
between nodes. The TIS data set has m D 928 and a D 13; 375. Each attribute
weight requires 8 B (double precision ﬂoating point) and each iteration causes 2
transfers of the weight vector, so a total of about 190 MB is transferred during run
time. The average data transfer rate then is about 300KB/s, which is reasonable
given the protocol overhead.

572
M. Slavik et al.
Fig. 22.4 Run time performance results on the Leukemia Data Set

22
A Biomedical Case Study in Gene Selection and Filtering
573
Fig. 22.5 Run time performance results on the Lung Cancer Data Set

574
M. Slavik et al.
Fig. 22.6 Run time performance results on the TIS Data Set
4.4
Accuracy Evaluation Results
For both PReliefFp and PReliefFg, each node carries out the gene selection on its
local instance subset, whereas the single machine based ReliefF actually performs
gene selection on the whole data set. Consequently, a possible concern is that

22
A Biomedical Case Study in Gene Selection and Filtering
575
Table 22.2 Average runtime
of new algorithms for n D 8
(seconds)
Data set
PReliefFp
PReliefFg
AML-ALL Leukemia
1.43
1.55
Lung cancer
12.6
15.6
TIS
2062.2
640.2
Table 22.3 Efﬁciency of new algorithms
PReliefFp
PReliefFp
Number of nodes
Leukemia
Lung cancer
TIS
Lung cancer
TIS
2
87.5
96.2
99.9
87.3
97.3
51.9
3
79.8
94.1
99.9
75.5
93.7
35.0
4
73.4
91.4
99.8
74.4
92.1
26.5
5
66.2
88.6
99.7
67.0
88.3
21.2
6
63.2
86.5
99.7
64.4
87.8
17.8
7
56.7
85.3
99.6
58.1
87.1
15.3
8
55.2
81.6
99.5
57.2
83.6
13.4
Average
68.9
89.1
99.7
69.1
90.0
25.86
whether the quality of the genes selected from PReliefFp and PReliefFg are of the
same quality or inferior to the ones selected from the ReliefF. For this purpose, we
verify the accuracies of the classiﬁers built from the genes selected from different
methods. More speciﬁcally, we ﬁlter the data sets to varying numbers of genes
using the selection algorithms and build classiﬁcation models with the resulting data
subsets. Two learning methods used in the study include Support Vector Machines
(SVM) and k nearest neighbors (k-NN). For all methods, the WEKA tool [23] is
used with default parameter settings.
In these experiments, the number of genes selected is varied from 10 to 240. After
that, the data set containing the selected genes is use to build the different classiﬁers
using the above four learning methods. Due to the small number of instances in
most data sets, fourfold cross-validation is used to test the model accuracy. Here the
accuracy is the percentage of instances that are correctly classiﬁed; the true positive
rate plus the true negative rate. This metric gives a good indication of the overall
accuracy of the method.
Figures 22.7, 22.8 and 22.9 show the accuracy of the classiﬁers across a range of
quantities of genes selected. The results in the Figs. 22.7, 22.8 and 22.9 assert that
•
PReliefFp is functionally equivalent to ReliefF. This is already known from the
algorithm analysis, but is veriﬁed here.
•
PReliefFg gives essentially equal accuracy to ReliefF when a  m (Leukemia
and Lung Cancer). This is predictable since the boosting process has only
minimal effect in this case.
•
PReliefFg gives marginally better performance when m  a (TIS). By itself
this result is not impressive, but recall that PReliefFg runs much faster on these
data sets. In fact, PReliefFg gives marginally better accuracy in this case while
requiring only 4% of the time required by ReleifF!

576
M. Slavik et al.
Fig. 22.7 Leukemia data set accuracy results
5
Conclusion
Genes expression data are now commonly used in the molecular biology and clinical
trial to link the correlations between the expression of certain types of genes and
diseases. While many tools exist in ﬁnding such correlations, they are generally
challenged by the large number of genes under their investigation, especially
considering that many genes present themselves in a random manner (or at least
the reasons of triggering those genes are yet to be found) or express in all types of
tissues (i.e. housekeeping genes). Selecting informative genes can eventually help

22
A Biomedical Case Study in Gene Selection and Filtering
577
Fig. 22.8 Lung cancer data set accuracy results
ﬁnd the genuine interactions between genes and further build enhanced prediction
models. Numerous research has shown that selecting a number of informative genes
can indeed help build models with better prediction accuracies than the ones trained
from the raw data. In this paper, we argued that although many approaches exist for
choosing informative genes, these methods, unfortunately, are not suitable for data

578
M. Slavik et al.
Fig. 22.9 TIS data set accuracy results
intensive computing environments and are incapable of handling large data sets,
where the algorithms may easily take hours before the users can see the results.
Consequently, we proposed two parallel gene selection approaches based on the
well known ReliefF feature selection method. Our design employed the master
and the worker architecture, and the master dispatches the data to the workers
to further carry out the selection process in an independent and a cooperative
manner. Experimental results from real-world microarray expression data and an
eight nodes cluster computers show that both versions, PReliefFp and PReliefFg,
linearly speedup with respect to the number of clusters, and the runtime performance
of our methods can be as less as 4% of the single machine based method. By using

22
A Biomedical Case Study in Gene Selection and Filtering
579
two typical classiﬁcation methods as learners, we also conﬁrmed that the models
trained from the genes selected from our method have the same or even better
accuracies than those selected from the original ReliefF method.
References
1. Moore, R., Baru, C., Marciano, R., Rajasekar, A., and Wan, M., Data-Intensive Computing, in,
The Grid: Blueprint for a New Computing Infrastructure, Foster, I., and C. Kesselman, Morgan
Kaufmann, San Francisco, 1999.
2. Rosenthal, A., Mork, P., Li, M., Stanford, J., Koester, D., and Reynolds, P., Cloud computing: A
new business paradigm for biomedical information sharing, Journal of Biomedical Informatics,
43(2):342–353, 2010.
3. Liora, X.: Data-Intensive Computing for Competent Genetic Algorithms: A Pilot Study
using Meandre, in Proceedings of the 11th Annual conference on Genetic and evolutionary
computation, GECCO, (2009).
4. Fox, G., Qiu, X., Beason, S., Choi, J., Ekanayake, J., Gunarathne, T., Rho, M., Tang, H.,
Devadasan, N., and Liu, G., Biomedical Case Studies in Data Intensive Computing, in
Proceedings of the 1st International Conference on Cloud Computing, CloudCom’09, (2009).
5. Zhu, X., Li, B., Wu, X., He, D., and Zhang, C., CLAP: Collaborative Pattern Mining for
Distributed Information Systems, Decision Support Systems, http://you.myipcn.org/science/
article/pii/S0167923611001102, (2011).
6. Slavik, M. and Zhu, X. and Mahgoub, I. and Shoaib, M.: Parallel Selection of Informative
Genes for Classiﬁcation, in Proceedings of the First International Conference on Bioinformat-
ics and Computational Biology (BICoB), New Orleans, April (2009).
7. Kamal, A., Gene Selection for Sample Sets with Biased Distributes, Master Thesis, Florida
Atlantic University, http://www.cse.fau.edu/Qxqzhu/students/akamal thesis 2009.pdf, (2009)
8. Researchers Pinpoint Genes Involved in Breast Cancer Growth, Cancer Celll, University of
Illinois
at
Chicago,
http://www.hopkinsbreastcenter.org/artemis/200308/feature6.html,
July 22, (2003).
9. Logsdon, C., Simeone, D., Binkley, C., Arumugam, T., Greenson, J., Giordano, T., Misek, D.,
and Hanash, S., Molecular proﬁling of pancreatic adenocarcinoma and chronic pancreatitis
identiﬁes multiple genes differentially regulated in pancreatic cancer, Cancer Research,
63:2649–2657, (2003).
10. Golub, T. et al.: Molecular classiﬁcation of cancer: class discovery and class prediction by gene
expression monitoring, Science, 286:531–537, (1999).
11. Xiong, M. et al.: Biomarker identiﬁcation by feature wrappers, Genome Research, 11:
1878–1887, (2001).
12. Baker, S. and Kramer, B.: Identifying genes that contribute most to good classiﬁcation in
microarrays, BMC Bioinformatics, 7:407, (2006).
13. Segal, E. et al.: Module networks: identifying regulatory modules and their condition-speciﬁc
regulators from gene expression data. Nature Genetics 34(2):166–176, 2003
14. Quinlan, J.: C4.5: Programs for Machine learning M. Kaufmann (1993)
15. Hua, J. et al.: Optimal number of features as a function of sample size for various classiﬁcation
rules. Bioinformatics, 21:1509–1515, (2005).
16. Zhan, J. and Deng, H., Gene selection for classiﬁcation of microarray data based on the Bayes
error, BMC Bioinformatics, 8:370, (2007).
17. Diaz, R. and Alvarez, S.: Gene selection and classiﬁcation of microarray data using random
forest, BMC Bioinformatics, 7:3, (2006).
18. Mamitsuka, H.: Selecting features in microarray classiﬁcation using ROC curves, Pattern
Recognition, 39:2393–2404, (2006).

580
M. Slavik et al.
19. Dobbin, K. et al.: How large a training set is needed to develop a classiﬁer for microarray data,
Clinical Cancer Research, 14(1), (2008).
20. Mukherjee, S. and Roberts, S.: A Theoretical Analysis of Gene Selection, Proc. of IEEE
Computer Society Bioinformatics Conference, 131–141, 2004.
21. Li T. et al., A comparative study of feature selection and multiclass classiﬁcation methods for
tissue classiﬁcation based on gene expression, Bioinformatics, 20:2429–2437, 2004
22. Statnikov A. et al., A comprehensive evaluation of multicategory classiﬁcation methods for
microarray gene expression cancer diagnosis. Bioinformatics, 21(5):631–643, 2005.
23. Witten, Frank, E.: Data Mining: Practical Machine Learning Tools and Techniques Morgan
Kaufmann (1999)
24. Plackett, R., Karl Pearson and the Chi-Squared Test. International Statistical Review, 51(1):
59–72, 1983
25. Robnik-ˇSikonja, Marko, Kononenko, Igor: Theoretical and Empirical Analysis of ReliefF and
RReliefF Mach. Learn., Vol. 53, 23–69 (2003)
26. Gropp, W. et al.: MPICH2 User’s Guide Avail: http://www.mcs.anl.gov/research/projects/
mpich2/index.php (2008)
27. Kohavi, R. and John, G, Wrappers for Feature Subset Selection, Artiﬁcial Intelligence,
97(1-2):273–324, 1997.
28. Kent Ridge Biomedical Data Set Repository, http://sdmc.i2r.a-star.edu.sg/rp/

Chapter 23
Design Space Exploration for Efﬁcient Data
Intensive Computing on SoCs
Rosilde Corvino, Abdoulaye Gamati´e, and Pierre Boulet
1
Introduction
In the last half century, the parallel computing has been used to model and solve
complex problems related to meteorology, physics phenomena, mathematics, etc.
Today, a large part of commercial applications uses parallel computing systems
to process large sets of data in sophisticated ways, e.g., web searching, ﬁnancial
modeling and medical imaging. A certain form of parallelism is simple to put in
practice and may consist in merely replicating overloaded key components or even
whole complex processors. But, the design of a well-balanced processing system
that is free from computing bottlenecks and provides high computing performance
with high area and power efﬁciency, is far more challenging [1].
Especially in embedded systems devoted to data intensive computing, the hard
design requirements on the provided computing performance and on hardware
efﬁciency make compulsory the usage of application-speciﬁc co-processors, usually
realized as hardware accelerators. The optimized design of these parallel hardware
accelerators involves many complex research issues and answers the need of
automatic design ﬂows addressing the complexity of an automatic design processes
allowing a fast and efﬁcient design space exploration for data intensive computing
systems. According to many existing works [2–4], such a ﬂow should reﬁne
one or more abstract speciﬁcations of an application and provide a systematic
exploration method that effectively selects optimized design solutions. For example,
the platform-based design methodology [4–6] abstracts the underlying hardware
R. Corvino ()
University of Technology Eindhoven, Den Dolech 2, 5612 AZ, Eindhoven, The Netherlands
e-mail: r.corvino@tue.nl
A. Gamati´e • P. Boulet
LIFL/CNRS and Inria, Parc Scientiﬁque de la Haute Borne, Park Plaza – Bˆatiment A,
40 avenue Halley, Villeneuve d’Ascq, France
e-mail: abdoulaye.gamatie@liﬂ.fr; pierre.boulte@liﬂ.fr
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 23, © Springer Science+Business Media, LLC 2011
581

582
R. Corvino et al.
architecture implementation by the means of Application Programmer Interface
(API). Typically, most of the existing methods [7] use, as starting point of the
exploration analysis, two abstract speciﬁcations: one giving the behavior to be
realized, i.e., the application functional speciﬁcation, and one giving the resources
used to realize the intended behavior, i.e., the architecture structural speciﬁcation.
The use of abstract input speciﬁcations makes solvable complex design pro-
cesses. However, by abstracting implementation details, several correlated problems
of the architecture exploration are orthogonalized, i.e., considered as independent.
This can signiﬁcantly decrease the quality of the design exploration and selection
results. A largely accepted orthogonalization for data intensive computing systems
consider apart (1) the design of data transfer and storage micro-architectures
and (2) the synthesis of simple hardwired computing cores or instruction sets
[8–10]. Actually, these two problems overlap. Thus, in order to have an efﬁcient
synthesis process, their orthogonalization should be reasonably chosen. Moreover,
the synthesis process should extract the constraints for one problem from the
solutions of the other and vice versa, so that the two problems mutually inﬂuence
each other. Consequently, in such a way, the effect of the orthogonalization on the
quality of selected design solutions is minimized.
The two orthogonalized problems are deﬁned as follows. The synthesis of
computational cores is mostly related to the instructions scheduling and binding
problems and is a very active topic of research [11], but usually does not consider
aspects related to data intensive computing behaviors. On the contrary, the design of
an efﬁcient data transfer and storage micro-architecture is a major and most complex
problem in designing hardware accelerators for data intensive applications. In fact,
the applicability of the data parallelism itself depends on the possibility to easily
distribute data on parallel computing resources. This problem is strictly related to the
computation and data partitioning, and the data parallelism exploration. Moreover,
the data transfer and storage micro-architecture affects the three most important
optimization criteria of a hardware accelerator design: power consumption, area
occupancy and temporal performance [9,10].
2
Related Works
We review some of the existing literature about the previously introduced research
challenges. We incrementally present these challenges by starting from the more
general context of design automation. Then, we present a design problem strictly
related to the data intensive computing systems, i.e., the data transfer and storage
micro-architecture design. Finally, we discuss challenges about application func-
tional speciﬁcation with the corresponding data partitioning and parallelization
problem. We also deal with challenges related to architecture structural speciﬁcation
with the associated resources usage scheduling problem. For each one of these
research challenges, we enumerate major issues and we brieﬂy introduce possible
solutions.

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
583
2.1
Design Automation
Traditionally, Electronic Design Automation (EDA) tools have supported the design
process from the Register Transfer Level (RTL) to the circuit layout. Since 1990s,
EDA has expanded to new tasks in order to support Electronic System Level
(ESL) design. The tasks that support the ESL design involve, among others,
the system level speciﬁcation and exploration and the behavioral synthesis. The
behavioral synthesis is also called High Level Synthesis and infers a RTL model
from a functional speciﬁcation of a system. Three main ESL design paradigms
are becoming increasingly accepted [2]: the function-based ESL methodology,
the architecture-based ESL methodology and the function/architecture codesign
methodology.
The function-based methodology [12] is based on Models of Computation
(MoCs), which specify in an abstract way how several computing tasks communi-
cate and execute in parallel. The MoC-based speciﬁcation can be directly translated
into an RTL model or into a compiled program. Consequently, in an optimized
methodology, the application-speciﬁc architecture (or architecture template) is
already known and automatically generated from a given MoC-based speciﬁcation.
The architecture-based ESL methodology uses hardware component models that
can be speciﬁed at different abstraction levels. Usually it classiﬁes the hardware
components into two categories the computing processors and the transactions,
i.e., communication between processors. The computing processors can represent
either programmable ﬂexible computer, whose functionality is completely speciﬁed
after the hardware implementation and in a traditional compilation phase, or
customizable hardware template whose behavior and structure can be speciﬁed at
design-time. The architecture-based ESL methodology has to solve the problem of
scheduling and binding applications on a given architecture. Even if this approach
allows architectural exploration, it is limited by the complexity of the input
speciﬁcations, usually in SystemC or Architecture Description Languages. Indeed,
the architectural exploration lacks of automation. Thus, it cannot be systematically
and extensively performed. Moreover, the ESL architecture exploration is often
based on HLS synthesis whose capability does not scale with the complexity of
analyzed systems and is usually limited to mono-processor systems.
The function/architecture codesign-based ESL methodology [7] is the most
ambitious methodology.It uses a MoC to specify the functional application behavior
and an abstract architectural model to specify the architecture of the system. It
reﬁnes concurrently and in a top-down manner these two models in order to achieve
the actual system realization. This approach has two limitations: ﬁrst, it always
constructs the architecture from scratch and does not take into account that in
most of industrial ﬂows, already existing platforms are used and possibly extended;
second, it does not exploit the fact that for a given application-domain optimal,
architectures or architecture templates are already known. For these reasons a meet-
in-the middle method is required [13] to reasonably bind and lead design space
exploration of application-speciﬁc systems.

584
R. Corvino et al.
Data intensive computing systems are typical systems that could beneﬁt from
such an ESL methodology. In this context the HLS would be used to synthesize the
single processors or hardware accelerators of the whole system; it would need an
ESL front-end exploration to orchestrate the system level data transfer and storage
mechanisms. One of the biggest challenge to provide such a competitive ESL front-
end exploration is to perform a design space exploration (DSE) able to ﬁnd in a short
time an optimal hardware realization of a target application. We can distinguish
two kinds of DSE approaches: exact and exhaustive approach and approximated
and heuristic-based approach. Ascia et al. [14] brieﬂy survey these approaches and
propose to mix them in order to reduce the exploration time and approximate the
optimal solution more precisely. In this chapter we present our methodology that is
aimed to provide an ESL exploration front-end for a HLS-based design ﬂow. The
used DSE is exhaustive and thus provides exact solutions but it includes intelligent
pruning mechanisms that dramatically decrease the exploration time.
2.2
Data Transfer and Storage Micro-Architecture Exploration
The problem of ﬁnding an efﬁcient system level data transfer and storage micro-
architecture has been largely studied in the literature and is related to several
research issues [8].
We classify these issues into two categories: issues concerning the refactoring of
functional speciﬁcations and issues concerning architecture design process. Usually
a functional behavior of an application is speciﬁed according to a sequential
(or partially parallel) computing model. Such a speciﬁcation may not expose
all the parallelism possibilities intrinsic to an application, i.e., possibilities that
are only limited by the data and computational dependencies of the application.
On the other side, the requirements of the architectural design usually constrain
the actual achievable parallelism level. For this reason, application refactoring
and architectural structure exploration are needed concurrently. These involve the
application task partitioning, the data partitioning, the selection of an appropriate
and efﬁcient memory hierarchy and communication structure of the ﬁnal system.
An efﬁcient memory hierarchy exploration includes an estimation of the storage
requirement and the choice of a hierarchy structure, with the corresponding data
mapping.
Balasa et al. [15] survey works on storage requirement estimation and conclude
that they are limited to the case of a single memory. One of the limit of parallelism
exploitation, especially in data intensive computing systems, is the usage of
shared memories requiring memory access arbitration that further delays inter-tasks
communications [10,16]. Advocate the development of distributed shared memories
to improve the performance of data intensive computing systems. Many studies exist
on methods to restructure a high-level speciﬁcation of an application in order to map
it onto a parallel architecture. These techniques are usually loop transformations

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
585
[17], which enhance data locality and allow parallelism. Data-parallel applications
are usually mapped onto systolic architectures [18], i.e., nets of interconnected
processors, in which data are streamed through in a rhythmic way.
Amar et al. [19] propose to map a high-level data parallel speciﬁcation onto a
Kahn Process Network (KPN), which is a network of processing nodes intercon-
nected by FIFO queues. We argue that KPNs are not suitable to take into account
the transfer of multidimensional data. The FIFO queues can be used only when
two communicating processors produce and consume data in the same order. In
the other cases, a local memory is necessary. When the FIFO queues are used, two
communicating processors can simultaneously access the queue to read or write
data. There is an exception when the ﬁrst processor attempts to write in a full
queue or the second processor attempts to read an empty queue. In these cases, the
processors have to stall. If two inter-depending processors stall, a deadlock occurs.
When a local memory is used, the risk of conﬂicts on a memory location imposes
that the processor producing and the processor consuming data execute at different
times. This creates a bottleneck in the pipeline execution, which can be avoided
by using a double buffering mechanism [20]. Such a mechanism consists in using
two buffers that can be accessed at the same time without conﬂict: the ﬁrst buffer
is written while the second one is read and vice versa. The works exploring the
hierarchy structure [21, 22] are usually limited to the usage of cache memories.
The caches are power and time consuming memories because of the fetch-on-miss
and cache prediction mechanisms. We argue that an internal memory with a pre-
calculated data fetching is more efﬁcient for application speciﬁc circuits. In addition,
Chen et al. [23] show that local memories with a “pre-execution pre-fetching” are a
suitable choice to hide the I/O latency in data intensive computing systems.
2.3
Application Functional Speciﬁcation
Our target applications are data-parallel and have a predictable behavior, i.e., static
number and address of data accesses, compile-time known number of iterations.
Examples of these applications are a large number of image processing, radar,
matrix computations. A well-adapted MoC for these applications is the multidimen-
sional synchronous data ﬂow (MDSDF) model that represents an application as a set
of repeated actors communicating by multidimensional FIFOs [24]. This MoC is an
abstraction of a loop-based code with static data accesses [25], compared to which
it has a higher expressivity to infer data parallelism. In MDSDFs, data dependencies
and rates are given, but the model lacks of information on processing and data
access latencies. For an efﬁcient exploration of possible hardware implementation
a temporal analysis is needed. A way to capture the temporal behavior of a system
is to associate abstract clocks to the system components. In particular for iterative
applications, such a clock can be a periodic binary word capturing the rhythm of a
event presence [26].

586
R. Corvino et al.
From 1970s, loop transformations, such as fusion, tiling, paving and unrolling
[27] are used to adapt a loop-based model to an architecture. Since ten years, they
are more and more used in loop-based high level synthesis tools or SDF-based
ESL frameworks, e.g., Daedalus [28], in order to optimize memory hierarchy and
parallelism [27, 29]. These ESL frameworks use the polyhedral model to repre-
sent loop-based application speciﬁcations and their corresponding transformations.
The polyhedral model allows only for afﬁne and disjoint data and computation
partitioning, while MDSDF-like models can allow the analysis of more complex
data manipulations, e.g. they handle sliding windows to minimize the number of
data accesses when the application uses superimposed data blocks. In [30], loop
transformations are adapted to the case of a MDSDF-like model. In our method, we
also use an MDSDF-like model, called Array-OL, and restructure the application
MoC-based speciﬁcation through refactoring equivalent to loop transformations.
This allows to enhance the data locality and the parallelism level exposed by the
application speciﬁcation and to explore possible optimized data and computation
partitioning
2.4
Architecture Structural Speciﬁcation
The growth of single processor performances has reached a limit. Nowadays,
dealing with parallelism is one of the major challenges in computer design.
In domain-speciﬁc computer, used parallelization techniques have been mostly
based on Message Passing Interface. The design of such a complex computing
architecture requires a speciﬁc abstract model and analysis allowing to address the
complexity of an application mapping. Until now, no method seems to provide
a widely applicable solution to map applications on wide arrays of processors
[31]. For predictable applications, a possible well adapted platform is a tile-based
multiprocessor system whose abstract model, referred to as template, is described in
[6]. This abstract model involves processing tiles communicating through a Network
on Chip (NoC). Each tile contains local memories (for an inter-tiles communication
based on distributed shared memories) and computing elements. It is able to perform
computing task, local memory access arbitration and management of data transfer
on the NoC.
Using a comparable underlying tile-based platform in our method, we propose a
different abstract model using hardware optimizations to improve the performance
of data intensive computing system. Our architecture abstract model involves close
and remote communications that can be mapped on NoCs, point-to-point or on
bus-based communications. The communications are realized through a double
buffering mechanism, i.e., two buffers are used in an alternate manner to store data
of a producing or a consuming processor. This makes it possible, for each processor
to perform in parallel the data access and the computation. Such a mechanism
dramatically enhances the system computing capabilities and avoids the data access
bottleneck that is the main cause of computing performance decrease, especially

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
587
in data intensive computing systems. The actual realization of such a processor has
been studied in [10] and implemented through a HLS-based ﬂow [32]. The proposed
method uses a static scheduling to map the application tasks onto the hardware
architecture. Consequently, this reduces the complexity of data transfer controller
and the corresponding area overhead in the implemented hardware.
3
Overview of Our Method
Our method provides an exact DSE approach to map data parallel applications onto
a speciﬁc hardware accelerator. The analysis starts from a high level speciﬁcation
deﬁned in the Array-OL language [33]. The method is based on a customizable
architecture including parallel processors and distributed local memories used
to hide the data transfer latency. The target application is transformed in order
to enhance its data parallelism through data partitioning and the architecture is
customized exploring different application-speciﬁc customizations. The blocks of
manipulated data are streamed into the architecture. For the implementation of data
intensive computing systems, several parallelism levels are possible:
•
Inter-task parallelism realized through a systolic processing of blocks of data
•
Parallelism between the data-access and the computation realized through the
double buffering mechanism
•
Data parallelism in a single task realized through pipelining the processing of
data stream or through the instantiation of parallel hardware resources
The architecture parallelism level and the size of data blocks streamed through
the architecture are chosen in order to hide the latency of data transfers with the
computing time. Figure 23.1 presents the general ﬂow of our methodology. The
inputs are a MoC-based speciﬁcation of the application and a library of abstract
conﬁgurable components, namely processing elements and connections. The ESL
design consists in exploring different refactoring transformations of the speciﬁcation
and possible data transfer and storage micro-architectures for the hardware system.
Mapping constraints put in relation a given refactored application speciﬁcation and
a micro-architecture customization. Indeed, the micro-architecture conﬁgurations
are inferred from a static resource-based scheduling of the application tasks. This
scheduling has to meet the mapping constraints that are due to the architecture
properties and are aimed to implement optimized data intensive computing systems.
This method can be considered as a meet-in-the middle approach because many
hardware optimizations for data intensive computing applications are already
included into the architecture template. The data transfer and storage conﬁguration
of the template is inferred from the analysis and refactoring of the application
speciﬁcation.
To summarize, the proposed method uses a tiling of task repetitions to model the
partitioning and mapping of the application tasks into a multiprocessor system. A
large space of mapping solutions is automatically explored and efﬁciently pruned

588
R. Corvino et al.
CU
CU
CTRL
Mapping
Refactoring
Customization
System Level Exploration
MoC−based
Specification
Abstract components
Library
Code Generation
ASIP
synthesis
HLS
Estimation
CTRL
CU
CU
CU
CTRL
CU
CTRL
Fig. 23.1 Overview of the proposed method
to the pareto solutions of the explored space. As shown in Fig. 23.1, one or more
pareto mapping solutions can be realized through an HLS-based ﬂow or through an
ASIP customization ﬂow. In this case a back-end code generator has to be added to
the proposed method to ensure the interface with the HLS or ASIP customization.
In [32] a code generator is presented, which builds a loop-based C-code from a
customization of the abstract model of a single processor. The generated C-code is
used as an optimized input of an HLS-based design ﬂow.
To illustrate our approach we refer to an application called low-pass spatial (LPS)
ﬁlter. LPS ﬁlter is used to eliminate the high spatial frequency in the retina model
[34, 35]. It is composed of four inter-dependent ﬁlters as shown in Fig. 23.2. Each
ﬁlter performs a low pass ﬁltering according to a given direction. For example,
HFLR computes the value of the pixel of coordinates .i; j/ at instant t D 1, by
computing the pondered sum of the pixel .i; j  1/ at instant t D 1 and the pixel
.i; j/ at a previous instant t D 0.
3.1
Outline
The rest of this Chapter is organized as follows: in Sect. 4 of this paper, we
describe the Array-OL formalism for the application speciﬁcation and its associated

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
589
Fig. 23.2 Speciﬁcation of a Low Pass Spatial Filter: HFLR, VFTD, HFRL and VFBU respectively
stand for Horizontal Filter Left Right, Vertical Filter Top Down, HF Right Left and VF Bottom Up.
pt.i; j / is a pixel of coordinates .i; j / at an instant t and a is the ﬁltering factor
refactoring transformations. Then, in Sect. 5, we propose an abstract architecture
model for data parallel applications. In Sect. 6, we deﬁne a corresponding mapping
model based on static scheduling of the application tasks. In Sect. 7, we deﬁne a
method to systematically apply a set of Array-OL refactoring transformations in
order to optimize the memory hierarchy and the communication sub-system for a
target application. In Sect. 8, we deﬁne the used DSE approach and we illustrate
the method on the low pass spatial ﬁlter example. In Sect. 9, we show a detailed
example in which our method is applied. Finally, in Sect. 10, we give a summary of
our proposition.
4
High-level Design of Data Parallel Applications
with Array-OL
Array-OL (Array-Oriented Language) is a formalism able to represent data intensive
applications as a pipeline of tasks applied on multidimensional data arrays [33].
The analyzed applications must have afﬁne array references and a compilation-
time known behavior. An Array-OL representation of the LPSF is given as in
Fig. 23.3. Each ﬁlter has an inter-repetition dependency and inter-task dependencies,
i.e., the result of a task repetition is either used to compute the next task repetition
or passed to the following task.
In an Array-OL representation we distinguish three kinds of tasks: elementary,
compound (or composed) and repetitive tasks. An elementary task, e.g., image
generator in Fig. 23.3, is an atomic black box taken from a library and it cannot be
decomposed in simpler tasks. A compound task, e.g., LPSF ﬁlter in Fig. 23.3, can be
decomposed in simpler interconnected task hierarchy. A repetitive task, e.g., HFLR
ﬁlter in Fig. 23.3, speciﬁes how a task is repeated on different subsets of data: data
parallelism. In an Array-OL representation, no information is given on the hardware
realization of tasks. Figure 23.4 shows a detailed Array-OL representation for the
HFLR ﬁlter.

590
R. Corvino et al.
Image
display
Image
generator
HFLR
VFTD
HFRL
VFBU
input port
output port
LPSF
Fig. 23.3 Sketch of an Array-OL speciﬁcation for the LPSF ﬁlter
d=<1,0>
R1 < 480; 1080; ∞ >
< 1920; 1080; ∞ >
T1
T2
P=
⎛
⎝
4 0 0
0 1 0
0 0 1
⎞
⎠
<1920;1080;∞>
O=
⎛
⎝
0
0
0
⎞
⎠
F=
⎛
⎝
1
0
0
⎞
⎠
F =
⎛
⎝
1
0
0
⎞
⎠
O =
⎛
⎝
0
0
0
⎞
⎠
P =
⎛
⎝
4 0 0
0 1 0
0 0 1
⎞
⎠
sp =< 4 >
sp =< 4 >
HFLR
Fig. 23.4 Array-OL speciﬁcation of a HFLR ﬁlter. The ﬁlter executes 4801080 repetitions (R1)
per frame of 1920  1080 pixels. The task repetitions are executed on an inﬁnite ﬂow of frames.
Temporal and spatial dimensions are all array dimensions without any distinction with each other.
But usually the inﬁnite dimension is dedicated to the time. The tilers (T1 and T2) describe the
shape, the size and the parsing order of the patterns that partition the processed frames. The pattern
sp, that tiles the input data and is described by T1, is monodimensional. It contains four pixels and
lies along the horizontal-spatial dimension. The vector d is an inter-repetition dependency vector
The information on the array tiling is given by: the origin vector O, the ﬁtting
matrix F and the paving matrix P. The origin vector speciﬁes the coordinates of
the ﬁrst datum to be processed. The ﬁtting matrix F says how to parse each data
pattern and the paving matrix P says how the chosen pattern covers the data array.
The size of the pattern is denoted by sp. Array-OL is a single data assignment and a
deterministic language, i.e., each datum can be written only once and any possible
scheduling, which respects the data dependencies, produces the same result. For this
reason, an application described in Array-OL can be statically scheduled. A crucial
problem for an Array-OL description is to deﬁne the optimal granularity of the
repetitions, i.e., ﬁnding an optimal data paving in order to optimize the target
implementation architecture.
Glitia et al. [36] describe several Array-OL transformations. In our work, we use
the fusion, the change paving and the tiling. These transformations are respectively
equivalent to the loop fusion [37], loop unrolling [38] and loop tiling [39, 40].
One of the biggest problem for a competitive exploration is to deﬁne an optimal
composition of these transformations. In fact, by composing them we can optimize
the following criteria: the calculation overhead, the hierarchy complexity, the size

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
591
of the intermediate array and the parallelism level. We propose to compose them in
order to ﬁnd a efﬁcient architecture to mask the data access and transfer latencies.
5
A Target Customizable Architecture
We propose a synchronous architectural model (Fig. 23.5) to realize data parallel
applications. This model can be customized with respect to the data transfer, the
data storage and the computing process to be accomplished. All the data transfers
are synchronized and managed by FIFO queues and the task execution depends on
the presence of data in the FIFO queues. The data transferred by the FIFOs are stored
CU
LM
LM
CTRL
EXT MEM
CTRL
P1
P4
P6
P5
P2
P3
CTRL
CU
CTRL
CTRL
CTRL
CU
CTRL
CU
CU
CU
CU
CU
CTRL
CU
a
b
EXT. MEM
CU
CU
CU
Fig. 23.5 Target customizable architecture. (a) Abstract model. (b) Detailed model: customizable
sub-system for data transfer and storage

592
R. Corvino et al.
into local memories and the access to these memories is managed by a local memory
controller. At a higher abstraction level (Fig. 23.5a), the architecture is a mesh of
communicating processors Pk with an external memory (EXT. MEM) interface and
a global controller (CTRL). The global controller starts the communication with the
external memory by injecting data into the mesh of processors. It also synchronizes
the external memory accesses. As shown in Fig. 23.5b, each single processor has
essentially three customizable parts: a local memory controller (CTRL), a set of
buffers (Local Memory LM ) and a computing unit (CU ). The behavior of such
a processor has been tested at the RTL level in [32] and can be summarized as
follows. The local memories are based on double buffer mechanism so that the local
memory controller and the computing unit can execute their tasks in parallel. The
local memory controller has to perform two main actions: (1) managing the double
buffering mechanism (2) compute the local memory address to load data from the
external memory and to read and transfer data to the computing units. A computing
unit realizes an application-speciﬁc computing task.
Two processors may communicate through a stand-alone FIFO queue or through
a local memory. In the former case, the local memory controller and the buffers
are not instantiated. In the latter case, the buffers and the local memory controller
are instantiated. Each buffer is a single port memory because the realization of
multiport memories is not mature yet and it is subject to many technological
problems [41]. For this reason, a processor receiving more data ﬂows includes
several distinct buffers, one for each communication ﬂow. Consequently, all data
transfers concerning different data ﬂows can be realized in parallel with different
local buffers and without any conﬂict. For a given application, the data to be
processed are ﬁrst stored into the global shared external memory. Then, after that
the global controller starts the communication with the external memory , the data
are streamed per groups of parallel blocks into the architecture. The size of data
blocks is chosen, on the one hand, in order to mask the data access latency (thanks
to the double buffering) and on the other hand, in order to respect the constraints on
the external memory bandwidth.
In the proposed abstract model of the application architecture several features can
be customized through an application-driven design process. These features are:
1. The communication structure of the whole system, i.e., decide if two pro-
cessors communicate through a distributed shared local memory or through a
global external memory. The local communication speeds up the processing
temporal performance but requires the instantiation of additional local memories.
An exploration is needed to establish which kind of communication is the
most proﬁtable for the analyzed application, according to the data and task
dependencies intrinsic to the application.
2. The number of computing units in a single processor. A single processor
realizes a given repetitive task. Having more computing units allows to improve
the data parallelism level of the system and to treat in parallel different blocks of
data from the same input data set.

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
593
3. The number and size of the local memories. This feature ensures the realization
of data parallelism. Indeed, by exploring different data partitioning of the
manipulated data, the method can infer the sizes and number of needed local
memories.
4. The functional behavior of a computing unit. A computing unit realizes the
core instructions of an application task and it has to be speciﬁed by the user of
the design methodology.
6
A Static Temporal Model for Data Transfer and Storage
Micro-Architecture Exploration
In the following section we specify the temporal model used to estimate the
static scheduling and mapping of the application tasks onto a customized system
architecture. We present this model progressively giving details on the ﬁnal system.
First, in Sect. 6.1, we present the static scheduling of a single processor with respect
to its computing task. Then, in Sects. 6.2 and 6.3, we present the static scheduling
of communicating processors, with respect to their data transfer and storage tasks.
For each one of the presented scheduling we will give a sketch of the underlying
conﬁgurable hardware architecture, a generic execution timeline of the application
tasks running on the conﬁgurable hardware architecture and the associated mapping
constraints. The mapping constraints ensure the correctness of system computed
response, they are based on the use of optimized hardware mechanisms for data
intensive computing and allow to explore possible hardware conﬁgurations and to
select those that are best ﬁts for the analyzed application.
6.1
Temporal Behavior of a Single Processor
and Its Computing Time
A given processor is allocated to the execution of a repeated task in an application.
For such a task, its core part, i.e., the set of instructions encompassed by the
innermost task repetition, is realized by the computing unit inside the dedicated
processor. This computing unit is pipelined and can be either an application speciﬁc
hardwired data-path or an application speciﬁc instruction set.
As shown by Fig. 23.6, the iterations of a task can be pipelined on the same
hardware, but in order to avoid conﬂicts on stream or memory accesses, a minimum
interval of time, called Initiation Interval – II, has to pass between the beginning of
two successive iterations [42].
The time to access data grows with the increase of the number of iterations
while the datapath latency remains constant. If the number of pipelined iterations

594
R. Corvino et al.
Output Computation
Time to access the input
for a single computation
ITN
IT4
IT3
IT2
IT1
Time to access data
data path latency
LM or FIFO access
Computing
output FIFO transfer
Time
Fig. 23.6 A pipeline of iterations (IT ) during which two sub-tasks are realized: access to a Local
Memory (LMj ) and output computing. If the number of iterations N is very high, the datapath
latency can be neglected w.r.t. the time spent to access data
is sufﬁciently high, the datapath latency can be neglected with respect to the time
spent to access data. This simpliﬁcation leads to a transaction-based model, with a
reasonably high precision on the data transfer latencies and a lower precision on the
execution time of a computing unit. Finally, the computing time of a processor is
deﬁned as follows:
Deﬁnition 1. Let ck
x be the Initiation Interval of a processor Pk. The time to
compute a set of data xj on a computing unit of the processor Pk is: tk
com.xj / D
cL
x xj :
As shown by Fig. 23.6, the Initiation Interval of a new task repetition depends on
the latency of the data transfer either on the input or on the output FIFO queues of
the repeated task. The FIFO latency giving the worst case time response is taken as
Initiation Interval of the repeated task [42].
6.2
Data Transfer and Storage Model of Communicating
Processors
As shown in Fig. 23.7a, a processor can have input data blocks coming from the
external memory and data blocks coming from neighbor or own local memories.
The transfer of data blocks coming from the external memory are always sequen-
tialized to avoid possible conﬂict on concurrent accesses to the external memory.

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
595
...
Data transfer
from external
memory
Data transfer
from local
memories
a
b
xi
Pk
Time
sync
Data pre-fetch j
Output Computation i
Fig. 23.7 The double buffering mechanism allows to mask the time to fetch with the time to
compute. (a) Sketch of a processor with its input and output data ﬂows. (b) Timeline for sequential
groups of iterations on a single processor. The processor has multiple input ﬂows (j ) and a single
output ﬂow (i).
The transfers of data blocks coming from local memories are realized in parallel.
Indeed, when a processor receives different data blocks from uncorrelated data
sets, it allocates a dedicated local double buffer to store each transferred data
block. To summarize, each processor of the proposed architecture can store several
input data blocks, cumulatively called xj , but produce a single output data block,
called xi.
Thanks to the double buffering mechanism, each processor fetches the data
needed to compute the next group of iterations while it computes the current group
of iterations. One of the aim of the proposed method is to choose a data block size
that allows to mask the time to fetch with the time to compute. As a result, such a
block size must respect the following mapping constrain:
max
j ftfetch.xj/g  tcom.xi/:
(23.1)
As illustrated in Fig. 23.7b, the execution timeline of a processor have two
parallel contributions: the time to pre-fetch and the time to compute. The duration of
the longest of these contributions synchronizes the beginning of the computations of
a new set of application task repetitions. The time to pre-fetch a set of data depends
on the communication type, i.e., when the processor receives data from the external
memory or from another processor. We propose a model for each of these transfer
types.
6.2.1
Data Transfer with External Memory
The external memory is accessed in a burst mode [43], i.e., there is a latency before
accessing the ﬁrst datum of a burst, than a new datum of the burst can be accessed
at each processor cycle. A whole data partition is contained in a memory burst and
the access to a whole data partition is an atomic operation. It is possible to store m

596
R. Corvino et al.
data per word of the external memory (for example we can have four pixels of eight
bits per each memory word of 32 bits). Under the above hypotheses, we propose the
following deﬁnition:
Deﬁnition 2. Given L and m denoting respectively the latency before accessing a
burst and the number of data per external memory word, the time to fetch a set of
data xj from an external memory is:
tfetch.xj / D L.j/ C xj
m
with xj ; m 2 N:
The latency L.j/ has three contributions: the latency due to the other data
transfers between the target architecture and the external memory, the latency due
to the burst address decode and the latency due to other data transfers which are
external to the target architecture (these two last contributions are indicated as Lm).
In the worst case, this latency is:
L.j/ D Lm C
X
z¤j
n
Lm C xz
m
o
(23.2)
with xz 2 XMem and XMem being respectively the set of all the data transfers (in
input and output) between the given architecture and the external memory. Hence,
we have:
tfetch.xj / D NkLm C 1
mVect.1/XMem:
(23.3)
where Nk is the number of data blocks exchanged between the target architecture
and the external memory. The expression Vect.1/ indicates a line vector of coordi-
nates 1.
6.2.2
Transfer Between Two Processors Through the Local Memories
In the proposed architecture, the data computed by a processor are immediately
fetched into a second processor, in a dedicated buffer. Thus, the time to fetch a set
of data xj into a processor Pk corresponds to the time to compute a set of data
xj by a processor Pl. Consequently, the latency of the data transfer between two
processors is deﬁned as follows:
Deﬁnition 3. Given a couple of producing and consuming processors that com-
municate through a distributed shared local memory LMj and exchange data
blocks of size xj The time that a consuming processor employs to pre-fetch xj
is equal to the time that the producing processor employs to compute xj ., i.e.
tconsumer
fetch
.xj / D tproducer
com
:

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
597
External memory bus
x0
3R
Pl
Pk
xj
xi
6R
1R
3R
II=3
R: number of input data read to compute a single output
II: Initiation Interval
II=6
xd
Fig. 23.8 A processor receiving three input data blocks and producing a single output data block
Following is an example applying the mapping constraint (23.1) and Deﬁnitions
1, 2 and 3.
Example 1. Let Pk be a processor, as presented in Fig. 23.8. It has three input ﬂows:
one from the external memory, one from a processor Pl and another due to an inter-
repetition. To avoid a deadlock, all the data needed by an inter-repetition are stored
in internal memories without any latency. From Deﬁnitions 1 and 2, the time to fetch
is tfetch D maxfL C x0
m ; 3xjg and the time to compute is tcom D 6xi. Thanks to the
access afﬁnity we know that, x0 D 6xi and xj D 3xi. By applying the mapping
constraint (23.1), we have that either L C 6xi
m  6xi, possible only if m > 1 or
9xi > 6xi, which is impossible. In this last case, the usage of data parallelism can
mask the time to fetch.
6.2.3
Scaling the Parallelism Level
The method, as it has been presented until now, takes into account two levels
of parallelism: (1) the parallelism between data access and computation (2) the
parallelism due to the pipeline of iterations on the same processor. It is possible to
further scale the parallelism level, by computing in parallel independent groups of
data. We indicate with Ncuk the number of parallel computation units of a processor
Pk, which is able to compute Ncuk blocks of data in parallel. To apply the data
parallelism, we distinguish between the processors communicating with the external
memory and the processors communicating with each other. For the processors
communicating with the external memory the number of parallel computation units
Ncuk is limited by the memory bandwidth. When a processor writes a data ﬂow into

598
R. Corvino et al.
the external memory, its number of parallel computation units Ncuk is limited to m
the number of data per memory word:
Ncuk < m:
(23.4)
When a processor receives an input data ﬂow from the external memory, all the
data are sequentially streamed on a unique input FIFO queue. It is possible to
duplicate the internal memories and the computation units, but the time to fetch
the input data is multiplied by the number of parallel computation units Ncuk, while
the computation time will be divided by a factor Ncuk:
Ncuk

L C xj
m

< cxkxi
Ncuk
:
(23.5)
For the processor communicating with each other the parallelism level reduces
both the time to fetch and the time to compute. The inequality (23.1) becomes
Cxl xj
Ncul

Cxk xi
Ncuk . Thanks to the afﬁnity of the array accesses, i.e., xj D kij xi with
kij 2 N, we can infer a constraint on the parallelism level of two communicating
processors:
Ncuk
Cxk
 Ncul
Cxlkij
:
(23.6)
6.3
Temporal Behavior of the Whole Architecture
The proposed method uses a load balancing criterion to execute all the network
communications and applies a periodic global synchronization of the task repeti-
tions. The number of instantiated local memories and computation units is inferred
in order to maximize the load balancing between the communicating processors.
To exemplify the temporal behavior in the whole architecture we describe two
kinds of communications: the communication between processors having the same
parallelism level Fig. 23.9a and the communication between processors having a
different parallelism level (see Fig. 23.9b, c).
The temporal timelines of these kinds of communication are given in Fig. 23.10.
If the two processors have the same parallelism level, their computing units have
a comparable execution time. If the two processors have a different parallelism
level, their computation units with a lower parallelism level are faster then those
of processors with a higher parallelism level. In the current analysis we suppose that
the whole system has a single clock frequency. Furthermore, the execution time of
a computation unit depends on the number of data accessed by the unit itself. As a
result, a faster computation unit accesses a lower number of data. Another solution
could be to use different clock frequencies for each processor. The different clock

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
599
CTRL
CTRL
cmd
cmd
CU1
CU2
CU3
CU4
CU1
CU2
CU3
CU4
CU1
CU1
CU2
CU2
CU3
CU4
CTRL
cmd
cmd
CU2
CU1
CU2
CU1
CU4
CU3
CTRLMUX
CTRL
cmd
cmd
CTRL DeMUX
a
b
c
Fig. 23.9 Three kinds of communications. (a) Two processors having the same parallelism level
use a Point to point communication controller. (b) A processor having a higher parallelism level
passes its output to the neighbor by a multiplexing communication controller. (c) A processor hav-
ing a lower parallelism level passes its output to the neighbor by a demultiplexing communication
controller
cycles would be multiple of an equivalent clock cycles and the multiplicity factors
would be used to correct the formula in order to determine the execution time, and
as a consequence, the inferred parallelism level.

600
R. Corvino et al.
time (cycles)
sets of ITs computed in parallel
sets of ITs computed in parallel
sets of ITs computed in parallel
CU1
CU2
CU3
CU4
CU1
CU2
CU1
CU2
CU1
CU2
CU3
CU4
CU1
CU2
CU3
CU4
CU1
CU2
CU3
CU4
nodei
nodej
nodei
nodei
nodej
nodej
time (cycles)
time (cycles)
a
b
c
Fig. 23.10 Example of a communication timelines. (a) Case of two communicating nodes having
the same parallelism level. Usage of a CTRL PtoP. (b) Case of two communicating nodes. The
ﬁrst node has a higher parallelism level. Usage of a CTRL MUX. (c) Case of two communicating
nodes. The ﬁrst node has a lower parallelism level. Usage of a CTRL DeMUX
7
Implementation of Our Methodology
We generalize the mapping constraint (23.1) and Deﬁnitions 1, 2 and 3 to the whole
processor network, by taking into account the processors interconnections. For that,
we consider the following deﬁnitions presented in a progressive way.
Deﬁnition 4. Given a network of processors, a column-vector X of possible data
block sizes produced or consumed by the processors in the network is: X D
.x0; : : : ; xn1/ where xi is a data block produced or consumed by Pk, 8k 2
Œ0; n  1.
The vector X is associated with two vectors Xout and Xin. The coordinates of Xin
(respectively Xout) are either 0 or those of X at the same position and corresponding
to the input (respectively output) data blocks in the network. A coordinate xj of Xin
equals kij xi, where xi 2 Xout and kij 2 N. The data blocks of size xj can be
received either from the external or from another processor of the network.

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
601
In the following deﬁnitions, we give the relation between Xin and Xout and
we distinguish the case when the input data blocks are received from the external
memory.
Deﬁnition 5. Let XMem
in
be a vector of possible sizes for the data blocks read from
the external memory. The matrices Kı and KMem
ı
giving the mapping between the
sizes of input and output data blocks are:
Xin D KıXout
for all the input communications
XMem
in
D KMem
ı
Xout
for the communications from the external memory.
where Xout and Xin are vectors of possible sizes for respectively output and input
data blocks.
An element ıij of Kı (or KMem
ı
) is deﬁned as follows:
ıij D
 kij if xj D kij xi and xj 2 Xin. or respectively xj 2 XMem
in
/
0 otherwise.
Deﬁnition 6. Let XMem
out
and XCom
out
be two vectors of possible sizes of data blocks
respectively written into the external memory and exchanged by the processors. We
deﬁne two matrices I Mem
ı
and I Com
ı
so that:
XMem
out
D I Mem
ı
Xout
XCom
out
D I Com
ı
KıXout
where Kı gives the mapping between the sizes of input and output data blocks for
all the input communications. An element ıij of I Mem
ı
(or I Com
ı
) is:
ıij D
 1 if xj 2 XMem
out
. or respectively xj 2 Xin n XMem
in
/
0 otherwise.
Deﬁnition 7. Given a processor Pk, a column vector Cx giving the Initiation Inter-
val per processor, is Cx D fcxk W cxk is the Initiation Interval of a processor Pkg:
Deﬁnition 8. Given a processor Pk, a column vector Ncu giving the number of
parallel computation units per processor, is
Ncu D fNcuk W Ncuk is the number of parallel computation units in Pkg
From the above deﬁnitions, we infer the mapping criteria to mask the data access
latency for the input, output and internal communications of the target architecture.
Mapping Criterion 1.
Input communication from the external memory. Let be
XMem D XMem
in
C XMem
out
D .KMem
ı
C I Mem
ı
/Xout and Diag.Ncu/ a diagonal matrix
whose diagonal elements are the coordinates of vector Ncu. It is possible to write the

602
R. Corvino et al.
Eq. 23.3 as follows: tfetch.XMem/DDiag.Ncu/Vect.1/T .NkLmC
Vect.1/.KMem
ı
CI Mem
ı
/Xout
m
/
and
Deﬁnition 1. as follows: tcom.XMem
in
/D
Diag.Cx/KMem
ı
Xout
Diag.Ncu/
. The substitution of tfetch.XMem/
and tcom.XMem
in
/ in mapping constraint (23.1), gives:

Diag.Cx/KMem
ı
Diag.Ncu/Diag.Ncu/Vect.1/T Vect.1/.KMem
ı
C I Mem
ı
/
m

Xout  Vect.1/T NkLm:
Mapping Criterion 2. Output communication to the external memory. Let N T
cuI Mem
ı
be the number of parallel computing units of the processors communicating with the
external memory. From inequality (23.4), we infer:
Diag.Ncu/I Mem
ı
 m:
Mapping Criterion 3. Given XCom
out
of Deﬁnition 6. Let Ii and Ij be two squared
matrices so that the left multiplication IjI Com
ı
selects the j th line of I Com
ı
. From
inequality (23.6), we infer:
8i; j
IjDiag.Cx/I Com
ı
KıDiag.N 1
cu /  IiDiag.Cx/Diag.N 1
cu /:
The above mapping criteria form a system of inequalities whose variables are the
parallelism level Ncu and the size of the transferred data blocks Xout. Solving the
system of inequalities means to ﬁnd Ncu and Xout that mask the time to access data
and respect the external memory bandwidth.
8
Design Space Exploration
We propose a design space exploration (DSE) approach in order to efﬁciently map
an application described in Array-OL onto an architecture as proposed in Fig. 23.5.
The DSE chooses a task fusion that improves the execution time and uses the
minimal inter-task patterns. Then, it changes the data paving in order to mask
the latency of the data accesses. The space of the exploration is a set of solutions
with a given parallelism, a fusion conﬁguration and data block sizes that meet the
constraints on the external memory bandwidth. The results of the exploration is a set
of solutions which are optimal (also termed pareto ) with respect to two optimization
criteria: the architecture latency and the internal memory amount.
8.1
Optimization Criteria
A processor Pi contains a buffer of size LMj per each input ﬂow:
LMj.Pi/ D 2Ncuixj :

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
603
The factor 2 is due to the double buffering. The total amount of used internal
memory is:
IM D
X
i
X
j
LMj .Pi/:
We deﬁne the architecture latency as the latency to compute a single output image.
As in our model, the times to access data are always masked, we can approximate
the architecture latency with the time necessary to execute all the output transfers
towards the external memory:
AL D Imagesize4.I Mem
ı
/:
where 4 denotes the determinant. Imagesize can be inferred from the Array-OL
speciﬁcation.
8.2
Array-OL Model on the Target Architecture
An Array-OL model can directly be mapped onto an architecture like that presented
in Fig. 23.5, by using the following rules:
1. The analysis starts from a canonical Array-OL speciﬁcation, which is deﬁned to
be equivalent to a perfectly loop-nested code [44]: it cannot contain repetition
around the composition of repetitive task.
2. The hierarchy of the Array-OL model is analyzed from the highest to the lowest
level.
3. At the highest level, we infer the parameters of the mapping, i.e., Cx, I Mem
ı
, I com
ı
,
Kı and KMem
ı
. Given a task taski, let spin.taski/ and spin.j/ be respectively the
size of the output pattern and input patterns (j). The elements of Kı and KMem
ı
are computed as:
ıi;j D
4.Diag.spin.j//
4.Diag.spout.taski//
(23.7)
An element cxi of Cx is cxi D maxj fıi;jg. The values of I Mem
ı
and I com
ı
elements
depend on the inter-task links.
4. At each level we distinguish among an elementary, a composite or a repetitive
task.
•
If the task is elementary or a composition of elementary tasks, a set of library
elements is instantiated to realize it.
•
When a task is a repetition we distinguish two cases: if it is a repetition
of a single task we instantiate a single processor; if it is a repetition of

604
R. Corvino et al.
<1>
<1,1><1,1>
<A>
<B>
<1,1>
<1,1>>
<1,1>
<B>
<A>
<1,1>
<A,B>
>
B
,
A
<
>
B
,
A
<
<A,B>
<1>
Output port
Input port
Fig. 23.11 A possible canonical Array-OL speciﬁcation for a LPSF ﬁlter
a compound task we instantiate a set of processors in a SIMD, MIMD or
pipelined conﬁguration. The repetitions of the same task is iterated on the
same hardware or executed in parallel according to mapping criteria.
5. Each instantiated processor contains at least a datapath (of library elements) and
may contain some local buffers and a local memory controller.
Example 2. Mapping constraints for a LPSF ﬁlter in an Array-OL speciﬁcation.
Figure 23.11 shows a possible canonical speciﬁcation for a LPSF ﬁlter.
The speciﬁcation contains four elementary tasks, thus we instantiate four proces-
sors .P1; P2; P3; P4/. The user’s speciﬁed parameters are: m D 4 and Lm D 30.
From the Array-OL speciﬁcation analysis, we infer the following parameters of the
mapping criteria:
Cx D .1; 1; 1; 1/I
I Mem
ı
D
0
BBBBBBB@
0 0 0 0 0 0
0 0 0 0 0 0
0 0 1 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 1
1
CCCCCCCA
I I com
ı
D
0
BBBBBBB@
0 0 0 0 0 0
0 1 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 1 0
0 0 0 0 0 0
1
CCCCCCCA
I
Kı D
0
BBBBBBB@
0 1 0 0 0 0
0 0 1 0 0 0
0 0 0 0 0 0
0 0 0 0 1 0
0 0 0 0 0 1
0 0 0 0 0 0
1
CCCCCCCA
I KMem
ı
D
0
BBBBBBB@
0 1 0 0 0 0
0 0 0 0 0 0
0 0 0 0 0 0
0 0 0 0 1 0
0 0 0 0 0 0
0 0 0 0 0 0
1
CCCCCCCA
A 4,000 lines Java code1 automatically extracts and processes these parameters
and obtains the following results by solving the inequality system of the mapping
criteria: Ncu  .4; 4; 4; 4/; Xout  .480; 480; 480; 480/. These constraints are used
to change the data paving in the Array-OL speciﬁcation. The Java program also
1Available on demand at http://www.es.ele.tue.nl/rcorvino/tools.html

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
605
computes the optimization criteria to enable the successive design space exploration
and perform the consequent solution classiﬁcation and selection: IM D 15K.data/
AL D 4M.cycles/.
8.3
DSE Flow
By starting from a canonical form, all the possible task fusions are explored
(according to a method explained in the next paragraph). The obtained Array-OL
model is mapped onto an architecture as presented in Fig. 23.5 and the repetition
granularity of the merged tasks is changed in order to mask the external memory
latency. The granularity is changed through a change paving and in order to have
4.Diag.sout
p .taski///  xi. Finally, the obtained solutions are evaluated against the
amount of internal memory used and the architecture latency. The pareto solutions
are chosen.
8.4
Reducing the Space of Possible Fusions
To reduce the exploration space of the possible fusions, we propose to adapt
the method used by Talal et al. [45] to generate optimal coalition structures.
Given a number of tasks n, we can map the space of possible fusions onto an
integer partition of n. An integer partition is a set of positive integer vectors
whose components add up to n. For example, the integer partition of n D 4 is
Œ1; 1; 1; 1; Œ2; 1; 1; Œ2; 2; Œ3; 1; Œ4. Each vector of the integer partition can be a
mapping for more possible fusions as shown in Fig. 23.12. Let a macrotask be the
result of a fusion, as proposed by Talal et al., we reduce the number of sub-spaces
by merging the sub-spaces whose solutions contain the same number of macrotasks.
For the example of Fig. 23.12, the sub-spaces mapped on the integer partitions [3,1]
and [2,2] are merged. In this way, the number of sub-spaces is limited to n.
This mapping reduces the number of comparisons between the possible solu-
tions. In fact we search for the pareto solutions of each sub-space and we compare
them to each other in order to ﬁnd the pareto solutions of the whole space. In
Fig. 23.12, we perform 32 comparisons instead of 56. The pareto solutions are
denoted by P soli in Fig. 23.12. Among these solutions, a user can choose the most
adapted to his or her objectives. In our case, we have chosen the solution P sol1,
which has the most advantageous trade-off between the used internal memory and
the architecture latency. The mapping constraints for this solution are given in
Example 2. Blocks of data parsing the input image from the top left to the bottom
right corner are pipelined on the HFLR and VFTD ﬁlters. It is possible to process up

606
R. Corvino et al.
IM (data)
AL (cycles)
15 K
8 M
17 K
6 M
2 M
6 M
17
4 M
2 M
4 M
2 M
2 M
Lm=30
m=4
15 K
4 M
2 M
4 M
1,1,1,1
4
3,1
2,2
2,1,1
Psol1
Psol2
Ncu=(4,4,4,4)
Cx=(1,1,1,1)
Fig. 23.12 Exploration for a LPSF ﬁlter. Solutions merging VFTD and HFRL have to store a
whole image. Thus, they use 2M of internal memory
to four parallel blocks of data per ﬁlter. Each block has to contain at least 60 data to
mask the time to fetch. The execution of HFRL and VFBU ﬁlters is similar except
that it starts from the bottom right and stops to the top left corner.
9
Case Study: Application to Hydrophone Monitoring
In this section, we apply our method to a more complex example. The considered
application implements hydrophonemonitoring. A hydrophoneis an electroacoustic
transductor that converts the underwater sounds in electric signals for further
analysis. The signals corresponding to underwater sounds ﬁrst undergo through
a spectral analysis performed by a Fast Fourier Transform (FFT). Then, they are
partitioned in different beams representing different monitored spatial directions,
frequencies and captors. The beams are treated via successive processing steps in
order to extract characteristic frequencies. The functional blocks of the application
are represented in Fig. 23.13.
Table 23.1 gives the sizes of the input and output arrays processed by each
functional block. It also gives the sizes of input and output patterns that each task
uses to consume and produce input and output arrays respectively.

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
607
Hydrophone
Freq.
Hydrophone Monitoring
Bands
Int1
Stab
Int2
Norm
Beams
FFT
Fig. 23.13 Array-OL speciﬁcation of a hydrophone monitoring application
Table 23.1 Sizes of input and output arrays of the hydrophone monitoring application blocs
Block
Input array
Output array
Input
pattern
Output
pattern
Number of
task repetitions
FFT
f512  1g
f512  256  1g
512
256
f512; 1g
Beams
f512  256  1g
f128  1  200g
192
1
f128; 200; 1g
f128  200  192g
192
Norm
f128  1  200g
f128  1  200g
1
1
f128; 200; 1g
Bands
f128  1  200g
f128  1g
200
1
f128; 1g
Int1
f128  1g
f128  1g
8
1
f128; 1g
Stab
f128  1g
f128  1g
8
1
f128; 1g
f128  8g
8
Int2
f128  1g
f128  1g
8
1
f128; 1g
Each block takes its inputs from the preceding block
The Beams and Stab blocks have secondary inputs that are tables of coefﬁcients
9.1
Analysis Steps
The input speciﬁcation of the application is given as a series of repeated tasks. The
textual representation of this speciﬁcation, i.e., our actual input of the automatic
DSE tool, is shown in Fig. 23.14.
The set of task repetitions can be multidimensional, with an inﬁnite dimension.
In order to realize such an application, it is necessary to specify which repetitions
are mapped in space (thus realized by parallel physical processors) and which are
mapped in time (thus realized in a pipeline on the same hardware). In our analysis,
we consider that all repetitions indexed by ﬁnite numerical values can potentially
be realized in parallel on different hardware resources. The repetitions indexed by
1 are realized in a pipeline. It is possible to transform the speciﬁed model to re-
distribute repetitions from time to space and conversely. Once we have the input
speciﬁcation, the analysis goes through the following steps:

608
R. Corvino et al.
#####################################
#
TN=Task Name
#
I(O)DN=Input (Output) Dependency Name
#
I(O)O=Input (Output) Origin
#
I(0)P=Input (Output) Paving
#
I(0)F=Input (Output) Fitting
#
I(O)A=Input (Output) Array
#
I(O)M=Input (Output) Motif
#####################################
#
FFT
#####################################
TN
FFT
TR {512,-1}
IDN FFT_in
IA {512,-1}
IO {0,0}
IP {{1,0},{0,512}}
IF {0,1}
IM {512}
ODN FFT_out
OA {512,256,-1}
OO {0,0,0}
OP {{1,0,0},{0,0,1}}
OF {{0,1,0}}
OM {256}
END
#####################################
...
#####################################
#
Beams
#####################################
TN
Beams
TR {128,200,-1}
IDN Beams_in
IA {512,256,-1}
IO {0,28,0}
IP {{4,0,0},{0,1,0},{0,0,1}}
IF {{1,0,0}}
IM {192}
IDN Beams_in1
IA {128,200,192}
IO {0,0,0}
IP {{1,0,0},{0,1,0},{0,0,0}}
IF {{0,0,1}}
IM {192}
ODN Beams_out
OA {128,200,-1}
OO {0,0,0}
OP {{1,0,0},{0,1,0},{0,0,1}}
OF {{0,0,0}}
OM {1}
#####################################
...
Fig. 23.14 Partial input textual speciﬁcation of our DSE tool for the hydrophone monitoring
1. For each task, the Initiation Interval is computed, according to formula (23.7).
Then the following results are obtained:
II.FTT/ D 2
II.Beams/ D 192
II.Norm/ D 1
II.Bands/ D 200
II.Int1/ D 8
II.Stab/ D 8
II.Int2/ D 8
2. In order to divide and conquer the space of possible task fusions within the
application, we consider the integer partitions of 7, where 7 is the number of tasks
in the hydrophone monitoring. The integer partitions of 7 are 15 as presented in
Table 23.2. Each of these partitions maps a group of possible task fusions that are
represented in the column “Fusions” of Table 23.2. In this column, the notation
*—t1, t2—* represents the fusion of two tasks t1 and t2.
For each integer partition, we only consider the possible fusions. Other fusions
conﬁgurations are not possible because of inter-task dependencies that are
captured and analyzed through a Links matrix, given in Table 23.3.
3. For each fusion, we compute the parametric matrices of the mapping criteria
1, 2 and 3, which are Diag.Cx/, KMem
ı
and I Mem
ı
. These matrices are used to
automatically infer the inequality constraints on the internal buffer sizes Xout
and parallelism level NCU of a task-fusion implementation. These matrices have

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
609
Table 23.2 Possible integer partitions of 7, mapping sub-spaces of fusion solutions
Partition
Fusions
1
1
1
1
1
1
1
[FFT, Beams, Norm, Bands, Int1, Stab, Int2]
2
1
1
1
1
1
[*—FFT, Beams—*,Norm, Bands, Int1, Stab, Int2]
[FFT, *—Beams, Norm—*, Bands, Int1, Stab, Int2]
[FFT, Beams, *—Norm, Bands—*, Int1, Stab, Int2]
[FFT, Beams, Norm, *—Bands, Int1—*, Stab, Int2]
[FFT, Beams, Norm, Bands, *—Int1, Stab—*, Int2]
[FFT, Beams, Norm, Bands, Int1, *—Stab, Int2—*]
3
1
1
1
1
[*—FFT, Beams, Norm—*, Bands, Int1, Stab, Int2]
[FFT, *—Beams, Norm, Bands—*, Int1, Stab, Int2]
: : :
2
2
1
1
1
[*—FFT, Beams—*,*—Norm, Bands—*, Int1, Stab, Int2]
[*—FFT, Beams—*, Norm, *—Bands, Int1—*, Stab, Int2]
[*—FFT, Beams—*, Norm, Bands, *—Int1, Stab—*, Int2]
[*—FFT, Beams—*, Norm, Bands, Int1, *—Stab, Int2—*]
[FFT, *—Beams, Norm—*, Bands, Int1, *—Stab, Int2—*]
: : :
4
1
1
1
: : :
5
1
1
: : :
4
2
1
: : :
3
3
1
: : :
3
2
2
: : :
6
1
: : :
5
2
: : :
4
3
: : :
7
[*—FFT, Beams, Norm, Bands, Int1, Stab, Int2—*]
Table 23.3 Links
matrix
giving
the
task
dependencies
of
hydrophone application. A “1” means that taskj depends on taski
FFT
Beams
Norm
Bands
Int1
Stab
Int2
FFT
0
1
0
0
0
0
0
Beams
0
0
1
0
0
0
0
Norm
0
0
0
1
0
0
0
Bands
0
0
0
0
1
0
0
Int1
0
0
0
0
0
1
0
Stab
0
0
0
0
0
0
1
Int2
0
0
0
0
0
0
0
different values and dimensions depending on the considered fusion. Below, we
illustrate the methods by giving the parametric matrices and the mapping criteria
for two possible fusions.
Example 3. This example considers the case where tasks are not merged, which
is denoted by [FFT, Beams, Norm, Bands, Int1, Stab, Int2]. Figure 23.15 gives
a simpliﬁed representation of the tasks and how they communicate with each

610
R. Corvino et al.
Fig. 23.15 Example of conﬁguration: no merged tasks
other and with the external memory. In this case, as task are not merged, all
communications are achieved through the external memory. Thus, there are 16
links associated with unknown variables X.
These variables give qualitative and quantitative characterizations of a speciﬁc
communication. Indeed, from a qualitative viewpoint, they can be classiﬁed
as output or input links, denoted Xout and Xin respectively. They can also be
classiﬁed as links with the external memory or inter-task links, denoted XMem
and XCom respectively. For the example of Fig. 23.15, all the links are XMem.
From a quantitative viewpoint, the value of the X variables, computed through
the mapping criteria 1, 2 and 3, gives the minimum storage requirement needed
to ensure the correctness of communications. The parametric matrices are:
Diag.Cx/ D
0
BBBBBBBBBBBBBBBBBBBBBBBBBBBB@
2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
192
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
192
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
192
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
200
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
200
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
8
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
8
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
8
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
8
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
8
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
8
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
8
1
CCCCCCCCCCCCCCCCCCCCCCCCCCCCA
,

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
611
Kı D
0
BBBBBBBBBBBBBBBBBBBBBBBBBBBBB@
0 2 0 0 0
0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 192 0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 192 0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 0
0 1 0 0
0 0 0 0 0 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 0
0 0 0 200 0 0 0 0 0 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 0
0 0 0 0
0 8 0 0 0 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 8 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 8 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 0 0 8
0 0 0 0 0
0 0 0 0
0 0 0 0 0 0 0
1
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCA
,
KMem
ı
D
0
BBBBBBBBBBBBBBBBBBBBBBBBBBBBB@
0 2 0 0 0
0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 192 0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 192 0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 0
0 1 0 0
0 0 0 0 0 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 0
0 0 0 200 0 0 0 0 0 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 0
0 0 0 0
0 8 0 0 0 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 0 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 8 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 8 0 0
0 0 0 0 0
0 0 0 0
0 0 0 0 0 0 8
0 0 0 0 0
0 0 0 0
0 0 0 0 0 0 0
1
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCA
,
I Mem
ı
D
0
BBBBBBBBBBBBBBBBBBBBBBBBBBBBB@
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
1
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCA
,
I Com
ı
D
0
BBBBBBBBBBBBBBBBBBBBBBBBBBBBB@
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
1
CCCCCCCCCCCCCCCCCCCCCCCCCCCCCA
By using these matrices with the mapping criteria 1, 2 and 3, we have a system
of linearly independent inequalities, in the unknown variables NCU and Xout. The
resolution of this system of inequalities gives:
Xout 
Œ0:0;
640:0;
0:0;
0:0; 4:987013;
0:0;
960:0;
0:0;
9:552238;
0:0;
213:33333; 0:0;
0:0;
112:94118;
0:0;
213:33333T
where Xout D .0; 1; 0; 0; 1; 0; 1; 0; 1; 0; 1; 0; 0; 1; 0; 1/X, and Ncu  Œ4:0; 4:0;
4:0; 4:0; 4:0; 4:0; 4:0T :
Example 4. This example considers the case where all tasks of the application
are merged, denoted [*—FFT, Beams, Norm, Bands, Int1, Stab, Int2—*]. Fig-
ure 23.16 gives the communication structure, which counts ten links: four with
the external memory and six inter-task links. For this example, the parametric
matrices are:

612
R. Corvino et al.
Diag.Cx/ D
0
BBBBBBBBBBBBBBB@
2 0
0
0
0
0
0 0 0 0
0 2
0
0
0
0
0 0 0 0
0 0 192 0
0
0
0 0 0 0
0 0
0
192 0
0
0 0 0 0
0 0
0
0
1
0
0 0 0 0
0 0
0
0
0 200 0 0 0 0
0 0
0
0
0
0
8 0 0 0
0 0
0
0
0
0
0 8 0 0
0 0
0
0
0
0
0 0 8 0
0 0
0
0
0
0
0 0 0 8
1
CCCCCCCCCCCCCCCA
;
Kı D
0
BBBBBBBBBBBBBBB@
0 2 0
0
0
0
0 0 0 0
0 0 0 192 0
0
0 0 0 0
0 0 0 192 0
0
0 0 0 0
0 0 0
0
1
0
0 0 0 0
0 0 0
0
0 200 0 0 0 0
0 0 0
0
0
0
8 0 0 0
0 0 0
0
0
0
0 0 8 0
0 0 0
0
0
0
0 0 8 0
0 0 0
0
0
0
0 0 0 8
0 0 0
0
0
0
0 0 0 0
1
CCCCCCCCCCCCCCCA
; KMem
ı
D
0
BBBBBBBBBBBBBBB@
0 2 0
0
0 0 0 0 0 0
0 0 0
0
0 0 0 0 0 0
0 0 0 192 0 0 0 0 0 0
0 0 0
0
0 0 0 0 0 0
0 0 0
0
0 0 0 0 0 0
0 0 0
0
0 0 0 0 0 0
0 0 0
0
0 0 0 0 0 0
0 0 0
0
0 0 0 0 8 0
0 0 0
0
0 0 0 0 0 0
0 0 0
0
0 0 0 0 0 0
1
CCCCCCCCCCCCCCCA
;
I Com
ı
D
0
BBBBBBBBBBBBBBB@
0 0 0 0 0 0 0 0 0 0
0 1 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0
0 0 0 1 0 0 0 0 0 0
0 0 0 0 1 0 0 0 0 0
0 0 0 0 0 1 0 0 0 0
0 0 0 0 0 0 1 0 0 0
0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 1 0
0 0 0 0 0 0 0 0 0 0
1
CCCCCCCCCCCCCCCA
;
I Mem
ı
D
0
BBBBBBBBBBBBBBB@
0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0 0 1
1
CCCCCCCCCCCCCCCA
By using these matrices with the mapping criteria 1, 2 and 3, we have a system
of linearly independent inequalities, in the unknown variables NCU and X.
The resolution of this system yields:
Xout  Œ0:0; 240:0; 0:0; 2:5; 768000:0; 3840:0; 480:0; 0:0; 60:0; 480:0T
where Xout D .0; 1; 0; 1; 1; 1; 1; 0; 1; 1/X, and Ncu  Œ4:0; 4:0; 4:0; 4:0; 4:0;
4:0; 4:0T :

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
613
Fig. 23.16 Example of
conﬁguration: all tasks are
merged
4. The previous method is applied to all fusion possibilities of Table 23.2. For
each possibility, the method computes the maximum parallelism level and the
data granularity that meets the external memory bandwidth. The total number of
all analyzed fusions is 54. Among them, three are selected. The duration of the
overall exploration is of 3 s.
10
Summary
We presented a method to explore the space of possible data transfer and storage
micro-architectures for data intensive applications. Such a method is very useful
in order to ﬁnd efﬁcient implementations of these applications, which meet the
performance requirements on system-on-chip. It starts from a canonical represen-
tation of an application in a language, named Array-OL, and applies a set of loop
transformations so as to infer an application-speciﬁc architecture that masks the
data transfer time with the time to perform the computations. For that purpose, we
proposed a customizable model of target architecture including FIFO queues and
double buffering mechanism. The mapping of an application onto this architecture
is performed through a ﬂow of Array-OL model transformations aimed to improve
the parallelism level and to reduce the size of the used internal memories. We used a

614
R. Corvino et al.
method based on an integer partition to reduce the space of explored transformation
scenarios. The method has been illustrated on a case study consisting of an
implementation of a hydrophone monitoring application as found in sonar signal
processing. Our method is aimed to serve in Gaspard2, an Array-OL framework
able to map Array-OL models onto different kinds of target architectures [46].
While the proposed implementation of our method uses mapping (and schedul-
ing) constraints to provide optimal solutions, it is limited to the case of simple
linear pipelines and does not provide sufﬁcient precision on the temporal behavior
of the data transfer. We are currently working on an improved method using
abstract clocks to precisely describe the data transfer and storage of data intensive
computing systems. The new method will be able to explore several application
model transformations as task fusion (as in the current method), paving change,
unrolling and tiling.
References
1. Tony Hey, Stewart Tansley, and Kristin Tolle, editors. The Fourth Paradigm: Data-Intensive
Scientiﬁc Discovery. 2009.
2. Jianwen Zhu and Nikil Dutt. Electronic system-level design and high-level synthesis. In Laung-
Terng Wang, Yao-Wen Chang, and Kwang-Ting (Tim) Cheng, editors, Electronic Design
Automation, pages 235–297. Morgan Kaufmann, Boston, 2009.
3. Felice Balarin, Massimiliano Chiodo, Paolo Giusto, Harry Hsieh, Attila Jurecska, Luciano
Lavagno, Claudio Passerone, Alberto Sangiovanni-Vincentelli, Ellen Sentovich, Kei Suzuki,
and Bassam Tabbara.
Hardware-software co-design of embedded systems: the POLIS
approach. Kluwer Academic Publishers, Norwell, MA, USA, 1997.
4. R. Ernst, J. Henkel, Th. Benner, W. Ye, U. Holtmann, D. Herrmann, and M. Trawny.
The cosyma environment for hardware/software cosynthesis of small embedded systems.
Microprocessors and Microsystems, 20(3):159–166, 1996.
5. B. Kienhuis, E. Deprettere, K. Vissers, and P. Van Der Wolf. An approach for quantitative
analysis of application-speciﬁc dataﬂow architectures.
In Application-Speciﬁc Systems,
Architectures and Processors, 1997. Proceedings., IEEE International Conference on, pages
338–349, Jul 1997.
6. Sander Stuijk. Predictable Mapping of Streaming Applications on Multiprocessors. PhD thesis,
Technische Universiteit Eindhoven, The Nederlands, 2007.
7. Andreas Gerstlauer and Daniel D. Gajski. System-level abstraction semantics. In Proceedings
of the 15th international symposium on System Synthesis, ISSS ’02, pages 231–236, New York,
NY, USA, 2002. ACM.
8. P. R. Panda, F. Catthoor, N. D. Dutt, K. Danckaert, E. Brockmeyer, C. Kulkarni, A. Vandercap-
pelle, and P. G. Kjeldsberg. Data and memory optimization techniques for embedded systems.
ACM Trans. Des. Autom. Electron. Syst., 6:149–206, April 2001.
9. F. Catthoor, K. Danckaert, C. Kulkarni, E. Brockmeyer, P. G. Kjeldsberg, T. Van Achteren,
and T. Omnes. Data access and storage management for embedded programmable processors.
Springer, 2002.
10. Rosilde Corvino, Abdoulaye Gamati´e, and Pierre Boulet. Architecture exploration for efﬁcient
data transfer and storage in data-parallel applications. In Pasqua D’Ambra, Mario Guarracino,
and Domenico Talia, editors, Euro-Par 2010 - Parallel Processing, volume 6271 of Lecture
Notes in Computer Science, pages 101–116. Springer Berlin/Heidelberg, 2010.

23
Design Space Exploration for Efﬁcient Data Intensive Computing on SoCs
615
11. Lech J´ozwiak, Nadia Nedjah, and Miguel Figueroa. Modern development methods and tools
for embedded reconﬁgurable systems: A survey. Integration, the VLSI Journal, 43(1):1–33,
2010.
12. Edward A. Lee and David G. Messerschmitt. Synchronous Data Flow. Proceedings of the
IEEE, 75(9):1235–1245, September 1987.
13. A. Sangiovanni-Vincentelli and G. Martin.
Platform-based design and software design
methodology for embedded systems. Design Test of Computers, IEEE, 18(6):23–33, Nov/Dec
2001.
14. Giuseppe Ascia, Vincenzo Catania, Alessandro G. Di Nuovo, Maurizio Palesi, and Davide
Patti. Efﬁcient design space exploration for application speciﬁc systems-on-a-chip. Journal of
Systems Architecture, 53(10):733–750, 2007.
15. F Balasa, P Kjeldsberg, A Vandecappelle, M Palkovic, Q Hu, H Zhu, and F Catthoor. Storage
Estimation and Design Space Exploration Methodologies for the Memory Management of
Signal Processing Applications. Journal of Signal Processing Systems, 53(1):51–71, Nov 2008.
16. Yong Chen, Surendra Byna, Xian-He Sun, Rajeev Thakur, and William Gropp. Hiding i/o
latency with pre-execution prefetching for parallel applications. In ACM/IEEE Supercomputing
Conference (SC’08), page 40, 2008.
17. P. R. Panda, F. Catthoor, N. D. Dutt, K. Danckaert, E. Brockmeyer, C. Kulkarni, A. Vandercap-
pelle, and P. G. Kjeldsberg. Data and memory optimization techniques for embedded systems.
ACM Transactions on Design Automation of Electronic Systems, 6(2):149–206, 2001.
18. H T Kung. Why systolic architectures. Computer, 15(1):37–46, 1982.
19. Abdelkader Amar, Pierre Boulet, and Philippe Dumont. Projection of the Array-OL Spec-
iﬁcation Language onto the Kahn Process Network Computation Model.
In ISPAN ’05:
Proceedings of the 8th International Symposium on Parallel Architectures, Algorithms and
Networks, pages 496–503, 2005.
20. D. Kim, R. Managuli, and Y. Kim. Data cache and direct memory access in programming
mediaprocessors. Micro, IEEE, 21(4):33–42, Jul 2001.
21. Jason D. Hiser, Jack W. Davidson, and David B. Whalley.
Fast, Accurate Design Space
Exploration of Embedded Systems Memory Conﬁgurations. In SAC ’07: Proceedings of the
2007 ACM symposium on Applied computing, pages 699–706, New York, NY, USA, 2007.
ACM.
22. Q. Hu, P. G. Kjeldsberg, A. Vandecappelle, M. Palkovic, and F. Catthoor.
Incremental
hierarchical memory size estimation for steering of loop transformations. ACM Transactions
on Design Automation of Electronic Systems, 12(4):50, 2007.
23. Yong Chen, Surendra Byna, Xian-He Sun, Rajeev Thakur, and William Gropp. Hiding I/O
latency with pre-execution prefetching for parallel applications. In SC ’08: Proceedings of the
2008 ACM/IEEE conference on Supercomputing, pages 1–10, 2008.
24. P.K. Murthy and E.A. Lee. Multidimensional synchronous dataﬂow. IEEE Transactions on
Signal Processing, 50(8):2064–2079, Aug. 2002.
25. F. Deprettere and T. Stefanov.
Afﬁne nested loop programs and their binary cyclo-static
dataﬂow counterparts. In Proc. of Conf. on Application Speciﬁc Systems, Architectures, and
Processors, pages 186–190, 2006.
26. Albert Cohen, Marc Duranton, Christine Eisenbeis, Claire Pagetti, Florence Plateau, and Marc
Pouzet. N-synchronous kahn networks: a relaxed model of synchrony for real-time systems. In
POPL ’06: Conference record of the 33rd ACM SIGPLAN-SIGACT symposium on Principles
of programming languages, pages 180–193, 2006.
27. Sylvain Girbal, Nicolas Vasilache, C´edric Bastoul, Albert Cohen, David Parello, Marc Sigler,
and Olivier Temam. Semi-automatic composition of loop transformations for deep parallelism
and memory hierarchies. Journal of Parallel Programming, 34:261–317, 2006.
28. Mark Thompson, Hristo Nikolov, Todor Stefanov, Andy D. Pimentel, Cagkan Erbas, Simon
Polstra, and Ed F. Deprettere. A framework for rapid system-level exploration, synthesis, and
programming of multimedia mp-socs.
In Proceedings of the 5th IEEE/ACM international
conference on Hardware/software codesign and system synthesis, CODES+ISSS’07, pages 9–
14, New York, NY, USA, 2007. ACM.

616
R. Corvino et al.
29. Scott Fischaber, Roger Woods, and John McAllister. Soc memory hierarchy derivation from
dataﬂow graphs. Journal of Signal Processing Systems, 60:345–361, 2010.
30. Calin Glitia and Pierre Boulet.
High Level Loop Transformations for Systematic Signal
Processing Embedded Applications. Research Report RR-6469, INRIA, 2008.
31. S.H. Fuller and L.I. Millett. Computing performance: Game over or next level? Computer,
44(1):31–38, Jan. 2011.
32. Rosilde Corvino. Exploration de l’espace des architectures pour des syst`emes de traitement
d’image, analyse faite sur des blocs fondamentaux de la r´etine num´erique.
PhD thesis,
Universit´e Joseph-Fourier - Grenoble I, France, 2009.
33. Calin Glitia, Philippe Dumont, and Pierre Boulet. Array-OL with delays, a domain speciﬁc
speciﬁcation language for multidimensional intensive signal processing.
Multidimensional
Systems and Signal Processing (Springer Netherlands), 2010.
34. B.C. de Lavarene, D. Alleysson, B. Durette, and J. Herault. Efﬁcient demosaicing through
recursive ﬁltering.
In IEEE International Conference on Image Processing (ICIP 07),
volume 2, Oct. 2007.
35. Jeanny H´erault and Barth´el´emy Durette. Modeling visual perception for image processing.
Computational and Ambient Intelligence (LNCS Springer Berlin/Heidelberg), pages 662–675,
2007.
36. Calin Glitia and Pierre Boulet.
High level loop transformations for systematic signal
processing embedded applications. Embedded Computer Systems: Architectures, Modeling,
and Simulation (Springer), pages 187–196, 2008.
37. Ken Kennedy and Kathryn S. McKinley. Maximizing loop parallelism and improving data
locality via loop fusion and distribution. In Proceedings of the 6th International Workshop
on Languages and Compilers for Parallel Computing, pages 301–320, London, UK, 1994.
Springer-Verlag.
38. Frank Hannig, Hritam Dutta, and J¨urgen Teich.
Parallelization approaches for hardware
accelerators – loop unrolling versus loop partitioning. Architecture of Computing Systems –
ARCS 2009, pages 16–27, 2009.
39. Jingling Xue. Loop tiling for parallelism. Kluwer Academic Publishers, 2000.
40. Preeti Ranjan Panda, Hiroshi Nakamura, Nikil D. Dutt, and Alexandru Nicolau. Augmenting
loop tiling with data alignment for improved cache performance.
IEEE Transactions on
Computers, 48:142–149, 1999.
41. Lushan Liu, Pradeep Nagaraj, Shambhu Upadhyaya, and Ramalingam Sridhar. Defect analysis
and defect tolerant design of multi-port srams. J. Electron. Test., 24(1–3):165–179, 2008.
42. Robert Schreiber, Shail Aditya, Scott Mahlke, Vinod Kathail, B Rau, Darren Cronquist,
and Mukund Sivaraman.
Pico-npa: High-level synthesis of nonprogrammable hardware
accelerators. The Journal of VLSI Signal Processing, 31(2):127–142, Jun 2002.
43. Imondi GC, Zenzo M, and Fazio MA. Pipelined Burst Memory Access, US patent, August
2008. patent.
44. Nawaaz Ahmed, Nikolay Mateev, and Keshav Pingali.
Synthesizing transformations for
locality enhancement of imperfectly-nested loop nests.
International Journal of Parallel
Programming, 29(5):493–544, Oct 2001.
45. Talal Rahwan, Sarvapali Ramchurn, Nicholas Jennings, and Andrea Giovannucci. An anytime
algorithm for optimal coalition structure generation. Journal of Artiﬁcial Intelligence Research
(JAIR), 34:521–567, April 2009.
46. Abdoulaye Gamati´e, S´ebastien Le Beux, ´Eric Piel, Rabie Ben Atitallah, Anne Etien, Philippe
Marquet, and Jean-Luc Dekeyser. A model driven design framework for massively parallel
embedded systems. ACM Transactions on Embedded Computing Systems (TECS) ACM (To
appear), preliminary version at http://hal.inria.fr/inria-00311115/, 2010.

Chapter 24
Information Quality and Relevance
in Large-Scale Social Information Systems
Munmun De Choudhury
1
Introduction
With the “Internet revolution”, the past decade has witnessed a radical
transformation in the organization of information production over a host of perva-
sive social applications. This technological change, primarily in the domain of how
we engage in information, knowledge and culture-sharing in pervasive environments
is in turn, promoting the beginning of a variety of economic, social and cultural
adaptations in our everyday lives. Online social networks and social media, via
loosely or tightly woven collaborations, have introduced several emergent practices
for better democratic participation, for fostering a self-reﬂective culture as well
as in encouraging distributed cooperative efforts, immersive entertainment and
investigative reporting.
Such pervasive high degree of online activity often manifests itself in social me-
dia streams, such as status updates on Facebook, tweets on Twitter, and news items
on Digg. In almost all of these websites, while end users can ‘broadcast’ information
that interests them, they can also ‘listen’ to their contacts by subscribing to their
respective content streams. Consequently, these avenues have emerged as modern
means of real-time content dissemination to users for temporal events [18, 22, 24].
This is also supported by the statistics that by April 2010, Twitter was receiving
over 600 million search queries per day.1 Additionally, as of May 2010, YouTube
1Hufﬁngton Post. Twitter User Statistics Revealed: http://www.hufﬁngtonpost.com/2010/04/14/
twitter-user-statistics-r n 537992.html, Apr. 2010.
M. De Choudhury ()
Rutgers University, New Brunswick, NJ
e-mail: m.dechoudhury@rutgers.edu
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 24, © Springer Science+Business Media, LLC 2011
617

618
M. De Choudhury
alone has more than 2 billion views a day,2 24 h of video uploaded per minute, and
accounts for 10% of all internet and more than 20% of HTTP trafﬁc.3
Note that a major catalyst for this nature of information production at a
massive scale has been triggered by the advances in the fabrication technology of
computation in pervasive systems, and its consequent percolation effects into the
technologies of communication and storage of large-scale pervasive applications.
This declining price has placed the authority of information production and cultural
evolution in the hands of the commonplace individual on the Web: e.g., the social
network Facebook today features more than half a billion users round the globe.
These social applications have thus improved the structure of the public sphere:
today practically anyone, anywhere and anytime can engage in contributing socio-
economic and politically salient observations, discourse, comments and insights
about a range of happenings spanning business, entertainment, politics or sports. To
take a few examples, in the recent years Twitter has played an instrumental role in
reﬂecting and revealing news related information on timely events to Internet users.
In early 2010, Twitter became a signiﬁcant response to the Haiti earthquake as it
helped spread messages, photos and pleas to assist the rehabilitation teams, NGOs
and the local establishments in the crisis management process, powered by instant
inter-connectivity through several pervasive devices like cellular phones, PDAs etc.
(ref. Mashable4). Similarly, with 10,000 protesters thronging the streets in Moldova
in the spring of 2009 to protest against their country’s Communist government,
the action was labeled as the “Twitter Revolution”, because of the serendipity
of the means by which the demonstrators came together. During the elections in
Iran Twitter enabled bringing information to the global audience regarding the
demonstrations, riots and protests from the local residents (ref. Washington Times5).
Social networks and social media in general also played a helpful and instrumental
role during Barack Obama’s presidential campaign in 2008.
While these changes in our newly emergent “networked lives” are highly
encouraging, our interactions on the multi-faceted pervasive social systems have
begun to respond to the celebrated “information overload” problem. In a generic
sense, information overload refers to the difﬁculty an individual can have under-
standing an issue and making decisions that can be caused by the presence of
too much information. With the host of pervasive social applications that embody
our information creation and consumption behavior today, the over-availability of
information injected to us in a variety of context has begun to pose challenges to
the end users in making sense of relevant, legitimate and authoritative content as
well as in identifying the risk of misinformation. For example, recent studies have
revealed that the social media Twitter generates 55–60 million posts (or “tweets”)
2http://www.digitalbuzzblog.com/infographic-youtube-statistics-facts-ﬁgures/
3http://www.datacenterknowledge.com/archives/2007/06/22/youtube-10-percent-of-all-internet-
trafﬁc/
4http://mashable.com/2010/01/12/haiti-earthquake-pictures/
5http://www.washingtontimes.com/news/2009/jun/16/irans-twitter-revolution/

24
Information Quality and Relevance in Large-Scale Social Information Systems
619
everyday; other studies have also indicated that about a quarter of the posts on
Twitter are essentially spam (ref. Read Write Web). Obviously, the average user,
whose typical source of information is a sequence of subscriptions to different
social media, news websites and online forums in the form of a continuous stream
(e.g. RSS Feeds), is today facing the daunting task of tackling this information
overload, thereby also suffering from scarcity of attention in comprehending the
presented content altogether. Additionally, the abundance of information people
are exposed to through social media and other technology-based sources (such as
emails) could be having an impact on the thought process, obstructing deep thinking,
understanding, impedes the formation of memories and makes learning more
difﬁcult. This condition of “cognitive overload” results in diminished information
retaining ability and failing to connect remembrances to experiences stored in the
long-term memory, leaving thoughts “thin and scattered.”6 The question raised in
this chapter therefore attempts to seek answer to: how do we identify those pieces of
information that are relevant and interesting?
1.1
Challenges
These issues of the availability of unprecedented information on pervasive social
systems can be addressed via two mechanisms: (1) selecting relevant information,
and (2) identifying interesting information. Typically, an individual would be
interested to identify only those items in his or her subscribed information stream
that are relevant to a set of topics per his or her interest. Additionally, the individual
would also like to uncover information from their set of online friends which might
not be typically presented in these feeds, but that provide elaborate knowledge on a
few speciﬁc topics.
Addressing these two mechanisms of information dissipation to users in turn
requires the consideration of a variety of factors in the light of pervasive systems,
thus making this line of research challenging. (a) First, the notion of relevance and
interestingness of content is often dependent upon the interests of the user (i.e.
subjective): hence what is relevant for one user might be completely irrelevant for
another. (b) Second, due to the pervasive nature of how these social application
disseminate content (say, on a desktop at workplace, on a mobile handheld device
while traveling etc.), the span of relevant content might vary over time and space for
the same user; i.e. it would depend upon the context. (c) Third, the psychological
ability of different users in processing different types of information is likely to
be different; hence for some users, the set of relevant information presented can
be sufﬁciently large, while for others comparatively smaller. (d) Fourth, there is
also the challenge of variable degrees of usage of these information streams in
the exploration of content: some users might rely more on traditional media such
6http://www.telegraph.co.uk/science/science-news/8184149/Email-has-turned-us-into-lab-rats.
html

620
M. De Choudhury
as television or print media for useful content, thus making social applications to
cater to a broad range of interests not necessarily pertaining to timely happenings.
On the other hand, it is likely that for certain other users social media such as
Twitter are primary sources of information seeking and consumption. Hence the
deﬁnition of relevance would vary in each case depending on the availability of
multiple information dispersion channels in our day-to-day lives. (e) Finally, while
identifying relevant content to present to the users, we will in turn need to develop
informed interface designs that can take into account the cognitive implications of
how we perceive and process information in our short and long-term memory.
1.2
Impact
Data analysis of any nature, in the midst of the unprecedented explosion of
information availability on the social Web facilitated by the host of pervasive
applications, calls for a robust usage of cloud computing architecture as well as
a more nuanced consideration of the “human factors” working behind our models
of usage of these applications. Addressing the problem of identifying relevant
information generated in today’s pervasive social systems will therefore make both
computational and qualitative advances to the ﬁeld at large.
From a computational perspective, the algorithms developed to pursue this vision
will provide insights into what are the characteristics of the large-scale information
streams that are being generated in these spaces: e.g. does information of this nature
have an inherent signature to its structure or semantics? Will these signatures let us
analyze the information space more efﬁciently over the cloud or otherwise? How do
we sample such information spaces that can make the usage of social applications
more ubiquitous to time and space? In the qualitative sense, this research will
enable better understanding of what kind of pervasive interfaces or devices are more
suitable to what kind of demographic distribution of users. Additionally, we will be
able to identify what are good usability and evaluation metrics to gauge the success
of certain pervasive applications over others. Together, both the perspectives will
encourage future opportunities in understanding the broader impact of information
creation and consumption in the light of today’s variegated pervasive social
applications within the individual, in collaborative settings or even in our society.
2
Overview of Prior Literature
Although the burst of informational content on the Web due to the emergence of
social media sites is relatively new, there is a rich body of statistical, data mining
and social sciences literature that investigates efﬁcient methods for sampling large
data spaces [10,15,17]. Sociologists have studied the impact of snowball sampling
and random-walk based sampling of nodes in a social network on graph attributes
and other network phenomena [15]. Recently, sampling of large online networks

24
Information Quality and Relevance in Large-Scale Social Information Systems
621
(e.g., the Internet and social networks) has gained much attention [1,13,19,20,23]
in terms of how different techniques impact the recovery of overall network metrics,
like degree of distribution, path lengths, etc., as well as dynamic phenomena over
networks such as diffusion and community evolution.
There has also been considerable prior research on recommending, ﬁltering and
searching social media content on the web [2, 4, 6, 11, 22, 25]. More recently, to
tackle the issue of the availability of large scale social media content, Bernstein
et al. [4] proposed a Twitter application called “Eddi” that allows users to quickly
ﬁnd popular discussions in their Twitter feed by searching, or navigating a tag cloud,
timeline or categories. In other work, to better direct user attention Chen et al. [6]
explored three dimensions for designing a recommender of massive information
streams of social media content: content sources, topic interest models for users,
and social voting.
The literature closest to this problem domain lies in the area of discovering
relevant information in large communication systems. Stutzbach et al. [28] studied
the problem of selecting representative samples of peer properties such as peer
degree, link bandwidth, or the number of ﬁles shared in the context of dynamic
P2P networks. They proposed a degree-correction method to random walk-based
peer selection to achieve unbiased sampling in these systems. In light of search and
management of large-scale informational content, prior work has studied contexts
such as email streams [30].
2.1
Limitations
While this prior work has attempted to address the issue of how to manage and
present relevant content for large repositories of social media content, no principled
way of selecting or pruning such large spaces has been proposed. These spaces
are unique, because of the nature of user generated content, including its high
dimensionality and diversity. Besides, most of the above mentioned work on social
media sampling focused on how the sampling process impacts graph structure
and graph dynamics. Thus, the focus of the sampling strategies was to prune the
space of nodes or edges. However, this does not provide insights into the various
characteristics (e.g., degree of diffusion, topical content, level of diversity, etc.) of
social media spaces in general. Hence the methods typically developed for sampling
nodes/edges are not readily applicable in our case of discovering or sampling
relevant content from multi-dimensional social media spaces.
3
Case Study: Topic-Centric Relevance of Social Media
Content
In the light of the above related literature, we observe that retrieving relevant social
media content for the end user given a certain topic is a challenging task not only
because the social media information space exhibits profuse scale and exceedingly

622
M. De Choudhury
high rate of growth,7 but also because it features a rich set of attributes (e.g., network
properties of the content author, geographic location, timestamp of post, presence of
multiple themes and so on). Affected by these attributes, the information authorities
of relevant social media content are, therefore, likely to be emergent and temporally
dynamic, in turn rendering the content deemed relevant over a given topic, to be
temporally changing as well. Hence approaches utilizing static structural metrics
(such as HITS, PageRank) might not sufﬁce in this context because they are likely
to point to the celebrities, journalists, A-list bloggers or government bodies whose
posted content might not be deemed relevant to the end-user at all points in time.
Consequently, it appears that traditional search engines, such as Google and Bing
are not well equipped with the capability of searching for social media content
(also see [2]).
3.1
Content Diversity in Social Media
There have been recent attempts to tackle the problem of retrieval of social media
content in a commercial setting, including tools such as Bing Social8 and the native
search engine on Twitter.9 However, we note that the retrieval mechanisms on both
of these tools do not adequately address the challenges discussed in the previous
paragraphs, because they rely on content presentation based on a ﬁxed attribute,
ignoring the rich span of attributes that the Twitter information space features. For
example, while Twitter search gives a list of tweets on a topical query that are
ordered reverse chronologically (i.e. most recent tweets), there is no scope for the
end user to seek content that might be posted by authors in geographically disparate
locations, or content that includes pointers to external information sources via
URLs. Although Bing Social goes one step beyond the temporal recency attribute,
and yields URLs that have been shared widely among users on Twitter, the end user
might still intend to seek content that have been conversational on Twitter (to know
about conﬂicting or agreed upon opinions), or wish to see tweets spanning a variety
of themes on a topic (e.g., political versus economic perspectives).
Hence it is intuitive that while exploring or searching for social media content on
a given topic, an end user might like information ﬁltered by only a speciﬁc attribute
(i.e. information that is homogeneous), or can be interested in content that features a
“mixing” over a wide array of attributes (i.e. information that is heterogeneous). We
take an example for each case. Suppose an end user is looking for relevant Twitter
content after the release of the Windows Phone in November 2010. It would be
natural to display tweets that are homogeneous in terms of authorship, i.e. tweets
7Supported by the Hufﬁngton Post article: http://www.hufﬁngtonpost.com/2010/04/14/twitter-
user-statisticsr n 537992.html, Apr. 2010.
8http://www.bing.com/social/
9http://www.search.twitter.com/

24
Information Quality and Relevance in Large-Scale Social Information Systems
623
posted primarily by the technical experts. On the other hand, if the user wanted to
learn about the oil spill in the Gulf of Mexico that took place in summer of 2010,
a good set of social media items for the user to explore would span over a range of
attributes like author, geography and themes such as Politics or Finance.
3.2
Subjectivity of Relevance
Note that an outstanding challenge in this problem is the subjective notion of
relevance; and hence how to assess the quality of topic-centric sets of social
media content, especially in the face of absence of any ground truth knowledge.
Relevance performance has traditionally been addressed objectively in informa-
tion retrieval contexts using metrics such as precision/recall [3], new ranking
mechanisms [11, 21], relevance feedback [3], eye gazing patterns [5], quantifying
expected contribution of the retrieved information in accomplishing the end user
task [29] and so on. However except for a very few pieces of prior research
that has considered user perception based metrics in the context of information
retrieval [9, 14, 31], evaluation of the subjective notion of relevance remains fairly
under-investigated.
3.3
Proposed Contributions
Our answer to this question is motivated by two key factors: (1) characteristics
of the generated social media content, and (2) the potential to assess the quality
of the topic-centric sets of social media content using measures related to human
information processing.
Note that prior research [32] suggests that individuals’ involvement and percep-
tion of informational items differs signiﬁcantly depending on the attributes of the
item presented, such as what the topic is about or how diverse the information is.
As a consequence, there can be different sets of attributes, or variable degrees of
information diversity across those attributes, that an end user will ﬁnd useful when
exploring information on a topic.
Second, we propose assessing the quality of topic-centric results sets by mea-
suring aspects of human information processing when users are engaged with the
content. Because there may not be a clear best result in the same way that there is a
best web page result for many web queries, we assume that the best information will
be interpreted as interesting and informative, and will be more engaging to the user
during reading [8] and better remembered later [26, 27]. These cognitive measures
should align with the aforementioned attributes of the information space. As an
example, if a user wanted to learn about the oil spill in the Gulf of Mexico that took
place in summer of 2010, a good set of social media items for the user to explore

624
M. De Choudhury
would match a desired level of diversity in characteristics like author, geography and
topic classiﬁcation such as politics or ﬁnance, and also be informative, engaging,
and memorable.
In this light, the following are the major contributions of the work discussed in
this case study [12]:
•
Characterize social media information spaces through an entropy-based measure
known as diversity that captures the relative representation of different attributes
featured in the information.
•
Identify the importance of different informational attributes in deﬁning informa-
tion relevance, based on feedback from a number of users.
•
Propose a content selection methodology for social media content exploration
that attempts to construct relevant item sets on a given topic by matching a
desired level of diversity. The method is motivated by information theoretic
concepts in which sets of information units are progressively constructed from
the overall media stream such that the entropy of the new set has minimum
distortion with respect to the desired degree of diversity.
3.4
Content Selection Methodology
We begin by characterizing the notion of social media diversity, followed by the
various dimensions used to describe such content.
3.4.1
Diversity Spectrum
Social media content today features a wide array of attributes, ranging from
numerous geographic locations, extent of diffusion of the topic in the associated
social network, and so on. As a consequence, social media information spaces are
inherently diverse.
In this light, we conjecture that the presented content to an end user should match
a certain level of diversity, or breadth, that is cognitively conducive to his or her
task. We thus deﬁne a conceptual structure that characterizes the nature of the social
media information space in terms of “entropy” [7,16]. Considered in an information-
theoretic sense, the diversity of content quantiﬁes the degree of “randomness” or
“uncertainty” in the data.10 This structure is called the “diversity spectrum”.
10The diversity index of a sample population has been widely used by researchers in different
areas ranging from economics, ecology and statistics, to measure the differences among members
of the population consisting of various types of objects. Although there are a host of measures
to estimate such diversity (e.g., species richness, concentration ratio, etc.), the most popular and
robust measure by far is Shannon’s entropy based quantiﬁcation [16]. This motivated us to utilize
an information theoretic formulation to represent the diversity existing in social information spaces.

24
Information Quality and Relevance in Large-Scale Social Information Systems
625
Table 24.1 Description of different social information dimensions (posts on Twitter, or tweets, in
this context)
1.
Diffusion property of the tweet—measured via whether the given tweet is a “retweet”
(RT tag)
2.
Responsivity nature of the tweet—measured via whether a given tweet is a “reply” from
one user to another
3.
Presence of external information reference in the tweet—whether the tweet has a URL
in it
4.
Temporal relevance of the information, i.e., time-stamp of posting of the tweet
5.
The thematic association of the tweet within a set of broadly deﬁned categories—such
as “business, ﬁnance”, “politics”, “sports” or “technology, internet”. This association
is derived using the natural language toolkit, OpenCalais (www.opencalais.com) that
utilizes the content of the tweet, as well as the information about any URL that it
might contain, to return a thematic distribution over the tweet. Note that the set of
topics is pre-deﬁned by the OpenCalais domain; hence making the topical
associations of tweets to be semantically meaningful as against standard topics
models prevalent in the machine learning community. Nevertheless, we acknowledge
that alternatively, any popular topic model that performs clustering of textual content
into topics (in an unsupervised manner) can be used
6.
Geographic dimension of the tweet—measured via the time-zone information on the
proﬁle of the tweet creator
7.
Authority dimension of the creator of the tweet—measured via the number of followers
of the user who posts the particular tweet
8.
Hub dimension of the creator of the tweet—measured via the number of
followings/friends of the user who posts the particular tweet
9.
Degree of activity of the creator of the tweet—measured via the number of statuses of
the user who posts the particular tweet; i.e., the number of tweets the creator had
posted up to that point in time
3.4.2
Content Dimensions
We deﬁne several attributes (referred to as “dimensions”) along which we can
explore and organize social media information content given a particular topic [12].
A description of the different dimensions used in our work is given in Table 24.1.
We note here that because we use Twitter as our test social media platform, some
of our content dimensions are Twitter-speciﬁc. However the method and analysis
presented in this paper are generalizable over other forms of dimensions deﬁned
in the context of other social media. For example, if our interest is to determine
relevant social media content appearing on an individual’s Facebook News Feed, an
important dimension for content selection might be how frequently the individual
responds to the particular type of content (via “comments” and “likes”).
For comparison across dimensions as well as across tweets, we normalize the
dimensions using a simple logistic function of the form, 1=.1 C exp .ai//, where
ai is the i-th dimension of a tweet.
Given, (1) a stream of tweets from all users in a time span, and ﬁltered over
a certain topic , say, T; (2) a diversity parameter !; and (3) a set size s, our
goal is to determine a (sub-optimal) tweet set, T 
! .s/, such that its diversity level

626
M. De Choudhury
(or entropy) is as close as possible to the desired ! and also has a suitable ordering
of tweets in the set in terms of the entropy measure. This involves the following
steps: (a) estimating the importance of the different dimensions that characterize
the tweet information space; (b) developing a greedy approach-based optimization
technique that minimizes the distortion of a set of tweets from the desired diversity
parameter; and ﬁnally (c) organizing the tweets in the result set based on the relative
distances of their entropies from the desired diversity of the entire set.11
3.4.3
Generating Content Sets
We present our proposed social content exploration methodology in this section. We
start with a ﬁltered set of tweets T, or simply T corresponding to the topic . Let
ti 2 R1K be the dimensional representation of a tweet for a set of K dimensions.
Our goal is: (1) to determine a set of tweets of a certain size s, such that it
corresponds to a pre-speciﬁed measure of the diversity parameter on the diversity
spectrum, given as !, and (2) develop an organizational framework for the selected
set of tweets in the set, such that it enforces some ordering on the nature of the
content in terms of entropy.
We refer to this step as entropy distortion minimization, for the purpose of
yielding a set of tweets of size s and corresponding to a pre-speciﬁed diversity.
To construct the set T 
!.s/ that needs to be presented to a speciﬁc user on a topic
of diversity !, we start with an empty set, and pick any tweet from T at random.
We iteratively keep on adding tweets from T , say ti, such that the distortion (in
terms of `1 norm) of entropy of the sample (say, T i
!) on addition of the tweet ti is
least with respect to the speciﬁed diversity measure !. That is, we agglomeratively
choose tweet ti 2 T , whose addition gives the minimum distortion of normalized
entropy of T i
! with respect to !, where ! is simply the pre-speciﬁed diversity
parameter, as speciﬁed on the diversity spectrum. This can be formalized as follows:
ti 2 T i
! if and only if, kHO.T i
!/  !k`1 < kHO.T j
! /  !k`1; 8tj 2 T , where
HO.T i
!/ D  PK
kD1 P.ti k/  log P.ti k/=Hmax, ti 2 T and Hmax being given as
ln K:
arg
min
ti 2T ;ti …T .i1/
!
kHO.T i
!/  !k`1;
(24.1)
Note that we continue the iterative process of adding a tweet ti to the sample
HO.T i
!/ until we hit the requisite size s. Finally, we get the optimal tweet set as:
T 
! .s/.
11Note that we do not make apriori assumptions about what value of the diversity parameter is
more desirable for the content selection task. Instead, diversity is a parameter in our experimental
design, and we provide discussions on how the choice of its value affects the end-user’s perception
of information consumption.

24
Information Quality and Relevance in Large-Scale Social Information Systems
627
3.5
Experimental Evaluation
3.5.1
Experimental Setup
We now discuss the generation of tweet sets for content exploration based on
Twitter data. We utilized the “full ﬁre hose” of tweets and their associated user
information over the month of June 2010 [12]. This dataset was made available to
our company through an agreement with Twitter. The different pieces of information
we used in this paper (in anonymized format) were: tweet id, tweet text, tweet
creator’s id, tweet creator’s username, reply id, reply username, posting time, tweet
creator’s demographics, such as number of followers, number of followings, count
of status updates, time-zone and location information. The entire dataset comprised
approximately 1.4 Billion tweets, with an average of 55 Million tweets per day.
The data were segmented into 24 h long logs, a dimensional representation of the
tweets was generated based on the content dimensions discussed above, and ﬁnally
the proposed content exploration algorithm was run on each of them, given a certain
topic and a pre-speciﬁed diversity parameter value. This process generated tweet
sets with three pieces of information for each tweet: the tweet content, the username
of its creator and its posting time. The size of the tweet sets was determined based
on pre-speciﬁed “sizes”, such as a ten-item sized tweet set.12
Baseline Techniques
We compared our proposed social media content selection method to three baseline
techniques and to two versions of current state-of-the-art methods:
1. Baseline 1 (or B1): where we (a) do not use the entropy distortion minimization
technique for selecting tweets for exploration; instead tweets are selected based
on a random range of entropies; and (b) use an unweighted distribution of the
tweet dimensions.
2. Baseline 2 (or B2): where we (a) do not use the entropy distortion minimization
technique; instead tweets are selected based on a random range of entropies; and
(b) use the user survey based rating on different dimensions as the corresponding
weights.
3. Baseline 3 (or B3): where we (a) use our proposed entropy distortion minimiza-
tion technique for selecting tweets for exploration; and (b) use an unweighted
distribution of the tweet dimensions.
12Although our proposed content selection technique can generate tweet sets of any given size,
we considered sets of a reasonably small size (ten items) in our experimental design. The goal
was to ensure that while going through the user study and evaluating different sets, the end-user
participant was not overwhelmed by the quantity of information presented.

628
M. De Choudhury
Table 24.2 Participant
demographics
Topics
Participants
Male(%)
Female(%)
Age
“Oil spill”
29
75
25
28
“iPhone”
37
56
44
25
4. Most Recent Tweets (or MR): where we generate a set of tweets of a pre-speciﬁed
size, based on their timestamps of posting. Filtered by a topic, the tweet set
comprises the tweets with the ‘most recent’ timestamp on the particular day
under consideration.
5. Most Tweeted URL-based tweets (or MTU): where we determine all the URLs
that were shared (via tweets) on the particular topic and on the given day.
Thereafter we sort them by the number of times they were mentioned in different
tweets throughout the day. We generate the tweet set of a certain size s, by
selecting the top s most-tweeted URLs from the sorting process; and then
yielding the “ﬁrst” tweet on the same day that mentioned each of the s URLs.
In the remainder of this book chapter we will denote our proposed method of
content exploration as “Proposed Method” (PM).
3.5.2
User Study Design
We conducted a user study in order to compare our proposed technique to the
baseline methods we identiﬁed [12]. Sets of tweets generated by the different
methods were shown to participants in order to determine for which method the
presented content was most interesting, most informative, the most engaging and
memorable. Participants were 67 employees of a large technology company who
were compensated for their time with a $10 lunch coupon. Participants were
required to be Twitter users at least two times per week. We observe from Table 24.2
that the distribution of gender is slightly skewed towards male, reﬂecting the
demographics of our company, and the median age was less than 30.
We also explored how extensively participants used Twitter as a source of news
versus for connecting socially with others. The distribution of usage purposes
in Fig. 24.1 shows that most participants used it for both purposes, but that
usage is skewed towards the right side of the distribution. This suggests that our
participant population is relevant to our core scenario of topic-based microblog
consumption.
Measures
We included ﬁve dependent measures to evaluate user performance with the
different content selection techniques [12]. Our measures fell into two categories
that we refer to as explicit and implicit.

24
Information Quality and Relevance in Large-Scale Social Information Systems
629
Fig. 24.1 Usage distribution of Twitter as a “social” and “media” tool in our user study
Explicit Measures
Explicit measures consisted of three 7-point Likert scale ratings made after reading
each tweet set. The ratings corresponded to the following three aspects of tweet set
quality as perceived by the participant: interestingness and informativeness.
Implicit Measures
We used two measures considered as implicit because they were not based on direct,
explicit evaluation by participants. The ﬁrst was a normalized version of subjective
duration assessment [8], and we refer to it as “normalized perceived duration” or
NPD. NPD is computed using the function: .Di.j/
O
Di.j//=Di, where Di.j/ and
ODi.j/ are respectively the actual and perceived time taken to go through tweet set j
by the participant i. Note that ideally, if the information presented in the tweet set j
is very engaging, the participant would underestimate the time taken to go through
the tweet, and hence NPD would be a positive value [8]. This would support the idea
that the technique that was used to generate the set j was more engaging. In less
engaging scenarios, NPD has been shown to be negative, hence, relative comparison
across NPD measures of different techniques seems reasonable.
Our second implicit measure was recognition memory for tweets to which the
user saw verses tweets not seen. It is derived as: .jni.‘yes’jj/j=jn.j/j/, where
jni.‘yes’jj/j is the number of tweets from set j that participant i correctly
recognized as having seen and jn.j/j is the total number of tweets from set j that
appear in the recognition test. More memorable content read should generate better
scores on this recognition task.
3.5.3
Discussion of Results
We present the experimental results based on our user study in this section.We will
analyze the performance of the different methods in terms of the different evaluation
(cognitive) measures.

630
M. De Choudhury
Fig. 24.2 Mean performance of different measures along with standard errors
For the remaining four measures, the results are presented in Fig. 24.2. In this
ﬁgure, the results shown are collapsed across the three values of diversity d and
the two topics, “Oil Spill” and “iPhone”. We see that our proposed method (again,
that utilizes the entropy distortion minimization technique and uses user feedback-
based weighting of tweet dimensions) generally yields the best performance for
these measures as well.
Given the number of analyses, we present results of statistical comparisons in a
separate table (Table 24.3). In comparing our selection technique (PM ) to the other
methods, we observe that the most signiﬁcant difference was for the MR  PM
comparison. This indicates that the approach of showing the most recent tweets on
a topic (a commonly used technique) yields results sets that are less interesting, less
informative, less engaging to read, and less recognized later. Baseline 1, effectively
a random sample of on topic tweets, also performed poorly, though the improvement
of our method for degree of recognition was only trend level signiﬁcant (p < 0:1).

24
Information Quality and Relevance in Large-Scale Social Information Systems
631
Table 24.3 Statistical signiﬁcance of our proposed content exploration method against other baseline techniques
Interestingness
Informativeness
NPD
Deg. recognition
p
t
d
p
t
d
p
t
d
p
t
d
B1  PM
0.0028
2:86
7.83
0.0097
2:39
5.13
0.0074
2:51
0.45
0.0974
1:31
2.78
B2  PM
0.0278
1:95
6.95
0.1175
1:19
1.44
0.0104
2:37
0.45
0.1055
1:26
0.79
B3  PM
0.2401
0:71
3.94
0.3518
0:38
8.19
0.1386
1:09
0.49
0.4117
0:22
0.22
MR  PM
0.0003
3:59
14.1
<0.0001
4:28
452.8
0.0031
2:84
0.51
0.0052
2:64
1.58
MTU  PM
0.0607
1:57
6.22
0.1715
0:96
1.97
0.0041
2:72
0.63
0.2142
0:79
0.57

632
M. De Choudhury
Baseline 2 (B2), which incorporates the user weightings on the tweet dimen-
sions, but not the entropy minimization technique, also generated tweet sets that
were less interesting and less engaging to read than those from our proposed method.
Tweet sets containing the most tweeted URLs (MTU) performed fairly well, though
were less engaging to read and nearly less interesting (p D 0:06) than those
generated by our proposed method. Finally, Baseline 3 (B3), which is similar to
the proposed method except that it does not utilize the weightings on the tweet
dimensions, did not perform not signiﬁcantly differently from our proposed method.
To summarize the results, our predictions were largely conﬁrmed, with our
proposed method PM generally faring better across all measures. In particular,
simple but common techniques like showing a set of recent tweets on a topic, were
considerably worse than the proposed technique. A somewhat smarter approach
of focusing on tweets with commonly shared links performed better but still was
not rated as engaging to read. In terms of the two components of our proposed
method (entropy minimization and incorporating user-generated weights on tweet
dimensions), the two approaches in conjunction with one another seem to have had
the strongest effect (PM performed best overall), though the entropy minimization
component may be more helpful (B3 was closer in performance to PM than
was B2).
4
Conclusions
From the results of the case study it appears that even na´ıve methods of content
selection, such as methods that use an unweighted version of the dimensions or those
that do not attempt to match a desired level of diversity, seem to deliver a certain
degree of information utility to the user, say in terms of interestingness or in aiding
memory encoding of the information. We conjecture that this is due to the high
dimensionality and richness of social media content today: because tweets often
exhibit multifaceted dimensional attributes, at least some dimensions are always
likely to be of interest to some set of users for some tasks.
Even as the different tweet dimensions are better leveraged, the relatively high
dimensionality of the social media information space begs the question of whether
users are able to discern differences in levels of diversity. For example, a user
may ask for a less diverse set of items with respect to geography and topic
classiﬁcation in the hopes of ﬁnding content speciﬁc to economy related issues
surrounding a local election. As the system reduces the level of diversity in the
result set to accommodate this request, will this be noticeable to the user? Our
results suggest that it would be, but that the answer may be complicated, in that
users may have greater difﬁculty discerning variations in levels of diversity that
are closer to the middle of the scale. As mentioned, understanding the potentially
nonlinear relationship between actual and perceived diversity will help us better
design interfaces that allow users to scale sets of social media items along different
dimensions. Fleshing this out is an area for future research.

24
Information Quality and Relevance in Large-Scale Social Information Systems
633
Finally, we comment on our choice of measures. As indicated in the introduction,
evaluating topic-based sets of social media items may require different measures
than traditional web search results. For many results (though certainly not all) in
web search, there is a clear best result. If the user searches for ‘New York Times’,
the newspaper’s home page should be the top result. In contrast, there may never
be a single ‘best’ tweet for any given topic. Instead, we focused on the perceptions
of the user, both explicit (the tweets should be rated as informative and interesting)
and implicit (the tweets should be engaging to read and memorable). Our measures
were largely consistent with one another, with essentially the same pattern of results
across our tweet selection conditions for all measures. While we argue that implicit
measures are less subject to bias on the part of the user, we note that the recognition
measure faces the issue that even terrible results might be highly memorable for
many different reasons. Exploring additional measures is also an area for future
work.
Topic-based exploration appears to be gaining traction as a use case for social
media. In this chapter, via a case study we addressed two problems related to this
scenario: (1) what is the best technique for selecting results, and (2) how should
we measure the effectiveness of these techniques. This paper compared several
methods for selecting social media information content, with an eye toward the
notion that the best results are those perceived to be interesting and informative, and
are engaging to read and memorable. A signiﬁcant challenge and opportunity lies
in the fact that information generated over social media sites like Twitter features a
very high degree of diversity. The case study discussed here took advantage of this
diversity by selecting items that together matched a speciﬁed level of diversity in the
results. On the whole, the proposed method fared better than the baseline conditions,
particularly better than the recency-driven approach that is commonly seen. These
results bear on measurement techniques for social media content exploration and on
interface design in these high dimensionality information spaces.
References
1. Dimitris Achlioptas, Aaron Clauset, David Kempe, and Cristopher Moore. On the bias of
traceroute sampling: or, power-law degree distributions in regular graphs. In Proceedings of
the thirty-seventh annual ACM symposium on Theory of computing, STOC ’05, pages 694–703,
New York, NY, USA, 2005. ACM.
2. Eugene Agichtein, Carlos Castillo, Debora Donato, Aristides Gionis, and Gilad Mishne.
Finding high-quality content in social media. In Proceedings of the international conference
on Web search and web data mining, WSDM ’08, pages 183–194, New York, NY, USA, 2008.
ACM.
3. Ricardo A. Baeza-Yates and Berthier Ribeiro-Neto. Modern Information Retrieval. Addison-
Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1999.
4. Suh B. Hong L. Chen J. Kairam S. Bernstein, M. and E.H. Chi. Eddi: Interactive topic-based
browsing of social status streams. In ACM User Interface Software and Technology (UIST)
conference, 2010. To appear.

634
M. De Choudhury
5. Georg Buscher, Andreas Dengel, and Ludger van Elst. Query expansion using gaze-based
feedback on the subdocument level. In Proceedings of the 31st annual international ACM
SIGIR conference on Research and development in information retrieval, SIGIR ’08, pages
387–394, New York, NY, USA, 2008. ACM.
6. Jilin Chen, Rowan Nairn, Les Nelson, Michael Bernstein, and Ed Chi. Short and tweet:
experiments on recommending content from information streams. In CHI ’10: Proceedings of
the 28th international conference on Human factors in computing systems, pages 1185–1194,
New York, NY, USA, 2010. ACM.
7. Thomas M. Cover and Joy A. Thomas. Elements of information theory. Wiley-Interscience,
New York, NY, USA, 1991.
8. M. Czerwinski, E. Horvitz, and E. Cutrell. Subjective duration assessment: An implicit probe
for software usability. In Proceedings of IHM-HCI, pages 167–170, September 2001.
9. P J Daniels. Cognitive models in information retrieval—an evaluative review. J. Doc., 42:
272–304, December 1986.
10. Gautam Das, Nick Koudas, Manos Papagelis, and Sushruth Puttaswamy. Efﬁcient sampling of
information in social networks. In SSM, pages 67–74, 2008.
11. Anish Das Sarma, Atish Das Sarma, Sreenivas Gollapudi, and Rina Panigrahy. Ranking
mechanisms in twitter-like forums. In Proceedings of the third ACM international conference
on Web search and data mining, WSDM ’10, pages 21–30, New York, NY, USA, 2010. ACM.
12. Munmun De Choudhury, Scott Counts, and Mary Czerwinski. Identifying relevant social media
content: leveraging information diversity and user cognition. In Proceedings of the 22nd ACM
conference on Hypertext and hypermedia, HT ’11, pages 161–170, New York, NY, USA, 2011.
ACM.
13. Munmun De Choudhury, Y-R Lin, Hari Sundaram, K.S. Candan, Lexing Xie, and Aisling
Kelliher. How does the data sampling strategy impact the discovery of information diffusion
in social media? In ICWSM ’10: Proceedings of the 4th International Conference on Weblogs
and Social Media, Washington D.C., May 2010. AAAI Press, AAAI Press.
14. Nigel Ford. Modeling cognitive processes in information seeking: from popper to pask. J. Am.
Soc. Inf. Sci. Technol., 55:769–782, July 2004.
15. O. Frank. Sampling and estimation in large social networks. Social Networks, 1(91):101, 1978.
16. Lou Jost. Entropy and diversity. Oikos, 113(2):363–375, May 2006.
17. W. Kellogg. Information rates in sampling and quantization. Information Theory, IEEE
Transactions on, 13(3):506 – 511, jul 1967.
18. Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue Moon. What is twitter, a social
network or a news media? In Proceedings of the 19th international conference on World wide
web, WWW ’10, pages 591–600, New York, NY, USA, 2010. ACM.
19. J. Leskovec and C. Faloutsos. Sampling from large graphs. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge discovery and data mining, page 636. ACM,
2006.
20. Arun S. Maiya and Tanya Y. Berger-Wolf. Sampling community structure. In Proceedings of
the 19th international conference on World wide web, WWW ’10, pages 701–710, New York,
NY, USA, 2010. ACM.
21. Qiaozhu Mei, Jian Guo, and Dragomir Radev. Divrank: the interplay of prestige and diversity
in information networks. In Proceedings of the 16th ACM SIGKDD international conference
on Knowledge discovery and data mining, KDD ’10, pages 1009–1018, New York, NY, USA,
2010. ACM.
22. Owen Phelan, Kevin McCarthy, and Barry Smyth. Using twitter to recommend real-time
topical news. In Proceedings of the third ACM conference on Recommender systems, RecSys
’09, pages 385–388, New York, NY, USA, 2009. ACM.
23. P. Rusmevichientong, D.M. Pennock, S. Lawrence, and C.L. Giles. Methods for sampling
pages uniformly from the world wide web. In AAAI Fall Symposium on Using Uncertainty
Within Computation, pages 121–128, 2001.

24
Information Quality and Relevance in Large-Scale Social Information Systems
635
24. Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. Earthquake shakes twitter users:
real-time event detection by social sensors. In Proceedings of the 19th international conference
on World wide web, WWW ’10, pages 851–860, New York, NY, USA, 2010. ACM.
25. Marc Smith, Vladimir Barash, Lise Getoor, and Hady W. Lauw. Leveraging social context for
searching social media. In Proceeding of the 2008 ACM workshop on Search in social media,
SSM ’08, pages 91–94, New York, NY, USA, 2008. ACM.
26. S. M. Smith. Remembering in and out of context. Journal of Experimental Psychology: Human
Learning and Memory, 5(5):460–471, 1979.
27. G. Sperling. A model for visual memory tasks. Human Factors, 5:19–31, 1963.
28. D. Stutzbach, R. Rejaie, N. Dufﬁeld, S. Sen, and W. Willinger. Sampling techniques for large,
dynamic graphs. In INFOCOM 2006: Proceedings of the 25th IEEE International Conference
on Computer Communications, pages 1–6. IEEE, April 2006.
29. Pertti Vakkari. Relevance and contributing information types of searched documents in task
performance. In Proceedings of the 23rd annual international ACM SIGIR conference on
Research and development in information retrieval, SIGIR ’00, pages 2–9, New York, NY,
USA, 2000. ACM.
30. Steve Whittaker and Candace Sidner. Email overload: exploring personal information man-
agement of email. In CHI ’96: Proceedings of the SIGCHI conference on Human factors in
computing systems, pages 276–283, New York, NY, USA, 1996. ACM.
31. Yunjie (Calvin) Xu and Zhiwei Chen. Relevance judgment: What do information users consider
beyond topicality? J. Am. Soc. Inf. Sci. Technol., 57:961–973, May 2006.
32. Judith Lynne Zaichkowsky. Measuring the involvement construct. Journal of Consumer
Research: An Interdisciplinary Quarterly, 12(3):341–52, 1985.


Chapter 25
Geospatial Data Management with Terraﬂy
Naphtali Rishe, Borko Furht, Malek Adjouadi, Armando Barreto,
Evgenia Cheremisina, Debra Davis, Ouri Wolfson, Nabil Adam,
Yelena Yesha, and Yaacov Yesha
1
Introduction
March 11, 2011, 2:46 pm – A magnitude 9.0 earthquake strikes just off the northeast coast
of Japan. Within an hour, a tsunami estimated at over 30 feet high hits the coast of Japan,
sweeping away entire villages and killing tens of thousands of people [1].
N. Rishe () • M. Adjouadi • A. Barreto • D. Davis
NSF Industry-University Cooperative Research Center for Advanced Knowledge, Enablement
(CAKE.ﬁu.edu) at Florida International, Florida Atlantic and Dubna Universities, Miami, FL,
USA
e-mail: rishe@ﬁu.edu; adjouadi@ﬁu.edu; Armando.Barreto@ﬁu.edu; dledavis@cs.ﬁu.edu
B. Furht
NSF Industry-University Cooperative Research Center for Advanced Knowledge, Enablement
(CAKE.ﬁu.edu) at Florida International, Florida Atlantic and Dubna Universities, Boca Raton,
FL, USA
e-mail: borko@cse.fau.edu
E. Cheremisina
NSF Industry-University Cooperative Research Center for Advanced Knowledge, Enablement
(CAKE.ﬁu.edu) at Florida International, Florida Atlantic and Dubna Universities, Moscow,
Russia
e-mail: e.cheremisina@geosys.ru
O. Wolfson
Computational Transportation Science Program (CTS.cs.uic.edu), University
of Illinois at Chicago, Miami, FL, USA
e-mail: wolfson@cs.uic.edu
N. Adam
Science and Technology Directorate, U.S. Department of Homeland Security
(DHS.gov), Washington, DC, USA
e-mail: Nabil.Adam@dhs.gov
Y. Yesha • Y. Yesha
NSF Industry-University Cooperative Research Center for Multicore Productivity,
Research (CHMPR.umbc.edu) at the University of Maryland Baltimore County,
Baltimore, MD, USA
e-mail: yeyesha@umbc.edu; yayesha@umbc.edu
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 25, © Springer Science+Business Media, LLC 2011
637

638
N. Rishe et al.
April 20, 2010 – A massive explosion on the BP Deepwater Horizon drilling platform in the
Gulf of Mexico caused the largest oil spill in US history, killing 11 workers and spilling an
estimated 4.9 billion barrels of oil [2].
Disasters, whether environmental or manmade, have catastrophic impacts that
require both quick action and long-term interventions. Mitigating the effects of those
disasters requires knowledge about similar events and advanced disaster planning.
Major challenges in disaster planning and intervention include a lack of up-to-
date information on situational and environmental conditions, major communication
gaps, and a lack of effective coordination in planning and recovery operations. With
a worldwide average of 387 natural disasters a year alone [3], it is imperative that
solutions are found to combat and eliminate these problems. The implementation
of cutting-edge technology, in particular, is key to advancement of the science and
solutions for disaster mitigation [4,5].
1.1
The Challenge
Implementing the types of cutting-edge technology needed for the diverse needs
of disaster-related solutions is complex and challenging. Massive amounts of
data are required, and this data is often heterogeneous, from divergent sources,
and consists of both structured and unstructured data. Geospatial and remotely-
sensed data, such as geo-referenced satellite imagery and aerial photography,
provides particularly critical information that either is not available in other forms,
or is not otherwise easily conveyed. This type of data, however, is inherently
very large, thus signiﬁcantly increasing the complexity of possible technological
solutions [6–10].
There are also numerous challenges with the use of existing GIS tools for
processing and analyzing geospatial data [8]. The primary challenges involved
include:
1. The use of multiple, disparate tools are often necessary, some of which are
expensive, and require specialized skills and training to use;
2. The data must often be imported into these separate tools, each of which may
require the data to be in different formats; and
3. The ability to combine heterogeneous types of data is not always possible using
the GIS tools currently available.
Another difﬁculty is that the typical end-users’ technological backgrounds are very
diverse, ranging from scientists to disaster mitigation and recovery planners to on-
the-ground disaster responders. This requires the availability of robust systems and
tools that are very ﬂexible and easy to use, so that end-users can focus on the work
that they need to get done without having to worry about the technology behind the
system they are using [11,12]. Because of the nature of disasters, new and updated

25
Geospatial Data Management with Terraﬂy
639
information needs to be made available in a timely manner, and in a form that can
quickly and efﬁciently be consolidated and conveyed to multiple, diverse users at
the same time.
Interestingly, the challenges faced by those in the disaster mitigation ﬁeld are
not unique. There are numerous ﬁelds that rely on the use of geospatial and
remotely sensed data. The same types of technological solutions designed for
disaster mitigation can also be implemented in and have major impacts on other
ﬁelds. The most apparent are ﬁelds in scientiﬁc discovery such as ecological and
environmental research, archeology, oceanography, and meteorology, among many
others. Other ﬁelds that have more ordinary, day-to-day impact on people’s lives
would also greatly beneﬁt from similar solutions. This includes areas ranging from
government operations such as public works and urban planning to business ﬁelds
such as real estate and tourism.
For example, in real estate, there are many variables that affect the value and
desirability of a particular property: neighborhood, local businesses, crime rates,
roads and transportation, school quality, level of urbanization, etc. As with disaster
preparedness and recovery, the data necessary to provide needed information
often comes from diverse, heterogeneous data sources that may be structured or
unstructured. Geospatial data, once again, can provide information that is otherwise
not available or in a format that is difﬁcult to interpret. All this data must be
collected, consolidated, analyzed and visualized in an easy to understand format to
be effectively utilized. This challenge is further compounded in that the technical
backgrounds of potential users are very diverse, ranging from engineers and
surveyors, to developers, real estate agents and potential buyers.
1.2
A Solution: TerraFly
Identifying a solution to these intense and complex issues may seem overwhelming.
Through the use of innovative research and cutting-edge technology, TerraFly
has been created to provide a ﬂexible, robust and forward-thinking solution to
these multifaceted challenges. Speciﬁcally, TerraFly is designed to efﬁciently and
effectively deal with the challenges involved in handling and analysis of massive
amounts of heterogeneous geospatial and related data, as well as with the challenges
that users encounter when attempting to use traditional GIS tools.
This article discusses geospatial data analytics using TerraFly as a case study.
An overview of TerraFly will ﬁrst be presented, followed by discussions of
TerraFly’s capabilities in data handling, information visualization, ﬂexibility and
customization, and speciﬁc domains in which TerraFly is currently used, such as
disaster planning and recovery, science and research, real estate and travel.
TerraFly is a technology and tools for visualization and querying of geospatial
data. The visualization component of the system provides users with the experience
of virtual “ﬂight” over maps comprised of aerial and satellite imagery overlaid
with geo-referenced data. The data drilling and querying component of the system

640
N. Rishe et al.
allows the users to easily explore geospatial data, to create geospatial queries,
and get instant answers supported by high-performance multidimensional search
mechanisms. TerraFly’s server farm ingests, geo-locates, cleanses, mosaics, and
cross-references 40 TB of basemap data and user-speciﬁc data streams. TerraFly’s
Application Programming Interface allows rapid deployment of interactive Web ap-
plications; it has been used to produce systems for disaster mitigation, ecology, real
estate, tourism, and municipalities. TerraFly’s Web-based client interface is accessi-
ble from anywhere via any standard Web browser, with no client software to install.
2
TerraFly’s Advanced Geospatial Data Processing
2.1
Overview
The cornerstones of TerraFly is in its portability, ﬂy-over technology, ability to
integrate multiple sets of geospatial and related data into customizable, multi-
layered products, and inclusion of powerful but easy to use visualization, querying
and analysis tools. As can be seen on TerraFly’s landing page (see Fig. 25.1) [13],
users are able to easily select the geographic area they are interested in exploring.
By streaming incremental imagery tiles, TerraFly enables users to engage in virtual
ﬂights (see Fig. 25.2) where they maintain full control over ﬂight speed, direction
and altitude (spatial resolutions) via an intuitive, ﬂash-based navigation system.
Fig. 25.1 TerraFly landing page

25
Geospatial Data Management with Terraﬂy
641
Fig. 25.2 TerraFly ﬂight and data layers control layout
TerraFly’s data-mining tools are capable of delivering an extensive amount of
data related to user-speciﬁed geographical locations [14–17]. Unlike most GIS
applications [18], TerraFly eliminates the need for the end-user to deal with
any technical aspects of the system. Users are able to easily query for data of
interest, and have that data automatically visualized in the form of non-obstructing
geo-referenced overlays, or data layers, combined with spatial imagery [19–22].
The most popular types of overlaid data include NAVTEQ NAVSTREETS street
vectors, World OpenStreetMaps, property parcels, Yellow pages, White pages,
demographics, Geographic places (Worldwide from NGA and other sources, USGS
Geographic Names Information System), services, hotels, and real estate listings.
This is just a sampling of datasets.
In addition to data overlays, TerraFly provide users with a drill-down detailed
information page on a point or area (see Fig. 25.3). For example, users can use
TerraFly’s address locator capability to “ﬂy” to a speciﬁed address, and then request
more speciﬁc information about that particular location. To do this, the user clicks
on the particular point of interest on the spatial image. A preview page will pop up
in the ﬂight window that contains a summary of information about that particular
location, along with links to more detailed location information. To view the more

642
N. Rishe et al.
Fig. 25.3 Sample of TerraFly’s data drill down
detailed information, the user clicks on the associated link in the preview page, and
the user will be taken to a new page where more detailed local information, such
as demographic data, local restaurants and businesses, etc. is displayed below the
ﬂight window (see Fig. 25.4).
The TerraFly system has querying and analysis capabilities that are the result of
intensive, cutting-edge and innovative computing research. The tools available in
TerraFly include user-friendly geospatial querying, data drill-down, interfaces with
real-time data suppliers, demographic analysis, annotation, route dissemination via
autopilots, customizable applications, production of aerial atlases, and an applica-
tion programming interface (API) for web sites. Many of TerraFly’s capabilities and
the technology behind them, such as TerraFly’s underlying data storage mechanism,
client-server interaction, user interface, ability to overlay additional information
layers, and ergonomics of use and maintenance, have been described in numerous
professional publications [23–30]. More recent advancements in TerraFly’s data
handling capabilities, namely improved user-centric data integration and mapping
tools, data repository, and advanced data indexing and preprocessing, are described
in the remainder of this section. Advances in TerraFly’s data visualization capa-
bilities, such as times series visualization, the customizable autopilot and the data
dispenser systems, are described in Sect. 3.

25
Geospatial Data Management with Terraﬂy
643
Fig. 25.4 Demographics and quality of life overview page
2.2
Data Repository
A critical component of TerraFly is its data repository, and a major strength lies
in its integration of heterogeneous data sources including relational and semantic
databases and web sources with spatial data. TerraFly’s data repository was one of
the ﬁrst GIS databases that were able to store heterogeneous data in one database
[23–26,31,32]. As with other GIS tools, there are two main types of geo-referenced
spatial data that TerraFly must handle: raster (satellite and aerial photography) and
vector data (points, lines and polygons) [6,33]. TerraFly’s data repository currently
stores the following:
Raster Data. The entire collection of Digital Orthophoto Quarter Quadrangle
(DOQQs) produced by the USGS (1 m-resolution orthorectiﬁed aerial photography
of the entire USA, including once-over 13 TB coverage and 5 TB of multi-temporal
updates); the entire collection of USGS Urban Area High Resolution Orthoimagery
(15–30cm imagery covering 135 metropolitan areas – 15 TB), and Landsat imagery
[34] covering USA and parts of the world, imagery from local sources (7 cm/pixel
and up), and a vast collection of satellite imagery, particularly from GeoEye and
Ikonos satellites.
Vector Data. The TerraFly vector collection includes 1.5 billion geolocated objects,
50 billion data ﬁelds, 1 billion polylines, 120 million polygons, including: all World
roads, Worldwide geographic places and features, 24 billion demographic data
items (3,000 ﬁelds  8 million objects), 1 billion economic data items including
the US Census demographic and socioeconomic datasets [35], 110 million USA

644
N. Rishe et al.
cadastre polygons and detailed data on each parcel, DEM Elevation data, 15 million
records of businesses (with company stats, management roles, contacts and radius
demographics for each business), 2 million physicians with expertise detail, various
public place databases (including the USGS GNIS [36] and NGA GNS), Wikipedia,
extensive global environmental data (including daily feeds from NASA and NOAA
satellites and the USGS water gauges), and hundreds of other datasets.
New Data is constantly being updated and added to TerraFly’s data repository. For
a current listing of available data, please see [37].
2.3
TerraFly’s Advanced Data Geocoding Capabilities
A commonly used feature of GIS and mapping systems is the ability to geocode
street addresses. As with most GIS and mapping applications, precise and accurate
geocoding of available data in TerraFly is critical. If implemented with sufﬁcient
precision, this ability can satisfy the needs of many businesses’ day-to-day functions
(e.g., realtors, attorneys, engineers, etc.), as well as the more complex needs of
government and research (e.g., information retrieval from spatial databases) [38].
For many years, most mapping systems have used standard interpolation geocoding
that estimates where on a street a particular address is located. There are a number
of assumptions associated with these standard methodologies, such as consistent
standardization of street numbers in all areas and substantial accuracy of the
underlying data. Because it is an estimate, and not a coordinate point associated
with a speciﬁc building, this type of geocoding has a certain level of inaccuracy
and often results in near misses. For example, when using one of these standard
methods, the resulting point often lies outside of the parcel property lines of the
building of interest and sometimes a few buildings away.
To address this issue and to provide the signiﬁcantly higher level of precision
required for many TerraFly applications, roof-top geocoding has been implemented
in TerraFly via the inclusion of First American Parcel Point Nationwide Cadastre
data [39] along with efﬁcient data management algorithms. This data set contains
attributes that are intended to support data integration related to land parcels
across jurisdictional boundaries, and includes parcel boundaries, parcel centroid,
addresses, Assessor’s Parcel Numbers (APN) and ownership information [39, 40]
(ownership information is not available in all counties).
After initial data cleanup to remove data with missing components and incor-
rectly formatted records, the data was cross-referenced with other datasets and
a precise geocoding component was created for TerraFly. Speciﬁcally, TerraFly’s
rooftop geocoder was created using spatial indexes and data structures already
used in TerraFly, with street address interpolation and string matching algorithms.
In short, the process is as follows:
1. When a query for a particular address is made, TerraFly uses a standard interpola-
tion methodology to generate approximate coordinates for the requested location.

25
Geospatial Data Management with Terraﬂy
645
2. These coordinates are then used to perform a nearest neighbor query to retrieve
any nearby parcels from the data set.
3. A local search for the best matching parcel is performed on the results.
4. If a match is found for that parcel in the database, the coordinates for that record
are returned to the user.
TerraFly also has location-sensitive geocoding. If the user is focused in a particular
location and provides a partial address or a partial description of a geographic or
social place, the system will look for the best match, weighing the factors of place
importance and its proximity to the user’s current location.
2.4
Image Mosaics, Raster Data Analysis and Amelioration
As is seen in Sect. 2.2, TerraFly has nearly 40 TB of aerial and satellite imagery in
its data repository. As with vector data, TerraFly requires high quality raster data
to perform many of its applications. However, not all of the imagery is of a high
enough quality for appropriate visualization, and sometimes multiple images for the
same locations, are shot at different times or by different instruments. Nevertheless,
some users desire to see original unaltered imagery. Most users, however, desire to
see a pixel-by-pixel mosaic of the best imagery available. “Best” is a user group
speciﬁc criterion, and TerraFly accepts various deﬁnitions of what “best” means for
a particular user group. The default “best” is the freshest, the sharpest, and the most
natural-color imagery. Thus, in preparation for mosaicing, TerraFly performs image
analysis on its raster data.
Two different types of imagery analysis are conducted to improve TerraFly’s
image quality: (1) detection of black regions in individual tiles and (2) histogram
analysis on an entire data set. When imagery data is acquired or post-processed,
individual data tiles may contain areas of black interspersed or surrounded by good
quality imagery. As a result, detection of black regions is performed tile by tile,
and results of the analysis are stored as meta-data inside each tile. When tiles are
retrieved, this meta-data is accessed and algorithms are used to determine how to
best mosaic that particular tile with better quality imagery for the best quality output
to the user.
Entire data sets are sometimes affected by color distortion that is not easily
detected when imagery is analyzed on a tile by tile basis. Therefore, histogram
analysis is conducted on the entire data set. To provide end users with the most
accurate and ﬂexible data product, results of the histogram analysis are stored
separately from the original data set. This provides the user with the option of seeing
either the original or corrected imagery.
Both of these types of image analysis involve data-intensive computing, par-
ticularly on the large data sets inherent to spatial data. TerraFly’s raster analysis
applications have been ported to MapReduce [41,42], a highly efﬁcient framework
that automates the use of parallel processing through the use of mappers and

646
N. Rishe et al.
reducers (see [43] for more details on MapReduce). For detection of black regions,
each mapper is assigned a certain number of tiles to analyze, depending on the
size of the data set and the number of mappers. No reducer is needed for further
processing. For histogram analysis, each mapper is responsible for analyzing a
portion of the data set, and the partial result is sent to the reducer. The reducer
then combines all of the results and computes the ﬁnal output. TerraFly’s use of
MapReduce has resulted in a dramatic improvement in computing time, and has
shown close to linear scalability [41].
2.5
Spatial Keyword Indexing (SKI)
Spatial data is inherently very large, complex and often heterogeneous in na-
ture. This makes meticulous and efﬁcient data management a major challenge,
particularly when dealing with extremely large databases such as TerraFly’s data
repository. However, appropriate data indexing can be used to make querying of
data more efﬁcient. To address this and improve performance, TerraFly includes
an innovative, hybrid method to efﬁciently process top-k spatial queries with
conjunctive Boolean constraints on textual content [16].
Speciﬁcally, this method combines an R-tree structure and text indexing into a
location-aware inverted index by using spatial references in posting lists. R-Trees
are often used as an indexing mechanism for spatial search query processing [18,
44,45]. In the TerraFly-SKI hybrid method, an R-tree index is modiﬁed in the upper
level nodes with the addition of a list of leaf nodes that have the same parent. An
inverted ﬁle is altered to contain a list of pointers to some of the R-tree’s nodes,
creating a spatial inverted ﬁle. To process a query, the R-tree is traversed in a top-
down method using a best-ﬁrst traversal algorithm. If at least one object exists that
satisﬁes the Boolean condition in the subtree, then a node entry is made into the
priority queue. Otherwise, the unnecessary subtree traversal is eliminated. The result
is a disk-resident, dual-index data structure that is used to effectively and proactively
prune the search space [16].
Although this method produces improved performance, the indexing process can
take a substantial amount of time. How much time is needed depends on the size
of the database, as well as the size of the lexicon found in the spatial inverted ﬁle.
For a database that contains N objects, where those objects are used to construct the
SKI’s modiﬁed R-tree, the number of insert operations is O.N /. When constructing
the spatial inverted ﬁle, the most expensive operation involves sorting the lexicon,
with a construction time of O.N C V log.V //, where V is the size of the lexicon.
MapReduce has been used to improve image processing computing time for data
in TerraFly’s data repository [41]. The use of parallel computing to improve the
efﬁcient construction of inverted indices has been studied by other researchers [43].
TerraFly’s work with MapReduce has been leveraged to improve processing time in
SKI construction. In this process, the R-tree structure is built with two MapReduce
pairs as in [2]. The output includes references to R-tree nodes as intermediate data
for use in the following job. Considering each object as a document, a MapReduce

25
Geospatial Data Management with Terraﬂy
647
job also builds the spatial inverted ﬁle on the database lexicon. The MapReduce
compound uses the intermediate data generated in the ﬁrst iteration.
The resulting SKI’s data structures are stored remotely in the Cloud, and are
downloaded to a local host to serve interactive queries. In this process, the SKI data
structures are partitioned because the number of “smaller” SKI structures is equal
to the number of Reducers used in the MapReduce job. As a result, queries are
processed with a modiﬁed version of the search algorithm proposed in [16,27].
3
Advanced Data Visualization Capabilities
New and innovative technologies and functionality are continually being developed
and added to TerraFly’s already extensive capabilities. Section 2 presented key
advances in TerraFly’s backend and data handling capabilities. Those technological
advances are functions that end-users may not be entirely aware of, but that affect
them and the quality of their work substantially. In this section, we discuss some
of TerraFly’s most recent advancements in data visualization and user-centric
capabilities. Although equally as important as the advances made on the back end,
end users ability to interact with the system is more directly affected by advances to
TerraFly’s visualization engine. Speciﬁcally, this section discusses TerraFly’s time
series visualization capabilities, auto pilot, and data dispenser.
3.1
Time Series Visualization
A powerful capability is the TerraFly TimeSeries application. This application has
a unique ability to provide efﬁcient and ergonomic dissemination of imagery with
spatio-temporal data overlays. In other words, the TerraFly TimeSeries application
can retrieve geospatial and remotely sensed imagery of the same geographic location
that was acquired during different time periods. The system is then able to create an
animated sequence over time to clearly show historical changes.
To accomplish this, the TimeSeries application uses the coordinates of the
current map center that it has received from the TerraFly API to send a request
for information on available images to the TerraFly SO service (imagery source
service). The SO service provides the TimeSeries application with an XML for-
matted string that contains information about available imagery sources, resolutions
and acquisition dates. Upon receipt of this information, the TimeSeries application
parses the XML [46] and orders the sources by acquisition date, starting with
earliest date to the latest date. It then creates a time-line panel that accurately
reﬂects the proportion of the number of days between acquisition dates, and then
searches for the closest resolution images of the requested location. The TimeSeries
application then generates URLs needed to request the corresponding imagery.
Once the imagery servers provide the requested images, the TerraFly API loads
the imagery and creates an overlay in the viewing area. The TimeSeries application

648
N. Rishe et al.
Fig. 25.5 Ishinomaki, Japan, before the earthquakes and tsunami of March 11, 2011
creates the animated time sequence by fading-in and -out the corresponding images
in the timeline. Speciﬁcally, this fading-in and -out effect is achieved by changing
the transparency parameter of these images from low to high (fade-in) or high to
low (fade-out) every 40 ms at a rate of 25 frames/s.
The resulting time series can be quite dramatic and useful, particularly in
disaster mitigation applications. When an extreme event occurs, certain features or
characteristics of a particular area might be radically changed. With this unique
use and visualization of historic data, the TerraFly TimeSeries application enables
taking the important information and creating a new synthetic view of an emergent
reality. This can not only aid in response to the disaster, but it also provides
researchers with a rich source of data that can be studied to ﬁnd better ways to
plan and respond to similar types of disasters.
The recent earthquakes and tsunami that hit Japan in March of 2011 provides
an example of the power of this technology. As can be seen in Figs. 25.5–25.7, the
before, during and after geospatial images, respectively, of the tsunami are quite
dramatic. In the image from April 4, 2010, entire neighborhoods can clearly been
seen as having been established and intact. The image from March 12, 2011, the day
after the tsunami hit, shows that water has rushed well inland, inundating those
neighborhoods. The ﬁnal image, from March 19, 2011, shows the aftermath once
the waters have ﬁnally receded. Those neighborhoods were completely decimated.
Although images from the ground would show the devastation, the remotely sensed
images allow researchers to better study the overall impact and patterns of this
catastrophic event.

25
Geospatial Data Management with Terraﬂy
649
Fig. 25.6 Ishinomaki, Japan, the day after the earthquakes and tsunami of March 11, 2011
Fig. 25.7 Ishinomaki, Japan, 8 days after the earthquakes and tsunami of March 11, 2011

650
N. Rishe et al.
Fig. 25.8 TerraFly’s autopilot ﬂight sequence creation tool
3.2
Auto Pilot
TerraFly includes the autopilot technology that allows users to preplan and map
out a customized ﬂight path of interest. With this tool, end users can quickly and
easily select speciﬁc destinations over spatial and remotely sensed images, and the
system will automatically create that ﬂight path as a series of point destinations
at the speed and altitude (resolution) desired by the user. The speed and altitude
need not remain static during the automated ﬂight. At any point in the predeﬁned
ﬂight sequence, users may include changes in speed, and zoom in (i.e., view higher
resolution data) or zoom out (i.e., view lower resolution data) at will. The user is
also able to determine which additional features will be displayed while traveling in
the ﬂight path. In essence, any feature or information that is possible to view when
the user is in manual mode can be added as a component of the requested ﬂight path.
The Autopilot ﬂight sequence creation tool can be seen in Fig. 25.8.
Once a ﬂight sequence is deﬁned, the ﬂight path is overlaid the geospatial images
in the ﬂight path window. As can be seen in Fig. 25.9, users are able to ﬂy along the
requested ﬂight path in the main TerraFly ﬂight window without any intervention by
the user. Users will also be able to see a zoomed out overview of the overall ﬂight
sequence in a smaller window found in the lower right-hand corner of the ﬂight area.
TerraFly’s autopilot technology is not merely for entertainment value. There
are numerous applications for this technology, including education, emergency
preparedness in urban areas, and the study of crops in rural areas. To best illustrate
this, imagine this scenario. A local ofﬁce of emergency preparation and response

25
Geospatial Data Management with Terraﬂy
651
Fig. 25.9 TerraFly’s autopilot ﬂight path window
is notiﬁed of the imminent approach of a hurricane to their region. Ofﬁcials and
employees must make preparations to both mitigate effects of the hurricane, as well
as be prepared to respond in the aftermath. With TerraFly’s autopilot technology,
coordinated autopilot ﬂight paths could be created prior to the storm event and
assigned to speciﬁc responders. Once the storm as passed and new data is available
regarding storm impacts in the area, each responder could quickly and easily engage
their assigned ﬂight sequence to view and interpret impact results. The responders
would not have to manually control movement over the area of interest and could,
instead, just focus on gaining the information that they need. Further, this same ﬂight
path could easily be used over and over as updates and new information come in.
The auto-piloted path can be annotated with voice clips, images, and data.
3.3
Data Dispenser
There are times when end users are in need of being able to work with spatial data
outside of TerraFly. For example, an environmentalist may need to run complex,
domain speciﬁc analyses that cannot be accomplished in TerraFly itself. Alternately,

652
N. Rishe et al.
Fig. 25.10 Customized web page created by TerraFly’s data dispenser
a business may need a large, high quality poster print of a particular area but lack
to equipment to produce the poster. To address these needs, TerraFly provides a
ﬂexible and user friendly data dispenser that end users can use to acquire any spatial
and related data for a location of their choice [47].
TerraFly’s data dispenser provides users with fast and convenient access to a
map or a remotely sensed image. The data dispenser has been designed with an easy
to use, intuitive interface that allows users to acquire data without any specialized
training or tools. Users are able to easily choose, mark and dispense satellite images
or aerial photos of any size, and in varied formats. TerraFly’s dispenser can also
provide the user with textual geo-referenced data associated with a dispensed image.
When combined with the requested imagery, this data gives the user a unique
information package associated with the geographical area of interest. Numerous
products, both digital and printed, are available to the user. End users can order
large poster prints, aerial photos and topomaps, atlases as auto-formatted PDF ﬁles,
photo prints, GeoEye satellite products, reports, and more [48].
To the user, requesting data via TerraFly is very simple. The download button
is clicked, the user selects the area of interest using the mouse to manipulate a
bounding box, and then selects “download” from the menu. Once this request is
sent to the system, TerraFly’s data dispenser module searches TerraFly’s imagery
database for all image tiles that can be used to generate the imagery for the user-
deﬁned area. TerraFly’s information databases are searched for all the possible
data reports related to the selected area. TerraFly also has the capability to search
data sources on other sites. Currently, TerraFly also searches the GeoEye Archives
for the availability of GeoEye and Ikonos imagery. As can be seen in Fig. 25.10,
once the searches are complete, the system generates a customized web page that
presents users with all of the unique product options available for the area of
interest.

25
Geospatial Data Management with Terraﬂy
653
4
Application Domains
TerraFly’s use of innovative research and cutting-edge technology has created a
ﬂexible, robust and forward-thinking solution that has multiple domain applications.
The wide range and types of data available in TerraFly makes the system useful to
a much broader user base than conventional geographic information systems. In
this section, an overview of the primary domains that currently use TerraFly are
presented, namely, disaster mitigation and response, research and scientiﬁc inquiry,
real estate, travel and tourism, and government operations and public interest.
4.1
Disaster Mitigation and Response
When a disaster occurs, fundamental aspects of life dramatically change. Major
impacts often include changes to our physical environment that are far reaching and
easy to discern. The importance of the positive impact that information technology
can have on disaster mitigation and response has been discussed in several US
government and international reports. They all agree that greater resources in
information technology will improve our ability to plan for and respond to disasters,
ultimately saving lives and property [4,5].
Discussions throughout this paper have touched upon TerraFly’s prominence as
a tool for disaster mitigation and response. As has been noted, TerraFly has multiple
capabilities that researchers, planners and responders can use to vastly improve
disaster planning and response. For example, as was seen in Sect. 3.1 TerraFly’s
TimeSeries visualization capabilities can quickly and easily show users physical
changes to our environment as a result of a disaster. However, other types of impacts,
such as economic impacts, can also be visualized in TerraFly.
As can be seen in Fig. 25.11, TerraFly has the ability to provide visualization of
economic changes, such as changes in property values, as the result of a disaster.
The example presents patterns of changes in property values affected by the BP
Deep Water Oil Spill. Unlike major impacts on the physical environment, this type
of impact is one that is not clearly seen with the naked eye. Instead, combining,
overlaying, and analyzing different types of pertinent data are required to gain this
understanding.
4.2
Research and Scientiﬁc Inquiry
TerraFly is also extensively used as a tool for research and scientiﬁc inquiry.
TerraFly is used to support and engage in many domains of computing research
such as data processing, query optimization, parallel processing, storage of mas-
sive amounts of data, etc. TerraFly also provides numerous data analysis and

654
N. Rishe et al.
Fig. 25.11 The effect of the BP deepwater oil spill on property values
Fig. 25.12 In situ data and graphs in TerraFly for use in hydrological analysis
visualization tools for scientists in various domains. Figure 25.12 shows Hydrology
data analysis tools in TerraFly. With this tool’s intuitive interface, users are able
to view and analyze data, and have results of their analyses displayed as imagery,
charts and tables.
Additional functionality found in TerraFly, such as time series data, key word
searches, and layer control, is also available on this screen. For example, users

25
Geospatial Data Management with Terraﬂy
655
Fig. 25.13 Geospatial-temporal plots
are able to select a date range and view time series animation of changes, as well
as graphs that plot these changes over time. As can be seen in Fig. 25.13, very
detailed geospatial temporal plots are available to users of this application. As with
all TerraFly ﬂight windows, locations are searchable by address, and more detailed
data for speciﬁc points is available at the click of the mouse. For example, clicking
on one of the stations provides information about that particular site.
4.3
Real Estate
TerraFly also provides tools for various business domains, such as Real Estate. Real
estate related services are provided to real estate agents, buyers and sellers. TerraFly
downloads real estate sale listings from the Multiple Listing Service (MLS) and
overlays this data onto spatial imagery. As can be seen in Fig. 25.14, variables
such as property types, prices and square footage are overlaid, with more detailed
information and additional variables available at the click of the mouse. Users
are able to quickly and easily see the number and location of homes for sale in
a neighborhood, as well as nearby features that may affect the desirability of a
property such as proximity to a park or school. Realtors can take potential buyers
on virtual tours of a neighborhood, and map out driving and transportation routes of
interest.
TerraFly’s Real Estate component also provides powerful data mining capabili-
ties. Again, as can be seen in Fig. 25.14, end users are able to search for properties

656
N. Rishe et al.
Fig. 25.14 TerraFly’s real estate consumer application interface
based on attributes such as asking price, number of bedrooms, square footage and
various other keywords. A common search in South Florida, for example, is for
ocean front condominiums or single family homes with a pool.
The power of this type of capability is not fully appreciated until one attempts
to glean the same information without the ability to visualize it in a tool such as
TerraFly. For example, imagine you are a buyer looking to purchase a home in a
speciﬁc neighborhood. What you would typically be given is a list of properties for
sale with their addresses and other relevant data such as square footage. Trying
to visualize the locations of these properties in your mind is rather difﬁcult,
particularly in relation to desirable and undesirable features in the neighborhood.
Further, visualization in TerraFly provides signiﬁcantly more information about a
particular property than a standard MLS listing. For instance, a home’s address
may indicate that it is on a small quiet street, but there is no information on
what surrounds that property. It may be that the property backs up to a noisy
highway or busy street. This would easily be seen in TerraFly, but would not
be apparent to anyone using the typical MLS listing until the home is visited in
person.
Another real estate application available in TerraFly can used to study real estate
value trends over time. As can be seen in Fig. 25.15, users can view and analyze
valuation trends of speciﬁc census block groups by selecting the census block of
interest and entering start and end dates. Data such as average price per square foot
is overlaid onto aerial imagery, and can be animated over time. The application also
includes a trending graph so that users can see average changes in prices over their
speciﬁed dates, as well as a list of sales prices by date.

25
Geospatial Data Management with Terraﬂy
657
Fig. 25.15 Historical property value trends
4.4
Travel and Tourism
TerraFly’s capabilities are also useful to the travel and tourism industry. As can
be seen in Fig. 25.16, TerraFly’s auto pilot mapping tool can be used to create
virtual tours of vacation destinations of interest. Tour operators can provide potential
clients with a bird’s eye view of tour packages that highlight points of interest
with pertinent information and high resolution geo-referenced images of particular
locations. Tour operators could even use TerraFly to customize tour packages for
individuals, creating the tour route with the direct input of their clients.
For example, tour operators can create a tour through Washington, DC, to show
to potential clients. As they take their clients on a virtual tour, the clients can see
various points of interest and let the tour operator know which destinations and
points of interest they are interested in including in their tour package and which
are of not interest to them. The tour operator could immediately make changes in
the auto pilot to reﬂect the client’s wishes and immediately show them the resulting
new planned tour.
4.5
Government and Public Interest
The wide range and types of data available in TerraFly, along with TerraFly’s robust,
ﬂexible and easy to use functionality is especially useful for government agencies
and public interest. TerraFly’s value as a tool for disaster planning and recovery
has been illustrated throughout this paper. There are, however, numerous day-to-
day functions that government agencies are responsible for that TerraFly is well

658
N. Rishe et al.
Fig. 25.16 Sample tour using TerraFly’s auto plot tool
suited to support. In fact, as can be seen in Fig. 25.17, several local municipalities in
Florida (The City of Coral Gables, The City of North Miami Beach, and The City of
Miami Gardens, among others) have adopted TerraFly on their web sites to provide
citizens with up to date information in their area, as well as tools to help make it
easier for residents to work with their local government agencies and ofﬁces.
In addition, much of the data collected and used by government is well suited for
integration into TerraFly. For example, as can be seen in Fig. 25.18, crime incident
report data is associated with speciﬁc locations. TerraFly can easily process and
overlay that data on geospatial imagery, providing both professional users and lay
people with data visualization that is much more intuitive and easy to understand.
Further, this data can be combined with other vector data available for that same
area. The data could then be analyzed to determine if there are any trends or
associations affecting the occurrence of particular types of incidences.
Importing property tax assessment data into TerraFly also aids in government
operations. As can be seen in Fig. 25.19, by being able to view text records alongside
with visual property imagery, tax assessor employees can more easily compare and
analyze assessments between similar and dissimilar properties. This provides more
consistency and accuracy in assessment procedures and valuation. Further, with the
use of TerraFly’s time series application, tax assessor employees can compare recent
imagery with historical imagery to determine whether any changes have been make
to a property that would necessitate a change in assessment value.

25
Geospatial Data Management with Terraﬂy
659
Fig. 25.17 Local municipalities that use TerraFly on their web sites
Fig. 25.18 Crime incidence report data overlaid on geospatial imagery

660
N. Rishe et al.
Fig. 25.19 Property tax assessment for individual properties
Fig. 25.20 White pages
Finally, there are also numerous capabilities in TerraFly that individuals ﬁnd
useful in their day-to-day lives. Overlaying data from the White Pages, for example,
improves its usability for many individuals (see Fig. 25.20). Users are able to search
for needed information in the White Pages not just by a person’s full name, but also

25
Geospatial Data Management with Terraﬂy
661
by attributes such as address, phone number, or the names of other individuals in
the home. Alternately, users can search the geospatial imagery visually to ﬁnd a
particular location, and related information.
5
TerraFly with Other Systems: GIS-INTEGRO
TerraFly interacts and cross-pollinates with other Web GIS systems, including
NASA WorldWind and the GIS-INTEGRO system developed by the Russian
Academy of Science and Dubna International University. The GIS-INTEGRO
system provides analytics data for disaster mitigation. The TerraFly and GIS-
INTEGRO teams are performing joint research that focuses on expanding and
improving algorithms and methodologies used for integrated object analysis and
related processes. The teams are also furthering the development of intelligent user
interfaces for each stage of the research and analysis process, from data acquisition,
georeferencing, data integrity and quality assurance, and multi-level analysis to
pre-print requirements for hard-copy published maps. There are numerous areas
of application for these technologies, including decision-making support systems
for mineral exploitation and environmental protection management. Components of
this work include:
Pattern recognition algorithms (Holotype): Algorithms designed to compute simi-
larity measures and matrices for heterogeneous objects, including resolving issues
associated with recognition of objects in situations where only incomplete informa-
tion is available.
Multi-functional geo-information server: Algorithms designed to integrate remote
geo-informational resources with spatial modeling during disaster situations. The
use of spatial modeling during a disaster (or very shortly thereafter) helps provide
critical information on the current state of an affected area. This is accomplished
by integrating up-to-the-minutes information with historical data, and providing a
holistic evaluation of current environmental properties and impacts.
Ecological modeling: Methodologies designed for modeling ecological data and
the structure of ecological informational space, as well as determining the natural
and anthropogenic factors that affect the ecological state in regions of interest.
Geophysics: Consolidating vast amounts of geophysical algorithms and evaluations
of mineral reserves.
6
Conclusion and Future Directions
Through the implementation of innovative techniques and technologies, TerraFly
provides users with GIS capabilities without the need to learn complex interfaces
or deal with the technology behind the system. It is a robust, user-friendly system

662
N. Rishe et al.
that has wide appeal to many different types of users, and application to many
different domains. TerraFly has been covered by both popular and specialized
media, including TV (e.g. Fox and Discovery), radio (NPR), newspapers (e.g. New
York Times, USA Today), magazines (e.g. Science) and journals (e.g. Nature). The
project’s primary sponsor is the National Science Foundation (NSF). Of the 53,000
NSF-funded projects in 2009, it chose 120, including TerraFly, for the NSF annual
report to congress [49].
Several new projects and new directions are currently being pursued that will
improve the technical capabilities and expand TerraFly’s functions. One of them is
the incorporation of social media, including using information gathered from social
media as a viable data source. As the popularity of social media has increased, so
has its potential for providing up-to-the-minute information on pertinent happenings
in the world. If implemented appropriately, this could have a substantial impact on
disaster response. It is no longer unusual for news of a particular event to be posted
on a social networking site before any other media or type of communication. The
challenge, however, is trying to determine how to most efﬁciently and effectively
ﬁlter pertinent and accurate data from massive streams of messy, irrelevant, and
inaccurate data.
Efforts are being made to expand data sharing capabilities and communication,
particularly for disaster planners and responders. Areas being examined to aid
in this include: (1) creation of a “citizens sensor network” that can potentially
provide near instantaneous geolocation and visual coverage of extreme events; (2)
development of techniques to better manage a solid communication infrastructure
for disaster responders by providing services via the cloud and (3) generate
models and simulations to better predict outcomes, that can subsequently be used
to disseminate mission critical information and provide guidance for emergency
response.
As the usefulness of spatial and related data increases and expands to new
domains, efforts will also increase to determine how the needs of these new domains
can be met while still providing a robust, innovative and intuitive system.
Acknowledgments This research was supported in part by NSF grants CNS-0821345, CNS-
1126619, HRD-0833093, IIP-0829576, CNS-1057661, IIS-1052625, CNS-0959985, OISE-
1157372, CNS-1042341, CNS-0837556, DGE-0549489, IIS-0957394, IIS-0847680, IIP-0934339,
and OISE-0730065, DHS contract HSHQDC 11 C 00019, and support of USGS, IBM, CoreLogic,
and ADCi. We greatly appreciate the support and guidance of our NSF Program Directors Rita
Virginia Rodriguez, Richard Smith, Demetrios Kazakos, Victor Santiago, Maria Zemankova,
and Juan Figueroa. We would like to thank Patrick Hogan of NASA World Wind [50] for his
continued guidance and support in research and development of TerraFly’s open-source APIs, 3D
visual environment for visualizing spatio-temporal data with moving objects, and keyword-spatio-
temporal queries on moving objects. We have greatly beneﬁted from collaborating with current
and former researchers at NASA Goddard Space Flight Center, particularly Milton Halem, Bill
Teng, Patrick Coronado, and Bill Campbell. IBM has provided invaluable support and guidance,
particularly Oded Cohn, Juan Caraballo, Adalio Sanchez, and Howard Ho.

25
Geospatial Data Management with Terraﬂy
663
References
1. International Tsunami Information Center. “11 March 2011, MW 9.0, Near the East Coast
of Honshu Japan Tsunami”. [Online] 2011. http://itic.ioc-unes-co.org/index.php?option=com
content&view=article&id=1713&Itemid=2365&lang=en.
2. Gulf Oil Spill. NOAA.gov. [Online] http://www.education.noaa.gov/Ocean and Coasts/Oil
Spill.html.
3. D. Guha-Sapir. “Disasters in Numbers 2010”. Center for the Epidemiology of Disasters.
[Online] 2011. http://www.cred.be/publications?order=ﬁeld year value&sort=desc.
4. Select Bipartisan Committee to Investigate the Preparation for and Response to Hurricane
Katrina. “A Failure of Initiative: Final Report of the Select Bipartisan Committee to Investigate
the Preparation for and Response to Hurricane Katrina”. U.S. House of Representatives,
Feb. 15th, 2006.
5. National Commission of the BP Deepwater Horizon Oil Spill and Offshore Drilling. “Deep
Water: The gulf oil disaster and the future of offshore drilling”. Report to the President. 2011.
6. H. Samet. “The design and analysis of spatial data structures”. Addison-Wesley, Reading,
MA, 1990.
7. T. Keating, W. Phillips, and K. Ingram. “An Integrated Topologic Database Design for
Geographic Information Systems”. Photogrammetric Engineering and Remote Sensing,
vol. 53, no. 10, 1987, pp. 1399–1402.
8. R.A. Lorie and A. Meier. “Using a Relational DBMS for Geographical Databases”. Geo-
Processing, vol. 2, 1984, pp. 243–257.
9. M. Egenhofer. “Why not SQL!”. International Journal on Geographical Information Systems,
vol. 6, no. 2, p. 71–85, 1992.
10. H. Samet. “Applications of spatial data structures”. Addison-Wesley, Reading, MA, 1990.
11. H.V. Jagadish, A. Chapman, A. Elkiss, M. Jayapandian, Y. Li, A. Nandi, and C. Yu.
“Making Database Systems Usable,”. ACM’s Special Interest Group on Management of Data
(SIGMOD), June 11–14, 2007, Beijing, China.
12. A.E. Wade. “Hitting the Relational Wall”. Objectivity Inc. White Paper. [Online] 2005. http://
www.objectivity.com/pages/object-oriented-database-vs-relational-database/default.html.
13. TerraFly Landing Page. TerraFly. [Online] http://terraﬂy.ﬁu.edu.
14. N. Rishe, W. Teng, H. Rui, S. Graham, M. Gutierrez. “Web-based Dissemination of TRMM
Data via TerraFly.”. EOS Transactions, American Geophysical Union, vol. 85, no. 47, Fall
Meeting Supplement, December 2004.
15. N. Rishe, J. Yuan, R. Athauda, S.C. Chen, X. Lu, X. Ma, A. Vaschillo, A. Shaposhnikov, D.
Vasilevsky. “Semantic Access: Semantic Interface for Querying Databases”. ACM SIGMOD
Digital Symposium Collection DiSC01. June 2003. pp. 591–594.
16. A. Cary, O. Wolfson, and N. Rishe. “Efﬁcient and Scalable Method for Processing Top-k
Spatial Boolean Queries”. Proceedings of the 22nd International Conference on Scientiﬁc
and Statistical Database Management. Published as Lecture Notes in Computer Science. Vols.
6187/2010: Scientiﬁc and Statistical Database Management, Springer Berlin/Heidelberg, 2010,
pp. 87–95.
17. A. Prasad Sistla, Ouri Wolfson, Bo Xu, Naphtali Rishe. “Answer-Pairs and Processing
of Continuous Nearest-Neighbor Queries”. Proceedings of the 2011 The Seventh ACM
SIGACT/SIGMOBILE International Workshop on Foundations of Mobile Computing (FOMC
2011). San Jose, California, June 9th, 2011.
18. N. Roussopoulos, C. Faloutsos, and T. Sellis. “Nearest Neighbor Queries”. Proceedings of the
ACM SIGMOD International Conference on Management of Data, pp. 71–79, 1995.
19. N. Rishe and O. Wolfson. “Thin Client Technologies for Spatial Data Visualization.”.
Proceedings of the National Science Foundation Computing Research Infrastructure 2007
PI Meeting: Computer Science Department Boston University (NSF CRI 2007 PI Meeting).
June 3–5, 2007, Boston, Massachusetts. pp. 84–88.

664
N. Rishe et al.
20. P. Szczurek, B. Xu, O. Wolfson, J. Lin, N. Rishe. “Prioritizing Travel Time Reports in
Peer-to-Peer Trafﬁc Dissemination”. Proceedings of the IEEE International Symposium on
Communication Systems, Networks and Digital Signal Processing (7th CSNDSP). Newcastle,
U.K. July 21–23, 2010. pp. 454–458.
21. Bo Xu, O. Wolfson, C. Naiman, N. Rishe, R. M. Tanner. “A Feasibility Study on Disseminating
Spatio-temporal Information via Vehicular Ad-hoc Networks.”. Proceedings of the Third Inter-
national Workshop on Vehicle-to-Vehicle Communications 2007 (V2VCOM 2007). Istanbul,
Turkey. pp. 146–151.
22. O. Wolfson, B. Xu, H. Yin, N. Rishe. “Discovery Using Spatio-temporal Information in Mobile
Ad-Hoc Networks. Web and Wireless Geographical Information Systems, 5th International
Workshop, W2GIS 2005, Lausanne, Switzerland, December 15–16, 2005. SpringerVerlag
Lecture Notes in Computer Science 3833. pp. 129–142.
23. N. Rishe. “TerraFly: NASA Regional Applications Center”. The AMPATH Workshop: Iden-
tifying Areas of Scientiﬁc Collaboration Between the US and the AMPATH Service Area,
Florida International University, Miami. Conference Report. August 15–17, 2001 p. 7–8.
24. D. Davis-Chu, N. Prabakar, N. Rishe, A. Selivonenko. “A System for Continuous, Real-Time
Search and Retrieval of Georeferenced Objects”. Proceedings of the ISCA 2nd International
Conference on Information Reuse and Integration (IRI-2000). Nov. 1–3, 2000. pp. 82–85.
25. N. Prabhakaran, V. Maddineni, and N. Rishe. “Spatial Overlay of Vector Data on Raster Data
in a Semantic Object-Oriented Database Environment.”. International Conference on Imaging
Science, Systems, and Technology (CISST ’99), June 28 - July 1, 1999, Las Vegas, Nevada,
pp. 100–104.
26. N. Rishe, S. Chen, N. Prabakar, M.A. Weiss, W. Sun, A. Selivonenko, D. Davis-Chu. “Terraﬂy:
A High-Performance Web-Based Digital Library System for Spatial Data Access”. ICDE
2001: International Conference on Data Engineering, April 2–6, 2001, Heidelberg, Germany.
pp. 17–19.
27. A. Cary, Y. Yesha, M. Adjouadi, N. Rishe. “Leveraging Cloud Computing in Geodatabase
Management”. Proceedings of the 2010 IEEE Conference on Granular Computing GrC-2010.
Silicon Valley, August 14–16, 2010. pp. 73–78.
28. P. Szczurek, B. Xu, O.i Wolfson, J. Lin, N. Rishe. “Learning the relevance of parking infor-
mation in VANETs”. Proceedings of the seventh ACM international workshop on VehiculAr
InterNETworking. Chicago, Illinois. September 24, 2010. ISBN:978-1-4503-0145-9. pp. 81–
82, September 2010.
29. D. Ayala, J. Lin, O. Wolfson, N. Rishe, M. Tanizaki. “Communication Reduction for Floating
Car Data-based Trafﬁc Information Systems”. Second International Conference on Advanced
Geographic Information Systems, Applications, and Services, pp. 44–51, February 10–
16, 2010.
30. W. Teng, N. Rishe, H. Rui. “Enhancing access and use of NASA satellite data via TerraFly.”.
Proceedings of the ASPRS 2006 Annual Conference, May 1–5, 2006, Reno, NV.
31. N. Rishe. “Database Design: The Semantic Modeling Approach”. McGraw-Hill, 1992.
32. N. Rishe, and Q. Li. “Storage of Spatial Data in Semantic Databases.”. Proceedings of
the 1994 ASME International Computer in Engineering Conference, Minneapolis, MN,
pp. 793–800, Sept 11–14, 1994.
33. G. Mufﬁn. “Raster versus Vector Data Encoding and Handling: A Commentary”. Photogram-
metric Engineering and Remote Sensing, Vol. 53, No. 10, pp.1397–1398, 1987.
34. Landsat Project Policy and History: Landsat 7 Mission Speciﬁcations, NASA Goddard
Space Flight Center,. [Online] http://ltpwww.gsfc.nasa.gov/LANDSAT/CAMPAIGN DOCS/
PROJECT/L7 Speciﬁcations.html.
35. Tiger Overview, United States Census Bureau, Tiger/Line data. [Online] http://www.census.
gov/geo/www/tiger/overview.html.
36. USGS mapping Information: Geographic Names Information System (GNIS). [Online] http://
mapping.usgs.gov/www/gnis/.
37. TerraFly data coverage. TerraFly. [Online] http://n0.cs.ﬁu.edu/terraﬂy.coverage.htm.

25
Geospatial Data Management with Terraﬂy
665
38. Federal Geographic Data Committee (FGDC) Subcommittee for Cadastral Data. “Today’s
Cadastral Information Customers and Requirements”. FGDC Cadastral Subcommittee. [On-
line] 2008. http://nationalcad.org/data/documents/cadastral%20data%20customers.pdf.
39. CoreLogic. “CoreLogic ParcelPoint”. [Online] 2011. http://www.faspatial.com/databases/
parcelpoint?format=pdf.
40. N. von Meyer, B. Ader, Z. Nagy, D. Stage, B. Ferguson, K. Benson, B. Johnson, S.
Kirkpatrick, R. Stevens, and D. Mates. “Parcel Identiﬁers for Cadastral Core Data: Concepts
and Issues”. FGDC Cadastral Subcommittee. [Online] July 2002. http://www.nationalcad.org/
data/documents/parcelID.pdf.
41. A. Cary, Z. Sun, V. Hristidis, and N. Rishe. “Experiences on Processing Spatial Data with
MapReduce” in Springer Lecture Notes in Computer Science. Vols. 5566/2009: Scientiﬁc
and Statistical Database Management. (Proceedings of the 21st International Conference on
Scientiﬁc and Statistical Database Management). New Orleans, Louisiana, USA. June 1–5,
2009.), pp. 302–319.
42. Z. Sun, T. Li, N. Rishe. “Large-Scale Matrix Factorization using MapReduce.” Proceedings
of the 2010 IEEE International Conference on Data Mining. Sydney, Australia. December 13,
2010. ISBN: 978-0-7695-4257-7. pp. 1242–1248.
43. J. Dean and S. Ghemawat. “MapReduce: Simpliﬁed Data Processing on Large Clusters”.
Communications of the ACM vol. 51, no. 1 (January 2008), pp. 107–113.
44. A. Guttman. “R-trees: A dynamic index structure for spatial searching”. Proceedings
of the ACM SIGACT-SIGMOD Conference on the Principles of Database Systems,
pp. 569–592, 1984.
45. R. Finkey, J. Bentley. “Quadtrees: A data structure for retrieval on Composite Keys”. Acta
Informatica, vol. 4, no. 1, pp. 1–9, 1974.
46. Extensible Markup Language (SML) 1.0 (Fifth Edition). W3C Recommendation. [Online]
November 26, 2008. http://www.w3.org/TR/REC-xml/.
47. N. Rishe, M. Gutierrez, A. Selivonenko, S. Graham. “TerraFly: A Tool for Visualizing and
Dispensing Geospatial Data.” Imaging Notes, Summer 2005. Vol. 20, 2, pp. 22–23.
48. GeoEye Imagery Collection. GeoEye. [Online] 2011. http://www.geoeye.com/CorpSite/
products-and-services/imagery-collection/Default.aspx.
49. National Science Foundation. FY 2010 Budget Request to Congress. National Science Founda-
tion. [Online] May 7th, 2009. http://www.nsf.gov/about/budget/fy2010/pdf/entire fy2010.pdf.
50. NASA World Wind. [Online] http://worldwind.arc.nasa.gov/.


Chapter 26
An Application for Processing Large
and Non-Uniform Media Objects
on MapReduce-Based Clusters
Rainer Schmidt and Matthias Rella
1
Introduction
Infrastructure as a Service (IaaS) is a resource provisioning model that enables
customers to access large-scale computer infrastructures via services over the
Internet. It allows users to remotely host data and deploy individual applications
using resources that are leased from infrastructure providers. A major strength of
this approach is its broad applicability which is supported by the separation of
application implementation and hosting. One of the most prominent frameworks
that utilizes the IaaS paradigm for data-intensive computations has been introduced
by Google. MapReduce implements a simple but powerful programming model for
the processing of large data sets that can be executed on clusters of commodity
computers. The framework targets applications that process large amounts of
textual data (as required, for example, when generating a search index), which are
parallelized on a master-worker principle. Scalability and robustness are supported
through features like distributed and redundant storage, automated load-balancing,
and data locality awareness.
We present research on applying a widely used programming model for data-
intensive computations to the problem domain of audiovisual (AV) data. We
describe a method and corresponding application for analyzing video archives
based on the MapReduce programming model [1]. The application automates data
compression and decomposition by utilizing native codec libraries and parallel
processing based on ﬁle partitions. It can thereby take advantage of scalable
computational environments in order to speed up execution times and throughput.
Users are provided with full programmatic control over the processing logic (as
opposed to the execution of wrapped 3rd party applications). The application utilizes
R. Schmidt () • M. Rella
Austrian Institute of Technology, Donau-City-Strasse 1, 1220 Vienna,
Austria
e-mail: rainer.schmidt@ait.ac.at
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 26, © Springer Science+Business Media, LLC 2011
667

668
R. Schmidt and M. Rella
Java-based abstractions for relevant concepts like containers, tracks, and frames
allowing one to easily implement and incorporate custom application logic. We have
implemented a set of examples that perform tasks such as applying ﬁlters (e.g. for
face recognition) or performing text extraction on a collection of arbitrary input
video ﬁles. A range of use-cases that support digital libraries and archives exist.
Examples include error checking and repair of TV/video collections, the application
of preservation actions, or the on-demand generation of access copies for different
client devices.
2
Data-Intensive Computing
2.1
Execution Environments
In 2004, Dean and Ghemawat introduced MapReduce [1], a programming model
and implementation that is capable of processing vast amounts of data on clusters
of commodity computers. Data is shared using a distributed ﬁle system (GoogleFS)
that provides a decentralized storage layer on top of local storage disks, called a
shared nothing architecture. Other data-intensive computing environments include
Apache Hadoop,1 Microsoft Dryad [5], and Nephele [10], which provide a range
of languages, programming models, and runtime implementations for processing
data on large storage and computing networks. A major beneﬁt of utilizing such
data processing environments compared to using low-level parallel libraries (like
for example MPI) is the automated synchronization and handling of IO operations.
Within their application scope, these systems can efﬁciently handle issues like
workload distribution, coordination, and error handling. This in turn enables users to
easily implement extremely robust applications that can be executed over thousands
of simultaneously running nodes.
2.2
Processing Structured Data
Programming patterns like MapReduce and All-Pairs [6] are speciﬁcally designed
for implementing applications that perform operations on ﬁles-based and structured
content. These programming models provide a small set of abstractions allowing
their users to express and execute data-intensive workloads. MapReduce provides
a relatively simple programming model that is based on two interfaces; one for
distributing workload among the cluster nodes, and one for aggregating the results.
The programming model has proven to be applicable to a range of problems that
1http://hadoop.apache.org/

26
Media Processing with MapReduce
669
deal with the processing of textual data (e.g. graph processing and data mining).
It has also been demonstrated that these programming models are applicable
to types of scientiﬁc applications, like loosely-coupled and massively parallel
problems, e.g. found in bioinformatics [4]. Data is typically accessible via an
underlying distributed ﬁle system, which provide distributed and fault-tolerant
storage. Database and data warehouse systems like HBase and HIVE [9] have been
built on top of Hadoop’s distributed ﬁle system (HDFS) providing distributed data
stores. These systems relieve the users from the burden of creating applications
based on the programming model primitives only and allow them to perform more
familiar database operations based on a query language instead. Compared to the
functionality provided by generic RDBMS systems, some limitations regarding the
application of programming models have however been identiﬁed [11].
2.3
Application to Binary Data
A range of use-cases for processing archived binary content exist, these include
tasks like scaling, transcoding, and feature extraction of images, audio, and video
content. An example for handling binary data in an MapReduce application for
scientiﬁc data analysis is provided by Ekanayake et al. [3]. The authors describe
a MapReduce application that must combine different features from generated
binary histogram ﬁles. Apache Hadoop already provides basic support for pro-
cessing binary data like for example dealing with compressed input/output ﬁles
on the distributed ﬁle system. When handling large compressed text ﬁles (e.g. for
performing log ﬁle analysis), it is important to extract and process only parts of
the data in the map task. Speciﬁc binary formats (like Hadoop’s SequenceFile) can
provide an efﬁcient intermediate representation for handling large data volumes on
HDFS supporting serialization, compression, and splitting. However, these binary
formats are designed to provide storage representations that efﬁciently encode large
data sets. In order to process audiovisual data, it will be important to implement
suitable data abstractions as well as storage handlers to directly access and decode
natively binary data on the ﬂy. Once these abstractions are in place, users should
be able to utilize the available abstraction (like MapReduce) to easily implement
data-intensive operation for processing audiovisual content.
3
Architecture and Application Design
3.1
Approach to the Problem
Related work describes a strategy for transcoding video data in a cloud computing
scenario, called Split&Merge architecture [7]. The idea is to ensure ﬁxed duration

670
R. Schmidt and M. Rella
times for video encoding by scaling-up the number of cloud nodes used to process
chunks of data depending on the size of the payload ﬁle. Here, we take a similar but
more generic approach and focus on its application to a widely used programming
model (and corresponding execution environment).Designing an application around
a standard programming model in general, provides a number of well known
beneﬁts like encapsulation, robustness, and portability. In large distributed systems
like grids, clusters, or enterprise systems it is key to employ deﬁned programming
and deployment models in order to achieve speedup and scalability. The application
presented in this article has been implemented using the MapReduce programming
model and Apache Hadoop as an execution environment.
3.2
Data Placement
Figure 26.1 shows a MapReduce application for processing a set of video ﬁles (AV1,
AV2, AV3). The ﬁles are available on Hadoop’s distributed ﬁle system (HDFS)
which provides a shared storage network across the worker nodes (node1–3). Files
that are stored on HDFS are automatically broken up into blocks (typically of
64 MB) and stored (and replicated) on different data nodes based on a placement
policy. During runtime, the execution environment facilitates the processing of
payload data by assigning parts of the data (called splits) to worker nodes. Input
splits are of the same size as storage blocks per default and Hadoop spawns one
map task for each split on a worker node. It thereby tries to assign map tasks
to workers that reside closely to the input splits in order to maximize local read
operations. A number of input splits may be accessed remotely by map operations
as the scheduling of map tasks is subject to load balancing.
3.3
Data Decomposition
In order to support the parallel processing of binary input data it is important to
provide suitable mechanisms to divide the data into parts that can be interpreted
by the application. Most binary formats cannot be broken into chunks at arbitrary
positions or do not support splitting at all. For processing sets of relatively small
ﬁles one can overcome this problem by dividing the payload on a per-ﬁle basis [8].
Audiovisual content however tends to be large and its processing can easily become
too resource demanding to be performed on a single processor in a reasonable time
frame. It is therefore desirable to process single video ﬁles using multiple nodes
in parallel in order to speed up the overall execution time. This is in particular
important for applications that perform such operations on demand based on user
requests, for example when triggered via a web interface.

26
Media Processing with MapReduce
671
Fig. 26.1 A data ﬂow for processing audiovisual data using the MapReduce model. This
application takes m audiovisual (AV) input ﬁles as input and generates n AV output ﬁles. In a ﬁrst
phase, the MapReduce framework creates a set of mappers for each input ﬁle and assigns them to
ﬁle partitions (splits). The Mappers subdivide and process the input splits based on interpretable
binary chunks (records). For each record, a mapper creates a set of intermediate output data streams
which are grouped using keys (K), representing (parts of) the input ﬁle. Reducers remotely read
the intermediate data streams and generate a result ﬁle for each key
Video ﬁles formats are complex as they typically provide containers for different
data streams which are compressed and multiplexed. Digital video images (frames)
provide a natural unit for decomposing video materials into independently process-
able parts. Video ﬁle formats (like AVI, Flash, Quicktime) are however complex,
containing a range of data streams like audio, video or text tracks. The different
tracks are typically compressed based on a compression format (like mp3, h264,
MJPEG) and must be decoded before being interpretable. One approach to obtain
video frames from input splits is to migrate the data into an uncompressed format
before processing it within a parallel application. This has been evaluated using an
RGB-based raw data stream that provided a constant byte-size per frame, as shown
in Fig. 26.2. However, the migration itself is a resource and storage consuming
process, which hinders the application of this approach for large volumes of content.
The application presented here supports the parallel processing of video content
directly from the compressed original formats. The idea is to split videos at key
frame positions and perform the decompression of the data chunks in parallel within

672
R. Schmidt and M. Rella
Fig. 26.2 Using a raw RGB format. The raw data is automatically partitioned in blocks by the ﬁle
system and dispersed over the physical nodes P. During the ﬁrst execution phase (map), the cluster
nodes are assigned with data portions that correspond to single video frames. In this step, the
input frames F are read and frame processing takes place. For each video frame F, a corresponding
output frame O is written and registered within a local index. This index provides key-value pairs
that translate between a frame identiﬁer and a pointer to the corresponding data. In an ideal case,
each cluster node will only read and write from/to the local data partition. Indexing, however, is
required as the data source and order of the frames processed by a cluster node are subject to load
balancing and cannot be determined in advance. Also, a minimal data transfer across partitions
is required for frames that are split between data partitions. During the second processing phase
(reduce), the locally generated index maps are reduced into a single result map. Support is provided
by a corresponding driver implementation that directly retrieves the output video stream from the
distributed ﬁle system based on its index map
the MapReduce application. Video compression combines image compression and
temporal motion compensation in order to reduce the amount of data required to
encode video sequences. Key frames denote independently encoded frames that do
not depend on other video data to be decoded. In order to split a media byte-stream
into parts it is important to identify these key frame positions within the media
container. Using the Hadoop MapReduce framework, we have implemented the
required concepts (like input format, record reader, compression codec) that support
the automated splitting and parallel processing of compressed media streams

26
Media Processing with MapReduce
673
(Sect. 4). This allows us to process heterogeneous collections of audiovisual content
directly from the storage location without enforcing restrictions on the container and
compression formats.
3.4
User Deﬁned Functions
The MapReduce programming model allows users to write parallel applications by
implementing the functions map and reduce. Data portions are automatically gener-
ated and passed between the map/reduce functions using a generic data model based
on key-value pairs. This programming model allows users to easily implement par-
allel applications without having to deal with cumbersome parallelization strategies.
The model has been widely used for analyzing and transforming large-scale data sets
like text documents, database query results, or log ﬁles that can reside on different
storage systems [2]. In order to enable the development of such user-deﬁned func-
tion for processing audiovisual content, we have identiﬁed two major requirements:
(1) the framework must provide a mechanism to generate meaningful records that
can be processed within the map function, and (2) the framework must provide the
required abstractions and mechanisms to analyze and transform the records.
4
Implementation
4.1
Software Stack
The application has been implemented based on Apache Hadoop’s distributed ﬁle
system (HDFS) and MapReduce environment (release 0.21.0). For handling media
streams from Java, we utilize the Xuggler2 open-source library, which provides
a very stable wrapper around FFmpeg’s libav libaries3. In previous versions of
this application, other Java media frameworks and bindings to native applica-
tions have been evaluated (including JMF, FMJ, Theora-Java, FOBS, Jffmpeg,
FFMPEG-Java). It is in general desirable to base an application for user-deﬁned
data processing on a high-level language like Java as this can greatly simplify
the readability of the code. Due to dependencies to native libraries, the video
processing application demands the installation of FFmpeg on every cluster node.
Other dependencies may result from the incorporation of native special purpose
libraries like e.g. for optical character recognition. It is important to note that
virtualization and cloud technology provide a well suited model for efﬁciently
2http://www.xuggle.com/
3http://www.ffmpeg.org/

674
R. Schmidt and M. Rella
deploying such environments within large data centers on demand. In previous
work, we have instantiated a cluster with >150 video processing nodes on Amazon’s
utility cloud4 based on a single virtual machine image having the entire software
stack pre-installed.
4.2
Application Design
In the following, we provide an overview of the application design, and brieﬂy
describe some of the application’s basic abstractions and their implementation.
4.2.1
AV Splittable Compression Codec
One of the most critical issues when dealing with the parallel processing of large
ﬁles is handling compression. A compression codec provides a helpful abstraction
allowing one to easily read/write from/to compressed data streams. Hadoop provides
codec implementations for a set of ﬁle compression codecs including gzip, bzip2,
LZO, and DEFLATE as part of its API. It is however critical to consider if a
ﬁle format supports splitting for processing it with MapReduce. Hadoop utilizes
a speciﬁc interface called SplittableCompressionCodec to denote codecs
that support the compression/decompression of streams at arbitrary positions,
for example determined by split borders. Codecs like bzip2 that implement this
interface are highly valuable in this context as they support the partitioning and
parallel processing of compressed input data. We have implemented AV Splittable
Compression Codec, a class that supports the compression, decompression, and
splitting of audiovisual ﬁles.
4.2.2
AV Input Stream
Splitting of binary data is a challenging task and typically not supported by most
compression formats. In order to achieve this, it must be possible to detect positions
where the compressed data can be decomposed into blocks. This class implements
a splittable input stream for compressed audiovisual content. As shown in Fig. 26.3,
split boundaries must be repositioned to key frame positions by the codec in order to
support decomposition of the data stream. Hence, during execution the reader must
be advanced from an arbitrary position within the data stream to the next key frame
position. This is done by utilizing a key frame index that is automatically generated
from the container prior to the execution. In order to produce an interpretable data
stream from the adjusted ﬁle split, the stream reader prepends the container’s header
4http://aws.amazon.com

26
Media Processing with MapReduce
675
Fig. 26.3 A data pipeline for extracting records (audio/video frames) from splits of compressed
input data. Split borders (s1, s2) are adjusted (s1x, s2x) based on key frames discovered within the
multiplexed data stream. The adjusted input splits together with the container’s header information
are fed into a decompressor. The extracted packets are ﬁnally decoded into interpretable binary
records and passed to the map function
Fig. 26.4 Method signature of a user deﬁned map function for processing video frames as buffered
images
information (kept in memory) to each data portion. It is however not required to read
the entire split into memory as the payload is directly read from HDFS.
4.2.3
Frame Record Reader
Record readers are plugged into the input ﬁle format of a particular MapReduce job.
They typically convert the data provided by the input stream into a set of key/value
pairs (called records) that are processed within the map and reduce tasks. We utilize
the concept of packets, which are logical data entities read and uncompressed from
the input sequences. Packets are subsequently decoded (optionally error checked
and resampled) into objects of a speciﬁc data type. For example, a frame record
reader can utilize the above described concepts in order to obtain audio/video
frames from a generic input split (Fig. 26.5). This enables application developers
to conveniently implement user deﬁned MapReduce functions for processing the
content (Fig. 26.4).

676
R. Schmidt and M. Rella
Fig. 26.5 Sequence of interaction between FrameRecordReader and the application components
to (1) initialize AVInputStream and (2) read video frames from a given input split
4.2.4
Output Generation
Record writers provide the inverse concept to record readers. They write the
delivered job outputs in the form of key/value pairs to the ﬁle system. Output ﬁles
are produced per reduce task and might have to be merged in a post-processing
stage. To continue the example above, a frame record writer writes audio/video
frames to an instance of AV Output Stream which can be obtained from AV Splittable
Compression Codec. The codec implementation provides a generic vehicle which
is customizable regarding the compression formats used to encode the diverse
data tracks. However, other output generators may be implemented and produced
different types of results, like data sets, images, or text. Output generation is
typically implemented within the reduce task and may process intermediate results
that are generated by a number of corresponding mapper tasks.
5
Evaluation
In the following, we provide an evaluation that investigates the impact of input ﬁle
size and compression on the application’s performance and scalability.
5.1
Experiment Setup
The evaluation has been conducted on a dedicated testing infrastructure that
comprises a front-end and ﬁve worker nodes (Single Core 1.86 GHz Intel CPU,

26
Media Processing with MapReduce
677
Table 26.1 Payload data ﬁle sizes are depending on encoding and duration
Encoding
Input ﬁle
GOP length
Bitrate
30 min
60 min
90 min
120 min
1 frames
2,380 kb/s
535 MB
1,071 MB
1,607 MB
2,141 MB
10 frames
436 kb/s
98 MB
196 MB
294 MB
393 MB
100 frames
341 kb/s
76 MB
153 MB
230 MB
306 MB
Table 26.2 Execution time (sec.) and throughput (frames per second) on ﬁve nodes with static
split size (left) and dynamic split size adaption (right)
GOP
30 min
60 min
90 min
120 min
GOP
30 min
60 min
90 min
120 min
1
921/49
1,720/52
2,623/51
3,480/51
1
814/55
1,624/55
2,441/55
3,333/54
10
3,686/12
3,853/23
3,890/35
4,400/41
10
772/58
1,499/60
2,236/60
2,988/60
100
4,910/9
4,923/18
4,911/27
4,944/36
100
754/60
1,440/62
2,119/64
2,830/64
1.5 GB RAM) connected through Gigabit Ethernet. For benchmarking, the video
processing application was conﬁgured to decode every video frame of an input ﬁle
and traverse the content for a given time period. Although the application supports
a very large range of formats due to its bindings to FFmpeg, we have utilized a
set of homogeneous input ﬁles in order to generate comparable results. The ﬁles
(shown in Table 26.1) differ in bitrate and duration only and utilize mp3 (48 KHz)
and MPEG4 (25 fps) as compression formats and AVI as a container. The GOP
(Group of Pictures) length basically determines the amount of successive pictures
between two key frames, which also inﬂuences the achievable compression ratio.
The application has been executed for each ﬁle sequentially as well as on 1–5
cluster nodes.
5.2
Results and Improvements
The left part of Table 26.2 shows performance results that have been obtained using
a static input split size that corresponds to the ﬁle system’s block size (i.e. the default
conﬁguration). The results show execution times that increase horizontally (with
growing duration) as well as vertically (with growing compression rate). Here, a
higher compression rate of the payload data has a signiﬁcantly negative impact
on the application throughput. This effect however is caused by an imbalanced
workload distribution, as the content is split and distributed using data chunks
of a ﬁxed size. This strategy however provides only a reasonably fair workload
distribution if every frame is encoded as a key frame (GOP length = 1). Compression
algorithms like motion compensation disrupt this even density of information within
the byte stream. Hence, the size of a byte stream does not provide an adequate
measure for the workload it imposes on the application. For video content, it is
therefore important to balance the workload (i.e. the uncompressed frames) based
on GOPs rather than ﬁxed chunk sizes. We have implemented a simple algorithm

678
R. Schmidt and M. Rella
1
2
3
4
5
number of nodes
0
5000
10000
15000
20000
time (s)
120 min. (distr.)
90 min. (distr.)
60 min. (distr.)
30 min. (distr.)
Avg.
Speedup
Avg.
Efﬁciency
#n
1
0,99
99,8%
2
1,90
95,2%
3
2,89
96,2%
4
3,74
93,6%
5
4,62
92,4%
Fig. 26.6 Application performance for different input ﬁles on 1–5 worker nodes
that adjusts the split size based on the average GOP, block, and frame size, in
order to achieve a better workload distribution. The results in the right part of
Table 26.2 were obtained using the dynamic input split adaption algorithm. Here,
we see an overall improved throughput rate that is independent of the volumes
of content. Also, higher compression rates slightly improve the throughput due
to the smaller input ﬁle size. Figure 26.6 shows the application performance on
different numbers of nodes. In the tested setting, the application showed almost
linear speedup, allowing one to efﬁciently reduce response times by increasing the
number of worker nodes, e.g. important when processing content on demand.
5.3
Deployment on Amazon’s Utility Cloud
In the following, we provide an evaluation that investigates scalability and efﬁciency
of the application when executed in a large-scale cloud environment.
5.3.1
Cloud Deployment
Amazon’s Elastic Compute Cloud (EC2) service provides an adaptive computing
infrastructure that is accessible via the IaaS model. Users can lease resources on-
demand on an hourly basis and instantiate public or individually pre-conﬁgured
machine images (AMIs). A set of instance types allows the user to select a
speciﬁc hardware environment for hosting an application. In this evaluation, the
default small instance type (1.7 GB of memory, 1 virtual core, 160 GB of local
storage, 32-bit platform, Fedora Linux 8) has been utilized. Additionally required
software bundles like Java v1.6, Hadoop v0.21.0, and Xuggler v4.0 have been
pre-installed. The customized machine image has been bundled and permanently

26
Media Processing with MapReduce
679
stored on Amazon’s storage service (S3). This setup together with Hadoop’s built-
in support for EC2, allows us to launch the media processing application on clusters
of arbitrary size within the cloud infrastructure. It was however required to adjust
several conﬁguration parameters of the execution environment in order to cope with
the hardware limitations of the virtual machine instance.
The EC2 setup has been evaluated using one of the input ﬁles (120 min. duration,
1 frame per GOP, about 2 GB ﬁle size) that has already been utilized in the
previously described evaluation. As the distributed ﬁle system is not suitable as a
permanent data storage in a volatile cluster set-up, it was backed-up up by the S3
storage service. This allows us to permanently store input ﬁles on S3 and utilize
HDFS for provisioning the data during the computation only. In general, it is a
common setup to use S3 as a tertiary storage device and HDFS as a form of cache for
handling MapReduce workﬂows on Amazon’s infrastructure. This is in particular
supported by Hadoop’s distcp-tool, which facilitates data transfer and distribution
from S3 to HDFS. Alternatively, one could make use of Hadoop’s S3 programming
interfaces to bypass HDFS at all.
5.4
Experimental Results
For the cloud-based evaluation, the application has been executed on a cluster that
was scaled from four up to 128 nodes by successively duplicating the number of
instances. The utilized input ﬁle consists of 180.000 frames and the application has
been conﬁgured to process each frame for 100 ms. Hence, the theoretic minimal
time a single CPU requires for processing the entire input ﬁle is 300 min (18.000s).
For calculating speedup and efﬁciency, an effectively measured sequential execution
time of 21.504s was used. The evaluation results have been performed using a set of
shell scripts that automatically adjust the cluster size and trigger the job execution.
Figure 26.7 show the obtained execution times for different nodes as well as speedup
and efﬁciency values. Here, it is important to note that the values for efﬁciency and
speedup reﬂect the average performance over the whole execution time.
5.5
Evaluation of Application Scalability
The performance results described above show that the application still scales for
a single input ﬁle even when executed on a large cluster. However, the obtained
efﬁciency values signiﬁcantly decrease with a growing number of nodes on a large
cluster. This result might be unexpected as the application targets a massively
parallel problem which should provide almost linear speedup if executed on multiple
nodes (even on large clusters). Interestingly, the parallel application does execute at
the same parallel efﬁciency independently of the cluster size. In the given setup, the
results in Fig. 26.7 are misleading as they do not differentiate between execution

680
R. Schmidt and M. Rella
4
8
16
32
64
128
number of nodes
0
1000
2000
3000
4000
5000
6000
7000
time (s)
#n
Time
Speedup
Efﬁciency
4 6282s
3.42
85.6%
8 3261s
6.59
82.4%
16 1585s
13.56
84.8%
32
886s
24.27
75.8%
64
621s
34.64
54.1%
128
409s
52.53
41.0%
Fig. 26.7 Application performance in a large-scale cloud environment using a cluster that was
scaled from four to 128 nodes. The application was utilized to process each frame of a 2 GB input
video ﬁle
Fig. 26.8 Schematic diagram for the utilization of various cluster nodes during the execution of a
parallel data processing application. Job A utilizes a cluster with ﬁve nodes, job B a cluster with 10
nodes. The application life-cycle comprises of the following phases: (i) sequential initialization,
(j) parallel execution, (k) phase out, (m) sequential post-processing, and termination. The gray
areas (a1–a4, b1–b4) illustrate the number of nodes that are not in use by the application during
initialization, phase out, and post-processing phase
time that is consumed by one computer node and time that is consumed by multiple
computer nodes. This is however an important factor when it comes to the evaluation
of short running and massively parallel jobs.
Figure 26.8 schematically shows the life-cycle of a parallel application for data
processing. Basically, the application consists of a parallel execution phase (j)
that is enclosed by sequential phases at application startup (i) and termination.
Obviously, the application consumes a different number of computing resources
in different phases. This fact has to be considered when assessing the application

26
Media Processing with MapReduce
681
0
50000
100000
150000
200000
number of processed frames
0
0,5
1
efficiency
4 nodes
128 nodes
Fig. 26.9 Parallel application efﬁciency over time on four and 128 cluster nodes. Here, the
efﬁciency values indicate the processing time per frame in the parallel execution phase compared
to the processing times per frame in a sequential setup. In both conﬁgurations, the application
performs at about the same efﬁciency of approximately 87%. The run on 128 nodes shows a
gradially decreasing throughput at the end of the parallel execution (phase-out) as well as a
short phase-in stage at the beginning
performance. The phase out stage (k) starts when the payload has been processed
and the ﬁrst worker node it decommissioned by the system and ends when the last
worker is decommissioned. The maximum duration spent in this stage corresponds
approximately to the time a worker node needs to process a single input split. Due
to network delays, fail over, node restarts, and varying processing times, the worker
nodes usually do not terminate at exactly the same time. While the parallel execution
phase j can be accelerated by increasing the number of the employed cluster nodes,
this is not true for sequential application parts (i-j, m-x). These parts show constant
execution times which do not decrease with a growing number of cluster nodes.
For long running jobs, the duration spent for sequential processing is usually
insigniﬁcant compared to the overall processing time and therefore does not affect
efﬁciency and speedup. In this experiment, we have scaled the parallel application
part to an extend where the sequential application parts have a major impact on the
overall execution time. Consequently, the sequential application parts (in particular
the startup time) have a signiﬁcant impact on the overall execution time.
Using the cloud infrastructure, we have measured startup times of the application
between 35 and 140 s. The performance variations are due to the fact that the utilized
infrastructure is shared by a range of users. The initial startup phase, however,
constitutes a signiﬁcant part of the overall application execution time measured
on large clusters. Also, the phase out stage demands a considerable fraction of
the overall parallel processing time. Figure 26.9 illustrates the efﬁciency measured
during the application execution within the parallel phases j and m. The results show

682
R. Schmidt and M. Rella
an effective parallel performance of 87% independently of the actual cluster size5.
Concludingly, we note that for evaluating the performance of massively parallel and
short running jobs it is important to consider parallel and sequential application
execution separately. Here, we have shown that Hadoop provides a well suited
framework for processing of single video ﬁles in a massively parallel environment
on demand. Further improving the overall execution time can be achieved by
speeding up the sequential execution on the cluster nodes. This, however, is typically
a matter of computer hardware.
6
Conclusion
Although supported by the enormous power of a cloud infrastructure, it is a grand
challenge to build applications that scale in order to process massive workloads with
reasonable response times. Most notably, one has to cope with the complexity of co-
ordinating the involved subsystems (like application servers, computing farms, and
database systems) in order to achieve scalability and robustness. Here, we target a
more generic approach to support the processing of large volumes of digital content
in cloud-based environments. We have developed a method that exploits MapRe-
duce as the underlying programming model for the processing of large video ﬁles.
The application is based on Apache Hadoop, an open-source software framework for
data-intensive computing that can be deployed on IaaS resources. The application
is capable of handling a large variety of ﬁle formats using native video codecs and
implements the required concepts to generate meaningful records like audio/video
frames, which can be processed using common Java abstractions and user-deﬁned
logic. Data can be processed in parallel based on ﬁle chunks in order to speed up
execution times. It thereby can take advantage of a scalable computational environ-
ment in order to speed up execution times. This article discusses the programming
model and its application to binary data and summarize key concepts of the imple-
mentation. The application has been evaluated in a dedicated testing environment as
well as on a large-scale cloud infrastructure. Furthermore, we provide insights on
balancing compressed binary workload as well as assessing the application’s parallel
efﬁciency. We motivate the employment of this approach in order to achieve min-
imal response times for Internet-accessible applications that maintain audiovisual
content.
5The oscillating effect is due to the fact that nodes receive and ﬁnish map tasks at the same time
throughout the job duration. Between these processing phases efﬁciency is remarkably low as
almost all workers wait for new tasks to be scheduled by the master node.

26
Media Processing with MapReduce
683
References
1. Dean, J., Ghemawat, S.: Mapreduce: simpliﬁed data processing on large clusters. Commun.
ACM 51, 107–113 (January 2008)
2. Dean, J., Ghemawat, S.: Mapreduce: a ﬂexible data processing tool. Commun. ACM 53(1),
72–77 (2010)
3. Ekanayake, J., Pallickara, S., Fox, G.: Mapreduce for data intensive scientiﬁc analyses. In:
eScience, 2008. eScience ’08. IEEE Fourth International Conference on. pp. 277–284 (2008)
4. Gunarathne, T., Wu, T.L., Qiu, J., Fox, G.: Cloud computing paradigms for pleasingly
parallel biomedical applications. In: Proceedings of the 19th ACM Int. Symposium on High
Performance Distributed Computing. pp. 460–469. HPDC ’10 (2010)
5. Isard, M., Budiu, M., Yu, Y., Birrell, A., Fetterly, D.: Dryad: distributed data-parallel programs
from sequential building blocks. In: Proceedings of the 2nd ACM SIGOPS/EuroSys European
Conf. on Computer Systems 2007. pp. 59–72 (2007)
6. Moretti, C., Bui, H., Hollingsworth, K., Rich, B., Flynn, P., Thain, D.: All-pairs: An abstraction
for data-intensive computing on campus grids. IEEE Transactions on Parallel and Distributed
Systems 21, 33–46 (2010)
7. Pereira, R., Azambuja, M., Breitman, K., Endler, M.: An architecture for distributed high
performance video processing in the cloud. In: Proceedings of the 2010 IEEE 3rd International
Conference on Cloud Computing. pp. 482–489. CLOUD ’10 (2010)
8. Schmidt, R., Sadilek, C., King, R.: A service for data-intensive computations on virtual
clusters. Intensive Applications and Services, International Conference on 0, 28–33 (2009)
9. Thusoo, A., Sarma, J., Jain, N., Shao, Z., Chakka, P., Zhang, N., Antony, S., Liu, H., Murthy, R.:
Hive - a petabyte scale data warehouse using hadoop. In: Data Engineering (ICDE), 2010 IEEE
26th International Conference on. pp. 996–1005 (2010)
10. Warneke, D., Kao, O.: Nephele: efﬁcient parallel data processing in the cloud. In: Proceedings
of the 2nd Workshop on Many-Task Computing on Grids and Supercomputers. pp. 8:1–8:10.
MTAGS ’09 (2009)
11. Yang, H.c., Dasdan, A., Hsiao, R.L., Parker, D.S.: Map-reduce-merge: simpliﬁed relational
data processing on large clusters. In: Proceedings of the 2007 ACM SIGMOD international
conference on Management of data. pp. 1029–1040. SIGMOD ’07 (2007)


Chapter 27
Feature Selection Algorithms for Mining High
Dimensional DNA Microarray Data
David J. Dittman, Taghi M. Khoshgoftaar, Randall Wald,
and Jason Van Hulse
1
Introduction
The World Heath Organization identiﬁed cancer as the second largest contributor
to death worldwide, surpassed only by cardiovascular disease. The death count for
cancer in 2002 was 7.1 million and is expected to rise to 11.5 million annually
by 2030 [17]. In 2009, the International Conference on Machine Learning and
Applications, or ICMLA, proposed a challenge regarding gene expression proﬁles
in human cancers. The goal of the challenge was the “identiﬁcation of functional
clusters of genes from gene expression proﬁles in three major cancers: breast, colon
and lung.” The identiﬁcation of these clusters may further our understanding of
cancer and open up new avenues of research.
One of the main goals of data mining is to classify instances given speciﬁc
information. Classiﬁcation has many important applications, ranging from ﬁnding
problem areas with a computer program’s code to predicting if a person is likely to
have a speciﬁc disease. However, one of the biggest obstacles to proper classiﬁcation
is high dimensional data (data where there are a large number of features in each
instance). A very useful tool for working with high dimensional data is feature
selection, which is the process of choosing a subset of features and analyzing only
those features. Only the selected features will be used for building models; the rest
are discarded. Despite the elimination of possible data, feature selection can lead to
the creation of more efﬁcient and accurate classiﬁers [24].
An example of a type of data which absolutely needs feature selection is DNA
microarray data. The creation of the DNA microarray was a recent technological
and chemical advance in the ﬁeld of genetic research. To take advantage of the
fact that messenger RNA (mRNA), the blueprints that encode all of the proteins
made within a given cell, will readily bind to complementary DNA (cDNA), the
D.J. Dittman () • T.M. Khoshgoftaar • R. Wald • J.V. Hulse
FAU, Boca Raton, FL
e-mail: dittmandj@gmail.com; khoshgof@fau.edu; rwald1@fau.edu; jvanhulse@gmail.com
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 27, © Springer Science+Business Media, LLC 2011
685

686
D.J. Dittman et al.
microarray uses thousands of cDNA probes developed from the entire genome to
react with mRNA from the sample [17]. By determining which probes show the
greatest reactivity, we can determine which mRNA was most active and therefore
which genes are important to the sample. However, as the DNA microarray tests for
thousands of probes at a time we run into the problem of high dimensionality, and
therefore feature selection is necessary [19].
Feature selection holds a very important role in DNA microarray data outside of
streamlining the computation time of the classiﬁer. Since the features in question are
DNA probes from various genes, the chosen features are also points of interest for
further research. For example, if a particular gene appears in the top features in two
or more different cancers, then there may be a link between those cancers within
that particular gene. This selection can lead to new avenues of research beyond the
creation of a new classiﬁer.
This chapter presents an analysis of the three cancer sets supplied by the ICMLA
for their 2009 challenge. The complete data set of all three cancers contained 400
samples: 200 breast cancer samples, 130 colon cancer samples, and 70 lung cancer
samples. The samples were run on the Affymetrix U-133 Plus 2.0 gene chip, which
contains 54,613 probes derived from approximately 20,000 different genes. In other
words, each sample contains 54,613 features. The features were ranked according
to a set of eighteen feature rankers with respect to each of the three cancer data
sets. After the ranking was completed, the features were run through ﬁve different
learners for classiﬁcation of the instances. The above process was repeated four
times with differing numbers of features used in the classiﬁcation. In addition to
the classiﬁcation experiment, the ordered lists of features from each cancer were
compared to one another with each comparison only being performed on the lists
generated from a single feature ranker.
In terms of classiﬁcation, the results of each ranker and learner combination were
analyzed using the area under the Receiver Operating Characteristic (ROC) curve
metric and the area under the Precision Recall Curve (PRC) metric. Our study chose
our rankers and learners on the basis of their diversity. This diversity allowed us to
perform a thorough survey on these data mining techniques and their results on DNA
microarray data. We did this survey to ﬁnd the best values for all of these factors in
a wide variety of circumstances.
The premise behind the comparison between the ranked lists is that by comparing
the cancers to one another we concentrate on the features that are common across
multiple tissue types and eliminate features that are signiﬁcant only to the region
the speciﬁc cancer is infecting. This comparison will result in ﬁnding only those
features that are found across more then one cancer and therefore are possibly
signiﬁcant for cancer in general. By keeping the feature ranker static in each
comparison, we ensure that we do not confuse the biases of the cancers with the
biases of the feature rankers. Also, since the comparisons will be between two
ordered lists (i.e., when comparing two cancer types for a ﬁxed ﬁlter, the features are
ordered from most to least signiﬁcant), the position of the match will lend credence
to its signiﬁcance.

27
Feature Selection Algorithms for Mining High Dimensional DNA Microarray Data
687
This chapter will go into all of the steps by which we came to our conclusions.
In Sect. 2 we will discuss the eighteen ﬁlter based feature rankers used in this study.
Section 3 contains the methods by which we performed the study. Next, Sect. 4 has
the results that came from the classiﬁcation and the ranked list comparisons. Lastly,
in Sect. 5 we will discuss the conclusions that were drawn from the results as well
as the possible future work that can be derived from the conclusions.
2
Feature Rankers
There are two main approaches to feature selection, ﬁlter and wrapper. Filter
feature selection techniques analyze the features without any regard to a classiﬁer.
The ﬁlter approach uses only the raw data set and attempts to decide which
features are to be used to create the best classiﬁer. Since no classiﬁer can be
used, ﬁlter methods must rely on statistical measures. By using various statistical
tests the ﬁlter method tries to make decisions on features based on their levels
of relevancy (how related the feature is to the chosen class) and redundancy (if
the feature gives information already found in other features). Wrappers, unlike
ﬁlter approaches, use classiﬁers when making a decision, and often the classiﬁer
used to calculate the scoring of a particular subset is the same one that will be
used in the post selection analysis. There are two main disadvantages in the use of
wrapper based feature selection techniques: limited utility of chosen features and
slow computation time. The limited utility comes from the fact that the features
are chosen based on a speciﬁc classiﬁer and have little relevance outside of the
classiﬁer. The slow computational time comes from the fact that wrappers need
to build a seperate model for each of the subsets that are tested whereas ﬁlters
will rank each feature and the the top features are chosen to build the model.
Wrapper techniques attempt to compensate for the computation time by applying
searching algorithms. However, building a classiﬁer is very involved even for only
one model and is compoundedwhen multiple models are being built [10]. Therefore,
for the scope of our experiment we only use ﬁlter based feature rankers. The feature
rankers chosen can be placed into three categories: common ﬁlter based feature
selection techniques, a new ﬁlter technique called Signal to Noise, and threshold-
based feature selection techniques (TBFS) that were developed by this research team
(Table 27.1).
2.1
Non-TBFS Filter Based Feature Selection Techniques
This section describes the non-TBFS ﬁlter-based feature ranking techniques con-
sidered in this work: chi-squared [25], information gain [9, 20, 25], gain ra-
tio [20, 25], two versions of ReliefF [13, 14], symmetric uncertainty [10, 25], and
signal-to-noise [5]. All of these feature selection methods, with the exception of

688
D.J. Dittman et al.
Table 27.1 List of 18
ﬁlter-based feature selection
techniques
Abbreviation
Name
2
2 statistic
GR
Gain ratio
IG
Information gain
RF
ReliefF
RFW
ReliefF—Weight by distance
SU
Symmetric uncertainty
S2N
Signal-to-noise
F
F-measure
OR
Odds ratio
Pow
Power
PR
Probabiltiy ratio
GI
Gini index
MI
Mutual information
KS
Kolmogorov-Smirnov statistic
Dev
Deviance
GM
Geometric mean
AUC
Area under the ROC curve
PRC
Area under the precision-recall curve
signal-to-noise, are available within the Weka machine learning tool [25]. Since
most of these methods are widely known and for space considerations, only a brief
summary is provided; the interested reader can consult with the included references
for further details.
The chi-squared method (2) utilizes the 2 statistic to measure the strength of
the relationship between each independent variable and the class [18]. Information
Gain (IG) determines the signiﬁcance of a feature based on the amount by which
the entropy of the class decreases when considering that feature [26]. Gain Ratio
(GR) is a reﬁnement of Information Gain, adjusting for features that have a large
number of values. GR attempts to maximize the information gain of the feature
while minimizing the number of values [12]. Symmetric Uncertainty (SU) also
adjusts IG to account for attributes with more values, and normalizes its value
to lie in the range Œ0; 1 [21]. These techniques utilize the method of Fayyad and
Irani [7] to discretize continuous attributes, and all four methods are bivariate, only
considering the relationship between each attribute and the class and excluding the
other independent variables.
ReliefF randomly samples an example instance from the data and ﬁnds its nearest
neighbor from the same and opposite class. The values of the attributes of the nearest
neighbors are compared to the sampled instance and used to update relevance scores
for each attribute. This process is repeated for m examples, as speciﬁed by the user.
ReliefF (RF) extends Relief by handling noise and multi class data sets [14]. RF is
implemented within Weka [25] with the “weight nearest neighbors by their distance”
parameter set to false. ReliefF-W (RFW) is similar to RF except the “weight nearest
neighbors by their distance” parameter is set to true.

27
Feature Selection Algorithms for Mining High Dimensional DNA Microarray Data
689
The signal-to-noise ratio, or S2N, as it relates to classiﬁcation or feature
selection, represents how well a feature separates two classes. The equation for
signal to noise is:
S2N D .P  N /=.P C N /
where P and N are the mean values of that particular attribute in all of the
instances which belong to a speciﬁc class, which is either P or N (the positive and
negative classes). P and N are the standard deviations of that particular attribute
as it relates to the class. The larger the S2N ratio, the more relevant a feature is to
the dataset [5].
2.2
Threshold-Based Feature Selection Techniques
This section describes the TBFS method for feature ranking. Similar to 2, IG, GR,
and SU, TBFS is a bivariate procedure; each attribute is evaluated against the class,
independent of all other features in the dataset. After normalizing each attribute
to have a range between 0 and 1, the features are tested for each threshold value
t 2 Œ0; 1 according to two different classiﬁcation rules. For classiﬁcation rule 1,
examples with a normalized value greater than t are classiﬁed P while examples
with a normalized value less than t are classiﬁed as N (assuming each instance
x is assigned to one of two classes c.x/ 2 fP; N g). For classiﬁcation rule 2,
examples with a normalized value greater than t are classiﬁed N while examples
with a normalized value less than t are classiﬁed as P . Two different classiﬁcation
rules must be considered to account for the fact that for some attributes, large values
of the attribute may have a greater correlation with the positive class, while for
other attributes, large values of the attribute may have a greater correlation with the
negative class. Metric ! is calculated using the formulas provided in Algorithm 1,
either at each threshold t or across all thresholds for both classiﬁcation rules. Finally,
the metric resulting from the classiﬁcation rule which provides the most optimal
value is used as the relevancy measure for that attribute relative to metric !.
Many of the metrics ! (e.g., AUC, PRC, GM, F, KS) are primarily used to
measure the performance of classiﬁcation models, using the posterior probabilities
computed by such models to classify examples as either negative or positive
depending on the classiﬁcation threshold. The normalized attribute values can be
thought of as posterior probabilities, e.g., p.P j x/ D
OXj .x/ for classiﬁcation
rule 1, and the metrics ! are computed against this “posterior.” Since we are
using the normalized attribute values rather than building a classiﬁer and using
its posterior probabilities, these constitute ﬁlter feature selection methods instead
of wrapper feature selection methods. Intuitively, attributes where positive and
negative examples are evenly distributed along the distribution of X produce weak
measures ! and poor relevancy scores similar to how poor predictive models have
positive and negative examples evenly distributed along the distribution of the

690
D.J. Dittman et al.
Algorithm 1: Threshold-based feature selection algorithm
input :
a. Dataset D with features Xj ; j D 1; : : : ; m;
b. Each instance x 2 D is assigned to one of two classes c.x/ 2 fP; Ng;
c. jPj D jfx 2 Djc.x/ D Pgj, jNj D jfx 2 Djc.x/ D Ngj;
d. The value of attribute Xj for instance x is denoted Xj .x/;
e. Metric ! 2 fF, OR, Pow, PR, GI, MI, KS, Dev, GM, AUC, PRCg.
output: Ranking R D fr1; r2; : : : ; rmg where attribute Xj is the rj -th most signiﬁcant attribute as determined
by metric !.
for Xj ; j D 1; : : : ; m do
Normalize Xj 7! OXj D
Xj min.Xj /
max.Xj /min.Xj / ;
OXj 2 Œ0; 1;
for t 2 Œ0; 1 do
Compute Basic Metrics:
Classiﬁcation Rule 1: 8 x 2 D; Oct .x/ D P ”
OXj .x/ > t, otherwise Oct.x/ D N.
TP.t/ D jfxj.Oct.x/ D P/ \ .c.x/ D P/gj ; TN.t/ D jfxj.Oct.x/ D N/ \ .c.x/ D N/gj ;
FP.t/ D jfxj.Oct.x/ D P/ \ .c.x/ D N/gj ; FN.t/ D jfxj.Oct.x/ D N/ \ .c.x/ D P/gj ;
TPR.t/ D jTP.t/j
jP j ; TNR.t/ D jTN.t/j
jNj ; FPR.t/ D 1  TNR.t/; FNR.t/ D 1  TPR.t/;
PRE.t/ D
jTP.t/j
jTP.t/jCjFP.t/j ; NPV.t/ D
jTN.t/j
jTN.t/jCjFN.t/j .
Compute Final Metrics:
Metric !
Calculation
F1. OXj /
=
max
t2Œ0;1
.1Cˇ2/PRE.t/TPR.t/
ˇ2PRE.t/CTPR.t/ ; ˇ D 1
OR1. OXj /
=
max
t2Œ0;1
TP.t/TN.t/
FP.t/FN.t/
Pow1. OXj /
=
max
t2Œ0;1 ..1  FPR.t//k  .1  TPR.t//k/; k D 5
PR1. OXj /
=
max
t2Œ0;1
TPR.t/
FPR.t/
GI1. OXj /
=
min
t2Œ0;1 Œ2PRE.t/.1  PRE.t// C 2NPV.t/.1  NPV.t//
KS1. OXj /
=
max
t2Œ0;1 jTPR.t/  FPR.t/j
GM1. OXj /
=
max
t2Œ0;1
pTPR.t/  TNR.t/
AUC1. OXj /
=
Area under the curve generated by .FPR.t/; TPR.t//; t 2 Œ0; 1
PRC1. OXj /
=
Area under the curve generated by .PRE.t/; TPR.t//; t 2 Œ0; 1
MI1. OXj /
=
max
t2Œ0;1
P
Oct 2fP;N g
P
c2fP;N g p.Oct ; c/ log
p.Oct ;c/
p.Oct /p.c/ where p.Oct D ˛; c D ˇ/ D
jfxj.Oct .x/D˛/\.c.x/Dˇ/gj
jP jCjNj
, p.Oct D ˛/ D jfxjOct .x/D˛gj
jP jCjN j
, p.c D ˛/ D jfxjc.x/D˛gj
jP jCjN j
,
˛; ˇ 2 fP; Ng
Dev1. OXj /
=
min
t2Œ0;1
P
x2St .v.x/  v.S t//2 C P
x2 NSt .v.x/  v. NS t//2
where S t D fxj OXj > tg,
NS t D fxj OXj  tg, v.S t/ D jS tj1 P
x2St v.x/, v. NS t/ D j NS tj
1 P
x2 NSt v.x/, and
v.x/ D 1 if x 2 P, otherwise, v.x/ D 0
Compute the same basic metrics and ﬁnal metrics (denoted as !2) as listed above, but using:
Classiﬁcation Rule 2: 8 x 2 D; Oct .x/ D N ”
OX j .x/ > t, otherwise Oct .x/ D P.
!. OX j / D max

!1.Oxj /; !2.Oxj /

where !1 is the original basic metric
Create attribute ranking R using !. OXj /8j
posterior probability produced by the model. Note further that TBFS can easily
be extended to include additional metrics. What differentiates each TBFS technique
from the others is which metric is used to calculate the values used in the rankings.
Next we will go more in depth on each of these metrics.
F-Measure is a derivative of the true positive rate (TPR) and precision (PRE).
The formula for the F-measure maximized over all thresholds is:
F-measure D max
t2Œ0;1
.1 C ˇ2/  TPR.t/  PRE.t/
ˇ2  TPR.t/ C PRE.t/

27
Feature Selection Algorithms for Mining High Dimensional DNA Microarray Data
691
ˇ is a parameter that can be changed by the user to place more weight on either
the true positive rate or precision. We decided to use a value of 1 for ˇ. Both the
true positive rate and precision are measured throughout the range of thresholds
and applied to the equation. The value that is the largest becomes the ofﬁcial
measurement for the attribute [25].
Odds ratio is another TBFS technique. Odds ratio is deﬁned as:
OR D max
t2Œ0;1
TP.t/TN.t/
FP.t/FN.t/
where TP.t/ is the number of true positives, TN.t/ is the number of true negatives,
FP.t/ is the number of false positives, and FN.t/ is the number of false negatives.
After applying the odds ratio metric across the range of thresholds the largest value
is the recorded value of the feature [8].
The Kolmogorov-Smirnov statistic, or KS, is a measurement of separability. The
goal of KS is to measure the maximum difference between the distributions of the
members of each class. The formula for the KS statistic is [22]:
KS D max
t2Œ0;1 jTPR.t/  FPR.t/j
Power is very similar to KS in that it is the maximum distance between the curves
of 1  FPR and 1  TPR, where FPR is the false positive rate and TPR is the true
positive rate. 1  FPR is also known as the true negative rate, TNR, and 1  TPR is
also known as the false negative rate, FNR. The important difference between Power
and KS is that there is an additional variable k, whose value is assigned by the user
(in this work we used k D 5), which is applied to the equation such that the power
equation is [8]:
Power D max
t2Œ0;1

.TNR.t//k  .FNR.t//k
Probability ratio, or PR, is a simple and convenient TBFS method. The ratio is
deﬁned as:
PR D max
t2Œ0;1
TPR.t/
FPR.t/
In the end, this metric searches for the threshold that maximizes precision [8].
Gini index, or GI, was introduced by Breiman et al. [4] as an aspect of the CART
algorithm. The Gini index is a measurement of how likely it is that an instance will
be labeled incorrectly. An example of incorrect labeling is a positive value that was
labeled as a negative. The equation for the Gini index is:
GI D min
t2Œ0;1Œ2PRE.t/.1  PRE.t// C 2NPV.t/.1  NPV.t//

692
D.J. Dittman et al.
where NPV.t/ is the negative predictive value, or the percentage of instances
predicted to be negative that are actually negative at threshold t. Since lower values
here mean lower chances of misclassiﬁcation, lower is better, and so the minimum
Gini index score is the chosen score for the attribute [4].
Another frequently used TBFS technique is mutual information. Mutual infor-
mation, like information gain, is a measure of entropy or uncertainty. They differ in
that mutual information measures the joint probability of a feature to a class whereas
information gain measures the entropy of the feature within the data set. The actual
deﬁnition of mutual information is “the amount by which the knowledge provided
by the feature vector decreases the uncertainty about the class” [3]. The equation for
mutual information is:
MI D max
t2Œ0;1
X
Oct2fP;Ng
X
c2fP;Ng
p.Oct; c/ log p.Oct; c/
p.Oct/p.c/
where c represents the actual class of the instance and Oct is the predicted class of
the instance [3]
Deviance, like Gini index, is a metric in which it is the minimum value over all
the thresholds that is the chosen value for the attribute. Deviance measures the sum
of the squared errors from the mean class based on a threshold t [24].
Geometric mean is a quick and useful metric for feature selection. The equation
for the geometric mean is the square root of the product of the true positive rate and
the true negative rate. A geometric mean of one would mean that the attribute is
perfectly correlated. The most useful feature of the geometric mean is the fact that
not only does it maximize the true positive rate and the true negative rate but it keeps
them balanced which is often the preferred state [24]. The maximum geometric
mean across the thresholds is the score of the attribute.
Receiver Operating Characteristic, or ROC, curves are a graph of the true
positive rate on the y-axis versus the false positive rate on the x-axis. This curve
is created by mapping the points along the range of the thresholds. The curve
itself represents the trade off between the rate of detection and the rate of false
alarms. In order to acquire a single numeric value for the purpose of ranking the
predictive power of the attribute, the Area Under the ROC Curve, or AUC, is
measured and recorded. The larger the area, between zero and one, the more power
the attribute has [6].
The Precision-Recall Curve, or PRC, is a curve which plots the precision on the
x-axis and the recall on the y-axis across the entire range of thresholds. The concept
is very similar to that of the ROC curve. Like the ROC curve, it is the area under the
PRC curve that is used as a single numeric value for ranking purposes. The closer
the area is to one, the stronger the predictive power of the attribute [22].

27
Feature Selection Algorithms for Mining High Dimensional DNA Microarray Data
693
3
Methods
The ﬁrst step in the analysis is to rank the entire set of features for each of the three
cancer sets. The TBFS techniques require that there only be two classes involved.
As such, the data set was split into two classes, the class of interest (one of the three
cancers) and the other class (the other two remaining classes). For example, if the
class of interest was breast cancer, then the other class would be both lung and colon
cancers. A mixed set of eighteen feature rankers consisting of six common ﬁlter
based feature selection techniques in addition to eleven threshold based ﬁlter tech-
niques proposed by our research group and a rarely used feature ranker called signal
to noise was chosen for the analysis of the features in the cancer sets. Each cancer
set had their features ranked eighteen different times, once for each feature ranker.
After the rankings were performed the data was classiﬁed with the chosen learners.
3.1
Classiﬁcation
Before the learners can be applied the number of features to use must be determined.
We decided to use four different feature set sizes based on a percentage of the total
data set. The feature set sizes used were: 0.02%, 0.05%, 0.10%, and 0.20% of the
total number of features. The reason why we choose such small numbers of features
is that while every cell in the body contains the entire genetic code of the organism,
the genes that are actually activated and used in the cell are a very small subset [17].
Even though the number of features may be small, previous research shows the sizes
used are appropriate for classiﬁcation purposes.
In this experiment we used ﬁve different classiﬁers or learners in order to make
each classiﬁcation. The ﬁve learners used were the Support Vector Machine, Naive
Bayes, k-Nearest Neighbors, Logistic Regression, and Multi-Layer Perceptron.
These learners were chosen as each takes a unique approach towards classiﬁcation
when compared to the other learners. The speciﬁcs of the learners will be explained
in their respective sections. The data was run through each learner for binary
classiﬁcation with respect to a targeted cancer (breast, colon, or lung). The two
possible classes are either an instance of the targeted cancer or an instance not of
the targeted cancer.
In order to evaluate the learner and its parameters we used ﬁvefold cross
validation. Cross validation is a commonly used method for evaluating a classiﬁer.
This is done by splitting the data set into the number of folds speciﬁed. The ﬁrst
n  1 folds are used to train the classiﬁer and the last fold is used to test it. The
ranking and the building of the classiﬁer described above occurs within the training
portion. The process is repeated until every fold has been the test fold and each
result is recorded [24]. In order to avoid a bad split and to gain a better picture of the
performance of the classiﬁer the cross validation process itself was repeated three
more times, for a total of four. The evaluation was repeated four times with varying

694
D.J. Dittman et al.
numbers of features used in the classiﬁcation. The overall process was repeated
three times so each of the three cancers could be the target. In total 5 folds  4 runs
 18 feature rankers  4 different numbers of features  3 cancers  5 learners D
21,600 models built. The particulars of the speciﬁc learners are explained below.
3.1.1
Support Vector Machine
One of the most efﬁcient ways to classify between two classes is to assume that
both classes are linearly separated from each other. This assumption allows us
to use a discriminant to split the instances into the two classes before looking
at the distribution between the classes. A linear discriminant uses the formula
g.xjw; !0/ D wT x C !0. In the case of the linear discriminant we only need to
learn the weight vector, w and the bias !0. One aspect that must be addressed is
that there can be multiple discriminants that correctly classify the two classes. The
Support Vector Machine, or SVM, is a linear discriminant classiﬁer which assumes
that the best discriminant maximizes the distance between the two classes. This is
measured in the distance from the discriminant to the samples of both classes [16].
3.1.2
Naive Bayes
Using a discriminant classiﬁer like SVM is not the only classiﬁer that one can use
to determine the classiﬁcation of an instance. Another way is to use a probability
model in which the features that were chosen by the feature rankers are used
as the conditions for the probability of the sample being a member of the class.
A basic probability model would look like p.CjF1; : : : ; Fn/ where Fi is the value
of each feature used and C is the class of the instance. This model is known as
the posterior and we assign the instance to the class for which it has the largest
posterior [23].
Unfortunately it is quite difﬁcult to determine the posterior directly. Thus it is
necessary to use Bayes’ rule which states that the posterior equals the ratio of the
prior multiplied by the likelihood over the evidence or
p.CjF1; : : : ; Fn/ D p.C/p.F1; : : : ; FnjC/
p.F1; : : : ; Fn/
In reality the formula above can be simpliﬁed by certain assumptions. The evidence,
p.F1; : : : ; Fn/, is always constant for the speciﬁc data set and therefore can be
ignored for the purposes of classiﬁcation. The likelihood formula, p.F1; : : : ; FnjC/
can be simpliﬁed to Q
i p.FijC/ due to the naive assumption that all of the features
are conditionally independent of all of the other features. This naive assumption
with the removal of the evidence parameter creates the Naive Bayes classiﬁer [23].
p.CjF1; : : : ; Fn/ D p.C/
Y
i
p.FijC/

27
Feature Selection Algorithms for Mining High Dimensional DNA Microarray Data
695
3.1.3
MLP
Multilayer Perceptron (MLP) is a type of artiﬁcial neural network. Artiﬁcial neural
networks consist of nodes which are arranged in sets called layers. Each node in a
layer has a connection coming from every node in the layer before it and to every
node in the layer after it. Each node takes the weighted sum of all of the input nodes.
Along with the weighted sums an activation function is also applied. The application
of the activation function to the result of the weighted sum allows for a more clearly
deﬁned result by further separating the instances in the two classes from each other.
Neural networks are well known for being robust to redundant features. However,
neural networks sometimes have problems with overﬁtting [11].
3.1.4
k-Nearest Neighbors
The k-nearest neighbors, or k-NN, learner is an example of an instance based
and lazy learning algorithm. Instance based algorithms use only the training data
without creating statistics to base their hypotheses. The k-NN learner does this by
calculating the distance of the test sample from every training instance, and the
predicted class is derived from the k nearest neighbors.
In the k-NN learner, when we get a test sample we would like to classify, we
tabulate the classes for each of the k closest training samples (we used a k of ﬁve
for our experiment) and we determine the weight of each neighbor by taking a
measurement of 1=distance where distance is the distance from the test sample.
After the classes and weights are tabulated we add all of the weights from the
neighbors of the positive class together and all of the weights of the negative class
together. The prediction will be the class with the largest cumulative weight [23].
The k-NN learner can use any metric that is appropriate to calculate the distance
between the samples. The standard metric used in k-NN is Euclidean Distance
deﬁned as
d.x; y/ D
v
u
u
t
n
X
iD1
.xi  yi/2
3.1.5
Logistic Regression
Logistic Regression is a statistical technique that can be used to solve binary
classiﬁcation problems. Based on the training data, a logistic regression model is
created which is used to decide the class membership of future instances [15].

696
D.J. Dittman et al.
Algorithm 2: Algorithm for comparative analysis
input :
1. A ﬁle for each cancer in which each line contains the feature ranker and a list of rankings
r sorted by the probe keyp ;
2. The chosen comparison;
output:
Two tables: One containing the number of matches at each of the chosen points, the other
containing the average percent overlap at the speciﬁc points.
for Each Feature Ranker do
1. Load the sorted list of that feature ranker for each cancer into separate arrays where
the index represents the ranking i.e., breast1[r] = p;
2. Reverse the key value relationship of the above arrays so that the index is the probe
key i.e., breast2[p] = r. This is for efﬁcient searching for the probe in other cancers;
for r; r D 1; : : : ; 200 do
1. Perform comparison at position r;
2. Adjust the number of matches, m, according to the results of the comparison and
store at Count[r];
3. Calculate percent overlap, o: o D .m=r/  100 and declare PERSUM[r] as
PERSUM[r  1] Co;
3. Output the number of matches at the chosen points;
4. Compute the average of the percent overlaps at each of the chosen points i.e.,
PERSUMŒ10=10 and output;
3.2
Ranked List Comparisons
There were four comparisons performed for each subset of ordered lists. There were
18 such subsets, one for each feature ranker, each of which consisted of the ranked
lists from each of the three cancers that was created using that speciﬁc feature ranker.
The two types of comparisons that were performed were (1) the overlap of probes
that appear in all three cancers and (2) the three possible pairwise comparisons:
Breast and Colon cancers, Breast and Lung cancers, and Colon and Lung cancers.
All of the comparisons follow the structure of Algorithm 2. Each comparison was
evaluated for two measurements at various points, the number of common probes
and the mean percentage of common probes per total probes. The number of
common probes was chosen to show the similarity at each speciﬁc point. The mean
percentage of common probes per total probes will show the average fraction, taken
from the beginning to the point in question, of all probes being considered that
appear in both/all lists. For example, the mean percentage of common probes per
total probes within the top ten features will be higher for a case where the common
probes occurred in the ﬁrst ﬁve versus the case where the common probes were
found in the last ﬁve. By using a combination of the two readings we can get a sense
of how much overlap there is and where the overlap occurs. These measurements
were taken at the following points: 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 125, 150,
175, 200. The points were chosen to give a good picture of the range of the top 200
features.

27
Feature Selection Algorithms for Mining High Dimensional DNA Microarray Data
697
4
Results
The results for both the Classiﬁcation and the Ranked List Comparisons are shown
below. We will ﬁrst present the results from the classiﬁcation.
4.1
Classiﬁcation
Figures 27.1–27.15 provide charts to better visualize the results of the classiﬁcation.
Each line represents one of the eighteen feature rankers and the four points represent
the average value of all of the runs using that percentage of the total number
of features. Each graph holds constant which cancer data set and which learner
were used.
Each of the models (built with a given choice of learner, ﬁlter, number of features,
and dataset) were evaluated by plotting the classiﬁcation results for both the ROC
and PRC Curves. This is different from the use of the ROC and PRC curves within
the TBFS set of feature rankers. When using ROC and PRC as classiﬁcation metrics,
the curves are calculated using the posterior probabilities produced by the classiﬁer
trained with the selected features and tested on the test data. The area under these
two curves is used to determine the effectiveness of the classiﬁers.
There are three factors that need to be chosen in order to perform the clas-
siﬁcation: learner, ﬁlter, and percentage of features used in the model. Each of
these factors showed a pattern when used for classiﬁcation. In terms of the ﬁlter,
in all three classes there was no single ﬁlter that stood out as the best performer
for any of the learners. In reality, a majority of the feature rankers performed
similarly well in each learner. However, in every learner there was at least one
ﬁlter that consistently performed the worst. Table 27.2 contains the worst ﬁlter
Fig. 27.1 Breast results: naive Bayes

698
D.J. Dittman et al.
Fig. 27.2 Breast results: MLP
Fig. 27.3 Breast results: 5-NN
Fig. 27.4 Breast results: SVM

27
Feature Selection Algorithms for Mining High Dimensional DNA Microarray Data
699
Fig. 27.5 Breast results: logistic regression
Fig. 27.6 Colon results: naive Bayes
Fig. 27.7 Colon results: MLP

700
D.J. Dittman et al.
Fig. 27.8 Colon results: 5-NN
Fig. 27.9 Colon results: SVM
Fig. 27.10 Colon results: logistic regression

27
Feature Selection Algorithms for Mining High Dimensional DNA Microarray Data
701
Fig. 27.11 Lung results: naive Bayes
Fig. 27.12 Lung results: MLP
Fig. 27.13 Lung results: 5-NN

702
D.J. Dittman et al.
Fig. 27.14 Lung results: SVM
Fig. 27.15 Lung results: logistic regression
under the various combinations of factors. If the entry covers multiple rows then
that entry is the worst ﬁlters for all of the overlapping combinations. For example,
the table shows that Pow and PR are the worst ﬁlters for Naive Bayes under both
the ROC and PRC curves for any percentage of totat number of features used.
One of the more clearly visible trends was the performance of the ﬁve learners
(Naive Bayes, MLP, 5-NN, Logistic Regression, and SVM) in regards to the three
classes (breast, colon, and lung). In all of the classes SVM was the best learner
except for one exception; see table for details. It should also be noted that each
learner required a different percentage of the total number of features to maximize
performance. When considering each learner across all classes, the percentage of the
total number of features necessary generally stays the same with some shifting for
a speciﬁc class. Table 27.3 contain the best learner and best percentage of features
used under the various combinations of factors. To avoid confusion it should be

27
Feature Selection Algorithms for Mining High Dimensional DNA Microarray Data
703
Table 27.2 Classiﬁcation results: worst ﬁlter
Worst ﬁlters
(Percentages represent the percentage of features used)
Breast
Colon
Lung
<.01%
.01%
ALL
<.01%
.01%
Naive Bayes
ROC
Pow PR
Pow PR
RF PR
GI PR
GI PR
PRC
MLP
ROC
Pow PR
Pow PR
GR
Pow PR
Pow PR
PRC
RF RFW
RF RFW
5-NN
ROC
RFW PR
RFW PR
S2N OR
S2N GR
S2N GR
PRC
SVM
ROC
RFW RF
Pow PR
RFW
GR SU
PRC
PRC
Logistic regression
ROC
Pow PR
Pow PR
SU
GM SU
GM SU
PRC
Table 27.3 Classiﬁcation results: best learner and best percentage of features used
Best learner
Breast
Colon
Lung
ROC
SVM
SVM
MLP
PRC
SVM
SVM
SVM
Best percentage of features used
Breast
Colon
Lung
Naive Bayes (%)
0.02
0.02
0.05
MLP (%)
0.02
0.02
0.02
5-NN (%)
0.05
0.05
0.10
SVM (%)
0.05
0.02
0.10
Logistic regression (%)
0.02
0.02
0.02
noted that both tables are across all ﬁlters and that the Best Learner is across all
percentage of total features used and the Best Percentage Used is across both ROC
and PRC curves.
4.2
Ranked List Comparisons
4.2.1
Comparison Between All Three Cancers
There were very few probes that were signiﬁcant in all three cancers. None of the
eighteen feature rankers found any probes within the top forty features in each
cancer for that ranker. The ﬁrst one to ﬁnd a probe common to all three cancers was
RFW, which found one within the top ﬁfty features. However, RFW did not ﬁnd
another common probe until searching through the top seventy probes. The ranker
that found the most probes within the top 200 features was IG with thirteen common
probes. Following IG was 2, RFW, and RF with six, ﬁve, and four common probes,
respectively. Eleven out of the eighteen feature rankers did not ﬁnd any features

704
D.J. Dittman et al.
Table 27.4 Correlation between breast and colon cancers: count
Ranker
10
20
30
40
50
60
70
80
90
100
125
150
175
200
2
0
0
1
1
3
6
8
10
13
15
19
28
43
54
GR
0
0
1
4
6
11
17
21
25
30
52
65
77
90
IG
0
1
1
1
5
6
9
12
14
15
22
39
56
64
RF
6
10
16
20
21
26
35
39
45
48
66
82
98
113
RFW
2
9
16
21
23
27
29
33
39
44
56
68
78
95
SU
0
0
0
2
4
8
11
17
19
23
34
48
62
79
F
0
0
0
2
4
6
7
9
14
17
35
49
61
77
OR
2
5
8
12
16
17
19
25
31
36
50
61
75
92
Pow
0
0
0
0
0
0
0
0
0
0
1
1
3
6
PR
0
0
0
0
0
0
0
1
1
1
1
1
1
2
GI
0
0
0
0
2
11
16
21
24
28
39
45
56
64
MI
0
0
0
2
4
7
8
9
14
16
29
41
51
68
KS
0
0
0
2
4
6
7
13
13
16
21
35
44
57
Dev
0
0
0
2
4
6
6
8
9
11
22
35
43
56
GM
0
0
0
2
4
6
7
13
13
17
21
31
39
54
AUC
1
2
2
5
8
10
12
13
16
19
25
38
52
62
PRC
0
0
0
1
2
4
6
7
7
10
13
14
16
20
S2N
0
2
6
6
13
19
23
27
28
34
49
64
81
100
within the top 200 of each cancer that appeared in all three. The numerical results
of this comparison could not be presented due to space limitations.
4.2.2
Comparison Between Breast and Colon Cancer
As one can see from Tables 27.4 and 27.5, there was a large correlation between
the breast and colon cancer data sets. That is to say, there were more features in
common among the higher rankings between breast and colon than between either
and lung. These tables show the average percent similarity (overlap) or the number
of features in common (count) among the ﬁrst N (where N is the number in the top
row) features from each cancer, for a given feature ranker. The feature ranker that
had the most common features between the two cancers was RF with 113 common
features out of 200, followed closely by signal to noise with 100 common probes out
of 200. RF was also found to be a very useful ranker in Altidor et al. [1]. Apropos
to this, Table 27.6 shows the top 20 probes found by RF between breast and colon
cancer; these are a target for future research. Four of the feature rankers, RF, RFW,
AUC, and OR, found common features within the top ten features of the breast
and colon data sets and sixteen of the rankers found common features within the top
ﬁfty features of each data set. The two that did not ﬁnd much correlation between the
two cancers were the Pow and PR rankers with six and two features, respectively,
found in the top 200 features. It should be noted that while not all of the rankers
found many matches, even those which found few matches between breast and colon
performed better on this pair than on either considered with lung.

27
Feature Selection Algorithms for Mining High Dimensional DNA Microarray Data
705
Table 27.5 Correlation between breast and colon cancers: overlap (numbers are percentages)
Ranker
10
20
30
40
50
60
70
80
90
100
125
150
175
200
2
0
0
1:01
1:47
2:13
2:97
4:13
5:02
5:99
6:85
8:36
9:85
11:58
13:26
GR
0
0
0:11
2:03
3:85
5:90
8:21
10:26
12:12
13:77
18:30
22:28
25:34
27:69
IG
0
0:51
1:67
1:96
2:82
3:91
5:11
6:29
7:24
8:09
9:55
11:52
13:99
16:16
RF
43:35
45:41
48:55
49:46
48:52
47:21
46:89
47:24
47:43
47:62
48:06
48:97
49:85
50:70
RFW
7:46
19:43
30:82
35:99
38:48
39:79
40:20
40:27
40:62
40:98
41:56
42:15
42:58
43:07
SU
0
0
0
0:66
1:74
3:54
4:95
6:62
8:25
9:59
12:60
15:57
18:03
20:49
F
0
0
0
0:67
1:79
3:08
4:12
5:06
5:98
6:95
10:06
13:44
16:30
18:82
OR
19:41
17:95
20:30
22:11
23:71
24:69
24:98
25:45
26:31
27:27
29:51
31:31
32:77
34:33
Pow
0
0
0
0
0
0
0
0
0
0
0:01
0:13
0:23
0:48
PR
0
0
0
0
0
0
0
0:08
0:20
0:29
0:41
0:46
0:48
0:55
GI
0
0
0
0
0:65
2:48
4:93
7:47
9:63
11:54
15:16
17:76
19:61
21:17
MI
0
0
0
0:74
1:85
3:09
4:20
5:05
5:90
6:89
9:60
12:14
14:48
16:62
KS
0
0
0
0:90
2:11
3:54
4:51
5:61
6:68
7:46
9:39
11:10
13:00
14:84
Dev
0
0
0
0:82
1:83
3:08
3:95
4:61
5:24
5:78
7:63
9:78
11:85
13:69
GM
0
0
0
0:77
2:01
3:45
4:49
5:59
6:66
7:53
9:35
11:10
12:61
14:20
AUC
1
4:93
5:93
7:76
9:09
10:31
11:39
11:99
12:54
13:09
14:20
15:93
17:67
19:33
PRC
0
0
0
0:63
0:98
1:81
2:55
3:27
3:82
4:40
5:54
6:26
6:61
7:04
S2N
0
1:34
6:15
8:86
11:18
14:26
17:16
19:22
20:63
21:79
24:71
27:56
29:87
32:21

706
D.J. Dittman et al.
Table 27.6 First 20 matches
of RF between breast and
colon cancers
203,824 at
229,777 at
228,912 at
205,506 at
604,74 at
201,884 at
221,245 s at
222,592 s at
205,190 at
218,687 s at
209,211 at
1,554,945 x at
230,772 at
227,475 at
218,796 at
201,329 s at
230,914 at
209,847 at
219,010 at
218,322 s at
4.2.3
Comparison Between Lung and Colon Cancers and Lung
and Breast Cancers
Lung cancer had very little correlation with either breast or colon cancer. When
looking at the number of features that are unique to the top 200 features, ﬁve of
the rankers had no similar genes and the two rankers with the lowest number of
unique features were IG and RFW with 178 unique features. There was more of a
correlation between lung and breast cancers as there were similarities found within
the top twenty features in one of feature rankers, RF, and one whose ﬁrst similarity
was in the top thirty features, RFW. Between lung and colon, only two feature
rankers found matches within the top 200 features: 2 and AUC. Unfortunately due
to space limitations the actual results could not be displayed within this paper. The
feature rankers as a whole found very little similarity when compared to the results
from the comparison between breast and colon cancers.
5
Conclusion
The goal of this chapter was to present a thorough analysis of the use of the data
mining techniques of classiﬁcation and feature selection on the domain of gene
expression data. The classiﬁcation results and the ranked list comparisons resulted
in a list of recommendations for performing classiﬁcation on gene expression data
as well as adding evidence to a possible link between genetic based cancers and
potential new areas of research.
5.1
Classiﬁcation
The purpose of testing all of the combinations of the 18 feature rankers and the
5 learners was to ﬁnd the optimum factors needed for use in the classiﬁcation
of DNA microarray data. Our study chose our ﬁlters and learners on the basis
of their diversity. Our learners consist of a Bayesian classiﬁer (Naive Bayes), an
Artiﬁcial Neural Network (MLP), an instance based classiﬁer (5-NN), a generalized

27
Feature Selection Algorithms for Mining High Dimensional DNA Microarray Data
707
linear classiﬁer (SVM), and a statistical technique that can be used for binary
classiﬁcation (LR). The 18 feature rankers were chosen in order to test a wide variety
of statistical measures as well as test both common, uncommon, and relatively new
techniques side by side. These facts allowed us to perform a thorough survey on
these data mining techniques and their results on DNA microarray data. The ﬁrst
factor is the optimum learner for the correct data set. The optimum learner was
SVM for all data sets except when using Lung with ROC, in which case MLP
was best.
The next factor is the optimum number of features to use. In general, the number
of features necessary to effectively classify instances in all three data sets is very
small when compared to the full set of 54,613 features. Naive Bayes, MLP, and
Logistic Regression used only 0.02% (about 10) of the total number of features
with one exception for the lung cancer dataset in Naive Bayes which required
0.05% (about 27) of the total number of features. The 5-NN learner required 0.05%
of the total number of features except for the lung cancer dataset which required
0.10% (about 54) of the total number of features. The only learner which required
a different number of features for each data set was SVM which required 0.02%
for the colon cancer dataset, 0.05% for the breast cancer data set, and 0.10%
for the lung cancer data set. As we can clearly see, even the largest number of
features needed, 0.10%, is very small compared to the total number of features
available. From a biological point of view this makes sense as the DNA microarray
is designed to test the sample placed on it across the whole genome. However,
the number of genes that are active in a particular sample is much smaller then
the size of the whole genome. Therefore the use of feature selection is absolutely
necessary in order to determine which genes are the most important to the sample
at hand.
The last factor that needs to be addressed is which ranker to use with each learner.
We discovered that there is no clear ranker that is superior to each dataset/learner
combination. However there are some ﬁlters that are clearly inferior in terms of
classiﬁcation for these combinations and should be avoided when performing these
experiments. The ﬁlters to avoid in the breast cancer data set are Pow and PR for
Naive Bayes, MLP, and Logistic Regression; PR and RFW for 5-NN; and Pow,
PR, RFW, and RF, for SVM. In the colon cancer dataset the ﬁlters to avoid are
RF and PR for Naive Bayes; GR for MLP; S2N and OR for 5-NN; GR and SU
for SVM; and SU for Logistic Regression. Lastly, in the lung cancer data set the
ﬁlters to avoid are PR and GI for Naive Bayes; PR, Pow, RF, and RFW for MLP;
S2N and GR for 5-NN; PRC and RFW for SVM; and GM and SU for Logistic
Regression.
When we combined the optimum setting for the three factors (ﬁlter, learner, and
number of features) we come up with optimum strategies for each dataset. For the
breast cancer data set we recommend using the SVM learner with 0.05% of the total
number of features while avoiding Pow, PR, RFW, and RF. Our recommendations
for the colon cancer dataset are the SVM learner using 0.02% of the total number of
features and avoiding GR and SU. Finally in the lung cancer dataset we recommend

708
D.J. Dittman et al.
using either the MLP learner using 0.02% of the total number of features or the
SVM learner using 0.05% of the total number of features. With both learners we
recommend avoiding PR, Pow, RF, and RFW.
5.2
Ranked List Comparisons
The ranked list comparisons allowed us to determine not only which genes are
important to the cancers (the rankings themselves), but what genes carry a strong
link between the cancers. The combination of ranking followed by comparison adds
a component of importance or weight to a comparison. Without the ranking a match
means very little because while there is match, we would have no indication that
either gene is important to the disease or not. This technique uses the ranking to
focus our view only to those genes which are statistically important to the data set
and by extension the cancers.
While all rankers will rank all genes, it is important to give priority to those genes
found at the top of their ranked lists. When two cancers share many of the same
genes among the tops of their rankings, it suggests that there may be an important
genetic link between them. The large correlation within the top ranked probes
between the breast and colon cancer data sets lends credence to the possibility of a
link between cancers that have a genetic basis. This is further supported by the lack
of correlation within even the top 200 probes between lung cancer and the other two
cancers. This is due to the fact that the basis for lung cancer is the “activation of
procarcinogens that lead to adduct formation and subsequent failure of DNA repair,
which should normally remove these adducts” [2]. Deeper analysis into the genes
the correlating probes are built from would be beneﬁcial to the understanding of
hereditary cancers. Also, applying the comparison analysis between other known
genetic based cancers, e.g., ovarian, prostate, skin, could lead to further support for
these genes.
From the perspectives of the feature rankers, four of the rankers stood out;
RF, RFW, AUC, and OR. These rankers found matches among the higher ranking
features across all of the comparisons. The majority of the remaining feature
rankers performed moderately with the exception of two, POW and PR, which were
especially poor. We believe the difference in performance is due to each ranker’s
ability to take into account both classes as well as properly deal with imbalanced
data. RF, RFW, AUC, and OR all focus on both classes and take measures to
account for the possible imbalance of the number of instances in each class. POW
and PR both only look at the instances classiﬁed as positive and take no measures
in dealing with class imbalance. The remaining metrics take some measures to
meet these challenges but fall short of the measures taken by the top four metrics.
It should also be noted that both Pow and PR also performed poorly when used as
the feature ranker in classiﬁcation. Once again we believe that this outcome is the
results of the inability of both rankers to account for both classes and deal with class
imbalance.

27
Feature Selection Algorithms for Mining High Dimensional DNA Microarray Data
709
5.3
Future Work
Our dataset was very unique in that each of the three cancers are very different
from each other, from area of effect to exposure to environmental factors. Therefore,
the next step in this research is to search for these patterns in a data set which
contains more subtle differences. For example, in breast cancer, we could try to
classify instances that arose from different locations in the breast, such as from the
milk duct and the lobules that supply the milk. Another interesting factor of our
dataset is that we were classifying between instances of three different cancers and
not between cancerous and noncancerous instances. Therefore, another interesting
study is to use both cancerous and non cancerous instances to see if the patterns and
factors discovered above hold true in these other datasets. This also applies to the
ranked list comparisons in that using these new datasets may contain further insight
into whether or not the features selected are unique to cancer or if the distinguishing
features are unique only to the domain of the cancer (breast, colon, and lung).
The application of the ranked list comparisons can also be applied to other
areas of research that involve multiple classes which contain the same features.
For example, a study which compares patient recovery times after various surgical
procedures among different types of hospitals would be an excellent area to apply
the technique described in this paper. This proposed study contains all of the
necessary aspects which makes the comparison technique appropriate. The classes
are the types of hospitals themselves, i.e., public, private, research. The features are
the surgical procedures and the data used for the purpose of rankings is the patient
recovery time. Possible results may show areas in which one or more of the hospitals
are in need of assistance. There are other areas where the comparison technique can
be applied: in a situation with three or more classes the comparison technique will
show how similar each class is to the other classes in terms of features.
References
1. W. Altidor, T. M. Khoshgoftaar, and J. Van Hulse. Classiﬁcation with feature selection from
noisy data: A comparative study of ﬁlters. Technical report, Florida Atlantic University May
2010.
2. C. Amos, W. Xu, and S. MR. Is there a genetic basis for lung cancer susceptibility. Recient
Results in Cancer Research, pages 3–12, 1999.
3. R. Battiti. Using mutual information for selecting features in supervised neural net learning.
IEEE Transactions On Neural Networks, pages 537–550, 1994.
4. L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classiﬁcation And Regression Trees.
Chapman and Hall, 1993.
5. X. Chen and M. Wasikowski. Fast: a roc-based feature selection metric for small samples and
imbalanced data classiﬁcation problems. In KDD ’08: Proc. 14th ACM SIGKDD Int’l Conf.
Knowldege Discovery and Data Mining, pages 124–132, New York, NY, 2008. ACM.
6. W. J. Conover. Practical Nonparametric Studies. John Wiley and Sons, 2nd edition, 1971.
7. U. M. Fayyad and K. B. Irani. On the handling of continuous-valued attributes in decision tree
generation. Machine Learning, 8:87–102, 1992.

710
D.J. Dittman et al.
8. G. Forman. An extensive empirical study of feature selection metrics for text classiﬁcation.
J. Mach. Learn. Res., pages 3:1289–1305, 2003.
9. M. A. Hall and G. Holmes. Benchmarking attribute selection techniques for discrete class data
mining. IEEE Transactions on Knowledge and Data Engineering, 15(6):392–398, Novem-
ber/December 2003.
10. M. A. Hall and L. A. Smith. Feature selection for machine learning: Comparing a correlation-
based ﬁlter approach to the wrapper. In Proceedings of the Twelfth International Florida
Artiﬁcial Intelligence Research Society Conference, pages 235–239, May 1999.
11. S. Haykin. Neural Networks: A Comprehensive Foundation 2nd edition. Prentice Hall, 1998.
12. Y. Hong, S. Kwong, C. Y, and Q. Ren. Consensus unsupervised feature ranking from multiple
views. Pattern Recognition Letters, pages 595–602, 2008.
13. K. Kira and L. A. Rendell. The feature selection problem: Traditional methods and a new
solution. In AAAI ’92: Proc. 10th Nat’l Conf. on Artiﬁcial Intelligence, number 10, pages
129–134. John Wiley & Sons, Ltd., July 1992.
14. I. Kononenko. Estimating attributes: Analysis and extensions of relief. Lecture Notes in
Computer Science, pages 171–182, 1994.
15. S. Le Cessie and J. Van Houwelingen. Ridge estimators in logistic regression. Applied
Statistics, pages 191–201, 1992.
16. T.-Y. Liu. EasyEnsemble and feature selection for imbalance data sets. In Proceedings of
the 2009 International Joint Conference on Bioinformatics, Systems Biology and Intelligent
Computing, pages 517–520, Washington, DC, USA, 2009. IEEE Computer Society.
17. National Center for Biotechnology Information. Microarrays factsheet, 2007. http://www.ncbi.
nlm.nih.gov/About/primer/microarrays.htm.
18. J. Olsson and D. Oard. Combining feature selectors for textclassiﬁcation. In CIKM 06:
Proceedings of the 15th ACM international conference on Information and knowledge
management, pages 124–132, New York, NY, 2006. ACM.
19. G. Piatetsky-Shapiro, T. Khabaza, and S. Ramaswamy. Capturing best practice for microarray
gene expression data analysis. In KDD 03: Proceedings of the ninth ACM SIGKDD interna-
tional conference on Knowledge discovery and data mining, pages 407–415, New York, NY,
December 2009. ACM.
20. J. R. Quinlan. C4.5: Programs For Machine Learning. Morgan Kaufmann, San Mateo,
California, 1993.
21. L. Rokach, B. Chizi, and M. O. Feature selection by combining multiple methods. Advances
in Web Intelligence and Data Mining, pages 295–304, 2006.
22. N. Seliya, T. M. Khoshgoftaar, and J. Van Hulse. A study on the relationships of classiﬁer
performance metrics. In Proceedings of the 21st IEEE International Conference on Tools with
Artiﬁcal Intelligence (ICTAI’09), pages 59–66, Newark, NJ, November 2009. IEEE Computer
Society.
23. J. Souza, N. Japkowicz, and S. Matwin. Stochfs: A framework for combining feature selection
outcomes through a stochastic process. In Knowledge Discovery in Databases: PKDD 2005,
volume 3721, pages 667–674. 2005.
24. J. Van Hulse, T. M. Khoshgoftaar, A. Napolitano, and R. Wald. Feature selection with high
dimentional imbalanced data. In Proceedings of the 9th IEEE International Conference on
Data Mining - Workshops (ICDM’09), pages 507–514, Miami, FL, December 2009. IEEE
Computer Society.
25. I. H. Witten and E. Frank. Data Mining: Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, 2nd edition, 2005.
26. Y. Yang and J. Pedersen. A comparative study on feature selection in text categorization.
In ICML 97: Proc. 14th Intl Conf. Machine Learning, pages 412–420. ICML, 2009.

Chapter 28
Application of Random Matrix Theory
to Analyze Biological Data
Feng Luo, Pradip K. Srimani, and Jizhong Zhou
1
Introduction
The development of high-throughput biological techniques, such as, gene
expression microarray [1, 2], mass spectrometry [3], single-nucleotide polymor-
phism (SNP) arrays [4], next generation sequencing [5], yeast two hybrid screening
[6], and synthetic genetic arrays [7] makes it possible to generate genotypic, tran-
scriptional, proteomic, and other measurements about cellular systems on a massive
scale. The application of these high-throughput techniques may revolutionize all
aspects of biological research. However, the massive data generated by high-
throughput techniques are typically noisy and high dimensional, presenting great
challenges to extract biological information from them. The high dimension of data
may cause certain methods (e.g., standard linear discriminates) to fail, incurs high
computation cost and complicates generalization of processing methods [8]. Thus,
reducing dimensionality of data and ﬁltering out noise or randomness from data are
required to analyze high-throughput biological data.
Random Matrix Theory (RMT) [9, 10], initially proposed by Wigner and
Dyson in the 1960s to study the spectrum of complex nuclei [11], is a powerful
mathematical tool for modeling and characterizing complex physical systems. RMT
has been successfully used to investigate the properties and behavior of diverse
complex systems, such as the spectra of large atoms [11], metal insulator transitions
in disorder systems [12], the spectra of quasi-periodic systems [13, 14], chaotic
systems [15], brain response [16], and the stock market [17–20]. The properties
 The research of Luo and Srimani was partially supported by NSF Grant DBI-0960586.
F. Luo • P.K. Srimani ()
School of Computing, Clemson University, Clemson SC, USA
e-mail: luofeng@cs.clemson.edu; srimani@cs.clemson.edu
J. Zhou
Institute for Environmental Genomics, University of Oklahoma, Norman OK, USA
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 28, © Springer Science+Business Media, LLC 2011
711

712
F. Luo et al.
of the matrices describing these real systems are compared with the predictions of
RMT; the set of properties closely resembling the predictions by RMT indicate the
randomness inside the systems, and properties deviating from the predictions of
RMT indicate system-speciﬁc information and signals.
Recent studies [21–25] indicate that the RMT techniques can be used to develop
objective mathematical criteria to identify system-speciﬁc signiﬁcant relations from
the correlation matrix of microarray proﬁles, and then to construct robust gene co-
expression networks. It has also been shown [26] that system-speciﬁc projection
of microarray data can help improve the classiﬁcation of cancer patients. These
ﬁndings suggest that RMT can be an alternative powerful tool to analyze high-
throughput biological data. The RMT-based approaches appear to be promising to
develop theoretical foundation for objective and automatic selection of signiﬁcant
signals from data.
In addition, high throughput biological data, especially biological networks,
opened up great opportunities for systems-level understanding and interpretation of
cellular systems [27]. For example, topological properties of biological networks
generated by high-throughput techniques will help to understand the properties
of biological systems, such as modularity, hierarchy, and asymptotic power law
distribution of degree [27]. As a tool for modeling complex system, the Random
matrix theory, an already proven tool for modeling many complex systems, is
poised to be extremely useful to investigate biological networks and consequently
to biological systems research in general.
In this chapter, we ﬁrst introduce the universal predictions of random matrix
theory. Then, we describe the applications of random matrix theory to construct gene
co-expression networks, analyze biological networks and system-speciﬁc projection
of high dimensional biological data.
2
Random Matrix Theory
RMT makes universal predictions for real and symmetrical random matrices based
on the properties of their eigenvalues and eigenvectors. The properties of a matrix
that describes these real systems closely following the predictions of RMT indicate
the random and noise inside the system. Similarly, properties deviating from the
prediction of RMT indicate system-speciﬁc information and signals. We discuss
some of the relevant issues in this section; interested reader may ﬁnd detailed
treatments in [9].
2.1
Universal Predictions of RMT
We consider several aspects of universal predictions:

28
Application of Random Matrix Theory to Analyze Biological Data
713
(a) Eigenvalue distribution. A random correlation matrix C, which is computed
from an N  L data matrix D, where N is the number of elements and L is the
number of measures, is expressed as follows:
C D 1
LDDT
(28.1)
In the case of limit N! 1 and L! 1 and Q D L=N .>1/ is ﬁxed, the
distribution of the eigenvalues of C is given by [28]
./ D
Q
22
p
.max  /.  min/

(28.2)
where the eigenvalue  is bounded as min    max, and min and max
are the minimum and maximum eigenvalues of C and 2 is the variance of the
elements in D, as follows:
max = min D 2
 
1 C 1
Q ˙ 2
s
1
Q
!
(28.3)
Plerou et al. [18] compared the eigenvalue distribution of the correlation matrix
of stocks with the theoretical prediction (Eq. 28.2), and determined that the bulk
of the eigenvalues closely followed the prediction. However, a few large and
small eigenvalues disagree with the prediction, which are signals that indicate
the real “information” in the stock correlation matrix.
(b) Distribution of Eigenvector Components. For the random correlation matrix
C, the distribution of its eigenvectors’ components uk is expected to follow a
Gaussian distribution with a mean of zero and unit variance [10]:
.u/ D
1
p
2
exp
u2
2

(28.4)
Results from [18] and [17], dealing with correlation matrices of stocks show
that the majority of the component distributions of eigenvectors corresponding
to eigenvalues in the bulk predicted by RMT closely agreed with Eq. 28.4. How-
ever, the component distributions of eigenvectors that correspond to eigenvalues
outside of the RMT predication deviated from Eq. 28.4. This conﬁrms that
the large and small eigenvalues of the correlation matrix of stock are system-
speciﬁc signals and that their corresponding eigenvectors contain some system
speciﬁc information.
(c) Inverse Participation Ratios. Another property used to characterize the eigen-
vectors of a random matrix are the inverse participation ratios Ik [19],

714
F. Luo et al.
Ik D
N
X
lD1
Œukl4
(28.5)
where ukl are components of eigenvector k. For random matrices, we see
Ik  1=N. The high Ik value indicates a strong localized participant in the
eigenvector. The deviation of the Ik from theoretical prediction of random
matrix shows the local collective signals [19].
(d) Classiﬁcation of Ensembles. RMT characterizes the real and symmetrical
random matrices into two universal ensembles: Gaussian orthogonal ensemble
(GOE) and Poisson ensemble, using the correlations between eigenvalues.
One of the eigenvalue correlation measures is the nearest neighbor spacing
distribution (NNSD), P.s/, where s is the random variable denoting the spacing.
The strong correlation between the eigenvalues leads to statistics as described
by GOE; on the other hand, if there is no correlation between eigenvalues, the
eigenvalue spacing distribution follows Poisson statistics. From RMT, the P.s/
of the GOE statistics closely follows the Wigner–Dyson distribution:
PGOE.s/  0:5s exp

s2
4

(28.6)
P.s/ for the Poisson distribution is
PPoisson.s/ D exp.s/
(28.7)
The difference between the Wigner–Dyson and Poisson distributions is evident in
the regime of small s, where, PGOE.s ! 0/ D 0 and PPoisson.s ! 0/ D 1.
The GOE indicates a noisy system. Deviations from universal GOE predictions
can be used to distinguish the system-speciﬁc, nonrandom properties of complex
systems from noise [19]. On the other hand, the Poisson ensemble of the RMT
indicates a collective system, in which elements can be separated into several
relatively independent clusters, whereby elements in the same cluster will have very
similar behaviors and properties [29,30].
The eigenvalue correlation properties are the local properties of the random
matrix and are only determined by the over-all symmetries of the system [9].
On the other hand, the eigenvalue distribution and the component distributions of
the eigenvectors are global properties of the random matrix. The global and local
properties usually appear unrelated [9]. Numerical experiments showed that the
local properties of eigenvalues of a real symmetric matrix become independent
of the probability distribution of the matrix elements and the global properties of
matrix when N ! 1 [9]. The local properties, like NNSD, only depend on the
overall symmetries of the system, like real symmetry, or Hermitian.

28
Application of Random Matrix Theory to Analyze Biological Data
715
2.2
Eigenvalue Unfolding
In general, the density of eigenvalues of a matrix is not constant, which makes it
difﬁcult to separate global variation of the eigenvalue density from the under uni-
versal properties [13, 14]. In order to uncover the universal eigenvalue correlations
of different matrices, RMT requires spectral unfolding to have constant density
of eigenvalues. The purpose of unfolding is to transform a spectrum with non-
constant density of the eigenvalue to another one with uniform distribution. The
unfolding procedure can be achieved using different methods [31,32]. Zhong et al.
[13, 14] proposed that the integrated density of the eigenvalue was quite smooth
and could be used to perform spectral unfolding. In order to achieve the unfolding,
eigenvalues yielded by numerical calculation are ﬁrst degenerated and ordered.
Then, the integrated density function D./ over a ﬁnite interval L is calculated as
following:
D./ D 1
N
N
X
iD1
1i <
(28.8)
where i is the ith eigenvalue and N is the total number of eigenvalues. The L is
the unfolding pace. Dividing by N will scaling the D./ to the interval [0,1]. Then,
D./ will be ﬁtted with a cubic spline to obtain the smoothened integrated density
of eigenvalue, Nav.i/. And, the unfolded eigenvalues are calculated as
ei D Nav .i/
(28.9)
The procedure of the unfolding method is summarized in Algorithm 3.
Algorithm 3: (Eigenvalue unfolding)
Input: Correlation matrix of biological data.
Parameters: Unfolding pace L
Output: Unfolded eigenvalues
Step 1. Calculate the eigenvalues i .1 < i < N / of the matrix of order N .
Step 2. Compute the integrated density function D./.
Step 3. Fit a smooth cubic spline Nav.i/ through D./.
Step 4. Compute the unfolded eigenvalues (Eq. 28.14).
The nearest neighbor level spacing can then be obtained as Si D eiC1  ei and the
distribution of the nearest neighbor level spacing is obtained by computing the probability
density for a given spacing s.
3
Construction of Gene Co-Expression Networks
from Microarray Data using RMT
The microarray technology [1, 2], which allows for the simultaneous measurement
of genome wide expressions, has opened up opportunities for the systems-level
understanding and elucidation of cellular systems. Because microarray data is

716
F. Luo et al.
typically noisy, highly dimensional and signiﬁcantly under sampled, automatic
extraction of meaningful and conﬁdent biological hypothesis from microarray
proﬁles is an important task. It is shown that the random matrix theory (RMT) can
provide an objective mathematical criterion to identify system-speciﬁc signiﬁcant
relationships. One can then get the gene co-expression networks from microarray
proﬁles.
3.1
Sharp Transitions from GOE to Poisson Ensemble
in Correlation Matrices of Microarray Proﬁles
It has been suggested that the functional modules in a cellular system are a “critical
level of biological organization” [33]. An important characteristic of a modular
system is collectivity, i.e., the similarities of the behavior or properties between
elements in the same module are signiﬁcantly higher than the similarities between
elements from different modules. Furthermore, it has been well recognized that
only a portion of gene modules change their expressions under certain conditions.
Expressions of genes inside the same module of these active modules possess
higher similarities than those of genes from different modules. On the other hand,
expressions of genes that do not respond to conditions just ﬂuctuate at the balance
level. The correlations between gene expressions inside the same “active” modules
are higher than those of genes from different modules and those of genes from
“inactive” modules. Thus, the correlation matrix M of the microarray expressions
is a mixture of two parts: (1) high correlation part Mc, indicating the correla-
tions between gene expressions inside modules speciﬁcally related to activities in
biological systems, and (2) the weak correlation part Mr, that indicates random
correlations between gene expressions. Thus, M D Mr C Mc. We hypothesize
that the two universal predictions of RMT are applicable to biological systems.
The NNSD of M will follow GOE and the NNSD of Mc will follow Poisson
statistics. The transition of NNSD between GOE and Poisson statistics can serve
as a reference point to distinguish system-speciﬁc, nonrandom, collective properties
embedded in the correlation matrix of microarray data from random. This reference
point is mathematically deﬁned and can be used as a threshold to identify gene co-
expression networks in an automatic and objective fashion [21,22].
To determine the existence of a transition from GOE to Poisson statistics in the
correlation matrix of microarray proﬁle, we applied RMT to the yeast cell cycling
microarray data [34]. This microarray had 5,293 genes with 70 time points available.
Missing values were estimated using the K nearest neighbor based method [35].
We ﬁrst applied RMT to study the gene expression correlation matrix M, whose
elements were pair-wise Pearson correlation coefﬁcients (r) (in the range of [1:0,
1.0]) of two gene expressions. A series of correlation matrices were constructed
by gradually removing small correlations in the original correlation matrix, from
which NNSDs were obtained A clear, sharp transition of NNSD from GOE to

28
Application of Random Matrix Theory to Analyze Biological Data
717
Fig. 28.1 The normalized
NNSDs of correlation
matrices of yeast cell cycle
gene expressions at different
cutoff values. They were
plotted against the curves of
Wigner surmise (green line)
and Poisson distribution (red
dot line). The x-axis is the
level spacing s and the y-axis
is the probability of NNSDs
Poisson distribution was observed from these correlation matrices (Fig. 28.1) [21].
Based on a ¦2 test .p D 0:001/, NNSD completely transformed into Poisson
distribution at the correlation cutoff rt D 0:77. Similar sharp transitions from
GOE to Poisson were also observed in the matrices whose entries represent mutual
information between the expressions of two genes. Thus, the correlation cutoff at
which NNSD is completely transformed into Poisson distribution at the signiﬁcance
level of p D 0:001 is determined as the threshold to deﬁne system-speciﬁc signif-
icant gene relations (correlations). All relations beyond this threshold are deemed
signiﬁcant.
To determine the presence of transitions in gene expression data from other
organisms, the RMT method was used to analyze microarray data from a variety
of organisms, e.g., Shewanella oneidensis, Escherichia coli, Arabidopsis thaliana,
Drosophila, mouse, and human [21]. Clear transitions of NNSD from GOE to
Poisson distribution were observed for all microarray proﬁles.

718
F. Luo et al.
3.2
Constructing Gene Co-expression Networks from
Microarray Proﬁles by RMT
A gene co-expression network is a network in which each gene is a node. A link is
formed between two genes if the correlation between their expressions is greater
than the threshold. Based on our studies on the NNSD of correlation matrices
of microarray proﬁles, we proposed RMT-based (Algorithm 4) to construct gene
co-expression network. This algorithm has been successfully applied to several
microarray studies [21,23–25].
Algorithm 4: (The RMT-based algorithm to construct gene co-expression
network)
Input: Gene microarray data e1; : : :; en 2 <d.
Parameters: Number of missing value allowed.
Output: Gene co-expression network.
Step 1. Estimate the missing values in expression ﬁle if there are missing values in the
expression ﬁle.
Step 2. Construct the correlation matrix C from microarray data.
Step 3. Determine the threshold th of signiﬁcant correlations using RMT.
1. Set the cutoff D maximum of Ci; j  0:01.
2. Construct a cut correlation matrix C 0 by set the values, whose absolute value in original C
is less than the cutoff, to 0.
3. Compute the eigenvalues .1; : : :; n/ of C 0.
4. Compute the unfolded eigenvalues .ui; : : :; un/ to correct the ﬂuctuations in the density of
the eigenvalue.
5. Compute the distribution P.s/ of nearest neighbor spacing s, where si D uiC1  ui and ui
are in ascendant order.
6. Perform Chi-square test of P.s/ against Poisson distribution. If the P.s/ follows the
Poisson distribution at a conﬁdence level of p D 0:001, stop and the cutoff is the threshold
th for constructing gene co-expression network. Else, cutoff D cutoff 0:01 and go to (2).
Step 4. Construct gene co-expression network. From the correlation matrix, we construct a
co-expression network, in which each gene is a node and there is a link between two genes if
the absolute correlation measure between their expressions is greater than the threshold th.
For the yeast cell cycling dataset, we construct a gene co-expression network at
the cutoff value rt D 0:77, where NNSD is completely transformed into Poisson
distribution. Figure 28.2 shows this co-expression network. We annotated each
gene according to the gene annotation from the Saccharomyces Genome Database
(SGD) [36] and the Munich Information Center for Protein Sequences (MIPS)
[37]. Remarkably, a functionally coherent set of genes were found to be connected
together [21], which is consistent with the guilt-by-association principle [38].
The properties of obtained co-expression network, e.g., scale-free, hierarchical,
modular and small world, are consistent with general network theory [21]. For
example, the average clustering coefﬁcient of this network is 0.323, which implies

28
Application of Random Matrix Theory to Analyze Biological Data
719
Fig. 28.2 The gene co-expression network of the yeast cell cycling data was obtained at cutoff
0.77. Sub-networks with more than four genes are shown. A total of 804 genes are shown. For
sub-networks with three genes, only those forming statistically signiﬁcant cycles [40] are shown.
Each node represents a gene, and the width of the line represents the correlation coefﬁcient of the
two linked genes. The blue and gray lines indicate the respective positive and negative correlation
coefﬁcients. The colors were assigned to nodes according to their functional categories [21]. The
text in the map indicates the major functional category of each sub-network and the dashed circles
separate sub-networks into smaller sub-modules
a high degree of modularity. The average clustering coefﬁcient .C.k// of all genes
with k links follows the scaling law: C.k/  k0:37. This signiﬁes a hierarchical
modularity although the scaling exponent of 0.37 differs from the values obtained
from metabolic modular networks. The analysis of this network’s connectivity
distribution shows a power-law distribution with a degree exponent of 1.5, which
is analogous with previous results on microarray expression proﬁles [39].
Because microarray data contain a high inherent variability, we explored if the
gene co-expression networks remain stable upon the addition of more noise. We
added increasing levels of Gaussian noise to the yeast cell cycling microarray
proﬁles. The mean of noise is 0 with standard deviation .Noise/ is set to 1%, 2%, 5%,
10%, 20%, 30%, 40%, and 50% of the average of absolute expression value of whole
dataset. Then, we determined the new correlation thresholds for the perturbed data
(Fig. 28.3c) and constructed the corresponding gene co-expression networks. When
30% more noise was added, 79.4% of the original links and 86.5% of the original
genes were still preserved (Fig. 28.3a and b), indicating that the RMT approach is
robust in tolerating additional noise level [21].

720
F. Luo et al.
Fig. 28.3 The robustness test of the RMT approach. (a) Preserved links over total links in the
networks perturbed (red), and over total links in original networks (green) at different levels of
noise. (b) Preserved genes over total genes in the networks perturbed (red), and over total genes
in original networks (green) at different levels of noise. (c) Increased noise decreases the cutoff
threshold
3.3
Comparison of RMT-based Method with Statistical Testing
Against Randomized Expressions
To further evaluate the effectiveness of threshold determination by the RMT-based
method, we compared it to a widely used method that determines signiﬁcant
relations by statistically comparing it to the column randomized expression proﬁles
[41]. Both mutual information and Pearson correlations are used to measure the
relationships between expressions. Two yeast microarray data sets are used for
comparison: the yeast cell cycle [34] and yeast environmental stress responses
[42]. Table 28.1 lists the obtained thresholds and the number of genes and links
in the corresponding co-expression networks. We used the Gene Ontology Slim
category from the Saccharomyces Genome Database (SGD) database [36, 43] to
classify the links. A link connecting two genes in the same Gene Ontology Slim
category is assumed to be “true.” From Table 28.1, all of the co-expression networks
obtained by the RMT method have more than 50% “true” links; whereas for the
randomized method, only the co-expression network built from the yeast cell cycle
have more than 50% “true” links. However, the co-expression network built from
the yeast environmental stress responses have a very low percentage of “true”
links. The RMT-based method identiﬁed the proper network from both data sets,
while the randomization based method failed to identify the proper network from
the yeast environmental stress responses data. Though more systematic studies
are needed, this preliminary comparison proves that statistically comparison to
randomized data is not always successful. Moreover, it proves that the RMT method
is advantageous over randomization-based method in identifying system-specify
information embedded inside microarray proﬁles.

28
Application of Random Matrix Theory to Analyze Biological Data
721
Table 28.1 Comparison of thresholds and co-expression networks obtained by RMT and statisti-
cal testing against randomized expressions on two yeast microarray expression proﬁles
Microarray
Yeast cell cycle [34]
Yeast environmental stress responses [42]
data
RMT
Randomization
RMT
Randomization
Measure
MI
Pearson
MI
Pearson
MI
Pearson
MI
Pearson
rt
1.17
0.77
1.25
0.688
1.361 0.91
0.67
0.534
Number of genes
861
966
342
2,398
402
523
3,883
5,266
Number of links
1764
3,346
591
15,853
4,991 7,160
625,161 1585,805
“True” links (%)
57.88 64.94
64.13 52.27
77.28 75.53
32.30
29.86
4
Analysis of Biological Networks
The cell is a complex system that contains numerous functionally diverse elements,
including protein, DNA, RNA, and small molecules. Understanding the funda-
mental principles and behavioral properties of the cell as a system has become a
key research activity in the post-genomic era. Complex biological systems, such
as cells, can be modeled at highly level abstraction as networks [44], in which
each node represents an element in biological systems and each link represents the
relationship between two elements. Studying the properties of biological networks
enables to understanding the characteristics of biological systems. High-throughput
biological technologies now are able to generate large size biological networks, such
as protein interaction networks (PINs) [45, 46], metabolic networks [47, 48], gene
co-expression networks [22, 49, 50]. The biological networks can be unweighted
and undirected, like PINs and GINs; or weighted and undirected, like co-expression
networks; or directed network, like regulatory networks. The biological networks
(either weighted or unweighted) are usually represented by their adjacency matrix,
distance matrix, and the Laplacian Matrix [51,52].
Research on the topological properties of large scale biological networks of cell
constituents has shown that biological networks share some fundamental topological
properties, including scale-free, small-world, hierarchical, modular [27], and self-
similar [53] properties, with other complex systems, such as the Internet and
social networks. Unfortunately, the huge number of constituents and their complex
relationships in the cell make the mathematical modeling of large-scale biological
systems challenging. It is of signiﬁcant importance to understand the nature of
the structure and interactions of biological networks for achieving quantitative
description of their functions. Here, we use RMT to analyze the properties of
biological networks.
4.1
Understanding the Properties of Biological Networks
Using RMT
Random matrix theory has been used to study two biological networks of yeast
[54]. The ﬁrst network is the core protein interaction network of yeast obtained

722
F. Luo et al.
from the DIP [54] database (version ScereCR20041003)generated from the ﬁltering
of large high-throughput protein interaction data using two different computational
methods [55]. After removal of all self-connecting links, the ﬁnal protein interaction
network includes 2,609 yeast proteins and 6,355 interactions. The second network
is the yeast metabolic network constructed by Jeong et al. [56] from the data in the
WIT database [57]. After removal of redundant links, the ﬁnal metabolic network
has 1,511 chemical substrate and intermediate states and 3,807 interactions. The
original metabolic network is a directed network. To make the metabolism network
symmetric for RMT study, we changed the directed network to an undirected
network by replacing all directed links in the network with undirected links.
Two biological networks are represented by two real symmetric matrices. The
dimension of each matrix is the number of constituents in the network. The
elements in the matrices are set to 1 if there is a direct interaction between the
constituents; otherwise, the elements are set to 0. We calculated the NNSD of these
two matrices for RMT analysis by direct diagonalization of the matrix. Figure 28.4
shows the NNSDs of these two networks. One can see that the NNSDs of the
protein interaction network are well described by the Wigner–Dyson distribution.
The NNSDs of the metabolism network are also very close to the Wigner–Dyson
distribution, especially in the region representing small values of s. The slight
deviation may be due to the incomplete nature of the deﬁned network.
Farkas et al. [58] studied a transcription regulatory network in yeast using RMT.
They constructed the transcription regulatory network from microarray data of 287
single gene deletion yeast mutant strains [59]. For each pair of genes, there is a
link connecting them if the correlation between 30 expressions of 287 beyond a pre-
deﬁned threshold. To elucidate the structure of this transcription regulatory network,
they compared the inverse participation ratio (IPR) of the eigenvectors of its adjacent
matrix with the IPRs of different network models, such as random graph [60], small-
world network [61], and scale free network [62]. They showed that the IPR of the
transcription regulatory network in yeast is similar to that of scale free network.
4.2
Understanding the Organization of Biological
Network Using RMT
Biological networks have modular structures with more interactions between
elements inside the same module and fewer interactions between different modules
[16–18]. Girvan and Newman [19] proposed that the deletion of edges with high
betweenness can separate the network, while keeping the modular structure in the
network intact. To test the modularity of these two networks, we gradually removed
the interaction links between the constituents in the two yeast networks using
the Girvan–Newman algorithm [19] and calculated the NNSDs of the remaining
networks. A transition of NNSD from a Wigner–Dyson distribution to a Poisson
distribution was clearly observed in both cases (Fig. 28.4). Meanwhile, both yeast

28
Application of Random Matrix Theory to Analyze Biological Data
723
Fig. 28.4 The NNSDs transitions indicated the structural change of yeast biological networks.
(a) The original DIP yeast core PIN [54]. (b) The DIP yeast core PIN with 1,230 links removed.
(c) DIP yeast core PIN with different number of removed links (m): 0 (navy), 800 (blue), 1,200
(cyan), and 1,230 (red). (d) The original yeast metabolic network [56]. (e) The yeast metabolic
network with 770 links removed. (f) Metabolic networks with different number of removed links
(m): 0 (navy), 400 (blue), 700 (cyan), and 770 (red). The networks in (a, b, d, e) were produced
using Biolayout [63]. Smooth and dashed black lines in (c) and (f) are the GOE distribution and
the Poisson distribution, respectively
networks were decomposed into modules (Fig. 28.4b and e). We used the chi-
squared test to determine the transition point. For the DIP yeast core PIN [54],
Chi-squared testing showed that NNSD follows a Poisson distribution after removal
of 1,230 links (Fig. 28.4b). The remaining protein-protein interaction network

724
F. Luo et al.
Fig. 28.5 Chi-Square test
values of NNSDs against
Poisson distribution are
plotted against percentage of
links rewired. The red line
indicates the critical value of
Chi-Square test at p D 0:001
contains 107 modules with sizes ranging from 2 to 778 proteins. For the yeast
metabolic network [56], NNSD follows a Poisson distribution after removal of 770
links and the remaining network has 17 modules with sizes ranging from 4 to 602
chemical substrates and proteins (Fig. 28.4e).
Based on the predictions of RMT, both the randomness of network connections,
indicated by GOE, and the systems-speciﬁc modular organization, indicated by
Poisson distribution, exist in biological networks. This kind of structure may
be a universal nature of biological networks [64–66]. Results demonstrate that
the structure and behavior of biological networks may be studied well using
RMT.
4.3
Understanding the Structural Change of Co-expression
Network using RMT
To determine the structural sensitivity of gene co-expression networks, we randomly
rewired a small percentage of the links in the gene co-expression network from
the yeast cell cycling data [34] using the Maslov–Sneppen procedure [67]. Next,
we constructed the corresponding correlation matrices, from which NNSDs were
obtained. A random rewiring of as low as 0.4% of the links caused the NNSDs to
deviate from Poisson distribution (Fig. 28.5) [21]. Therefore, our RMT approach
is able to detect even the minute topological changes in gene co-expression
networks.

28
Application of Random Matrix Theory to Analyze Biological Data
725
5
System-Speciﬁc Feature Selection from Biological
Data Using RMT
5.1
System-Speciﬁc Signals in Biological Data
Due to the prohibitive cost of biological experiments and samples, many HtB data
do not satisfy the condition: L=N > 1, where N is the number of elements and L is
the number of measures; thus, the global universal properties of correlation matrices
of HtB data do not adequately agree with the predictions of random correlation
matrices by RMT.
Our experimental studies (Fig. 28.6) show the eigenvalue distribution of correla-
tion matrix of yeast cell cycle microarray data [34] and corresponding randomized
expressions – although the bulk eigenvalues of the correlation matrix of original mi-
croarray proﬁles are less than the eigenvalues of randomized expressions, the largest
Fig. 28.6 A comparison of distribution of degenerated eigenvalues of the yeast cell cycle
microarray data’s correlation matrix with that of randomized expressions. Inside: the complete
eigenvalue distribution of the correlation matrix of the yeast cell cycle data

726
F. Luo et al.
Fig. 28.7 The IPR of the eigenvectors of the correlation matrices of the yeast cell cycle microarray
data (black dot) and randomized expressions (green dot). The red line indicates the minimal IPR
of randomized case
eigenvalue of the original microarray proﬁle correlation matrix is approximately
nine times greater than the randomly expressed eigenvalues – similar analyses are
performed on stock data [18].
Furthermore, the IPR values of eigenvectors can also indicate information about
the systems [18]. We compared the IPR values of eigenvectors of correlation matrix
of the yeast cell cycle microarray data with those of randomized expressions. As
shown in Fig. 28.7, we note:
•
The IPR of the eigenvectors corresponding to the largest eigenvalue are less
than the IPR of the randomized cases. This agrees with the above component
distribution analysis that the largest eigenvalue and its corresponding eigenvector
represent the effect of certain common factor.
•
The low values of IPR of eigenvectors corresponding to the second, third, and
fourth largest eigenvalue indicate that there is more than one common factor.
•
The eigenvectors corresponding to small eigenvalues have large values of
IPR. This indicates that the eigenvectors of small eigenvalues actually contain
localized information, which is usually discarded by PCA based approaches.
Those results implied that comparing the properties of the eigenvalues and eigen-
vectors of correlation matrices of high-throughput biological data with the universal
predictions of RMT (if L=N > 1) and with those of eigenvalues of correlation
matrices of randomized data (if L=N < 1) is expected to reveal these system-speciﬁc
signals. Therefore, projections on system-speciﬁc signals will help improve further
analyses of biological data.

28
Application of Random Matrix Theory to Analyze Biological Data
727
5.2
Classiﬁcations of Cancer Patients Using System-Speciﬁc
Signals of High-Throughput Biological Data
Patient classiﬁcation is one important medical application of high-throughput
biological data. We conducted a preliminary study on cancer patient classiﬁcation
using microarray and mass spec data. We developed a RMT-based method to project
microarray data and mass spec data into system-speciﬁc dimensions. We used the
IPR of eigenvectors to select the system-speciﬁc dimensions. The detail of the
method is summarized in Algorithm 5.
Algorithm 5: (System-speciﬁc project of biological data)
Input: the biological data matrix E.m  n/.
Output: The system-speciﬁc projected data matrix Eproject.
Step 1. Decompose the data matrix E using SVD: E D UƒVT, where U is an m  m
unitary eigenvector matrix, V is an n  n unitary matrix and ƒ is a m  n diagonal matrix
with the nonnegative singular values as the diagonal entries.
Step 2. Normalize the data matrix to zero mean and standard deviation one and calculate the
correlation matrix C of normalized matrix. Calculate the eigenvalues i.1 < i < m/ and
eigenvectors U of C and calculate the IPRs of U .
Step 3. Column randomized the data matrix E to obtain a new matrix E0. Normalize E0 to
zero mean and standard deviation one and calculate the correlation matrix C 0 of normalized
matrix. Calculate the eigenvalues 0i.1 < i < m/ and eigenvectors U 0 of C and calculate
the IPR’s of U 0.
Step 4. Identify the signal eigenvalues if their corresponding eigenvector’s IPR greater than
the maximum value of IPR’s.
Step 5. Obtain the corresponding signal singular values for the original HtB data matrix. The
singular values ƒdata from the original data matrix are related to the eigenvalues ƒcov of its
covariance matrix: ƒcov D ƒdata  ƒdata
T, when the original data is centralized with zero
means. [Note that after normalizing the original data matrix, the covariance matrix of the
data matrix will be identical to the correlation matrix.]
Step 6. Project the data matrix E on signal dimensions. First, we will construct an m  k
signal eigenvector matrix Usignal by only keep the eigenvectors corresponding to signal
eigenvalues in original eigenvector U . Then, a dimension reduced .m  k/ projected data
matrix Eproject can be constructed by project original data matrix to signal eigenvector space:
Eproject D Usignal
TE.
We demonstrated our method on the classiﬁcation of cancer patients using two
microarray datasets. The ﬁrst microarray data is a proﬁling of 40 tumor and 22
normal colon tissues [68]. The second microarray data is a proﬁling of 50 normal
and 52 cancer prostate tissues. Figure 28.8 showed the comparison of the inverse
participation ratios Ik of the eigenvectors corresponding to nonzero eigenvalues of
the original microarray proﬁles’ correlation matrix (black squares) and of column
randomized data’s correlation matrix (green diamonds). We compared our method
with PCA-based method and non-projection. For the PCA based preprocess, we
selected the large eigenvalues explaining 90% variance of the data. We then
applied support vector machine (SVM), a classiﬁcation method, on the original and

728
F. Luo et al.
Fig. 28.8 The IPR of the eigenvectors of the correlation matrices of the two cancer microarray
data (black square) and randomized expressions (green diamonds): (a) Colon cancer microarray.
(b) Prostate cancer microarray
projected data. Table 28.2 shows the performances of SVMs on RMT preprocessed
data to be superior to the performances of SVMs on original and PCA preprocessed
data. This results clearly indicate that RMT based preprocess is better than the PCA
based preprocess for the classiﬁcation of patients using high-throughput biological
data. System-speciﬁc projection of biological data may help to improve the patient
classiﬁcation performance.

28
Application of Random Matrix Theory to Analyze Biological Data
729
Table 28.2 Comparison of performances of SVMs on original, PCA preprocessed and RMT
preprocessed data for the cancer patients classiﬁcation based on microarray data. The leave-one-out
method is used to validate the classiﬁcation
Data
Original
PCA based
RMT based
Colon cancer microarray [68]
0.9032
0.9032
0.9677
Prostate cancer microarray [69]
0.9608
0.9706
0.9902
6
Conclusions
This chapter describes the application of random matrix theory to analyze different
biological data. RMT has been traditionally used in characterizing the non-random
phenomena in physical, material and social systems, including heavy nuclei,
metal insulator transitions and the stock markets. It has been well recognized in
these systems that RMT analyses are efﬁcient for distinguishing system-speciﬁc,
nonrandom properties from random noise. Recent investigations show that RMT
tools and methodologies have potential in analyzing biological systems as well
which are very different from other physical systems.
It is to be noted that in some cases, there is no transition of NNSD observed from
the correlation matrix of microarray data, which indicates that there is no collective
structure in the system. Thus, the RMT approach could not be able to generate a
meaningful threshold. The examples include occasions that the experiment points
are very limited and similarities between expressions of all genes are high; or very
messy microarrays that have been poorly carried out.
As a matter of fact, the correlation matrices of some high-throughput biological
data, such as microarray data, are not general matrices with random elements,
or even normal correlation matrices in statistic due to the number of microarray
experiments is much less than the number of genes analyzed. Hence, current
predictions of RMT about the global properties, such as eigenvalue distribution,
may be invalid for these correlation matrices from microarray data. For example,
the eigenvalue distribution of correlation matrix of yeast cell cycle microarray data
follows Cauchy distribution [21]. However, the overall symmetry of the correlation
matrices is still a real symmetry. The independent character of local properties from
RMT makes it possible to compare the NNSD of correlation matrices with the
theoretical predictions of RMT.
On the other hand, there are structures/properties related to the biological infor-
mation in global properties of the eigenvalues and eigenvectors of the correlation
matrices. We compared the global properties of the eigenvalues and eigenvectors of
the correlation matrices of high-throughput biological data with those of correlation
matrices created from randomized expressions to reveal system speciﬁc signals of a
particular biological system embedded in high-throughput biological data.
The RMT-based approach has shown promise to provide a reliable, sensitive,
and robust method to identify system-speciﬁc signals from the high-throughput
biological data. The automatic and objective nature of RMT-based approach makes

730
F. Luo et al.
it more advantageous in studying little understood biological systems. Further ex-
ploration of this method should provide powerful tools to analyze high dimensional
biological data.
References
1. Lockhart, D.J., et al., Expression monitoring by hybridization to high-density oligonucleotide
arrays. Nat Biotechnol, 1996. 14(13): p. 1675–80.
2. Schena, M., et al., Quantitative monitoring of gene expression patterns with a complementary
DNA microarray. Science, 1995. 270(5235): p. 467–70.
3. Flory, M.R., et al., Advances in quantitative proteomics using stable isotope tags. Trends
Biotechnol, 2002. 20(12 Suppl): p. S23–9.
4. Zhou, X. and D.T.W. Wong, Single Nucleotide Polymorphism Mapping Array Assay. 2007.
p. 295–314.
5. Shendure, J. and H. Ji, Next-generation DNA sequencing. Nat Biotech, 2008. 26(10):
p. 1135–1145.
6. Joung, J.K., E.I. Ramm, and C.O. Pabo, A bacterial two-hybrid selection system for studying
protein–DNA and protein–protein interactions. Proc Natl Acad Sci U S A, 2000. 97(13):
p. 7382.
7. Tong, A.H.Y. and C. Boone, Synthetic genetic array analysis in Saccharomyces cerevisiae.
METHODS IN MOLECULAR BIOLOGY-CLIFTON THEN TOTOWA-, 2005. 313: p. 171.
8. Hilario, M. and A. Kalousis, Approaches to dimensionality reduction in proteomic biomarker
studies. Brief Bioinform, 2008. 9(2): p. 102–118.
9. Mehta, M., Random Matrices, 3nd edition. Academic Press, 2004.
10. Guhr, T., A. Muller-Groeling, and H.A. Weidenmuller, Random-matrix theories in quantum
physics: common concepts. Physics Reports, 1998. 299(4–6): p. 189–425.
11. Wigner, E., Random Matrices in Physics. SIAM Review, 1967. 9: p. 1–23.
12. Hofstetter, E. and M. Schreiber, Statistical properties of the eigenvalue spectrum of the three-
dimensional Anderson Hamiltonian. Physical Review B, 1993. 48(23): p. 16979.
13. Zhong, J.X. and T. Geisel, Level ﬂuctuations in quantum systems with multifractal eigenstates.
Physical Review E, 1999. 59: p. 4071–4074.
14. Zhong, J.X., et al., Level-Spacing Distributions of Planar Quasiperiodic Tight-Binding Models.
Physical Review Letters, 1998. 80(18): p. 3996.
15. Bohigas, O., M.J. Giannoni, and C. Schmit, Characterization of Chaotic Quantum Spectra and
Universality of Level Fluctuation Laws. Physical Review Letters, 1984. 52(1): p. 1.
16. Seba, P., Random Matrix Analysis of Human EEG Data. Physical Review Letters, 2003. 91(19):
p. 198104.
17. Laloux, L., et al., Noise Dressing of Financial Correlation Matrices. Physical Review Letters,
1999. 83(7): p. 1467.
18. Plerou, V., et al., Random matrix approach to cross correlations in ﬁnancial data. Physical
Review E, 2002. 65(6): p. 066126.
19. Plerou, V., et al., Universal and Nonuniversal Properties of Cross Correlations in Financial
Time Series. Physical Review Letters, 1999. 83(7): p. 1471.
20. Kwapien, J., S. Drozdz, and P.O. Oswiecimka, The bulk of the stock market correlation matrix
is not pure noise. Physica A: Statistical Mechanics and its Applications, 2006. 359: p. 589–606.
21. Luo, F., et al., Constructing gene co-expression networks and predicting functions of unknown
genes by random matrix theory. BMC Bioinformatics, 2007. 8(1): p. 299.
22. Luo, F., et al., Application of random matrix theory to microarray data for discovering
functional gene modules. Phys Rev E Stat Nonlin Soft Matter Phys, 2006. 73(3 Pt 1): p. 031924.

28
Application of Random Matrix Theory to Analyze Biological Data
731
23. Yang, Y., et al., Characterization of the Shewanella oneidensis Fur gene: roles in iron and acid
tolerance response. BMC Genomics, 2008. 9 Suppl 1: p. S11.
24. Ficklin, S.P., F. Luo, and F.A. Feltus, The Association of Multiple Interacting Genes with
Speciﬁc Phenotypes in Rice Using Gene Coexpression Networks. Plant Physiol., 2010. 154(1):
p. 13–24.
25. Zhou, J., et al., Functional Molecular Ecological Networks. mBio, 2010. 1(4): p. e00169-10–
e00169-19.
26. Luo, F., et al., Application of random matrix theory to biological networks. Physics Letters A,
2006. 357(6): p. 420–423.
27. Barabasi, A.L. and Z.N. Oltvai, Network biology: understanding the cell’s functional organi-
zation. Nat Rev Genet, 2004. 5(2): p. 101–13.
28. Sengupta, A.M. and P.P. Mitra, Distributions of singular values for some random matrices.
Phys Rev E Stat Phys Plasmas Fluids Relat Interdiscip Topics, 1999. 60(3): p. 3389–92.
29. Drozdz, S., et al., Collectivity embedded in complex spectra of ﬁnite interacting Fermi systems:
Nuclear example. Physical Review E, 1998. 57(4): p. 4016.
30. Malevergne, Y. and D. Sornette, Collective origin of the coexistence of apparent random
matrix theory noise and of factors in large sample correlation matrices. Physica A: Statistical
Mechanics and its Applications, 2003. 331(3–4): p. 660–668.
31. Bruus, H. and J.-C. Angl‘es d’Auriac, Energy level statistics of the two-dimensional Hubbard
model at low ﬁlling. Physical Review B, 1997. 55(14): p. 9142.
32. Cowan, G. A survey of unfolding methods for particle physics. 2002.
33. Hartwell, L.H., et al., From molecular to modular cell biology. Nature, 1999. 402(6761 Suppl):
p. C47–52.
34. Spellman, P.T., et al., Comprehensive identiﬁcation of cell cycle-regulated genes of the
yeast Saccharomyces cerevisiae by microarray hybridization. Mol Biol Cell, 1998. 9(12):
p. 3273–97.
35. Troyanskaya, O., et al., Missing value estimation methods for DNA microarrays. Bioinformat-
ics, 2001. 17(6): p. 520–5.
36. Cherry, J.M., et al., SGD: Saccharomyces Genome Database. Nucleic Acids Res, 1998. 26(1):
p. 73–9.
37. Mewes, H.W., et al., MIPS: a database for protein sequences, homology data and yeast genome
information. Nucleic Acids Res, 1997. 25(1): p. 28–30.
38. Quackenbush, J., Genomics. Microarrays–guilt by association. Science, 2003. 302(5643):
p. 240–1.
39. Bhan, A., D.J. Galas, and T.G. Dewey, A duplication growth model of gene expression
networks. Bioinformatics, 2002. 18(11): p. 1486–93.
40. Shen-Orr, S.S., et al., Network motifs in the transcriptional regulation network of Escherichia
coli. Nat Genet, 2002. 31(1): p. 64–8.
41. Butte, A.J. and I.S. Kohane, Mutual information relevance networks: functional genomic
clustering using pairwise entropy measurements. Pac Symp Biocomput, 2000: p. 418–29.
42. Gasch, A.P., et al., Genomic expression programs in the response of yeast cells to environmental
changes. Mol Biol Cell, 2000. 11(12): p. 4241–57.
43. Hong EL, B.R. Christie, KR, Costanzo MC, Dwight SS, Engel SR, Fisk DG, Hirschman JE,
Livstone MS, Nash R, Park J, Oughtred R, Skrzypek M, Starr B, Theesfeld CL, Andrada
R, Binkley G, Dong Q, Lane C, Hitz B, Miyasato S, Schroeder M, Sethuraman A, Weng S,
Dolinski K, Botstein D, and Cherry JM., Saccharomyces Genome Database.
44. Barabasi, A.-L. and Z.N. Oltvai, Network biology: understanding the cell’s functional organi-
zation. Nature Reviews Genetics, 2004. 5(2): p. 101–113.
45. Ito, T., et al., A comprehensive two-hybrid analysis to explore the yeast protein interactome.
Proc Natl Acad Sci U S A, 2001. 98(8): p. 4569–74.
46. Uetz, P., et al., A comprehensive analysis of protein-protein interactions in Saccharomyces
cerevisiae. Nature, 2000. 403(6770): p. 623–7.

732
F. Luo et al.
47. Vo, T.D., H.J. Greenberg, and B.O. Palsson, Reconstruction and functional characterization of
the human mitochondrial metabolic network based on proteomic and biochemical data. J Biol
Chem, 2004. 279(38): p. 39532–40.
48. Ma, H. and A.P. Zeng, Reconstruction of metabolic networks from genome data and analysis
of their global structure for various organisms. Bioinformatics, 2003. 19(2): p. 270–7.
49. Horvath, S. and J. Dong, Geometric Interpretation of Gene Coexpression Network Analysis.
PLoS Comput Biol, 2008. 4(8): p. e1000117.
50. Stuart, J.M., et al., A Gene-Coexpression Network for Global Discovery of Conserved Genetic
Modules. Science, 2003. 302(5643): p. 249–255.
51. Patrick, N.M. and M. Michael, Laplacian spectra as a diagnostic tool for network structure and
dynamics. Physical Review E (Statistical, Nonlinear, and Soft Matter Physics), 2008. 77(3):
p. 031102.
52. Mohar, B., The Laplacian spectrum of graphs, in Graph Theory, Combinatorics, and Applica-
tions, G.C. Y. Alavi, O. R. Oellermann, A. J. Schwenk, Editor. 1991, Wiley. p. 871–898.
53. Song, C., S. Havlin, and H.A. Makse, Self-similarity of complex networks. Nature, 2005.
433(7024): p. 392–5.
54. Xenarios, I., et al., DIP, the Database of Interacting Proteins: a research tool for studying
cellular networks of protein interactions. 2002. p. 303–305.
55. Deane, C.M., et al., Protein interactions: two methods for assessment of the reliability of high
throughput observations. Mol Cell Proteomics, 2002. 1: p. 349–356.
56. Jeong, H., et al., The large-scale organization of metabolic networks. Nature, 2000. 407(6804):
p. 651–654.
57. Overbeek, R., et al., WIT: integrated system for high-throughput genome sequence analysis
and metabolic reconstruction. Nucleic Acids Res, 2000. 28(1): p. 123.
58. Farkas, I., et al., The topology of the transcription regulatory network in the yeast, Saccha-
romyces cerevisiae. Physica A: Statistical Mechanics and its Applications, 2003. 318(3–4):
p. 601–612.
59. Winzeler, E.A., et al., Functional characterization of the S. cerevisiae genome by gene deletion
and parallel analysis. Science, 1999. 285(5429): p. 901.
60. Erd s, P. and A. R´enyi, On the evolution of random graphs. 1960: Citeseer.
61. Watts, D.J. and S.H. Strogatz, Collective dynamics of ‘small-world’ networks. Nature, 1998.
393(6684): p. 440–2.
62. Barabasi, A.L. and R. Albert, Emergence of scaling in random networks. Science, 1999.
286(5439): p. 509–12.
63. Enright, A.J. and C.A. Ouzounis, BioLayout – an automatic graph layout algorithm for
similarity visualization. Bioinformatics, 2001. 17(9): p. 853–4.
64. Cohen, J.D. and F. Tong, NEUROSCIENCE: The Face of Controversy. Science, 2001.
293(5539): p. 2405–2407.
65. Jalan, S. and J.N. Bandyopadhyay, Random matrix analysis of complex networks. Physical
Review E (Statistical, Nonlinear, and Soft Matter Physics), 2007. 76(4): p. 046107.
66. Bandyopadhyay, J.N. and S. Jalan, Universality in complex networks: Random matrix analysis.
Physical Review E (Statistical, Nonlinear, and Soft Matter Physics), 2007. 76(2): p. 026109.
67. Maslov, S. and K. Sneppen, Speciﬁcity and stability in topology of protein networks. Science,
2002. 296(5569): p. 910–3.
68. Alon, U., et al., Broad patterns of gene expression revealed by clustering analysis of tumor
and normal colon tissues probed by oligonucleotide arrays. Proc Natl Acad Sci U S A, 1999.
96(12): p. 6745–50.
69. Singh, D., et al., Gene expression correlates of clinical prostate cancer behavior. Cancer Cell,
2002. 1(2): p. 203–9.

Chapter 29
Keyword Search on Large-Scale Structured,
Semi-Structured, and Unstructured Data
Bin Zhou
1
Introduction
Keyword search is a type of search that looks for matching documents which contain
one or more keywords speciﬁed by a user [44]. Recently, the great success of
Web search engines (e.g., Google, Yahoo!, and Microsoft’s Bing) has shown the
simplicity and the power of keyword search on large collections of Web documents.
Typically, a user needs to submit to a Web search engine a keyword query which
contains one or more keywords. The Web search engine then searches the pools
of indexed Web documents and returns a ranked list of matching documents to the
user. The simplicity and the power of keyword search have made it the most popular
way for millions of users to retrieve information on the Web through Web search
engines.
In the current information era, in addition to textual Web documents, there
also exist vast collections of structured, semi-structured, and unstructured data
in various applications that contain abundant text information. Typical examples
of those data sources include relational tables, XML data, labeled graphs, and
so on. For example, Table 29.1 shows some relational tables used to store the
product information of various tablet PCs on the market. Some dimensions in
the relational tables contain textual data, such as Hardware in Table 29.1a and
Review in Table 29.1b. As another example, Fig. 29.1 shows the XML data for
some conference proceedings. The XML data can be represented as a tree structure
which contains a set of attributes and their corresponding attribute values. Some
speciﬁc attributes, such as title and author, contain rich textual information.
Traditionally, in order to retrieve information from large collections of structured,
semi-structured, and unstructured data, a user has to learn some speciﬁc structured
B. Zhou
Department of Information Systems, University of Maryland,
Baltimore County (UMBC), Baltimore, USA
e-mail: bzhou@umbc.edu
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 29, © Springer Science+Business Media, LLC 2011
733

734
B. Zhou
Table 29.1 The relational tables of some tablet products
Product
Manufacturer
OS
Hardware
iPad 2
Apple
iOS
wiﬁ
TF101
ASUS
Android
wiﬁ, GPS
  
  
  
  
(a) Product details
User
Product
Review
Mike
iPad 2
Stylish, light-weighted
Mike
TF101
Cheap, powerful
Jane
iPad 2
Easy to use
  
  
  
(b) Product reviews
Fig. 29.1 The XML data of
some conference proceedings
name
year
2011
SIGMOD
title author
...
paper
title author...
paper
conference
OLAP
name
data
name
Han
Ding
...
query languages, such as SQL and XQuery [18]. In some particular scenarios
such as forming a SQL query in a relational database or issuing an XQuery over
large-scale XML data, a user also needs to be familiar with the underlying data
schemas. The data schemas may be very complex and fast evolving. As a result, it
is impractical for those general users to look up information using the traditional
methods based on structured query languages.
Due to the simplicity and the power of keyword search, it is natural to extend
keyword search to retrieve information from large collections of structured, semi-
structured, and unstructured data. Thus, developing efﬁcient and effective keyword
search techniques for different types of data sources is an important and interesting
research problem [2,9–11,46,48,49].
In the past decade, many efﬁcient and effective techniques for keyword search
have been developed. In this chapter, we survey several representative techniques
in the literature. These techniques have several desirable characteristics which are
very useful in different application scenarios.
The remainder of the chapter is organized as follows. In Sect. 2, we provide an
overview of the challenging problems in keyword search. In Sect. 3, we discuss sev-
eral important solutions developed to support keyword search in various scenarios,
namely information retrieval-based solutions (Sect. 3.1), database systems-based so-
lutions (Sect. 3.2), and the solutions based on the integration of information retrieval
and database techniques (Sect. 3.3). In Sect. 4, we discuss various techniques of
query suggestion with the purpose of improving the effectiveness of keyword search.
Section 5 concludes the chapter, and presents some interesting future research
directions.

29
Keyword Search on Large-Scale Structured...
735
2
Keyword Search: The Problem
Generally, keyword search refers to a type of search method that ﬁnds matching
objects which are relevant to a keyword query speciﬁed by a user.
Deﬁnition 29.1 (Keyword Query). An m-keyword query Q is a set of keywords of
size m, denoted as Q D fk1; k2; : : : ; kmg.
Given a user’s keyword query Q, the core task in the keyword search problem is
ﬁnding related and meaningful answers from different data sources. Many studies
about keyword search in various aspects have been conducted in recent years.
Figure 29.2 provides an overview of the keyword search problem on large-scale
data.
There are several questions need to be answered before the discussion of the
keyword search technique. The ﬁrst question is about the data model. What types of
data sources the keyword search technique can be applied on? The second question
is about the answer model. For a given data source, what types of results should be
returned as valid answers? Thus, to understand the problem of keyword search well,
it is necessary to ﬁrst understand the different data models and answer models.
2.1
Data Models of Keyword Search
In the current information era, data are accumulated in different formats. Generally
speaking, different types of data can be classiﬁed into three major categories, namely
structured data, semi-structured data, and unstructured data. The technique of
keyword search has been successfully applied on all of these different types of data
to retrieve useful information.
2.1.1
Structured Data
This refers to the type of data that reside in ﬁxed dimensions/ﬁelds. Typical
examples of structured data include relational databases and spreadsheets.
Structured data are widely used in practice. Recently, the burst of the Internet
has given rise to an ever increasing amount of text data associated with multiple
attributes. For instance, customer reviews of tablets in an online shopping
website are always associated with attributes like Manufacturer, Hardware,
and OS. Such data can be easily maintained using a relational database where
speciﬁc text attributes (e.g., Review) are used to store the text data. An example of
the relational data schema is provided in Table 29.1.

736
B. Zhou
Keyword Search
Query Suggestion
Structured Data
Relational Tables
Unstructured Data
Web Pages
Search Logs
Semi−Structured Data
Answer Models
Single−Entity Model
Multiple−Entity Model
Data−Driven
Usage−Driven
Data Models
Two Tasks
XML Data
Fig. 29.2 Overview of the keyword search problem
2.1.2
Unstructured Data
Contrast with structured data, unstructured data do not reside in any ﬁxed dimen-
sions/ﬁelds. In most of the cases, unstructured data refer to free-form text data which
are ubiquitous in many applications.
Typical examples of unstructured data are Web documents, E-mail messages,
Word processing documents, and many more. A huge amounts of information the
human beings are dealing with everyday are unstructured text data.
2.1.3
Semi-Structured Data
This type of data is a form of structured data which do not conform with a speciﬁc
data schema or a data model. However, semi-structured data use attributes or tags
to separate semantic elements and enforce attribute hierarchies. XML data is one
example of the semi-structured data.
In recent years, semi-structured data are increasingly popular due to the advent
of the Internet. In many situations, full-text documents and relational databases are
not be able to represent the rich sets of the information. Many applications (e.g.,
object-oriented databases) require a medium for exchanging information. In semi-
structured data, textual information is widely used to represent the attribute values.
For example, the attribute values of name and title in Fig. 29.1 contain rich text
information.
In terms of the data models of keyword search, keyword search can be applied not
only on unstructured data (e.g., Web documents), but also on structured data (e.g.,
relational tables) and semi-structured data (e.g., XML data and labeled graphs).
For instance, the problem of keyword search on labeled graphs tries to assemble
semantically relevant connections (modeled as a subgraph) among keywords in

29
Keyword Search on Large-Scale Structured...
737
the query [15, 16, 20, 24]. As another example, the problem of keyword search
on relational tables focuses on ﬁnding individual tuples matching a set of query
keywords from one table or the join of multiple tables [1,6,22,32,33,37,38].
2.2
Answer Models of Keyword Search
In different data models, the answers to keyword search are quite different. In the
following, we discuss the answer models using three representative data models,
including Relational Tables, XML Data, and Web Pages.
Without loss of generality, the answer models of keyword search can be roughly
classiﬁed into two categories: the single-entity answer model and the multiple-entity
answer model.
2.2.1
Single-Entity Answer Model
In this answer model, all the keywords in a query must appear together in a single
entity in the data. Here an entity refers to a tuple in a relational table, a node in the
XML data, or an individual Web document.
Consider the relational tables in Table 29.1. For a user keyword query
Q D fcheap; powerfulg, the individual tuple about Mike’s review on TF101
in Table 29.1b matches all the keywords in Q. Thus, this tuple is returned as a valid
answer to Q.
Consider the XML data in Fig. 29.1. For a user keyword query Q D fOLAPg, the
node which contains the keyword OLAP matches all the keywords in Q. Thus, this
node is returned as a valid answer to Q.
In Web search engines, the revelent Web documents which contain all the
keywords in a user keyword query Q are retrieved. Those Web documents are
considered to be valid answers to Q, and are ranked by the Web search engines
and shown to the user.
2.2.2
Multiple-Entity Answer Model
In this answer model, the keywords in a query can be distributed into several entities
which are closely related.
Consider the relational tables in Table 29.1. For a user keyword query
Q D fcheap; GPSg, there is no single tuple matching both of the two keywords in
Q. However, Table 29.1b can be joined with Table 29.1a since there is a foreign key
reference from one to the other.1 The join result of the second tuple in Table 29.1a
1The dimension Product is a primary key in Table 29.1a, and a foreign key in Table 29.1b.

738
B. Zhou
and the second tuple in Table 29.1b contains both of the two keywords cheap and
GPS, and thus, is returned as a valid answer to Q.
Consider the XML data in Fig. 29.1. For a user keyword query Q D fOLAP; Hang,
there is no individual node containing both of the two keywords in Q. However, the
two nodes are within the same subtree in Fig. 29.1 where the paper node is a root
node of this subtree. This subtree captures some semantic relationships among the
two keywords. As a result, this subtree rooted at the paper node is returned as a
valid answer to Q.
In Web search engines, given a user keyword query Q, it is quite often that
all the keywords in Q do not appear together in a single Web document. In [31],
Li et al. proposed to incorporate the hyper-links on the Web to retrieve useful
information. The concept of information unit, which can be considered as a logical
Web document containing multiple Web pages as one atomic unit linked by hyper-
links, is adopted to ﬁnd valid answers to Q.
The answer models play an important role in the development of keyword search
techniques. In the literature, a set of different deﬁnitions based on the multiple-entity
answer model have been studied. A more detailed review of speciﬁc answer models
will be discussed in Sect. 3.3.
2.3
Core Tasks in Keyword Search
In the keyword search problem, the core task is ﬁnding related and meaningful
answers from different data sources. Another core task is providing related queries
as suggestions to assist users’ search process.
With the assistance of keyword search, both experienced users and inexperienced
users can easily retrieve valuable information from large-scale structured, semi-
structured, and unstructured data. However, in most cases, a keyword query typically
is very short, which on average only contains 2 or 3 keywords.2 Such short keyword
queries may not be able to precisely capture users’ information needs. As a result,
keyword queries in practice are often ambiguous. To improve the effectiveness of
keyword search, most popular commercial Web search engines currently provide a
useful service called query suggestion. By conjecturing a user’s real search intent,
the Web search engines can recommend a set of related queries which may better
reﬂect the user’s true information need.
Intuitively, a search intent can be deﬁned based on the clusterings of valid
answers.
Deﬁnition 29.2 (Search Intent). For a keyword query Q and a data model D, we
assume there are n valid answers, denoted as A D fAns1; Ans2; : : : ; Ansng. A sim-
ilarity function Sim.Ansi; Ansj / (1  i; j  n) is used to calculate the similarity
2http://www.keyworddiscovery.com/keyword-stats.html

29
Keyword Search on Large-Scale Structured...
739
between two answers Ansi and Ansj . A search intent Ci is captured by a subset of
answers Ci D fAnsi1; Ansi2; : : : ; Ansiti g (1  i1 < i2 < : : : < ti  n), such that
the answers in Ci are similar to each other while the answers in Ci and A  Ci
are dissimilar, that is, 8Ansj1; Ansj2 2 Ci and Ansj3 … Ci, Sim.Ansj1; Ansj2/ >
Sim.Ansj1; Ansj3/ and Sim.AAnsj1; Ansj2/ > Sim.Ansj2; Ansj3/.
To provide meaningful recommendations, query suggestion has to take into
account different types of information sources. Generally, all the existing query
suggestion methods for keyword search can be roughly classiﬁed into either data-
driven methods or usage-driven methods. The data-driven query suggestion methods
utilize the data itself. They apply data mining techniques to identify the correlations
among data, and select closely related queries as recommendations. The usage-
driven query suggestion methods focus on the search activities. For example, search
logs trace users’ search behavior. In Web search engines, a commonly used query
suggestion method [3] is to ﬁnd common queries following the current user query
in search logs and use those queries as suggestions for each other.
As keyword search is a useful tool to retrieve information from large-scale
structured, semi-structured, and unstructured data, developing efﬁcient and effective
keyword search techniques on different data sources has been an important research
direction in the areas of data mining, database systems, Web Search and information
retrieval.
3
Keyword Search: The Techniques
Keyword search is a well-studied problem in the world of text documents and
Web search engines. The Informational Retrieval (IR) community has utilized
the keyword search techniques for searching large-scale unstructured data, and
has developed various techniques for ranking query results and evaluating their
effectiveness [4,35]. Meanwhile, the Database (DB) community has mostly focused
on large-collections of structured data, and has designed sophisticated techniques
for efﬁciently processing structured queries over the data [18]. In recent years,
emerging applications such as customer support, health care, and XML data
management require high demands of processing abundant mixtures of structured
and unstructured data. As a result, the integration of Databases and Information
Retrieval technologies becomes very important [2,10,46].
Keyword search provides great ﬂexibility for analyzing both structured and
unstructured data that contain abundant text information. In this section, we
summarize some representative studies in different research areas including In-
formation Retrieval, Databases, and the integration of Databases and Information
Retrieval.

740
B. Zhou
keyword
search
in
ir
db
...
2, 3
1, 2, 3
1, 3
1, 2, 3
1, 2, 3
An inverted list
Keyword search in IR
Keyword search in DB
Keyword search in IR and DB
...
Document
Text
1
2
3
...
A set of documents
Fig. 29.3 An example of the inverted list
3.1
Information Retrieval-Based Keyword Search
In Information Retrieval, keyword search is a type of search method that looks for
matching documents which contain one or more keywords speciﬁed by a user. The
keyword search technique in information retrieval concerns with two major issues:
how to retrieve the set of relevant documents, and how to rank the set of relevant
documents.
3.1.1
Document Retrieval
The Boolean retrieval model [35] is one of the most popular models for information
retrieval in which users can pose any keyword queries in the form of a Boolean
expression of keywords, that is, keywords are combined with some Boolean
operators such as AND, OR, and NOT. The Boolean retrieval model views each
document as just a set of keywords. A document either matches or does not match
a keyword query.
Inverted lists are commonly adopted as the data structure for efﬁciently answer-
ing various keyword queries in the Boolean retrieval model [4, 19, 35, 51, 52]. The
basic idea of an inverted list is to keep a dictionary of keywords. Then, for each key-
word, the index structure has a list that records which documents the keyword occurs
in. Figure 29.3 shows a simple example of the inverted list for a set of documents.
In the case of large document collections, the resulting number of matching
documents using the Boolean retrieval model can far more than what a human being
could possibly scan through. Accordingly, it is essential for a search system to rank
the documents matching a keyword query properly. This model is called ranked
retrieval model [35]. The vector space model is usually adopted to represent the
documents and the keyword queries. The relevance of a document with respect to a
keyword query can be measured using the well-known Cosine similarity.
3.1.2
Document Ranking
An important and necessary post-search activity for keyword search in Information
Retrieval is the ranking of search results [4,27,36]. In general, the ranking metrics

29
Keyword Search on Large-Scale Structured...
741
take into account two important factors. One is the relevance between a document
and a keyword query. The other is the importance of the document itself. The term-
based ranking and the link-based ranking are the two most popular ranking methods
used widely in practice.
The term-based ranking methods, such as TFIDF [4], captures the relevance
between documents and keyword queries based on the content information in the
documents. A document d and a keyword query q can be regarded as sets of
keywords, respectively. The TFIDF score of a document d with respect to a keyword
query q is deﬁned as
TFIDF.d; q/ D
X
t2d\q
TF.t/  IDF.t/;
(29.1)
where TF.t/ is the term frequency of keyword t in d, and IDF.t/ is the inverse
document frequency of keyword t which is the total number of documents in the
collections divided by the number of documents that contain t.
The link-based ranking methods, such as PageRank [36] and HITS [27], are
widely adopted by Web search engines for ranking search results.
The Web can be modeled as a directed Web graph G D .V; E/, where V is the
set of Web pages, and E is the set of hyperlinks. A link from page p to page q
is denoted by an edge p ! q. A page p may have multiple hyperlinks pointing
to page q, however, in the Web graph, only one edge p ! q is formed. Such
structure modeling not only can make the Web graph simple, but also can let some
mathematical models, such as the Markov chain model, be suitable for analyzing
the Web graph.
PageRank [36] measures the importance of a page p by considering how
collectively other Web pages point to p directly or indirectly. Formally, for a Web
page p, the PageRank score [36] is deﬁned as
PR.p; G/ D d
X
pi 2M.p/
PR.pi; G/
OutDeg.pi/ C 1  d
N
;
(29.2)
where M.p/ D fqjq ! p 2 Eg is the set of pages having a hyperlink pointing
to p, OutDeg.pi/ is the out-degree of pi (that is, the number of hyperlinks from
pi pointing to some pages other than pi), d is a damping factor which models
the random transitions on the Web, and N D jV j is the total number of pages in
the Web graph. The second additive term on the right side of the equation, 1d
N ,
is traditionally referred to as the random jump probability and corresponds to a
minimal amount of PageRank score that every page gets by default.
In PageRank [36], each page on the Web has a measure of page importance
that is independent of any information need or keyword query. Intuitively, the
importance of a page is proportional to the sum of the importance scores of pages
linking to it.

742
B. Zhou
Similarly, in HITS [27], a query is used to select a subgraph from the Web.
From this subgraph, two kinds of nodes are identiﬁed: authoritative pages to which
many pages link, and hub pages that contain comprehensive collections of links to
authoritative pages.
3.2
Database Systems-Based Keyword Search
Relational databases are widely used in practice. Recently, the burst of the Internet
has given rise to an ever increasing amount of text data associated with multiple
attributes. An an example, Table 29.1 shows some relational tables used to store
the product information of various tablet PCs on the market. Dimensions such as
Hardware in Table 29.1a and Review in Table 29.1b contain rich textual data.
The database management systems, also known as DBMS (e.g., Oracle,
Microsoft’s SQL Server, MySQL), utilize a built-in full-text search engine to
retrieve the tuples that contain keywords in some text attributes [42,43,45]. To make
sure that full-text queries can be applied on a given table in a database, the database
administrator has to create a full-text index on the table in advance. The full-text
index includes one or more text attributes in the table. The results to the full-text
queries are individual tuples in the corresponding relational databases.
For example, consider a relational database which contains a set of relational
schemas R D fR1; R2; : : : ; Rng. To select all the tuples from R which contain a user
speciﬁed keyword k1, those major commercial DBMSs support a predicate function
contain.A1; k1/, where A1 is a text attribute in R. The corresponding SQL query
is the following:
SELECT  FROM R WHERE contain.A1; k1/.
In recent years, the Boolean retrieval model [4,19,35,51,52] and natural language
processing techniques have been integrated into the full-text functionalities in those
database management systems.
For example, the following SQL query is to ﬁnd all the tuples in R that contain
keyword k1 provided that the attributes A1 and A2 are all and the only text attributes
in the relation R:
SELECT  FROM R WHERE contain.A1; k1/ OR contain.A2; k1/.
3.3
Databases & Information Retrieval-Based Keyword Search
The integration of Databases and Information Retrieval provides ﬂexible ways for
users to search information using keyword search. A few critical challenges have
been identiﬁed such as how to model the query answers in a semantic way and how

29
Keyword Search on Large-Scale Structured...
743
to address the ﬂexibility in scoring and ranking models. To obtain more details in
this direction, survey studies in [2, 9–11, 46, 48, 49] provide excellent insights into
those issues.
3.3.1
Graph Model of the Data
Quite a few studies about keyword search on relational databases model the
database as a graph. For instance, several keyword search prototype systems (e.g.,
DBXplorer [1], Discover [22], BANKS [6], BLINKS [20], and SPARKS [33]) have
been developed to use the graph model to represent the relational databases. In the
graph model, nodes refer to entities, such as a tuple in a relational database; and
edges refer to relationships among entities, such as primary-foreign key relationship
in relational databases.
Depending on speciﬁc applications, either directed graph model or undirected
graph model can be adopted. For instance, while some systems (e.g., DBXplorer [1]
and Discover [22]) model the data as an undirected graph, some other systems (e.g.,
BANKS [6], BLINKS [20], and SPARKS [33]) model the data as a directed graph.
In general, the undirected graph model can be considered as a special case of
the directed graph model, since each undirected edge in an undirected graph can be
represented using two directed edges in the correspondingdirected graph. Moreover,
the tightness of connections between two nodes in the graph is not necessary to be
symmetric. Thus, modeling directionality in graphs becomes a natural strategy in
many applications.
The nodes in a graph contain a ﬁnite set of keywords. For instance, in the graph
model of a relational database, the keywords that each node associated with can be
those keywords extracted from the textual attributes of the corresponding tuple in
the relational database.
Deﬁnition 29.3 (Graph Model of the Data). The complex data are modeled using
a directed graph G D .V; E/. Each node v 2 V represents an entity in the data.
For a pair of nodes vi; vj 2 V , there exists a directed edge eij D vi ! vj if the
corresponding entities of vi and vj in the data have a speciﬁc relation.
The dictionary, D, is a ﬁnite set of keywords that appear in the data. There is
a node-to-keyword mapping  W V ! 2D, which lists the ﬁnite set of keywords
contained in a node. We use W.v/ to represent the set of keywords that a node
v contains. The number of distinct keywords contained in a node v is denoted
as jW.v/j.
There is also a keyword-to-node mapping  W D ! 2V , which lists the ﬁnite
set of nodes that contain the keyword. We use S.w/ to represent the set of nodes
that contain a keyword w. The number of distinct nodes containing keyword w is
denoted as jS.w/j.
There is a cost function ı W E ! RC, which models the distance of the nodes
between each other.

744
B. Zhou
3.3.2
Keyword Search on Graphs
Keyword search on graphs tries to assemble semantically relevant connections in the
graph among those keywords in the query. In the directed graph model, an answer
to a keyword query on graphs can be deﬁned using the rooted tree semantic [20].
Deﬁnition 29.4 (Rooted Tree Semantic). Given an m-keyword query Q D fk1; k2
; : : : ; kmg and a directed graph G, an answer to Q is a rooted tree with a root node
r and m leaf nodes, represented as n1; : : : ; nm, where r and ni’s are nodes in G
(may not be distinct) satisfying the following two properties:
Containment: 8i, ni contains keyword ki, that is, ki 2 W.ni/;
Connectivity: 8i, there exists a directed path in G from r to ni.
Intuitively, in the rooted tree semantic, the directed paths from the root node to
the leaf nodes in an answer describe how the keywords are semantically related in
the graph.
Most of the previous studies concentrate on ﬁnding minimal rooted trees from
relational databases and graphs [1, 6, 16, 21, 22, 25, 26, 32, 33]. Recently, several
other types of semantics of query answers have been proposed. Under the graph
model of a relational database, BLINKS [20] proposes to ﬁnd distinct roots as
answers to a keyword query. An m-keyword query ﬁnds a collection of tuples that
contain all the keywords reachable from a root tuple within a user-given distance.
BLINKS [20] builds a bi-level index for fast keyword search on graphs. Recently,
Qin et al. [38] model a query answer as a community. A community contains several
core vertices connecting all the keywords in the query. Li et al. [28] study keyword
search on large collections of heterogenous data. An r-radius Steiner graph semantic
is proposed to model the query answers. Later on, Qin et al. [37] consider all
the previous semantics of query answers, and show that the current commercial
database management systems are powerful enough to support keyword queries
in relational databases efﬁciently without any additional indexing to be built and
maintained.
We can assign numerical scores to each answer to a keyword query on graphs. In
many cases, the score of an answer is a function of all the weights on the edges in the
answer. Several models of the scoring function have been proposed in the literature.
For instance, the model used in [6] deﬁnes the score of an answer as the number of
edges in the answer. The model used in [24] treats an answer as a set of paths, with
one path per keyword, where each path starts from the root node and points to the
leaf node that contains the keyword in the query. The score of an answer is deﬁned
as the sum of all the path lengths in the answer. In general, if the scoring function of
the answers is monotonic, the keyword search algorithms on graphs are not affected
by which speciﬁc scoring function is used.

29
Keyword Search on Large-Scale Structured...
745
3.3.3
Related Studies
As a concrete step to provide an integrated platform for text-rich and data-rich
applications, keyword search on relational databases becomes an active topic in
database research. Several interesting and effective solutions and prototype systems
have been developed [1,6,20,22,33,40].
For instance, DBXplorer [1] is a keyword-based search system implemented
using a commercial relational database and Web server. DBXplorer returns all rows,
either from individual tables or by joining multiple tables using primary-foreign
keys, such that each row contains all keywords in a query. It uses a symbol table
as the key data structure to look up the respective locations of query keywords in
the database. DISCOVER [22] produces without redundancy all joining networks of
tuples on primary and foreign keys, where a joining network represents a tuple that
can be generated by joining some tuples in multiple tables. Each joining network
collectively contains all keywords in a query. Both DBXplorer and DISCOVER
exploit the schema of the underlying databases. Hristidis et al. [21] develop efﬁcient
methods which can handle queries with both AND and OR semantics and exploit
ranking techniques to retrieve top-k answers.
BANKS [6] models a database as a graph where tuples are nodes and application-
oriented relationships are edges. Under such an extension, keyword search can be
generalized on trees and graphs. BANKS searches for minimum Steiner trees [17]
that contain all keywords in the query. Some effective heuristics are exploited to
approximate the Steiner tree problem, and thus the algorithm can be applied to
huge graphs of tuples. To improve the search efﬁciency on large graphs, Kacholia
et al. [24] introduce the bidirectional expansion techniques to improve the search
efﬁciency on large graph databases. Dalvi et al. [15] study the problem of keyword
search on graphs which are signiﬁcantly large to be held in main memory. To
provide keyword search on graphs which are stored in external memory, Dalvi
et al. [15] build a graph representation which combines both condensed version
of the graph and the original graph together. The resulting graph representation is
always memory resident.
Because keyword search on relational databases and graphs takes both vertex
labels and graph structures into account, there are many possible strategies for
ranking answers. Different ranking strategies reﬂect designers’ respective concerns.
Various effective IR-style ranking criteria and search methods are developed, such
as those studies in [11,16,28,32,33,38].
While most of the existing studies focus on returning subgraphs that contain
all the keywords, ObjectRank [5] returns individual tuples as answers. It applies a
modiﬁed PageRank algorithm to keyword search in a database for which there is a
natural ﬂow of authority between the objects (e.g., tuples in relational databases).
To calculate the global importance of an object, a random surfer has the same
probability to start from any object in the base set [41]. ObjectRank returns objects
which have high authority with respect to all query keywords.

746
B. Zhou
4
Keyword Search: Query Suggestion Techniques
In those popular commercial Web search engines, query suggestion has become a
well-accepted functionality to assist the users to explore and formulate their precise
information needs during the search process. The objective of query suggestion in
search engines is transforming an initial search query into a better one which is
capable of satisfying the users’ real information need by retrieving more relevant
documents. There are quite a lot of studies conducted on generating different
types of query suggestions, such as query auto-completion [13], query spelling
correction [30], query expansion [47], and query rewriting [23]
To provide a good query suggestion, it is necessary to understand the users’ pre-
cise search intents behind search queries. Generally, additional information rather
than search queries should be taken into consideration for query understanding.
These pieces of information include users’ explicit feedbacks (e.g., [34]), users’
implicit feedbacks (e.g., [7, 8]), users’ personal proﬁles (e.g., [12]), search result
snippets (e.g., [39]), and so on.
Search logs contain crowd intelligence accumulated from millions of users, that
is, a large number of people simultaneously converge upon the same point of
knowledge. As such, search logs recently have been widely used for the purpose
of query suggestion. For instance, Cui et al. [14] extract probabilistic correlations
between query keywords and document terms by analyzing search logs and used the
correlations to select high-quality expansion terms for new queries. Jones et al. [23]
identify typical substitutions the users made to their queries from search logs, and
leverage the information to improve the quality of user queries. Recently, search
context has been identiﬁed as one piece of important source to understand users’
search behavior. For instance, Cao et al. [7, 8] propose a general context-aware
model for query suggestion and ranking. These studies indicate that search contexts
are effective for disambiguating keyword queries and improving the quality of
multiple search services.
Most recently, the query suggestion technique has attracted some attention
for keyword search on structured and semi-structured data. Zhou et al. [50]
propose a query relaxation scheme to deal with imprecise document models and
heterogeneous schemas. The major idea is to utilize duplicates in co-related data sets
to identify important correlations. Then, these correlations are used to appropriately
relax users’ search queries. Recently, Li et al. [29] study a keyword search problem
on XML data by mining promising result types. Li et al. [29] claim that keyword
query is hard to be precise. As a result, the number of returned answers may be
too huge, and only a few types of them are interesting to the users. With the aim to
help users disambiguate possible interpretations of the query, Li et al. [29] propose a
ranking methods by taking into account the query results for different interpretation.
Only the most promising result types are returned.

29
Keyword Search on Large-Scale Structured...
747
5
Conclusions
As our world is now in its information era, huge amounts of structured, semi-
structured, and unstructured data are accumulated everyday. A real universal
challenge is to ﬁnd useful information from large collections of data to capture
users’ information needs. Keyword search on large-scale data is a fast-growing
research direction to meet this challenge. Many kinds of techniques have been
developed in the past decade.
In this chapter, we focus on some challenging problems in keyword search on
large collections of structured, semi-structured, and unstructured data. We provide
an overview of the keyword search problem on large-scale data. Two important
tasks, keyword search and query suggestion, are discussed. The task of keyword
search highly relies on understanding two important concepts, that is, the data
model and the answer model. The techniques surveyed in this chapter have several
desirable characteristics which are useful in different application scenarios.
With the simplicity and the power of the keyword search technique, it is
interesting to re-examine and explore many related problems, extensions and
applications in the future. Some of them are listed here.
•
Data-driven keyword search. The current keyword search technique in Web
search engines tries to match the user query keywords against a large collection
of textual pages. In recent years, many research activities have been focusing on
either providing meaningful query suggestions or improving the matching tech-
niques between the queries and the pages. However, other than the unstructured
textual pages on the Web, there also exist vast collections of structured data (e.g.,
relational tables) and semi-structured data (e.g., XML data and labeled graphs).
There is no effort on analyzing whether the data are suitable for answering
speciﬁc keyword queries. What if the best answer to a query is contained in
structured/semi-structured data other than unstructured textual pages? What if
some semantics are included in the query?
Consider a keyword query movies in Washington DC, the user in fact
is looking for the showtime lists of movies in Washington DC. If we consider
this query as a matching of keywords over unstructured textual pages, only those
Web pages containing the keywords will be returned to the user. However, in this
case, the movies on shown in Washington DC without a keyword movie will not
be returned. In fact, such queries can be answered better using structured/semi-
structured data. For instance, a showtime movie listing table would provide
accurate answers to the query movies in Washington DC.
Generally, sorting the results based on IR-style relevance may not be the best
solution in some particular scenarios. We may need to perform a deeper analysis
of the query in order to understand the semantics and return better results by
using appropriate data sources.
•
Real-time keyword search. The Web 2.0 technique has created huge interactive
information sharing. One obstacle nowadays requiring further exploration is

748
B. Zhou
searching important information at real-time (e.g., popular tweets on Twitter),
even before they are widely spread on the Web. To address the problem, it is
necessary to understand and predict the patterns of information spread. In the
context of Web 2.0, people are actively involved in the information spread.
To understand the dynamics of real-time information search, fundamentally,
two important issues need to be considered. The ﬁrst issue is to reason how
information moves between people through social connections. The second
issue is to trace the small fragments of information themselves. For practical
applications, it is also interesting to obtain a better understanding on how to
leverage the vast network data for real-time search and how to maintain the fast-
evolving data.
References
1. Sanjay Agrawal, Surajit Chaudhuri, and Gautam Das. DBXplorer: A system for keyword-based
search over relational databases. In Proceedings of the 18th International Conference on Data
Engineering (ICDE’02), pages 5–16, Washington, DC, USA, 2002. IEEE Computer Society.
2. S. Amer-Yahia, P. Case, T. R¨olleke, J. Shanmugasundaram, and G. Weikum. Report on the
DB/IR panel at sigmod 2005. SIGMOD Record, 34(4):71–74, 2005.
3. Ricardo A. Baeza-Yates, Carlos A. Hurtado, and Marcelo Mendoza. Query recommendation
using query logs in search engines. In EDBT Workshops, volume 3268 of Lecture Notes in
Computer Science, pages 588–596. Springer, 2004.
4. Ricardo A. Baeza-Yates and Berthier A. Ribeiro-Neto. Modern Information Retrieval. ACM
Press/Addison-Wesley, 1999.
5. Andrey Balmin, Vagelis Hristidis, and Yannis Papakonstantinou. Objectrank: authority-based
keyword search in databases. In Proceedings of the Thirtieth international conference on Very
large data bases (VLDB’04), pages 564–575. VLDB Endowment, 2004.
6. Gaurav Bhalotia, Arvind Hulgeri, Charuta Nakhe, Soumen Chakrabarti, and S. Sudarshan.
Keyword searching and browsing in databases using banks. In Proceedings of the 18th
International Conference on Data Engineering (ICDE’02), pages 431–440. IEEE Computer
Society, 2002.
7. Huanhuan Cao, Daxin Jiang, Jian Pei, Enhong Chen, and Hang Li. Towards context-aware
search by learning a very large variable length hidden markov model from search logs. In
Proceedings of the 18th International World Wide Web Conference (WWW’09), pages 191–200,
Madrid, Spain, April 20-24 2009.
8. Huanhuan Cao, Daxin Jiang, Jian Pei, Qi He, Zhen Liao, Enhong Chen, and Hang Li. Context-
aware query suggestion by mining click-through and session data. In Proceeding of the 14th
ACM SIGKDD international conference on Knowledge discovery and data mining (KDD’08),
pages 875–883, New York, NY, USA, 2008. ACM.
9. Surajit Chaudhuri and Gautam Das. Keyword querying and ranking in databases. PVLDB,
2(2):1658–1659, 2009.
10. Surajit Chaudhuri, Raghu Ramakrishnan, and Gerhard Weikum. Integrating DB and IR
technologies: What is the sound of one hand clapping? In Proceedings of the 2nd Biennial
Conference on Innovative Data Systems Research (CIDR’05), pages 1–12, 2005.
11. Yi Chen, Wei Wang, Ziyang Liu, and Xuemin Lin. Keyword search on structured and semi-
structured data. In Proceedings of the 2009 ACM SIGMOD International Conference on
Management of Data (SIGMOD’09), pages 1005–1010. ACM, 2009.

29
Keyword Search on Large-Scale Structured...
749
12. Paul Alexandru Chirita, Claudiu S. Firan, and Wolfgang Nejdl. Personalized query expansion
for the web. In Proceedings of the 30th annual international ACM SIGIR conference on
Research and development in information retrieval (SIGIR’07), pages 7–14, New York, NY,
USA, 2007. ACM.
13. Kenneth Church and Bo Thiesson. The wild thing! In Proceedings of the ACL 2005 on
Interactive poster and demonstration sessions (ACL’05), pages 93–96, Morristown, NJ, USA,
2005. Association for Computational Linguistics.
14. Hang Cui, Ji-Rong Wen, Jian-Yun Nie, and Wei-Ying Ma. Probabilistic query expansion
using query logs. In Proceedings of the 11th international conference on World Wide Web
(WWW’02), pages 325–332, New York, NY, USA, 2002. ACM.
15. Bhavana Bharat Dalvi, Meghana Kshirsagar, and S. Sudarshan. Keyword search on external
memory data graphs. Proc. VLDB Endow., 1(1):1189–1204, 2008.
16. Bolin Ding, Jeffrey Xu Yu, Shan Wang, Lu Qin, Xiao Zhang, and Xuemin Lin. Finding
top-k min-cost connected trees in databases. In Proceedings of the 23rd IEEE International
Conference on Data Engineering (ICDE’07), pages 836–845, Washington, DC, USA, 2007.
IEEE Computer Society.
17. S. E. Dreyfus and R. A. Wagner. The steiner problem in graphs. Networks, 1:195–207, 1972.
18. Hector Garcia-Molina, Jeffrey D. Ullman, and Jennifer Widom. Database Systems: The
Complete Book. Prentice Hall Press, Upper Saddle River, NJ, USA, 2 edition, 2008.
19. Donna Harman, R. Baeza-Yates, Edward Fox, and W. Lee. Inverted ﬁles. In Information
retrieval: data structures and algorithms, pages 28–43, Upper Saddle River, NJ, USA, 1992.
Prentice-Hall, Inc.
20. Hao He, Haixun Wang, Jun Yang, and Philip S. Yu. Blinks: ranked keyword searches on graphs.
In Proceedings of the 2007 ACM SIGMOD International Conference on Management of Data
(SIGMOD’07), pages 305–316, New York, NY, USA, 2007. ACM.
21. Vagelis Hristidis, Luis Gravano, and Yannis Papakonstantinou. Efﬁcient ir-style keyword
search over relational databases. In Proceedings of the 29st international conference on Very
large data bases (VLDB’03), pages 850–861, 2003.
22. Vagelis Hristidis and Yannis Papakonstantinou. Discover: Keyword search in relational
databases. In Proceedings of the 28st international conference on Very large data bases
(VLDB’02), pages 670–681. Morgan Kaufmann, 2002.
23. Rosie Jones, Benjamin Rey, Omid Madani, and Wiley Greiner. Generating query substitutions.
In Proceedings of the 15th international conference on World Wide Web (WWW’06), pages
387–396, New York, NY, USA, 2006. ACM.
24. Varun Kacholia, Shashank Pandit, Soumen Chakrabarti, S. Sudarshan, Rushi Desai, and
Hrishikesh Karambelkar. Bidirectional expansion for keyword search on graph databases. In
Proceedings of the 31st international conference on Very large data bases (VLDB’05), pages
505–516. ACM, 2005.
25. Varun Kacholia, Shashank Pandit, Soumen Chakrabarti, S. Sudarshan, Rushi Desai, and
Hrishikesh Karambelkar. Bidirectional expansion for keyword search on graph databases. In
Proceedings of the 31st international conference on Very large data bases (VLDB’05), pages
505–516. ACM, 2005.
26. Benny Kimelfeld and Yehoshua Sagiv. Finding and approximating top-k answers in keyword
proximity search. In Proceedings of the twenty-ﬁfth ACM SIGMOD-SIGACT-SIGART sympo-
sium on Principles of database systems (PODS’06), pages 173–182, New York, NY, USA,
2006. ACM.
27. Jon M. Kleinberg. Authoritative sources in a hyperlinked environment. In Proceedings of the
9th Annual ACM-SIAM Symposium on Discrete Algorithm (SODA’98), pages 668–677. ACM,
1998.
28. Guoliang Li, Beng Chin Ooi, Jianhua Feng, Jianyong Wang, and Lizhu Zhou. Ease: an
effective 3-in-1 keyword search method for unstructured, semi-structured and structured data.
In Proceedings of the 2008 ACM SIGMOD international conference on Management of data
(SIGMOD’08), pages 903–914, New York, NY, USA, 2008. ACM.

750
B. Zhou
29. Jianxin Li, Chengfei Liu, Rui Zhou, and Wei Wang. Suggestion of promising result types
for xml keyword search. In Proceedings of the 13th International Conference on Extending
Database Technology (EDBT’10), pages 561–572. ACM, 2010.
30. Mu Li, Yang Zhang, Muhua Zhu, and Ming Zhou. Exploring distributional similarity based
models for query spelling correction. In Proceedings of the 21st International Conference on
Computational Linguistics and the 44th annual meeting of the Association for Computational
Linguistics (ACL’06), pages 1025–1032, Morristown, NJ, USA, 2006. Association for Compu-
tational Linguistics.
31. Wen-Syan Li, K. Selc¸uk Candan, Quoc Vu, and Divyakant Agrawal. Query relaxation by
structure and semantics for retrieval of logical web documents. IEEE Trans. on Knowl. and
Data Eng., 14(4):768–791, 2002.
32. Fang Liu, Clement Yu, Weiyi Meng, and Abdur Chowdhury. Effective keyword search in
relational databases. In Proceedings of the 2006 ACM SIGMOD international conference on
Management of data (SIGMOD’06), pages 563–574, New York, NY, USA, 2006. ACM.
33. Yi Luo, Xuemin Lin, Wei Wang, and Xiaofang Zhou. Spark: top-k keyword query in
relational databases. In Proceedings of the 2007 ACM SIGMOD international conference on
Management of data (SIGMOD’07), pages 115–126, New York, NY, USA, 2007. ACM.
34. Mark Magennis and Cornelis J. van Rijsbergen. The potential and actual effectiveness of
interactive query expansion. In Proceedings of the 20th annual international ACM SIGIR
conference on Research and development in information retrieval (SIGIR’97), pages 324–332,
New York, NY, USA, 1997. ACM.
35. Christopher D. Manning, Prabhakar Raghavan, and Hinrich SchRutze. Introduction to Informa-
tion Retrieval. Cambridge University Press, New York, NY, USA, 2008.
36. Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation
ranking: Bringing order to the web. Technical report, Stanford University, 1998.
37. Lu Qin, Je Xu Yu, and Lijun Chang. Keyword search in databases: the power of rdbms.
In Proceedings of the 35th SIGMOD International Conference on Management of Data
(SIGMOD’09), pages 681–694, Providence, Rhode Island, USA, 2009. ACM Press.
38. Lu Qin, Jeffrey Xu Yu, Lijun Chang, and Yufei Tao. Querying communities in rela-
tional databases. In Proceedings of the 25th International Conference on Data Engineering
(ICDE’09), pages 724–735. IEEE, 2009.
39. Mehran Sahami and Timothy D. Heilman. A web-based kernel function for measuring the
similarity of short text snippets. In Proceedings of the 15th international conference on World
Wide Web (WWW’06), pages 377–386, New York, NY, USA, 2006. ACM.
40. Kamal Taha and Ramez Elmasri. Bussengine: a business search engine. Knowledge and
Information Systems, 23(2):153–197, 2010.
41. Hanghang Tong, Christos Faloutsos, and Jia-Yu Pan. Random walk with restart: fast solutions
and applications. Knowledge and Information Systems, 14(3):327–346, 2008.
42. http://dev.mysql.com/doc/refman/5.0/en/fulltext-search.html.
43. http://download.oracle.com/docs/cd/B28359 01/text.111/b28303/toc.htm.
44. http://en.wikipedia.org/wiki/Keyword search.
45. http://msdn.microsoft.com/en-us/library/ms142571.aspx.
46. Gerhard Weikum. DB&IR: both sides now. In Proceedings of the 2007 ACM SIGMOD
international conference on Management of data (SIGMOD’07), pages 25–30, New York, NY,
USA, 2007. ACM.
47. Ji-Rong Wen, Jian-Yun Nie, and Hong-Jiang Zhang. Clustering user queries of a search engine.
In Proceedings of the 10th international conference on World Wide Web (WWW’01), pages
162–168, New York, NY, USA, 2001. ACM.
48. Jeffrey Xu Yu, Lu Qin, and Lijun Chang. Keyword Search in Databases. Synthesis Lectures on
Data Management. Morgan & Claypool Publishers, 2010.
49. Jeffrey Xu Yu, Lu Qin, and Lijun Chang. Keyword search in relational databases: A survey.
IEEE Data Eng. Bull., 33(1):67–78, 2010.

29
Keyword Search on Large-Scale Structured...
751
50. Xuan Zhou, Julien Gaugaz, Wolf-Tilo Balke, and Wolfgang Nejdl. Query relaxation using
malleable schemas. In Proceedings of the 2007 ACM SIGMOD international conference on
Management of data (SIGMOD’07), pages 545–556, New York, NY, USA, 2007. ACM.
51. N. Ziviani, E. Silva de Moura, G. Navarro, and R. Baeza-Yates. Compression: A key for next
generation text retrieval systems. Computers, 33(11):37–44, 2000.
52. J. Zobel, A. Moffat, and K. Ramamohanarao. Inverted ﬁles versus signature ﬁles for text
indexing. ACM Transactions on Database Systems, 1(1):1–30, 1998.


Chapter 30
A Distributed Publish/Subscribe System
for Large Scale Sensor Networks
Masato Yamanouchi, Ryota Miyagi, Satoshi Matsuura, Satoru Noguchi,
Kazutoshi Fujikawa, and Hideki Sunahara
1
Introduction
Content-based network [5, 6] provides publish/subscribe system. This system is
useful to provide event driven mechanisms for notiﬁcations and alert messages.
Recently, various types of sensors are released and installed everywhere. If CBN
(Content Based Network) could manage these huge data on large scale sensor
network, event driven applications signiﬁcantly enhance their utility values. For
example, seismograph are generating seismographic data every second. But most
of data are infrequently-used. Many large scale sensor networks are proposed, but
almost existing systems only manage raw sensor data and these data are provided
by pull technology. Raw data is not useful. It needs to manage and calculate to
be a various data. For example, It is not useful by raw pressure data alone. But
rainfall probability is useful that is calculated by pressure data and rainfall data. Pull
technology generates a unnecessary load. Most of seismographic data generated by
seismograph are shows usual earthquake activity. But It needs to detect seismicity
changes to use for earthquake evacuation. Pull technology have to pull the data
from server at regular time intervals. In other words pull technology generates
unnecessary load every poling time.
In this paper, we use CBN to manage a large scale sensor networks and add
a data processing component onto CBN. If a system ﬁlter/calculate sensor data
on the way of publish/subscribe processes, users could receive more valuable
M. Yamanouchi () • H. Sunahara
Graduate School of Media Design, Keio University, 4-1-1 Hiyoshi, Kouhoku-ku,
Yokohama, Kanagawa, 223-8526, Japan
e-mail: masato-y@kmd.keio.ac.jp; suna@kmd.keio.ac.jp
R. Miyagi • S. Matsuura • S. Noguchi • K. Fujikawa
Graduate School of Information Science, Nara Institute of Science and Technology,
8916-5 Takayama-cho, Ikoma-shi, Nara, 630-0192, Japan
e-mail: ryota-m@is.naist.jp; satoru-n@is.naist.jp; matsuura@is.naist.jp; fujikawa@itc.naist.jp
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5 30, © Springer Science+Business Media, LLC 2011
753

754
M. Yamanouchi et al.
data and application ﬁelds could be expanded. For example, if we receive alert
messages related with integration values of rain/electricity on a particular area, we
take ﬂexible measures to cope with ﬂoods/power failures. If we could constantly
receive short-range weather forecasts of a local area, these information is useful for
businesses (e.g., construction works, road works) and our daily lives (e.g., washing
clothes, shopping). On the contrary, a data processing component probably causes
high-loaded, because many published data are concentrated on this component.
On the proposed system, a data component divides calculation processes into
small parts, and other data components cooperatively calculate these small parts.
Our proposed system secure scalability because of these cooperative calculation
processes. The organization of this paper is as follows. Section 2 describes related
work of large scale sensor network and CBN, Publish/Subscribe systems. Section 3
shows design of our proposed system. Section 4 shows performance evaluations of
our implementation. In Sect. 5, we summarize this paper.
2
Related Work
This section describes our goal and related work of large scale sensor network and
content-based network system.
2.1
Research Target
Recently many sensors are spread into globe and connected to the network. But
most of sensor networks are constructed in own domain and use for own purpose.
For example, Japan Meteorological Agency has a weather sensor network in Japan.
They install the weather sensor all over Japan at 20 km radius. Weather data are used
for weather forecast. Also Ministry of Land, Infrastructure, Transport and Tourism
has a weather sensor. They install the weather sensor around the road and use for
road warning system.
Global warming and other environment issues cause climate changes. Various
forms of highly local disaster occurred worldwide. Local disaster occurred smaller
than 20 km radius. It is difﬁcult to observe and detect local climatic phenomenon by
existing system. It needs 100m radius to observe pinpoint climatic phenomenon.
However, if possible that can aggregate the weather data of several domain by
common infrastructure, It can circulate high density weather data. Raw data is not
useful data. It need to calculate raw data to became a useful data. Our system needs
a scalability of calculation process to provide useful data on large scale sensor
network.
Sensor generates a data continuance. But most of data are infrequently-used.
Push mechanism is compatible to our system than pull mechanism. For example,
disaster evacuation system needs real time data. But pull mechanism has a delay.

30
A Distributed Publish/Subscribe System for Large Scale Sensor Networks
755
Fig. 30.1 Goal system of our research
Pulling application do not know that data exceed threshold value or not. It have to
get the sensor data at regular interval. In other words, it has a gap between creation
time of sensor data and poling time. Close the poling time reduce the delay but it
generates a high load.
Meanwhile, Push mechanism can deliver the data instantly to application that the
data exceeded threshold value. Push mechanism sends only exceeded data. It does
not cause a high load.
User requires many kind of calculated data. Subscription needs ﬂexibility.
Subscription is conﬁguration ﬁle that writes requirement of data by user.
Figure 30.1 shows our goal system of this research. Our goal is construct a sensor
delivery system that has push mechanism, scalable and ﬂexible data calculation.
2.2
Large Scale Sensor Network
2.2.1
Live E!
Live E! [16] distributed servers use a tree topology. Figure 30.2 shows architecture
of Live E! system. Owners of sensors add Live E! servers to manage sensor data.
Owners also can control data accesses by the servers. This tree topology is suited

756
M. Yamanouchi et al.
Fig. 30.2 Live E! architecture
for implementing an authentication mechanism in distributed environments such
as DNS. Live E! servers separately manage proﬁles of sensor devices and sensor
data and periodically receive these proﬁles from children sites. Each Live E! server
creates an index table from these proﬁles. Using this index table as a routing
table, Live E! servers support multi-attribute search as well as an authentication
mechanism in distributed environments. However, Live E! servers only provide raw
data and do not provide pub/sub system or calculated data.
2.2.2
IrisNet
IrisNet is one of large scale sensor networks [12]. IrisNet uses a two-tier archi-
tecture, comprised of two different agents. These agents are called sensing agents
(SAs) and organizing agents (OAs). Figure 30.3 shows architecture of IrisNet. OAs
are organized into groups and one service consists of one OA group. A group
of OAs creates the DNS like distributed database and provides query processing
infrastructure for a speciﬁc service (e.g., parking space ﬁnder, person ﬁnder). An
OA manages a part of a tree topology database and this tree structure is suitable
for a XML document tree. Users send XPath queries to a web servers. A web
server can process XPath queries by following the path tree of OAs. IrisNet has

30
A Distributed Publish/Subscribe System for Large Scale Sensor Networks
757
Fig. 30.3 IrisNet architecture
a load distribution mechanism based on structures of addresses. However, IrisNet
does not have data processing mechanism and also does not provide pushing
mechanism.
2.2.3
Global Sensor Network
Global Sensor Network (GSN) project [2, 3] is constructing the large scale sensor
network. GSN proves a common infrastructure for multi domain sensor networks.
GSN virtualize the several kind of sensors and absorb the difference of sensor.
Figure 30.4 shows architecture of GSN. GSN has virtual sensor manager layer
(VSM), query manager layer (QM) and GSN/Web/Web-service interface layer.
VSM includes Life Cycle Manager (LCM) and Input Stream Manager (ISM).
LCM provides data fetch from virtual sensor and proﬁle management of virtual
sensor. ISM provides resource management, stream data management, connection
between sensors, delay and fault detection. QM includes query repository (QR),
query processor (QP) and notiﬁcation manager (NM). QR provides ongoing query
management. QP parse SQL and execute the query. NM manages result of query.
GSN/Web/Web-service interface layer provides data access through Web. It can
extends by wrapper including HTTP wrapper, TinyOS wrapper, USB camera
wrapper, RFID wrapper, URP wrapper and serial communication wrapper.
Pushing mechanism may possible by wrapper but it is not clearly written. GSN
has stream data processing mechanism but it does not have a load distribution
mechanism of data processing.

758
M. Yamanouchi et al.
Fig. 30.4 Global sensor network architecture
2.3
Content-Based Network
2.3.1
Siena
We adopt push mechanism based on Content-Based Network (CBN). One of the
major studies on CBN ﬁeld is Scalable Internet Event Notiﬁcation Architectures
(Siena) [5, 6]. Siena provides Pub/Sub systems consisting of distributed servers.
Subscribers describe a ﬁlter or a pattern (combination of ﬁlters) as a subscription
rule and send it to a particular server. The server broadcast this subscription rule
and all servers know this rule. Through this broadcast process, a distribution tree
is created. Each server checks data from publishers and sends matched data with
subscription rules through distribution trees. If Siena handles simple raw data on
simple server topology, it works well, but Siena does not consider handles complex
data or data processing. As the same of GSN, Siena does not have a load distribution
mechanism of data processing.
2.3.2
Borealis
Borealis [1] is a second-generation distributed stream processing engine. They
constructed a distributed stream processing function based on Aurora [4] and
Medusa [25]. Figure 30.5 shows architecture of Borealis and Fig. 30.6 shows query
processor of Borealis. Local optimizer searches a bottleneck of performance.If local

30
A Distributed Publish/Subscribe System for Large Scale Sensor Networks
759
Fig. 30.5 Borealis architecture
Fig. 30.6 Borealis query processor
optimizer ﬁnd the bottleneck, local optimizer orders to priority scheduler, box
processor and load shedder to defrag. Priority scheduler decides box exececution
sequence by priority. Borealis adopt boxes-and-arrow model from Aurora. Box
shows query operator and arrow shows data ﬂow between boxes. All stream
can write by boxes and arrow. Box processor changes own processing function.

760
M. Yamanouchi et al.
Table 30.1 Summary of related works
Related works
Pushing mechanism
Scalability
Flexible calculation
Dynamic load sharing
Live E!




IrisNet




Global sensor
4

4

network
Siena




Borealis


4

 High, 4 Middle,  Low
Load shedder cut low priority query under high load. Neighborhood optimizer (NH)
optimize load between local and neighbor node from load information. Borealis has
scalability of calculation processing. But ﬂexibility of calcuelation is weak. And it
does not consider about burst high load.
2.4
Summary of Existing Systems
Live E! adopt distributed server architecture for load reduction. But Live E! servers
only provide raw data and do not provide pub/sub system or calculated data.
IrisNet has a load distribution mechanism based on structures of addresses. User
can reach the data easily by XPath query. However, IrisNet does not have data
processing mechanism and also does not provide pushing mechanism.
GSN virtualize the several kind of sensors and absorb the difference of sensor.
GSN has stream data processing mechanism but it does not have a load distribution
mechanism of data processing. Pushing mechanism may possible by wrapper but it
is not clearly written. Also GSN does not consider about dynamic load. In case of
sensor networks, sometime it will get a dynamic load. For example, weather sensor
network will get a tone of query when typhoon comes. Citizen want to know weather
data of own area.
These large scale sensor networks do not consider supporting push mechanisms.
They have scalability but it does not consider about ﬂexibility of calculation and
dynamic load sharing.
Siena provides Pub/Sub systems consisting of distributed servers. If Siena
handles simple raw data on simple server topology, it works well, but Siena does
not consider handles complex data or data processing. As the same of GSN, Siena
does not have a load distribution mechanism of data processing and dynamic load
sharing.
Borealis check a bottleneck of performance and optimize the bottleneck. Borealis
has scalability of calculation processing. But ﬂexibility of calculation is weak. And
it does not consider about dynamic load sharing.
Siena and Borealis has pushing mechanism. But Siena does not have scalability.
Both CBN does not have ﬂexibility of calculation and dynamic load sharing.
Table 30.1. shows summary of related work.

30
A Distributed Publish/Subscribe System for Large Scale Sensor Networks
761
3
Design of Our System
A purpose of this work is to process published data on scalable content-based
network. Scalable includes dynamic high load. On our proposed system, routers
deﬁne routing paths from publishers to subscribers and create data components
which have both publish and subscribe mechanism. Data components processes
sensor data, so that our system provides data processing mechanism besides
publish/subscribe mechanism. In this section, we describe a methodology of the
proposed system and its features.
3.1
Overview of System
Figure 30.7 shows overview of our proposed system. Our system has two types of
node. Router node and management node. Router node has calculation component
and message transfer function. Router node process the calculation and transfer
the data to subscribers by registered subscription. Management node accept the
subscription from users and register to the appropriate router node. Management
node select appropriate router node by calculation load, upload frequency, number
of sensors, duplication of subscription. Management node manages division and
combine of data component. This is the key mechanism of our system that
mechanism can process dynamic high load. Management node also hold publisher
Fig. 30.7 Overview of system

762
M. Yamanouchi et al.
Fig. 30.8 System
architecture
information, subscriber information and subscription information. Figure 30.8
shows the system architecture. Our proposed system is based on CBN. CBN pri-
marily has a function that manages message transfer from publishers to subscribers.
Routing paths are deﬁned by broadcast tree. Independent of this function, our
proposed system manages assignment of data components. Data components have
both a publishing function and a subscribing function. Data components subscribe
several published data and process these data in order to republish more valuable
data. For example, data components ﬁlter lots of rain data from publishers and alert
heavy rain messages in a particular area.
3.2
Process/Data Flow
Figure 30.9 shows process ﬂow of our system. Figure 30.10 shows example of
process ﬂow. User 1 register a subscription to management node that subscription
written about request of temperature data in Ikoma city. Management node select
an appropriate router (router A) to set a subscription from publisher information.
Router A allow the temperature data to router C. But Router A deny other all data
that data is not written in subscription. Router C delivery the temperature data of
Ikoma city to User 1 through Router D and Router F.
3.3
Data Format and Subscription Rule
Data formats of raw sensor data and calculated data are the same. This is because,
subscribers receive data from publishers, not considering differences between raw
sensor data and calculated data. This abstraction is useful to ﬁlter or calculate
data repeatedly through several subscription rules. Figure 30.11 is an example of
data format. “dataID” means sequence number of sensor data. “dataID” is useful

30
A Distributed Publish/Subscribe System for Large Scale Sensor Networks
763
Fig. 30.9 Process ﬂow
Fig. 30.10 Example of process ﬂow

764
M. Yamanouchi et al.
Fig. 30.11 Data format
to manage the order of sensor data and we have plan to implement traceability
mechanism into our system with “dataID”. We determine the data format, referring
the Live E! [16] and sMAP [9] data formats. Live E! data format mainly handle
weather information and sMAP has a concept that data format should be simple
and easy to use. Users (subscribers) make subscription rules consisting of mainly
“publishers”, “ﬁlter” and “calculation”. Figure 30.12 is an example of subscription
rules. Users deﬁne a observation area by “publishers” tag. The structure of
“publishers” tag is array and users select publishers by location, ID and sensor
types. If users need data processing, they use “ﬁlter” tag and “calculation” tag. In
“ﬁlter” tag, users deﬁne ranges and sensor types. In “calculation” tag, users select
calculation operators. Proposed system provides ﬁve operators (average, max, min,
count, range). On our system, data processing is represented by combination of
“ﬁlter” tags and “calculation” tags. “ﬁlter”/“calculation” tag determines just one
next process by “next” tag. In other words, data processing never branches out more
than one “ﬁlter”/“calculation”. Because combinations of “ﬁlter” and “calculation”
does not have ramiﬁcation, it is easy to divide these combinational processes into
several parts. If subscription rules are creates, data components manage these rules
and trans fer or calculate sensor data based on rules.
3.4
Division of Data Component
Data components enhance utility value of sensor data on CBN. On the contrary, it
is possible that data components cause high-loaded situations, because many pub-
lished data gather on a data components and the data components process/republish
these data. In addition, routers around the data components have to transfer
many messages. We should consider load distribution on data components besides
optimization of data components assignment. If a data component is high-loaded,

30
A Distributed Publish/Subscribe System for Large Scale Sensor Networks
765
Fig. 30.12 Subscription rule
the data component is divided based on ‘publishers’ and combination of ‘ﬁlter’,
‘calculation’ tags. The procedure of this division is shown as follows.
1. High-loaded data component (DC-A) divides own subscription rule
2. DC-A sends divided these rules to around components
3. Around components process a part of data
4. DC-A subscribes processed data from around components
5. DC-A publishes the same data and DC-A’s load drops
For example, if a high-loaded data component subscribes a square region, the
data component divides this region into four regions by dividing ranges of lati-
tude/longitude into equals halves. Another way of division is that a data component
divides a subscription region by its address. Address has a hierarchical structure
(e.g., a city has some towns). Data components use this hierarchical structure for

766
M. Yamanouchi et al.
Fig. 30.13 Calculation
format
divisions. Sensor network often gets a dynamic high load. For example, weather
sensor network will get a tone of query when typhoon comes. Citizen want to
know weather data of own area. It generates a dynamic high load but dividing the
data components reduce the load. Processing the dynamic high load is important
mechanism for sensor networks.
3.5
Calculation/Filter in Data Component
Calculation format and ﬁlter format are similar. Difference between calculation
and ﬁlter format is “type” and “time”. The attribute of type in calculation format
is “calculationType” and ﬁlter format is “ﬁlterType”. calculation format needs to
designate “time” attribute but ﬁlter does not need “time” attribute. Figure 30.13
shows example format of calculation. “publishersNum” designate the data to
process which data is published by whom. “calculationTarget” can select calculation
target from “value” and “timestamp”. “time” sets time span of calculation data. Next
process will written in “next” attribute. Calculation format and ﬁlter format can nest
the process. It will execute by nesting order.
3.6
Management Node
Management node stores publisher, subscriber, subscription and router node infor-
mation. Management node also has four basic functions, provide stored information,
decide subscription ID, divide subscription process, move and set subscription.
Management node select appropriate router node by router node information
(calculation load, upload frequency, number of sensors, duplication of subscription)
when user register a subscription to management node. Management node check
router node periodically and refresh own router node information up-to-date. If
router node fall into high load, management node catch the trouble and solve the
high load by dividing or moving the subscriptions. Management node work as
commander for our CBN.

30
A Distributed Publish/Subscribe System for Large Scale Sensor Networks
767
4
Evaluation
This section shows performance evaluations about divisions of data components.
Load distribution is one of the most important mechanism on our proposed system.
4.1
Divisions of Data Components
4.1.1
Experiment Environment
Figure 30.14 shows topology of experiment. Table 30.2 shows speciﬁcation of each
node. We generate the dummy data on virtual sensor and send to router C. We put
the ﬁlter subscription to router C. Router C drop the data from virtual sensor which
data matches to subscription rule. Other data will forward to router B. We increase
a router C to divide the calculation process. Router B has a average subscription.
Router B calculates average from data that router C forwarded. Router B forward
the data to router A after calculation. We take a CPU load and response time to
evaluate the performance. Figure 30.15 shows functions in router C. We implement
a initial function and ﬁltering function to router C. We measure a response time
without initial function that shows in Fig. 30.15. Initial function affect the CPU load
of router C. In case of 5,000 data/s, initial function affect 1% of CPU load. In case
of 35,000data/s, initial function affect 11% of CPU load. This means system limit
is little bit higher than our result but it is about the same.
Fig. 30.14 Topology of experiment
Table 30.2 Node
speciﬁcation
Node
CPU
Memory
OS
Router C
Intel Core 2 Duo 2.00 GHz
4 GB
ubuntu 11.04
Router A & B
Intel Core 2 Duo 2.66 GHz
4 GB
Mac OS X
Virtual sensor
AMD Athlon64X2 2.60 GHz
4 GB
Windows7

768
M. Yamanouchi et al.
Fig. 30.15 Functions in router C
Fig. 30.16 CPU load of single node
4.1.2
Result of Experiment
We measure a CPU load, packet loss rate and response time on no division, 2
division, 4 division for evaluation. Figures 30.16–30.18 shows CPU load, packet
loss rate and response time of no division. Figures 30.19–30.21 shows CPU load,
packet loss rate and response time of 2 division. Figures 30.22–30.24 shows CPU
load, packet loss rate and response time of 4 division. On this experiment, we

30
A Distributed Publish/Subscribe System for Large Scale Sensor Networks
769
Fig. 30.17 Packet loss rate of single node
Fig. 30.18 Response time of single node
use 100 virtual sensors and each sensor generates 50–1,400data/s each 50 data/s
interval. We omitted some data on ﬁgure. 5,000data/s in ﬁgure means 100 virtual
sensors and 50 data/s at each sensor. Figure 30.16 shows 35,000data/s is the limit of
no division. Figure 30.19 shows 70,000data/s is the limit of 2 division. Figure 30.22
shows 14,000data/s is the limit of 4 division. Other ﬁgure also shows similar result.
Figure 30.25 shows comparison of response time after initialize and stabled.

770
M. Yamanouchi et al.
Fig. 30.19 CPU load of 2 division
Fig. 30.20 Packet loss rate of 2 division
4.1.3
Consideration
All Figure shows it takes few second to stabilization. We implemented by JAVA.
Probably it related to socket initialization in JAVA. CPU load get into unstable on
over load situation. It cause high rate of packet loss. In ﬁgure of packet loss, packet
loss rate goes to minus. It exceed the maximum data of generation. For example,

30
A Distributed Publish/Subscribe System for Large Scale Sensor Networks
771
Fig. 30.21 Response time of 2 division
Fig. 30.22 CPU load of 4 division
ﬁgure shows 200% when virtual sensor generates 5,000data/s but also router A
receive 15,000 data/s. Packet stack while initializing the socket. All stacked packet
will send after initialize. It cause a minus of packet loss rate.
Figure 30.25 show a linear increase with division. It means our proposed system
capable to dynamic load and scalability. One of the news site of Japan, Yahoo!

772
M. Yamanouchi et al.
Fig. 30.23 Packet loss rate of 4 division
Fig. 30.24 Response time of 4 division
Japan has a web page of earthquake. Normally earthquake page count 20,000 page
view per second. But access increase to more than 250,000 page view per second
during the earthquake which earthquake occurred in Nigata 2006 [19]. Our system
can delivery the sensor data in 10 s on normal situation. Our system can also handle
dynamic load by dividing to eight data components and it does not change response
time.

30
A Distributed Publish/Subscribe System for Large Scale Sensor Networks
773
Fig. 30.25 Comparing response time
5
Conclusion
Recently, many sensor devices are installed everywhere and it is required that large
sensor networks managing these sensor data. However almost existing systems only
manage raw sensor data and these data are provided by pull technology. Then,
we add data processing components onto content-based networks. Content-based
networks provide publish/subscribe system in distributed environment and appro-
priate for notiﬁcations and alerts. Publish/subscribe mechanism and data processing
components enhance the application ﬁelds in ubiquitous sensing environment. In
this paper, we describe how to add data processing components onto content-based
networks. It is possible that data processing components causes concentrations of
sensor data, however we distribute the load of data components by dividing sub-
scription rules based on geographical locations or address structures. Performance
evaluations of our implementation shows that load distribution mechanism works
well, and proposed system secure the scalability by adding data components. It
means our proposed system has possibilities of dynamic load distribution. As the
future works, we should optimize data components assignment, considering not only
path lengths but message trafﬁc, RTT and other criteria.
References
1. Abadi, D.J., Ahmad, Y., Balazinska, M., Cetintemel, U., Cherniack, M., Hwang, J.H.,
Lindner, W., Maskey, A.S., Rasin, A., Ryvkina, E., Tatbul, N., Xing, Y., , Zdonik, S.: The
design of the borealis stream processing engine. In: 2nd Biennial Conference on Innovative
Data Systems Research (CIDR’05) (2005)

774
M. Yamanouchi et al.
2. Aberer, K., Hauswirth, M., Salehi, A.: Global sensor networks. Tech. rep., EPFL (2006)
3. Aberer, K., Hauswirth, M., Salehi, A.: A middleware for fast and ﬂexible sensor network
deployment. In: U. Dayal, K.Y. Whang, D.B. Lomet, G. Alonso, G.M. Lohman, M.L. Kersten,
S.K. Cha, Y.K. Kim (eds.) VLDB, pp. 1199–1202. ACM (2006). URL http://dblp.uni-trier.de/
db/conf/vldb/vldb2006.html#AbererHS06
4. Carney, D., etintemel, U.C., Cherniack, M., Convey, C., Lee, S., Seidman, G., Stonebraker, M.,
Tatbul, N., Zdonik, S.: Monitoring streams - a new class of datamanagement applications. In:
VLDB Conference (2002)
5. Carzaniga, A., Rutherford, M., Wolf, A.: A routing scheme for content-based networking. In:
Proceedings of IEEE INFOCOM 2004, pp. 918–928 (2004)
6. Carzaniga, A., Wolf, A.: Forwarding in a content-based network. In: Proceedings of the
2003 conference on Applications, technologies, architectures, and protocols for computer
communications, pp. 163–174 (2003)
7. Chen, J., Kher, S., Somani, A.: Distributed fault detection of wireless sensor networks. In:
DIWANS ’06: Proceedings of the 2006 workshop on Dependability issues in wireless ad
hoc networks and sensor networks, pp. 65–72. ACM Press, New York, NY, USA (2006).
DOI 10.1145/1160972.1160985. URL http://dx.doi.org/10.1145/1160972.1160985
8. Chen, J., Kher, S., Somani, A.: Distributed fault detection of wireless sensor networks. In:
Proceedings of the 2006 workshop on Dependability issues in wireless ad hoc networks and
sensor networks, p. 72. ACM (2006)
9. Dawson-Haggerty, S., Jiang, X., Tolle, G., Ortiz, J., Culler, D.: smap - a simple measurement
and actuation proﬁle for physical information. In: Eighth ACM Conference on Embedded
Networked Sensor Systems (SenSys ’10) (2010)
10. Doui, S., Matsuura, S., Fujikawa, K., Sunahara, H.: Overlay network considering the time and
location of data generation. In: The 2007 International Symposium on Applications and the
Internet, SAINT2007, DAS-P2P 2007 (2007)
11. Franklin, M.J., Jeffery, S.R., Krishnamurthy, S., Reiss, F., Rizvi, S., 0002, E.W., Cooper, O.,
Edakkunni, A., Hong, W.: Design considerations for high fan-in systems: The hiﬁap-
proach. In: CIDR, pp. 290–304 (2005). URL http://dblp.uni-trier.de/db/conf/cidr/cidr2005.
html#FranklinJKRR0CEH05
12. Gibbons, P.B., Karp, B., Ke, Y., Nath, S., Seshan, S.: Irisnet: An architecture for a
worldwide sensor web. IEEE Pervasive Computing 02(4), 22–33 (2003). DOI http://doi.
ieeecomputersociety.org/10.1109/MPRV.2003.1251166
13. Han, C.C., Kumar, R., Shea, R., Srivastava, M.: Sensor network software update management:
a survey. International Journal of Network Management pp. 283–294 (2005)
14. JMA: AMeDAS. http://www.jma.go.jp/jp/amedas/ (2011)
15. Kuramitsu, K.: Discovering periodic unusualness in sensor data stream. In: IPSJ SIG Technical
Report, pp. 7–10 (2004)
16. LiveE!: Live E! Project. http://www.live-e.org/ (2011)
17. Matsuura, S., Fujikawa, K., Sunahara, H.: Mill: An information management and retrieval
method considering geographical location on ubiquitous environment. In: SAINT Work-
shops, pp. 14–17. IEEE Computer Society (2006). URL http://dblp.uni-trier.de/db/conf/saint/
saint2006w.html#MatsuuraFS06
18. Matsuura, S., Fujikawa, K., Sunahara, H.: Applying overlay networks to ubiquitous sen-
sor management. In: T. Hara (ed.) WSN Technologies for the information Explosion,
pp. 231–247. Springer (2010)
19. Nikkei: ITPro. http://itpro.nikkeibp.co.jp/article/COLUMN/20060227/230846/ (2011)
20. Papadimitriou, S., Brockwell, A., Faloutsos, C.: Adaptive, hands-off stream mining. In:
VLDB2004, pp. 560–571 (2004)
21. Sgroi, M., Wolisz, A., Sangiovanni-Vincentelli, A., Rabaey, J.: A service-based universal
application interface for ad hoc wireless sensor and actuator networks. In: W.W. (Inﬁneon),
J.R.U. Berkeley), E.A. (Philips) (eds.) Ambient intelligence. Springer Verlag (2005)

30
A Distributed Publish/Subscribe System for Large Scale Sensor Networks
775
22. Shneidman, J., Pietzuch, P., Ledlie, J., Roussopoulos, M., Seltzer, M., Welsh, M.: Hourglass:
An infrastructure for connecting sensor networks and applications. Tech. rep., Harvard
University (2004)
23. WIDE: WIDE Project. http://www.wide.ad.jp (2011)
24. Yamanouchi, M., Matsuura, S., Sunahara, H.: A fault detection system for large scale sensor
networks considering reliability of sensor data. In: Proceedings of the 9th Annual International
Symposium on Applications and the Internet(SAINT2009) (2009)
25. Zdonik, S., Stonebraker, M., Cherniack, M., etintemel, U.C., Balazinska, M., Balakrishnan, H.:
The aurora and medusa projects. IEEE Data Engineering Bulletin (2003)


Index
A
Abstract clocks
ActivationClock type, 342, 343
afﬁne clock relation, 341–342
deﬁnitions, 339–341
Gaspard2, 345
N-synchronous Kahn networks, 345
pipelined execution, 344
synchronous dataﬂow programs, 344
synchronous reactive programming, 339
timedProcessing stereotype, 343
Timesquare simulation tool, 344
AFS. See Andrew ﬁle system
Allocation (Alloc) package, 325
Amazon Web services, 512–513
American Recovery and Reinvestment Act
(ARRA), 42
Andrew ﬁle system (AFS), 112–113
Application programming interface (API), 472
Arbitrary semantic SQL updates, 440–443
ARRA. See American Recovery and
Reinvestment Act
Array-oriented language
afﬁne array, 589
array tiling, 590
formalism, 588–589
HFLR ﬁlter, 589, 590
LPSF ﬁlter, 589, 590
refactoring transformations, 589
ASKAP. See Australian SKA Pathﬁnder
Asymmetric cryptography algorithm, 488
Atomicity, consistency, isolation, durability
(ACID), 9
Attribute migration tool (AMT), 85
Audiovisual (AV) data, 667
data decomposition, 670, 673
input stream, 674–675
output stream, 676
splittable compression codec, 674
Australian SKA Pathﬁnder (ASKAP), 43, 44
Authorization and Access (A&A) control, 449
AV data. See Audiovisual data
B
Berkeley Storage Manager (BeStMan), 531
Boolean retrieval model, 740
Borealis query processor, 758–760
Building decision model, 400
C
Can pay cloud providers (cCPs), 172
cooperative game, 183, 184
HDCF system, 186
non-cooperative game, 183, 184
social welfare, 183, 185
total proﬁt, 183
total utility, revenue function, 183, 185
CBN. See Content-based network
cDNA. See Complementary DNA
Cloud computing systems, 109, 115–116,
452–453
accelerate step, 133
Amazon clouds’ cost model, 131
cost beneﬁt, 131
datasets storage strategy
benchmarking algorithms, 143
cost rate based storage strategy,
138–139
CostRi, 137–138
CTT construction, DDG, 144–145
B. Furht and A. Escalante (eds.), Handbook of Data Intensive Computing,
DOI 10.1007/978-1-4614-1415-5, © Springer Science+Business Media, LLC 2011
777

778
Index
Cloud computing systems (cont.)
CTT-SP algorithm, minimum cost
benchmarking, 145–147
data provenance and DDG, 135–136
generation costs and usage frequencies,
138
local-optimization based storage
strategy (see Local-optimization
based storage strategy)
provSets, 137
scientiﬁc application data classiﬁcation,
134
SwinCloud system (see SwinCloud
system)
total cost, 136
de-dispersion ﬁles, 132
Nectar system, 131
Parkes Radio Telescope, 132
pay-as-you-go model, 130
problem analysis and research issues,
133–134
pulsar searching workﬂow, 132
scientiﬁc datasets, 129
virtualization technology, 130
Colorado River Basin (CRB), 49
Common Astronomy Software Applications
(CASA), 47
Complementary DNA (cDNA), 685–686
Computation and storage trade-off. See Cloud
computing systems
Contemporary clustering, 286
adjustments, 294–295
assessment, 290, 295
density-based clustering, 288–289
ensemble clustering (see Ensemble
clustering)
hierarchical clustering, 289–290
partitional clustering, 287–288
quality measures, 293–294
visualization techniques, 294
Content-based network (CBN)
Borealis architecture, 758–760
Siena, 758
Content selection methodology
content dimensions, 625–626
content set generation, 626
diversity spectrum, 624
Converged network adapter (CNA), 11
Cooperative game
distributed algorithm, 180–181
KKT-conditions, 180
Lagrangian multiplier, 178
pCP and cCPs beneﬁts, 178
total proﬁt, 179
Core scientiﬁc meta-data model (CSMD),
266–268
Cost transitive tournament shortest path
(CTT-SP) algorithm
CostCPU, 141
linear DDG, 140–141
minimum cost benchmarking, 145–147
pseudo-code, 141, 142
strategy rules, 142–143
sub linear DDGs, 143
Cryptography security algorithm, 488, 489
CSMD. See Core scientiﬁc meta-data model
CTT-SP. See Cost transitive tournament
shortest path algorithm
Customer relationship management (CRM),
380
D
Database management systems (DBMS), 742
Database scalability service provider (DSSP),
458
Database systems
ACID rules, 456
Amazon S3, 458
DSSP, 458
Hadoop, 457, 458
Java library, 458
MapReduce architecture, 457
NoSQL, 456, 457
Unix security model, 458
Data center interconnect network (DCINs)
layer 2 and 3 and storage connectivity, 17
layer 2 extension, 18–19
layer 4 network services, 21
location and application-ID separation,
20–21
network requirements, 17–18
Data center network (DCN)
access layer, 11
aggregation layer, 11
bandwidth uniformity, 12–13
core layer, 11
end-of-row switch, 11
enterprise-class business applications, 10
ﬂat network topology and multi-pathing,
15–16
L2/L3 networking product, 12
low latency, 13–14
multi-layer topology, 12
non-interfering network, 15
reliability and utilization, 14
tiered data centers, 17
top-of-rack switch, 11

Index
779
traditional data center network architecture,
10
tree-like topology, 12
Data-centric programming language, 87
Data cloud computing
Amazon Web Services, 512–513
community cloud, 503
data accountability, 506
data availability, 506
data conﬁdentiality, 505
data integrity, 505–506
data placement, 507, 516–517
data provenance, 506
data remnance, 507
dynamic resource allocation model, 502
E-banking, 508–509
E-government, 510–511
E-health, 509–510
Google AppEngine, 513
hybrid cloud, 503
IaaS, 504
Jericho Cloud Cube model, 504–505
Linthicum model, 504
Microsoft Azure, 514
PaaS, 503–504
private cloud, 502
Proofpoint, 514–515
public cloud, 502
Rackspace, 516
SaaS, 503
Salesforce, 515
security and privacy requirements, 511–512
SLA, 501–502
Sun open clouds, 515
threat modelling, 507–508
trust management, 517
types, 502
video surveillance services, 510
Data cloud web services, 315–317
Data conﬁdentiality and integrity, 485, 487
Data decomposition
audiovisual content, 670, 673
RGB-based raw data stream, 671, 672
video ﬁle formats, 671
Data dependency
abstract clocks (see Abstract clocks)
characteristics, data intensive applications,
323–324
data read/write and manipulation, 323
Gaspard2, 325
Marte design packages, 325–326
modeling paradigm, 326
monodimensional data speciﬁcation
models, 326–327
MPI, 324
multidimensional data speciﬁcation
models, 327–328
RSM, 324 (see also Repetitive structure
modeling)
SoCs, 324
Data dependency graph (DDG), 130
Data-Grey-Box Web Services (DGB-WS), 312
Data integrity, 255, 505–506
Data intensive analysis
computational science, 249–250
crystallography analysis workﬂow, 256
data and repetition rates, 256–257
data collection, co-analysis, 259–260
data exchange, sharing and publication,
273–274
data formats, 254
data integrity, 255
data management challenges, 251
data management practices (see Data
management practices)
data ownership and citation, 262–263
data provenance, collaboration and sharing,
260–262
data rates, 250
data volumes, 257
eco-system support, 263
EMBL, 250–251
experimental science, 252
high level framework, 275–276
image informatics, 275
leadership computing facilities, 258
long term perspective, 278–280
MEDICi workﬂow system, 271, 276–277
metadata generation and association, 253
neutron instrument simulation package,
258
Nucleic Acids Research, 251
open-source cloud, 272
ORNL’s SNS, 257–258
parallelized algorithms, 272–273
proteomics mass spectrometry pipeline,
PNNL, 271
science-enabling tools and technologies,
257
spectrum beneﬁts, 271–272
standardization efforts, 269–270
Tech-X Corporation’s open source tool
Orbiter, 277–279
UFnet, 277
Data intensive clouds
CPs, 171, 172
data intensive pCP with cCPs, 172, 174
high bandwidth sensor systems, 173

780
Index
Data intensive clouds (cont.)
MMORPGs, 173–174
social networks, 173
Data intensive computing systems
complete architectures, 459–460
database systems
ACID rules, 456
Amazon S3, 458
DSSP, 458
Hadoop, 457, 458
Java library, 458
MapReduce architecture, 457
NoSQL, 456, 457
Unix security model, 458
data intensive applications
data warehousing, 455
ﬁnancial systems, 456
medical information, computerization
and exchange, 454–455
scientiﬁc collaboration, 454
social networks, 455
privacy, 450
security
A&A, 449
analysis stage, 461
conﬁdentiality/secrecy, 448
denial of service, 448
design stage, 461–463
direct attacks, 448
domain analysis stage, 460–461
I&A, 449
IDS, 449
information hiding, 449
integrity, 448
logging and auditing, 449
malware, 448, 450
method of operation, 449
misuse activities approach, 450
non-repudiation, 448
reference architecture/domain-speciﬁc
architecture, 460
requirements stage, 461
role-based access control, 460
secure systems design methodology,
460, 461
threats, 449
vulnerability, 449
web architectures
cloud computing, 452–453
grid computing, 452
REST, 451
SOA, 451–452
Data-intensive software systems
architectural challenges, 27, 28
canonical software architecture techniques,
27
constrained queries, 36–37
consumption and (re-)use, 33
data capturing and generation, 26
data curation, 32–33
data dissemination, 28–29, 31–32
data processing and analysis, 37–39
guided queries, 36
information modeling, 28, 34, 39–40
massive data manipulation, 25
metadata search, 29
middleware
Apache OODT, 53–54
DSSA, 51
OODT, 51, 52
NASA Earth science decadal missions, 26
open queries, 36
processing and resource management, 34
query handler, 37
representative science data systems (see
Representative science data systems)
research-grade code, 35
scientiﬁc algorithms, 26
search feature, 35
total volume, 30–31
workﬂow management component, 29
Data management practices
core data management infrastructure
components, 268–269
CSMD, 266–268
harmonized data policies, 265
ICAT, 267–268
LCLS, 264
life cycle support, 266, 267
PaN-data policy document, 265
SDM, 266
security objectives, 264
US user facilities, 266
Data parallelism, 582, 587
Data retrieval process, 477–478
Data security and privacy
access controls and authorization levels
account policy enforcement, 472
audit trail, 473
data and code distribution, 471
ETL engine, 472
least privilege principle, 472
streamlining account management, 472
data at rest, security, 473–475
data in transit, security, 475–476
data retrieval process, 477–478
data transformation code/algorithm,
476–477

Index
781
data value classiﬁcation and data labeling,
470–471
human resources, 469–470
individual record retrieval and data extracts,
478–479
network threat and vulnerability
management, 480–481
secure design, 468–469
security auditing, 482
Data transfer and storage micro-architecture
exploration, 584–585
communicating processors
data block size, 595
double buffering mechanism, 594, 595
external memory, data transfer, 595–596
parallelism, scaling, 597–598
temporal behavior, 598–600
two processors, data transfer, 596–597
single processor, 593–594
Data transformation code/algorithm, 476–477
Data warehousing, 455
DCIN. See Data center interconnect network
DCN. See Data center network
Declarative programming language, 86, 91
Dendrogram, 289
Design space exploration (DSE) approach
array-OL model, 603–605
exploration space reduction, 605–606
granularity, 605
optimization criteria, 602–603
Digital object identiﬁers (DOIs), 262–263
Distributed hash tables (DHTs), 113
Distributed publish/subscribe system
broadcast tree, 762
calculation/ﬁlter, 766
CBN
borealis architecture, 758–760
Siena, 758
CPU load, 768–770
data format and subscription rule, 762–764
experiment environment, 767–768
high-loaded data component (DC-A), 765
JAVA, 770
key mechanism, 761
large scale sensor networks
GSN, 757–758
IrisNet, 756–757
Live E!, 755–756
load distribution, 767
management node, 761, 766
packet loss rate, 769
process/data ﬂow, 762
pull technology, 753
research target, 754–755
router node, 761
system architecture, 762
XPath query, 760
Document ranking, 740–742
Document retrieval, 740
DOIs. See Digital object identiﬁers
DSSP. See Database scalability service
provider
Dynamo system, 116
E
Earth system grid (ESG), 531–532
ECL. See Enterprise control language
ECMP. See Equal cost multipath
Electronic design automation (EDA) tools,
582–584
Electronic system level (ESL) design, 583
EMBL Nucleotide Sequence Database. See
European Molecular Biology
Laboratory Nucleotide Sequence
Database
Ensemble clustering
assessment, 292–293
consensus-clustering, 290
multiple algorithms, 290
pairwise-similarities approach, 291–292
Ensemble feature ranking methods
ANOVA, 372–373
attribute noise, 351
clean scenario, 364–365
combination method, 358
correlations, 367–370
dataset characteristics, 360–361
ﬁlter-based feature ranking techniques, 349
focused ensembles, 359–360
general ensembles, 359
Kendall Tau correlations, 350
KRC, 362, 364
machine learning, 351
noise injection mechanism, 361–363
noisy scenario, 365–366
nonrandom feature selection approach, 351
optimized feature selection approach, 351
random feature selection approach, 351
robustness, 369, 371
corrupted and clean scenarios, 369
mean absolute error, 372
measures, 372
noise distribution, 370
standard ﬁlter-based feature ranking
techniques
chi-squared statistic, 353
gain ratio, 353–354

782
Index
Ensemble feature ranking methods (cont.)
information gain, 353
ReliefF, 354
ReliefF-W, 354
symmetric uncertainty, 354–55
TBFS (see Threshold-based feature
selection techniques)
Enterprise control language (ECL)
advantages and key beneﬁts, 86–88
compilation
execution, 78–79
generation, 78
optimizations, 77
parsing, 76–77
transformation, 77–78
XML representation, 75
data analysis and mining applications, 72
development tools and user interfaces
AMT, 85
ECLWatch web-based utility, 85, 86
Query Builder IDE application, 82, 85
Windows-based GUI, 82
features and capabilities
attribute deﬁnition, 73
C++ code, 73
JOIN operation syntax, 74
LOCAL mode, 75, 76
local SORT operation, 74–75
parallel programming language, 72
PARSE function, 75
PATTERN statements, 75
Thor system, 74
transform functions, 73
Google, 72
log analysis programming
data transformation, 79, 81
execution graph, 79, 81
log data, 79
log ﬁle analysis job, 82, 84
log ﬁle output format, 81, 83
macro ﬁle, 79, 80
output ﬁle, 81, 82
MapReduce programming model, 71
vs. Pig feature, 92–101
benchmark results, PigMix, 103
data-parallel execution, 90
Eclipse plug-in, PigPen, 92
Hadoop installations, 90
Hadoop MapReduce environment, 92
ITERATE and DISTINCT operation, 91
PigMix Test L3, 102
PigMix Wiki page, 103
ROLLUP and DEDUP operation, 91
SORT algorithm, 104
source code, 91
tuples, 91
Pig Latin programs, 72
vs. SQL, aggregated data analysis, 88–90
Workqueue, 72
Entropy distortion minimization technique,
630
Equal cost multipath (ECMP), 14, 16
ESFRI. See European Strategy Forum on
Research Infrastructures
European Molecular Biology Laboratory
(EMBL) Nucleotide Sequence
Database, 250–251
European Strategy Forum on Research
Infrastructures (ESFRI), 278–279
EVLA. See Expanded very large array
Expanded very large array (EVLA), 47, 48
External linking process
authority ﬁle, 226
batch mode external linking, 228
entity resolution, 225, 228, 229
keybuild process, 227
LINKPATH statement, 225, 226
online query, 225, 228, 229
reference ﬁeld, 228
Roxie service, 228
UBER key, 226
F
Fair Information Principles (FIC), 517
Flexible image transport system (FITS),
528–529
G
Gain ratio (GR), 688
Gaussian orthogonal ensemble (GOE), 714,
716–717
Gene co-expression networks
conﬁdent biological hypothesis, 716
microarray proﬁles, 718–720
microarray technology, 715
Poisson ensemble, sharp transitions,
716–717
statistical testing vs. randomized
expressions, 720–721
Generalization multidimensional synchronous
dataﬂow (GMDSDF), 327, 328
General Relativity (GR) theory, 44–45
Generic Component Model (GCM ) package,
325
Gene selection and ﬁltering
benchmark gene expression data sets, 570

Index
783
data intensive computing environments,
570
ﬁlter and wrapper approaches, 564
machine learning methods, 564
microarray gene expression data, 564
PReliefFg
gene quality, 574
k nearest neighbors, 575
Leukemia data set, 575, 576
Lung cancer data set, 575, 577
parallel gene selection algorithms,
567–570
support vector machines, 575
TIS data set, 575, 578
PReliefFp
gene quality, 574
k nearest neighbors, 567
Leukemia data set, 575, 576
Lung cancer data set, 575, 577
parallel gene selection algorithms, 567
support vector machines, 575
TIS data set, 575, 578
ReliefF algorithm, 565–567
runtime evaluation
efﬁciency, 571, 575
execution times, 571, 575
Leukemia Data Set, 571, 572
Lung Cancer Data Set, 571, 573
TIS Data Set, 571, 574
Geosciences
block entropy, 521
cluster analysis, 521
data access
DataTurbine, 530–531
ESG, 531–532
Hadoop MapReduce, 533–534
Kepler, 534–535
OPeNDAP, 529
PostGIS, 529–530
SciDB, 532–533
data process
data analysis, 525–526
data capture, 524
data collection, 523–524
visualization process, 525
FITS, 528–529
geo-data, 522
geo-phenomena, 521
HDF5, 528
interoperability and data integration, 522
N-dimensional array model, 527–528
NetCDF, 528
object-based model and ﬁeld-based model,
526–527
Geospatial data management
auto pilot, 650–651
cutting-edge technology, 638
data dispenser, 651–652
disaster mitigation and response, 653, 654
GIS-INTEGRO system, 661
GIS tools, 638
government and public interest, 657–661
real estate, 655–657
research and scientiﬁc inquiry, 653–655
TerraFly (see TerraFly)
time series visualization, 648, 649
fading-in and -out effect, 648
SO service, 647
spatio-temporal data overlays, 647
time-line panel, 647
travel and tourism, 657, 658
GFS. See Google ﬁle system
Gini index (GI), 691–692
Girvan–Newman algorithm, 722
Globally unique identiﬁer (GUID), 114
Global sensor network (GSN), 757–758
GMDSDF. See Generalization
multidimensional synchronous
dataﬂow
GOE. See Gaussian orthogonal ensemble
Google App Engine, 161–162, 513
Google ﬁle system (GFS), 110, 115–116
Google MapReduce framework, 157–159
Graphical Array Speciﬁcation for Parallel and
Distributed Computing (Gaspard2),
325
Grid computing, 452
Grid secure storage system (GS3/ termination
phase algorithm, 490, 491
GR theory. See General relativity theory
GSN. See Global sensor network
H
Hadoop distributed ﬁle system (HDFS), 7, 8,
110, 669, 670
Hadoop storage architecture, 7–8
Hardware Resource Modeling (HRM) package,
325
HDFS. See Hadoop distributed ﬁle system
Hierarchical data format 5 (HDF5), 528
High dimensional DNA microarray data
Affymetrix U-133 Plus 2.0 gene chip, 686
breast, colon and lung cancer, 685

784
Index
High dimensional DNA microarray data (cont.)
cardiovascular disease, 685
cDNA, 685–686
classiﬁcation
best learner and percentage features,
702, 703
cross validation method, 693
feature set sizes, 693
k -nearest neighbors, 695, 697, 698,
700, 701
logistic regression, 695, 697, 699, 700,
702
MLP, 695, 697–699, 701
naive bayes, 694, 697, 699, 701
SVM, 694, 697, 698, 700, 702, 707
worst ﬁlter, 697, 703
comparative analysis algorithm, 696
DNA probes, 686
feature rankers
ﬁlter-based feature selection techniques,
687, 688
non-TBFS ﬁlter, 687–689
signal to noise technique, 687
TBFS (see Threshold-based feature
selection technique)
wrapper techniques, 687
ICMLA, 685
mRNA, 685
ranked list comparisons, 703, 708
breast vs. colon cancer, 704–706
lung vs. breast cancer, 706
lung vs. colon cancer, 706
High-performance cluster computing (HPCC),
4
advantages, 105
big data problem, 59, 61–62
commodity computing clusters, 65
database systems, 65
Data Delivery Engine, 67
data-intensive computing
applications, 60–61
cluster conﬁgurations, 62–63
data and programs collocation, 63–64
deﬁnition, 62
inherent scalability, 64
National Science Foundation, 62
programming model utilization, 64
reliability and availability, 64
data-parallelism, 61
data reﬁnery, 66
ECL (see Enterprise control language)
fundamental challenges, data-intensive
computing, 59
Hadoop clusters, 68
2 HPCC Thor system cluster, 69–70
LexisNexis approach, 65–66
MapReduce architecture, 65
open source implementation, 65
Roxie system cluster, 67, 68, 70–71
scalable platform, 65
Thor processing cluster, 67
High performance network architectures
client-server applications, 6
DCINs (see Data center interconnect
network)
DCN (See Data center network)
Hadoop storage architecture, 7–8
hardware platform, 3
IDC, 4
interactive online services, 5–6
Internet-scale applications, 4
large-scale data parallelism applications,
4–5
MapReduce paradigm, 21
MegaStore storage architecture, 8–9
multiple data centers, 9
Host identity protocol (HIP), 20
HPCC. See High-performance cluster
computing
Hubble Space Telescope (HST), 47
Hybrid MPI/OpenMP parallel ﬁnite element
simulation
data dependence, 546
domain decomposition strategy, 547
element-based partitioning, 548–550
end-to-end approach, 547
function-level performance analysis, 556,
557
loop-level ﬁne grain parallelism, 549
master thread, 551
mesh generation and model domain
partitioning, 547–548
modular programming approach, 546
MPI and OpenMP level parallelism, 550,
551
nodal forces, 552
process-level coarse parallelism, 549
three-level hybrid parallel programming
model, 547
I
ICAT. See Information catalogue
Identiﬁcation and Authentication (I&A), 449
Information catalogue (ICAT), 267–268
Information quality and relevance. See
Large-scale social information
systems

Index
785
Information retrieval (IR), 740–742
Infrastructure as a service (IaaS), 504, 667
Initiation interval, 593
Input Stream Manager (ISM), 757
Integer partition, 605
Internal linking process
entity type and unique identiﬁer, 221–222
INGESTFILE statement, 224
output results, 223
post iteration cluster statistics, 223, 224
record match decision, 222
rule-based systems, 222
THRESHOLD statement, 224
WsECL interface, 225
International Conference on Machine Learning
and Applications (ICMLA), 685
Internet engineering task force (IETF), 16
Intrusion detection systems (IDS), 449
Inverse participation ratio (IPR), 713–714, 722
IPR. See Inverse participation ratio
IR. See Information retrieval
J
Jaguar and JaguarPF system architecture, 553
Jericho Cloud Cube model, 504–505
K
Kahn process networks (KPN), 326–327
Kendall rank correlations (KRC), 362, 364
Kerckhoffs’ principle, 487
Keyword search
answer models
multiple-entity, 737–738
single-entity, 737
core tasks, 738–739
databases and information retrieval
BANKS models, 745
BLINKS, 744
DBXplorer, 745
DISCOVER, 745
graph model, 743
ObjectRank, 745
rooted tree semantic, 744
data models
semi-structured data, 736–737
structured data, 735–736
unstructured data, 736
deﬁnition, 735
efﬁcient and effective techniques, 734
IR
document ranking, 740–742
document retrieval, 740
query suggestion techniques, 746
relational databases, 742
SQL and XQuery, 734
tablet products, 733, 734
Web document collections, 733
Web search engine, 733
XML data, 733–734
Kolmogorov-Smirnov (KS) statistics, 691
Kosmos ﬁle system (KFS), 110
KPN. See Kahn process networks
L
Large and non-uniform media objects. See
MapReduce-based clusters
Large-scale data analytics
contemporary clustering, 286
adjustments, 294–295
assessment, 290, 295
density-based clustering, 288–289
ensemble clustering (see ensemble
clustering)
hierarchical clustering, 289–290
partitional clustering, 287–288
quality measures, 293–294
visualization techniques, 294
feedback-driven process
data cloud web services, 315–317
data-intensive process, 311
degree of satisfaction, 301
degree of similarity, 300
ETL tools, 314
ﬂexible consensus clusterings, 301–304
merge strategy, 296, 304
pairwise-similarities, 297
Pearson correlation coefﬁcient, 298
process-level extension, 313–314
process perspective, 317–319
reﬁne, 296, 305
restructure, 296, 305
scoring function, 301
service-level extension, 312–313
service-oriented architecture, 311
SOA-aware approach, 311
soft cluster assignments, 297
split, 296, 304
triadic pairwise-similarity, 299–300
two-dimensional vectors, distance
measures, 297, 298
visual-interactive interface (see
Visual-interactive interface)
XML-based SOAP protocol, 311
Large scale distributed systems
asymmetric cryptography algorithm, 488

786
Index
Large scale distributed systems (cont.)
cryptography security algorithm, 488, 489
data I/O algorithms, 490
delete time, 497–499
GS3 interface library and API
data I/O operations, 493–495
initialization phase, 493, 494
termination operation, 495–496
initialization algorithm, 489–490
Kerckhoffs’ principle, 487
PKI, 487
read time, 497, 498
storage architecture, 491–493
symmetric cryptography algorithm, 487
termination phase algorithm, 490, 491
write time, 496–497
X509 certiﬁcate, 488
Large-scale ground motion simulations
anelastic wave propagation effects, 541
dynamic source model, 540
ﬁnite difference method, 541
ﬁnite element method, 541
friction law, 540
kinematic source model, 540
M8 simulations, 543
numerical methods, 540–541
parallel computing, 541
rupture model, 540
seismic wave propagation, 540
ShakeOut simulations, 542
TeraShake, 541, 542
Large scale sensor network
GSN, 757–758
IrisNet, 755–756
Live E!, 755–756
Large-scale social information systems
broadcast information, 617
cognitive overload, 619
data analysis, 620
Eddi application, 621
information dissipation mechanisms, 619
information overload, 618
internet revolution, 617
massive scale production, 618
pervasive systems, 618
real-time content dissemination, 617
sampling strategies, 621
topic-centric relevance, social media
content
baseline techniques, 627–628
content diversity, 622–623
content selection methodology (see
Content selection methodology)
explicit measures, 629
full ﬁre hose, 627
human information processing, 623
implicit measures, 629
mean performance, standard errors,
630
proposed method, 632
relevance subjectivity, 623
social media information space, 621
statistical signiﬁcance, 630, 631
Twitter usage distribution, 628, 629
Twitter revolution, 618
Lexis Nexis HPCC, 459–460
LexisNexis solution
computation cost, 402
indexing quality, 403–404
Levenshtein edit distance, 402
linking performance, 407–408
run time performance, 404–407
SALT linking technology, 403
Life Cycle Manager (LCM), 757
Linac Coherent Light Source (LCLS), 264
Link-based ranking methods, 741
Load balancing techniques
cloud-scale applications, 161–162
data stream process, 160–161
data volumes, 157
dynamic load balancing schemes
cloud computing, 164–165
discrete event simulation, 166–167
scheduling decisions, 163
stream based scenarios, 164
Google MapReduce framework
datasets processing, 157
Hadoop framework, 159
Map phase, 158
MapReduce computation, 158
Reduce phase, 158
Microsoft Dryad framework, 159, 160
static load balancing schemes, 162–163
Local-optimization based storage strategy
CTT-SP algorithm, linear DDG, 140–141
improved CTT-SP algorithm
CostCPU, 141
pseudo-code, 141, 142
strategy rules, 142–143
sub linear DDGs, 143
Locator/ID separation protocol (LISP), 20
M
MapReduce architecture, 457
MapReduce-based clusters
application performance evaluation, 678
Amazon’s utility cloud, 678–679

Index
787
execution time and throughput
application, 677
front-end and ﬁve worker nodes,
676–677
large-scale cloud environment, 679, 680
payload data ﬁle sizes, 677
scalability evaluation, 679–682
architecture and application design
data decomposition (see Data
decomposition)
data placement, 670, 671
split & merge architecture, 669
user deﬁned function, 673
automates data compression and
decomposition, 667
AV input stream, 674–675
AV output stream, 676
AV splittable compression codec, 674
binary data application, 669
digital libraries, 668
distributed ﬁle system, 668
frame record reader, 675, 676
IaaS paradigm, 667
software stack, 673–674
structure data process, 668–669
MapReduce programming model, 667, 671
Maslov–Sneppen procedure, 724
Massively multiplayer online role-playing
games (MMORPGs), 173–174
MDSDF. See Multidimensional synchronous
dataﬂow
MegaStore storage architecture, 8–9
Message passing interface (MPI), 324
Messenger RNA (mRNA), 685
Microsoft Azure platform, 162
Microsoft Dryad framework, 159, 160
Middleware
Apache OODT, 53–54
DSSA, 51
OODT, 51, 52
MLP. See Multilayer perceptron
MMORPGs. See Massively multiplayer online
role-playing games
Model and analysis real-time embedded
(Marte) system, 325
MODIS dust radiative forcing of snow
(MOD-DRFS) algorithm, 50
Multidimensional synchronous dataﬂow
(MDSDF) model, 327, 328, 585
Multilayer perceptron (MLP), 695, 697–699,
701
Munich Information Center for Protein
Sequences (MIPS), 718
N
National Centers for Environmental Prediction
(NCEP), 523
National Ecological Observatory Network
(NEON), 522
National Environmental Satellite and
Information Service (NESDIS), 521
Nearest neighbor spacing distribution (NNSD),
714, 716–717, 723–724
Network common data format (NetCDF), 528
NeXus ﬁles, 269–270
NNSD. See Nearest neighbor spacing
distribution
Noise injection mechanism, 361–363
Non Functional Properties (NFP) package, 326
O
Oak Ridge National Laboratory Spallation
Neutron Source (ORNL SNS),
257–258
Obfuscation, 479
Object Oriented Data Technology (OODT), 51,
52
Open Geospatial Consortium (OGC), 522
Open Source Project for Network Data Access
Protocol (OPeNDAP), 529
ORNL SNS. See Oak Ridge National
Laboratory Spallation Neutron
Source
P
Parallel earthquake simulations
AMD hex-core Opteron chip architecture,
553, 554
data-intensive computing, 545–546
dynamic rupture simulations, 543–545
hybrid MPI/OpenMP parallel ﬁnite element
simulation
data dependence, 546
domain decomposition strategy, 547
element-based partitioning, 548–550
end-to-end approach, 547
function-level performance analysis,
556, 557
loop-level ﬁne grain parallelism, 549
master thread, 551
mesh generation and model domain
partitioning, 547–548
modular programming approach, 546
MPI and OpenMP level parallelism,
550, 551

788
Index
Parallel earthquake simulations (cont.)
nodal forces, 552
process-level coarse parallelism, 549
three-level hybrid parallel programming
model, 547
Jaguar and JaguarPF system architecture,
553
large-scale ground motion simulations
anelastic wave propagation effects, 541
dynamic source model, 540
ﬁnite difference method, 541
ﬁnite element method, 541
friction law, 540
kinematic source model, 540
M8 simulations, 543
numerical methods, 540–541
parallel computing, 541
rupture model, 540
seismic wave propagation, 540
ShakeOut simulations, 542
TeraShake, 541, 542
quad- and hex-core Cray XT
supercomputers, 552
SCEC TPV210, 554–555
Cray XT4, 559
dip-slip component, 555, 556
function-level performance, 558
particle velocity, 555, 557
relative speedup, 557–559
rupture time contours, 555, 556
Parallel processing and multiprocessors
Amdahl’s law, 243
CPU
hardware resources, 240
subsystem, 235
utilization, 243
data intensive cluster, 235
data oriented algorithms, 237
distributed processing platform, 236
embarrassingly parallel problems, 243
multiple cores, 238–239
multiple nodes, 237–238
multiple Thor process, 240, 242
open source HPCC platform, 235
processor utilization, 239
programming languages, 244
RAID, 241
segmentation, isolation and virtualization,
236–237
smarter programmers, 244–245
solid state drives, 241–242
TCP transmissions, 242
WRED, 242
Parallel ReliefF, global weighting (PReliefFg)
gene quality, 574
k nearest neighbors, 575
Leukemia data set, 575, 576
Lung cancer data set, 575, 577
parallel gene selection algorithms, 567–570
support vector machines, 575
TIS data set, 575, 578
Parallel ReliefF, private weighting (PReliefFp)
gene quality, 574
k nearest neighbors, 567
Leukemia data set, 575, 576
Lung cancer data set, 575, 577
parallel gene selection algorithms, 567
support vector machines, 575
TIS data set, 575, 578
Peer-to-peer (P2P) storage systems, 485
DHTs, 113
OceanStore, 113–114
PAST, 114–115
Platform as a Service (PaaS), 503–504
P2P storage systems. See Peer-to-peer storage
systems
PRC. See Precision recall curve
Precision recall curve (PRC), 686, 692
Primary cloud providers (pCP), 170
Probabilistic record linkage, 191–192
Protein interaction networks (PINs), 721
Public key infrastructure (PKI), 487
Q
Query translator, 434–436
R
Random matrix theory (RMT)
biological data
cancer patients classiﬁcations, 727–729
system-speciﬁc signals, 725–726
biological network analysis
core protein interaction network,
721–722
Maslov–Sneppen procedure, 724
metabolic network, 722
modular structures, 722
NNSDs transitions, 723
post-genomic era, 721
structural change, 724
topological properties, 721
Wigner–Dyson distribution, 722
cellular systems, 712
complex systems, 711

Index
789
eigenvalue unfolding, 715
gene co-expression networks
conﬁdent biological hypothesis, 716
microarray proﬁles, 718–720
microarray technology, 715
Poisson ensemble, sharp transitions,
716–717
statistical testing vs. randomized
expressions, 720–721
high-throughput biological techniques, 711
topological properties, 712
universal predictions
eigenvalue distribution, 713
eigenvector components, 713
ensembles classiﬁcation, 714
IPR, 713–714
Ranked retrieval model, 740
Receiver operating characteristic (ROC) curve,
686, 692
Record linkage methodology
building decision model, 400
business beneﬁt record linkage, 382
components, 395, 396
confusion matrix f record pairs
classiﬁcation, 401
content enrichment and intelligence,
381–382
CRM, 380
data mining tool, 378
data pre-processing, 395–396
decision model, 395
deﬁnition and notation, 383–384
deterministic record linkage/rule based
linking method, 384–385
duplicates detection, 394
F-measure, 402
fraud detection, 381
geocoding, 382
government administration, 381
health care sector, 378
indexing/blocking, 397–398
internal and external linking, 394
inventory and cost management, 381
law enforcement, 381
LexisNexis solution
computation cost, 402
indexing quality, 403–404
Levenshtein edit distance, 402
linking performance, 407–408
run time performance, 404–407
SALT linking technology, 403
linking process ﬂow chart, 394, 395
matching and linking, 400
modern approach
additive logistic regression, 390–391
machine learning methods, 389
SALT approach, 393–394
unsupervised record linkage, 391–393,
395
weight vector, 389
online search applications, 382
pharmaceutical companies, 379
precision error, 401
probabilistic linkage model
Bayesian decision model, 385
error based probability method,
386–388
expectation and maximization method,
388–389
linking rule, 385
match and non-match probabilities, 385
recall error, 401
record comparison and weight vectors,
399–400
type I and type II error, 401
Regional climate model evaluation database
(RCMED), 42
Regional climate model evaluation system
(RCMES), 42
Regional climate model evaluation toolkit
(RCMET), 43
Repetitive structure modeling (RSM), 324, 326
Array-OL domain-speciﬁc language, 329
array tiling and paving, 332–333
data-parallelism, R-Rule, 331
downscaler model, Marte
Downscaler component, 336, 337
frame generator component, 335
FrameProducer component, 336
fusion operation, 337, 338
HorizontalFilter component, 336, 337
image downscaling, 335
UML Marte-like notation, 336
elementary, composed and hierarchical
tasks, 330–331
environment and environment composition,
330
features, 329
inter-repetition dependency, 334, 335
repetitive task model, 331, 332
Representational state transfer (REST), 451
Representative science data systems
astronomy
ASKAP, EMU survey, 43, 44
baryonic matter, 43
CASA, 47

790
Index
Representative science data systems (cont.)
cosmology, 43
dark energy, 43
data curation, 47
EVLA, 47, 48
GR theory, 44–45
HST, 47
multi-feed system, 46
pulsar survey, 45, 46
telescope parameters and processing
requirements, 44
climate modeling
ARRA, 42
atmospheric model, 40
dissemination and processing statistics,
41
laws of physics, 40
RCMED, 42
RCMES, 42
RCMET, 43
snow hydrology
Apache OODT, 50–51
CRB, 49
MOD-DRFS algorithm, 50
SnowDS and installation, 49
snow-/glacier- melt, 48
two-fold goal, 40
Resource allocation games
cloud providers, 169, 170
computational grids environment, 170
convergence, 181–182
data parallelism, 169
distributed resource management
mechanisms, 171
dynamic federation platform
cCPs and pCPs, 176
cooperative game (see Cooperative
game)
cost function, 175
data intensive clouds (see Data intensive
clouds)
non-cooperative game, 176–178
notations, 174
price function, 176
proﬁt, 175
revenue function, 175–176
market-based approaches, 170
performance analysis, cCPs
cooperative game, 183, 184
HDCF system, 186
non-cooperative game, 183, 184
social welfare, 183, 185
total proﬁt, 183
total utility, revenue function, 183, 185
Ring ﬁle system (RFS)
CDF, 121, 122
DHT, 117
discrete iterative simulation, 121
failure and recovery, 118–119
fault tolerance, 121–123
vs. GFS, design and failure analysis,
119–120
MapReduce performance, 123, 124
metaservers, chunkservers and clients, 117
node types, 117
operation sequence, 118
sixteen 8-core HP DL160, 121
throughput, load conditions, 123
RMT. See Random matrix theory
ROC. See Receiver operating characteristic
curve
Roxie system cluster, 67, 68, 70–71
RSM. See Repetitive structure modeling
Rule-based record linkage, 191
S
Saccharomyces Genome Database (SGD), 718,
720
SBM. See Semantic binary model
Scalable automated linking technology (SALT)
advantages, 198
applications
data hygiene, 215–216
data ingest, 218–220
data integration process, 213
data preparation process, 212
data proﬁling, 214–215
data source consistency checking,
216–218
delta ﬁle, 217–219
external linking process (see External
linking process)
internal linking process (see Internal
linking process)
speciﬁcity generation, 220–221
attribute ﬁles, 204
base ﬁle searching, 229–231
big data problem, 189, 231
ECL code, 190, 198
key beneﬁts, 232–233
LexisNexis Risk Solutions, 189
linkpaths, 205, 206
process, 198–199
record linkage, 203–204
blocking/searching, 196–197
data cleansing, 193–194
data ingest, 196

Index
791
data integration, 192
data proﬁling, 192–193
deﬁnition, approaches and historical
perspective, 190–192
matching weight and threshold
computation, 196
normalization, 194
parsing, 193
record match decision, 197–198
standardization, 194–195
weight assignment, 197
record matching ﬁeld weight computation
ﬁeld value speciﬁcity, 200, 201
iterative process, 201
state codes, 202
TF-IDF, 200
record matching threshold computation,
202
remote linking, 231
speciﬁcation language
ATTRIBUTEFILE statement, 211
BESTTYPE statement, 208–209
BLOCKLINK statement, 212
CONCEPT statement, 211
DATEFIELD statement, 210
FIELD statement, 209
FIELDTYPE statement, 208
FILENAME statement, 206
FUZZY statement, 210
IDFIELD statement, 207
IDNAME statement, 207
INGESTFILE statement, 211
keywords, 205
LATLONG statement, 210–211
LINKPATH statement, 211–212
MODULE statement, 206
NINES statement, 208
OPTIONS statement, 206
POPULATION statement, 207
PROCESS statement, 206–207
RECORDS statement, 207
RELATIONSHIP statement, 212
RIDFIELD statement, 207
SOURCERIDFIELD statement, 210
SOURCFIELD statement, 210
THRESHOLD statement, 212
Scalable storage system
cloud computing, 109, 115–116
distributed ﬁle system, 109–110
AFS, 112–113
NFS, 111–112
distributed key-value stores, 125
GFS, 110
Google datacenter, 109
HDFS, 110
KFS, 110
metadata management, 124
metadata operations, 110
petabyte scale ﬁle system, 124
POSIX directory access semantics, 124
P2P storage systems, 110, 124–125 (see
also Peer-to-peer storage systems)
RFS (see Ring ﬁle system)
web services, 109
Scientiﬁc data management (SDM), 266
Scientiﬁc experimental user facilities. See Data
intensive analysis
SDF networks. See Synchronous dataﬂow
networks
Security auditing, 482
Semantic binary model (SBM), 421
attribute, 425
binary relationship, 424
categories, 422
objects, 422
University application, 422, 425
Semantic binary object data model (Sem-
ODM), 418
Semantic schemas
advantages, 421
physical observations, 421–423
relational schema, 421
SQL query, 421, 424
Semantic SQL query language (Sem-SQL),
418
Semantic SQL virtual tables formal deﬁnition,
438–440
Semantic wrapper
capabilities, 419–420
database of interest, 418
graphical query languages, 417
knowledge base, 434
knowledge base tool
category inheritance hierarchy, 431
geography database, 429–433
geography semantic schema, 429, 431
mapping information, 428
virtual table, 433
middleware, 418, 437–438
multi-database environment, 418
object approach, 417
query translator, 434–436
relational database systems and SQL,
416–417
SBM (see Semantic binary model)
semantic modeling approach, 418

792
Index
Semantic wrapper (cont.)
semantic schemas
advantages, 421
physical observations, 421–423
relational schema, 421
SQL query, 421, 424
semantic SQL syntax, 426–428
Sem-ODM, 418
Sem-SQL, 418
Service level agreements (SLA), 501–502
Service-oriented architectures (SOA), 451–452
Shortest path bridging (SPB), 16, 17
Siena, 758
Software as a Service (SaaS), 503
Software development life cycle (SDLC), 481
Spanning tree protocol (STP), 14
Spatial keyword indexing (SKI), 646–647
Spatial security policy, 486
SPB. See Shortest path bridging
Standard programming model, 670
Support vector machine (SVM), 391, 693, 694,
698, 727
SVM. See Support vector machine
SwinCloud system
cost-effectiveness, 148
pulsar searching application simulation and
results
accelerated de-dispersion ﬁles, 151
DDG, 149
de-dispersion ﬁles, 149
pulsar searching workﬂow, 150
store all datasets strategy, 149, 150
structure, 148
Swinburne University of Technology, 147
VMWare installation, 147–148
Symmetric cryptography algorithm, 487
Synchronous dataﬂow (SDF) networks,
326–327
Systems-on-Chip (SoCs), 324
abstract architecture model, 589
application functional speciﬁcation,
585–586
architecture structural speciﬁcation,
586–587
array-oriented language
afﬁne array, 589
array tiling, 590
formalism, 588–589
HFLR ﬁlter, 589, 590
LPSF ﬁlter, 589, 590
refactoring transformations, 589
ASIP customization ﬂow, 588
computational core synthesis, 582
data parallelism, 582, 587
data transfer and storage micro-architecture
exploration (see Data transfer
and storage micro-architecture
exploration)
design automation, 582–584
DSE approach (see Design space
exploration approach)
hydrophone monitoring
communication structure, 611, 613
external memory, communication, 609,
610
functional blocks, 606, 607
input and output arrays, 606, 607
integer partitions, 608, 609
linearly independent inequalities, 611,
612
links matrix, 608, 609
parametric matrices, 610, 611
partial input textual speciﬁcation, 607,
608
inter-task parallelism, 587
loop-based C-code, 588
low pass spatial ﬁlter, 588, 589
mapping model, 589
orthogonalization, 582
pareto mapping solutions, 588
processor network, 600–602
target customizable architecture, 591–593
T
TBFS. See Threshold-based feature selection
techniques
Term-based ranking methods, 741
Term frequency-inverse document frequency
(TF-IDF), 200
TerraFly
application programming interface, 640
data geocoding capabilities, 644–645
data-mining tools, 641
data repository, 643–644
data visualization capabilities, 642
drill-down detailed information page, 641,
642
ﬂight and data layers control layout, 640,
641
ﬂight window, 642, 643
geospatial data, visualization and querying,
639
image mosaics, raster data analysis and
amelioration, 645–646
landing page, 640
SKI, 646–647
Web-based client interface, 640

Index
793
Thor processing cluster, 67
Threshold-based feature selection techniques
(TBFS), 350
area under PRC, 358, 692
area under ROC, 358, 692
classiﬁcation rule, 689
deviance, 357
feature relevancy, 355
F-measure, 355–356, 690–691
geometric mean, 357, 692
gini index, 356, 691–692
KS statistic measures, 357, 691
mutual information, 356–357
odds ratio, 356, 691
power, 356
precision rate, 690, 691
probability ratio, 356, 691
TPR(t) formula, 355, 690, 691
Topic-centric relevance, social media content
baseline techniques, 627–628
content diversity, 622–623
content selection methodology (see Content
selection methodology)
explicit measures, 629
full ﬁre hose, 627
human information processing, 623
implicit measures, 629
mean performance, standard errors, 630
proposed method, 632
relevance subjectivity, 623
social media information space, 621
statistical signiﬁcance, 630, 631
Twitter usage distribution, 628, 629
Transparent Interconnection of Lots of Links
(TRILL), 14, 17
TRILL. See Transparent Interconnection of
Lots of Links
True positive rate (TPR), 690
Twitter revolution, 618
U
Usage-driven methods, 739
User facility network (UFnet), 277
V
Valiant load balancing (VLB) technique, 15, 16
Value Speciﬁcation Language (VSL) package,
326
Virtual ﬁle system (VFS), 111
Virtual local area network (VLAN), 11, 13
Virtual sensor manager layer (VSM), 757
Visual-interactive interface
attribute view, 309–310
cluster composition and relations, 307–309
clusters and inter-cluster distances, 306,
307
fuzzy c-means algorithm, 306
VLB technique. See Valiant load balancing
technique
Volunteer computing, 485
W
Web architectures
cloud computing, 452–453
grid computing, 452
REST, 451
SOA, 451–452
Weighted random early detection (WRED),
242
Wigner–Dyson distribution, 714, 722
Windowed synchronous dataﬂow (WSDF),
327, 328



Kernel Methods for Machine Learning with Math
and Python

Joe Suzuki
Kernel Methods for Machine
Learning with Math
and Python
100 Exercises for Building Logic

Joe Suzuki
Graduate School of Engineering Science
Osaka University
Toyonaka, Osaka, Japan
ISBN 978-981-19-0400-4
ISBN 978-981-19-0401-1 (eBook)
https://doi.org/10.1007/978-981-19-0401-1
Â© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature
Singapore Pte Ltd. 2022
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, speciï¬cally the rights of reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microï¬lms or in any other physical way, and transmission or
information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciï¬c statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional afï¬liations.
This Springer imprint is published by the registered company Springer Nature Singapore Pte Ltd.
The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721,
Singapore

Preface
How to Overcome Your Kernel Weakness
Among machine learning methods, kernels have always been a particular weakness
of mine. I tried to read â€œIntroduction to Kernel Methodsâ€ by Kenji Fukumizu (in
Japanese) but failed many times. I invited Prof. Fukumizu to give an intensive lecture
at Osaka University and listened to the course for a week with the students, but I
could not understand the bookâ€™s essence. When I ï¬rst started writing this book, my
goal was to rid my sense of weakness. However, now that this book is completed, I
can tell readers how they can overcome their own kernel weaknesses.
Most people, even machine learning researchers, do not understand kernels and
use them. If you open this page, I believe you have a positive feeling that you want
to overcome your weakness.
The shortest path I would most recommend for achieving this is to learn mathe-
matics by starting from the basics. Kernels work according to the mathematics behind
them. It is essential to think through this concept until you understand it. The mathe-
matics needed to understand kernels are called functional analysis (Chap. 2). Even if
you know linear algebra or differential and integral calculus, you may be confused.
Vectors are ï¬nite dimensional, but a set of functions is inï¬nite dimensional and can
be treated as linear algebra. If the concept of completeness is new to you, I hope you
will take the time to learn about it. However, if you get through this second chapter,
I think you will understand everything about kernels.
This book is the third volume (of six) in the 100 Exercises for Building Logic set.
Since this is a book, there must be a reason for publishing it (the so-called cause)
when existing books on kernels can be found. The following are some of the features
of this book.
1.
The mathematical propositions of kernels are proven, and the correct conclu-
sions are stated so that the reader can reach the essence of kernels.
v

vi
Preface
2.
As in the other books in the 100 Mathematical Problems in Machine Learning
series, source programs and running examples are presented to promote under-
standing. It is not easy for readers to understand the results if only mathematical
formulas are given, and this is especially true for kernels.
3.
Once the reader understands the basic topics of functional analysis (Chap. 2), the
applications in the subsequent chapters are discussed, and no prior knowledge
of mathematics is assumed.
4.
This kernel considers both the kernel of the RKHS and the kernel of the Gaussian
process. A clear distinction is made between the two treatments. In this book,
the two types of kernels are discussed in Chaps. 5 and 6, respectively.
We surveyed books on kernels both in Japan and overseas but found that none satisï¬ed
two or more of the above characteristics.
I have experienced many failures leading up to the publication of this book. Every
year, I give a lecture (at the graduate school of Osaka University). Each area of
machinelearningisstudiedbysolving100mathematicalandprogrammingexercises.
Sparse estimation (2018) and graphical models (2019) have gained popularity, and
the 2020 kernel lecture has more than 100 students enrolled. However, although I
prepared for the lectures for more than 2 days every week, the talks did not go well,
probably due to my weakness regarding the subject. This was evident from the class
questionnaires provided by the students. However, I analyzed each of these problems
and made improvements, and this book was born.
I hope that readers will learn about kernels efï¬ciently without following the same
path that I took (consuming much time and energy through trial and error). Reading
this book does not mean that you will write a paper immediately, but it will give
you a solid foundation. You will be able to read kernel papers smoothly, which had
previously seemed difï¬cult, and you will be able to see the whole kernel paradigm
from a higher level. This book is also enjoyable, even for researchers in machine
learning. We hope that you will use this book to achieve success in your respective
ï¬elds.
What Makes KMMP Unique?
I have summarized the features of this book as follows.
1.
Developing logic
We mathematically formulate and solve each ML problem and build those
programs to grasp the subjectâ€™s essence. The KMMP (Kernel methods for
Machine learning with Math and Python) instills â€œlogicâ€ in the minds of the
readers. The reader will acquire both the knowledge and ideas of ML. Even
if new technology emerges, they will be able to follow the changes smoothly.
After solving the 100 problems, most students would say, â€œI learned a lotâ€.

Preface
vii
2.
Not just a story
If programming codes are available, you can immediately take action. It is
unfortunate when an ML book does not offer the source codes. Even if a package
is available, if we cannot see the inner workings of the programs, all we can do
is input data into those programs. In KMMP, the program codes are available
for most of the procedures. In cases where the reader does not understand the
math, the codes will help them know what it means.
3.
Not just a how-to book: an academic book written by a university professor.
This book explains how to use the package and provides examples of executions
for those unfamiliar with them. Still, because only the inputs and outputs are
visible, we can see the procedure as a black box. In this sense, the reader will
have limited satisfaction because they will not obtain the subjectâ€™s essence.
KMMP intends to show the reader the heart of ML and is more of a full-ï¬‚edged
academic book.
4.
Solve 100 exercises: problems are improved with feedback from university
students
The exercises in this book have been used in university lectures and reï¬ned
based on studentsâ€™ feedback. The best 100 problems were selected. Each chapter
(except the exercises) explains the solutions, and you can solve all of the
exercises by reading the book.
5.
Self-contained
All of us havebeendiscouragedbyphrases suchas â€œfor thedetails, pleaserefer to
the literature XXâ€. Unless you are an enthusiastic reader or researcher, nobody
will seek out those references. In this book, we have presented the material
so that consulting external references is not required. Additionally, the proofs
are simple derivations, and the complicated proofs are given in the appendices
at the end of each chapter. KMMP completes all discussions, including the
appendices.
6.
Readersâ€™ pages: questions, discussion, and program ï¬les The reader can ask any
question on the book via https://bayesnet.org/books.
Osaka, Japan
November 2021
Joe Suzuki

Acknowledgments
The author wishes to thank Mr. Bing Yuan Zhang, Mr. Tian Le Yang, Mr. Ryosuke
Shimmura, Mr. Tomohiro Kamei, Ms. Rieko Tasaka, Mr. Keito Odajima, Mr. Daiki
Fujii, Mr. Hongming Huang, and all graduate students at Osaka University, for
pointing out logical errors in mathematical expressions and programs. Furthermore, I
would like to take this opportunity to thank Dr. Hidetoshi Matsui (Shiga University),
Dr. Michio Yamamoto (Okayama University), and Dr. Yoshikazu Terada (Osaka
University) for their advice on functional data analysis in seminars and workshops.
This English book is based mainly on the Japanese book published by Kyoritsu
Shuppan Co., Ltd. in 2021. The author would like to thank Kyoritsu Shuppan Co.,
Ltd., particularly its editorial members Mr. Tetsuya Ishii and Ms. Saki Otani. The
author also appreciates Ms. Mio Sugino, Springer, preparing the publication and
providing advice on the manuscript.
Osaka, Japan
November 2021
Joe Suzuki
ix

Contents
1
Positive Deï¬nite Kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Positive Deï¬niteness of a Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.3
Positive Deï¬nite Kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.4
Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.5
Bochnerâ€™s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
1.6
Kernels for Strings, Trees, and Graphs . . . . . . . . . . . . . . . . . . . . . . . . .
16
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
Exercises 1âˆ¼15 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
2
Hilbert Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.1
Metric Spaces and Their Completeness . . . . . . . . . . . . . . . . . . . . . . . .
29
2.2
Linear Spaces and Inner Product Spaces . . . . . . . . . . . . . . . . . . . . . . .
33
2.3
Hilbert Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.4
Projection Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.5
Linear Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.6
Compact Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
Appendix: Proofs of Propositions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
Exercises 16âˆ¼30 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
3
Reproducing Kernel Hilbert Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
3.1
RKHSs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
3.2
Sobolev Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
3.3
Mercerâ€™s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
Exercises 31âˆ¼45 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
4
Kernel Computations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.1
Kernel Ridge Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.2
Kernel Principle Component Analysis . . . . . . . . . . . . . . . . . . . . . . . . .
97
4.3
Kernel SVM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
4.4
Spline Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
xi

xii
Contents
4.5
Random Fourier Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
4.6
NystrÃ¶m Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
4.7
Incomplete Cholesky Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . .
118
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
Exercises 46âˆ¼64 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
5
The MMD and HSIC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
129
5.1
Random Variables in RKHSs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
129
5.2
The MMD and Two-Sample Problem . . . . . . . . . . . . . . . . . . . . . . . . . .
132
5.3
The HSIC and Independence Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139
5.4
Characteristic and Universal Kernels . . . . . . . . . . . . . . . . . . . . . . . . . .
150
5.5
Introduction to Empirical Processes . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
157
Exercises 65âˆ¼83 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
161
6
Gaussian Processes and Functional Data Analyses . . . . . . . . . . . . . . . . .
167
6.1
Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
6.2
Classiï¬cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
175
6.3
Gaussian Processes with Inducing Variables . . . . . . . . . . . . . . . . . . . .
180
6.4
Karhunen-LÃ³eve Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
6.5
Functional Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
194
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
203
Exercises 83âˆ¼100 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
205
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207

Chapter 1
Positive Deï¬nite Kernels
In data analysis and various information processing tasks, we use kernels to evaluate
the similarities between pairs of objects. In this book, we deal with mathematically
deï¬ned kernels called positive deï¬nite kernels. Let the elements x, y of a set E
correspond to the elements (functions) (x), (y) of a linear space H called the
reproducing kernel Hilbert space. The kernel k(x, y) corresponds to the inner product
âŸ¨(x), (y)âŸ©H in the linear space H. Additionally, by choosing a nonlinear map ,
this kernel can be applied to various problems. The set E may be a string, a tree, or a
graph, even if it is not a real-numbered vector, as long as the kernel satisï¬es positive
deï¬niteness. After deï¬ning probability and Lebesgue integrals in the second half,
we will learn about kernels by using characteristic functions (Bochnerâ€™s theorem).
1.1
Positive Deï¬niteness of a Matrix
Let n â‰¥1; we say that a square matrix A is symmetric if A âˆˆRnÃ—n is equal to its
transpose (AâŠ¤= A)1, and we say that A is nonnegative deï¬nite if all the eigenvalues
are nonnegative.
Proposition 1 (nonnegative deï¬nite matrix) The following three conditions are
equivalent for a symmetric matrix A âˆˆRnÃ—n.
1. A matrix B âˆˆRnÃ—n exists such that A = BâŠ¤B.
2. xâŠ¤Ax â‰¥0 for any x âˆˆRn.
3. The eigenvalues of A are nonnegative.
Proof: 1.=â‡’2. holds because A = BâŠ¤B=â‡’xâŠ¤Ax = xâŠ¤BâŠ¤Bx = âˆ¥Bxâˆ¥2 â‰¥0. 2.=â‡’3.
follows from the fact that xâŠ¤Ax â‰¥0, x âˆˆRn =â‡’0 â‰¤yâŠ¤Ay = yâŠ¤Î»y = Î»âˆ¥yâˆ¥2 for
1 We write the transpose of matrix A as AâŠ¤.
Â© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022
J. Suzuki, Kernel Methods for Machine Learning with Math and Python,
https://doi.org/10.1007/978-981-19-0401-1_1
1

2
1
Positive Deï¬nite Kernels
an eigenvalue Î» of A and its eigenvector y âˆˆRn. 3.=â‡’1. holds since Î»1, . . . , Î»n â‰¥
0=â‡’A = P DPâŠ¤= P
âˆš
D
âˆš
DPâŠ¤= (
âˆš
DPâŠ¤)âŠ¤âˆš
DPâŠ¤, where D and
âˆš
D are diag-
onal matrices with elements Î»1, . . . , Î»n and âˆšÎ»1, . . . , âˆšÎ»n, and P is the correspond-
ing orthogonal matrix.
â–¡
A nonnegative deï¬nite matrix A is symmetric. In this book, we say that a non-
negative deï¬nite matrix is positive deï¬nite if all of its eigenvalues are positive. In
addition, we assume that the elements of any matrix are real. However, the following
fact is often useful when we deal with complex numbers and Fourier transformations.
Corollary 1 For a nonnegative deï¬nite matrix A âˆˆRnÃ—n, we have that zâŠ¤Az â‰¥0
for any z âˆˆCn, where i = âˆšâˆ’1 is the imaginary unit, and we write the conjugate
x âˆ’iy of z = x + iy âˆˆC with x, y âˆˆR as z.
Proof: Since there exists a B âˆˆRnÃ—n such that A = BâŠ¤B for a nonnegative deï¬nite
matrix A âˆˆRnÃ—n, we have that
zâŠ¤Az = (Bz)âŠ¤Bz = |Bz|2 â‰¥0
for any z = [z1, . . . , zn] âˆˆCn.
â–¡
Example 1
# In this chapter, we assume that the following has been executed.
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import style
style.use("seabornâˆ’ticks")
n = 3
B = np.random.normal(size=nâˆ—âˆ—2).reshape(3, 3)
A = np.dot(B.T, B)
values, vectors = np.linalg.eig(A)
print("values:\n", values, "\n\nvectors:\n", vectors, "\n")
values:
[0.09337468 7.75678625 4.43554113]
vectors:
[[ 0.49860775
0.84350568
0.199721
]
[ 0.39606374 -0.42663779
0.81308899]
[-0.77105371
0.32631023
0.54680692]]

1.1 Positive Deï¬niteness of a Matrix
3
S = []
for i in range(10):
z = np.random.normal(size = n)
y = np.squeeze(z.T.dot(A.dot(z)))
S.append(y)
if (i+1) % 5 == 0:
print("S[%d:%d]:"%((iâˆ’4),i), S[iâˆ’4:i])
S[0:4]: [23.24608872999895, 6.601263342526701, 5.334515801733688, 14.886876186736613]
S[5:9]: [18.85503241886245, 34.30290091714191, 1.025291282540866, 29.59512428090335]
1.2
Kernels
Let E be a set. We often express similarity between elements x, y âˆˆE by using
a bivariate function k : E Ã— E â†’R not just for data analysis but also for various
information processing tasks. The larger k(x, y) is, the more similar x, y are. We
call such a function k : E Ã— E â†’R a kernel.
Example 2 (Epanechnikov kernel) We use the kernel k : E Ã— E â†’R such that
k(x, y) = D
|x âˆ’y|
Î»

D(t) =
 3
4(1 âˆ’t2), |t| â‰¤1
0,
Otherwise
for Î» > 0, and we construct the following function (the Nadaraya-Watson estimator)
from observations (x1, y1), . . . , (xN, yN) âˆˆE Ã— R:
Ë†f (x) =
N
i=1 k(x, xi)yi
N
j=1 k(x, x j)
.
For a given input xâˆ—âˆˆE that is different from the N pairs of inputs, we return
the weighted sum of y1, . . . , yN,
k(xâˆ—, x1)
N
j=1 k(xâˆ—, x j)
, . . . ,
k(xâˆ—, xN)
N
j=1 k(xâˆ—, x j)
,
as the output Ë†f (xâˆ—). Because we assume that a larger k(x, y) yields a more similar
x, y âˆˆE, the more similar xâˆ—and xi are, the larger the weight of yi.
Given an input xâˆ—âˆˆE for i = 1, . . . , N, we weight yi such that xi âˆ’Î» â‰¤xâˆ—â‰¤
xi + Î» is proportional to k(xi, xâˆ—). If we make the Î» value smaller, we predict yâˆ—by
using only the (xi, yi) for which xi and xâˆ—are close. We display the output obtained
when we execute the following code in Fig.1.1.

4
1
Positive Deï¬nite Kernels
-3
-2
-1
0
1
2
3
-2
-1
0
1
2
3
x
y
Î» = 0.05
Î» = 0.35
Î» = 0.5
Fig. 1.1 We use the Epanechnikov kernel and Nadaraya-Watson estimator to draw the curves for
Î» = 0.05, 0.35, 0.5. Finally, we obtain the optimal Î» value and present it in the same graph
n = 250
x = 2 âˆ—np.random.normal(size = n)
y = np.sin(2 âˆ—np.pi âˆ—x) + np.random.normal(size = n) / 4 # Data Generation
def D(t): # Function Deï¬nition D
return np.maximum(0.75 âˆ—(1 âˆ’tâˆ—âˆ—2), 0)
def k(x, y, lam): # Function Deï¬nition K
return D(np.abs((x âˆ’y) / lam))
def f(z, lam): # Function Deï¬nition f
S = 0; T = 0
for i in range(n):
S = S + k(x[i], z, lam) âˆ—y[i]
T = T + k(x[i], z, lam)
return S / T
plt.ï¬gure(num=1, ï¬gsize=(15, 8),dpi=80)
plt.xlim(âˆ’3, 3); plt.ylim(âˆ’2, 3)
plt.xticks(fontsize = 14); plt.yticks(fontsize = 14)
plt.scatter(x, y, facecolors=â€™noneâ€™, edgecolors = "k", marker = "o")
xx = np.arange(âˆ’3, 3, 0.1)
yy = [[] for _ in range(3)]
lam = [0.05, 0.35, 0.50]
color = ["g", "b", "r"]
for i in range(3):
for zz in xx:
yy[i].append(f(zz, lam[i]))
plt.plot(xx, yy[i], c = color[i], label = lam[i])
plt.legend(loc = "upper left", frameon = True, prop={â€™sizeâ€™:14})
plt.title("Nadarayaâˆ’Watson Estimator", fontsize = 20)

1.3 Positive Deï¬nite Kernels
5
1.3
Positive Deï¬nite Kernels
The kernels that we consider in this book satisfy the positive deï¬niteness criterion
deï¬ned below. Suppose k : E Ã— E â†’R is symmetric, i.e., k(x, y) = k(y, x), x, y âˆˆ
E. For x1, . . . , xn âˆˆE (n â‰¥1), we say that the matrix
â¡
â¢â£
k(x1, x1) Â· Â· Â· k(x1, xn)
...
...
...
k(xn, x1) Â· Â· Â· k(xn, xn)
â¤
â¥â¦âˆˆRnÃ—n
(1.1)
is the Gram matrix w.r.t. a k of order n. We say that k is a positive deï¬nite kernel2 if
the Gram matrix of order n is nonnegative deï¬nite for any n â‰¥1 and x1, . . . , xn âˆˆE.
Example 3 The kernel in Example 2 does not satisfy positive deï¬niteness. In
fact, when Î» = 2, n = 3, and x1 = âˆ’1, x2 = 0, x3 = 1, the matrix consisting of
KÎ»(xi, yi) can be written as
â¡
â£
k(x1, x1) k(x1, x2) k(x1, x3)
k(x2, x1) k(x2, x2) k(x2, x3)
k(x3, x1) k(x3, x2) k(x3, x3)
â¤
â¦=
â¡
â£
3/4
9/16
0
9/16
3/4
9/16
0
9/16
3/4
â¤
â¦
and the determinant is computed as 33/26 âˆ’35/210 âˆ’35/210 = âˆ’33/29. In general,
the determinant of a matrix is the product of its eigenvalues, and we ï¬nd that at least
one of the three eigenvalues is negative.
Example 4 For random variables {Xi}âˆ
i=1 that are not necessarily independent, if
k(Xi, X j) is the covariance between Xi, X j, the Gram matrix of any order is the
covariance matrix among a ï¬nite number of X j, which means that k is positive
deï¬nite. We discuss Gaussian processes based on this fact in Chap.6.
By assuming positive deï¬niteness, the theory of kernels will be developed in this
book. Hereafter, when we state kernels, we are referring to positive deï¬nite kernels.
Let H be a linear space (vector space) equipped with an inner product âŸ¨Â·, Â·âŸ©H.
Then, we often construct a positive deï¬nite kernel with
k(x, y) = âŸ¨(x), (y)âŸ©H.
(1.2)
By using an arbitrary map  : E â†’H. We say that such a  is a feature map. In
this chapter, we may assume that the linear space H is the Euclidean space H = Rd
of dimensionality d with the standard inner product âŸ¨x, yâŸ©Rd = xâŠ¤y, x, y âˆˆRd. We
deï¬ne the linear space and inner product concepts in Chap.2.
Proposition 2 The kernel k : E Ã— E â†’R deï¬ned in (1.2) is positive deï¬nite.
2 Although it seems appropriate to say â€œa nonnegative deï¬nite kernelâ€, the custom of saying â€œa
positive deï¬nite kernelâ€ has been established.

6
1
Positive Deï¬nite Kernels
Proof: We arbitrarily ï¬x n = 1, 2, Â· Â· Â· and x1, Â· Â· Â· , xn âˆˆE and denote the Gram
matrix (1.1) by K. Then, from the deï¬nition of inner products, for an arbitrary
z = [z1, Â· Â· Â· , zn] âˆˆRn, we have
zâŠ¤K z =
n

i=1
n

j=1
zi z jâŸ¨(xi), (x j)âŸ©H = âŸ¨
n

i=1
zi(xi),
n

j=1
z j(x j)âŸ©H = âˆ¥
n

j=1
z j(x j)âˆ¥2
H â‰¥0 ,
where we write âˆ¥aâˆ¥H := âŸ¨a, aâŸ©1/2
H for a âˆˆH.
â–¡
Proposition 3 Ifthematrices A, B arenonnegativedeï¬nite,thensoistheHadamard
product A â—¦B (elementwise multiplication).
Proof: See the appendix at the end of this chapter.
Proposition 3 is helpful for proving the second part of the following proposition.
Proposition 4 If the kernels k1, k2, . . . are positive deï¬nite, then so are the following
E Ã— E â†’R:
1. ak1 + bk2 (a, b â‰¥0) ,
2. k1k2,
3. the limit3 of {ki} when it converges,
4. k has only one value a â‰¥0 (constant function), and
5.
f (x)k(x, y) f (y) (x, y âˆˆE) for an arbitrary f : E â†’R ,
where the third point claims that the limit kâˆ(x, y) := lim
iâ†’âˆki(x, y) satisï¬es positive
deï¬niteness for any x, y âˆˆE.
Proof: ak1 + bk2 is positive deï¬nite because
xâŠ¤Ax â‰¥0, xâŠ¤Bx â‰¥0=â‡’xâŠ¤(aA + bB)x â‰¥0
for A, B âˆˆRnÃ—n. The product k1k2 is positive deï¬nite because if A = (Ai, j), B =
(Bi, j) are nonnegative deï¬nite, then so is the Hadamard product A â—¦B (Proposition
3). The third statement assumes the existence of a positive integer n such that
Bâˆ=
n

j=1
n

h=1
z jzhkâˆ(x j, xh) = âˆ’Ïµ
for x1, Â· Â· Â· , xn âˆˆE, z1, . . . , zn âˆˆR, and Ïµ > 0. Then, the difference between Bi :=
n
j=1
n
h=1 z jzhki(x j, xh) â‰¥0 and Bâˆbecomes arbitrarily close to zero as i â†’âˆ.
However, the difference is at least Ïµ > 0, which is a contradiction and means that
Bâˆâ‰¥0. If a kernel takes only a (nonnegative) constant value a, since all the values
in (1.1) are a â‰¥0, we have
3 the limit of ki(x, y) for each (x, y) âˆˆE.

1.3 Positive Deï¬nite Kernels
7
â¡
â¢â£
a Â· Â· Â· a
... ... ...
a Â· Â· Â· a
â¤
â¥â¦=
â¡
â¢â£
âˆša/n Â· Â· Â· âˆša/n
...
...
...
âˆša/n Â· Â· Â· âˆša/n
â¤
â¥â¦
âŠ¤â¡
â¢â£
âˆša/n Â· Â· Â· âˆša/n
...
...
...
âˆša/n Â· Â· Â· âˆša/n
â¤
â¥â¦.
The last claim is due to the implication
xâŠ¤Ax â‰¥0 , x âˆˆRn=â‡’xâŠ¤DADx â‰¥0 , x âˆˆRn ,
which we can examine by substituting y = Dx into yâŠ¤Ay â‰¥0. In particular, we
may regard A and D as the matrix (1.1) and diagonal matrix with the elements
f (x1), Â· Â· Â· , f (xn), respectively.
â–¡
In addition, the f (x) f (y) obtained by substituting k(x, y) = 1 for x, y âˆˆE in
the last item of Proposition 4 is positive deï¬nite. Moreover, the
k(x, y)
âˆšk(x, x)k(y, y)
(1.3)
obtained by substituting f (x) = {k(x, x)}âˆ’1/2 for k(x, x) > 0 (x âˆˆE) in the last
item of Proposition 4 is positive deï¬nite. Furthermoreâ€ the value obtained by substi-
tuting n = 2, x1 = x, and x2 = y into (1.1) is nonnegative, and the absolute value of
(1.3) does not exceed one. We say that (1.3) is the positive deï¬nite kernel obtained
by normalizing k(x, y).
Example 5 (Linear Kernel) Let E := Rd. Then, the kernel k(x, y) = xâŠ¤Ay =
âŸ¨Bx, ByâŸ©H, x, y âˆˆRd using the nonnegative deï¬nite matrix A = BâŠ¤B âˆˆRdÃ—d,
B âˆˆRdÃ—d is positive deï¬nite because it corresponds to the case in which the map 
in Proposition 2 is E âˆ‹x â†’Bx âˆˆH. In particular, if A is the unit matrix, then the
map  is the identity map. In this sense, the positive deï¬nite kernel is an extension
of the inner product k(x, y) = xâŠ¤y.
Example 6 (Exponential Type) Let Î² > 0, n â‰¥0, and x, y âˆˆRd. Then,
km(x, y) := 1 + Î²xâŠ¤y + Î²2
2 (xâŠ¤y)2 + Â· Â· Â· + Î²m
m! (xâŠ¤y)m
(1.4)
(m â‰¥1) is a polynomial of the products of positive deï¬nite kernels, and the coef-
ï¬cients are nonnegative. From the ï¬rst two items of Proposition 4, this kernel is a
positive deï¬nite kernel. Additionally, because (1.4) is a Taylor expansion up to the
order m, from the third item of Proposition 4,
kâˆ(x, y) := exp(Î²xâŠ¤y) = lim
mâ†’âˆkm(x, y)
is a positive deï¬nite kernel as well.
Example 7 (Gaussian Kernel) The kernel

8
1
Positive Deï¬nite Kernels
k(x, y) := exp{âˆ’1
2Ïƒ2 âˆ¥x âˆ’yâˆ¥2} , Ïƒ > 0
(1.5)
for x, y âˆˆRd can be written as
exp{âˆ’1
2Ïƒ2 âˆ¥x âˆ’yâˆ¥2} = exp{âˆ’âˆ¥xâˆ¥2
2Ïƒ2 } exp{xâŠ¤y
Ïƒ2 } exp{âˆ’âˆ¥yâˆ¥2
2Ïƒ2 } .
Thus, from the ï¬fth item of Proposition 4 and the fact that exp(Î²xâŠ¤y) with Î² = Ïƒâˆ’2
is positive deï¬nite, we see that (7) is positive deï¬nite.
Example 8 (Polynomial Kernel) The kernel
km,d(x, y) := (xâŠ¤y + 1)m ,
(1.6)
for x, y âˆˆRd, d = 1, 2, . . . is a polynomial of positive deï¬nite kernels (linear kernels
xâŠ¤y), and its coefï¬cients are nonnegative. From the ï¬rst two items of Proposition 4,
(1.6) is positive deï¬nite.
Example 9 If we normalize the linear kernel by (1.3), we obtain xâŠ¤y/âˆ¥xâˆ¥âˆ¥yâˆ¥,
where we denote âˆ¥aâˆ¥:= âŸ¨a, aâŸ©1/2 for a âˆˆRn. The Gaussian kernel (1.5) remains the
same even if we normalize it. The polynomial kernel becomes

xâŠ¤y + 1
âˆš
xâŠ¤x + 1

yâŠ¤y + 1
m
if we normalize it.
The converse is true for Proposition 2, which will be proven in Chap.3: for
any nonnegative deï¬nite kernel k, there exists a feature map  : E â†’H such that
k(x, y) = âŸ¨(x), (y)âŸ©H.
Example 10 (Polynomial Kernel) Let m, d â‰¥1. The feature map of the kernel
km,d(x, y) = (xâŠ¤y + 1)m with x, y âˆˆRd is
m,d(x1, Â· Â· Â· , xd) = (

m!
m0!m1! Â· Â· Â· md!xm1
1 Â· Â· Â· xmd
d )m0,m1,...,mdâ‰¥0 ,
where the indices (m0, m1, Â· Â· Â· , md) range over m0, m1, Â· Â· Â· , md â‰¥0 and m0 +
m1 + Â· Â· Â· + md = m, and we assume that an order exists among the indices
(m0, m1, Â· Â· Â· , md). If we use the multinomial theorem,
(
d

i=0
zi)m =

m0+m1+Â·Â·Â·+md=m
m!
m0!m1! Â· Â· Â· md!zm1
1 Â· Â· Â· zmd
d
(z0 = 1), we see that

1.3 Positive Deï¬nite Kernels
9
(xâŠ¤y + 1)m = âŸ¨m,d(x), m,d(y)âŸ©H
with x0 = y0 = 1. For example, we have
1,2(x1, x2) = [1, x1, x2]
2,2(x1, x2) = [1, x2
1, x2
2,
âˆš
2x1,
âˆš
2x2,
âˆš
2x1x2]
because
âŸ¨2,1(x1, x2), 2,1(y1.y2)âŸ©H = 1 + x1y1 + x2y2 = 1 + xâŠ¤y = k(x, y)
âŸ¨2,2(x1, x2), 2,2(y2.y2)âŸ©H = 1 + x2
1 y2
1 + x2
2 y2
2 + 2x1y1 + 2x2y2 + 2x1x2y1y2
= (1 + x1y1 + x2y2)2 = (1 + xâŠ¤y)2 = k(x, y) .
Example 11 (Inï¬nite-Dimensional Polynomial Kernel) Let 0 < r â‰¤âˆ, d â‰¥1, and
E := {x âˆˆRd|âˆ¥xâˆ¥2 < âˆšr}. Let f : (âˆ’r,r) â†’R be Câˆ. We assume that the func-
tion can be Taylor-expanded by
f (x) =
âˆ

n=0
anxn , x âˆˆ(âˆ’r,r) .
If a0 > 0, a1, a2, . . . â‰¥0, then f (xâŠ¤y) is a positive deï¬nite kernel for x, y âˆˆE. The
exponential type is an inï¬nite-dimensional polynomial kernel and is positive deï¬nite.
Example 12 In Example 2, we use the Nadaraya-Watson estimator to determine the
Gaussian kernel (Figs.1.2 and 1.3).
def K(x, y, sigma2) :
return np.exp(âˆ’np. linalg .norm(x âˆ’y)âˆ—âˆ—2/2/sigma2)
def F(z, sigma2) :
# Function Definition f
S=0; T=0
for i in range(n) :
S = S + K(x[ i ] , z, sigma2) âˆ—y[ i ]
T = T + K(x[ i ] , z, sigma2)
return S / T
We often obtain the optimal value for each of the kernel parameters via cross
validation (CV)4. If the parameters take continuous values, we select a ï¬nite number
of candidates and obtain the evaluation value for each parameter as follows. Divide
the N samples into K groups and conduct estimation with the samples belonging to
group K âˆ’1 group. Perform testing with the samples belonging to the one remaining
groupandcalculatethecorrespondingscore.Repeattheprocedure K times(changing
4 Joe Suzuki, â€œStatistical Learning with Math and Pythonâ€, Chap.4, Springer.

10
1
Positive Deï¬nite Kernels
-3
-2
-1
0
1
2
3
-2
-1
0
1
2
3
x
y
Ïƒ2 = 0.01
Ïƒ2 = 0.001
Ïƒ2 = Ïƒ2
best
Fig. 1.2 Smoothing by predicting the values of x outside the N sample points via the Nadaraya-
Watson estimator. We choose the best parameter for the Gaussian kernel via cross validation
Group 1
Group 2
Â· Â· Â·
Group k âˆ’1
Group k
First
Test
Estimation
Â· Â· Â·
Estimation
Estimation
Second
Estimation
Test
Â· Â· Â·
Estimation
Estimation
...
...
...
...
...
(k âˆ’1)-th
Estimation
Estimation
Â· Â· Â·
Test
Estimation
k-th
Estimation
Estimation
Estimation
Test
Fig. 1.3 A rotation employed for cross validation. Each group consists of N/k samples; we divide
the samples into k groups based on their sample IDs. 1 âˆ¼N
k , N
k + 1 âˆ¼2N
k , . . . , (k âˆ’2) N
k + 1 âˆ¼
(k âˆ’1) N
k , (k âˆ’1) N
k + 1 âˆ¼N
the test group) and ï¬nd the sum of the obtained scores. In that way, we evaluate
the performance of the kernel based on one parameter. Execute this process for all
parameter candidates and use the parameters with the best evaluation values.
WeobtaintheoptimalvalueoftheparameterÏƒ2 viaCV.Weexecutethisprocedure,
setting Ïƒ2 = 0.01, 0.001.
n = 100
x = 2 âˆ—np.random.normal(size = n)
y = np.sin(2 âˆ—np.pi âˆ—x) + np.random.normal(size = n) / 4 # Data Generation
# The Curves for sigma2 = 0.01, 0.001
plt.ï¬gure(num=1, ï¬gsize=(15, 8),dpi=80)
plt.scatter(x, y, facecolors=â€™noneâ€™, edgecolors = "k", marker = "o")
plt.xlim(âˆ’3, 3)
plt.ylim(âˆ’2, 3)
plt.xticks(fontsize = 14); plt.yticks(fontsize = 14)
xx = np.arange(âˆ’3, 3, 0.1)

1.3 Positive Deï¬nite Kernels
11
yy = [[] for _ in range(2)]
sigma2 = [0.001, 0.01]
color = ["g", "b"]
for i in range(2):
for zz in xx:
yy[i].append(F(zz, sigma2[i]))
plt.plot(xx, yy[i], c = color[i], label = sigma2[i])
plt.legend(loc = "upper left", frameon = True, prop={â€™sizeâ€™:20})
plt.title("Nadarayaâˆ’Watson Estimator", fontsize = 20)
# Optimum lambda Values
m = int(n / 10)
sigma2_seq = np.arange(0.001, 0.01, 0.001)
SS_min = np.inf
for sigma2 in sigma2_seq:
SS = 0
for k in range(10):
test = range(kâˆ—m, (k+1)âˆ—m)
train = [x for x in range(n) if x not in test]
for j in test:
u, v = 0, 0
for i in train:
kk = K(x[i], x[j], sigma2)
u = u + kk âˆ—y[i]
v = v + kk
if v != 0:
z = u/v
SS = SS + (y[j] âˆ’z)âˆ—âˆ—2
if SS < SS_min:
SS_min = SS
sigma2_best = sigma2
print("Best sigma2 = ", sigma2_best)
Best sigma2 =
0.003
plt.ï¬gure(num = 1, ï¬gsize=(15, 8),dpi = 80)
plt.scatter(x, y, facecolors = â€™noneâ€™, edgecolors = "k", marker = "o")
plt.xlim(âˆ’3, 3)
plt.ylim(âˆ’2, 3)
plt.xticks(fontsize = 14); plt.yticks(fontsize = 14)
xx = np.arange(âˆ’3, 3, 0.1)
yy = [[] for _ in range(3)]
sigma2 = [0.001, 0.01, sigma2_best]
labels = [0.001, 0.01, "sigma2_best"]
color = ["g", "b", "r"]
for i in range(3):
for zz in xx:
yy[i].append(F(zz, sigma2[i]))
plt.plot(xx, yy[i], c = color[i], label = labels[i])
plt.legend(loc = "upper left", frameon = True, prop={â€™sizeâ€™: 20})
plt.title("Nadarayaâˆ’Watson Estimator", fontsize = 20)

12
1
Positive Deï¬nite Kernels
1.4
Probability
Each set is an event when the sets are closed by set operations (union, intersection,
and complement).
Example 13 We consider a set consisting of the subsets of E = {1, 2, 3, 4, 5, 6}
(dice eyes) that are closed by set operations:
{E, {}, {1, 3}, {5}, {2, 4, 6}, {1, 3, 5}, {2, 4, 5, 6}, {1, 2, 3, 4, 6}} .
If any of these eight elements undergo the union, intersection, or complement oper-
ations, the result remains one of these eight elements. In that sense, we can say
that these eight elements are closed by the set operations. The subsets {1, 3} and
{2, 4, 5, 6} are events, but {2, 4} is not. On the other hand, for the entire set E, if we
include {1}, {2}, {3}, {4}, {5}, {6} as events, 26 events should be considered. Even if
the entire set E is identical, whether it is an event differs depending on the set F of
events.
In the following, we start our discussion after deï¬ning the entire set E and the
set F of subsets (events) of E closed by the set operations. Any open interval (a, b)
with a, b âˆˆR is a subset of the whole real number system R. Applying set operations
(union, set product, and set complement) to multiple open intervals does not form an
open interval, but the result remains a subset of R. We call any subset of R obtained
from an open set by set operations a Borel set of R, and we denote such a subset as
B. A set obtained by further applying set operations to Borel sets remains a Borel
set.
Example 14 For a, b âˆˆR, the following are Borel sets: {a} = âˆ©âˆ
n=1(a âˆ’1/n, a +
1/n),
[a, b) = {a} âˆª(a, b),
(a, b] = {b} âˆª(a, b),
[a, b] = {a} âˆª(a, b],
R =
âˆªâˆ
n=0(âˆ’2n, 2n), Z = âˆªâˆ
n=0{âˆ’n, n}, and [
âˆš
2, 3) âˆªZ.
As described above, we assume that we have deï¬ned the entire set E and the
set F of events. At this time, the Î¼ : F â†’[0, 1] that satisï¬es the following three
conditions is called a probability.
1. Î¼(A) â‰¥0, A âˆˆF,
2. Ai âˆ©A j = {}=â‡’Î¼(âˆªâˆ
i=1Ai) = âˆ
i=1 Î¼(Ai), and
3. Î¼(E) = 1.
We say that Î¼ is a measure if Î¼ satisï¬es the ï¬rst two conditions, and we say that
this measure is ï¬nite if Î¼(E) takes a ï¬nite value. We say that (E, F, Î¼) is either a
probability space or a measure space, depending on whether Î¼(E) = 1 or not.
For probability and measure spaces, if {e âˆˆE|X(e) âˆˆB} is an event for any
Borel set B, which means that {e âˆˆE|X(e) âˆˆB} âˆˆF, we say that the function
X : E â†’R is measurable in X. In particular, if we have a probability space, X is a
random variable. Whether X is measurable depends on (E, F) rather than (E, F, Î¼).

1.4 Probability
13
The notion of measurability might be complex for a beginner to understand.
However, it seems smoother if we intuitively understand that the function X : E â†’R
depends on an element of F rather than an element of E.
Example 15 (Dice Eyes) Suppose that X : E â†’R for E = {1, 2, 3, 4, 5, 6} is given
by
X(e) =
1, e = 1, 3, 5
0, e = 2, 4, 6 .
Then, if F = {{1, 3, 5}, {2, 4, 6}, {}, E}, then X is a random variable. In fact, since
X is measurable,
{e âˆˆE|X(e) âˆˆ{1}} = {1, 3, 5}
{e âˆˆE|X(e) âˆˆ[âˆ’2, 3)} = E
{e âˆˆE|X(e) âˆˆ[0, 1)} = {2, 4, 6}
for the Borel set B = {1}, [âˆ’2, 3), [0, 1). Even if we choose the Borel set B, the
set {e âˆˆE|X(e) âˆˆB} is one of {1, 3, 5}, {2, 4, 6}, {}, E. On the other hand, if F =
{{1, 2, 3}, {4, 5, 6}, {}, E}, then X is not a random variable.
In the following, assuming that the function f : E â†’R is measurable, we deï¬ne
the Lebesgue integral

E
f dÎ¼. We ï¬rst assume that f is nonnegative. For a sequence
of exclusive subsets {Bk} of F, we deï¬ne

k

inf
eâˆˆBk f (e)

Î¼(Bk) .
(1.7)
If âˆªk Bk = E and the supremum of (1.7) w.r.t. {Bk},
sup
{Bk}

k

inf
eâˆˆBk f (e)

Î¼(Bk),
takes a ï¬nite value, we say that the supremum is the Lebesgue integral of the mea-
surable function f for (E, B, Î¼), and we write

E
f dÎ¼. When the function f is not
necessarily nonnegative, we divide E into E+ := {e âˆˆE| f (e) â‰¤0}} and Eâˆ’:= {e âˆˆ
E| f (e) â‰¥0}}, and we deï¬ne the above quantity for each of f+ := f, fâˆ’:= âˆ’f . If
both

f+dÎ¼,

fâˆ’dÎ¼ take ï¬nite values, we say that

f dÎ¼ :=

f+dÎ¼ âˆ’

fâˆ’dÎ¼
is the Lebesgue integral of f for (E, B, Î¼).
If X isarandomvariable,theassociatedBorelsetsaretheeventsfortheprobability
Î¼(Â·). We say that the probability of event X â‰¤x for x âˆˆR
FX(x) := Î¼([âˆ’âˆ, x)) =

(âˆ’âˆ,x]
dÎ¼ ,

14
1
Positive Deï¬nite Kernels
is the distribution function and that fX is the probability density function of X if we
can write FX as
FX(x) =
 x
âˆ’âˆ
fX(t)dt .
We say that Î¼ is absolutely continuous if the probability Î¼(B) diminishes when
the width sum of the intervals in any Borel set B approaches zero. The necessary
and sufï¬cient condition ensuring that a probability density function exists for the
probability Î¼ is that Î¼ is absolutely continuous. If X takes a ï¬nite number of values,
the probability density function does not exist, which means that Î¼ is not absolutely
continuous. If X takes values of a1 < Â· Â· Â· < am, then the distribution function can be
written as
FX(x) =

j:a jâ‰¤x
Î¼({a j}) .
Example 16 Suppose that X follows the standard Gaussian distribution. If we make
Ïµ > 0 close to zero, FX(x + Ïµ) âˆ’FX(x âˆ’Ïµ) (for any x âˆˆR) approaches zero, which
means that the probability is absolutely continuous. On the other hand, suppose that
X takes values of 0, 1; even if we make Ïµ > 0 close to zero, FX(1 + Ïµ) âˆ’FX(1 âˆ’Ïµ)
doesnotapproachzero,whichmeansthattheprobabilityisnotabsolutelycontinuous.
If we use the Lebesgue integral, we can express the probability without distin-
guishing between discrete and continuous variables.
Example 17 For E = R, if the probability density function fX exists, the expecta-
tion of X can be written as

E xdÎ¼ =
 âˆ
âˆ’âˆt fX(t)dt. On the other hand, if X takes
values of a1 < Â· Â· Â· < am, we have

E xdÎ¼ = m
j=1 a jÎ¼({a j}).
1.5
Bochnerâ€™s Theorem
We consider the case in which a kernel is a function of the difference between
x, y âˆˆE. According to Bochnerâ€™s theorem, which is the main topic of this section
and will be used in later chapters, the kernel should coincide with a characteristic
function in terms of probability and statistics (up to a constant).
When utilizing a univariate function Ï† : E â†’R, we often use kernels in the form
k(x, y) = Ï†(x âˆ’y), such as the Gaussian kernel. The kernel k being positive deï¬nite
is equivalent to the inequality
n

i=1
n

j=1
ziz jÏ†(xi âˆ’x j) â‰¥0 , z = [z1, . . . , zn] âˆˆRn
(1.8)
for an arbitrary n â‰¥1, x1, . . . , xn âˆˆE.
Let i = âˆšâˆ’1 be the imaginary unit. We deï¬ne the characteristic function of a
random variable X by Ï• : Rd â†’C:

1.5 Bochnerâ€™s Theorem
15
Ï•(t) := E[exp(itâŠ¤X)] =

E
exp(itâŠ¤x)dÎ¼(x) , t âˆˆRd ,
where E[Â·] denotes the expectation. If Î¼ is absolutely continuous (i.e., the probability
density function fX exists), then Ï•(t) := E[exp(itâŠ¤X)] =

E
exp(itâŠ¤x) fX(x)dx is
the Fourier transformation of fX(x) = dÎ¼(x)
dx , and fX(x) can be recovered from Ï•(x)
via the inverse Fourier transformation
fX(x) = 1
2Ï€
 âˆ
âˆ’âˆ
Ï•(t)eâˆ’itâŠ¤xdt .
Example 18 The characteristic function of the Gaussian distribution with a mean
of Î¼ and a variance of Ïƒ2, f (x) =
1
âˆš
2Ï€Ïƒ2 exp{âˆ’(x âˆ’Î¼)2
2Ïƒ2
}, is
Ï•(t) =
1
âˆš
2Ï€Ïƒ2
 âˆ
âˆ’âˆ
exp{itx} exp{âˆ’(x âˆ’Î¼)2
2Ïƒ2
}dx
=
1
âˆš
2Ï€Ïƒ2
 âˆ
âˆ’âˆ
exp[âˆ’{x âˆ’(Î¼ + itÏƒ2)}2
2Ïƒ2
]dx Â· exp{iÎ¼t âˆ’t2Ïƒ2
2 }
= exp{iÎ¼t âˆ’t2Ïƒ2
2 } .
The characteristic function of the Laplace distribution f (x) = Î±
2 exp{âˆ’Î±|x|} with
a parameter Î± > 0 is
 âˆ
âˆ’âˆ
exp{itx}Î±
2 exp{âˆ’Î±|x|}dx = Î±
2 {
 0
âˆ’âˆ
exp[(it + Î±)x]dx +
 âˆ
0
exp[(it âˆ’Î±)x]dx}
= Î±
2 {

e(it+Î±)x
it + Î±
0
âˆ’âˆ
âˆ’

e(itâˆ’Î±)x
it âˆ’Î±
âˆ
0
} =
Î±2
t2 + Î±2 .
Proposition 5 (Bochner) Suppose that Ï† : Rn â†’R is continuous. Then, condition
(1.8) holds for anarbitraryn â‰¥1with x = [x1, . . . , xn] âˆˆRn and z = [z1, . . . , zn] âˆˆ
Rn if and only if Ï† coincides with the characteristic function w.r.t. a probability Î¼ up
to a constant, i.e., there exists a ï¬nite measure Î· such that
Ï†(t) =

E
exp(itâŠ¤x)dÎ·(x), t âˆˆRn.
(1.9)
Proof: See the appendix at the end of this chapter.
Because a kernel evaluates a similarity between two elements in E, we do not
care much about the multiplication of constants. In the following, we say that a
probability Î¼ is the probability of kernel k if Î¼ is the ï¬nite measure Î· when dividing

16
1
Positive Deï¬nite Kernels
the kernel k in Proposition 5 by a constant. Note that we only consider a kernel k(Â·, Â·)
whose range is real in this book, although the range of the characteristic function is
generally Cn.
In the following, we denote âˆ¥tâˆ¥2 by
d
j=1 t2
j for t = [t1, . . . , td] âˆˆRd.
Example 19 (Gaussian Kernel) k(x, y) = exp{âˆ’1
2Ïƒ2 âˆ¥x âˆ’yâˆ¥2
2}, x, y âˆˆRd coin-
cides with the characteristic function exp{âˆ’âˆ¥tâˆ¥2
2
2Ïƒ2 }, t = x âˆ’y âˆˆRd of the Gaussian
distribution with a mean of 0 and a covariance matrix (Ïƒ2)âˆ’1I âˆˆRdÃ—d.
Example 20 (Laplacian Kernel) k(x, y) = 1
2Ï€
1
âˆ¥x âˆ’yâˆ¥2
2 + Î²2 , x, y âˆˆRn coin-
cides with the characteristic function
Î²2
âˆ¥tâˆ¥2
2 + Î²2 , t = x âˆ’y âˆˆRn of the Laplace
distribution with a parameter Î± = Î² > 0 up to the constant multiplication [2Ï€Î²2]âˆ’1.
We can construct the kernel for this distribution if the probability density function
exists. However, if we restrict our search to the kernels whose ranges are real, we
need to choose the parameters so that the characteristic function takes real values.
For example, the Gaussian kernel obtained by setting the mean to zero takes real
values.
1.6
Kernels for Strings, Trees, and Graphs
As discussed in Chap.4, the space E of the covariates is projected via the feature
map  : E â†’H. The method of evaluating similarity via the inner product (kernel)
in another linear space ( RKHS ) has been widely used in machine learning and data
science. If the similarities between the elements of the set E are accurately repre-
sented, then this approach yields improved regression and classiï¬cation processing
performance. As this is a kernel conï¬guration method, we provide the notions of
convolutional and marginalized kernels and illustrate them by introducing string,
tree, and graph kernels.
First, we deï¬ne positive deï¬nite kernels k1, . . . , kd for the sets E1, . . . , Ed. Sup-
pose that we deï¬ne a set E and a map R : E1 Ã— Â· Â· Â· Ã— Ed â†’E. Then, we deï¬ne the
kernel E Ã— E âˆ‹(x, y) â†’k(x, y) âˆˆR by
k(x, y) =

Râˆ’1(x)

Râˆ’1(y)
d
i=1
ki(xi, yi) ,
(1.10)
where
Râˆ’1(x) isthesumover(x1, . . . , xd) âˆˆE1 Ã— Â· Â· Â· Ed suchthat R(x1, . . . , xd) =
x. A kernel in the form of (1.10) is called a convolutional kernel [13]. Since each
ki(xi, yi) is positive deï¬nite, k(x, y) is also positive deï¬nite (according to the ï¬rst
two items of Proposition 4).

1.6 Kernels for Strings, Trees, and Graphs
17
Example 21 (StringKernel) Let p beasetofstringsconsistingof p â‰¥0characters
in a ï¬nite set , and let âˆ—:= âˆªii. For example, if  = {A, T, G, C}, we have
AGGCGT G âˆˆ7. Then, we deï¬ne the kernel
k(x, y) :=

uâˆˆ p
cu(x)cu(y)
for x, y âˆˆâˆ—, where cu(x) denotes the number of occurrences of u âˆˆ p in x âˆˆâˆ—.
The following represents sample code for deï¬ning this string kernel.
def string_kernel(x, y, p) :
m, n = len(x) , len(y)
S = 0
for i in range(m) :
for j in range(i , n) :
if x[ i :( i+p)] == y[ j :( j+p) ]:
S = S + 1
return S
Then, we execute the procedure.
C = ["a", "b", "c"]
m = 10
w = np.random.choice(C, m, replace = True)
x = ""
for i in range(m):
x = x + w[i]
n = 12
w = np.random.choice(C, n, replace = True)
y = ""
for i in range(n):
y = y + w[i]
x
â€™ababbcaaacâ€™
y
â€™ccbcbcaaacaaâ€™
string_kernel(x,y,2)
58

18
1
Positive Deï¬nite Kernels
Suppose that d = 3, E1 = E3 = âˆ—, and E2 =  p. Then, if we concatenate
(x1, x2, x3) âˆˆE1 Ã— E2 Ã— E3, then we may state that R(x1, x2, x3) = x âˆˆE. If x2 =
u and y2 = u appear cu(x) times in x and cu(y) times in y, respectively, then by
setting k1(x1, y1) = k3(x3, y3) = 1 and k2(x2, y2) = I (x2 = y2 = u), we have
cu(x)cu(y) =

R(x1,x2,x3)=x

R(y1,y2,y3)=y
1 Â· I (x2 = y2 = u) Â· 1
k(x, y) =

u
cu(x)cu(y) =

R(x1,x2,x3)=x

R(y1,y2,y3)=y
1 Â· I (x2 = y2) Â· 1 .
Thus, we observe that the string kernel can be expressed by (1.10), where I (A) takes
values of one and zero depending on whether condition A is satisï¬ed.
Example 22 (Tree Kernel) Suppose that we assign a label to each vertex of trees
x, y. We wish to evaluate the similarity between x, y based on how many subtrees
are shared. We denote by ct(x), ct(y) the numbers of occurrences of subtree t in x, y,
respectively. Then, the kernel
k(x, y) :=

t
ct(x)ct(y)
(1.11)
is positive deï¬nite. In fact, for x1, . . . , xn âˆˆE and an arbitrary z1, . . . , zn âˆˆR, we
have
n

i=1
n

j=1
ziz jk(xi, x j) =

t
{
n

i=1
zict(xi)}2 â‰¥0 .
Let Vx, Vy be the sets of vertices in trees x, y, respectively; we write I (u, t) = 1 and
I (u, t) = 0 depending on whether t has u as a vertex or not. Since (1.11) can be
written as ct(x) = 
uâˆˆVx I (u, t), ct(y) = 
vâˆˆVy I (v, t), we have
k(x, y) =

uâˆˆVx

vâˆˆVy

t
I (u, t)I (v, t) =

uâˆˆVx

vâˆˆVy
c(u, v) ,
where c(u, v) = 
t I (u, t)I (v, t) is the number of common subtrees in x and y
such that the vertices u âˆˆVx and v âˆˆVy are their roots. We assume that a label l(v)
is assigned to each v âˆˆV and determine whether they coincide.
1. For the descendants u1, . . . , um and v1, . . . , vn of u and v, if any of the following
hold, then we deï¬ne c(u, v) := 0:
(a) l(u) Ì¸= l(v),
(b) m Ì¸= n,
(c) there exists i = 1, . . . , m such that l(ui) Ì¸= l(vi),
2. otherwise, we deï¬ne

1.6 Kernels for Strings, Trees, and Graphs
19
c(u, v) :=
m

i=1
{1 + c(ui, vi)}.
For example, suppose that we assign one of the labels A, T, G, C to each vertex
in Fig.1.4. We may write this in a Python function as follows, where we assume that
we assign no identical labels to the vertices at the same level of the tree. Note that
the function calls itself (it is a recursive function). For example, the function requires
the value C(4, 2) when it obtains C(1, 1).
def C(i , j ) :
S, T = s[ i ] , t [ j ]
# Return zero when verteces i and j of the trees s and t do not coincides
if S[0] != T[0]:
return 0
# Return zero when either verteces i or j of the trees s and t does not have a descendant
if S[1] is None:
return 0
if T[1] is None:
return 0
if len(S[1]) != len(T[1]) :
return 0
U = []
for x in S[1]:
U.append(s[x][0])
U1 = sorted(U)
V = []
for y in T[1]:
V.append( t [y][0])
V1 = sorted(V)
m = len(U)
# Return zero when the labels of the descendants do not coincide
for h in range(m) :
if U1[h] != V1[h]:
return 0
U2 = np. array(S[1])[np. argsort(U)]
V2 = np. array(T[1])[np. argsort(V)]
W = 1
for h in range(m) :
W = W âˆ—(1 + C(U2[h] , V2[h]) )
return W
def k(s , t ) :
m, n = len(s) , len( t )
kernel = 0
for i in range(m) :
for j in range(n) :
if C(i , j ) > 0:
kernel = kernel + C(i , j )
return kernel
s = [[] for _ in range(6)]
s[0] = ["G", [1, 3]]; s[1] = ["T", [2]]; s[2] = ["C", None]
s[3] = ["A", [4, 5]]; s[4] = ["C", None]; s[5] = ["T", None]
t = [[] for _ in range(9)]
t[0] = ["G", [1, 4]]; t[1] = ["A", [2, 3]]; t[2] = ["C", None]
t[3] = ["T", None]; t[4] = ["T", [5, 6]]; t[5] = ["C", None]

20
1
Positive Deï¬nite Kernels
2
3
4
1
5
6
T
A
G
C
T
C
2
5
1
6
7
3
4
8
9
A
T
G
C
A
C
T
C
T
Fig. 1.4 A tree kernel evaluates the similarity in terms of which the labels A, G, C, T are assigned
to the vertices of the trees
t[6] = ["A", [7, 8]]; t[7] = ["C", None]; t[8] = ["T", None]
for i in range(6):
for j in range(9):
if C(i, j) > 0:
print(i, j, C(i, j))
0 0 2
3 1 1
3 6 1
k(s , t )
4
Thus, the sum 4 will be the kernel value.
Let X and Y be discrete random variables that take values in EX and EY, respec-
tively, and let P(y|x) be the conditional probability of X = x âˆˆEX given Y = y âˆˆ
EY. Suppose that we are given a positive deï¬nite kernel kXY : EXY Ã— EXY â†’R for
EXY := EX Ã— EY. We deï¬ne the marginalized kernel by
k(x, xâ€²) :=

yâˆˆEY

yâ€²âˆˆEY
kXY((x, y), (xâ€², yâ€²))P(y|x)P(yâ€²|xâ€²) , x, xâ€² âˆˆEX
(1.12)
for x, xâ€² âˆˆEX (Tsuda et al. [32]). We claim that the marginalized kernel is positive
deï¬nite. In fact, kXY being positive deï¬nite implies the existence of the feature map
 : EXY âˆ‹(x, y) â†’(x, y) such that
kXY((x, y), (xâ€², yâ€²)) = âŸ¨((x, y)), ((xâ€², yâ€²))âŸ©.

1.6 Kernels for Strings, Trees, and Graphs
21
Thus,thereexistsanotherfeaturemap EX âˆ‹x â†’
yâˆˆEY P(y|x)((x, y))suchthat
k(x, xâ€²) :=

yâˆˆEY

yâ€²âˆˆEY
P(y|x)P(yâ€²|xâ€²)âŸ¨((x, y)), ((xâ€², yâ€²))âŸ©
= âŸ¨

yâˆˆEY
P(y|x)((x, y)),

yâ€²âˆˆEY
P(yâ€²|xâ€²)((xâ€², yâ€²))âŸ©.
We may deï¬ne (1.12) for the conditional density function f of Y given X as follows:
k(x, xâ€²) :=

yâˆˆEY

yâ€²âˆˆEY

kY|X((x, y), (xâ€², yâ€²)) f (y|x) f (yâ€²|xâ€²)dydyâ€²
for x, xâ€² âˆˆEX.
Example 23 (Graph Kernel (Kashima et al. [19]) We construct a kernel that
expresses the similarity between (directed) graphs G1, G2 that may contain a loop
according to the set of paths connecting two vertices.
Let V, E be the sets of vertices and (directed) edges, respectively. We express each
path of length m by a sequence consisting of vertices and edges: (v0, e1, . . . , em, vm),
v0, v1, . . . , vm âˆˆV , and e1, . . . , em âˆˆE. We assume that a label is assigned to each of
the vertices and edges of the two graphs, and we deï¬ne the probability of the sequence
Ï€ = (v0, e1, . . . , em, vm) by the products of the associated conditional probabilities
p(Ï€) := p(v0)p(v1|v0) Â· Â· Â· p(vm|vmâˆ’1). To this end, we consider a random walk in
which we ï¬rst choose v0 âˆˆV with a probability of p(v0) = 1/|V | (|V |: the cardi-
nality of V ) and repeatedly choose either to stop at that point with a probability
of p or to move to a neighbor vertex via one of the connected directed edges with
an equiprobability times 1 âˆ’p, where the stopping probability 0 < p < 1 should
be determined in an a priori manner. For example, if the random walk arrives at a
vertex v that connects to |V (v)| vertices, then the probability of moving to one of
the neighboring vertices is (1 âˆ’p)/|V (v)|. For example, for 1 â†’4 â†’3 â†’5 â†’3
in Fig.1.5, the labels are A, e, A, d, D, a, B, c, D. If p = 1/3, then the probability
of the directed path can be obtained via the following code.
def k(s , p) :
return prob(s , p) / len(node)
def prob(s , p) :
if len(node[s[0]]) == 0:
return 0
if len(s) == 1:
return p
m = len(s)
S = (1 âˆ’p) / len(node[s[0]]) âˆ—prob(s[1:m] , p)
return S

22
1
Positive Deï¬nite Kernels
Fig. 1.5 Evaluating
similarity via a graph kernel
2
1
4
3
5
C
A
A
D
B
c
b
b
a
c
d
e
We demonstrate the execution of the code below:
node = [[] for _ in range(5)]
node[0] = [2, 4]; node[1] = [4]; node[2] = [1, 5]
node[3] = [1, 5]; node[4] = [3]
k([0 , 3, 2, 4, 2], 1 / 3)
0.0016460905349794243
Because ï¬ve vertices exist, we multiply by 1/5, choose one of the next two
transitions, and so on.
1
5 Â·
2
3 Â· 1
2

Â· (2
3 Â· 1) Â·
2
3 Â· 1
2

Â· (2
3 Â· 1) Â· 1
3 =
22
5 Ã— 35 .
Because these probabilities are different in the directed graphs G1, G2, we denote
them by p(Ï€|G1) and p(Ï€|G). We express the label sequence of the path Ï€ (of length
2m + 1) by L(Ï€) and deï¬ne the graph kernel by
k(G1, G2) :=

Ï€1

Ï€2
p(Ï€1|G1)p(Ï€2|G2)I[L(Ï€1) = L(Ï€2)].
We ï¬nd that this kernel is a marginalized kernel if kXY((G1, Ï€1), (G2, Ï€2)) =
I[L(Ï€1) = L(Ï€2)].
Appendix
Many books have proofs because Fubiniâ€™s theorem, Lebesgueâ€™s dominant conver-
gence theorem, and Levyâ€™s convergence theorem are general theorems. We have
abbreviated these statements and proofs. The proof of Proposition 5 was provided
by Ito [15].

Appendix
23
Proof of Proposition 3
D is a diagonal matrix whose components are the eigenvalues Î»i â‰¥0 of the non-
negative deï¬nite matrix A, and U is an orthogonal matrix whose column vectors
are unit eigenvectors ui that are orthogonal to each other. Then, we can write
A = U DU âŠ¤= n
i=1 Î»iuiuâŠ¤
i . Similarly, if Î¼i, vi, i = 1, . . . , n are the eigenvalues
and eigenvectors of matrix B, respectively, then we can write B = n
i=1 Î¼ivivâŠ¤
i . At
this moment, we have
(uiuâŠ¤
i ) â—¦(v jvâŠ¤
j )=(ui,kui,lÂ·v j,kv j,l)k,l=(ui,kv j,k Â· ui,lv j,l)k,l = (ui â—¦v j)(ui â—¦v j)âŠ¤.
Note that this matrix is nonnegative deï¬nite. In fact, if we write ui â—¦v j = [y1,
. . . , yn] âˆˆRn, then component (h,l) of (ui â—¦v j)(ui â—¦v j)âŠ¤is yh yl, which means that
n
h=1
n
l=1 zhzl yh yl = (n
h=1 zh yh)2 â‰¥0 for any z1, . . . , zn. Since matrices A and B are
nonnegative deï¬nite, we have that Î»i, Î¼ j â‰¥0 for each i, j = 1, Â· Â· Â· , n, which means
that
A â—¦B =
n

i=1
n

j=1
Î»iÎ¼ j(uiuâŠ¤
i ) â—¦(v juâŠ¤
j ) =
n

i=1
n

j=1
Î»iÎ¼ j(ui â—¦v j)(ui â—¦v j)âŠ¤
is nonnegative deï¬nite.
â–¡
Proof of Proposition 5
We only show the case in which Ï†(0) = Î·(E) = 1 because the extension is straight-
forward. Suppose that (1.9) holds. Then, we have
n

j=1
n

k=1
z j zkÏ†(x j âˆ’xk) =

E
n

j=1
z jeix j t
n

k=1
zkeâˆ’ixktdÎ·(t) =

E
|
n

j=1
z jeix j t|2dÎ·(t) â‰¥0,
and (1.8) follows. Conversely, suppose that (1.8) holds. Since the matrix consisting
of Ï†(xi âˆ’x j) for the (i, j)th element is nonnegative deï¬nite and symmetric, we
have that Ï†(x) = Ï†(âˆ’x), x âˆˆR. If we substitute n = 2, x1 = u, and x2 = 0, then
we obtain
[z1, z2]

1
Ï†(u)
Ï†(u)
1
  z1
z2

â‰¥0
and Ï†(u)2 â‰¤1 because the determinant is nonnegative. Since Ï† is bounded and
continuous, it is uniformly continuous. On the other hand, eâˆ’âˆ¥tâˆ¥2/neâˆ’ixt is uniformly
continuous as well. In the following, we show that
fn(x) := 1
2Ï€
 âˆ
âˆ’âˆ
Ï†(t)eâˆ’âˆ¥tâˆ¥2/neâˆ’ixâŠ¤tdt

24
1
Positive Deï¬nite Kernels
is a probability density function, and the characteristic function Ï†n approaches Ï†
as n â†’âˆ. If we verify the claim, by Levyâ€™s convergence theorem [15], Ï† is the
characteristic function. We show the d = 1 case ï¬rst.
fn(x) = 1
2Ï€
 âˆ
âˆ’âˆ
Ï†(t)eâˆ’t2/neâˆ’ixtdt.
For a > 0, we have
 a
âˆ’a
fn(x)dx = 1
2Ï€
 a
âˆ’a
 âˆ
âˆ’âˆ
Ï†(t)eâˆ’t2/neâˆ’ixtdtdx = 1
2Ï€
 âˆ
âˆ’âˆ
Ï†(t)eâˆ’t2/n 2 sin at
t
dt ,
where we use Fubiniâ€™s theorem for the last equality. Then, for b > 0, from
 b
0
sin(at)
da = 1âˆ’cos bt
t
â‰¥0,
 âˆ
âˆ’âˆ
1 âˆ’cos t
t2
dt = Ï€, and Ï†(0) = 1, as b â†’âˆ, we have
1
b
 b
0
{
 a
âˆ’a
fn(x)dx}da = 1
b
 b
0
1
2Ï€
 âˆ
âˆ’âˆ
Ï†(t)eâˆ’t2/n 2 sin at
t
dadt
= 1
2Ï€
 âˆ
âˆ’âˆ
Ï†(t)eâˆ’t2/n 2(1 âˆ’cos tb)
t2b
dt= 1
2Ï€
 âˆ
âˆ’âˆ
Ï†(u
b )eâˆ’(u/b)2/n 2(1 âˆ’cos u)
u2
du â†’1 ,
where we use the dominant convergence theorem for the last equality. In general, for
a g : R â†’R that is monotonically increasing and bounded from above, we have
lim
yâ†’âˆ
1
y
 y
0
g(x)dx = lim
xâ†’âˆg(x) .
Thus, we have
 âˆ
âˆ’âˆ
fn(x)dx = 1.
Finally, we show that Ï†n â†’Ï† (n â†’âˆ):
Ï†n(z) := lim
aâ†’âˆ
 a
âˆ’a
eiza 1
2Ï€
 âˆ
âˆ’âˆ
Ï†(t)eâˆ’t2/neâˆ’itadt
= lim
aâ†’âˆ
1
2Ï€
 âˆ
âˆ’âˆ
Ï†(t)eâˆ’t2/n 2 sin a(t âˆ’z)
t âˆ’z
dt
= lim
bâ†’âˆ
1
b
 b
0
da 1
2Ï€
 âˆ
âˆ’âˆ
Ï†(t)eâˆ’t2/n 2 sin a(t âˆ’z)
t âˆ’z
dt
= lim
bâ†’âˆ
1
2Ï€
 âˆ
âˆ’âˆ
Ï†(t)eâˆ’t2/n 2(1 âˆ’cos b(t âˆ’z))
b(t âˆ’z)2
dt
= lim
bâ†’âˆ
1
2Ï€
 âˆ
âˆ’âˆ
Ï†(z + s
b)eâˆ’(z+s/b)2/n 2(1 âˆ’cos s)
s2
ds = Ï†(z)eâˆ’z2/n â†’Ï†(z).

Appendix
25
For a general d â‰¥1, if we use âˆ¥tâˆ¥2
2 = t2
1 + . . . + t2
d,
 a1
âˆ’a1
Â· Â· Â·
 ad
âˆ’ad
eâˆ’i(x1t1+Â·Â·Â·xdtd)dx1 Â· Â· Â· dxd = 2 sin a1x1
t1
Â· Â· Â· 2 sin adxd
td
,
and
 bi
0
2 sin aixi
ti
dai = 2(1 âˆ’cos tibi)
t2
i bi
,
(i = 1, . . . , d), then the same claim can be obtained.
â–¡
Exercises 1âˆ¼15
1. Show that the following three conditions are equivalent for a symmetric matrix
A âˆˆRnÃ—n.
(a) There exists a square matrix B such that A = BâŠ¤B.
(b) xâŠ¤Ax â‰¥0 for an arbitrary x âˆˆRn.
(c) All the eigenvalues of A are nonnegative.
In addition, using Python, generate a square matrix B âˆˆRnÃ—n with real elements
by generating random numbers to obtain A = BâŠ¤B. Then, randomly generate
ï¬ve more x âˆˆRn (n = 5) to examine whether xâŠ¤Ax is nonnegative for each
value.
2. Consider the Epanechnikov kernel deï¬ned by k : E Ã— E â†’R
k(x, y) = D
|x âˆ’y|
Î»

D(t) =
 3
4(1 âˆ’t2), |t| â‰¤1
0,
Otherwise
for Î» > 0. Suppose that we write a kernel for Î» > 0 and (x, y) âˆˆE Ã— E in
Python as shown below:
def k(x, y, lam) :
return D(np.abs((x âˆ’y) / lam)) .
Specify the function D using Python. Moreover, deï¬ne the function f that
makes a prediction at z âˆˆE based on the Nadaraya-Watson estimator by uti-
lizing the function k such that z, Î» are the inputs of f and k, respectively,
and (x1, y1), . . . , (xN, yN) are global. Then, execute the following to examine
whether the functions D, f work properly.

26
1
Positive Deï¬nite Kernels
n = 250
x = 2 âˆ—np.random.normal(size = n)
y = np.sin(2 âˆ—np.pi âˆ—x) + np.random.normal(size = n) / 4
plt.ï¬gure(num=1, ï¬gsize=(15, 8),dpi=80)
plt.xlim(âˆ’3, 3); plt.ylim(âˆ’2, 3)
plt.xticks(fontsize = 14); plt.yticks(fontsize = 14)
plt.scatter(x, y, facecolors=â€™noneâ€™, edgecolors = "k", marker = "o")
xx = np.arange(âˆ’3, 3, 0.1)
yy = [[] for _ in range(3)]
lam = [0.05, 0.35, 0.50]
color = ["g", "b", "r"]
for i in range(3):
for zz in xx:
yy[i].append(f(zz, lam[i]))
plt.plot(xx, yy[i], c = color[i], label = lam[i])
plt.legend(loc = "upper left", frameon = True, prop={â€™sizeâ€™:14})
plt.title("Nadarayaâˆ’Watson Estimator", fontsize = 20)
Replace the Epanechnikov kernel with the Gaussian kernel, the exponential type,
and the polynomial kernel and execute them.
3. Show that the determinant of A âˆˆR3Ã—3 coincides with the product of the three
eigenvalues. In addition, show that if the determinant is negative, at least one of
the eigenvalues is negative.
4. Show that the Hadamard product of nonnegative deï¬nite matrices of the same
size is nonnegative deï¬nite. Show also that the kernel obtained by multiplying
positive deï¬nite kernels is positive deï¬nite.
5. Show that a square matrix whose elements consist of the same nonnegative value
is nonnegative deï¬nite. Show further that a kernel that outputs a nonnegative
constant is positive deï¬nite.
6. Find the feature map 3,2(x1, x2) of the polynomial kernel k3,2(x, y) = (xâŠ¤y +
1)3 for x, y âˆˆR2 to derive
k3,2(x, y) = 3,2(x1, x2)âŠ¤3,2(x1, x2) .
7. Use Proposition 4 to show that the Gaussian and polynomial kernels and expo-
nential types are positive deï¬nite. Show also that the kernel obtained by nor-
malizing a positive deï¬nite kernel is positive deï¬nite. What kernel do we obtain
when we normalize the exponential type and the Gaussian kernel?
8. The following procedure chooses the optimal parameter Ïƒ2 of the Gaussian
kernel via 10-fold CV when applying the Nadaraya-Watson estimator to the
samples. Change the 10-fold CV procedure to the N-fold (leave-one-out) CV
process to ï¬nd the optimal Ïƒ2, and draw the curve by executing the procedure
below:
def K(x, y, sigma2):
return np.exp(âˆ’np.linalg.norm(x âˆ’y)âˆ—âˆ—2/2/sigma2)
n = 100
x = 2 âˆ—np.random.normal(size = n)
y = np.sin(2 âˆ—np.pi âˆ—x) + np.random.normal(size = n) / 4

Exercises 1âˆ¼15
27
m = int(n / 10)
sigma2_seq = np.arange(0.001, 0.01, 0.001)
SS_min = np.inf
for sigma2 in sigma2_seq:
SS = 0
for k in range(10):
test = range(kâˆ—m,(k+1)âˆ—m)
train = [x for x in range(n) if x not in test]
for j in test:
u, v = 0, 0
for i in train:
kk = K(x[i], x[j], sigma2)
u = u + kk âˆ—y[i]
v = v + kk
if not(v==0):
z=u/v
SS = SS + (y[j] âˆ’z)âˆ—âˆ—2
if SS < SS_min:
SS_min = SS
sigma2_best = sigma2
print("Best sigma2 = ", sigma2_best)
9. For a probability space (E, F, Î¼) with E = {1, 2, 3, 4, 5, 6} and a map X : E â†’
R, show that if
X(e) =
1, e = 1, 3, 5
0, e = 2, 4, 6
and F = {{1, 2, 3}, {4, 5, 6}, {}, E}, then X is not a random variable (not mea-
surable).
10. Derive the characteristic function of the Gaussian distribution
f (x) =
1
âˆš
2Ï€
exp{âˆ’(x âˆ’Î¼)2
2Ïƒ2
} with a mean of Î¼ and a variance of Ïƒ2 and ï¬nd the
condition for the characteristic function to be a real function. Do the same for
the Laplace distribution f (x) = Î±
2 exp{âˆ’Î±|x|} with a parameter Î± > 0.
11. Obtain the kernel value between the left tree and itself in Fig.1.4. Construct and
execute a program to ï¬nd this value.
12. Randomly generate binary sequences x, y of length 10 to obtain the string kernel
value k(x, y).
def string_kernel(x, y) :
m, n = len(x) , len(y)
S = 0
for i in range(m) :
for j in range(i , m) :
for k in range(n) :
if x[( iâˆ’1): j ] == y[(kâˆ’1):(k+jâˆ’i ) ]:
S = S + 1
return S
13. Show that the string, tree, and marginalized kernels are positive deï¬nite. Show
also that the string and graph kernels are convolutional and marginalized kernels,
respectively.
14. How can we compute the path probabilities below when we consider a random
walk in the directed graph of Fig.1.5 if the stopping probability is p = 1/3?

28
1
Positive Deï¬nite Kernels
(a) 3 â†’1 â†’4 â†’3 â†’5,
(b) 1 â†’2 â†’4 â†’1 â†’2,
(c) 3 â†’5 â†’3 â†’5.
15. What inconvenience occurs when we execute the procedure below to compute
a graph kernel? Illustrate this inconvenience with an example.
def k(s , p) :
return prob(s , p) / len(node)
def prob(s , p) :
if len(node[s[0]]) == 0:
return 0
if len(s) == 1:
return p
m = len(s)
S = (1 âˆ’p) / len(node[s[0]]) âˆ—prob(s[1:m] , p)
return S

Chapter 2
Hilbert Spaces
When considering machine learning and data science issues, in many cases, the
calculus and linear algebra courses taken during the ï¬rst year of university provide
sufï¬cient background information. However, we require knowledge of metric spaces
and their completeness, as well as linear algebras with nonï¬nite dimensions, for
kernels. If your major is not mathematics, we might have few opportunities to study
these topics, and it may be challenging to learn them in a short period. This chapter
aims to learn Hilbert spaces, the projection theorem, linear operators, and (some of)
thecompactoperatorsnecessaryforunderstandingkernels.Unlikeï¬nite-dimensional
linear spaces, ordinary Hilbert spaces require scrutiny of their completeness.
2.1
Metric Spaces and Their Completeness
Let M be a set. We say that a bivariate function d : M Ã— M â†’R is a distance if
1. d(x, y) â‰¥0;
2. d(x, y) = 0 â‡â‡’x = y;
3. d(x, y) = d(y, x); and
4. d(x, z) â‰¤d(x, y) + d(y, z)
for x, y, z âˆˆM, and the pair (M, d) is a metric space1.
Let E be a subset of the metric space M. We say that E is an open set if a positive
constant Ïµ exists such that U(x, Ïµ) := {y âˆˆM|d(x, y) < Ïµ} âŠ†E for each x âˆˆE.
Moreover, we say that y âˆˆM is a convergence point of E if U(y, Ïµ) âˆ©E Ì¸= {} for an
arbitrary Ïµ > 0, and E is a closed set if E contains all the convergence points of E.
1 We call M a metric space rather than (M, d) when we do not stress d or when d is apparent.
Â© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022
J. Suzuki, Kernel Methods for Machine Learning with Math and Python,
https://doi.org/10.1007/978-981-19-0401-1_2
29

30
2
Hilbert Spaces
Example 24 The set M = [0, 1] is a closed set because the neighborhood U(y, Ïµ) of
y /âˆˆM has no intersection with M if we make the radius Ïµ > 0 smaller, which means
that M contains all the convergence points of M. On the other hand, M = (0, 1) is
an open set because M contains the neighborhood U(y, Ïµ) of y âˆˆM if we make the
radius Ïµ > 0 smaller. If we add {0}, {1} to the interval (0, 1), (0, 1], [0, 1), we obtain
the closed set [0, 1].
We say that the minimum closed set in M that contains E is the closure of E, and
we write this as E. If E is not a closed set, then E does not contain all the convergence
points. Thus, the closure is the set of convergence points of E. Moreover, we say that
E is dense in M if E = M, which is equivalent to the following conditions. â€œy âˆˆE
exists such that d(x, y) < Ïµ for an arbitrary Ïµ > 0 and x âˆˆMâ€, and â€œeach point in M
is a convergence point of Eâ€. Furthermore, we say that M is separable if it contains
a dense subset that consists of countable points.
Example 25 For the distance d(x, y) := |x âˆ’y| with x, y âˆˆR and the metric space
(R, d), each irrational number a âˆˆR\Q is a convergence point of Q. In fact, for an
arbitrary Ïµ > 0, the interval (a âˆ’Ïµ, a + Ïµ) contains a rational number b âˆˆQ. Thus, Q
does not contain the convergence point a /âˆˆQ and is not a closed set in R. Moreover,
the closure of Q is R (Q is dense in R). Furthermore, since Q is a countable set, we
ï¬nd that R is separable.
Let (M, d) be a metric space. We say that a sequence {xn} in2 M converges to
x âˆˆM if d(xn, x) â†’0 as n â†’âˆfor x âˆˆM, and we write this as xn â†’x. On
the other hand, we say that a sequence {xn} in M is Cauchy if d(xm, xn) â†’0 as
m, n â†’âˆ, i.e., if supm,nâ‰¥N d(xm, xn) â†’0 as N â†’âˆ.
If {xn} converges to some x âˆˆM, then it is a Cauchy sequence. However, the
converse does not hold. We say that a metric space (M, d) is complete if each
Cauchy sequence {xn} in M converges to an element in M. We say that (M, d) is
bounded if there exists a C > 0 such that d(x, y) < C for an arbitrary x, y âˆˆM, and
the minimum and maximum values are the upper and lower limits if M is bounded
from above and below, respectively.
Example 26 An arbitrary Cauchy sequence is bounded. In fact, for any Ïµ > 0, we
can choose N := N(Ïµ) such that m, n â‰¥N=â‡’d(xm, xn) < Ïµ, and we have
min{x1, . . . , xNâˆ’1, xN âˆ’Ïµ} â‰¤xn â‰¤max{x1, . . . , xNâˆ’1, xN + Ïµ} .
Example 27 (Q is Not Complete) The sequence {an} deï¬ned by a1=1, an+1 =
1
2an + 1
an
(n â‰¥1) is in Q. However, we can prove that {an} is a Cauchy sequence in
Q but an â†’
âˆš
2 /âˆˆQ (Exercise 17).
Proposition 6 R is complete.
2 {xn} with xn âˆˆM for each n.

2.1 Metric Spaces and Their Completeness
31
Proof: If {xn} is a Cauchy sequence in R, then {xn} is bounded (Example 26). If
we write the upper and lower limits of {xn}âˆ
n=s as ls, ms, respectively, the monotone
sequences {ms}, {ls} in R share the same limit. In fact, from the above assumption,
we can make ls âˆ’ms = sup{|x p âˆ’xq| : p, q â‰¥s} as small as possible. Thus, R is
complete.
â–¡
If the number of dimensions is ï¬nite, we may check completeness for each dimen-
sion, and we see that Rp is complete for any p â‰¥1.
Suppose that we arbitrarily set a neighborhood U(P) for each P âˆˆM beforehand.
We say that a set M is compact if there exist ï¬nite m and P1, . . . , Pm âˆˆM such that
M âŠ†âˆªm
i=1U(Pi).
Example 28 Let M = (0, 1), and suppose that we deï¬ne the neighborhoodU(x) :=
( 1
2 x, 3
2 x) for each x âˆˆM beforehand. Then, for any n and x1, . . . , xn âˆˆM, we have
(0, 1) âŠˆâˆªn
i=1(1
2 xi, 3
2 xi) ,
which means that M is not compact.
Proposition 7 (Heine-Borel) For Rp, any bounded closed set M is compact.
Proof: Suppose that we have set a neighborhood U(P) for each P âˆˆM and that
M âŠ†âˆªm
i=1U(Pi) cannot be realized by any m and P1, . . . , Pm. If we divide the
closed set (rectangular) that contains M âŠ†Rp into two components for each dimen-
sion, then at least one of the 2p rectangles cannot be covered by a ï¬nite number of
neighborhoods. If we repeat this procedure, then the volume of the rectangle that a
ï¬nite number of neighborhoods cannot cover becomes sufï¬ciently small for the cen-
ter to converge to a Pâˆ—âˆˆM; furthermore, we can cover the rectangle with U(Pâˆ—),
which is a contradiction.
â–¡
Let (M1, d1), (M2, d2) be metric spaces. We say that the map f : M1 â†’M2 is
continuous at x âˆˆM1 if for any Ïµ > 0, there exists Î´(x, Ïµ) such that for y âˆˆM1,
d1(x, y) < Î´(x, Ïµ)=â‡’d2( f (x), f (y)) < Ïµ .
(2.1)
In particular, if there exists Î´(x, Ïµ) that does not depend on x âˆˆM1 in (2.1), we say
that f is uniformly continuous.
Example 29 The function f (x) = 1/x deï¬ned on the interval (0, 1] is contin-
uous but is not uniformly continuous. In fact, if we make x approach y after
ï¬xing y, we can make d2( f (x), f (y)) = | 1
x âˆ’1
y | as small as possible, which
means that f is continuous in (0, 1]. However, when we make x approach y to
make d2( f (x), f (y)) smaller than a constant, we observe that for each Ïµ > 0, the
smaller y is, the smaller d1(x, y) = |x âˆ’y| should be. Thus, no such Î´(Ïµ) for which
d1(x, y) < Î´(Ïµ)=â‡’d2( f (x), f (y)) < Ïµ exists if Î´(Ïµ) does not depend on x, y âˆˆM
(Fig.2.1).

32
2
Hilbert Spaces
Fig. 2.1 The function
f (x) = 1/x is not uniformly
continuous over (0, 1]. To
make the | f (x) âˆ’f (y)|
value smaller than a
constant, we need to make
the |x âˆ’y| value smaller
when x, y are close to 0 (red
lines) than that when x, y are
far away from 0 (blue lines).
Thus, Î´ > 0 depends on the
locations of x, y
0.1
0.2
0.3
0.4
0.5
0.6
2
4
6
8
10
x
f(x) = 1/x
f(x) = 1/x
Proposition 8 A continuous function over a bounded closed set is uniformly con-
tinuous.
Proof: Let f : E â†’R be a continuous function deï¬ned over a bounded closed set
M. Because the function f is continuous, for an arbitrary Ïµ > 0, there exists a (z)
for each z âˆˆM such that
d1(x, z) < (z)=â‡’d2( f (x), f (z)) < Ïµ .
(2.2)
From Proposition 7, we can prepare a ï¬nite number of neighborhoods to cover M.
Let U1, . . . ,Um be neighborhoods with centers z1, . . . , zm and radii (z1)/2, . . . ,
(zm)/2.Supposethatwechoose x, y âˆˆM suchthatd1(x, y) < Î´ := 1
2 min
1â‰¤iâ‰¤m (zi).
Because x belongs to one of the U1, . . . ,Um, without loss of generality, we assume
that x âˆˆUi. From the distance property, we have
d1(x, zi) < 1
2(zi) < (zi)
d1(y, zi) â‰¤d1(x, y) + d1(x, zi) < (zi) .
Combining these inequalities, from the assumption that f is continuous and (2.2),
we have That
d2( f (x), f (y)) â‰¤d2( f (x), f (zi)) + d2( f (y), f (zi)) < Ïµ + Ïµ = 2Ïµ .
Since Ïµ > 0 is arbitrary and Î´ does not depend on x, y, f is uniformly continuous.
â–¡
Example 30 We can prove that â€œa deï¬nite integral exists for a continuous function
deï¬ned over a closed interval [a, b]â€ by virtue of Proposition 8. If we divide a < b

2.1 Metric Spaces and Their Completeness
33
into n equal-length segments as a = x0 < . . . < xn = b, then for an arbitrary Ïµ > 0,
we require
 n

i=1
b âˆ’a
n
sup
xiâˆ’1<x<xi
f (x)

âˆ’
 n

i=1
b âˆ’a
n
inf
xiâˆ’1<x<xi f (x)

< Ïµ
to deï¬ne the deï¬nite integral. Because we assume that f is uniformly continuous,
the condition | f (x) âˆ’f (y)| < Ïµ/(b âˆ’a), x, y âˆˆ[xiâˆ’1, xi] is satisï¬ed if we make
Î´ = xi âˆ’xiâˆ’1 = b âˆ’a
n
smaller (i.e., we make n larger).
2.2
Linear Spaces and Inner Product Spaces
We say that a set V is a linear space3 if it satisï¬es the following conditions: for
x, y âˆˆV and Î± âˆˆR,
1. x + y âˆˆV and
2. Î±x âˆˆV.
Example 31 If we deï¬ne the sum of x = [x1, . . . , xd], y = [y1, . . . , yd] âˆˆRd and
the multiplication by a constant Î± âˆˆR as x + y = [x1 + y1, . . . , xd + yd] and Î±x =
[Î±x1, . . . , Î±xd], respectively, then the d-dimensional Euclidean space Rd forms a
linear space.
Example 32 (L2 Space) The set L2[0, 1] of functions f : [0, 1] â†’R for which
 1
0 { f (x)}2dx takes a ï¬nite value is a linear space because
 1
0
{ f (x) + g(x)}2dx â‰¤2
 1
0
f (x)2dx + 2
 1
0
g(x)2dx < âˆ
 1
0
{Î± f (x)}2dx = Î±2
 1
0
f (x)2dx < âˆ
for Î± âˆˆR when
 1
0 { f (x)}2dx < âˆand
 1
0 g(x)2dx < âˆ.
Let V be a linear space. We say that any bivariate function âŸ¨Â·, Â·âŸ©: V Ã— V â†’R
that obeys the four conditions below is an inner product:
1. âŸ¨x, xâŸ©â‰¥0;
2. âŸ¨Î±x + Î²y, zâŸ©= Î±âŸ¨x, zâŸ©+ Î²âŸ¨y, zâŸ©;
3. âŸ¨x, yâŸ©= âŸ¨y, xâŸ©; and
4. âŸ¨x, xâŸ©= 0 â‡â‡’x = 0
for x, y, z âˆˆV and Î±, Î² âˆˆR.
3 The same as a vector space.

34
2
Hilbert Spaces
Example 33 For the linear space Rd in Example 31,
âŸ¨x, yâŸ©=
d

i=1
xi yi
for x = [x1, . . . , xd], and y = [y1, . . . , yd] âˆˆRd is an inner product because the ï¬rst
three conditions are obvious, and the last condition holds because
âŸ¨x, xâŸ©= 0 â‡â‡’
d

i=1
x2
i = 0 â‡â‡’x = 0 .
For a given linear space, we need to choose its inner product. We say that a linear
space equipped with an inner product is an inner product space.
Example 34 (Inner Product of the L2 Space) Let L2[0, 1] be the linear space in
Example 32. The bivariate function
âŸ¨f, gâŸ©=
 1
0
f (x)g(x)dx
f, g âˆˆL2[0, 1] is not an inner product because the last condition fails. If f (1/2) = 1
and f (x) = 0 for x Ì¸= 1/2, then we have
âŸ¨f, f âŸ©=
 1
0
f (x)2dx = 0 .
Strictly speaking, we construct such an inner product, identifying4 f, g âˆˆL2 if
and only if
 1
0 { f (x) âˆ’g(x)}2dx = 0, which may rarely be noticed.
Let V be a linear space. We say that any function âˆ¥Â· âˆ¥: V â†’R that obeys the
four conditions below is a norm.
1. âˆ¥xâˆ¥â‰¥0;
2. âˆ¥avâˆ¥= |a|âˆ¥xâˆ¥;
3. âˆ¥x + yâˆ¥â‰¤âˆ¥xâˆ¥+ âˆ¥yâˆ¥(triangle inequality); and
4. âˆ¥xâˆ¥= 0 â‡â‡’x = 0
for x, y âˆˆV and a âˆˆR.
If we deï¬ne the norm of V , then we can construct a metric space with the distance
d(x, y) = âˆ¥x âˆ’yâˆ¥. If we deï¬ne the inner product of V , the function
âˆ¥xâˆ¥= âŸ¨x, xâŸ©1/2
(2.3)
4 We deï¬ne the equivalent relation âˆ¼such that f âˆ’g âˆˆ{h|
 1
0 {h(x)}2dx = 0} â‡â‡’f âˆ¼g and
construct the inner product for the quotient space L2/ âˆ¼.

2.2 Linear Spaces and Inner Product Spaces
35
satisï¬es the four conditions of a norm, and we call this norm a norm induced by an
inner product.
In Examples 32 and 34, we introduced L2 over E = [0, 1] using the Riemann
integral. However, in general, we deï¬ne L2(E, F, Î¼) according to the set of f :
E â†’R for which

E
f 2dÎ¼
(2.4)
is ï¬nite5 for the measure space (E, F, Î¼).
Example 35 (Uniform Norm) The set of continuous functions over [a, b] forms a
linear space. The uniform norm deï¬ned by
âˆ¥f âˆ¥:= sup
xâˆˆ[a,b]
| f (x)| , f âˆˆC[a, b]
is not induced by an inner product but satisï¬es the norm conditions:
âˆ¥f âˆ¥= 0 â‡â‡’f (x) = 0 , x âˆˆ[a, b] .
In this book, we often use the Cauchy-Schwarz inequality:
|âŸ¨x, yâŸ©| â‰¤âˆ¥xâˆ¥âˆ¥yâˆ¥
(2.5)
for x, y âˆˆV . Apparently, (2.5) holds for y = 0. If y Ì¸= 0, since the inner product of
y and z := x âˆ’âŸ¨x, yâŸ©
âˆ¥yâˆ¥2 y is zero, we have
âˆ¥xâˆ¥2 = âˆ¥z + âŸ¨x, yâŸ©
âˆ¥yâˆ¥2 yâˆ¥2 = âˆ¥zâˆ¥2 + âˆ¥âŸ¨x, yâŸ©
âˆ¥yâˆ¥2 yâˆ¥2 â‰¥âŸ¨x, yâŸ©2
âˆ¥yâˆ¥2
,
where the equality holds if and only if z = 0, which occurs exactly when one of x, y
is a constant multiplication of the other. Moreover, we can examine the triangle norm
inequality via (2.5):
âˆ¥x + yâˆ¥2=âˆ¥xâˆ¥2 + 2|âŸ¨x, yâŸ©|+âˆ¥yâˆ¥2 â‰¤âˆ¥xâˆ¥2 + 2âˆ¥xâˆ¥âˆ¥yâˆ¥+ âˆ¥yâˆ¥2 = (âˆ¥xâˆ¥+ âˆ¥yâˆ¥)2 .
Because we did not use the last condition of inner products in deriving the Cauchy-
Schwarz inequality, we may apply this inequality to any bivariate function that sat-
isï¬es the ï¬rst three conditions.
Using Cauchy-Schwarzâ€™s inequality, we can prove the continuity of an inner prod-
uct.
5 We often abbreviate (E, F, Î¼) or specify the interval as [a, b] (as in L2[a, b]).

36
2
Hilbert Spaces
-0.5
0
0.5
1.0
1.5
0.0
0.4
0.8
n = 1
x
y
-0.5
0
0.5
1.0
1.5
0.0
0.4
0.8
n = 2
x
y
-0.5
0
0.5
1.0
1.5
0.0
0.4
0.8
n=3
x
y
-0,5
0
0.5
1.0
1.5
0.0
0.4
0.8
n = 4
x
y
Fig. 2.2 We illustrate Example 37 when fn â†’f . The function is continuous for ï¬nite values of n
but is not continuous as n â†’âˆ
Proposition 9 (Continuity of an Inner Product) Let {xn}, {yn} be sequences in a
linear space V . For x, y âˆˆV , if xn â†’x and yn â†’y as n â†’âˆ, then âŸ¨xn, ynâŸ©â†’
âŸ¨x, yâŸ©, where we denote by âˆ¥Â· âˆ¥the norm induced by the inner product of V .
Proof: The proposition follows from âˆ¥xnâˆ¥â‰¤âˆ¥xâˆ¥+ âˆ¥xn âˆ’xâˆ¥â†’âˆ¥xâˆ¥(n â†’âˆ) and
|âŸ¨xn, ynâŸ©âˆ’âŸ¨x, yâŸ©| â‰¤|âŸ¨xn, yn âˆ’yâŸ©| + |âŸ¨xn âˆ’x, yâŸ©| â‰¤âˆ¥xnâˆ¥Â· âˆ¥yn âˆ’yâˆ¥+ âˆ¥xn âˆ’xâˆ¥Â· âˆ¥yâˆ¥â†’0.
â–¡
2.3
Hilbert Spaces
We say that a vector space in which a norm is deï¬ned and the distance is complete
is a Banach space. Hereafter, we denote by C(E) the set of continuous functions
deï¬ned over E.
Example 36 If p â‰¥1, the vector space Rp is complete under the standard inner
product and is a Hilbert space. On the other hand, the vector space Qp is not complete
under the standard inner product and does not make a Hilbert space in this case.

2.3 Hilbert Spaces
37
Example 37 The set of continuous functions [0, 1] â†’R forms a linear space
C[0, 1]. We consider the function
fn(t) :=
â§
âªâªâªâªâ¨
âªâªâªâªâ©
0,
0 â‰¤t â‰¤1
2
n(t âˆ’1
2), 1
2 < t < 1
2 + 1
n
1,
1
2 + 1
n â‰¤t â‰¤1 .
For m â‰¥n, we have
âˆ¥fn âˆ’fmâˆ¥2
2 =
 1
0
| fn(t) âˆ’fm(t)|2dt =

1
2 + 1
n
1
2
| fn(t) âˆ’fm(t)|2dt
=

1
2 + 1
m
1
2
[n(t âˆ’1
2) âˆ’m(t âˆ’1
2)]2dt +

1
2 + 1
n
1
2 + 1
m
[n(t âˆ’1
2) âˆ’1]2dt
= (n âˆ’m)2
3m3
âˆ’(n âˆ’m)3
3m3n
= (n âˆ’m)2
3m2n
< 1
3n â†’0 .
Thus, { fn} is a Cauchy sequence in C[0, 1] (Fig.2.2). However, fn converges to a
function that is not continuous:
f (t) :=
0, 0 â‰¤t â‰¤1
2
1, 1
2 < t â‰¤1
âˆ¥fn âˆ’f âˆ¥2 =
 1
0
âˆ¥fn(t) âˆ’f (t)âˆ¥2dt =

1
2 + 1
n
1
2
[n(t âˆ’1
2) âˆ’1]2dt = 1
3n â†’0 .
As we have seen, C[a, b] is not complete w.r.t. the L2 norm. However, it is
complete w.r.t. the uniform norm:
Proposition 10 C[a, b] is complete w.r.t. the uniform norm.
Proof: Let { fn} be a Cauchy sequence in C[a, b], which means that
sup
m,nâ‰¥N
sup
xâˆˆ[a,b]
| fm(x) âˆ’fn(x)| â†’0
(2.6)
as N â†’âˆ. Then, the real sequence { fn(x)} is a Cauchy sequence for each x âˆˆ[a, b]
and converges to a real value (Proposition 6). If we deï¬ne the function f (x) with
x âˆˆE by limnâ†’âˆfn(x), supnâ‰¥N fn(x) and infnâ‰¥N fn(x) converge to f (x) from
above and from below, respectively. From (2.6), we see that
| fN(x) âˆ’f (x)| â‰¤sup
nâ‰¥N
fn(x) âˆ’inf
nâ‰¥N fn(x) = sup
m,nâ‰¥N
| fm(x) âˆ’fn(x)|

38
2
Hilbert Spaces
uniformly converges to 0 for an arbitrary x âˆˆ[a, b], which implies that C[a, b] is
complete.
â–¡
Because any inner product does not induce the uniform norm, C[a, b] is a Banach
space but is not a Hilbert space.
Proposition 11 C[a, b] is separable w.r.t. the uniform norm.
For the proof, we use the Stone-Weierstrass theorem (Proposition 12)[30, 31, 34,
35]. The term algebra is used to denote the linear space A that deï¬nes associative â€œÂ·â€
and commutative â€œ+â€ properties and satisï¬es
x Â· (y + z) = x Â· y + x Â· z
(y + z) Â· x = y Â· x + z Â· x
Î±(x Â· y) = (Î±x) Â· y
for x, y, z âˆˆA and Î± âˆˆR, where the ï¬rst two properties are identical if Â· is commu-
tative. The general theory may be complex, but we only suppose that + and Â· are the
standard addition and multiplication operations and that A is either a polynomial or
continuous function.
Example 38 (Polynomial Ring) Let +,Â· be the standard commutative addition and
multiplication operations. The polynomial ring R[x, y, z] is a set of polynomials with
indeterminates (variables) x, y, z and is an algebra with R commutative coefï¬cients.
R[x, y, z] is a linear space, and if two elements belong to R[x, y, z], multiplication
by R and addition among the elements belong to R[x, y, z]. Moreover, the three laws
follow for the elements in R[x, y, z].
Proposition 12 (Stone-Weierstrass [30, 31, 34, 35]) Let E and A be a compact set
and an algebra, respectively. Under the following conditions, A is dense in C(E).
1. No x âˆˆE exists such that f (x) = 0 for all f âˆˆA.
2. For each pair x, y âˆˆE with x Ì¸= y, f âˆˆA exists such that f (x) Ì¸= f (y).
We refer to Proposition 12 several times. Although we abbreviate its proof in this
book, please follow it because it contains no complicated derivation processes. We
can use this proposition to prove that a neural network can approximate any contin-
uous function.
Proof of Proposition 11: A set A of polynomials with indeterminate x and real
coefï¬cients satisï¬es the two conditions in Proposition 12. Hence, A is dense in
C[a, b]. Furthermore, if we restrict the coefï¬cients of A to Q, then A is a countable
set, which means that C[a, b] is separable.
â–¡
Proposition 13 C[a, b], a < b, is dense in L2[a, b].

2.3 Hilbert Spaces
39
For the proof, see the appendix at the end of this chapter. We say that a function in
the form
m

k=1
h(Bk)I (Bk)
(2.7)
with exclusive B1, . . . , Bm âˆˆF and h : F â†’Râ‰¥0 is a simple function. Equation
(1.7) in Chap.1 approximates this function by using a simple function. The proof
in the appendix shows that a simple function approximates an arbitrary f âˆˆL2 and
that a continuous function approximates an arbitrary simple function.
â–¡
Proposition 14 (Riesz-Fischer) L2 is complete6. In other words, L2 is a Banach
space and a Hilbert space.
The outline of the proof is as follows; see the appendix for details. It is sufï¬cient
to show that â€œ{ fn} is a Cauchy sequence in L2 =â‡’there exists an f âˆˆL2 such that
âˆ¥fn âˆ’f âˆ¥â†’0â€. We deï¬ne the f to which the Cauchy sequence { fn} in L2 converges
and derive âˆ¥fn âˆ’f âˆ¥â†’0 and f âˆˆL2.
1. Let { fn} be an arbitrary Cauchy sequence in L2.
2. There exists {nk} such that âˆ¥âˆ
k=1 | fnk+1 âˆ’fnk|âˆ¥2 < âˆ.
3. We show the existence of f : E â†’R such that Î¼{x âˆˆE| limkâ†’âˆfnk(x) =
f (x)} = Î¼(E).
4. We show that âˆ¥fn âˆ’f âˆ¥â†’0 and f âˆˆL2[a, b].
â–¡
Let V and âŸ¨Â·, Â·âŸ©be an inner product space and its inner product, respectively.
We say that x, y âˆˆV are orthogonal if âŸ¨x, yâŸ©= 0 and that a sequence {e j} in V
is orthonormal if âˆ¥e jâˆ¥= 1 for each j and each pair ei, e j (i Ì¸= j) is orthogonal.
Moreover, we say that {e j} is an orthonormal basis of V if {e j} is an orthonormal
sequence and each x âˆˆV can be expressed by x = âˆ
j=1 Î± je j using the real Î± j.
Proposition 15 For an orthonormal sequence {e j} in a Hilbert space H, we have
the following properties :
1.
âˆ

i=1
âŸ¨x, eiâŸ©2 â‰¤âˆ¥xâˆ¥2 (Besselâ€™s inequality);
2.
âˆ

i=1
âŸ¨x, eiâŸ©ei converges;
3.
âˆ

i=1
Î±iei converges â‡â‡’âˆ
i=1 Î±2
i < âˆ; and
4. y =
âˆ

i=1
Î±iei=â‡’Î±i = âŸ¨y, eiâŸ©.
6 L p is complete for any p â‰¥1, although we abbreviate this proof.

40
2
Hilbert Spaces
Proof: See the appendix at the end of this chapter.
Suppose that A is a subset of a linear space V equipped with a norm, and let
span(A) be the closure of span(A), which is a linear combination of the elements in
A. Suppose that {xn} is a sequence in a Hilbert space and that each xn is orthogonal.
Then, the sequence {en} constructed below is orthonormal
vi := xi âˆ’
iâˆ’1

j=1
âŸ¨xi, e jâŸ©e j , ei = vi/âˆ¥viâˆ¥, i = 1, 2, . . .
and satisï¬es span{xn} = span{en} (Gram-Schmidt).
Proposition 16 Let {e j} be an orthonormal sequence in a Hilbert space H. The
following conditions are equivalent:
1. {ei} is an orthonormal sequence in H;
2. For an arbitrary x âˆˆH, âŸ¨x, eiâŸ©= 0, k = 1, 2, . . . , =â‡’x = 0;
3. span{ei} is dense in H;
4. The equality (Parseval) of Besselâ€™s inequality (the ï¬rst of Proposition 15);
5. For arbitrary x, y âˆˆH, âŸ¨x, yâŸ©= âˆ
k=1âŸ¨x, ekâŸ©âŸ¨y, ekâŸ©; and
6. For an arbitrary x âˆˆH, x = âˆ
j=1âŸ¨x, e jâŸ©e j.
Proof: See the appendix at the end of this chapter.
Example 39 (Fourier Series Expansion) By approximating f âˆˆL2[âˆ’Ï€, Ï€] by
fm(x) = a0 +
m

n=1
(an cos nx + bn sin nx)
(2.8)
and computing the {an}, {bn} that minimizes âˆ¥f âˆ’fmâˆ¥, we obtain âˆ¥f âˆ’fmâˆ¥â†’0
as m â†’âˆ. However, âˆ¥f âˆ’fmâˆ¥= 0 does not occur for any m. We see that
f /âˆˆspan(A) for A := {1, cos x, sin x, cos 2x, sin 2x, Â· Â· Â· } and that span(A) is not
a closed set. We add all the f /âˆˆspan(A) such that âˆ¥f âˆ’fmâˆ¥â†’0 (convergence
points) for span(A) to obtain the closure span(A). Moreover, span(A) is dense in
L2[âˆ’Ï€, Ï€]. Then,
{
1
âˆš
2Ï€
, cos x
âˆšÏ€ , sin x
âˆšÏ€ , cos 2x
âˆšÏ€ , sin 2x
âˆšÏ€ , Â· Â· Â· }
which are the {1, cos x, sin x, cos 2x, sin 2x, Â· Â· Â· } divided by their norms, form-
ing an orthonormal basis of the Hilbert space L2[âˆ’Ï€, Ï€] that consists of the
functions expressed by the Fourier series as in (2.8), where we regard âŸ¨f, gâŸ©:=
 Ï€
âˆ’Ï€ f (x)g(x)dx as the inner product of f, g âˆˆH, and we use
 Ï€
âˆ’Ï€ cos mx sin nxdx =
0 and
 Ï€
âˆ’Ï€ cos2 mxdx =
 Ï€
âˆ’Ï€ sin2 nxdx = Ï€ for m, n > 0.
Proposition 17 For a Hilbert space H, being separable is equivalent to having an
orthonormal basis.

2.3 Hilbert Spaces
41
Proof: If {e j} forms an orthonormal basis of H, we can express an arbitrary x âˆˆ
H as x = âˆ
j=1âŸ¨x, e jâŸ©e j. E := {x âˆˆH : âŸ¨x, e jâŸ©âˆˆQ, j = 1, 2, . . .} is dense and
countable.Infact,{âŸ¨x, e jâŸ©}and{e j}arecountablesets,asis E,thatis,thecombination
of these two sets, which means that H is separable. On the other hand, if H is
separable, by extracting linearly independent elements from H that are dense and
countable via Gram-Schmidtâ€™s method, we can construct an orthonormal basis {en}.
Thus, we see that any linear combination of these elements is dense in H. From the
third part of Proposition 16, {ei} is an orthonormal basis of H.
â–¡
Proposition 17 implies the following:
Proposition 18 L2[a, b] is separable under the L2 norm.
In this book, we assume that the Hilbert space we deal with is separable.
2.4
Projection Theorem
Let V and M be a linear space equipped with an inner product and its subspace,
respectively. We deï¬ne the orthogonal complement of M as
MâŠ¥:= {x âˆˆV : âŸ¨x, yâŸ©= 0 for all y âˆˆM} .
For subspaces M1 and M2 of V , we write the direct sum of M1, M2 as M1 + M2.
In particular, when M1, M2 are orthogonal to each other, i.e.,
x1 âŠ¥x2 , x1 âˆˆM1 , x2 âˆˆM2 ,
(2.9)
we write it as M1 âŠ•M2 := {x1 + x2 : x1 âˆˆM1, x2 âˆˆM2}.
Proposition 19 (Projection Theorem) Let M be a closed subset of a Hilbert space
H. Then, for an arbitrary x âˆˆH, there exists a y âˆˆM that minimizes âˆ¥x âˆ’yâˆ¥.
Moreover such a y satisï¬es
âŸ¨x âˆ’y, zâŸ©= 0 , z âˆˆM
(2.10)
and is unique.
Proof: Given an x âˆˆH, we consider a y âˆˆM such that x = y + (x âˆ’y), y âˆˆM,
and x âˆ’y âˆˆMâŠ¥. For the proof, we utilize the following steps.
1. Show that a sequence {yn} in M such that
lim
nâ†’âˆâˆ¥x âˆ’ynâˆ¥2 = inf
yâˆˆM âˆ¥x âˆ’yâˆ¥2
is a Cauchy sequence.

42
2
Hilbert Spaces
2. Demonstrate the existence of a y âˆˆM such that yn â†’y âˆˆM.
3. Show that 2aâŸ¨x âˆ’y, z âˆ’yâŸ©â‰¤a2âˆ¥z âˆ’yâˆ¥2 for arbitrary 0 < a < 1 and z âˆˆM,
and derive a contradiction in the inequality when we assume that âŸ¨xâˆ’y, zâˆ’yâŸ©>0.
4. Show that âŸ¨x âˆ’y, zâŸ©â‰¤0.
5. Substitute âˆ’z instead of z to obtain the proposition.
For details, see the appendix at the end of this chapter.
â–¡
Equation(2.10) implies that an arbitrary x âˆˆH can be uniquely decomposed into
x = y + (x âˆ’y) with y âˆˆM and x âˆ’y âˆˆMâŠ¥, which means that
H = M âŠ•MâŠ¥.
(2.11)
Example 40 For a positive deï¬nite kernel k : E Ã— E â†’R and each x âˆˆE, we
regard k(x, Â·) : E â†’R as a function over E. In general, the space spanned by
{k(x, Â·)}xâˆˆE and its closure H := span({k(x, Â·)}xâˆˆE) is a linear space. We show that
the inner product is âŸ¨k(x, Â·), k(y, Â·)âŸ©H = k(x, y) and demonstrate its completeness
in Chap.3. For x1, . . . , xN âˆˆE, M := span({k(xi, Â·)}N
i=1) forms a ï¬nite-dimensional
linear space, and H can be written as (2.11) by using
MâŠ¥= { f âˆˆH|âŸ¨f, k(xi, Â·)âŸ©H = 0, i = 1, . . . , N} .
If f = f1 + f2, for f1 âˆˆM and f2 âˆˆMâŠ¥, we have
âˆ¥f âˆ¥2
H = âˆ¥f1âˆ¥2
H + âˆ¥f2âˆ¥2
H + 2âŸ¨f1, f2âŸ©H = âˆ¥f1âˆ¥2
H + âˆ¥f2âˆ¥2
H â‰¥âˆ¥f1âˆ¥2
H ,
which is used in Chap.4 for kernel calculations.
Proposition 20 Let H and M be a Hilbert space and its subset7, respectively. Then,
we have the following:
1. MâŠ¥is a closed subset of H.
2. M âŠ†(MâŠ¥)âŠ¥.
3. If M is a subspace, then (MâŠ¥)âŠ¥= M, where M is a closure of the set M.
Proof: For the ï¬rst item, we see that MâŠ¥is a subspace. From continuity of an inner
product (Proposition 9), if xn â†’x as n â†’âˆfor a sequence {xn} in MâŠ¥, then we
have for a âˆˆM,
âŸ¨x, aâŸ©= lim
nâ†’âˆâŸ¨xn, aâŸ©= 0 ,
which means that MâŠ¥is closed. The second item is due to
x âˆˆM=â‡’âŸ¨x, yâŸ©= 0 , y âˆˆMâŠ¥=â‡’x âˆˆ(MâŠ¥)âŠ¥.
For the third item, from the ï¬rst two properties, taking the closure on both sides
of M âŠ†(MâŠ¥)âŠ¥yields M âŠ†(MâŠ¥)âŠ¥. From Proposition 19, an arbitrary x âˆˆ(MâŠ¥)âŠ¥
7 This is not necessarily a subspace.

2.4 Projection Theorem
43
can be written as y âˆˆM âˆ©(MâŠ¥)âŠ¥= M and z âˆˆM
âŠ¥âˆ©(MâŠ¥)âŠ¥. However, we have
M
âŠ¥âˆ©(MâŠ¥)âŠ¥âŠ†MâŠ¥âˆ©(MâŠ¥)âŠ¥= {0}, which means that z = 0, and we obtain the
third item.
â–¡
2.5
Linear Operators
Let X1, X2 be linear spaces with norms of âˆ¥Â· âˆ¥1, âˆ¥Â· âˆ¥2, respectively, and let T :
X1 â†’X2 be the map that linearly transforms an element in X1 to an element in X2.
We call such a T a linear operator. We deï¬ne an image and a kernel by
Im(T ) := {T x : x âˆˆX1} âŠ†X2
and
Ker(T ) := {x âˆˆX1 : T x = 0} âŠ†X1 ,
and we call the dimensionality of Im(T ) the rank of T . We say that the linear operator
T : X1 â†’X2 is bounded if for each x âˆˆX1, there exists a constant C > 0 such that
âˆ¥T xâˆ¥2 â‰¤Câˆ¥xâˆ¥1 .
We write the set of such T â€™s as B(X1, X2). In particular, we write B(X1, X2) as
B(X) when X1 = X2 = X.
Proposition 21 A linear operator T is bounded if and only if T is uniformly con-
tinuous.
Proof: If T is uniformly continuous, then there exists a Î´ > 0 such that âˆ¥xâˆ¥1 â‰¤
Î´ =â‡’âˆ¥T xâˆ¥2 â‰¤1. Since âˆ¥Î´x
âˆ¥xâˆ¥âˆ¥â‰¤Î´, we have
âˆ¥T xâˆ¥2 = âˆ¥T ( Î´x
âˆ¥xâˆ¥1
)âˆ¥2
âˆ¥xâˆ¥1
Î´
â‰¤âˆ¥xâˆ¥1
Î´
for any x Ì¸= 0. On the other hand, if T is bounded, there exists a constant C that does
not depend on x âˆˆX1 such that
âˆ¥T (xn âˆ’x)âˆ¥2 â‰¤Câˆ¥xn âˆ’xâˆ¥1
for any {xn} and an x âˆˆX1 such that xn â†’x as n â†’âˆ.
â–¡
Hereafter, we deï¬ne the operator norm of T âˆˆB(X1, X2) by
âˆ¥T âˆ¥:=
sup
xâˆˆX1,âˆ¥xâˆ¥1=1
âˆ¥T xâˆ¥2 .
(2.12)

44
2
Hilbert Spaces
Thus, for an arbitrary x âˆˆX1, we have
âˆ¥T xâˆ¥2 â‰¤âˆ¥T âˆ¥âˆ¥xâˆ¥1 .
Example 41 Let X1 := Rp and X2 := Rq. If the norm is the Euclidean norm, then
we can write the linear operator T : Rp â†’Rq as T : x â†’Bx by using some B âˆˆ
RqÃ—p. If the matrix B is square, the norm âˆ¥T âˆ¥is the square root of the maximum
eigenvalue of the nonnegative deï¬nite matrix A := BâŠ¤B.
âˆ¥T âˆ¥2 = max
âˆ¥xâˆ¥=1 xâŠ¤BâŠ¤Bx = max
âˆ¥xâˆ¥=1 âˆ¥Bxâˆ¥2 .
Example 42 For K : [0, 1]2 â†’R, let
 1
0
 1
0
K 2(x, y)dxdy
be ï¬nite. We deï¬ne the integral operator by the linear operator T in L2[0, 1] such
that
(T f )(Â·) =
 1
0
K(Â·, x) f (x)dx
(2.13)
for f âˆˆL2[0, 1]. Note that (2.13) belongs to L2[0, 1] and that T is bounded: From
|(T f )(x)|2 â‰¤
 1
0
K 2(x, y)dy
 1
0
f 2(y)dy = âˆ¥f âˆ¥2
2
 1
0
K 2(x, y)dy ,
we have
âˆ¥T f âˆ¥2
2 =
 1
0
|(T f )(x)|2dx â‰¤âˆ¥f âˆ¥2
2
 1
0
 1
0
K 2(x, y)dxdy .
We call such a K an integral operator kernel and distinguish between the positive
deï¬nite kernels we deal with in this book.
In particular, we call any linear operator with X2 = R a linear functional.
Proposition 22 (Rieszâ€™s Representation Theorem) Let H be a Hilbert space with
an inner product âŸ¨Â·, Â·âŸ©and a norm of âˆ¥Â· âˆ¥, and let T âˆˆB(H, R). Then, there exists
a unique eT âˆˆH such that
T f = âŸ¨f, eT âŸ©, f âˆˆH
(2.14)
and âˆ¥T âˆ¥= âˆ¥eT âˆ¥.

2.5 Linear Operators
45
Proof: See the appendix at the end of this chapter.
Example 43 (RKHS) Let x âˆˆE, and let Tx : H â†’R be the map from f âˆˆH to
f (x). Then, Tx is linear because
Tx(af + bg) = (af + bg)(x) = af (x) + bg(x) = aTx( f ) + bTx(g) .
We assume that Tx is bounded for each x âˆˆE. Then, from Proposition 22, there
exists a kx âˆˆH such that
f (x) = Tx( f ) = âŸ¨f, kxâŸ©
x âˆˆE, and âˆ¥Txâˆ¥= âˆ¥kxâˆ¥.
Proposition 23 (Adjoint Operator) Let Hi be a Hilbert space with an inner product
âŸ¨Â·, Â·âŸ©i for i = 1, 2 and T âˆˆB(H1, H2). Then, there exists a T âˆ—âˆˆB(H2, H1) such
that
âŸ¨T x1, x2âŸ©2 = âŸ¨x1, T âˆ—x2âŸ©1 , x1 âˆˆH1, x2 âˆˆH2 .
Proof: If we ï¬x x2 âˆˆH2 and regard âŸ¨T x1, x2âŸ©2 as a function of x1 âˆˆH1, then
from x1 â†’âŸ¨T x1, x2âŸ©2 â‰¤âˆ¥x1âˆ¥2âˆ¥x2âˆ¥2, T is a bounded operator w.r.t. x1 âˆˆH1. From
Proposition 22, for each x2 âˆˆH2, there exists y2(x2) âˆˆH1 such that âŸ¨T x1, x2âŸ©2 =
âŸ¨x1, y(x2)âŸ©1. If we deï¬ne T âˆ—x2 = y2(x2), then T âˆ—is a bounded linear map. The
boundness property is due to
âˆ¥T âˆ—x2âˆ¥2
1 = |âŸ¨x2, T T âˆ—x2âŸ©|2 â‰¤âˆ¥T âˆ¥âˆ¥T âˆ—x2âˆ¥1âˆ¥x2âˆ¥2 .
â–¡
We call the T âˆ—in Proposition 23 the adjoint operator of T . In particular, if T âˆ—= T ,
we call such an operator T self-adjoint.
Example 44 Let H = Rp. We can express any T âˆˆB(H) by a square matrix T âˆˆ
RpÃ—p. From
âŸ¨T x, yâŸ©= xâŠ¤T âŠ¤y = âŸ¨x, T âŠ¤yâŸ©,
we see that the adjoint T âˆ—is the transpose matrix of T âŠ¤and that T can be written as
a symmetric matrix if and only if T is self-adjoint.
Example 45 For the integral operator of L2[0, 1] in Example 42, from Fubiniâ€™s
theorem, we have that
âŸ¨T f, gâŸ©=
 1
0
 1
0
K(x, y) f (x)g(y)dxdy = âŸ¨f,
 1
0
K(y, Â·)g(y)dyâŸ©,
and y â†’(T âˆ—g)(y) =
 1
0 K(x, y)g(x)dx is an adjoint operator. If the integral oper-
ator kernel K is symmetric, the operator T is self-adjoint.

46
2
Hilbert Spaces
2.6
Compact Operators
Let (M, d) and E be a metric space and a subset of M, respectively. If any inï¬nite
sequence in E contains a subsequence that converges to an element in E, then we
say that E is sequentially compact. If {xn} has a subsequence that converges to x,
then x is a convergence point of {xn}.
Example 46 Let E := R and d(x, y) := |x âˆ’y| for x, y âˆˆR. Then, E is not
sequentially compact. In fact, the sequence xn = n has no convergence points. For
E = (0, 1], the sequence xn = 1/n converges to 0 /âˆˆ(0, 1] as n â†’âˆ, and the con-
vergence point of any subsequence is only 0. Therefore, E = (0, 1] is not sequentially
compact.
Proposition 24 Let (M, d) and E be a metric space and a subset of M, respectively.
Then, E is sequentially compact if and only if E is compact.
Proof: Many books on geometry deal with the proof of equivalence. See such books
for the details of this proof.
In this section, we explain compactness by using the terminology of sequential
compactness.
Let X1, X2 be linear spaces equipped with norms, and let T âˆˆB(X1, X2). We
say that T is compact if {T xn} contains a convergence subsequence for any bounded
sequence {xn} in X1.
Example 47 The orthonormal basis {e j} in a Hilbert space H is bounded because
âˆ¥e jâˆ¥= 1. However, for an identity map, we have that âˆ¥ei âˆ’e jâˆ¥=
âˆš
2 for any i Ì¸= j.
Thus, the sequence e1, e2, . . . does not have any convergence points in H. Hence,
the identity operator for any inï¬nite-dimensional Hilbert space is not compact.
Proposition 25 For any bounded linear operator T , the following hold.
1. If the rank is ï¬nite, then the operator T is compact.
2. If a sequence of ï¬nite-rank operators {Tn} exists such that âˆ¥Tn âˆ’T âˆ¥â†’0 as
n â†’âˆ, then T is compact8.
Proof: See the appendix at the end of this chapter.
Let H and T âˆˆB(H) be a Hilbert space and its bounded linear operator, respec-
tively. If Î» âˆˆR and 0 Ì¸= e âˆˆH exist such that
T e = Î»e ,
(2.15)
then we say that Î» and e are an eigenvalue and an eigenvector of T , respectively.
Proposition 26 Let T âˆˆB(H) and e j âˆˆKer(T âˆ’Î» j I) for j = 1, 2, . . .. If the
eigenvalues Î» j Ì¸= 0 have different values, then
8 It is known that the converse is true.

2.6 Compact Operators
47
1. e j is linearly independent.
2. If T is self-adjoint, then {e j} are orthogonal.
Proof: See the appendix at the end of this chapter.
Example 48 Let T âˆˆB(H) be a compact operator. For each eigenvalue Î» Ì¸= 0,
the eigenspace Ker(T âˆ’Î»I) has a ï¬nite dimensionality. In fact, if Ker(T âˆ’Î»I) is
of inï¬nite dimensionality for an eigenvalue Î» Ì¸= 0, then Î» contains inï¬nitely many
eigenvectors e j, and if we apply the operator T to them, then as in Example 47,
{Î»e j} does not have any convergence subsequence. Thus, T is not compact, which
is a contradiction.
Example 49 For any C > 0, the absolute values of a ï¬nite number of eigenvalues
Î»i for a compact operator T exceed C. Suppose that the absolute values of an inï¬nite
number of eigenvalues Î»1, Î»2, . . . exceed C. Let M0 := {0}, Mi := span{e1, . . . , ei},
e j âˆˆKer(T âˆ’Î» j I), j = 1, 2, . . ., i = 1, 2, . . .. Since the {e1, . . . , ei} are linearly
independent, each Mi âˆ©MâŠ¥
iâˆ’1 is one dimensional for i = 1, 2, . . .. Thus, if we
deï¬ne the orthonormal sequence xi âˆˆKer(T âˆ’Î»i I) âˆ©MâŠ¥
iâˆ’1, i = 1, 2, . . . via Gram-
Schmidt, then we have
âˆ¥T xi âˆ’T xkâˆ¥2 = âˆ¥T xiâˆ¥2 + âˆ¥T xkâˆ¥2 â‰¥2C2
for i > k. Thus, {T xi} has no convergence subsequence.
Example 49 implies that the set of nonzero eigenvalues of T is countable.
We summarize the above discussion and its implications below.
Proposition 27 Let T be a self-adjoint compact operator of a Hilbert space H.
Then, the set of nonzero eigenvalues of T is ï¬nite, or the sequence of eigenvalues
converges to zero. Each eigenvalue has a ï¬nite multitude, and any pair of eigen-
vectors corresponding to different eigenvalues is an orthogonal pair. Let Î»1, Î»2, . . .
be a sequence of eigenvalues such that |Î»1| â‰¥|Î»2| â‰¥Â· Â· Â· , and let e1, e2, . . . be any
corresponding eigenvectors that are orthogonal (orthogonalized eigenvectors via
Gram-Schmidt) if they possess the same eigenvalue. Then, {e j} is the orthonormal
basis of Im(T ), and we can express T by
T x =
âˆ

j=1
Î» jâŸ¨x, e jâŸ©e j
(2.16)
for each x âˆˆH.
Proof: We utilize the following steps, where the second item is equivalent to
(Ker(T ))âŠ¥= Im(T ) because T = T âˆ—.
1. Show that H = Ker(T ) âŠ•(Ker(T ))âŠ¥.
2. Show that (Ker(T ))âŠ¥= Im(T âˆ—).

48
2
Hilbert Spaces
3. Show that span{e j| j â‰¥1} âŠ†Im(T ).
4. Show that span{e j| j â‰¥1} âŠ‡Im(T ).
See the appendix at the end of this chapter.
â–¡
We say that an operator T is nonnegative deï¬nite if
âŸ¨T x, xâŸ©= âŸ¨
âˆ

i=1
Î»iâŸ¨x, eiâŸ©ei,
âˆ

j=1
âŸ¨x, e jâŸ©e jâŸ©=
âˆ

i=1
Î»iâŸ¨x, eiâŸ©2 â‰¥0
for arbitrary H âˆ‹x = âˆ
i=1âŸ¨x, eiâŸ©ei; this condition is equivalent to Î»1 â‰¥0,
Î»2 â‰¥0, . . ..
Proposition 28 If T is nonnegative deï¬nite, we have
Î»k =
max
eâˆˆspan{e1,...,ekâˆ’1}âŠ¥
âŸ¨T e, eâŸ©
âˆ¥eâˆ¥2
(2.17)
which expresses the maximum value over the Hilbert space H when k = 1.
Proof: The claim follows from (2.16) and Î» j â‰¥0:
max
eâˆˆ{e1,...,ekâˆ’1}âŠ¥âˆ¥eâˆ¥=1âŸ¨T e, eâŸ©= max
âˆ¥eâˆ¥=1
âˆ

j=k
Î» jâŸ¨e, e jâŸ©2 = Î»k .
â–¡
Let H1, H2 beHilbertspaces,{ei}anorthonormalbasisof H1,and T âˆˆB(H1, H2).
If
âˆ

i=1
âˆ¥T eiâˆ¥2
takes a ï¬nite value, we say that T is a Hilbert-Schmidt (HS) operator, and we write
the set of HS operators in B(H1, H2) as BH S(H1, H2).
We deï¬ne the inner product of T1, T2 âˆˆBH S(H1, H2) and the HS norm of T âˆˆ
BH S(H1, H2) by âŸ¨T1, T2âŸ©H S := âˆ
j=1âŸ¨T1e j, T2e jâŸ©2 and
âˆ¥T âˆ¥H S := âŸ¨T, T âŸ©1/2
H S =
 âˆ

i=1
âˆ¥T eiâˆ¥2
2
1/2
,
respectively.
Proposition 29 The HS norm value of T âˆˆB(H1, H2) does not depend on the choice
of orthonormal basis {ei}.
Proof: Let {e1,i}, {e2, j} be arbitrary orthonormal bases of Hilbert spaces H1, H2,
and let T1, T2 âˆˆB(H1, H2). Then, for Tke1,i = âˆ
j=1âŸ¨Tke1,i, e2, jâŸ©2e2, j, T âˆ—
k e2, j =
âˆ
i=1âŸ¨T âˆ—
k e2, j, e1,iâŸ©1e1,i, and k = 1, 2, we have

2.6 Compact Operators
49
âˆ

i=1
âŸ¨T1e1,i, T2e1,iâŸ©2 =
âˆ

i=1
âˆ

j=1
âŸ¨T1e1,i, e2, jâŸ©2âŸ¨T2e1,i, e2, jâŸ©2
=
âˆ

i=1
âˆ

j=1
âŸ¨e1,i, T âˆ—
1 e2, jâŸ©1âŸ¨e1,i, T âˆ—
2 e2, jâŸ©1 =
âˆ

i=1
âŸ¨T âˆ—
1 e2, j, T âˆ—
2 e2, jâŸ©1 ,
which means that both sides do not depend on the choices of {e1,i}, {e2, j}. In particu-
lar,if T1 = T2 = T ,weseethatâˆ¥T âˆ¥2
H S doesnotdependonthechoicesof{e1,i}, {e2, j}.
â–¡
Proposition 30 An HS operator is compact.
Proof: Let T âˆˆB(H1, H2) be an HS operator, x âˆˆH1, and
Tnx :=
n

i=1
âŸ¨T x, e2iâŸ©2e2i ,
where {e2i} is an orthonormal basis of H2. Since the image of Tn is of ï¬nite
dimensionality, Tn is compact. Thus, from the second item of Proposition25, it is
sufï¬cient to show that âˆ¥T âˆ’Tnâˆ¥â†’0 as n â†’âˆ. However, since (T âˆ’Tn)x =
âˆ
i=n+1âŸ¨T x, e2,iâŸ©2e2,i, we have that when âˆ¥xâˆ¥1 â‰¤1,
âˆ¥(T âˆ’Tn)xâˆ¥2
2 =
âˆ

i=n+1
âŸ¨T x, e2,iâŸ©2
2 =
âˆ

i=n+1
âŸ¨x, T âˆ—e2,iâŸ©2
1 â‰¤
âˆ

i=n+1
âˆ¥T âˆ—e2,iâˆ¥2 .
Because T âˆ—is an HS operator, the right-hand side converges to zero, where T is an
HS operator if and only if T âˆ—is an HS operator due to the derivation in Proposition29.
â–¡
Example 50 When an operator is expressed by a matrix T = (Ti, j) such that T âˆˆ
B(Rm, Rn), m, n â‰¥1, the HS norm becomes the squared sum of the mn elements
of this matrix. In fact, if T is expressed by a matrix RnÃ—m, then the HS norm is the
Frobenius norm:
âˆ¥T âˆ¥2
H S =
n

i=1
âˆ¥T eX,iâˆ¥2 =
n

j=1
âˆ¥T âˆ—eY, jâˆ¥2 =
m

i=1
n

j=1
T 2
i, j ,
where eX,i âˆˆRm is a column vector such that the ith element is one and the other
elements are zeros, and eY, j âˆˆRn is a column vector such that the jth element is one
and the other elements are zeros.
Let T âˆˆB(H) be nonnegative deï¬nite and {ei} be an orthonormal basis of H. If
âˆ¥T âˆ¥T R :=
âˆ

j=1
âŸ¨T e j, e jâŸ©

50
2
Hilbert Spaces
is ï¬nite, we say that âˆ¥T âˆ¥T R is the trace norm of T and that T is a trace class. Similar to
an HS norm value, a trace norm value does not depend on the choice of orthonormal
basis {e j}.
If we substitute x = e j into (2.16) in Proposition 27, then we have T x = Î»e j and
obtain that
âˆ¥T âˆ¥T R :=
âˆ

j=1
âŸ¨T e j, e jâŸ©=
âˆ

j=1
Î» j .
On the other hand, from
âˆ¥T âˆ¥2
H S =
âˆ

i=1
âˆ

j=1
âŸ¨T ei,1, e j,2âŸ©2 =
âˆ

j=1
Î»2
j,
we have
âˆ¥T âˆ¥H S â‰¤

Î»1
âˆ

i=1
Î»i
1/2
=

Î»1âˆ¥T âˆ¥T R .
Thus, we have established the following proposition.
Proposition 31 If T âˆˆB(H) is a trace class, it is a compact HS class.
Appendix: Proofs of Propositions
Proof of Proposition13
We show that a simple function approximates an arbitrary f âˆˆL2
2 and that a contin-
uous function approximates an arbitrary simple function. Hereafter, we denote the
L2 norm by âˆ¥Â· âˆ¥.
Since f âˆˆL2 is measurable, if f is nonnegative, the sequence { fn} of simple
functions deï¬ned by
fn(Ï‰) =
(k âˆ’1)2âˆ’n, (k âˆ’1)2âˆ’n â‰¤f (Ï‰) < k2âˆ’n, 1 â‰¤k â‰¤n2n
n,
n â‰¤f (Ï‰) â‰¤âˆ
satisï¬es 0 â‰¤f1(Ï‰) â‰¤f2(Ï‰) â‰¤Â· Â· Â· â‰¤f (Ï‰) and | fn(Ï‰) âˆ’f (Ï‰)|2 â†’0 almost surely.
Since the right-hand side of | fn(Ï‰) âˆ’f (Ï‰)|2 â‰¤4{ f (Ï‰)}2 is ï¬nite when integrated,
from the dominant convergence theorem, we have
âˆ¥fn âˆ’f âˆ¥2 â†’0 .
We can show a similar derivation for a general f that is not necessarily nonnegative,
as derived in Chap.1.

Appendix: Proofs of Propositions
51
On the other hand, let A be a closed subset of [a, b], and let K A be the indi-
cator function (K A(e) = 1 if e âˆˆA; K A(e) = 0 otherwise). If we deï¬ne h(x) :=
inf yâˆˆA{|x âˆ’y|} and g A
n (x) :=
1
1 + nh(x), then g A
n is continuous, g A
n (x) â‰¤1 for
x âˆˆ[a, b], g A
n (x) = 1 for x âˆˆA, and lim
nâ†’âˆg A
n (x) = 0 for x âˆˆB := [a, b]\A. Thus,
we have
lim
nâ†’âˆâˆ¥g A
n âˆ’K Aâˆ¥= lim
nâ†’âˆ

B
g A
n (x)2dx
1/2
=

B
lim
nâ†’âˆg A
n (x)2dx
1/2
= 0 ,
where the second equality follows from the dominant convergence theorem. More-
over, if A, Aâ€² are disjoint, then Î±g A
n + Î±â€²g Aâ€²
n with Î±, Î±â€² > 0 approximates Î±K A +
Î±â€²K Aâ€². In fact, we have
âˆ¥Î±g A
n + Î±â€²g Aâ€²
n âˆ’(Î±K A + Î±â€²K Aâ€²)âˆ¥â‰¤Î±âˆ¥g A
n âˆ’K Aâˆ¥+ Î±â€²âˆ¥g Aâ€²
n âˆ’K Aâ€²âˆ¥.
Hence, a sequence of continuous functions can approximate an arbitrary simple
function.
â–¡
Proof of Proposition14
Suppose that { fn} is a Cauchy sequence in L2, which means that
lim
Nâ†’âˆsup
m,nâ‰¥N
âˆ¥fm âˆ’fnâˆ¥2 = 0 .
(2.18)
Then, there exists a sequence {nk} such that

âˆ

k=1
| fnk+1 âˆ’fnk|

2
â‰¤
âˆ

k=1
âˆ¥fnk+1 âˆ’fnkâˆ¥2 < âˆ.
Thus, almost surely, we have
âˆ

k=1
| fnk+1(x) âˆ’fnk(x)| < âˆ.
(2.19)
For arbitrary r < t and x âˆˆE, from the triangle inequality, we have
| fnr (x) âˆ’fnt(x)| â‰¤
tâˆ’1

k=r
| fnk+1(x) âˆ’fnk(x)| .
Combined with (2.19), the real sequence { fnk(x)}âˆ
k=1 is almost surely Cauchy. Since
the entire real system is complete (Proposition 6), we deï¬ne f (x) := limkâ†’âˆfnk(x)

52
2
Hilbert Spaces
for x âˆˆE such that { fnk(x)}âˆ
k=1 is Cauchy, and we deï¬ne f (x) := 0 for the other
x âˆˆE. From (2.18), for an arbitrary Ïµ > 0, we have
âˆ¥f âˆ’fnâˆ¥2 =

E
| fn âˆ’f |2dÎ¼ =

E
lim inf
kâ†’âˆ| fn âˆ’fnk |2dÎ¼ â‰¤lim inf
kâ†’âˆ

E
| fn âˆ’fnk |2dÎ¼ < Ïµ
as n â†’âˆ, where the ï¬rst inequality is due to Fatouâ€™s lemma. Furthermore, since
fn, f âˆ’fn âˆˆL2 and L2 is a linear space, we have f âˆˆL2.
â–¡
Proof of Proposition15
The ï¬rst item holds because
0 â‰¤âˆ¥x âˆ’
n

i=1
âŸ¨x, eiâŸ©eiâˆ¥2 = âˆ¥xâˆ¥2 âˆ’
n

i=1
âŸ¨x, eiâŸ©2
for all n. For the second item, letting n > m, sn := n
k=1âŸ¨x, ekâŸ©ek, we have
âˆ¥sn âˆ’smâˆ¥2 = âŸ¨
n

k=m+1
âŸ¨x, ekâŸ©ek,
n

k=m+1
âŸ¨x, ekâŸ©ekâŸ©=
n

k=m+1
|âŸ¨x, ekâŸ©|2,
which diminishes as n, m â†’âˆaccording to the ï¬rst item. For the third item, we
have
âˆ¥sn âˆ’smâˆ¥2 = âŸ¨
n

k=m+1
Î±kek,
n

k=m+1
Î±kekâŸ©=
n

k=m+1
Î±2
k = Sn âˆ’Sm
for sn := n
i=1 Î±iei, Sn := n
i=1 Î±2
i , and n > m. Thus, the third item follows from
the equivalence: {sn} is Cauchy â‡â‡’{Sn} is Cauchy.
The last item holds because âŸ¨y, eiâŸ©= lim
nâ†’âˆâŸ¨
n

j=1
Î± je j, eiâŸ©= Î±i for y = âˆ
j=1 Î± je j,
which follows from the continuity of inner products (Proposition 9).
â–¡
Proof of Proposition16
For 1.=â‡’6., since {ei} is an orthonormal basis of H, we may write an arbitrary
x âˆˆH as x = âˆ
i=1 Î±iei, Î±i âˆˆR. From the fourth item of Proposition 15, we have
Î±i = âŸ¨x, eiâŸ©and obtain 6. 6.=â‡’5. is obtained by substituting x = âˆ
i=1âŸ¨x, eiâŸ©ei,
y = âˆ
i=1âŸ¨y, eiâŸ©ei into âŸ¨x, yâŸ©. 5.=â‡’4. is obtained by substituting x = y in 5. 4.=â‡’3.
is due to
âˆ¥x âˆ’
n

k=1
âŸ¨x, ekâŸ©ekâˆ¥2 = âˆ¥xâˆ¥2 âˆ’
n

k=1
|âŸ¨x, ekâŸ©|2 â†’0
as n â†’âˆfor each x âˆˆH. For 3.=â‡’2., note the implication âŸ¨x, ekâŸ©= 0, k = 1, 2, . . .
=â‡’x âŠ¥span{ek}, which implies that x âŠ¥span{ek} from the continuity of inner prod-

Appendix: Proofs of Propositions
53
ucts (Proposition 9). Thus, we have âŸ¨x, xâŸ©= 0 and x = 0. For 2.=â‡’1., from the sec-
ond item of Proposition 15, y = âˆ
i=1âŸ¨z, eiâŸ©ei converges for each z âˆˆH. Therefore,
for each j, we have
âŸ¨z âˆ’y, e jâŸ©= âŸ¨z, e jâŸ©âˆ’lim
nâ†’âˆâŸ¨
n

i=1
âŸ¨z, eiâŸ©, e jâŸ©= âŸ¨z.e jâŸ©âˆ’âŸ¨z.e jâŸ©= 0 .
From the assumption of 2., we have that z âˆ’y = 0 and that z can be written as
âˆ
i=1âŸ¨z, eiâŸ©ei.
â–¡
Proof of Proposition19
Let M be a closed subset of H. We show that for each x âˆˆH, there exists a unique
y âˆˆM that minimizes âˆ¥x âˆ’yâˆ¥and that we have
âŸ¨x âˆ’y, z âˆ’yâŸ©â‰¤0
(2.20)
for z âˆˆM. To this end, we ï¬rst show that any sequence {yn} in M for which
lim
nâ†’âˆâˆ¥x âˆ’ynâˆ¥2 = inf
yâˆˆM âˆ¥x âˆ’yâˆ¥2
(2.21)
is Cauchy. Since M is a linear space, we have (yn + ym)/2 âˆˆM and
âˆ¥yn âˆ’ymâˆ¥2 = 2âˆ¥x âˆ’ynâˆ¥2 + 2âˆ¥x âˆ’ymâˆ¥2 âˆ’4âˆ¥x âˆ’yn + ym
2
âˆ¥2
â‰¤2âˆ¥x âˆ’ynâˆ¥2 + 2âˆ¥x âˆ’ymâˆ¥2 âˆ’4 inf
yâˆˆM âˆ¥x âˆ’yâˆ¥2 â†’0 .
Hence, {yn} is Cauchy. Then, suppose that more than one lower limit y exists, and
let u Ì¸= v be such a y. For example, for {yn}, let y2mâˆ’1 â†’u, and let y2m â†’v satisfy
(2.21). However, this limit is not Cauchy and contradicts the discussion shown thus
far. Hence, the y that achieves the limit in (2.21) is unique. In the following, we
assume that y gives the lower limit.
Moreover, note that
âˆ¥x âˆ’{az + (1 âˆ’a)y}âˆ¥2 â‰¥âˆ¥x âˆ’yâˆ¥2 â‡â‡’2aâŸ¨x âˆ’y, z âˆ’yâŸ©â‰¤a2âˆ¥z âˆ’yâˆ¥2
for arbitrary 0 < a < 1 and z âˆˆM, and if âŸ¨x âˆ’y, z âˆ’yâŸ©> 0, the inequality ï¬‚ips
for small a > 0. Thus, we have âŸ¨x âˆ’y, z âˆ’yâŸ©â‰¤0.
Finally, if we substitute z = 0, 2y into (2.20), we have âŸ¨x âˆ’y, yâŸ©= 0. Therefore,
(2.20) implies that âŸ¨x âˆ’y, zâŸ©â‰¤0 for z âˆˆM. We obtain the proposition by replacing
z with âˆ’z.
â–¡

54
2
Hilbert Spaces
Proof of Proposition22
If the operator T maps to zero for any element, then eT = 0 satisï¬es the desired
condition. Thus, we assume that T outputs a nonzero value for at least one input.
From the ï¬rst item of Proposition 20, Ker(T )âŠ¥is a closed subset of H and contains
a y such that T y = 1. Thus, for an arbitrary x âˆˆH, we have
T (x âˆ’(T x)y) = T x âˆ’T xT y = 0
and x âˆ’(T x)y âˆˆKer(T ). Since y âˆˆKer(T )âŠ¥, we have âŸ¨x âˆ’(T x)y, yâŸ©= 0 and
âŸ¨x, yâŸ©= T xâŸ¨y, yâŸ©= T xâˆ¥yâˆ¥2 .
Thus, eT = y/âˆ¥yâˆ¥2 satisï¬es the desired condition.
To demonstrate uniqueness, if eâ€²
T satisï¬es the same condition, then âŸ¨x, eT âˆ’eâ€²
T âŸ©=
0 for any x âˆˆH, which means that eT = eâ€²
T .
Furthermore, since âˆ¥T xâˆ¥= âŸ¨x, eT âŸ©â‰¤âˆ¥xâˆ¥âˆ¥eT âˆ¥for x âˆˆH, we have that âˆ¥T âˆ¥â‰¤
âˆ¥eT âˆ¥when âˆ¥xâˆ¥= 1. Additionally, we obtain the inverse inequality âˆ¥eT âˆ¥=
1
âˆ¥yâˆ¥=
âˆ¥T yâˆ¥
âˆ¥yâˆ¥â‰¤âˆ¥T âˆ¥.
â–¡
Proof of Proposition25
For the ï¬rst item, note that if {xn} is bounded, so is {T xn}. Moreover, if the image
of T is of ï¬nite dimensionality, then {T xn} is also compact (Proposition 7)9. For the
second item, we use the so-called diagonal argument. In the following, we denote the
norms of H1, H2 by âˆ¥Â· âˆ¥1, âˆ¥Â· âˆ¥2. Let {xk} be a bounded sequence in X1. From the
compactness of T1, there exists {x1,k} âŠ†{x0,k} := {xk} such that {T1x1,k} converges to
a y1 âˆˆH2 as k â†’âˆ. Then, there exists {x2,k} âŠ†{x1,k} such that {T2x2,k} converges
to a y2 âˆˆH2 as k â†’âˆ. If we repeat this process, the sequence {yn} in H2 converges.
In fact, for each n, there exists a large kn such that
âˆ¥Tnxn,k âˆ’ynâˆ¥2 < 1
n , k â‰¥kn .
If we make {kn} monotone, then for m < n, we obtain
ym âˆ’yn=(ym âˆ’Tmxn,kn)+(Tnxn,kn âˆ’yn) + (Tmxn,kn âˆ’T xn,kn) + (T xn,kn âˆ’Tnxn,kn) .
Thus, as m, n â†’âˆ, we have
âˆ¥ym âˆ’ynâˆ¥â‰¤1
m + 1
n + âˆ¥Tm âˆ’T âˆ¥Â· âˆ¥xn,knâˆ¥1 + âˆ¥Tn âˆ’T âˆ¥Â· âˆ¥xn,knâˆ¥1 â†’0
. Since H2 is complete, there exists a y âˆˆH2 such that {yn} converges. Since
9 This statement is called Bolzano-Weierstrassâ€™s theorem for sequential compactness rather than
Heine-Borelâ€™s theorem. The two theorems coincide for metric spaces.

Appendix: Proofs of Propositions
55
âˆ¥T xn,kn âˆ’yâˆ¥2 â‰¤âˆ¥T âˆ’Tnâˆ¥âˆ¥xn,knâˆ¥1 + âˆ¥Tnxn,kn âˆ’ynâˆ¥2 + âˆ¥yn âˆ’yâˆ¥2 â†’0
as n â†’âˆ, we have shown that {T xn} has a convergent subsequence in H2.
â–¡
Proof of Proposition26
By induction, we show that
n

j=1
c je j = 0=â‡’c1 = c2 = Â· Â· Â· = cn = 0 .
(2.22)
For n = 2, suppose that c1e1 + c2e2 = 0. Then, we have T (c1e1 + c2e2) = Î»1c1e1 +
Î»2c2e2 = 0. From these two equations and Î»1 Ì¸= Î»2, we have c1 = c2 = 0. Thus, we
obtain (2.22) for n = 2. For n = k, k+1
j=1 c je j = 0 and k+1
j=1 Î» jc je j = 0 imply that
0 = Î»k+1
k+1

j=1
c je j âˆ’
k+1

j=1
Î» jc je j =
k

j=1
(Î»k+1 âˆ’Î» j)c je j .
From Î»k+1Ì¸=Î» j, if we assume that câ€²
j := (Î»k+1 âˆ’Î» j)c jÌ¸=0, then from k
j=1 câ€²
je j = 0
and the assumption of induction, we have câ€²
1 = Â· Â· Â· = câ€²
k = 0, which means that c1 =
Â· Â· Â· = ck = 0 and ck+1ek+1 = âˆ’k
j=1 c je j = 0. Thus ck+1 = 0. Moreover, under
the condition that T is self-adjoint, from âŸ¨ei, e jâŸ©= âŸ¨ei, Î»âˆ’1
j T e jâŸ©= Î»âˆ’1
j âŸ¨T ei, e jâŸ©=
Î»âˆ’1
j Î»iâŸ¨ei, e jâŸ©and Î»i Ì¸= Î» j for i Ì¸= j, we have âŸ¨ei, e jâŸ©= 0. Thus, the {e j} are orthog-
onal.
â–¡
Proof of Proposition27
We ï¬rst show that
Ker(T )âŠ¥= Im(T âˆ—) .
(2.23)
For x1 âˆˆKer(T ) and x2 âˆˆH, we see that âŸ¨x1, T âˆ—x2âŸ©1 = âŸ¨T x1, x2âŸ©2 = 0 and that x1
is orthogonal to any element of Im(T âˆ—). Thus, we have
Ker(T ) âŠ†(Im(T âˆ—))âŠ¥
. Moreover, if x1 âˆˆ(Im(T âˆ—))âŠ¥, from T âˆ—(T x1) âˆˆIm(T âˆ—), we have
âˆ¥T x1âˆ¥2 = âŸ¨x1, T âˆ—T x1âŸ©1 = 0 ,
which means that x1 âˆˆKer(T ) and establishes inverse inclusion. Thus, we have
shown that Ker(T ) = (Im(T âˆ—))âŠ¥. Furthermore, if we apply the third item of Propo-
sition 20, we obtain
(Ker(T ))âŠ¥= Im(T âˆ—) .

56
2
Hilbert Spaces
Note that since Ker(T ) is an orthogonal complement of subset Im(T âˆ—) of H, the ï¬rst
item of Proposition 20 and (2.11) can be applied. Since T âˆˆB(H) is self-adjoint
(T âˆ—= T ), we can write (2.23) further as
H = Ker(T ) âŠ•Im(T ) .
Hence, in order to show that (2.16), it is sufï¬cient to prove that
Im(T ) = span{e j : j â‰¥1} .
(2.24)
Note that for each ï¬nite n = 1, 2, . . . and c1, c2. . . . , cn âˆˆR, we have
n

j=1
c je j = T (
n

j=1
Î»âˆ’1
j c je j)
and span{e j| j â‰¥1} âŠ†Im(T ). Even if we perform closure on both sides, the inclusion
relation does not change. Thus, we have span{e j| j â‰¥1} âŠ†Im(T ). Furthermore, we
decompose (2.11)
Im(T ) = span{e j| j â‰¥1} âŠ•N ,
where N = span{e j| j â‰¥1}
âŠ¥âˆ©Im(T ). Note that T y âˆˆspan{e j| j â‰¥1} for y âˆˆspan
{e j| j â‰¥1}, and
âŸ¨T x, yâŸ©= âŸ¨x, T yâŸ©= 0
for x âˆˆN because T is self-adjoint. Thus, we have T x âˆˆN.
Now, in general, we have
âˆ¥T âˆ¥= w(T ) := sup
âˆ¥xâˆ¥=1
|âŸ¨T x, xâŸ©| .
(2.25)
In fact,
|âŸ¨T x, yâŸ©| = |1
4âŸ¨T (x + y), x + yâŸ©âˆ’1
4âŸ¨T (x âˆ’y), x âˆ’yâŸ©|
â‰¤1
4|âŸ¨T (x + y), x + yâŸ©| + |1
4âŸ¨T (x âˆ’y), x âˆ’yâŸ©|
â‰¤1
4(w(T )(âˆ¥x + yâˆ¥2 + âˆ¥x âˆ’yâˆ¥2) = 1
2w(T )(âˆ¥xâˆ¥2 + âˆ¥yâˆ¥2),
and if we take the upper limit under âˆ¥xâˆ¥= âˆ¥yâˆ¥= 1, we obtain
âˆ¥T âˆ¥= sup
âˆ¥xâˆ¥=1
âŸ¨T x,
T x
âˆ¥T xâˆ¥âŸ©â‰¤
sup
âˆ¥xâˆ¥=âˆ¥yâˆ¥=1
âŸ¨T x, yâŸ©â‰¤w(T ) .

Appendix: Proofs of Propositions
57
On the other hand, we have
w(T ) â‰¤sup
âˆ¥xâˆ¥=1
âˆ¥T xâˆ¥Â· âˆ¥xâˆ¥= sup
âˆ¥xâˆ¥=1
âˆ¥T xâˆ¥= âˆ¥T âˆ¥
and (2.25).
In addition, we know that either Â±âˆ¥T âˆ¥is an eigenvalue of T . In fact, from (2.25),
there exists a sequence {xn} in H with âˆ¥xnâˆ¥= 1 such that âŸ¨T xn, xnâŸ©â†’âˆ¥T âˆ¥or
âŸ¨T xn, xnâŸ©â†’âˆ’âˆ¥T âˆ¥(the upper and lower limits are convergence points). For the
former case, we have
0 â‰¤âˆ¥T xn âˆ’âˆ¥T âˆ¥xnâˆ¥2 = âˆ¥T xnâˆ¥2 + âˆ¥T âˆ¥2âˆ¥xnâˆ¥2 âˆ’2âˆ¥T âˆ¥âŸ¨T xn, xnâŸ©â†’0 .
From compactness of T , there exists {xnk} (âŠ†{xn}) such that T xnk â†’y âˆˆH. From
T xnk âˆ’âˆ¥T âˆ¥xnk â†’0, there exists 0 Ì¸= x âˆˆH such that âˆ¥T âˆ¥xnk â†’âˆ¥T âˆ¥x. From
âˆ¥T âˆ¥x = y = limkâ†’âˆT xnk, we have that T x = âˆ¥T âˆ¥x and that âˆ¥T âˆ¥is an eigen-
value of T . For the latter case, âˆ’âˆ¥T âˆ¥is an eigenvalue of T .
Finally, we assume that there exists an x âˆˆN such that âˆ¥T xâˆ¥Ì¸= 0. Let TN be the
restriction of T on N. Because âˆ¥TNâˆ¥> 0, either âˆ¥TNâˆ¥or âˆ’âˆ¥TNâˆ¥is an eigenvalue
of T . The existence of an eigenvalue on N contradicts the chosen orthonormal basis
{e j}âˆ
j=1. Therefore, when x âˆˆN, we have T x = 0, which means that N âŠ†Im(T ) âˆ©
Ker(T ) = {0}. Thus, we have established (2.16).
â–¡
Exercises 16âˆ¼30
16. Choose the closed sets among the sets below. For the nonclosed sets, ï¬nd their
closures.
(a) âˆªâˆ
n=1[n âˆ’1
n , n + 1
n ];
(b) {2, 3, 5, 7, 11, 13, . . .};
(c) R âˆ©Z;
(d) {(x, y) âˆˆR2| x2 + y2 < 1 when x â‰¥0, x2 + y2 â‰¤1 when x < 0 }.
17. Show that the sequence a1 = 1, an+1 = 1
2an + 1
an
converges to
âˆš
2 as n â†’âˆ.
18. Let f : M â†’R be a function deï¬ned over a bounded closed set M, and we
deï¬ne (z1), . . . , (zm) for some m â‰¥1 and z1, . . . , zm such that
d(x, z) < (z)=â‡’d( f (x), f (z)) < Ïµ
for z âˆˆM.

58
2
Hilbert Spaces
(a) Why can the neighborhoods cover M ?
Let x, y âˆˆM satisfy d1(x, y) < Î´ := 1
2 min
1â‰¤iâ‰¤m (zi). Without loss of generality,
we assume that x âˆˆUi with a center at zi and a radius of (zi)/2. Prove the
following.
(a) d1(x, zi) < 1
2(zi) < (zi).
(b) d1(y, zi) â‰¤d1(x, y) + d1(x, zi) < (zi).
(c) d2( f (x), f (y)) â‰¤d2( f (x), f (zi)) + d2( f (y), f (zi)) < Ïµ + Ïµ = 2Ïµ.
(d) f is uniformly continuous.
19. Usingthefactthatanycontinuousfunctionoveraboundedclosedsetisuniformly
continuous, show that a continuous function over [0, 1] is a Riemann integral.
20. that the Cauchy-Schwarz inequality (2.5) holds if and only if one of x, y is a
constant multiplied by the other.
21. Show that a one-indeterminate polynomial ring A is an algebra. In addition,
show that the set of functions f âˆˆA over E := [0, 1] is dense in C(E).
22. Derive Riesz-Fischerâ€™s theorem stating that â€œL2 is completeâ€ (Proposition 14)
according to the following steps in the appendix.
(a) Let { fn} be an arbitrary Cauchy sequence.
(b) There exists a sequence {nk} such that âˆ¥âˆ
k=1 | fnk+1 âˆ’fnk|âˆ¥2 < âˆ.
(c) Prove the existence of an f : E â†’R such that Î¼{x âˆˆE| limkâ†’âˆfnk(x) =
f (x)} = Î¼(E).
(d) Show that âˆ¥fn âˆ’f âˆ¥â†’0 and f âˆˆL2[a, b].
23. Show that the basis of the Fourier series expansion
{
1
âˆš
2Ï€
, cos x
âˆšÏ€ , sin x
âˆšÏ€ , cos 2x
âˆšÏ€ , sin 2x
âˆšÏ€ , Â· Â· Â· }
is orthonormal.
24. Derive Proposition 19 according to the following steps in the appendix. What
are the derivations of (a) through (e)?
(a) Show that a sequence {yn} in M for which
lim
nâ†’âˆâˆ¥x âˆ’ynâˆ¥2 = inf
yâˆˆM âˆ¥x âˆ’yâˆ¥2
converges in M. Hereafter, let y satisfy yn â†’y âˆˆM.
(b) Show that 2aâŸ¨x âˆ’y, z âˆ’yâŸ©â‰¤a2âˆ¥z âˆ’yâˆ¥2 for 0 < a < 1 and z âˆˆM.
(c) Show that the inequality âŸ¨x âˆ’y, z âˆ’yâŸ©> 0 contains a contradiction.
(d) Show that âŸ¨x âˆ’y, zâŸ©â‰¤0.
(e) Obtain the proposition by replacing z with âˆ’z.
25. Show that the linear operator norm (2.12) satisï¬es the triangle inequality.

Exercises 16âˆ¼30
59
26. Show that the integral operator (2.13) is a bounded linear operator and that it is
self-adjoint when K is symmetric.
27. Let (M, d) be a metric space with M := R and a Euclidean distance d. Show that
each of the following E âŠ†M is not sequentially compact. Furthermore, show
that they are not compact without using the equivalence between compactness
and sequential compactness.
(a) E = [0, 1)and
(b) E = Q.
28. Proposition 27 is derived according to the following steps in the appendix. What
are the derivations of (a) through (c)?
(a) Show that H1 = Ker(T ) âŠ•Im(T ).
(b) Show that span{e j| j â‰¥1} âŠ†Im(T ).
(c) Show that span{e j| j â‰¥1} âŠ‡Im(T ).
Why do we need to show (2.25)?
29. Show that the HS and trace norms satisfy the triangle inequality.
30. Show that if T âˆˆB(H) is a trace class, then it is also an HS class, and show that
if T âˆˆB(H) is a trace class, it is also compact.

Chapter 3
Reproducing Kernel Hilbert Space
Thus far, we have learned that a feature map  : E âˆ‹x â†’k(x, Â·) is obtained by the
positive deï¬nite kernel k : E Ã— E â†’R. In this chapter, we generate a linear space
H0 based on its image k(x, Â·)(x âˆˆE) and construct a Hilbert space H by completing
this linear space, where H is called reithe reproducing kernel Hilbert space (RKHS),
which satisï¬es the reproducing prsoperty of the kernel k (k is the reproducing kernel
of H). In this chapter, we ï¬rst understand that there is a one-to-one correspondence
between the kernel k and the RKHS H and that H0 is dense in H (via the Moore-
Aronszajn theorem). Furthermore, we introduce the RKHS represented by the sum
of RKHSs and apply it to Sobolev spaces. We prove Mercerâ€™s theorem regarding
integral operators in the second half of this chapter and compute their eigenvalues
and eigenfunctions. This chapter is the core of the theory contained in this book, and
the later chapters correspond to its applications.
3.1
RKHSs
Let H be a Hilbert space whose elements are functions f : E â†’R.
A function k : E Ã— E â†’R is said to be a reproducing kernel of a Hilbert space
H with an inner product âŸ¨Â·, Â·âŸ©H if it satisï¬es the following two conditions.
1. For each x âˆˆE, we have
k(x, Â·) âˆˆH.
(3.1)
2. Reproducing property: for each f âˆˆH and x âˆˆE,
f (x) = âŸ¨f, k(x, Â·)âŸ©H.
(3.2)
When H has a reproducing kernel, we say that H is a reproducing kernel Hilbert
space (RKHS). The reproducing property (3.2) is called a kernel trick.
Â© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022
J. Suzuki, Kernel Methods for Machine Learning with Math and Python,
https://doi.org/10.1007/978-981-19-0401-1_3
61

62
3
Reproducing Kernel Hilbert Space
Example 51 Let {e1, . . . , ep} be an orthonormal basis of a ï¬nite-dimensional
Hilbert space H. If we deï¬ne
k(x, y) :=
p

i=1
ei(x)ei(y)
(3.3)
for x, y âˆˆE, then we have k(x, Â·) âˆˆH and
âŸ¨e j(Â·), k(x, Â·)âŸ©H =
p

i=1
âŸ¨e j, eiâŸ©Hei(x) = e j(x)
for each 1 â‰¤j â‰¤p. Thus, for any f (Â·) = p
i=1 fiei(Â·) âˆˆH, fi âˆˆR, we have
âŸ¨f (Â·), k(x, Â·)âŸ©H = f (x) (reproducing property). Therefore, H is an RKHS, and (3.3)
is a reproducing kernel.
Proposition 32 The reproducing kernel k of the RKHS H is unique, symmetric
k(x, y) = k(y, x), and nonnegative deï¬nite.
Proof: If k1, k2 are RKHSs of H, then by the reproducing property, we have that
f (x) = âŸ¨f, k1(x, Â·)âŸ©H = âŸ¨f, k2(x, Â·)âŸ©H .
In other words,
âŸ¨f, k1(x, Â·) âˆ’k2(x, Â·)âŸ©H = 0
holds for all f âˆˆH, x âˆˆE for which k1 = k2 (Proposition 16). Additionally, the
symmetry of a reproducing kernel follows from that of its inner product:
k(x, y) = âŸ¨k(x, Â·), k(y, Â·)âŸ©H = âŸ¨k(y, Â·), k(x, Â·)âŸ©H = k(y, x) .
The nonnegative deï¬niteness of the reproducing kernel can be shown as follows.
n

i=1
n

j=1
zi z jk(xi, x j) =
n

i=1
n

j=1
zi z jâŸ¨k(xi, Â·), k(x j, Â·)âŸ©H = âŸ¨
n

i=1
zik(xi, Â·),
n

j=1
z jk(x j, Â·)âŸ©H â‰¥0
â–¡.
Proposition 33 A Hilbert space H is an RKHS if and only if Tx( f ) = f (x) ( f âˆˆH)
is bounded at each x âˆˆE for the linear functional Tx : H âˆ‹f â†’f (x) âˆˆR.
Proof: If H has a reproducing kernel k, then at each x âˆˆE, we have
âŸ¨f (Â·), k(x, Â·)âŸ©H = Tx( f ) , f âˆˆH .
Thus, we have

3.1 RKHSs
63
|Tx( f )| = |âŸ¨f (Â·), k(x, Â·)âŸ©H| â‰¤âˆ¥f âˆ¥Â· âˆ¥k(x, Â·)âˆ¥= âˆ¥f âˆ¥

k(x, x) .
Conversely, if the linear functional Tx( f ) = f (x) is bounded for x âˆˆE, from Propo-
sition 22, there exists a kx : E â†’R such that
âŸ¨f (Â·), kx(Â·)âŸ©H = f (x) , f âˆˆH .
In other words, a reproducing kernel exists.
â–¡
In Proposition 32, we showed that a reproducing kernel is unique once its RKHS
is determined, but the following proposition asserts the converse.
Proposition 34 (Aronszajn [1]) Let k : E Ã— E â†’R be a positive deï¬nite kernel.
Then, the Hilbert space H with the reproducing kernel k is unique. Moreover, for
k(x, Â·) âˆˆH, x âˆˆE holds, and the generated linear space is dense in H.
The proof is given by the following procedure.
1. Deï¬ne the inner product âŸ¨Â·, Â·âŸ©H0 of H0 := span{k(x, Â·)|x âˆˆE}.
2. For any Cauchy sequence { fn} in H0 and each x âˆˆE, the real sequence { fn(x)}
is a Cauchy sequence, and we have the convergence value f (x) := lim
nâ†’âˆfn(x)
(Proposition 6). Let H be such a set of f .
3. Deï¬ne the inner product âŸ¨Â·, Â·âŸ©H of the linear space H.
4. Show that H0 is dense in H.
5. Show that any Cauchy sequence { fn} in H converges to some element of H as
n â†’âˆ(completeness of H).
6. Show that k is a reproducing kernel of H.
7. Show that such an H is unique.
See the appendix at the end of the chapter for details1.
â–¡
Example 52 (Linear Kernel)Let âŸ¨Â·, Â·âŸ©E be the inner product of E := Rd. Then, the
linear space
H := {âŸ¨x, Â·âŸ©E|x âˆˆE}
is complete since it has ï¬nite dimensions (Proposition 6). Moreover, H is an RKHS
with the reproducing kernel k(x, y) = âŸ¨x, yâŸ©E, x, y âˆˆE.
Example 53 Let E be a ï¬nite set {x1, . . . , xn}, and let k : E Ã— E â†’R be a positive
deï¬nite kernel; then, the linear space
H := {
n

i=1
Î±ik(xi, Â·)|Î±1, . . . , Î±n âˆˆR}
is a reproducing kernel Hilbert space. We deï¬ne the inner product by
1 The proof is due to [33].

64
3
Reproducing Kernel Hilbert Space
âŸ¨f (Â·), g(Â·)âŸ©H = aâŠ¤Kb
for f (Â·), g(Â·) âˆˆH,where f (Â·) = n
j=1 a jk(x j, Â·) âˆˆH,a = [a1, . . . , an]âŠ¤âˆˆRn and
g(Â·) = n
j=1 b jk(x j, Â·) âˆˆH, b = [b1, . . . , bn]âŠ¤âˆˆRn via the Gram matrix
K :=
â¡
â¢â£.
k(x1, x1) Â· Â· Â· k(x1, xn)
...
...
...
k(xn, x1) Â· Â· Â· k(xn, xn)
â¤
â¥â¦.
Then, for each xi, i = 1, 2, . . ., we have
âŸ¨f (Â·), k(xi, Â·)âŸ©H = [a1, . . . , an]Kei =
n

j=1
a jk(x j, xi) = f (xi)
(reproducing property), where ei is an n-dimensional column vector in which we set
component i and the other components to 1 and 0, respectively.
Example 54 (Polynomial Kernel)Let âŸ¨Â·, Â·âŸ©E be the inner product between the ele-
ments in E. The Hilbert space H obtained by completing the linear space H0
generated by (âŸ¨x, Â·âŸ©E + 1)d âˆˆR (x âˆˆE) is an RKHS with the reproducing kernel
k(x, y) = (âŸ¨x, yâŸ©E + 1)d for x, y âˆˆE.
Example 55 Let k(x, y) be the kernel expressed by a function Ï†(x âˆ’y) as con-
sidered in Sect.1.5. If we require k(x, y) to take real values, the associated proba-
bility density functions must be even functions such as those of the Gaussian and
Laplace distributions. Otherwise, since the imaginary part of t â†’ei(xâˆ’y)t is odd,
the kernel k might take imaginary values. Now, using L2(E, Î·) âˆ‹F : E = R â†’C
whose real and imaginary parts are even and odd, respectively, we consider the linear
spaceconsistingof f : E â†’Rwith f (x) =

E F(t)eixtdÎ·(t).Thefunction F(t) â†’
f (x) =

E F(t)eixtdÎ·(t) is injective (if

E F(t)eixtdÎ·(t) = 0, then the inverse
Fourier transform F(t) = 0). If its inner product is âŸ¨f, gâŸ©H =

E F(t)G(t)dÎ·(t)
for F, G âˆˆL2(E, Î·), then L2(E, Î·) and
H = {E âˆ‹x â†’

E
F(t)eixtdÎ·(t) âˆˆR|F âˆˆL2(E, Î·)}
are isomorphic as an inner product space. Note that H has a reproducing kernel
E Ã— E â†’R with
k(x, y) =

E
eâˆ’i(xâˆ’y)tdÎ·(t) .
In fact, we have k(x, y) =

E eâˆ’ixteiytdÎ·(t). Thus, if we set G(t) = eâˆ’ixt, we obtain
âŸ¨f (Â·), k(x, Â·)âŸ©H =

E
F(t)G(t)dÎ·(t) =

E
F(t)eixtdÎ·(t) = f (x)

3.1 RKHSs
65
for f (y) =

E
F(t)eiytdÎ·(t) and k(x, y) =

E
G(t)eiytdÎ·(t). For different kernels
k(x, y), such as the Gaussian and Laplacian kernels, the measure Î·(t) will be differ-
ent, and the corresponding RKHS H will be different.
Example 56 Let E := [0, 1]. Using the real-valued function F with
 1
0 F(u)2du <
âˆ, we consider the set H of functions f : E â†’R, f (t) =
 1
0 F(u)(t âˆ’u)0
+du,
where we denote (z)0
+ = 1 and (z)0
+ = 0 when z â‰¥0 and when z < 0, respectively.
The linear space H is complete for the norm âˆ¥f âˆ¥2 =
 1
0 F(u)2du (Proposition 14)
if the inner product is âŸ¨f, gâŸ©H =
 1
0 F(u)G(u)du for f (t) =
 1
0 F(u)(t âˆ’u)0
+du
and g(t) =
 1
0 G(u)(t âˆ’u)0
+du. This Hilbert space H is the RKHS for k(x, y) =
min{x, y}. In fact, for each z âˆˆE, we see that
âŸ¨f (z), k(x, z)âŸ©H =âŸ¨
 1
0
F(u)(z âˆ’u)0
+du,
 1
0
(x âˆ’u)0
+(z âˆ’u)0
+duâŸ©H =
 1
0
F(u)(x âˆ’u)0
+du = f (x).
Thus far, we have obtained the RKHS corresponding to each positive deï¬nite
kernel, but a necessary condition exists for a Hilbert space H to be an RKHS. If that
condition is not satisï¬ed, we can claim that it is not an RKHS.
Proposition 35 Let H be an RKHS consisting of functions on E. If limnâ†’âˆ| fn âˆ’
f âˆ¥H = 0 f, f1, f2, . . . âˆˆH,thenforeach x âˆˆE,limnâ†’âˆ| fn(x) âˆ’f (x)| = 0holds.
Proof: In fact, we have that for each x âˆˆE,
| fn(x) âˆ’f (x)| â‰¤âˆ¥fn âˆ’f âˆ¥

k(x, x) .
â–¡
Example 57 H := L2[0, 1] is not an RKHS. In fact, for a sequence { fn} with
fn(x) = xn, the norm converges to âˆ¥fnâˆ¥2
H =
 1
0 f 2
n (x)dx =
1
2n+1 â†’0. However,
for f (x) = 0 with x âˆˆE, we have âˆ¥fn âˆ’f âˆ¥H â†’0, and | fn(1) âˆ’f (1)| = 1 â†›0.
This contradicts the fact that H is an RKHS (Proposition 35).
Example 57 illustrates that L2[0, 1] is too large, and as we will see in the next section,
the Sobolev space restricted to L2[0, 1] is an RKHS.
3.2
Sobolev Space
We ï¬rst show that if k1, k2 are reproducing kernels, the sum k1 + k2 is also a repro-
ducing kernel. To this end, we show the following.
Proposition 36 If H1, H2 are Hilbert spaces, so is the direct product F := H1 Ã— H2
under the inner product

66
3
Reproducing Kernel Hilbert Space
âŸ¨( f1, f2), (g1, g2)âŸ©F := âŸ¨f1, g1âŸ©H1 + âŸ¨f2, g2âŸ©H2
(3.4)
for f1, g1 âˆˆH1, f2, g2 âˆˆH2.
Proof: From âˆ¥( f1, f2)âˆ¥2
F = âˆ¥f1âˆ¥2
H1 + âˆ¥f2âˆ¥H2, we have
âˆ¥f1,n âˆ’f1,mâˆ¥H1, âˆ¥f2,n âˆ’f2,mâˆ¥H2
â‰¤

âˆ¥f1,n âˆ’f1,mâˆ¥2
H1 + âˆ¥f2,n âˆ’f2,nâˆ¥2
H2 = âˆ¥( f1,n, f2,n) âˆ’( f1,m, f2,m)âˆ¥F .
Thus, we have
{( f1,n, f2,n)} is Cauchy
=â‡’
{ f1,n}, { f2,n} is Cauchy
=â‡’
f1 âˆˆH1, f2 âˆˆH2 exists such that f1,n â†’f1, f2,n â†’f2
=â‡’
âˆ¥( f1,n, f2,n) âˆ’( f1, f2)âˆ¥F = âˆ¥( f1,n âˆ’f1, f2,n âˆ’f2)âˆ¥F
=

âˆ¥f1,n âˆ’f1âˆ¥2 + âˆ¥f2,n âˆ’f2âˆ¥2 â†’0 ,
which means that F is complete.
â–¡
Let
H := H1 + H2 := { f1 + f2| f1 âˆˆH1, f2 âˆˆH2}
be the direct sum of H1, H2, and deï¬ne the linear map from F to H by u :
F âˆ‹( f1, f2) â†’f1 + f2 âˆˆH. Then, we can decompose F into N := uâˆ’1(0) and
its orthogonal complement N âŠ¥. If we restrict u to N âŠ¥to obtain the injection
v : N âŠ¥â†’H, then the bivariate function
âŸ¨f, gâŸ©H := âŸ¨vâˆ’1( f ), vâˆ’1(g)âŸ©F
(3.5)
for f, g âˆˆH forms an inner product. Note that N âŠ¥is a closed subspace of the Hilbert
space F.
Proposition 37 If the direct sum H of Hilbert spaces H1, H2 has the inner product
(3.5), then H is complete (a Hilbert space).
Proof: Since F is a Hilbert space (Proposition 36) and N âŠ¥is its closed subset, N âŠ¥
is complete. Thus, we have
âˆ¥fn âˆ’fmâˆ¥H â†’0=â‡’âˆ¥vâˆ’1( fn âˆ’fm)âˆ¥F â†’0
=â‡’
g âˆˆF exists such that âˆ¥vâˆ’1( fn) âˆ’gâˆ¥F â†’0
=â‡’
âˆ¥fn âˆ’v(g)âˆ¥H â†’0, v(g) âˆˆH .
â–¡
Proposition 38 (Aronszajn [1]) Let k1, k2 be the reproducing kernels of RKHSs
H1, H2, respectively. Then, k = k1 + k2 is the reproducing kernel of the Hilbert
space
H := H1 âŠ•H2 := { f1 + f2| f1 âˆˆH1, f2 âˆˆH2}
such that the inner product is (3.5) and the norm is

3.2 Sobolev Space
67
âˆ¥f âˆ¥2
H =
min
f = f1+ f2, f1âˆˆH1, f2âˆˆH2{âˆ¥f1âˆ¥2
H1 + âˆ¥f2âˆ¥2
H2}
(3.6)
for f âˆˆH.
The proof proceeds as follows.
1. Let f âˆˆH and N âŠ¥âˆ‹( f1, f2) := vâˆ’1( f ). We deï¬ne k(x, Â·):=k1(x, Â·)+k2(x, Â·)
and (h1(x, Â·), h2(x, Â·)) := vâˆ’1(k(x, Â·)), and we show that
âŸ¨f1, h1(x, Â·)âŸ©1 + âŸ¨f2, h2(x, Â·)âŸ©2 = âŸ¨f1, k1(x, Â·)âŸ©1 + âŸ¨f2, k2(x, Â·)âŸ©2 .
2. Using the above, we present the reproducing property âŸ¨f, k(x, Â·)âŸ©H = f (x) of
k.
3. We show that the norm of H is (3.6).
For details, see the Appendix at the end of this chapter.
â–¡
In the following, we construct the Sobolev space as an example of an RKHS and
obtain its kernel.
Let W1[0, 1] be the set of f â€™s deï¬ned over [0, 1] such that f is differentiable
almost everywhere and f â€² âˆˆL2[0, 1]. Then, we can write each f âˆˆW1[0, 1] as
f (x) = f (0) +
 x
0
f â€²(y)dy .
(3.7)
Similarly, let Wq[0, 1] be the set of f â€™s deï¬ned over [0, 1] such that f is differentiable
q âˆ’1 times and q times almost everywhere and f (q) âˆˆL2[0, 1]. If we deï¬ne
Ï†i(x) := xi
i! , i = 0, 1, . . .
and
Gq(x, y) := (x âˆ’y)qâˆ’1
+
(q âˆ’1)!
,
then we can Taylor-expand each f âˆˆWq[0, 1] as follows.
f (x) =
qâˆ’1

i=0
f (i)(0)Ï†i(x) +
 1
0
Gq(x, y) f (q)(y)dy.
(3.8)
In fact, we have the partial integral
 1
0
Gq(x, y) f (q)(y)dy =

Gq(x, y) f (qâˆ’1)(y)
1
0 âˆ’
 1
0
{ d
dy Gq(x, y)} f (qâˆ’1)(y)dy
= âˆ’
xqâˆ’1
(q âˆ’1)! f (qâˆ’1)(0) +
 1
0
Gqâˆ’1(x, y) f (qâˆ’1)(y)dy

68
3
Reproducing Kernel Hilbert Space
and obtain (3.7) by repeatedly applying this integral to the right-hand side of (3.8).
For the transformation, we use
 1
0
Gq(x, y)h(y)dy =
 1
0
(x âˆ’y)qâˆ’1
+
(q âˆ’1)! h(y)dy
=
1
(q âˆ’1)!
qâˆ’1

i=0
q âˆ’1
i

xi
 x
0
(âˆ’y)qâˆ’1âˆ’ih(y)dy .
and the differentiation
 1
0
d
dy {Gq(x, y)h(y)}dy =
1
(q âˆ’2)!
qâˆ’2

i=0
q âˆ’2
i

{âˆ’xi
 x
0
(âˆ’y)qâˆ’2âˆ’ih(y)dy}
= âˆ’
 1
0
Gqâˆ’1(x, y)h(y)dy .
Hereafter, we write each element of Wq[0, 1] as
qâˆ’1

i=0
Î±iÏ†i(x) +
 1
0
Gq(x, y)h(y)dy
(3.9)
Î±0 = f (0), . . . , Î±qâˆ’1 = f (qâˆ’1)(0) âˆˆR, h âˆˆL2[0, 1].
Although more than one Hilbert space Wq[0, 1] exists with different deï¬nitions
of inner products, we consider the Hilbert space H that can be written as the direct
sum of H0 and H1, which is deï¬ned below. Let
H0 := span{Ï†0, . . . , Ï†qâˆ’1},
and deï¬ne its inner product by
âŸ¨f, gâŸ©H0 =
qâˆ’1

i=0
f (i)(0)g(i)(0)
for f, g âˆˆH0. We ï¬nd that the inner product âŸ¨Â·, Â·âŸ©H0 satisï¬es the requirement of inner
products and that {Ï†0, . . . , Ï†qâˆ’1} is an orthonormal basis. Since the inner product
space H0 is of ï¬nite dimensionality, it is apparently a Hilbert space. We deï¬ne another
inner product space H1 as
H1 := {
 1
0
Gq(x, y)h(y)dy|h âˆˆL2[0, 1]} .
Since h âˆˆL2[0, 1], if we deï¬ne the inner product as

3.2 Sobolev Space
69
âŸ¨f, gâŸ©H1 =
 1
0
f (q)(y)g(q)(y)dy
for f, g âˆˆH, then we have
âˆ¥fm âˆ’fnâˆ¥H1 â†’0 â‡â‡’âˆ¥f (q)
m
âˆ’f (q)
n âˆ¥L2[0,1] â†’0,
and there exists an f âˆˆH1 such that
âˆ¥fn âˆ’f âˆ¥H1 â†’0 â‡â‡’âˆ¥f (q)
n
âˆ’f (q)âˆ¥L2[0,1] â†’0 .
From Proposition 14, we have âˆ¥fm âˆ’fnâˆ¥H1 â†’0=â‡’âˆ¥fn âˆ’f âˆ¥H1 â†’0 (complete-
ness), and H1 is a Hilbert space. Moreover, from
f (x) =
qâˆ’1

i=0
Î±iÏ†i(x) âˆˆH1=â‡’h = f (q) = 0
and
f (x) =
 1
0
Gq(x, y)h(y)dy âˆˆH0=â‡’Î±0 = f (0) = 0, . . . , Î±qâˆ’1 = f (qâˆ’1)(0) = 0 ,
we have that H0 âˆ©H1 = {0}. From Proposition 38, for f = f0 + f1, g = g0 + g1,
f0, g0 âˆˆH0, and f1, g1 âˆˆH1, the inner product is
âŸ¨f, gâŸ©Wq[0,1] = âŸ¨f0 + f1, g0 + g1âŸ©Wq[0,1] = âŸ¨f0, g0âŸ©H0 + âŸ¨f1, g1âŸ©H1.
The reproducing kernels of H0, H1 are respectively
k0(x, y) :=
qâˆ’1

i=0
Ï†i(x)Ï†i(y)
and
k1(x, y) :=
 1
0
Gq(x, z)Gq(y, z)dz ,
where k0 is derived from Example 3.2, and k1 is derived from
âŸ¨f (Â·), k1(x, Â·)âŸ©H1 = âŸ¨
 1
0
Gq(Â·, z)h(z)dz,
 1
0
Gq(x, z)Gq(Â·, z)dzâŸ©H1
=
 1
0
Gq(x, z)h(z)dz = f (x)

70
3
Reproducing Kernel Hilbert Space
for arbitrary f (Â·) =
 1
0
Gq(Â·, z)h(z)dz âˆˆH and x âˆˆE (the uniqueness is due to
Proposition 32).
Furthermore, we can construct Wq[0, 1] such that its kernel is
k(x, y) = k0(x, y) + k1(x, y)
for x, y âˆˆE.
3.3
Mercerâ€™s Theorem
Let (E, F, Î¼) be a measure space. We assume that the integral operator kernel
K : E Ã— E â†’R is a measurable function and is not necessarily nonnegative deï¬nite.
Suppose that
 
EÃ—E K 2(x, y)dÎ¼(x)dÎ¼(y) takes ï¬nite values. Then, we deï¬ne
the integral operator TK by
(TK f )(Â·) :=

E
K(x, Â·) f (x)dÎ¼(x)
(3.10)
for f âˆˆL2(E, B, Î¼). Since
âˆ¥TK f âˆ¥2 =

E
{(TK f )(x)}2dÎ¼(x) â‰¤
 
EÃ—E
{K(x, y)}2dÎ¼(x)dÎ¼(y)

E
{ f (z)}2dÎ¼(z)
= âˆ¥f âˆ¥2
 
EÃ—E
{K(x, y)}2dÎ¼(x)dÎ¼(y) ,
we have TK âˆˆB(L2(E, B, Î¼)) and
âˆ¥TKâˆ¥â‰¤
 
EÃ—E
K 2(x, y)dÎ¼(x)dÎ¼(y)
1/2
.
In the following, we assume that K : E Ã— E â†’R is continuous and that the entire
set E is compact (such as E = [0, 1]). Thus, we assume that the integral operator
kernel K is uniformly continuous (Proposition 8).
Lemma 1 For each f âˆˆL2(E, F, Î¼), TK f (Â·) is uniformly continuous.
Proof: Since E Ã— Eâ†’R is uniformly continuous, we achieve |K(x, y)âˆ’K(x, z)| <
Ïµ by making |y âˆ’z| smaller for arbitrary x âˆˆE and Ïµ > 0. Thus, we have


E
K(x, y) f (x)dÎ¼(x) âˆ’

E
K(x, z) f (x)dÎ¼(x)
 â‰¤Ïµâˆ¥f âˆ¥.
â–¡

3.3 Mercerâ€™s Theorem
71
Proposition 39 TK is a compact operator.
Proof: By Proposition 12, for an arbitrary Ïµ > 0, there exist n(Ïµ) â‰¥1 and an R-
coefï¬cient bivariate polynomial Kn(Ïµ)(x, y) := n(Ïµ)
i=1 gi(x)yi whose order of y is at
most n(Ïµ) such that
sup
x,yâˆˆE
|K(x, y) âˆ’Kn(Ïµ)(x, y)| < Ïµ ,
where g1, . . . , gn(Ïµ) are R-coefï¬cient univariable polynomials. If we abbreviate n(Ïµ)
as n and write the integral operator corresponding to Kn as TKn, then we may regard
TKn f (Â·) =
n

i=0
yi

E
f (x)gi(x)dÎ¼(x)
as
TKn f : H âˆ‹f â†’[

E
f (x)g0(x)dÎ¼(x), . . . ,

E
f (x)gn(x)dÎ¼(x)] âˆˆRn+1 .
Since the rank of TKn is ï¬nite, from the ï¬rst item of Proposition 25, TKn is a compact
operator. Moreover, since
âˆ¥(TKn âˆ’TK ) f âˆ¥2 =

E
(

E
[Kn(x, y) âˆ’K(x, y)] f (y)dÎ¼(y))2dÎ¼(x) â‰¤Ïµ2âˆ¥f âˆ¥2Î¼2(E) ,
from Proposition 25, TK is a compact operator.
â–¡
In the following, we assume that K is symmetric. Then, from Example 45, TK is
self-adjoint. Thus, from Proposition 39, we have that
TK x =
âˆ

j=1
Î» jâŸ¨e j, xâŸ©e j
using {Î» j} and {e j} that satisfy Proposition 27. Moreover, Lemma 1 implies the
following:
Lemma 2
e j(y) = Î»âˆ’1
j

E
K(x, y)e j(x)dÎ¼(x)
is uniformly continuous w.r.t. y.
Example 58 (Brown Motion) We obtain the eigenvalues and eigenfunctions
{(Î» j, e j)} when the integral operator kernel in L2[0, 1] is K(x, y) = min{x, y},
x, y âˆˆE = [0, 1], (the subspace H1 of the Sobolev space W1[0, 1]). Since
TK f (x) =
 1
0
K(x, y) f (y)dy =
 x
0
y f (y)dy + x
 1
x
f (y)dy ,

72
3
Reproducing Kernel Hilbert Space
the eigen equation is
 1
0
min(x, y)e(y)dy = Î»e(x) ,
(3.11)
i.e.,
 x
0
ye(y)dy + x
 1
x
e(y)dy = Î»e(x) .
If we differentiate the both sides by x, we obtain
xe(x) +
 1
x
e(y)dy âˆ’xe(x) = Î»eâ€²(x) ,
i.e.,
 1
x
e(y)dy = Î»eâ€²(x) .
(3.12)
If we further differentiate both sides by x, then we obtain e(x) = âˆ’Î»eâ€²â€²(x) and
e(y) = Î± sin(y/
âˆš
Î») + Î² cos(y/
âˆš
Î») .
If we substitute x = 0 into (3.11), then we have e(0) = 0, which is equivalent to
Î² = 0. From (3.12), we have eâ€²(1) = 0, i.e., Î± cos(1/
âˆš
Î») = 0. Thus, we obtain
1/
âˆš
Î» = (2 j âˆ’1)Ï€/2 , j = 1, 2, . . . .
Therefore, the eigenvalues are
Î» j =
4
{(2 j âˆ’1)Ï€}2 ,
(3.13)
and the orthonormal eigenfunctions are
e j(x) =
âˆš
2 sin
(2 j âˆ’1)Ï€
2
x

,
(3.14)
where to derive Î± =
âˆš
2, we use
 1
0
sin2( y
âˆš
Î»
)dy =
 1
0
1 âˆ’cos( 2y
âˆš
Î»)
2
dy = 1
2 âˆ’1
2[
âˆš
Î»
2 sin 2y
âˆš
Î»
]1
0 = 1
2 .
Example 59 (Zhu et al.[36])For a Gaussian kernel,
K(x, y) = exp
âˆ’(x âˆ’y)2
2Ïƒ 2


3.3 Mercerâ€™s Theorem
73
if we regard the ï¬nite measure Î¼ in (3.10) of the integral operator kernel as a Gaus-
sian distribution with a mean of 0 and a variance of Ë†Ïƒ 2; then, the eigenvalue and
eigenfunction are
Î» j =

2a
A B j
and
e j(x) = exp(âˆ’(c âˆ’a)x2)Hj(
âˆš
2cx) ,
where Hj is a Hermite polynomial of order j:
Hj(x) := (âˆ’1) j exp(x2) d j
dx j exp(âˆ’x2) ,
aâˆ’1 := 4Ë†Ïƒ 2, bâˆ’1 := 2Ïƒ 2, c :=
âˆš
a2 + 2ab, A := a + b + c, and B := b/A. The
proof is not difï¬cult but rather monotonous and long. See the Appendix at the end
of this chapter for details. Note that for a Gaussian kernel with a parameter Ïƒ 2, if the
measure is also a Gaussian distribution with a mean of 0 and a variance of Ë†Ïƒ 2, we
can compute the eigenvalues from Î² := Ë†Ïƒ 2
Ïƒ 2 = b
2a :

2a
A B j =

2a
a + b +
âˆš
a2 + 2ab
(
b
a + b +
âˆš
a2 + 2ab
) j
= [1/2 + Î² +

1/4 + Î²]âˆ’1/2(
Î²
1/2 + Î² + âˆš1/4 + Î² ) j ,
which forms a geometric sequence. For example, if Ïƒ 2 = Ë†Ïƒ 2 = 1, then the eigenvalue
is
Î» j = (3 âˆ’
âˆš
5
2
) j+1/2 .
The Hermite polynomials are H1(x) = 2x, H2(x) = âˆ’2 + 4x2, and H3(x) = 12x âˆ’
8x3 (H0(1) = 1, Hj(x) = 2x Hjâˆ’1(x) âˆ’H â€²
jâˆ’1(x)), and the other quantities are
c =

a2 + 2ab = (4Ë†Ïƒ 2)âˆ’1
1 + 4Ë†Ïƒ 2/Ïƒ 2 =
âˆš
5
4 , a = (4Ë†Ïƒ 2)âˆ’1 = 1
4 .
We show the eigenfunction Ï† j for j = 1, 2, 3 in Fig.3.1. The code is as follows.
3-start
# In
t h i s
chapter ,
we assume
t h a t
the
f o l l o w i n g
has
been
executed .
import numpy as np
import
m a t p l o t l i b . pyplot
as
p l t
from
m a t p l o t l i b
import
s t y l e
s t y l e . use ("seabornâˆ’ticks")

74
3
Reproducing Kernel Hilbert Space
def
Hermite ( j ) :
i f
j == 0:
return
[ 1 ]
a = [ 0 ] âˆ—( j + 2)
b = [ 0 ] âˆ—( j + 2)
a [ 0 ] = 1
for
i
in
range (1 ,
j + 1) :
b [ 0 ] = âˆ’a [ 1 ]
for k in
range ( i + 1) :
b [ k ] = 2 âˆ—a [ k âˆ’1] âˆ’( k + 1) âˆ—a [ k + 1]
for h in
range ( j + 2) :
a [ h ] = b [ h ]
return b [ : ( j +1) ]
Hermite ( 1 )
# 1 s t
order
Hermite
Polynomial
[0, 2]
Hermite ( 2 )
# 2nd order
Hermite
Polynomial
[-2, 0, 4]
Hermite ( 3 )
# 3 rd
order
Hermite
Polynomial
[0, -12, 0, 8]
def H( j ,
x ) :
coef = Hermite ( j )
S = 0
for
i
in
range ( j + 1) :
S = S + np . a r r a y ( coef [ i ] )
âˆ—( x âˆ—âˆ—
i )
return S
cc = np . s q r t ( 5 )
/
4
a = 1/4
def
phi ( j ,
x ) :
return np . exp ( âˆ’( cc âˆ’a ) âˆ—x âˆ—âˆ—2) âˆ—H( j ,
np . s q r t (2 âˆ—cc ) âˆ—x )
c o l o r = ["b" , "g" , "r" , "k"]
p =
[ [ ]
for _ in
range ( 4 ) ]
x = np . l i n s p a c e ( âˆ’2 , 2 ,
100)
for
i
in
range ( 4 ) :
for k in x :
p [ i ] . append ( phi ( i ,
k ) )
p l t . p l o t ( x ,
p [ i ] ,
c = c o l o r [ i ] ,
l a b e l = "jâ£=â£%d"%i )

3.3 Mercerâ€™s Theorem
75
p l t . ylim ( âˆ’2 ,
8)
p l t . y l a b e l ("phi")
p l t . t i t l e ("Characteristicâ£functionâ£ofâ£Gaussâ£Kernel")
In this section, we prove Mercerâ€™s theorem for integral operators and illustrate
some examples. Hereafter, we assume that K and TK are nonnegative deï¬nite.
Proposition 40 An integral operator TK is nonnegative deï¬nite if and only if K :
E Ã— E â†’R is nonnegative deï¬nite, i.e., K is a positive deï¬nite kernel.
Proof: See the Appendix at the end of this chapter.
Proposition 41 (Mercer [21]) Let K : E Ã— E â†’R be a continuous positive deï¬-
nite kernel and TK be the corresponding integral operator. Let {(Î» j, e j)}âˆ
j=1 be the
sequence of eigenvalues and eigenvectors of TK. Then, we can write
K(x, y) =
âˆ

j=1
Î» je j(x)e j(y),
and this sum absolutely and uniformly converges.
By absolute convergence, we mean that the sum of the absolute values converges,
and by uniform convergence, we mean that the upper bound of the error that does
not depend on x, y âˆˆE converges to zero.
Proof: Note that Kn(x, y) := K(x, y) âˆ’n
j=1 Î» je j(x)e j(y) is continuous and that
the integral operator TKn is nonnegative deï¬nite. In fact, for each f âˆˆL2(E,
F, Î¼), we have
âŸ¨TKn f, f âŸ©= âŸ¨TK f, f âŸ©âˆ’
n

j=1
Î» jâŸ¨f, e jâŸ©2 =
âˆ

j=n+1
Î» jâŸ¨f, e jâŸ©2 â‰¥0 .
Fig. 3.1 The eigenfunctions
for the Gaussian kernel and
Gaussian distribution, where
Ïƒ 2 = Ë†Ïƒ 2 = 1. If j is odd, the
eigenfunctions are even and
odd functions, respectively
-2
-1
0
1
2
-6 -4 -2
0
2
4
6
8
x
Ï†j
j = 0
j = 1
j = 2
j = 3
Gaussian Kernel EigenFunctions

76
3
Reproducing Kernel Hilbert Space
Thus, from Proposition 40, Kn is nonnegative deï¬nite, and Kn(x, x) â‰¥0. Thus, for
all x âˆˆE, we have
âˆ

j=1
Î» je2
j(x) â‰¤K(x, x) .
(3.15)
Moreover, for any set J consisting of positive numbers, we have

jâˆˆJ
|Î» je j(x)e j(y)| â‰¤
â›
â
jâˆˆJ
Î» je2
j(x)
â
â 
1/2 â›
â
jâˆˆJ
Î» je2
j(y)
â
â 
1/2
,
(3.16)
which means that from (3.15),

jâˆˆJ
|Î» je j(x)e j(y)| â‰¤{K(x, x)K(y, y)}1/2
for x, y âˆˆE. From (3.16), we have
âˆ

j=n+1
|Î» je j(x)e j(y)| â‰¤
â›
â
âˆ

j=n+1
Î» je2
j(x)
â
â 
1/2 â›
â
âˆ

j=n+1
Î» je2
j(y)
â
â 
1/2
and the right-hand side monotonically converges to 0 as n grows. Since E is compact,
the left-hand side uniformly converges according to the lemma below.
Lemma 3 (Dini) Let E be a compact set. For a continuous function fn : E â†’R,
if fn(x) monotonically converges to f (x) for a continuous f and each x âˆˆE, then
the convergence is uniform.
Proof: See the Appendix at the end of this chapter.
Thus, for an arbitrary Ïµ > 0, there exists an n such that
sup
x,yâˆˆE
âˆ

j=n+1
|Î» je j(x)e j(y)| < Ïµ,
(3.17)
and this sum absolutely and uniformly converges.
â–¡
Example 60 (The Kernel Expressed by the Difference Between Two Variables)Let
E = [âˆ’1, 1]. An integral operator for which K : E Ã— E â†’R can be expressed by
K(x, z) = Ï†(x âˆ’z) (Ï† : E â†’R) is TK f (x) =

E Ï†(x âˆ’y) f (y)dy, which can be
expressed by (Ï† âˆ—f )(x) using convolution: (g âˆ—h)(u) =

E g(u âˆ’v)h(v). Here-
after, we assume that the cycle of Ï† is two, i.e., Ï†(x) = Ï†(x + 2Z). In this case,
e j(x) = cos(Ï€ jx) is the eigenfunction of TK. In fact, since Ï† is an even function and
is cyclic, we have

3.3 Mercerâ€™s Theorem
77
TK e j(x) =

E
Ï†(x âˆ’y) cos(Ï€ jy)dy =
 1âˆ’x
âˆ’1âˆ’x
Ï†(âˆ’u) cos(Ï€ j(x + u))du =

E
Ï†(u) cos(Ï€ j(x + u))du
and
TKe j(x) = {

E
Ï†(u) cos(Ï€ ju)du} cos(Ï€ jx) âˆ’{

E
Ï†(u) sin(Ï€ ju)du} sin(Ï€ jx)
= Î» j cos(Ï€ jx)
from the Addition theorem cos(Ï€ j(x + u)) = cos(Ï€ jx) cos(Ï€ ju) âˆ’sin(Ï€ jx)
sin(Ï€ ju), where Î» j =

E Ï†(u) cos(Ï€ ju)du. Similarly, sin(Ï€ jx) is an eigenfunction,
and Î» j is the corresponding eigenvalue. Thus, from Mercerâ€™s theorem, we have
K(x, y) =
âˆ

j=0
Î» j{cos(Ï€ jx) cos(Ï€ jy) + sin(Ï€ jx) sin(Ï€ jy)} =
âˆ

j=0
Î» j cos{Ï€ j(x âˆ’y)} .
Example 61 (Polynomial Kernel) For the polynomial kernel in Example 8, let m =
2, d = 1. We compute the eigenfunction of K(x, y) = (1 + xy)2 over x, y âˆˆE =
[âˆ’1, 1] by setting e(x) := a0 + a1x + a2x2. By comparing

E
K(x, y)e(y)dy =

E
(1 + xy)2e(y)dy =

E
e(y)dy + {2

E
ye(y)dy}x + {

E
y2 f (y)dy}x2
with Î»e(x), we obtain
â§
âªâªâªâªâªâ¨
âªâªâªâªâªâ©

E
(a0 + a1y + a2y2)dy
= Î»a0
2

E
y(a0 + a1y + a2y2)dy = Î»a1

E
y2(a0 + a1y + a2y2)dy = Î»a2
.
We solve the eigenequation w.r.t. the following matrix:
â¡
â¢â¢â¢â¢â¢â£

E
dy

E ydy

E y2dy
2

E
ydy 2

E y2dy 2

E y3dy

E
y2dy

E y3dy

E y4dy
â¤
â¥â¥â¥â¥â¥â¦
â¡
â£
a0
a1
a2
â¤
â¦= Î»
â¡
â£
a0
a1
a2
â¤
â¦.
Now, we consider the general method for approximately obtaining eigenvalues
and eigenvectors in Mercerâ€™s theorem. Let X be a random variable in E. Then, for
the integral operator Tx âˆˆB(H) (x âˆˆE) deï¬ned by
TK : L2 âˆ‹Ï† â†’

E
K(Â·, x)Ï†(x)dÎ¼(x) âˆˆL2 ,

78
3
Reproducing Kernel Hilbert Space
there exist Î»1 â‰¥Î»2 â‰¥. . . and Ï†1, Ï†2, . . . âˆˆL2 such that
TKÏ† j = Î»Ï† j
and

E
Ï† jÏ†kdÎ¼ = Î´ j.k .
We say that the probability Î¼ has generated x1, . . . , xm âˆˆE with m â‰¥1, and we
approximate the generation as
1
m
m

j=1
K(x j, y)Ï†i(x j) = Î»iÏ†i(y) , y âˆˆE
(3.18)
i = 1, 2, . . .. Since we have
1
m
m

i=1
Ï† j(xi)Ï†k(xi) = Î´ j,k
if we substitute x1, . . . , xm into y in (3.18), we ï¬nd that there exists an orthogonal
matrix U âˆˆRmÃ—m such that
KmU = U ,
where Km âˆˆRmÃ—m is the Gram matrix and  is the diagonal matrix with the elements
Î»(m)
1
= mÎ»1, . . . , Î»(m)
m
= mÎ»m. If we substitute Ï†i(x j) = âˆšmU j,i, Î»i = Î»(m)
i
m
into
(3.18), we obtain
Ï†i(Â·) =
âˆšm
Î»(m)
i
m

j=1
K(x j, Â·)U j,i .
(3.19)
We require that the distribution of x1, . . . , xm âˆˆE coincide with the measure Î¼
of the integral operator. It is known that if we make m larger in Î»(m)
i
/m, the term
converges to the eigenvalue Î»i. For the proof and the convergence process, consult
Baker (Theorem 3.4 [3]).
We write the procedure using the Python as below.
Example 62 We obtain the eigenvalue and eigenfunction by using the following
program with a Gaussian kernel, where the measure required for the deï¬nition of
the integral kernel should be the same as the measure used when providing random
numbers. Even with the same Gaussian kernel, if x1, . . . , xN follows a different dis-
tribution, we obtain different eigenvalues and eigenfunctions. We compare the cases
in which N = 300 and N = 1000 to ï¬nd that the eigenvalues and eigenfunctions
coincide (Figs.3.2 and 3.3).

3.3 Mercerâ€™s Theorem
79
0
20
40
60
80
100
0.00 0.05 0.10 0.15 0.20 0.25
# Eigenvalues
EigenValues
m = 1000
m = 300
The First 100 Eigenvalues
Fig. 3.2 The eigenvalues obtained in Example 62. We compare the cases involving m = 1000
samples and the ï¬rst m = 300 samples. The largest eigenvalues for both cases coincide
# Kernel
D e f i n i t i o n
sigma = 1
def k ( x ,
y ) :
return np . exp ( âˆ’(x âˆ’y ) âˆ—âˆ—2
/
sigma âˆ—âˆ—2)
# Generate
Samples and
Define
the Gram Matrix
m = 300
x = np . random . randn (m) âˆ’2 âˆ—np . random . randn (m) âˆ—âˆ—2 + 3 âˆ—np . random . randn (m)
âˆ—âˆ—3
# Eigenvalues
and
E i g e n v e c t o r s
K = np . zeros ( (m, m) )
for
i
in
range (m) :
for
j
in
range (m) :
K[ i ,
j ] = k ( x [ i ] ,
x [ j ] )
values ,
v e c t o r s = np . l i n a l g . eig (K)
lam = values
/ m
alpha = np . zeros ( (m, m) )
for
i
in
range (m) :
alpha [ : ,
i ] = v e c t o r s [ i ,
: ]
âˆ—np . s q r t (m)
/
( values [ i ] + 10e âˆ’16)
# Display
Graph
def F ( y ,
i ) :
S = 0
for
j
in
range (m) :
S = S + alpha [ j ,
i ] âˆ—k ( x [ j ] ,
y )
return S
i = 1
## Execute
i t
changing
i
def G( y ) :
return F ( y , i )
w = np . l i n s p a c e ( âˆ’2 , 2 ,
100)
p l t . p l o t (w, G(w) )
p l t . t i t l e ("Eigenâ£Valuesâ£andâ£theirâ£Eigenâ£Functions")
Finally, we present the RKHS obtained from Mercerâ€™s theorem (Proposition 41).
In Example 57, we pointed out that the condition was too loose for the L2-space to
be an RKHS. The following proposition suggests the restrictions that we should add.

80
3
Reproducing Kernel Hilbert Space
-2
-1
0
1
2
x
Eigenfunction
-2
-1
0
1
2
0.5
1.0
1.5
2.0
x
Eigenfunction
First Eigenfunction
m = 1000
m = 300
-2
-1
0
1
2
x
Eigenfunction
-2
-1
0
1
2
-1.0 -0.5 0.0 0.5 1.0 1.5
x
Eigenfunction
Second Eigenfunction
m = 1000
m = 300
-2
-1
0
1
2
x
Eigenfunction
-2
-1
0
1
2
-1.0-0.5 0.0 0.5 1.0 1.5 2.0
x
Eigenfunction
Third Eigenfunction
m = 1000
m = 300
-2
-1
0
1
2
x
Eigenfunction
-2
-1
0
1
2
-1.5 -1.0 -0.5 0.0 0.5 1.0
x
Eigenfunction
Fourth Eigenfunction
m = 1000
m = 300
Fig. 3.3 The eigenfunctions obtained in Example 62. We show a comparison between the functions
of the m = 1000 samples and the ï¬rst m = 300 samples. The eigenfunctions coincide for the ï¬rst
largest three eigenvalues, but they are far from each other for the fourth eigenvalue. However, the
fourth eigenvalues coincide
Proposition 42 Let {(Î» j, e j)} be an eigenvalue of an integral operator with a posi-
tive deï¬nite kernel k and an orthonormal eigenfunction. In this case.
H = {
âˆ

j=1
Î² je j|
âˆ

j=1
Î²2
j
Î» j
< âˆ}
âŸ¨f, gâŸ©H :=
âˆ

j=1

E
f (x)e j(x)dÎ·(x)

E
g(x)e j(x)dÎ·(x)
Î» j
(3.20)
gives the RKHS.
The proposition claims that if we restrict the elements âˆ
j=1 Î² je j for which
âˆ
j=1 Î²2
j < âˆto those for which âˆ
j=1
Î²2
j
Î» j < âˆ, the L2 space becomes an RKHS.
Proof: From the deï¬nition of the inner product (3.20), we can write âŸ¨ei, e jâŸ©H =
1
Î»i Î´i, j. Thus, we have

3.3 Mercerâ€™s Theorem
81

E
{
âˆ

j=1
Î² je j(x)}2dÎ²(x) < âˆâ‡â‡’
âˆ

j=1
Î²2
j
Î» j
< âˆ,
and H is a Hilbert space. From Mercerâ€™s theorem, we can write k(x, Â·) = âˆ
j=1 Î» je j
(x)e j(Â·), so we have
âˆ

j=1
{Î» je j(x)}2
Î» j
=
âˆ

j=1
Î» je j(x)e j(x) = k(x, x) < âˆ
and k(x, Â·) âˆˆH. Finally, since

E k(Â·, y)e j(y)dÎ·(y) = Î» je j(Â·), we have
âŸ¨f, k(Â·, x)âŸ©H =
âˆ

j=1
1
Î» j

E
f (y)e j(y)dÎ·(y)

E
k(x, y)e j(y)dÎ·(y)
=
âˆ

j=1
{

E
f (y)e j(y)dÎ·(y)}e j(x) = f (x),
which is the reproducing property.
â–¡
As seen from the proof, the eigenvector {e j} of Mercerâ€™s theorem is orthonormal
in the L2 space, but in the obtained RKHS, the norm is Î»âˆ’1/2
j
. We can see that the
RKHS reduces {Î² j} faster than the L2 space.
Appendix
Proof of Proposition34
Let k : E Ã— E â†’R be the positive deï¬nite kernel of a Hilbert space H. We show
that for the linear space H0 spanned by k(x, Â·), x âˆˆE, the bivariate function
âŸ¨f, gâŸ©H0 =
m

i=1
n

j=1
aib jk(xi, y j)
is an inner product between
f (Â·) =
m

i=1
aik(xi, Â·) and g(Â·) =
n

j=1
b jk(y j, Â·) âˆˆH0.
(3.21)
âŸ¨f, gâŸ©H0 =
m

i=1
aig(xi) =
m

j=1
b j f (x j)

82
3
Reproducing Kernel Hilbert Space
does not depend on the choice of f, g in (3.21). In particular, âŸ¨f, gâŸ©H0 is symmetric.
Since k is a positive deï¬nite kernel, we have
âˆ¥f âˆ¥2 =
m

i=1
n

j=1
aia jk(xi, x j) â‰¥0 .
Moreover, from
| f (x)| = |âŸ¨f (Â·), k(x, Â·)âŸ©H0| â‰¤âˆ¥f âˆ¥H0

k(x, x),
we have âˆ¥f âˆ¥H0 = 0=â‡’f = 0. In the following, we construct the linear space H
obtained by completing H0.
Let { fn} be a Cauchy sequence in H0. For an arbitrary x âˆˆE and m, n â‰¥1, we
have
| fm(x) âˆ’fn(x)| â‰¤âˆ¥fm âˆ’fnâˆ¥H0

k(x, x),
and { fn(x)} is Cauchy. Since this sequence is a real sequence, it has a convergence
point for each x âˆˆE. In the following, let H be the set of f : E â†’R such that the
{ fn(x)} for which { fn} is Cauchy in H0 converges to f (x) for each x âˆˆE. In general,
H0 is a subset of H. In the following, we deï¬ne an inner product in H and prove that
H is an RKHS with a reproducing kernel k.
Lemma 4 Suppose that { fn} is a Cauchy sequence in H0. If the sequence { fn(x)}
converges to 0 for each x âˆˆE, then we have
lim
nâ†’âˆâˆ¥fnâˆ¥H0 = 0 .
Proof of Lemma 4: Since a Cauchy sequence is bounded (Example 26), there exists
a B > 0 such that âˆ¥fnâˆ¥< B, n = 1, 2, . . .. Moreover, since the above sequence
is a Cauchy sequence, for an arbitrary Ïµ > 0, there exists an N such that n >
N=â‡’âˆ¥fn âˆ’fNâˆ¥< Ïµ/B. Thus, for fN(x) = p
i=1 Î±ik(xi, x) âˆˆH0, Î±i âˆˆR, xi âˆˆE,
and i = 1, 2, . . ., we have that when n > N
âˆ¥fnâˆ¥2
H0 = âŸ¨fn âˆ’fN, fnâŸ©H0 + âŸ¨fN, fnâŸ©H0 â‰¤âˆ¥fn âˆ’fNâˆ¥H0âˆ¥fnâˆ¥H0 +
p

i=1
Î±i| fn(xi)| .
Each of the ï¬rst and second terms is at most Ïµ since we have fn(xi) â†’0 as n â†’âˆ
for each i = 1, . . . , p. Hence, we have Lemma 4.
â–¡
For Cauchy sequences { fn}, {gn} in H0, we deï¬ne f, g âˆˆH such that { fn(x)},
{gn(x)} converge to f (x), g(x), respectively, for each x âˆˆE. Then, {âŸ¨fn, gnâŸ©H0} is
Cauchy:
|âŸ¨fn, gnâŸ©H0 âˆ’âŸ¨fm, gmâŸ©H0| = |âŸ¨fn, gn âˆ’gmâŸ©H0 + âŸ¨fn âˆ’fm, gmâŸ©H0|
â‰¤âˆ¥fnâˆ¥H0âˆ¥gn âˆ’gmâˆ¥H0 + âˆ¥fn âˆ’fmâˆ¥H0âˆ¥gmâˆ¥H0 .

Appendix
83
Since {âŸ¨fn, gnâŸ©H0} is real and Cauchy, it converges (Proposition 6). The inner product
obtained by convergence depends only on f (x), g(x) (x âˆˆE).
Let { f â€²
n}, {gâ€²
n} be other Cauchy sequences in H0 that converge to f, g for each
x âˆˆE. Then, { fn âˆ’f â€²
n}, {gn âˆ’gâ€²
n} are Cauchy sequences that converge to 0 for each
x âˆˆE, and from Lemma 4, we have âˆ¥fn âˆ’f â€²
nâˆ¥H0, âˆ¥gn âˆ’gâ€²
nâˆ¥H0 â†’0 as n â†’âˆ,
which means that
|âŸ¨fn, gnâŸ©H0 âˆ’âŸ¨f â€²
n, gâ€²
nâŸ©H0| = |âŸ¨fn, gn âˆ’gâ€²
nâŸ©H0 + âŸ¨fn âˆ’f â€²
n, gâ€²
nâŸ©H0|
â‰¤âˆ¥fnâˆ¥H0âˆ¥gn âˆ’gâ€²
nâˆ¥H0 + âˆ¥fn âˆ’f â€²
nâˆ¥H0âˆ¥gâ€²
nâˆ¥H0 â†’0 .
Thus, the convergence point of {âŸ¨fn, gnâŸ©H0} does not depend on { fn}, {gn} but on
f, g âˆˆH. We deï¬ne the inner product of H by
âŸ¨f, gâŸ©H := lim
nâ†’âˆâŸ¨fn, gnâŸ©H0 .
To show that this expression satisï¬es the deï¬nition of an inner product, we assume
that âˆ¥f âˆ¥H = âŸ¨f, f âŸ©H = 0. Then, for each x âˆˆE, as n â†’âˆ, from
| fn(x)| = |âŸ¨fn(Â·), k(x, Â·)âŸ©| â‰¤

k(x, x)âˆ¥fnâˆ¥H0 â†’0,
we have | f (x)| = limnâ†’âˆ| fn(x)| = 0.
Moreover, since we have deï¬ned f âˆˆH according to lim
nâ†’âˆfn(x) (x âˆˆE) for
any Cauchy sequence { fn} in H0 that converges to f , from the deï¬nition of inner
products, we have
âˆ¥f âˆ’fnâˆ¥H = lim
mâ†’âˆâˆ¥fm âˆ’fnâˆ¥H0 â†’0
(3.22)
n â†’âˆ, and H0 is dense in H.
Weshowthat H iscomplete.Let{ fn}beaCauchysequencein H.Fromdenseness,
there exists a sequence { f â€²
n} in H0 such that
âˆ¥fn âˆ’f â€²
nâˆ¥H â†’0
(3.23)
as n â†’âˆ. Therefore, given an arbitrary Ïµ > 0, for m, n > N, we have
âˆ¥fn âˆ’f â€²
nâˆ¥H, âˆ¥fm âˆ’f â€²
mâˆ¥H, âˆ¥fn âˆ’fmâˆ¥H < Ïµ/3 and
âˆ¥f â€²
n âˆ’f â€²
mâˆ¥H0 = âˆ¥f â€²
n âˆ’f â€²
mâˆ¥H â‰¤âˆ¥fn âˆ’f â€²
nâˆ¥H + âˆ¥fn âˆ’fmâˆ¥H + âˆ¥fm âˆ’f â€²
mâˆ¥H â‰¤Ïµ
for f â€²
n, f â€²
m âˆˆH0 âŠ†H. Thus, { f â€²
n} is a Cauchy sequence in H0, and we deï¬ne f âˆˆ
H by the convergence of f (x) for each x âˆˆE. Moreover, from (3.22), we have
âˆ¥f âˆ’f â€²
nâˆ¥H â†’0. Combining this with (3.23), we obtain
âˆ¥f âˆ’fnâˆ¥H â‰¤âˆ¥f âˆ’f â€²
nâˆ¥H + âˆ¥f â€²
n âˆ’fnâˆ¥H â†’0
as n â†’âˆ. Hence, H is complete.

84
3
Reproducing Kernel Hilbert Space
Next, we show that k is the corresponding reproducing kernel of the Hilbert space
H. Property (3.1) holds immediately because k(x, Â·) âˆˆH0 âŠ†H, x âˆˆE. For another
property (3.2), since f âˆˆH is a limit of the Cauchy sequence { fn} in H0 at x âˆˆE,
we have
f (x) = lim
nâ†’âˆfn(x) = lim
nâ†’âˆâŸ¨fn(Â·), k(x, Â·)âŸ©H0 = âŸ¨f, k(x, Â·)âŸ©H .
Finally, we show that such an H uniquely exists. Suppose that G exists and shares
the same properties possessed by H. Since H is a closure of H0, G should contain
H as a subspace. Since H is closed, from(2.11), we write G = H âŠ•H âŠ¥. However,
since k(x, Â·) âˆˆH, x âˆˆE and âŸ¨f (Â·), k(x, Â·)âŸ©G = 0 for f âˆˆH âŠ¥, we have f (x) = 0,
x âˆˆE, which means that H âŠ¥= {0}.
â–¡
Proof of Proposition38
From our assumption, we have k(x, Â·) = k1(x, Â·) + k2(x, Â·) âˆˆH for each x âˆˆE.
We deï¬ne N âŠ¥âˆ‹(h1(x, Â·), h2(x, Â·)) := vâˆ’1(k(x, Â·)) for each x âˆˆE, where h1(x, Â·),
h2(x, Â·) are elements in H1, H2 for x âˆˆE, but h1, h2 are not necessarily reproducing
kernels k1, k2 of H1, H2, respectively. Since k(x, Â·) = k1(x, Â·) + k2(x, Â·), we have
h1(x, Â·) âˆ’k1(x, Â·) + h2(x, Â·) âˆ’k2(x, Â·) = k(x, Â·) âˆ’k(x, Â·) = 0
and z := (h1(x, Â·) âˆ’k1(x, Â·), h2(x, Â·) âˆ’k2(x, Â·)) âˆˆN, so
0 = âŸ¨0, f âŸ©H = âŸ¨z, ( f1, f2)âŸ©F
for f âˆˆH and N âŠ¥âˆ‹( f1, f2) := vâˆ’1( f ). Thus, we have
âŸ¨f1, h1(x, Â·)âŸ©1 + âŸ¨f2, h2(x, Â·)âŸ©2 = âŸ¨f1, k1(x, Â·)âŸ©1 + âŸ¨f2, k2(x, Â·)âŸ©2,
which implies the reproducing property:
âŸ¨f, k(x, Â·)âŸ©H = âŸ¨vâˆ’1( f ), vâˆ’1(k(x, Â·))âŸ©F = âŸ¨( f1, f2), (h1(x, Â·), h2(x, Â·))âŸ©F
= âŸ¨( f1, f2), (k1(x, Â·), k2(x, Â·))âŸ©F = f1(x) + f2(x) = f (x) .
Furthermore, let ( f1, f2) âˆˆF, f := f1 + f2, and (g1, g2) := ( f1, f2) âˆ’vâˆ’1( f ).
Then, from (g1, g2) âˆˆN and vâˆ’1( f ) âˆˆN âŠ¥, we have
âˆ¥( f1, f2)âˆ¥2
F = âˆ¥vâˆ’1( f )âˆ¥2
F + âˆ¥(g1, g2)âˆ¥2
F .
Combining this with (3.4) and (3.5), we have
âˆ¥f âˆ¥2
H = âˆ¥vâˆ’1( f )âˆ¥2
F â‰¤âˆ¥( f1, f2)âˆ¥2
F = âˆ¥f1âˆ¥2
H1 + âˆ¥f2âˆ¥2
H2,
where the equality holds when ( f1, f2) = vâˆ’1( f ).
â–¡

Appendix
85
Proof of Example59
We use the equality [10]
 âˆ
âˆ’âˆ
exp(âˆ’(x âˆ’y)2)Hj(Î±x)dx = âˆšÏ€(1 âˆ’Î±2) j/2Hj(
Î±y
(1 âˆ’Î±2)1/2 ) .
Suppose that

E p(y)dy = 1. If we have

E
k(x, y)Ï† j(y)p(y)dy = Î»Ï† j(x),
then

E
Ëœk(x, y) ËœÏ† j(y)dy = Î» ËœÏ† j(x)
for Ëœk(x, y) := p(x)1/2k(x, y)p(y)1/2, ËœÏ† j(x) := p(x)1/2Ï† j(x). Thus, it is sufï¬cient
to show that we obtain the right-hand side by substituting
p(x) :=

2a
Ï€ exp(âˆ’2ax2)
Ëœk(x, y) :=

2a
Ï€ exp(âˆ’ax2) exp(âˆ’b(x âˆ’y)2) exp(âˆ’ay2)
ËœÏ† j(x) := (2a
Ï€ )1/4 exp(âˆ’cx2)Hj(
âˆš
2cx)
into the left-hand side for E = (âˆ’âˆ, âˆ). The left-hand side becomes
 âˆ
âˆ’âˆ
(2a
Ï€ )3/4 exp(âˆ’ax2) exp(âˆ’b(x âˆ’y)2) exp(âˆ’ay2) exp(âˆ’cy2)Hj(
âˆš
2cy)dy
= (2a
Ï€ )3/4
 âˆ
âˆ’âˆ
exp{âˆ’(a + b + c)(y âˆ’
b
a + b + c x)2 + [
b2
a + b + c âˆ’(a + b)]x2}Hj(
âˆš
2cy)dy
= (2a
Ï€ )3/4 exp(âˆ’cx2)
 âˆ
âˆ’âˆ
exp{âˆ’(z âˆ’
b
âˆša + b + c
x)2}Hj(
âˆš
2c
âˆša + b + c
z)
dz
âˆša + b + c
= (2a
Ï€ )1/4

2a
Ï€(a + b + c) exp(âˆ’cx2)âˆšÏ€(1 âˆ’
2c
a + b + c ) j/2Hj(
âˆš
2cx)
=

2a
a + b + c (
b
a + b + c ) j(2a
Ï€ )1/4 exp(âˆ’cx2)Hj(
âˆš
2cx) =

2a
A B j ËœÏ† j(x),
where we deï¬ne z := yâˆša + b + c, Î± :=
âˆš
2c
âˆša + b + c
and use

86
3
Reproducing Kernel Hilbert Space
(1 âˆ’Î±2)1/2 =

1 âˆ’
2c
a + b + c =

a + b âˆ’c
a + b + c =

(a + b)2 âˆ’c2
(a + b + c)2 =
b
a + b + c .
â–¡
Proof of Proposition40
Since K is uniformly continuous, if d is the distance E Ã— E, there exists a Î´n such
that
d((x1, y1), (x2, y2)) < Î´n=â‡’|K(x1, y1) âˆ’K(x2, y2)| < nâˆ’1
for n = 1, 2, . . . and arbitrary x1, x2, y1, y2 âˆˆE. Since E is compact, we can cover
it with a ï¬nite number of balls {En,i}m
i=1 of diameter Î´n. If we arbitrarily choose vi âˆˆ
En,i and deï¬ne Kn(x, y) := K(vi, v j) for (x, y) âˆˆEn,i Ã— En, j, from the uniform
continuity of K, we obtain
max
(x,y)âˆˆEÃ—E |K(x, y) âˆ’Kn(x, y)| < 1
n .
Let TK, TKn be the integral operators of K, Kn. Then, we have
|âŸ¨TK f, f âŸ©âˆ’âŸ¨TKn f, f âŸ©| â‰¤nâˆ’1âˆ¥f âˆ¥2
and
âŸ¨TKn f, f âŸ©=
m

i=1
m

j=1
K(vi, v j)

En,i
f (x)dÎ¼(x)

En, j
f (y)dÎ¼(y)
foranarbitraryn,andwehaveâŸ¨TK f, f âŸ©â‰¥0.Conversely,supposethatâŸ¨TK f, f âŸ©â‰¥0.
If there exist x1, . . . , xm âˆˆE, z1, . . . , zm âˆˆR such that m
i=1
m
j=1 ziz jk(xi, x j) <
0, since K is uniformly continuous, there exist E1, . . . , Em âˆˆF such that
max
xh,yhâˆˆEh,h=1,...,m
m

i=1
m

j=1
ziz j K(xi.y j) < 0
and Î¼(E1), . . . , Î¼(Em) > 0. However, from the mean value theorem, we have
âŸ¨TK f, f âŸ©:=
m

i=1
m

j=1
ziz j{Î¼(Ei)Î¼(E j)}âˆ’1

Ei

E j
k(x, y)dÎ¼(x)dÎ¼(y) < 0 .
for f = m
i=1 zi{Î¼(Ei)}âˆ’1IEi, which contradicts the fact that TK is positive deï¬nite.
â–¡

Appendix
87
Proof of Lemma3
We assume that fn(x) monotonically increases as n grows for each x âˆˆE. Let Ïµ > 0
be arbitrary. For each x âˆˆE, let n(x) be the minimum n such that | fn(x) âˆ’f (x)| <
Ïµ. From continuity, for each x âˆˆE, we set U(x) so that
y âˆˆU(x)=â‡’| f (x) âˆ’f (y)| < Ïµ, | fn(x)(x) âˆ’fn(x)(y)| < Ïµ .
Then, we have
f (y) âˆ’fn(x)(y) â‰¤f (x) + Ïµ âˆ’fn(x)(y) â‰¤fn(x)(x) + 2Ïµ âˆ’fn(x)(y) â‰¤| fn(x)(x) âˆ’fn(x)(y)| + 2Ïµ < 3Ïµ .
Moreover, since E is compact, we may suppose that E âŠ†âˆªm
i=1U(xi). If N is the
maximum value of n(x1), . . . , n(xm), for n â‰¥N, we have
f (y) âˆ’fn(y) â‰¤f (y) âˆ’fn(xi)(y) â‰¤3Ïµ
for each y âˆˆE and each i for which y âˆˆU(xi).
â–¡
Exercises 31âˆ¼45
31. Proposition 34 can be derived according to the following steps. Which part of
the proof in the appendix does each step correspond to?
(a) Deï¬ne the inner product âŸ¨Â·, Â·âŸ©H0 of H0 := span{k(x, Â·) : x âˆˆE}.
(b) For any Cauchy sequence { fn} in H0 and each x âˆˆE, the real sequence
{ fn(x)} is Cauchy, so it converges to a f (x) := lim
nâ†’âˆfn(x) (Proposition 6).
Let H be such a set of f s.
(c) Deï¬ne the inner product âŸ¨Â·, Â·âŸ©H of the linear space H.
(d) Show that H0 is dense in H.
(e) Show that any Cauchy sequence { fn} in H converges to some element of H
as n â†’âˆ(completeness of H).
(f) Show that k is a reproducing kernel of H.
(g) Show that such an H is unique.
32. In Examples 55 and 56, the inner product is âŸ¨f, gâŸ©H =
 1
0 F(u)G(u)du, and the
RKHS is
H = {E âˆ‹x â†’

E
F(t)J(x, t)dÎ·(t) âˆˆR|F âˆˆL2(E, Î·)} .
What are the J(x, t) in Examples 55 and 56? Also, how is the kernel k(x, y)
represented in general by using J(x, t)?
33. Proposition 38 can be derived according to the following steps. Which part of
the proof in the appendix does each step correspond to?

88
3
Reproducing Kernel Hilbert Space
(a) Fix f âˆˆH arbitrarilydeï¬ne N âŠ¥âˆ‹( f1, f2) := vâˆ’1( f ),k(x, Â·) := k1(x, Â·) +
k2(x, Â·), and (h1(x, Â·), h2(x, Â·)) := vâˆ’1(k(x, Â·)), and show that
âŸ¨f1, h1(x, Â·)âŸ©1 + âŸ¨f2, h2(x, Â·)âŸ©2 = âŸ¨f1, k1(x, Â·)âŸ©1 + âŸ¨f2, k2(x, Â·)âŸ©2
(b) Using (a), prove the reproducing property of k: âŸ¨f, k(x, Â·)âŸ©H = f (x).
(c) Show that the norm of H is (3.6)
34. Show that each f âˆˆWq[0, 1] can be the Taylor series expanded by
f (x) =
qâˆ’1

i=0
f (i)(0)Ï†i(x) +
 1
0
Gq(x, y) f (q)(y)dy
using
Ï†i(x) := xi
i! , i = 0, 1, . . .
and
Gq(x, y) := (x âˆ’y)qâˆ’1
+
(q âˆ’1)!
.
35. Show that Wq[0, 1] = H0 âŠ•H1, where
H0 = {
qâˆ’1

i=0
Î±iÏ†i(x)|Î±0, . . . , Î±qâˆ’1 âˆˆR}
H1 = {
 1
0
Gq(x, y)h(y)dy|h âˆˆL2[0, 1]}
(You need to show the inclusion relation on both sides of the set). In addition,
show that H0 âˆ©H1 = {0}.
36. We consider the integral operator Tk of k(x, y) = min{x, y}, in L2[0, 1], where
x, y âˆˆE = [0, 1]. Substitute
Î» j =
4
{(2 j âˆ’1)Ï€}2
e j(x) =
âˆš
2 sin
(2 j âˆ’1)Ï€
2
x

into Tke j = Î» je j to examine the equality.
37. Show that the eigenvalues in Example 59 form a geometric sequence with the
initial values and ratio that are determined by Î² := Ë†Ïƒ 2/Ïƒ 2.

Exercises 31âˆ¼45
89
38. In Example 59, the following program obtains eigenvalues and eigenfunctions
under the assumption that Ïƒ 2 = Ë†Ïƒ 2 = 1. We can change the program to set the
values of Ïƒ 2, Ë†Ïƒ 2 in ## and add Ïƒ 2, Ë†Ïƒ 2 as an argument to the function phi in ###
and run it to output a graph.
def H( j ,
x ) :
i f
j == 0:
return 1
e l i f
j == 1:
return 2 âˆ—x
e l i f
j == 2:
return âˆ’2 + 4 âˆ—xâˆ—âˆ—2
e l s e :
return 4 âˆ—x âˆ’8 âˆ—xâˆ—âˆ—3
cc = np . s q r t ( 5 )
/
4
a = 1/4
##
def
phi ( j ,
x )
: ###
return np . exp ( âˆ’( cc âˆ’a ) âˆ—x âˆ—âˆ—2) âˆ—H( j ,
np . s q r t (2 âˆ—cc ) âˆ—x )
c o l o r = ["b" , "g" , "r" , "k"]
x = np . l i n s p a c e ( âˆ’2 , 2 ,
100)
p l t . p l o t ( x ,
phi (0 , x ) , c = c o l o r [ 0 ] ,
l a b e l = "jâ£=â£0")
p l t . ylim ( âˆ’2 ,
8)
p l t . y l a b e l ("phi")
for
i
in
range (0 ,
3) :
p l t . p l o t ( x ,
phi ( i ,
x ) , c = c o l o r [ i + 1] ,
l a b e l = "jâ£=â£%d"%i )
p l t . t i t l e ("Characteristicâ£functionâ£ofâ£Gaussâ£Kernel")
39. Show the following:
(a) The function fn(x) = n2(1 âˆ’x)xn+1 deï¬ned over [0, 1] converges at each
x âˆˆ[0, 1], but its upper bound does not converge (it is not uniformly con-
vergent).
(b) The function fn(x) = (1 âˆ’x)xn+1 deï¬ned over [0, 1] converges uniformly
(using Lemma 3).
(c) The series âˆ
n=0
(âˆ’1)n
âˆšn+1 converges absolutely.
40. In Example 58, suppose that the period of Ï† is 2Ï€ instead of 2. What are the
eigenvalues and eigenfunctions of Tk? Additionally, derive the kernel k.
41. What eigenequations should be solved in Example 61 when m = 3, d = 1?
42. Deï¬ne and execute the following part of the program in Example 62 as a function.
The input for this includes data x, a kernel k, and the i of the ith eigenvalue. The
output is a function F.
K = np . zeros ( (m, m) )
for
i
in
range (m) :
for
j
in
range (m) :
K[ i ,
j ] = k ( x [ i ] ,
x [ j ] )
values ,
v e c t o r s = np . l i n a l g . eig (K)

90
3
Reproducing Kernel Hilbert Space
lam = values
/ m
alpha = np . zeros ( (m, m) )
for
i
in
range (m) :
alpha [ : ,
i ] = v e c t o r s [ : ,
i ] âˆ—np . s q r t (m)
/
( values [ i ] + 10e âˆ’16)
def F ( y ,
i ) :
S = 0
for
j
in
range (m) :
S = S + alpha [ j ,
i ] âˆ—k ( x [ j ] ,
y )
return S
43. In Example 62, for the Gaussian kernel, random numbers are generated accord-
ing to the normal distribution, and we obtain the corresponding eigenvalues and
eigenfunctions. When the number of samples is large, theoretically, the eigenval-
ues are reduced exponentially (Example 59). What happens with the polynomial
kernel k(x, y) = (1 + xy)2 when m = 2 and d = 1? Output the eigenvalues and
eigenfunctions as the Gaussian kernel.
44. If we construct (3.19) using the solution of KmU = U, show that the result is
a solution of (3.18) and that it is orthogonal with a magnitude of 1.
45. In Proposition 42, Î² j should originally satisfy âˆ
j=1 Î²2
j < âˆ. However, this is
not stated in the assertion of Proposition 42. Why is this the case?

Chapter 4
Kernel Computations
In Chap.1, we learned that the kernel k(x, y) âˆˆR represents the similarity between
two elements x, y in a set E. Chapter3 described the relationships between a kernel
k, its feature map E âˆ‹x â†’k(x, Â·) âˆˆH, and its reproducing kernel Hilbert space
H. In this chapter, we consider k(x, Â·) to be a function of E â†’R for each x âˆˆE,
and we perform data processing for N actual data pairs (x1, y1), . . . , (xN, yN) of
covariates and responses. The xi, i = 1, . . . , N (row vectors) are p-dimensional and
given by the matrix X âˆˆRNÃ—p. The responses yi (i = 1, . . . , N) may be real or
binary. This chapter discusses kernel ridge regression, principal component analysis,
support vector machines (SVMs), and splines, and we ï¬nd the f âˆˆH that minimizes
the objective function under various constraints. It is known that we can write the
optimal f in the form N
i=1 Î±ik(xi, Â·) (representation theorem), and the problem
reduces to ï¬nding the optimal Î±1, . . . , Î±N.
In the second half, we address the problem of computational complexity. The
computation of a kernel takes more than O(N 3), and real-time calculation is hard
when N is greater than 1000. In particular, we consider how to reduce the rank of the
Gram matrix K. Speciï¬cally, we learn actual procedures for random Fourier features,
NystrÃ¶m approximation, and incomplete Cholesky decomposition.
4.1
Kernel Ridge Regression
We say that ï¬nding the Î² âˆˆRp (column vector) that minimizes N
i=1(yi âˆ’xiÎ²)2
is the least-squares problem. If we assume that we have executed the central-
ization process such that yi â†yi âˆ’Â¯y and xi, j â†xi, j âˆ’Â¯x j for Â¯y = 1
N
N

i=1
yi and
Â© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022
J. Suzuki, Kernel Methods for Machine Learning with Math and Python,
https://doi.org/10.1007/978-981-19-0401-1_4
91

92
4
Kernel Computations
Â¯x j = 1
N
N

i=1
xi, j and that the matrix XâŠ¤X is nonsingular, we can obtain the solution
as Ë†Î² = (XâŠ¤X)âˆ’1XâŠ¤y from X = (xi, j) and y = (yi). In the following, we prepare
a kernel k : E Ã— E â†’R and consider the problem of ï¬nding the f âˆˆH that mini-
mizes
L :=
N

i=1
(yi âˆ’f (xi))2.
As we considered in Example 40, we express the RKHS H as the sum of
M := span({k(xi, Â·)}N
i=1)
and
MâŠ¥= { f âˆˆH|âŸ¨f, k(xi, Â·)âŸ©H = 0, i = 1, . . . , N} .
If we set f = f1 + f2, f1 âˆˆM, f2 âˆˆMâŠ¥, then we have
N

i=1
(yi âˆ’f (xi))2 =
N

i=1
(yi âˆ’f1(xi))2 =
N

i=1
(yi âˆ’
N

j=1
Î±k(x j, xi))2
(4.1)
and E := Rp; we then obtain
f (xi) = âŸ¨f1(Â·) + f2(Â·), k(xi, Â·)âŸ©H = âŸ¨f1(Â·), k(xi, Â·)âŸ©H = f1(xi)
for i = 1, . . . , N. Thus, the minimization of L reduces to that of
L =
N

i=1
{yi âˆ’
N

j=1
Î± jk(x j, xi)}2 = âˆ¥y âˆ’KÎ±âˆ¥2 ,
(4.2)
where K = (k(xi, x j))i, j=1,...,N is a Gram matrix, and the norm âˆ¥zâˆ¥of z = [z1, . . . ,
zN] âˆˆR denotes
N
i=1 z2
i . The above principle is the representation theorem.
If we differentiate L by Î±, we have âˆ’K(y âˆ’KÎ±) = 0. If K is positive deï¬nite
rather than nonnegative deï¬nite, then the solution becomes Ë†Î± = K âˆ’1y.
If we use the Ë†f âˆˆH obtained as above that minimizes (4.2), then we can predict
the value of y given a new x âˆˆRp via
Ë†f (x) =
n

i=1
Ë†Î±ik(xi, x).

4.1 Kernel Ridge Regression
93
We can construct a procedure to compute Î± as follows:
# We i n s t a l l
skfda
module
beforehand
pip
i n s t a l l
cvxopt
# In
t h i s
chapter ,
we assume
t h a t
the
f o l l o w i n g
have
been
executed .
import numpy as np
import
pandas
as pd
from
s k l e a r n . decomposition
import PCA
import
cvxopt
from cvxopt
import
s o l v e r s
from cvxopt
import
matrix
import
m a t p l o t l i b . pyplot
as
p l t
from
m a t p l o t l i b
import
s t y l e
s t y l e . use ("seabornâˆ’ticks")
from numpy . random import
randn # Gaussian random numbers
from
scipy . s t a t s
import norm
def
alpha ( k ,
x ,
y ) :
n = len ( x )
K = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K[ i ,
j ] = k ( x [ i ] ,
x [ j ] )
return np . l i n a l g . inv (K + 10eâˆ’5 âˆ—np . i d e n t i t y ( n ) ) . dot ( y )
# Add 10^( âˆ’5)
I
to K f o r
making
i t
i n v e r t i b l e
Example 63 Utilizing the function alpha, we execute kernel regression via poly-
nomial and Gaussian kernels for n = 50 data (Î» = 0.1). We present the output in
Fig.4.1.
def k_p ( x ,
y ) :
# Kernel
D e f i n i t i o n
return
( np . dot ( x . T ,
y ) + 1) âˆ—âˆ—3
def k_g ( x ,
y ) :
# Kernel
D e f i n i t i o n
return np . exp ( âˆ’(x âˆ’y ) âˆ—âˆ—2
/
2)
lam = 0.1
n = 50; x = np . random . randn ( n ) ;
y = 1 + x + xâˆ—âˆ—2 + np . random . randn ( n ) # Data
Generation
alpha_p = alpha ( k_p ,
x ,
y )
alpha_g = alpha ( k_g ,
x ,
y )
z = np . s o r t ( x ) ;
u =
[ ] ;
v = [ ]
for
j
in
range ( n ) :
S = 0
for
i
in
range ( n ) :
S = S + alpha_p [ i ] âˆ—k_p ( x [ i ] ,
z [ j ] )
u . append ( S )
S = 0
for
i
in
range ( n ) :
S = S + alpha_g [ i ] âˆ—k_g ( x [ i ] ,
z [ j ] )
v . append ( S )
p l t . s c a t t e r ( x ,
y ,
f a c e c o l o r s =â€™noneâ€™ ,
e d g e c o l o r s = "k" ,
marker = "o")

94
4
Kernel Computations
-1.0
-0.5
0.0
0.5
1.0
-1
0
1
2
3
4
5
Kernel Regression
x
y
Polynomial Kernel
Gaussian Kernel
Fig. 4.1 We execute kernel regression by using polynomial and Gaussian kernels
p l t . p l o t ( z ,
u ,
c = "r" ,
l a b e l = "Polynomialâ£Kernel")
p l t . p l o t ( z ,
v ,
c = "b" ,
l a b e l = "Gaussâ£Kernel")
p l t . xlim ( âˆ’1 ,
1)
p l t . ylim ( âˆ’1 ,
5)
p l t . x l a b e l ("x")
p l t . y l a b e l ("y")
p l t . t i t l e ("Kernelâ£Regression")
p l t . legend ( loc = "upperâ£left" ,
frameon = True ,
prop ={â€™sizeâ€™: 1 4 } )
We cannot obtain the solution of a linear regression problem when the rank of X
is smaller than p, i.e., N < p. Thus, we often minimize
N

i=1
(yi âˆ’xiÎ²)2 + Î»âˆ¥Î²âˆ¥2
2
for cases in which Î» > 0. We call such a modiï¬cation of linear regression a ridge. The
Î² to be minimized is given by (XâŠ¤X + Î»I)âˆ’1XâŠ¤y. In fact, we derive the formula
by differentiating
âˆ¥y âˆ’XÎ²âˆ¥2 + Î»Î²âŠ¤Î²
by Î² and equating it to zero; we obtain
âˆ’XâŠ¤(y âˆ’XÎ²) + Î»Î² = 0 .
We consider extending ridge regression to the problem of ï¬nding the f âˆˆH that
minimizes

4.1 Kernel Ridge Regression
95
Lâ€² :=
N

i=1
(yi âˆ’f (xi))2 + Î»âˆ¥f âˆ¥2
H .
(4.3)
Since f1 and f2 are orthogonal, we have
âˆ¥f âˆ¥2
H = âˆ¥f1âˆ¥2
H + âˆ¥f2âˆ¥2
H + 2âŸ¨f1, f2âŸ©H = âˆ¥f1âˆ¥2
H + âˆ¥f2âˆ¥2
H â‰¥âˆ¥f1âˆ¥2
H .
(4.4)
From (4.1), (4.3), and (4.4), we also have
Lâ€² â‰¥
N

i=1
(yi âˆ’f1(xi))2 + Î»âˆ¥f1âˆ¥2
H .
If we note that the second term can be expressed by
âˆ¥f1âˆ¥2
H = âŸ¨
N

i=1
Î±ik(xi, Â·),
N

j=1
Î± jk(x j, Â·)âŸ©H =
N

i=1
N

j=1
Î±iÎ± jâŸ¨k(xi, Â·), k(x j, Â·)âŸ©H = Î±âŠ¤KÎ±
for Î± = [Î±1, . . . , Î±N]âŠ¤, then the minimization of Lâ€² reduces to that of
âˆ¥y âˆ’KÎ±âˆ¥2 + Î»Î±âŠ¤KÎ± .
(4.5)
If we differentiate the equation by Î± and set it equal to zero, we obtain
âˆ’K(y âˆ’KÎ±) + Î»KÎ± = 0 .
If K is nonsingular, we have
Ë†Î± = (K + Î»I)âˆ’1y .
(4.6)
Finally, if we use the Ë†f âˆˆH that minimizes the (4.3) obtained thus far, we can
predict the value of y given a new x âˆˆRp via
Ë†f (x) =
n

i=1
Ë†Î±ik(xi, x) .
For example, we can construct a procedure that ï¬nds Î± as follows:
def
alpha ( k ,
x ,
y ) :
n = len ( x )
K = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K[ i ,
j ] = k ( x [ i ] ,
x [ j ] )
return np . l i n a l g . inv (K + lam âˆ—np . i d e n t i t y ( n ) ) . dot ( y )

96
4
Kernel Computations
Fig. 4.2 We execute kernel
ridge regression using
polynomial and Gaussian
kernels
-1.0
-0.5
0.0
0.5
1.0
-1
0
1
2
3
4
5
Kernel Ridge
x
y
Example 64 Using the function alpha, we execute kernel ridge regression for poly-
nomial and Gaussian kernels and n = 50 data(Î» = 0.1). We show the outputs in
Fig.4.2.
def k_p ( x ,
y ) :
# Kernel
D e f i n i t i o n
return
( np . dot ( x . T ,
y ) + 1) âˆ—âˆ—3
def k_g ( x ,
y ) :
# Kernel
D e f i n i t i o n
return np . exp ( âˆ’(x âˆ’y ) âˆ—âˆ—2
/
2)
lam = 0.1
n = 50; x = np . random . randn ( n ) ;
y = 1 + x + xâˆ—âˆ—2 + np . random . randn ( n )
#
Data
Generation
alpha_p = alpha ( k_p ,
x ,
y )
alpha_g = alpha ( k_g ,
x ,
y )
z = np . s o r t ( x ) ;
u =
[ ] ;
v = [ ]
for
j
in
range ( n ) :
S = 0
for
i
in
range ( n ) :
S = S + alpha_p [ i ] âˆ—k_p ( x [ i ] ,
z [ j ] )
u . append ( S )
S = 0
for
i
in
range ( n ) :
S = S + alpha_g [ i ] âˆ—k_g ( x [ i ] ,
z [ j ] )
v . append ( S )
p l t . s c a t t e r ( x ,
y ,
f a c e c o l o r s =â€™noneâ€™ ,
e d g e c o l o r s = "k" ,
marker = "o")
p l t . p l o t ( z ,
u ,
c = "r" ,
l a b e l = "Polynomialâ£Kernel")
p l t . p l o t ( z ,
v ,
c = "b" ,
l a b e l = "Gaussâ£Kernel")
p l t . xlim ( âˆ’1 ,
1)
p l t . ylim ( âˆ’1 ,
5)
p l t . x l a b e l ("x")
p l t . y l a b e l ("y")
p l t . t i t l e ("Kernelâ£Ridge")
p l t . legend ( loc = "upperâ£left" ,
frameon = True ,
prop ={â€™sizeâ€™: 1 4 } )

4.2 Kernel Principle Component Analysis
97
4.2
Kernel Principle Component Analysis
We review the procedure of principal component analysis (PCA) when we do not use
any kernel. We centralize each of the columns in the matrix X and vector y. We ï¬rst
compute the v1 := v âˆˆRp that maximizes vâŠ¤XâŠ¤Xv under vâŠ¤v = 1. Similarly, for
i = 2, . . . , p, we repeatedly compute vi with the vâŠ¤v = 1 that maximizes vâŠ¤XâŠ¤Xv
and is orthogonal to v1, Â· Â· Â· , viâˆ’1 âˆˆRp. In the actual cases, we do not use all of
the v1, Â· Â· Â· , vp but compress Rp to the v1, Â· Â· Â· , vm (1 â‰¤m â‰¤p) with the largest
eigenvalues. We compute the v âˆˆRp that maximizes
vâŠ¤XâŠ¤Xv âˆ’Î¼(vâŠ¤v âˆ’1)
(4.7)
with a Î¼ > 0 Lagrange coefï¬cient to ï¬nd v âˆˆRp with the vâŠ¤v = 1 that maximizes
vâŠ¤XâŠ¤Xv. In PCA, we often compute
â¡
â¢â£
xv1
...
xvm
â¤
â¥â¦âˆˆRm
for each row vector x âˆˆRp using the obtained v1, . . . , vm âˆˆRp. We call such a value
the score of x, which is the vector obtained by projecting x onto the m elements.
We may apply a problem that is similar to PCA for an RKHS H via the feature
map  : E âˆ‹xi â†’k(xi, Â·) âˆˆH rather than the PCA in Rp. To this end, we consider
the problem of ï¬nding the f âˆˆH that maximizes
N

j=1
f (xi)2 âˆ’Î¼(âˆ¥f âˆ¥2
H âˆ’1)
(4.8)
with an Î¼ > 0 Lagrange coefï¬cient.
If we use the linear kernel (the standard inner product), we can express f âˆˆH by
f (Â·) = âŸ¨w, Â·âŸ©E with w âˆˆE. Thus, (4.7) and (4.8) coincide. The centralization in the
kernel PCA is for the Gram matrix K rather than the matrix X. For the other part,
the extension follows in the same manner.
As discussed in the previous section, we apply the representation theorem. Thus,
for f1 âˆˆM := span({k(xi, Â·)}N
i=1) and f2 âˆˆMâŠ¥, we have
N

i=1
f (xi)2=
N

i=1
âŸ¨f1(Â·)+ f2(Â·), k(xi, Â·)âŸ©2
H =
N

i=1
âŸ¨f1(Â·), k(xi, Â·)âŸ©2
H=
N

i=1
f1(xi)2
=
N

i=1
{
N

j=1
Î± jk(x j, xi)}2 =
N

i=1
N

r=1
N

s=1
Î±rÎ±sk(xr, xi)k(xs, xi) = Î±âŠ¤K 2Î±

98
4
Kernel Computations
âˆ¥f1 + f2âˆ¥2
H = âˆ¥f1âˆ¥2
H + âˆ¥f2âˆ¥2
H â‰¥âˆ¥f1âˆ¥2
H
= âˆ¥
N

j=1
Î± jk(x j, Â·)âˆ¥2
H =
N

r=1
N

s=1
Î±rÎ±sk(xr, xs) = Î±âŠ¤KÎ± .
Hence, we can formulate (4.8) as the maximization of
Î±âŠ¤K âŠ¤KÎ± âˆ’Î¼(Î±âŠ¤KÎ± âˆ’1) .
If we substitute Î² = K 1/2Î±, then since K is symmetric, we have
Î²âŠ¤KÎ² âˆ’Î¼(Î²âŠ¤Î² âˆ’1) .
Let Î»1, . . . , Î»N and u1, . . . , uN be the eigenvalues and eigenvectors of the eigenequa-
tion KÎ² = Î»Î², respectively. Then, we have [26]
Î± = K âˆ’1/2Î² =
1
âˆš
Î»
Î² =
u1
âˆš
Î»1
, . . . , uN
âˆš
Î»N
.
If we centralize the Gram matrix K = (k(xi, x j)), then the (i, j)th element of the
modiï¬ed Gram matrix is
âŸ¨k(xi, Â·) âˆ’1
N
N

h=1
k(xh, Â·), k(x j, Â·) âˆ’1
N
N

h=1
k(xh, Â·)âŸ©H
= k(xi, x j) âˆ’1
N
N

h=1
k(xi, xh) âˆ’1
N
N

l=1
k(x j, xl)
+ 1
N 2
N

h=1
N

l=1
k(xh, xl) .
(4.9)
To obtain the score (size 1 â‰¤m â‰¤p) of x âˆˆRp (row vector), we use the ï¬rst m
columns of A = [Î±1, . . . , Î±N]âŠ¤âˆˆRNÃ—p. Let xi âˆˆRp and Î±i âˆˆRm be a row vector
of X and the ith column of A âˆˆRNÃ—m, respectively. Then,
N

i=1
Î±ik(xi, x) âˆˆRm
is the score of x âˆˆRp.
Compared to ordinary PCA, kernel PCA requires a computational time of O(N 3).
Therefore, when N is large compared to p, the computational complexity may be
enormous. In the Python, we can write the procedure as follows:

4.2 Kernel Principle Component Analysis
99
def
k e r n e l _ p c a _ t r a i n ( x ,
k ) :
n = x . shape [ 0 ]
K = np . zeros ( ( n ,
n ) )
S = [ 0 ] âˆ—n ; T = [ 0 ] âˆ—n
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K[ i ,
j ] = k ( x [ i ,
: ] ,
x [ j ,
: ] )
for
i
in
range ( n ) :
S [ i ] = np . sum(K[ i ,
: ] )
for
j
in
range ( n ) :
T[ j ] = np . sum(K[ : ,
j ] )
U = np . sum(K)
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K[ i ,
j ] = K[ i ,
j ] âˆ’S [ i ]
/
n âˆ’T[ j ]
/
n + U /
nâˆ—âˆ—2
val ,
vec = np . l i n a l g . eig (K)
idx = val . a r g s o r t ( ) [:: âˆ’1]
# decreasing
order
as R
val = val [ idx ]
vec = vec [ : , idx ]
alpha = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
alpha [ : ,
i ] = vec [ : ,
i ]
/
val [ i ]âˆ—âˆ—0.5
return
alpha
def
k e r n e l _ p c a _ t e s t ( x ,
k ,
alpha , m,
z ) :
n = x . shape [ 0 ]
pca = np . zeros (m)
for
i
in
range ( n ) :
pca = pca + alpha [ i ,
0:m] âˆ—k ( x [ i ,
: ] ,
z )
return
pca
In kernel PCA, when we use the linear kernel, the scores are consistent with those
of PCA without any kernel. For simplicity, we assume that X is normalized. If we do
not use the kernel, then by the singular value decomposition of X = UV âŠ¤(U âˆˆ
RNÃ—p,  âˆˆRpÃ—p, V âˆˆRpÃ—p), the multiplication of
1
Nâˆ’1 XâŠ¤X =
1
Nâˆ’1V 2V âŠ¤and
V âŠ¤is
1
Nâˆ’1 XâŠ¤XV =
1
Nâˆ’1V 2. Thus, each column of V is a principal component
vector, and the scores of x1, . . . , xN âˆˆRp (row vector) are the ï¬rst m columns of
XV = UV âŠ¤Â· V = U .
On the other hand, for the linear kernel, we may write the Gram matrix as
K = X XâŠ¤= U2U âŠ¤and have KU = X XâŠ¤U = U2. That is, each column of
U is Î²1, . . . , Î²N, and the columns Î±1, . . . , Î±N of K âˆ’1/2U are the principal compo-
nent vectors. Therefore, the scores of x1, . . . , xN âˆˆRp (row vectors) are the ï¬rst m
columns of
K Â· K âˆ’1/2U = U2U âŠ¤Â· (U2U âŠ¤)âˆ’1/2 Â· U = U .
Furthermore, we compare the results in terms of centralization. Equation (4.9) is
xix j âˆ’1
N
N

h=1
xixh âˆ’1
N
N

l=1
x jxl + 1
N
N

h=1
N

l=1
xlxh = (xi âˆ’Â¯x)(x j âˆ’Â¯x)

100
4
Kernel Computations
for the linear kernel, which is consistent with that of the ordinary PCA approach.
Therefore, the obtained score is the same.
Example 65 We performed kernel PCA on a data set called US Arrests in Python.
We wished to project the ratio of the resident population living in urban areas and the
incidence rates of homicide, violent crime, and assaults on women (number of arrests
per 100,000 people) for all 50 states of the U.S. onto the axes of two variables using
PCA. We performed kernel PCA with a Gaussian kernel (Ïƒ 2 = 0.01, 0.08), kernel
PCA with a linear kernel, and ordinary PCA. We observed that the differences in the
features of the 50 states were not evident in the results of ordinary PCA and kernel
PCA with the linear kernel (Fig.4.3a,b). With the Gaussian kernel (Ïƒ 2 = 0.08), the 50
states were divided into four categories (Fig.4.3c). As far as the data were concerned,
Californiaâ€™s ï¬gures (fewer homicides for a higher urban population) differed from
thoseoftheotherstates.Nevertheless,whenwesetÏƒ = 0.01,thedifferencesbetween
-100
-50
0
50
100
150
-30
-20
-10
0
10
20
(a) Standard PCA
First
Second
1
2
3
4
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
3637
38
39
40
41
42
43
44
45
46
47
48
49
50
5
-350
-300
-250
-200
-150
-100
-50
-80
-70
-60
-50
-40
-30
(b) Kernel PCA (Linear)
First
Second
1
2
3
4
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
5
-0.4
-0.2
0.0
0.2
0.4
-0.4
-0.2
0.0
0.2
0.4
(c) Kernel PCA (Ïƒ2 = 0.08)
First
Second
1
2
3
4
6
7
89
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
4445
46
47
48
49
50
5
-0.2
0.0
0.2
0.4
0.6
-0.2
0.0
0.2
0.4
(d) Kernel PCA (Ïƒ2 = 0.01)
First
Second
1
2
3
4
6
7
8
9 10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48 49
50
5
Fig. 4.3 For the US Arrests data, we ran the ordinary PCA and kernel PCA methods (linear;
Gaussian with Ïƒ 2 = 0.08, 0.01), and we display the scores here. In the ï¬gure, 1 to 50 are the IDs
given to the states, and Californiaâ€™s ID is 5 (written in red). The results of the kernel PCA approach
differ greatly depending on what kernel we choose. Additionally, since kernel PCA is unsupervised,
it is not possible to use CV to select the optimal parameters. The scores of ordinary PCA and PCA
with the linear kernel should be identical. Although the directions of both axes are opposite, which
is common in PCA, we can conclude that they match

4.3 Kernel SVM
101
California and the other 49 states became clear (Fig.4.3d). We used the following
code for the execution of the compared approaches:
# def
k ( x ,
y ) :
#
r e t u r n
np . dot ( x . T ,
y )
sigma2 = 0.01
def k ( x ,
y ) :
return np . exp(âˆ’np . l i n a l g . norm ( x âˆ’y ) âˆ—âˆ—2
/
2
/
sigma2 )
X = pd . read_csv (â€™https://raw.githubusercontent.com/selva86/datasets/master/
USArrests.csvâ€™)
x = X. values [: ,: âˆ’1]
n = x . shape [ 0 ] ;
p = x . shape [ 1 ]
alpha = k e r n e l _ p c a _ t r a i n ( x ,
k )
z = np . zeros ( ( n ,
2) )
for
i
in
range ( n ) :
z [ i ,
: ]
= k e r n e l _ p c a _ t e s t ( x ,
k ,
alpha ,
2 , x [ i ,
: ] )
min1 = np . min ( z [ : ,
0 ] ) ;
min2 = np . min ( z [ : ,
1 ] )
max1 = np . max( z [ : ,
0 ] ) ; max2 = np . max( z [ : ,
1 ] )
p l t . xlim ( min1 ,
max1 )
p l t . ylim ( min2 ,
max2 )
p l t . x l a b e l ("First")
p l t . y l a b e l ("Second")
p l t . t i t l e ("Kernelâ£PCAâ£(Gaussâ£0.01)")
for
i
in
range ( n ) :
i f
i
!=
4:
p l t . t e x t ( x = z [ i ,
0] , y = z [ i ,
1] ,
s = i )
p l t . t e x t ( z [4 ,
0] ,
z [4 ,
1] ,
5 , c = "r")
4.3
Kernel SVM
Consider binary discrimination using support vector machines (SVMs). Given X âˆˆ
RNÃ—p and y âˆˆ{1, âˆ’1}N, we ï¬nd the boundary Y = XÎ² + Î²0 with the Î² âˆˆRp and
Î²0 âˆˆR that maximize the margin. Let Î³ â‰¥0. We wish to maximize the margin M
by ranging (Î²0, Î²) âˆˆR Ã— Rp and Ïµi â‰¥0, i = 1, . . . , N to satisfy
N

i=1
Ïµi â‰¤Î³
and
yi(Î²0 + xiÎ²) â‰¥M(1 âˆ’Ïµi) , i = 1, . . . , N.
We often formulate this as the problem of minimizing
1
2âˆ¥Î²âˆ¥2 + C
N

i=1
Ïµi
(4.10)

102
4
Kernel Computations
under yi(xiÎ² + Î²0) â‰¥1 âˆ’Ïµi, Ïµi â‰¥0 for i = 1, . . . , N by using a constant C > 0
(the prime problem). We further transform it into the problem of ï¬nding 0 â‰¤Î±i â‰¤C,
i = 1, 2, . . . , N that maximizes
N

i=1
Î±i âˆ’1
2
N

i=1
N

j=1
Î±iÎ± jyiy jxixâŠ¤
j
(4.11)
under N
i=1 Î±iyi = 0, where xi is the ith row vector of X (the dual problem)1. The
constant C > 0 is a parameter that represents the ï¬‚exibility of the boundary surface.
Thehigherthevalueis,themoresamplesareusedtodeterminetheboundary(samples
with Î±i Ì¸= 0, i.e., support vectors). Although we sacriï¬ce the ï¬t of the data, we reduce
the boundary variation caused by sample data to prevent overtraining. Then, from
the support vectors, we can calculate the slope of the boundary with the following
formula:
Î² =
N

i=1
Î±iyixâŠ¤
i âˆˆRp .
Then, suppose that we replace the boundary surface with a curved surface by replac-
ing the inner product xixâŠ¤
j with a general nonlinear kernel k(xi, x j). Then, we can
obtain complicated boundary surfaces rather than planes. However, the theoretical
basis for replacing the product with a kernel is not clear.
Therefore, in the following, we derive the same results by formulating the opti-
mization using k : E Ã— E â†’R. As in to the previous application of the representa-
tion theorem, we ï¬nd the f âˆˆH that minimizes
1
2âˆ¥f âˆ¥2
H + C
N

i=1
Ïµi âˆ’
N

i=1
Î±i[yi{ f (xi) + Î²0} âˆ’(1 âˆ’Ïµi)] âˆ’
N

i=1
Î¼iÏµi .
(4.12)
Noting that f (xi) = f1 (xi), i = 1, . . . , N and âˆ¥f âˆ¥H â‰¥âˆ¥f1âˆ¥H, we ï¬nd Î³1, . . . , Î³N
such that f (Â·) = N
i=1 Î³ik(xi, Â·).
The Karush-Kuhn-Tucker (KKT) condition results in the following nine equa-
tions:
yi{ f (xi) + Î²0} âˆ’(1 âˆ’Ïµi) â‰¥0
Ïµi â‰¥0
Î±i[yi{ f (xi) + Î²0} âˆ’(1 âˆ’Ïµi)] = 0
Î¼iÏµi = 0
1 We see this derivation in several references, such as Joe Suzuki, â€œStatistical Learning with Math
and Pythonâ€ (Springer); C. M. Bishop, â€œPattern Recognition and Machine Learningâ€ (Springer);
Hastie, Tibshirani, and Fridman, â€œElements of Statistical Learningâ€ (Springer); and other primary
machine learning books.

4.3 Kernel SVM
103

j
Î³ jk(xi, x j) âˆ’

j
Î± jy jk(xi, x j) = 0
(4.13)

i
Î±iyi = 0
C âˆ’Î±i âˆ’Î¼i = 0
(4.14)
Î¼i â‰¥0 , 0 â‰¤Î±i â‰¤C.
Next, suppose that f0, f1, . . . , fm : Rp â†’R are convex and differentiable at Î² =
Î²âˆ—. In general, Eqs.(4.15,4.16, and 4.17) are called the KKT condition2.
Proposition 43 (KKT Condition) Suppose that f1(Î²) â‰¤0, . . . , fm(Î²) â‰¤0. Then,
Î² = Î²âˆ—âˆˆRp minimizes f0(Î²) if and only if
f1(Î²âˆ—), . . . , fm(Î²âˆ—) â‰¤0
(4.15)
and Î±1, . . . , Î±m â‰¥0 exist such that
Î±1 f1(Î²âˆ—) = Â· Â· Â· = Î±m fm(Î²âˆ—) = 0
(4.16)
âˆ‡f0(Î²âˆ—) +
m

i=1
Î±iâˆ‡fi(Î²âˆ—) = 0 .
(4.17)
Utilizing these nine equations, from (4.13)(4.14), we can express (4.12) as
N

i=1
Î±i âˆ’1
2
N

i=1
N

j=1
Î±iÎ± jyiy jk(xi.x j) .
(4.18)
Comparing (4.11) and (4.18), we observe that the dual problem replaces xâŠ¤
i x j with
k(xi, x j) for the formulation without any kernel.
In fact, if we set f (Â·) = âŸ¨Î², Â·âŸ©H, Î² âˆˆRp, k(x, y) = xâŠ¤y (x, y âˆˆRp), then we
obtain the dual problem for a linear kernel (4.11).
Example 66 By using the following function svm_2, we can compare how the
bounds differ between a linear kernel (the standard inner product) and a nonlin-
ear kernel (a polynomial kernel), as shown in Fig.4.4. cvxopt is a Python module
for solving quadratic programming problems. The function cvxopt calculates Î±.
def
K_linear ( x , y ) :
return x .T@y
def
K_poly ( x , y ) :
return
(1+ x .T@y) âˆ—âˆ—2
2 For the proof, see Chap.9 of Joe Suzuki â€œStatistical Learning with R/Pythonâ€ (Springer).

104
4
Kernel Computations
def svm_2 (X, y , C,K) :
eps =0.0001
n=X. shape [ 0 ]
P=np . zeros ( ( n , n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
P [ i , j ]=K(X[ i , : ] , X[ j , : ] ) âˆ—y [ i ]âˆ—y [ j ]
#
S p e c i f y
i t
via
the
matrix
f u n c t i o n
in
the
package
matrix
P= matrix ( P+np . eye ( n ) âˆ—eps )
A= matrix (âˆ’y . T . astype ( np . f l o a t ) )
b= matrix ( np . a r r a y ( [ 0 ] ) . astype ( np . f l o a t ) )
h= matrix ( np . a r r a y ( [C]âˆ—n +[0]âˆ—n ) . reshape ( âˆ’1 ,1) . astype ( np . f l o a t ) )
G= matrix ( np . c o n c a t e n a t e ( [ np . diag ( np . ones ( n ) ) , np . diag (âˆ’np . ones ( n ) ) ] ) )
q= matrix ( np . a r r a y ([ âˆ’1]âˆ—n ) . astype ( np . f l o a t ) )
r e s =cvxopt . s o l v e r s . qp (P , q , A=A,
b=b ,G=G,
h=h )
alpha =np . a r r a y ( r e s [â€™xâ€™ ] )
#x
i s
the
alpha
in
the
t e x t
beta =(( alpha âˆ—y ) .T@X) . reshape ( 2 , 1 )
index = ( eps < alpha [ : ,
0 ] ) & ( alpha [ : ,
0] < C âˆ’eps )
beta_0 =np . mean ( y [ index ]âˆ’X[ index , : ] @beta )
return {â€™alphaâ€™: alpha , â€™betaâ€™: beta , â€™beta_0â€™: beta_0 }
def
p l o t _ k e r n e l (K, l i n e ) :
#
S p e c i f y
the
l i n e s
via
the
l i n e
argument
r e s =svm_2 (X, y , 1 ,K)
alpha = r e s [â€™alphaâ€™ ] [ : , 0 ]
beta_0 = r e s [â€™beta_0â€™]
def
f ( u , v ) :
S= beta_0
for
i
in
range (X. shape [ 0 ] ) :
S=S+ alpha [ i ]âˆ—y [ i ]âˆ—K(X[ i , : ] , [ u , v ] )
return S [ 0 ]
# ww i s
the
h e i g h t
of
f ( x , y ) . We can draw
the
contour .
uu=np . arange ( âˆ’2 ,2 ,0.1) ;
vv=np . arange ( âˆ’2 ,2 ,0.1) ; ww=[]
for v in vv :
w=[]
for u in uu :
w. append ( f ( u , v ) )
ww. append (w)
p l t . contour ( uu , vv ,ww, l e v e l s =0 , l i n e s t y l e s = l i n e )
Fig. 4.4 After generating
samples, we draw linear
(planar) and nonlinear
(curved) boundaries with
support vector machines
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
X[,1]
X[,2]
0
0

4.4 Spline Curves
105
a =3; b=âˆ’1
n=200
X=randn ( n , 2 )
y=np . sign ( aâˆ—X[ : , 0 ] + bâˆ—X[: ,1]âˆ—âˆ—2+0.3âˆ—randn ( n ) )
y=y . reshape ( âˆ’1 ,1)
for
i
in
range ( n ) :
i f
y [ i ]==1:
p l t . s c a t t e r (X[ i , 0 ] ,X[ i , 1 ] , c="red")
e l s e :
p l t . s c a t t e r (X[ i , 0 ] ,X[ i , 1 ] , c="blue")
p l o t _ k e r n e l ( K_poly , l i n e ="dashed")
p l o t _ k e r n e l ( K_linear , l i n e ="solid")
pcost
dcost
gap
pres
dres
0: -6.6927e+01 -4.6679e+02
2e+03
3e+00
1e-14
1: -4.2949e+01 -2.9229e+02
5e+02
4e-01
8e-15
2: -2.8717e+01 -1.0653e+02
1e+02
1e-01
6e-15
3: -2.5767e+01 -4.7367e+01
3e+01
2e-02
4e-15
4: -2.6165e+01 -3.1836e+01
8e+00
5e-03
4e-15
5: -2.6940e+01 -2.8267e+01
2e+00
7e-04
3e-15
6: -2.7243e+01 -2.7483e+01
3e-01
1e-04
3e-15
7: -2.7325e+01 -2.7330e+01
6e-03
1e-06
3e-15
8: -2.7327e+01 -2.7328e+01
9e-05
2e-08
3e-15
9: -2.7328e+01 -2.7328e+01
9e-07
2e-10
3e-15
Optimal solution found.
pcost
dcost
gap
pres
dres
0: -8.1804e+01 -4.7816e+02
2e+03
3e+00
3e-15
1: -5.3586e+01 -3.0647e+02
4e+02
4e-01
3e-15
2: -4.1406e+01 -8.6880e+01
6e+01
3e-02
5e-15
3: -4.7360e+01 -5.9604e+01
1e+01
6e-03
2e-15
4: -4.9819e+01 -5.5157e+01
6e+00
2e-03
2e-15
5: -5.0999e+01 -5.3276e+01
2e+00
7e-04
2e-15
6: -5.1869e+01 -5.2122e+01
3e-01
9e-06
3e-15
7: -5.1966e+01 -5.2010e+01
4e-02
1e-06
2e-15
8: -5.1986e+01 -5.1988e+01
3e-03
5e-15
3e-15
9: -5.1987e+01 -5.1987e+01
8e-05
1e-15
2e-15
10: -5.1987e+01 -5.1987e+01
2e-06
3e-15
3e-15
Optimal solution found.
4.4
Spline Curves
Let J â‰¥1. We say that the function

106
4
Kernel Computations
g(x) = Î²1 + Î²2x + Î²3x2 + Î²4x3 +
J

j=1
Î² j+4(x âˆ’Î¾ j)3
+
(4.19)
=
â§
âªâ¨
âªâ©
g0(x) = Î²1 + Î²2x + Î²3x2 + Î²4x3,
x < Î¾1
g j(x) = g jâˆ’1(x) + Î² j+4(x âˆ’Î¾ j)3,
Î¾ j â‰¤x < Î¾ j+1
gJ (x) = Î²1 + Î²2x + Î²3x2 + Î²4x3 + J
j=1 Î² j+4(x âˆ’Î¾ j)3, x â‰¥Î¾J
with the constants Î²1, . . . , Î²J+4 âˆˆR is a spline function of order three with knots
0 < Î¾1 < Â· Â· Â· < Î¾J < 1. We may deï¬ne the spline function of order three by the
function g, which is a piecewise polynomial for each of the J + 1 intervals whose
g, gâ€², gâ€²â€² are continuous at the J knots. The spline expressed by (4.19) consists of a
linear space, and
1, x, x2, x3, (x âˆ’Î¾1)3
+, . . . , (x âˆ’Î¾J)3
+
(4.20)
can be its basis.
In particular, we consider the natural spline of order three in which we pose more
conditions such as
gâ€²â€²(Î¾1) = gâ€²â€²â€²(Î¾1) = 0
(4.21)
and
gâ€²â€²(Î¾J) = gâ€²â€²â€²(Î¾J) = 0 .
(4.22)
The resulting curve is not of order three in x â‰¤Î¾1, Î¾J â‰¤x, and we approximate it
by a line. The linear space of natural splines possesses J dimensions. In fact, from
(4.21), we have
gâ€²â€²â€²(Î¾1) = 6Î²4 = 0
gâ€²â€²(Î¾1) = 2Î²3 + 6Î²4Î¾1 = 0 â‡â‡’Î²3 = Î²4 = 0 .
Additionally, from (4.22), we have
gâ€²â€²â€²(Î¾J) = 6Î²4 + 6
J

j=1
Î² j+4 = 0
gâ€²â€²(Î¾J) = 2Î²3 + 6Î²4Î¾J + 6
J

j=1
Î² j+4(Î¾J âˆ’Î¾ j) = 0
â‡â‡’
J

j=1
Î² j+4 =
J

j=1
Î² j+4Î¾ j = 0 .
Thus, the Î²J+3, Î²J+4 values are determined by the other Î² j; j = 1, 2, 5, . . . , J + 2.
In the following, we consider the problem of ï¬nding the f : [0, 1] â†’R that
minimizes

4.4 Spline Curves
107
N

i=1
{yi âˆ’f (xi)}2 + Î»
 1
0
{ f â€²â€²(x)}2dx
(4.23)
given samples (x1, y1), . . . , (xN, yN) âˆˆR Ã— R. The second term is zero if the func-
tion is a straight line, but it becomes a signiï¬cant value if the function deviates from
a straight line. In other words, this term represents the complexity of the function f .
The constant Î» â‰¥0 balances the two terms, and if it is large, the curve is smooth; if
the constant is small, the curve follows the sample closely. Note that, in general, the
bounds Î¾1, . . . , Î¾J and x1, . . . , xN are deï¬ned separately.
In this case, it is known that the f that minimizes (4.23) is a natural spline of order
three such that f (xi) = yi, i = 1, . . . , N at the N boundaries Î¾1 = x1, . . . , Î¾N =
xN 3. However, f is once differentiable everywhere and twice differentiable almost
everywherewith
 1
0 { f â€²â€²(x)}2dx < âˆ,whichimpliesthat f isanelementof W2[0, 1].
A similar proposition holds for the general Wq[0, 1].
Example 67 In the case of a natural spline with q = 2, if we choose the basis
g1, . . . , gN appropriately, such as g(Â·) = N
j=1 Î² jg j(Â·), and G = (
 1
0 g(q)
i
(x)g(q)
j
(x)dx) âˆˆRNÃ—N, y = [y1, . . . , yN], then we obtain the optimal
[Î²1, . . . , Î²N]âŠ¤= (XâŠ¤X + Î»G)âˆ’1XâŠ¤y .
Figure4.5 shows the graphs obtained for Î» = 1, 30, 80.
# d ,
h
d e f i n e
the
f u n c t i o n
t h a t
o b t a i n s
the
b a s i s
def d ( j ,
x ,
knots ) :
K = len ( knots )
return
( np . maximum ( ( xâˆ’knots [ j ] ) âˆ—âˆ—3 ,
0)
âˆ’np . maximum ( ( xâˆ’knots [Kâˆ’1]) âˆ—âˆ—3 ,0) ) / ( knots [Kâˆ’1]âˆ’knots [ j ] )
def h ( j ,
x ,
knots ) :
K = len ( knots )
i f
j == 0:
return 1
e l i f
j == 1:
return x
e l s e :
return d ( j âˆ’1, x ,
knots )âˆ’d (Kâˆ’2, x ,
knots )
# G g i v e s
values
i n t e g r a t i n g
the
f u n c t i o n s
t h a t
are
d i f f e r e n t i a t e d
twice
def G( x ) :
# The x
values
are
ordered
in
ascending
n = len ( x )
g = np . zeros ( ( n ,
n ) )
for
i
in
range (2 , nâˆ’1) :
for
j
in
range ( i ,
n ) :
g [ i ,
j ] = 12âˆ—( x [ nâˆ’1]âˆ’x [ n âˆ’2]) âˆ—( x [ nâˆ’2]âˆ’x [ j âˆ’2]) \
âˆ—( x [ nâˆ’2]âˆ’x [ i âˆ’2]) / ( x [ nâˆ’1]âˆ’x [ i âˆ’2]) / \
( x [ nâˆ’1]âˆ’x [ j âˆ’2]) +(12âˆ—x [ nâˆ’2]+6âˆ—x [ j âˆ’2]âˆ’18âˆ—x [ i âˆ’2]) \
âˆ—( x [ nâˆ’2]âˆ’x [ j âˆ’2]) âˆ—âˆ—2/( x [ nâˆ’1]âˆ’x [ i âˆ’2]) / ( x [ nâˆ’1]âˆ’x [ j âˆ’2])
g [ j ,
i ] = g [ i ,
j ]
return g
3 See Chap.7 of this series (â€œStatistical Learning with R/Pythonâ€ (Springer)) for the proof.

108
4
Kernel Computations
Fig. 4.5 Instead of giving
knots or the number of knots
in the smoothing spline, we
specify the Î» value, which
indicates smoothness.
Comparing Î» = 1, 30, 80, as
we increase the value of Î»,
the spline does not follow the
observed data, but it
becomes smoother
-5
0
5
-5
0
5
g(x)
-5
0
5
x
g(x)
Î» = 1
Î» = 30
Î» = 80
Smoothing Spline (n = 100)
# MAIN
n = 100
x = np . random . uniform ( âˆ’5 , 5 , n )
y = x + np . s i n ( x ) âˆ—2 + np . random . randn ( n )
# Data
Generation
index = np . a r g s o r t ( x )
x = x [ index ] ;
y = y [ index ]
X = np . zeros ( ( n ,
n ) )
X[ : ,
0] = 1
for
j
in
range (1 , n ) :
for
i
in
range ( n ) :
X[ i ,
j ] = h ( j ,
x [ i ] ,
x )
# Generation
of
Matrix X
GG = G( x )
# Generation
of
Matrix G
lam_set = [1 ,
30 ,
80]
c o l _ s e t = ["red" , "blue" , "green"]
p l t . f i g u r e ( )
p l t . ylim ( âˆ’8 ,
8)
p l t . x l a b e l ("x")
p l t . y l a b e l ("g(x)")
for
i
in
range ( 3 ) :
lam = lam_set [ i ]
gamma = np . dot ( np . dot ( np . l i n a l g . inv ( np . dot (X. T , X) +lamâˆ—GG) ,X. T) , y )
def g ( u ) :
S = gamma [ 0 ]
for
j
in
range (1 , n ) :
S = S + gamma[ j ]âˆ—h ( j ,
u ,
x )
return S
u_seq = np . arange ( âˆ’8 , 8 ,
0 . 0 2 )
v_seq = [ ]
for u in
u_seq :
v_seq . append ( g ( u ) )
p l t . p l o t ( u_seq ,
v_seq ,
c = c o l _ s e t [ i ] ,
l a b e l = "$\lambdaâ£=â£%d$"%lam_set [
i ] )
p l t . legend ( )
p l t . s c a t t e r ( x ,
y ,
f a c e c o l o r s =â€™noneâ€™ ,
e d g e c o l o r s = "k" ,
marker = "o")
p l t . t i t l e ("smoothâ£splineâ£(n=100)")
Text ( 0 . 5 ,
1. 0 , â€™smoothâ£splineâ£(n=100)â€™)
Generalizing (4.23), we consider minimizing

4.4 Spline Curves
109
N

i=1
{yi âˆ’f (xi)}2 + Î»
 1
0
{ f (q)(x)}2dx .
(4.24)
First, each f = f0 + f1 ( f0 âˆˆH0 and f1 âˆˆH1 in Wq[0, 1]) can be written with the
appropriate linear operators P0 âˆˆB(H, H0) and P1 âˆˆB(H, H1) as f0 = P0 f âˆˆH0,
f1 = P1 f âˆˆH1. Since âŸ¨f0, f1âŸ©H = 0, f0, f1 minimize âˆ¥f âˆ’f0âˆ¥H and âˆ¥f âˆ’f1âˆ¥H,
respectively. Furthermore, P0, P1 are self-adjoint. In fact, from Proposition 19, for
each i = 0, 1, we have
âŸ¨Pi f, gâŸ©H = âŸ¨fi, g0 + g1âŸ©H = âŸ¨fi, giâŸ©H = âŸ¨f0 + f1, giâŸ©H = âŸ¨f, PigâŸ©H
for f0, g0 âˆˆH0, f1, g1 âˆˆH1, f = f0 + f1, g = g0 + g1. Moreover, we have Pi f âˆˆ
Hi and P2
i f = Pi f . Thus, we can write the norm of the second term in (4.24) as
 1
0
| f (q)(x)|2dx = âˆ¥P1 f âˆ¥2
H1 = âŸ¨P1 f, P1 f âŸ©H1 = âŸ¨f, P2
1 f âŸ©H = âŸ¨f, P1 f âŸ©H
and can express (4.24) as
N

i=1
{yi âˆ’f (xi)}2 + Î»âŸ¨f, P1 f âŸ©H
(4.25)
for
f âˆˆWq[0, 1].
Let
f = g + h âˆˆH,
g âˆˆM := span{Ï†0(Â·), . . . , Ï†qâˆ’1(Â·),
k(x1, Â·), . . . k(xN, Â·)}, and h âˆˆMâŠ¥. Then, for i = 1, . . . , N, we have
f (xi) = âŸ¨g + h, k(xi, Â·)âŸ©H = g(xi)
âˆ¥P1 f âˆ¥H1 â‰¥âˆ¥P1gâˆ¥H1
(the representation theorem). Thus, we may restrict the range of f to M for searching
the optimum to ï¬nd Î±1, . . . , Î±N, Î²1, . . . , Î²q in
g(Â·) =
qâˆ’1

i=0
Î²iÏ†i(Â·) +
N

i=1
Î±ik(xi, Â·) .
(4.26)
In natural spline functions, we regard the differential of order q at x = xN as zero,
which means that
g(q)(xN) = . . . = g(2qâˆ’1)(xN) = 0,
(4.27)
and the dimensionality of span{k1(xi, Â·)|i = 1, . . . , N} is N âˆ’q. For spline functions
of order three (q = 2), (4.27) corresponds to (4.22). The basis {1, x} w.r.t. the lines
in x â‰¤x1 corresponds to {Ï†0(x), . . . , Ï†qâˆ’1}. Thus, we ï¬nd the optimal solution in
the subspace of Wq[0, 1] for N.

110
4
Kernel Computations
Proposition 44 Let r âˆˆWq[0, 1] be a natural spline with knots x1, . . . , xN and a
maximum order of 2q âˆ’1, and suppose that g âˆˆWq[0, 1] satisï¬es g(xi) = r(xi) for
i = 1, 2, . . . , N. Then, we have
 1
0
{r(q)(x)}2dx â‰¤
 1
0
{g(q)(x)}2dx.
Moreover, the maximum order of s is q âˆ’1 such that s(xi) = 0 for s := g âˆ’r and
i = 1, 2, . . . , N, and if N â‰¥q, then the function s is zero.
Proof: See the appendix at the end of this chapter.
Since the natural splines of the highest order 2q âˆ’1 possess N dimensions, an
r âˆˆWq[0, 1]existsthatsharesthevaluesr(x1) = g(x1), . . . ,r(xN) = g(xN)atthe N
boundaries x1, . . . , xN. Among them, since the second term in (4.25) is the optimum,
the natural spline of the highest order 2q âˆ’1 is optimal.
To summarize the above, the problem of ï¬nding the f that minimizes (4.25) in
Wq[0, 1] reduces to ï¬nding the solution over the range of (4.26),(4.27). In other
words, we can think of the problem in a subspace with N dimensions.
Moreover, the basis consists of N elements regardless of whether q â‰¥1, and if
we set g(Â·) = N
j=1 Î² jg j(Â·), the problem is to ï¬nd the Î²1, . . . , Î²N that minimize
N

i=1
{yi âˆ’
n

i=1
N

j=1
Î² jg j(xi)}2 + Î»
N

i=1
N

j=1
Î²iÎ² j
 1
0
g(q)
i
(x)g(q)
j (x)dx.
Let X = (g j(xi)) âˆˆRNÃ—N, G = (
 1
0 g(q)
i
(x)g(q)
j (x)dx) âˆˆRNÃ—N, and y = [y1, . . . ,
yN]âŠ¤. The optimal solution Î² = [Î²1, . . . , Î²N]âŠ¤is given by
Î² = (XâŠ¤X + Î»G)âˆ’1XâŠ¤y .
4.5
Random Fourier Features
In the following, we examine computational cost reduction methods.
In particular, in this section, we learn about random Fourier features, which we
can apply to the case where the kernel k(x, y) (x, y âˆˆE) is a function of x âˆ’y.
Proposition 45 (Rahimi and Recht [23]) Suppose that k : E Ã— E âˆ‹(x, y) â†’
k(x, y) âˆˆR is a function of x âˆ’y. Then, we have
k(x, y) = 2EÏ‰,b cos(wâŠ¤x + b) cos(wâŠ¤y + b) ,
(4.28)
where the expectation EÏ‰,b is calculated over Ï‰ âˆ¼Î¼ (the probability of k in Propo-
sition 5) and b âˆˆ[0, 2Ï€) (the uniform distribution).

4.5 Random Fourier Features
111
Proof: The claim is due to Bochnerâ€™s theorem (Proposition 5). See the appendix at
the end of this chapter for details.
Based on Proposition 45, we generate
âˆš
2 cos(Ï‰âŠ¤x + b) m â‰¥1 times, i.e.,
(wi, bi), i = 1, . . . , m, and construct the function
zi(x) =
âˆš
2 cos(Ï‰âŠ¤
i x + bi) i = 1, . . . , m.
From the law of large numbers, the constructed
Ë†k(x, y) := 1
m
m

i=1
zi(x)zi(y)
approaches k(x, y). Utilizing this fact, when m is small compared to N, the method
to reduce the complexity of kernel computation is called random Fourier features
(RFF).
We claim that the RFF possesses the following property:
P(|k(x, y) âˆ’Ë†k(x, y)| â‰¥Ïµ) â‰¤2 exp(âˆ’mÏµ2/8) .
(4.29)
Proposition 46 (Hoeffdingâ€™s Inequality) For independent random variables Xi, i =
1, . . . , n, each of which takes values in [ai, bi], and an arbitrary Ïµ > 0, we have
P(|X âˆ’E[X]| â‰¥Ïµ) â‰¤2 exp(âˆ’
2n2Ïµ2
n
i=1(bi âˆ’ai)2 ) ,
(4.30)
where X denotes the sample mean (X1 + . . . + Xn)/n.
Proof: We use the Chernoff bound and Hoeffdingâ€™s lemma, which are shown below.
Lemma 5 (Chernoff Bound) For a random variable X and an arbitrary Ïµ > 0, we
have
P(X â‰¥Ïµ) â‰¤inf
s>0 eâˆ’sÏµE[esX] .
(4.31)
To prove this lemma, we use the following lemma.
Lemma 6 (Markovâ€™s Inequality) For a random variable X that takes nonnegative
values, we have
P(X â‰¥Ïµ) â‰¤E[X]
Ïµ
.
Lemma 6 is due to
E[X] = E[X Â· I (X â‰¥Ïµ)] + E[X Â· I (X < Ïµ)] â‰¥E[X Â· I (X â‰¥Ïµ)] â‰¥ÏµP(X â‰¥Ïµ) .
Lemma 5 follows from lemma 6 and the fact that

112
4
Kernel Computations
P(X â‰¥Ïµ) = P(sX â‰¥sÏµ) = P(exp(sX) â‰¥exp(sÏµ)) â‰¤eâˆ’sÏµE[esX]
for s > 0. To prove Proposition 46, we use the following lemma:
Lemma 7 (Hoeffding) Suppose that a random variable X satisï¬es E[X] = 0 for
a â‰¤X â‰¤b. Then, for an arbitrary Ïµ > 0, we have
E

eÏµX
â‰¤eÏµ2(bâˆ’a)2/8.
(4.32)
Proof: See the appendix at the end of this chapter.
Returning to the proof of Proposition 46, let Sn := n
i=1 Xi, and apply Lemma 5
to obtain
P(Sn âˆ’E[Sn] â‰¥Ïµ) â‰¤min
s>0 eâˆ’sÏµE[exp{s(Sn âˆ’E[Sn])}] .
In particular, since X1, . . . , Xn are independent, we have
eâˆ’sÏµE[exp{s(Sn âˆ’E[Sn])}] = eâˆ’sÏµ
n
i=1
E[es(Xiâˆ’E[Xi])] .
Moreover, by applying Lemma 7, we obtain
P(Sn âˆ’E[Sn] â‰¥Ïµ) â‰¤min
s>0 exp{âˆ’sÏµ + s2
8
n

i=1
(bi âˆ’ai)2} ,
in which the minimum value is attained when s := 4Ïµ/ n
i=1(bi âˆ’ai)2, and we have
P(Sn âˆ’E[Sn] â‰¥Ïµ) â‰¤exp{âˆ’2Ïµ2/
n

i=1
(bi âˆ’ai)2} .
Furthermore, if we replace X1, . . . , Xn with âˆ’X1, . . . , âˆ’Xn, we obtain
P(Sn âˆ’E[Sn] â‰¤âˆ’Ïµ) â‰¤exp{âˆ’2Ïµ2/
n

i=1
(bi âˆ’ai)2} .
Hence, we have
P(|Sn âˆ’E[Sn]| â‰¥Ïµ) = 1 âˆ’P(|Sn âˆ’E[Sn]| â‰¤Ïµ)
â‰¤P(Sn âˆ’E[Sn] â‰¥Ïµ) + P(Sn âˆ’E[Sn] â‰¤âˆ’Ïµ) â‰¤2 exp{âˆ’2Ïµ2/
n

i=1
(bi âˆ’ai)2} .
If we substitute Â¯X = Sn/n, we obtain Proposition 46.
â–¡

4.5 Random Fourier Features
113
m=20
m=100
m=400
-0.4
-0.2
0.0
0.2
0.4
0.6
Fig. 4.6 In the RFF approximation, we generated Ë†k(x, y) 1000 times by changing m. We observe
that they all have zero centers, and the larger m is, the smaller the estimation error is
Since E[Ë†k(x, y)] = k(x, y) and âˆ’2 â‰¤zi(x)zi(y) â‰¤2, using Proposition 46, we
obtain (4.29)4.
Example 68 From Example 19, since the probability of a Gaussian kernel has a
mean of 0 and a covariance matrix Ïƒ âˆ’2I âˆˆRdÃ—d, we generate the d-dimensional
random numbers and uniform random numbers independently and construct the m
functions zi(x) =
âˆš
2 cos(Ï‰âŠ¤
i x + bi), i = 1, . . . , m. We draw a boxplot of Ë†k(x, y) âˆ’
k(x, y) by generating (x, y) 1000 times with d = 1 and m = 20, 100, 400 in Fig.4.6.
We observe that Ë†k(x, y) âˆ’k(x, y) has a mean of 0 (Ë†k(x, y) is an unbiased estimator),
and the larger m is, the smaller the variance is. The program is written as follows:
sigma =10
sigma2=sigma âˆ—âˆ—2
def k ( x , y ) :
return np . exp ( âˆ’(xâˆ’y ) âˆ—âˆ—2/(2âˆ—sigma2 ) )
def z ( x ) :
return np . s q r t ( 2 /m) âˆ—np . cos (wâˆ—x+b )
def
zz ( x , y ) :
return np . sum( z ( x ) âˆ—z ( y ) )
u=np . zeros ((1000 ,3) )
m_seq =[20 ,100 ,400]
for
i
in
range (1000) :
x=randn ( 1 )
y=randn ( 1 )
for
j
in
range ( 3 ) :
m=m_seq [ j ]
w=randn (m) / sigma
b=np . random . rand (m) âˆ—2âˆ—np . pi
u [ i , j ]= zz ( x , y )âˆ’k ( x , y )
4 The original paper by Rahimi and Recht (2007) and subsequent work proved more rigorous upper
and lower bounds than these [2].

114
4
Kernel Computations
f i g =
p l t . f i g u r e ( )
ax = f i g . add_subplot (1 ,
1 ,
1)
ax . boxplot ( [ u [ : , 0 ] ,
u [ : , 1 ] ,
u [ : , 2 ] ] ,
l a b e l s =[â€™20â€™ , â€™100â€™ , â€™400â€™ ] )
ax . s e t _ x l a b e l (â€™mâ€™)
ax . s e t _ y l i m ( âˆ’0.5 ,
0 . 6 )
p l t . show ( )
The solution Î± = [Î±1, . . . , Î±N] with f (Â·) = N
i=1 Î±ik(xi, Â·) for kernel ridge
regression with the Gram matrix K is given by (4.6) (Sect.4.1). If we obtain the
Ë†f that approximates f and approximates the Gram matrix K via RFF as Ë†K = Z ZâŠ¤,
then we obtain Ë†f (Â·) = N
i=1 Ë†Î±i Ë†k(xi, Â·) by using Ë†Î± âˆˆRN for ( Ë†K + Î»IN)Ë†Î± = y for
Z = (Z j(xi)) âˆˆRNÃ—m and the unit IN âˆˆRNÃ—N.
Using Woodburyâ€™s formula, for U âˆˆRrÃ—s, V âˆˆRsÃ—r,r, s â‰¥1,
U(Is + VU) = (Ir + UV )U .
And we have
ZâŠ¤(Z ZâŠ¤+ Î»IN)âˆ’1 = (ZâŠ¤Z + Î»Im)âˆ’1ZâŠ¤.
Let x âˆˆE be a value other than the x1, . . . , xN used for estimation, and let z(x) :=
[z1(x), . . . , zm(x)] (row vector). Then, for
Ë†Î² := (ZâŠ¤Z + Î»Im)âˆ’1ZâŠ¤y ,
(4.33)
we have
Ë†f (x) =
N

i=1
Î±i Ë†k(x, xi) = z(x)
N

i=1
zâŠ¤(xi)Ë†Î±i = z(x)ZâŠ¤Ë†Î± = z(x)ZâŠ¤( Ë†K + Î»IN)âˆ’1y
= z(x)(ZâŠ¤Z + Î»Im)âˆ’1ZâŠ¤y = z(x) Ë†Î² .
Then, for the new x âˆˆE, we can ï¬nd its value from Ë†f (x) = z(x) Ë†Î². The computa-
tional complexity of (4.33) is O(m2N) for the multiplication of ZâŠ¤Z, O(m3) for
ï¬nding the inverse of ZâŠ¤Z + Î»Im âˆˆRmÃ—m, O(Nm) for the multiplication of ZâŠ¤y,
and O(m2) for multiplying (ZâŠ¤Z + Î»Im)âˆ’1 and ZâŠ¤y. Thus, overall, the process
requires only O(N 2m) complexity at most. On the other hand, the process takes
O(N 3) time when using the kernel without approximation. If m = N/10, the com-
putational time becomes 1/100. Obtaining Ë†f (x) from a new x âˆˆE also takes only
O(m) time.
Example 69 We applied RFF to kernel ridge regression. For N = 200 data, we used
m = 20 for the approximation. We plotted the curve for Î» = 10âˆ’6, 10âˆ’4 (Fig.4.7).
The program is as follows:

4.5 Random Fourier Features
115
-1.5 -1.0 -0.5 0.0 0.5 1.0 1.5
0
2
4
6
8
10
Î» = 10âˆ’6, m = 20, N = 200
x
y
W/O. Approx.
W. Approx.
-1.5 -1.0 -0.5
0.0
0.5
1.0
-2
0
2
4
6
8
Î» = 10âˆ’4, m = 20, N = 200
x
y
W/O. Approx.
W. Approx.
Fig. 4.7 We applied RFF to kernel ridge regression. On the left and right are Î» = 10âˆ’6 and
Î» = 10âˆ’4, respectively
sigma =10
sigma2=sigma âˆ—âˆ—2
# Function
z
m=20
w=randn (m) / sigma
b=np . random . rand (m) âˆ—2âˆ—np . pi
def z ( u ,m) :
return np . s q r t ( 2 /m) âˆ—np . cos (wâˆ—u+b )
# Gaussian
Kernel
def k ( x , y ) :
return np . exp ( âˆ’(xâˆ’y ) âˆ—âˆ—2/(2âˆ—sigma2 ) )
# Data
Generation
n=200
x=randn ( n ) /2
y=1+5âˆ—np . s i n ( x / 1 0 ) +5âˆ—xâˆ—âˆ—2+ randn ( n )
x_min=np . min ( x ) ; x_max=np . max( x ) ; y_min=np . min ( y ) ; y_max=np . max( y )
lam =0.001
#lam =0.9
# Low Rank Approximated
Function
def
a l p h a _ r f f ( x , y ,m) :
n=len ( x )
Z=np . zeros ( ( n ,m) )
for
i
in
range ( n ) :
Z[ i , : ] = z ( x [ i ] ,m)
beta =np . dot ( np . l i n a l g . inv ( np . dot (Z . T , Z) +lamâˆ—np . eye (m) ) , np . dot (Z . T , y ) )
return ( beta )
# Usual
Function
def
alpha ( k , x , y ) :
n=len ( x )
K=np . zeros ( ( n , n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K[ i , j ]= k ( x [ i ] , x [ j ] )
alpha =np . dot ( np . l i n a l g . inv (K+lamâˆ—np . eye ( n ) ) , y )
return
alpha
# Numerical
Comparison
a l p h a _ h a t = alpha ( k , x , y )
b e t a _ h a t = a l p h a _ r f f ( x , y ,m)
r =np . s o r t ( x )

116
4
Kernel Computations
u=np . zeros ( n )
v=np . zeros ( n )
for
j
in
range ( n ) :
S=0
for
i
in
range ( n ) :
S=S+ a l p h a _ h a t [ i ]âˆ—k ( x [ i ] , r [ j ] )
u [ j ]=S
v [ j ]= np . sum( b e t a _ h a t âˆ—z ( r [ j ] ,m) )
p l t . s c a t t e r ( x ,
y ,
f a c e c o l o r s =â€™noneâ€™ ,
e d g e c o l o r s = "k" ,
marker = "o")
p l t . p l o t ( r ,
u ,
c = "r" ,
l a b e l = "w/oâ£Approx")
p l t . p l o t ( r ,
v ,
c = "b" ,
l a b e l = "withâ£Approx")
p l t . xlim ( âˆ’1.5 ,
2)
p l t . ylim ( âˆ’2 ,
8)
p l t . x l a b e l ("x")
p l t . y l a b e l ("y")
p l t . t i t l e ("Kernelâ£Regression")
p l t . legend ( loc = "upperâ£left" ,
frameon = True ,
prop ={â€™sizeâ€™: 1 4 } )
The RFF are said to have no signiï¬cant degradation due to approximation in
practice. Still, this does cause an issue regarding theoretical guarantees.
4.6
NystrÃ¶m Approximation
We consider ï¬nding the coefï¬cient estimates (K + Î»I)âˆ’1y in kernel ridge regression.
Suppose that we can realize a low-rank matrix decomposition of K = RRâŠ¤with
R âˆˆRNÃ—m in a computationally inexpensive way. In this case, we can complete the
estimation task quickly. Note that we have
(RRâŠ¤+ Î»IN)âˆ’1 = 1
Î»{IN âˆ’R(RâŠ¤R + Î»Im)âˆ’1RâŠ¤} ,
(4.34)
which is due to Sherman-Morrison-Woodburyâ€™s formula5: r, s â‰¥1, A âˆˆRsÃ—s, U âˆˆ
RsÃ—r, C âˆˆRrÃ—r, V âˆˆRrÃ—s
(A + UCV )âˆ’1 = Aâˆ’1 âˆ’Aâˆ’1U(Câˆ’1 + V Aâˆ’1U)âˆ’1V Aâˆ’1
(4.35)
with r = m, s = N,A = Î»IN, U = R, C = Ir, and V = RâŠ¤.
Computing the left side of (4.34) requires an inverse matrix operation of size N,
while computing the right side involves the product of N Ã— m and m Ã— m matrices
and an inverse matrix operation of size m. The computations on the left- and right-
hand sides require O(N 3) and O(N 2m) complexity, respectively. In the following
part of this section, we show that with some approximation, the decomposition of
K = RRâŠ¤is completed in O(Nm2) time, i.e., the calculation of the ridge regression
is performed in O(Nm2). In other words, if N/m = 10, the computational time is
only 1/100.
5 Joe Suzuki, â€œStatistical Learning with Math and R/Pythonâ€.

4.6 NystrÃ¶m Approximation
117
In Sect.3.3, based on (3.18), we considered approximating the eigenfunctions
from x1, . . . , xm âˆˆE by
Ï†i(Â·) =
âˆšm
Î»(m)
i
m

j=1
k(x j, Â·)U j,i.
Let m â‰¤N; from the ï¬rst m samples
x1, . . . , xm
of x1, . . . , xm, xm+1, . . . , xN, we construct Ï†i and Î»i. Then, via
vi := [Ï†i(x1)/
âˆš
N, . . . , Ï†i(xN)/
âˆš
N] âˆˆRN
Î»(N)
i
:= NÎ»i
KN =
m

i=1
Î»(N)
i
vivâŠ¤
i ,
we approximate the Gram matrix KN w.r.t. x1, . . . , xN. In order to decompose RRâŠ¤,
we may set it as
R =

Î»(N)
i
[v1, . . . , vm] .
To compute R, we require O(m3) and O(Nm2) time complexities for obtaining
the eigenvalue and eigenvector of Km and v1, . . . , vm âˆˆRN, respectively. Thus, the
computation completes O(Nm2) time in total.
Example 70 Wecomparedtheresultsofkernelridgeregressionwith N = 300, m =
10, 20, and Î» = 10âˆ’5, 10âˆ’3 (Fig.4.8). For these data, when Î» â‰¥1, the graphs
obtainedwithandwithoutapproximationwereconsistent.Form = 10, 20,thecurves
were almost identical. We observed that the approximation error was smaller when
Î» was small for RFF, while the error was smaller when Î» was large for the NystrÃ¶m
approximation.
sigma2=1
def k ( x , y ) :
return np . exp ( âˆ’(xâˆ’y ) âˆ—âˆ—2/(2âˆ—sigma2 ) )
n=300
x=randn ( n ) /2
y=3 âˆ’2âˆ—xâˆ—âˆ—2 + 3âˆ—xâˆ—âˆ—3 + 2âˆ—randn ( n )
lam =10âˆ—âˆ—( âˆ’5)
m=10
K=np . zeros ( ( n , n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K[ i , j ]= k ( x [ i ] , x [ j ] )
# Low Rank Approximated
Function

118
4
Kernel Computations
def
alpha_m (K, x , y ,m) :
n=len ( x )
U,D,V=np . l i n a l g . svd (K[ :m, :m] )
u=np . zeros ( ( n ,m) )
for
i
in
range (m) :
for
j
in
range ( n ) :
u [ j , i ]= np . s q r t (m/ n ) âˆ—np . sum(K[ j , :m]âˆ—U[ :m, i ] /D[ i ] )
mu=Dâˆ—n /m
R=np . zeros ( ( n ,m) )
for
i
in
range (m) :
R [ : , i ]= np . s q r t (mu[ i ] ) âˆ—u [ : , i ]
Z=np . l i n a l g . inv ( np . dot (R. T ,R) +lamâˆ—np . eye (m) )
alpha =np . dot ( ( np . eye ( n )âˆ’np . dot ( np . dot (R, Z) ,R. T) ) , y ) / lam
return ( alpha )
# Usual
Function
def
alpha (K, x , y ) :
alpha =np . dot ( np . l i n a l g . inv (K+lamâˆ—np . eye ( n ) ) , y )
return
alpha
# Numerical
Comparison
alpha_1 = alpha (K, x , y )
alpha_2 =alpha_m (K, x , y ,m)
r =np . s o r t ( x )
w=np . zeros ( n )
v=np . zeros ( n )
for
j
in
range ( n ) :
S_1=0
S_2=0
for
i
in
range ( n ) :
S_1=S_1+alpha_1 [ i ]âˆ—k ( x [ i ] , r [ j ] )
S_2=S_2+alpha_2 [ i ]âˆ—k ( x [ i ] , r [ j ] )
w[ j ]= S_1
v [ j ]= S_2
p l t . s c a t t e r ( x ,
y ,
f a c e c o l o r s =â€™noneâ€™ ,
e d g e c o l o r s = "k" ,
marker = "o")
p l t . p l o t ( r , w,
c = "r" ,
l a b e l = "w/oâ£Approx")
p l t . p l o t ( r ,
v ,
c = "b" ,
l a b e l = "withâ£Approx")
p l t . xlim ( âˆ’1.5 ,
2)
p l t . ylim ( âˆ’2 ,
8)
p l t . x l a b e l ("x")
p l t . y l a b e l ("y")
p l t . t i t l e ("Kernelâ£Regression")
p l t . legend ( loc = "upperâ£left" ,
frameon = True ,
prop ={â€™sizeâ€™: 1 4 } )
4.7
Incomplete Cholesky Decomposition
In general, we can decompose a positive deï¬nite matrix A âˆˆRNÃ—N into A = RRâŠ¤.
By using a lower triangular matrix R with nonnegative diagonal components. Such
a decomposition is called the Cholesky decomposition of A.
Proposition 47 For a positive deï¬nite matrix A âˆˆRnÃ—n, there exists a Cholesky
decomposition A = RRâŠ¤that is unique if and only if A is positive deï¬nite.
Many books cover this material. For the proofs, see, for example, [9].
The following is the Cholesky decomposition procedure. We construct the process
so that we can stop anytime to obtain an approximation of RRâŠ¤with rank r â‰¤N.
1. In the initial stage, B = A, and R is a zero matrix.

4.7 Incomplete Cholesky Decomposition
119
-1.0
-0.5
0.0
0.5
1.0
-1
0
1
2
3
4
5
Î» = 10âˆ’5, m = 10
x
y
-1.0
-0.5
0.0
0.5
1.0
-1
0
1
2
3
4
5
Î» = 10âˆ’5, m = 20
x
y
-1.0
-0.5
0.0
0.5
1.0
-1
0
1
2
3
4
5
Î» = 10âˆ’3, m = 10
x
y
-1.0
-0.5
0.0
0.5
1.0
-1
0
1
2
3
4
5
Î» = 10âˆ’3, m = 20
x
y
Fig. 4.8 We approximated data with N = 300 and ranks m = 10, 20. The upper and lower subï¬g-
ures display the results obtained when running Î» = 10âˆ’5 and Î» = 10âˆ’3, respectively. The red and
blue lines are the results obtained without approximation and with approximation, respectively. The
accuracy is almost the same as that in the case without approximation when m = 20. The larger the
value of Î» is, the smaller the approximation error becomes
2. For eachi = 1, . . . ,r, the ï¬rsti columns of R are set so that B j,i = N
h=1 R j,h Ri,h
for j = 1, . . . , N. In other words, the setup is complete through the ith column
of B.
R =
â¡
â¢â¢â¢â¢â¢â¢â¢â¢â¢â¢â£
R1,1
0
Â· Â· Â·
Â· Â· Â· Â· Â· Â· 0
...
...
...
Â· Â· Â· Â· Â· Â· 0
Ri,1
...
Ri,i
0 Â· Â· Â· 0
Ri+1,1
...
Ri+1,i 0 Â· Â· Â· 0
...
...
...
...
... ...
RN,1 Â· Â· Â· RN,i
0 Â· Â· Â· 0
â¤
â¥â¥â¥â¥â¥â¥â¥â¥â¥â¥â¦
.
In this case, we swap the two subscripts in B by multiplying a matrix Q from the
front and rear of B.
3. The ï¬nal result is that RRâŠ¤= B = PâŠ¤AP with P = Q1 Â· Â· Â· QN. Therefore, A =
P RRâŠ¤PâŠ¤, and we have that P R(P R)âŠ¤is the Cholesky decomposition.

120
4
Kernel Computations
Here, to replace the (i, j) rows and (i, j) columns of the symmetric matrix B, let
Q be the matrix obtained by replacing the (i, j), ( j, i) and (i, i), ( j, j) components
of the unit matrix with 1 and 0, respectively, and multiplying B by the symmetric
matrix Q from the front and rear of B. For example,
QBQ =
â¡
â£
1 0 0
0 0 1
0 1 0
â¤
â¦
â¡
â£
b11 b12 b13
b21 b22 b23
b31 b32 b33
â¤
â¦
â¡
â£
1 0 0
0 0 1
0 1 0
â¤
â¦=
â¡
â£
b11 b13 b12
b31 b22 b32
b21 b23 b33
â¤
â¦.
Speciï¬cally, for i = 1, 2, Â· Â· Â· ,r, we perform the following steps: Assume that
Ïµ > 0.
1. Let k be the j (i â‰¤j â‰¤N) that maximizes R2
j, j = B j, j âˆ’iâˆ’1
h=1 R2
j,h.
(a) Swap the ith and kth rows and ith and kth columns of B.
(b) Let Qi,k := 1, Qk,i := 1, Qi,i := 0, Qk,k := 0.
(c) Swap Ri,1, Â· Â· Â· , Ri,iâˆ’1 and Rk,1, Â· Â· Â· , Rk,iâˆ’1.
(d) Ri,i =

Bk,k âˆ’iâˆ’1
h=1 R2
k,h.
2. End if Ri,i < Ïµ.
3. R j,i =
1
Ri,i
(B j,i âˆ’
iâˆ’1

h=1
R j,h Ri,h) for each j = i + 1, Â· Â· Â· , N.
Once the ith column is completed, B j,i = N
h=1 R j,h Ri,h follows, and R j,i remains
the same after that for each j = 1, . . . , N. Then, B = RRâŠ¤follows if the procedure
completes up to r = N.
At the beginning of each i = 1, 2, . . . ,r, we select the j that maximizes R2
j, j =
B j, j âˆ’iâˆ’1
h=1 R2
j,h â‰¥0. In step 3, the components of the jth ( j = i + 1, . . . , N) rows
of the ith column join, but we divide them by Ri,i. Compared to the case where other
values are selected as Ri,i in step 1, the absolute value of R j,i after dividing by
Rii becomes smaller, and the B j, j âˆ’i
h=1 R2
j,h in the next step becomes larger for
each j. If R2
r,r takes a negative value, then regardless of the selection order, there
is no solution to the Cholesky decomposition, contradicting Proposition 47 (the
uniqueness of the solution is also guaranteed). Even in the case of an incomplete
Cholesky decomposition, we use the ï¬rst r columns when running r = N.
We show the code for executing the incomplete Cholesky decomposition below:
def
im_ch (A,m) :
n=A. shape [ 1 ]
R=np . zeros ( ( n , n ) )
P=np . eye ( n )
for
i
in
range (m) :
max_R=âˆ’np . i n f
for
j
in
range ( i , n ) :
RR=A[ j , j ]
for h in
range ( i ) :
RR=RRâˆ’R[ j , h ]âˆ—âˆ—2
i f RR>max_R :
k= j

4.7 Incomplete Cholesky Decomposition
121
max_R=RR
R[ i , i ]= np . s q r t ( max_R )
i f
k != i :
for
j
in
range ( i ) :
w=R[ i , j ] ; R[ i , j ]=R[ k , j ] ; R[ k , j ]=w
for
j
in
range ( n ) :
w=A[ j , k ] ; A[ j , k ]=A[ j , i ] ; A[ j , i ]=w
for
j
in
range ( n ) :
w=A[ k , j ] ; A[ k , j ]=A[ i , j ] ; A[ i , j ]=w
Q=np . eye ( n ) ; Q[ i , i ]=0; Q[ k , k ]=0; Q[ i , k ]=1; Q[ k , i ]=1
P=np . dot (P ,Q)
i f
i <n :
for
j
in
range ( i +1 ,n ) :
S=A[ j , i ]
for h in
range ( i ) :
S=Sâˆ’R[ i , h ]âˆ—R[ j , h ]
R[ j , i ]=S /R[ i , i ]
return np . dot (P ,R)
# Data
Generation : Make matrix A nonnegative
d e f i n i t e
n=5
D=np . matrix ( [ [ np . random . r a n d i n t (âˆ’n ,
n )
for
i
in
range ( n ) ]
for
j
in
range ( n )
] )
A=np . dot (D,D. T)
A
matrix([[ 68,
11,
6, -14,
13],
[ 11,
23,
0, -17,
11],
[
6,
0,
24,
36, -16],
[-14, -17,
36,
75, -24],
[ 13,
11, -16, -24,
45]])
L=im_ch (A, 5 )
np . dot (L , L . T)
array([[ 46.,
22.,
36.,
2., -7.],
[ 22.,
31.,
14., -19.,
-8.],
[ 36.,
14.,
41.,
-6., -20.],
[
2., -19.,
-6.,
46.,
21.],
[ -7.,
-8., -20.,
21.,
22.]])
## Incomplete
Cholesky
Decomposition
of
rank
t h r e e
L=im_ch (A, 3 ) np . l i n a l g . eig (A)

122
4
Kernel Computations
(array([1.01074272e+02, 5.66112516e+01, 2.41344415e+01, 9.62653508e-02,
4.08376941e+00]),
matrix([[-0.56411909, -0.45552615, -0.22499121, -0.46541646, 0.45500775],
[ 0.30282894, -0.79305647, 0.04313883, -0.11440623, -0.51420456],
[-0.40884563, 0.15490099, -0.68851469, 0.1243158 , -0.56510533],
[ 0.31554644, -0.28590181, - 0.51655213, 0.58928787, 0.45233208],
[-0.56862992, -0.24046456, 0.45457608, 0.63842315, -0.06792108]]))
## A cannot
be
recovered
B=np . dot (L , L . T)
B
array([[ 46.
,
2.
,22.
,
-7.
,
36.
],
[
2.
,
46.
, -19.
,
21.
,
-6.
],
[ 22.
, -19.
,
31.
,
-8.
,
14.
],
[ -7.
,
21.
,
-8.
,
12.74957882,
-11.52827918],
[ 36.
,
-6.
,
14.
, -11.52827918,
33.00601685]])
# The
f i r s t
t h r e e
e i g e n v a l u e s
of B are
c l o s e
to
those
of A .
np . l i n a l g . eig (B)
(array([ 9.53379063e+01, 5.65627665e+01, 1.68549229e+01, 1.42156846e-14,
-3.05830677e-15]),
array([[-0.60391559, -0.44498737, -0.0412799 , 0.6556516 , 0.01706317],
[ 0.31630878, -0.79905862, -0.14333816, -0.30429563, -0.42390544],
[-0.44500967, 0.16578492, -0.79173017, -0.36203582, -0.1781387 ],
[ 0.24801368, -0.27858603, -0.38477667, 0.11142099, 0.84426293],
[-0.52506222, -0.24165419, 0.45040025, -0.57796243, 0.27477214]]))
# The rank
of B i s
t h r e e .
np . l i n a l g . matrix_rank (B)
3

Appendix
123
Appendix
Proof of Proposition44
Since r is a natural spline function whose highest order is 2q âˆ’1 and it satisï¬es
r(q)(0) = Â· Â· Â· = r(2qâˆ’1)(0) = r(q)(1) = Â· Â· Â· = r(2qâˆ’1)(1) = 0 ,
we have
 1
0
r(q)(x)s(q)(x)dx =

r(q)(x)s(qâˆ’1)(x)
1
0 âˆ’
 1
0
r(q+1)(x)s(qâˆ’1)(x)dx
= âˆ’
 1
0
r(q+1)(x)s(qâˆ’1)(x)dx = Â· Â· Â· = (âˆ’1)qâˆ’1
 1
0
r(2qâˆ’1)(x)sâ€²(x)dx
= (âˆ’1)qâˆ’1
Nâˆ’1

j=1
r(2qâˆ’1)(x+
j )

s(x j+1) âˆ’s(x j)

= 0 ,
(4.36)
where we use s(xi) = 0 for i = 1, . . . , N. Moreover, r(2qâˆ’1)(x+
j ) is the (2q âˆ’1)th
right differential coefï¬cient of r, and it has a constant value during x j < x < x j+1.
Thus, we have the following inequality in the proposition:
 1
0
{g(q)(x)}2dx =
 1
0
{r(q)(x) + s(q)(x)}2dx
=
 1
0
{r(q)(x)}2dx +
 1
0
{s(q)(x)}2dx + 2
 1
0
r(q)(x)s(q)(x)dx
=
 1
0
{r(q)(x)}2dx +
 1
0
{s(q)(x)}2dx â‰¥
 1
0
{r(q)(x)}2dx ,
(4.37)
where the third equality is due to (4.36). On the other hand, from g,r âˆˆWq[0, 1]
and s âˆˆWq[0, 1], we have
s(x) =
qâˆ’1

i=0
s(i)(0)
i!
xi +
 1
0
(x âˆ’u)qâˆ’1
+
(q âˆ’1)! s(q)(u)du.
Therefore, when the equality of (4.37) holds, i.e.,
 1
0 {s(q)(x)}2dx = 0, we have
s(q)(x) = 0 almost everywhere. Hence,
s(x) =
qâˆ’1

i=0
s(i)(0)
i!
xi ,

124
4
Kernel Computations
which means that s(xi) = 0 for i = 1, 2, . . . , N. Thus, if N exceeds the order of the
polynomial q âˆ’1, then we require s(x) = 0 for x âˆˆ[0, 1].
â–¡
Proof of Proposition45
From the additive theorem, we have
2 cos(Ï‰âŠ¤x + b) cos(Ï‰âŠ¤y + b) = cos(wâŠ¤(x âˆ’y)) + cos(wâŠ¤(x + y) + 2b) .
Since the expectation of the second term w.r.t. b when ï¬xing Ï‰ is zero, we have
EÏ‰,b[
âˆš
2 cos(Ï‰âŠ¤x + b) Â·
âˆš
2 cos(Ï‰âŠ¤y + b)] = EÏ‰ cos(wâŠ¤(x âˆ’y)) .
If we apply Eulerâ€™s formula eiÎ¸ = cos Î¸ + i sin Î¸ to Proposition 5, then k(x, y) takes
a real value. Thus, we have E[sin(Ï‰âŠ¤(x âˆ’y))] = 0, and k(x, y) can be written as
EÏ‰ exp(iÏ‰âŠ¤(x âˆ’y)) = EÏ‰[cos(Ï‰âŠ¤(x âˆ’y)) + i sin(Ï‰âŠ¤(x âˆ’y))] = EÏ‰[cos(Ï‰âŠ¤(x âˆ’y))] .
From Proposition 5, we obtain (4.28).
â–¡
Proof of Lemma 7
Let Ïµ > 0. Since eÏµx is convex w.r.t. x, if we take the expectation on the both sides
of
eÏµX â‰¤X âˆ’a
b âˆ’a eÏµb + b âˆ’X
b âˆ’a eÏµa
for b > a, then
E[eÏµX] â‰¤
âˆ’a
b âˆ’a eÏµb +
b
b âˆ’a eÏµa = Î¸eÏµ(1âˆ’Î¸)(bâˆ’a) + (1 âˆ’Î¸)eâˆ’ÏµÎ¸(bâˆ’a) = exp{âˆ’Î¸s + log(1 âˆ’Î¸ + Î¸es)}
for s = Ïµ(b âˆ’a) and Î¸ =
âˆ’a
b âˆ’a . Therefore, it is sufï¬cient for the exponent f (s) :=
âˆ’Î¸s + log(1 âˆ’Î¸ + Î¸es) to be at most s2/8. Since
f â€²(s) = âˆ’Î¸ +
Î¸es
1 âˆ’Î¸ + Î¸es
and f (0) = f â€²(0) = 0, we have
f â€²â€²(s) =
(1 âˆ’Î¸) Â· Î¸es
(1 âˆ’Î¸ + Î¸es)2 = Ï†(1 âˆ’Ï†) â‰¤1
4
for Ï† =
Î¸es
1 âˆ’Î¸ + Î¸es . Hence, a Î¼ âˆˆR exists such that

Appendix
125
f (s) = f (0) + f â€²(0)(s âˆ’0) + 1
2 f â€²â€²(Î¼)(s âˆ’0)2 â‰¤s2
8 ,
which implies (4.31).
â–¡
Exercises 46âˆ¼64
46. Let k be a kernel and (x1, y1), . . . , (xN, yN) be samples, and let f (Â·) :=
N
i=1 Î±ik(xi, Â·). If we minimize N
i=1{yi âˆ’f (xi)}2 + Î»âˆ¥f âˆ¥2, Î» > 0 (kernel
ridge regression),why does this mean that we have minimized over f âˆˆH?
In addition, express the optimal value of Î± = [Î±1, . . . , Î±N]âŠ¤using the Gram
matrix K âˆˆRNÃ—N and y = [y1, . . . , yN]âŠ¤.
47. In kernel PCA, let k be a kernel and x1, . . . , xN be samples, and let f (Â·) :=
N
i=1 Î±ik(xi, Â·). If we maximize (4.8), why does this mean that we have maxi-
mized it over f âˆˆH? Additionally, express the eigenequations obtained when
Î² = K 1/2Î± by using the Gram matrix K âˆˆRNÃ—N.
48. InkernelPCA,wewishtoï¬ndÎ± foracenteredGrammatrix,asin(4.9).Complete
the function kernel_pca_train by ï¬lling in the space below.
def
k e r n e l _ p c a _ t r a i n ( x ,
k ) :
n = x . shape [ 0 ]
K = np . zeros ( ( n ,
n ) )
S = [ 0 ] âˆ—n ; T = [ 0 ] âˆ—n
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K[ i ,
j ] = k ( x [ i ,
: ] ,
x [ j ,
: ] )
for
i
in
range ( n ) :
S [ i ] = np . sum(K[ i ,
: ] )
for
j
in
range ( n ) :
T[ j ] = np . sum(K[ : ,
j ] )
U = np . sum(K)
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K[ i ,
j ] = K[ i ,
j ] âˆ’S [ i ]
/
n âˆ’T[ j ]
/
n + U /
nâˆ—âˆ—2
val ,
vec = np . l i n a l g . eig (K)
idx = val . a r g s o r t ( ) [:: âˆ’1]
# decreasing
order
as R
val = val [ idx ]
vec = vec [ : , idx ]
alpha = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
alpha [ : ,
i ] = vec [ : ,
i ]
/
val [ i ]âˆ—âˆ—0.5
return
alpha
Based on the Î± obtained from the data
X, kernel k, and function
kernel_pca_train, we wish to calculate the score of z âˆˆRNÃ—p (any of the
x1 . . . , xN) (up to 1 â‰¤m â‰¤p dimensions). Complete the function below:
def
k e r n e l _ p c a _ t e s t ( x ,
k ,
alpha , m,
z ) :
n = x . shape [ 0 ]

126
4
Kernel Computations
pca = np . zeros (m)
for
i
in
range ( n ) :
pca = pca + alpha [ i ,
0:m] âˆ—k ( x [ i ,
: ] ,
z )
return
pca
Check whether the constructed function works with the following program:
sigma2 = 0.01
def k ( x ,
y ) :
return np . exp(âˆ’np . l i n a l g . norm ( x âˆ’y ) âˆ—âˆ—2
/
2
/
sigma2 )
X = pd . read_csv (â€™https://raw.githubusercontent.com/selva86/datasets/
master/USArrests.csvâ€™)
x = X. values [: ,: âˆ’1]
n = x . shape [ 0 ] ;
p = x . shape [ 1 ]
alpha = k e r n e l _ p c a _ t r a i n ( x ,
k )
z = np . zeros ( ( n ,
2) )
for
i
in
range ( n ) :
z [ i ,
: ]
= k e r n e l _ p c a _ t e s t ( x ,
k ,
alpha ,
2 , x [ i ,
: ] )
min1 = np . min ( z [ : ,
0 ] ) ;
min2 = np . min ( z [ : ,
1 ] )
max1 = np . max( z [ : ,
0 ] ) ; max2 = np . max( z [ : ,
1 ] )
p l t . xlim ( min1 ,
max1 )
p l t . ylim ( min2 ,
max2 )
p l t . x l a b e l ("First")
p l t . y l a b e l ("Second")
p l t . t i t l e ("Kernelâ£PCAâ£(Gaussâ£0.01)")
for
i
in
range ( n ) :
i f
i
!=
4:
p l t . t e x t ( x = z [ i ,
0] , y = z [ i ,
1] ,
s = i )
p l t . t e x t ( z [4 ,
0] ,
z [4 ,
1] ,
5 , c = "r")
49. Show that the ordinary PCA and kernel PCA with a linear kernel output the same
score.
50. Derive the KKT condition for the kernel SVM (4.12).
51. In Example 66, instead of linear and polynomial kernels, use a Gaussian kernel
with different values of Ïƒ 2 (three different types), and draw the boundary curve
in the same graph.
52. From (4.21) and (4.22), derive J
j=1 Î² j+4 = 0 and J
j=1 Î² j+4Î¾ j = 0.
53. Prove Proposition 44 according to the following steps:
(a) Show that
 1
0
r(q)(x)s(q)(x)dx = 0.
(b) Show that
 1
0 {g(q)(x)}2dx â‰¥
 1
0 {r(q)(x)(x)}2dx.
(c) When the equality in (b) holds, show that s(x) =
qâˆ’1

i=0
s(i)(0)
i!
xi.
(d) Show that the function s decreases when the equality in (b) holds and N
exceeds the degree q âˆ’1 of the polynomial.
54. In RFF, instead of ï¬nding the kernel k(x, y), we ï¬nd its unbiased estimator
Ë†k(x, y). Show that the average of Ë†k(x, y) is k(x, y). Moreover, construct a func-
tion that outputs Ë†k(x, y) from (x, y) âˆˆE for m = 100 by using the constants
and functions in the program below. Furthermore, compare the result with the
value output by the Gaussian kernel and conï¬rm that it is correct.

Exercises 46âˆ¼64
127
sigma =10
sigma2=sigma âˆ—âˆ—2
def z ( x ) :
return np . s q r t ( 2 /m) âˆ—np . cos (wâˆ—x+b )
def
zz ( x , y ) :
return np . sum( z ( x ) âˆ—z ( y ) )
55. Derive the Chernoff bound.
56. Show that Proposition 46 implies (4.29).
57. The RFF are based on Bochnerâ€™s theorem (Proposition 5). What relationship
exists between them?
58. In RFF, after randomly generating (w1, b1), . . . , (wm, bm), we obtain Z =
(z j(xi)) âˆˆR for i = 1, . . . , N and j = 1, . . . , m. If we use Ë†K = Z ZâŠ¤rather
than K = (k(xi, x j)) âˆˆRNÃ—N, show that Ë†f (x) = m
i=1 Ë†Î±i Ë†k(x, xi) (x âˆˆE) can
be expressed by Ë†f (x) = z(x) Ë†Î² using Ë†Î² in (4.33). Moreover, prove Woodburyâ€™s
formula:
U(Is + VU) = (Ir + UV )U
for U âˆˆRrÃ—s, V âˆˆRsÃ—r,r, s â‰¥1.
59. Evaluate the number of computations required to obtain (4.33) for the RFF. In
addition, evaluate the computational complexity of ï¬nding Ë†f (x) for the new
x âˆˆE.
60. To ï¬nd the coefï¬cient estimates (K + Î»I)âˆ’1y in kernel ridge regression, we
wish to decompose the low-rank matrix K = RRâŠ¤with R âˆˆRNÃ—m. If we can
decompose K = RRâŠ¤, evaluate the computations on the left- and right-hand
sides, where we assume that ï¬nding the inverse of the matrix A âˆˆRnÃ—n takes
O(n3).
61. We wish to ï¬nd the coefï¬cient Ë†Î± of the kernel ridge regression by using the
NystrÃ¶m approximation. If we use the left-hand side of (4.34) instead of the
right-hand side, what changes would be necessary in the following code?
def
alpha_m (K, x , y ,m) :
n=len ( x )
U,D,V=np . l i n a l g . svd (K[ :m, :m] )
u=np . zeros ( ( n ,m) )
for
i
in
range (m) :
for
j
in
range ( n ) :
u [ j , i ]= np . s q r t (m/ n ) âˆ—np . sum(K[ j , :m]âˆ—U[ :m, i ] /D[ i ] )
mu=Dâˆ—n /m
R=np . zeros ( ( n ,m) )
for
i
in
range (m) :
R [ : , i ]= np . s q r t (mu[ i ] ) âˆ—u [ : , i ]
Z=np . l i n a l g . inv ( np . dot (R. T ,R) +lamâˆ—np . eye (m) )
alpha =np . dot ( ( np . eye ( n )âˆ’np . dot ( np . dot (R, Z) ,R. T) ) , y ) / lam
return ( alpha )

128
4
Kernel Computations
sigma =10;
sigma2=sigma ^2
z= f u n c t i o n ( x )
s q r t ( 2 /m) âˆ—cos (wâˆ—x+b )
zz= f u n c t i o n ( x , y ) sum( z ( x ) âˆ—z ( y ) )
alpha .m= f u n c t i o n ( k , x , y ,m) {
n= l e n g t h ( x ) ; K= matrix (0 , n , n ) ;
for ( i
in
1: n ) for ( j
in
1: n )K[ i , j ]= k ( x [ i ] ,
x [ j ] )
A=svd (K[ 1 :m, 1 :m] )
u= a r r a y ( dim=c ( n ,m) ) ;
for ( i
in
1:m) for ( j
in
1: n ) u [ j , i ]= s q r t (m/ n ) âˆ—sum(K[ j , 1 :m]âˆ—A$u [ 1 :m, i ] ) /
A$d [ i ]
mu=A$dâˆ—n /m
R= s q r t (mu [ 1 ] ) âˆ—u [ , 1 ] ;
for ( i
in
2:m)R=cbind (R, s q r t (mu[ i ] ) âˆ—u [ , i ] )
alpha =( diag ( n )âˆ’R%âˆ—%solve ( t (R)%âˆ—%R+lambdaâˆ—diag (m) )%âˆ—%t (R) )%âˆ—%y / lambda
return ( as . v e c t o r ( alpha ) )
}
62. In Step 1 of the procedure for the incomplete Cholesky decomposition, each
time we choose the j (i â‰¤j â‰¤N) that maximizes R2
j, j = B j, j âˆ’iâˆ’1
h=1 R2
j,h as
k. Show that Bk,k âˆ’iâˆ’1
h=1 R2
k,h in Step 1(d) is nonnegative.
63. Show that when the incomplete Cholesky decomposition process is completed
up to the rth column, we have
B ji =
i
h=1
R jh R jh
for each i = 1, . . . ,r and j = i + 1, . . . , N.
64. Generate a nonnegative deï¬nite matrix of size 5 Ã— 5 and run im_ch to perform
the incomplete Cholesky decomposition of rank three.

Chapter 5
The MMD and HSIC
Inthis chapter, weintroducetheconcept of randomvariables X : E â†’RinanRKHS
and discuss testing problems in RKHSs. In particular, we deï¬ne a statistic and its
null hypothesis for the two-sample problem and the corresponding independence
test. We do not know the distribution according to the null hypothesis under a ï¬nite
sample in either case. Therefore, we introduce a permutation test and a U-statistic
with which we construct the process and run the program. Then, we study the notions
of characteristics and universal kernels to learn what kernels are valid for such tests.
Finally, we learn about empirical processes, which are often used in the mathematical
analyses of machine learning and deep learning methods.
5.1
Random Variables in RKHSs
In Chap.1, we proved that a function X : E â†’R that takes values in R is measurable
if {Ï‰ âˆˆE|X(Ï‰) âˆˆB} for any Borel set B is an event (element) in F, and we call
such an X a random variable.
In the following, we say that a kernel k is measurable if the set of (x, y) such that
k(x, y) âˆˆB is an event in E Ã— E, and we assume that any kernel k is measurable.
Moreover, in this chapter, the expectation E[k(X, X)] of k(x, x) âˆˆR, x âˆˆE, is
bounded, which means that both E[âˆšk(X, X)] â‰¤âˆšE[k(X, X)] are bounded.
Proposition 48 Let k : E Ã— E â†’R be measurable. Then, the map  : E âˆ‹x â†’
k(x, Â·) âˆˆH is measurable. Thus, k(X, Â·) is a random variable in H for any random
variable X that takes values in E.
Proof: See the appendix at the end of this chapter.
Let X : E â†’R be a random variable. The linear functional T : H â†’R with
T ( f ) := E[ f (X)] = E[âŸ¨f (Â·), k(X, Â·)âŸ©H] â‰¤E[âˆ¥f âˆ¥H

k(X, X)] â‰¤âˆ¥f âˆ¥HE[

k(X, X)]
Â© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022
J. Suzuki, Kernel Methods for Machine Learning with Math and Python,
https://doi.org/10.1007/978-981-19-0401-1_5
129

130
5
The MMD and HSIC
satisï¬es T ( f )
âˆ¥f âˆ¥H
â‰¤E[

k(X, X)] < âˆ. From Proposition 22, there exists an m X âˆˆH
such that
E[ f (X)] = âŸ¨f (Â·), m X(Â·)âŸ©H
for any f âˆˆH. We call such an m X the expectation of k(X, Â·), and we write m X(Â·) =
E[k(X, Â·)]. Then, we have
E[âŸ¨f (Â·), k(X, Â·)âŸ©H] = âŸ¨f (Â·), E[k(X, Â·)]âŸ©H ,
which means that we can change the order of the inner product and expectation
operations. Let EX, EY be sets. We deï¬ne the tensor product H0 of RKHSs HX and
HY consisting of kernels kX : EX â†’R and kY : EY â†’R, respectively, by the set
of functions EX Ã— EY â†’R, f (x, y) = m
i=1 fX,i(x) fY,i(y), fX,i âˆˆHX, fY,i âˆˆHY
for (x, y) âˆˆEX Ã— EY, and we deï¬ne the inner product and norm by
âŸ¨f, gâŸ©H0 =
m

i=1
n

j=1
âŸ¨fX,i, gX, jâŸ©HX âŸ¨fY,i, gY, jâŸ©HY
and âˆ¥f âˆ¥2
H0 = âŸ¨f, f âŸ©H0, respectively for f = m
j=1 fX, j fY, j, fX,i âˆˆHX, fY,i âˆˆHY
and g = n
j=1 gX, jgY, j, gX, j âˆˆHX, gY, j âˆˆHY. In fact, we have
âŸ¨f, gâŸ©H0 =
m

i=1
n

j=1

r

t
Î±i,rÎ³ j,tkX(xr, xt)

s

u
Î²i,sÎ´ j,ukY(ys, yu)
=
m

i=1

r

s
Î±i,rÎ²i,sg(xr, ys) =
n

j=1

t

u
Î³ j,tÎ´ j,u f (xt, yu)
for fX,i(Â·) = 
r Î±i,rkX(xr, Â·), fY,i(Â·) = 
s Î²i,skY(ys, Â·), gX, j(Â·) = 
t Î³ j,tkX(xt, Â·),
and gY, j(Â·) = 
u Î´ j,ukY(yu, Â·), which means that the functions do not depend on the
expressions of f, g.
If we complete H0, we can construct a linear space H consisting of the func-
tions f = âˆ
i=1
âˆ
j=1 ai, jeX.ieY, j such that âˆ¥f âˆ¥2 := âˆ
i=1
âˆ
j=1 a2
i, j < âˆ, and
the inner product is âŸ¨f, gâŸ©H = âˆ
i=1
âˆ
j=1 ai, jbi, j, where g = âˆ
j=1 bi, jeX,ieY, j
(âˆ
i=1
âˆ
j=1 b2
i, j < âˆ) and {eX,i}, {eY, j} are orthonormal bases of HX, HY, respec-
tively. Then, H0 is a dense subspace in H, and H is a Hilbert space. We say that
H0 is the direct product of HX, HY and write HX âŠ—HY. H is the set of functions f
such that f (x) := limnâ†’âˆfn(x) for any Cauchy sequence { fn} in H0 and x âˆˆE.
The claim follows from a similar discussion as that in Steps 1-5 of Proposition34.
Proposition 49 (Neveu [22]) The direct product HX âŠ—HY of RKHSs HX, HY with
reproducing kernels kX, kY is an RKHS with a reproducing kernel kXkY.
Proof: The derivation utilizes the following steps [1].

5.1 Random Variables in RKHSs
131
1. Show that |g(x, y)| â‰¤âˆškX(x, x)âˆškY(y, y)âˆ¥gâˆ¥for g âˆˆHX âŠ—HY and x âˆˆEX,
y âˆˆEY, which means that H is an RKHS due to Proposition 33.
2. Show that k(x, Â·, y, â‹†) := kX(x, Â·)kY(y, â‹†) âˆˆH when we ï¬x x âˆˆEX, y âˆˆEY.
3. Show that g(x, y) = âŸ¨f (Â·, â‹†), k(x, Â·, y, â‹†)âŸ©H.
For details, consult the proof at the end of this chapter.
â–¡
Then,weintroducethenotionofexpectationw.r.t.thevariables X, Y.Ifweassume
that E[kX(X, X)] and E[kY(Y, Y)] are ï¬nite, then EXY[kX(X, Â·)kY(Y, Â·)] is obtained
by taking the expectation of kX(x, Â·)kY(y, Â·) âˆˆHX âŠ—HY w.r.t. XY:
EXY[âˆ¥kX(X, Â·)kY(Y, Â·)âˆ¥HXâŠ—HY ] = EXY[âˆ¥kX(X, Â·)âˆ¥HX âˆ¥kY(Y, Â·)âˆ¥HY ]
= EXY[

kX(X, X)kY(Y, Y)] â‰¤

EX[kX(X, X)]EY[kY(Y, Y)] .
Thus, the left-hand side takes a ï¬nite value, and we have
EXY [ f (X, Y)] = EXY [âŸ¨f, kX(X, Â·)kY (Y, Â·)âŸ©] â‰¤âˆ¥f âˆ¥HX âŠ—HY EXY [âˆ¥kX(X, Â·)kY (Y, Â·)âˆ¥HX âŠ—HY ]
for f âˆˆHX âŠ—HY. From Proposition 22 (Rieszâ€™s representation theorem), there
exists an m XY âˆˆHX âŠ—HY such that
EXY[ f (X, Y)] = âŸ¨f, m XYâŸ©,
and we write
m XY := EXY[kX(X, Â·)kY(Y, Â·)] ,
which means that we can change the order of the inner product and expectation
operations:
EXY[âŸ¨f, kX(X, Â·)kY(Y, Â·)âŸ©] = âŸ¨f, EXY[kX(X, Â·)kY(Y, Â·)]âŸ©.
Moreover, for the m X, mY of X, Y, the expectation m XmY belongs to HX âŠ—HY,
and we have
âŸ¨f g, m XmYâŸ©HXâŠ—HY = âŸ¨f, m XâŸ©HX âŸ¨g, mYâŸ©HY = EX[ f (X)]EY[g(Y)]
f âˆˆHX, g âˆˆHY, which means that we multiply the expectations of X, Y even if
they are not independent. Thus, we call
m XY âˆ’m XmY
the covariate of (X, Y) in HX âŠ—HY, which belongs to HX âŠ—HY.
Proposition 50 For each f âˆˆHX, g âˆˆHY, there exist 	XY âˆˆB(HY, HX) and
	Y X âˆˆB(HX, HY) such that

132
5
The MMD and HSIC
âŸ¨f g, m XY âˆ’m XmYâŸ©HXâŠ—HY = âŸ¨	Y X f, gâŸ©HY = âŸ¨f, 	XYgâŸ©HX .
(5.1)
Proof: The operators 	Y X, 	XY are conjugates of each other, and from Proposition
22, if one exists, so does the other. We prove the existence of 	XY. The linear
functional
Tg : HX âˆ‹f â†’âŸ¨f g, m XY âˆ’m XmYâŸ©HXâŠ—HY âˆˆR
for an arbitrary g âˆˆHY is bounded from
âŸ¨f g, m XY âˆ’m XmYâŸ©HXâŠ—HY â‰¤âˆ¥f âˆ¥HX âˆ¥gâˆ¥HY âˆ¥m XY âˆ’m XmYâˆ¥HXâŠ—HY ,
and there exists an hg âˆˆHX such that Tg f = âŸ¨f, hgâŸ©HX from Proposition 22. Thus,
there exists 	XY : HY âˆ‹g â†’hg âˆˆHX such that
âŸ¨f g, m XY âˆ’m XmYâŸ©HXâŠ—HY = âŸ¨f, 	XYgâŸ©HX .
The boundness of 	XY is due to
âˆ¥	XYgâˆ¥HX = âˆ¥hgâˆ¥HX = âˆ¥Tgâˆ¥â‰¤âˆ¥gâˆ¥HY âˆ¥m XY âˆ’m XmYâˆ¥HXâŠ—HY .
â–¡
We call 	XY, 	Y X the mutual covariance operators.
Let H and k be an RKHS and its reproducing kernel respectively, and let P be
the set of distributions that X follows. Then, we can deï¬ne the map
P âˆ‹Î¼ â†’

k(x, Â·)dÎ¼(x) âˆˆH ,
which we call the embedding of probabilities in the RKHS. Suppose that the map is
injective, i.e., if the expectations

k(x, Â·)dÎ¼1(x) and

k(x, Â·)dÎ¼2(x) have the same
value, then the probabilities Î¼1, Î¼2 coincide. We call such a reproducing kernel k of
an RKHS H characteristic.
We learn some applications by using characteristic kernels, such as two-sample
problems and independence tests, and we consider the associated theory in later
sections of this chapter.
5.2
The MMD and Two-Sample Problem
Gretton et al. (2008),[11] proposed a statistical testing approach for testing whether
two distributions share given independent sequences x1, . . . , xm âˆˆR and y1, . . . ,
yn âˆˆR. We write the two distributions as P, Q and regard P = Q as the null hypothe-
sis.Let H andk beanRKHSanditsreproducingkernelrespectively;wedeï¬nem P :=
EP[k(X, Â·)] =

E k(x, Â·)d P(x), m Q := EQ[k(X, Â·)] =

E k(x, Â·)dQ(x) âˆˆH.
We

5.2 The MMD and Two-Sample Problem
133
note that the random variable X : E â†’R is measurable, and either P or Q is the
probability distribution that X follows.
Let F be a set of functions that satisï¬es a condition. In general, the quantity
deï¬ned by
sup
f âˆˆF
{EP[ f (X)] âˆ’EQ[ f (X)]}
is called the MMD (maximum mean discrepancy), and we assume that
F := { f âˆˆH|âˆ¥f âˆ¥H â‰¤1} ,
which means that we regard the MMD as
MMD2 = sup
f âˆˆF
{EP[ f (X)] âˆ’EQ[ f (X)]}2 = sup
f âˆˆF
{âŸ¨m P, f âŸ©âˆ’âŸ¨m Q, f âŸ©}2
= sup
f âˆˆF
{âŸ¨m P âˆ’m Q, f âŸ©}2 = âˆ¥m P âˆ’m Qâˆ¥2
H .
If the kernel k is characteristic, then we have
MMD = 0 â‡â‡’m P = m Q â‡â‡’P = Q
(5.2)
and
MMD2
= âŸ¨m P, m PâŸ©+ âŸ¨mQ, mQâŸ©âˆ’2âŸ¨m P, mQâŸ©
= âŸ¨EX[k(X, Â·)], EXâ€²[k(Xâ€², Â·)]âŸ©+ âŸ¨EY [k(Y, Â·)], EY â€²[k(Y â€², Â·)]âŸ©âˆ’2âŸ¨EX[k(X, Â·)], EY [k(Y, Â·)]âŸ©
= EX Xâ€²[k(X, Xâ€²)] + EYY â€²[k(Y, Y â€²)] âˆ’2EXY [k(X, Y)] ,
where Xâ€² and X (Y â€² and Y) are independent and follow the same distribution. How-
ever, we do not know m X, mY from the two-sample data. Thus, we execute the test
using their estimates:

MMD
2
B := 1
m2
m

i=1
m

j=1
k(xi, x j) + 1
n2
n

i=1
n

j=1
k(yi, y j) âˆ’2
mn
m

i=1
n

j=1
k(xi, y j)
(5.3)
1
m(m âˆ’1)
m

i=1

jÌ¸=i
k(xi, x j) +
1
n(n âˆ’1)
n

i=1

jÌ¸=i
k(yi, y j) âˆ’2
mn
m

i=1
n

j=1
k(xi, y j) .
(5.4)
Then, the estimate (5.4) is unbiased while (5.3) is biased:
E[
1
m(m âˆ’1)
m

i=1

jÌ¸=i
k(Xi, X j)] = 1
m
m

i=1
EXi [
1
m âˆ’1

jÌ¸=i
EX j [k(Xi, X j)]] = EX Xâ€²[k(X, Xâ€²)].

134
5
The MMD and HSIC
-0.01
0.00
0.01
0.02
0.03
0
20
40
60
The Same Dist. (Permutation)
MMD
2
U
Density
0.00
0.02
0.04
0.06
0.08
0 10
30
50
Diï¬€erent Dist. (Permutation)
MMD
2
U
Density
Fig. 5.1 Permutation test for the two-sample problem. The distributions of X, Y are the same (left)
and different (right). The blue and red dotted lines show the statistics and the borders of the rejection
region, respectively.
However, similar to the HSIC in the next section, we do not know the distribution
of the MMD estimate under P = Q. We consider executing one of the following
processes.
1. Construct a histogram of the MMD estimate values randomly by changing the
values of x1, . . . , xm and y1, . . . , yn (permutation test).
2. Compute an asymptotic distribution from the distribution of U statistics.
For the former, for example, we may construct the following procedure.
Example 71 We perform a permutation test on two sets of 100 samples that fol-
low the standard Gaussian distribution (Fig.5.1 Left). For the unbiased estimator of
M M D2, we use 
M M D
2
U in (5.6) instead of (5.4) for a later comparison. We also
double the standard deviation of one set of samples and perform the permutation
test again (Fig.5.1 Right). The reason why 
MMD
2
U also takes negative values is that
when the true value of the M M D is close to zero, the value can also be negative
since it is an unbiased estimator.
# In
t h i s
chapter ,
we assume
t h a t
the
f o l l o w i n g
has
been
executed .
import numpy as np
from
scipy . s t a t s
import kde
import
i t e r t o o l s
import math
import
m a t p l o t l i b . pyplot
as
p l t
from
m a t p l o t l i b
import
s t y l e
s t y l e . use ( " seaborn âˆ’t i c k s " )
sigma = 1
def k ( x ,
y ) :
return np . exp ( âˆ’(x âˆ’y ) âˆ—âˆ—2
/
sigma âˆ—âˆ—2)
# Data
Generation
n = 100
xx = np . random . randn ( n )
yy = np . random . randn ( n )
# The
d i s t r i b u t i o n s
are
equal

5.2 The MMD and Two-Sample Problem
135
# yy = 2 âˆ—np . random . randn ( n ) # The
d i s t r i b u t i o n s
are
not
equal
x = xx ;
y = yy
#
D i s t r i b u t i o n
of
the
n u l l
h y p o t h e s i s
T = [ ]
for h in
range (100) :
index1 = np . random . choice ( n ,
s i z e = i n t ( n / 2 ) ,
r e p l a c e = False )
index2 = [ x for x in
range ( n )
i f
x not
in
index1 ]
x =
l i s t ( xx [ index2 ] ) +
l i s t ( yy [ index1 ] )
y =
l i s t ( xx [ index1 ] ) +
l i s t ( yy [ index2 ] )
S = 0
for
i
in
range ( n ) :
for
j
in
range ( n ) :
i f
i
!=
j :
S = S + k ( x [ i ] ,
x [ j ] ) + k ( y [ i ] ,
y [ j ] )
\
âˆ’k ( x [ i ] ,
y [ j ] ) âˆ’k ( x [ j ] ,
y [ i ] )
T . append ( S /
n
/
( n âˆ’1) )
v = np . q u a n t i l e (T ,
0 . 9 5 )
#
S t a t i s t i c s
S = 0
for
i
in
range ( n ) :
for
j
in
range ( n ) :
i f
i
!=
j :
S = S + k ( x [ i ] ,
x [ j ] ) + k ( y [ i ] ,
y [ j ] )
\
âˆ’k ( x [ i ] ,
y [ j ] ) âˆ’k ( x [ j ] ,
y [ i ] )
u = S /
n
/
( n âˆ’1)
# Display
of
the
graph
x = np . l i n s p a c e ( min ( min (T) , u ,
v ) , max(max(T) , u ,
v ) ,
200)
d e n s i t y = kde . gaussian_kde (T)
p l t . p l o t ( x ,
d e n s i t y ( x ) )
p l t . a x v l i n e ( x = u ,
c = " r " ,
l i n e s t y l e
= "âˆ’âˆ’" )
p l t . a x v l i n e ( x = v ,
c = "b" )
For the latter approach, we construct the following quantities. For m â‰¥1 sym-
metric variables and h : Em â†’R, we call the quantity
UN :=
1
 N
m


1â‰¤i1,...,imâ‰¤N
h(xi1, . . . , xim)
(5.5)
the U-statistic w.r.t. h of order m, where 	i1,...,im ranges over
 N
m

(i1, . . . , im) âˆˆ
{1, . . . , N}mâ€™s. We use this quantity for estimating the expectation E[h(X1, . . . , Xm)]
given samples x1, . . . , xN. Note that any U statistic is unbiased. In fact, we have
E[
1
 N
m


i1<...<im
h(Xi1, . . . , Xim)]
=
1
 N
m


i1<...<im
Eh(Xi1, . . . , Xim) = Eh(X1, . . . , Xm) .
We call the quantity

136
5
The MMD and HSIC
VN :=
1
N m
N

i1=1
Â· Â· Â·
N

im=1
h(xi1, . . . , xim)
the V -statistic w.r.t. h
In the following, we conduct a statistical test with the null hypothesis that X, Y
are identically distributed when m = n. Under this null hypothesis, the operations
of taking the means of EX[Â·] and EY[Â·] have the same meaning.
We can deï¬ne an unbiased estimator of M M D2 in addition to (5.4). In the fol-
lowing, we consider the unbiased estimator

MMD
2
U =
1
n(n âˆ’1)

iÌ¸= j
h(zi, z j)
for
h(zi, z j) := k(xi, x j) + k(yi, y j) âˆ’k(xi, y j) âˆ’k(x j, yi)
(5.6)
with zi = (xi, yi).
We deï¬ne
hc(z1, . . . , zc) := EZc+1Â·Â·Â·Zmh(z1, . . . , zc, Zc+1, . . . , Zm),
which is obtained by taking the expectation of the U statistics (5.5) over Zc+1, . . . ,
Zm for 1 â‰¤c â‰¤m. Moreover, we deï¬ne
Ëœhc(z1, . . . , zc) := hc(z1, . . . , zc) âˆ’Î¸
for Î¸ = E[h(Z1, . . . , Zm)].
Example 72 For (5.6), we have h2(z1, z2) = h(z1, z2) since m = 2. Under the null
hypothesis, X, Y follow the same distribution, and we have
h1(z1) = EZ2[h(z1, Z2)] = E[k(xi, X j)] + E[k(yi, Y j)] âˆ’E[k(xi, Y j)] âˆ’E[k(x j, Yi)] = 0 .
Moreover, under the null hypothesis, since Î¸ = Eh(Z1, . . . , Zm) = 0, we have
Ëœh2(z1, z2) = h(z1, z2).
Hereafter, we set the number of samples as N(= m = n).
Proposition 51 (Serï¬‚ing [27]) Suppose that the U statistics are Eh2 < âˆand that
h1(z1) is zero (degenerated). Let Î»1, Î»2, . . . be the eigenvalues of the conjugate
integral operation
L2 âˆ‹f (Â·) â†’

Ë†h2(Â·, y) f (y)dÎ·(y)

5.2 The MMD and Two-Sample Problem
137
whose kernel is Ëœh2(z1, z2)1. Then, N times the U statistics converges to the random
variable
âˆ

j=1
Î» j(Ï‡2
j âˆ’1)
as m â†’âˆ, where Ï‡2
1 , Ï‡2
2 , . . . are random variables that are independent of each
other and follow a Ï‡2 distribution with one degree of freedom.
For the proof, see Sect.5.5.2 of Serï¬‚ing [27](page 193-199).
Note that Ëœh2(z1, z2) = h(z1, z2) is given by (5.6), which is symmetric but not
nonnegative deï¬nite. Therefore, Mercerâ€™s theorem cannot be applied. However. an
integral operator is generally compact (Proposition 39), and if the kernel of an integral
operator is symmetric, then the integral operator is self-adjoint (e.g., 45). Therefore,
from Proposition 27, eigenvalues and eigenfunctions exist. However, since they are
not nonnegative deï¬nite, some eigenvalues may not be nonnegative.
In the following, we write {Î»i}âˆ
i=1 and {Ï†i(Â·)}âˆ
i=1 as the eigenvalues and eigen-
functions, respectively, of the integral operator
TËœh : L2[E, Î¼] âˆ‹f â†’

E
Ëœh2(Â·, y) f (y)dÎ·(y) âˆˆL2[E, Î·]
For the kernel h2 when Î· = P = Q. Then, we have

E
h2(x, y)Ï†i(y)dÎ·(y) = Î»iÏ†i(x)

E
Ï†i(x)Ï† j(x)dÎ·(x) = Î´i, j .
(5.7)
Utilizing Proposition 51, we ï¬nd that N 
MMD
2
U converges to the random variable
âˆ

j=1
Î» j(Ï‡2
j âˆ’1)
as the sample size N â†’âˆ.
Example 73 With two sets of 100 samples that follow the standard Gauss distribu-
tion, we obtain eigenvalues by using the method described in Sect.3.3 and construct
a distribution following the null hypothesis using the U-statistic to perform the test
(Fig.5.2 Left). We also perform the same test by doubling the standard deviation of
one pair of samples (Fig.5.2 Right).
1 The
kernel
K : E Ã— E â†’R
in
the
integral
operation
L2(E, Î·) âˆ‹f â†’K f (Â·) =

E K(Â·, x) f (x)dÎ·(x) is called a kernel of the integral operator even if it is not positive deï¬-
nite.

138
5
The MMD and HSIC
0.00
0.02
0.04
0.06
0
20 40 60 80
The Same Dist. (U-Statistics)
MMD
2
U
Density
0.00 0.02 0.04 0.06 0.08 0.10
0 10
30
50
Diï¬€erent Dist. (U-Statistics)
MMD
2
U
Density
Fig. 5.2 Test performed using the U-statistic for the two-sample problem. The same (left) and
different (right) distributions of X, Y are employed. The blue line is the statistic, and the red dotted
line is the boundary of the rejection region. We can see that the distribution obtained according to
the null hypothesis has almost the same shape as that in Fig.5.1
sigma = 1
def k ( x ,
y ) :
return np . exp ( âˆ’(x âˆ’y ) âˆ—âˆ—2
/
sigma âˆ—âˆ—2)
# Data
Generation
n = 100
x = np . random . randn ( n )
y = np . random . randn ( n )
# The
D i s t r i b u t i o n s
are
equal
#y = 2 âˆ—np . random . randn ( n ) # The
D i s t r i b u t i o n s
are
not
equal
#
D i s t r i b u t i o n
under
n u l l
h y p o t h e s i s
K = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K[ i
, j ] = k ( x [ i ] ,
x [ j ] ) + k ( y [ i ] ,
y [ j ] )
\
âˆ’k ( x [ i ] ,
y [ j ] ) âˆ’k ( x [ j ] ,
y [ i ] )
lam ,
vec = np . l i n a l g . eig (K)
lam = lam
/
n
r = 20
z = [ ]
for h in
range (10000) :
z . append ( np . longdouble ( 1 / n âˆ—( np . sum( lam [ 0 : r ]
âˆ—( np . random . c h i s q u a r e ( df = 1 ,
s i z e = r ) âˆ’1) ) ) ) )
v = np . q u a n t i l e ( z ,
0 . 9 5 )
#
S t a t i s t i c s
S = 0
for
i
in
range ( n âˆ’1) :
for
j
in
range ( i + 1 , n ) :
S = S + k ( x [ i ] ,
x [ j ] ) + k ( y [ i ] ,
y [ j ] )
\
âˆ’k ( x [ i ] ,
y [ j ] ) âˆ’k ( x [ j ] ,
y [ i ] )
u = np . longdouble ( S /
n
/
( n âˆ’1) )
x = np . l i n s p a c e ( min ( min ( z ) , u ,
v ) , max(max( z ) , u ,
v ) ,
200)
# Display
of
the
graph
d e n s i t y = kde . gaussian_kde ( z )
p l t . p l o t ( x ,
d e n s i t y ( x ) )
p l t . a x v l i n e ( x = v ,
c = " r " ,
l i n e s t y l e
= "âˆ’âˆ’" )
p l t . a x v l i n e ( x = u ,
c = "b" )

5.3 The HSIC and Independence Test
139
5.3
The HSIC and Independence Test
Let (E, F, P) be a probability space. We say that events A, B âˆˆF are independent
if P(A)P(B) = P(A âˆ©B).
Suppose that sequences x1, . . . , xN âˆˆR and y1, . . . , yN âˆˆR with the same length
N â‰¥1 have occurred according to the distributions of the random variables X, Y. We
wish to test the independence of X, Y, where both xi, x j and yi, y j are independent,
but we do not know whether xi, yi are independent.
For example, if the empirical correlation coefï¬cient
Ë†Ï =
(1/N) N
i=1(xi âˆ’Â¯x) N
i=1(yi âˆ’Â¯y)
{(1/N) N
i=1(xi âˆ’Â¯x)2}1/2{(1/N) N
i=1(yi âˆ’Â¯y)2}1/2
is close to zero for Â¯x := (1/N) N
i=1 xi and Â¯y := (1/N) N
i=1 yi, then we may say
that the variables are independent.
Example 74 (Gaussian Distribution) For simplicity, we assume that X, Y follows
the standard Gaussian distribution. If X, Y are independent (written as X âŠ¥âŠ¥Y), then
their covariance
E[XY] =

E

E
xy fXY (x, y)dxdy =

E

E
xy fX(x) fY (y)dxdy =

E
x fX(x)dx

E
y fY (y)dy
is 0, and ÏXY = E[XY] = 0 follows. Since we can write fXY(x, y) as
1
2
	
1 âˆ’Ï2
XY
exp{âˆ’
1
2(1 âˆ’Ï2
XY)(x2 âˆ’2ÏXY xy + y2)} ,
we see that ÏXY = 0 implies fXY(x, y) = fX(x) fY(y). Hence, ÏXY = 0 â‡â‡’X âŠ¥âŠ¥
Y follows.
However, as in the following derivation, in general, ÏXY = 0 does not mean that
X âŠ¥âŠ¥Y.
Example 75 Let X = cos Î¸ and Y = sin Î¸. We uniformly generate random vari-
ables 0 â‰¤Î¸ < 2Ï€, which means that (X, Y) is uniform over the unit circle. If X is
determined, then Y = Â±
âˆš
1 âˆ’X2, and if one of X, Y is determined, then at most
two possibilities exist for the other. Therefore, the two variables are not independent.
However, since the mean of X, Y is Î¼X = Î¼Y = 0, the covariance can be calculated
as
EXY[(X âˆ’Î¼X)(Y âˆ’Î¼Y)] = EXY[XY] = EXY[cos Î¸ sin Î¸] = 1
2EXY[sin 2Î¸] = 0
and the correlation coefï¬cient ÏXY is 0.

140
5
The MMD and HSIC
To this end, Gretton et al. [12] thought that to test the independence of random
variables X, Y, if we map E âˆ‹X â†’kX(X, Â·) âˆˆHX and E âˆ‹Y â†’kY(Y, Â·) âˆˆHY
for kernels kX, kY, and perform the test of independence based on the covari-
ance between kX(X, Â·) and kY(Y, Â·), such an inconvenience would not occur. They
devised a statistical test for E[kX(X, Â·)kY(Y, Â·)] = E[kX(X, Â·)]E[kY(Y, Â·)] rather than
E[XY] = E[X]E[Y]. We deï¬ne
H SIC(X, Y) := âˆ¥m XmY âˆ’m XYâˆ¥2
HXâŠ—HY âˆˆR,
which is the norm of the covariance m XmY âˆ’m XY âˆˆH1 âŠ—H2, i.e., the Hilbert-
Schmidt information criterion (HSIC). Since the HSIC is a norm, it is zero only if
m XmY âˆ’m XY âˆˆHXâŠ—Y is zero. The HSIC is the M M D2 when m P = m XmY with
m Q = m XY.
Proposition 52 (Gretton et al.[12]) When the reproducing kernels kX, kY : E â†’R
in HX, HY are both characteristic kernels, the random variables X, Y that take values
in E being independent and H SIC(X, Y) = 0 are equivalent, i.e.,
H SIC(X, Y) = 0 â‡â‡’X âŠ¥âŠ¥Y.
Proof: If kX, kY are both characteristic, then from Proposition 55 (see below), kXkY
is also characteristic. Therefore, for m X(Â·) :=

E kX(x, Â·)d PX(x) âˆˆHX. mY(Â·) :=

E kY(y, Â·)d PY(y) âˆˆHY,andm XY(Â·, â‹†) :=

E kX(x, Â·)kY(y, â‹†)d PXY(x, y) âˆˆHXâŠ—Y,
the map PXâŠ—Y âˆ‹PXY â†’m XY âˆˆHXâŠ—Y is injective. Hence, we have
X âŠ¥âŠ¥Y â‡â‡’PXY = PX PY â‡â‡’m XY = m XmY â‡â‡’H SIC(X, Y) = 0 ,
where the second â‡â‡’is due to (5.2).
â–¡
From Propositions 50 and 52, we have that
Corollary 2 both of the reproducing kernels kX, kY : E â†’R of HX, HY are char-
acteristic, and we obtain
	XY = 	Y X = 0 â‡â‡’X âŠ¥âŠ¥Y .
If we abbreviate âˆ¥Â· âˆ¥XâŠ—Y and âŸ¨Â·, Â·âŸ©XâŠ—Y as âˆ¥Â· âˆ¥and âŸ¨Â·, Â·âŸ©, respectively, then we
have
âˆ¥m XYâˆ¥2 = âŸ¨EXY[kX(X, Â·)kY(Y, Â·)], EXâ€²Y â€²[kX(Xâ€², Â·)kY(Y â€², Â·)]âŸ©
= EXYEXâ€²Y â€²[âŸ¨kX(X, Â·)kY(Y, Â·), kX(Xâ€², Â·)kY(Y â€², Â·)âŸ©
= EXY Xâ€²Y â€²[kX(X, Xâ€²)kY(Y, Y â€²)] ,

5.3 The HSIC and Independence Test
141
âŸ¨m XY, m XmYâŸ©= âŸ¨EXY[kX(X, Â·)kY(Y, Â·)], EXâ€²[kX(Xâ€², Â·)]EY â€²[kY(Y â€², Â·)]âŸ©
= EXY{EXâ€²[âŸ¨kX(X, Â·)kY(Y, Â·), kX(Xâ€², Â·)EY â€²[kY(Y â€², Â·)]âŸ©]}
= EXY{EXâ€²[kX(X, Xâ€²)]EY â€²[âŸ¨kY(Y, Â·), kY(Y â€², Â·)âŸ©]}
= EXY{EXâ€²[kX(X, Xâ€²)]EY â€²[kY(Y, Y â€²)]} ,
and
âˆ¥m XmYâˆ¥2 = âŸ¨EX[kX(X, Â·)]EY[kY(Y, Â·)], EXâ€²[kX(Xâ€², Â·)]EY â€²[kY(Y â€², Â·)]âŸ©
= EXEXâ€²[kX(X, Xâ€²)]EYEY â€²[kY(Y, Y â€²)] ,
where X, Xâ€² (Y, Y â€²) are independent and follow the same distribution. Hence, we
can write H SIC(X, Y) as
H SIC(X, Y) := âˆ¥m XY âˆ’m XmYâˆ¥2
= EX Xâ€²YY â€²[kX(X, Xâ€²)kY(Y, Y â€²)] âˆ’2EXY{EXâ€²[kX(X, Xâ€²)]EY â€²[kY(Y, Y â€²)]}
+EX Xâ€²[kX(X, Xâ€²)]EYY â€²[kY(Y, Y â€²)] .
(5.8)
When applying the HSIC, we often construct the following estimator, replacing
the mean by the relative frequency.

H SIC := 1
N 2

i

j
kX(xi, x j)kY(yi, y j) âˆ’2
N 3

i

j
kX(xi, x j)

h
kY(yi, yh)
+ 1
N 4

i

j
kX(xi, x j)

h

r
kY(yh, yr)
(5.9)
For example, we can write the HSIC in the Python as follows.
def HSIC_1 ( x ,
y ,
k_x ,
k_y ) :
n = len ( x )
S = 0
for
i
in
range ( n ) :
for
j
in
range ( n ) :
S = S + k_x ( x [ i ] ,
x [ j ] ) âˆ—k_y ( y [ i ] ,
y [ j ] )
T = 0
for
i
in
range ( n ) :
T_1 = 0
for
j
in
range ( n ) :
T_1 = T_1 + k_x ( x [ i ] ,
x [ j ] )
T_2 = 0
for
l
in
range ( n ) :
T_2 = T_2 + k_y ( y [ i ] ,
y [ l ] )
T = T + T_1 âˆ—T_2
U = 0
for
i
in
range ( n ) :
for
j
in
range ( n ) :
U = U + k_x ( x [ i ] ,
x [ j ] )
V = 0
for
i
in
range ( n ) :
for
j
in
range ( n ) :
V = V + k_y ( y [ i ] ,
y [ j ] )
return S / nâˆ—âˆ—2âˆ’2âˆ—T / nâˆ—âˆ—3+Uâˆ—V/ nâˆ—âˆ—4

142
5
The MMD and HSIC
We often write the statistics as 
H SIC =
1
N 2 trace(K X H KY H), where K X =
(kX(xi, x j))i, j, KY = (kY(yi, y j))i, j, H := I âˆ’1
N E, I âˆˆRNÃ—N is the unit matrix,
and E âˆˆRNÃ—N is a matrix in which all the elements are ones. In fact, we have
trace(K X H KY H) =

i
(K X H KY H)i,i =

i

j
(K X H)i, j(KY H) j,i
=

i

j
{

h
kX(xi, xh)(Î´h, j âˆ’1
N )}{

h
kY (y j, yh)(Î´h,i âˆ’1
N )}
=

i

j
{kX(xi, x j)kY (yi, y j) âˆ’1
N kX(xi, x j)

h
kY (yi, yh)
âˆ’1
N kY (yi, y j)

h
kX(xi, xh) + 1
N 2

h
kX(xi, xh)

r
kY (y j, yr)}
=

i

j
kX(xi, x j)kY (yi, y j) âˆ’2
N

i

j
kX(xi, x j)

h
kY (yi, yh)
+ 1
N 2

i

h
kX(xi, xh)

j

r
kY (y j, yr) .
def HSIC_1 ( x ,
y ,
k_x ,
k_y ) :
n = len ( x )
K_x = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K_x [ i ,
j ] = k_x ( x [ i ] ,
x [ j ] )
K_y = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K_y [ i ,
j ] = k_y ( y [ i ] ,
y [ j ] )
E = np . ones ( ( n ,
n ) )
H = np . i d e n t i t y ( n ) âˆ’E / n
return np . sum( np . diag ( np . diag ( K_x . dot (H) . dot ( K_y ) . dot (H) ) ) )
/
nâˆ—âˆ—2
Example 76 We execute the above process for Ïƒ 2 = 1 and
kX(x, y) = kY(x, y) = exp(âˆ’1
2Ïƒ 2 ||x âˆ’y||2)
(Gaussian kernel) as follows.
def k_x ( x ,
y ) :
return np . exp(âˆ’np . l i n a l g . norm ( x âˆ’y ) âˆ—âˆ—2/2)
k_y = k_x
k_z = k_x
n = 100
for
a
in
[0 ,
0. 1 ,
0. 2 ,
0. 4 ,
0. 6 ,
0 . 8 ] :
# a
i s
the
c o r r e l a t i o n
x = np . random . randn ( n )
z = np . random . randn ( n )
y = a âˆ—x + np . s q r t (1 âˆ’a âˆ—âˆ—2) âˆ—z
print ( HSIC_1 ( x ,
y ,
k_x ,
k_y ) )

5.3 The HSIC and Independence Test
143
0.0006847868161461435
0.004413058917908441
0.004693757443490376
0.01389332860758824
0.010176397492526468
0.0364733529032461
We deï¬ne the HSIC ||m XY âˆ’m XmY||2 when we test X âŠ¥âŠ¥Y for m X =
EX[kX(X, Â·)],mY = EY[kY(Y, Â·)],andm XY = EXY[kX(X, Â·)kY(Y, Â·)].Ifwetest X âŠ¥
âŠ¥{Y, Z} between X and {Y, Z}, then we extend the HSIC to ||m XY Z âˆ’m XmY Z||2.
For the MMD, we test whether the simultaneous probability of X, Y, Z and the prod-
uct of the probabilities of X and (Y, Z) are equal. Therefore, we can change kY(y, Â·)
to kY(y, Â·)kZ(z, Â·). We add arguments to the function HSIC_1 to construct the function
HSIC_2 and perform the following operations.
def HSIC_2 ( x ,
y ,
z ,
k_x ,
k_y ,
k_z ) :
n = len ( x )
S = 0
for
i
in
range ( n ) :
for
j
in
range ( n ) :
S = S + k_x ( x [ i ] ,
x [ j ] )
âˆ—k_y ( y [ i ] ,
y [ j ] )
\
âˆ—k_z ( z [ i ] ,
z [ j ] )
T = 0
for
i
in
range ( n ) :
T_1 = 0
for
j
in
range ( n ) :
T_1 = T_1 + k_x ( x [ i ] ,
x [ j ] )
T_2 = 0
for
l
in
range ( n ) :
T_2 = T_2 + k_y ( y [ i ] ,
y [ l ] )
âˆ—k_z ( z [ i ] ,
z [ j ] )
T = T + T_1 âˆ—T_2
U = 0
for
i
in
range ( n ) :
for
j
in
range ( n ) :
U = U + k_x ( x [ i ] ,
x [ j ] )
V = 0
for
i
in
range ( n ) :
for
j
in
range ( n ) :
V = V + k_y ( y [ i ] ,
y [ j ] )
âˆ—k_z ( z [ i ] ,
z [ j ] )
return S / nâˆ—âˆ—2âˆ’2âˆ—T / nâˆ—âˆ—3+Uâˆ—V/ nâˆ—âˆ—4
The smaller the value of 
H SIC is, the more likely independence is, but for
random variables X, Y,U, V , the condition 
H SIC(X, Y) < 
H SIC(U, V ) does not
mean that X, Y is closer to independence than U, V . However, in practice, the HSIC
is often used as the criterion to measure the certainty of independence.
Example 77 (LiNGAM [16, 28]) We wish to know the cause-and-effect relation
among the random variables X, Y, Z from their independent N realizations x, y, z.
For example, we assume that X, Y are generated based on either Model 1 (in which
X = e1 and Y = aX + e2 for a constant a âˆˆR and zero-mean independent variables
e1, e2) or Model 2 (in which Y = eâ€²
1 and X = aâ€²Y + eâ€²
2 for a constant aâ€² âˆˆR and zero-
mean independent variables eâ€²
1, eâ€²
2). We choose the model with a higher probability

144
5
The MMD and HSIC
between e1 âŠ¥âŠ¥e2 and eâ€²
1 âŠ¥âŠ¥eâ€²
2. Then, we can apply the function HSIC_1, where e2, eâ€²
2
are calculated from y âˆ’ax, x âˆ’aâ€²y. For example, using the function
def
cc ( x ,
y ) :
return np . sum( np . dot ( x . T ,
y ) )
/
len ( x )
def
f ( u ,
v ) :
return u âˆ’cc ( u ,
v ) / cc ( v ,
v ) âˆ—v
we can estimate a and aâ€² via f(y,x) and f(x,y), respectively. When we have three
variables X, Y, Z, we ï¬rst determine the upstream variable. To this end, using the
function HSIC_2, we compare three independence cases: between x and its residue
(f(y,x), f(z,x)), between y and its residue (f(z,y), f(x,y)), and between z and
its residue (f(x,z), f(y,z)). For example, if we choose the ï¬rst pair, then X is the
upstream variable.
Then, we choose the midstream variable among the unselected two variables.
For example, if X is selected in the ï¬rst round, then we compare two independence
sets f(y_x,z_xy) and f(z_x,y_zx). If we use the notation of the program, these are
y_x=f(y,x) and z_xy=f(z_x,y_x).
# Data
Generation
n = 30
x = np . random . randn ( n ) âˆ—âˆ—2âˆ’np . random . randn ( n ) âˆ—âˆ—2
y = 2 âˆ—x + np . random . randn ( n ) âˆ—âˆ—2 âˆ’np . random . randn ( n ) âˆ—âˆ—2
z = x + y +np . random . randn ( n ) âˆ—âˆ—2 âˆ’np . random . randn ( n ) âˆ—âˆ—2
x = x âˆ’np . mean ( x )
y = y âˆ’np . mean ( y )
z = z âˆ’np . mean ( z )
# Estimate
the
Upstream
x_y = f ( x ,
y ) ;
y_z = f ( y ,
z ) ;
z_x = f ( z ,
x )
x_z = f ( x ,
z ) ;
z_y = f ( z ,
y ) ;
y_x = f ( y ,
x )
v1 = HSIC_2 ( x ,
y_x ,
z_x ,
k_x ,
k_y ,
k_z )
v2 = HSIC_2 ( y ,
z_y ,
x_y ,
k_y ,
k_z ,
k_x )
v3 = HSIC_2 ( z ,
x_z ,
y_z ,
k_z ,
k_x ,
k_y )
i f
v1 < v2 :
i f
v1 < v3 :
top = 1
e l s e :
top = 3
e l s e :
i f
v2 < v3 :
top = 2
e l s e :
top = 3
# Estimate
the
DownStream
x_yz = f ( x_y ,
z_y )
y_zx = f ( y_z ,
x_z )
z_xy = f ( z_x ,
y_x )
i f
top == 1:
v1 = HSIC_1 ( y_x ,
z_xy ,
k_y ,
k_z )
v2 = HSIC_1 ( z_x ,
y_zx ,
k_z ,
k_y )
i f
v1 < v2 :
middle = 2
bottom = 3
e l s e :

5.3 The HSIC and Independence Test
145
middle = 3
bottom = 2
i f
top == 2:
v1 = HSIC_1 ( z_y ,
x_yz ,
k_y ,
k_z )
v2 = HSIC_1 ( x_y ,
z_xy ,
k_z ,
k_y )
i f
v1 < v2 :
middle = 3
bottom = 1
e l s e :
middle = 1
bottom = 3
i f
top == 3:
v1 = HSIC_1 ( z_y ,
x_yz ,
k_z ,
k_x )
v2 = HSIC_1 ( x_y ,
z_xy ,
k_x ,
k_z )
i f
v1 < v2 :
middle = 1
bottom = 2
e l s e :
middle = 2
bottom = 1
# Display
the
R e s u l t
print ( " top = " ,
top )
print ( " middle = " ,
middle )
print ( " bottom = " ,
bottom )
top =
1
middle =
3
bottom =
2
In the following, as in the case of the two-sample problem, we construct the
distribution under the null hypothesis X âŠ¥âŠ¥Y in two ways.
1. By shifting either x1, . . . , xN or y1, . . . , yN to make X, Y independent, repeatedly
obtain the resulting 
H SIC values to create a histogram that expresses the null
hypothesis (permutation test).
2. Compute the (asymptotic) distribution from a statistic whose asymptotic distri-
bution is known (U-statistic).
We perform the test by using the procedure shown in the following example.
Example 78 The following procedure randomly rearranges the order of one of the
two nonindependent sequences to make them independent, estimates the null distri-
bution of the HSIC values and tests whether they are independent or not (Fig.5.3).
## Enumerate x and show
the
d i s t r i b u t i o n
of HSIC as a histogram
##
## Data
Generation ##
x = np . random . randn ( n )
y = np . random . randn ( n )
u = HSIC_1 ( x ,
y ,
k_x ,
k_y )
## Enumerate x and
c o n s t r u c t
the
n u l l
h y p o t h e s i s
m = 100
w = [ ]
for
i
in
range (m) :
x = x [ np . random . choice ( n ,
n ,
r e p l a c e = False ) ]
w. append ( HSIC_1 ( x ,
y ,
k_x ,
k_y ) )
## Set
the
r e j e c t i o n
region

146
5
The MMD and HSIC
v = np . q u a n t i l e (w,
0 . 9 5 )
x = np . l i n s p a c e ( min ( min (w) , u ,
v ) , max(max(w) , u ,
v ) ,
200)
## Graphical
Output
d e n s i t y = kde . gaussian_kde (w)
p l t . p l o t ( x ,
d e n s i t y ( x ) )
p l t . a x v l i n e ( x = v ,
c = " r " ,
l i n e s t y l e
= "âˆ’âˆ’" )
p l t . a x v l i n e ( x = u ,
c = "b" )
Now, let us use the unbiased estimate of the HSIC, 
H SICU, to ï¬nd the theoretical
asymptotic distribution according to the null hypothesis. Noting that

H SIC = 1
N 4
N

i=1
N

j=1
N

q=1
N

r=1
h(zi, z j, zq, zr)
for zi = (xi, yi), we have
h(zi, z j, zq, zr)= 1
4!
i, j,h,r

(t,u,v,w)
{kX(xt, xu)kY (yt, yu)+kX(xt, xu)kY (yv, yw)âˆ’2kX(xt, xu)kY (yt, yv)} ,
where i, j,q,r
(t,u,v,w) denotes the sum such that (i, j, q,r) ranges over (t, u, v, w) =
(i, j, h,r), i.e., the sum over the permutations of (i, j, h,r). If we modify this esti-
mate to make it an unbiased estimator, we obtain

H SICU =
1
 N
4


i< j<q<r
h(zi, z j, zq, zr),
where 
i, j,q,r is the sum that ranges over 1 â‰¤i, j, q,r â‰¤N without any overlap.
For example, we can construct the program as follows. Since the program con-
sumes memory, the number of samples should be limited to 100 or less. Additionally,
0.001
0.003
0.005
0.007
0 100
300
HSIC (Permutation, Independent)
HSIC
Density
0.001
0.003
0.005
0.007
0 100
300
HSIC (Permutation, Dependent)
HSIC
Density
Fig. 5.3 The distribution follows the null hypothesis when using the unbiased estimator 
H SICU
of the HSIC. The blue line is the statistic, and the red dotted line is the boundary with the rejection
region

5.3 The HSIC and Independence Test
147
since the estimator is different from 
H SIC, it produces different values for the same
data. The values of 
H SICU are smaller than those of 
H SIC.
def h ( i ,
j ,
q ,
r ,
x ,
y ,
k_x ,
k_y ) :
M =
l i s t ( i t e r t o o l s . combinations ( [ i ,
j ,
q ,
r ] ,
4) )
m = len (M)
S = 0
for
j
in
range (m) :
t = M[ j ] [ 0 ]
u = M[ j ] [ 1 ]
v = M[ j ] [ 2 ]
w = M[ j ] [ 3 ]
S = S + k_x ( x [ t ] ,
x [ u ] ) âˆ—k_y ( y [ t ] ,
y [ u ] ) \
+ k_x ( x [ t ] ,
x [ u ] ) âˆ—k_y ( y [ v ] ,
y [w] )
\
âˆ’2 âˆ—k_x ( x [ t ] ,
x [ u ] ) âˆ—k_y ( y [ t ] ,
y [ v ] )
return S / m
def HSIC_U( x ,
y ,
k_x ,
k_y ) :
M =
l i s t ( i t e r t o o l s . combinations ( range ( n ) ,
4) )
m = len (M)
S = 0
for
j
in
range (m) :
S = S + h (M[ j ] [ 0 ] , M[ j ] [ 1 ] , M[ j ] [ 2 ] , M[ j ] [ 3 ] ,
x ,
y ,
k_x ,
k_y )
return S /
math . comb ( n ,
4)
The function h1(Â·) is the zero function. For h2(Â·, Â·), we use the following formula.
Proposition 53 (Chwialkowski-Gretton[5]) Let
ËœkX(x, xâ€²) = kX(x, xâ€²) âˆ’EXâ€²kX(x, Xâ€²) âˆ’EXkX(X, xâ€²) + EX Xâ€²kX(X, Xâ€²)
and
ËœkY(y, yâ€²) = kY(y, yâ€²) âˆ’EY â€²kY(y, Y â€²) âˆ’EYkY(Y, yâ€²) + EYY â€²kY(Y, Y â€²) .
Then, h2(Â·, Â·) is given by
h2(z, zâ€²) = 1
6
ËœkX(x, xâ€²)ËœkY(y, yâ€²)
z = (x, y), zâ€² = (xâ€², yâ€²).
Proof: The derivation is due to simple transformations. See the original paper for the
proof.
Mercerâ€™s theorem is not applicable since the kernel h2 of the integral operator is
not nonnegative deï¬nite. However, since the kernel is symmetric and its integral oper-
ator is self-adjoint, eigenvalues {Î»i} and eigenfunctions {Ï†i} exist (Proposition27).
Therefore, as in the case involving the two-sample problem, the null distribution
can be calculated by using Proposition 51. Moreover, the mean of h2 is zero, i.e.,
Ëœh2 = h2.

148
5
The MMD and HSIC
-0.001 0.000 0.001 0.002 0.003
0 200
600
HSIC (U-Statistic, Independent)
HSICU
Density
0.0000.0010.0020.0030.0040.005
0
400
800 1200
HSIC (U-Statistics, Dependent)
HSICU
Density
Fig. 5.4 The distribution follows the null hypothesis when using the unbiased estimator of the
HSIC, i.e., 
H SICU. The blue line is the statistic, and the red dotted line is the boundary with the
rejection region. The distribution of the null hypothesis is different from that of the estimator 
H SIC
used in the permutation test. In particular, when X, Y are independent, the unbiased estimator can
take a negative value because the true value of the HSIC is zero
Example 79 Calculate the eigenvalues of the Gram matrix of the positive deï¬nite
kernel h2 and divide them by N to obtain the desired eigenvalues (Sect.3.3). Then,
ï¬nd the distribution that follows the null hypothesis and calculate the rejection region.
We construct the following program and execute it. We input a random number that
follows a Gaussian distribution with N = 100 samples. In Fig.5.4, the left panel
shows a correlation coefï¬cient of 0, and the right panel shows a correlation coefï¬cient
of 0.2.
sigma = 1
def k ( x ,
y ) :
return np . exp ( âˆ’(xâˆ’y ) âˆ—âˆ—2/ sigma âˆ—âˆ—2)
k_x = k ;
k_y = k
## Data
Generation
n = 100; x = np . random . randn ( n )
a = 0
# Independent
#a=0.2
##
C o r r e l a t i o n
0.2
y = aâˆ—x + np . s q r t (1 âˆ’a âˆ—âˆ—2) âˆ—np . random . randn ( n )
# y=rnorm ( n ) âˆ—2
## The
d i s t r i b u t i o n s
are
not
equal
## Null
Hypothesis
K_x = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K_x [ i
, j ] = k_x ( x [ i ] ,
x [ j ] )
K_y = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K_y [ i ,
j ] = k_y ( y [ i ] ,
y [ j ] )
F = np . zeros ( n )
for
i
in
range ( n ) :
F [ i ] = np . sum( K_x [ i ,
: ] )
/
n
G = np . zeros ( ( n ) )
for
i
in
range ( n ) :
G[ i ] = np . sum( K_y [ i ,
: ] )
/
n
H = np . sum( F )
/
n
I = np . sum(G)
/
n
K = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K[ i ,
j ] = ( K_x [ i ,
j ] âˆ’F [ i ] âˆ’F [ j ] + H)
\

5.3 The HSIC and Independence Test
149
âˆ—( K_y [ i ,
j ] âˆ’G[ i ] âˆ’G[ j ] + I )
/
6
r = 20
lam ,
vec = np . l i n a l g . eig (K)
lam = lam
/
n
print ( lam )
z = [ ]
for
s
in
range (10000) :
z . append ( 1 / n âˆ—( np . sum( lam [ 0 : r ] âˆ—( np . random . c h i s q u a r e ( df = 1 ,
s i z e = r ) âˆ’1) )
) )
v = np . q u a n t i l e ( z ,
0 . 9 5 )
##
S t a t i s t i c s
u = HSIC_U( x ,
y ,
k_x ,
k_y )
## Graphical
Output
x = np . l i n s p a c e ( min ( min ( z ) , u ,
v ) , max(max( z ) , u ,
v ) ,
200)
d e n s i t y = kde . gaussian_kde ( z )
p l t . p l o t ( x ,
d e n s i t y ( x ) )
p l t . a x v l i n e ( x = v ,
c = " r " ,
l i n e s t y l e
= "âˆ’âˆ’" )
p l t . a x v l i n e ( x = u ,
c = "b" )
[9.43372312e-03 7.60184058e-03 6.84791271e-03 4.71833989e-03
2.91610998e-03 2.58505938e-03 2.57260480e-03 2.14716454e-03
1.52927314e-03 1.40680064e-03 1.30527061e-03 1.03809542e-03
8.17408141e-04 6.63885663e-04 5.98972166e-04 5.18116129e-04
4.66211054e-04 3.33278079e-04 3.08102237e-04 2.52215392e-04
2.08148331e-04 2.01305856e-04 1.61855802e-04 1.24737146e-04
9.77431144e-05 9.05299170e-05 6.74173717e-05 5.94551451e-05
4.79841522e-05 3.68125212e-05 3.47962810e-05 2.59468073e-05
2.27016212e-05 1.75277782e-05 1.44134018e-05 1.34476814e-05
1.07720204e-05 8.32532742e-06 8.02962726e-06 6.61660031e-06
6.03329520e-06 4.82571716e-06 4.39769787e-06 3.07993953e-06
2.92646288e-06 1.93843247e-06 1.43720831e-06 1.32156280e-06
1.21961656e-06 9.39545869e-07 8.74760924e-07 6.82246149e-07
6.17680338e-07 5.35249706e-07 4.13572087e-07 2.89912505e-07
2.63638065e-07 2.13422066e-07 1.43007791e-07 1.39668484e-07
1.11861810e-07 9.06782617e-08 8.18246166e-08 5.59605198e-08
4.96061526e-08 4.03184819e-08 3.74865518e-08 1.94370484e-08
1.55836383e-08 1.28960568e-08 9.98179017e-09 6.29967857e-09
4.60116540e-09 3.71943499e-09 2.78758512e-09 2.47338744e-09
1.54467729e-09 1.31346990e-09 8.12941854e-10 4.71936633e-10
3.62355934e-10 2.28630320e-10 1.71319496e-10 8.99150656e-11
5.81595026e-11 4.02249130e-11 2.95420131e-11 1.60077238e-11
1.00890685e-11 7.29486504e-12 3.89343807e-12 2.24468397e-12
1.62584385e-12 9.95269698e-13 6.98421746e-13 1.93405677e-13
4.44822157e-14 3.34480294e-14 3.34678155e-15 1.12240660e-14]
Instead of writing the HSIC as âˆ¥m XY âˆ’m XmYâˆ¥2
HXâŠ—HY , by using the HS norm, we
may write it as âˆ¥	XYâˆ¥2
H S or âˆ¥	Y Xâˆ¥2
H S. In fact, if {eX,i} and {eY, j} are orthonormal
bases of HX, HY, respectively, then from the deï¬nition of âˆ¥Â· âˆ¥H S (Sect.2.6) and
(5.1), we have

150
5
The MMD and HSIC
âˆ¥	Y Xâˆ¥2
H S =
âˆ

i=1
âˆ¥	Y XeX,iâˆ¥2
HY =
âˆ

i=1
âˆ

j=1
âŸ¨eY, j, 	Y XeX,iâŸ©2
HY
=
âˆ

i=1
âˆ

j=1
âŸ¨eX,i âŠ—eY, j, m XY âˆ’m XmYâŸ©2
HXâŠ—HY = âˆ¥m XY âˆ’m XmYâˆ¥2
HXâŠ—HY .
Similarly, âˆ¥	XYâˆ¥2
H S has the same value.
5.4
Characteristic and Universal Kernels
Let H and k be an RKHS and its reproducing kernel, respectively. Let P be the set
of distributions that a random variable X follows. Then, we can deï¬ne the map
P âˆ‹Î¼ â†’

k(x, Â·)dÎ¼(x) âˆˆH ,
which we call the embedding of probabilities in the RKHS.
Since each element of H is generated by k(x, Â·) (x âˆˆE) or can be written as the
limit of its sequence, we describe the condition as

E
k(x, y)dÎ¼(x) = 0 , y âˆˆE =â‡’Î¼ = 0
for Î¼ := Î¼1 âˆ’Î¼2. Moreover,

E k(x, y)dÎ¼(x) = 0 (y âˆˆE) implies that

E

E
k(x, y)dÎ¼(x)dÎ¼(y) = 0 .
(5.10)
If we use k(x, y) = Ï†(x âˆ’y) =

E
ei(xâˆ’y)wdÎ·(w) (Proposition 5), then we can write
(5.10) as

E
|

E
eiwxdÎ¼(x)|2dÎ·(w) = 0 .
In other words, for the measure Î·,
Ë†Î¼(w) :=

E
eiwxdÎ¼(x) = 0
(5.11)
almost surely holds.
In the following, let Î· be a ï¬nite measure. We call the set of x âˆˆE such that
Î·(U(x, Ïµ)) > 0 for any Ïµ > 0 the support of Î· and denote it by E(Î·). We note that
the support of a ï¬nite measure is always a closed set. In fact, for x âˆˆE\E(Î·), if

5.4 Characteristic and Universal Kernels
151
the radius Ïµ of the open set U(x, Ïµ) is sufï¬ciently small, then E(Î·),U(x, Ïµ) has no
intersection.
Here, if E(Î·) = E, then (5.11) means that Î¼ = 0, i.e., Î¼1 = Î¼2. On the other
hand, if E(Î·) âŠŠE, then a Î¼ Ì¸= 0 exists such that (5.11) holds.
Proposition 54 k(x, y) = Ï†(x âˆ’y) is characteristic if and only if the support of
the ï¬nite measure Î· of k(x âˆ’y) =

E ei(xâˆ’y)wdÎ·(w) coincides with E.
For the proof of necessity, see the Appendix at the end of this chapter.
Example 80 The Gaussian kernel in Example 19 and the Laplace kernel in Example
20 are zero-mean Gaussian- and Laplace-distributed, respectively, and the support is
the entire interval. Therefore, they are characteristic kernels. On the other hand, the
kernel
k(x, y) = Ï†(x âˆ’y)
obtained from the characteristic function
Ï†(t) = 2(1 âˆ’cos(at))
a2t2
whose probability distribution is a triangular distribution,
f (x) =

(1 âˆ’|x|
a )/a, |x| < a
0,
Otherwise
is not a characteristic kernel if its support is not equal to E.
Proposition 55 If the reproducing kernel k1, k2 expressed by the bivariate difference
between RKHSs H1, H2 are both characteristic, then the reproducing kernel k1k2 of
RKHS H1 âŠ—H2 is also characteristic.
Proof: If both of k1(x1, y1) = Ï†1(x1 âˆ’y1), k2(x2, y2) = Ï†2(x2 âˆ’y2) are character-
istic, then the supports of Î·1, Î·2 are E. Since k1(x1, y1)k2(x2, y2) can be expressed
by
Ï†1(x1 âˆ’y1)Ï†2(x2 âˆ’y2) =

E

E
ei(x1âˆ’y1)âŠ¤w1ei(x2âˆ’y2)âŠ¤w2dÎ·1(w1)dÎ·2(w2)
=

E

E
ei(x1âˆ’y1,x2âˆ’y2)âŠ¤(w1,w2)dÎ·1(w1)dÎ·2(w2) ,
k1, k2 is characteristic as well.
â–¡
Let E be a compact set. Suppose that the RKHS H induced by the kernel k : E Ã—
E â†’H is a dense (under the uniform norm) subset of the set C(E) of continuous
functions E â†’R. Then, we say that the kernel k is universal.
To show that the kernel k is universal, we only need to see if the correspond-
ing RKHS satisï¬es the two Stone-Weierstrass conditions (Proposition 12). Proposi-
tion 56 gives a sufï¬cient condition for the kernel k to be universal (see Chap.2 for

152
5
The MMD and HSIC
the deï¬nition of an algebra). However, for practical purposes, Corollary 3 deduced
from Proposition 56 is often used.
Proposition 56 (Steinwart [29]) Let E be compact, and let k : E Ã— E â†’R be a
continuous function with k(x, x) > 0 for x âˆˆE. If there exists an injective feature
map
 : E âˆ‹x â†’(x) = (1(x), 2(x), . . .) âˆˆl2 := {(Î±1, Î±2, . . .) âˆˆRâˆ|
âˆ

i=1
Î±2
j < âˆ}
and A := span{1, 2, . . .} is an algebra, then k is a universal kernel.
Proof: Since k(x, x) > 0, x âˆˆE, the ï¬rst condition of Proposition 12 is satisï¬ed.
Since k(Â·, Â·) is continuous, (x) = k(x, Â·) âˆˆl2 is also continuous at each x âˆˆE.
Moreover, since  is injective, the second condition of Proposition 12 is satisï¬ed,
and A is dense in C(E). Furthermore, any 
i Î±ii(Â·) âˆˆA is also an element of
RKHS H with k as the reproducing kernel. Let {ei} be an orthonormal basis of
H and f := 
i Î±iei âˆˆH. Then, âŸ¨f (Â·), (x)âŸ©= f (x) holds, which further implies
that 
i Î±ii(x) = âŸ¨
i Î±iei, 
i i(x)eiâŸ©= f (x).
â–¡
Corollary 3 The inï¬nite-dimensional polynomial kernel (Example 11) is a universal
kernel in each compact set of E.
Proof: The feature map  is injective. Moreover, A := span{m1,...,md|m1, . . . ,
md â‰¥0} is a d-variable polynomial (algebra), and from Proposition 56, the kernel k
is universal.
â–¡
Example 81 (Gaussian Kernel) The exponential type (Example6) kâˆis the univer-
sal kernel from Corollary 3. The feature map of the Gaussian kernel (Example 7) is the
(x) of kâˆdivided by Î³ (x) := k(x, x)1/2 > 0. For f âˆˆC(E), since Î³ f âˆˆC(E),
if we let âˆ¥Î³ f (Â·) âˆ’
i Î±i(Â·)âˆ¥âˆâ‰¤âˆ¥Î³ âˆ¥âˆÏµ, then we have
âˆ¥f (Â·) âˆ’

i
Î±iÎ³ âˆ’1(Â·)âˆ¥âˆâ‰¤âˆ¥Î³ âˆ¥âˆ’1
âˆâˆ¥Î³ f (Â·) âˆ’

i
Î±i(Â·)âˆ¥âˆâ‰¤Ïµ
Therefore, the Gaussian kernel is universal as well.
The necessary and sufï¬cient condition for Proposition 54 assume that the kernel
is a function of the difference between two variables. The following is a sufï¬cient
condition, but it refers to kernels in general.
Proposition 57 A universal kernel on a compact set is characteristic.
Proof: See the Appendix at the end of the chapter.
Example 82 The Gaussian kernel is characteristic. If a characteristic kernel based
on a triangular distribution (Example 80) has a support up to a distance a > 0 from
the origin and E is a compact set that includes some points outside the support, then
its kernel is not universal.

5.5 Introduction to Empirical Processes
153
5.5
Introduction to Empirical Processes
In this section, we study a mathematical approach to machine learning called the
empirical process. We analyze the accuracy of the MMD estimators by using the
Rademacher complexity and concentration inequalities. Through this example, we
learn the concept of empirical processes. The derivation performed in this section is
based on Gretton et al.â€™s [11] proof of a proposition regarding the accuracy of the
two-sample problem.
In this section, we prove the following proposition. We deï¬ne the MMD by
sup
f âˆˆF
EP[ f (X)] âˆ’EQ[ f (X)],
where F is a class of functions. This chapter also deals with the case in which
F := { f âˆˆH|âˆ¥f âˆ¥H â‰¤1}.
Proposition 58 Suppose that a kmax exists such that 0 â‰¤k(x, y) â‰¤kmax for each
x, y âˆˆE. Then, for any Ïµ > 0, we have
P

| 
M M D
2
B âˆ’M M D2| > 4kmax
N
+ Ïµ

â‰¤2 exp

âˆ’Ïµ2
N
4kmax

,
where the estimator 
M M D
2
B of M M D2 is given by (5.3), and we assume that the
number of samples for x, y is equal to N and that P Ì¸= Q.
For the proof of Proposition 58, we use an inequality that slightly generalizes
Proposition 46.
Proposition 59 (McDiarmid)Let f : Em â†’R implythataci < âˆ(i = 1, Â· Â· Â· , m)
exists satisfying
sup
x,x1,...,xm
| f (x1, . . . , xm) âˆ’f (x1, . . . .xiâˆ’1, x, xi+1, . . . , xm)| â‰¤ci .
For any probability measure P, Ïµ > 0 and X1, . . . , Xm, we have
P ( f (x1, . . . , xm) âˆ’EX1Â·Â·Â·Xm f (X1, . . . , Xm) > Ïµ) < exp

âˆ’
2Ïµ2
m
i=1 c2
i

(5.12)
and
P (| f (x1, . . . , xm) âˆ’EX1Â·Â·Â·Xm f (X1, . . . , Xm)| > Ïµ) < 2 exp

âˆ’
2Ïµ2
m
i=1 c2
i

.
(5.13)
Proof: Hereafter, we denote f (X1, Â· Â· Â· , X N) and E[ f (X1, Â· Â· Â· , X N)] by f and E[ f ],
respectively. If we deï¬ne

154
5
The MMD and HSIC
V1 := EX2Â·Â·Â·X N [ f |X1] âˆ’EX1Â·Â·Â·X N [ f ]
...
Vi := EXi+1Â·Â·Â·X N [ f |X1, Â· Â· Â· , Xi] âˆ’EXiÂ·Â·Â·X N [ f |X1, Â· Â· Â· , Xiâˆ’1]
...
VN := f âˆ’EX N [ f |X1, Â· Â· Â· , X Nâˆ’1]
for i = 1, i = 2, Â· Â· Â· , N âˆ’1, and i = N, then we have
f âˆ’EX1Â·Â·Â·X N [ f ] =
N

i=1
Vi.
(5.14)
From
EXi{EXi+1Â·Â·Â·X N [ f |X1, Â· Â· Â· , Xi]|X1, Â· Â· Â· , Xiâˆ’1} = EXiÂ·Â·Â· ,X N [ f |X1, Â· Â· Â· , Xiâˆ’1] ,
we have
EXi[Vi|X1, Â· Â· Â· , Xiâˆ’1] = 0 .
(5.15)
From (5.14), we have
f âˆ’E[ f ] > Ïµ â‡â‡’exp{t
N

i=1
Vi} > etÏµ for arbitrary t > 0 .
If we apply Markovâ€™s inequality (Lemma 6) to the latter equation, then we have
P( f âˆ’E[ f ] â‰¥Ïµ) â‰¤inf
t>0 eâˆ’tÏµE[exp{t
N

i=1
Vi}] .
(5.16)
Moreover, from (5.15), we apply Lemma 7 to obtain
E[exp{t
N

i=1
Vi}] = EX1Â·Â·Â·X Nâˆ’1[exp{t
Nâˆ’1

i=1
Vi}EX N [exp{tVN}|X1, Â· Â· Â· , X Nâˆ’1]]
â‰¤EX1Â·Â·Â·X Nâˆ’1[exp{t
Nâˆ’1

i=1
Vi}] exp{t2c2
N/8}
= exp{t2
8
N

i=1
c2
i } .

5.5 Introduction to Empirical Processes
155
Therefore, from (5.16), we have
P( f âˆ’E[ f ] â‰¥Ïµ) â‰¤inf
t>0 exp{âˆ’tÏµ + t2
8
N

i=1
c2
i } .
The right-hand side is minimized when t = 4Ïµ/ N
i=1 c2
i , and we obtain (5.12).
Replacing f with âˆ’f , we obtain the other inequality. From both inequalities, we
have (5.13).
â–¡
In the following, we denote by
F := { f âˆˆH|âˆ¥f âˆ¥H â‰¤1}
the unit ball in the universal (see Sect.5.4 for the deï¬nition of universality) RKHS
H w.r.t. a compact E and assume that the kernel of H is less than or equal to kmax.
Hereafter, let X1, . . . .Xm be independent random variables that follow probability
P, and let Ïƒ1, . . . , Ïƒm be independent random variables, each of which takes a value
of Â±1 equiprobably. Then, we say that the quantity
RN(F) := EÏƒ sup
f âˆˆF
| 1
m
m

i=1
Ïƒi f (xi)|
(5.17)
is an empirical Rademacher complexity, where EÏƒ is the operation that takes the
expectation w.r.t. Ïƒ1, . . . , Ïƒm. If we further take the expectation of (5.17) w.r.t. the
probability P, then we call the obtained value R(F, P) the Rademacher complexity.
Proposition 60 (Bartlett-Mendelson[4]) Let kmax := maxx,yâˆˆE k(x, y). Then, we
have the following inequality:
RN(F) â‰¤

kmax
N .
In particular, for an arbitrary probability P, we have
R(F, P) â‰¤

kmax
N .
Proof: From âˆ¥f âˆ¥H â‰¤1 and k(x, x) â‰¤kmax, we have
RN(F) = EÏƒ[sup
f âˆˆF
| 1
N
N

i=1
Ïƒi f (xi)|] = EÏƒ[sup
f âˆˆF
| 1
N
N

i=1
ÏƒiâŸ¨k(xi, Â·), f (Â·)âŸ©H|]
= EÏƒ[sup
f âˆˆF
|âŸ¨f, 1
N
N

i=1
Ïƒik(xi, Â·)âŸ©H|]

156
5
The MMD and HSIC
â‰¤EÏƒ[sup
f âˆˆF
âˆ¥f âˆ¥H



âŸ¨1
N
N

i=1
Ïƒik(xi, Â·), 1
N
N

i=1
Ïƒik(xi, Â·)âŸ©H]
â‰¤EÏƒ[



 1
N 2
N

i=1
N

j=1
ÏƒiÏƒ jk(xi, x j)] â‰¤



EÏƒ[ 1
N 2
N

i=1
N

j=1
ÏƒiÏƒ jk(xi, x j)]
=



 1
N 2
N

i=1
N

j=1
Î´i, jk(xi, x j) â‰¤

kmax
N
,
where we use
E[ÏƒiÏƒ j] = Ïƒ 2
i Î´i, j = Î´i, j
in the derivation. We obtain the other inequality by taking the expectation w.r.t. the
probability P.
â–¡
Propositions 59 and 60 are inequalities used for mathematical analysis in machine
learning as well as for the proof of Proposition 58.
Proof of Proposition 58: If we deï¬ne
f (x1, . . . , xN, y1, . . . , yN)
:= âˆ¥1
N k(x1, Â·) + . . . + 1
N k(xN, Â·) âˆ’1
N k(y1, Â·) âˆ’. . . âˆ’1
N k(yN, Â·) ,
then from the triangular inequality, we obtain
| f (x1, . . . , xN, y1, . . . , yN) âˆ’f (x1, . . . , x jâˆ’1, x, x j+1, . . . , xN, y1, . . . , yN)|
â‰¤1
N âˆ¥k(x j, Â·) âˆ’k(x, Â·)âˆ¥â‰¤2
N

kmax .
(5.18)
Next, we obtain the upper bound of the expectation of
|M M D2 âˆ’
M M D
2
B| = | sup
f âˆˆF
{EP( f ) âˆ’EQ( f )} âˆ’sup
f âˆˆF
{ 1
N
N

i=1
f (xi) âˆ’1
N
N

j=1
f (y j)}|
â‰¤sup
f âˆˆF
|EP( f ) âˆ’EQ( f ) âˆ’{ 1
N
N

i=1
f (xi) âˆ’1
N
N

j=1
f (y j)}| .
Then, we perform the following derivation:
EX,Y sup
f âˆˆF
|EP( f ) âˆ’EQ( f ) âˆ’{ 1
N
N

i=1
f (Xi) âˆ’1
N
N

i=1
f (Yi)}|
= EX,Y sup
f âˆˆF
|EXâ€²{ 1
N
N

i=1
f (Xâ€²
i) âˆ’1
N
N

i=1
f (Xi)} âˆ’EY â€²{ 1
N
N

j=1
f (Y â€²
j)) âˆ’1
N
N

j=1
f (Y j)}|

5.5 Introduction to Empirical Processes
157
â‰¤EX,Y,Xâ€²,Y â€² sup
f âˆˆF
| 1
N
N

i=1
f (Xâ€²
i) âˆ’1
N
N

i=1
f (Xi) âˆ’1
N
N

i=1
f (Y â€²
i ) + 1
N
N

i=1
f (Yi)|
= EX,Y,Xâ€²,Y â€²,Ïƒ,Ïƒ â€² sup
f âˆˆF
| 1
N
N

i=1
Ïƒi{ f (Xâ€²
i) âˆ’f (Xi)} + 1
N
N

i=1
Ïƒ â€²
i { f (Y â€²
i ) âˆ’f (Yi)}|
â‰¤EX,Xâ€²,Ïƒ sup
f âˆˆF
| 1
N
N

i=1
Ïƒi{ f (Xâ€²
i) âˆ’f (Xi)}| + EY,Y â€²,Ïƒ â€² sup
f âˆˆF
| 1
N
n

j=1
Ïƒ â€²
j{ f (Y â€²
j) âˆ’f (Y j)}|
â‰¤2[R(F, P) + R(F, Q)] â‰¤2[(kmax/N)1/2 + (kmax/N)1/2] = 4

kmax
N
,
(5.19)
where the ï¬rst inequality is due to Jensenâ€™s inequality, the second stems from the tri-
angular inequality, the third is derived from the deï¬nition of Rademacher complexity,
and the fourth due is obtained from the inequality of Rademacher complexity (Propo-
sition 60). From (5.18) and (5.19), for ci = 2
N
âˆškmax and f = M M D2 âˆ’
M M D
2,
we have
EX1...X N f â‰¤4

kmax
N
.
Finally, we obtain Proposition 58 from Proposition 59.
Hence, Proposition 60 follows from (5.18) and Proposition 59.
â–¡
Appendix
The essential part of the proof of Proposition 54 was given by Fukumizu [7] but has
been rewritten as a concise derivation to make it easier for beginners to understand.
Proof of Proposition 48
The fact that E âˆ‹x â†’k(x, Â·) âˆˆH is measurable means that E[k(X, Â·)] can be
treated as a random variable. However, the events in E Ã— E are the direct prod-
ucts of the events generated by each E (the elements of F Ã— F). Therefore, if the
function E Ã— E âˆ‹(x, y) â†’k(x, y) âˆˆR is measurable, then the function E âˆ‹y â†’
k(x, y) âˆˆR is measurable for each x âˆˆE (even if y âˆˆE is ï¬xed, (x, y) â†’k(x, y)
is still measurable). In the following, we show that any function belonging to H is
measurable. First, we note that H0 = span{k(x, Â·)|x âˆˆE} is dense in H. Addition-
ally, we note that for the sequence { fn} in H0, âˆ¥f âˆ’fnâˆ¥H â†’0 (n â†’âˆ) means that
| f (x) âˆ’fn(x)| â†’0 for each x âˆˆE (Proposition 35). The following lemma implies
that f is measurable.
Lemma 8 If fn : E â†’R is measurable and fn(x) converges to f (x) for each x âˆˆ
E, then f : E â†’R is also measurable.
Proof: The proof follows after the proof of this proposition.

158
5
The MMD and HSIC
We assume that Lemma 8 is valid. We deï¬ne the measurability of  : E âˆ‹x â†’
k(x, Â·) âˆˆH by
{x âˆˆE | âˆ¥f âˆ’k(x, Â·)âˆ¥H < Î´} âˆˆF
for any f âˆˆH and Î´ > 0 (this is an extension to the case where H = R). Moreover,
we have
âˆ¥f âˆ’k(x, Â·)âˆ¥H < Î´ â‡â‡’k(x, x) âˆ’2 f (x) < Î´2 âˆ’âˆ¥f âˆ¥2
H .
In addition, since k(Â·, Â·) is measurable, E âˆ‹x â†’k(x, x) âˆˆR is also measurable.
Moreover, since f (x) is measurable, so is k(x, x) âˆ’2 f (x). Thus,  is measurable.
â–¡
Proof of Lemma 8
It is sufï¬cient to show that f âˆ’1(B) âˆˆF for any open set B. We ï¬x B âŠ†R arbitrarily
and let Fm := {y âˆˆB|U(y, 1/m) âŠ†B}, where U(y,r) := {x âˆˆR | d(x, y) < r}.
From the deï¬nition, we have the following two equations.
f (x) âˆˆB â‡â‡’for some m, f (x) âˆˆFm
f (x) âˆˆFm â‡â‡’for some k, fn(x) âˆˆFm , n â‰¥k .
In other words, we have
f âˆ’1(B) = âˆªm f âˆ’1(Fm) = âˆªm âˆªk âˆ©nâ‰¥k f âˆ’1
n (Fm) âˆˆF .
â–¡
Proof of Proposition 49
The evaluation is ï¬nite for arbitrary g = âˆ
i=1
âˆ
j=1 eX,ieY, j âˆˆHX âŠ—HY and
(x, y) âˆˆE. In fact, we have
|g(x, y)| â‰¤
âˆ

i=1
âˆ

j=1
|ai, j| Â· |eX,i(x)| Â· |eY, j(y)|â‰¤
âˆ

i=1
|eX,i(x)| Â·
â›
â
âˆ

j=1
e2
Y, j(y)
â
â 
1/2 â›
â
âˆ

j=1
a2
i, j(y)
â
â 
1/2
,
(5.20)
where we apply Cauchy-Schwarzâ€™s inequality (2.5) to âˆ
j=1. If we set kY(y, Â·) =

j h j(y)eY, j(Â·), then from âŸ¨eY,i(Â·), kY(y, Â·)âŸ©= eY,i(y), we have hi(y) = eY,i(y) and
kY(y, Â·) = âˆ
j=1 eY, j(y)eY, j(Â·). Thus, we obtain
âˆ

j=1
e2
Y, j(y) = kY(y, y)
(5.21)

Appendix
159
and
âˆ

i=1
|eX,i(x)| Â·
â›
â
âˆ

j=1
a2
i, j
â
â 
1/2
â‰¤
â›
â
âˆ

i=1
e2
X,i(x)
â
â 
1/2 â›
â
âˆ

i=1
âˆ

j=1
a2
i, j
â
â 
1/2
=

kX(x, x)âˆ¥gâˆ¥,
(5.22)
where we apply Cauchy-Schwarzâ€™s inequality (2.5) to âˆ
i=1. Note that (5.20), (5.21),
and (5.22) imply that |g(x, y)| â‰¤âˆškX(x, x)âˆškY(y, y)âˆ¥gâˆ¥. Thus, HX âŠ—HY is an
RKHS.
From kX(x, Â·) âˆˆHX, kY(y, Â·) âˆˆHY, we have that k(x, Â·, y, â‹†) := kX(x, Â·)
kY(y, Â·) âˆˆHX âŠ—HY for k(x, xâ€², y, yâ€²) := kX(x, xâ€²)kY(y, yâ€²). From
g(x, y) =
âˆ

i=1
âˆ

j=1
ai, jeX,i(x)eY, j(y) =
âˆ

i=1
âˆ

j=1
ai, jâŸ¨eX,i(Â·), kX(x, Â·)âŸ©HX âŸ¨eY,i(â‹†), kY (y, â‹†)âŸ©HY
=
âˆ

i=1
âˆ

j=1
ai, jâŸ¨eX,i(Â·)eY, j(â‹†), k(x, Â·, y, â‹†)âŸ©H = âŸ¨
âˆ

i=1
âˆ

j=1
ai, jeX,i(Â·)eY, j(Â·), k(x, Â·, y, â‹†)âŸ©H
= âŸ¨g(Â·, â‹†), k(x, Â·, y, â‹†)âŸ©,
k is the reproducing kernel of HX âŠ—HY.
â–¡
Proof of Proposition 54 (Necessity)
Let W be an open set centered at the origin with a radii of Ïµ > 0 and w0 âˆˆE.
We assume that w0 + W has a measure of 0 and show that this contradicts another
assumption, i.e., that k(x, y) = Ï†(x âˆ’y) is a characteristic kernel. In this case, Î·
is an even function, and âˆ’w0 + W is also of measure 0 (Â±w0 + W âŠ†E\E(Î·)),
where we use the fact that g(w) := (Ïµ âˆ’âˆ¥wâˆ¥2)(d+1)/2
+
is nonnegative deï¬nite when
E = Rd (d â‰¥1) (see [8] for the proof). From Proposition 5 (Bochnerâ€™s Theorem),
there exists a ï¬nite measure Î¼ such that g(w) =

E eiwâŠ¤xÎ¼(x). Moreover, the closure
of Â±w0 + W is the support of
h(w) = g(w âˆ’w0) + g(w + w0) =

E
eiwâŠ¤x2 cos(wâŠ¤
0 x)dÎ¼(x) .
Since the support of h has no intersection with E(Î·) and Â±w0 /âˆˆW, we have h(0) =
0. Therefore, we obtain Î½(E) = 0 for
Î½(B) :=

B
2 cos(w0x)dÎ¼(x) , B âˆˆF .
Since g is not zero, Î½ is not the zero measure. Thus, using the total variation
|Î½|(B) := sup
âˆªBi=B
n

i=1
|Î½(Bi)| , B âˆˆF ,

160
5
The MMD and HSIC
F
U
0
1
fn
f1
Fig. 5.5 Proof of Proposition 57. As n grows, the slope of fn rapidly increases at the border of
F, U. Therefore, if

E fnd P =

E fndQ for all { fn}, we require P = Q (Dudley â€œReal Analysis
and Probabilityâ€[6])
where sup is the supremum when dividing F into Bi âˆˆF, we deï¬ne the constant
c := |Î½|(E) and the ï¬nite measures Î¼1 := 1
c|Î½| and Î¼2 := 1
c{|Î½| âˆ’Î½}. From Î½(E) =
0, we observe that Î¼1 and Î¼2 are both probabilities and that Î¼1 Ì¸= Î¼2. Additionally,
we have
c(dÎ¼1 âˆ’dÎ¼2) = dÎ½ = 2 cos(w0x)dÎ¼ .
From Fubiniâ€™s theorem, we can write the difference between the expectations w.r.t.
probabilities Î¼1, Î¼2 as

E
Ï†(x âˆ’y)dÎ¼1(y) âˆ’

E
Ï†(x âˆ’y)dÎ¼2(y) = 1
c

E
Ï†(x âˆ’y)2 cos(wâŠ¤
0 y)dÎ¼(y)
= 1
c

2 cos(wâŠ¤
0 y)

ei(xâˆ’y)âŠ¤wdÎ·dÎ¼(y) = 1
c

E
eixâŠ¤wh(w)dÎ·(w) .
However, since the supports of h and Î· do not intersect, the value is zero, which
contradicts the assumption that Ï†(x âˆ’y) is a characteristic kernel.
â–¡
Proof of Proposition 57
For any bounded continuous f , if

E f d P =

E f dQ holds, this implies that P = Q
(Fig.5.5). In fact, let U be an open subset of E, and let V be its complement.
Furthermore, let d(x, V ) := infyâˆˆV d(x, y) and fn(x) := min(1, nd(x, V )). Then,
fn is a bounded continuous function on E, and fn(x) â‰¤I (x âˆˆU) and fn(x) â†’
I (x âˆˆU) as n â†’âˆfor each x âˆˆR; Thus, by the monotonic convergence theorem,

E fnd P â†’P(U) and

E fndQ â†’Q(U) hold. By our assumption,

E fnd P =

E fndQ and P(U) = Q(U), i.e., P(V ) = Q(V ) holds2 In other words, every event
is guaranteed to be a closure event. Let E be a compact set. For each element g âˆˆH in
theRKHS H oftheuniversalkernel,thesameargumentfollowssincesupxâˆˆE | f (x) âˆ’
g(x)| can be arbitrarily small for any f âˆˆC(E). That is, if

gd P =

gdQ holds
for any g âˆˆH, then P = Q, so the universal kernel is characteristic.
â–¡
2 If E is compact, then for any A âˆˆF, P(A) = {P(V )|V is a closed set, V âŠ†A, V âˆˆV} (Theorem
7.1.3, Dudley [6]).

Exercises 65âˆ¼83
161
Exercises 65âˆ¼83
65. Proposition 49 can be derived according to the following steps. Which part of
the proof in the Appendix does each step correspond to?
(a) Show that |g(x, x, y, y)| â‰¤âˆškX(x, x)âˆškY(y, y)âˆ¥gâˆ¥for g âˆˆHX âŠ—HY and
x âˆˆEX, y âˆˆEY (from Proposition 33, this implies that H is some RKHS).
(b) Show that k(x, Â·, y, â‹†) := kX(x, Â·)kY(y, â‹†) âˆˆH when x âˆˆEX, y âˆˆEY are
ï¬xed.
(c) Show that f (x, y) = âŸ¨f (Â·, â‹†), k(x, Â·, y, â‹†)âŸ©H.
66. Howcanwedeï¬netheaverageoftheelementsof HXâŠ•Y ,m XY=EXY[kX(Â·)kY(Â·)]?
Deï¬ne the average in the same way that we deï¬ned m X using Rieszâ€™s lemma
(Proposition 22).
67. Show that 	Y X âˆˆB(HX, HY) exists such that
âŸ¨f g, m XY âˆ’m XmYâŸ©HXâŠ—HY = âŸ¨	Y X f, gâŸ©HY
for each f âˆˆHX, g âˆˆHY.
68. The MMD is generally deï¬ned as sup f âˆˆF{EP[ f (X)] âˆ’EQ[ f (X)]} for some set
F of functions. Assuming that F := { f âˆˆH|âˆ¥f âˆ¥H â‰¤1}, show that the MMD is
âˆ¥m P âˆ’m Qâˆ¥H. Furthermore, show that we can transform the MMD as follows.
MMD2 = EX Xâ€²[k(X, Xâ€²)] + EYY â€²[k(Y, Y â€²)] âˆ’2EXY[k(X, Y)] ,
where Xâ€² and X (Y â€² and Y) are independent random variables that follow the
same distribution.
69. Show that the squared MMD estimator (5.4) is unbiased.
70. In the two-sample problem solved by a permutation test in Example71, for the
case when the numbers of samples are m, n (can be different) instead of the same
n and m, n are both even numbers, modify the entire program in Example 71 to
examine whether it works correctly (m = n in Example 71).
71. For the function h in (5.6), show that h1 is a function that always takes a value
of zero and that Ëœh2 and h coincide as functions.
72. Show that the fact that random variables X, Y that follow Gaussian distributions
are independent is equivalent to the condition that their correlation coefï¬cient
is zero. Additionally, give an example of two variables whose correlation coef-
ï¬cient is zero but that are not independent.
73. Prove the following equation.
âˆ¥m XY âˆ’m XmYâˆ¥2 = EX Xâ€²YY â€²[kX(X, Xâ€²)kY(Y, Y â€²)]
âˆ’2EXY{EXâ€²[kX(X, Xâ€²)]EY â€²[kY(Y, Y â€²)]} + EX Xâ€²[kX(X, Xâ€²)]EYY â€²[kY(Y, Y â€²)].
74. Show that the HSIC estimator

162
5
The MMD and HSIC

H SIC :=
1
N2

i

j
kX(xi, x j)kY (yi, y j) âˆ’2
N3

i

j
kX(xi, x j)

h
kY (yi, yh)
+ 1
N4

i

j
kX(xi, x j)

h

r
kY (yh, yr)
canbewrittenas 
H SIC = trace(K X H KY H) using K X = (kX(xi, x j))i, j, KY =
(kY(yi, y j))i, j, and H = I âˆ’1
N E, where I âˆˆRNÃ—N is the unit matrix and E âˆˆ
RNÃ—N is the matrix such that all the elements are ones. Additionally, construct
Python programs for each computation. Moreover, examine that both output the
same results for the Gaussian kernels k_x and k_y with Ïƒ 2 = 1; generate random
numbers for the standard Gaussian variables X and Y whose correlations are
a = 0, 0.1, 0.2, 0.4, 0.6, 0.8.
75. When we test the independence X âŠ¥âŠ¥{Y, Z} of X and {Y, Z}, the HSIC
is extended as ||m XY Z âˆ’m XmY Z||2. That is, we can transform kY(y, Â·) into
kY(y, Â·)kZ(z, Â·). Construct the function HSIC_2 by adding arguments to the func-
tion HSIC_1; generate a random number according to X âŠ¥âŠ¥{Y, Z}, and verify
that the obtained value is sufï¬ciently small.
76. Utilizing the class of LiNGAM and the function
def
cc ( x ,
y ) :
return np . sum( np . dot ( x . T ,
y ) )
/
len ( x )
def
f ( u ,
v ) :
return u âˆ’cc ( u ,
v ) / cc ( v ,
v ) âˆ—v
we wish to estimate whether each variable X, Y, Z is either upstream, midstream,
or downstream. Fill in the blanks by generating random numbers X, Y, Z that
do not follow the Gaussian distribution, and estimate which variables among
X, Y, Z are upstream, midstream, and downstream from the random numbers
alone.
#Data
g e n e r a t i o n
n = 30
x = np . random . randn ( n )âˆ—âˆ—2âˆ’np . random . randn ( n ) âˆ—âˆ—2
y = 2 âˆ—x + np . random . randn ( n ) âˆ—âˆ—2 âˆ’np . random . randn ( n ) âˆ—âˆ—2
z = x + y +np . random . randn ( n ) âˆ—âˆ—2 âˆ’np . random . randn ( n ) âˆ—âˆ—2
x = x âˆ’np . mean ( x )
y = y âˆ’np . mean ( y )
z = z âˆ’np . mean ( z )
## Estimate
UpStream ##
def
cc ( x ,
y ) :
return np . sum( np . dot ( x . T ,
y ) / len ( x ) )
def
f ( u ,
v ) :
return u âˆ’cc ( u ,
v ) / cc ( v ,
v ) âˆ—v
x_y = f ( x ,
y ) ;
y_z = f ( y ,
z ) ;
z_x = f ( z ,
x )
x_z = f ( x ,
z ) ;
z_y = f ( z ,
y ) ;
y_x = f ( y ,
x )
v1 = HSIC_2 ( x ,
y_x ,
z_x ,
k_x ,
k_y ,
k_z )
v2 = HSIC_2 ( y ,
z_y ,
x_y ,
k_y ,
k_z ,
k_x )
v3 = HSIC_2 ( z ,
x_z ,
y_z ,
k_z ,
k_x ,
k_y )

Exercises 65âˆ¼83
163
i f
v1 < v2 :
i f
v1 < v3 :
top = 1
e l s e :
top = 3
e l s e :
i f
v2 < v3 :
top = 2
e l s e :
top = 3
## Estimate
MidStream ##
x_yz = f ( x_y ,
z_y )
y_zx = f ( y_z ,
x_z )
z_xy = f ( z_x ,
y_x )
i f
top == 1:
v1 = ## Blank
( 1 )
##
v2 = ## Blank
( 2 )
##
i f
v1 < v2 :
middle = 2
bottom = 3
e l s e :
middle = 3
bottom = 2
i f
top == 2:
v1 = ## Blank
( 3 )
##
v2 = ## Blank
( 4 )
##
i f
v1 < v2 :
middle = 3
bottom = 1
e l s e :
middle = 1
bottom = 3
i f
top == 3:
v1 = ## Blank
( 5 )
##
v2 = ## Blank
( 6 )
##
i f
v1 < v2 :
middle = 1
bottom = 2
e l s e :
middle = 2
bottom = 1
## Output
the
R e s u l t s
##
print ( " top = " ,
top )
print ( " middle = " ,
middle )
print ( " bottom = " ,
bottom )
77. We wish to make two sequences independent by shifting one of x1, . . . , xN or
y1, . . . , yN, and then we want to repeat the process of calculating 
H SIC. We
wish to create a histogram that expresses a distribution that follows the null
hypothesis. For this purpose, we constructed the following program. Why can
we obtain the null hypothesis (X, Y are independent) by permutation? Where
in the program do we obtain the HSIC statistics, and where do we obtain the
multiple HSIC values that follow the null hypothesis?
# Data
Generation
fx = np . random . randn ( n )
y = np . random . randn ( n )

164
5
The MMD and HSIC
u = HSIC_1 ( x ,
y ,
k_x ,
k_y )
m = 100
w = [ ]
for
i
in
range (m) :
x = x [ np . random . choice ( n ,
n ,
r e p l a c e = False ) ]
w. append ( HSIC_1 ( x ,
y ,
k_x ,
k_y ) )
v = np . q u a n t i l e (w,
0 . 9 5 )
x = np . l i n s p a c e ( min ( min (w) , u ,
v ) , max(max(w) , u ,
v ) ,
200)
d e n s i t y = kde . gaussian_kde (w)
p l t . p l o t ( x ,
d e n s i t y ( x ) )
p l t . a x v l i n e ( x = v ,
c = " r " ,
l i n e s t y l e
= "âˆ’âˆ’" )
p l t . a x v l i n e ( x = u ,
c = "b" )
78. In the MMD (Sect.5.2) and HSIC (Sect.5.3), we cannot apply Mercerâ€™s theorem
because the kernel of the integral operator is not nonnegative deï¬nite. However,
in both cases, the integral operator possesses eigenvalues and eigenfunctions.
Why?
79. Show that k(x, y) = Ï†(x âˆ’y), Ï†(t) = eâˆ’|t| is a characteristic kernel.
80. In the proof of Proposition 54 (necessity, Appendix), we used the fact that
g(w) := (Ïµ âˆ’âˆ¥wâˆ¥2)(d+1)/2
+
is nonnegative deï¬nite [8]. Verify that this fact is
correct for d = 1 by proving the following equality.
1
2Ï€
 Ïµ
âˆ’Ïµ
g(w)eâˆ’iwxdw = 1 âˆ’cos(xÏµ)
Ï€x2
.
81. Why is the exponential type a universal kernel? Why is the characteristic kernel
based on a triangular distribution not a universal kernel?
82. Explain why the three equations and four inequalities hold in the following
derivation of the upper bound on the Rademacher complexity.
RN(F) = EÏƒ[sup
f âˆˆF
| 1
N
N

i=1
Ïƒi f (xi)|] = EÏƒ[sup
f âˆˆF
| 1
N
N

i=1
ÏƒiâŸ¨k(xi, Â·), f (Â·)âŸ©H|]
= EÏƒ[sup
f âˆˆF
|âŸ¨f, 1
N
N

i=1
Ïƒik(xi, Â·)âŸ©H|]
â‰¤EÏƒ[sup
f âˆˆF
âˆ¥f âˆ¥H



âŸ¨1
N
N

i=1
Ïƒik(xi, Â·), 1
N
N

i=1
Ïƒik(xi, Â·)âŸ©H]
â‰¤EÏƒ[



 1
N 2
N

i=1
N

j=1
ÏƒiÏƒ jk(xi, x j)]
â‰¤



EÏƒ[ 1
N 2
N

i=1
N

j=1
k(xi, x j)] â‰¤

kmax
N .

Exercises 65âˆ¼83
165
83. Explain why the one equality and four inequalities hold for the derivation of the
upper bound of |M M D2 âˆ’
M M D
2
B| below.
EX,Y sup
f âˆˆF
|EXâ€²{ 1
N
N

i=1
f (xâ€²
i) âˆ’1
N
N

i=1
f (xi)} âˆ’EY â€²{ 1
N
N

j=1
f (yâ€²
j)) âˆ’1
N
N

j=1
f (y j)}|
â‰¤EX,Y,Xâ€²,Y â€² sup
f âˆˆF
| 1
N
N

i=1
f (xâ€²
i) âˆ’1
N
N

i=1
f (xi) âˆ’1
N
N

i=1
f (yâ€²
i) + 1
N
N

i=1
f (yi)|
= EX,Y,Xâ€²,Y â€²,Ïƒ,Ïƒâ€² sup
f âˆˆF
| 1
N
N

i=1
Ïƒi{ f (xâ€²
i) âˆ’f (xi)} + 1
N
N

i=1
Ïƒ â€²
i { f (yâ€²
i) âˆ’f (yi)}|
â‰¤EX,Xâ€²,Ïƒ sup
f âˆˆF
| 1
N
N

i=1
Ïƒi{ f (xâ€²
i) âˆ’f (xi)}| + EY,Y â€²,Ïƒâ€² sup
f âˆˆF
| 1
N
n

j=1
Ïƒ â€²
j{ f (yâ€²
j) âˆ’f (y j)}|
â‰¤2[R(F, P) + R(F, Q)]
â‰¤2[(kmax/N)1/2 + (kmax/N)1/2].

Chapter 6
Gaussian Processes and Functional Data
Analyses
A stochastic process may be deï¬ned either as a sequence of random variables {Xt}tâˆˆT ,
where T is a set of times, or as a function Xt(Ï‰) : T â†’R of Ï‰ âˆˆ. We deï¬ne
a Gaussian process as a stochastic process {Xt} such that Xt (t âˆˆT â€²) follows a
multivariate Gaussian distribution for any ï¬nite subset T â€² of T . In this chapter, we
generalize the one-dimensional T to a multidimensional set E for the consideration
of Gaussian processes. We mainly deal with the variations of Ï‰ âˆˆ in f (Ï‰, x),
while thus far, we have dealt with the variations of x âˆˆE in f (Ï‰, x). The Gaussian
process has been applied to various aspects of machine learning. We examine the
relation between Gaussian processes and kernels. The chapterâ€™s ï¬rst half consists
of regression, classiï¬cation, and computational reduction treatments, and the last
part studies the Karhunen-LÃ³euvre expansion and its surrounding theory. Finally,
we study functional data analyses, which are closely related to stochastic processes.
6.1
Regression
Let E and (, F, Î¼) be a set and a probability space. If the correspondence between
 âˆ‹Ï‰ â†’f (Ï‰, x) âˆˆR is measurable for each x âˆˆE, i.e., if f (Ï‰, x) is a random
variable at each x âˆˆE, then we say that f :  Ã— E â†’R is a stochastic process.
Moreover, if the random variables f (Ï‰, x1), . . . , f (Ï‰, xN) follow an N-variable
Gaussian distribution for any N â‰¥1 and any ï¬nite number of elements x1, . . . .xN âˆˆ
E, then we call f a Gaussian process. We deï¬ne the covariance between xi, x j âˆˆE
by


{ f (Ï‰, xi) âˆ’m(xi)}{ f (Ï‰, x j) âˆ’m(x j)}dÎ¼(Ï‰) ,
where m(x) :=

 f (Ï‰, x)dÎ¼(Ï‰) is the expectation of f (Ï‰, x) for x âˆˆE. Then, no
matter what N and x1, . . . , xN we choose, their covariance matrices are nonnegative
Â© The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2022
J. Suzuki, Kernel Methods for Machine Learning with Math and Python,
https://doi.org/10.1007/978-981-19-0401-1_6
167

168
6
Gaussian Processes and Functional Data Analyses
deï¬nite. Thus, we can write the covariance matrix by using a positive deï¬nite kernel
k : E Ã— E â†’R. Therefore, the Gaussian process can be uniquely expressed in terms
of a pair (m, k) containing the mean m(x) of each x âˆˆE and the covariance k(x, xâ€²)
of each (x, xâ€²) âˆˆE Ã— E.
In general, a random variable is a map of  â†’R, and we should make Ï‰ explicit,
i.e., f (Ï‰, x), but for simplicity, for the time being, we make Ï‰ implicit, i.e., f (x),
even if it is a random variable.
Example 83 Let m X âˆˆRN and kX X âˆˆRNÃ—N be the mean and covariance matrix,
respectively, of the Gaussian process (m, k) at x1, . . . , xN âˆˆE := R. In general, for
a mean Î¼ and a covariance matrix  âˆˆRNÃ—N,  is nonnegative deï¬nite, and there
exists a lower triangular matrix R âˆˆRNÃ—N with  = RRâŠ¤(Cholesky decomposi-
tion). Therefore, to generate random numbers that follow N(m X, kX X) from N inde-
pendent random numbers u1, . . . , uN that follow the standard Gaussian distribution,
we can calculate fX := RXu + m X âˆˆRN for kX X := RX RâŠ¤
X with u = [u1, . . . , uN].
In fact, the expectation and the covariance matrix of fX are m X and
E[( fX âˆ’m X)( fX âˆ’m X)âŠ¤] = E[RXuuâŠ¤RâŠ¤
X ] = RXE[uuâŠ¤]RâŠ¤
X = RX RâŠ¤
X = kX X,
respectively This procedure can be described in Python as follows.
#
I n s t a l l
the
module
skfda
via
pip
i n s t a l l
s c i k i t âˆ’fda
# In
t h i s
chapter ,
we assume
t h a t
the
f o l l o w i n g
has
been
executed .
import numpy as np
import
m a t p l o t l i b . pyplot
as
p l t
from
m a t p l o t l i b
import
s t y l e
from
s k l e a r n . decomposition
import PCA
import
skfda
#
D e f i n i t i o n
of
(m,
k )
def m( x ) :
return 0
def k ( x ,
y ) :
return np . exp ( âˆ’(xâˆ’y ) âˆ—âˆ—2/2)
#
D e f i n i t i o n
of
gp_sample
def
gp_sample ( x , m,
k ) :
n = len ( x )
m_x = m( x )
k_xx = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
k_xx [ i ,
j ] = k ( x [ i ] ,
x [ j ] )
R = np . l i n a l g . cholesky ( k_xx ) # lower
t r i a n g u l a r
matrix
u = np . random . randn ( n )
return R. dot ( u ) + m_x

6.1 Regression
169
# Generate
the
random numbers and
c o n s t r u c t
the
covariance
matrix
to
compare
i t
with
k_xx
x = np . arange ( âˆ’2 , 3 ,
1)
n = len ( x )
r = 100
z = np . zeros ( ( r ,
n ) )
for
i
in
range ( r ) :
z [ i ,
: ]
= gp_sample ( x , m,
k )
k_xx = np . zeros ( ( n ,
2) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
k_xx [ i ,
j ] = k ( x [ i ,
: ] ,
x [ j ,
: ] )
print ("cov(z):\n" , np . cov ( z ) ,"\n")
print ("k_xx:\n" , k_xx )
cov ( z ) :
[[ 2.37424382e-01 1.09924256e-03 4.32681142e-01 ... -1.92328002e-01
1.46703390e-03 -7.31927129e-01]
[ 1.09924256e-03 6.17989727e-02 -8.41300879e-02 ... 1.37955079e-01
1.22155905e-01 3.53607212e-02]
[ 4.32681142e-01 -8.41300879e-02 1.51374514e+00 ... -6.82936090e-01
-8.44371416e-02 -1.82595208e+00]
...
[-1.92328002e-01 1.37955079e-01 -6.82936090e-01 ... 1.16493686e+00
4.07945653e-01 7.10299634e-01]
[ 1.46703390e-03 1.22155905e-01 -8.44371416e-02 ... 4.07945653e-01
2.88425891e-01 -4.34847631e-03]
[-7.31927129e-01 3.53607212e-02 -1.82595208e+00 ... 7.10299634e-01
-4.34847631e-03 2.60520510e+00]]
k_xx:
[[1.00000000e+00 6.06530660e-01 1.35335283e-01 1.11089965e-02
3.35462628e-04]
[6.06530660e-01 1.00000000e+00 6.06530660e-01 1.35335283e-01
1.11089965e-02]
[1.35335283e-01 6.06530660e-01 1.00000000e+00 6.06530660e-01
1.35335283e-01]
[1.11089965e-02 1.35335283e-01 6.06530660e-01 1.00000000e+00
6.06530660e-01]
[3.35462628e-04 1.11089965e-02 1.35335283e-01 6.06530660e-01
1.00000000e+00]]
In general, E does not have to be R. Gaussian processes are a class of stochastic
processes, and we might have the impression that the set E is the entire real number
set or a subset of it, but in fact, there is no further restriction as long as we deï¬ne the
positive deï¬nite kernel k on E Ã— E. Once we choose (m, k), we generate N-variate
Gaussian random variables according to (m, k), regardless of the selected E.

170
6
Gaussian Processes and Functional Data Analyses
Example 84 For E = R2, we can similarly obtain random numbers that follow the
N-variate multivariate Gaussian distribution.
#
D e f i n i t i o n
of
(m,
k )
def m( x ) :
return 0
def k ( x ,
y ) :
return np . exp(âˆ’np . sum ( ( xâˆ’y ) âˆ—âˆ—2) / 2 )
#
D e f i n i t i o n
of
Function
gp_sample
def
gp_sample ( x , m,
k ) :
n = x . shape [ 0 ]
m_x = m( x )
k_xx = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
k_xx [ i ,
j ] = k ( x [ i ] ,
x [ j ] )
R = np . l i n a l g . cholesky ( k_xx ) # lower
t r i a n g u l a r
matrix
u = np . random . randn ( n )
return R. dot ( u ) + m_x
# Generate
the
random numbers and
c o n s t r u c t
the
covariance
matrix
to
compare
i t
with
k_xx
n = 5
r = 100
z = np . zeros ( ( r ,
n ) )
for
i
in
range ( r ) :
z [ i ,
: ]
= gp_sample ( x , m,
k )
k_xx = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
k_xx [ i ,
j ] = k ( x [ i ] ,
x [ j ] )
print ("cov(z):\n" , np . cov ( z ) ,"\n")
print ("k_xx:\n" , k_xx )
cov ( z ) :
[[ 1.52140938
0.2585143
0.67840535 ... -0.57075902 -0.53404263
0.20940014]
[ 0.2585143
0.47450778
0.36954164 ... -0.30224707 -0.47093046
-0.03341687]
[ 0.67840535
0.36954164
0.77124364 ... -0.34846439 -0.40226404
0.37837525]
...
[-0.57075902 -0.30224707 -0.34846439 ...
0.42199909
0.46715201
0.04255153]
[-0.53404263 -0.47093046 -0.40226404 ...
0.46715201
0.59461419
0.09420804]
[ 0.20940014 -0.03341687
0.37837525 ...
0.04255153
0.09420804
0.40676413]]

6.1 Regression
171
k_xx:
[[1.00000000e+00 6.06530660e-01 1.35335283e-01 1.11089965e-02
3.35462628e-04]
[6.06530660e-01 1.00000000e+00 6.06530660e-01 1.35335283e-01
1.11089965e-02]
[1.35335283e-01 6.06530660e-01 1.00000000e+00 6.06530660e-01
1.35335283e-01]
[1.11089965e-02 1.35335283e-01 6.06530660e-01 1.00000000e+00
6.06530660e-01]
[3.35462628e-04 1.11089965e-02 1.35335283e-01 6.06530660e-01
1.00000000e+00]]
Then, as in the usual regression procedure, we assume that x1, . . . , xN âˆˆE and
y1, . . . , yN âˆˆR are generated according to
yi = f (xi) + Ïµi
(6.1)
through the use of an unknown function f : E â†’R, where Ïµi follows a Gaussian
distribution with a mean of 0 and a variance of Ïƒ 2 and is independent for each
i = 1, . . . , N. The likelihood is
N

i=1
[
1
âˆš
2Ï€Ïƒ 2 exp{âˆ’(yi âˆ’f (xi))2
2Ïƒ 2
}]
when the function f is known (ï¬xed). In the following, we assume that the func-
tion f randomly varies, and we regard the Gaussian process (m, k) as its prior
distribution. That is, we consider the model fX âˆ¼N(m X, kX X) with yi| f (xi) âˆ¼
N( f (xi), Ïƒ 2) as fX = ( f (x1), . . . , f (xN)). Then, we calculate the posterior distri-
bution of f (z1), . . . , f (zn) corresponding to z1, . . . , zn âˆˆE, which is different from
x1, . . . , xN. The variations in y1, . . . , yN is due to the variations in f and Ïµi. Thus,
the covariance matrix is
kX X + Ïƒ 2I = (k(xi, x j) + Ïƒ 2Î´i, j)i, j=1,...,N âˆˆRNÃ—N .
On the other hand, the variation in f (z1), . . . , f (zn) is due only to the variation
in f . Therefore, the covariance matrix is kZ Z = (k(zi, z j))i, j=1,...,n âˆˆRnÃ—n. More-
over, the variances of yi and f (z j) are those of f (xi) and f (z j), respectively,
and the covariance matrix of Y = [y1, . . . , yN] and fZ = [ f (z1), Â· Â· Â· , f (zn)] is
kX Z = (k(xi, z j))i=1,...,N, j=1,...,n. In summary, the simultaneous distribution of Y
and fZ is
 Y
fZ

âˆ¼N
m X
m Z

,
 kX X + Ïƒ 2I kX Z
kZ X
kZ Z

.

172
6
Gaussian Processes and Functional Data Analyses
In the following, we show that the posterior probability of the function f (Â·) given the
value of Y is still a Gaussian process. To this end, we use the following proposition.
Proposition 61 Suppose that the simultaneous distribution of random variables a âˆˆ
RN, b âˆˆRn can be expressed by
a
b

âˆ¼N
Î¼a
Î¼b

,
 A C
CâŠ¤B

,
where Î¼a, Î¼b are the expectations, A âˆˆRNÃ—N, B âˆˆRnÃ—n are the covariance matrices
(A:positivedeï¬nite; B:nonnegativedeï¬nite),andC âˆˆRNÃ—n isthecovariancematrix
between them. Then, the conditional probability of b given a is
b|a âˆ¼N(Î¼b + CâŠ¤Aâˆ’1(a âˆ’Î¼a), B âˆ’CâŠ¤Aâˆ’1C) .
(6.2)
Proof: Consult Lauritzen, â€œGraphical Modelsâ€ [20] p256.
Hence, from Proposition 61, the posterior distribution of fZ âˆˆRn under Y âˆˆRN
is N(Î¼â€², â€²), where
Î¼â€² := m Z + kZ X(kX X + Ïƒ 2I)âˆ’1(Y âˆ’m X) âˆˆRn
and
â€² := kZ Z âˆ’kZ X(kX X + Ïƒ 2I)âˆ’1kX Z âˆˆRnÃ—n .
If we set n = 1 and z1 = x, then the distribution of f (x) becomes
mâ€²(x) := m(x) + kx X(kX X + Ïƒ 2I)âˆ’1(Y âˆ’m X)
(6.3)
kâ€²(x, x) := k(x, x) âˆ’kx X(kX X + Ïƒ 2I)âˆ’1kXx .
(6.4)
We summarize the discussion as follows.
Proposition 62 Suppose that the prior distribution of f (Â·) is a Gaussian process
(m, k). If we obtain x1, . . . , xN, y1, . . . , yN according to (6.1), the posterior proba-
bility of f (Â·) is a Gaussian process (mâ€², kâ€²), where mâ€², kâ€² are given by (6.3) and (6.4),
respectively.
In the actual calculation, it takes O(N 3) time to calculate (K + Ïƒ 2I)âˆ’1. To com-
plete the whole process in O(N 3/3), we use the following method. By Cholesky
decomposition, we obtain an L âˆˆRNÃ—N such that
LLâŠ¤= kX X + Ïƒ 2I ,
which can be completed in O(N 3/3) time. Then, let the solutions of LÎ³ = kXx, LÎ² =
y âˆ’m(x), and LâŠ¤Î± = Î² be Î³ âˆˆRN, Î² âˆˆRN, and Î± âˆˆRN, respectively. Since L is

6.1 Regression
173
a lower triangular matrix, these calculations take at most O(N 2) time. Additionally,
we have
(kX X + Ïƒ 2I)âˆ’1(Y âˆ’m X) = (LLâŠ¤)âˆ’1LÎ² = (LLâŠ¤)âˆ’1LLâŠ¤Î± = Î±
and
kx X(kX X + Ïƒ 2I)âˆ’1kXx = (LÎ³ )âŠ¤(LLâŠ¤)âˆ’1LÎ³ = Î³ âŠ¤Î³ .
Finally, from Î±, Î², Î³ , we have
mâ€²(x) = m(x) + kx XÎ±
and
kâ€²(x, x) = k(x, x) âˆ’Î³ âŠ¤Î³ .
We can write the calculations of m(x), k(x, x) in forms that are completed in
O(N 3) and O(N 3/3) time in Python as follows.
def gp_1 ( x_pred ) :
h = np . zeros ( n )
for
i
in
range ( n ) :
h [ i ] = k ( x_pred ,
x [ i ] )
R = np . l i n a l g . inv (K + sigma_2âˆ—np . i d e n t i t y ( n ) )
# O( n ^3)
Computation
mm = mu( x_pred ) + np . dot ( np . dot ( h . T , R) ,
( yâˆ’mu( x ) ) )
ss = k ( x_pred ,
x_pred ) âˆ’np . dot ( np . dot ( h . T , R) , h )
return {"mm":mm, "ss": ss }
def gp_2 ( x_pred ) :
h = np . zeros ( n )
for
i
in
range ( n ) :
h [ i ] = k ( x_pred ,
x [ i ] )
L = np . l i n a l g . cholesky (K + sigma_2âˆ—np . i d e n t i t y ( n ) )
# O( n ^ 3 / 3 )
Computation
alpha = np . l i n a l g . solve (L ,
np . l i n a l g . solve (L . T ,
( y âˆ’mu( x ) ) ) ) # O( n ^2)
Computation
mm = mu( x_pred ) + np . sum( np . dot ( h . T ,
alpha ) )
gamma = np . l i n a l g . solve (L . T ,
h )
# O( n ^2)
Computation
ss = k ( x_pred ,
x_pred ) âˆ’np . sum(gammaâˆ—âˆ—2)
return {"mm":mm, "ss": ss }
Example 85 For comparison purposes, we executed the functions gp_1 and gp_2.
We can see the difference achieved by Cholesky decomposition, which reduced the
computational complexity (Fig.6.1).
sigma_2 = 0.2
def k ( x ,
y ) :
# Covariance
Function
return np . exp ( âˆ’(x âˆ’y ) âˆ—âˆ—2
/
2
/
sigma_2 )
def mu( x ) :
# Mean Function
return x

174
6
Gaussian Processes and Functional Data Analyses
n = 100
x = np . random . uniform ( s i z e = n ) âˆ—6 âˆ’3
y = np . s i n ( x
/
2) + np . random . randn ( n )
K = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K[ i ,
j ] = k ( x [ i ] ,
x [ j ] )
## Measure
Execution
Time
import
time
s t a r t 1
= time . time ( )
gp_1 ( 0 )
end1 = time . time ( )
print ("time1â£=" , end1âˆ’s t a r t 1 )
s t a r t 2
= time . time ( )
gp_2 ( 0 )
end2 = time . time ( )
print ("time2â£=â£" , end2âˆ’s t a r t 2 )
# The 3 sigma
width
around
the
average
u_seq = np . arange ( âˆ’3 ,
3. 1 ,
0 . 1 )
v_seq =
[ ] ;
w_seq = [ ]
for u in
u_seq :
r e s = gp_1 ( u )
v_seq . append ( r e s ["mm"] )
w_seq . append ( r e s ["ss"] )
p l t . f i g u r e ( )
p l t . xlim ( âˆ’3 ,
3)
p l t . ylim ( âˆ’3 ,
3)
p l t . s c a t t e r ( x ,
y ,
f a c e c o l o r s =â€™noneâ€™ ,
e d g e c o l o r s = "k" ,
marker = "o")
p l t . p l o t ( u_seq ,
v_seq )
p l t . p l o t ( u_seq ,
np . sum ( [ v_seq ,
[ i âˆ—3 for
i
in w_seq ] ] ,
a x i s = 0) , c = "b")
p l t . p l o t ( u_seq ,
np . sum ( [ v_seq ,
[ i âˆ—( âˆ’3)
for
i
in w_seq ] ] ,
a x i s = 0) , c = "b
")
p l t . show ( )
n = 100
p l t . f i g u r e ( )
p l t . xlim ( âˆ’3 ,
3)
p l t . ylim ( âˆ’3 ,
3)
## Five
times ,
changing
the
samples
c o l o r = ["r" , "g" , "b" , "k" , "m"]
for h in
range ( 5 ) :
x = np . random . uniform ( s i z e = n ) âˆ—6 âˆ’3
y = np . s i n ( np . pi âˆ—x
/
2) + np . random . randn ( n )
sigma_2 = 0.2
K = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K[ i ,
j ] = k ( x [ i ] ,
x [ j ] )
u_seq = np . arange ( âˆ’3 ,
3. 1 ,
0 . 1 )
v_seq = [ ]
for u in
u_seq :
r e s = gp_1 ( u )
v_seq . append ( r e s ["mm"] )
p l t . p l o t ( u_seq ,
v_seq ,
c = c o l o r [ h ] )
time1 = 0.009966373443603516
time2 =
0.057814598083496094
If we compare the equation
mâ€²(x) := kx X(kX X + Ïƒ 2I)âˆ’1Y

6.1 Regression
175
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
z
f(z)
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
Index
0
Fig. 6.1 We show the range of 3 Ïƒ above and below the average (left) and executed different
samples ï¬ve times (right)
obtained by substituting m X = m(x) = 0 into the average formula of the Gaussian
process (6.3) with the equation
kx,X Ë†Î± = kx X(K + Î»I)âˆ’1Y
obtained by multiplying the kernel ridge regression formula (4.6) by kx,X from the
left, we observe that the former is a speciï¬c case of the latter when setting Î» = Ïƒ 2.
6.2
Classiï¬cation
We consider the classiï¬cation problem next. We assume that the random variable Y
takes the value Y = Â±1 and that its conditional probability given x âˆˆE is
P(Y = 1|x) =
1
1 + exp(âˆ’f (x)) ,
(6.5)
where the Gaussian process f :  Ã— E â†’R is used. We wish to estimate f from
the actual x1, . . . , xN âˆˆRp (row vector) and y1, . . . , yN âˆˆ{âˆ’1, 1}. To maximize the
likelihood, we minimize the negative log-likelihood
N
	
i=1
log[1 + exp{âˆ’yi f (xi)}] .
If we set
fX = [ f1, . . . , fN]âŠ¤= [ f (x1), . . . , f (xN)]âŠ¤âˆˆRN, vi := eâˆ’yi fi, and
l( fX) := 
N
i=1 log(1 + vi), then we have

176
6
Gaussian Processes and Functional Data Analyses
âˆ‚vi
âˆ‚fi
= âˆ’yivi , âˆ‚l( fX)
âˆ‚fi
= âˆ’yivi
1 + vi
, âˆ‚2l( fX)
âˆ‚f 2
i
=
vi
(1 + vi)2 ,
where we use y2
i = 1. Given an initial value, we wait for the Newton-Raphson update
fX â†fX âˆ’(âˆ‡2l( fX))âˆ’1âˆ‡l( fX) to converge. The update formula is
fX â†fX + W âˆ’1u ,
where u =
 yivi
1 + vi

i=1,...,N
and W = diag

vi
(1 + vi)2

i=1,...,N
. In other words, we
repeat the following two steps:
1. Obtain v, u, and W from fX.
2. Calculate fX + W âˆ’1u and substitute it into fX
for v := [v1, . . . , vN]âŠ¤âˆˆRN.
Next, we consider maximizing the likelihood
N

i=1
1
1 + exp{âˆ’yi f (xi)} multiplied
by the prior distribution of fX, i.e., ï¬nding the solution with the maximum posterior
probability. Here, the mean is often set to 0 as the prior probability of f in the
formulation of (6.5). Suppose ï¬rst that the prior probability of fX âˆˆRN is
1

(2Ï€)N det kX X
exp{âˆ’f âŠ¤
X kâˆ’1
X X fX
2
} ,
where kX X is the Gram matrix (k(xi, x j))i, j=1,...,N. If we set
L( fX) = l( fX) + 1
2 f âŠ¤
X kâˆ’1
X X fX + 1
2 log det kX X + N
2 log 2Ï€ ,
(6.6)
then we have
âˆ‡L( fX) = âˆ‡l( fX) + kâˆ’1
X X fX = âˆ’u + kâˆ’1
X X fX
(6.7)
and
âˆ‡2L( fX) = âˆ‡2l( fX) + kâˆ’1
X X = W + kâˆ’1
X X .
(6.8)
Thus, we may express the update formula as
fX â†fX + (W + kâˆ’1
X X)âˆ’1(u âˆ’kâˆ’1
X X fX)
= (W + kâˆ’1
X X)âˆ’1{(W + kâˆ’1
X X) fX âˆ’kâˆ’1
X X fX + u}
= (W + kâˆ’1
X X)âˆ’1(W fX + u) .
However, since the size of fX is the number of samples N, it takes an enormous
amount of time to calculate the inverse matrix. We try to improve the efï¬ciency of
this process as follows. Utilizing the Woodbury-Sherman-Morrison formula

6.2 Classiï¬cation
177
(A + UW V âŠ¤)âˆ’1 = Aâˆ’1 âˆ’Aâˆ’1U(W âˆ’1 + V âŠ¤Aâˆ’1U)âˆ’1V âŠ¤Aâˆ’1
(6.9)
with A âˆˆRnÃ—n (nonsingular), W âˆˆRmÃ—m, and U, V âˆˆRnÃ—m, if we set A = kâˆ’1
X X and
U = V = I, we obtain
(W + kâˆ’1
X X)âˆ’1
= kX X âˆ’kX X(W âˆ’1 + kX X)âˆ’1kX X
= kX X âˆ’kX XW 1/2(I + W 1/2kX XW 1/2)âˆ’1W 1/2kX X .
(6.10)
Thus, we can obtain an L such that I + W 1/2kX XW 1/2 = LLâŠ¤(Cholesky decom-
position) in O(N 3/3) time. Letting Î³ := W fX + u, we ï¬nd a Î² such that LÎ² =
W 1/2kX XÎ³ and an Î± such that LâŠ¤W âˆ’1/2Î± = Î² in O(N 2) time, and we substitute
kX X(Î³ âˆ’Î±) into fX. We repeat this procedure until convergence is achieved. In fact,
we have the following equation:
LLâŠ¤W âˆ’1/2Î± = LÎ² = W 1/2kX XÎ³
kX X(Î³ âˆ’Î±)
= kX X{Î³ âˆ’W 1/2(LLâŠ¤)âˆ’1W 1/2kX XÎ³ } = {kX X âˆ’kX XW 1/2(LLâŠ¤)âˆ’1W 1/2kX X}Î³
= {kX X âˆ’kX XW 1/2(I + W 1/2kX XW 1/2)âˆ’1W 1/2kX X}Î³ = (W + kâˆ’1
X X)âˆ’1(W f + u) ,
where the last equality is due to (6.10).
Example 86 By using the ï¬rst N = 100 of the 150 Iris data (the ï¬rst 50 points
and the next 50 points are Setosa and Versicolor data, respectively), we found the
fX = [ f1, . . . , fN] with the maximum posterior probability. The output showed that
f1, . . . , f50 and f51, . . . , f100 were positive and negative, respectively.
from
s k l e a r n . d a t a s e t s
import
l o a d _ i r i s
df =
l o a d _ i r i s ( )
##
I r i s
Data
x = df . data [0 : 1 0 0 ,
0 : 4 ]
y = np . a r r a y ([1]âˆ—50 + [ âˆ’1]âˆ—50)
n = len ( y )
## Compute
Kernel
v alues
f o r
the
four
c o v a r i a t e s
def k ( x ,
y ) :
return np . exp ( np . sum( âˆ’(x âˆ’y ) âˆ—âˆ—2)
/
2)
K = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K[ i ,
j ] = k ( x [ i ,
: ] ,
x [ j ,
: ] )
eps = 0.00001
f = [ 0 ] âˆ—n
g = [ 0 . 1 ]
âˆ—n
while np . sum ( ( np . a r r a y ( f )âˆ’np . a r r a y ( g ) ) âˆ—âˆ—2) > eps :
i = i + 1
g = f
## Save
the
data
before
update
f o r
comparison
f = np . a r r a y ( f )
y = np . a r r a y ( y )
v = np . exp(âˆ’yâˆ—f )
u = y âˆ—v
/
(1 + v )

178
6
Gaussian Processes and Functional Data Analyses
w = ( v
/
(1 + v ) âˆ—âˆ—2)
W = np . diag (w)
W_p = np . diag (wâˆ—âˆ—0.5)
W_m = np . diag (wâˆ—âˆ—( âˆ’0.5) )
L = np . l i n a l g . cholesky ( np . i d e n t i t y ( n ) +np . dot ( np . dot (W_p, K) , W_p) )
gamma = W. dot ( f ) + u
beta = np . l i n a l g . solve (L ,
np . dot ( np . dot (W_p, K) , gamma) )
alpha = np . l i n a l g . solve ( np . dot (L . T , W_m) ,
beta )
f = np . dot (K,
(gammaâˆ’alpha ) )
print ( l i s t ( f ) )
[2.9017597728506903, 2.6661877410648125, 2.735999714975541,
2.5962146616446793,
2.8888259653434902, 2.4229904289075734, 2.7128370653298717, 2.8965829899125057,
2.263839959450692, 2.722794155018708, 2.6757868220665557, 2.80427691289934,
2.629916582197861, 2.129058875598969, 1.9947371858903622, 1.7255773341842824,
2.502403800007298, 2.894767948521167, 2.211715451090947, 2.7578887454424845,
2.5807025000167654, 2.7884335002993703, 2.4501472162360978, 2.598252566107158,
2.49363291477457, 2.5892721299617927, 2.7995603132602014, 2.854337885531593,
2.8580336326051525, 2.682198311711416, 2.6631803480277263, 2.6529515170091735,
2.409809417765029, 2.1570288906747956, 2.738196179682446, 2.777507355522734,
2.6054932709605585, 2.8486244905053546, 2.342636360704147, 2.8826825981318938,
2.887406385036485, 1.561916989035174, 2.4541693614670925, 2.64939855108404,
2.4071165717812315, 2.633906076076528, 2.7271240196093944, 2.6732162909902857,
2.749570997237667, 2.884288422112919, -1.870441763888594, -2.5373817133813237,
-1.9327372577182746, -2.5318858849974895, -2.579366785986732, -2.7850146869568233,
-2.381782737110541, -1.4675208896283152, -2.48651854962823, -2.356973904782517,
-1.6007721537062138, -2.8113237365649777, -2.3813705315823492, -2.734406212419866,
-2.3307697118699044, -2.3416642385980246, -2.6148174861101388, -2.748088114594368,
-2.3729716167430452,-2.6288003128782993,-2.2519078422840106,-2.7892659188354783,
-2.3456928166252453, -2.7042381425156226, -2.712488583609864, -2.502955837329986,
-2.1624797840208307, -2.01571028937641, -2.8402609758000015, -2.19474914254942,
-2.474090451450695, -2.3426425865837377, -2.755051287500358, -2.1890836679248933,
-2.415174453933202, -2.3822860688193157, -2.275106022961632, -2.4967546817420585,
-2.6958800533399927, -2.6643571619340682, -2.64863153643803, -2.7718459450746087,
-2.8075095809378467, -1.5468264491591686, -2.81472767183598, -2.756340788852421,
-2.8346439145882236,-2.8498009687645762,-1.1822836980388691,-2.8458627499007214]
To execute classiï¬cation for a new value x by using the estimated Ë†f âˆˆRN, we
perform the following steps. Similar to the regression case, if we apply Proposition
61 to

fX
f (x)

âˆ¼N
0
0

,
 kX X kXx
kx X kxx

,
then we obtain
f (x)| fX âˆ¼N(mâ€²(x), kâ€²(x, x)) ,
(6.11)
where
mâ€²(x) = kx Xkâˆ’1
X X fX
and
kâ€²(x, x) = kxx âˆ’kx Xkâˆ’1
X XkXx .

6.2 Classiï¬cation
179
If we observe Y âˆˆ{âˆ’1, 1}N, we obtain the estimate Ë†f with the Newton-Raphson
method and calculate Ë†W. We consider the Laplace approximation of fX|Y, i.e., we
approximate the Gaussian distribution as follows (Rasmussen-Williams [25]):
fX|Y âˆ¼N( Ë†f , ( Ë†W + kâˆ’1
X X)âˆ’1).
(6.12)
That is, the covariance matrix is the inverse of Ë†W + kâˆ’1
X X, which is the Hessian
âˆ‡2L( Ë†f ) of (6.8). Then, the variations in (6.11),(6.12) are independent, and f (x|Y) =
N(mâˆ—, kâˆ—). Note that we can calculate the posterior probability for each x âˆˆE:
mâˆ—= kx Xkâˆ’1
X X Ë†f
(6.13)
kâˆ—= kxx âˆ’kx Xkâˆ’1
X XkXx + kx Xkâˆ’1
X X( Ë†W + kâˆ’1
X X)âˆ’1kâˆ’1
X XkXx
= kxx âˆ’kx X( Ë†W âˆ’1 + kX X)âˆ’1kXx ,
where the last transformation is due to A = kX X and W = Ë†W âˆ’1 in (6.9), and U, V
are the unit matrices. Thus, we can compute the expectation (prediction value) w.r.t.
f (x)|Y âˆ¼N(mâˆ—, kâˆ—) in the sigmoid function
P(Y = 1|x) =
1
1 + exp(âˆ’f (x)) :

E
1
1 + exp(âˆ’z)
1
âˆš2Ï€kâˆ—
exp[âˆ’1
2kâˆ—
{z âˆ’mâˆ—}2]dz.
(6.14)
To implement this step, we only need to compute Ë†u from Ë†f . Since (6.7) is zero
when the updates converge, from (6.13), we have that
mâˆ—= kx X Ë†u
and
(kX X + W âˆ’1)âˆ’1 = W 1/2W âˆ’1/2(kX X + W âˆ’1)âˆ’1W âˆ’1/2W 1/2
= W 1/2(I + W 1/2kX XW 1/2)âˆ’1W 1/2 .
Hence, we compute Î± âˆˆRN such that I + Ë†W 1/2kX X Ë†W 1/2 = LLâŠ¤(Cholesky decom-
position) and LÎ± = Ë†W 1/2kXx in O(N 3/3) time. Then, we have
kâˆ—= kxx âˆ’Î±âŠ¤Î±
since we have
kx XW 1/2(LLâŠ¤)âˆ’1W 1/2kXx = kx XW 1/2(Lâˆ’1)âŠ¤Lâˆ’1W 1/2kXx = Î±âŠ¤Î± .

180
6
Gaussian Processes and Functional Data Analyses
We can describe the procedure for ï¬nding the value of (6.14) in Python as follows.
We assume that the procedure starts immediately after the procedure of Example 86
completes. 6-I
def
pred ( z ) :
kk = np . zeros ( n )
for
i
in
range ( n ) :
kk [ i ] = k ( z ,
x [ i ,
: ] )
mu = np . sum( kk âˆ—u )
# Mean
alpha = np . l i n a l g . solve (L ,
np . dot (W_p,
kk ) )
sigma2 = k ( z ,
z ) âˆ’np . sum( alpha âˆ—âˆ—2)
#
Variance
m = 1000
b = np . random . normal (mu,
sigma2 ,
s i z e = m)
pi = np . sum ( ( 1 + np . exp(âˆ’b ) ) âˆ—âˆ—( âˆ’1) )
/ m
#
P r e d i c t i o n
return
pi
Example 87 Immediately after processing Example 86, we entered numerical val-
ues for the four covariates of Iris into the function pred and calculated the probability
of them being Setosa values (1 minus the probability of them being Versicolor val-
ues). When we input the average values of the covariates for Setosa and Versicolor,
we observed that the probabilities were close to 1 and 0, respectively.
z = np . zeros ( 4 )
for
j
in
range ( 4 ) :
z [ j ] = np . mean ( x [ : 5 0 ,
j ] )
pred ( z )
0.9466371383148896
for
j
in
range ( 4 ) :
z [ j ] = np . mean ( x [50:100 ,
j ] )
pred ( z )
0.05301765489687672
6.3
Gaussian Processes with Inducing Variables
A Gaussian process generally involves O(N 3) calculations. To avoid such an incon-
venience, we choose Z := [z1, Â· Â· Â· , zM] âˆˆE M and approximate the generation pro-
cess
fX âˆ¼N(m X, kX X)
f (x)| fX âˆ¼N(m(x) + kx Xkâˆ’1
X X( fX âˆ’m X), k(x, x) âˆ’kx Xkâˆ’1
X XkXx)

6.3 Gaussian Processes with Inducing Variables
181
y| f (x) âˆ¼N( f (x), Ïƒ 2)
by
fZ âˆ¼N(m Z, kZ Z)
(6.15)
f (x)| fZ âˆ¼N(m(x) + kx Zkâˆ’1
Z Z( fZ âˆ’m Z), k(x, x) âˆ’kx Zkâˆ’1
Z ZkZx)
(6.16)
y| f (x) âˆ¼N( f (x), Ïƒ 2) ,
(6.17)
where
m Z = (m(z1), Â· Â· Â· , m(zM)),
kZ Z = (k(zi, z j))i, j=1,Â·Â·Â· ,M,
and
kx Z =
[k(x, z1), Â· Â· Â· , k(x, zM)] (row vector).
Under the following assumption, we obtain Proposition 63.
Assumption 1 Each occurrence of (6.16) is independent for x = x1, Â· Â· Â· , xN.
Proposition 63 Let  âˆˆRNÃ—N be a diagonal matrix whose elements are Î»(xi),
i = 1, Â· Â· Â· , N. Under the generation process outlined in (6.15), (6.16), (6.17) and
Assumption 1, we have
fZ|Y âˆ¼N(Î¼ fZ|Y,  fZ|Y) ,
Î¼ fZ|Y=m Z+kZ Z Qâˆ’1kZ X(+Ïƒ 2IN)âˆ’1(Y âˆ’m X) ,
(6.18)
and
 fZ|Y = kZ Z Qâˆ’1kZ Z ,
(6.19)
where
Q := kZ Z + kZ X( + Ïƒ 2IN)âˆ’1kX Z âˆˆRMÃ—M
(6.20)
with Î»(x) := k(x, x) âˆ’kx Zkâˆ’1
Z ZkZx.
Proof: From (6.16) and Assumption 1,
fX| fZ âˆ¼N(m X + kX Zkâˆ’1
Z Z( fZ âˆ’m Z), )
for fX := [ f (x1), Â· Â· Â· , f (xN)]. Moreover, the expectations of Y and fX are equal,
and only the variance Ïƒ 2 is different, so we have
Y| fZ âˆ¼N(m X + kX Zkâˆ’1
Z Z( fZ âˆ’m Z),  + Ïƒ 2IN) .
Thus, the simultaneous distribution of fZ, Y is
(the exponents of p( fZ) and p(Y| fZ))
= âˆ’1
2( fZ âˆ’m Z)âŠ¤kâˆ’1
Z Z( fZ âˆ’m Z)
âˆ’1
2{Y âˆ’(m X + kZ Xkâˆ’1
Z Z( fZ âˆ’m Z))}âŠ¤( + Ïƒ 2IN)âˆ’1

182
6
Gaussian Processes and Functional Data Analyses
Â·{Y âˆ’(m X + kZ Xkâˆ’1
Z Z( fZ âˆ’m Z))} .
(6.21)
If we differentiate (6.21) by fZ, setting a = fZ âˆ’m Z and b = Y âˆ’m X yields
âˆ’kâˆ’1
Z Za + kâˆ’1
Z ZkZ X( + Ïƒ 2IN)âˆ’1(b âˆ’kZ Xkâˆ’1
Z Za)
= kâˆ’1
Z ZkZ X( + Ïƒ 2IN)âˆ’1b âˆ’kâˆ’1
Z Z{kZ Z + kZ X( + Ïƒ 2IN))kX Z}kâˆ’1
Z Za
= kâˆ’1
Z ZkZ X( + Ïƒ 2IN)âˆ’1b âˆ’kâˆ’1
Z Z Qkâˆ’1
Z Za
= kâˆ’1
Z Z Qkâˆ’1
Z Z{kZ Z Qâˆ’1kZ X( + Ïƒ 2IN)âˆ’1b âˆ’a}
= âˆ’âˆ’1
fZ|Y( fZ âˆ’Î¼ fZ|Y) .
(6.22)
Therefore, the terms w.r.t. fZ in (6.21) are only
âˆ’1
2( fZ âˆ’Î¼ fZ|Y)âŠ¤âˆ’1
fZ|Y( fZ âˆ’Î¼ fZ|Y),
(6.23)
and we obtain the proposition.
â–¡
Proposition 64 Under the generation process outlined in (6.15), (6.16), (6.17) and
Assumption 1, we have
Y âˆ¼N(Î¼Y, Y)
with
Î¼Y := m X
(6.24)
and
Y :=  + Ïƒ 2IN + kX Zkâˆ’1
Z ZkX Z .
(6.25)
Proof: Since the expectation Î¼Y of Y is m X, we obtain the covariance matrix Y. Let
a := fZ âˆ’m Z and b := Y âˆ’m X. Then, the exponents (6.21) and (6.23) of p(Y, fZ)
and p( fZ|Y) are, respectively,
âˆ’1
2aâŠ¤kâˆ’1
Z Za âˆ’1
2(b âˆ’kZ Xkâˆ’1
Z Za)âŠ¤( + Ïƒ 2I)âˆ’1(b âˆ’kZ Xkâˆ’1
Z Za)
(6.26)
and
âˆ’1
2 (a âˆ’kZ Z Qâˆ’1kZ X ( + Ïƒ 2IN )âˆ’1b)âŠ¤(kZ Z Qâˆ’1kZ Z )âˆ’1(a âˆ’kZ Z Qâˆ’1kZ X( + Ïƒ 2IN )âˆ’1b) .
(6.27)
From (6.20), we have
âˆ’1
2aâŠ¤kâˆ’1
Z Za âˆ’1
2(kZ Xkâˆ’1
Z Za)âŠ¤( + Ïƒ 2I)âˆ’1kZ Xkâˆ’1
Z Za = âˆ’1
2aâŠ¤kâˆ’1
Z Z Qkâˆ’1
Z Za .
From p(Y, f2) = p( f2|Y)p(Y), the difference between (6.26) and (6.27) is the expo-
nent of p(Y), which is

6.3 Gaussian Processes with Inducing Variables
183
âˆ’1
2bâŠ¤( + Ïƒ 2I)âˆ’1b + 1
2bâŠ¤( + Ïƒ 2IN)âˆ’1kX Z Qâˆ’1kZ X( + Ïƒ 2IN)âˆ’1b ,
where we may set a = 0 because no terms will remain w.r.t. a. Furthermore, if we
set A =  + Ïƒ 2IN, U = kX Z, V = kZ X, and W = kâˆ’1
Z Z in the Woodbury-Sherman-
Morrison formula (6.9), then we have
âˆ’1
2bâŠ¤( + Ïƒ 2IN + kX Zkâˆ’1
Z ZkX Z)âˆ’1b
and obtain (6.25).
â–¡
Proposition 65 Under the generation process outlined in (6.15), (6.16), (6.17) and
Assumption 1, for each x âˆˆE, we have
f (x)|Y âˆ¼N(Î¼(x), Ïƒ 2(x))
Î¼(x):=m(x)+kx Zkâˆ’1
Z Z(Î¼ fZ|Y âˆ’m Z)=m(x)+kx Z Qâˆ’1kZ X(+Ïƒ 2IN)âˆ’1(Y âˆ’m X)
(6.28)
Ïƒ 2(x) := k(x, x) âˆ’kx Z(K âˆ’1
Z Z âˆ’Qâˆ’1)kZx .
Proof: First, we note that Y â†’fZ â†’f (x) forms a Markov chain in this order. In the
following, we consider the distribution of f (x)|Y instead of f (x)| fZ, i.e., the distri-
bution of f (x)| fZ and fZ|Y. In (6.16), the term with a mean of kx Zkâˆ’1
Z Z( fZ âˆ’m Z)
becomes kx Zkâˆ’1
Z Z(Î¼ fZ|Y âˆ’m Z) when averaged over fZ|Y. Thus, we obtain (6.28).
Moreover, if we take the variance of that term with respect to fZ|Y, we obtain the
same value as the variance of kx Zkâˆ’1
Z Z( fZ âˆ’Î¼ fZ|Y), so we have
E[kx Zkâˆ’1
Z Z( fZ âˆ’Î¼ fZ |Y )( fZ âˆ’Î¼ fZ |Y )âŠ¤kâˆ’1
Z ZkZx] = kx Zkâˆ’1
Z Z fZ |Y kâˆ’1
Z ZkZx = kx Z Qâˆ’1kZx,
(6.29)
where fZ varies with the given Y. Furthermore, from (6.16), since the variance
Î»(x) = k(x, x) âˆ’kx Zkâˆ’1
Z ZkZx of f (x)| fZ is independent of fZ, we can write the
variance of f (x)|Y as the sum of the variance Î»(x) of f (x)| fZ and (6.29). In other
words, we have Ïƒ 2(x) = Î»(x) + kx Z Qâˆ’1kZx.
â–¡
In a case when the inducing variable method is employed, the calculations of
kZ Z, kx Z take O(M2) and O(M), respectively, the calculation of  takes O(N),
and the calculations of Q and Qâˆ’1 take O(N M2) and O(M3), respectively. The
multiplication process is also completed in O(N M2). On the other hand, without
the inducing variable method, it takes O(N 3) of computational time. In the inducing
variable method, we do not use the matrix K X X âˆˆRNÃ—N.
We can randomly select the inducing points z1, Â· Â· Â· , zM from x1, Â· Â· Â· , xN or via
K-means clustering.
Example 88 Based on the above discussion, we constructed the function gp_ind by
using the inducing variable method and compared its performance with that of gp_1,
which does not use the inducing variable method.

184
6
Gaussian Processes and Functional Data Analyses
sigma_2 =
0.05 # should
be
e s t i m a t e d
def k ( x ,
y ) :
# Covariance
f u n c t i o n
return np . exp ( âˆ’(x âˆ’y ) âˆ—âˆ—2
/
2
/
sigma_2 )
def mu( x ) :
# Mean f u n c t i o n
return x
# Data
Generation
n = 200
x = np . random . uniform ( s i z e = n ) âˆ—6 âˆ’3
y = np . s i n ( x
/
2) + np . random . randn ( n )
eps = 10âˆ—âˆ—( âˆ’6)
m = 100
K = np . zeros ( ( n ,
n ) )
for
i
in
range ( n ) :
for
j
in
range ( n ) :
K[ i ,
j ] = k ( x [ i ] ,
x [ j ] )
index = np . random . choice ( n ,
s i z e = m,
r e p l a c e = False )
z = x [ index ]
m_x = 0
m_z = 0
K_zz = np . zeros ( (m, m) )
for
i
in
range (m) :
for
j
in
range (m) :
K_zz [ i ,
j ] = k ( z [ i ] ,
z [ j ] )
K_xz = np . zeros ( ( n , m) )
for
i
in
range ( n ) :
for
j
in
range (m) :
K_xz [ i ,
j ] = k ( x [ i ] ,
z [ j ] )
K_zz_inv = np . l i n a l g . inv ( K_zz + np . diag ([10âˆ—âˆ—eps ]âˆ—m) )
lam = np . zeros ( n )
for
i
in
range ( n ) :
lam [ i ] = k ( x [ i ] ,
x [ i ] )âˆ’np . dot ( np . dot ( K_xz [ i ,
0:m] ,
K_zz_inv ) , K_xz [ i , 0 :
m] )
lam_0_inv = np . diag ( 1 / ( lam+sigma_2 ) )
Q = K_zz + np . dot ( np . dot ( K_xz . T ,
lam_0_inv ) , K_xz )
## Computation
of Q
does
not
r e q u i r e O( n ^3)
Q_inv = np . l i n a l g . inv (Q + np . diag ( [ eps ] âˆ—m) )
muu = np . dot ( np . dot ( np . dot ( Q_inv ,
K_xz . T) ,
lam_0_inv ) , yâˆ’m_x)
d i f = K_zz_inv âˆ’Q_inv
R = np . l i n a l g . inv (K + sigma_2 âˆ—np . i d e n t i t y ( n ) )
## O( n ^3)
omputation
i s
r e q u i r e d
def
gp_ind ( x_pred )
: ## Inducing
Variable
Method
h = np . zeros (m)
for
i
in
range (m) :
h [ i ] = k ( x_pred ,
z [ i ] )
mm = mu( x_pred ) + h . dot (muu)
ss = k ( x_pred ,
x_pred ) âˆ’h . dot ( d i f ) . dot ( h )
return {"mm": mm, "ss":
ss }
def gp_1 ( x_pred ) :
## W/O Inducing
Variable
Method
h = np . zeros ( n )
for
i
in
range ( n ) :
h [ i ] = k ( x_pred ,
x [ i ] )
mm = mu( x_pred ) + np . dot ( np . dot ( h . T , R) , yâˆ’mu( x ) )
ss = k ( x_pred ,
x_pred ) âˆ’np . dot ( np . dot ( h . T , R) , h )
return {"mm": mm, "ss":
ss }
x_seq = np . arange ( âˆ’2 ,
2. 1 ,
0 . 1 )
mmv =
[ ] ;
ssv = [ ]

6.3 Gaussian Processes with Inducing Variables
185
for u in
x_seq :
mmv. append ( gp_ind ( u ) ["mm"] )
ssv . append ( gp_ind ( u ) ["ss"] )
p l t . f i g u r e ( )
p l t . p l o t ( x_seq , mmv,
c = "r")
p l t . p l o t ( x_seq ,
np . a r r a y (mmv) + 3 âˆ—np . s q r t ( np . a r r a y ( ssv ) ) ,
c = "r" ,
l i n e s t y l e
= "âˆ’âˆ’")
p l t . p l o t ( x_seq ,
np . a r r a y (mmv) âˆ’3 âˆ—np . s q r t ( np . a r r a y ( ssv ) ) ,
c = "r" ,
l i n e s t y l e
= "âˆ’âˆ’")
p l t . xlim ( âˆ’2 ,
2)
p l t . p l o t ( np . min (mmv) , np . max(mmv) )
x_seq = np . arange ( âˆ’2 ,
2. 1 ,
0 . 1 )
mmv =
[ ] ;
ssv
=[]
for u in
x_seq :
mmv. append ( gp_1 ( u ) ["mm"] )
ssv . append ( gp_1 ( u ) ["ss"] )
mmv = np . a r r a y (mmv)
ssv = np . a r r a y ( ssv )
p l t . p l o t ( x_seq , mmv,
c = "b")
p l t . p l o t ( x_seq , mmv + 3 âˆ—np . s q r t ( ssv ) , c = "b" ,
l i n e s t y l e
= "âˆ’âˆ’")
p l t . p l o t ( x_seq , mmv âˆ’3 âˆ—np . s q r t ( ssv ) , c = "b" ,
l i n e s t y l e
= "âˆ’âˆ’")
p l t . s c a t t e r ( x ,
y ,
f a c e c o l o r s =â€™noneâ€™ ,
e d g e c o l o r s = "k" ,
marker = "o")
6.4
Karhunen-LÃ³eve Expansion
In this section, we continue to study the probability space (, F, P) and the map f :
 Ã— E âˆ‹(Ï‰, x) â†’f (Ï‰, x) âˆˆH. We assume that H is a general separable Hilbert
space. In the following, we continue to denote f (Ï‰, x) by f (x) as a random variable
for each x âˆˆE. In particular, we assume that f is a mean-square continuous process
(Fig.6.2), which is deï¬ned by
lim
nâ†’âˆE| f (Ï‰, xn) âˆ’f (Ï‰, x)|2 = 0
(6.30)
for an arbitrary {xn} in E that converges to x âˆˆE. We do not assume a Gaussian
process, and we give the expectation at x âˆˆE and the covariance at x, y âˆˆE by
m(x) = E f (Ï‰, x)
and
k(x, y) = Cov( f (Ï‰, x), f (Ï‰, y)) .
In Chap.5, we obtained the expectation and covariance of k(X, Â·); in this section,
however, x, y âˆˆE are not random, and the randomness of m, k is due to that of
f (Ï‰, Â·).
In the following, we assume that E is compact.

186
6
Gaussian Processes and Functional Data Analyses
Fig. 6.2 The red and blue
curves show the results
obtained by the inducing
variable and standard
Gaussian processes,
respectively
-2
-1
0
1
2
-3
-2
-1
0
1
2
3
x
0
Proposition 66
f is a mean-square continuous process if and only if m and k are
continuous.
Proof: See the Appendix at the end of this chapter.
In the following, we assume that m â‰¡0 to simplify the discussion. Since E is
compact, we assume that the diameter of each Ei is less than or equal to 1/n.
However, each Ei is a metric space, and we deï¬ne the diameter by the maximum
distance among the elements in Ei. Thus, there exists a partition of E (âˆªM(n)
i=1 Ei = E,
Ei âˆ©E j = Ï†, i Ì¸= j) and a number of partitions M(n). Then, we deï¬ne
I f (g; {(Ei, xi)}1â‰¤iâ‰¤M(n)) :=
M(n)
	
i=1
f (Ï‰, xi)

Ei
g(y)dÎ¼(y)
for a pair of interior points {(Ei, xi)}1â‰¤iâ‰¤M(n) and g âˆˆL2(E, B(E), Î¼). Hence, we
have


{I f (g; {(Ei, xi)}1â‰¤iâ‰¤M(n))}2d P(Ï‰) â‰¤M(n)
M(n)
	
i=1


{ f (Ï‰, xi)}2{

Ei
g(u)dÎ¼(u)}2
d P = M(n)
M(n)
	
i=1
k(xi, xi)

Ei
{g(u)}2dÎ¼(u) < âˆ
and I f (g; {(Ei, xi)}1â‰¤iâ‰¤M(n)) âˆˆL2(, F, P). Although this value is different
depending on the choices of the region decomposition and the points inside the
regions, the difference in I f converges to zero as n goes to inï¬nity. In fact, we have

6.4 Karhunen-LÃ³eve Expansion
187
E

|I f (g; {(Ei, xi)}1â‰¤iâ‰¤M(n)) âˆ’I f (g; {(Eâ€²
j, xâ€²
j)}1â‰¤jâ‰¤M(nâ€²))|2
=
M(n)
	
i=1
M(n)
	
iâ€²=1
k(xi, xiâ€²)

Ei
g(u)dÎ¼(u)

Eiâ€²
g(v)dÎ¼(v)
+
M(nâ€²)
	
j=1
M(nâ€²)
	
jâ€²=1
k(x j, x jâ€²)

E j
g(u)dÎ¼(u)

E jâ€²
g(v)dÎ¼(v)
âˆ’2
M(n)
	
i=1
M(nâ€²)
	
j=1
k(xi, x jâ€²)

Ei
g(u)dÎ¼(u)

E jâ€²
g(v)dÎ¼(v) .
Since k is uniformly continuous, each double sum on the right-hand side converges
to

E

E
k(u, v)g(u)g(v)dÎ¼(u)dÎ¼(v) .
Since the Cauchy sequence converges to zero, its convergence destination I f (Ï‰, g)
is contained in L2(Ï‰, F, P) regardless of the choice of {(Ei, xi)}1â‰¤iâ‰¤M(n).
If the eigenvalues and eigenfunctions obtained from the integral operator Tk âˆˆ
B(L2(E, B(E), Î¼)),
Tkg(Â·) =

E
k(y, Â·)g(y)dÎ¼(y) , g âˆˆL2(E, B(E), Î¼),
are {Î» j}âˆ
j=1 and {e j(Â·)}âˆ
j=1, by Mercerâ€™s theorem, we can express the covariance
function k as
k(x, y) =
âˆ
	
j=1
Î» je j(x)e j(y) ,
(6.31)
where the sum absolutely and uniformly converges on that support.
Then, we have the following claim.
Proposition 67 If { f (Ï‰, x)}xâˆˆE is a mean-square continuous process with a mean
of zero, we have
1. E[I f (Ï‰, g)] = 0.
2. E[I f (Ï‰, g) f (Ï‰, x)] =

E k(x, y)g(y)dÎ¼(y), x âˆˆE.
3. E[I f (Ï‰, g)I f (Ï‰, h)] =

E

E k(x, y)g(x)h(y)dÎ¼(x)dÎ¼(y).
These properties hold for each g, h âˆˆL2(E, F, Î¼), and in particular, we have that
E[I f (Ï‰, ei)I f (Ï‰, e j)] = Î´i, jÎ»i .
(6.32)
Proof: For the proofs of the above three items, see the Appendix at the end of this
chapter. We obtain (6.32) by substituting Mercerâ€™s theorem (6.31), g = ei, and h = e j
into the third item:

188
6
Gaussian Processes and Functional Data Analyses
E[I f (Ï‰, ei)I f (Ï‰, e j)] =

E

E
âˆ
	
r=1
Î»rer(x)er(y)ei(x)e j(y)dÎ¼(x)dÎ¼(y).
â–¡
Furthermore, we have the following theorem.
Proposition 68 (Karhunen-LÃ³eve [17, 18])Suppose that { f (Ï‰, x)}xâˆˆE is a mean-
square continuous process with a mean of zero. Then, we have
lim
nâ†’âˆsup
xâˆˆE
E| f (Ï‰, x) âˆ’fn(Ï‰, x)|2 = 0
for fn(Ï‰, x) := 
n
j=1 I f (Ï‰, e j)e j(x).
Proof: From (6.32), we have
E[ fn(Ï‰, x)2] = E[{
n
	
j=1
I f (Ï‰, e j)e j(x)}2]
=
n
	
i=1
n
	
j=1
E[I f (Ï‰, ei)I f (Ï‰, e j)]ei(x)e j(x) =
n
	
j=1
Î» je2
j(x) .
Moreover, from (6.31) and the second item of Proposition 67, we have
E[ fn(Ï‰, x) f (Ï‰, x)] = E[
n
	
j=1
I f (Ï‰, e j)e j(x) f (Ï‰, x)] =
n
	
j=1
e j(x)

E
k(x, y)e j(y)dÎ¼(y)
=
n
	
j=1
Î» je2
j(x)

E
e2
j(y)dÎ¼(y) =
n
	
j=1
Î» je2
j(x) ,
which means that
E| fn(Ï‰, x) âˆ’f (Ï‰, x)|2 = E[ fn(Ï‰, x)2] âˆ’2E[ fn(Ï‰, x) f (Ï‰, x)] + E[ f (Ï‰, x)2]
=
n
	
j=1
Î» je2
j(x) âˆ’2
n
	
j=1
Î» je2
j(x) + k(x, x) = k(x, x) âˆ’
n
	
j=1
Î» je2
j(x) .
â–¡
In a general mean-square continuous process (without assuming a Gaussian pro-
cess), the series expansion provided by Karhunen-LÃ³eveâ€™s theorem makes I f (Ï‰, e j)/
Î» j a random variable with a mean of 0 and a variance of 1. Instead, if we assume
a Gaussian process such that f (x) (x âˆˆE) follows a Gaussian distribution, then we
can write
fn(x) =
n
	
j=1
z j

Î» je j(x) ,
(6.33)

6.4 Karhunen-LÃ³eve Expansion
189
where z j follows an independent standard Gaussian distribution.
Let E := [0, 1] and (, F, P) be a probability space. Then, we call the map
 Ã— E âˆ‹(Ï‰, x) â†’f (Ï‰, x) âˆˆR that satisï¬es the following conditions a Brownian
motion.
1. f (Ï‰, 0) = 0, f (Ï‰, x) âˆ’f (Ï‰, y) âˆ¼N(0, y âˆ’x), and 0 â‰¤x < y.
2. f (Ï‰, x2) âˆ’f (Ï‰, x1), . . . , f (Ï‰, xnâˆ’1) âˆ’f (Ï‰, xn) are independent for any n =
1, 2, . . . and 0 â‰¤x1 < x2 < . . . < xn.
3. There exists an  âˆˆF with a probability of 1, and E âˆ‹x â†’f (Ï‰, x) is contin-
uous for each Ï‰ âˆˆ.
In this case, we have the following proposition.
Proposition 69 The map  Ã— E âˆ‹(Ï‰, x) â†’f (Ï‰, x) âˆˆR is a Brownian motion if
and only if the following three conditions are satisï¬ed simultaneously.
1. It is a Gaussian process.
2. The covariance function of x, y âˆˆE is given by k(x, y) = min(x, y).
3.
f (Ï‰, Â·) is continuous with a probability of 1.
Proof:Theï¬rsttwoconditionsinthedeï¬nitionimplytheï¬rstconditioninProposition
69. Moreover, if x < y, then we have
E[ f (Ï‰, x) f (Ï‰, y)] = E[ f (Ï‰, x)2] + E[ f (Ï‰, x){ f (Ï‰, y) âˆ’f (Ï‰, x)}] = x ,
which implies the second condition of Proposition 69. On the contrary, supposing
that m â‰¡0 for simplicity, if we assume that the ï¬rst two items of Proposition 69
hold, then because k(x, x) = x, when x â‰¤y â‰¤z, we have
E[ f (Ï‰, x){ f (Ï‰, y) âˆ’f (Ï‰, z)}] = k(x, y) âˆ’k(x, z) = x âˆ’x = 0
and
E[ f (Ï‰, y){ f (Ï‰, y) âˆ’f (Ï‰, z)}] = k(y, y) âˆ’k(y, z) = y âˆ’y = 0 ,
which implies that
E[{ f (Ï‰, x) âˆ’f (Ï‰, y)}{ f (Ï‰, y) âˆ’f (Ï‰, z)}] = 0 .
Moreover, from k(0, 0) = 0, the variance of f (Ï‰, 0) is zero, so we have f (Ï‰, 0) = 0.
Furthermore, we have
E[{ f (Ï‰, x) âˆ’f (Ï‰, y)}2] = k(x, x) âˆ’2k(x, y) + k(y, y) = x âˆ’2x + y = y âˆ’x .
â–¡
Example 89 (Brownian Motion as a Gaussian Process)For the integral operator
(Example 58) on the covariance function of a Brownian motion k(x, y) = min(x, y)
(x, y âˆˆE), its eigenvalues and eigenfunctions are (3.13) and (3.14), respectively.

190
6
Gaussian Processes and Functional Data Analyses
Utilizing these eigenvalues and eigenfunctions, we can expand f (Ï‰, Â·) as follows.
In particular, from (6.33), we have
fn(x) =
n
	
j=1
z j(Ï‰)

Î» je j(x)
for z j âˆ¼N(0, 1). We generate the series {zi(Ï‰)}n
i=1 m times (Fig.6.3; n = 10;
m = 7). We execute it with the following code.
def lam ( j ) :
## EigenValue
return 4
/
( ( 2 âˆ—
j âˆ’1) âˆ—np . pi ) âˆ—âˆ—2
def
ee ( j ,
x ) :
##
D e f i n i t i o n
of
E i g e n f u n c t i o n
return np . s q r t ( 2 ) âˆ—np . s i n ( ( 2 âˆ—
j âˆ’1) âˆ—np . pi
/
2 âˆ—x )
n = 10; m = 7
##
D e f i n i t i o n
of
Gaussian
Process
def
f ( z ,
x ) :
n = len ( z )
S = 0
for
i
in
range ( n ) :
S = S + z [ i ] âˆ—ee ( i ,
x ) âˆ—np . s q r t ( lam ( i ) )
return S
p l t . f i g u r e ( )
p l t . xlim (0 ,
1)
p l t . x l a b e l ("x")
p l t . y l a b e l ("f(omega,â£x)")
colormap =
p l t . cm . g i s t _ n c a r
# n i p y _ s p e c t r a l ,
Set1 , Paired
c o l o r s = [ colormap ( i )
for
i
in np . l i n s p a c e (0 ,
0. 8 , m) ]
for
j
in
range (m) :
z = np . random . randn ( n )
x_seq = np . arange (0 ,
3.001 ,
0.001)
y_seq = [ ]
for x in
x_seq :
y_seq . append ( f ( z ,
x ) )
p l t . p l o t ( x_seq ,
y_seq ,
c = c o l o r s [ j ] )
p l t . t i t l e ("Brownâ£Motion")
We introduce the MatÃ©rn class, which is a class of kernels used in stochastic
processes rather than the RKHS of machine learning. Such a kernel is k(x, y) = Ï•(z)
(z := x âˆ’y in x.y âˆˆE), where Ï•(z) is
Ï•(z) :=
21âˆ’Î½
(Î½)
 âˆš
2Î½z
l
 KÎ½(
âˆš
2Î½z
l
) ,
(6.34)
Î½,l > 0 are the parameters of the kernel, and KÎ½ is a variant Bessel function of the
second kind.
KÎ½(x) := Ï€
2
Iâˆ’Î±(x) âˆ’IÎ±(x)
sin(Î±x)

6.4 Karhunen-LÃ³eve Expansion
191
0.0
0.2
0.4
0.6
0.8
1.0
-2
-1
0
1
2
x
f(Ï‰, x)
Brownian Motion
Fig. 6.3 We generated the sample paths of Brownian motions seven times. Each run involved a
sum of up to 10 terms
IÎ±(x) :=
âˆ
	
m=0
1
m!(m + Î± + 1)
x
2
2m+Î±
.
In practice, we use (6.34), in which p is a positive integer and Î½ = p + 1/2. In
the 1-dimensional case, we have
Ï•Î½(z) = exp

âˆ’
âˆš
2Î½z
l

(p + 1)
(2p + 1)
p
	
i=0
(p + i)!
i!(p âˆ’i)!
âˆš
8Î½z
l
pâˆ’i
.
(6.35)
For example, we express Î½ = 5/2, 3/2, 1/2 as follows. In particular, we call the
stochastic process with Î½ = 1/2 the Ornstein-Uhlenbeck process.
Ï•5/2(z) =

1 +
âˆš
5z
l
+ 5z2
3l2

exp(âˆ’
âˆš
5z
l
)
Ï•3/2(z) =

1 +
âˆš
3z
l

exp(âˆ’
âˆš
3z
l
)
Ï•1/2(z) = exp(âˆ’z/l).
For example, if we write this process in Python, we have the following code.

192
6
Gaussian Processes and Functional Data Analyses
0.0
0.1
0.2
0.3
0.4
0.5
0
2
4
6
8
10
z
Kernel Values Ï•(z)
Î½ = 1.5
Î½ = 2.5
Î½ = 3.5
Î½ = 4.5
Î½ = 5.5
Î½ = 6.5
Î½ = 7.5
Î½ = 8.5
Î½ = 9.5
Î½ = 10.5
Matern Kernel (l=0.1)
0.0
0.1
0.2
0.3
0.4
0.5
0
2
4
6
8
10
z
Kernel Values Ï•(z)
Î½ = 1.5
Î½ = 2.5
Î½ = 3.5
Î½ = 4.5
Î½ = 5.5
Î½ = 6.5
Î½ = 7.5
Î½ = 8.5
Î½ = 9.5
Î½ = 10.5
Matern Kernel (l = 0.02)
Fig. 6.4 The values of the MatÃ©rn kernel for Î½ = 1/2, 3/2, . . . , m + 1/2 (Example 90). l = 0.1
(left) and l = 0.02 (right)
from
scipy . s p e c i a l
import gamma
def
matern ( nu ,
l ,
r ) :
p = nu âˆ’1
/
2
S = 0
for
i
in
range ( i n t ( p +1) ) :
S = S + gamma( p + i + 1)
/ gamma( i + 1)
/ gamma( p âˆ’i + 1) \
âˆ—( np . s q r t (8 âˆ—nu ) âˆ—r
/
l ) âˆ—âˆ—( p âˆ’i )
S = S âˆ—gamma( p + 2)
/ gamma(2 âˆ—p + 1) âˆ—np . exp(âˆ’np . s q r t (2 âˆ—nu ) âˆ—r
/
l )
return S
Example 90 We present the MatÃ©rn kernel values for l = 0.1, 0.02 with Î½ =
1/2, 3/2, . . . , m + 1/2 (Fig.6.4).
m = 10
l = 0.1
colormap =
p l t . cm . g i s t _ n c a r
# n i p y _ s p e c t r a l ,
Set1 , Paired
c o l o r = [ colormap ( i )
for
i
in np . l i n s p a c e (0 ,
1 ,
len ( range (m) ) ) ]
x = np . l i n s p a c e (0 ,
0. 5 ,
200)
p l t . p l o t ( x ,
matern (1 âˆ’1/2 ,
l ,
x ) , c = c o l o r [ 0 ] ,
l a b e l = r"$\nu=%d$"%1)
p l t . ylim (0 ,
10)
for
i
in
range (2 , m + 1) :
p l t . p l o t ( x ,
matern ( i âˆ’1/2 ,
l ,
x ) , c = c o l o r [ i âˆ’1] ,
l a b e l = r"$\nu=%d$"
%i )
p l t . legend ( loc = "upperâ£right" ,
frameon = True ,
prop ={â€™sizeâ€™: 1 4 } )
In the case of the MatÃ©rn kernel and in general, we cannot analytically obtain
the eigenvalues and eigenfunctions, as in the cases involving Gaussian kernels and
Brownian motion. Even in those cases, if we assume a Gaussian process, then we can
ï¬nd x1, . . . , xn âˆˆE to obtain its Gram matrix, which will be a covariance matrix.

6.4 Karhunen-LÃ³eve Expansion
193
0.0
0.2
0.4
0.6
0.8
1.0
-3
-2
-1
0
1
2
3
x
y
OU Process (Î½ = 1/2, l = 0.1)
0.0
0.2
0.4
0.6
0.8
1.0
-3
-2
-1
0
1
2
3
x
y
Matern Process (Î½ = 3/2, l = 0.1)
Fig. 6.5 The Orstein-Uhlenbeck process (Î½ = 1.2, top) and the MatÃ©rn process (Î½ = 3/2, top) for
l = 0.1
Thus, it is sufï¬cient to generate n-variate random numbers that follow a Gaussian
distribution. The above method is approximate, but it is very versatile.
Example 91 We display the Orstein-Uhlenbeck process (Î½ = 1.2, top) and the
MatÃ©rn process (Î½ = 3/2, top) with n = 100 and l = 0.1 (Fig.6.5).
colormap =
p l t . cm . g i s t _ n c a r
# n i p y _ s p e c t r a l ,
Set1 , Paired
c o l o r s = [ colormap ( i )
for
i
in np . l i n s p a c e (0 ,
0. 8 ,
5) ]
def
rand_100 ( Sigma ) :
L = np . l i n a l g . cholesky ( Sigma )
## Cholesky
decomposition
of
covariance
matrix

194
6
Gaussian Processes and Functional Data Analyses
u = np . random . randn (100)
y = L . dot ( u )
## Generate random numbers
with
zeroâˆ’mean and
the
covariance
matrix
return y
x = np . l i n s p a c e (0 ,
1 ,
100)
z = np . abs ( np . s u b t r a c t . o u t e r ( x , x ) )
# compute
d i s t a n c e
matrix ,
d_ { i j } = | x_i
âˆ’x_j |
l = 0.1
Sigma_OU = np . exp(âˆ’z
/
l )
## OU:
matern ( 1 / 2 , l , z )
i s
slow
y = rand_100 ( Sigma_OU )
p l t . f i g u r e ( )
p l t . p l o t ( x ,
y )
p l t . ylim ( âˆ’3 ,3)
for
i
in
range ( 5 ) :
y = rand_100 ( Sigma_OU )
p l t . p l o t ( x ,
y ,
c = c o l o r s [ i ] )
p l t . t i t l e ("OUâ£processâ£(nuâ£=â£1/2,â£lâ£=â£0.1)")
Sigma_M = matern ( 3 / 2 ,
l ,
z )
## Matern
y = rand_100 ( Sigma_M )
p l t . f i g u r e ( )
p l t . p l o t ( x ,
y )
p l t . ylim ( âˆ’3 ,
3)
for
i
in
range ( 5 ) :
y = rand_100 ( Sigma_M )
p l t . p l o t ( x ,
y ,
c = c o l o r s [ i ] )
p l t . t i t l e ("Maternâ£processâ£(nuâ£=â£3/2,â£lâ£=â£0.1)")
6.5
Functional Data Analysis
Let (, F, P) and H be a probability space and a separable Hilbert space, respec-
tively. Let F :  â†’H be a measurable map, i.e., {h âˆˆH| âˆ¥g âˆ’hâˆ¥< r}) is an ele-
ment of F for each open set (g âˆˆH,r âˆˆ(0, âˆ) in H. We call such an F :  â†’H
a random element of H. Intuitively, a random element is a random variable that
takes a value in H. Thus far, we have assumed that f :  Ã— E â†’R is measurable
at each x âˆˆE (stochastic process). This section addresses situations in which we do
not assume such measurability. For simplicity, we write F(Ï‰) as F, similar to the
elements of H.
Although we do not go into details in this book, it is known that the following
relationship holds between stochastic processes and random elements. It is only
necessary to understand the close relationship between the two.
Proposition 70 (Hsing-Eubank [14])
1. If f :  Ã— E â†’R is measurable w.r.t.  Ã— E and f (Ï‰, Â·) âˆˆH, for Ï‰ âˆˆ, then
f (Ï‰, Â·) is a random element of H.
2. If f (Â·, x) â†’R is measurable for each x âˆˆE and f (Ï‰, Â·) is continuous for each
Ï‰ âˆˆ, then f (Ï‰, Â·) is a random element.
3. If f :  Ã— E â†’R is a (zero-mean) mean-square continuous process and its
covariance function is k, then a random element of H exists such that the covari-
ance operator is H âˆ‹g â†’

E k(Â·, y)g(y)dÎ¼(y) âˆˆH.

6.5 Functional Data Analysis
195
4. A random element in an RKHS H(k) with a measurable reproducing kernel k is
a stochastic process, and a stochastic process that takes values in RKHS H(k) is
a random element of H(k).
For the proof, see Chap.7 in [14].
In this section, we learn the properties of random elements and apply them to
functional data analysis [24].
First, we consider the average E[âŸ¨F, gâŸ©] of âŸ¨F, gâŸ©for each g âˆˆH under E[âˆ¥Fâˆ¥] <
âˆ. Since g â†’E[âŸ¨F, gâŸ©] is a linear functional, there exists a unique m âˆˆH such
that
E[âŸ¨F, gâŸ©] = âŸ¨m, gâŸ©
(6.36)
from Proposition 22. We write this formally as m = E[F], which is the deï¬nition of
the mean of a random element F.
Proposition 71 If Eâˆ¥Fâˆ¥2 < âˆ, then
Eâˆ¥F âˆ’mâˆ¥2 = Eâˆ¥Fâˆ¥2 âˆ’âˆ¥mâˆ¥2
holds.
Proof: If we substitute g = m into (6.36), we obtain
Eâˆ¥F âˆ’mâˆ¥2 = Eâˆ¥Fâˆ¥2 âˆ’2EâŸ¨F, mâŸ©+ âˆ¥mâˆ¥2 = Eâˆ¥Fâˆ¥2 âˆ’2âŸ¨m, mâŸ©+ âˆ¥mâˆ¥2 .
â–¡
Since Eâˆ¥Fâˆ¥2 < âˆimplies that Eâˆ¥Fâˆ¥< âˆ, we proceed with our discussion by
assuming the former case.
Regarding covariance, if H = Rp, then the covariance matrix is
E[(F âˆ’E[F])(F âˆ’E[F])âŠ¤] = E[(F âˆ’E[F]) âŠ—(F âˆ’E[F])] âˆˆRpÃ—p
for F âˆˆRp. For the general Hilbert space H, the correspondence
H 2 âˆ‹(g, h) â†’E[âŸ¨F âˆ’m, gâŸ©âŸ¨F âˆ’m, hâŸ©] âˆˆR
is linear for each of g and h. Moreover, if Eâˆ¥Fâˆ¥2 < âˆ, then it is bounded from
E[âŸ¨F âˆ’m, gâŸ©âŸ¨F âˆ’m, hâŸ©] â‰¤Eâˆ¥F âˆ’mâˆ¥2 Â· âˆ¥gâˆ¥âˆ¥hâˆ¥â‰¤Eâˆ¥Fâˆ¥2 Â· âˆ¥gâˆ¥âˆ¥hâˆ¥.
If we deï¬ne u âŠ—v âˆˆB(H) by
H âˆ‹w â†’(u âŠ—v)w = âŸ¨u, wâŸ©v âˆˆH
for u, v, w âˆˆH, then a K âˆˆB(H) exists such that

196
6
Gaussian Processes and Functional Data Analyses
E[âŸ¨F âˆ’m, gâŸ©âŸ¨F âˆ’m, hâŸ©] = E[âŸ¨{(F âˆ’m) âŠ—(F âˆ’m)}g, hâŸ©] = âŸ¨Kg, hâŸ©= âŸ¨g, K âˆ—hâŸ©.
If we exchange g, h, we obtain the same value, so K and K âˆ—coincide, and each is
self-conjugated. Such a K is called a covariance operator, and we formally write this
as K = E[(F âˆ’m) âŠ—(F âˆ’m)].
Proposition 72
E[(F âˆ’m) âŠ—(F âˆ’m)] = E[F âŠ—F] âˆ’m âŠ—m
Proof: From (6.36), for arbitrary g, h âˆˆH, we have
E[âŸ¨m, gâŸ©âŸ¨F, hâŸ©] = âŸ¨m, gâŸ©âŸ¨E[F], hâŸ©= âŸ¨m, gâŸ©âŸ¨m, hâŸ©= âŸ¨(m âŠ—m)g, hâŸ©.
From
âŸ¨F âˆ’m, gâŸ©âŸ¨F âˆ’m, hâŸ©= âŸ¨F, gâŸ©âŸ¨F, hâŸ©âˆ’âŸ¨F, gâŸ©âŸ¨m, hâŸ©âˆ’âŸ¨F, gâŸ©âŸ¨m, hâŸ©+ âŸ¨m, gâŸ©âŸ¨m, hâŸ©,
we have
E[âŸ¨{(F âˆ’m) âŠ—(F âˆ’m)}g, hâŸ©] = E[âŸ¨{F âŠ—F âˆ’m âŠ—m}g, hâŸ©] .
â–¡
In the following, for simplicity, we proceed with our discussion by assuming that
m = 0.
Proposition 73 If m = 0 and Eâˆ¥Fâˆ¥2 < âˆ, then
1. The covariance operator K is nonnegative deï¬nite and is a trace class operator
whose trace is
âˆ¥Kâˆ¥T R = Eâˆ¥Fâˆ¥2 .
2. With probability 1, F âˆˆIm(K) holds.
Proof: For g âˆˆH,
âŸ¨Kg, gâŸ©= E[âŸ¨F, gâŸ¨F, gâŸ©] â‰¥0
holds,whichmeansthat K isnonnegativedeï¬nite.Moreover,if{e j}isanorthonormal
basis, we have
âˆ¥Kâˆ¥T R =
âˆ
	
j=1
âŸ¨Ke j, e jâŸ©=
âˆ
	
j=1
âŸ¨E[F âŠ—F]e j, e jâŸ©= Eâˆ¥Fâˆ¥2 < âˆ.
For the second item, note that in general, we have
(Im(K))âŠ¥= Ker(K) .
(6.37)

6.5 Functional Data Analysis
197
In fact, if we set g âˆˆKer(K), then K is self-adjoint (K = K âˆ—) and
âŸ¨g, KhâŸ©= âŸ¨Kg, hâŸ©= 0 , h âˆˆH .
Therefore, g is orthogonal to any element of Im(K), and we require g âˆˆIm(K)âŠ¥.
Conversely, if g âˆˆIm(K)âŠ¥, then K Kg âˆˆIm(K) and âˆ¥Kgâˆ¥= âŸ¨g, K KgâŸ©= 0, i.e.,
we have g âˆˆKer(K). This means that for g âˆˆIm(K)âŠ¥, we have E[âŸ¨F, gâŸ©2] =
âŸ¨Kg, gâŸ©= 0, and F is orthogonal to any g âˆˆIm(K)âŠ¥with a probability of 1. There-
fore, from Proposition 20, with a probability of 1, we have
F âˆˆ(Im(K))âŠ¥âŠ¥= Im(K) .
â–¡
Additionally, from Propositions 27 and 31 and the ï¬rst item of Proposition 73,
the following holds.
Proposition 74 The eigenvalue function {e j} of the covariance operator K is an
orthonormal basis of Im(K); the corresponding eigenvalues {Î» j}âˆ
j=1 are nonnegative,
monotonically decrease, and converge to 0. Furthermore, the multitude of each of
the nonzero eigenvalues is ï¬nite.
Additionally, from Propositions 73 and 74, the following holds.
Proposition 75 If { f j} is an orthonormal basis of H, then we have
Eâˆ¥F âˆ’
n
	
j=1
âŸ¨F, f jâŸ©f jâˆ¥2 = Eâˆ¥Fâˆ¥2 âˆ’
n
	
j=1
âŸ¨K f j, f jâŸ©,
(6.38)
which is minimized when f j = e j (1 â‰¤j â‰¤n).
Proof: The following two equations imply (6.38).
Eâˆ¥F âˆ’
n
	
j=1
âŸ¨F, f jâŸ©f jâˆ¥2=Eâˆ¥Fâˆ¥2+Eâˆ¥
n
	
j=1
âŸ¨F, f jâŸ©f jâˆ¥2 âˆ’2E
â¡
â£âŸ¨F,
n
	
j=1
âŸ¨F, f jâŸ©f jâŸ©
â¤
â¦
Eâˆ¥
n
	
j=1
âŸ¨F, f jâŸ©f jâˆ¥2 = E
â¡
â£âŸ¨F,
n
	
j=1
âŸ¨F, f jâŸ©f jâŸ©
â¤
â¦=
n
	
j=1
E

âŸ¨F, f jâŸ©2
=
n
	
j=1
âŸ¨K f j, f jâŸ©.
Then, from Eâˆ¥Fâˆ¥2 = âˆ¥Kâˆ¥T R = 
âˆ
j=1 Î» j (Proposition 73) and Proposition 28, we
obtain Proposition 75.
â–¡
For example, from the independent realizations F1, . . . , FN of the random element
F, via
m N = 1
N
N
	
i=1
Fi
(6.39)

198
6
Gaussian Processes and Functional Data Analyses
KN = 1
N
N
	
i=1
(Fi âˆ’m N) âŠ—(Fi âˆ’m N) ,
(6.40)
we can estimate the mean m and covariance operator1 K.
In the following, we examine how to perform principal component analysis (PCA)
based on functional data analysis [24].
Then, to obtain the eigenfunctions and eigenvalues, for x1, . . . , xn âˆˆE, 1 â‰¤
n â‰¤N, Fi : E â†’R, we apply the ordinary (nonfunctional) PCA approach to
X = (Fi(xk)) (i = 1, . . . , N and k = 1, . . . , n).
1. Prepare the basis function Î· = [Î·1, . . . , Î·m]âŠ¤: E â†’Rm.
2. Calculate W = (wi, j) =

E Î·(x)Î·(x)âŠ¤dx such that W = (wi, j) =

E Î·i(x)Î· j
(x)dx.
3. Find C = (ci, j)i=1,...,N, j=1,...,m âˆˆRNÃ—m such that Fi(x) = 
m
j=1 câŠ¤
i Î· j(x).
4. Find the coefï¬cients d1, . . . , dm of the estimated mean function m N(x) :=
1
N

N
i=1 Fi(x) (m N(x) = 
m
j=1 dâŠ¤
j Î· j(x)).
5. Since the variance function is
k(x, y) = 1
N
N
	
i=1
{Fi(x) âˆ’mN (x)}{Fi(y) âˆ’mN (y)} = 1
N Î·(x)T (C âˆ’d)âŠ¤(C âˆ’d)Î·(y),
if we set the eigenvectors as Ï†(x) = bâŠ¤Î·(x) (b âˆˆRm), then the eigenvalue prob-
lem for the covariance operator

E
k(x, y)Ï†(y)dy = Î»Ï†(x)
under bâŠ¤Wb = 1 reduces to the problem of ï¬nding a b such that
Î·(x)âŠ¤1
N (C âˆ’d)âŠ¤(C âˆ’d)Î·(x)Î·(x)âŠ¤b = Î»Î·(x)âŠ¤b ,
which is equivalent to
1
N (C âˆ’d)âŠ¤(C âˆ’d)Wb = Î»b .
In particular, if we set u := W 1/2b, it becomes the problem of ï¬nding a u âˆˆRm
such that
1
N W 1/2(C âˆ’d)âŠ¤(C âˆ’d)W 1/2u = Î»u
under âˆ¥uâˆ¥= 1.
1 The denominator of KN may be N âˆ’1.

6.5 Functional Data Analysis
199
Example 92 For E = [âˆ’Ï€, Ï€], if we set
Î· j(x) =
â§
âªâ¨
âªâ©
1
âˆš
2Ï€ ,
j = 1
1
âˆšÏ€ cos kx, j = 2k
1
âˆšÏ€ sin kx, j = 2k + 1
,
we have
 Ï€
âˆ’Ï€
Î·i(x)Î· j(x)dx = Î´i, j,
and W is the unit matrix of size p. Therefore, the eigenequation becomes 1
n (C âˆ’
d)âŠ¤(C âˆ’d)u = Î»u, and we can apply C âˆˆRnÃ—p instead of the design matrix to the
PCA procedure (even if we set d = 0 in the above procedure, the centering step
will be completed automatically). In this example, we apply Canadian weather data
from the fda package containing a daily list of, the temperature and precipitation
for each day of the year in each Canadian city. We construct the following programs
in various ways. We do not give the n functions from the beginning but from N =
365 days, as this represents the change in temperature by a linear sum of p bases
(Fourie transformation). Therefore, we can say that the function is discretized using
a sufï¬ciently large p.
X,
y = skfda . d a t a s e t s . f e t c h _ w e a t h e r ( return_X_y=True ,
as_frame=True )
df = X. i l o c [ : ,
0 ] . values
def g ( j ,
x ) :
##
Basis
c o n s i s t i n g
of p elements
i f
j == 0:
return 1
/
np . s q r t (2 âˆ—np . pi )
i f
j % 1 == 0:
return np . cos ( ( j
/ /
2) âˆ—x )
/
np . s q r t ( np . pi )
e l s e :
return np . s i n ( ( j
/ /
2) âˆ—x )
/
np . s q r t ( np . pi )
def
beta ( x ,
y ) :
##
C o e f f i c i e n t s
in
f r o n t
of
the p elements
X = np . zeros ( (N,
p ) )
for
i
in
range (N) :
for
j
in
range ( p ) :
X[ i ,
j ] = g ( j ,
x [ i ] )
beta = np . dot ( np . dot ( np . l i n a l g . inv ( np . dot (X. T , X)
+ 0.0001âˆ—np . i d e n t i t y ( p ) ) , X. T) , y )
return np . squeeze ( beta )
N = 365; n = 35; m = 5; p = 100;
df = df . c o o r d i n a t e s [ 0 ] . d a t a _ m a t r i x
C = np . zeros ( ( n ,
p ) )
for
i
in
range ( n ) :
x = np . arange (1 , N+1) âˆ—(2 âˆ—np . pi
/ N) âˆ’np . pi
y = df [ i ]
C[ i ,
: ]
= beta ( x ,
y )
pca = PCA( )
pca . f i t (C)
B = pca . components_ . T
xx = C. dot (B)
Each line of C âˆˆRnÃ—p is the coefï¬cient (p) of a function. Then, B âˆˆRpÃ—m
(m â‰¤p) is the principal component vector, and xx is the score of each function. The
mth column vector of B is the vector of m principal components (the coefï¬cients

200
6
Gaussian Processes and Functional Data Analyses
-3
-2
-1
0
1
2
3
-10
0
10
20
Reconstructions for m = 2, 3, 4, 5, 6
Dates (trandformed from Jan. 1 through Dec. 31 to âˆ’Ï€ through Ï€)
Temperature (Celsius)
Original
m = 2
m = 3
m = 4
m = 5
m = 6
Fig. 6.6 We present the output of approximating Torontoâ€™s annual temperature by using m =
2, 3, 4, 5, 6 principal components. As m increases, the data are faithfully recovered from the original
data
in front of Î· j(x) fpr j = 1, . . . , p). We change m and the function z and run the
following program to see if we can recover the original function.
def z ( i , m,
x ) :
##
The
approximated
f u n c t i o n
using m components
r a t h e r
than m
S = 0
for
j
in
range ( p ) :
for k in
range (m) :
for
r
in
range ( p ) :
S = S + C[ i ,
j ] âˆ—B[ j ,
k ] âˆ—B[ r ,
k ] âˆ—g ( r ,
x )
return S
x_seq = np . arange (âˆ’np . pi ,
np . pi ,
2 âˆ—np . pi
/
100)
p l t . f i g u r e ( )
p l t . xlim(âˆ’np . pi ,
np . pi )
#
p l t . ylim ( âˆ’15 , 25)
p l t . x l a b e l ("Days")
p l t . y l a b e l ("Temp(C)")
p l t . t i t l e ("Reconstructionâ£forâ£eachâ£m")
p l t . p l o t ( x ,
df [ 1 3 ] ,
l a b e l = "Original")
for m in
range (2 ,
7) :
p l t . p l o t ( x_seq ,
z (13 , m,
x_seq ) , c = c o l o r [m] ,
l a b e l = "m=%d"%m)
p l t . legend ( loc = "lowerâ£center" ,
ncol = 2)
Figure6.6 shows the output of approximating the annual temperature in Toronto
by using m = 2, 3, 4, 5, 6 principal components.
Next, we list the principal components in order of increasing eigenvalue and draw
a graph of their contribution ratio (Fig.6.7).

6.5 Functional Data Analysis
201
lam = pca . e x p l a i n e d _ v a r i a n c e _
r a t i o
= lam
/ sum( lam ) # Or use
pca . e x p l a i n e d _ v a r i a n c e _ r a t i o _
p l t . p l o t ( range (1 ,
6) ,
r a t i o [ : 5 ] )
p l t . x l a b e l ("PC1â£throughâ£PC5")
p l t . y l a b e l ("Ratio")
p l t . t i t l e ("Ratio")
The principal component function is a function with the principal component
vector as the coefï¬cients of the basis. It differs from the output of the scikit-fda in
two ways.
1. Because the dates of the year are normalized from January 1-December 31
to [âˆ’Ï€, Ï€], the value of the principal component function is multiplied by
âˆš365/(2Ï€), and the score function is multiplied by âˆš2Ï€/365.
2. Some principal component vectors are multiplied by âˆ’1, resulting in an upside-
down function approximation (which is unavoidable if the packages are different).
Theï¬rst,second,andthirdprincipalcomponentfunctionsappearasshowninFig.6.8.
We use the following program. The ï¬rst principal component is the effect for the
whole year, with winter temperatures inï¬‚uencing the variations between cities.
def h ( coef ,
x ) :
##
Define a
f u n c t i o n
using
c o e f f i c i e n t s
S = 0
for
j
in
range ( p ) :
S = S + coef [ j ] âˆ—g ( j ,
x )
return S
print (B)
p l t . f i g u r e ( )
p l t . xlim(âˆ’np . pi ,
np . pi )
p l t . ylim ( âˆ’1 ,
1)
for
j
in
range ( 3 ) :
p l t . p l o t ( x_seq ,
h (B [ : ,
j ] ,
x_seq ) , c = c o l o r s [ j ] ,
l a b e l = "PC%d"%( j +1) )
p l t . legend ( loc = "best")
[[-5.17047156e-01 -2.43880782e-01 7.48988279e-02 ... 5.48412387e-04
-1.22748578e-03 8.07866592e-01]
[-7.31215100e-01 -3.44899509e-01 1.05922938e-01 ... 7.75572240e-04
-1.73592707e-03 -5.71437836e-01]
Fig. 6.7 Contribution of
temperature to Canadian
weather. We can calculate
the contribution rate as in the
ordinary case where
functional data analysis is
not used
1
2
3
4
5
0.0 0.2 0.4 0.6 0.8
Contribution Ratio
The ï¬rst component through ï¬fth
Contribution Ratio

202
6
Gaussian Processes and Functional Data Analyses
-3
-2
-1
0
1
2
3
-1.0
-0.5
0.0
0.5
1.0
Principal Component Functions
Date (transformed Jan. 1 throuugh Dec. 31 to âˆ’Ï€ through Ï€)
The values of principal component functions
First
Second
Third
Fig. 6.8 The ï¬rst, second, and third principal component functions for temperature in the Canadian
weather data. Some of the principal component functions are multiplied by âˆ’1, which means that
they are upside down compared to those of other packages. Additionally, because the horizontal
axis is normalized by [âˆ’Ï€, Ï€], the value of each eigenfunction is multiplied by âˆš365/(2Ï€)
[ 3.13430279e-01 -6.12932605e-01 1.50738649e-01 ... -6.99018707e-03
1.19586600e-03 -5.41866413e-02]
...
[ 3.08129173e-05 -2.83373696e-03 8.28893867e-03 ... 1.35931675e-01
-2.34867483e-01 1.23616273e-03]
[ 1.47021532e-03 3.25749669e-03 -4.83933350e-03 ... -1.32596334e-01
1.60270831e-01 8.94103792e-03]
[ 1.47021531e-03 3.25749669e-03 -4.83933350e-03 ... -1.32596334e-01
1.60270831e-01 -1.17997796e-02]]
place = X. i l o c [ : ,
1]
index = [9 ,
11 ,
12 ,
13 ,
16 ,
23 ,
25 ,
26]
o t h e r s = [ x for x in
range (34)
i f
x not
in
index ]
f i r s t
= [ place [ i ] [ 0 ]
for
i
in
index ]
print ( f i r s t )
p l t . f i g u r e ( )
p l t . xlim ( âˆ’15 , 25)
p l t . ylim ( âˆ’25 , âˆ’5)
p l t . x l a b e l ("PC1")
p l t . y l a b e l ("PC2")
p l t . t i t l e ("Canadianâ£Weather")
p l t . s c a t t e r ( xx [ others ,
0] ,
xx [ others ,
1] ,
marker = "x" , c = "k")
for
i
in
range ( 8 ) :
l =
p l t . t e x t ( xx [ index [ i ] ,
0] ,
xx [ index [ i ] ,
1] ,
s =
f i r s t [ i ] ,
c = c o l o r [ i ] )
[â€™Qâ€™ , â€™Mâ€™ , â€™Oâ€™ , â€™Tâ€™ , â€™Wâ€™ , â€™Câ€™ , â€™Vâ€™ , â€™Vâ€™]

Appendix
203
Fig. 6.9 Canadian weather
temperature scores, with
warmer regions such as
Vancouver and Victoria
appearing furthest to the left
in the ï¬rst principal
component
-20
-10
0
10
20
30
-15
-10
-5
0
5
10
The Scores in Canadian Weather
First
Second
Q
MO
T
W
C
VV
Q
M
O
T
W
C
V
V
Quebec
Montreal
Ottawa
Toronto
Winnipeg
Calgary
Vancouver
Victoria
Appendix
Proof of Proposition 66
Proof: Since the expectation and variance of f (Ï‰, x) âˆ’m(x) are 0 and k(x, x),
respectively, and the covariance between f (Ï‰, x) âˆ’m(x) and f (Ï‰, y) âˆ’m(y) is
k(x, y), from
E[| f (Ï‰, x) âˆ’f (Ï‰, y)|2]
= E[({ f (Ï‰, x) âˆ’m(x)} âˆ’{ f (Ï‰, y) âˆ’m(y)} âˆ’{m(x) âˆ’m(y)})2]
= k(x, x) + k(y, y) âˆ’2k(x, y) + {m(x) âˆ’m(y)}2.
(6.41)
The continuity of m, k implies (6.30). Conversely, if we assume (6.30), then the
continuity of m is obtained from
|m(x) âˆ’m(y)| = |E[ f (Ï‰, x) âˆ’f (Ï‰, y)]| â‰¤{E[| f (Ï‰, x) âˆ’f (Ï‰, y)|2]}1/2 .
Without loss of generality, if we assume that m â‰¡0, then we have
k(x, y) âˆ’k(xâ€², yâ€²) = {k(x, y) âˆ’k(xâ€², y)} + {k(xâ€², y) âˆ’k(xâ€², yâ€²)},
and each of the right-hand side terms are bounded by
|k(x, y) âˆ’k(xâ€², y)| = |E[ f (Ï‰, x) f (Ï‰, y)] âˆ’E[ f (Ï‰, xâ€²) f (Ï‰, y)]|
â‰¤E[ f (Ï‰, y)2]1/2E[{ f (Ï‰, x) âˆ’f (Ï‰, xâ€²)}2]1/2 = {k(y, y)}1/2{E[| f (Ï‰, x) âˆ’f (Ï‰, xâ€²)|2]}1/2
and
|k(xâ€², y) âˆ’k(xâ€², yâ€²)| â‰¤{k(xâ€², xâ€²)}1/2{E[| f (Ï‰, y) âˆ’f (Ï‰, yâ€²)|2]}1/2 .
Thus, we have established the continuity of k.
â–¡

204
6
Gaussian Processes and Functional Data Analyses
Proof of Proposition 67
We deï¬ne I (n)
f (g) := I f (g; {(Ei, xi)}1â‰¤iâ‰¤M(n)). Then, we have E[I (n)
f (g)] = 0. From
E[ f (Ï‰, x)] = 0, x âˆˆE, and the convergence proven thus far, we obtain the ï¬rst
claim:
|E[I f (Ï‰, g)]| = |E[I f (Ï‰, g) âˆ’I (n)
f (g)]| â‰¤{E[{I f (Ï‰, g) âˆ’I (n)
f (g)}2]}1/2 â†’0
as n â†’âˆ. From the uniform continuity of k, we obtain the second claim:
|E[I f (Ï‰, g) f (Ï‰, x)] âˆ’

E
k(x, y)g(y)dÎ¼(y)|
â‰¤
E[{I f (Ï‰, g) âˆ’I (n)
f (g)} f (Ï‰, x)]
 +|E[I (n)
f (g) f (Ï‰, x)âˆ’

E
k(x, y)g(y)dÎ¼(y)]|
â‰¤|{E[{I f (Ï‰, g) âˆ’I (n)
f (g)}2]}1/2{E[ f (Ï‰, x)2]}1/2
+|
M(n)
	
i=1

Ei
|k(x, xi) âˆ’k(x, y)|g(y)dÎ¼(y)| â†’0
as n â†’âˆ. From
E[I (n)
f (g)I (n)
f (h)] =
M(n)
	
i=1
M(n)
	
j=1
k(xi, x j)

Ei
g(x)dÎ¼(x)

E j
h(y)dÎ¼(y)
â†’

E

E
k(x, y)g(x)h(y)dÎ¼(x)dÎ¼(y) ,
we obtain the third claim:
|E[I f (Ï‰, g)I f (Ï‰, h)] âˆ’

E

E
k(x, y)g(x)h(y)dÎ¼(x)dÎ¼(y)|
â‰¤|E[{I f (Ï‰, g) âˆ’I (n)
f (g)}{I f (Ï‰, h) âˆ’I (n)
f (h)} + {I f (Ï‰, g) âˆ’I (n)
f (g)}I (n)
f (h)
+{I f (Ï‰, h) âˆ’I (n)
f (h)}I (n)
f (g)]
+|E[I (n)
f (g)I (n)
f (h)] âˆ’

E

E
k(x, y)g(x)h(y)dÎ¼(x)dÎ¼(y)|
â‰¤|(E[{I f (Ï‰, g) âˆ’I (n)
f (g)}2])1/2(E[{I f (Ï‰, h) âˆ’I (n)
f (h)}2])1/2
+(E[{I f (Ï‰, g) âˆ’I (n)
f (g)}2])1/2(E[I (n)
f (h)2])1/2
+(E[{I f (Ï‰, h) âˆ’I (n)
f (h)}2])1/2(E[I (n)
f (g)2])1/2
+
	
i
	
j

Ei

E j
|k(x, y) âˆ’k(xi, x j)|g(x)h(y)dÎ¼(x)dÎ¼(y) â†’0 .
â–¡

Exercises 83âˆ¼100
205
Exercises 83âˆ¼100
84. Construct a function gp_sample that generates random numbers f (Ï‰, x1), . . . ,
f (Ï‰, xN) from the mean function m, the covariance function k, and x1, . . . ,
xN âˆˆE foraset E.Then,setm, k togenerate100randomnumbersandexamine
if the covariance matrix matches the m, k.
85. Using Proposition 61, prove (6.3) and (6.4).
86. In the following program, other than the Cholesky decomposition, is there any
step that requires a calculation with O(N 3) complexity?
def gp_2 ( x_pred ) :
h = np . zeros ( n )
for
i
in
range ( n ) :
h [ i ] = k ( x_pred ,
x [ i ] )
L = np . l i n a l g . cholesky (K + sigma_2âˆ—np . i d e n t i t y ( n ) )
alpha = np . l i n a l g . solve (L ,
np . l i n a l g . solve (L . T ,
( y âˆ’mu( x ) ) ) )
mm = mu( x_pred ) + np . sum( np . dot ( h . T ,
alpha ) )
gamma = np . l i n a l g . solve (L . T ,
h )
ss = k ( x_pred ,
x_pred ) âˆ’np . sum(gammaâˆ—âˆ—2)
return {"mm":mm, "ss": ss }
87. Show from (6.5) that the negated log-likelihood of x1, . . . , xN âˆˆRp, y1, . . . ,
yN âˆˆ{âˆ’1, 1} is
N
	
i=1
log[1 + exp{âˆ’yi f (xi)}] .
88. Explain that Lines 19 through 24 of the program in Example 86 are used to
update fX â†(W + kâˆ’1
X X)âˆ’1(W fX + u).
89. Replace the ï¬rst 100 Iris data (50 Setosa, 50 Versicolor) with the 51st to 150th
data (50 Versicolor, 50 Versinica) in Example 86 to execute the program.
90. In the proof of Proposition 65, why is it acceptable to replace fZ in (6.16) of
the generation process by Î¼ fZ|Y to Î¼(x)? In Ïƒ 2(x), the variations due to fZ|Y
and f (x)| fZ are independent. Why can we assume that they are independent?
91. In Example 88, there is a step in which the function gp_ind that realizes the
inducing variable method avoids processing O(N 3) calculations. Where is this
step?
92. Show that a stochastic process is a mean-square continuous process if and only
if its mean and covariance functions are continuous.
93. From Mercerâ€™s theorem (6.31) and Proposition 67, derive Karhunen-LÃ³eveâ€™s
theorem. Additionally, for n = 10, generate ï¬ve sample paths of Brownian
motion.
94. From the formula for the MatÃ©rn kernel (6.35), derive Ï•5/2 and Ï•3/2. Addition-
ally, illustrate the value of the MatÃ©rn kernel (Î½ = 1, . . . , 10) for l = 0.05, as
in Fig.6.4.
95. Illustrate the sample path of the MatÃ©rn kernel with Î½ = 5/2, l = 0.1.
96. Give an example of a random element that does not involve a stochastic process
and an example of a stochastic process that does not involve a random element.

206
6
Gaussian Processes and Functional Data Analyses
97. Prepare a basis function Î· = [Î·1, . . . , Î·p] : E â†’Rp and construct a procedure
to ï¬nd m N(x) in (6.39). Then, input the Canadian weather data for N = 35 and
output the result. Additionally, construct a procedure to ï¬nd KN(x) in (6.40)
and output it as a matrix of size p Ã— p.
98. Suppose that we prepare p basis functions as
{
1
âˆš
2Ï€
, cos x
âˆšÏ€ , sin x
âˆšÏ€ , cos 2x
âˆšÏ€ , sin 2x
âˆšÏ€ , Â· Â· Â· }
for E = [âˆ’Ï€, Ï€]. Why is W = (wi, j) =

E Î·(x)Î·(x)âŠ¤dx a unit matrix?
99. Using the Canadian weather data (precipitation for each day of the year) instead
of temperature for each day of the year, ï¬nd the principal component functions
and eigenvalues and output graphs similar to those in Figs.6.8 and 6.9.
100. Using the scikit-fda, ï¬nd the principal component functions and eigenvalues
for both temperature and precipitation for each day of the year and output
graphs similar to those in Figs.6.8 and 6.9.

Bibliography
1. N. Aronszajn, Theory of reproducing kernels. Trans. Am. Math. Soc. 68, 337â€“404 (1950)
2. H. Avron, M. Kapralov, C. Musco, A. Velingker and A. Zandieh. Random fourier fea-
tures for kernel ridge regression: Approximation bounds and statistical guarantees. ArXiv,
abs/1804.09893, 2017
3. C. Baker. The Numerical Treatment of Integral Equations (Claredon Press, 1978)
4. P. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and struc-
tural results. In J. Mach. Learn. Res., 2001
5. K.P. Chwialkowski and A. Gretton. A kernel independence test for random processes. In ICML,
2014)
6. R. Dudley. Real Analysis and Probability (Cambridge Studies in Advanced Mathematics, 1989)
7. K. Fukumizu. Introduction to Kernel Methods (kaneru hou nyuumon) (Asakura, 2010). (In
Japanese)
8. T. Gneiting, Compactly supported correlation functions. J. Multivar. Anal. 83, 493â€“508 (2002)
9. G.H. Golub, C.F. Van Loan, Matrix Computations, 3rd edn. (Johns Hopkins, Baltimore, 1996)
10. I.S. Gradshteyn, I.M. Ryzhik, R.H. Romer, Tables of integrals, series, and products. Am. J.
Phys. 56, 958â€“958 (1988)
11. A. Gretton, K. Borgwardt, M. Rasch, B. SchÃ¶lkopf, A. Smola, A kernel two-sample test. J.
Mach. Learn. Res. 13, 723â€“773 (2012)
12. A. Gretton, R. Herbrich, A. Smola, O. Bousquet, B. SchÃ¶lkopf, Kernel methods for measuring
independence. J. Mach. Learn. Res. 6, 2075â€“2129 (2005)
13. D. Haussler. Convolution kernels on discrete structures. Technical Report UCSC-CRL-99-10,
UCSC, 1999
14. T. Hsing and R. Eubank. Theoretical Foundations of Functional Data Analysis, with an Intro-
duction to Linear Operators (Wiley, 2015)
15. K. ItÃµ. An Introduction to Probability Theory (Cambridge University Press, 1984)
16. Y. Kano and S. Shimizu. Causal inference using nonnormality. In Proceedings of the Annual
Meeting of the Behaviormetric Society of Japan 47, 2004
17. K. Karhunen. Ãœber lineare methoden in der wahrscheinlichkeitsrechnung. Ann. Acad. Sci.
Fennicae. Ser. A. I. Math.-Phys 37, 1â€“79 (1947)
18. K. Karhunen. Probability theory. Vol. II (Springer-Verlag, 1978)
19. H. Kashima, K. Tsuda and A. Inokuchi. Marginalized kernels between labeled graphs. In ICML,
2003
Â© The Editor(s) (if applicable) and The Author(s), under exclusive license
to Springer Nature Singapore Pte Ltd. 2022
J. Suzuki, Kernel Methods for Machine Learning with Math and Python,
https://doi.org/10.1007/978-981-19-0401-1
207

208
Bibliography
20. S. Lauritzen. Graphical Models (Oxford Science Publications, 1996)
21. J. Mercer. Functions of positive and negative type and their connection with the theory of
integral equations. Philosophical Transactions of the Royal Society A, pp. 441â€“458, 1909
22. J. Neveu, Processus aIeatoires gaussiens, Seminaire Math. Sup. Les presses de Iâ€™Universite de
Montreal, 1968
23. A. Rahimi and B. Recht, Random features for large-scale kernel machines. In Advances in
neural information processing systems, 2007
24. J. Ramsay and B.W. Silverman. Functional Data Analysis (Springer Series in Statistics, 2005)
25. C. Rasmussen and C.K.I. Williams. Gaussian Processes for Machine Learning (MIT Press,
2006)
26. B. SchÃ¶lkopf, A. Smola and K. MÃ¼ller. Kernel principal component analysis, In ICANN, 1997
27. R. Serï¬‚ing. Approximation Theorems of Mathematical Statistics (Wiley, 1980)
28. S. Shimizu, P. Hoyer, A. HyvÃ¤rinen, A.J. Kerminen, A linear non-gaussian acyclic model for
causal discovery. J. Mach. Learn. Res. 7, 2003â€“2030 (2006)
29. I. Steinwart, On the inï¬‚uence of the kernel on the consistency of support vector machines. J.
Mach. Learn. Res. 2, 67â€“93 (2001)
30. M.H. Stone, Applications of the theory of boolean rings to general topology. Trans. Am. Math.
Soc. 41(3), 375â€“481 (1937)
31. M.H. Stone, The generalized weierstrass approximation theorem. Math. Mag. 21(4), 167â€“184
(1948)
32. K. Tsuda, T. Kin, K. Asai, Marginalized kernels for biological sequences. Bioinformatics
18(Suppl 1), S268-75 (2002)
33. J.-P. Vert. Aronszajnâ€™s theorem, 2017. https://members.cbio.mines-paristech.fr/~jvert/svn/
kernelcourse/notes/aronszajn.pdf
34. K. Weierstrass. Ãœber die analytische darstellbarkeit sogenannter willkÃ¼rlicher functionen
einer reellen verÃ¤nderlichen. Sitzungsberichte der KÃ¶niglich PreuÃŸischen Akademie der Wis-
senschaften zu Berlin, pp. 633â€“639, 1885. Erste Mitteilung
35. K. Weierstrass. Ãœber die analytische darstellbarkeit sogenannter willkÃ¼rlicher functionen
einer reellen verÃ¤nderlichen. Sitzungsberichte der KÃ¶niglich PreuÃŸischen Akademie der Wis-
senschaften zu Berlin, pp. 789â€“805, 1885. Zweite Mitteilung
36. H. Zhu, C.K.I. Williams, R. Rohwer and Michal Morciniec. Gaussian regression and optimal
ï¬nite dimensional linear models. In Neural Networks and Machine Learning, 1997


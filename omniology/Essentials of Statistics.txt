David Brink
Essentials of Statistics
Download free books at

2 
 
 
 
  
David Brink
 
Statistics
Download free eBooks at bookboon.com

3 
 
 
 
Statistics 
Â© 2010 David Brink & Ventus Publishing ApS 
ISBN 978-87-7681-408-3
 
Download free eBooks at bookboon.com

Statistics
 
4 
Contents
Contents 
1
Preface
11
2
Basic concepts of probability theory
12
2.1
Probability space, probability function, sample space, event . . . . . . . . . . . .
12
2.2
Conditional probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2.3
Independent events . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
2.4
The Inclusion-Exclusion Formula
. . . . . . . . . . . . . . . . . . . . . . . . .
14
2.5
Binomial coefï¬cients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
2.6
Multinomial coefï¬cients
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
3
Random variables
18
3.1
Random variables, deï¬nition . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
3.2
The distribution function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
3.3
Discrete random variables, point probabilities . . . . . . . . . . . . . . . . . . .
19
3.4
Continuous random variables, density function
. . . . . . . . . . . . . . . . . .
20
3.5
Continuous random variables, distribution function . . . . . . . . . . . . . . . .
20
3.6
Independent random variables
. . . . . . . . . . . . . . . . . . . . . . . . . . .
20
3.7
Random vector, simultaneous density, and distribution function . . . . . . . . . .
21
Download free eBooks at bookboon.com

Statistics
 
5 
Contents
4
Expected value and variance
21
4.1
Expected value of random variables
. . . . . . . . . . . . . . . . . . . . . . . .
21
4.2
Variance and standard deviation of random variables
. . . . . . . . . . . . . . .
22
4.3
Example (computation of expected value, variance, and standard deviation)
. . .
23
4.4
Estimation of expected value Âµ and standard deviation Ïƒ by eye
. . . . . . . . .
23
4.5
Addition and multiplication formulae for expected value and variance
. . . . . .
24
4.6
Covariance and correlation coefï¬cient . . . . . . . . . . . . . . . . . . . . . . .
24
5
The Law of Large Numbers
26
5.1
Chebyshevâ€™s Inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
5.2
The Law of Large Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
5.3
The Central Limit Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
5.4
Example (distribution functions converge to Î¦)
. . . . . . . . . . . . . . . . . .
27
6
Descriptive statistics
27
6.1
Median and quartiles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
6.2
Mean value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
6.3
Empirical variance and empirical standard deviation . . . . . . . . . . . . . . . .
28
6.4
Empirical covariance and empirical correlation coefï¬cient
. . . . . . . . . . . .
29
7
Statistical hypothesis testing
29
7.1
Null hypothesis and alternative hypothesis . . . . . . . . . . . . . . . . . . . . .
29
7.2
Signiï¬cance probability and signiï¬cance level . . . . . . . . . . . . . . . . . . .
29
Download free eBooks at bookboon.com
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Statistics
 
6 
Contents
7.3
Errors of type I and II . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
7.4
Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
8
The binomial distribution Bin(n, p)
30
8.1
Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
8.2
Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
8.3
Point probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
8.4
Expected value and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
8.5
Signiï¬cance probabilities for tests in the binomial distribution
. . . . . . . . . .
32
8.6
The normal approximation to the binomial distribution
. . . . . . . . . . . . . .
32
8.7
Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
8.8
Conï¬dence intervals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
9
The Poisson distribution Pois(Î»)
35
9.1
Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
9.2
Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
9.3
Point probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
9.4
Expected value and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
9.5
Addition formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
9.6
Signiï¬cance probabilities for tests in the Poisson distribution . . . . . . . . . . .
36
9.7
Example (signiï¬cant increase in sale of Skodas) . . . . . . . . . . . . . . . . . .
36
9.8
The binomial approximation to the Poisson distribution . . . . . . . . . . . . . .
37
9.9
The normal approximation to the Poisson distribution . . . . . . . . . . . . . . .
37
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Statistics
 
7 
Contents
9.10 Example (signiï¬cant decrease in number of complaints) . . . . . . . . . . . . . .
38
9.11 Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
9.12 Conï¬dence intervals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
10 The geometrical distribution Geo(p)
39
10.1 Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
10.2 Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
10.3 Point probabilities and tail probabilities
. . . . . . . . . . . . . . . . . . . . . .
39
10.4 Expected value and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
11 The hypergeometrical distribution HG(n, r, N)
41
11.1 Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
11.2 Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
11.3 Point probabilities and tail probabilities
. . . . . . . . . . . . . . . . . . . . . .
41
11.4 Expected value and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
11.5 The binomial approximation to the hypergeometrical distribution . . . . . . . . .
42
11.6 The normal approximation to the hypergeometrical distribution . . . . . . . . . .
42
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Statistics
 
8 
Contents
12 The multinomial distribution Mult(n, p1, . . . , pr)
43
12.1 Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
12.2 Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
12.3 Point probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
12.4 Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
13 The negative binomial distribution NB(n, p)
44
13.1 Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
13.2 Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
13.3 Point probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
13.4 Expected value and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
13.5 Estimators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
14 The exponential distribution Exp(Î»)
45
14.1 Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
14.2 Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
14.3 Density and distribution function . . . . . . . . . . . . . . . . . . . . . . . . . .
46
14.4 Expected value and variance . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
15 The normal distribution
46
15.1 Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
15.2 Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
15.3 Density and distribution function . . . . . . . . . . . . . . . . . . . . . . . . . .
47
15.4 The standard normal distribution . . . . . . . . . . . . . . . . . . . . . . . . . .
47
15.5 Properties of Î¦
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
15.6 Estimation of the expected value Âµ . . . . . . . . . . . . . . . . . . . . . . . . .
48
15.7 Estimation of the variance Ïƒ2 . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
15.8 Conï¬dence intervals for the expected value Âµ
. . . . . . . . . . . . . . . . . . .
49
15.9 Conï¬dence intervals for the variance Ïƒ2 and the standard deviation Ïƒ . . . . . . .
50
15.10Addition formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
16 Distributions connected with the normal distribution
50
16.1 The Ï‡2 distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
16.2 Studentâ€™s t distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
16.3 Fisherâ€™s F distribution
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
17 Tests in the normal distribution
53
17.1 One sample, known variance, H0 : Âµ = Âµ0 . . . . . . . . . . . . . . . . . . . . .
53
17.2 One sample, unknown variance, H0 : Âµ = Âµ0 (Studentâ€™s t test) . . . . . . . . . .
53
17.3 One sample, unknown expected value, H0 : Ïƒ2 = Ïƒ2
0
. . . . . . . . . . . . . . .
54
17.4 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
17.5 Two samples, known variances, H0 : Âµ1 = Âµ2 . . . . . . . . . . . . . . . . . . .
56
17.6 Two samples, unknown variances, H0 : Âµ1 = Âµ2 (Fisher-Behrens) . . . . . . . .
57
17.7 Two samples, unknown expected values, H0 : Ïƒ2
1 = Ïƒ2
2 . . . . . . . . . . . . . .
57
Download free eBooks at bookboon.com

Statistics
 
9 
Contents
17.8 Two samples, unknown common variance, H0 : Âµ1 = Âµ2 . . . . . . . . . . . . .
58
17.9 Example (comparison of two expected values) . . . . . . . . . . . . . . . . . . .
59
18 Analysis of variance (ANOVA)
60
18.1 Aim and motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
18.2 k samples, unknown common variance, H0 : Âµ1 = Â· Â· Â· = Âµk
. . . . . . . . . . .
60
18.3 Two examples (comparison of mean values from three samples)
. . . . . . . . .
61
19 The chi-squared test (or Ï‡2 test)
63
19.1 Ï‡2 test for equality of distribution
. . . . . . . . . . . . . . . . . . . . . . . . .
63
19.2 The assumption of normal distribution . . . . . . . . . . . . . . . . . . . . . . .
65
19.3 Standardized residuals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
19.4 Example (women with ï¬ve children) . . . . . . . . . . . . . . . . . . . . . . . .
65
19.5 Example (election)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
19.6 Example (deaths in the Prussian cavalry) . . . . . . . . . . . . . . . . . . . . . .
68
20 Contingency tables
70
20.1 Deï¬nition, method
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
20.2 Standardized residuals
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
20.3 Example (studentsâ€™ political orientation) . . . . . . . . . . . . . . . . . . . . . .
71
20.4 Ï‡2 test for 2 Ã— 2 tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
20.5 Fisherâ€™s exact test for 2 Ã— 2 tables
. . . . . . . . . . . . . . . . . . . . . . . . .
73
20.6 Example (Fisherâ€™s exact test) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
21 Distribution-free tests
74
21.1 Wilcoxonâ€™s test for one set of observations . . . . . . . . . . . . . . . . . . . . .
75
21.2 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
21.3 The normal approximation to Wilcoxonâ€™s test for one set of observations . . . . .
77
21.4 Wilcoxonâ€™s test for two sets of observations . . . . . . . . . . . . . . . . . . . .
77
21.5 The normal approximation to Wilcoxonâ€™s test for two sets of observations . . . .
78
22 Linear regression
79
22.1 The model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
22.2 Estimation of the parameters Î²0 and Î²1
. . . . . . . . . . . . . . . . . . . . . .
79
22.3 The distribution of the estimators . . . . . . . . . . . . . . . . . . . . . . . . . .
80
22.4 Predicted values Ë†yi and residuals Ë†ei . . . . . . . . . . . . . . . . . . . . . . . . .
80
22.5 Estimation of the variance Ïƒ2 . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
22.6 Conï¬dence intervals for the parameters Î²0 and Î²1 . . . . . . . . . . . . . . . . .
80
22.7 The determination coefï¬cient R2 . . . . . . . . . . . . . . . . . . . . . . . . . .
81
22.8 Predictions and prediction intervals . . . . . . . . . . . . . . . . . . . . . . . . .
81
22.9 Overview of formulae . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
22.10Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
A Overview of discrete distributions
86
Download free eBooks at bookboon.com

Statistics
 
10 
Contents
B
Tables
87
B.1
How to read the tables
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
B.2
The standard normal distribution . . . . . . . . . . . . . . . . . . . . . . . . . .
89
B.3
The Ï‡2 distribution (values x with FÏ‡2(x) = 0.500 etc.) . . . . . . . . . . . . . .
92
B.4
Studentâ€™s t distribution (values x with FStudent(x) = 0.600 etc.) . . . . . . . . .
94
B.5
Fisherâ€™s F distribution (values x with FFisher(x) = 0.90) . . . . . . . . . . . . .
95
B.6
Fisherâ€™s F distribution (values x with FFisher(x) = 0.95) . . . . . . . . . . . . .
96
B.7
Fisherâ€™s F distribution (values x with FFisher(x) = 0.99) . . . . . . . . . . . . .
97
B.8
Wilcoxonâ€™s test for one set of observations . . . . . . . . . . . . . . . . . . . . .
98
B.9
Wilcoxonâ€™s test for two sets of observations, Î± = 5% . . . . . . . . . . . . . . .
99
C Explanation of symbols
100
D Index
102
Download free eBooks at bookboon.com

Statistics
 
11 
Preface
1
Preface
Many students ï¬nd that the obligatory Statistics course comes as a shock. The set textbook is
difï¬cult, the curriculum is vast, and secondary-school maths feels inï¬nitely far away.
â€œStatisticsâ€ offers friendly instruction on the core areas of these subjects. The focus is overview.
And the numerous examples give the reader a â€œrecipeâ€ for solving all the common types of exer-
cise. You can download this book free of charge.
Download free eBooks at bookboon.com

Statistics
 
12 
Basic concepts of probability theory
2
Basic concepts of probability theory
2.1
Probability space, probability function, sample space, event
A probability space is a pair (â„¦, P) consisting of a set â„¦and a function P which assigns to each
subset A of â„¦a real number P(A) in the interval [0, 1]. Moreover, the following two axioms are
required to hold:
1. P(â„¦) = 1,
2. P (âˆ
n=1 An) = âˆ
n=1 P(An) if A1, A2, . . . is a sequence of pairwise disjoint subsets of â„¦.
The set â„¦is called a sample space. The elements Ï‰ âˆˆâ„¦are called sample points and the subsets
A âŠ†â„¦are called events. The function P is called a probability function. For an event A, the
real number P(A) is called the probability of A.
From the two axioms the following consequences can be deduced:
3. P(Ã˜) = 0,
4. P(A\B) = P(A) âˆ’P(B) if B âŠ†A,
5. P(âˆA) = 1 âˆ’P(A),
6. P(A) â‰§P(B) if B âŠ†A,
7. P(A1 âˆªÂ· Â· Â· âˆªAn) = P(A1) + Â· Â· Â· + P(An) if A1, . . . , An are pairwise disjoint events,
8. P(A âˆªB) = P(A) + P(B) âˆ’P(A âˆ©B) for arbitrary events A and B.
EXAMPLE. Consider the set â„¦= {1, 2, 3, 4, 5, 6}. For each subset A of â„¦, deï¬ne
P(A) = #A
6
,
where #A is the number of elements in A. Then the pair (â„¦, P) is a probability space. One can
view this probability space as a model for the for the situation â€œthrow of a diceâ€.
EXAMPLE. Now consider the set â„¦= {1, 2, 3, 4, 5, 6} Ã— {1, 2, 3, 4, 5, 6}. For each subset A of â„¦,
deï¬ne
P(A) = #A
36
.
Now the probability space (â„¦, P) is a model for the situation â€œthrow of two diceâ€. The subset
A = {(1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6)}
is the event â€œa pairâ€.
2.2
Conditional probability
For two events A and B the conditional probability of A given B is deï¬ned as
P(A | B) := P(A âˆ©B)
P(B)
.
Download free eBooks at bookboon.com

Statistics
 
13 
Basic concepts of probability theory
We have the following theorem called computation of probability by division into possible causes:
Suppose A1, . . . , An are pairwise disjoint events with A1 âˆªÂ· Â· Â· âˆªAn = â„¦. For every event B it
then holds that
P(B) = P(A1) Â· P(B | A1) + Â· Â· Â· + P(An) Â· P(B | An) .
EXAMPLE. In the French Open ï¬nal, Nadal plays the winner of the semiï¬nal between Federer
and Davydenko. A bookmaker estimates that the probability of Federer winning the semiï¬nal is
75%. The probability that Nadal can beat Federer is estimated to be 51%, whereas the probability
that Nadal can beat Davydenko is estimated to be 80%. The bookmaker therefore computes the
probability that Nadal wins the French Open, using division into possible causes, as follows:
P(Nadal wins the ï¬nal)
=
P(Federer wins the semiï¬nal)Ã—
P(Nadal wins the ï¬nal|Federer wins the semiï¬nal)+
P(Davydenko wins the semiï¬nal)Ã—
P(Nadal wins the ï¬nal|Davydenko wins the semiï¬nal)
=
0.75 Â· 0.51 + 0.25 Â· 0.8
=
58.25%
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Statistics
 
14 
Basic concepts of probability theory
2.3
Independent events
Two events A and B are called independent, if
P(A âˆ©B) = P(A) Â· P(B) .
Equivalent to this is the condition P(A | B) = P(A), i.e. that the probability of A is the same as
the conditional probability of A given B.
Remember: Two events are independent if the probability of one of them is not affected by
knowing whether the other has occurred or not.
EXAMPLE. A red and a black dice are thrown. Consider the events
A: red dice shows 6,
B: black dice show 6.
Since
P(A âˆ©B) = 1
36 = 1
6 Â· 1
6 = P(A) Â· P(B) ,
A and B are independent. The probability that the red dice shows 6 is not affected by knowing
anything about the black dice.
EXAMPLE. A red and a black dice are thrown. Consider the events
A: the red and the black dice show the same number,
B: the red and the black dice show a total of 10.
Since
P(A) = 1
6 , but P(A | B) = 1
3 ,
A and B are not independent. The probability of two of a kind increases if one knows that the sum
of the dice is 10.
2.4
The Inclusion-Exclusion Formula
Formula 8 on page 12 has the following generalization to three events A, B, C:
P(AâˆªB âˆªC) = P(A)+P(B)+P(C)âˆ’P(Aâˆ©B)âˆ’P(Aâˆ©C)âˆ’P(B âˆ©C)+P(Aâˆ©B âˆ©C) .
This equality is called the Inclusion-Exclusion Formula for three events.
EXAMPLE. What is the probability of having at least one 6 in three throws with a dice? Let A1 be
the event that we get a 6 in the ï¬rst throw, and deï¬ne A2 and A3 similarly. Then, our probability
can be computed by inclusion-exclusion:
P
=
P(A1 âˆªA2 âˆªA3)
=
P(A1) + P(A2) + P(A3) âˆ’P(A1 âˆ©A2) âˆ’P(A1 âˆ©A3) âˆ’P(A2 âˆ©A3)
+P(A1 âˆ©A2 âˆ©A3)
=
1
6 + 1
6 + 1
6 âˆ’1
62 âˆ’1
62 âˆ’1
62 + 1
63
â‰ˆ
41%
Download free eBooks at bookboon.com

Statistics
 
15 
Basic concepts of probability theory
The following generalization holds for n events A1, A2, . . . , An with union A = A1 âˆªÂ· Â· Â· âˆªAn:
P(A) =

i
P(Ai) âˆ’

i<j
P(Ai âˆ©Aj) +

i<j<k
P(Ai âˆ©Aj âˆ©Ak) âˆ’Â· Â· Â· Â± P(A1 âˆ©Â· Â· Â· âˆ©An) .
This equality is called the Inclusion-Exclusion Formula for n events.
EXAMPLE. Pick ï¬ve cards at random from an ordinary pack of cards. We wish to compute the
probability P(B) of the event B that all four suits appear among the 5 chosen cards.
For this purpose, let A1 be the event that none of the chosen cards are spades. Deï¬ne A2, A3,
and A4 similarly for hearts, diamonds, and clubs, respectively. Then
âˆB = A1 âˆªA2 âˆªA3 âˆªA4 .
The Inclusion-Exclusion Formula now yields
P(âˆB) =

i
P(Ai) âˆ’

i<j
P(Ai âˆ©Aj) +

i<j<k
P(Ai âˆ©Aj âˆ©Ak) âˆ’P(A1 âˆ©A2 âˆ©A3 âˆ©A4) ,
that is
P(âˆB) = 4 Â·

39
5


52
5
 âˆ’6 Â·

26
5


52
5
 + 4 Â·

13
5


52
5
 âˆ’0 â‰ˆ73.6%
We thus obtain the probability
P(B) = 1 âˆ’P(âˆB) = 26.4%
EXAMPLE. A school class contains n children. The teacher asks all the children to stand up and
then sit down again on a random chair. Let us compute the probability P(B) of the event B that
each pupil ends up on a new chair.
We start by enumerating the pupils from 1 to n. For each i we deï¬ne the event
Ai : pupil number i gets his or her old chair
Then
âˆB = A1 âˆªÂ· Â· Â· âˆªAn .
Now P(âˆB) can be computed by the Inclusion-Exclusion Formula for n events:
P(âˆB) =

i
P(Ai) âˆ’

i<j
P(Ai âˆ©Aj) + Â· Â· Â· Â± P(A1 âˆ©Â· Â· Â· âˆ©An) ,
thus
P(âˆB)
=

n
1

1
n âˆ’

n
2

1
n(n âˆ’1) + Â· Â· Â· Â±

n
n

1
n!
=
1 âˆ’1
2! + Â· Â· Â· Â± 1
n!
Download free eBooks at bookboon.com

Statistics
 
16 
Basic concepts of probability theory
We conclude
P(B) = 1 âˆ’P(âˆB) = 1
2! âˆ’1
3! + 1
4! âˆ’Â· Â· Â· Â± 1
n!
It is a surprising fact that this probability is more or less independent of n: P(B) is very close to
37% for all n â‰¥4.
2.5
Binomial coefï¬cients
The binomial coefï¬cient

n
k

(read as â€œn over kâ€) is deï¬ned as
n
k

=
n!
k!(n âˆ’k)! =
1 Â· 2 Â· 3 Â· Â· Â· n
1 Â· 2 Â· Â· Â· k Â· 1 Â· 2 Â· Â· Â· (n âˆ’k)
for integers n and k with 0 â‰¦k â‰¦n. (Recall the convention 0! = 1.)
The reason why binomial coefï¬cients appear again and again in probability theory is the fol-
lowing theorem:
The number of ways of choosing k elements from a set of n elements is

n
k

.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Statistics
 
17 
Basic concepts of probability theory
For example, the number of subsets with 5 elements (poker hands) of a set with 52 elements (a
pack of cards) is equal to
52
5

= 2598960 .
An easy way of remembering the binomial coefï¬cients is by arranging them in Pascalâ€™s tri-
angle where each number is equal to the sum of the numbers immediately above:

0
0

1

1
0
 
1
1

1 1

2
0
 
2
1
 
2
2

1 2 1

3
0
 
3
1
 
3
2
 
3
3

1 3 3 1

4
0
 
4
1
 
4
2
 
4
3
 
4
4

1 4 6 4 1

5
0
 
5
1
 
5
2
 
5
3
 
5
4
 
5
5

1 5 10 10 5 1

6
0
 
6
1
 
6
2
 
6
3
 
6
4
 
6
5
 
6
6

1 6 15 20 15 6 1
...
...
One notices the rule

n
n âˆ’k

=
n
k

, e.g.
10
7

=
10
3

.
2.6
Multinomial coefï¬cients
The multinomial coefï¬cients are deï¬ned as

n
k1 Â· Â· Â· kr

=
n!
k1! Â· Â· Â· kr!
for integers n and k1, . . . , kr with n = k1 + Â· Â· Â· + kr. The multinomial coefï¬cients are also called
generalized binomial coefï¬cients since the binomial coefï¬cient

n
k

is equal to the multinomial coefï¬cient

n
k l

with l = n âˆ’k.
Download free eBooks at bookboon.com

Statistics
 
18 
Rancom variables
Normally, one can forget about the probability space and simply think of the following rule of
thumb:
Remember: A random variable is a function taking different values with different probabilities.
The probability that the random variable X takes certain values is written in the following way:
P(X = x): the probability that X takes the value x âˆˆR,
P(X < x): the probability that X takes a value smaller than x,
P(X > x): the probability that X takes a value greater than x,
etc.
One has the following rules:
P(X â‰¤x)
=
P(X < x) + P(X = x)
P(X â‰¥x)
=
P(X > x) + P(X = x)
1
=
P(X < x) + P(X = x) + P(X > x)
3.2
The distribution function
The distribution function of a random variable X is the function F : R â†’R given by
F(x) = P(X â‰¤x) .
F(x) is an increasing function with values in the interval [0, 1] and moreover satisï¬es F(x) â†’1
for x â†’âˆ, and F(x) â†’0 for x â†’âˆ’âˆ.
By means of F(x), all probabilities of X can be computed:
P(X < x)
=
limÎµâ†’0 F(x âˆ’Îµ)
P(X = x)
=
F(x) âˆ’limÎµâ†’0 F(x âˆ’Îµ)
P(X â‰¥x)
=
1 âˆ’limÎµâ†’0 F(x âˆ’Îµ)
P(X > x)
=
1 âˆ’F(x)
3
Random variables
3.1
Random variables, deï¬nition
Consider a probability space (â„¦, P). A random variable is a map X from â„¦into the set of real
numbers R.
X
â„¦
 Download free eBooks at bookboon.com

Statistics
 
19 
Rancom variables
3.3
Discrete random variables, point probabilities
A random variable X is called discrete if it takes only ï¬nitely many or countably many values.
For all practical purposes, we may deï¬ne a discrete random variable as a random variable taking
only values in the set {0, 1, 2, . . . }. The point probabilities
P(X = k)
determine the distribution of X. Indeed,
P(X âˆˆA) =

kâˆˆA
P(X = k)
for any A âŠ†{0, 1, 2, . . . }. In particular we have the rules
P(X â‰¤k)
=
k
i=0 P(X = i)
P(X â‰¥k)
=
âˆ
i=k P(X = i)
The point probabilities can be graphically illustrated by means of a pin diagram:
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
â€¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
â€¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
â€¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Statistics
 
20 
Rancom variables
3.4
Continuous random variables, density function
A random variable X is called continuous if it has a density function f(x). The density function,
usually referred to simply as the density, satisï¬es
P(X âˆˆA) =

tâˆˆA
f(t)dt
for all A âŠ†R. If A is an interval [a, b] we thus have
P(a â‰¤X â‰¤b) =
 b
a
f(t)dt .
One should think of the density as the continuous analogue of the point probability function in the
discrete case.
3.5
Continuous random variables, distribution function
For a continuous random variable X with density f(x) the distribution function F(x) is given by
F(x) =
 x
âˆ’âˆ
f(t)dt .
The distribution function satisï¬es the following rules:
P(X â‰¤x)
=
F(x)
P(X â‰¥x)
=
1 âˆ’F(x)
P(|X| â‰¤x)
=
F(x) âˆ’F(âˆ’x)
P(|X| â‰¥x)
=
F(âˆ’x) + 1 âˆ’F(x)
3.6
Independent random variables
Two random variables X and Y are called independent if the events X âˆˆA and Y âˆˆB are in-
dependent for any subsets A, B âŠ†R. Independence of three or more random variables is deï¬ned
similarly.
Remember: X and Y are independent if nothing can be deduced about the value of Y from
knowing the value of X.
1
P(X=k)
k
2
3
4
5
6
7
0,2
0,1
0
Download free eBooks at bookboon.com

Statistics
 
21 
Expected value and variance
EXAMPLE. Throw a red dice and a black dice and consider the random variables
X: number of pips of red dice,
Y : number of pips of black dice,
Z: number of pips of red and black dice in total.
X and Y are independent since we can deduce nothing about X by knowing Y . In contrast, X
and Z are not independent since information about Z yields information about X (if, for example,
Z has the value 10, then X necessarily has one of the values 4, 5 and 6).
3.7
Random vector, simultaneous density, and distribution function
If X1, . . . , Xn are random variables deï¬ned on the same probability space (â„¦, P) we call X =
(X1, . . . , Xn) an (n-dimensional) random vector. It is a map
X : â„¦â†’Rn .
The simultaneous (n-dimensional) distribution function is the function F : Rn â†’[0, 1] given by
F(x1, . . . , xn) = P(X1 â‰¤x1 âˆ§Â· Â· Â· âˆ§Xn â‰¤xn) .
Suppose now that the Xi are continuous. Then X has a simultaneous (n-dimensional) density
f : Rn â†’[0, âˆ[ satisfying
P(X âˆˆA) =

xâˆˆA
f(x) dx
for all A âŠ†Rn. The individual densities fi of the Xi are called marginal densities, and we obtain
them from the simultaneous density by the formula
f1(x1) =

Rnâˆ’1 f(x1, . . . , xn) dx2 . . . dxn
stated here for the case f1(x1).
Remember: The marginal densities are obtained from the simultaneous density by â€œintegrating
away the superï¬‚uous variablesâ€.
4
Expected value and variance
4.1
Expected value of random variables
The expected value of a discrete random variable X is deï¬ned as
E(X) =
âˆ

k=1
P(X = k) Â· k .
The expected value of a continuous random variable X with density f(x) is deï¬ned as
E(X) =
 âˆ
âˆ’âˆ
f(x) Â· x dx .
Often, one uses the Greek letter Âµ (â€œmuâ€) to denote the expected value.
Download free eBooks at bookboon.com

Statistics
 
22 
Expected value and variance
4.2
Variance and standard deviation of random variables
The variance of a random variable X with expected value E(X) = Âµ is deï¬ned as
var(X) = E((X âˆ’Âµ)2) .
If X is discrete, the variance can be computed thus:
var(X) =
âˆ

k=0
P(X = k) Â· (k âˆ’Âµ)2 .
If X is continuous with density f(x), the variance can be computed thus:
var(X) =
 âˆ
âˆ’âˆ
f(x)(x âˆ’Âµ)2 dx .
The standard deviation Ïƒ (â€œsigmaâ€) of a random variable X is the square root of the variance:
Ïƒ(X) =

var(X) .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Statistics
 
23 
Expected value and variance
4.3
Example (computation of expected value, variance, and standard deviation)
EXAMPLE 1. Deï¬ne the discrete random variable X as the number of pips shown by a certain
dice. The point probabilities are P(X = k) = 1/6 for k = 1, 2, 3, 4, 5, 6. Therefore, the expected
value is
E(X) =
6

k=1
1
6 Â· k = 1 + 2 + 3 + 4 + 5 + 6
6
= 3.5 .
The variance is
var(X) =
6

k=1
1
6 Â· (k âˆ’3.5)2 = (1 âˆ’3.5)2 + (2 âˆ’3.5)2 + Â· Â· Â· + (6 âˆ’3.5)2
6
= 2.917 .
The standard deviation thus becomes
Ïƒ(X) =
âˆš
2.917 = 1.708 .
EXAMPLE 2. Deï¬ne the continuous random variable X as a random real number in the interval
[0, 1]. X then has the density f(x) = 1 on [0, 1]. The expected value is
E(X) =
 1
0
x dx = 0.5 .
The variance is
var(X) =
 1
0
(x âˆ’0.5)2 dx = 0.083 .
The standard deviation is
Ïƒ =
âˆš
0.083 = 0.289 .
4.4
Estimation of expected value Âµ and standard deviation Ïƒ by eye
If the density function (or a pin diagram showing the point probabilities) of a random variable is
given, one can estimate Âµ and Ïƒ by eye. The expected value Âµ is approximately the â€œcentre of
massâ€ of the distribution, and the standard deviation Ïƒ has a size such that more or less two thirds
of the â€œprobability massâ€ lie in the interval Âµ Â± Ïƒ.
f(x)
X
Âµ-r
Âµ
Âµ+r
0,2
0,1
Download free eBooks at bookboon.com

Statistics
 
24 
Expected value and variance
4.5
Addition and multiplication formulae for expected value and variance
Let X and Y be random variables. Then one has the formulae
E(X + Y )
=
E(X) + E(Y )
E(aX)
=
a Â· E(X)
var(X)
=
E(X2) âˆ’E(X)2
var(aX)
=
a2 Â· var(X)
var(X + a)
=
var(X)
for every a âˆˆR. If X and Y are independent, one has moreover
E(X Â· Y )
=
E(X) Â· E(Y )
var(X + Y )
=
var(X) + var(Y )
Remember: The expected value is additive. For independent random variables, the expected value
is multiplicative and the variance is additive.
4.6
Covariance and correlation coefï¬cient
The covariance of two random variables X and Y is the number
Cov(X, Y ) = E((X âˆ’EX)(Y âˆ’EY )) .
One has
Cov(X, X)
=
var(X)
Cov(X, Y )
=
E(X Â· Y ) âˆ’EX Â· EY
var(X + Y )
=
var(X) + var(Y ) + 2 Â· Cov(X, Y )
The correlation coefï¬cient Ï (â€œrhoâ€) of X and Y is the number
Ï = Cov(X, Y )
Ïƒ(X) Â· Ïƒ(Y ) ,
where Ïƒ(X) =

var(X) and Ïƒ(Y ) =

var(Y ) are the standard deviations of X and Y . It is
here assumed that neither standard deviation is zero. The correlation coefï¬cient is a number in the
interval [âˆ’1, 1]. If X and Y are independent, both the covariance and Ï equal zero.
Remember: A positive correlation coefï¬cient implies that normally X is large when Y large, and
vice versa. A negative correlation coefï¬cient implies that normally X is small when Y is large,
and vice versa.
EXAMPLE. A red and a black dice are thrown. Consider the random variables
X: number of pips of red dice,
Y : number of pips of red and black dice in total.
Download free eBooks at bookboon.com

Statistics
 
25 
Expected value and variance
If X is large, Y will normally be large too, and vice versa. We therefore expect a positive correla-
tion coefï¬cient. More precisely, we compute
E(X)
=
3.5
E(Y )
=
7
E(X Â· Y )
=
27.42
Ïƒ(X)
=
1.71
Ïƒ(Y )
=
2.42
The covariance thus becomes
Cov(X, Y ) = E(X Â· Y ) âˆ’E(X) Â· E(Y ) = 27.42 âˆ’3.5 Â· 7 = 2.92 .
As expected, the correlation coefï¬cient is a positive number:
Ï = Cov(X, Y )
Ïƒ(X) Â· Ïƒ(Y ) =
2.92
1.71 Â· 2.42 = 0.71 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
â€œThe perfect start 
of a successful, 
international career.â€
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Statistics
 
26 
The Law of Large Numbers
5
The Law of Large Numbers
5.1
Chebyshevâ€™s Inequality
For a random variable X with expected value Âµ and variance Ïƒ2, we have Chebyshevâ€™s Inequal-
ity:
P(|X âˆ’Âµ| â‰¥a) â‰¤Ïƒ2
a2
for every a > 0.
5.2
The Law of Large Numbers
Consider a sequence X1, X2, X3, . . . of independent random variables with the same distribution
and let Âµ be the common expected value. Denote by Sn the sums
Sn = X1 + Â· Â· Â· + Xn .
The Law of Large Numbers then states that
P

Sn
n âˆ’Âµ
 > Îµ

â†’0 for n â†’âˆ
for every Îµ > 0. Expressed in words:
The mean value of a sample from any given distribution converges to the expected value of that
distribution when the size n of the sample approaches âˆ.
5.3
The Central Limit Theorem
Consider a sequence X1, X2, X3, . . . of independent random variables with the same distribution.
Let Âµ be the common expected value and Ïƒ2 the common variance. It is assumed that Ïƒ2 is positive.
Denote by Sâ€²
n the normed sums
Sâ€²
n = X1 + Â· Â· Â· + Xn âˆ’nÂµ
Ïƒâˆšn
.
By â€œnormedâ€ we understand that the Sâ€²
n have expected value 0 and variance 1. The Central Limit
Theorem now states that
P(Sâ€²
n â‰¤x) â†’Î¦(x) for n â†’âˆ
for all x âˆˆR, where Î¦ is the distribution function of the standard normal distribution (see section
15.4):
Î¦(x) =
 x
âˆ’âˆ
1
âˆš
2Ï€ eâˆ’1
2 t2dt .
The distribution function of the normed sums Sâ€²
n thus converges to Î¦ when n converges to âˆ.
This is a quite amazing result and the absolute climax of probability theory! The surprising
thing is that the limit distribution of the normed sums is independent of the distribution of the Xi.
Download free eBooks at bookboon.com

Statistics
 
27 
Descriptive statistics
6
Descriptive statistics
6.1
Median and quartiles
Suppose we have n observations x1, . . . , xn. We then deï¬ne the median x(0.5) of the observations
as the â€œmiddle observationâ€. More precisely,
x(0.5) =

x(n+1)/2
if n is odd
(xn/2 + xn/2+1)/2
if n is even
where the observations have been sorted according to size as
x1 â‰¤x2 â‰¤Â· Â· Â· â‰¤xn .
5.4
Example (distribution functions converge to Î¦)
Consider a sequence of independent random variables X1, X2, . . . all having the same point prob-
abilities
P(Xi = 0) = P(Xi = 1) = 1
2.
The sums Sn = X1 + Â· Â· Â· + Xn are binomially distributed with expected value Âµ = n/2 and
variance Ïƒ2 = n/4. The normed sums thus become
Sâ€²
n = X1 + Â· Â· Â· + Xn âˆ’Âµ/2
âˆšn/2
.
The distribution of the Sâ€²
n is given by the distribution function Fn. The Central Limit Theorem
states that Fn converges to Î¦ for n â†’âˆ. The ï¬gure below shows Fn together with Î¦ for n =
1, 2, 10, 100. It is a moment of extraordinary beauty when one watches the Fn slowly approaching
Î¦:
Download free eBooks at bookboon.com

Statistics
 
28 
Descriptive statistics
Similarly, the lower quartile x(0.25) is deï¬ned such that 25% of the observations lie below
x(0.25), and the upper quartile x(0.75) is deï¬ned such that 75% of the observations lie below
x(0.75).
The interquartile range is the distance between x(0.25) and x(0.75), i.e. x(0.75) âˆ’x(0.25).
6.2
Mean value
Suppose we have n observations x1, . . . , xn. We deï¬ne the mean or mean value of the observa-
tions as
Â¯x =
n
i=1 xi
n
6.3
Empirical variance and empirical standard deviation
Suppose we have n observations x1, . . . , xn. We deï¬ne the empirical variance of the observa-
tions as
s2 =
n
i=1(xi âˆ’Â¯x)2
n âˆ’1
.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
â–¶â–¶enroll by September 30th, 2014 and 
â–¶â–¶save up to 16% on the tuition!
â–¶â–¶pay in 10 installments / 2 years
â–¶â–¶Interactive Online education
â–¶â–¶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Statistics
 
29 
Statistical hypothesis testing
The empirical standard deviation is the square root of the empirical variance:
s =
n
i=1(xi âˆ’Â¯x)2
n âˆ’1
.
The greater the empirical standard deviation s is, the more â€œdispersedâ€ the observations are around
the mean value Â¯x.
6.4
Empirical covariance and empirical correlation coefï¬cient
Suppose we have n pairs of observations (x1, y1), . . . , (xn, yn). We deï¬ne the empirical covari-
ance of these pairs as
Covemp =
n
i=1(xi âˆ’Â¯x)(yi âˆ’Â¯y)
n âˆ’1
.
Alternatively, Covemp can be computed as
Covemp =
n
i=1 xiyi âˆ’nÂ¯xÂ¯y
n âˆ’1
.
The empirical correlation coefï¬cient is
r =
empirical covariance
(empirical standard deviation of the x)(empirical standard deviation of the y) = Covemp
sxsy
.
The empirical correlation coefï¬cient r always lies in the interval [âˆ’1, 1].
Understanding of the empirical correlation coefï¬cient. If the x-observations are independent of
the y-observations, then r will be equal or close to 0. If the x-observations and the y-observations
are dependent in such a way that large x-values usually correspond to large y-values, and vice
versa, then r will be equal or close to 1. If the x-observations and the y-observations are dependent
in such a way that large x-values usually correspond to small y-values, and vice versa, then r will
be equal or close to â€“1.
7
Statistical hypothesis testing
7.1
Null hypothesis and alternative hypothesis
A statistical test is a procedure that leads to either acceptance or rejection of a null hypothesis
H0 given in advance. Sometimes H0 is tested against an explicit alternative hypothesis H1.
At the base of the test lie one or more observations. The null hypothesis (and the alternative
hypothesis, if any) concern the question which distribution these observations were taken from.
7.2
Signiï¬cance probability and signiï¬cance level
One computes the signiï¬cance probability P, that is the probability â€“ if H0 is true â€“ of obtaining
an observation which is as extreme, or more extreme, than the one given. The smaller P is, the
less plausible H0 is.
Download free eBooks at bookboon.com

Statistics
 
30 
The binomial distribution Bin(n,p)
Often, one chooses a signiï¬cance level Î± in advance, typically Î± = 5%. One then rejects H0
if P is smaller than Î± (and one says, â€œH0 is rejected at signiï¬cance level Î±â€). If P is greater than
Î±, then H0 is accepted (and one says, â€œH0 is accepted at signiï¬cance level Î±â€ or â€œH0 cannot be
rejected at signiï¬cance level Î±â€).
7.3
Errors of type I and II
We speak about a type I error if we reject a true null hypothesis. If the signiï¬cance level is Î±,
then the risk of a type I error is at most Î±.
We speak about a type II error if we accept a false null hypothesis.
The strength of a test is the probability of rejecting a false H0. The greater the strength, the
smaller the risk of a type II error. Thus, the strength should be as great as possible.
7.4
Example
Suppose we wish to investigate whether a certain dice is fair. By â€œfairâ€ we here only understand
that the probability p of a six is 1/6. We test the null hypothesis
H0 : p = 1
6 (the dice is fair)
against the alternative hypothesis
H1 : p > 1
6 (the dice is biased)
The observations on which the test is carried out are the following ten throws of the dice:
2, 6, 3, 6, 5, 2, 6, 6, 4, 6 .
Let us in advance agree upon a signiï¬cance level Î± = 5%. Now the signiï¬cance probability P
can be computed. By â€œextreme observationsâ€ is understood that there are many sixes. Thus, P is
the probability of having at least ï¬ve sixes in 10 throws with a fair dice. We compute
P =
10

k=5
10
k

(1/6)k(5/6)10âˆ’k = 0.015
(see section 8 on the binomial distribution). Since P = 1.5% is smaller than Î± = 5%, we reject
H0. If the same test was performed with a fair dice, the probability of committing a type I error
would be 1.5%.
8
The binomial distribution Bin(n, p)
8.1
Parameters
n: number of tries
p: probability of success
In the formulae we also use the â€œprobability of failureâ€ q = 1 âˆ’p.
Download free eBooks at bookboon.com

Statistics
 
31 
The binomial distribution Bin(n,p)
8.2
Description
We carry out n independent tries that each result in either success or failure. In each try the
probability of success is the same, p. Consequently, the total number of successes X is binomially
distributed, and we write X âˆ¼Bin(n, p). X is a discrete random variable and takes values in the
set {0, 1, . . . , n}.
8.3
Point probabilities
For k âˆˆ{0, 1, . . . , n}, the point probabilities in a Bin(n, p) distribution are
P(X = k) =
n
k

Â· pk Â· qnâˆ’k .
See section 2.5 regarding the binomial coefï¬cients

n
k

.
EXAMPLE. If a dice is thrown twenty times, the total number of sixes, X, will be binomially
distributed with parameters n = 20 and p = 1/6. We can list the point probabilities P(X = k)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Statistics
 
32 
The binomial distribution Bin(n,p)
and the cumulative probabilities P(X â‰¥k) in a table (expressed as percentages):
k
0
1
2
3
4
5
6
7
8
9
P(X = k)
2.6
10.4
19.8
23.8
20.2
12.9
6.5
2.6
0.8
0.2
P(X â‰¥k)
100
97.4
87.0
67.1
43.3
23.1
10.2
3.7
1.1
0.3
8.4
Expected value and variance
Expected value:
E(X) = np.
Variance:
var(X) = npq.
8.5
Signiï¬cance probabilities for tests in the binomial distribution
We perform n independent experiments with the same probability of success p and count the
number k of successes. We wish to test the null hypothesis H0 : p = p0 against an alternative
hypothesis H1.
H0
H1
Signiï¬cance probability
p = p0
p > p0
P(X â‰¥k)
p = p0
p < p0
P(X â‰¤k)
p = p0
p Ì¸= p0

l P(X = l)
where in the last line we sum over all l for which P(X = l) â‰¤P(X = k).
EXAMPLE. A company buys a machine that produces microchips. The manufacturer of the ma-
chine claims that at most one sixth of the produced chips will be defective. The ï¬rst day the
machine produces 20 chips of which 6 are defective. Can the company reject the manufacturerâ€™s
claim on this background?
SOLUTION. We test the null hypothesis H0 : p = 1/6 against the alternative hypothesis H1 :
p > 1/6. The signiï¬cance probability can be computed as P(X â‰¥6) = 10.2% (see e.g. the table
in section 8.3). We conclude that the company cannot reject the manufacturerâ€™s claim at the 5%
level.
8.6
The normal approximation to the binomial distribution
If the parameter n (the number of tries) is large, a binomially distributed random variable X
will be approximately normally distributed with expected value Âµ = np and standard deviation
Ïƒ = âˆšnpq. Therefore, the point probabilities are approximately
P(X = k) â‰ˆÏ•
k âˆ’np
âˆšnpq

Â·
1
âˆšnpq
where Ï• is the density of the standard normal distribution, and the tail probabilities are approxi-
mately
P(X â‰¤k) â‰ˆÎ¦

k + 1
2 âˆ’np
âˆšnpq

Download free eBooks at bookboon.com

Statistics
 
33 
The binomial distribution Bin(n,p)
P(X â‰¥k) â‰ˆ1 âˆ’Î¦

k âˆ’1
2 âˆ’np
âˆšnpq

where Î¦ is the distribution function of the standard normal distribution (Table B.2).
Rule of thumb. One may use the normal approximation if np and nq are both greater than 5.
EXAMPLE (continuation of the example in section 8.5). After 2 weeks the machine has produced
200 chips of which 46 are defective. Can the company now reject the manufacturerâ€™s claim that
the probability of defects is at most one sixth?
SOLUTION. Again we test the null hypothesis H0 : p = 1/6 against the alternative hypothesis
H1 : p > 1/6. Since now np â‰ˆ33 and nq â‰ˆ167 are both greater than 5, we may use the normal
approximation in order to compute the signiï¬cance probability:
P(X â‰¥46) â‰ˆ1 âˆ’Î¦

46 âˆ’1
2 âˆ’33.3
âˆš
27.8

â‰ˆ1 âˆ’Î¦(2.3) â‰ˆ1.1%
Therefore, the company may now reject the manufacturerâ€™s claim at the 5% level.
8.7
Estimators
Suppose k is an observation from a random variable X âˆ¼Bin(n, p) with known n and unknown
p. The maximum likelihood estimate (ML estimate) of p is
Ë†p = k
n .
This estimator is unbiased (i.e. the expected value of the estimator is p) and has variance
var(Ë†p) = pq
n .
The expression for the variance is of no great practical value since it depends on the true (un-
known) probability parameter p. If, however, one plugs in the estimated value Ë†p in place of p, one
gets the estimated variance
Ë†p(1 âˆ’Ë†p)
n
.
EXAMPLE. We consider again the example with the machine that has produced twenty microchips
of which the six are defective. What is the maximum likelihood estimate of the probability param-
eter? What is the estimated variance?
SOLUTION. The maximum likelihood estimate is
Ë†p = 6
20 = 30%
and the variance of Ë†p is estimated as
0.3 Â· (1 âˆ’0.3)
20
= 0.0105 .
Download free eBooks at bookboon.com

Statistics
 
34 
The binomial distribution Bin(n,p)
The standard deviation is thus estimated to be
âˆš
0.0105 â‰ˆ0.10. If we presume that Ë†p lies within
two standard deviations from p, we may conclude that p is between 10% and 50%.
8.8
Conï¬dence intervals
Suppose k is an observation from a binomially distributed random variable X âˆ¼Bin(n, p) with
known n and unknown p. The conï¬dence interval with conï¬dence level 1 âˆ’Î± around the point
estimate Ë†p = k/n is

Ë†p âˆ’u1âˆ’Î±/2

Ë†p(1 âˆ’Ë†p)
n
, Ë†p + u1âˆ’Î±/2

Ë†p(1 âˆ’Ë†p)
n

.
Loosely speaking, the true value p lies in the conï¬dence interval with the probability 1 âˆ’Î±.
The number u1âˆ’Î±/2 is determined by Î¦(u1âˆ’Î±/2) = 1 âˆ’Î±/2 where Î¦ is the distribution
function of the standard normal distribution. It appears e.g. from Table B.2 that with conï¬dence
level 95% one has
u1âˆ’Î±/2 = u0.975 = 1.96 .
EXERCISE. In an opinion poll from the year 2015, 62 out of 100 persons answer that they intend
to vote for the Green Party at the next election. Compute the conï¬dence interval with conï¬dence
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Statistics
 
35 
The Poisson distribution Pois(Î»)
level 95% around the true percentage of Green Party voters.
SOLUTION. The point estimate is Ë†p = 62/100 = 0.62. A conï¬dence level of 95% yields Î± =
0.05. Looking up in the table (see above) gives u0.975 = 1.96. We get
1.96

0.62 Â· 0.38
100
= 0.10 .
The conï¬dence interval thus becomes
[0.52 , 0.72] .
So we can say with a certainty of 95% that between 52% and 72% of the electorate will vote for
the Green Party at the next election.
9
The Poisson distribution Pois(Î»)
9.1
Parameters
Î»: Intensity
9.2
Description
Certain events are said to occur spontaneously, i.e. they occur at random times, independently
of each other, but with a certain constant intensity Î». The intensity is the average number of
spontaneous events per time interval. The number of spontaneous events X in any given concrete
time interval is then Poisson distributed, and we write X âˆ¼Pois(Î»). X is a discrete random
variable and takes values in the set {0, 1, 2, 3, . . . }.
9.3
Point probabilities
For k âˆˆ{0, 1, 2, 3 . . . } the point probabilities in a Pois(Î») distribution are
P(X = k) = Î»k
k! exp(âˆ’Î») .
Recall the convention 0! = 1.
EXAMPLE. In a certain shop an average of three customers per minute enter. The number of
customers X entering during any particular minute is then Poisson distributed with intensity Î» =
3. The point probabilities (as percentages) can be listed in a table as follows:
k
0
1
2
3
4
5
6
7
8
9
â‰¥10
P(X = k)
5.0
14.9
22.4
22.4
16.8
10.1
5.0
2.2
0.8
0.3
0.1
9.4
Expected value and variance
Expected value:
E(X) = Î».
Variance:
var(X) = Î».
Download free eBooks at bookboon.com

Statistics
 
36 
The Poisson distribution Pois(Î»)
9.5
Addition formula
Suppose that X1, . . . , Xn are independent Poisson distributed random variables. Let Î»i be the
intensity of Xi, i.e. Xi âˆ¼Pois(Î»i). Then the sum
X = X1 + Â· Â· Â· + Xn
will be Poisson distributed with intensity
Î» = Î»1 + Â· Â· Â· + Î»n ,
i.e. X âˆ¼Pois(Î»).
9.6
Signiï¬cance probabilities for tests in the Poisson distribution
Suppose that k is an observation from a Pois(Î») distribution with unknown intensity Î». We wish
to test the null hypothesis H0 : Î» = Î»0 against an alternative hypothesis H1.
H0
H1
Signiï¬cance probability
Î» = Î»0
Î» > Î»0
P(X â‰¥k)
Î» = Î»0
Î» < Î»0
P(X â‰¤k)
Î» = Î»0
Î» Ì¸= Î»0

l P(X = l)
where the summation in the last line is over all l for which P(X = l) â‰¤P(X = k).
If n independent observations k1, . . . , kn from a Pois(Î») distribution are given, we can treat
the sum k = k1 + Â· Â· Â· + kn as an observation from a Pois(n Â· Î») distribution.
9.7
Example (signiï¬cant increase in sale of Skodas)
EXERCISE. A Skoda car salesman sells on average 3.5 cars per month. The month after a radio
campaign for Skoda, seven cars are sold. Is this a signiï¬cant increase?
SOLUTION. The sale of cars in the given month may be assumed to be Poisson distributed with a
certain intensity Î». We test the null hypothesis
H0 : Î» = 3.5
against the alternative hypothesis
H1 : Î» > 3.5 .
The signiï¬cance probability, i.e. the probability of selling at least seven cars given that H0 is true,
is
P =
âˆ

k=7
(3.5)k
k!
exp(âˆ’3.5) = 0.039 + 0.017 + 0.007 + 0.002 + Â· Â· Â· = 0.065 .
Since P is greater than 5%, we cannot reject H0. In other words, the increase is not signiï¬cant.
Download free eBooks at bookboon.com

Statistics
 
37 
The Poisson distribution Pois(Î»)
9.8
The binomial approximation to the Poisson distribution
The Poisson distribution with intensity Î» is the limit distribution of the binomial distribution with
parameters n and p = Î»/n when n tends to âˆ. In other words, the point probabilities satisfy
P(Xn = k) â†’P(X = k) for n â†’âˆ
for X âˆ¼Pois(Î») and Xn âˆ¼Bin(n, Î»/n). In real life, however, one almost always prefers to use
the normal approximation instead (see the next section).
9.9
The normal approximation to the Poisson distribution
If the intensity Î» is large, a Poisson distributed random variable X will to a good approximation
be normally distributed with expected value Âµ = Î» and standard deviation Ïƒ =
âˆš
Î». The point
probabilities therefore are
P(X = k) â‰ˆÏ•
k âˆ’Î»
âˆš
Î»

Â· 1
âˆš
Î»
where Ï•(x) is the density of the standard normal distribution, and the tail probabilities are
P(X â‰¤k) â‰ˆÎ¦

k + 1
2 âˆ’Î»
âˆš
Î»

Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.mastersopenday.nl
Visit us and find out why we are the best!
Masterâ€™s Open Day: 22 February 2014
Join the best at
the Maastricht University
School of Business and
Economics!
Top masterâ€™s programmes
â€¢ 33rd place Financial Times worldwide ranking: MSc 
International Business
â€¢ 1st place: MSc International Business
â€¢ 1st place: MSc Financial Economics
â€¢ 2nd place: MSc Management of Learning
â€¢ 2nd place: MSc Economics
â€¢ 2nd place: MSc Econometrics and Operations Research
â€¢ 2nd place: MSc Global Supply Chain Management and 
Change
Sources: Keuzegids Master ranking 2013; Elsevier â€˜Beste Studiesâ€™ ranking 2012; 
Financial Times Global Masters in Management ranking 2012
Maastricht
University is
the best specialist
university in the
Netherlands
(Elsevier)

Statistics
 
38 
The Poisson distribution Pois(Î»)
P(X â‰¥k) â‰ˆ1 âˆ’Î¦

k âˆ’1
2 âˆ’Î»
âˆš
Î»

where Î¦ is the distribution function of the standard normal distribution (Table B.2).
Rule of thumb. The normal approximation to the Poisson distribution applies if Î» is greater than
nine.
9.10
Example (signiï¬cant decrease in number of complaints)
EXERCISE. The ferry Deutschland between RÃ¸dby and Puttgarten receives an average of 180
complaints per week. In the week immediately after the ferryâ€™s cafeteria was closed, only 112
complaints are received. Is this a signiï¬cant decrease?
SOLUTION. The number of complaints within the given week may be assumed to be Poisson
distributed with a certain intensity Î». We test the null hypothesis
H0 : Î» = 180
against the alternative hypothesis
H1 : Î» < 180 .
The signiï¬cance probability, i.e. the probability of having at most 112 complaints given H0, can
be approximated by the normal distribution:
P = Î¦

112 + 1
2 âˆ’180
âˆš
180

= Î¦(âˆ’5.03) < 0.0001 .
Since P is very small, we can clearly reject H0. The number of complaints has signiï¬cantly
decreased.
9.11
Estimators
Suppose k1, . . . kn are independent observations from a random variable X âˆ¼Pois(Î») with un-
known intensity Î». The maximum likelihood estimate (ML estimate) of Î» is
Ë†Î» = (k1 + Â· Â· Â· + kn)/n .
This estimator is unbiased (i.e. the expected value of the estimator is Î») and has variance
var(Ë†Î») = Î»
n .
More precisely, we have
nË†Î» âˆ¼Pois(nÎ») .
If we plug in the estimated value Ë†Î» in Î»â€™s place, we get the estimated variance

var(Ë†Î») =
Ë†Î»
n .
Download free eBooks at bookboon.com

Statistics
 
39 
The geometrical distribution Geo(p)
9.12
Conï¬dence intervals
Suppose k1, . . . , kn are independent observations from a Poisson distributed random variable X âˆ¼
Pois(Î») with unknown Î». The conï¬dence interval with conï¬dence level 1 âˆ’Î± around the point
estimate Ë†Î» = (k1 + Â· Â· Â· + kn)/n is
ï£®
ï£°Ë†Î» âˆ’u1âˆ’Î±/2

Ë†Î»
n , Ë†Î» + u1âˆ’Î±/2

Ë†Î»
n
ï£¹
ï£».
Loosely speaking, the true value Î» lies in the conï¬dence interval with probability 1 âˆ’Î±.
The number u1âˆ’Î±/2 is determined by Î¦(u1âˆ’Î±/2) = 1 âˆ’Î±/2, where Î¦ is the distribution
function of the standard normal distribution. It appears from, say, Table B.2 that
u1âˆ’Î±/2 = u0.975 = 1.96
for conï¬dence level 95%.
EXAMPLE (continuation of the example in section 9.10). In the ï¬rst week after the closure of the
ferryâ€™s cafeteria, a total of 112 complaints were received. We consider k = 112 as an observation
from a Pois(Î») distribution and wish to ï¬nd the conï¬dence interval with conï¬dence level 95%
around the estimate
Ë†Î» = 112 .
Looking up in the table gives u0.975 = 1.96. The conï¬dence interval thus becomes

112 âˆ’1.96
âˆš
112 , 112 + 1.96
âˆš
112

â‰ˆ[91 , 133]
10
The geometrical distribution Geo(p)
10.1
Parameters
p: probability of success
In the formulae we also use the â€œprobability of failureâ€ q = 1 âˆ’p.
10.2
Description
A series of experiments are carried out, each of which results in either success or failure. The
probability of success p is the same in each experiment. The number W of failures before the ï¬rst
success is then geometrically distributed, and we write W âˆ¼Geo(p). W is a discrete random
variable and takes values in the set {0, 1, 2, . . . }. The â€œwait until successâ€ is V = W + 1.
10.3
Point probabilities and tail probabilities
For k âˆˆ{0, 1, 2 . . . } the point probabilities in a Geo(p) distribution are
P(X = k) = qkp .
Download free eBooks at bookboon.com

Statistics
 
40 
The geometrical distribution Geo(p)
In contrast to most other distributions, we can easily compute the tail probabilities in the geomet-
rical distribution:
P(X â‰¥k) = qk .
EXAMPLE. Pin diagram for the point probabilities in a geometrical distribution with probability
of success p = 0.5:
0.5
0.4
0.3
0.2
0.1
0
1
2
3
4
5
6
P(X=k)
k
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Statistics
 
41 
The hypergeometrical distribution HG(n,r,N)
10.4
Expected value and variance
Expected value:
E(W) = q/p.
Variance:
var(W) = q/p2.
Regarding the â€œwait until successâ€ V = W + 1, we have the following useful rule:
Rule. The expected wait until success is the reciprocal probability of success: E(V ) = 1/p.
EXAMPLE. A gambler plays lotto each week. The probability of winning in lotto, i.e. the proba-
bility of guessing correctly seven numbers picked randomly from a pool of 36 numbers, is
p =
36
7
âˆ’1
â‰ˆ0.0000001198 .
The expected wait until success is thus
E(V ) = pâˆ’1 =
36
7

weeks = 160532 years .
11
The hypergeometrical distribution HG(n, r, N)
11.1
Parameters
r: number of red balls
s: number of black balls
N: total number of balls (N = r + s)
n: number of balls picked out (n â‰¤N)
11.2
Description
In an urn we have r red balls and s black balls, in total N = r + s balls. We now pick out
n balls from the urn, randomly and without returning the chosen balls to the urn. Necessarily
n â‰¤N. The number of red balls Y amongst the balls chosen is then hypergeometrically dis-
tributed and we write Y âˆ¼HG(n, r, N). Y is a discrete random variable with values in the set
{0, 1, . . . , min{n, r}}.
11.3
Point probabilities and tail probabilities
For k âˆˆ{0, 1, . . . , min{n, r}} the point probabilities of a HG(n, r, N) distribution are
P(Y = k) =

r
k

Â·

s
n âˆ’k


N
n

.
Download free eBooks at bookboon.com

Statistics
 
42 
The hypergeometrical distribution HG(n,r,N)
EXAMPLE. A city council has 25 members of which 13 are Conservatives. A cabinet is formed by
picking ï¬ve council members at random. What is the probability that the Conservatives will have
absolute majority in the cabinet?
SOLUTION. We have here a hypergeometrically distributed random variable Y âˆ¼HG(5, 13, 25)
and have to compute P(Y â‰¥3). Let us ï¬rst compute all point probabilities (as percentages):
k
0
1
2
3
4
5
P(Y = k)
1.5
12.1
32.3
35.5
16.1
2.4
The sought-after probability thus becomes
P(Y â‰¥3) = 35.5% + 16.1% + 2.4% = 54.0%
11.4
Expected value and variance
Expected value:
E(Y ) = nr/N.
Variance:
var(Y ) = nrs(N âˆ’n)/(N 2(N âˆ’1)).
11.5
The binomial approximation to the hypergeometrical distribution
If the number of balls picked out, n, is small compared to both the number of red balls r and the
number of black balls s, it becomes irrelevant whether the balls picked out are returned to the urn
or not. We can thus approximate the hypergeometrical distribution by the binomial distribution:
P(Y = k) â‰ˆP(X = k)
for Y âˆ¼HG(n, r, N) and X âˆ¼Bin(n, r/N). In practice, this approximation is of little value
since it is as difï¬cult to compute P(X = k) as P(Y = k).
11.6
The normal approximation to the hypergeometrical distribution
If n is small compared to both r and s, the hypergeometrical distribution can be approximated by
the normal distribution with the same expected value and variance.
The point probabilities thus become
P(Y = k) â‰ˆÏ•
ï£«
ï£¬
ï£­
k âˆ’nr/N

nrs(N âˆ’n)/(N2(N âˆ’1))
ï£¶
ï£·
ï£¸Â·
1

nrs(N âˆ’n)/(N2(N âˆ’1))
where Ï• is the density of the standard normal distribution. The tail probabilities become
P(Y â‰¤k) â‰ˆÎ¦
ï£«
ï£¬
ï£­
k + 1
2 âˆ’nr/N

nrs(N âˆ’n)/(N2(N âˆ’1))
ï£¶
ï£·
ï£¸
Download free eBooks at bookboon.com

Statistics
 
43 
The multinomial distribution Mult(n, p1,...pr)
P(Y â‰¥k) â‰ˆ1 âˆ’Î¦
ï£«
ï£¬
ï£­
k âˆ’1
2 âˆ’nr/N

nrs(N âˆ’n)/(N2(N âˆ’1))
ï£¶
ï£·
ï£¸
where Î¦ is the distribution function of the standard normal distribution (Table B.2).
12
The multinomial distribution Mult(n, p1, . . . , pr)
12.1
Parameters
n: number of tries
p1: 1st probability parameter
...
pr: rth probability parameter
It is required that p1 + Â· Â· Â· + pr = 1.
12.2
Description
We carry out n independent experiments each of which results in one out of r possible outcomes.
The probability of obtaining an outcome of type i is the same in each experiment, namely pi. Let
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Get Help Now
Go to www.helpmyassignment.co.uk for more info
Need help with your
dissertation?
Get in-depth feedback & advice from experts in your 
topic area. Find out what you can do to improve
the quality of your dissertation!

Statistics
 
44 
The negative binomial distribution NB(n,p)
Si denote the total number of outcomes of type i. The random vector S = (S1, . . . , Sr) is then
multinomially distributed and we write S âˆ¼Mult(n, p1, . . . , pr). S is discrete and takes values in
the set {(k1, . . . kr) âˆˆZr | ki â‰¥0 , k1 + Â· Â· Â· + kr = n}.
12.3
Point probabilities
For k1 + Â· Â· Â· + kr = n the point probabilities of a Mult(n, p1, . . . , pr) distribution are
P(S = (k1, . . . , kr)) =

n
k1 Â· Â· Â· kr

Â·
r

i=1
pki
i
EXAMPLE. Throw a dice six times and, for each i, let Si be the total number of iâ€™s. Then
S = (S1, . . . , S6) is a multinomially distributed random vector: S âˆ¼Mult(6, 1/6, . . . , 1/6).
The probability of obtaining, say, exactly one 1, two 2s, and three sixes is
P(S = (1, 2, 0, 0, 0, 3)) =

6
1 2 0 0 0 3

Â· (1/6)1 Â· (1/6)2 Â· (1/6)3 â‰ˆ0.13%
Here, the multinomial coefï¬cient (see also section 2.6) is computed as

6
1 2 0 0 0 3

=
6!
1!2!0!0!0!3! = 720
12 = 60 .
12.4
Estimators
Suppose k1, . . . , kr is an observation from a random vector S âˆ¼Mult(n, p1, . . . , pr) with known
n and unknown pi. The maximum likelihood estimate of pi is
Ë†pi = ki
n .
This estimator is unbiased (i.e. the estimatorâ€™s expected value is pi) and has variance
var(Ë†pi) = pi(1 âˆ’pi)
n
.
13
The negative binomial distribution NB(n, p)
13.1
Parameters
n: number of tries
p: probability of success
In the formulae we also use the letter q = 1 âˆ’p.
Download free eBooks at bookboon.com

Statistics
 
45 
The exponential distribution (Î»)
13.2
Description
A series of independent experiments are carried out each of which results in either success or
failure. The probability of success p is the same in each experiment. The total number X of failures
before the nâ€™th success is then negatively binomially distributed and we write X âˆ¼NB(n, p). The
random variable X is discrete and takes values in the set {0, 1, 2, . . . }.
The geometrical distribution is the special case n = 1 of the negative binomial distribution.
13.3
Point probabilities
For k âˆˆ{0, 1, 2 . . . } the point probabilities of a NB(k, p) distribution are
P(X = k) =
n + k âˆ’1
n âˆ’1

Â· pn Â· qk .
13.4
Expected value and variance
Expected value:
E(X) = nq/p.
Variance:
var(X) = nq/p2.
13.5
Estimators
The negative binomial distribution is sometimes used as an alternative to the Poisson distribution
in situations where one wishes to describe a random variable taking values in the set {0, 1, 2, . . . }.
Suppose k1, . . . , km are independent observations from a NB(n, p) distribution with unknown
parameters n and p. We then have the following estimators:
Ë†n =
Â¯k2
s2 âˆ’Â¯k
,
Ë†p =
Â¯k
s2
where Â¯k and s2 are the mean value and empirical variance of the observations.
14
The exponential distribution Exp(Î»)
14.1
Parameters
Î»: Intensity
14.2
Description
In a situation where events occur spontaneously with the intensity Î» (and where the number of
spontaneous events in any given time interval thus is Pois(Î») distributed), the wait T between
two spontaneous events is exponentially distributed and we write T âˆ¼Exp(Î»). T is a continuous
random variable taking values in the interval [0, âˆ[.
Download free eBooks at bookboon.com

Statistics
 
46 
The normal distribution
14.3
Density and distribution function
The density of the exponential distribution is
f(x) = Î» Â· exp(âˆ’Î»x) .
The distribution function is
F(x) = 1 âˆ’exp(âˆ’Î»x) .
14.4
Expected value and variance
Expected value:
E(T) = 1/Î».
Variance:
var(T) = 1/Î»2.
15
The normal distribution
15.1
Parameters
Âµ: expected value
Ïƒ2: variance
Remember that the standard deviation Ïƒ is the square root of the variance.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
By 2020, wind could provide one-tenth of our planetâ€™s 
electricity needs. Already today, SKFâ€™s innovative know-
how is crucial to running a large proportion of the 
worldâ€™s wind turbines. 
Up to 25 % of the generating costs relate to mainte-
nance. These can be reduced dramatically thanks to our 
systems for on-line condition monitoring and automatic 
lubrication. We help make it more economical to create 
cleaner, cheaper energy out of thin air. 
By sharing our experience, expertise, and creativity, 
industries can boost performance beyond expectations. 
Therefore we need the best employees who can 
meet this challenge!
The Power of Knowledge Engineering
Brain power
Plug into The Power of Knowledge Engineering. 
Visit us at www.skf.com/knowledge

Statistics
 
47 
The normal distribution
15.2
Description
The normal distribution is a continuous distribution. If a random variable X is normally distribut-
ed, then X can take any values in R and we write X âˆ¼N(Âµ, Ïƒ2).
The normal distribution is the most important distribution in all of statistics. Countless natu-
rally occurring phenomena can be described (or approximated) by means of a normal distribution.
15.3
Density and distribution function
The density of the normal distribution is the function
f(x) =
1
âˆš
2Ï€Ïƒ2 exp

âˆ’(x âˆ’Âµ)2
2Ïƒ2

.
It is symmetric, i.e.
f(âˆ’x) = f(x) .
The distribution function of the normal distribution
F(x) =
 x
âˆ’âˆ
1
âˆš
2Ï€Ïƒ2 exp

âˆ’(t âˆ’Âµ)2
2Ïƒ2

dt
is difï¬cult to compute. Instead, one uses the formula
F(x) = Î¦
x âˆ’Âµ
Ïƒ

where Î¦ is the distribution function of the standard normal distribution which can be looked up in
Table B.2. From the table the following fact appears:
Fact: In a normal distribution, about 68% of the probability mass lies within one standard devi-
ation from the expected value, and about 95% of the probability mass lies within two standard
deviations from the expected value.
15.4
The standard normal distribution
A normal distribution with expected value Âµ = 0 and variance Ïƒ2 = 1 is called a standard normal
distribution. The standard deviation in a standard normal distribution equals 1 (obviously). The
density Ï•(t) of a standard normal distribution is
Ï•(t) =
1
âˆš
2Ï€ exp

âˆ’1
2t2

.
The distribution function Î¦ of a standard normal distribution is
Î¦(x) =
 x
âˆ’âˆ
1
âˆš
2Ï€ exp

âˆ’1
2t2

dt .
On can look up Î¦ in Table B.2.
Download free eBooks at bookboon.com

Statistics
 
48 
The normal distribution
15.5
Properties of Î¦
The distribution function Î¦ of a standard normally distributed random variable X âˆ¼N(0, 1)
satisï¬es
P(X â‰¤x)
=
Î¦(x)
P(X â‰¥x)
=
Î¦(âˆ’x)
P(|X| â‰¤x)
=
Î¦(x) âˆ’Î¦(âˆ’x)
P(|X| â‰¥x)
=
2 Â· Î¦(âˆ’x)
Î¦(âˆ’x)
=
1 âˆ’Î¦(x)
15.6
Estimation of the expected value Âµ
Suppose x1, x2, . . . , xn are independent observations of a random variable X âˆ¼N(Âµ, Ïƒ2). The
maximum likelihood estimate (ML estimate) of Âµ is
Ë†Âµ = x1 + Â· Â· Â· + xn
n
.
This is simply the mean value and is written Â¯x. The mean value is an unbiased estimator of Âµ (i.e.
the estimatorâ€™s expected value is Âµ). The variance of the mean value is
var2(Â¯x) = Ïƒ2
n .
More precisely, Â¯x is itself normally distributed:
Â¯x âˆ¼N(Âµ, Ïƒ2
n ) .
15.7
Estimation of the variance Ïƒ2
Suppose x1, . . . , xn are independent observations of a random variable X âˆ¼N(Âµ, Ïƒ2). Normally,
the variance Ïƒ2 is estimated by the empirical variance
s2 =
(xi âˆ’Â¯x)2
n âˆ’1
.
The empirical variance s2 is an unbiased estimator of the true variance Ïƒ2.
Warning: The empirical variance is not the maximum likelihood estimate of Ïƒ2. The maximum
likelihood estimate of Ïƒ2 is
(xi âˆ’Â¯x)2
n
but this is seldom used since it is biased and usually gives estimates which are too small.
Download free eBooks at bookboon.com

Statistics
 
49 
The normal distribution
15.8
Conï¬dence intervals for the expected value Âµ
Suppose x1, . . . , xn are independent observations of a normally distributed random variable X âˆ¼
N(Âµ, Ïƒ2) and that we wish to estimate the expected value Âµ. If the variance Ïƒ2 is known, the
conï¬dence interval for Âµ with conï¬dence level 1 âˆ’Î± is as follows:

Â¯x âˆ’u1âˆ’Î±/2
Ïƒ
âˆšn , Â¯x + u1âˆ’Î±/2
Ïƒ
âˆšn

.
The number u1âˆ’Î±/2 is determined by Î¦(u1âˆ’Î±/2) = 1 âˆ’Î±/2 where Î¦ is the distribution function
of the standard normal distribution. It appears from, say, Table B.2 that
u1âˆ’Î±/2 = u0.975 = 1.96
for conï¬dence level 95%.
If the variance Ïƒ2 is unknown, the conï¬dence interval for Âµ with conï¬dence level 1 âˆ’Î± is

Â¯x âˆ’t1âˆ’Î±/2(n âˆ’1)

s2
n , Â¯x + t1âˆ’Î±/2(n âˆ’1)

s2
n

where s2 is the empirical variance (section 6.3). The number t1âˆ’Î±/2 is determined by F(u1âˆ’Î±/2) =
1 âˆ’Î±/2, where F is the distribution function of Studentâ€™s t distribution with n âˆ’1 degrees of
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Statistics
 
50 
Distribution connected with the normal distribution
freedom. It appears from, say, Table B.4 that
n
2
3
4
5
6
7
8
9
10
11
12
t1âˆ’Î±/2
12.7
4.30
3.18
2.78
2.57
2.45
2.36
2.31
2.26
2.23
2.20
for conï¬dence level 95%.
15.9
Conï¬dence intervals for the variance Ïƒ2 and the standard deviation Ïƒ
Suppose x1, . . . , xn are independent observations of a normally distributed random variable X âˆ¼
N(Âµ, Ïƒ2). The conï¬dence interval for the variance Ïƒ2 with conï¬dence level 1 âˆ’Î± is:

(n âˆ’1)s2
Ï‡2
Î±/2
, (n âˆ’1)s2
Ï‡2
1âˆ’Î±/2

where s2 is the empirical variance (section 6.3). The numbers Ï‡2
Î±/2 and Ï‡2
1âˆ’Î±/2 are determined
by F(Ï‡2
Î±/2) = Î±/2 and F(Ï‡2
1âˆ’Î±/2) = 1 âˆ’Î±/2 where F is the distribution function of the Ï‡2
distribution with n âˆ’1 degrees of freedom (Table B.3).
Conï¬dence intervals for the standard deviation Ïƒ with conï¬dence level 1 âˆ’Î± are computed
simply by taking the square root of the limits of the conï¬dence intervals for the variance:
 
(n âˆ’1)s2
Ï‡2
Î±/2
,

(n âˆ’1)s2
Ï‡2
1âˆ’Î±/2

15.10
Addition formula
A linear function of a normally distributed random variable is itself normally distributed. If, in
other words, X âˆ¼N(Âµ, Ïƒ2) and a, b âˆˆR (a Ì¸= 0), then
aX + b âˆ¼N(aÂµ + b, a2Ïƒ2) .
The sum of independent normally distributed random variables is itself normally distributed.
If, in other words, X1, . . . , Xn are independent with Xi âˆ¼N(Âµi, Ïƒ2
i ), then we have the addition
formula
X1 + Â· Â· Â· + Xn âˆ¼N(Âµ1 + Â· Â· Â· + Âµn, Ïƒ2
1 + Â· Â· Â· + Ïƒ2
n) .
16
Distributions connected with the normal distribution
16.1
The Ï‡2 distribution
Let X1, . . . , Xn âˆ¼N(0, 1) be independent standard normally distributed random variables. The
distribution of the sum of squares
Q = X2
1 + Â· Â· Â· + X2
n
is called the Ï‡2 distribution with n degrees of freedom. The number of degrees of freedom is
commonly symbolized as df.
Download free eBooks at bookboon.com

Statistics
 
51 
Distribution connected with the normal distribution
16.2
Studentâ€™s t distribution
Let X be a normally distributed random variable with expected value Âµ and variance Ïƒ2. Let the
random variables Â¯X and S2 be the mean value and empirical variance, respectively, of a sample
consisting of n observations from X. The distribution of
T =
Â¯X âˆ’Âµ

S2/n
is then independent of both Âµ and Ïƒ2 and is called Studentâ€™s t distribution with n âˆ’1 degrees of
freedom.
A t distributed random variable T with df degrees of freedom has expected value
E(T) = 0
for df â‰¥2, and variance
var(T) =
df
df âˆ’2
for df â‰¥3.
The density of the t distribution is
f(x) = K Â·

1 + x2
df
âˆ’(df+1)/2
A Ï‡2 distributed random variable Q with df degrees of freedom has expected value
E(Q) = df
and variance
var(Q) = 2 Â· df .
The density of the Ï‡2 distribution is
f(x) = K Â· x
df
2 âˆ’1 Â· eâˆ’x
2
where df is the number of degrees of freedom and K is a constant. In practice, one doesnâ€™t use
the density, but rather looks up the distribution function in Table B.3. The graph below shows the
density function with df = 1, 4, 10, 20 degrees of freedom.
df=1
df=4
df=10
df=20
Download free eBooks at bookboon.com

Statistics
 
52 
Distribution connected with the normal distribution
where df is the number of degrees of freedom and K is a constant. In practice, one doesnâ€™t use
the density, but rather looks up the distribution function in Table B.4. The graph below shows
the density of the t distribution with df = 1, 2, 3 degrees of freedom and additionally the density
Ï•(x) of the standard normal distribution. As it appears, the t distribution approaches the standard
normal distribution when df â†’âˆ.
16.3
Fisherâ€™s F distribution
Let X1 and X2 be independent normally distributed random variables with the same variance. For
i = 1, 2 let the random variable S2
i be the empirical variance of a sample of size ni from Xi. The
df=3
df=2
df=1
(x)
Ï•
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Statistics
 
53 
Tests in the normal distribution
distribution of the quotient
V = S2
1
S2
2
is called Fisherâ€™s F distribution with n1 âˆ’1 degrees of freedom in the numerator and n2 âˆ’1
degrees of freedom in the denominator.
The density of the F distribution is
f(x) = K Â·
xdf1/2âˆ’1
(df2 + df1x)df/2
where K is a constant, df1 the number of degrees of freedom in the numerator, df2 the number
of degrees of freedom in the denominator, and df = df1 + df2. In practice, one doesnâ€™t use the
density, but rather looks up the distribution function in Table B.5.
17
Tests in the normal distribution
17.1
One sample, known variance, H0 : Âµ = Âµ0
Let there be given a sample x1, . . . , xn of n independent observations from a normal distribution
with unknown expected value Âµ and known variance Ïƒ2. We wish to test the null hypothesis
H0 : Âµ = Âµ0 .
For this purpose, we compute the statistic
u =
âˆšn(Â¯x âˆ’Âµ0)
Ïƒ
=
n
i=1 xi âˆ’nÂµ0
âˆš
nÏƒ2
.
The signiï¬cance probability now appears from the following table, where Î¦ is the distribution
function of the standard normal distribution (Table B.2).
Alternative
Signiï¬cance
hypothesis
probability
H1 : Âµ > Âµ0
Î¦(âˆ’u)
H1 : Âµ < Âµ0
Î¦(u)
H1 : Âµ Ì¸= Âµ0
2 Â· Î¦(âˆ’|u|)
Normally, we reject H0 if the signiï¬cance probability is less than 5%.
17.2
One sample, unknown variance, H0 : Âµ = Âµ0 (Studentâ€™s t test)
Let there be given a sample x1, . . . , xn of n independent observations from a normal distribution
with unknown expected value Âµ and unknown variance Ïƒ2. We wish to test the null hypothesis
H0 : Âµ = Âµ0 .
For this purpose, we compute the statistic
t =
âˆšn(Â¯x âˆ’Âµ0)
s
=
n
i=1 xi âˆ’nÂµ0
âˆš
n Â· s2
,
Download free eBooks at bookboon.com

Statistics
 
54 
Tests in the normal distribution
where s2 is the empirical variance (see section 6.3).
The signiï¬cance probability now appears from the following table where FStudent is the dis-
tribution function of Studentâ€™s t distribution with df = n âˆ’1 degrees of freedom (Table B.4).
Alternative
Signiï¬cance
hypothesis
probability
H1 : Âµ > Âµ0
1 âˆ’FStudent(t)
H1 : Âµ < Âµ0
1 âˆ’FStudent(âˆ’t)
H1 : Âµ Ì¸= Âµ0
2 Â· (1 âˆ’FStudent(|t|))
Normally, we reject H0 if the signiï¬cance probability is less than 5%.
EXAMPLE. The headmaster of a school wishes to conï¬rm statistically that his students have per-
formed signiï¬cantly miserably in the 2008 ï¬nal exams. For this purpose, n = 10 students are
picked at random. Their ï¬nal scores are
x1
x2
x3
x4
x5
x6
x7
x8
x9
x10
7.6
7.7
7.5
5.8
5.7
7.9
5.4
6.7
7.9
9.4
The national average for 2008 is 8.27. It is reasonable to assume that the ï¬nal scores are normally
distributed. However, the variance is unknown. Therefore, we apply Studentâ€™s t test to test the null
hypothesis
H0 : Âµ = 8.27
against the alternative hypothesis
H1 : Âµ < 8.27 .
We compute the mean value of the observations as Â¯x = 7.17 and the empirical standard deviation
as s = 1.26. We obtain the statistic
t =
âˆš
10(7.17 âˆ’8.27)
1.26
= âˆ’2.76 .
Looking up in Table B.4 under df = nâˆ’1 = 9 degrees of freedom gives a signiï¬cance probability
1 âˆ’FStudent(âˆ’t) = 1 âˆ’FStudent(2.76)
between 1% and 2.5%. We may therefore reject H0 and conï¬rm the headmasterâ€™s assumption that
his students have performed signiï¬cantly poorer than the rest of the country.
17.3
One sample, unknown expected value, H0 : Ïƒ2 = Ïƒ2
0
THEOREM. Let there be given n independent observations x1, . . . , xn from a normal distribution
with variance Ïƒ2. The statistic
q = (n âˆ’1)s2
Ïƒ2
=
n
i=1(xi âˆ’Â¯x)2
Ïƒ2
is then Ï‡2 distributed with df = n âˆ’1 degrees of freedom (here s2 is the empirical variance).
Download free eBooks at bookboon.com

Statistics
 
55 
Tests in the normal distribution
Let there be given a sample x1, . . . , xn of n independent observations from a normal distribution
with unknown expected value Âµ and unknown variance Ïƒ2. We wish to test the null hypothesis
H0 : Ïƒ2 = Ïƒ2
0 .
For this purpose, we compute the statistic
q = (n âˆ’1)s2
Ïƒ2
0
=
n
i=1(xi âˆ’Â¯x)2
Ïƒ2
0
where s2 is the empirical variance.
The signiï¬cance probability can now be read from the following table where FÏ‡2 is the distri-
bution function of the Ï‡2 distribution with df = n âˆ’1 degrees of freedom (Table B.3).
Alternative
Signiï¬cance
hypothesis
probability
H1 : Ïƒ2 > Ïƒ2
0
1 âˆ’FÏ‡2(q)
H1 : Ïƒ2 < Ïƒ2
0
FÏ‡2(q)
H1 : Ïƒ2 Ì¸= Ïƒ2
0
2 Â· min{FÏ‡2(q), 1 âˆ’FÏ‡2(q)}
Normally, H0 is rejected if the signiï¬cance probability is smaller than 5%.
Note: In practice, we always test against the alternative hypothesis H1 : Ïƒ2 > Ïƒ2
0.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
EXPERIENCE THE POWER OF 
FULL ENGAGEMENTâ€¦
     RUN FASTER.
          RUN LONGER..
                RUN EASIERâ€¦
READ MORE & PRE-ORDER TODAY 
WWW.GAITEYE.COM
Challenge the way we run

Statistics
 
56 
Tests in the normal distribution
17.4
Example
Consider the following twenty observations originating from a normal distribution with unknown
expected value and variance:
91
97
98
112
91
97
116
108
108
100
107
98
92
103
100
99
98
104
104
97
We wish to test the null hypothesis
H0: the standard deviation is at most 5 (i.e. the variance is at most 25)
against the alternative hypothesis
H1: the standard deviation is greater than 5 (i.e. the variance is greater than 25).
The empirical variance is found to be s2 = 45.47 and we thus ï¬nd the statistic
q = (20 âˆ’1) Â· 45.47
52
= 34.56 .
By looking up in Table B.3 under df = 19 degrees of freedom, we ï¬nd a signiï¬cance probability
around 2%. We can thus reject H0.
(Actually, the observations came from a normal distribution with expected value Âµ = 100 and
standard deviation Ïƒ = 6. The test is thus remarkably sensitive.)
17.5
Two samples, known variances, H0 : Âµ1 = Âµ2
Let there be given a sample x1, . . . , xn from a normal distribution with unknown expected value
Âµ1 and known variance Ïƒ2
1. Let there in addition be given a sample y1, . . . , ym from a normal
distribution with unknown expected value Âµ2 and known variance Ïƒ2
2. It is assumed that the two
samples are independent of each other.
We wish to test the null hypothesis
H0 : Âµ1 = Âµ2 .
For this purpose, we compute the statistic
u =
Â¯x âˆ’Â¯y

Ïƒ2
1/n + Ïƒ2
2/m
.
The signiï¬cance probability is read from the following table where Î¦ is the distribution function
of the standard normal distribution (Table B.3).
Alternative
Signiï¬cance
hypothesis
probability
H1 : Âµ1 > Âµ2
Î¦(âˆ’u)
H1 : Âµ1 < Âµ2
Î¦(u)
H1 : Âµ1 Ì¸= Âµ2
2 Â· Î¦(âˆ’|u|)
Normally, we reject H0 if the signiï¬cance probability is smaller than 5%.
Note: In real life, the preconditions of this test are rarely met.
Download free eBooks at bookboon.com

Statistics
 
57 
Tests in the normal distribution
17.6
Two samples, unknown variances, H0 : Âµ1 = Âµ2 (Fisher-Behrens)
Let the situation be as in section 17.5, but suppose that the variances Ïƒ2
1 and Ïƒ2
2 are unknown. The
problem of ï¬nding a suitable statistic to test the null hypothesis
H0 : Âµ1 = Âµ2
is called the Fisher-Behrens problem and has no satisfactory solution.
If n, m > 30, one can re-use the test from section 17.5 with the alternative statistic
uâˆ—=
Â¯x âˆ’Â¯y

s2
1/n + s2
2/m
where s2
1 and s2
2 are the empirical variances of the xâ€™s and yâ€™s, respectively.
17.7
Two samples, unknown expected values, H0 : Ïƒ2
1 = Ïƒ2
2
Let there be given a sample x1, . . . , xn from a normal distribution with unknown expected value
Âµ1 and unknown variance Ïƒ2
1. In addition, let there be given a sample y1, . . . , ym from a normal
distribution with unknown expected value Âµ2 and unknown variance Ïƒ2
2. It is assumed that the
two samples are independent of each other.
We wish to test the null hypothesis
H0 : Ïƒ1 = Ïƒ2 .
For this purpose, we compute the statistic
v = s2
1
s2
2
= empirical variance of the xâ€™s
empirical variance of the yâ€™s .
Further, put
vâˆ—= max

v, 1
v

.
The signiï¬cance probability now appears from the following table where FFisher is the distribution
function of Fisherâ€™s F distribution with n âˆ’1 degrees of freedom in the numerator and m âˆ’1
degrees of freedom in the denominator (Table B.5).
Alternative
Signiï¬cance
hypothesis
probability
H1 : Ïƒ2
1 > Ïƒ2
2
1 âˆ’FFisher(v)
H1 : Ïƒ2
1 < Ïƒ2
2
1 âˆ’FFisher(1/v)
H1 : Ïƒ2
1 Ì¸= Ïƒ2
2
2 Â· (1 âˆ’FFisher(vâˆ—))
Normally, H0 is rejected if the signiï¬cance probability is smaller than 5%.
If H0 is accepted, the common variance Ïƒ2
1 = Ïƒ2
2 is estimated by the â€œpooledâ€ variance
s2
pool =
n
i=1(xi âˆ’Â¯x)2 + m
i=1(yi âˆ’Â¯y)2
n + m âˆ’2
= (n âˆ’1)s2
1 + (m âˆ’1)s2
2
n + m âˆ’2
.
Download free eBooks at bookboon.com

Statistics
 
58 
Tests in the normal distribution
17.8
Two samples, unknown common variance, H0 : Âµ1 = Âµ2
Let there be given a sample x1, . . . , xn from a normal distribution with unknown expected value
Âµ1 and unknown variance Ïƒ2. In addition, let there be given a sample y1, . . . , ym from a normal
distribution with unknown expected value Âµ2 and the same variance Ïƒ2. It is assumed that the
two samples are independent of each other.
We wish to test the null hypothesis
H0 : Âµ1 = Âµ2 .
For this purpose, we compute the statistic
t =
Â¯x âˆ’Â¯y

(1/n + 1/m)s2
pool
where s2
pool is the â€œpooledâ€ variance as given in section 17.7.
The signiï¬cance probability now appears from the following table where FStudent is the dis-
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
PDF components for PHP developers
www.setasign.com
SETASIGN
This e-book  
is made with 
SetaPDF

Statistics
 
59 
Tests in the normal distribution
tribution function of Studentâ€™s t distribution with n + m âˆ’2 degrees of freedom (Table B.4).
Alternative
Signiï¬cance
hypothesis
probability
H1 : Âµ1 > Âµ2
1 âˆ’FStudent(t)
H1 : Âµ1 < Âµ2
1 âˆ’FStudent(âˆ’t)
H1 : Âµ1 Ì¸= Âµ2
2 Â· (1 âˆ’FStudent(|t|))
Normally, H0 is rejected if the signiï¬cance probability is smaller than 5%.
17.9
Example (comparison of two expected values)
Suppose we are given seven independent observations from a normally distributed random vari-
able X:
x1 = 26 , x2 = 21 , x3 = 15 , x4 = 7 , x5 = 15 , x6 = 28 , x7 = 21
and also four independent observations from a normally distributed random variable Y :
y1 = 29 , y2 = 31 , y3 = 17 , y4 = 22 .
We wish to test the hypothesis
H0 : E(X) = E(Y ) .
In order to be able to test this, we need to test ï¬rst whether X and Y have the same variance.
We therefore test the auxiliary hypothesis
Hâˆ—
0 : var(X) = var(Y )
against the alternative
Hâˆ—
1 : var(X) Ì¸= var(Y ) .
For this purpose, we compute the statistic
v = s2
1
s2
2
= 52.3
41.6 = 1.26
as in section 17.7, as well as
vâˆ—= max

v, 1
v

= 1.26 .
Looking up in Table B.5 with 7 âˆ’1 = 6 degrees of freedom in the numerator and 4 âˆ’1 = 3
degrees of freedom in the denominator shows that the signiï¬cance probability is clearly greater
than 20%, and we may therefore accept the auxiliary hypothesis Hâˆ—
0.
Now we return to the test of H0 against the alternative hypothesis
H1 : E(X) Ì¸= E(Y ) .
The â€œpooledâ€ variance is found to be
s2
pool = 6s2
1 + 3s2
2
9
= 48.8 .
Download free eBooks at bookboon.com

Statistics
 
60 
Analysis of variance (ANOVA)
The statistic thereby becomes
t =
Â¯x âˆ’Â¯y

(1/7 + 1/4)s2
pool
=
19 âˆ’24.8

(1/7 + 1/4)48.8
= âˆ’1.31 .
Therefore, the signiï¬cance probability is found to be
2 Â· (1 âˆ’FStudent(|t|)) = 2 Â· (1 âˆ’FStudent(1.31)) â‰ˆ2 Â· (1 âˆ’0.90) = 20%
by looking up Studentâ€™s t distribution with 7 + 4 âˆ’2 = 9 degrees of freedom in Table B.4.
Consequently, we cannot reject H0.
18
Analysis of variance (ANOVA)
18.1
Aim and motivation
Analysis of variance, also known as ANOVA, is a clever method of comparing the mean values
from more than two samples. Analysis of variance is a natural extension of the tests in the previous
chapter.
18.2
k samples, unknown common variance, H0 : Âµ1 = Â· Â· Â· = Âµk
Let X1, . . . , Xk be k independent, normally distributed random variables, with expected values
Âµ1, . . . , Âµk and common variance Ïƒ2. From each Xi, let there be given a sample consisting of ni
observations. Let Â¯xj and s2
j be mean value and empirical variance of the sample from Xj.
We wish to test the null hypothesis
H0 : Âµ1 = Â· Â· Â· = Âµk
against all alternatives. For this purpose, we estimate the common variance Ïƒ2 in two different
ways.
The variance estimate within the samples is
s2
I =
1
n âˆ’k
k

j=1
(nj âˆ’1)s2
j .
The variance estimate between the samples is
s2
M =
1
k âˆ’1
k

j=1
nj(Â¯xj âˆ’Â¯x)2 .
s2
I estimates Ïƒ2 regardless of whether H0 is true or not. s2
M only estimates Ïƒ2 correctly if H0 is
true. If H0 is false, then s2
M estimates too high.
Now consider the statistic
v = s2
M
s2
I
.
Download free eBooks at bookboon.com

Statistics
 
61 
Analysis of variance (ANOVA)
The signiï¬cance probability is
1 âˆ’FFisher(v)
where FFisher is the distribution function of Fisherâ€™s F distribution with k âˆ’1 degrees of freedom
in the numerator and n âˆ’k degrees of freedom in the denominator (Table B.5).
18.3
Two examples (comparison of mean values from three samples)
Let three samples be given:
sample 1: 29, 28, 29, 21, 28, 22, 22, 29, 26, 26
sample 2: 22, 21, 18, 28, 23, 25, 25, 28, 23, 26
sample 3: 24, 23, 26, 20, 33, 23, 26, 24, 27, 22
It is assumed that the samples originate from independent normal distributions with common
variance. Let Âµi be the expected value of the iâ€™th normal distribution. We wish to test the null
hypothesis
H0 : Âµ1 = Âµ2 = Âµ3 .
(As a matter of fact, all the observations originate from a normal distribution with expected value
25 and variance 10, so the test shouldnâ€™t lead to a rejection of H0.) We thus have k = 3 samples
each consisting of ni = 10 observations, a total of n = 30 observations. A computation gives the
following variance estimate within the samples:
s2
I = 10.91
and the following variance estimate between the samples:
s2
M = 11.10
(Since we know that H0 is true, both s2
I and s2
M should estimate Ïƒ2 = 10 well, which they also
indeed do.) Now we compute the statistic:
v = s2
M
s2
I
= 11.10
10.91 = 1.02 .
Looking up in Table B.5 under k âˆ’1 = 2 degrees of freedom in the numerator and n âˆ’k = 27
degrees of freedom in the denominator shows that the signiï¬cance probability is more than 10%.
The null hypothesis H0 cannot be rejected.
Somewhat more carefully, the computations can be summed up in a table as follows:
Download free eBooks at bookboon.com

Statistics
 
62 
Analysis of variance (ANOVA)
Sample number
1
2
3
29
22
24
28
21
23
29
18
26
21
28
20
28
23
33
22
25
23
22
25
26
29
28
24
26
23
27
26
26
22
Mean value Â¯xj
26.0
23.9
24.8
Empirical variance s2
j
10.22
9.88
12.62
Â¯x = 24.9
(grand mean value)
s2
I = (s2
1 + s2
2 + s2
3)/3 = 10.91
(variance within samples)
s2
M = 5 (Â¯xj âˆ’Â¯x)2 = 11.10
(variance between samples)
v = s2
M/s2
I = 1.02
(statistic)
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
www.sylvania.com
We do not reinvent  
the wheel we reinvent 
light.
Fascinating lighting offers an infinite spectrum of 
possibilities: Innovative technologies and new  
markets provide both opportunities and challenges. 
An environment in which your expertise is in high 
demand. Enjoy the supportive working atmosphere 
within our global group and benefit from international 
career paths. Implement sustainable ideas in close 
cooperation with other specialists and contribute to 
influencing our future. Come and join us in reinventing 
light every day.
Light is OSRAM

Statistics
 
63 
The chi-squared test (or x2 test)
If we add 5 to all the observations in sample 3, we get the following table instead:
Sample number
1
2
3
29
22
29
28
21
28
29
18
31
21
28
25
28
23
38
22
25
28
22
25
31
29
28
29
26
23
32
26
26
27
Mean value Â¯xj
26.0
23.9
29.8
Empirical variance s2
j
10.22
9.88
12.62
Â¯x = 26.6
(grand mean value)
s2
I = (s2
1 + s2
2 + s2
3)/3 = 10.91
(variance within samples)
s2
M = 5 (Â¯xj âˆ’Â¯x)2 = 89.43
(variance between samples)
v = s2
M/s2
I = 8.20
(statistic)
Note how the variance within the samples doesnâ€™t change, whereas the variance between the
samples is now far too large. Thus, the statistic v = 8.20 also becomes large and the signiï¬cance
probability is seen in Table B.5 to be less than 1%. Therefore, we reject the null hypothesis H0 of
equal expected values (which was also to be expected, since H0 is now manifestly false).
19
The chi-squared test (or Ï‡2 test)
19.1
Ï‡2 test for equality of distribution
The reason why the Ï‡2 distribution is so important is that it can be used to test whether a given
set of observations comes from a certain distribution. In the following sections, we shall see many
examples of this. The test, which is also called Pearsonâ€™s Ï‡2 test or Ï‡2 test for goodness of ï¬t, is
carried out as follows:
1. First, divide the observations into categories. Let us denote the number of categories by k and
the number of observations in the iâ€™th category by Oi. The total number of observations is thus
n = O1 + Â· Â· Â· + Ok.
2. Formulate a null hypothesis H0. This null hypothesis must imply what the probability pi is that
an observation belongs to the iâ€™th category.
Download free eBooks at bookboon.com

Statistics
 
64 
The chi-squared test (or x2 test)
3. Compute the statistic
Ï‡2 =
k

i=1
(Oi âˆ’Ei)2
Ei
.
As mentioned, Oi is the observed number in the iâ€™th category. Further, Ei is the expected number
in the iâ€™th category (expected according to the null hypothesis, that is): Ei = npi. Incidentally,
the statistic Ï‡ =

Ï‡2 is sometimes called the discrepancy.
4. Find the signiï¬cance probability
P = 1 âˆ’F(Ï‡2)
where F = FÏ‡2 is the distribution function of the Ï‡2 distribution with df degrees of freedom
(look up in Table B.3). H0 is rejected if P is smaller than 5% (or whatever signiï¬cance level one
chooses). The number of degrees of freedom is normally df = k âˆ’1, i.e. one less than the number
of categories. If, however, one uses the observations to estimate the probability parameters pi of
the null hypothesis, df becomes smaller.
Remember: Each estimated parameter costs one degree of freedom.
Note: It is logical to reject H0 if Ï‡2 is large, because this implies that the difference between the
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
360Â°
thinking.
Â© Deloitte & Touche LLP and affiliated entities.
Discover the truth at www.deloitte.ca/careers 

Statistics
 
65 
The chi-squared test (or x2 test)
observed and the expected numbers is large.
19.2
The assumption of normal distribution
Since the Ï‡2 test rests upon a normal approximation, it only applies provided there are not too few
observations.
Remember: The Ï‡2 test applies if the expected number Ei is at least ï¬ve in each category. If,
however, there are more than ï¬ve categories, an expected number of at least three in each category
sufï¬ces.
19.3
Standardized residuals
If the null hypothesis regarding equality of distribution is rejected by a Ï‡2 test, this was because
some of the observed numbers deviated widely from the expected numbers. It is then interesting
to investigate exactly which observed numbers are extreme. For this purpose, we compute the
standardized residuals
ri =
Oi âˆ’npi

npi(1 âˆ’pi)
=
Oi âˆ’Ei

Ei(1 âˆ’pi)
for each category. If the null hypothesis were true, each ri would be normally distributed with
expected value Âµ = 0 and standard deviation Ïƒ = 1. Therefore:
Remember: Standardized residuals numerically greater than 2 are signs of an extreme observed
number.
It can very well happen that standardized residuals numerically greater than 2 occur even though
the Ï‡2 test does not lead to rejection of the null hypothesis. This does not mean that the null hy-
pothesis should be rejected after all. In particular when one has a large number of categories, it
will not be unusual to ï¬nd some large residuals.
Warning: Only compute the standardized residuals if the null hypothesis has been rejected by a
Ï‡2 test.
19.4
Example (women with ï¬ve children)
EXERCISE. A hospital has registered the sex of the children of 1045 women who each have ï¬ve
children. Result:
Oi
5 girls
58
4 girls + 1 boy
149
3 girls + 2 boys
305
2 girls + 3 boys
303
1 girl + 4 boys
162
5 boys
45
Download free eBooks at bookboon.com

Statistics
 
66 
The chi-squared test (or x2 test)
Test the hypothesis H0 that, at every birth, the probability of a boy is the same as the probability
of a girl.
SOLUTION. If H0 is true, the above table consists of 1045 observations from a Bin(5, 1/2) distri-
bution. The point probabilities in a Bin(5, 1/2) distribution are
pi
5 girls
0.0313
4 girls + 1 boy
0.1563
3 girls + 2 boys
0.3125
2 girls + 3 boys
0.3125
1 girl + 4 boys
0.1563
5 boys
0.0313
The expected numbers Ei = 1045 Â· pi then become
Ei
5 girls
32.7
4 girls + 1 boy
163.3
3 girls + 2 boys
326.6
2 girls + 3 boys
326.6
1 girl + 4 boys
163.3
5 boys
32.7
The statistic is computed:
Ï‡2 = (58 âˆ’32.7)2
32.7
+ (149 âˆ’163.3)2
163.3
+ (305 âˆ’326.6)2
326.6
+
(303 âˆ’326.6)2
326.6
+ (162 âˆ’163.3)2
163.3
+ (45 âˆ’32.7)2
32.7
= 28.6 .
Since the observations are divided into six categories, we compare the statistic with the Ï‡2 distri-
bution with df = 6 âˆ’1 = 5 degrees of freedom. Table B.3 shows that the signiï¬cance probability
is well below 0.5%. We can therefore with great conï¬dence reject the hypothesis that the boy-girl
ratio is Bin(5, 1/2) distributed.
Let us ï¬nally compute the standardized residuals:
ri
5 girls
4.5
4 girls + 1 boy
â€“1.2
3 girls + 2 boys
â€“1.4
2 girls + 3 boys
â€“1.6
1 girl + 4 boys
â€“0.1
5 boys
2.2
We note that it is the numbers of women with ï¬ve children of the same sex which are extreme and
make the statistic large.
Download free eBooks at bookboon.com

Statistics
 
67 
The chi-squared test (or x2 test)
19.5
Example (election)
EXERCISE. At the election for the Danish parliament in February 2005, votes were distributed
among the parties as follows (as percentages):
A
B
C
F
O
V
Ã˜
others
25.8
9.2
10.3
6.0
13.3
29.0
3.4
3.0
In August 2008, an opinion poll was carried out in which 1000 randomly chosen persons were
asked which party they would vote for now. The result was:
A
B
C
F
O
V
Ã˜
others
242
89
98
68
141
294
43
25
Has the popularity of the different parties changed since the election?
SOLUTION. We test the null hypothesis H0 that the result of the opinion poll is an observation
from a multinomial distribution with k = 8 categories and probability parameters pi as given in
the table above. The expected observations (given the null hypothesis) are:
A
B
C
F
O
V
Ã˜
others
258
92
103
60
133
290
34
30
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
We will turn your CV into 
an opportunity of a lifetime
Do you like cars? Would you like to be a part of a successful brand?
We will appreciate and reward both your enthusiasm and talent.
Send us your CV. You will be surprised where it can take you.
Send us your CV on
www.employerforlife.com

Statistics
 
68 
The chi-squared test (or x2 test)
Now we compute the statistic Ï‡2:
Ï‡2 =
8

i=1
(Oi âˆ’Ei)2
Ei
= (242 âˆ’258)2
258
+ Â· Â· Â· + (25 âˆ’30)2
30
= 6.15 .
By looking up in Table B.3 under the Ï‡2 distribution with df = 8 âˆ’1 = 7 degrees of freedom, it
is only seen that the signiï¬cance probability is below 50%. Thus, we have no statistical evidence
to conclude that the popularity of the parties has changed.
Let us ignore the warning in section 19.3 and compute the standardized residuals. For category
A, for example, we ï¬nd
r =
242 âˆ’1000 Â· 0.258

1000 Â· 0.258 Â· 0.742
= âˆ’1.16 .
Altogether we get
A
B
C
F
O
V
Ã˜
others
â€“1.16
â€“0.33
â€“0.52
1.06
0.74
0.28
1.57
â€“0.93
Not surprisingly, all standardized residuals are numerically smaller than 2.
19.6
Example (deaths in the Prussian cavalry)
In the period 1875â€“1894 the number of deaths caused by horse kicks was registered in 10 of the
regiments of the Prussian cavalry. Of the total of 200 â€œregiment-yearsâ€, there were 109 years with
no deaths, 65 years with one death, 22 years with two deaths, three years with three deaths, and
one year with four deaths. We wish to investigate whether these numbers come from a Poisson
distribution Pois(Î»).
In order to get expected numbers greater than ï¬ve (or at least to come close to that), we group
the years with three and four deaths into a single category and thus obtain the following observed
numbers Oi of years with i deaths:
i
Oi
0
109
1
65
2
22
â‰¥3
4
The intensity Î» is estimated as Ë†Î» = 122/200 = 0.61, since there were a total of 122 deaths during
the 200 regiment-years. The point probabilities of a Pois(0.61) distribution are
i
pi
0
0.543
1
0.331
2
0.101
â‰¥3
0.024
Download free eBooks at bookboon.com

Statistics
 
69 
The chi-squared test (or x2 test)
The expected numbers thus become
i
Ei
0
108.7
1
66.3
2
20.2
â‰¥3
4.8
The reader should let himself be impressed by the striking correspondence between expected and
observed numbers. It is evidently superï¬‚uous to carry the analysis any further, but let us compute
the statistic anyway:
Ï‡2 = (109 âˆ’108.7)2
108.7
+ (65 âˆ’66.3)2
66.3
+ (22 âˆ’20.2)2
20.2
+ (4 âˆ’4.8)2
4.8
= 0.3 .
Since there are four categories and we have estimated one parameter using the data, the statistic
should be compared with the Ï‡2 distribution with df = 4 âˆ’1 âˆ’1 = 2 degrees of freedom. As
expected, Table B.3 shows a signiï¬cance probability well above 50%.
Incidentally, the example comes from Ladislaus von Bortkiewiczâ€™s 1898 book Das Gesetz der
kleinen Zahlen.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
AXA Global 
Graduate Program
Find out more and apply

Statistics
 
70 
Contingency tables
20
Contingency tables
20.1
Deï¬nition, method
Suppose that a number of observations are given and that the observations are divided into cate-
gories according to two different criteria. The number of observations in each category can then
be displayed in a contingency table. The purpose of the test presented here is to test whether there
is independence between the two criteria used to categorize the observations.
METHOD. Let there be given an r Ã— s table, i.e. a table with r rows and s columns:
a11
a12
. . .
. . .
a1s
a21
a22
. . .
. . .
a2s
...
...
...
...
...
...
ar1
ar2
. . .
. . .
ars
It has row sums Ri = s
j=1 aij, column sums Sj = r
i=1 aij, and total sum
N =

i,j
aij .
These are the observed numbers O. The row probabilities are estimated as
Ë†piÂ· = Ri
N ,
and the column probabilities as
Ë†pÂ·j = Sj
N .
If there is independence between rows and columns, the cell probabilities can be estimated as
Ë†pij = Ë†piÂ·Ë†pÂ·j = RiSj
N2 .
We can thus compute the expected numbers E:
R1S1
N
R1S2
N
. . .
. . .
R1Ss
N
R2S1
N
R2S2
N
. . .
. . .
R2Ss
N
...
...
...
...
...
...
RrS1
N
RrS2
N
. . .
. . .
RrSs
N
Download free eBooks at bookboon.com

Statistics
 
71 
Contingency tables
since the expected number in the (i, j)â€™th cell is
E = N Ë†pij = RiSj/N .
Now we compute the statistic
Ï‡2 =
 (O âˆ’E)2
E
=
 (aij âˆ’RiSj/N)2
RiSj/N
where the summation is carried out over each cell of the table. If the independence hypothesis
holds true and the expected number is at least 5 in each cell, then the statistic is Ï‡2 distributed
with
df = (r âˆ’1)(s âˆ’1)
degrees of freedom.
Important! If the data are given as percentages, they must be expressed as absolute numbers
before insertion into the contingency table.
20.2
Standardized residuals
If the independence hypothesis is rejected by a Ï‡2 test, one might, as in section 19.3, be interest-
ed in determining which cells contain observed numbers deviating extremely from the expected
numbers. The standardized residuals are computed as
rij =
Oij âˆ’RiSj/n

RiSj/n

1 âˆ’Ri/n

1 âˆ’Sj/n
 .
If the independence hypothesis were true, each rij would be normally distributed with expected
value Âµ = 0 and standard deviation Ïƒ = 1. Standardized residuals numerically greater than 2 are
therefore signs of an extreme observed number.
20.3
Example (studentsâ€™ political orientation)
EXERCISE. At three Danish universities, 488 students were asked about their faculty and which
party they would vote for if there were to be an election tomorrow. The result (in simpliï¬ed form)
was:
A
B
C
F
O
V
Ã˜
Ri
Humanities
37
48
15
26
4
17
10
157
Natural Sci.
32
38
19
18
7
51
2
167
Social Sci.
32
24
15
7
12
69
5
164
Sj
101
110
49
51
23
137
17
488
Investigate whether there is independence between the studentsâ€™ political orientation and their
faculty.
Download free eBooks at bookboon.com

Statistics
 
72 
Contingency tables
SOLUTION. We are dealing with a 3 Ã— 7 table and perform a Ï‡2 test for independence. First, the
expected numbers
E = RiSj
488
are computed and presented in a table:
A
B
C
F
O
V
Ã˜
Humanities
32.5
35.4
15.8
16.4
7.4
44.1
5.5
Natural Sci.
34.6
37.6
16.8
17.5
7.9
46.9
5.8
Social Sci.
33.9
37.0
16.5
17.1
7.7
46.0
5.7
Now the statistic
Ï‡2 =
 (O âˆ’E)2
E
can be computed, since the observed numbers O are the numbers in the ï¬rst table:
Ï‡2 = (37 âˆ’32.5)2
32.5
+ Â· Â· Â· + (5 âˆ’5.7)2
5.7
= 60.9 .
The statistic is to be compared with a Ï‡2 distribution with df = (3 âˆ’1)(7 âˆ’1) = 12 degrees of
freedom. Table B.3 shows that the signiï¬cance probability is well below 0.1%, and we therefore
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
ibili
because 
e Graduate Programme  
for Engineers and Geoscientists
Month 16
I was a construction
supervisor in 
the North Sea 
advising and 
helping foremen 
solve problems
I was a
he
s
Real work 
International opportunities 

ree work placements
al 
Internationa
or

ree wo
I wanted real responsibili 
 I joined MITAS because 
www.discovermitas.com

Statistics
 
73 
Contingency tables
conï¬dently reject the independence hypothesis.
Let us now compute the standardized residuals to see in which cells the observed numbers are
extreme. We use the formula for rij in section 20.2 and get
A
B
C
F
O
V
Ã˜
Humanities
1.1
2.9
â€“0.2
3.0
â€“1.6
â€“5.8
2.4
Natural Sci.
â€“0.6
0.1
0.7
0.2
â€“0.4
0.9
â€“2.0
Social Sci.
â€“0.5
â€“3.0
â€“0.5
â€“3.2
1.9
4.9
â€“0.4
We ï¬nd that there are extreme observations in many cells.
20.4
Ï‡2 test for 2 Ã— 2 tables
A contingency table with two rows and two columns is called a 2Ã—2 table. Let us write the
observed numbers as follows:
a
b
c
d
The statistic thus becomes
Ï‡2 =
ad âˆ’bc
N
2  1
E11
+
1
E12
+
1
E21
+
1
E22

where N = a+b+c+d is the total number of observations, and Eij is the expected number in the
ijâ€™th cell. The statistic Ï‡2 is to be compared with the Ï‡2 distribution with df = (2âˆ’1)(2âˆ’1) = 1
degree of freedom.
If we wish to perform a one-sided test of the independence hypothesis, the statistic
u =
ad âˆ’bc
N
  1
E11
+
1
E12
+
1
E21
+
1
E22

is used instead. Under the independence hypothesis, u will be standard normally distributed.
20.5
Fisherâ€™s exact test for 2 Ã— 2 tables
Given a 2 Ã— 2 table, nothing stands in the way of using the Ï‡2 test, but there is a better test in this
situation called Fisherâ€™s exact test. Fisherâ€™s exact test does not use any normal approximation,
and may therefore still be applied when the number of expected observations in one or more of
the cells is smaller than ï¬ve.
METHOD. Let there be given a 2Ã—2 table:
a
b
c
d
with row sums R1 = a+b and R2 = c+d and column sums S1 = a+c and S2 = b+d and total
sum N = R1 + R2 = S1 + S2 = a + b + c + d. We test the independence hypothesis H0 against
Download free eBooks at bookboon.com

Statistics
 
74 
Distribution-free costs
the alternative hypothesis H1 that the â€œdiagonal probabilitiesâ€ p11 and p22 are greater than what
they would have been had there been independence. (This situation can always be arranged by
switching the rows if necessary.) The conditional probability of obtaining exactly the 2Ã—2 table
above, given that the row sums are R1 and R2, and that the column sums are S1 and S2, is
Pconditional = R1! R2! S1! S2!
N! a! b! c! d!
.
The signiï¬cance probability in Fisherâ€™s exact test is the sum of Pconditional taken on all 2Ã—2 tables
with the same row and column sums as in the given table, and which are at least as extreme as the
given table:
PFisher =
min{b,c}

i=0
R1! R2! S1! S2!
N! (a + i)! (b âˆ’i)! (c âˆ’i)! (d + i)! .
The independence hypothesis H0 is rejected if PFisher is smaller than 5% (or whatever signiï¬-
cance level one has chosen).
ADDENDUM: If a two-sided test is performed, i.e. if one does not test against any speciï¬c alter-
native hypothesis, the signiï¬cance probability becomes 2 Â· PFisher. It is then necessary that the
2Ã—2 table is written in such a way that the observed numbers in the diagonal are greater than the
expected numbers (this can always be obtained by switching the rows if necessary).
20.6
Example (Fisherâ€™s exact test)
In a medical experiment concerning alternative treatments, ten patients are randomly divided into
two groups with ï¬ve patients in each. The patients in the ï¬rst group receive acupuncture, while
the patients in the other group receive no treatment. It is then seen which patients are ï¬t or ill at
the end of the experiment. The result can be presented in a 2 Ã— 2 table:
ï¬t
ill
acupuncture
4
1
no treatment
2
3
The signiï¬cance probability in Fisherâ€™s exact test is computed as
PFisher =
1

i=0
5! 5! 6! 4!
10! (4 + i)! (1 âˆ’i)! (2 âˆ’i)! (3 + i)! = 26% .
With such a large signiï¬cance probability, there is no evidence that acupuncture had any effect.
21
Distribution-free tests
In all tests considered so far, we have known something about the distribution from which the
given samples originated. We knew, for example, that the distribution was a normal distribution
even though we didnâ€™t know the expected value or the standard deviation.
Sometimes, though, one knows nothing at all about the underlying distribution. It then be-
comes necessary to use a distribution-free test (also known as a non-parametric test). The two
examples considered in this chapter are due to Frank Wilcoxon (1892â€“1965).
Download free eBooks at bookboon.com

Statistics
 
75 
Distribution-free costs
21.1
Wilcoxonâ€™s test for one set of observations
Let there be given n independent observations d1, . . . , dn from an unknown distribution. We test
the null hypothesis
H0: The unknown distribution is symmetric around 0.
Each observation di is given a rank which is one of the numbers 1, 2, . . . , n. The observation
with the smallest numerical value is assigned rank 1, the observation with the second smallest
numerical value is assigned rank 2, etc. Now deï¬ne the statistics
t+ = (ranks corresponding to positive di),
tâˆ’= (ranks corresponding to negative di).
(One can check at this point whether t+ + tâˆ’= n(n + 1)/2; if not, one has added the numbers
incorrectly.) If H0 holds true, then t+ and tâˆ’should be more or less equal. When to reject H0
depends on which alternative hypothesis is tested against.
If we test H0 against the alternative hypothesis
H1: The unknown distribution primarily gives positive observations,
then H0 is rejected if tâˆ’is extremely small. Choose a signiï¬cance level Î± and consult Table B.8
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
MASTER IN MANAGEMENT
mim.admissions@ie.edu
Follow us on IE MIM Experience
www.ie.edu/master-management
#10 WORLDWIDE
MASTER IN MANAGEMENT 
FINANCIAL TIMES
55 Nationalities
in class
5 Specializations
Personalize your program 
Length: 1O MONTHS
Av. Experience: 1 YEAR
Language: ENGLISH / SPANISH
Format: FULL-TIME
Intakes: SEPT / FEB
â€¢ STUDY IN THE CENTER OF MADRID AND TAKE ADVANTAGE OF THE UNIQUE OPPORTUNITIES
  THAT THE CAPITAL OF SPAIN OFFERS
â€¢ PROPEL YOUR EDUCATION BY EARNING A DOUBLE DEGREE THAT BEST SUITS YOUR
  PROFESSIONAL GOALS
â€¢ STUDY A SEMESTER ABROAD AND BECOME A GLOBAL CITIZEN WITH THE BEYOND BORDERS
  EXPERIENCE
93%
OF MIM STUDENTS ARE
WORKING IN THEIR SECTOR 3 MONTHS
FOLLOWING GRADUATION

Statistics
 
76 
Distribution-free costs
under n and Î±. If tâˆ’is smaller than or equal to the table value, H0 is rejected. If tâˆ’is greater than
the table value, H0 is accepted.
If we test H0 against the alternative hypothesis
H1: The unknown distribution primarily gives negative observations,
then H0 is rejected if t+ is extremely small. Choose a signiï¬cance level Î± and consult Table B.8
under n and Î±. If t+ is smaller than or equal to the table value, H0 is rejected. If t+ is greater than
the table value, H0 is accepted.
If we donâ€™t test H0 against any particular alternative hypothesis, the null hypothesis is rejected
if the minimum t := min{t+, tâˆ’} is extremely small. Choose a signiï¬cance level Î± and consult
Table B.8 under n and Î±/2 (if, for example, we choose the signiï¬cance level Î± = 5%, then we
look up in the table under n and 0.025). If t is smaller than or equal to the table value, we reject
H0. If t is greater than the table value, we accept H0.
The above test applies in particular when two sets of observations x1, . . . , xn and y1, . . . , yn
are given and di is the difference between the â€œbefore valuesâ€ xi and the â€œafter valuesâ€ yi, i.e.
di = xi âˆ’yi. If there are only random, unsystematic differences between the before and after
values, it follows that the diâ€™s are distributed symmetrically around 0.
21.2
Example
An experiment involving ten patients is carried out to determine whether physical exercise lowers
blood pressure. At the beginning of the experiment, the patientsâ€™ blood pressures are measured.
These observations are denoted x1, . . . , x10. After a month of exercise, the blood pressures are
measured again. These observations are denoted y1, . . . , y10. We now test the null hypothesis
H0: Physical exercise has no inï¬‚uence on blood pressure. The ten differences di = xi âˆ’yi are
therefore distributed symmetrically around 0,
against the alternative hypothesis
H1: Physical exercise causes the blood pressure to decrease. The ten differences di are therefore
primarily positive.
We compute the ranks and t+ and tâˆ’:
Person
1
2
3
4
5
6
7
8
9
10
Before xi
140
125
110
130
170
165
135
140
155
145
After yi
137
137
102
104
172
125
140
110
140
126
Difference di
3
â€“12
8
26
â€“2
40
â€“5
30
15
19
Rank
2
5
4
8
1
10
3
9
6
7
t+ = 2 + 4 + 6 + 7 + 8 + 9 + 10 = 46,
tâˆ’= 1 + 3 + 5 = 9.
Download free eBooks at bookboon.com

Statistics
 
77 
Distribution-free costs
We shall reject H0 if tâˆ’= 9 is extremely small. Table B.8 with signiï¬cance level Î± = 5% shows
that â€œextremely smallâ€ means â‰¦10. Conclusion: The test shows that the null hypothesis H0 must
be rejected against the alternative hypothesis H1 at signiï¬cance level 5%.
21.3
The normal approximation to Wilcoxonâ€™s test for one set of observations
Table B.8 includes values up to n = 50. If the number of observations is greater, a normal dis-
tribution approximation can be applied. Indeed, if the null hypothesis is true, the statistic t+ is
approximately normally distributed with expected value
Âµ = n(n + 1)
4
and standard deviation
Ïƒ =

n(n + 1)(2n + 1)
24
.
The signiï¬cance probability is therefore found by comparison of the statistic
z = t+ âˆ’Âµ
Ïƒ
with Table B.2 of the standard normal distribution.
EXAMPLE. Let us use the normal approximation to ï¬nd the signiï¬cance probability in the previous
example (even though n here is smaller than 50 and the approximation therefore is not highly
precise). We get Âµ = 27.5 and Ïƒ = 9.81. The statistic therefore becomes z = 1.89, which gives
a signiï¬cance probability of 2.9%. The conclusion is thus the same, namely that H0 is rejected at
signiï¬cance level 5%.
21.4
Wilcoxonâ€™s test for two sets of observations
Suppose we have two sets x1, . . . , xn and y1, . . . , ym of independent observations. We test the
null hypothesis
H0: The observations come from the same distribution.
Each of the n + m observations is assigned a rank which is one of the numbers 1, 2, . . . , n + m.
The observation with the smallest numerical value is assigned rank 1, the observation with the
second smallest numerical value is assigned rank 2, etc. Deï¬ne the statistic
tx = (ranks of the xiâ€™s).
Whether H0 is rejected or not depends on which alternative hypothesis we test against.
If we test H0 against the alternative hypothesis
H1: The xiâ€™s are primarily smaller than the yiâ€™s,
then H0 is rejected if tx is extremely small. Look up in Table B.9 under n and m. If tx is smaller
than or equal to the table value, then H0 is rejected at signiï¬cance level Î± = 5%. If tx is greater
Download free eBooks at bookboon.com

Statistics
 
78 
Distribution-free costs
than the table value, then H0 is accepted at signiï¬cance level Î± = 5%.
If we test H0 against the alternative hypothesis
H1: The xiâ€™s are primarily greater than the yiâ€™s,
then one has to switch the roles of xiâ€™s and yiâ€™s and continue as described above.
If we donâ€™t test H0 against any particular alternative hypothesis, then the null hypothesis is reject-
ed if the minimum
t := min{tx, n(n + m + 1) âˆ’tx}
is extremely small. Look up in Table B.9 under n and m. If t is smaller than or equal to the table
value, then H0 is rejected at signiï¬cance level Î± = 10%. If t is greater than the table value, then
H0 is accepted at signiï¬cance level 10%.
21.5
The normal approximation to Wilcoxonâ€™s test for two sets of observations
Table B.9 applies for moderate values of n and m. If the number of observations is greater, one
can use a normal distribution approximation. Indeed, if the null hypothesis holds true, the statistic
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Statistics
 
79 
Linear regression
tx is approximately normally distributed with expected value
Âµ = n(n + m + 1)
2
and standard deviation
Ïƒ =

nm(n + m + 1)
12
.
The signiï¬cance probability is then found by comparing the statistic
z = tx âˆ’Âµ
Ïƒ
with Table B.2 of the standard normal distribution.
22
Linear regression
22.1
The model
Suppose we have a sample consisting of n pairs of observations
(x1, y1), (x2, y2), . . . , (xn, yn) .
We propose the model that each yi is an observation from a random variable
Yi = Î²0 + Î²1xi + Ei
where the Eiâ€™s are independent normally distributed random variables with expected value 0 and
common variance Ïƒ2. Thus we can express each yi as
yi = Î²0 + Î²1xi + ei
where ei is an observation from Ei. We call yi the response variable, xi the declaring variable
and ei the remainder term.
22.2
Estimation of the parameters Î²0 and Î²1
Let Â¯x be the mean value of the xiâ€™s and Â¯y the mean value of the yiâ€™s. Deï¬ne the sum of products
of errors as
SPExy =
n

i=1
(xi âˆ’Â¯x)(yi âˆ’Â¯y)
and the sum of squares of errors as
SSEx =
n

i=1
(xi âˆ’Â¯x)2
The parameters Î²0 and Î²1 of the regression equation are now estimated as
ï£±
ï£²
ï£³
Ë†Î²1 = SPExy
SSEx
Ë†Î²0 = Â¯y âˆ’Ë†Î²1Â¯x
Download free eBooks at bookboon.com

Statistics
 
80 
Linear regression
22.3
The distribution of the estimators
If the modelâ€™s assumptions are met, the estimator Ë†Î²0 is normally distributed with expected value
Î²0 (the estimator thus is unbiased) and variance Ïƒ2(1/n + Â¯x2/SSEx). In other words, it holds
that
Ë†Î²0 âˆ¼N

Î²0, Ïƒ2
 1
n +
Â¯x2
SSEx

.
Moreover, the estimator Ë†Î²1 is normally distributed with expected value Î²1 (this estimator is there-
fore unbiased too) and variance Ïƒ2/SSEx. In other words, it holds that
Ë†Î²1 âˆ¼N

Î²1,
Ïƒ2
SSEx

.
22.4
Predicted values Ë†yi and residuals Ë†ei
From the estimates Ë†Î²0 and Ë†Î²1, the predicted value of yi can be computed for each i as
Ë†yi = Ë†Î²0 + Ë†Î²1xi .
The iâ€™th residual Ë†ei is the difference between the actual value yi and the predicted value Ë†yi:
Ë†ei = yi âˆ’Ë†yi .
The residual Ë†ei is an estimate of the remainder term ei.
22.5
Estimation of the variance Ïƒ2
We introduce the sum of squares of residuals as
SSR =
n

i=1
Ë†e2
i .
The variance Ïƒ2 of the remainder terms is now estimated as
s2 = SSR
n âˆ’2 .
This estimator is unbiased (but different from the maximum likelihood estimator).
22.6
Conï¬dence intervals for the parameters Î²0 and Î²1
After estimating the parameters Î²0 and Î²1, we can compute the conï¬dence intervals with conï¬-
dence level 1 âˆ’Î± around the estimates Ë†Î²0 and Ë†Î²1. These are
ï£±
ï£´
ï£´
ï£²
ï£´
ï£´
ï£³
Ë†Î²0 Â± t1âˆ’Î±/2s

1
n +
Â¯x2
SSEx
Ë†Î²1 Â± t1âˆ’Î±/2
s
âˆšSSEx
The number t1âˆ’Î±/2 is determined by F(u1âˆ’Î±/2) = 1 âˆ’Î±/2, where F is the distribution function
of Studentâ€™s t distribution with n âˆ’1 degrees of freedom (see also section 15.8).
Download free eBooks at bookboon.com

Statistics
 
81 
Linear regression
22.7
The determination coefï¬cient R2
In order to investigate how well the model with the estimated parameters describes the actual
observations, we compute the determination coefï¬cient
R2 = SSEy âˆ’SSR
SSEy
.
R2 lies in the interval [0, 1] and measures the part of the variation of the yiâ€™s which the model
describes as a linear function of the xiâ€™s.
Remember: The greater the determination coefï¬cient R2 is, the better the model describes the
observations.
22.8
Predictions and prediction intervals
Let there be given a real number x0. The function value
y0 = Î²0 + Î²1x0
is then estimated, or predicted, as
Ë†y0 = Ë†Î²0 + Ë†Î²1x0 .
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
â€œThe perfect start 
of a successful, 
international career.â€
CLICK HERE 
to discover why both socially 
and academically the University 
of Groningen is one of the best 
places for a student to be 
www.rug.nl/feb/education
Excellent Economics and Business programmes at:

Statistics
 
82 
Linear regression
The conï¬dence interval, or prediction interval, with conï¬dence level 1 âˆ’Î± around the estimate
Ë†y0 is
Ë†y0 Â± t1âˆ’Î±/2s

1 + 1
n + (x0 âˆ’Â¯x)2
SSEx
.
The number t1âˆ’Î±/2 is determined by F(u1âˆ’Î±/2) = 1 âˆ’Î±/2, where F is the distribution function
of Studentâ€™s t distribution with n âˆ’2 degrees of freedom (see also section 15.8).
22.9
Overview of formulae
Sx = n
i=1 xi
The sum of the xiâ€™s
Â¯x = Sx/n
The mean value of the xiâ€™s
SSx = n
i=1 x2
i
The sum of the squares of the xiâ€™s
SSEx = n
i=1(xi âˆ’Â¯x)2 = SSx âˆ’S2
x/n
The sum of the squares of the errors
s2
x = SSEx/(n âˆ’1)
Empirical variance of the xiâ€™s
SPxy = n
i=1 xiyi
The sum of the products
SPExy = n
i=1(xi âˆ’Â¯x)(yi âˆ’Â¯y) = SPxy âˆ’SxSy/n
The sum of the products of the errors
Ë†Î²1 = SPExy/SSEx
The estimate of Î²1
Ë†Î²0 = Â¯y âˆ’Ë†Î²1Â¯x
The estimate of Î²0
Ë†yi = Ë†be0 + Ë†Î²1xi
Predicted value of yi
Ë†ei = yi âˆ’Ë†yi
The iâ€™th residual
SSR = n
i=1 Ë†e2
i = SSEy âˆ’SPE2
xy/SSEx
The sum of the squares of the residuals
s2 = SSR/(n âˆ’2)
The estimate Ïƒ2
R2 = 1 âˆ’SSR/SSEy
The determination coefï¬cient
22.10
Example
EXERCISE. It is claimed that the temperature in the Andes Mountains decreases by six degrees per
1000 metres. The following temperatures were measured simultaneously at ten different localities
Download free eBooks at bookboon.com

Statistics
 
83 
Linear regression
in the same region:
Altitude xi
Temperature yi
(metres)
(degrees)
500
15
1000
14
1500
11
2000
6
2500
â€“1
3000
2
3500
0
4000
â€“4
4500
â€“8
5000
â€“14
We use a linear regression model
yi = Î²0 + Î²1xi + ei
where the remainder terms ei are independent normally distributed with expected value 0 and the
same (unknown) variance Ïƒ2.
1) Estimate the parameters Î²0 and Î²1.
2) Determine the conï¬dence interval with conï¬dence level 95% for Î²1.
3) Can the hypothesis H0 : Î²1 = âˆ’0.006 be accepted?
4) To how large degree can the difference of temperature be explained as a linear function of the
altitude?
SOLUTION. First we perform the relevant computations:
Sx = 10
i=1 xi = 27500
Sy = 10
i=1 yi = 21
Â¯x = Sx/10 = 2750
Â¯y = Sy/10 = 2.1
SSx = 10
i=1 x2
i = 96250000
SSy = 10
i=1 y2
i = 859
SSEx = SSx âˆ’S2
x/10 = 20625000
SSEy = SSy âˆ’S2
y/10 = 814.9
SPxy = 10
i=1 xiyi = âˆ’68500
SPExy = SPxy âˆ’SxSy/10 = âˆ’126250
Ë†Î²1 = SPExy/SSEx = âˆ’0.0061
Ë†Î²0 = Â¯y âˆ’Ë†Î²1Â¯x = 18.9
SSR = SSEy âˆ’SPE2
xy/SSEx = 42.1
s2 = SSR/8 = 5.26
R2 = 1 âˆ’SSR/SSEy = 0.948
1) It appears directly from the computations that the estimates of Î²0 and Î²1 are
Ë†Î²0 = 18.9 ,
Ë†Î²1 = âˆ’0.0061 .
Download free eBooks at bookboon.com

Statistics
 
84 
Linear regression
2) Table B.4, under df = 10 âˆ’1 = 9 degrees of freedom, shows that t0.975 = 2.26 (see also
section 15.8). The conï¬dence interval around Ë†Î²1 thus becomes

âˆ’0.0061 âˆ’2.26
âˆš
5.26
âˆš
20625000 , âˆ’0.0061 + 2.26
âˆš
5.26
âˆš
20625000

= [âˆ’0.0072 , âˆ’0.0050] .
3) The hypothesis H0 : Î²1 = âˆ’0.006 is accepted, since this value lies within the conï¬dence
interval.
4) The part of the temperature difference describable as a linear function of the altitude is nothing
other than the determination coefï¬cient
R2 = 94.8% .
The fact that R2 is large (close to 100%) shows that the actual temperatures are quite close to those
predicted. This also appears from the ï¬gure below, which shows that the actual temperatures are
very close to the regression line:
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
American online      
LIGS University 
â–¶â–¶enroll by September 30th, 2014 and 
â–¶â–¶save up to 16% on the tuition!
â–¶â–¶pay in 10 installments / 2 years
â–¶â–¶Interactive Online education
â–¶â–¶visit www.ligsuniversity.com to 
      find out more!
is currently enrolling in the
Interactive Online BBA, MBA, MSc, 
DBA and PhD  programs:
Note: LIGS University is not accredited by any 
nationally recognized accrediting agency listed 
by the US Secretary of Education. 
More info here. 

Statistics
 
85 
Linear regression
Download free eBooks at bookboon.com

Statistics
 
86 
Overview of discrete distributions
A
Overview of discrete distributions
Distribution
Description
Values
Point proba-
bilities
Mean
value
Variance
Binomial
distribution
Bin(n, p)
Number
of
successes in
n tries
k = 0, 1, . . . , n

n
k

pkqnâˆ’k
np
npq
Poisson
distribution
Pois(Î»)
Number
of
spontaneous
events in a
time interval
k = 0, 1, . . .
Î»k
k! eâˆ’Î»
Î»
Î»
Geometrical
distribution
Geo(p)
Number
of
fail-
ures
before
success
k = 0, 1, . . .
qkp
q/p
q/p2
Hyper-
geometrical
distribution
HG(n, r, N)
Number
of
red
balls
among
n
balls
k
= 0, . . .
. . . , min{n, r}
r
k

s
n âˆ’k

N
n

nr/N
nrs(Nâˆ’n)
N2(Nâˆ’1)
Negative
binomial
distribution
NB(n, p)
Number
of
failures
before
the
nâ€™th success
k = 0, 1, . . .

n + k âˆ’1
n âˆ’1

Â· pn Â· qk
nq/p
nq/p2
Multi-
nomial-
distribution
Mult(n, . . . )
Number
of
sample
points
of
each type
(k1, . . . , kr)
where
 ki = n

n
k1 Â· Â· Â· kr

Â·  pki
i
â€”
â€”
Download free eBooks at bookboon.com

Statistics
 
87 
Tables
B
Tables
B.1
How to read the tables
Table B.2 gives values of the distribution function
Î¦(u) =
 u
âˆ’âˆ
1
âˆš
2Ï€ exp

âˆ’1
2t2

dt
of the standard normal distribution.
Table B.3 gives values of x for which the distribution function F = FÏ‡2 of the Ï‡2 distribution
with df degrees of freedom takes the values F(x) = 0.500, F(x) = 0.600, etc.
Table B.4 gives values of x for which the distribution function F = FStudent of Studentâ€™s t distri-
bution with df degrees of freedom takes the values F(x) = 0.600, F(x) = 0.700, etc.
Table B.5, Table B.6 and Table B.7 give values of x for which the distribution function F =
FFisher of Fisherâ€™s F distribution with n degrees of freedom in the numerator (top line) and
m degrees of freedom in the denominator (leftmost column) takes the values F(x) = 0.90,
F(x) = 0.95, and F(x) = 0.99, respectively.
Download free eBooks at bookboon.com

Statistics
 
88 
Tables
Table B.8 and Table B.9 give critical values for Wilcoxonâ€™s tests for one and two sets of observa-
tions. See Chapter 21 for further details.
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more

Statistics
 
89 
Tables
B.2
The standard normal distribution
u
Î¦(u)
Î¦(âˆ’u)
0.00
0.5000
0.5000
0.01
0.5040
0.4960
0.02
0.5080
0.4920
0.03
0.5120
0.4880
0.04
0.5160
0.4840
0.05
0.5199
0.4801
0.06
0.5239
0.4761
0.07
0.5279
0.4721
0.08
0.5319
0.4681
0.09
0.5359
0.4641
0.10
0.5398
0.4602
0.11
0.5438
0.4562
0.12
0.5478
0.4522
0.13
0.5517
0.4483
0.14
0.5557
0.4443
0.15
0.5596
0.4404
0.16
0.5636
0.4364
0.17
0.5675
0.4325
0.18
0.5714
0.4286
0.19
0.5753
0.4247
0.20
0.5793
0.4207
0.21
0.5832
0.4168
0.22
0.5871
0.4129
0.23
0.5910
0.4090
0.24
0.5948
0.4052
0.25
0.5987
0.4013
0.26
0.6026
0.3974
0.27
0.6064
0.3936
0.28
0.6103
0.3897
0.29
0.6141
0.3859
0.30
0.6179
0.3821
0.31
0.6217
0.3783
0.32
0.6255
0.3745
0.33
0.6293
0.3707
0.34
0.6331
0.3669
0.35
0.6368
0.3632
u
Î¦(u)
Î¦(âˆ’u)
0.36
0.6406
0.3594
0.37
0.6443
0.3557
0.38
0.6480
0.3520
0.39
0.6517
0.3483
0.40
0.6554
0.3446
0.41
0.6591
0.3409
0.42
0.6628
0.3372
0.43
0.6664
0.3336
0.44
0.6700
0.3300
0.45
0.6736
0.3264
0.46
0.6772
0.3228
0.47
0.6808
0.3192
0.48
0.6844
0.3156
0.49
0.6879
0.3121
0.50
0.6915
0.3085
0.51
0.6950
0.3050
0.52
0.6985
0.3015
0.53
0.7019
0.2981
0.54
0.7054
0.2946
0.55
0.7088
0.2912
0.56
0.7123
0.2877
0.57
0.7157
0.2843
0.58
0.7190
0.2810
0.59
0.7224
0.2776
0.60
0.7257
0.2743
0.61
0.7291
0.2709
0.62
0.7324
0.2676
0.63
0.7357
0.2643
0.64
0.7389
0.2611
0.65
0.7422
0.2578
0.66
0.7454
0.2546
0.67
0.7486
0.2514
0.68
0.7517
0.2483
0.69
0.7549
0.2451
0.70
0.7580
0.2420
0.71
0.7611
0.2389
u
Î¦(u)
Î¦(âˆ’u)
0.72
0.7642
0.2358
0.73
0.7673
0.2327
0.74
0.7704
0.2296
0.75
0.7734
0.2266
0.76
0.7764
0.2236
0.77
0.7794
0.2206
0.78
0.7823
0.2177
0.79
0.7852
0.2148
0.80
0.7881
0.2119
0.81
0.7910
0.2090
0.82
0.7939
0.2061
0.83
0.7967
0.2033
0.84
0.7995
0.2005
0.85
0.8023
0.1977
0.86
0.8051
0.1949
0.87
0.8078
0.1922
0.88
0.8106
0.1894
0.89
0.8133
0.1867
0.90
0.8159
0.1841
0.91
0.8186
0.1814
0.92
0.8212
0.1788
0.93
0.8238
0.1762
0.94
0.8264
0.1736
0.95
0.8289
0.1711
0.96
0.8315
0.1685
0.97
0.8340
0.1660
0.98
0.8365
0.1635
0.99
0.8389
0.1611
1.00
0.8413
0.1587
1.01
0.8438
0.1562
1.02
0.8461
0.1539
1.03
0.8485
0.1515
1.04
0.8508
0.1492
1.05
0.8531
0.1469
1.06
0.8554
0.1446
1.07
0.8577
0.1423
Download free eBooks at bookboon.com

Statistics
 
90 
Tables
u
Î¦(u)
Î¦(âˆ’u)
1.08
0.8599
0.1401
1.09
0.8621
0.1379
1.10
0.8643
0.1357
1.11
0.8665
0.1335
1.12
0.8686
0.1314
1.13
0.8708
0.1292
1.14
0.8729
0.1271
1.15
0.8749
0.1251
1.16
0.8770
0.1230
1.17
0.8790
0.1210
1.18
0.8810
0.1190
1.19
0.8830
0.1170
1.20
0.8849
0.1151
1.21
0.8869
0.1131
1.22
0.8888
0.1112
1.23
0.8907
0.1093
1.24
0.8925
0.1075
1.25
0.8944
0.1056
1.26
0.8962
0.1038
1.27
0.8980
0.1020
1.28
0.8997
0.1003
1.29
0.9015
0.0985
1.30
0.9032
0.0968
1.31
0.9049
0.0951
1.32
0.9066
0.0934
1.33
0.9082
0.0918
1.34
0.9099
0.0901
1.35
0.9115
0.0885
1.36
0.9131
0.0869
1.37
0.9147
0.0853
1.38
0.9162
0.0838
1.39
0.9177
0.0823
1.40
0.9192
0.0808
1.41
0.9207
0.0793
1.42
0.9222
0.0778
1.43
0.9236
0.0764
1.44
0.9251
0.0749
u
Î¦(u)
Î¦(âˆ’u)
1.45
0.9265
0.0735
1.46
0.9279
0.0721
1.47
0.9292
0.0708
1.48
0.9306
0.0694
1.49
0.9319
0.0681
1.50
0.9332
0.0668
1.51
0.9345
0.0655
1.52
0.9357
0.0643
1.53
0.9370
0.0630
1.54
0.9382
0.0618
1.55
0.9394
0.0606
1.56
0.9406
0.0594
1.57
0.9418
0.0582
1.58
0.9429
0.0571
1.59
0.9441
0.0559
1.60
0.9452
0.0548
1.61
0.9463
0.0537
1.62
0.9474
0.0526
1.63
0.9484
0.0516
1.64
0.9495
0.0505
1.65
0.9505
0.0495
1.66
0.9515
0.0485
1.67
0.9525
0.0475
1.68
0.9535
0.0465
1.69
0.9545
0.0455
1.70
0.9554
0.0446
1.71
0.9564
0.0436
1.72
0.9573
0.0427
1.73
0.9582
0.0418
1.74
0.9591
0.0409
1.75
0.9599
0.0401
1.76
0.9608
0.0392
1.77
0.9616
0.0384
1.78
0.9625
0.0375
1.79
0.9633
0.0367
1.80
0.9641
0.0359
1.81
0.9649
0.0351
u
Î¦(u)
Î¦(âˆ’u)
1.82
0.9656
0.0344
1.83
0.9664
0.0336
1.84
0.9671
0.0329
1.85
0.9678
0.0322
1.86
0.9686
0.0314
1.87
0.9693
0.0307
1.88
0.9699
0.0301
1.89
0.9706
0.0294
1.90
0.9713
0.0287
1.91
0.9719
0.0281
1.92
0.9726
0.0274
1.93
0.9732
0.0268
1.94
0.9738
0.0262
1.95
0.9744
0.0256
1.96
0.9750
0.0250
1.97
0.9756
0.0244
1.98
0.9761
0.0239
1.99
0.9767
0.0233
2.00
0.9772
0.0228
2.01
0.9778
0.0222
2.02
0.9783
0.0217
2.03
0.9788
0.0212
2.04
0.9793
0.0207
2.05
0.9798
0.0202
2.06
0.9803
0.0197
2.07
0.9808
0.0192
2.08
0.9812
0.0188
2.09
0.9817
0.0183
2.10
0.9821
0.0179
2.11
0.9826
0.0174
2.12
0.9830
0.0170
2.13
0.9834
0.0166
2.14
0.9838
0.0162
2.15
0.9842
0.0158
2.16
0.9846
0.0154
2.17
0.9850
0.0150
2.18
0.9854
0.0146
Download free eBooks at bookboon.com

Statistics
 
91 
Tables
u
Î¦(u)
Î¦(âˆ’u)
2.19
0.9857
0.0143
2.20
0.9861
0.0139
2.21
0.9864
0.0136
2.22
0.9868
0.0132
2.23
0.9871
0.0129
2.24
0.9875
0.0125
2.25
0.9878
0.0122
2.26
0.9881
0.0119
2.27
0.9884
0.0116
2.28
0.9887
0.0113
2.29
0.9890
0.0110
2.30
0.9893
0.0107
2.31
0.9896
0.0104
2.32
0.9898
0.0102
2.33
0.9901
0.0099
2.34
0.9904
0.0096
2.35
0.9906
0.0094
2.36
0.9909
0.0091
2.37
0.9911
0.0089
2.38
0.9913
0.0087
2.39
0.9916
0.0084
2.40
0.9918
0.0082
2.41
0.9920
0.0080
2.42
0.9922
0.0078
2.43
0.9925
0.0075
2.44
0.9927
0.0073
2.45
0.9929
0.0071
2.46
0.9931
0.0069
2.47
0.9932
0.0068
2.48
0.9934
0.0066
2.49
0.9936
0.0064
2.50
0.9938
0.0062
2.51
0.9940
0.0060
2.52
0.9941
0.0059
2.53
0.9943
0.0057
2.54
0.9945
0.0055
2.55
0.9946
0.0054
u
Î¦(u)
Î¦(âˆ’u)
2.56
0.9948
0.0052
2.57
0.9949
0.0051
2.58
0.9951
0.0049
2.59
0.9952
0.0048
2.60
0.9953
0.0047
2.61
0.9955
0.0045
2.62
0.9956
0.0044
2.63
0.9957
0.0043
2.64
0.9959
0.0041
2.65
0.9960
0.0040
2.66
0.9961
0.0039
2.67
0.9962
0.0038
2.68
0.9963
0.0037
2.69
0.9964
0.0036
2.70
0.9965
0.0035
2.71
0.9966
0.0034
2.72
0.9967
0.0033
2.73
0.9968
0.0032
2.74
0.9969
0.0031
2.75
0.9970
0.0030
2.76
0.9971
0.0029
2.77
0.9972
0.0028
2.78
0.9973
0.0027
2.79
0.9974
0.0026
2.80
0.9974
0.0026
2.81
0.9975
0.0025
2.82
0.9976
0.0024
2.83
0.9977
0.0023
2.84
0.9977
0.0023
2.85
0.9978
0.0022
2.86
0.9979
0.0021
2.87
0.9979
0.0021
2.88
0.9980
0.0020
2.89
0.9981
0.0019
2.90
0.9981
0.0019
2.91
0.9982
0.0018
2.92
0.9982
0.0018
u
Î¦(u)
Î¦(âˆ’u)
2.93
0.9983
0.0017
2.94
0.9984
0.0016
2.95
0.9984
0.0016
2.96
0.9985
0.0015
2.97
0.9985
0.0015
2.98
0.9986
0.0014
2.99
0.9986
0.0014
3.00
0.9987
0.0013
3.10
0.9990
0.0010
3.20
0.9993
0.0007
3.30
0.9995
0.0005
3.40
0.9997
0.0003
3.50
0.9998
0.0002
3.60
0.9998
0.0002
3.70
0.9999
0.0001
3.80
0.9999
0.0001
3.90
1.0000
0.0000
4.00
1.0000
0.0000
Download free eBooks at bookboon.com

Statistics
 
92 
Tables
B.3
The Ï‡2 distribution (values x with FÏ‡2(x) = 0.500 etc.)
df
0.500
0.600
0.700
0.800
0.900
0.950
0.975
0.990
0.995
0.999
1
0.45
0.71
1.07
1.64
2.71
3.84
5.02
6.63
7.88
10.83
2
1.39
1.83
2.41
3.22
4.61
5.99
7.38
9.21
10.60
13.82
3
2.37
2.95
3.66
4.64
6.25
7.81
9.35
11.34
12.84
16.27
4
3.36
4.04
4.88
5.99
7.78
9.49
11.14
13.28
14.86
18.47
5
4.35
5.13
6.06
7.29
9.24
11.07
12.83
15.09
16.75
20.52
6
5.35
6.21
7.23
8.56
10.64
12.59
14.45
16.81
18.55
22.46
7
6.35
7.28
8.38
9.80
12.02
14.07
16.01
18.48
20.28
24.32
8
7.34
8.35
9.52
11.03
13.36
15.51
17.53
20.09
21.95
26.12
9
8.34
9.41
10.66
12.24
14.68
16.92
19.02
21.67
23.59
27.88
10
9.34
10.47
11.78
13.44
15.99
18.31
20.48
23.21
25.19
29.59
11
10.34
11.53
12.90
14.63
17.28
19.68
21.92
24.72
26.76
31.26
12
11.34
12.58
14.01
15.81
18.55
21.03
23.34
26.22
28.30
32.91
13
12.34
13.64
15.12
16.98
19.81
22.36
24.74
27.69
29.82
34.53
14
13.34
14.69
16.22
18.15
21.06
23.68
26.12
29.14
31.32
36.12
15
14.34
15.73
17.32
19.31
22.31
25.00
27.49
30.58
32.80
37.70
16
15.34
16.78
18.42
20.47
23.54
26.30
28.85
32.00
34.27
39.25
17
16.34
17.82
19.51
21.61
24.77
27.59
30.19
33.41
35.72
40.79
18
17.34
18.87
20.60
22.76
25.99
28.87
31.53
34.81
37.16
42.31
19
18.34
19.91
21.69
23.90
27.20
30.14
32.85
36.19
38.58
43.82
20
19.34
20.95
22.77
25.04
28.41
31.41
34.17
37.57
40.00
45.31
21
20.34
21.99
23.86
26.17
29.62
32.67
35.48
38.93
41.40
46.80
22
21.34
23.03
24.94
27.30
30.81
33.92
36.78
40.29
42.80
48.27
23
22.34
24.07
26.02
28.43
32.01
35.17
38.08
41.64
44.18
49.73
24
23.34
25.11
27.10
29.55
33.20
36.42
39.36
42.98
45.56
51.18
25
24.34
26.14
28.17
30.68
34.38
37.65
40.65
44.31
46.93
52.62
26
25.34
27.18
29.25
31.79
35.56
38.89
41.92
45.64
48.29
54.05
27
26.34
28.21
30.32
32.91
36.74
40.11
43.19
46.96
49.64
55.48
28
27.34
29.25
31.39
34.03
37.92
41.34
44.46
48.28
50.99
56.89
29
28.34
30.28
32.46
35.14
39.09
42.56
45.72
49.59
52.34
58.30
30
29.34
31.32
33.53
36.25
40.26
43.77
46.98
50.89
53.67
59.70
31
30.34
32.35
34.60
37.36
41.42
44.99
48.23
52.19
55.00
61.10
32
31.34
33.38
35.66
38.47
42.58
46.19
49.48
53.49
56.33
62.49
33
32.34
34.41
36.73
39.57
43.75
47.40
50.73
54.78
57.65
63.87
34
33.34
35.44
37.80
40.68
44.90
48.60
51.97
56.06
58.96
65.25
35
34.34
36.47
38.86
41.78
46.06
49.80
53.20
57.34
60.27
66.62
Download free eBooks at bookboon.com

Statistics
 
93 
Tables
df
0.500
0.600
0.700
0.800
0.900
0.950
0.975
0.990
0.995
0.999
36
35.34
37.50
39.92
42.88
47.21
51.00
54.44
58.62
61.58
67.99
37
36.34
38.53
40.98
43.98
48.36
52.19
55.67
59.89
62.88
69.35
38
37.34
39.56
42.05
45.08
49.51
53.38
56.90
61.16
64.18
70.70
39
38.34
40.59
43.11
46.17
50.66
54.57
58.12
62.43
65.48
72.05
40
39.34
41.62
44.16
47.27
51.81
55.76
59.34
63.69
66.77
73.40
41
40.34
42.65
45.22
48.36
52.95
56.94
60.56
64.95
68.05
74.74
42
41.34
43.68
46.28
49.46
54.09
58.12
61.78
66.21
69.34
76.08
43
42.34
44.71
47.34
50.55
55.23
59.30
62.99
67.46
70.62
77.42
44
43.34
45.73
48.40
51.64
56.37
60.48
64.20
68.71
71.89
78.75
45
44.34
46.76
49.45
52.73
57.51
61.66
65.41
69.96
73.17
80.08
46
45.34
47.79
50.51
53.82
58.64
62.83
66.62
71.20
74.44
81.40
47
46.34
48.81
51.56
54.91
59.77
64.00
67.82
72.44
75.70
82.72
48
47.34
49.84
52.62
55.99
60.91
65.17
69.02
73.68
76.97
84.04
49
48.33
50.87
53.67
57.08
62.04
66.34
70.22
74.92
78.23
85.35
50
49.33
51.89
54.72
58.16
63.17
67.50
71.42
76.15
79.49
86.66
51
50.33
52.92
55.78
59.25
64.30
68.67
72.62
77.39
80.75
87.97
52
51.33
53.94
56.83
60.33
65.42
69.83
73.81
78.62
82.00
89.27
53
52.33
54.97
57.88
61.41
66.55
70.99
75.00
79.84
83.25
90.57
54
53.33
55.99
58.93
62.50
67.67
72.15
76.19
81.07
84.50
91.87
55
54.33
57.02
59.98
63.58
68.80
73.31
77.38
82.29
85.75
93.17
56
55.33
58.04
61.03
64.66
69.92
74.47
78.57
83.51
86.99
94.46
57
56.33
59.06
62.08
65.74
71.04
75.62
79.75
84.73
88.24
95.75
58
57.33
60.09
63.13
66.82
72.16
76.78
80.94
85.95
89.48
97.04
59
58.33
61.11
64.18
67.89
73.28
77.93
82.12
87.17
90.72
98.32
60
59.33
62.13
65.23
68.97
74.40
79.08
83.30
88.38
91.95
99.61
61
60.33
63.16
66.27
70.05
75.51
80.23
84.48
89.59
93.19
100.89
62
61.33
64.18
67.32
71.13
76.63
81.38
85.65
90.80
94.42
102.17
63
62.33
65.20
68.37
72.20
77.75
82.53
86.83
92.01
95.65
103.44
64
63.33
66.23
69.42
73.28
78.86
83.68
88.00
93.22
96.88
104.72
65
64.33
67.25
70.46
74.35
79.97
84.82
89.18
94.42
98.11
105.99
66
65.33
68.27
71.51
75.42
81.09
85.96
90.35
95.63
99.33
107.26
67
66.33
69.29
72.55
76.50
82.20
87.11
91.52
96.83
100.55
108.53
68
67.33
70.32
73.60
77.57
83.31
88.25
92.69
98.03
101.78
109.79
69
68.33
71.34
74.64
78.64
84.42
89.39
93.86
99.23
103.00
111.06
70
69.33
72.36
75.69
79.71
85.53
90.53
95.02
100.43
104.21
112.32
Download free eBooks at bookboon.com

Statistics
 
94 
Tables
B.4
Studentâ€™s t distribution (values x with FStudent(x) = 0.600 etc.)
df
0.600
0.700
0.800
0.900
0.950
0.975
0.990
0.995
0.999
1
0.32
0.73
1.38
3.08
6.31
12.71
31.82
63.66 318.31
2
0.29
0.62
1.06
1.89
2.92
4.30
6.96
9.92
22.33
3
0.28
0.58
0.98
1.64
2.35
3.18
4.54
5.84
10.2
4
0.27
0.57
0.94
1.53
2.13
2.78
3.75
4.60
7.17
5
0.27
0.56
0.92
1.48
2.02
2.57
3.36
4.03
5.89
6
0.26
0.55
0.91
1.44
1.94
2.45
3.14
3.71
5.21
7
0.26
0.55
0.90
1.41
1.89
2.36
3.00
3.50
4.79
8
0.26
0.55
0.89
1.40
1.86
2.31
2.90
3.36
4.50
9
0.26
0.54
0.88
1.38
1.83
2.26
2.82
3.25
4.30
10
0.26
0.54
0.88
1.37
1.81
2.23
2.76
3.17
4.14
11
0.26
0.54
0.88
1.36
1.80
2.20
2.72
3.11
4.02
12
0.26
0.54
0.87
1.36
1.78
2.18
2.68
3.05
3.93
13
0.26
0.54
0.87
1.35
1.77
2.16
2.65
3.01
3.85
14
0.26
0.54
0.87
1.35
1.76
2.14
2.62
2.98
3.79
15
0.26
0.54
0.87
1.34
1.75
2.13
2.60
2.95
3.73
16
0.26
0.54
0.86
1.34
1.75
2.12
2.58
2.92
3.69
17
0.26
0.53
0.86
1.33
1.74
2.11
2.57
2.90
3.65
18
0.26
0.53
0.86
1.33
1.73
2.10
2.55
2.88
3.61
19
0.26
0.53
0.86
1.33
1.73
2.09
2.54
2.86
3.58
20
0.26
0.53
0.86
1.33
1.72
2.09
2.53
2.85
3.55
21
0.26
0.53
0.86
1.32
1.72
2.08
2.52
2.83
3.53
22
0.26
0.53
0.86
1.32
1.72
2.07
2.51
2.82
3.50
23
0.26
0.53
0.86
1.32
1.71
2.07
2.50
2.81
3.48
24
0.26
0.53
0.86
1.32
1.71
2.06
2.49
2.80
3.47
25
0.26
0.53
0.86
1.32
1.71
2.06
2.49
2.79
3.45
26
0.26
0.53
0.86
1.31
1.71
2.06
2.48
2.78
3.43
27
0.26
0.53
0.86
1.31
1.70
2.05
2.47
2.77
3.42
28
0.26
0.53
0.85
1.31
1.70
2.05
2.47
2.76
3.41
29
0.26
0.53
0.85
1.31
1.70
2.05
2.46
2.76
3.40
30
0.26
0.53
0.85
1.31
1.70
2.04
2.46
2.75
3.39
35
0.26
0.53
0.85
1.31
1.69
2.03
2.44
2.72
3.34
40
0.26
0.53
0.85
1.30
1.68
2.02
2.42
2.70
3.31
50
0.25
0.53
0.85
1.30
1.68
2.01
2.40
2.68
3.26
100
0.25
0.53
0.85
1.29
1.66
1.98
2.36
2.63
3.17
âˆ
0.25
0.52
0.84
1.28
1.64
1.96
2.33
2.58
3.09
Download free eBooks at bookboon.com

Statistics
 
95 
Tables
B.5
Fisherâ€™s F distribution (values x with FFisher(x) = 0.90)
1
2
3
4
5
6
7
8
9
10
1
39.86
49.50
53.59
55.83
57.24
58.20
58.91
59.44
59.86
60.19
2
8.53
9.00
9.16
9.24
9.29
9.33
9.35
9.37
9.38
9.39
3
5.54
5.46
5.39
5.34
5.31
5.28
5.27
5.25
5.24
5.23
4
4.54
4.32
4.19
4.11
4.05
4.01
3.98
3.95
3.94
3.92
5
4.06
3.78
3.62
3.52
3.45
3.40
3.37
3.34
3.32
3.30
6
3.78
3.46
3.29
3.18
3.11
3.05
3.01
2.98
2.96
2.94
7
3.59
3.26
3.07
2.96
2.88
2.83
2.78
2.75
2.72
2.70
8
3.46
3.11
2.92
2.81
2.73
2.67
2.62
2.59
2.56
2.54
9
3.36
3.01
2.81
2.69
2.61
2.55
2.51
2.47
2.44
2.42
10
3.29
2.92
2.73
2.61
2.52
2.46
2.41
2.38
2.35
2.32
11
3.23
2.86
2.66
2.54
2.45
2.39
2.34
2.30
2.27
2.25
12
3.18
2.81
2.61
2.48
2.39
2.33
2.28
2.24
2.21
2.19
13
3.14
2.76
2.56
2.43
2.35
2.28
2.23
2.20
2.16
2.14
14
3.10
2.73
2.52
2.39
2.31
2.24
2.19
2.15
2.12
2.10
15
3.07
2.70
2.49
2.36
2.27
2.21
2.16
2.12
2.09
2.06
16
3.05
2.67
2.46
2.33
2.24
2.18
2.13
2.09
2.06
2.03
17
3.03
2.64
2.44
2.31
2.22
2.15
2.10
2.06
2.03
2.00
18
3.02
2.62
2.42
2.29
2.20
2.13
2.08
2.04
2.00
1.98
19
3.01
2.61
2.40
2.27
2.18
2.11
2.06
2.02
1.98
1.96
20
3.00
2.59
2.38
2.25
2.16
2.09
2.04
2.00
1.96
1.94
21
2.98
2.57
2.36
2.23
2.14
2.08
2.02
1.98
1.95
1.92
22
2.97
2.56
2.35
2.22
2.13
2.06
2.01
1.97
1.93
1.90
23
2.96
2.55
2.34
2.21
2.11
2.05
1.99
1.95
1.92
1.89
24
2.95
2.54
2.33
2.19
2.10
2.04
1.98
1.94
1.91
1.88
25
2.94
2.53
2.32
2.18
2.09
2.02
1.97
1.93
1.89
1.87
26
2.93
2.52
2.31
2.17
2.08
2.01
1.96
1.92
1.88
1.86
27
2.92
2.51
2.30
2.17
2.07
2.00
1.95
1.91
1.87
1.85
28
2.92
2.50
2.29
2.16
2.06
2.00
1.94
1.90
1.87
1.84
29
2.91
2.50
2.28
2.15
2.06
1.99
1.93
1.89
1.86
1.83
30
2.90
2.49
2.28
2.14
2.05
1.98
1.93
1.88
1.85
1.82
31
2.90
2.48
2.27
2.14
2.04
1.97
1.92
1.88
1.84
1.81
32
2.89
2.48
2.26
2.13
2.04
1.97
1.91
1.87
0.84
1.81
33
2.89
2.47
2.26
2.12
2.03
1.96
1.91
1.86
1.83
1.80
34
2.88
2.47
2.25
2.12
2.02
1.96
1.90
1.86
1.82
1.79
35
2.88
2.46
2.25
2.11
2.02
1.95
1.90
1.85
1.82
1.79
Download free eBooks at bookboon.com

Statistics
 
96 
Tables
B.6
Fisherâ€™s F distribution (values x with FFisher(x) = 0.95)
1
2
3
4
5
6
7
8
9
10
1 161.45 199.50 215.71 224.58 230.16 233.99 236.77 238.88 240.54 241.88
2
18.51
19.00
19.16
19.25
19.30
19.33
19.35
19.37
19.38
19.40
3
10.13
9.55
9.28
9.12
9.01
8.94
8.89
8.85
8.81
8.79
4
7.71
6.94
6.59
6.39
6.26
6.16
6.09
6.04
6.00
5.96
5
6.61
5.79
5.41
5.19
5.05
4.95
4.88
4.82
4.77
4.74
6
5.99
5.14
4.76
4.53
4.39
4.28
4.21
4.15
4.10
4.06
7
5.59
4.74
4.35
4.12
3.97
3.87
3.79
3.73
3.68
3.64
8
5.32
4.46
4.07
3.84
3.69
3.58
3.50
3.44
3.39
3.35
9
5.12
4.26
3.86
3.63
3.48
3.37
3.29
3.23
3.18
3.14
10
4.96
4.10
3.71
3.48
3.33
3.22
3.14
3.07
3.02
2.98
11
4.84
3.98
3.59
3.36
3.20
3.09
3.01
2.95
2.90
2.85
12
4.75
3.89
3.49
3.26
3.11
3.00
2.91
2.85
2.80
2.75
13
4.67
3.81
3.41
3.18
3.03
2.92
2.83
2.77
2.71
2.67
14
4.60
3.74
3.34
3.11
2.96
2.85
2.76
2.70
2.65
2.60
15
4.54
3.68
3.29
3.06
2.90
2.79
2.71
2.64
2.59
2.54
16
4.49
3.63
3.24
3.01
2.85
2.74
2.66
2.59
2.54
2.49
17
4.45
3.59
3.20
2.96
2.81
2.70
2.61
2.55
2.49
2.45
18
4.43
3.55
3.16
2.93
2.77
2.66
2.58
2.51
2.46
2.41
19
4.41
3.52
3.13
2.90
2.74
2.63
2.54
2.48
2.42
2.38
20
4.38
3.49
3.10
2.87
2.71
2.60
2.51
2.45
2.39
2.35
21
4.35
3.47
3.07
2.84
2.68
2.57
2.49
2.42
2.37
2.32
22
4.33
3.44
3.05
2.82
2.66
2.55
2.46
2.40
2.34
2.30
23
4.31
3.42
3.03
2.80
2.64
2.53
2.44
2.37
2.32
2.27
24
4.29
3.40
3.01
2.78
2.62
2.51
2.42
2.36
4.62
2.25
25
4.27
3.39
2.99
2.76
2.60
2.49
2.40
2.34
2.28
2.24
26
4.25
3.37
2.98
2.74
2.59
2.47
2.39
2.32
2.27
2.22
27
4.24
3.35
2.96
2.73
2.57
2.46
2.37
2.31
2.25
2.20
28
4.22
3.34
2.95
2.71
2.56
2.45
2.36
2.29
2.24
2.19
29
4.21
3.33
2.93
2.70
2.55
2.43
2.35
2.28
2.22
2.18
30
4.20
3.32
2.92
2.69
2.53
2.42
2.33
2.27
2.21
2.16
31
4.18
3.30
2.91
2.68
2.52
2.41
2.32
2.25
2.20
2.15
32
4.17
3.29
2.90
2.67
2.51
2.40
2.31
2.24
2.19
2.14
33
4.16
3.28
2.89
2.66
2.50
2.39
2.30
2.23
2.18
2.13
34
4.15
3.28
2.88
2.65
2.49
2.38
2.29
2.23
2.17
2.12
35
4.15
3.27
2.87
2.64
2.49
2.37
2.29
2.22
2.16
2.11
Download free eBooks at bookboon.com

Statistics
 
97 
Tables
B.7
Fisherâ€™s F distribution (values x with FFisher(x) = 0.99)
1
2
3
4
5
6
7
8
9
10
1
4052
5000
5403
5625
5764
5859
5928
5981
6022
6056
2
98.50
99.00
99.17
99.25
99.30
99.33
99.36
99.37
99.39
99.40
3
34.12
30.82
29.46
28.71
28.24
27.91
27.67
27.49
27.35
27.23
4
21.20
18.00
16.69
15.98
15.52
15.21
14.98
14.80
14.66
14.55
5
16.26
13.27
12.06
11.39
10.97
10.67
10.46
10.29
10.16
10.05
6
13.75
10.92
9.78
9.15
8.75
8.47
8.26
8.10
7.98
7.87
7
12.25
9.55
8.45
7.85
7.46
7.19
6.99
6.84
6.72
6.62
8
11.26
8.65
7.59
7.01
6.63
6.37
6.18
6.03
5.91
5.81
9
10.56
8.02
6.99
6.42
6.06
5.80
5.61
5.47
5.35
5.26
10
10.04
7.56
6.55
5.99
5.64
5.39
5.20
5.06
4.94
4.85
11
9.65
7.21
6.22
5.67
5.32
5.07
4.89
4.74
4.63
4.54
12
9.33
6.93
5.95
5.41
5.06
4.82
4.64
4.50
4.39
4.30
13
9.07
6.70
5.74
5.21
4.86
4.62
4.44
4.30
4.19
4.10
14
8.86
6.51
5.56
5.04
4.69
4.46
4.28
4.14
4.03
3.94
15
8.68
6.36
5.42
4.89
4.56
4.32
4.14
4.00
3.89
3.80
16
8.53
6.23
5.29
4.77
4.44
4.20
4.03
3.89
3.78
3.69
17
8.40
6.11
5.18
4.67
4.34
4.10
3.93
3.79
3.68
3.59
18
8.30
6.01
5.09
4.58
4.25
4.01
3.84
3.71
3.60
3.51
19
8.22
5.93
5.01
4.50
4.17
3.94
3.77
3.63
3.52
3.43
20
8.13
5.85
4.94
4.43
4.10
3.87
3.70
3.56
3.46
3.37
21
8.05
5.78
4.87
4.37
4.04
3.81
3.64
3.51
3.40
3.31
22
7.98
5.72
4.82
4.31
3.99
3.76
3.59
3.45
3.35
3.26
23
7.91
5.66
4.76
4.26
3.94
3.71
3.54
3.41
3.30
3.21
24
7.85
5.61
4.72
4.22
3.90
3.67
3.50
3.36
3.26
3.17
25
7.80
5.57
4.68
4.18
3.85
3.63
3.46
3.32
3.22
3.13
26
7.75
5.53
4.64
4.14
3.82
3.59
3.42
3.29
3.18
3.09
27
7.71
5.49
4.60
4.11
3.78
3.56
3.39
3.26
3.15
3.06
28
7.67
5.45
4.57
4.07
3.75
3.53
3.36
3.23
3.12
3.03
29
7.63
5.42
4.54
4.04
3.73
3.50
3.33
3.20
3.09
3.00
30
7.59
5.39
4.51
4.02
3.70
3.47
3.30
3.17
3.07
2.98
31
7.56
5.36
4.48
3.99
3.67
3.45
3.28
3.15
3.04
2.96
32
7.53
5.34
4.46
3.97
3.65
3.43
3.26
3.13
3.02
2.93
33
7.50
5.31
4.44
3.95
3.63
3.41
3.24
3.11
3.00
2.91
34
7.47
5.29
4.42
3.93
3.61
3.39
3.22
3.09
2.98
2.89
35
7.45
5.27
4.40
3.91
3.59
3.37
3.20
3.07
2.96
2.88
Download free eBooks at bookboon.com

Statistics
 
98 
Tables
B.8
Wilcoxonâ€™s test for one set of observations
n
0.005
0.010
0.025
0.050
5
âˆ’
âˆ’
âˆ’
0
6
âˆ’
âˆ’
0
2
7
âˆ’
0
2
3
8
0
1
3
5
9
1
3
5
8
10
3
5
8
10
11
5
7
10
13
12
7
9
13
17
13
9
12
17
21
14
12
15
21
25
15
15
19
25
30
16
19
23
29
35
17
23
27
34
41
18
27
32
40
47
19
32
37
46
53
20
37
43
52
60
21
42
49
58
67
22
48
55
65
75
23
54
62
73
83
24
61
69
81
91
25
68
76
89
100
26
75
84
98
110
27
83
92
107
119
n
0.005
0.010
0.025
0.050
28
91
101
116
130
29
100
110
126
140
30
109
120
137
151
31
118
130
147
163
32
128
140
159
175
33
138
151
170
187
34
148
162
182
200
35
159
173
195
213
36
171
185
208
227
37
182
198
221
241
38
194
211
235
256
39
207
224
249
271
40
220
238
264
286
41
233
252
279
302
42
247
266
294
319
43
261
281
310
336
44
276
296
327
353
45
291
312
343
371
46
307
328
361
389
47
322
345
378
407
48
339
362
396
426
49
355
379
415
446
50
373
397
434
466
Download free eBooks at bookboon.com

Statistics
 
99 
Tables
B.9
Wilcoxonâ€™s test for two sets of observations, Î± = 5%
m = 1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
n = 1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2
2
2
2
2
3
3
3
4
4
4
4
5
5
6
6
3
5
5
6
6
7
8
8
9
10
10
11
11
12
13
13
4
9
9
10
11
12
13
14
15
16
17
18
19
20
21
22
5
14
15
16
17
19
20
21
23
24
26
27
28
30
31
33
6
20
21
23
24
26
28
29
31
33
35
37
38
40
42
44
7
27
28
30
32
34
36
39
41
43
45
47
49
52
54
56
8
35
37
39
41
44
46
49
51
54
56
59
62
64
67
69
9
44
46
49
51
54
57
60
63
66
69
72
75
78
81
84
10
54
56
59
62
66
69
72
75
79
82
86
89
92
96
99
11
65
67
71
74
78
82
85
89
93
97
100
104
108
112
116
12
77
80
83
87
91
95
99
104
108
112
116
120
125
129
133
13
90
93
97
101
106
110
115
119
124
128
133
138
142
147
152
14
104
108
112
116
121
126
131
136
141
146
151
156
161
166
171
15
119
123
127
132
138
143
148
153
159
164
170
175
181
186
192
16
135
139
144
150
155
161
166
172
178
184
190
196
201
207
213
17
152
156
162
168
173
179
186
192
198
204
210
217
223
230
236
18
170
175
180
187
193
199
206
212
219
226
232
239
246
253
259
19
190
194
200
207
213
220
227
234
241
248
255
262
270
277
284
20
210
214
221
228
235
242
249
257
264
272
279
287
294
302
310
21
231
236
242
250
257
265
272
280
288
296
304
312
320
328
336
22
253
258
265
273
281
289
297
305
313
321
330
338
347
355
364
23
276
281
289
297
305
313
322
330
339
348
357
366
374
383
392
24
300
306
313
322
330
339
348
357
366
375
385
394
403
413
422
25
325
331
339
348
357
366
375
385
394
404
414
423
433
443
453
Download free eBooks at bookboon.com

Statistics
 
100 
Explanation of symbols
C
Explanation of symbols
A, B, C
events
â„¦
sample space
P
probability function, signiï¬cance probability
P(A|B)
conditional probability of A given B
âˆ©, âˆª
intersection, union
âˆ§, âˆ¨
and, or
A âŠ†â„¦
A is a subset of â„¦
Ï‰ âˆˆâ„¦
Ï‰ belongs to â„¦
âˆA
complement of the set A
A\B
difference of the sets A and B (â€œA minus Bâ€)
f : â„¦â†’R
f is a map from â„¦into R
:=
equals by deï¬nition
|x|
absolute value of x (e.g. | âˆ’2| = 2)
N, Z, R
the set of natural, integral, real numbers
Ã˜
the empty set
[0, âˆ[
the interval {x âˆˆR | x â‰¥0}
X, Y
random variables
E(X)
the expected value of X
var(X)
the variance of X
Cov(X, Y )
the covariance of X and Y
Âµ
expected value
Ïƒ2
variance
Ïƒ
standard deviation
Bin
binomial distribution
Pois
Poisson distribution
Geo
geometrical distribution
HG
hypergeometrical distribution
Mult
multinomial distribution
NB
negative binomial distribution
Exp
exponential distribution
N
normal distribution
s2
empirical variance
s
empirical standard deviation
F(x)
distribution function
f(x)
density function
Î¦(x)
distribution function of standard normal distribution
Ï•(x)
density function of standard normal distribution
n
number of observations or tries
Î»
intensity (in a Poisson process)
R2
determination coefï¬cient
Download free eBooks at bookboon.com

Statistics
 
101 
Explanation of symbols
Ï
correlation coefï¬cient
Â¯x, Â¯y
mean value
df
number of degrees of freedom
Download free eBooks at bookboon.com
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
Click on the ad to read more
 
  
 
                . 

Statistics
 
102 
Index
accept, 29
addition formula, 24
addition formula for normal distribution, 50
alternative hypothesis, 29
analysis of variance, 60
ANOVA, 60
automobiles, 36
auxiliary hypothesis, 59
binomial coefï¬cient, 16
binomial distribution, 30
black balls, 41
Bortkiewicz, Ladislaus von, 69
cabinet, 42
Central Limit Theorem, 26
Chebyshevâ€™s inequality, 26
chi-squared, 63
chi-squared distribution, 50
city council, 42
conditional probability, 12
contingency table, 70
continuous random variable, 20
correlation coefï¬cient, 24
covariance, 24
cumulative probabilities, 32
declaring variable, 79
density, 20
density function, 20
determination coefï¬cient, 81
discrepancy, 64
discrete random variable, 19
disjoint events, 12
distribution function, 18
distribution-free test, 74
division into possible causes, 13
empirical correlation coefï¬cient, 29
empirical covariance, 29
empirical standard deviation, 28
empirical variance, 28
equality of distribution, 63
errors (of type I and II), 30
event, 12
expected number, 64
expected value, 21, 24
exponential distribution, 45
F distribution, 52
Fisherâ€™s exact test, 73
Fisherâ€™s F, 52
Fisher-Behrens problem, 57
French Open, 13
generalized binomial coefï¬cients, 17
geometrical distribution, 39
Gesetz der kleinen Zahlen, 69
goodness of ï¬t, 63
Green Party, 34
hypergeometrical distribution, 41
inclusion-exclusion, 14
independent events, 14
independent random variables, 20
intensity, 35, 45
interquartile range, 28
Law of Large Numbers, 26
linear regression, 79
lower quartile, 28
marginal density, 21
mean, 28
mean value, 28
median, 27
moment of extraordinary beauty, 27
multinomial coefï¬cients, 17
multinomial distribution, 43
multiplication formula, 24
negative binomial distribution, 44
non-parametric test, 74
Index
Download free eBooks at bookboon.com

Statistics
 
103 
Index
normal distribution, 46
normed sum, 26
null hypothesis, 29
observed number, 64
parameter, 30
Pascalâ€™s triangle, 17
Pearsonâ€™s test, 63
pin diagram, 19
point probability, 19
Poisson distribution, 35
pooled variance, 57
predicted value, 80
probability function, 12
probability of failure, 30
probability of success, 30
probability space, 12
Prussian cavalry, 68
quartile, 28
random variables, 18
random vector, 21
rank, 74
red balls, 41
regression, 79
reject, 29
remainder term, 79
residual, 80
response variable, 79
sample point, 12
sample space, 12
school class, 15
signiï¬cance level, 29
signiï¬cance probability, 29
simple linear regression, 79
simultaneous density, 21
simultaneous distribution function, 21
Skoda, 36
spontaneous event, 35, 45
standard deviation, 22
standard normal distribution, 47
standardized residuals, 65, 71
strength, 30
Studentâ€™s t, 51
t distribution, 51
test, 29
throw of a dice, 12
toss of a coin, 27
two-times-two table, 73
type I and II, 30
unbiased estimator, 33
upper quartile, 28
urn, 41
variance, 22, 24
variance between samples, 60
variance within samples, 60
Wilcoxonâ€™s test, 74
Download free eBooks at bookboon.com

